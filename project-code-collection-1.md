# Project Code Snapshot

Generated at 2025-12-21T18:10:03.154Z

---

## backend/admin-dashboard/api/dependencies.py

```py
"""
FastAPI Dependencies - ì˜ì¡´ì„± ì£¼ì…
"""

import os
from functools import lru_cache
from typing import Callable

from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer

from .models.schemas import User, UserRole
from .services.audit_service import AuditService
from .services.auth_service import AuthService
from .services.document_service import DocumentService
from .services.environment_service import EnvironmentService
from .services.script_service import ScriptService
from .services.health_service import HealthService
from .services.data_source_service import DataSourceService
from .services.database_service import DatabaseService
from .services.kafka_service import KafkaService

# OAuth2 ìŠ¤í‚´
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="/api/v1/admin/auth/token")

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = os.environ.get(
    "PROJECT_ROOT",
    os.path.dirname(
        os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
    ),
)
CONFIG_DIR = os.environ.get(
    "ADMIN_CONFIG_DIR",
    os.path.join(os.path.dirname(os.path.dirname(__file__)), "config"),
)
# SECRET_KEYëŠ” ë°˜ë“œì‹œ í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤
# í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” ê°•ë ¥í•œ ëœë¤ í‚¤ë¥¼ ì‚¬ìš©í•˜ì„¸ìš” (ì˜ˆ: openssl rand -hex 32)
_default_secret = "your-secret-key-change-in-production"
SECRET_KEY = os.environ.get("ADMIN_SECRET_KEY", _default_secret)

# í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ê¸°ë³¸ ì‹œí¬ë¦¿ í‚¤ ì‚¬ìš© ì‹œ ê²½ê³ 
if SECRET_KEY == _default_secret:
    import warnings
    warnings.warn(
        "ğŸ”´ SECURITY WARNING: Using default SECRET_KEY! "
        "Set ADMIN_SECRET_KEY environment variable in production. "
        "Generate a secure key with: openssl rand -hex 32",
        UserWarning,
    )


@lru_cache()
def get_auth_service() -> AuthService:
    """ì¸ì¦ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤"""
    return AuthService(
        config_dir=CONFIG_DIR,
        secret_key=SECRET_KEY,
    )


@lru_cache()
def get_environment_service() -> EnvironmentService:
    """í™˜ê²½ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤"""
    return EnvironmentService(
        project_root=PROJECT_ROOT,
        config_dir=CONFIG_DIR,
    )


@lru_cache()
def get_script_service() -> ScriptService:
    """ìŠ¤í¬ë¦½íŠ¸ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤"""
    return ScriptService(
        project_root=PROJECT_ROOT,
        config_dir=CONFIG_DIR,
    )


@lru_cache()
def get_document_service() -> DocumentService:
    """ë¬¸ì„œ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤"""
    return DocumentService(
        project_root=PROJECT_ROOT,
        config_dir=CONFIG_DIR,
    )


@lru_cache()
def get_audit_service() -> AuditService:
    """ê°ì‚¬ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤"""
    return AuditService(config_dir=CONFIG_DIR)


@lru_cache()
def get_health_service() -> HealthService:
    """í—¬ìŠ¤ ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤"""
    return HealthService(
        project_root=PROJECT_ROOT,
        config_dir=CONFIG_DIR,
    )


@lru_cache()
def get_data_source_service() -> DataSourceService:
    """ë°ì´í„° ì†ŒìŠ¤ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤"""
    return DataSourceService(
        project_root=PROJECT_ROOT,
        config_dir=CONFIG_DIR,
    )


@lru_cache()
def get_database_service() -> DatabaseService:
    """ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤"""
    return DatabaseService(
        project_root=PROJECT_ROOT,
        config_dir=CONFIG_DIR,
    )


@lru_cache()
def get_kafka_service() -> KafkaService:
    """Kafka/Redpanda ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤"""
    return KafkaService(
        project_root=PROJECT_ROOT,
        config_dir=CONFIG_DIR,
    )


async def get_current_user(
    token: str = Depends(oauth2_scheme),
    auth_service: AuthService = Depends(get_auth_service),
) -> User:
    """í˜„ì¬ ì¸ì¦ëœ ì‚¬ìš©ì ì¡°íšŒ"""
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )

    token_data = auth_service.verify_token(token)
    if not token_data:
        raise credentials_exception

    user = auth_service.get_user(token_data.user_id)
    if not user:
        raise credentials_exception

    if not user.is_active:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="User account is disabled",
        )

    return user


def require_role(required_role: UserRole) -> Callable:
    """íŠ¹ì • ì—­í•  ì´ìƒ ê¶Œí•œ ìš”êµ¬"""

    async def role_checker(
        current_user: User = Depends(get_current_user),
        auth_service: AuthService = Depends(get_auth_service),
    ) -> User:
        if not auth_service.check_permission(current_user.role, required_role):
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail=f"Requires {required_role.value} permission or higher",
            )
        return current_user

    return role_checker

```

---

## backend/admin-dashboard/api/main.py

```py
"""
Admin Dashboard API - FastAPI ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
"""

import os
from contextlib import asynccontextmanager
from datetime import datetime

from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from fastapi.staticfiles import StaticFiles

from .models.schemas import HealthCheck
from .routers import (
    auth,
    audit,
    documents,
    environments,
    scripts,
    public_auth,
    llm_providers,
    health_monitor,
    data_sources,
    ml_addons,
    ml_training,
    databases,
    kafka,
)

# ë²„ì „ ì •ë³´
VERSION = "1.0.0"


@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬"""
    # ì‹œì‘ ì‹œ
    print(f"ğŸš€ Admin Dashboard API v{VERSION} starting...")
    yield
    # ì¢…ë£Œ ì‹œ
    print("ğŸ‘‹ Admin Dashboard API shutting down...")


# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="NewsInsight Admin Dashboard API",
    description="í†µí•© TUI/Web Admin ëŒ€ì‹œë³´ë“œ API",
    version=VERSION,
    docs_url="/api/v1/admin/docs",
    redoc_url="/api/v1/admin/redoc",
    openapi_url="/api/v1/admin/openapi.json",
    lifespan=lifespan,
)

# CORS ì„¤ì •
CORS_ORIGINS = os.environ.get(
    "CORS_ORIGINS", "http://localhost:3000,http://localhost:5173,http://localhost:8080"
).split(",")

app.add_middleware(
    CORSMiddleware,
    allow_origins=CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ì „ì—­ ì˜ˆì™¸ í•¸ë“¤ëŸ¬
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """ì „ì—­ ì˜ˆì™¸ ì²˜ë¦¬"""
    return JSONResponse(
        status_code=500,
        content={
            "detail": str(exc),
            "type": type(exc).__name__,
        },
    )


# API ë¼ìš°í„° ë“±ë¡
API_PREFIX = "/api/v1/admin"
PUBLIC_API_PREFIX = "/api/v1"

# Admin ì „ìš© ë¼ìš°í„° (/api/v1/admin/...)
app.include_router(auth.router, prefix=API_PREFIX)
app.include_router(environments.router, prefix=API_PREFIX)
app.include_router(scripts.router, prefix=API_PREFIX)
app.include_router(documents.router, prefix=API_PREFIX)
app.include_router(audit.router, prefix=API_PREFIX)
app.include_router(llm_providers.router, prefix=API_PREFIX)
app.include_router(health_monitor.router, prefix=API_PREFIX)
app.include_router(data_sources.router, prefix=API_PREFIX)
app.include_router(ml_addons.router, prefix=API_PREFIX)
app.include_router(ml_training.router, prefix=API_PREFIX)
app.include_router(databases.router, prefix=API_PREFIX)
app.include_router(kafka.router, prefix=API_PREFIX)

# ê³µê°œ ë¼ìš°í„° (/api/v1/auth/...)
app.include_router(public_auth.router, prefix=PUBLIC_API_PREFIX)


# í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
@app.get("/health", response_model=HealthCheck, tags=["Health"])
@app.get(f"{API_PREFIX}/health", response_model=HealthCheck, tags=["Health"])
async def health_check():
    """í—¬ìŠ¤ì²´í¬"""
    return HealthCheck(
        status="healthy",
        version=VERSION,
        timestamp=datetime.utcnow(),
    )


# ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸
@app.get("/", tags=["Root"])
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "name": "NewsInsight Admin Dashboard API",
        "version": VERSION,
        "docs": "/api/v1/admin/docs",
        "health": "/health",
    }


# ì •ì  íŒŒì¼ ì„œë¹™ (Web UI)
WEB_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), "web", "dist")
if os.path.exists(WEB_DIR):
    app.mount("/", StaticFiles(directory=WEB_DIR, html=True), name="static")


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "api.main:app",
        host="0.0.0.0",
        port=int(os.environ.get("PORT", 8888)),
        reload=True,
    )

```

---

## backend/admin-dashboard/api/models/__init__.py

```py
# Admin Dashboard Models

```

---

## backend/admin-dashboard/api/models/schemas.py

```py
"""
Admin Dashboard - Pydantic Schemas
í™˜ê²½, ìŠ¤í¬ë¦½íŠ¸, ë¬¸ì„œ, ê°ì‚¬ ë¡œê·¸ ë“±ì˜ ë°ì´í„° ëª¨ë¸ ì •ì˜
"""

from datetime import datetime
from enum import Enum
from typing import Any, Optional

from pydantic import BaseModel, Field


# ============================================================================
# Enums
# ============================================================================
class EnvironmentType(str, Enum):
    ZEROTRUST = "zerotrust"
    LOCAL = "local"
    GCP = "gcp"
    AWS = "aws"
    PRODUCTION = "production"
    STAGING = "staging"


class RiskLevel(str, Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class TaskStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    SUCCESS = "success"
    FAILED = "failed"
    CANCELLED = "cancelled"


class UserRole(str, Enum):
    USER = "user"  # ì¼ë°˜ ì‚¬ìš©ì (íšŒì›ê°€ì…)
    VIEWER = "viewer"  # ê´€ë¦¬ììš© - ì½ê¸° ì „ìš©
    OPERATOR = "operator"  # ê´€ë¦¬ììš© - ìš´ì˜ì
    ADMIN = "admin"  # ê´€ë¦¬ììš© - ìµœê³  ê´€ë¦¬ì


class ServiceStatus(str, Enum):
    UP = "up"
    DOWN = "down"
    STARTING = "starting"
    STOPPING = "stopping"
    UNKNOWN = "unknown"


# ============================================================================
# Environment / Profile Models
# ============================================================================
class EnvironmentBase(BaseModel):
    name: str = Field(..., description="í™˜ê²½ ì´ë¦„ (ì˜ˆ: zerotrust, local)")
    env_type: EnvironmentType = Field(..., description="í™˜ê²½ íƒ€ì…")
    description: Optional[str] = Field(None, description="í™˜ê²½ ì„¤ëª…")
    compose_file: str = Field(..., description="Docker Compose íŒŒì¼ ê²½ë¡œ")
    env_file: Optional[str] = Field(None, description="í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ ê²½ë¡œ")
    is_active: bool = Field(True, description="í™œì„±í™” ì—¬ë¶€")
    priority: int = Field(0, description="ìš°ì„ ìˆœìœ„ (ë†’ì„ìˆ˜ë¡ ë¨¼ì € í‘œì‹œ)")


class EnvironmentCreate(EnvironmentBase):
    pass


class EnvironmentUpdate(BaseModel):
    name: Optional[str] = None
    description: Optional[str] = None
    compose_file: Optional[str] = None
    env_file: Optional[str] = None
    is_active: Optional[bool] = None
    priority: Optional[int] = None


class Environment(EnvironmentBase):
    id: str = Field(..., description="í™˜ê²½ ID")
    created_at: datetime
    updated_at: datetime

    class Config:
        from_attributes = True


# ============================================================================
# Environment Variable Models
# ============================================================================
class EnvVariableBase(BaseModel):
    key: str = Field(..., description="í™˜ê²½ ë³€ìˆ˜ í‚¤")
    value: str = Field(..., description="í™˜ê²½ ë³€ìˆ˜ ê°’")
    is_secret: bool = Field(False, description="ë¯¼ê° ì •ë³´ ì—¬ë¶€")
    description: Optional[str] = Field(None, description="ë³€ìˆ˜ ì„¤ëª…")


class EnvVariableCreate(EnvVariableBase):
    environment_id: str


class EnvVariableUpdate(BaseModel):
    value: Optional[str] = None
    is_secret: Optional[bool] = None
    description: Optional[str] = None
    comment: Optional[str] = Field(None, description="ë³€ê²½ ì‚¬ìœ ")


class EnvVariable(EnvVariableBase):
    id: str
    environment_id: str
    masked_value: str = Field(..., description="ë§ˆìŠ¤í‚¹ëœ ê°’")
    created_at: datetime
    updated_at: datetime

    class Config:
        from_attributes = True


class EnvVariableHistory(BaseModel):
    id: str
    variable_id: str
    old_value: str
    new_value: str
    changed_by: str
    comment: Optional[str]
    changed_at: datetime


# ============================================================================
# Script / Task Models
# ============================================================================
class ScriptParameter(BaseModel):
    name: str = Field(..., description="íŒŒë¼ë¯¸í„° ì´ë¦„")
    param_type: str = Field(
        "string", description="íŒŒë¼ë¯¸í„° íƒ€ì… (string, boolean, number)"
    )
    required: bool = Field(False, description="í•„ìˆ˜ ì—¬ë¶€")
    default: Optional[Any] = Field(None, description="ê¸°ë³¸ê°’")
    description: Optional[str] = Field(None, description="íŒŒë¼ë¯¸í„° ì„¤ëª…")


class ScriptBase(BaseModel):
    name: str = Field(..., description="ìŠ¤í¬ë¦½íŠ¸ ì´ë¦„")
    description: Optional[str] = Field(None, description="ìŠ¤í¬ë¦½íŠ¸ ì„¤ëª…")
    command: str = Field(..., description="ì‹¤í–‰í•  ëª…ë ¹ì–´")
    working_dir: Optional[str] = Field(None, description="ì‘ì—… ë””ë ‰í† ë¦¬")
    risk_level: RiskLevel = Field(RiskLevel.LOW, description="ìœ„í—˜ë„")
    estimated_duration: Optional[int] = Field(None, description="ì˜ˆìƒ ì†Œìš” ì‹œê°„(ì´ˆ)")
    allowed_environments: list[str] = Field(
        default_factory=list, description="í—ˆìš©ëœ í™˜ê²½ ëª©ë¡"
    )
    required_role: UserRole = Field(UserRole.OPERATOR, description="í•„ìš” ê¶Œí•œ")
    parameters: list[ScriptParameter] = Field(
        default_factory=list, description="íŒŒë¼ë¯¸í„° ìŠ¤í‚¤ë§ˆ"
    )
    pre_hooks: list[str] = Field(default_factory=list, description="ì‹¤í–‰ ì „ í›„í¬")
    post_hooks: list[str] = Field(default_factory=list, description="ì‹¤í–‰ í›„ í›„í¬")
    tags: list[str] = Field(default_factory=list, description="íƒœê·¸")


class ScriptCreate(ScriptBase):
    pass


class ScriptUpdate(BaseModel):
    name: Optional[str] = None
    description: Optional[str] = None
    command: Optional[str] = None
    working_dir: Optional[str] = None
    risk_level: Optional[RiskLevel] = None
    estimated_duration: Optional[int] = None
    allowed_environments: Optional[list[str]] = None
    required_role: Optional[UserRole] = None
    parameters: Optional[list[ScriptParameter]] = None
    pre_hooks: Optional[list[str]] = None
    post_hooks: Optional[list[str]] = None
    tags: Optional[list[str]] = None


class Script(ScriptBase):
    id: str
    created_at: datetime
    updated_at: datetime

    class Config:
        from_attributes = True


# ============================================================================
# Task Execution Models
# ============================================================================
class TaskExecutionRequest(BaseModel):
    script_id: str = Field(..., description="ì‹¤í–‰í•  ìŠ¤í¬ë¦½íŠ¸ ID")
    environment_id: str = Field(..., description="ëŒ€ìƒ í™˜ê²½ ID")
    parameters: dict[str, Any] = Field(
        default_factory=dict, description="ì‹¤í–‰ íŒŒë¼ë¯¸í„°"
    )


class TaskExecution(BaseModel):
    id: str = Field(..., description="ì‹¤í–‰ ID")
    script_id: str
    script_name: str
    environment_id: str
    environment_name: str
    status: TaskStatus
    parameters: dict[str, Any]
    started_at: datetime
    finished_at: Optional[datetime] = None
    executed_by: str
    exit_code: Optional[int] = None
    error_message: Optional[str] = None


class TaskLog(BaseModel):
    execution_id: str
    timestamp: datetime
    level: str  # INFO, WARN, ERROR
    message: str


# ============================================================================
# Service Status Models
# ============================================================================
class ContainerInfo(BaseModel):
    name: str
    image: str
    status: ServiceStatus
    health: Optional[str] = None
    ports: list[str] = Field(default_factory=list)
    created_at: Optional[datetime] = None
    started_at: Optional[datetime] = None


class EnvironmentStatus(BaseModel):
    environment_id: str
    environment_name: str
    containers: list[ContainerInfo]
    total_containers: int
    running_containers: int
    last_deployment: Optional[datetime] = None
    deployed_by: Optional[str] = None


# ============================================================================
# Document Models
# ============================================================================
class DocumentCategory(str, Enum):
    DEPLOYMENT = "deployment"
    TROUBLESHOOTING = "troubleshooting"
    ARCHITECTURE = "architecture"
    RUNBOOK = "runbook"
    GENERAL = "general"


class DocumentBase(BaseModel):
    title: str = Field(..., description="ë¬¸ì„œ ì œëª©")
    file_path: str = Field(..., description="íŒŒì¼ ê²½ë¡œ")
    category: DocumentCategory = Field(DocumentCategory.GENERAL, description="ì¹´í…Œê³ ë¦¬")
    tags: list[str] = Field(default_factory=list, description="íƒœê·¸")
    related_environments: list[str] = Field(
        default_factory=list, description="ê´€ë ¨ í™˜ê²½"
    )
    related_scripts: list[str] = Field(
        default_factory=list, description="ê´€ë ¨ ìŠ¤í¬ë¦½íŠ¸"
    )


class Document(DocumentBase):
    id: str
    content: Optional[str] = Field(None, description="Markdown ë‚´ìš©")
    last_modified: datetime

    class Config:
        from_attributes = True


# ============================================================================
# Audit Log Models
# ============================================================================
class AuditAction(str, Enum):
    LOGIN = "login"
    LOGOUT = "logout"
    VIEW = "view"
    CREATE = "create"
    UPDATE = "update"
    DELETE = "delete"
    EXECUTE = "execute"
    DEPLOY = "deploy"
    ROLLBACK = "rollback"


class AuditLog(BaseModel):
    id: str
    user_id: str
    username: str
    action: AuditAction
    resource_type: str  # environment, script, variable, etc.
    resource_id: Optional[str] = None
    resource_name: Optional[str] = None
    environment_id: Optional[str] = None
    environment_name: Optional[str] = None
    details: dict[str, Any] = Field(default_factory=dict)
    ip_address: Optional[str] = None
    user_agent: Optional[str] = None
    timestamp: datetime
    success: bool = True
    error_message: Optional[str] = None


class AuditLogFilter(BaseModel):
    user_id: Optional[str] = None
    action: Optional[AuditAction] = None
    resource_type: Optional[str] = None
    environment_id: Optional[str] = None
    start_date: Optional[datetime] = None
    end_date: Optional[datetime] = None
    success: Optional[bool] = None


# ============================================================================
# User / Auth Models
# ============================================================================
class UserBase(BaseModel):
    username: str
    email: Optional[str] = None
    role: UserRole = Field(UserRole.USER)
    is_active: bool = True


class UserCreate(UserBase):
    password: str


class UserRegister(BaseModel):
    """ì¼ë°˜ ì‚¬ìš©ì íšŒì›ê°€ì…ìš© ìŠ¤í‚¤ë§ˆ"""

    username: str = Field(
        ..., min_length=3, max_length=50, description="ì‚¬ìš©ìëª… (3-50ì)"
    )
    email: str = Field(..., description="ì´ë©”ì¼ ì£¼ì†Œ")
    password: str = Field(..., min_length=8, description="ë¹„ë°€ë²ˆí˜¸ (8ì ì´ìƒ)")


class User(UserBase):
    id: str
    created_at: datetime
    last_login: Optional[datetime] = None
    password_change_required: bool = Field(False, description="ë¹„ë°€ë²ˆí˜¸ ë³€ê²½ í•„ìš” ì—¬ë¶€")

    class Config:
        from_attributes = True


class SetupStatus(BaseModel):
    """ì´ˆê¸° ì„¤ì • ìƒíƒœ"""

    setup_required: bool = Field(..., description="ì´ˆê¸° ì„¤ì • í•„ìš” ì—¬ë¶€")
    has_users: bool = Field(..., description="ì‚¬ìš©ì ì¡´ì¬ ì—¬ë¶€")
    is_default_admin: bool = Field(False, description="ê¸°ë³¸ ê´€ë¦¬ì ê³„ì • ì‚¬ìš© ì—¬ë¶€")


class Token(BaseModel):
    """Internal token model with both tokens (used by auth service)"""

    access_token: str
    refresh_token: str
    token_type: str = "bearer"
    expires_in: int
    refresh_expires_in: int = 604800  # 7 days in seconds


class TokenResponse(BaseModel):
    """Response model for login/refresh - refresh token is sent via HTTP-Only cookie"""

    access_token: str
    token_type: str = "bearer"
    expires_in: int


class TokenData(BaseModel):
    user_id: str
    username: str
    role: UserRole
    exp: datetime


# ============================================================================
# Response Models
# ============================================================================
class PaginatedResponse(BaseModel):
    items: list[Any]
    total: int
    page: int
    page_size: int
    total_pages: int


class HealthCheck(BaseModel):
    status: str = "healthy"
    version: str
    timestamp: datetime


# ============================================================================
# Service Health Monitoring Models
# ============================================================================
class ServiceHealthStatus(str, Enum):
    HEALTHY = "healthy"
    UNHEALTHY = "unhealthy"
    DEGRADED = "degraded"
    UNREACHABLE = "unreachable"
    UNKNOWN = "unknown"


class ServiceHealth(BaseModel):
    service_id: str = Field(..., description="ì„œë¹„ìŠ¤ ID")
    name: str = Field(..., description="ì„œë¹„ìŠ¤ ì´ë¦„")
    status: ServiceHealthStatus = Field(..., description="í—¬ìŠ¤ ìƒíƒœ")
    message: Optional[str] = Field(default=None, description="ìƒíƒœ ë©”ì‹œì§€")
    response_time_ms: Optional[float] = Field(default=None, description="ì‘ë‹µ ì‹œê°„(ms)")
    url: Optional[str] = Field(default=None, description="í—¬ìŠ¤ì²´í¬ URL")
    checked_at: datetime = Field(..., description="ì²´í¬ ì‹œê°„")
    details: Optional[dict[str, Any]] = Field(default=None, description="ìƒì„¸ ì •ë³´")

    class Config:
        from_attributes = True


class InfrastructureHealth(BaseModel):
    service_id: str = Field(..., description="ì¸í”„ë¼ ì„œë¹„ìŠ¤ ID")
    name: str = Field(..., description="ì¸í”„ë¼ ì´ë¦„")
    status: ServiceHealthStatus = Field(..., description="í—¬ìŠ¤ ìƒíƒœ")
    message: Optional[str] = Field(default=None, description="ìƒíƒœ ë©”ì‹œì§€")
    port: Optional[int] = Field(default=None, description="í¬íŠ¸")
    checked_at: datetime = Field(..., description="ì²´í¬ ì‹œê°„")
    details: Optional[dict[str, Any]] = Field(default=None, description="ìƒì„¸ ì •ë³´")

    class Config:
        from_attributes = True


class OverallSystemHealth(BaseModel):
    status: ServiceHealthStatus = Field(..., description="ì „ì²´ ì‹œìŠ¤í…œ ìƒíƒœ")
    total_services: int = Field(..., description="ì „ì²´ ì„œë¹„ìŠ¤ ìˆ˜")
    healthy_services: int = Field(..., description="ì •ìƒ ì„œë¹„ìŠ¤ ìˆ˜")
    unhealthy_services: int = Field(..., description="ë¹„ì •ìƒ ì„œë¹„ìŠ¤ ìˆ˜")
    degraded_services: int = Field(..., description="ì €í•˜ ì„œë¹„ìŠ¤ ìˆ˜")
    total_infrastructure: int = Field(..., description="ì „ì²´ ì¸í”„ë¼ ìˆ˜")
    healthy_infrastructure: int = Field(..., description="ì •ìƒ ì¸í”„ë¼ ìˆ˜")
    average_response_time_ms: Optional[float] = Field(
        None, description="í‰ê·  ì‘ë‹µ ì‹œê°„"
    )
    services: list[ServiceHealth] = Field(
        default_factory=list, description="ì„œë¹„ìŠ¤ í—¬ìŠ¤ ëª©ë¡"
    )
    infrastructure: list[InfrastructureHealth] = Field(
        default_factory=list, description="ì¸í”„ë¼ í—¬ìŠ¤ ëª©ë¡"
    )
    checked_at: datetime = Field(..., description="ì²´í¬ ì‹œê°„")

    class Config:
        from_attributes = True


class ServiceMetrics(BaseModel):
    service_id: str
    cpu_usage_percent: Optional[float] = None
    memory_usage_mb: Optional[float] = None
    memory_limit_mb: Optional[float] = None
    request_count: Optional[int] = None
    error_count: Optional[int] = None
    avg_response_time_ms: Optional[float] = None
    collected_at: datetime

    class Config:
        from_attributes = True


class ServiceInfo(BaseModel):
    id: str = Field(..., description="ì„œë¹„ìŠ¤ ID")
    name: str = Field(..., description="ì„œë¹„ìŠ¤ ì´ë¦„")
    description: Optional[str] = Field(None, description="ì„¤ëª…")
    port: Optional[int] = Field(None, description="í¬íŠ¸")
    healthcheck: str = Field("/health", description="í—¬ìŠ¤ì²´í¬ ê²½ë¡œ")
    hostname: str = Field(..., description="í˜¸ìŠ¤íŠ¸ëª…")
    type: str = Field(..., description="ì„œë¹„ìŠ¤ íƒ€ì…")
    tags: list[str] = Field(default_factory=list, description="íƒœê·¸")


# ============================================================================
# Data Source Management Models
# ============================================================================
class DataSourceType(str, Enum):
    RSS = "rss"
    WEB = "web"
    API = "api"
    SOCIAL = "social"


class DataSourceStatus(str, Enum):
    ACTIVE = "active"
    INACTIVE = "inactive"
    ERROR = "error"
    TESTING = "testing"


class DataSourceBase(BaseModel):
    name: str = Field(..., description="ì†ŒìŠ¤ ì´ë¦„")
    source_type: DataSourceType = Field(..., description="ì†ŒìŠ¤ íƒ€ì…")
    url: str = Field(..., description="ì†ŒìŠ¤ URL")
    description: Optional[str] = Field(None, description="ì„¤ëª…")
    category: Optional[str] = Field(None, description="ì¹´í…Œê³ ë¦¬")
    language: str = Field("ko", description="ì–¸ì–´")
    is_active: bool = Field(True, description="í™œì„±í™” ì—¬ë¶€")
    crawl_interval_minutes: int = Field(60, description="ìˆ˜ì§‘ ì£¼ê¸°(ë¶„)")
    priority: int = Field(0, description="ìš°ì„ ìˆœìœ„")
    config: dict[str, Any] = Field(default_factory=dict, description="ì¶”ê°€ ì„¤ì •")


class DataSourceCreate(DataSourceBase):
    pass


class DataSourceUpdate(BaseModel):
    name: Optional[str] = None
    description: Optional[str] = None
    url: Optional[str] = None
    category: Optional[str] = None
    is_active: Optional[bool] = None
    crawl_interval_minutes: Optional[int] = None
    priority: Optional[int] = None
    config: Optional[dict[str, Any]] = None


class DataSource(DataSourceBase):
    id: str = Field(..., description="ì†ŒìŠ¤ ID")
    status: DataSourceStatus = Field(
        default=DataSourceStatus.ACTIVE, description="ìƒíƒœ"
    )
    last_crawled_at: Optional[datetime] = Field(
        default=None, description="ë§ˆì§€ë§‰ ìˆ˜ì§‘ ì‹œê°„"
    )
    total_articles: int = Field(default=0, description="ì´ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜")
    success_rate: float = Field(default=100.0, description="ì„±ê³µë¥ ")
    created_at: datetime
    updated_at: datetime

    class Config:
        from_attributes = True


class DataSourceStats(BaseModel):
    source_id: str
    total_crawls: int = 0
    successful_crawls: int = 0
    failed_crawls: int = 0
    total_articles: int = 0
    avg_articles_per_crawl: float = 0.0
    last_error: Optional[str] = None
    last_error_at: Optional[datetime] = None


class DataSourceTestResult(BaseModel):
    source_id: str
    success: bool
    message: str
    response_time_ms: Optional[float] = None
    sample_data: Optional[dict[str, Any]] = None
    tested_at: datetime


# ============================================================================
# Database Management Models
# ============================================================================
class DatabaseType(str, Enum):
    POSTGRESQL = "postgresql"
    MONGODB = "mongodb"
    REDIS = "redis"


class DatabaseInfo(BaseModel):
    db_type: DatabaseType
    name: str
    host: str
    port: int
    status: ServiceHealthStatus
    version: Optional[str] = None
    size_bytes: Optional[int] = None
    size_human: Optional[str] = None
    connection_count: Optional[int] = None
    max_connections: Optional[int] = None
    uptime_seconds: Optional[int] = None
    checked_at: datetime


class PostgresTableInfo(BaseModel):
    schema_name: str
    table_name: str
    row_count: int
    size_bytes: int
    size_human: str
    index_size_bytes: Optional[int] = None
    last_vacuum: Optional[datetime] = None
    last_analyze: Optional[datetime] = None


class PostgresDatabaseStats(BaseModel):
    database_name: str
    size_bytes: int
    size_human: str
    tables: list[PostgresTableInfo]
    total_tables: int
    total_rows: int
    connection_count: int
    max_connections: int
    checked_at: datetime


class MongoCollectionInfo(BaseModel):
    collection_name: str
    document_count: int
    size_bytes: int
    size_human: str
    avg_document_size_bytes: Optional[int] = None
    index_count: int
    total_index_size_bytes: Optional[int] = None


class MongoDatabaseStats(BaseModel):
    database_name: str
    size_bytes: int
    size_human: str
    collections: list[MongoCollectionInfo]
    total_collections: int
    total_documents: int
    checked_at: datetime


class RedisStats(BaseModel):
    used_memory_bytes: int
    used_memory_human: str
    max_memory_bytes: Optional[int] = None
    connected_clients: int
    total_keys: int
    expired_keys: int
    keyspace_hits: int
    keyspace_misses: int
    hit_rate: float
    uptime_seconds: int
    checked_at: datetime


# ============================================================================
# Kafka/Redpanda Management Models
# ============================================================================
class KafkaTopicInfo(BaseModel):
    name: str
    partition_count: int
    replication_factor: int
    message_count: Optional[int] = None
    size_bytes: Optional[int] = None
    retention_ms: Optional[int] = None
    is_internal: bool = False


class KafkaConsumerGroupInfo(BaseModel):
    group_id: str
    state: str
    members_count: int
    topics: list[str]
    total_lag: int
    lag_per_partition: dict[str, int]


class KafkaClusterInfo(BaseModel):
    broker_count: int
    controller_id: Optional[int] = None
    cluster_id: Optional[str] = None
    topics: list[KafkaTopicInfo]
    consumer_groups: list[KafkaConsumerGroupInfo]
    total_topics: int
    total_partitions: int
    total_messages: Optional[int] = None
    checked_at: datetime

```

---

## backend/admin-dashboard/api/routers/__init__.py

```py
# Admin Dashboard Routers
from . import (
    auth,
    audit,
    documents,
    environments,
    scripts,
    public_auth,
    llm_providers,
    health_monitor,
    data_sources,
    ml_addons,
    ml_training,
    databases,
    kafka,
)

```

---

## backend/admin-dashboard/api/routers/audit.py

```py
"""
Audit Router - ê°ì‚¬ ë¡œê·¸ API ì—”ë“œí¬ì¸íŠ¸
"""

import asyncio
import json
from datetime import datetime
from typing import AsyncGenerator, Optional

from fastapi import APIRouter, Depends, HTTPException, Query
from fastapi.responses import StreamingResponse

from ..models.schemas import AuditAction, AuditLog, AuditLogFilter, UserRole
from ..dependencies import get_audit_service, get_current_user, require_role

router = APIRouter(prefix="/audit", tags=["Audit Logs"])


# ============================================
# SSE Event Stream for Real-time Activity
# ============================================


async def activity_event_generator(
    audit_service,
    last_timestamp: Optional[datetime] = None,
) -> AsyncGenerator[str, None]:
    """
    Server-Sent Events generator for real-time activity stream.
    Polls for new audit logs and sends them as events.
    """
    poll_interval = 5  # seconds
    seen_ids = set()

    # Initialize with recent logs if no timestamp provided
    if last_timestamp is None:
        recent_logs, _ = audit_service.get_logs(page=1, page_size=20)
        for log in recent_logs:
            seen_ids.add(log.id)

    while True:
        try:
            # Get recent logs
            logs, _ = audit_service.get_logs(page=1, page_size=50)

            # Find new logs
            new_logs = []
            for log in logs:
                if log.id not in seen_ids:
                    new_logs.append(log)
                    seen_ids.add(log.id)

            # Send new logs as events
            for log in reversed(new_logs):  # Send oldest first
                event_data = {
                    "eventType": "activity",
                    "timestamp": log.timestamp.isoformat(),
                    "data": {
                        "id": log.id,
                        "userId": log.user_id,
                        "username": log.username,
                        "action": log.action.value,
                        "resourceType": log.resource_type,
                        "resourceId": log.resource_id,
                        "resourceName": log.resource_name,
                        "environmentId": log.environment_id,
                        "environmentName": log.environment_name,
                        "success": log.success,
                        "errorMessage": log.error_message,
                        "timestamp": log.timestamp.isoformat(),
                    },
                }
                yield f"event: activity\ndata: {json.dumps(event_data, ensure_ascii=False)}\n\n"

            # Send heartbeat
            heartbeat_data = {
                "eventType": "heartbeat",
                "timestamp": datetime.utcnow().isoformat(),
            }
            yield f"event: heartbeat\ndata: {json.dumps(heartbeat_data)}\n\n"

            # Limit seen_ids to prevent memory growth
            if len(seen_ids) > 1000:
                seen_ids = set(list(seen_ids)[-500:])

            await asyncio.sleep(poll_interval)

        except asyncio.CancelledError:
            break
        except Exception as e:
            error_data = {"eventType": "error", "message": str(e)}
            yield f"event: error\ndata: {json.dumps(error_data)}\n\n"
            await asyncio.sleep(poll_interval)


@router.get("/activity/stream")
async def stream_activity_events(
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """
    SSE endpoint for real-time activity events.

    Returns a Server-Sent Events stream with:
    - activity: New audit log entries
    - heartbeat: Keep-alive signal (every 5 seconds)
    - error: Error notifications

    Requires: OPERATOR role or higher
    """
    return StreamingResponse(
        activity_event_generator(audit_service),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # Disable nginx buffering
        },
    )


@router.get("/activity/recent")
async def get_recent_activity(
    limit: int = Query(20, ge=1, le=100, description="ìµœê·¼ í™œë™ ê°œìˆ˜"),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """
    ìµœê·¼ í™œë™ ëª©ë¡ ì¡°íšŒ (SSE ëŒ€ì•ˆ).
    í´ë§ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    Requires: OPERATOR role or higher
    """
    logs, total = audit_service.get_logs(page=1, page_size=limit)

    return {
        "activities": [
            {
                "id": log.id,
                "userId": log.user_id,
                "username": log.username,
                "action": log.action.value,
                "resourceType": log.resource_type,
                "resourceId": log.resource_id,
                "resourceName": log.resource_name,
                "environmentId": log.environment_id,
                "environmentName": log.environment_name,
                "success": log.success,
                "errorMessage": log.error_message,
                "timestamp": log.timestamp.isoformat(),
            }
            for log in logs
        ],
        "total": total,
    }


# ============================================
# Original Audit Endpoints
# ============================================


@router.get("/logs", response_model=list[AuditLog])
async def list_audit_logs(
    user_id: Optional[str] = Query(None, description="ì‚¬ìš©ì ID í•„í„°"),
    action: Optional[AuditAction] = Query(None, description="ì•¡ì…˜ í•„í„°"),
    resource_type: Optional[str] = Query(None, description="ë¦¬ì†ŒìŠ¤ íƒ€ì… í•„í„°"),
    environment_id: Optional[str] = Query(None, description="í™˜ê²½ ID í•„í„°"),
    start_date: Optional[datetime] = Query(None, description="ì‹œì‘ ë‚ ì§œ"),
    end_date: Optional[datetime] = Query(None, description="ì¢…ë£Œ ë‚ ì§œ"),
    success: Optional[bool] = Query(None, description="ì„±ê³µ/ì‹¤íŒ¨ í•„í„°"),
    page: int = Query(1, ge=1, description="í˜ì´ì§€ ë²ˆí˜¸"),
    page_size: int = Query(50, ge=1, le=200, description="í˜ì´ì§€ í¬ê¸°"),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """ê°ì‚¬ ë¡œê·¸ ì¡°íšŒ (Operator ì´ìƒ ê¶Œí•œ í•„ìš”)"""
    filter_params = AuditLogFilter(
        user_id=user_id,
        action=action,
        resource_type=resource_type,
        environment_id=environment_id,
        start_date=start_date,
        end_date=end_date,
        success=success,
    )

    logs, total = audit_service.get_logs(
        filter_params=filter_params,
        page=page,
        page_size=page_size,
    )

    return logs


@router.get("/logs/count")
async def get_audit_logs_count(
    user_id: Optional[str] = Query(None),
    action: Optional[AuditAction] = Query(None),
    resource_type: Optional[str] = Query(None),
    environment_id: Optional[str] = Query(None),
    start_date: Optional[datetime] = Query(None),
    end_date: Optional[datetime] = Query(None),
    success: Optional[bool] = Query(None),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """ê°ì‚¬ ë¡œê·¸ ì´ ê°œìˆ˜ ì¡°íšŒ"""
    filter_params = AuditLogFilter(
        user_id=user_id,
        action=action,
        resource_type=resource_type,
        environment_id=environment_id,
        start_date=start_date,
        end_date=end_date,
        success=success,
    )

    _, total = audit_service.get_logs(
        filter_params=filter_params,
        page=1,
        page_size=1,
    )

    return {"total": total}


@router.get("/logs/{log_id}", response_model=AuditLog)
async def get_audit_log(
    log_id: str,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """ê°ì‚¬ ë¡œê·¸ ìƒì„¸ ì¡°íšŒ"""
    log = audit_service.get_log_by_id(log_id)
    if not log:
        raise HTTPException(status_code=404, detail="Audit log not found")
    return log


@router.get("/users/{user_id}/activity", response_model=list[AuditLog])
async def get_user_activity(
    user_id: str,
    limit: int = Query(100, ge=1, le=500),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """íŠ¹ì • ì‚¬ìš©ì í™œë™ ì´ë ¥ ì¡°íšŒ (Admin ê¶Œí•œ í•„ìš”)"""
    return audit_service.get_user_activity(user_id, limit=limit)


@router.get(
    "/resources/{resource_type}/{resource_id}/history", response_model=list[AuditLog]
)
async def get_resource_history(
    resource_type: str,
    resource_id: str,
    limit: int = Query(100, ge=1, le=500),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """ë¦¬ì†ŒìŠ¤ ë³€ê²½ ì´ë ¥ ì¡°íšŒ"""
    return audit_service.get_resource_history(resource_type, resource_id, limit=limit)


@router.get("/statistics")
async def get_audit_statistics(
    start_date: Optional[datetime] = Query(None, description="ì‹œì‘ ë‚ ì§œ"),
    end_date: Optional[datetime] = Query(None, description="ì¢…ë£Œ ë‚ ì§œ"),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ê°ì‚¬ ë¡œê·¸ í†µê³„ (Admin ê¶Œí•œ í•„ìš”)"""
    return audit_service.get_statistics(start_date=start_date, end_date=end_date)


@router.delete("/logs/cleanup")
async def cleanup_old_logs(
    days: int = Query(90, ge=30, le=365, description="ë³´ê´€ ê¸°ê°„ (ì¼)"),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ì˜¤ë˜ëœ ë¡œê·¸ ì •ë¦¬ (Admin ê¶Œí•œ í•„ìš”)"""
    deleted_count = audit_service.clear_old_logs(days=days)
    return {
        "success": True,
        "message": f"Deleted {deleted_count} old logs (older than {days} days)",
        "deleted_count": deleted_count,
    }

```

---

## backend/admin-dashboard/api/routers/auth.py

```py
"""
Auth Router - ì¸ì¦/ê¶Œí•œ API ì—”ë“œí¬ì¸íŠ¸
"""

import os
from typing import Optional

from fastapi import APIRouter, Cookie, Depends, HTTPException, Request, Response, status
from fastapi.security import OAuth2PasswordRequestForm
from pydantic import BaseModel

from ..models.schemas import (
    AuditAction,
    Token,
    TokenResponse,
    User,
    UserCreate,
    UserRole,
    SetupStatus,
)
from ..dependencies import (
    get_audit_service,
    get_auth_service,
    get_current_user,
    require_role,
)

router = APIRouter(prefix="/auth", tags=["Authentication"])

# Cookie settings
REFRESH_TOKEN_COOKIE_NAME = "refresh_token"
# In production, set SECURE_COOKIES=true in environment
SECURE_COOKIES = os.getenv("SECURE_COOKIES", "false").lower() == "true"
# Cookie max age: 7 days in seconds
REFRESH_TOKEN_MAX_AGE = 7 * 24 * 60 * 60


def set_refresh_token_cookie(response: Response, refresh_token: str) -> None:
    """Set refresh token as HTTP-Only cookie"""
    response.set_cookie(
        key=REFRESH_TOKEN_COOKIE_NAME,
        value=refresh_token,
        httponly=True,
        secure=SECURE_COOKIES,  # HTTPS only in production
        samesite="lax",  # CSRF protection
        max_age=REFRESH_TOKEN_MAX_AGE,
        path="/api/v1/admin/auth",  # Restrict to auth endpoints only
    )


def clear_refresh_token_cookie(response: Response) -> None:
    """Clear the refresh token cookie"""
    response.delete_cookie(
        key=REFRESH_TOKEN_COOKIE_NAME,
        path="/api/v1/admin/auth",
        httponly=True,
        secure=SECURE_COOKIES,
        samesite="lax",
    )


class LoginRequest(BaseModel):
    username: str
    password: str


class ChangePasswordRequest(BaseModel):
    old_password: str
    new_password: str


class ResetPasswordRequest(BaseModel):
    new_password: str


class UpdateUserRequest(BaseModel):
    email: Optional[str] = None
    role: Optional[UserRole] = None
    is_active: Optional[bool] = None


class RefreshTokenRequest(BaseModel):
    """Optional body for refresh - prefer using HTTP-Only cookie"""

    refresh_token: Optional[str] = None


@router.post("/login", response_model=TokenResponse)
async def login(
    response: Response,
    form_data: OAuth2PasswordRequestForm = Depends(),
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """ë¡œê·¸ì¸ - ë¦¬í”„ë ˆì‹œ í† í°ì€ HTTP-Only ì¿ í‚¤ë¡œ ì „ì†¡ë©ë‹ˆë‹¤"""
    user = auth_service.authenticate(form_data.username, form_data.password)

    if not user:
        # ì‹¤íŒ¨ ë¡œê·¸
        audit_service.log(
            user_id="unknown",
            username=form_data.username,
            action=AuditAction.LOGIN,
            resource_type="auth",
            success=False,
            error_message="Invalid credentials",
        )
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid username or password",
            headers={"WWW-Authenticate": "Bearer"},
        )

    token = auth_service.create_access_token(user)

    # Set refresh token as HTTP-Only cookie
    set_refresh_token_cookie(response, token.refresh_token)

    # ì„±ê³µ ë¡œê·¸
    audit_service.log(
        user_id=user.id,
        username=user.username,
        action=AuditAction.LOGIN,
        resource_type="auth",
        success=True,
    )

    # Return only access token in body (refresh token is in cookie)
    return TokenResponse(
        access_token=token.access_token,
        token_type=token.token_type,
        expires_in=token.expires_in,
    )


@router.post("/token", response_model=TokenResponse)
async def login_for_access_token(
    response: Response,
    form_data: OAuth2PasswordRequestForm = Depends(),
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """OAuth2 í˜¸í™˜ í† í° ì—”ë“œí¬ì¸íŠ¸ - ë¦¬í”„ë ˆì‹œ í† í°ì€ HTTP-Only ì¿ í‚¤ë¡œ ì „ì†¡ë©ë‹ˆë‹¤"""
    return await login(response, form_data, auth_service, audit_service)


@router.get("/me", response_model=User)
async def get_current_user_info(
    current_user=Depends(get_current_user),
):
    """í˜„ì¬ ë¡œê·¸ì¸í•œ ì‚¬ìš©ì ì •ë³´"""
    return current_user


@router.post("/refresh", response_model=TokenResponse)
async def refresh_token(
    response: Response,
    request: Optional[RefreshTokenRequest] = None,
    refresh_token_cookie: Optional[str] = Cookie(None, alias="refresh_token"),
    auth_service=Depends(get_auth_service),
):
    """ë¦¬í”„ë ˆì‹œ í† í°ìœ¼ë¡œ ìƒˆ ì•¡ì„¸ìŠ¤ í† í° ë°œê¸‰

    ë¦¬í”„ë ˆì‹œ í† í°ì€ HTTP-Only ì¿ í‚¤ì—ì„œ ìë™ìœ¼ë¡œ ì½ìŠµë‹ˆë‹¤.
    ì¿ í‚¤ê°€ ì—†ëŠ” ê²½ìš° ìš”ì²­ ë³¸ë¬¸ì˜ refresh_tokenì„ ì‚¬ìš©í•©ë‹ˆë‹¤ (í•˜ìœ„ í˜¸í™˜ì„±).

    - ìƒˆ ë¦¬í”„ë ˆì‹œ í† í°ì€ HTTP-Only ì¿ í‚¤ë¡œ ì„¤ì •ë©ë‹ˆë‹¤
    - ê¸°ì¡´ ë¦¬í”„ë ˆì‹œ í† í°ì€ ì‚¬ìš© í›„ íê¸°ë©ë‹ˆë‹¤ (Rotation)
    """
    # Prefer cookie over body
    token_to_use = refresh_token_cookie
    if not token_to_use and request and request.refresh_token:
        token_to_use = request.refresh_token

    if not token_to_use:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Refresh token not provided",
            headers={"WWW-Authenticate": "Bearer"},
        )

    new_token = auth_service.refresh_access_token(token_to_use)

    if not new_token:
        # Clear invalid cookie
        clear_refresh_token_cookie(response)
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid or expired refresh token",
            headers={"WWW-Authenticate": "Bearer"},
        )

    # Set new refresh token as HTTP-Only cookie
    set_refresh_token_cookie(response, new_token.refresh_token)

    # Return only access token in body
    return TokenResponse(
        access_token=new_token.access_token,
        token_type=new_token.token_type,
        expires_in=new_token.expires_in,
    )


@router.post("/logout")
async def logout(
    response: Response,
    current_user=Depends(get_current_user),
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """ë¡œê·¸ì•„ì›ƒ - ì‚¬ìš©ìì˜ ëª¨ë“  ë¦¬í”„ë ˆì‹œ í† í° íê¸° ë° ì¿ í‚¤ ì‚­ì œ"""
    # ëª¨ë“  ë¦¬í”„ë ˆì‹œ í† í° íê¸° (Redisì—ì„œ)
    auth_service.revoke_all_user_tokens(current_user.id)

    # HTTP-Only ì¿ í‚¤ ì‚­ì œ
    clear_refresh_token_cookie(response)

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.LOGOUT,
        resource_type="auth",
    )

    return {"success": True, "message": "Logged out successfully"}


@router.post("/change-password")
async def change_password(
    request: ChangePasswordRequest,
    current_user=Depends(get_current_user),
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """ë¹„ë°€ë²ˆí˜¸ ë³€ê²½"""
    success = auth_service.change_password(
        user_id=current_user.id,
        old_password=request.old_password,
        new_password=request.new_password,
    )

    if not success:
        audit_service.log(
            user_id=current_user.id,
            username=current_user.username,
            action=AuditAction.UPDATE,
            resource_type="password",
            success=False,
            error_message="Invalid old password",
        )
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid old password",
        )

    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="password",
        success=True,
    )

    return {"success": True, "message": "Password changed successfully"}


# ============================================================================
# User Management (Admin only)
# ============================================================================
@router.get("/users", response_model=list[User])
async def list_users(
    active_only: bool = False,
    auth_service=Depends(get_auth_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ì‚¬ìš©ì ëª©ë¡ ì¡°íšŒ (Admin ê¶Œí•œ í•„ìš”)"""
    return auth_service.list_users(active_only=active_only)


@router.get("/users/{user_id}", response_model=User)
async def get_user(
    user_id: str,
    auth_service=Depends(get_auth_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ì‚¬ìš©ì ì¡°íšŒ (Admin ê¶Œí•œ í•„ìš”)"""
    user = auth_service.get_user(user_id)
    if not user:
        raise HTTPException(status_code=404, detail="User not found")
    return user


@router.post("/users", response_model=User, status_code=status.HTTP_201_CREATED)
async def create_user(
    data: UserCreate,
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ì‚¬ìš©ì ìƒì„± (Admin ê¶Œí•œ í•„ìš”)"""
    try:
        user = auth_service.create_user(data)

        audit_service.log(
            user_id=current_user.id,
            username=current_user.username,
            action=AuditAction.CREATE,
            resource_type="user",
            resource_id=user.id,
            resource_name=user.username,
            details={"role": data.role.value},
        )

        return user
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.patch("/users/{user_id}", response_model=User)
async def update_user(
    user_id: str,
    data: UpdateUserRequest,
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ì‚¬ìš©ì ì •ë³´ ìˆ˜ì • (Admin ê¶Œí•œ í•„ìš”)"""
    user = auth_service.update_user(
        user_id=user_id,
        email=data.email,
        role=data.role,
        is_active=data.is_active,
    )

    if not user:
        raise HTTPException(status_code=404, detail="User not found")

    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="user",
        resource_id=user_id,
        resource_name=user.username,
        details=data.model_dump(exclude_unset=True),
    )

    return user


@router.post("/users/{user_id}/reset-password")
async def reset_user_password(
    user_id: str,
    request: ResetPasswordRequest,
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ì‚¬ìš©ì ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™” (Admin ê¶Œí•œ í•„ìš”)"""
    user = auth_service.get_user(user_id)
    if not user:
        raise HTTPException(status_code=404, detail="User not found")

    success = auth_service.reset_password(user_id, request.new_password)

    if not success:
        raise HTTPException(status_code=500, detail="Failed to reset password")

    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="user",
        resource_id=user_id,
        resource_name=user.username,
        details={"action": "password_reset"},
    )

    return {"success": True, "message": "Password reset successfully"}


@router.delete("/users/{user_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_user(
    user_id: str,
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ì‚¬ìš©ì ì‚­ì œ (Admin ê¶Œí•œ í•„ìš”)"""
    # ìê¸° ìì‹ ì€ ì‚­ì œ ë¶ˆê°€
    if user_id == current_user.id:
        raise HTTPException(status_code=400, detail="Cannot delete yourself")

    user = auth_service.get_user(user_id)
    if not user:
        raise HTTPException(status_code=404, detail="User not found")

    if not auth_service.delete_user(user_id):
        raise HTTPException(status_code=500, detail="Failed to delete user")

    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DELETE,
        resource_type="user",
        resource_id=user_id,
        resource_name=user.username,
    )


# ============================================================================
# Setup Status (Public - no auth required)
# ============================================================================
@router.get("/setup-status", response_model=SetupStatus)
async def get_setup_status(
    auth_service=Depends(get_auth_service),
):
    """ì´ˆê¸° ì„¤ì • ìƒíƒœ í™•ì¸ (ì¸ì¦ ë¶ˆí•„ìš”)

    ì‹œìŠ¤í…œì´ ì´ˆê¸° ì„¤ì •ì´ í•„ìš”í•œ ìƒíƒœì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    - setup_required: ì´ˆê¸° ì„¤ì •ì´ í•„ìš”í•œì§€ ì—¬ë¶€
    - has_users: ì‚¬ìš©ìê°€ ì¡´ì¬í•˜ëŠ”ì§€ ì—¬ë¶€
    - is_default_admin: ê¸°ë³¸ ê´€ë¦¬ì ê³„ì •(admin/admin123)ì„ ì‚¬ìš© ì¤‘ì¸ì§€ ì—¬ë¶€
    """
    return auth_service.get_setup_status()

```

---

## backend/admin-dashboard/api/routers/data_sources.py

```py
"""
Data Sources Router
ë°ì´í„° ì†ŒìŠ¤ ê´€ë¦¬ API
"""

from typing import Optional
from fastapi import APIRouter, Depends, HTTPException, Query

from ..models.schemas import (
    DataSource,
    DataSourceCreate,
    DataSourceUpdate,
    DataSourceType,
    DataSourceStatus,
    DataSourceTestResult,
    UserRole,
    AuditAction,
)
from ..dependencies import (
    get_current_user,
    require_role,
    get_data_source_service,
    get_audit_service,
)
from ..services.data_source_service import DataSourceService
from ..services.audit_service import AuditService

router = APIRouter(prefix="/data-sources", tags=["Data Sources"])


@router.get("", response_model=list[DataSource])
async def list_data_sources(
    source_type: Optional[str] = Query(None, description="ì†ŒìŠ¤ íƒ€ì… í•„í„°"),
    status: Optional[str] = Query(None, description="ìƒíƒœ í•„í„°"),
    category: Optional[str] = Query(None, description="ì¹´í…Œê³ ë¦¬ í•„í„°"),
    is_active: Optional[bool] = Query(None, description="í™œì„±í™” ìƒíƒœ í•„í„°"),
    service: DataSourceService = Depends(get_data_source_service),
    current_user=Depends(get_current_user),
):
    """ë°ì´í„° ì†ŒìŠ¤ ëª©ë¡ ì¡°íšŒ"""
    type_filter = DataSourceType(source_type) if source_type else None
    status_filter = DataSourceStatus(status) if status else None

    return service.list_sources(
        source_type=type_filter,
        status=status_filter,
        category=category,
        is_active=is_active,
    )


@router.get("/categories", response_model=list[str])
async def get_categories(
    service: DataSourceService = Depends(get_data_source_service),
    current_user=Depends(get_current_user),
):
    """ëª¨ë“  ì¹´í…Œê³ ë¦¬ ëª©ë¡ ì¡°íšŒ"""
    return service.get_categories()


@router.get("/stats", response_model=dict)
async def get_stats(
    service: DataSourceService = Depends(get_data_source_service),
    current_user=Depends(get_current_user),
):
    """ë°ì´í„° ì†ŒìŠ¤ í†µê³„ ì¡°íšŒ"""
    return service.get_stats()


@router.get("/{source_id}", response_model=DataSource)
async def get_data_source(
    source_id: str,
    service: DataSourceService = Depends(get_data_source_service),
    current_user=Depends(get_current_user),
):
    """íŠ¹ì • ë°ì´í„° ì†ŒìŠ¤ ì¡°íšŒ"""
    source = service.get_source(source_id)
    if not source:
        raise HTTPException(status_code=404, detail="Data source not found")
    return source


@router.post("", response_model=DataSource, status_code=201)
async def create_data_source(
    data: DataSourceCreate,
    service: DataSourceService = Depends(get_data_source_service),
    audit_service: AuditService = Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """ìƒˆ ë°ì´í„° ì†ŒìŠ¤ ìƒì„±"""
    source = service.create_source(data)

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.CREATE,
        resource_type="data_source",
        resource_id=source.id,
        resource_name=source.name,
        details={"url": source.url, "type": source.source_type.value},
    )

    return source


@router.patch("/{source_id}", response_model=DataSource)
async def update_data_source(
    source_id: str,
    data: DataSourceUpdate,
    service: DataSourceService = Depends(get_data_source_service),
    audit_service: AuditService = Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """ë°ì´í„° ì†ŒìŠ¤ ìˆ˜ì •"""
    source = service.update_source(source_id, data)
    if not source:
        raise HTTPException(status_code=404, detail="Data source not found")

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="data_source",
        resource_id=source.id,
        resource_name=source.name,
        details=data.model_dump(exclude_unset=True),
    )

    return source


@router.delete("/{source_id}", status_code=204)
async def delete_data_source(
    source_id: str,
    service: DataSourceService = Depends(get_data_source_service),
    audit_service: AuditService = Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ë°ì´í„° ì†ŒìŠ¤ ì‚­ì œ"""
    source = service.get_source(source_id)
    if not source:
        raise HTTPException(status_code=404, detail="Data source not found")

    service.delete_source(source_id)

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DELETE,
        resource_type="data_source",
        resource_id=source_id,
        resource_name=source.name,
        details={},
    )


@router.post("/{source_id}/test", response_model=DataSourceTestResult)
async def test_data_source(
    source_id: str,
    service: DataSourceService = Depends(get_data_source_service),
    audit_service: AuditService = Depends(get_audit_service),
    current_user=Depends(get_current_user),
):
    """ë°ì´í„° ì†ŒìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    result = await service.test_source(source_id)

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.EXECUTE,
        resource_type="data_source",
        resource_id=source_id,
        details={"action": "test", "success": result.success},
        success=result.success,
        error_message=None if result.success else result.message,
    )

    return result


@router.post("/{source_id}/crawl")
async def trigger_crawl(
    source_id: str,
    service: DataSourceService = Depends(get_data_source_service),
    audit_service: AuditService = Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """ë°ì´í„° ìˆ˜ì§‘ íŠ¸ë¦¬ê±°"""
    result = await service.trigger_crawl(source_id)

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.EXECUTE,
        resource_type="data_source",
        resource_id=source_id,
        details={"action": "crawl", "success": result["success"]},
        success=result["success"],
        error_message=result.get("message") if not result["success"] else None,
    )

    return result


@router.post("/bulk/toggle-active")
async def bulk_toggle_active(
    source_ids: list[str],
    is_active: bool = Query(..., description="í™œì„±í™” ì—¬ë¶€"),
    service: DataSourceService = Depends(get_data_source_service),
    audit_service: AuditService = Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """ì—¬ëŸ¬ ë°ì´í„° ì†ŒìŠ¤ ì¼ê´„ í™œì„±í™”/ë¹„í™œì„±í™”"""
    updated = service.bulk_toggle_active(source_ids, is_active)

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="data_source",
        details={
            "action": "bulk_toggle_active",
            "source_ids": source_ids,
            "is_active": is_active,
            "updated_count": updated,
        },
    )

    return {"updated": updated}

```

---

## backend/admin-dashboard/api/routers/databases.py

```py
"""
Database Management Router
PostgreSQL, MongoDB, Redis ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ API
"""

from fastapi import APIRouter, Depends, HTTPException

from ..models.schemas import (
    DatabaseType,
    DatabaseInfo,
    PostgresDatabaseStats,
    MongoDatabaseStats,
    RedisStats,
    UserRole,
)
from ..dependencies import get_current_user, require_role, get_database_service
from ..services.database_service import DatabaseService

router = APIRouter(prefix="/databases", tags=["Database Management"])


@router.get("", response_model=list[DatabaseInfo])
async def list_databases(
    service: DatabaseService = Depends(get_database_service),
    current_user=Depends(get_current_user),
):
    """ëª¨ë“  ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ ì¡°íšŒ"""
    return await service.get_all_databases()


@router.get("/postgres/health", response_model=DatabaseInfo)
async def check_postgres_health(
    service: DatabaseService = Depends(get_database_service),
    current_user=Depends(get_current_user),
):
    """PostgreSQL í—¬ìŠ¤ ì²´í¬"""
    return await service.get_postgres_health()


@router.get("/mongo/health", response_model=DatabaseInfo)
async def check_mongo_health(
    service: DatabaseService = Depends(get_database_service),
    current_user=Depends(get_current_user),
):
    """MongoDB í—¬ìŠ¤ ì²´í¬"""
    return await service.get_mongo_health()


@router.get("/redis/health", response_model=DatabaseInfo)
async def check_redis_health(
    service: DatabaseService = Depends(get_database_service),
    current_user=Depends(get_current_user),
):
    """Redis í—¬ìŠ¤ ì²´í¬"""
    return await service.get_redis_health()


@router.get("/{db_type}/health", response_model=DatabaseInfo)
async def check_database_health(
    db_type: str,
    service: DatabaseService = Depends(get_database_service),
    current_user=Depends(get_current_user),
):
    """íŠ¹ì • ë°ì´í„°ë² ì´ìŠ¤ í—¬ìŠ¤ ì²´í¬"""
    db_type_lower = db_type.lower()

    if db_type_lower in ("postgres", "postgresql"):
        return await service.get_postgres_health()
    elif db_type_lower in ("mongo", "mongodb"):
        return await service.get_mongo_health()
    elif db_type_lower == "redis":
        return await service.get_redis_health()
    else:
        raise HTTPException(
            status_code=400,
            detail=f"Unknown database type: {db_type}. Supported: postgres, mongo, redis",
        )


@router.get("/postgres/stats", response_model=PostgresDatabaseStats)
async def get_postgres_stats(
    service: DatabaseService = Depends(get_database_service),
    current_user=Depends(get_current_user),
):
    """PostgreSQL ìƒì„¸ í†µê³„"""
    return await service.get_postgres_stats()


@router.get("/mongo/stats", response_model=MongoDatabaseStats)
async def get_mongo_stats(
    service: DatabaseService = Depends(get_database_service),
    current_user=Depends(get_current_user),
):
    """MongoDB ìƒì„¸ í†µê³„"""
    return await service.get_mongo_stats()


@router.get("/redis/stats", response_model=RedisStats)
async def get_redis_stats(
    service: DatabaseService = Depends(get_database_service),
    current_user=Depends(get_current_user),
):
    """Redis ìƒì„¸ í†µê³„"""
    return await service.get_redis_stats()

```

---

## backend/admin-dashboard/api/routers/documents.py

```py
"""
Document Router - ë¬¸ì„œ ê´€ë¦¬ API ì—”ë“œí¬ì¸íŠ¸
"""
from typing import Optional

from fastapi import APIRouter, Depends, HTTPException, Query

from ..models.schemas import Document, DocumentCategory, UserRole
from ..dependencies import get_current_user, get_document_service, require_role

router = APIRouter(prefix="/documents", tags=["Documents"])


@router.get("", response_model=list[Document])
async def list_documents(
    category: Optional[DocumentCategory] = Query(None, description="ì¹´í…Œê³ ë¦¬ í•„í„°"),
    tag: Optional[str] = Query(None, description="íƒœê·¸ í•„í„°"),
    environment: Optional[str] = Query(None, description="í™˜ê²½ í•„í„°"),
    search: Optional[str] = Query(None, description="ê²€ìƒ‰ì–´"),
    doc_service=Depends(get_document_service),
    current_user=Depends(get_current_user),
):
    """ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
    docs = doc_service.list_documents(
        category=category,
        tag=tag,
        environment=environment,
        search=search,
    )
    # ëª©ë¡ì—ì„œëŠ” content ì œì™¸
    for doc in docs:
        doc.content = None
    return docs


@router.get("/categories")
async def get_categories_summary(
    doc_service=Depends(get_document_service),
    current_user=Depends(get_current_user),
):
    """ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ ìˆ˜ ìš”ì•½"""
    return doc_service.get_categories_summary()


@router.get("/tags")
async def get_tags_summary(
    doc_service=Depends(get_document_service),
    current_user=Depends(get_current_user),
):
    """íƒœê·¸ë³„ ë¬¸ì„œ ìˆ˜ ìš”ì•½"""
    return doc_service.get_tags_summary()


@router.get("/related")
async def get_related_documents(
    environment: Optional[str] = Query(None, description="í™˜ê²½ ì´ë¦„"),
    script_id: Optional[str] = Query(None, description="ìŠ¤í¬ë¦½íŠ¸ ID"),
    doc_service=Depends(get_document_service),
    current_user=Depends(get_current_user),
):
    """ê´€ë ¨ ë¬¸ì„œ ì¡°íšŒ"""
    if not environment and not script_id:
        raise HTTPException(
            status_code=400,
            detail="At least one of environment or script_id is required",
        )

    docs = doc_service.get_related_documents(
        environment=environment,
        script_id=script_id,
    )
    # ëª©ë¡ì—ì„œëŠ” content ì œì™¸
    for doc in docs:
        doc.content = None
    return docs


@router.get("/{doc_id}", response_model=Document)
async def get_document(
    doc_id: str,
    doc_service=Depends(get_document_service),
    current_user=Depends(get_current_user),
):
    """ë¬¸ì„œ ìƒì„¸ ì¡°íšŒ (ë‚´ìš© í¬í•¨)"""
    doc = doc_service.get_document(doc_id)
    if not doc:
        raise HTTPException(status_code=404, detail="Document not found")
    return doc


@router.patch("/{doc_id}", response_model=Document)
async def update_document_metadata(
    doc_id: str,
    title: Optional[str] = None,
    category: Optional[DocumentCategory] = None,
    tags: Optional[list[str]] = Query(None),
    related_environments: Optional[list[str]] = Query(None),
    related_scripts: Optional[list[str]] = Query(None),
    doc_service=Depends(get_document_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìˆ˜ì • (Admin ê¶Œí•œ í•„ìš”)"""
    doc = doc_service.update_document_metadata(
        doc_id=doc_id,
        title=title,
        category=category,
        tags=tags,
        related_environments=related_environments,
        related_scripts=related_scripts,
    )
    if not doc:
        raise HTTPException(status_code=404, detail="Document not found")
    return doc


@router.post("/refresh")
async def refresh_documents(
    doc_service=Depends(get_document_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ë¬¸ì„œ ëª©ë¡ ìƒˆë¡œê³ ì¹¨ (Admin ê¶Œí•œ í•„ìš”)"""
    diff = doc_service.refresh_documents()
    return {
        "success": True,
        "message": f"Documents refreshed. {diff:+d} documents changed.",
        "total": len(doc_service.documents),
    }

```

---

## backend/admin-dashboard/api/routers/environments.py

```py
"""
Environment Router - í™˜ê²½ ê´€ë¦¬ API ì—”ë“œí¬ì¸íŠ¸
"""
from typing import Optional

from fastapi import APIRouter, Depends, HTTPException, Query, status
from fastapi.responses import StreamingResponse

from ..models.schemas import (
    AuditAction,
    Environment,
    EnvironmentCreate,
    EnvironmentStatus,
    EnvironmentUpdate,
    UserRole,
)
from ..dependencies import (
    get_audit_service,
    get_current_user,
    get_environment_service,
    require_role,
)

router = APIRouter(prefix="/environments", tags=["Environments"])


@router.get("", response_model=list[Environment])
async def list_environments(
    active_only: bool = Query(False, description="í™œì„± í™˜ê²½ë§Œ ì¡°íšŒ"),
    env_service=Depends(get_environment_service),
    current_user=Depends(get_current_user),
):
    """í™˜ê²½ ëª©ë¡ ì¡°íšŒ"""
    return env_service.list_environments(active_only=active_only)


@router.get("/{env_id}", response_model=Environment)
async def get_environment(
    env_id: str,
    env_service=Depends(get_environment_service),
    current_user=Depends(get_current_user),
):
    """í™˜ê²½ ìƒì„¸ ì¡°íšŒ"""
    env = env_service.get_environment(env_id)
    if not env:
        raise HTTPException(status_code=404, detail="Environment not found")
    return env


@router.post("", response_model=Environment, status_code=status.HTTP_201_CREATED)
async def create_environment(
    data: EnvironmentCreate,
    env_service=Depends(get_environment_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """í™˜ê²½ ìƒì„± (Admin ê¶Œí•œ í•„ìš”)"""
    env = env_service.create_environment(data)

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.CREATE,
        resource_type="environment",
        resource_id=env.id,
        resource_name=env.name,
        details={"data": data.model_dump()},
    )

    return env


@router.patch("/{env_id}", response_model=Environment)
async def update_environment(
    env_id: str,
    data: EnvironmentUpdate,
    env_service=Depends(get_environment_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """í™˜ê²½ ìˆ˜ì • (Admin ê¶Œí•œ í•„ìš”)"""
    env = env_service.update_environment(env_id, data)
    if not env:
        raise HTTPException(status_code=404, detail="Environment not found")

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="environment",
        resource_id=env.id,
        resource_name=env.name,
        details={"changes": data.model_dump(exclude_unset=True)},
    )

    return env


@router.delete("/{env_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_environment(
    env_id: str,
    env_service=Depends(get_environment_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """í™˜ê²½ ì‚­ì œ (Admin ê¶Œí•œ í•„ìš”)"""
    env = env_service.get_environment(env_id)
    if not env:
        raise HTTPException(status_code=404, detail="Environment not found")

    if not env_service.delete_environment(env_id):
        raise HTTPException(status_code=500, detail="Failed to delete environment")

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DELETE,
        resource_type="environment",
        resource_id=env_id,
        resource_name=env.name,
    )


@router.get("/{env_id}/status", response_model=EnvironmentStatus)
async def get_environment_status(
    env_id: str,
    env_service=Depends(get_environment_service),
    current_user=Depends(get_current_user),
):
    """í™˜ê²½ ìƒíƒœ ì¡°íšŒ (ì»¨í…Œì´ë„ˆ ìƒíƒœ)"""
    status = env_service.get_environment_status(env_id)
    if not status:
        raise HTTPException(status_code=404, detail="Environment not found")
    return status


@router.post("/{env_id}/up")
async def docker_compose_up(
    env_id: str,
    build: bool = Query(True, description="ì´ë¯¸ì§€ ë¹Œë“œ ì—¬ë¶€"),
    env_service=Depends(get_environment_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """Docker Compose Up ì‹¤í–‰"""
    env = env_service.get_environment(env_id)
    if not env:
        raise HTTPException(status_code=404, detail="Environment not found")

    success, output = await env_service.docker_compose_up(env_id, build=build)

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DEPLOY,
        resource_type="environment",
        resource_id=env_id,
        resource_name=env.name,
        environment_id=env_id,
        environment_name=env.name,
        details={"build": build},
        success=success,
        error_message=output if not success else None,
    )

    if not success:
        raise HTTPException(status_code=500, detail=output)

    return {"success": True, "message": "Services started successfully", "output": output}


@router.post("/{env_id}/down")
async def docker_compose_down(
    env_id: str,
    volumes: bool = Query(False, description="ë³¼ë¥¨ë„ ì‚­ì œí• ì§€ ì—¬ë¶€"),
    env_service=Depends(get_environment_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """Docker Compose Down ì‹¤í–‰"""
    env = env_service.get_environment(env_id)
    if not env:
        raise HTTPException(status_code=404, detail="Environment not found")

    # ë³¼ë¥¨ ì‚­ì œëŠ” Admin ê¶Œí•œ í•„ìš”
    if volumes and current_user.role != UserRole.ADMIN:
        raise HTTPException(
            status_code=403,
            detail="Admin permission required to delete volumes",
        )

    success, output = await env_service.docker_compose_down(env_id, volumes=volumes)

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.EXECUTE,
        resource_type="environment",
        resource_id=env_id,
        resource_name=env.name,
        environment_id=env_id,
        environment_name=env.name,
        details={"action": "down", "volumes": volumes},
        success=success,
        error_message=output if not success else None,
    )

    if not success:
        raise HTTPException(status_code=500, detail=output)

    return {"success": True, "message": "Services stopped successfully", "output": output}


@router.post("/{env_id}/restart")
async def docker_compose_restart(
    env_id: str,
    service: Optional[str] = Query(None, description="ì¬ì‹œì‘í•  ì„œë¹„ìŠ¤ ì´ë¦„"),
    env_service=Depends(get_environment_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """Docker Compose Restart ì‹¤í–‰"""
    env = env_service.get_environment(env_id)
    if not env:
        raise HTTPException(status_code=404, detail="Environment not found")

    success, output = await env_service.docker_compose_restart(env_id, service=service)

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.EXECUTE,
        resource_type="environment",
        resource_id=env_id,
        resource_name=env.name,
        environment_id=env_id,
        environment_name=env.name,
        details={"action": "restart", "service": service},
        success=success,
        error_message=output if not success else None,
    )

    if not success:
        raise HTTPException(status_code=500, detail=output)

    return {"success": True, "message": "Services restarted successfully", "output": output}


@router.get("/{env_id}/logs/{service}")
async def get_service_logs(
    env_id: str,
    service: str,
    tail: int = Query(100, ge=1, le=1000, description="ì¶œë ¥í•  ë¡œê·¸ ì¤„ ìˆ˜"),
    env_service=Depends(get_environment_service),
    current_user=Depends(get_current_user),
):
    """ì„œë¹„ìŠ¤ ë¡œê·¸ ì¡°íšŒ"""
    env = env_service.get_environment(env_id)
    if not env:
        raise HTTPException(status_code=404, detail="Environment not found")

    success, output = await env_service.get_service_logs(env_id, service, tail=tail)

    if not success:
        raise HTTPException(status_code=500, detail=output)

    return {"service": service, "logs": output}

```

---

## backend/admin-dashboard/api/routers/health_monitor.py

```py
"""
Service Health Monitoring Router
ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬ ë° ì‹œìŠ¤í…œ ìƒíƒœ ëª¨ë‹ˆí„°ë§ API
"""

from fastapi import APIRouter, Depends, HTTPException
from fastapi.responses import StreamingResponse
import asyncio
import json
from datetime import datetime

from ..models.schemas import (
    ServiceHealth,
    InfrastructureHealth,
    OverallSystemHealth,
    ServiceInfo,
    UserRole,
)
from ..dependencies import get_current_user, require_role, get_health_service
from ..services.health_service import HealthService

router = APIRouter(prefix="/health-monitor", tags=["Health Monitor"])


@router.get("/services", response_model=list[ServiceInfo])
async def list_services(
    service: HealthService = Depends(get_health_service),
    current_user=Depends(get_current_user),
):
    """ë“±ë¡ëœ ëª¨ë“  ì„œë¹„ìŠ¤ ëª©ë¡ ì¡°íšŒ"""
    services = service.get_all_services()
    return [
        ServiceInfo(
            id=s["id"],
            name=s["name"],
            description=s.get("description"),
            port=s.get("port"),
            healthcheck=s.get("healthcheck", "/health"),
            hostname=s["hostname"],
            type=s["type"],
            tags=s.get("tags", []),
        )
        for s in services
    ]


@router.get("/infrastructure", response_model=list[dict])
async def list_infrastructure(
    service: HealthService = Depends(get_health_service),
    current_user=Depends(get_current_user),
):
    """ì¸í”„ë¼ ì„œë¹„ìŠ¤ ëª©ë¡ ì¡°íšŒ"""
    return service.get_infrastructure_services()


@router.get("/check/{service_id}", response_model=ServiceHealth)
async def check_service(
    service_id: str,
    service: HealthService = Depends(get_health_service),
    current_user=Depends(get_current_user),
):
    """íŠ¹ì • ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬"""
    health = await service.check_service_health(service_id)
    return health


@router.get("/check-all", response_model=list[ServiceHealth])
async def check_all_services(
    service: HealthService = Depends(get_health_service),
    current_user=Depends(get_current_user),
):
    """ëª¨ë“  ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬ (ë³‘ë ¬ ì‹¤í–‰)"""
    return await service.check_all_services_health()


@router.get("/check-infrastructure", response_model=list[InfrastructureHealth])
async def check_infrastructure(
    service: HealthService = Depends(get_health_service),
    current_user=Depends(get_current_user),
):
    """ëª¨ë“  ì¸í”„ë¼ ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬"""
    return await service.check_infrastructure_health()


@router.get("/overall", response_model=OverallSystemHealth)
async def get_overall_health(
    service: HealthService = Depends(get_health_service),
    current_user=Depends(get_current_user),
):
    """ì „ì²´ ì‹œìŠ¤í…œ í—¬ìŠ¤ ìƒíƒœ ìš”ì•½"""
    return await service.get_overall_health()


@router.get("/stream")
async def stream_health_updates(
    interval: int = 10,
    service: HealthService = Depends(get_health_service),
    current_user=Depends(get_current_user),
):
    """ì‹¤ì‹œê°„ í—¬ìŠ¤ ìƒíƒœ ìŠ¤íŠ¸ë¦¬ë° (SSE)

    Args:
        interval: ì—…ë°ì´íŠ¸ ê°„ê²© (ì´ˆ, ê¸°ë³¸ 10ì´ˆ, ìµœì†Œ 5ì´ˆ)
    """
    interval = max(5, min(interval, 60))  # 5-60ì´ˆ ë²”ìœ„ ì œí•œ

    async def generate():
        while True:
            try:
                health = await service.get_overall_health()
                data = health.model_dump_json()
                yield f"data: {data}\n\n"
                await asyncio.sleep(interval)
            except Exception as e:
                error_data = json.dumps(
                    {"error": str(e), "timestamp": datetime.utcnow().isoformat()}
                )
                yield f"data: {error_data}\n\n"
                await asyncio.sleep(interval)

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )


@router.get("/last-check/{service_id}", response_model=ServiceHealth)
async def get_last_check(
    service_id: str,
    service: HealthService = Depends(get_health_service),
    current_user=Depends(get_current_user),
):
    """ë§ˆì§€ë§‰ í—¬ìŠ¤ ì²´í¬ ê²°ê³¼ ì¡°íšŒ (ìºì‹œëœ ê²°ê³¼)"""
    result = service.get_last_check(service_id)
    if not result:
        raise HTTPException(
            status_code=404,
            detail=f"No health check result found for service: {service_id}",
        )
    return result

```

---

## backend/admin-dashboard/api/routers/kafka.py

```py
"""
Kafka/Redpanda Monitoring Router
Kafka/Redpanda í´ëŸ¬ìŠ¤í„° ëª¨ë‹ˆí„°ë§ API
"""

from fastapi import APIRouter, Depends, HTTPException

from ..models.schemas import (
    KafkaTopicInfo,
    KafkaConsumerGroupInfo,
    KafkaClusterInfo,
    UserRole,
)
from ..dependencies import get_current_user, require_role, get_kafka_service
from ..services.kafka_service import KafkaService

router = APIRouter(prefix="/kafka", tags=["Kafka/Redpanda Monitoring"])


@router.get("/cluster", response_model=KafkaClusterInfo)
async def get_cluster_info(
    service: KafkaService = Depends(get_kafka_service),
    current_user=Depends(get_current_user),
):
    """Kafka/Redpanda í´ëŸ¬ìŠ¤í„° ì „ì²´ ì •ë³´ ì¡°íšŒ"""
    return await service.get_cluster_info()


@router.get("/topics", response_model=list[KafkaTopicInfo])
async def list_topics(
    service: KafkaService = Depends(get_kafka_service),
    current_user=Depends(get_current_user),
):
    """ëª¨ë“  í† í”½ ëª©ë¡ ì¡°íšŒ"""
    return await service.list_topics()


@router.get("/topics/{topic_name}", response_model=KafkaTopicInfo)
async def get_topic(
    topic_name: str,
    service: KafkaService = Depends(get_kafka_service),
    current_user=Depends(get_current_user),
):
    """íŠ¹ì • í† í”½ ìƒì„¸ ì •ë³´"""
    topic = await service.get_topic_detail(topic_name)
    if not topic:
        raise HTTPException(status_code=404, detail=f"Topic not found: {topic_name}")
    return topic


@router.get("/consumer-groups", response_model=list[KafkaConsumerGroupInfo])
async def list_consumer_groups(
    service: KafkaService = Depends(get_kafka_service),
    current_user=Depends(get_current_user),
):
    """ëª¨ë“  ì»¨ìŠˆë¨¸ ê·¸ë£¹ ì¡°íšŒ"""
    return await service.list_consumer_groups()


@router.get("/consumer-groups/{group_id}", response_model=KafkaConsumerGroupInfo)
async def get_consumer_group(
    group_id: str,
    service: KafkaService = Depends(get_kafka_service),
    current_user=Depends(get_current_user),
):
    """íŠ¹ì • ì»¨ìŠˆë¨¸ ê·¸ë£¹ ìƒì„¸ ì •ë³´"""
    group = await service.get_consumer_group_detail(group_id)
    if not group:
        raise HTTPException(
            status_code=404, detail=f"Consumer group not found: {group_id}"
        )
    return group


@router.get("/health")
async def check_health(
    service: KafkaService = Depends(get_kafka_service),
    current_user=Depends(get_current_user),
):
    """Kafka/Redpanda í—¬ìŠ¤ ì²´í¬"""
    return await service.check_health()

```

---

## backend/admin-dashboard/api/routers/llm_providers.py

```py
"""
LLM Provider Settings Router - ê´€ë¦¬ì ì „ì—­ LLM ì„¤ì • API ì—”ë“œí¬ì¸íŠ¸
"""

import os
from enum import Enum
from typing import Optional
from datetime import datetime

import httpx
from fastapi import APIRouter, Depends, HTTPException, Query, status
from pydantic import BaseModel, Field

from ..models.schemas import (
    AuditAction,
    UserRole,
)
from ..dependencies import (
    get_audit_service,
    get_current_user,
    require_role,
)

router = APIRouter(prefix="/llm-providers", tags=["LLM Providers"])


# ============================================================================
# Schemas
# ============================================================================


class LlmProviderType(str, Enum):
    OPENAI = "OPENAI"
    ANTHROPIC = "ANTHROPIC"
    GOOGLE = "GOOGLE"
    OPENROUTER = "OPENROUTER"
    OLLAMA = "OLLAMA"
    AZURE_OPENAI = "AZURE_OPENAI"
    TOGETHER_AI = "TOGETHER_AI"
    CUSTOM = "CUSTOM"


class LlmProviderTypeInfo(BaseModel):
    value: LlmProviderType
    displayName: str
    description: str
    requiresApiKey: bool
    defaultBaseUrl: Optional[str] = None


class LlmProviderSettingsRequest(BaseModel):
    providerType: LlmProviderType
    apiKey: Optional[str] = Field(None, description="API í‚¤ (ë¹„ìš°ë©´ ê¸°ì¡´ ê°’ ìœ ì§€)")
    defaultModel: str = Field(..., description="ê¸°ë³¸ ëª¨ë¸")
    baseUrl: Optional[str] = Field(None, description="Base URL (Ollama/Customìš©)")
    enabled: bool = Field(True, description="í™œì„±í™” ì—¬ë¶€")
    priority: int = Field(100, ge=1, le=999, description="ìš°ì„ ìˆœìœ„")
    maxTokens: int = Field(4096, ge=1, le=128000, description="ìµœëŒ€ í† í°")
    temperature: float = Field(0.7, ge=0, le=2, description="Temperature")
    timeoutMs: int = Field(60000, ge=1000, le=300000, description="íƒ€ì„ì•„ì›ƒ (ms)")
    azureDeploymentName: Optional[str] = Field(
        None, description="Azure deployment name"
    )
    azureApiVersion: Optional[str] = Field(None, description="Azure API version")


class LlmProviderSettings(BaseModel):
    id: int
    providerType: LlmProviderType
    userId: Optional[str] = None  # null = global setting
    hasApiKey: bool
    maskedApiKey: Optional[str] = None
    defaultModel: str
    baseUrl: Optional[str] = None
    enabled: bool
    priority: int
    maxTokens: int
    temperature: float
    timeoutMs: int
    azureDeploymentName: Optional[str] = None
    azureApiVersion: Optional[str] = None
    lastTestedAt: Optional[datetime] = None
    lastTestSuccess: Optional[bool] = None
    createdAt: datetime
    updatedAt: datetime


class LlmTestResult(BaseModel):
    providerType: LlmProviderType
    success: bool
    message: str
    latencyMs: Optional[int] = None
    testedAt: datetime


# Provider metadata (2025ë…„ 12ì›” ìµœì‹ )
LLM_PROVIDER_TYPES = [
    LlmProviderTypeInfo(
        value=LlmProviderType.OPENAI,
        displayName="OpenAI",
        description="GPT-5, GPT-4.1, o3/o4 ì¶”ë¡  ëª¨ë¸",
        requiresApiKey=True,
    ),
    LlmProviderTypeInfo(
        value=LlmProviderType.ANTHROPIC,
        displayName="Anthropic",
        description="Claude 4 Sonnet/Opus/Haiku",
        requiresApiKey=True,
    ),
    LlmProviderTypeInfo(
        value=LlmProviderType.GOOGLE,
        displayName="Google AI",
        description="Gemini 3 Pro, Gemini 2.5 Pro/Flash",
        requiresApiKey=True,
    ),
    LlmProviderTypeInfo(
        value=LlmProviderType.OPENROUTER,
        displayName="OpenRouter",
        description="125+ ëª¨ë¸ í†µí•© API (ë¬´ë£Œ ëª¨ë¸ í¬í•¨)",
        requiresApiKey=True,
    ),
    LlmProviderTypeInfo(
        value=LlmProviderType.OLLAMA,
        displayName="Ollama",
        description="ë¡œì»¬ LLM ì‹¤í–‰ (Llama 3.2, DeepSeek R1)",
        requiresApiKey=False,
        defaultBaseUrl="http://localhost:11434",
    ),
    LlmProviderTypeInfo(
        value=LlmProviderType.AZURE_OPENAI,
        displayName="Azure OpenAI",
        description="Azureì—ì„œ í˜¸ìŠ¤íŒ…í•˜ëŠ” OpenAI ëª¨ë¸",
        requiresApiKey=True,
    ),
    LlmProviderTypeInfo(
        value=LlmProviderType.TOGETHER_AI,
        displayName="Together AI",
        description="DeepSeek R1, Llama 405B ë“± ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸",
        requiresApiKey=True,
        defaultBaseUrl="https://api.together.xyz/v1",
    ),
    LlmProviderTypeInfo(
        value=LlmProviderType.CUSTOM,
        displayName="Custom API",
        description="OpenAI í˜¸í™˜ ì»¤ìŠ¤í…€ API",
        requiresApiKey=False,
    ),
]


# Backend service URL for data-collection-service
COLLECTOR_SERVICE_URL = os.environ.get("COLLECTOR_SERVICE_URL", "http://localhost:8081")


# ============================================================================
# Helper functions
# ============================================================================


async def call_collector_service(
    method: str,
    path: str,
    params: Optional[dict] = None,
    json_data: Optional[dict] = None,
) -> dict:
    """Call the data-collection-service API"""
    url = f"{COLLECTOR_SERVICE_URL}{path}"

    async with httpx.AsyncClient(timeout=30.0) as client:
        try:
            response = await client.request(
                method=method,
                url=url,
                params=params,
                json=json_data,
            )

            if response.status_code >= 400:
                detail = (
                    response.json().get("message", response.text)
                    if response.text
                    else "Unknown error"
                )
                raise HTTPException(
                    status_code=response.status_code,
                    detail=detail,
                )

            if response.status_code == 204:
                return {}

            return response.json()
        except httpx.RequestError as e:
            raise HTTPException(
                status_code=503,
                detail=f"Collector service unavailable: {str(e)}",
            )


# ============================================================================
# Endpoints
# ============================================================================


@router.get("/types", response_model=list[LlmProviderTypeInfo])
async def list_provider_types(
    current_user=Depends(get_current_user),
):
    """LLM Provider íƒ€ì… ëª©ë¡ ì¡°íšŒ"""
    return LLM_PROVIDER_TYPES


@router.get("/global", response_model=list[LlmProviderSettings])
async def list_global_settings(
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ì „ì—­ LLM ì„¤ì • ëª©ë¡ ì¡°íšŒ (Admin ê¶Œí•œ í•„ìš”)"""
    result = await call_collector_service("GET", "/api/v1/admin/llm-providers")
    return result


@router.get("/global/{provider_type}", response_model=LlmProviderSettings)
async def get_global_setting(
    provider_type: LlmProviderType,
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """íŠ¹ì • Providerì˜ ì „ì—­ ì„¤ì • ì¡°íšŒ (Admin ê¶Œí•œ í•„ìš”)"""
    result = await call_collector_service(
        "GET", f"/api/v1/admin/llm-providers/{provider_type.value}"
    )
    return result


@router.put("/global/{provider_type}", response_model=LlmProviderSettings)
async def save_global_setting(
    provider_type: LlmProviderType,
    data: LlmProviderSettingsRequest,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ì „ì—­ LLM ì„¤ì • ì €ì¥/ìˆ˜ì • (Admin ê¶Œí•œ í•„ìš”)"""
    # Ensure provider type matches
    if data.providerType != provider_type:
        raise HTTPException(
            status_code=400,
            detail="Provider type in path must match request body",
        )

    result = await call_collector_service(
        "PUT",
        f"/api/v1/admin/llm-providers/{provider_type.value}",
        json_data=data.model_dump(exclude_none=True),
    )

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="llm_provider",
        resource_id=provider_type.value,
        resource_name=f"Global {provider_type.value} Settings",
        details={
            "provider": provider_type.value,
            "enabled": data.enabled,
            "model": data.defaultModel,
        },
    )

    return result


@router.delete("/global/{provider_type}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_global_setting(
    provider_type: LlmProviderType,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ì „ì—­ LLM ì„¤ì • ì‚­ì œ (Admin ê¶Œí•œ í•„ìš”)"""
    await call_collector_service(
        "DELETE", f"/api/v1/admin/llm-providers/{provider_type.value}"
    )

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DELETE,
        resource_type="llm_provider",
        resource_id=provider_type.value,
        resource_name=f"Global {provider_type.value} Settings",
    )


@router.post("/test", response_model=LlmTestResult)
async def test_connection(
    provider_type: LlmProviderType = Query(..., description="Provider íƒ€ì…"),
    model: Optional[str] = Query(None, description="í…ŒìŠ¤íŠ¸í•  ëª¨ë¸"),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """LLM Provider ì—°ê²° í…ŒìŠ¤íŠ¸ (Admin ê¶Œí•œ í•„ìš”)"""
    params = {"providerType": provider_type.value}
    if model:
        params["model"] = model

    result = await call_collector_service(
        "POST", "/api/v1/llm-providers/test", params=params
    )
    return result


@router.get("/effective", response_model=list[LlmProviderSettings])
async def get_effective_settings(
    user_id: Optional[str] = Query(None, description="ì‚¬ìš©ì ID (ì—†ìœ¼ë©´ ì „ì—­ë§Œ)"),
    current_user=Depends(get_current_user),
):
    """ìœ íš¨ LLM ì„¤ì • ì¡°íšŒ (ì‚¬ìš©ì ì„¤ì • + ì „ì—­ fallback)"""
    params = {}
    if user_id:
        params["userId"] = user_id

    result = await call_collector_service(
        "GET", "/api/v1/llm-providers/effective", params=params
    )
    return result


@router.get("/enabled", response_model=list[LlmProviderSettings])
async def get_enabled_providers(
    user_id: Optional[str] = Query(None, description="ì‚¬ìš©ì ID"),
    current_user=Depends(get_current_user),
):
    """í™œì„±í™”ëœ LLM Provider ëª©ë¡ ì¡°íšŒ"""
    params = {}
    if user_id:
        params["userId"] = user_id

    result = await call_collector_service(
        "GET", "/api/v1/llm-providers/enabled", params=params
    )
    return result

```

---

## backend/admin-dashboard/api/routers/ml_addons.py

```py
"""
ML Addons Router - ML ì• ë“œì˜¨ ê´€ë¦¬ API ì—”ë“œí¬ì¸íŠ¸
"""

import os
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional

import httpx
from fastapi import APIRouter, Depends, HTTPException, Query, status
from pydantic import BaseModel, Field

from ..models.schemas import (
    AuditAction,
    UserRole,
)
from ..dependencies import (
    get_audit_service,
    get_current_user,
    require_role,
)

router = APIRouter(prefix="/ml-addons", tags=["ML Addons"])


# ============================================================================
# Configuration
# ============================================================================

CRAWLER_SERVICE_URL = os.environ.get(
    "CRAWLER_SERVICE_URL", "http://autonomous-crawler:8030"
)


# ============================================================================
# Schemas
# ============================================================================


class MLAddonType(str, Enum):
    """ML Addon íƒ€ì…"""

    SENTIMENT = "sentiment"
    FACTCHECK = "factcheck"
    BIAS = "bias"


class MLAddonStatus(str, Enum):
    """ML Addon ìƒíƒœ"""

    HEALTHY = "healthy"
    UNHEALTHY = "unhealthy"
    UNKNOWN = "unknown"
    WARMING_UP = "warming_up"


class MLAddonInfo(BaseModel):
    """ML Addon ì •ë³´"""

    key: str
    name: str
    description: str
    endpoint: str
    status: MLAddonStatus
    features: List[str]


class MLAddonHealthResponse(BaseModel):
    """ML Addon í—¬ìŠ¤ ì‘ë‹µ"""

    status: str
    auto_analysis_enabled: bool
    addons: Dict[str, Dict[str, Any]]


class MLAddonStatusResponse(BaseModel):
    """ML Addon ìƒíƒœ ì‘ë‹µ"""

    auto_analysis_enabled: bool
    parallel_analysis: bool
    addons: Dict[str, Any]


class MLAnalyzeRequest(BaseModel):
    """ML ë¶„ì„ ìš”ì²­"""

    article_id: int = Field(..., description="ê¸°ì‚¬ ID")
    title: str = Field(..., description="ê¸°ì‚¬ ì œëª©", min_length=1)
    content: str = Field(..., description="ê¸°ì‚¬ ë³¸ë¬¸", min_length=10)
    source: Optional[str] = Field(default=None, description="ì–¸ë¡ ì‚¬ëª…")
    url: Optional[str] = Field(default=None, description="ê¸°ì‚¬ URL")
    published_at: Optional[str] = Field(default=None, description="ë°œí–‰ì¼")
    addons: Optional[List[str]] = Field(
        default=None,
        description="ì‹¤í–‰í•  ì• ë“œì˜¨ ëª©ë¡ (sentiment, factcheck, bias). Noneì´ë©´ ëª¨ë‘ ì‹¤í–‰",
    )
    save_to_db: bool = Field(default=True, description="ê²°ê³¼ë¥¼ DBì— ì €ì¥í• ì§€ ì—¬ë¶€")


class MLBatchAnalyzeRequest(BaseModel):
    """ML ë°°ì¹˜ ë¶„ì„ ìš”ì²­"""

    articles: List[MLAnalyzeRequest] = Field(..., min_length=1, max_length=50)
    addons: Optional[List[str]] = None
    save_to_db: bool = True
    max_concurrent: int = Field(default=5, ge=1, le=20)


class MLAnalysisResult(BaseModel):
    """ML ë¶„ì„ ê²°ê³¼"""

    addon_type: str
    success: bool
    request_id: str
    results: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    latency_ms: int = 0
    analyzed_at: str


class BatchAnalysisResult(BaseModel):
    """ë°°ì¹˜ ë¶„ì„ ê²°ê³¼"""

    article_id: int
    sentiment: Optional[MLAnalysisResult] = None
    factcheck: Optional[MLAnalysisResult] = None
    bias: Optional[MLAnalysisResult] = None
    total_latency_ms: int = 0
    success_count: int = 0
    failure_count: int = 0


class MLAddonDBEntry(BaseModel):
    """DBì— ì €ì¥ëœ ML Addon ì •ë³´"""

    id: int
    addon_key: str
    name: str
    description: Optional[str] = None
    endpoint_url: str
    version: Optional[str] = None
    status: str
    config: Optional[Dict[str, Any]] = None
    created_at: datetime
    updated_at: datetime


class MLAddonCreateRequest(BaseModel):
    """ML Addon ìƒì„± ìš”ì²­"""

    addon_key: str = Field(..., description="ê³ ìœ  í‚¤ (ì˜ˆ: sentiment-addon)")
    name: str = Field(..., description="í‘œì‹œ ì´ë¦„")
    description: Optional[str] = Field(None, description="ì„¤ëª…")
    endpoint_url: str = Field(..., description="ì„œë¹„ìŠ¤ URL")
    version: Optional[str] = Field(None, description="ë²„ì „")
    config: Optional[Dict[str, Any]] = Field(None, description="ì¶”ê°€ ì„¤ì •")


class MLAddonUpdateRequest(BaseModel):
    """ML Addon ìˆ˜ì • ìš”ì²­"""

    name: Optional[str] = None
    description: Optional[str] = None
    endpoint_url: Optional[str] = None
    version: Optional[str] = None
    status: Optional[str] = None
    config: Optional[Dict[str, Any]] = None


# ============================================================================
# Helper functions
# ============================================================================


async def call_crawler_service(
    method: str,
    path: str,
    params: Optional[dict] = None,
    json_data: Optional[dict] = None,
    timeout: float = 60.0,
) -> dict:
    """Call the autonomous-crawler-service API"""
    url = f"{CRAWLER_SERVICE_URL}{path}"

    async with httpx.AsyncClient(timeout=timeout) as client:
        try:
            response = await client.request(
                method=method,
                url=url,
                params=params,
                json=json_data,
            )

            if response.status_code >= 400:
                detail = (
                    response.json().get("detail", response.text)
                    if response.text
                    else "Unknown error"
                )
                raise HTTPException(
                    status_code=response.status_code,
                    detail=detail,
                )

            if response.status_code == 204:
                return {}

            return response.json()
        except httpx.RequestError as e:
            raise HTTPException(
                status_code=503,
                detail=f"Crawler service unavailable: {str(e)}",
            )


# ============================================================================
# Endpoints - Health & Status
# ============================================================================


@router.get("/health", response_model=MLAddonHealthResponse)
async def ml_health_check(
    current_user=Depends(get_current_user),
):
    """
    ML ì‹œìŠ¤í…œ í—¬ìŠ¤ì²´í¬.

    ëª¨ë“  ML Addonì˜ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    """
    result = await call_crawler_service("GET", "/ml/health")
    return result


@router.get("/status", response_model=MLAddonStatusResponse)
async def ml_status(
    current_user=Depends(get_current_user),
):
    """
    ML ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ.

    í˜„ì¬ ì„¤ì • ë° Addon ì—°ê²° ìƒíƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    result = await call_crawler_service("GET", "/ml/status")
    return result


@router.get("/list", response_model=Dict[str, Any])
async def list_addons(
    current_user=Depends(get_current_user),
):
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ ML Addon ëª©ë¡.

    ê° ì• ë“œì˜¨ì˜ ê¸°ëŠ¥ê³¼ í˜„ì¬ ìƒíƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    result = await call_crawler_service("GET", "/ml/addons")
    return result


# ============================================================================
# Endpoints - Analysis
# ============================================================================


@router.post("/analyze", response_model=BatchAnalysisResult)
async def analyze_article(
    request: MLAnalyzeRequest,
    current_user=Depends(get_current_user),
):
    """
    ë‹¨ì¼ ê¸°ì‚¬ ML ë¶„ì„.

    ê¸°ì‚¬ì— ëŒ€í•´ sentiment, factcheck, bias ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    result = await call_crawler_service(
        "POST",
        "/ml/analyze",
        json_data=request.model_dump(exclude_none=True),
        timeout=120.0,
    )
    return result


@router.post("/analyze/simple")
async def analyze_text_simple(
    text: str = Query(..., min_length=10, description="ë¶„ì„í•  í…ìŠ¤íŠ¸"),
    source: Optional[str] = Query(None, description="ì¶œì²˜"),
    addons: Optional[str] = Query(None, description="ì• ë“œì˜¨ (ì‰¼í‘œ êµ¬ë¶„)"),
    current_user=Depends(get_current_user),
):
    """
    ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ML ë¶„ì„.

    ê¸°ì‚¬ ID ì—†ì´ í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    ê²°ê³¼ëŠ” DBì— ì €ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """
    addon_list = addons.split(",") if addons else None

    result = await call_crawler_service(
        "POST",
        "/ml/analyze/simple",
        json_data={
            "text": text,
            "source": source,
            "addons": addon_list,
        },
        timeout=120.0,
    )
    return result


@router.post("/analyze/batch")
async def analyze_batch(
    request: MLBatchAnalyzeRequest,
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    ë°°ì¹˜ ê¸°ì‚¬ ML ë¶„ì„.

    ì—¬ëŸ¬ ê¸°ì‚¬ë¥¼ í•œ ë²ˆì— ë¶„ì„í•©ë‹ˆë‹¤. (Admin ê¶Œí•œ í•„ìš”)
    """
    result = await call_crawler_service(
        "POST",
        "/ml/analyze/batch",
        json_data=request.model_dump(exclude_none=True),
        timeout=300.0,
    )
    return result


@router.post("/analyze/url")
async def analyze_url(
    url: str = Query(..., description="ë¶„ì„í•  URL"),
    current_user=Depends(get_current_user),
):
    """
    URLì—ì„œ ê¸°ì‚¬ë¥¼ í¬ë¡¤ë§í•˜ê³  ML ë¶„ì„ ìˆ˜í–‰.

    URLì˜ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•œ í›„ sentiment, factcheck, bias ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    result = await call_crawler_service(
        "POST",
        "/ml/analyze/url",
        params={"url": url},
        timeout=120.0,
    )
    return result


# ============================================================================
# Endpoints - Configuration
# ============================================================================


@router.post("/config/toggle")
async def toggle_auto_analysis(
    enabled: bool = Query(..., description="ìë™ ë¶„ì„ í™œì„±í™” ì—¬ë¶€"),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    ìë™ ML ë¶„ì„ í† ê¸€.

    í¬ë¡¤ë§ í›„ ìë™ ML ë¶„ì„ ê¸°ëŠ¥ì„ í™œì„±í™”/ë¹„í™œì„±í™”í•©ë‹ˆë‹¤. (Admin ê¶Œí•œ í•„ìš”)
    """
    result = await call_crawler_service(
        "POST",
        "/ml/config/toggle",
        params={"enabled": enabled},
    )

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="ml_config",
        resource_id="auto_analysis",
        resource_name="ML Auto Analysis",
        details={"enabled": enabled},
    )

    return result


# ============================================================================
# Endpoints - CRUD for ml_addon table (via collector service)
# ============================================================================

COLLECTOR_SERVICE_URL_DB = os.environ.get(
    "COLLECTOR_SERVICE_URL", "http://localhost:8081"
)


async def call_collector_service_db(
    method: str,
    path: str,
    params: Optional[dict] = None,
    json_data: Optional[dict] = None,
) -> dict:
    """Call the data-collection-service API for DB operations"""
    url = f"{COLLECTOR_SERVICE_URL_DB}{path}"

    async with httpx.AsyncClient(timeout=30.0) as client:
        try:
            response = await client.request(
                method=method,
                url=url,
                params=params,
                json=json_data,
            )

            if response.status_code >= 400:
                detail = (
                    response.json().get("message", response.text)
                    if response.text
                    else "Unknown error"
                )
                raise HTTPException(
                    status_code=response.status_code,
                    detail=detail,
                )

            if response.status_code == 204:
                return {}

            return response.json()
        except httpx.RequestError as e:
            raise HTTPException(
                status_code=503,
                detail=f"Collector service unavailable: {str(e)}",
            )


@router.get("/registered", response_model=List[MLAddonDBEntry])
async def list_registered_addons(
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    DBì— ë“±ë¡ëœ ML Addon ëª©ë¡ ì¡°íšŒ (Admin ê¶Œí•œ í•„ìš”)
    """
    result = await call_collector_service_db("GET", "/api/v1/admin/ml-addons")
    return result


@router.get("/registered/{addon_id}", response_model=MLAddonDBEntry)
async def get_registered_addon(
    addon_id: int,
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    íŠ¹ì • ML Addon ì¡°íšŒ (Admin ê¶Œí•œ í•„ìš”)
    """
    result = await call_collector_service_db(
        "GET", f"/api/v1/admin/ml-addons/{addon_id}"
    )
    return result


@router.post(
    "/registered", response_model=MLAddonDBEntry, status_code=status.HTTP_201_CREATED
)
async def create_addon(
    request: MLAddonCreateRequest,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    ìƒˆ ML Addon ë“±ë¡ (Admin ê¶Œí•œ í•„ìš”)
    """
    result = await call_collector_service_db(
        "POST",
        "/api/v1/admin/ml-addons",
        json_data=request.model_dump(exclude_none=True),
    )

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.CREATE,
        resource_type="ml_addon",
        resource_id=request.addon_key,
        resource_name=request.name,
        details={"endpoint": request.endpoint_url},
    )

    return result


@router.put("/registered/{addon_id}", response_model=MLAddonDBEntry)
async def update_addon(
    addon_id: int,
    request: MLAddonUpdateRequest,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    ML Addon ìˆ˜ì • (Admin ê¶Œí•œ í•„ìš”)
    """
    result = await call_collector_service_db(
        "PUT",
        f"/api/v1/admin/ml-addons/{addon_id}",
        json_data=request.model_dump(exclude_none=True),
    )

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="ml_addon",
        resource_id=str(addon_id),
        resource_name=result.get("name", f"Addon {addon_id}"),
        details=request.model_dump(exclude_none=True),
    )

    return result


@router.delete("/registered/{addon_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_addon(
    addon_id: int,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    ML Addon ì‚­ì œ (Admin ê¶Œí•œ í•„ìš”)
    """
    await call_collector_service_db("DELETE", f"/api/v1/admin/ml-addons/{addon_id}")

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DELETE,
        resource_type="ml_addon",
        resource_id=str(addon_id),
        resource_name=f"Addon {addon_id}",
    )


@router.post("/registered/{addon_id}/test")
async def test_addon(
    addon_id: int,
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    ML Addon ì—°ê²° í…ŒìŠ¤íŠ¸ (Admin ê¶Œí•œ í•„ìš”)
    """
    result = await call_collector_service_db(
        "POST", f"/api/v1/admin/ml-addons/{addon_id}/test"
    )
    return result

```

---

## backend/admin-dashboard/api/routers/ml_training.py

```py
"""
ML Training Router - ML ëª¨ë¸ í•™ìŠµ ê´€ë¦¬ API ì—”ë“œí¬ì¸íŠ¸
"""

import os
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional

import httpx
from fastapi import APIRouter, Depends, HTTPException, Query, status
from pydantic import BaseModel, Field

from ..models.schemas import (
    AuditAction,
    UserRole,
)
from ..dependencies import (
    get_audit_service,
    get_current_user,
    require_role,
)

router = APIRouter(prefix="/ml-training", tags=["ML Training"])


# ============================================================================
# Configuration
# ============================================================================

ML_TRAINER_URL = os.environ.get("ML_TRAINER_URL", "http://ml-trainer:8103")


# ============================================================================
# Schemas
# ============================================================================


class TrainingJobStatus(str, Enum):
    """í•™ìŠµ ì‘ì—… ìƒíƒœ"""

    PENDING = "pending"
    PREPARING = "preparing"
    TRAINING = "training"
    EVALUATING = "evaluating"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


class ModelType(str, Enum):
    """ëª¨ë¸ íƒ€ì…"""

    SENTIMENT = "sentiment"
    FACTCHECK = "factcheck"
    BIAS = "bias"
    CUSTOM = "custom"


class DatasetSource(str, Enum):
    """ë°ì´í„°ì…‹ ì†ŒìŠ¤"""

    HUGGINGFACE = "huggingface"
    LOCAL = "local"
    URL = "url"
    DATABASE = "database"


class TrainingJobCreate(BaseModel):
    """í•™ìŠµ ì‘ì—… ìƒì„± ìš”ì²­"""

    name: str = Field(..., description="ì‘ì—… ì´ë¦„")
    description: Optional[str] = Field(None, description="ì„¤ëª…")
    model_type: ModelType = Field(..., description="ëª¨ë¸ íƒ€ì…")
    base_model: str = Field(
        ..., description="ë² ì´ìŠ¤ ëª¨ë¸ (ì˜ˆ: monologg/koelectra-base-v3-discriminator)"
    )

    # Dataset configuration
    dataset_source: DatasetSource = Field(
        DatasetSource.HUGGINGFACE, description="ë°ì´í„°ì…‹ ì†ŒìŠ¤"
    )
    dataset_name: Optional[str] = Field(
        None, description="ë°ì´í„°ì…‹ ì´ë¦„ (HuggingFace ë˜ëŠ” ë¡œì»¬ ê²½ë¡œ)"
    )
    dataset_split: str = Field("train", description="ë°ì´í„°ì…‹ ë¶„í• ")
    text_column: str = Field("text", description="í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…")
    label_column: str = Field("label", description="ë¼ë²¨ ì»¬ëŸ¼ëª…")

    # Training hyperparameters
    num_epochs: int = Field(3, ge=1, le=100, description="ì—í¬í¬ ìˆ˜")
    batch_size: int = Field(16, ge=1, le=256, description="ë°°ì¹˜ í¬ê¸°")
    learning_rate: float = Field(2e-5, ge=1e-7, le=1e-2, description="í•™ìŠµë¥ ")
    warmup_ratio: float = Field(0.1, ge=0, le=1, description="ì›œì—… ë¹„ìœ¨")
    weight_decay: float = Field(0.01, ge=0, le=1, description="ê°€ì¤‘ì¹˜ ê°ì‡ ")
    max_seq_length: int = Field(512, ge=32, le=2048, description="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´")

    # Output configuration
    output_dir: Optional[str] = Field(None, description="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    save_steps: int = Field(500, description="ì €ì¥ ìŠ¤í… ê°„ê²©")
    eval_steps: int = Field(500, description="í‰ê°€ ìŠ¤í… ê°„ê²©")

    # Additional options
    fp16: bool = Field(False, description="FP16 í•™ìŠµ ì‚¬ìš©")
    gradient_accumulation_steps: int = Field(
        1, ge=1, le=64, description="ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…"
    )


class TrainingJob(BaseModel):
    """í•™ìŠµ ì‘ì—…"""

    id: str
    name: str
    description: Optional[str] = None
    model_type: ModelType
    base_model: str
    status: TrainingJobStatus

    # Progress
    progress: float = Field(0.0, description="ì§„í–‰ë¥  (0-100)")
    current_epoch: int = 0
    total_epochs: int = 0
    current_step: int = 0
    total_steps: int = 0

    # Metrics
    train_loss: Optional[float] = None
    eval_loss: Optional[float] = None
    eval_accuracy: Optional[float] = None
    eval_f1: Optional[float] = None

    # Timing
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    elapsed_seconds: int = 0
    estimated_remaining_seconds: Optional[int] = None

    # Output
    output_path: Optional[str] = None
    model_size_mb: Optional[float] = None

    # Error handling
    error_message: Optional[str] = None

    created_at: datetime
    updated_at: datetime


class TrainingJobList(BaseModel):
    """í•™ìŠµ ì‘ì—… ëª©ë¡"""

    jobs: List[TrainingJob]
    total: int
    pending: int
    running: int
    completed: int
    failed: int


class ModelInfo(BaseModel):
    """í•™ìŠµëœ ëª¨ë¸ ì •ë³´"""

    id: str
    name: str
    model_type: ModelType
    base_model: str
    training_job_id: str

    # Performance metrics
    accuracy: Optional[float] = None
    f1_score: Optional[float] = None
    precision: Optional[float] = None
    recall: Optional[float] = None

    # Model details
    model_path: str
    size_mb: float
    num_labels: int
    label_mapping: Optional[Dict[str, int]] = None

    # Deployment
    is_deployed: bool = False
    deployed_at: Optional[datetime] = None
    addon_key: Optional[str] = None

    created_at: datetime


class ModelList(BaseModel):
    """í•™ìŠµëœ ëª¨ë¸ ëª©ë¡"""

    models: List[ModelInfo]
    total: int


class DeployRequest(BaseModel):
    """ëª¨ë¸ ë°°í¬ ìš”ì²­"""

    addon_key: str = Field(..., description="ë°°í¬í•  ì• ë“œì˜¨ í‚¤ (ì˜ˆ: sentiment-addon)")
    replace_current: bool = Field(True, description="í˜„ì¬ ëª¨ë¸ êµì²´ ì—¬ë¶€")


class TrainingMetrics(BaseModel):
    """í•™ìŠµ ë©”íŠ¸ë¦­"""

    job_id: str
    step: int
    epoch: float
    train_loss: Optional[float] = None
    eval_loss: Optional[float] = None
    eval_accuracy: Optional[float] = None
    eval_f1: Optional[float] = None
    learning_rate: Optional[float] = None
    timestamp: datetime


# ============================================================================
# Helper functions
# ============================================================================


async def call_trainer_service(
    method: str,
    path: str,
    params: Optional[dict] = None,
    json_data: Optional[dict] = None,
    timeout: float = 30.0,
) -> dict:
    """Call the ml-trainer service API"""
    url = f"{ML_TRAINER_URL}{path}"

    async with httpx.AsyncClient(timeout=timeout) as client:
        try:
            response = await client.request(
                method=method,
                url=url,
                params=params,
                json=json_data,
            )

            if response.status_code >= 400:
                detail = (
                    response.json().get("detail", response.text)
                    if response.text
                    else "Unknown error"
                )
                raise HTTPException(
                    status_code=response.status_code,
                    detail=detail,
                )

            if response.status_code == 204:
                return {}

            return response.json()
        except httpx.RequestError as e:
            raise HTTPException(
                status_code=503,
                detail=f"ML Trainer service unavailable: {str(e)}",
            )


# ============================================================================
# Endpoints - Health & Status
# ============================================================================


@router.get("/health")
async def trainer_health_check(
    current_user=Depends(get_current_user),
):
    """
    ML Trainer ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬.
    """
    try:
        result = await call_trainer_service("GET", "/health")
        return result
    except HTTPException as e:
        if e.status_code == 503:
            return {
                "status": "unhealthy",
                "message": "ML Trainer service is unavailable",
            }
        raise


@router.get("/status")
async def trainer_status(
    current_user=Depends(get_current_user),
):
    """
    ML Trainer ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ.

    í˜„ì¬ í•™ìŠµ ì‘ì—… ìƒíƒœ ë° ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    result = await call_trainer_service("GET", "/status")
    return result


# ============================================================================
# Endpoints - Training Jobs
# ============================================================================


@router.get("/jobs", response_model=TrainingJobList)
async def list_training_jobs(
    status: Optional[TrainingJobStatus] = Query(None, description="ìƒíƒœ í•„í„°"),
    model_type: Optional[ModelType] = Query(None, description="ëª¨ë¸ íƒ€ì… í•„í„°"),
    limit: int = Query(20, ge=1, le=100, description="ìµœëŒ€ ê°œìˆ˜"),
    offset: int = Query(0, ge=0, description="ì˜¤í”„ì…‹"),
    current_user=Depends(get_current_user),
):
    """
    í•™ìŠµ ì‘ì—… ëª©ë¡ ì¡°íšŒ.
    """
    params = {"limit": limit, "offset": offset}
    if status:
        params["status"] = status.value
    if model_type:
        params["model_type"] = model_type.value

    result = await call_trainer_service("GET", "/jobs", params=params)
    return result


@router.get("/jobs/{job_id}", response_model=TrainingJob)
async def get_training_job(
    job_id: str,
    current_user=Depends(get_current_user),
):
    """
    íŠ¹ì • í•™ìŠµ ì‘ì—… ì¡°íšŒ.
    """
    result = await call_trainer_service("GET", f"/jobs/{job_id}")
    return result


@router.post("/jobs", response_model=TrainingJob, status_code=status.HTTP_201_CREATED)
async def create_training_job(
    request: TrainingJobCreate,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    ìƒˆ í•™ìŠµ ì‘ì—… ìƒì„±. (Admin ê¶Œí•œ í•„ìš”)

    í•™ìŠµ ì‘ì—…ì„ ìƒì„±í•˜ê³  íì— ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    result = await call_trainer_service(
        "POST",
        "/jobs",
        json_data=request.model_dump(exclude_none=True),
    )

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.CREATE,
        resource_type="training_job",
        resource_id=result.get("id", "unknown"),
        resource_name=request.name,
        details={
            "model_type": request.model_type.value,
            "base_model": request.base_model,
            "epochs": request.num_epochs,
        },
    )

    return result


@router.post("/jobs/{job_id}/start")
async def start_training_job(
    job_id: str,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    í•™ìŠµ ì‘ì—… ì‹œì‘. (Admin ê¶Œí•œ í•„ìš”)
    """
    result = await call_trainer_service("POST", f"/jobs/{job_id}/start", timeout=60.0)

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="training_job",
        resource_id=job_id,
        resource_name=f"Training Job {job_id}",
        details={"action": "start"},
    )

    return result


@router.post("/jobs/{job_id}/cancel")
async def cancel_training_job(
    job_id: str,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    í•™ìŠµ ì‘ì—… ì·¨ì†Œ. (Admin ê¶Œí•œ í•„ìš”)
    """
    result = await call_trainer_service("POST", f"/jobs/{job_id}/cancel")

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="training_job",
        resource_id=job_id,
        resource_name=f"Training Job {job_id}",
        details={"action": "cancel"},
    )

    return result


@router.delete("/jobs/{job_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_training_job(
    job_id: str,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    í•™ìŠµ ì‘ì—… ì‚­ì œ. (Admin ê¶Œí•œ í•„ìš”)

    ì™„ë£Œë˜ê±°ë‚˜ ì‹¤íŒ¨í•œ ì‘ì—…ë§Œ ì‚­ì œ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    """
    await call_trainer_service("DELETE", f"/jobs/{job_id}")

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DELETE,
        resource_type="training_job",
        resource_id=job_id,
        resource_name=f"Training Job {job_id}",
    )


@router.get("/jobs/{job_id}/metrics", response_model=List[TrainingMetrics])
async def get_training_metrics(
    job_id: str,
    limit: int = Query(100, ge=1, le=1000, description="ìµœëŒ€ ê°œìˆ˜"),
    current_user=Depends(get_current_user),
):
    """
    í•™ìŠµ ì‘ì—…ì˜ ë©”íŠ¸ë¦­ ì¡°íšŒ.

    í•™ìŠµ ì§„í–‰ ì¤‘ ê¸°ë¡ëœ loss, accuracy ë“±ì˜ ë©”íŠ¸ë¦­ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    result = await call_trainer_service(
        "GET", f"/jobs/{job_id}/metrics", params={"limit": limit}
    )
    return result


@router.get("/jobs/{job_id}/logs")
async def get_training_logs(
    job_id: str,
    lines: int = Query(100, ge=1, le=1000, description="ë¡œê·¸ ë¼ì¸ ìˆ˜"),
    current_user=Depends(get_current_user),
):
    """
    í•™ìŠµ ì‘ì—…ì˜ ë¡œê·¸ ì¡°íšŒ.
    """
    result = await call_trainer_service(
        "GET", f"/jobs/{job_id}/logs", params={"lines": lines}
    )
    return result


# ============================================================================
# Endpoints - Models
# ============================================================================


@router.get("/models", response_model=ModelList)
async def list_models(
    model_type: Optional[ModelType] = Query(None, description="ëª¨ë¸ íƒ€ì… í•„í„°"),
    deployed_only: bool = Query(False, description="ë°°í¬ëœ ëª¨ë¸ë§Œ"),
    limit: int = Query(20, ge=1, le=100, description="ìµœëŒ€ ê°œìˆ˜"),
    offset: int = Query(0, ge=0, description="ì˜¤í”„ì…‹"),
    current_user=Depends(get_current_user),
):
    """
    í•™ìŠµëœ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ.
    """
    params = {"limit": limit, "offset": offset, "deployed_only": deployed_only}
    if model_type:
        params["model_type"] = model_type.value

    result = await call_trainer_service("GET", "/models", params=params)
    return result


@router.get("/models/{model_id}", response_model=ModelInfo)
async def get_model(
    model_id: str,
    current_user=Depends(get_current_user),
):
    """
    íŠ¹ì • ëª¨ë¸ ì¡°íšŒ.
    """
    result = await call_trainer_service("GET", f"/models/{model_id}")
    return result


@router.post("/models/{model_id}/deploy")
async def deploy_model(
    model_id: str,
    request: DeployRequest,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    ëª¨ë¸ì„ ML Addonì— ë°°í¬. (Admin ê¶Œí•œ í•„ìš”)

    í•™ìŠµëœ ëª¨ë¸ì„ ì§€ì •ëœ ML Addon ì„œë¹„ìŠ¤ì— ë°°í¬í•©ë‹ˆë‹¤.
    """
    result = await call_trainer_service(
        "POST",
        f"/models/{model_id}/deploy",
        json_data=request.model_dump(),
        timeout=120.0,
    )

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="ml_model",
        resource_id=model_id,
        resource_name=f"Model {model_id}",
        details={
            "action": "deploy",
            "addon_key": request.addon_key,
            "replace_current": request.replace_current,
        },
    )

    return result


@router.delete("/models/{model_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_model(
    model_id: str,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    í•™ìŠµëœ ëª¨ë¸ ì‚­ì œ. (Admin ê¶Œí•œ í•„ìš”)

    ë°°í¬ë˜ì§€ ì•Šì€ ëª¨ë¸ë§Œ ì‚­ì œ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    """
    await call_trainer_service("DELETE", f"/models/{model_id}")

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DELETE,
        resource_type="ml_model",
        resource_id=model_id,
        resource_name=f"Model {model_id}",
    )


# ============================================================================
# Endpoints - Datasets
# ============================================================================


@router.get("/datasets")
async def list_datasets(
    source: Optional[DatasetSource] = Query(None, description="ë°ì´í„°ì…‹ ì†ŒìŠ¤ í•„í„°"),
    current_user=Depends(get_current_user),
):
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡ ì¡°íšŒ.
    """
    params = {}
    if source:
        params["source"] = source.value

    result = await call_trainer_service("GET", "/datasets", params=params)
    return result


@router.post("/datasets/search")
async def search_huggingface_datasets(
    query: str = Query(..., description="ê²€ìƒ‰ ì¿¼ë¦¬"),
    task: Optional[str] = Query(
        None, description="íƒœìŠ¤í¬ íƒ€ì… (text-classification ë“±)"
    ),
    language: Optional[str] = Query("ko", description="ì–¸ì–´"),
    limit: int = Query(10, ge=1, le=50),
    current_user=Depends(get_current_user),
):
    """
    HuggingFace ë°ì´í„°ì…‹ ê²€ìƒ‰.
    """
    params = {"query": query, "limit": limit}
    if task:
        params["task"] = task
    if language:
        params["language"] = language

    result = await call_trainer_service("POST", "/datasets/search", params=params)
    return result


@router.get("/datasets/{dataset_name}/preview")
async def preview_dataset(
    dataset_name: str,
    split: str = Query("train", description="ë°ì´í„°ì…‹ ë¶„í• "),
    num_samples: int = Query(5, ge=1, le=20, description="ìƒ˜í”Œ ìˆ˜"),
    current_user=Depends(get_current_user),
):
    """
    ë°ì´í„°ì…‹ ë¯¸ë¦¬ë³´ê¸°.

    ë°ì´í„°ì…‹ì˜ ìƒ˜í”Œ ë°ì´í„°ì™€ ìŠ¤í‚¤ë§ˆë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    result = await call_trainer_service(
        "GET",
        f"/datasets/{dataset_name}/preview",
        params={"split": split, "num_samples": num_samples},
    )
    return result


# ============================================================================
# Endpoints - Presets
# ============================================================================


@router.get("/presets")
async def list_training_presets(
    current_user=Depends(get_current_user),
):
    """
    í•™ìŠµ í”„ë¦¬ì…‹ ëª©ë¡.

    ì¼ë°˜ì ì¸ í•™ìŠµ ì„¤ì • í”„ë¦¬ì…‹ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    presets = [
        {
            "id": "korean-sentiment-koelectra",
            "name": "í•œêµ­ì–´ ê°ì„±ë¶„ì„ (KoELECTRA)",
            "model_type": "sentiment",
            "base_model": "monologg/koelectra-base-v3-discriminator",
            "recommended_datasets": [
                "nsmc",
                "klue/ner",
            ],
            "default_config": {
                "num_epochs": 3,
                "batch_size": 32,
                "learning_rate": 2e-5,
                "max_seq_length": 128,
            },
        },
        {
            "id": "korean-bias-kcbert",
            "name": "í•œêµ­ì–´ í¸í–¥ë¶„ì„ (KcBERT)",
            "model_type": "bias",
            "base_model": "beomi/KcBERT-base",
            "recommended_datasets": [],
            "default_config": {
                "num_epochs": 5,
                "batch_size": 16,
                "learning_rate": 3e-5,
                "max_seq_length": 256,
            },
        },
        {
            "id": "multilingual-factcheck",
            "name": "ë‹¤êµ­ì–´ íŒ©íŠ¸ì²´í¬ (mBERT)",
            "model_type": "factcheck",
            "base_model": "bert-base-multilingual-cased",
            "recommended_datasets": [],
            "default_config": {
                "num_epochs": 5,
                "batch_size": 16,
                "learning_rate": 2e-5,
                "max_seq_length": 512,
            },
        },
    ]
    return {"presets": presets, "total": len(presets)}


@router.post("/presets/{preset_id}/apply", response_model=TrainingJobCreate)
async def apply_preset(
    preset_id: str,
    dataset_name: Optional[str] = Query(
        None, description="ë°ì´í„°ì…‹ ì´ë¦„ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)"
    ),
    current_user=Depends(get_current_user),
):
    """
    í”„ë¦¬ì…‹ ì ìš©.

    ì„ íƒí•œ í”„ë¦¬ì…‹ì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµ ì„¤ì •ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    result = await call_trainer_service(
        "POST",
        f"/presets/{preset_id}/apply",
        params={"dataset_name": dataset_name} if dataset_name else None,
    )
    return result

```

---

## backend/admin-dashboard/api/routers/public_auth.py

```py
"""
Public Auth Router - ì¼ë°˜ ì‚¬ìš©ììš© ì¸ì¦ API (ê³µê°œ ì—”ë“œí¬ì¸íŠ¸)

- íšŒì›ê°€ì…: ì¸ì¦ ë¶ˆí•„ìš”
- ë¡œê·¸ì¸: ì¸ì¦ ë¶ˆí•„ìš”
- ë‚´ ì •ë³´: ì¸ì¦ í•„ìš”
- ë¡œê·¸ì•„ì›ƒ: ì¸ì¦ í•„ìš”
- ë¹„ë°€ë²ˆí˜¸ ë³€ê²½: ì¸ì¦ í•„ìš”
"""

from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.security import OAuth2PasswordRequestForm
from pydantic import BaseModel, EmailStr, Field

from ..models.schemas import AuditAction, Token, User, UserRegister, UserRole
from ..dependencies import (
    get_audit_service,
    get_auth_service,
    get_current_user,
)

router = APIRouter(prefix="/auth", tags=["Public Authentication"])


class RegisterRequest(BaseModel):
    """íšŒì›ê°€ì… ìš”ì²­"""

    username: str = Field(..., min_length=3, max_length=50, description="ì‚¬ìš©ìëª…")
    email: EmailStr = Field(..., description="ì´ë©”ì¼ ì£¼ì†Œ")
    password: str = Field(..., min_length=8, description="ë¹„ë°€ë²ˆí˜¸ (8ì ì´ìƒ)")


class ChangePasswordRequest(BaseModel):
    """ë¹„ë°€ë²ˆí˜¸ ë³€ê²½ ìš”ì²­"""

    old_password: str
    new_password: str = Field(..., min_length=8, description="ìƒˆ ë¹„ë°€ë²ˆí˜¸ (8ì ì´ìƒ)")


class UserPublicResponse(BaseModel):
    """ì¼ë°˜ ì‚¬ìš©ì ì‘ë‹µ (ë¯¼ê° ì •ë³´ ì œì™¸)"""

    id: str
    username: str
    email: str | None
    role: UserRole
    created_at: str


# ============================================================================
# Public Endpoints (No Auth Required)
# ============================================================================
@router.post("/register", response_model=Token, status_code=status.HTTP_201_CREATED)
async def register(
    request: RegisterRequest,
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """
    íšŒì›ê°€ì… (ê³µê°œ API)

    - ì‚¬ìš©ìëª…ì€ 3-50ì
    - ë¹„ë°€ë²ˆí˜¸ëŠ” 8ì ì´ìƒ
    - ì´ë©”ì¼ì€ ìœ íš¨í•œ í˜•ì‹ì´ì–´ì•¼ í•¨
    - ê°€ì… ì¦‰ì‹œ ë¡œê·¸ì¸ í† í° ë°˜í™˜
    """
    try:
        # ì¼ë°˜ ì‚¬ìš©ìë¡œ ê°€ì… (role: USER)
        user = auth_service.register_user(
            username=request.username,
            email=request.email,
            password=request.password,
        )

        # ê°€ì… ì„±ê³µ ì‹œ ë°”ë¡œ í† í° ë°œê¸‰
        token = auth_service.create_access_token(user)

        # ê°ì‚¬ ë¡œê·¸
        audit_service.log(
            user_id=user.id,
            username=user.username,
            action=AuditAction.CREATE,
            resource_type="user_registration",
            resource_id=user.id,
            success=True,
            details={"email": request.email},
        )

        return token

    except ValueError as e:
        # ì¤‘ë³µ ì‚¬ìš©ìëª…/ì´ë©”ì¼ ë“±
        audit_service.log(
            user_id="unknown",
            username=request.username,
            action=AuditAction.CREATE,
            resource_type="user_registration",
            success=False,
            error_message=str(e),
        )
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e),
        )


@router.post("/login", response_model=Token)
async def login(
    form_data: OAuth2PasswordRequestForm = Depends(),
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """
    ë¡œê·¸ì¸ (ê³µê°œ API)

    ì¼ë°˜ ì‚¬ìš©ìì™€ ê´€ë¦¬ì ëª¨ë‘ ì‚¬ìš© ê°€ëŠ¥
    """
    user = auth_service.authenticate(form_data.username, form_data.password)

    if not user:
        audit_service.log(
            user_id="unknown",
            username=form_data.username,
            action=AuditAction.LOGIN,
            resource_type="auth",
            success=False,
            error_message="Invalid credentials",
        )
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="ì•„ì´ë”” ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤",
            headers={"WWW-Authenticate": "Bearer"},
        )

    token = auth_service.create_access_token(user)

    audit_service.log(
        user_id=user.id,
        username=user.username,
        action=AuditAction.LOGIN,
        resource_type="auth",
        success=True,
    )

    return token


@router.post("/token", response_model=Token)
async def login_for_access_token(
    form_data: OAuth2PasswordRequestForm = Depends(),
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """OAuth2 í˜¸í™˜ í† í° ì—”ë“œí¬ì¸íŠ¸"""
    return await login(form_data, auth_service, audit_service)


# ============================================================================
# Protected Endpoints (Auth Required)
# ============================================================================
@router.get("/me", response_model=User)
async def get_current_user_info(
    current_user=Depends(get_current_user),
):
    """í˜„ì¬ ë¡œê·¸ì¸í•œ ì‚¬ìš©ì ì •ë³´"""
    return current_user


@router.post("/logout")
async def logout(
    current_user=Depends(get_current_user),
    audit_service=Depends(get_audit_service),
):
    """ë¡œê·¸ì•„ì›ƒ"""
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.LOGOUT,
        resource_type="auth",
    )

    return {"success": True, "message": "ë¡œê·¸ì•„ì›ƒë˜ì—ˆìŠµë‹ˆë‹¤"}


@router.post("/change-password")
async def change_password(
    request: ChangePasswordRequest,
    current_user=Depends(get_current_user),
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """ë¹„ë°€ë²ˆí˜¸ ë³€ê²½"""
    success = auth_service.change_password(
        user_id=current_user.id,
        old_password=request.old_password,
        new_password=request.new_password,
    )

    if not success:
        audit_service.log(
            user_id=current_user.id,
            username=current_user.username,
            action=AuditAction.UPDATE,
            resource_type="password",
            success=False,
            error_message="Invalid old password",
        )
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="í˜„ì¬ ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤",
        )

    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="password",
        success=True,
    )

    return {"success": True, "message": "ë¹„ë°€ë²ˆí˜¸ê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤"}


@router.delete("/me")
async def delete_my_account(
    current_user=Depends(get_current_user),
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """
    ë‚´ ê³„ì • ì‚­ì œ (íšŒì›íƒˆí‡´)

    ì£¼ì˜: ì´ ì‘ì—…ì€ ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
    """
    # ê´€ë¦¬ì ê³„ì •ì€ ìê¸° ì‚­ì œ ë¶ˆê°€
    if current_user.role == UserRole.ADMIN:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="ê´€ë¦¬ì ê³„ì •ì€ ì§ì ‘ ì‚­ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
        )

    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DELETE,
        resource_type="user_account",
        resource_id=current_user.id,
        success=True,
    )

    auth_service.delete_user(current_user.id)

    return {"success": True, "message": "ê³„ì •ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤"}


# ============================================================================
# Username/Email Availability Check (Public)
# ============================================================================
@router.get("/check-username/{username}")
async def check_username_availability(
    username: str,
    auth_service=Depends(get_auth_service),
):
    """ì‚¬ìš©ìëª… ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    exists = auth_service.get_user_by_username(username) is not None
    return {
        "username": username,
        "available": not exists,
    }


@router.get("/check-email/{email}")
async def check_email_availability(
    email: str,
    auth_service=Depends(get_auth_service),
):
    """ì´ë©”ì¼ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    exists = auth_service.get_user_by_email(email) is not None
    return {
        "email": email,
        "available": not exists,
    }


# ============================================================================
# Email Verification Endpoints (for Registration)
# ============================================================================
class SendVerificationRequest(BaseModel):
    """ì´ë©”ì¼ ì¸ì¦ ìš”ì²­"""
    username: str = Field(..., min_length=3, max_length=50, description="ì‚¬ìš©ìëª…")
    email: EmailStr = Field(..., description="ì´ë©”ì¼ ì£¼ì†Œ")
    password: str = Field(..., min_length=8, description="ë¹„ë°€ë²ˆí˜¸ (8ì ì´ìƒ)")


class VerifyEmailRequest(BaseModel):
    """ì´ë©”ì¼ ì¸ì¦ ì½”ë“œ ê²€ì¦ ìš”ì²­"""
    email: EmailStr = Field(..., description="ì´ë©”ì¼ ì£¼ì†Œ")
    code: str = Field(..., min_length=6, max_length=6, description="6ìë¦¬ ì¸ì¦ ì½”ë“œ")


class ResendVerificationRequest(BaseModel):
    """ì¸ì¦ ì½”ë“œ ì¬ë°œì†¡ ìš”ì²­"""
    email: EmailStr = Field(..., description="ì´ë©”ì¼ ì£¼ì†Œ")


@router.post("/send-verification")
async def send_verification_code(
    request: SendVerificationRequest,
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """
    ì´ë©”ì¼ ì¸ì¦ ì½”ë“œ ë°œì†¡ (íšŒì›ê°€ì… 1ë‹¨ê³„)
    
    - ì‚¬ìš©ìëª…, ì´ë©”ì¼, ë¹„ë°€ë²ˆí˜¸ë¥¼ ì €ì¥í•˜ê³  ì¸ì¦ ì½”ë“œ ìƒì„±
    - ì‹¤ì œ ì´ë©”ì¼ ë°œì†¡ì€ ë³„ë„ ì„œë¹„ìŠ¤ ì—°ë™ í•„ìš”
    - 10ë¶„ê°„ ìœ íš¨í•œ 6ìë¦¬ ì¸ì¦ ì½”ë“œ ë°˜í™˜
    """
    try:
        code = auth_service.create_email_verification(
            email=request.email,
            username=request.username,
            password=request.password,
        )
        
        # TODO: ì‹¤ì œ ì´ë©”ì¼ ë°œì†¡ ë¡œì§ (SMTP, SendGrid, AWS SES ë“±)
        # await send_email(
        #     to=request.email,
        #     subject="[NewsInsight] ì´ë©”ì¼ ì¸ì¦ ì½”ë“œ",
        #     body=f"ì¸ì¦ ì½”ë“œ: {code}\n\n10ë¶„ ì´ë‚´ì— ì…ë ¥í•´ì£¼ì„¸ìš”."
        # )
        
        audit_service.log(
            user_id="unknown",
            username=request.username,
            action=AuditAction.CREATE,
            resource_type="email_verification",
            success=True,
            details={"email": request.email},
        )
        
        # NOTE: ì¸ì¦ ì½”ë“œëŠ” ë³´ì•ˆìƒ ì‘ë‹µì— í¬í•¨í•˜ì§€ ì•ŠìŒ
        # ê°œë°œ/í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œëŠ” ë¡œê·¸ë¡œ í™•ì¸í•˜ê±°ë‚˜ DEBUG_EMAIL_CODE í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
        import os
        response = {
            "success": True,
            "message": "ì¸ì¦ ì½”ë“œê°€ ì´ë©”ì¼ë¡œ ë°œì†¡ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "email": request.email,
            "expires_in": 600,  # 10ë¶„
        }
        
        # ê°œë°œ í™˜ê²½ì—ì„œë§Œ ì½”ë“œ ë°˜í™˜ (DEBUG_EMAIL_CODE=true ì„¤ì • í•„ìš”)
        if os.getenv("DEBUG_EMAIL_CODE", "false").lower() == "true":
            response["code"] = code
            
        return response
        
    except ValueError as e:
        audit_service.log(
            user_id="unknown",
            username=request.username,
            action=AuditAction.CREATE,
            resource_type="email_verification",
            success=False,
            error_message=str(e),
        )
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e),
        )


@router.post("/verify-email", response_model=Token)
async def verify_email_code(
    request: VerifyEmailRequest,
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """
    ì´ë©”ì¼ ì¸ì¦ ì½”ë“œ ê²€ì¦ ë° íšŒì›ê°€ì… ì™„ë£Œ (íšŒì›ê°€ì… 2ë‹¨ê³„)
    
    - ì˜¬ë°”ë¥¸ ì¸ì¦ ì½”ë“œ ì…ë ¥ ì‹œ íšŒì›ê°€ì… ì™„ë£Œ
    - ê°€ì… ì„±ê³µ ì‹œ ë¡œê·¸ì¸ í† í° ë°˜í™˜
    - ìµœëŒ€ 5íšŒ ì‹œë„ ê°€ëŠ¥
    """
    try:
        user = auth_service.verify_email_code(
            email=request.email,
            code=request.code,
        )
        
        # í† í° ë°œê¸‰
        token = auth_service.create_access_token(user)
        
        audit_service.log(
            user_id=user.id,
            username=user.username,
            action=AuditAction.CREATE,
            resource_type="user_registration",
            resource_id=user.id,
            success=True,
            details={"email": request.email, "verified": True},
        )
        
        return token
        
    except ValueError as e:
        audit_service.log(
            user_id="unknown",
            username="unknown",
            action=AuditAction.UPDATE,
            resource_type="email_verification",
            success=False,
            error_message=str(e),
            details={"email": request.email},
        )
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e),
        )


@router.post("/resend-verification")
async def resend_verification_code(
    request: ResendVerificationRequest,
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """
    ì¸ì¦ ì½”ë“œ ì¬ë°œì†¡
    
    - ê¸°ì¡´ ì¸ì¦ ìš”ì²­ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ê°€ëŠ¥
    - ìƒˆë¡œìš´ 6ìë¦¬ ì½”ë“œ ìƒì„± ë° ìœ íš¨ ì‹œê°„ ì´ˆê¸°í™”
    """
    try:
        code = auth_service.resend_verification_code(request.email)
        
        # TODO: ì‹¤ì œ ì´ë©”ì¼ ë°œì†¡ ë¡œì§
        
        audit_service.log(
            user_id="unknown",
            username="unknown",
            action=AuditAction.UPDATE,
            resource_type="email_verification",
            success=True,
            details={"email": request.email, "action": "resend"},
        )
        
        # NOTE: ì¸ì¦ ì½”ë“œëŠ” ë³´ì•ˆìƒ ì‘ë‹µì— í¬í•¨í•˜ì§€ ì•ŠìŒ
        import os
        response = {
            "success": True,
            "message": "ì¸ì¦ ì½”ë“œê°€ ì¬ë°œì†¡ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "email": request.email,
            "expires_in": 600,
        }
        
        # ê°œë°œ í™˜ê²½ì—ì„œë§Œ ì½”ë“œ ë°˜í™˜ (DEBUG_EMAIL_CODE=true ì„¤ì • í•„ìš”)
        if os.getenv("DEBUG_EMAIL_CODE", "false").lower() == "true":
            response["code"] = code
            
        return response
        
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e),
        )

```

---

## backend/admin-dashboard/api/routers/scripts.py

```py
"""
Script Router - ìŠ¤í¬ë¦½íŠ¸/ì‘ì—… ê´€ë¦¬ API ì—”ë“œí¬ì¸íŠ¸
"""
from typing import Any, Optional

from fastapi import APIRouter, Depends, HTTPException, Query, status
from fastapi.responses import StreamingResponse

from ..models.schemas import (
    AuditAction,
    Script,
    ScriptCreate,
    ScriptUpdate,
    TaskExecution,
    TaskExecutionRequest,
    TaskStatus,
    UserRole,
)
from ..dependencies import (
    get_audit_service,
    get_current_user,
    get_environment_service,
    get_script_service,
    require_role,
)

router = APIRouter(prefix="/scripts", tags=["Scripts"])


@router.get("", response_model=list[Script])
async def list_scripts(
    environment: Optional[str] = Query(None, description="í™˜ê²½ ì´ë¦„ìœ¼ë¡œ í•„í„°"),
    tag: Optional[str] = Query(None, description="íƒœê·¸ë¡œ í•„í„°"),
    script_service=Depends(get_script_service),
    current_user=Depends(get_current_user),
):
    """ìŠ¤í¬ë¦½íŠ¸ ëª©ë¡ ì¡°íšŒ (ì‚¬ìš©ì ê¶Œí•œì— ë”°ë¼ í•„í„°ë§)"""
    return script_service.list_scripts(
        environment=environment,
        tag=tag,
        role=current_user.role,
    )


@router.get("/{script_id}", response_model=Script)
async def get_script(
    script_id: str,
    script_service=Depends(get_script_service),
    current_user=Depends(get_current_user),
):
    """ìŠ¤í¬ë¦½íŠ¸ ìƒì„¸ ì¡°íšŒ"""
    script = script_service.get_script(script_id)
    if not script:
        raise HTTPException(status_code=404, detail="Script not found")
    return script


@router.post("", response_model=Script, status_code=status.HTTP_201_CREATED)
async def create_script(
    data: ScriptCreate,
    script_service=Depends(get_script_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ìŠ¤í¬ë¦½íŠ¸ ìƒì„± (Admin ê¶Œí•œ í•„ìš”)"""
    script = script_service.create_script(data)

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.CREATE,
        resource_type="script",
        resource_id=script.id,
        resource_name=script.name,
        details={"data": data.model_dump()},
    )

    return script


@router.patch("/{script_id}", response_model=Script)
async def update_script(
    script_id: str,
    data: ScriptUpdate,
    script_service=Depends(get_script_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì • (Admin ê¶Œí•œ í•„ìš”)"""
    script = script_service.update_script(script_id, data)
    if not script:
        raise HTTPException(status_code=404, detail="Script not found")

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="script",
        resource_id=script.id,
        resource_name=script.name,
        details={"changes": data.model_dump(exclude_unset=True)},
    )

    return script


@router.delete("/{script_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_script(
    script_id: str,
    script_service=Depends(get_script_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ìŠ¤í¬ë¦½íŠ¸ ì‚­ì œ (Admin ê¶Œí•œ í•„ìš”)"""
    script = script_service.get_script(script_id)
    if not script:
        raise HTTPException(status_code=404, detail="Script not found")

    if not script_service.delete_script(script_id):
        raise HTTPException(status_code=500, detail="Failed to delete script")

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DELETE,
        resource_type="script",
        resource_id=script_id,
        resource_name=script.name,
    )


@router.post("/execute", response_model=TaskExecution)
async def execute_script(
    request: TaskExecutionRequest,
    script_service=Depends(get_script_service),
    env_service=Depends(get_environment_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(get_current_user),
):
    """ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰"""
    # ìŠ¤í¬ë¦½íŠ¸ ì¡°íšŒ
    script = script_service.get_script(request.script_id)
    if not script:
        raise HTTPException(status_code=404, detail="Script not found")

    # í™˜ê²½ ì¡°íšŒ
    env = env_service.get_environment(request.environment_id)
    if not env:
        raise HTTPException(status_code=404, detail="Environment not found")

    # ê¶Œí•œ í™•ì¸
    from ..services.auth_service import AuthService

    if not AuthService.check_permission(
        AuthService, current_user.role, script.required_role
    ):
        raise HTTPException(
            status_code=403,
            detail=f"Requires {script.required_role.value} permission",
        )

    # í™˜ê²½ í—ˆìš© í™•ì¸
    if script.allowed_environments and env.name not in script.allowed_environments:
        raise HTTPException(
            status_code=400,
            detail=f"Script not allowed for environment: {env.name}",
        )

    try:
        execution = await script_service.execute_script(
            script_id=request.script_id,
            environment_name=env.name,
            compose_file=env.compose_file,
            parameters=request.parameters,
            executed_by=current_user.username,
        )

        # ê°ì‚¬ ë¡œê·¸
        audit_service.log(
            user_id=current_user.id,
            username=current_user.username,
            action=AuditAction.EXECUTE,
            resource_type="script",
            resource_id=script.id,
            resource_name=script.name,
            environment_id=env.id,
            environment_name=env.name,
            details={
                "execution_id": execution.id,
                "parameters": request.parameters,
            },
        )

        return execution

    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.post("/execute/stream")
async def execute_script_stream(
    request: TaskExecutionRequest,
    script_service=Depends(get_script_service),
    env_service=Depends(get_environment_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(get_current_user),
):
    """ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ (ì‹¤ì‹œê°„ ì¶œë ¥ ìŠ¤íŠ¸ë¦¬ë°)"""
    # ìŠ¤í¬ë¦½íŠ¸ ì¡°íšŒ
    script = script_service.get_script(request.script_id)
    if not script:
        raise HTTPException(status_code=404, detail="Script not found")

    # í™˜ê²½ ì¡°íšŒ
    env = env_service.get_environment(request.environment_id)
    if not env:
        raise HTTPException(status_code=404, detail="Environment not found")

    # ê¶Œí•œ í™•ì¸
    role_priority = {UserRole.VIEWER: 0, UserRole.OPERATOR: 1, UserRole.ADMIN: 2}
    if role_priority.get(current_user.role, 0) < role_priority.get(
        script.required_role, 0
    ):
        raise HTTPException(
            status_code=403,
            detail=f"Requires {script.required_role.value} permission",
        )

    # í™˜ê²½ í—ˆìš© í™•ì¸
    if script.allowed_environments and env.name not in script.allowed_environments:
        raise HTTPException(
            status_code=400,
            detail=f"Script not allowed for environment: {env.name}",
        )

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.EXECUTE,
        resource_type="script",
        resource_id=script.id,
        resource_name=script.name,
        environment_id=env.id,
        environment_name=env.name,
        details={"parameters": request.parameters, "streaming": True},
    )

    async def generate():
        async for line in script_service.stream_execution_output(
            script_id=request.script_id,
            environment_name=env.name,
            compose_file=env.compose_file,
            parameters=request.parameters,
            executed_by=current_user.username,
        ):
            yield line

    return StreamingResponse(
        generate(),
        media_type="text/plain",
        headers={"X-Content-Type-Options": "nosniff"},
    )


@router.get("/executions", response_model=list[TaskExecution])
async def list_executions(
    script_id: Optional[str] = Query(None, description="ìŠ¤í¬ë¦½íŠ¸ IDë¡œ í•„í„°"),
    environment_id: Optional[str] = Query(None, description="í™˜ê²½ IDë¡œ í•„í„°"),
    status: Optional[TaskStatus] = Query(None, description="ìƒíƒœë¡œ í•„í„°"),
    limit: int = Query(50, ge=1, le=200, description="ì¡°íšŒ ê°œìˆ˜"),
    script_service=Depends(get_script_service),
    current_user=Depends(get_current_user),
):
    """ì‹¤í–‰ ì´ë ¥ ì¡°íšŒ"""
    return script_service.list_executions(
        script_id=script_id,
        environment_id=environment_id,
        status=status,
        limit=limit,
    )


@router.get("/executions/{execution_id}", response_model=TaskExecution)
async def get_execution(
    execution_id: str,
    script_service=Depends(get_script_service),
    current_user=Depends(get_current_user),
):
    """ì‹¤í–‰ ìƒì„¸ ì¡°íšŒ"""
    execution = script_service.get_execution(execution_id)
    if not execution:
        raise HTTPException(status_code=404, detail="Execution not found")
    return execution


@router.post("/executions/{execution_id}/cancel")
async def cancel_execution(
    execution_id: str,
    script_service=Depends(get_script_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """ì‹¤í–‰ ì¤‘ì¸ ì‘ì—… ì·¨ì†Œ"""
    execution = script_service.get_execution(execution_id)
    if not execution:
        raise HTTPException(status_code=404, detail="Execution not found")

    if execution.status != TaskStatus.RUNNING:
        raise HTTPException(status_code=400, detail="Execution is not running")

    if not script_service.cancel_execution(execution_id):
        raise HTTPException(status_code=500, detail="Failed to cancel execution")

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.EXECUTE,
        resource_type="execution",
        resource_id=execution_id,
        details={"action": "cancel"},
    )

    return {"success": True, "message": "Execution cancelled"}

```

---

## backend/admin-dashboard/api/services/__init__.py

```py
# Admin Dashboard Services

```

---

## backend/admin-dashboard/api/services/audit_service.py

```py
"""
Audit Service - ê°ì‚¬ ë¡œê·¸ ê´€ë¦¬ ì„œë¹„ìŠ¤
"""
import json
from datetime import datetime
from pathlib import Path
from typing import Any, Optional
from uuid import uuid4

from ..models.schemas import AuditAction, AuditLog, AuditLogFilter


class AuditService:
    """ê°ì‚¬ ë¡œê·¸ ì„œë¹„ìŠ¤"""

    def __init__(self, config_dir: str, max_logs: int = 10000):
        self.config_dir = Path(config_dir)
        self.logs_file = self.config_dir / "audit_logs.jsonl"
        self.max_logs = max_logs
        self._ensure_log_file()

    def _ensure_log_file(self) -> None:
        """ë¡œê·¸ íŒŒì¼ ì¡´ì¬ í™•ì¸"""
        self.config_dir.mkdir(parents=True, exist_ok=True)
        if not self.logs_file.exists():
            self.logs_file.touch()

    def log(
        self,
        user_id: str,
        username: str,
        action: AuditAction,
        resource_type: str,
        resource_id: Optional[str] = None,
        resource_name: Optional[str] = None,
        environment_id: Optional[str] = None,
        environment_name: Optional[str] = None,
        details: Optional[dict[str, Any]] = None,
        ip_address: Optional[str] = None,
        user_agent: Optional[str] = None,
        success: bool = True,
        error_message: Optional[str] = None,
    ) -> AuditLog:
        """ê°ì‚¬ ë¡œê·¸ ê¸°ë¡"""
        log_entry = AuditLog(
            id=f"audit-{uuid4().hex[:12]}",
            user_id=user_id,
            username=username,
            action=action,
            resource_type=resource_type,
            resource_id=resource_id,
            resource_name=resource_name,
            environment_id=environment_id,
            environment_name=environment_name,
            details=details or {},
            ip_address=ip_address,
            user_agent=user_agent,
            timestamp=datetime.utcnow(),
            success=success,
            error_message=error_message,
        )

        # JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥
        with open(self.logs_file, "a") as f:
            f.write(log_entry.model_dump_json() + "\n")

        # ë¡œê·¸ íŒŒì¼ í¬ê¸° ê´€ë¦¬
        self._rotate_if_needed()

        return log_entry

    def _rotate_if_needed(self) -> None:
        """ë¡œê·¸ íŒŒì¼ í¬ê¸° ê´€ë¦¬"""
        try:
            with open(self.logs_file, "r") as f:
                lines = f.readlines()

            if len(lines) > self.max_logs:
                # ì˜¤ë˜ëœ ë¡œê·¸ ì‚­ì œ (ìµœì‹  max_logsê°œë§Œ ìœ ì§€)
                with open(self.logs_file, "w") as f:
                    f.writelines(lines[-self.max_logs :])
        except Exception:
            pass

    def get_logs(
        self,
        filter_params: Optional[AuditLogFilter] = None,
        page: int = 1,
        page_size: int = 50,
    ) -> tuple[list[AuditLog], int]:
        """ê°ì‚¬ ë¡œê·¸ ì¡°íšŒ"""
        logs = []

        try:
            with open(self.logs_file, "r") as f:
                for line in f:
                    if line.strip():
                        try:
                            data = json.loads(line)
                            log = AuditLog(**data)
                            logs.append(log)
                        except (json.JSONDecodeError, Exception):
                            continue
        except FileNotFoundError:
            return [], 0

        # í•„í„° ì ìš©
        if filter_params:
            logs = self._apply_filter(logs, filter_params)

        # ìµœì‹ ìˆœ ì •ë ¬
        logs.sort(key=lambda x: x.timestamp, reverse=True)

        total = len(logs)

        # í˜ì´ì§€ë„¤ì´ì…˜
        start = (page - 1) * page_size
        end = start + page_size
        paginated_logs = logs[start:end]

        return paginated_logs, total

    def _apply_filter(
        self, logs: list[AuditLog], filter_params: AuditLogFilter
    ) -> list[AuditLog]:
        """í•„í„° ì ìš©"""
        filtered = logs

        if filter_params.user_id:
            filtered = [l for l in filtered if l.user_id == filter_params.user_id]

        if filter_params.action:
            filtered = [l for l in filtered if l.action == filter_params.action]

        if filter_params.resource_type:
            filtered = [
                l for l in filtered if l.resource_type == filter_params.resource_type
            ]

        if filter_params.environment_id:
            filtered = [
                l for l in filtered if l.environment_id == filter_params.environment_id
            ]

        if filter_params.start_date:
            filtered = [
                l for l in filtered if l.timestamp >= filter_params.start_date
            ]

        if filter_params.end_date:
            filtered = [l for l in filtered if l.timestamp <= filter_params.end_date]

        if filter_params.success is not None:
            filtered = [l for l in filtered if l.success == filter_params.success]

        return filtered

    def get_log_by_id(self, log_id: str) -> Optional[AuditLog]:
        """íŠ¹ì • ë¡œê·¸ ì¡°íšŒ"""
        try:
            with open(self.logs_file, "r") as f:
                for line in f:
                    if line.strip():
                        try:
                            data = json.loads(line)
                            if data.get("id") == log_id:
                                return AuditLog(**data)
                        except (json.JSONDecodeError, Exception):
                            continue
        except FileNotFoundError:
            pass

        return None

    def get_user_activity(
        self, user_id: str, limit: int = 100
    ) -> list[AuditLog]:
        """ì‚¬ìš©ì í™œë™ ì´ë ¥ ì¡°íšŒ"""
        logs, _ = self.get_logs(
            filter_params=AuditLogFilter(user_id=user_id),
            page=1,
            page_size=limit,
        )
        return logs

    def get_resource_history(
        self, resource_type: str, resource_id: str, limit: int = 100
    ) -> list[AuditLog]:
        """ë¦¬ì†ŒìŠ¤ ë³€ê²½ ì´ë ¥ ì¡°íšŒ"""
        logs = []

        try:
            with open(self.logs_file, "r") as f:
                for line in f:
                    if line.strip():
                        try:
                            data = json.loads(line)
                            if (
                                data.get("resource_type") == resource_type
                                and data.get("resource_id") == resource_id
                            ):
                                logs.append(AuditLog(**data))
                        except (json.JSONDecodeError, Exception):
                            continue
        except FileNotFoundError:
            pass

        logs.sort(key=lambda x: x.timestamp, reverse=True)
        return logs[:limit]

    def get_statistics(
        self, start_date: Optional[datetime] = None, end_date: Optional[datetime] = None
    ) -> dict[str, Any]:
        """ê°ì‚¬ ë¡œê·¸ í†µê³„"""
        logs, total = self.get_logs(
            filter_params=AuditLogFilter(start_date=start_date, end_date=end_date),
            page=1,
            page_size=self.max_logs,
        )

        # ì•¡ì…˜ë³„ í†µê³„
        action_counts = {}
        for log in logs:
            action = log.action.value
            action_counts[action] = action_counts.get(action, 0) + 1

        # ì‚¬ìš©ìë³„ í†µê³„
        user_counts = {}
        for log in logs:
            user = log.username
            user_counts[user] = user_counts.get(user, 0) + 1

        # ë¦¬ì†ŒìŠ¤ íƒ€ì…ë³„ í†µê³„
        resource_counts = {}
        for log in logs:
            resource = log.resource_type
            resource_counts[resource] = resource_counts.get(resource, 0) + 1

        # ì„±ê³µ/ì‹¤íŒ¨ í†µê³„
        success_count = sum(1 for l in logs if l.success)
        failure_count = total - success_count

        return {
            "total_logs": total,
            "action_counts": action_counts,
            "user_counts": user_counts,
            "resource_counts": resource_counts,
            "success_count": success_count,
            "failure_count": failure_count,
        }

    def clear_old_logs(self, days: int = 90) -> int:
        """ì˜¤ë˜ëœ ë¡œê·¸ ì‚­ì œ"""
        cutoff = datetime.utcnow().replace(
            hour=0, minute=0, second=0, microsecond=0
        )
        from datetime import timedelta

        cutoff = cutoff - timedelta(days=days)

        kept_logs = []
        deleted_count = 0

        try:
            with open(self.logs_file, "r") as f:
                for line in f:
                    if line.strip():
                        try:
                            data = json.loads(line)
                            log_time = datetime.fromisoformat(
                                data.get("timestamp", "").replace("Z", "+00:00")
                            )
                            if log_time >= cutoff:
                                kept_logs.append(line)
                            else:
                                deleted_count += 1
                        except (json.JSONDecodeError, Exception):
                            kept_logs.append(line)

            with open(self.logs_file, "w") as f:
                f.writelines(kept_logs)

        except FileNotFoundError:
            pass

        return deleted_count

```

---

## backend/admin-dashboard/api/services/auth_service.py

```py
"""
Auth Service - ì¸ì¦/ê¶Œí•œ ê´€ë¦¬ ì„œë¹„ìŠ¤

Refresh Token ë³´ì•ˆ:
- Refresh tokenì€ HTTP-Only, Secure ì¿ í‚¤ë¡œ ì „ì†¡
- Redisì— refresh token í•´ì‹œë¥¼ ì €ì¥í•˜ì—¬ ê²€ì¦
- Token Rotation: ê°±ì‹  ì‹œ ê¸°ì¡´ í† í° íê¸°, ìƒˆ í† í° ë°œê¸‰
"""

import asyncio
import hashlib
import os
import random
import secrets
import string
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional
from uuid import uuid4

import yaml
from jose import JWTError, jwt

from ..models.schemas import Token, TokenData, User, UserCreate, UserRole, SetupStatus


class RedisClient:
    """ê°„ë‹¨í•œ ë¹„ë™ê¸° Redis í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self, host: str = "redis", port: int = 6379):
        self.host = host
        self.port = port
        self._reader: Optional[asyncio.StreamReader] = None
        self._writer: Optional[asyncio.StreamWriter] = None

    async def connect(self) -> bool:
        """Redis ì—°ê²°"""
        try:
            self._reader, self._writer = await asyncio.wait_for(
                asyncio.open_connection(self.host, self.port), timeout=5.0
            )
            return True
        except Exception:
            return False

    async def close(self):
        """ì—°ê²° ì¢…ë£Œ"""
        if self._writer:
            self._writer.close()
            await self._writer.wait_closed()
            self._writer = None
            self._reader = None

    async def _send_command(self, *args) -> Optional[str]:
        """Redis ëª…ë ¹ ì „ì†¡"""
        try:
            if not self._writer:
                if not await self.connect():
                    return None

            # RESP í”„ë¡œí† ì½œë¡œ ëª…ë ¹ ì¸ì½”ë”©
            cmd = f"*{len(args)}\r\n"
            for arg in args:
                arg_str = str(arg)
                cmd += f"${len(arg_str)}\r\n{arg_str}\r\n"

            self._writer.write(cmd.encode())
            await self._writer.drain()

            # ì‘ë‹µ ì½ê¸°
            response = await asyncio.wait_for(self._reader.readline(), timeout=5.0)
            response_str = response.decode("utf-8", errors="ignore").strip()

            # Simple string (+OK)
            if response_str.startswith("+"):
                return response_str[1:]
            # Error (-ERR)
            elif response_str.startswith("-"):
                return None
            # Integer (:1)
            elif response_str.startswith(":"):
                return response_str[1:]
            # Bulk string ($5\r\nhello)
            elif response_str.startswith("$"):
                length = int(response_str[1:])
                if length == -1:
                    return None
                data = await self._reader.read(length + 2)  # +2 for \r\n
                return data[:-2].decode("utf-8", errors="ignore")
            # Null
            elif response_str == "$-1":
                return None

            return response_str
        except Exception:
            await self.close()
            return None

    async def set(self, key: str, value: str, ex: Optional[int] = None) -> bool:
        """í‚¤-ê°’ ì„¤ì • (ì„ íƒì  TTL)"""
        if ex:
            result = await self._send_command("SET", key, value, "EX", str(ex))
        else:
            result = await self._send_command("SET", key, value)
        return result == "OK"

    async def get(self, key: str) -> Optional[str]:
        """í‚¤ë¡œ ê°’ ì¡°íšŒ"""
        return await self._send_command("GET", key)

    async def delete(self, key: str) -> bool:
        """í‚¤ ì‚­ì œ"""
        result = await self._send_command("DEL", key)
        return result == "1"

    async def exists(self, key: str) -> bool:
        """í‚¤ ì¡´ì¬ ì—¬ë¶€"""
        result = await self._send_command("EXISTS", key)
        return result == "1"

    async def keys(self, pattern: str) -> list[str]:
        """íŒ¨í„´ìœ¼ë¡œ í‚¤ ê²€ìƒ‰"""
        try:
            if not self._writer:
                if not await self.connect():
                    return []

            cmd = f"*2\r\n$4\r\nKEYS\r\n${len(pattern)}\r\n{pattern}\r\n"
            self._writer.write(cmd.encode())
            await self._writer.drain()

            # Array ì‘ë‹µ ì½ê¸°
            response = await asyncio.wait_for(self._reader.readline(), timeout=5.0)
            response_str = response.decode("utf-8", errors="ignore").strip()

            if not response_str.startswith("*"):
                return []

            count = int(response_str[1:])
            if count <= 0:
                return []

            keys = []
            for _ in range(count):
                # Bulk string length
                length_line = await self._reader.readline()
                length = int(length_line.decode().strip()[1:])
                # Bulk string value
                data = await self._reader.read(length + 2)
                keys.append(data[:-2].decode("utf-8", errors="ignore"))

            return keys
        except Exception:
            return []


class AuthService:
    """ì¸ì¦/ê¶Œí•œ ì„œë¹„ìŠ¤"""

    # Redis í‚¤ ì ‘ë‘ì‚¬
    REFRESH_TOKEN_PREFIX = "refresh_token:"
    USER_TOKENS_PREFIX = "user_tokens:"

    def __init__(
        self,
        config_dir: str,
        secret_key: Optional[str] = None,
        algorithm: str = "HS256",
        access_token_expire_minutes: int = 60,
        refresh_token_expire_days: int = 7,
    ):
        self.config_dir = Path(config_dir)
        self.secret_key = secret_key or secrets.token_urlsafe(32)
        self.algorithm = algorithm
        self.access_token_expire_minutes = access_token_expire_minutes
        self.refresh_token_expire_days = refresh_token_expire_days
        self.users: dict[str, dict] = {}
        self.email_verifications: dict[str, dict] = {}

        # Redis ì„¤ì •
        self.redis_host = os.environ.get("REDIS_HOST", "redis")
        self.redis_port = int(os.environ.get("REDIS_PORT", "6379"))
        self._redis: Optional[RedisClient] = None

        # í´ë°±ìš© ë©”ëª¨ë¦¬ ì €ì¥ì†Œ (Redis ì—°ê²° ì‹¤íŒ¨ ì‹œ)
        self._memory_tokens: dict[str, dict] = {}

        self._load_users()

    async def _get_redis(self) -> Optional[RedisClient]:
        """Redis í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
        if self._redis is None:
            self._redis = RedisClient(self.redis_host, self.redis_port)
            if not await self._redis.connect():
                self._redis = None
        return self._redis

    def _hash_token(self, token: str) -> str:
        """í† í° í•´ì‹œ (SHA256)"""
        return hashlib.sha256(token.encode()).hexdigest()

    async def _store_refresh_token(
        self, jti: str, user_id: str, token_hash: str, expires_in_seconds: int
    ) -> bool:
        """Redisì— refresh token ì €ì¥"""
        redis = await self._get_redis()

        data = f"{user_id}:{token_hash}:{datetime.utcnow().isoformat()}"

        if redis:
            # Redisì— ì €ì¥
            success = await redis.set(
                f"{self.REFRESH_TOKEN_PREFIX}{jti}", data, ex=expires_in_seconds
            )
            if success:
                return True

        # Redis ì‹¤íŒ¨ ì‹œ ë©”ëª¨ë¦¬ í´ë°±
        self._memory_tokens[jti] = {
            "user_id": user_id,
            "token_hash": token_hash,
            "created_at": datetime.utcnow().isoformat(),
            "expires_at": (
                datetime.utcnow() + timedelta(seconds=expires_in_seconds)
            ).isoformat(),
        }
        return True

    async def _verify_refresh_token(self, jti: str, token_hash: str) -> Optional[str]:
        """Redisì—ì„œ refresh token ê²€ì¦, ìœ íš¨í•˜ë©´ user_id ë°˜í™˜"""
        redis = await self._get_redis()

        if redis:
            data = await redis.get(f"{self.REFRESH_TOKEN_PREFIX}{jti}")
            if data:
                parts = data.split(":", 2)
                if len(parts) >= 2:
                    stored_user_id, stored_hash = parts[0], parts[1]
                    # í•´ì‹œ ë¹„êµ
                    if secrets.compare_digest(stored_hash, token_hash):
                        return stored_user_id
            return None

        # Redis ì‹¤íŒ¨ ì‹œ ë©”ëª¨ë¦¬ í´ë°±
        token_data = self._memory_tokens.get(jti)
        if token_data:
            # ë§Œë£Œ í™•ì¸
            expires_at = datetime.fromisoformat(token_data["expires_at"])
            if datetime.utcnow() > expires_at:
                del self._memory_tokens[jti]
                return None

            # í•´ì‹œ ë¹„êµ
            if secrets.compare_digest(token_data["token_hash"], token_hash):
                return token_data["user_id"]

        return None

    async def _revoke_refresh_token(self, jti: str) -> bool:
        """refresh token íê¸°"""
        redis = await self._get_redis()

        if redis:
            return await redis.delete(f"{self.REFRESH_TOKEN_PREFIX}{jti}")

        # ë©”ëª¨ë¦¬ í´ë°±
        if jti in self._memory_tokens:
            del self._memory_tokens[jti]
            return True
        return False

    async def _revoke_all_user_tokens_async(self, user_id: str) -> int:
        """ì‚¬ìš©ìì˜ ëª¨ë“  refresh token íê¸° (ë¹„ë™ê¸°)"""
        revoked = 0
        redis = await self._get_redis()

        if redis:
            keys = await redis.keys(f"{self.REFRESH_TOKEN_PREFIX}*")
            for key in keys:
                data = await redis.get(key)
                if data and data.startswith(f"{user_id}:"):
                    await redis.delete(key)
                    revoked += 1
        else:
            # ë©”ëª¨ë¦¬ í´ë°±
            to_delete = [
                jti
                for jti, data in self._memory_tokens.items()
                if data.get("user_id") == user_id
            ]
            for jti in to_delete:
                del self._memory_tokens[jti]
                revoked += 1

        return revoked

    def _load_users(self) -> None:
        """ì‚¬ìš©ì ì •ë³´ ë¡œë“œ"""
        users_file = self.config_dir / "users.yaml"
        if users_file.exists():
            with open(users_file) as f:
                data = yaml.safe_load(f) or {}
                self.users = data.get("users", {})
        else:
            self._create_default_admin()

    def _create_default_admin(self) -> None:
        """ê¸°ë³¸ ê´€ë¦¬ì ê³„ì • ìƒì„±"""
        admin_id = f"user-{uuid4().hex[:8]}"
        now = datetime.utcnow()

        password_hash = self._hash_password("admin123")

        self.users[admin_id] = {
            "id": admin_id,
            "username": "admin",
            "email": "admin@localhost",
            "password_hash": password_hash,
            "role": UserRole.ADMIN.value,
            "is_active": True,
            "created_at": now.isoformat(),
            "last_login": None,
            "password_change_required": True,
        }

        self._save_users()

    def _save_users(self) -> None:
        """ì‚¬ìš©ì ì •ë³´ ì €ì¥"""
        self.config_dir.mkdir(parents=True, exist_ok=True)
        users_file = self.config_dir / "users.yaml"

        data = {"users": self.users}

        with open(users_file, "w") as f:
            yaml.dump(data, f, default_flow_style=False, allow_unicode=True)

    def _hash_password(self, password: str) -> str:
        """ë¹„ë°€ë²ˆí˜¸ í•´ì‹œ"""
        return hashlib.sha256(password.encode()).hexdigest()

    def _verify_password(self, password: str, password_hash: str) -> bool:
        """ë¹„ë°€ë²ˆí˜¸ ê²€ì¦"""
        return self._hash_password(password) == password_hash

    def authenticate(self, username: str, password: str) -> Optional[User]:
        """ì‚¬ìš©ì ì¸ì¦"""
        for user_data in self.users.values():
            if user_data.get("username") == username:
                if not user_data.get("is_active", False):
                    return None

                if self._verify_password(password, user_data.get("password_hash", "")):
                    user_data["last_login"] = datetime.utcnow().isoformat()
                    self._save_users()

                    return User(
                        id=user_data["id"],
                        username=user_data["username"],
                        email=user_data.get("email"),
                        role=UserRole(user_data["role"]),
                        is_active=user_data["is_active"],
                        created_at=datetime.fromisoformat(user_data["created_at"]),
                        last_login=datetime.fromisoformat(user_data["last_login"])
                        if user_data.get("last_login")
                        else None,
                        password_change_required=user_data.get(
                            "password_change_required", False
                        ),
                    )

        return None

    async def create_tokens(self, user: User) -> tuple[str, str, str]:
        """ì•¡ì„¸ìŠ¤ í† í°ê³¼ ë¦¬í”„ë ˆì‹œ í† í° ìƒì„±

        Returns:
            tuple: (access_token, refresh_token, jti)
        """
        access_expire = datetime.utcnow() + timedelta(
            minutes=self.access_token_expire_minutes
        )
        refresh_expire = datetime.utcnow() + timedelta(
            days=self.refresh_token_expire_days
        )

        jti = secrets.token_urlsafe(32)

        # Access token (ì§§ì€ ìˆ˜ëª…, í´ë¼ì´ì–¸íŠ¸ ì €ì¥)
        access_payload = {
            "sub": user.id,
            "username": user.username,
            "role": user.role.value,
            "exp": access_expire,
            "type": "access",
        }

        # Refresh token (ê¸´ ìˆ˜ëª…, HTTP-Only ì¿ í‚¤)
        refresh_payload = {
            "sub": user.id,
            "exp": refresh_expire,
            "type": "refresh",
            "jti": jti,
        }

        access_token = jwt.encode(
            access_payload, self.secret_key, algorithm=self.algorithm
        )
        refresh_token = jwt.encode(
            refresh_payload, self.secret_key, algorithm=self.algorithm
        )

        # Refresh token í•´ì‹œë¥¼ Redisì— ì €ì¥
        token_hash = self._hash_token(refresh_token)
        expires_in = self.refresh_token_expire_days * 24 * 60 * 60

        await self._store_refresh_token(jti, user.id, token_hash, expires_in)

        return access_token, refresh_token, jti

    def create_access_token(self, user: User) -> Token:
        """ë™ê¸° ë²„ì „: ì•¡ì„¸ìŠ¤ í† í°ë§Œ ìƒì„± (ê¸°ì¡´ í˜¸í™˜ì„±)

        Note: refresh tokenì€ create_tokens_async ì‚¬ìš© ê¶Œì¥
        """
        access_expire = datetime.utcnow() + timedelta(
            minutes=self.access_token_expire_minutes
        )

        access_payload = {
            "sub": user.id,
            "username": user.username,
            "role": user.role.value,
            "exp": access_expire,
            "type": "access",
        }

        access_token = jwt.encode(
            access_payload, self.secret_key, algorithm=self.algorithm
        )

        # ë™ê¸° ë²„ì „ì—ì„œëŠ” refresh tokenì„ ë¹ˆ ë¬¸ìì—´ë¡œ ë°˜í™˜
        # ì‹¤ì œ ì‚¬ìš© ì‹œ create_tokens_async ì‚¬ìš© ê¶Œì¥
        return Token(
            access_token=access_token,
            refresh_token="",
            token_type="bearer",
            expires_in=self.access_token_expire_minutes * 60,
            refresh_expires_in=0,
        )

    async def refresh_access_token(self, refresh_token: str) -> Optional[Token]:
        """ë¦¬í”„ë ˆì‹œ í† í°ìœ¼ë¡œ ìƒˆ í† í° ë°œê¸‰

        - ê¸°ì¡´ refresh token ê²€ì¦ (í•´ì‹œ ë¹„êµ)
        - ê¸°ì¡´ í† í° íê¸° (Token Rotation)
        - ìƒˆ í† í° ìŒ ë°œê¸‰
        """
        try:
            payload = jwt.decode(
                refresh_token, self.secret_key, algorithms=[self.algorithm]
            )

            if payload.get("type") != "refresh":
                return None

            user_id = payload.get("sub")
            jti = payload.get("jti")

            if not user_id or not jti:
                return None

            # Redisì—ì„œ í† í° í•´ì‹œ ê²€ì¦
            token_hash = self._hash_token(refresh_token)
            verified_user_id = await self._verify_refresh_token(jti, token_hash)

            if not verified_user_id or verified_user_id != user_id:
                return None

            # ì‚¬ìš©ì í™•ì¸
            user = self.get_user(user_id)
            if not user or not user.is_active:
                return None

            # ê¸°ì¡´ í† í° íê¸° (Token Rotation)
            await self._revoke_refresh_token(jti)

            # ìƒˆ í† í° ìŒ ë°œê¸‰
            new_access, new_refresh, _ = await self.create_tokens(user)

            return Token(
                access_token=new_access,
                refresh_token=new_refresh,
                token_type="bearer",
                expires_in=self.access_token_expire_minutes * 60,
                refresh_expires_in=self.refresh_token_expire_days * 24 * 60 * 60,
            )

        except JWTError:
            return None

    async def revoke_refresh_token(self, refresh_token: str) -> bool:
        """ë¦¬í”„ë ˆì‹œ í† í° íê¸°"""
        try:
            payload = jwt.decode(
                refresh_token, self.secret_key, algorithms=[self.algorithm]
            )
            jti = payload.get("jti")

            if jti:
                return await self._revoke_refresh_token(jti)
            return False
        except JWTError:
            return False

    def revoke_all_user_tokens(self, user_id: str) -> int:
        """ì‚¬ìš©ìì˜ ëª¨ë“  ë¦¬í”„ë ˆì‹œ í† í° íê¸° (ë™ê¸° ë˜í¼)"""
        try:
            loop = asyncio.get_event_loop()
            return loop.run_until_complete(self._revoke_all_user_tokens_async(user_id))
        except RuntimeError:
            # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ê±°ë‚˜ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš°
            # ë©”ëª¨ë¦¬ í´ë°±ë§Œ ì²˜ë¦¬
            to_delete = [
                jti
                for jti, data in self._memory_tokens.items()
                if data.get("user_id") == user_id
            ]
            for jti in to_delete:
                del self._memory_tokens[jti]
            return len(to_delete)

    async def revoke_all_user_tokens_async(self, user_id: str) -> int:
        """ì‚¬ìš©ìì˜ ëª¨ë“  ë¦¬í”„ë ˆì‹œ í† í° íê¸° (ë¹„ë™ê¸°)"""
        return await self._revoke_all_user_tokens_async(user_id)

    def verify_token(self, token: str) -> Optional[TokenData]:
        """í† í° ê²€ì¦"""
        try:
            payload = jwt.decode(token, self.secret_key, algorithms=[self.algorithm])

            user_id = payload.get("sub")
            username = payload.get("username")
            role = payload.get("role")
            exp = payload.get("exp")

            if not user_id or not username or not role or not exp:
                return None

            return TokenData(
                user_id=str(user_id),
                username=str(username),
                role=UserRole(role),
                exp=datetime.fromtimestamp(float(exp)),
            )

        except JWTError:
            return None

    def get_user(self, user_id: str) -> Optional[User]:
        """ì‚¬ìš©ì ì¡°íšŒ"""
        user_data = self.users.get(user_id)
        if not user_data:
            return None

        return User(
            id=user_data["id"],
            username=user_data["username"],
            email=user_data.get("email"),
            role=UserRole(user_data["role"]),
            is_active=user_data["is_active"],
            created_at=datetime.fromisoformat(user_data["created_at"]),
            last_login=datetime.fromisoformat(user_data["last_login"])
            if user_data.get("last_login")
            else None,
            password_change_required=user_data.get("password_change_required", False),
        )

    def get_user_by_username(self, username: str) -> Optional[User]:
        """ì‚¬ìš©ìëª…ìœ¼ë¡œ ì¡°íšŒ"""
        for user_data in self.users.values():
            if user_data.get("username") == username:
                return self.get_user(user_data["id"])
        return None

    def get_user_by_email(self, email: str) -> Optional[User]:
        """ì´ë©”ì¼ë¡œ ì¡°íšŒ"""
        for user_data in self.users.values():
            if user_data.get("email") == email:
                return self.get_user(user_data["id"])
        return None

    def list_users(self, active_only: bool = False) -> list[User]:
        """ì‚¬ìš©ì ëª©ë¡ ì¡°íšŒ"""
        users = []
        for user_data in self.users.values():
            if active_only and not user_data.get("is_active", False):
                continue

            users.append(
                User(
                    id=user_data["id"],
                    username=user_data["username"],
                    email=user_data.get("email"),
                    role=UserRole(user_data["role"]),
                    is_active=user_data["is_active"],
                    created_at=datetime.fromisoformat(user_data["created_at"]),
                    last_login=datetime.fromisoformat(user_data["last_login"])
                    if user_data.get("last_login")
                    else None,
                    password_change_required=user_data.get(
                        "password_change_required", False
                    ),
                )
            )

        return users

    def create_user(self, data: UserCreate) -> User:
        """ì‚¬ìš©ì ìƒì„±"""
        if self.get_user_by_username(data.username):
            raise ValueError(f"Username already exists: {data.username}")

        user_id = f"user-{uuid4().hex[:8]}"
        now = datetime.utcnow()

        self.users[user_id] = {
            "id": user_id,
            "username": data.username,
            "email": data.email,
            "password_hash": self._hash_password(data.password),
            "role": data.role.value,
            "is_active": data.is_active,
            "created_at": now.isoformat(),
            "last_login": None,
            "password_change_required": False,
        }

        self._save_users()

        return User(
            id=user_id,
            username=data.username,
            email=data.email,
            role=data.role,
            is_active=data.is_active,
            created_at=now,
            last_login=None,
            password_change_required=False,
        )

    def register_user(self, username: str, email: str, password: str) -> User:
        """ì¼ë°˜ ì‚¬ìš©ì íšŒì›ê°€ì…"""
        if self.get_user_by_username(username):
            raise ValueError(f"ì´ë¯¸ ì‚¬ìš© ì¤‘ì¸ ì‚¬ìš©ìëª…ì…ë‹ˆë‹¤: {username}")

        if self.get_user_by_email(email):
            raise ValueError(f"ì´ë¯¸ ì‚¬ìš© ì¤‘ì¸ ì´ë©”ì¼ì…ë‹ˆë‹¤: {email}")

        user_id = f"user-{uuid4().hex[:8]}"
        now = datetime.utcnow()

        self.users[user_id] = {
            "id": user_id,
            "username": username,
            "email": email,
            "password_hash": self._hash_password(password),
            "role": UserRole.USER.value,
            "is_active": True,
            "created_at": now.isoformat(),
            "last_login": None,
            "password_change_required": False,
        }

        self._save_users()

        return User(
            id=user_id,
            username=username,
            email=email,
            role=UserRole.USER,
            is_active=True,
            created_at=now,
            last_login=None,
            password_change_required=False,
        )

    def update_user(
        self,
        user_id: str,
        email: Optional[str] = None,
        role: Optional[UserRole] = None,
        is_active: Optional[bool] = None,
    ) -> Optional[User]:
        """ì‚¬ìš©ì ì •ë³´ ìˆ˜ì •"""
        user_data = self.users.get(user_id)
        if not user_data:
            return None

        if email is not None:
            user_data["email"] = email
        if role is not None:
            user_data["role"] = role.value
        if is_active is not None:
            user_data["is_active"] = is_active

        self._save_users()
        return self.get_user(user_id)

    def change_password(
        self, user_id: str, old_password: str, new_password: str
    ) -> bool:
        """ë¹„ë°€ë²ˆí˜¸ ë³€ê²½"""
        user_data = self.users.get(user_id)
        if not user_data:
            return False

        if not self._verify_password(old_password, user_data.get("password_hash", "")):
            return False

        user_data["password_hash"] = self._hash_password(new_password)
        user_data["password_change_required"] = False
        self._save_users()
        return True

    def reset_password(self, user_id: str, new_password: str) -> bool:
        """ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™” (ê´€ë¦¬ììš©)"""
        user_data = self.users.get(user_id)
        if not user_data:
            return False

        user_data["password_hash"] = self._hash_password(new_password)
        user_data["password_change_required"] = True
        self._save_users()
        return True

    def delete_user(self, user_id: str) -> bool:
        """ì‚¬ìš©ì ì‚­ì œ"""
        if user_id in self.users:
            del self.users[user_id]
            self._save_users()
            return True
        return False

    def check_permission(self, user_role: UserRole, required_role: UserRole) -> bool:
        """ê¶Œí•œ í™•ì¸"""
        role_priority = {
            UserRole.VIEWER: 0,
            UserRole.OPERATOR: 1,
            UserRole.ADMIN: 2,
        }

        user_level = role_priority.get(user_role, 0)
        required_level = role_priority.get(required_role, 0)

        return user_level >= required_level

    def get_setup_status(self) -> SetupStatus:
        """ì´ˆê¸° ì„¤ì • ìƒíƒœ í™•ì¸"""
        has_users = len(self.users) > 0

        is_default_admin = False
        setup_required = False

        if has_users:
            for user_data in self.users.values():
                if user_data.get("username") == "admin" and user_data.get(
                    "password_change_required", False
                ):
                    is_default_admin = True
                    setup_required = True
                    break
        else:
            setup_required = True

        return SetupStatus(
            setup_required=setup_required,
            has_users=has_users,
            is_default_admin=is_default_admin,
        )

    # ============================================================================
    # Email Verification Methods
    # ============================================================================

    def generate_verification_code(self) -> str:
        """6ìë¦¬ ì¸ì¦ ì½”ë“œ ìƒì„±"""
        return "".join(random.choices(string.digits, k=6))

    def create_email_verification(
        self, email: str, username: str, password: str
    ) -> str:
        """ì´ë©”ì¼ ì¸ì¦ ìš”ì²­ ìƒì„±"""
        if self.get_user_by_email(email):
            raise ValueError(f"ì´ë¯¸ ì‚¬ìš© ì¤‘ì¸ ì´ë©”ì¼ì…ë‹ˆë‹¤: {email}")

        if self.get_user_by_username(username):
            raise ValueError(f"ì´ë¯¸ ì‚¬ìš© ì¤‘ì¸ ì‚¬ìš©ìëª…ì…ë‹ˆë‹¤: {username}")

        code = self.generate_verification_code()
        expires_at = datetime.utcnow() + timedelta(minutes=10)

        self.email_verifications[email] = {
            "code": code,
            "username": username,
            "password_hash": self._hash_password(password),
            "expires_at": expires_at.isoformat(),
            "created_at": datetime.utcnow().isoformat(),
            "attempts": 0,
        }

        return code

    def verify_email_code(self, email: str, code: str) -> User:
        """ì´ë©”ì¼ ì¸ì¦ ì½”ë“œ ê²€ì¦ ë° íšŒì›ê°€ì… ì™„ë£Œ"""
        verification = self.email_verifications.get(email)

        if not verification:
            raise ValueError("ì¸ì¦ ìš”ì²­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

        verification["attempts"] += 1

        if verification["attempts"] > 5:
            del self.email_verifications[email]
            raise ValueError(
                "ì¸ì¦ ì‹œë„ íšŸìˆ˜ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            )

        expires_at = datetime.fromisoformat(verification["expires_at"])
        if datetime.utcnow() > expires_at:
            del self.email_verifications[email]
            raise ValueError("ì¸ì¦ ì½”ë“œê°€ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

        if verification["code"] != code:
            raise ValueError(
                f"ì˜ëª»ëœ ì¸ì¦ ì½”ë“œì…ë‹ˆë‹¤. (ë‚¨ì€ ì‹œë„: {5 - verification['attempts']}íšŒ)"
            )

        username = verification["username"]
        password_hash = verification["password_hash"]

        if self.get_user_by_email(email):
            del self.email_verifications[email]
            raise ValueError(f"ì´ë¯¸ ì‚¬ìš© ì¤‘ì¸ ì´ë©”ì¼ì…ë‹ˆë‹¤: {email}")

        if self.get_user_by_username(username):
            del self.email_verifications[email]
            raise ValueError(f"ì´ë¯¸ ì‚¬ìš© ì¤‘ì¸ ì‚¬ìš©ìëª…ì…ë‹ˆë‹¤: {username}")

        user_id = f"user-{uuid4().hex[:8]}"
        now = datetime.utcnow()

        self.users[user_id] = {
            "id": user_id,
            "username": username,
            "email": email,
            "password_hash": password_hash,
            "role": UserRole.USER.value,
            "is_active": True,
            "created_at": now.isoformat(),
            "last_login": None,
            "password_change_required": False,
            "email_verified": True,
        }

        self._save_users()
        del self.email_verifications[email]

        return User(
            id=user_id,
            username=username,
            email=email,
            role=UserRole.USER,
            is_active=True,
            created_at=now,
            last_login=None,
            password_change_required=False,
        )

    def resend_verification_code(self, email: str) -> str:
        """ì¸ì¦ ì½”ë“œ ì¬ë°œì†¡"""
        verification = self.email_verifications.get(email)

        if not verification:
            raise ValueError(
                "ì¸ì¦ ìš”ì²­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            )

        code = self.generate_verification_code()
        verification["code"] = code
        verification["expires_at"] = (
            datetime.utcnow() + timedelta(minutes=10)
        ).isoformat()
        verification["attempts"] = 0

        return code

```

---

## backend/admin-dashboard/api/services/data_source_service.py

```py
"""
Data Source Management Service
ë°ì´í„° ì†ŒìŠ¤ CRUD ë° ê´€ë¦¬ ì„œë¹„ìŠ¤
"""

import os
import uuid
from datetime import datetime
from pathlib import Path
from typing import Optional
import json
import yaml

try:
    import httpx
except ImportError:
    httpx = None  # type: ignore

from ..models.schemas import (
    DataSource,
    DataSourceCreate,
    DataSourceUpdate,
    DataSourceType,
    DataSourceStatus,
    DataSourceStats,
    DataSourceTestResult,
)


class DataSourceService:
    """ë°ì´í„° ì†ŒìŠ¤ ê´€ë¦¬ ì„œë¹„ìŠ¤"""

    def __init__(
        self,
        project_root: str,
        config_dir: str,
        collector_service_url: Optional[str] = None,
    ):
        self.project_root = Path(project_root)
        self.config_dir = Path(config_dir)
        self.config_file = self.config_dir / "data_sources.yaml"
        self._sources: dict[str, DataSource] = {}
        self.collector_service_url = collector_service_url or os.environ.get(
            "COLLECTOR_SERVICE_URL", "http://collector-service:8081"
        )
        self._load_sources()

    def _load_sources(self) -> None:
        """ì„¤ì • íŒŒì¼ì—ì„œ ë°ì´í„° ì†ŒìŠ¤ ë¡œë“œ"""
        if self.config_file.exists():
            with open(self.config_file, "r", encoding="utf-8") as f:
                data = yaml.safe_load(f) or {}
                for source_id, source_data in data.get("sources", {}).items():
                    try:
                        # Enum ë³€í™˜
                        source_data["source_type"] = DataSourceType(
                            source_data.get("source_type", "rss")
                        )
                        source_data["status"] = DataSourceStatus(
                            source_data.get("status", "active")
                        )
                        # datetime ë³€í™˜
                        for dt_field in ["created_at", "updated_at", "last_crawled_at"]:
                            if source_data.get(dt_field) and isinstance(
                                source_data[dt_field], str
                            ):
                                source_data[dt_field] = datetime.fromisoformat(
                                    source_data[dt_field]
                                )

                        self._sources[source_id] = DataSource(
                            id=source_id, **source_data
                        )
                    except Exception as e:
                        print(f"Error loading source {source_id}: {e}")

    def _save_sources(self) -> None:
        """ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì„¤ì • íŒŒì¼ì— ì €ì¥"""
        self.config_dir.mkdir(parents=True, exist_ok=True)

        data = {"sources": {}}
        for source_id, source in self._sources.items():
            source_dict = source.model_dump()
            # Enumì„ ë¬¸ìì—´ë¡œ ë³€í™˜
            source_dict["source_type"] = (
                source_dict["source_type"].value
                if hasattr(source_dict["source_type"], "value")
                else source_dict["source_type"]
            )
            source_dict["status"] = (
                source_dict["status"].value
                if hasattr(source_dict["status"], "value")
                else source_dict["status"]
            )
            # datetimeì„ ISO ë¬¸ìì—´ë¡œ ë³€í™˜
            for dt_field in ["created_at", "updated_at", "last_crawled_at"]:
                if source_dict.get(dt_field):
                    source_dict[dt_field] = (
                        source_dict[dt_field].isoformat()
                        if hasattr(source_dict[dt_field], "isoformat")
                        else source_dict[dt_field]
                    )
            # IDëŠ” í‚¤ë¡œ ì‚¬ìš©í•˜ë¯€ë¡œ ì œê±°
            del source_dict["id"]
            data["sources"][source_id] = source_dict

        with open(self.config_file, "w", encoding="utf-8") as f:
            yaml.dump(data, f, allow_unicode=True, default_flow_style=False)

    def list_sources(
        self,
        source_type: Optional[DataSourceType] = None,
        status: Optional[DataSourceStatus] = None,
        category: Optional[str] = None,
        is_active: Optional[bool] = None,
    ) -> list[DataSource]:
        """ë°ì´í„° ì†ŒìŠ¤ ëª©ë¡ ì¡°íšŒ"""
        sources = list(self._sources.values())

        if source_type:
            sources = [s for s in sources if s.source_type == source_type]
        if status:
            sources = [s for s in sources if s.status == status]
        if category:
            sources = [s for s in sources if s.category == category]
        if is_active is not None:
            sources = [s for s in sources if s.is_active == is_active]

        return sorted(sources, key=lambda x: (-x.priority, x.name))

    def get_source(self, source_id: str) -> Optional[DataSource]:
        """íŠ¹ì • ë°ì´í„° ì†ŒìŠ¤ ì¡°íšŒ"""
        return self._sources.get(source_id)

    def create_source(self, data: DataSourceCreate) -> DataSource:
        """ìƒˆ ë°ì´í„° ì†ŒìŠ¤ ìƒì„±"""
        source_id = str(uuid.uuid4())[:8]
        now = datetime.utcnow()

        source = DataSource(
            id=source_id,
            name=data.name,
            source_type=data.source_type,
            url=data.url,
            description=data.description,
            category=data.category,
            language=data.language,
            is_active=data.is_active,
            crawl_interval_minutes=data.crawl_interval_minutes,
            priority=data.priority,
            config=data.config,
            status=DataSourceStatus.ACTIVE
            if data.is_active
            else DataSourceStatus.INACTIVE,
            created_at=now,
            updated_at=now,
        )

        self._sources[source_id] = source
        self._save_sources()
        return source

    def update_source(
        self, source_id: str, data: DataSourceUpdate
    ) -> Optional[DataSource]:
        """ë°ì´í„° ì†ŒìŠ¤ ìˆ˜ì •"""
        source = self._sources.get(source_id)
        if not source:
            return None

        update_data = data.model_dump(exclude_unset=True)
        update_data["updated_at"] = datetime.utcnow()

        # is_active ë³€ê²½ ì‹œ statusë„ ì—…ë°ì´íŠ¸
        if "is_active" in update_data:
            if update_data["is_active"]:
                update_data["status"] = DataSourceStatus.ACTIVE
            else:
                update_data["status"] = DataSourceStatus.INACTIVE

        for key, value in update_data.items():
            if hasattr(source, key):
                setattr(source, key, value)

        self._save_sources()
        return source

    def delete_source(self, source_id: str) -> bool:
        """ë°ì´í„° ì†ŒìŠ¤ ì‚­ì œ"""
        if source_id in self._sources:
            del self._sources[source_id]
            self._save_sources()
            return True
        return False

    async def test_source(self, source_id: str) -> DataSourceTestResult:
        """ë°ì´í„° ì†ŒìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        source = self._sources.get(source_id)
        if not source:
            return DataSourceTestResult(
                source_id=source_id,
                success=False,
                message="Source not found",
                tested_at=datetime.utcnow(),
            )

        if httpx is None:
            return DataSourceTestResult(
                source_id=source_id,
                success=False,
                message="httpx not installed",
                tested_at=datetime.utcnow(),
            )

        # ì†ŒìŠ¤ URLì— ì§ì ‘ ìš”ì²­
        try:
            start_time = datetime.utcnow()
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(source.url)
                response_time = (datetime.utcnow() - start_time).total_seconds() * 1000

                if response.status_code == 200:
                    # ì†ŒìŠ¤ íƒ€ì…ì— ë”°ë¥¸ ìƒ˜í”Œ ë°ì´í„° íŒŒì‹±
                    sample_data = None
                    if source.source_type == DataSourceType.RSS:
                        sample_data = {
                            "content_type": response.headers.get(
                                "content-type", "unknown"
                            )
                        }
                    elif source.source_type == DataSourceType.API:
                        try:
                            sample_data = response.json()
                        except Exception:
                            sample_data = {"raw_length": len(response.text)}

                    # í…ŒìŠ¤íŠ¸ ì„±ê³µ ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸
                    source.status = DataSourceStatus.ACTIVE
                    self._save_sources()

                    return DataSourceTestResult(
                        source_id=source_id,
                        success=True,
                        message="Connection successful",
                        response_time_ms=response_time,
                        sample_data=sample_data,
                        tested_at=datetime.utcnow(),
                    )
                else:
                    return DataSourceTestResult(
                        source_id=source_id,
                        success=False,
                        message=f"HTTP {response.status_code}",
                        response_time_ms=response_time,
                        tested_at=datetime.utcnow(),
                    )
        except Exception as e:
            # í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸
            source.status = DataSourceStatus.ERROR
            self._save_sources()

            return DataSourceTestResult(
                source_id=source_id,
                success=False,
                message=f"Error: {str(e)}",
                tested_at=datetime.utcnow(),
            )

    async def trigger_crawl(self, source_id: str) -> dict:
        """ë°ì´í„° ìˆ˜ì§‘ íŠ¸ë¦¬ê±°"""
        source = self._sources.get(source_id)
        if not source:
            return {"success": False, "message": "Source not found"}

        if not source.is_active:
            return {"success": False, "message": "Source is not active"}

        if httpx is None:
            return {"success": False, "message": "httpx not installed"}

        # Collector Serviceì— ìˆ˜ì§‘ ìš”ì²­
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    f"{self.collector_service_url}/api/v1/crawl/trigger",
                    json={
                        "source_id": source_id,
                        "source_url": source.url,
                        "source_type": source.source_type.value,
                    },
                )

                if response.status_code in [200, 202]:
                    source.last_crawled_at = datetime.utcnow()
                    self._save_sources()
                    return {"success": True, "message": "Crawl triggered successfully"}
                else:
                    return {
                        "success": False,
                        "message": f"Failed: HTTP {response.status_code}",
                    }
        except Exception as e:
            return {"success": False, "message": f"Error: {str(e)}"}

    def get_categories(self) -> list[str]:
        """ëª¨ë“  ì¹´í…Œê³ ë¦¬ ëª©ë¡ ì¡°íšŒ"""
        categories = set()
        for source in self._sources.values():
            if source.category:
                categories.add(source.category)
        return sorted(categories)

    def get_stats(self) -> dict:
        """ë°ì´í„° ì†ŒìŠ¤ í†µê³„"""
        total = len(self._sources)
        active = sum(1 for s in self._sources.values() if s.is_active)
        by_type = {}
        by_status = {}

        for source in self._sources.values():
            type_key = source.source_type.value
            by_type[type_key] = by_type.get(type_key, 0) + 1

            status_key = source.status.value
            by_status[status_key] = by_status.get(status_key, 0) + 1

        total_articles = sum(s.total_articles for s in self._sources.values())

        return {
            "total_sources": total,
            "active_sources": active,
            "inactive_sources": total - active,
            "by_type": by_type,
            "by_status": by_status,
            "total_articles": total_articles,
        }

    def bulk_toggle_active(self, source_ids: list[str], is_active: bool) -> int:
        """ì—¬ëŸ¬ ì†ŒìŠ¤ í™œì„±í™”/ë¹„í™œì„±í™”"""
        updated = 0
        for source_id in source_ids:
            source = self._sources.get(source_id)
            if source:
                source.is_active = is_active
                source.status = (
                    DataSourceStatus.ACTIVE if is_active else DataSourceStatus.INACTIVE
                )
                source.updated_at = datetime.utcnow()
                updated += 1

        if updated > 0:
            self._save_sources()

        return updated

```

---

## backend/admin-dashboard/api/services/database_service.py

```py
"""
Database Management Service
PostgreSQL, MongoDB, Redis ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ ì„œë¹„ìŠ¤
"""

import os
from datetime import datetime
from pathlib import Path
from typing import Optional
import json

try:
    import httpx
except ImportError:
    httpx = None  # type: ignore

from ..models.schemas import (
    DatabaseType,
    DatabaseInfo,
    PostgresDatabaseStats,
    PostgresTableInfo,
    MongoDatabaseStats,
    MongoCollectionInfo,
    RedisStats,
    ServiceHealthStatus,
)


def format_bytes(size_bytes: int) -> str:
    """ë°”ì´íŠ¸ë¥¼ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    for unit in ["B", "KB", "MB", "GB", "TB"]:
        if size_bytes < 1024.0:
            return f"{size_bytes:.2f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.2f} PB"


class DatabaseService:
    """ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ ì„œë¹„ìŠ¤"""

    def __init__(self, project_root: str, config_dir: str):
        self.project_root = Path(project_root)
        self.config_dir = Path(config_dir)

        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë³´
        self.postgres_host = os.environ.get("POSTGRES_HOST", "postgres")
        self.postgres_port = int(os.environ.get("POSTGRES_PORT", "5432"))
        self.postgres_db = os.environ.get("POSTGRES_DB", "newsinsight")
        self.postgres_user = os.environ.get("POSTGRES_USER", "postgres")
        self.postgres_password = os.environ.get("POSTGRES_PASSWORD", "postgres")

        self.mongo_host = os.environ.get("MONGO_HOST", "mongo")
        self.mongo_port = int(os.environ.get("MONGO_PORT", "27017"))
        self.mongo_db = os.environ.get("MONGO_DB", "newsinsight")

        self.redis_host = os.environ.get("REDIS_HOST", "redis")
        self.redis_port = int(os.environ.get("REDIS_PORT", "6379"))

        self.timeout = 5.0

    async def get_all_databases(self) -> list[DatabaseInfo]:
        """ëª¨ë“  ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ ì¡°íšŒ"""
        databases = []

        # PostgreSQL
        postgres_info = await self.get_postgres_health()
        databases.append(postgres_info)

        # MongoDB
        mongo_info = await self.get_mongo_health()
        databases.append(mongo_info)

        # Redis
        redis_info = await self.get_redis_health()
        databases.append(redis_info)

        return databases

    async def get_postgres_health(self) -> DatabaseInfo:
        """PostgreSQL í—¬ìŠ¤ ì •ë³´"""
        try:
            # psycopg2 ì—†ì´ TCP ì—°ê²°ë¡œë§Œ ì²´í¬
            import asyncio

            reader, writer = await asyncio.wait_for(
                asyncio.open_connection(self.postgres_host, self.postgres_port),
                timeout=self.timeout,
            )
            writer.close()
            await writer.wait_closed()

            return DatabaseInfo(
                db_type=DatabaseType.POSTGRESQL,
                name=self.postgres_db,
                host=self.postgres_host,
                port=self.postgres_port,
                status=ServiceHealthStatus.HEALTHY,
                version="15.x",  # ì‹¤ì œë¡œëŠ” ì¿¼ë¦¬ë¡œ í™•ì¸ í•„ìš”
                checked_at=datetime.utcnow(),
            )
        except Exception as e:
            return DatabaseInfo(
                db_type=DatabaseType.POSTGRESQL,
                name=self.postgres_db,
                host=self.postgres_host,
                port=self.postgres_port,
                status=ServiceHealthStatus.UNREACHABLE,
                checked_at=datetime.utcnow(),
            )

    async def get_mongo_health(self) -> DatabaseInfo:
        """MongoDB í—¬ìŠ¤ ì •ë³´"""
        try:
            import asyncio

            reader, writer = await asyncio.wait_for(
                asyncio.open_connection(self.mongo_host, self.mongo_port),
                timeout=self.timeout,
            )
            writer.close()
            await writer.wait_closed()

            return DatabaseInfo(
                db_type=DatabaseType.MONGODB,
                name=self.mongo_db,
                host=self.mongo_host,
                port=self.mongo_port,
                status=ServiceHealthStatus.HEALTHY,
                version="7.x",
                checked_at=datetime.utcnow(),
            )
        except Exception as e:
            return DatabaseInfo(
                db_type=DatabaseType.MONGODB,
                name=self.mongo_db,
                host=self.mongo_host,
                port=self.mongo_port,
                status=ServiceHealthStatus.UNREACHABLE,
                checked_at=datetime.utcnow(),
            )

    async def get_redis_health(self) -> DatabaseInfo:
        """Redis í—¬ìŠ¤ ì •ë³´"""
        try:
            import asyncio

            reader, writer = await asyncio.wait_for(
                asyncio.open_connection(self.redis_host, self.redis_port),
                timeout=self.timeout,
            )

            # PING ëª…ë ¹
            writer.write(b"PING\r\n")
            await writer.drain()
            response = await asyncio.wait_for(reader.readline(), timeout=2.0)
            writer.close()
            await writer.wait_closed()

            is_healthy = b"+PONG" in response

            return DatabaseInfo(
                db_type=DatabaseType.REDIS,
                name="redis",
                host=self.redis_host,
                port=self.redis_port,
                status=ServiceHealthStatus.HEALTHY
                if is_healthy
                else ServiceHealthStatus.DEGRADED,
                version="7.x",
                checked_at=datetime.utcnow(),
            )
        except Exception as e:
            return DatabaseInfo(
                db_type=DatabaseType.REDIS,
                name="redis",
                host=self.redis_host,
                port=self.redis_port,
                status=ServiceHealthStatus.UNREACHABLE,
                checked_at=datetime.utcnow(),
            )

    async def get_postgres_stats(self) -> PostgresDatabaseStats:
        """PostgreSQL ìƒì„¸ í†µê³„ (ì‹¤ì œ ì—°ê²° í•„ìš”)"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” psycopg2 ë˜ëŠ” asyncpg ì‚¬ìš©
        # ì—¬ê¸°ì„œëŠ” ìƒ˜í”Œ ë°ì´í„° ë°˜í™˜
        return PostgresDatabaseStats(
            database_name=self.postgres_db,
            size_bytes=0,
            size_human="N/A",
            tables=[
                PostgresTableInfo(
                    schema_name="public",
                    table_name="news_articles",
                    row_count=0,
                    size_bytes=0,
                    size_human="N/A",
                ),
                PostgresTableInfo(
                    schema_name="public",
                    table_name="news_sources",
                    row_count=0,
                    size_bytes=0,
                    size_human="N/A",
                ),
            ],
            total_tables=0,
            total_rows=0,
            connection_count=0,
            max_connections=100,
            checked_at=datetime.utcnow(),
        )

    async def get_mongo_stats(self) -> MongoDatabaseStats:
        """MongoDB ìƒì„¸ í†µê³„"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” pymongo ì‚¬ìš©
        return MongoDatabaseStats(
            database_name=self.mongo_db,
            size_bytes=0,
            size_human="N/A",
            collections=[
                MongoCollectionInfo(
                    collection_name="ai_responses",
                    document_count=0,
                    size_bytes=0,
                    size_human="N/A",
                    index_count=1,
                ),
            ],
            total_collections=0,
            total_documents=0,
            checked_at=datetime.utcnow(),
        )

    async def get_redis_stats(self) -> RedisStats:
        """Redis ìƒì„¸ í†µê³„"""
        try:
            import asyncio

            reader, writer = await asyncio.wait_for(
                asyncio.open_connection(self.redis_host, self.redis_port),
                timeout=self.timeout,
            )

            # INFO ëª…ë ¹
            writer.write(b"INFO\r\n")
            await writer.drain()

            # ì‘ë‹µ ì½ê¸° (bulk string)
            response_lines = []
            while True:
                line = await asyncio.wait_for(reader.readline(), timeout=2.0)
                if not line or line == b"\r\n":
                    break
                response_lines.append(line.decode("utf-8", errors="ignore").strip())

            writer.close()
            await writer.wait_closed()

            # íŒŒì‹±
            info = {}
            for line in response_lines:
                if ":" in line and not line.startswith("#"):
                    key, value = line.split(":", 1)
                    info[key] = value

            used_memory = int(info.get("used_memory", 0))
            keyspace_hits = int(info.get("keyspace_hits", 0))
            keyspace_misses = int(info.get("keyspace_misses", 0))
            total_requests = keyspace_hits + keyspace_misses
            hit_rate = (
                (keyspace_hits / total_requests * 100) if total_requests > 0 else 0.0
            )

            # DB0ì—ì„œ í‚¤ ìˆ˜ ì¶”ì¶œ
            db0_info = info.get("db0", "")
            total_keys = 0
            if db0_info:
                for part in db0_info.split(","):
                    if part.startswith("keys="):
                        total_keys = int(part.split("=")[1])
                        break

            return RedisStats(
                used_memory_bytes=used_memory,
                used_memory_human=format_bytes(used_memory),
                max_memory_bytes=int(info.get("maxmemory", 0)) or None,
                connected_clients=int(info.get("connected_clients", 0)),
                total_keys=total_keys,
                expired_keys=int(info.get("expired_keys", 0)),
                keyspace_hits=keyspace_hits,
                keyspace_misses=keyspace_misses,
                hit_rate=hit_rate,
                uptime_seconds=int(info.get("uptime_in_seconds", 0)),
                checked_at=datetime.utcnow(),
            )
        except Exception as e:
            return RedisStats(
                used_memory_bytes=0,
                used_memory_human="N/A",
                connected_clients=0,
                total_keys=0,
                expired_keys=0,
                keyspace_hits=0,
                keyspace_misses=0,
                hit_rate=0.0,
                uptime_seconds=0,
                checked_at=datetime.utcnow(),
            )

```

---

## backend/admin-dashboard/api/services/document_service.py

```py
"""
Document Service - Markdown ë¬¸ì„œ ê´€ë¦¬ ì„œë¹„ìŠ¤
"""
import os
from datetime import datetime
from pathlib import Path
from typing import Optional
from uuid import uuid4

import yaml

from ..models.schemas import Document, DocumentBase, DocumentCategory


class DocumentService:
    """ë¬¸ì„œ ê´€ë¦¬ ì„œë¹„ìŠ¤"""

    def __init__(self, project_root: str, config_dir: str):
        self.project_root = Path(project_root)
        self.config_dir = Path(config_dir)
        self.docs_dirs = [
            self.project_root / "docs",
            self.project_root / "etc" / "infra-guides",
        ]
        self.documents: dict[str, Document] = {}
        self._scan_documents()

    def _scan_documents(self) -> None:
        """ë¬¸ì„œ ë””ë ‰í† ë¦¬ ìŠ¤ìº”"""
        config_file = self.config_dir / "documents.yaml"

        # ê¸°ì¡´ ì„¤ì • ë¡œë“œ
        existing_config = {}
        if config_file.exists():
            with open(config_file) as f:
                data = yaml.safe_load(f) or {}
                for doc_data in data.get("documents", []):
                    existing_config[doc_data.get("file_path")] = doc_data

        # ë¬¸ì„œ ìŠ¤ìº”
        for docs_dir in self.docs_dirs:
            if not docs_dir.exists():
                continue

            for md_file in docs_dir.rglob("*.md"):
                file_path = str(md_file.relative_to(self.project_root))
                abs_path = str(md_file)

                # ê¸°ì¡´ ì„¤ì •ì´ ìˆìœ¼ë©´ ì‚¬ìš©
                if abs_path in existing_config:
                    doc_data = existing_config[abs_path]
                    doc = Document(**doc_data)
                else:
                    # ìƒˆ ë¬¸ì„œ ìƒì„±
                    doc = self._create_document_from_file(md_file)

                self.documents[doc.id] = doc

        self._save_documents()

    def _create_document_from_file(self, file_path: Path) -> Document:
        """íŒŒì¼ì—ì„œ ë¬¸ì„œ ì •ë³´ ìƒì„±"""
        doc_id = f"doc-{uuid4().hex[:8]}"
        rel_path = str(file_path.relative_to(self.project_root))

        # íŒŒì¼ëª…ì—ì„œ ì œëª© ì¶”ì¶œ
        title = file_path.stem.replace("_", " ").replace("-", " ").title()

        # ì¹´í…Œê³ ë¦¬ ì¶”ë¡ 
        category = self._infer_category(file_path)

        # íƒœê·¸ ì¶”ë¡ 
        tags = self._infer_tags(file_path)

        # ê´€ë ¨ í™˜ê²½ ì¶”ë¡ 
        related_envs = self._infer_environments(file_path)

        # ìˆ˜ì • ì‹œê°„
        stat = file_path.stat()
        last_modified = datetime.fromtimestamp(stat.st_mtime)

        return Document(
            id=doc_id,
            title=title,
            file_path=str(file_path),
            category=category,
            tags=tags,
            related_environments=related_envs,
            related_scripts=[],
            last_modified=last_modified,
        )

    def _infer_category(self, file_path: Path) -> DocumentCategory:
        """íŒŒì¼ ê²½ë¡œì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ë¡ """
        path_str = str(file_path).lower()

        if "deploy" in path_str or "deployment" in path_str:
            return DocumentCategory.DEPLOYMENT
        elif "troubleshoot" in path_str or "debug" in path_str:
            return DocumentCategory.TROUBLESHOOTING
        elif "architecture" in path_str or "overview" in path_str:
            return DocumentCategory.ARCHITECTURE
        elif "runbook" in path_str or "guide" in path_str:
            return DocumentCategory.RUNBOOK
        else:
            return DocumentCategory.GENERAL

    def _infer_tags(self, file_path: Path) -> list[str]:
        """íŒŒì¼ ê²½ë¡œì—ì„œ íƒœê·¸ ì¶”ë¡ """
        tags = []
        path_str = str(file_path).lower()

        tag_keywords = [
            "docker",
            "kubernetes",
            "k8s",
            "consul",
            "cloudflare",
            "gcp",
            "aws",
            "api",
            "frontend",
            "backend",
            "database",
            "redis",
            "postgres",
            "mongo",
            "kafka",
        ]

        for keyword in tag_keywords:
            if keyword in path_str:
                tags.append(keyword)

        return tags

    def _infer_environments(self, file_path: Path) -> list[str]:
        """íŒŒì¼ ê²½ë¡œì—ì„œ ê´€ë ¨ í™˜ê²½ ì¶”ë¡ """
        envs = []
        path_str = str(file_path).lower()

        env_keywords = {
            "zerotrust": "zerotrust",
            "local": "local",
            "gcp": "gcp",
            "aws": "aws",
            "production": "production",
            "staging": "staging",
            "pmx": "production",
        }

        for keyword, env in env_keywords.items():
            if keyword in path_str and env not in envs:
                envs.append(env)

        return envs

    def _save_documents(self) -> None:
        """ë¬¸ì„œ ì„¤ì •ì„ íŒŒì¼ì— ì €ì¥"""
        self.config_dir.mkdir(parents=True, exist_ok=True)
        config_file = self.config_dir / "documents.yaml"

        data = {
            "documents": [doc.model_dump(mode="json") for doc in self.documents.values()]
        }

        # content í•„ë“œëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ
        for doc_data in data["documents"]:
            doc_data.pop("content", None)

        with open(config_file, "w") as f:
            yaml.dump(data, f, default_flow_style=False, allow_unicode=True)

    def list_documents(
        self,
        category: Optional[DocumentCategory] = None,
        tag: Optional[str] = None,
        environment: Optional[str] = None,
        search: Optional[str] = None,
    ) -> list[Document]:
        """ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
        docs = list(self.documents.values())

        if category:
            docs = [d for d in docs if d.category == category]

        if tag:
            docs = [d for d in docs if tag in d.tags]

        if environment:
            docs = [d for d in docs if environment in d.related_environments]

        if search:
            search_lower = search.lower()
            docs = [
                d
                for d in docs
                if search_lower in d.title.lower()
                or any(search_lower in t.lower() for t in d.tags)
            ]

        return sorted(docs, key=lambda x: x.title)

    def get_document(self, doc_id: str) -> Optional[Document]:
        """ë¬¸ì„œ ìƒì„¸ ì¡°íšŒ (ë‚´ìš© í¬í•¨)"""
        doc = self.documents.get(doc_id)
        if not doc:
            return None

        # íŒŒì¼ ë‚´ìš© ì½ê¸°
        file_path = Path(doc.file_path)
        if file_path.exists():
            try:
                with open(file_path, encoding="utf-8") as f:
                    doc.content = f.read()
            except Exception:
                doc.content = "Error reading file content"

        return doc

    def get_document_by_path(self, file_path: str) -> Optional[Document]:
        """íŒŒì¼ ê²½ë¡œë¡œ ë¬¸ì„œ ì¡°íšŒ"""
        for doc in self.documents.values():
            if doc.file_path == file_path:
                return self.get_document(doc.id)
        return None

    def update_document_metadata(
        self,
        doc_id: str,
        title: Optional[str] = None,
        category: Optional[DocumentCategory] = None,
        tags: Optional[list[str]] = None,
        related_environments: Optional[list[str]] = None,
        related_scripts: Optional[list[str]] = None,
    ) -> Optional[Document]:
        """ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìˆ˜ì •"""
        doc = self.documents.get(doc_id)
        if not doc:
            return None

        if title is not None:
            doc.title = title
        if category is not None:
            doc.category = category
        if tags is not None:
            doc.tags = tags
        if related_environments is not None:
            doc.related_environments = related_environments
        if related_scripts is not None:
            doc.related_scripts = related_scripts

        self._save_documents()
        return doc

    def refresh_documents(self) -> int:
        """ë¬¸ì„œ ëª©ë¡ ìƒˆë¡œê³ ì¹¨"""
        old_count = len(self.documents)
        self.documents.clear()
        self._scan_documents()
        return len(self.documents) - old_count

    def get_related_documents(
        self, environment: Optional[str] = None, script_id: Optional[str] = None
    ) -> list[Document]:
        """ê´€ë ¨ ë¬¸ì„œ ì¡°íšŒ"""
        docs = []

        if environment:
            docs.extend(
                [d for d in self.documents.values() if environment in d.related_environments]
            )

        if script_id:
            docs.extend(
                [d for d in self.documents.values() if script_id in d.related_scripts]
            )

        # ì¤‘ë³µ ì œê±°
        seen = set()
        unique_docs = []
        for doc in docs:
            if doc.id not in seen:
                seen.add(doc.id)
                unique_docs.append(doc)

        return unique_docs

    def get_categories_summary(self) -> dict[str, int]:
        """ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ ìˆ˜ ìš”ì•½"""
        summary = {}
        for doc in self.documents.values():
            cat = doc.category.value
            summary[cat] = summary.get(cat, 0) + 1
        return summary

    def get_tags_summary(self) -> dict[str, int]:
        """íƒœê·¸ë³„ ë¬¸ì„œ ìˆ˜ ìš”ì•½"""
        summary = {}
        for doc in self.documents.values():
            for tag in doc.tags:
                summary[tag] = summary.get(tag, 0) + 1
        return summary

```

---

## backend/admin-dashboard/api/services/environment_service.py

```py
"""
Environment Service - í™˜ê²½/í”„ë¡œí•„ ê´€ë¦¬ ì„œë¹„ìŠ¤
"""
import os
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Optional
from uuid import uuid4

import yaml

from ..models.schemas import (
    ContainerInfo,
    Environment,
    EnvironmentCreate,
    EnvironmentStatus,
    EnvironmentType,
    EnvironmentUpdate,
    ServiceStatus,
)


class EnvironmentService:
    """í™˜ê²½ ê´€ë¦¬ ì„œë¹„ìŠ¤"""

    def __init__(self, project_root: str, config_dir: str):
        self.project_root = Path(project_root)
        self.config_dir = Path(config_dir)
        self.environments: dict[str, Environment] = {}
        self._load_environments()

    def _load_environments(self) -> None:
        """ì„¤ì • íŒŒì¼ì—ì„œ í™˜ê²½ ì •ë³´ ë¡œë“œ"""
        config_file = self.config_dir / "environments.yaml"
        if config_file.exists():
            with open(config_file) as f:
                data = yaml.safe_load(f) or {}
                for env_data in data.get("environments", []):
                    env = Environment(**env_data)
                    self.environments[env.id] = env
        else:
            # ê¸°ë³¸ í™˜ê²½ ì„¤ì • ìƒì„±
            self._create_default_environments()

    def _create_default_environments(self) -> None:
        """ê¸°ë³¸ í™˜ê²½ ì„¤ì • ìƒì„±"""
        docker_dir = self.project_root / "etc" / "docker"
        configs_dir = self.project_root / "etc" / "configs"

        default_envs = [
            {
                "id": "env-zerotrust",
                "name": "zerotrust",
                "env_type": EnvironmentType.ZEROTRUST,
                "description": "Cloudflare Zero Trust ê¸°ë°˜ ë³´ì•ˆ í™˜ê²½",
                "compose_file": str(docker_dir / "docker-compose.zerotrust.yml"),
                "env_file": str(docker_dir / ".env"),
                "is_active": True,
                "priority": 100,
                "created_at": datetime.utcnow(),
                "updated_at": datetime.utcnow(),
            },
            {
                "id": "env-production",
                "name": "production",
                "env_type": EnvironmentType.PRODUCTION,
                "description": "í”„ë¡œë•ì…˜ í™˜ê²½",
                "compose_file": str(docker_dir / "docker-compose.production.yml"),
                "env_file": str(configs_dir / "production.env"),
                "is_active": True,
                "priority": 90,
                "created_at": datetime.utcnow(),
                "updated_at": datetime.utcnow(),
            },
            {
                "id": "env-staging",
                "name": "staging",
                "env_type": EnvironmentType.STAGING,
                "description": "ìŠ¤í…Œì´ì§• í™˜ê²½",
                "compose_file": str(docker_dir / "docker-compose.consul.yml"),
                "env_file": str(configs_dir / "staging.env"),
                "is_active": True,
                "priority": 80,
                "created_at": datetime.utcnow(),
                "updated_at": datetime.utcnow(),
            },
            {
                "id": "env-local",
                "name": "local",
                "env_type": EnvironmentType.LOCAL,
                "description": "ë¡œì»¬ ê°œë°œ í™˜ê²½",
                "compose_file": str(docker_dir / "docker-compose.consul.yml"),
                "env_file": str(configs_dir / "development.env"),
                "is_active": True,
                "priority": 70,
                "created_at": datetime.utcnow(),
                "updated_at": datetime.utcnow(),
            },
        ]

        for env_data in default_envs:
            env = Environment(**env_data)
            self.environments[env.id] = env

        self._save_environments()

    def _save_environments(self) -> None:
        """í™˜ê²½ ì„¤ì •ì„ íŒŒì¼ì— ì €ì¥"""
        self.config_dir.mkdir(parents=True, exist_ok=True)
        config_file = self.config_dir / "environments.yaml"

        data = {
            "environments": [
                env.model_dump(mode="json") for env in self.environments.values()
            ]
        }

        with open(config_file, "w") as f:
            yaml.dump(data, f, default_flow_style=False, allow_unicode=True)

    def list_environments(self, active_only: bool = False) -> list[Environment]:
        """í™˜ê²½ ëª©ë¡ ì¡°íšŒ"""
        envs = list(self.environments.values())
        if active_only:
            envs = [e for e in envs if e.is_active]
        return sorted(envs, key=lambda x: -x.priority)

    def get_environment(self, env_id: str) -> Optional[Environment]:
        """í™˜ê²½ ìƒì„¸ ì¡°íšŒ"""
        return self.environments.get(env_id)

    def get_environment_by_name(self, name: str) -> Optional[Environment]:
        """ì´ë¦„ìœ¼ë¡œ í™˜ê²½ ì¡°íšŒ"""
        for env in self.environments.values():
            if env.name == name:
                return env
        return None

    def create_environment(self, data: EnvironmentCreate) -> Environment:
        """í™˜ê²½ ìƒì„±"""
        env_id = f"env-{uuid4().hex[:8]}"
        now = datetime.utcnow()

        env = Environment(
            id=env_id,
            created_at=now,
            updated_at=now,
            **data.model_dump(),
        )

        self.environments[env_id] = env
        self._save_environments()
        return env

    def update_environment(
        self, env_id: str, data: EnvironmentUpdate
    ) -> Optional[Environment]:
        """í™˜ê²½ ìˆ˜ì •"""
        env = self.environments.get(env_id)
        if not env:
            return None

        update_data = data.model_dump(exclude_unset=True)
        for key, value in update_data.items():
            setattr(env, key, value)

        env.updated_at = datetime.utcnow()
        self._save_environments()
        return env

    def delete_environment(self, env_id: str) -> bool:
        """í™˜ê²½ ì‚­ì œ"""
        if env_id in self.environments:
            del self.environments[env_id]
            self._save_environments()
            return True
        return False

    def get_environment_status(self, env_id: str) -> Optional[EnvironmentStatus]:
        """í™˜ê²½ì˜ ì»¨í…Œì´ë„ˆ ìƒíƒœ ì¡°íšŒ"""
        env = self.environments.get(env_id)
        if not env:
            return None

        containers = self._get_docker_containers(env)

        return EnvironmentStatus(
            environment_id=env.id,
            environment_name=env.name,
            containers=containers,
            total_containers=len(containers),
            running_containers=sum(
                1 for c in containers if c.status == ServiceStatus.UP
            ),
        )

    def _get_docker_containers(self, env: Environment) -> list[ContainerInfo]:
        """Docker ì»¨í…Œì´ë„ˆ ìƒíƒœ ì¡°íšŒ"""
        containers = []

        if not Path(env.compose_file).exists():
            return containers

        try:
            # docker compose ps ì‹¤í–‰
            result = subprocess.run(
                [
                    "docker",
                    "compose",
                    "-f",
                    env.compose_file,
                    "-p",
                    "newsinsight",
                    "ps",
                    "--format",
                    "json",
                ],
                capture_output=True,
                text=True,
                timeout=30,
            )

            if result.returncode == 0 and result.stdout.strip():
                import json

                # ê° ì¤„ì´ JSON ê°ì²´ì¼ ìˆ˜ ìˆìŒ
                for line in result.stdout.strip().split("\n"):
                    if line.strip():
                        try:
                            container_data = json.loads(line)
                            status = self._parse_container_status(
                                container_data.get("State", "")
                            )
                            containers.append(
                                ContainerInfo(
                                    name=container_data.get("Name", "unknown"),
                                    image=container_data.get("Image", "unknown"),
                                    status=status,
                                    health=container_data.get("Health", None),
                                    ports=self._parse_ports(
                                        container_data.get("Ports", "")
                                    ),
                                )
                            )
                        except json.JSONDecodeError:
                            continue

        except subprocess.TimeoutExpired:
            pass
        except FileNotFoundError:
            pass

        return containers

    def _parse_container_status(self, state: str) -> ServiceStatus:
        """ì»¨í…Œì´ë„ˆ ìƒíƒœ íŒŒì‹±"""
        state_lower = state.lower()
        if "running" in state_lower:
            return ServiceStatus.UP
        elif "exited" in state_lower or "dead" in state_lower:
            return ServiceStatus.DOWN
        elif "starting" in state_lower or "created" in state_lower:
            return ServiceStatus.STARTING
        elif "stopping" in state_lower or "removing" in state_lower:
            return ServiceStatus.STOPPING
        return ServiceStatus.UNKNOWN

    def _parse_ports(self, ports_str: str) -> list[str]:
        """í¬íŠ¸ ë¬¸ìì—´ íŒŒì‹±"""
        if not ports_str:
            return []
        return [p.strip() for p in ports_str.split(",") if p.strip()]

    async def docker_compose_up(
        self, env_id: str, build: bool = True, detach: bool = True
    ) -> tuple[bool, str]:
        """Docker Compose Up ì‹¤í–‰"""
        env = self.environments.get(env_id)
        if not env:
            return False, "Environment not found"

        cmd = ["docker", "compose", "-f", env.compose_file, "-p", "newsinsight"]

        if build:
            cmd.extend(["up", "--build"])
        else:
            cmd.extend(["up"])

        if detach:
            cmd.append("-d")

        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=600,  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
                cwd=str(self.project_root),
            )
            return result.returncode == 0, result.stdout + result.stderr
        except subprocess.TimeoutExpired:
            return False, "Command timed out"
        except Exception as e:
            return False, str(e)

    async def docker_compose_down(
        self, env_id: str, volumes: bool = False
    ) -> tuple[bool, str]:
        """Docker Compose Down ì‹¤í–‰"""
        env = self.environments.get(env_id)
        if not env:
            return False, "Environment not found"

        cmd = ["docker", "compose", "-f", env.compose_file, "-p", "newsinsight", "down"]

        if volumes:
            cmd.append("-v")

        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=120,
                cwd=str(self.project_root),
            )
            return result.returncode == 0, result.stdout + result.stderr
        except subprocess.TimeoutExpired:
            return False, "Command timed out"
        except Exception as e:
            return False, str(e)

    async def docker_compose_restart(
        self, env_id: str, service: Optional[str] = None
    ) -> tuple[bool, str]:
        """Docker Compose Restart ì‹¤í–‰"""
        env = self.environments.get(env_id)
        if not env:
            return False, "Environment not found"

        cmd = [
            "docker",
            "compose",
            "-f",
            env.compose_file,
            "-p",
            "newsinsight",
            "restart",
        ]

        if service:
            cmd.append(service)

        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=120,
                cwd=str(self.project_root),
            )
            return result.returncode == 0, result.stdout + result.stderr
        except subprocess.TimeoutExpired:
            return False, "Command timed out"
        except Exception as e:
            return False, str(e)

    async def get_service_logs(
        self, env_id: str, service: str, tail: int = 100
    ) -> tuple[bool, str]:
        """ì„œë¹„ìŠ¤ ë¡œê·¸ ì¡°íšŒ"""
        env = self.environments.get(env_id)
        if not env:
            return False, "Environment not found"

        cmd = [
            "docker",
            "compose",
            "-f",
            env.compose_file,
            "-p",
            "newsinsight",
            "logs",
            "--tail",
            str(tail),
            service,
        ]

        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=30,
                cwd=str(self.project_root),
            )
            return result.returncode == 0, result.stdout + result.stderr
        except subprocess.TimeoutExpired:
            return False, "Command timed out"
        except Exception as e:
            return False, str(e)

```

---

## backend/admin-dashboard/api/services/health_service.py

```py
"""
Service Health Monitoring Service
ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬ ë° ìƒíƒœ ëª¨ë‹ˆí„°ë§
"""

import asyncio
import os
from datetime import datetime
from pathlib import Path
from typing import Optional
import json

try:
    import httpx
except ImportError:
    httpx = None  # type: ignore

from ..models.schemas import (
    ServiceHealthStatus,
    ServiceHealth,
    InfrastructureHealth,
    OverallSystemHealth,
)


class HealthService:
    """ì„œë¹„ìŠ¤ í—¬ìŠ¤ ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤"""

    def __init__(self, project_root: str, config_dir: str):
        self.project_root = Path(project_root)
        self.config_dir = Path(config_dir)
        self.services_config_path = (
            self.project_root / "etc" / "configs" / "services.json"
        )
        self._services_config: Optional[dict] = None
        self._last_check: dict[str, ServiceHealth] = {}
        self.timeout = 5.0  # í—¬ìŠ¤ì²´í¬ íƒ€ì„ì•„ì›ƒ (ì´ˆ)

    def _load_services_config(self) -> dict:
        """ì„œë¹„ìŠ¤ ì„¤ì • ë¡œë“œ"""
        if self._services_config is None:
            if self.services_config_path.exists():
                with open(self.services_config_path, "r") as f:
                    self._services_config = json.load(f)
            else:
                self._services_config = {
                    "services": {},
                    "infrastructure": {},
                    "service_urls": {},
                }
        return self._services_config or {}

    def get_all_services(self) -> list[dict]:
        """ëª¨ë“  ì„œë¹„ìŠ¤ ëª©ë¡ ì¡°íšŒ"""
        config = self._load_services_config()
        services = []

        # ë©”ì¸ ì„œë¹„ìŠ¤
        for service_id, service_info in config.get("services", {}).items():
            services.append(
                {
                    "id": service_id,
                    "name": service_info.get("name", service_id),
                    "description": service_info.get("description", ""),
                    "port": service_info.get("port"),
                    "healthcheck": service_info.get("healthcheck", "/health"),
                    "hostname": service_info.get("hostname", service_id),
                    "type": "service",
                    "tags": service_info.get("consul", {}).get("tags", []),
                }
            )

        # ML ì• ë“œì˜¨
        for addon_id, addon_info in config.get("ml-addons", {}).items():
            services.append(
                {
                    "id": addon_id,
                    "name": addon_info.get("name", addon_id),
                    "description": f"ML Addon - {addon_info.get('name', addon_id)}",
                    "port": addon_info.get("port"),
                    "healthcheck": addon_info.get("healthcheck", "/health"),
                    "hostname": addon_id,
                    "type": "ml-addon",
                    "tags": ["ml", "addon"],
                }
            )

        return services

    def get_infrastructure_services(self) -> list[dict]:
        """ì¸í”„ë¼ ì„œë¹„ìŠ¤ ëª©ë¡ ì¡°íšŒ"""
        config = self._load_services_config()
        infra_services = []

        for infra_id, infra_info in config.get("infrastructure", {}).items():
            infra_services.append(
                {
                    "id": infra_id,
                    "name": infra_id.capitalize(),
                    "port": infra_info.get("port"),
                    "image": infra_info.get("image"),
                    "healthcheck": infra_info.get("healthcheck"),
                    "type": "infrastructure",
                }
            )

        return infra_services

    async def check_service_health(self, service_id: str) -> ServiceHealth:
        """ë‹¨ì¼ ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬"""
        config = self._load_services_config()
        service_urls = config.get("service_urls", {})

        # ì„œë¹„ìŠ¤ ì •ë³´ ì°¾ê¸°
        service_info = None
        for services_dict in [config.get("services", {}), config.get("ml-addons", {})]:
            if service_id in services_dict:
                service_info = services_dict[service_id]
                break

        if not service_info:
            return ServiceHealth(
                service_id=service_id,
                name=service_id,
                status=ServiceHealthStatus.UNKNOWN,
                message="Service not found in configuration",
                checked_at=datetime.utcnow(),
            )

        # ì„œë¹„ìŠ¤ URL ê²°ì •
        base_url = service_urls.get(service_id)
        if not base_url:
            hostname = service_info.get("hostname", service_id)
            port = service_info.get("port", service_info.get("api_port"))
            base_url = f"http://{hostname}:{port}"

        healthcheck_path = service_info.get("healthcheck", "/health")
        health_url = f"{base_url}{healthcheck_path}"

        if httpx is None:
            return ServiceHealth(
                service_id=service_id,
                name=service_info.get("name", service_id),
                status=ServiceHealthStatus.UNKNOWN,
                message="httpx not installed",
                url=health_url,
                checked_at=datetime.utcnow(),
            )

        # í—¬ìŠ¤ ì²´í¬ ìˆ˜í–‰
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                start_time = datetime.utcnow()
                response = await client.get(health_url)
                response_time_ms = (
                    datetime.utcnow() - start_time
                ).total_seconds() * 1000

                if response.status_code == 200:
                    status = ServiceHealthStatus.HEALTHY
                    message = "Service is healthy"
                    try:
                        health_data = response.json()
                    except Exception:
                        health_data = None
                elif response.status_code >= 500:
                    status = ServiceHealthStatus.UNHEALTHY
                    message = f"Server error: {response.status_code}"
                    health_data = None
                else:
                    status = ServiceHealthStatus.DEGRADED
                    message = f"Unexpected status: {response.status_code}"
                    health_data = None

                health = ServiceHealth(
                    service_id=service_id,
                    name=service_info.get("name", service_id),
                    status=status,
                    message=message,
                    response_time_ms=response_time_ms,
                    url=health_url,
                    checked_at=datetime.utcnow(),
                    details=health_data,
                )

        except Exception as e:
            error_type = type(e).__name__
            if "Timeout" in error_type:
                status = ServiceHealthStatus.UNHEALTHY
                message = "Connection timeout"
            elif "Connect" in error_type:
                status = ServiceHealthStatus.UNREACHABLE
                message = "Connection refused - service may be down"
            else:
                status = ServiceHealthStatus.UNKNOWN
                message = f"Error: {str(e)}"

            health = ServiceHealth(
                service_id=service_id,
                name=service_info.get("name", service_id),
                status=status,
                message=message,
                url=health_url,
                checked_at=datetime.utcnow(),
            )

        self._last_check[service_id] = health
        return health

    async def check_all_services_health(self) -> list[ServiceHealth]:
        """ëª¨ë“  ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬ (ë³‘ë ¬)"""
        services = self.get_all_services()

        # ë³‘ë ¬ë¡œ í—¬ìŠ¤ ì²´í¬ ìˆ˜í–‰
        tasks = [self.check_service_health(service["id"]) for service in services]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        health_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                health_results.append(
                    ServiceHealth(
                        service_id=services[i]["id"],
                        name=services[i]["name"],
                        status=ServiceHealthStatus.UNKNOWN,
                        message=f"Error: {str(result)}",
                        checked_at=datetime.utcnow(),
                    )
                )
            else:
                health_results.append(result)

        return health_results

    async def check_infrastructure_health(self) -> list[InfrastructureHealth]:
        """ì¸í”„ë¼ ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬"""
        infra_services = self.get_infrastructure_services()
        results = []

        for infra in infra_services:
            infra_id = infra["id"]
            port = infra["port"]

            # ì¸í”„ë¼ë³„ í—¬ìŠ¤ì²´í¬ ìˆ˜í–‰
            if infra_id == "postgres":
                health = await self._check_postgres(port)
            elif infra_id == "mongo":
                health = await self._check_mongo(port)
            elif infra_id == "redis":
                health = await self._check_redis(port)
            elif infra_id == "consul":
                health = await self._check_consul(port)
            elif infra_id == "redpanda":
                health = await self._check_redpanda(port)
            else:
                health = InfrastructureHealth(
                    service_id=infra_id,
                    name=infra["name"],
                    status=ServiceHealthStatus.UNKNOWN,
                    message="Unknown infrastructure type",
                    checked_at=datetime.utcnow(),
                )

            results.append(health)

        return results

    async def _check_postgres(self, port: int) -> InfrastructureHealth:
        """PostgreSQL í—¬ìŠ¤ ì²´í¬"""
        try:
            reader, writer = await asyncio.wait_for(
                asyncio.open_connection("postgres", port), timeout=self.timeout
            )
            writer.close()
            await writer.wait_closed()

            return InfrastructureHealth(
                service_id="postgres",
                name="PostgreSQL",
                status=ServiceHealthStatus.HEALTHY,
                message="PostgreSQL is accepting connections",
                port=port,
                checked_at=datetime.utcnow(),
            )
        except asyncio.TimeoutError:
            return InfrastructureHealth(
                service_id="postgres",
                name="PostgreSQL",
                status=ServiceHealthStatus.UNHEALTHY,
                message="Connection timeout",
                port=port,
                checked_at=datetime.utcnow(),
            )
        except Exception as e:
            return InfrastructureHealth(
                service_id="postgres",
                name="PostgreSQL",
                status=ServiceHealthStatus.UNREACHABLE,
                message=f"Connection failed: {str(e)}",
                port=port,
                checked_at=datetime.utcnow(),
            )

    async def _check_mongo(self, port: int) -> InfrastructureHealth:
        """MongoDB í—¬ìŠ¤ ì²´í¬"""
        try:
            reader, writer = await asyncio.wait_for(
                asyncio.open_connection("mongo", port), timeout=self.timeout
            )
            writer.close()
            await writer.wait_closed()

            return InfrastructureHealth(
                service_id="mongo",
                name="MongoDB",
                status=ServiceHealthStatus.HEALTHY,
                message="MongoDB is accepting connections",
                port=port,
                checked_at=datetime.utcnow(),
            )
        except Exception as e:
            return InfrastructureHealth(
                service_id="mongo",
                name="MongoDB",
                status=ServiceHealthStatus.UNREACHABLE,
                message=f"Connection failed: {str(e)}",
                port=port,
                checked_at=datetime.utcnow(),
            )

    async def _check_redis(self, port: int) -> InfrastructureHealth:
        """Redis í—¬ìŠ¤ ì²´í¬"""
        try:
            reader, writer = await asyncio.wait_for(
                asyncio.open_connection("redis", port), timeout=self.timeout
            )
            # Redis PING ëª…ë ¹
            writer.write(b"PING\r\n")
            await writer.drain()
            response = await asyncio.wait_for(reader.readline(), timeout=2.0)
            writer.close()
            await writer.wait_closed()

            if b"+PONG" in response:
                return InfrastructureHealth(
                    service_id="redis",
                    name="Redis",
                    status=ServiceHealthStatus.HEALTHY,
                    message="Redis is responding to PING",
                    port=port,
                    checked_at=datetime.utcnow(),
                )
            else:
                return InfrastructureHealth(
                    service_id="redis",
                    name="Redis",
                    status=ServiceHealthStatus.DEGRADED,
                    message="Redis connected but unexpected response",
                    port=port,
                    checked_at=datetime.utcnow(),
                )
        except Exception as e:
            return InfrastructureHealth(
                service_id="redis",
                name="Redis",
                status=ServiceHealthStatus.UNREACHABLE,
                message=f"Connection failed: {str(e)}",
                port=port,
                checked_at=datetime.utcnow(),
            )

    async def _check_consul(self, port: int) -> InfrastructureHealth:
        """Consul í—¬ìŠ¤ ì²´í¬"""
        if httpx is None:
            return InfrastructureHealth(
                service_id="consul",
                name="Consul",
                status=ServiceHealthStatus.UNKNOWN,
                message="httpx not installed",
                port=port,
                checked_at=datetime.utcnow(),
            )

        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.get(f"http://consul:{port}/v1/status/leader")

                if response.status_code == 200:
                    leader = response.text.strip('"')
                    return InfrastructureHealth(
                        service_id="consul",
                        name="Consul",
                        status=ServiceHealthStatus.HEALTHY,
                        message=f"Consul leader: {leader}",
                        port=port,
                        checked_at=datetime.utcnow(),
                        details={"leader": leader},
                    )
                else:
                    return InfrastructureHealth(
                        service_id="consul",
                        name="Consul",
                        status=ServiceHealthStatus.DEGRADED,
                        message=f"Consul returned {response.status_code}",
                        port=port,
                        checked_at=datetime.utcnow(),
                    )
        except Exception as e:
            return InfrastructureHealth(
                service_id="consul",
                name="Consul",
                status=ServiceHealthStatus.UNREACHABLE,
                message=f"Connection failed: {str(e)}",
                port=port,
                checked_at=datetime.utcnow(),
            )

    async def _check_redpanda(self, port: int) -> InfrastructureHealth:
        """Redpanda/Kafka í—¬ìŠ¤ ì²´í¬"""
        if httpx is None:
            return InfrastructureHealth(
                service_id="redpanda",
                name="Redpanda",
                status=ServiceHealthStatus.UNKNOWN,
                message="httpx not installed",
                port=port,
                checked_at=datetime.utcnow(),
            )

        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.get("http://redpanda:9644/v1/status/ready")

                if response.status_code == 200:
                    return InfrastructureHealth(
                        service_id="redpanda",
                        name="Redpanda",
                        status=ServiceHealthStatus.HEALTHY,
                        message="Redpanda is ready",
                        port=port,
                        checked_at=datetime.utcnow(),
                    )
                else:
                    return InfrastructureHealth(
                        service_id="redpanda",
                        name="Redpanda",
                        status=ServiceHealthStatus.DEGRADED,
                        message=f"Redpanda returned {response.status_code}",
                        port=port,
                        checked_at=datetime.utcnow(),
                    )
        except Exception as e:
            return InfrastructureHealth(
                service_id="redpanda",
                name="Redpanda",
                status=ServiceHealthStatus.UNREACHABLE,
                message=f"Connection failed: {str(e)}",
                port=port,
                checked_at=datetime.utcnow(),
            )

    async def get_overall_health(self) -> OverallSystemHealth:
        """ì „ì²´ ì‹œìŠ¤í…œ í—¬ìŠ¤ ìƒíƒœ ìš”ì•½"""
        services_health = await self.check_all_services_health()
        infra_health = await self.check_infrastructure_health()

        # í†µê³„ ê³„ì‚°
        total_services = len(services_health)
        healthy_services = sum(
            1 for s in services_health if s.status == ServiceHealthStatus.HEALTHY
        )
        unhealthy_services = sum(
            1
            for s in services_health
            if s.status
            in [ServiceHealthStatus.UNHEALTHY, ServiceHealthStatus.UNREACHABLE]
        )
        degraded_services = sum(
            1 for s in services_health if s.status == ServiceHealthStatus.DEGRADED
        )

        total_infra = len(infra_health)
        healthy_infra = sum(
            1 for i in infra_health if i.status == ServiceHealthStatus.HEALTHY
        )

        # ì „ì²´ ìƒíƒœ ê²°ì •
        if unhealthy_services > 0 or healthy_infra < total_infra:
            if healthy_services == 0:
                overall_status = ServiceHealthStatus.UNHEALTHY
            else:
                overall_status = ServiceHealthStatus.DEGRADED
        elif degraded_services > 0:
            overall_status = ServiceHealthStatus.DEGRADED
        else:
            overall_status = ServiceHealthStatus.HEALTHY

        # í‰ê·  ì‘ë‹µ ì‹œê°„ ê³„ì‚°
        response_times = [
            s.response_time_ms
            for s in services_health
            if s.response_time_ms is not None
        ]
        avg_response_time = (
            sum(response_times) / len(response_times) if response_times else None
        )

        return OverallSystemHealth(
            status=overall_status,
            total_services=total_services,
            healthy_services=healthy_services,
            unhealthy_services=unhealthy_services,
            degraded_services=degraded_services,
            total_infrastructure=total_infra,
            healthy_infrastructure=healthy_infra,
            average_response_time_ms=avg_response_time,
            services=services_health,
            infrastructure=infra_health,
            checked_at=datetime.utcnow(),
        )

    def get_last_check(self, service_id: str) -> Optional[ServiceHealth]:
        """ë§ˆì§€ë§‰ í—¬ìŠ¤ ì²´í¬ ê²°ê³¼ ì¡°íšŒ"""
        return self._last_check.get(service_id)

```

---

## backend/admin-dashboard/api/services/kafka_service.py

```py
"""
Kafka/Redpanda Monitoring Service
Kafka/Redpanda í´ëŸ¬ìŠ¤í„° ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤
"""

import os
from datetime import datetime
from pathlib import Path
from typing import Optional

try:
    import httpx
except ImportError:
    httpx = None  # type: ignore

from ..models.schemas import (
    KafkaTopicInfo,
    KafkaConsumerGroupInfo,
    KafkaClusterInfo,
)


class KafkaService:
    """Kafka/Redpanda ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤"""

    def __init__(self, project_root: str, config_dir: str):
        self.project_root = Path(project_root)
        self.config_dir = Path(config_dir)

        # Redpanda Admin API ì„¤ì • (Kafka-compatible)
        self.redpanda_host = os.environ.get("REDPANDA_HOST", "redpanda")
        self.redpanda_admin_port = int(os.environ.get("REDPANDA_ADMIN_PORT", "9644"))
        self.redpanda_kafka_port = int(os.environ.get("REDPANDA_KAFKA_PORT", "9092"))

        self.admin_api_base = f"http://{self.redpanda_host}:{self.redpanda_admin_port}"
        self.timeout = 10.0

    async def get_cluster_info(self) -> KafkaClusterInfo:
        """í´ëŸ¬ìŠ¤í„° ì „ì²´ ì •ë³´ ì¡°íšŒ"""
        topics = await self.list_topics()
        consumer_groups = await self.list_consumer_groups()

        total_partitions = sum(t.partition_count for t in topics)
        total_messages = None

        # ë¸Œë¡œì»¤ ì •ë³´ ì¡°íšŒ ì‹œë„
        broker_count = 1  # ê¸°ë³¸ê°’
        controller_id = None
        cluster_id = None

        try:
            if httpx:
                async with httpx.AsyncClient(timeout=self.timeout) as client:
                    # Redpanda Admin API - brokers
                    response = await client.get(f"{self.admin_api_base}/v1/brokers")
                    if response.status_code == 200:
                        brokers = response.json()
                        broker_count = len(brokers) if isinstance(brokers, list) else 1

                    # Redpanda Admin API - cluster health
                    response = await client.get(
                        f"{self.admin_api_base}/v1/cluster/health_overview"
                    )
                    if response.status_code == 200:
                        health = response.json()
                        controller_id = health.get("controller_id")
        except Exception:
            pass

        return KafkaClusterInfo(
            broker_count=broker_count,
            controller_id=controller_id,
            cluster_id=cluster_id,
            topics=topics,
            consumer_groups=consumer_groups,
            total_topics=len(topics),
            total_partitions=total_partitions,
            total_messages=total_messages,
            checked_at=datetime.utcnow(),
        )

    async def list_topics(self) -> list[KafkaTopicInfo]:
        """ëª¨ë“  í† í”½ ëª©ë¡ ì¡°íšŒ"""
        topics = []

        try:
            if httpx:
                async with httpx.AsyncClient(timeout=self.timeout) as client:
                    # Redpanda Admin API - topics
                    response = await client.get(f"{self.admin_api_base}/v1/topics")
                    if response.status_code == 200:
                        topic_list = response.json()
                        for topic_data in topic_list:
                            if isinstance(topic_data, dict):
                                topics.append(
                                    KafkaTopicInfo(
                                        name=topic_data.get(
                                            "topic", topic_data.get("name", "unknown")
                                        ),
                                        partition_count=topic_data.get(
                                            "partition_count", 1
                                        ),
                                        replication_factor=topic_data.get(
                                            "replication_factor", 1
                                        ),
                                        message_count=topic_data.get("message_count"),
                                        size_bytes=topic_data.get("size_bytes"),
                                        retention_ms=topic_data.get("retention_ms"),
                                        is_internal=topic_data.get(
                                            "is_internal", False
                                        ),
                                    )
                                )
                            elif isinstance(topic_data, str):
                                # í† í”½ ì´ë¦„ë§Œ ë°˜í™˜ë˜ëŠ” ê²½ìš°
                                topic_detail = await self.get_topic_detail(topic_data)
                                if topic_detail:
                                    topics.append(topic_detail)
        except Exception as e:
            # ì—°ê²° ì‹¤íŒ¨ ì‹œ ìƒ˜í”Œ ë°ì´í„° ë°˜í™˜
            topics = [
                KafkaTopicInfo(
                    name="news-raw",
                    partition_count=3,
                    replication_factor=1,
                    is_internal=False,
                ),
                KafkaTopicInfo(
                    name="news-processed",
                    partition_count=3,
                    replication_factor=1,
                    is_internal=False,
                ),
                KafkaTopicInfo(
                    name="crawl-jobs",
                    partition_count=1,
                    replication_factor=1,
                    is_internal=False,
                ),
            ]

        return topics

    async def get_topic_detail(self, topic_name: str) -> Optional[KafkaTopicInfo]:
        """íŠ¹ì • í† í”½ ìƒì„¸ ì •ë³´"""
        try:
            if httpx:
                async with httpx.AsyncClient(timeout=self.timeout) as client:
                    response = await client.get(
                        f"{self.admin_api_base}/v1/topics/{topic_name}"
                    )
                    if response.status_code == 200:
                        data = response.json()
                        return KafkaTopicInfo(
                            name=data.get("topic", topic_name),
                            partition_count=len(data.get("partitions", []))
                            or data.get("partition_count", 1),
                            replication_factor=data.get("replication_factor", 1),
                            message_count=data.get("message_count"),
                            size_bytes=data.get("size_bytes"),
                            retention_ms=data.get("retention_ms"),
                            is_internal=data.get(
                                "is_internal", topic_name.startswith("_")
                            ),
                        )
        except Exception:
            pass

        return KafkaTopicInfo(
            name=topic_name,
            partition_count=1,
            replication_factor=1,
            is_internal=topic_name.startswith("_"),
        )

    async def list_consumer_groups(self) -> list[KafkaConsumerGroupInfo]:
        """ëª¨ë“  ì»¨ìŠˆë¨¸ ê·¸ë£¹ ì¡°íšŒ"""
        groups = []

        try:
            if httpx:
                async with httpx.AsyncClient(timeout=self.timeout) as client:
                    # Redpanda Admin API - consumer groups
                    response = await client.get(
                        f"{self.admin_api_base}/v1/consumer_groups"
                    )
                    if response.status_code == 200:
                        group_list = response.json()
                        for group_data in group_list:
                            if isinstance(group_data, dict):
                                groups.append(
                                    KafkaConsumerGroupInfo(
                                        group_id=group_data.get(
                                            "group_id",
                                            group_data.get("name", "unknown"),
                                        ),
                                        state=group_data.get("state", "Unknown"),
                                        members_count=group_data.get(
                                            "members_count",
                                            len(group_data.get("members", [])),
                                        ),
                                        topics=group_data.get("topics", []),
                                        total_lag=group_data.get("total_lag", 0),
                                        lag_per_partition=group_data.get(
                                            "lag_per_partition", {}
                                        ),
                                    )
                                )
                            elif isinstance(group_data, str):
                                # ê·¸ë£¹ IDë§Œ ë°˜í™˜ë˜ëŠ” ê²½ìš°
                                group_detail = await self.get_consumer_group_detail(
                                    group_data
                                )
                                if group_detail:
                                    groups.append(group_detail)
        except Exception as e:
            # ì—°ê²° ì‹¤íŒ¨ ì‹œ ìƒ˜í”Œ ë°ì´í„° ë°˜í™˜
            groups = [
                KafkaConsumerGroupInfo(
                    group_id="news-processor",
                    state="Stable",
                    members_count=2,
                    topics=["news-raw"],
                    total_lag=0,
                    lag_per_partition={},
                ),
                KafkaConsumerGroupInfo(
                    group_id="crawler-consumer",
                    state="Stable",
                    members_count=1,
                    topics=["crawl-jobs"],
                    total_lag=0,
                    lag_per_partition={},
                ),
            ]

        return groups

    async def get_consumer_group_detail(
        self, group_id: str
    ) -> Optional[KafkaConsumerGroupInfo]:
        """íŠ¹ì • ì»¨ìŠˆë¨¸ ê·¸ë£¹ ìƒì„¸ ì •ë³´"""
        try:
            if httpx:
                async with httpx.AsyncClient(timeout=self.timeout) as client:
                    response = await client.get(
                        f"{self.admin_api_base}/v1/consumer_groups/{group_id}"
                    )
                    if response.status_code == 200:
                        data = response.json()
                        return KafkaConsumerGroupInfo(
                            group_id=data.get("group_id", group_id),
                            state=data.get("state", "Unknown"),
                            members_count=len(data.get("members", [])),
                            topics=data.get("topics", []),
                            total_lag=data.get("total_lag", 0),
                            lag_per_partition=data.get("lag_per_partition", {}),
                        )
        except Exception:
            pass

        return KafkaConsumerGroupInfo(
            group_id=group_id,
            state="Unknown",
            members_count=0,
            topics=[],
            total_lag=0,
            lag_per_partition={},
        )

    async def check_health(self) -> dict:
        """Kafka/Redpanda í—¬ìŠ¤ ì²´í¬"""
        import asyncio

        try:
            # TCP ì—°ê²° ì²´í¬
            reader, writer = await asyncio.wait_for(
                asyncio.open_connection(self.redpanda_host, self.redpanda_kafka_port),
                timeout=5.0,
            )
            writer.close()
            await writer.wait_closed()

            return {
                "status": "healthy",
                "host": self.redpanda_host,
                "kafka_port": self.redpanda_kafka_port,
                "admin_port": self.redpanda_admin_port,
                "checked_at": datetime.utcnow().isoformat(),
            }
        except Exception as e:
            return {
                "status": "unreachable",
                "host": self.redpanda_host,
                "kafka_port": self.redpanda_kafka_port,
                "admin_port": self.redpanda_admin_port,
                "error": str(e),
                "checked_at": datetime.utcnow().isoformat(),
            }

```

---

## backend/admin-dashboard/api/services/script_service.py

```py
"""
Script Service - ìŠ¤í¬ë¦½íŠ¸/ì‘ì—… ê´€ë¦¬ ë° ì‹¤í–‰ ì„œë¹„ìŠ¤
"""
import asyncio
import os
import re
import signal
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Any, AsyncGenerator, Optional
from uuid import uuid4

import yaml

from ..models.schemas import (
    RiskLevel,
    Script,
    ScriptCreate,
    ScriptParameter,
    ScriptUpdate,
    TaskExecution,
    TaskLog,
    TaskStatus,
    UserRole,
)


# ============================================================================
# ë³´ì•ˆ: ìœ„í—˜ ëª…ë ¹ì–´ í•„í„°ë§
# ============================================================================

# ì ˆëŒ€ í—ˆìš©í•˜ì§€ ì•ŠëŠ” ìœ„í—˜í•œ ëª…ë ¹ì–´ íŒ¨í„´ë“¤
DANGEROUS_COMMAND_PATTERNS = [
    # ì‹œìŠ¤í…œ íŒŒê´´ ëª…ë ¹ì–´
    r'\brm\s+(-[a-zA-Z]*f[a-zA-Z]*\s+)?(-[a-zA-Z]*r[a-zA-Z]*\s+)?/\s*$',  # rm -rf /
    r'\brm\s+(-[a-zA-Z]*r[a-zA-Z]*\s+)?(-[a-zA-Z]*f[a-zA-Z]*\s+)?/\s*$',  # rm -fr /
    r'\brm\s+-[a-zA-Z]*\s+/\s*$',  # rm -* /
    r'\brm\s+--no-preserve-root',  # rm --no-preserve-root
    r':\s*\(\)\s*\{\s*:\s*\|\s*:\s*&\s*\}\s*;',  # fork bomb :(){ :|:& };:
    r'\bdd\s+.*of=/dev/(sd[a-z]|hd[a-z]|nvme)',  # dd to disk devices
    r'\bmkfs\s+',  # format filesystem
    r'\bfdisk\s+',  # partition table manipulation
    r'\bparted\s+',  # partition manipulation
    
    # ìœ„í—˜í•œ ê²½ë¡œ ì‚­ì œ
    r'\brm\s+.*\s+/boot\b',
    r'\brm\s+.*\s+/etc\b',
    r'\brm\s+.*\s+/usr\b',
    r'\brm\s+.*\s+/bin\b',
    r'\brm\s+.*\s+/sbin\b',
    r'\brm\s+.*\s+/lib\b',
    r'\brm\s+.*\s+/var\b',
    r'\brm\s+.*\s+/home\b',
    r'\brm\s+.*\s+/root\b',
    r'\brm\s+.*\s+/sys\b',
    r'\brm\s+.*\s+/proc\b',
    r'\brm\s+.*\s+/dev\b',
    
    # ê¶Œí•œ ìƒìŠ¹ ë° ë³´ì•ˆ ìš°íšŒ
    r'\bchmod\s+777\s+/',  # chmod 777 /
    r'\bchmod\s+-R\s+777\s+/',  # chmod -R 777 /
    r'\bchown\s+-R\s+.*:.*\s+/',  # chown -R on root
    
    # ë„¤íŠ¸ì›Œí¬ ê³µê²© ê´€ë ¨
    r'\bnc\s+-[a-zA-Z]*e',  # netcat with execute
    r'\bcurl\s+.*\|\s*(ba)?sh',  # curl pipe to shell
    r'\bwget\s+.*\|\s*(ba)?sh',  # wget pipe to shell
    
    # ì•”í˜¸í™”í ì±„êµ´ ë“± ì•…ìš© ê°€ëŠ¥ íŒ¨í„´
    r'\b(xmrig|minerd|cgminer|bfgminer)\b',
    
    # ì‹œìŠ¤í…œ ì¢…ë£Œ/ì¬ë¶€íŒ…
    r'\bshutdown\b',
    r'\breboot\b',
    r'\bhalt\b',
    r'\bpoweroff\b',
    r'\binit\s+[06]\b',
    
    # ì‚¬ìš©ì/íŒ¨ìŠ¤ì›Œë“œ ì¡°ì‘
    r'\bpasswd\s+root\b',
    r'\busermod\s+-[a-zA-Z]*\s+root\b',
    r'\buserdel\s+',
    r'\bgroupdel\s+',
    
    # ìœ„í—˜í•œ í™˜ê²½ë³€ìˆ˜ ì¡°ì‘
    r'\bexport\s+PATH=\s*$',  # PATH ë¹„ìš°ê¸°
    r'\bexport\s+LD_PRELOAD=',  # LD_PRELOAD ì¡°ì‘
    
    # ì‹œìŠ¤í…œ ë¡œê·¸ ì‚­ì œ
    r'\brm\s+.*(/var/log|\.log)',
    r'>\s*/var/log/',
    r'\btruncate\s+.*(/var/log|\.log)',
]

# ìœ„í—˜ í‚¤ì›Œë“œ (sudoì™€ í•¨ê»˜ ì‚¬ìš© ì‹œ ì¶”ê°€ ê²½ê³ )
DANGEROUS_WITH_SUDO = [
    r'\bsudo\s+rm\s+-[a-zA-Z]*r',
    r'\bsudo\s+rm\s+/',
    r'\bsudo\s+dd\b',
    r'\bsudo\s+mkfs\b',
    r'\bsudo\s+fdisk\b',
]


class CommandSecurityError(Exception):
    """ëª…ë ¹ì–´ ë³´ì•ˆ ê²€ì‚¬ ì‹¤íŒ¨ ì˜ˆì™¸"""
    def __init__(self, command: str, reason: str, pattern: str = None):
        self.command = command
        self.reason = reason
        self.pattern = pattern
        super().__init__(f"Security violation: {reason}")


def validate_command_security(command: str) -> tuple[bool, str]:
    """
    ëª…ë ¹ì–´ ë³´ì•ˆ ê²€ì‚¬
    
    Args:
        command: ì‹¤í–‰í•  ëª…ë ¹ì–´
        
    Returns:
        (is_safe, reason): ì•ˆì „ ì—¬ë¶€ì™€ ì´ìœ 
        
    Raises:
        CommandSecurityError: ìœ„í—˜í•œ ëª…ë ¹ì–´ ê°ì§€ ì‹œ
    """
    # ëª…ë ¹ì–´ ì •ê·œí™” (ì†Œë¬¸ì ë³€í™˜, ì—°ì† ê³µë°± ì œê±°)
    normalized = ' '.join(command.lower().split())
    
    # ìœ„í—˜ íŒ¨í„´ ê²€ì‚¬
    for pattern in DANGEROUS_COMMAND_PATTERNS:
        if re.search(pattern, normalized, re.IGNORECASE):
            raise CommandSecurityError(
                command=command,
                reason=f"ìœ„í—˜í•œ ëª…ë ¹ì–´ íŒ¨í„´ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤: ì‹œìŠ¤í…œ ë³´ì•ˆì„ ìœ„í•´ ì´ ëª…ë ¹ì–´ëŠ” ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                pattern=pattern
            )
    
    # sudo + ìœ„í—˜ ëª…ë ¹ì–´ ì¡°í•© ê²€ì‚¬
    for pattern in DANGEROUS_WITH_SUDO:
        if re.search(pattern, normalized, re.IGNORECASE):
            raise CommandSecurityError(
                command=command,
                reason=f"sudoì™€ í•¨ê»˜ ì‚¬ìš©ëœ ìœ„í—˜í•œ ëª…ë ¹ì–´ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ë³´í˜¸ë¥¼ ìœ„í•´ ì°¨ë‹¨ë©ë‹ˆë‹¤.",
                pattern=pattern
            )
    
    return True, "OK"


class ScriptService:
    """ìŠ¤í¬ë¦½íŠ¸ ê´€ë¦¬ ë° ì‹¤í–‰ ì„œë¹„ìŠ¤"""

    def __init__(self, project_root: str, config_dir: str):
        self.project_root = Path(project_root)
        self.config_dir = Path(config_dir)
        self.scripts: dict[str, Script] = {}
        self.executions: dict[str, TaskExecution] = {}
        self.running_processes: dict[str, subprocess.Popen] = {}
        self._load_scripts()

    def _load_scripts(self) -> None:
        """ì„¤ì • íŒŒì¼ì—ì„œ ìŠ¤í¬ë¦½íŠ¸ ì •ë³´ ë¡œë“œ"""
        config_file = self.config_dir / "scripts.yaml"
        if config_file.exists():
            with open(config_file) as f:
                data = yaml.safe_load(f) or {}
                for script_data in data.get("scripts", []):
                    # parametersë¥¼ ScriptParameter ê°ì²´ë¡œ ë³€í™˜
                    if "parameters" in script_data:
                        script_data["parameters"] = [
                            ScriptParameter(**p) if isinstance(p, dict) else p
                            for p in script_data["parameters"]
                        ]
                    script = Script(**script_data)
                    self.scripts[script.id] = script
        else:
            self._create_default_scripts()

    def _create_default_scripts(self) -> None:
        """ê¸°ë³¸ ìŠ¤í¬ë¦½íŠ¸ ì„¤ì • ìƒì„±"""
        scripts_dir = self.project_root / "scripts"
        now = datetime.utcnow()

        default_scripts = [
            {
                "id": "script-start",
                "name": "ì„œë¹„ìŠ¤ ì‹œì‘",
                "description": "ì„ íƒí•œ í™˜ê²½ì˜ Docker Compose ì„œë¹„ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.",
                "command": "docker compose -f {compose_file} -p newsinsight up -d",
                "working_dir": str(self.project_root),
                "risk_level": RiskLevel.LOW,
                "estimated_duration": 120,
                "allowed_environments": ["zerotrust", "local", "production", "staging"],
                "required_role": UserRole.OPERATOR,
                "parameters": [
                    ScriptParameter(
                        name="build",
                        param_type="boolean",
                        required=False,
                        default=True,
                        description="ì´ë¯¸ì§€ ë¹Œë“œ ì—¬ë¶€",
                    ),
                ],
                "tags": ["docker", "deploy"],
                "created_at": now,
                "updated_at": now,
            },
            {
                "id": "script-stop",
                "name": "ì„œë¹„ìŠ¤ ì¤‘ì§€",
                "description": "ì„ íƒí•œ í™˜ê²½ì˜ Docker Compose ì„œë¹„ìŠ¤ë¥¼ ì¤‘ì§€í•©ë‹ˆë‹¤.",
                "command": "docker compose -f {compose_file} -p newsinsight down",
                "working_dir": str(self.project_root),
                "risk_level": RiskLevel.MEDIUM,
                "estimated_duration": 60,
                "allowed_environments": ["zerotrust", "local", "production", "staging"],
                "required_role": UserRole.OPERATOR,
                "parameters": [],
                "tags": ["docker", "stop"],
                "created_at": now,
                "updated_at": now,
            },
            {
                "id": "script-restart",
                "name": "ì„œë¹„ìŠ¤ ì¬ì‹œì‘",
                "description": "ì„ íƒí•œ í™˜ê²½ì˜ Docker Compose ì„œë¹„ìŠ¤ë¥¼ ì¬ì‹œì‘í•©ë‹ˆë‹¤.",
                "command": "docker compose -f {compose_file} -p newsinsight restart {service}",
                "working_dir": str(self.project_root),
                "risk_level": RiskLevel.MEDIUM,
                "estimated_duration": 60,
                "allowed_environments": ["zerotrust", "local", "production", "staging"],
                "required_role": UserRole.OPERATOR,
                "parameters": [
                    ScriptParameter(
                        name="service",
                        param_type="string",
                        required=False,
                        default="",
                        description="ì¬ì‹œì‘í•  ì„œë¹„ìŠ¤ ì´ë¦„ (ë¹„ì›Œë‘ë©´ ì „ì²´ ì¬ì‹œì‘)",
                    ),
                ],
                "tags": ["docker", "restart"],
                "created_at": now,
                "updated_at": now,
            },
            {
                "id": "script-full-cleanup",
                "name": "ì „ì²´ ì •ë¦¬ (Full Cleanup)",
                "description": "ì»¨í…Œì´ë„ˆ, ë³¼ë¥¨, ì´ë¯¸ì§€, ìºì‹œë¥¼ ëª¨ë‘ ì •ë¦¬í•©ë‹ˆë‹¤. âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ ë³¼ë¥¨ë„ ì‚­ì œë©ë‹ˆë‹¤!",
                "command": """docker compose -f {compose_file} -p newsinsight down -v && \
docker builder prune -f && \
docker image prune -f && \
docker volume prune -f""",
                "working_dir": str(self.project_root),
                "risk_level": RiskLevel.CRITICAL,
                "estimated_duration": 180,
                "allowed_environments": ["local", "staging"],
                "required_role": UserRole.ADMIN,
                "parameters": [],
                "tags": ["docker", "cleanup", "dangerous"],
                "created_at": now,
                "updated_at": now,
            },
            {
                "id": "script-status",
                "name": "ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸",
                "description": "í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ì»¨í…Œì´ë„ˆ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.",
                "command": "docker compose -f {compose_file} -p newsinsight ps -a",
                "working_dir": str(self.project_root),
                "risk_level": RiskLevel.LOW,
                "estimated_duration": 5,
                "allowed_environments": ["zerotrust", "local", "production", "staging"],
                "required_role": UserRole.VIEWER,
                "parameters": [],
                "tags": ["docker", "status"],
                "created_at": now,
                "updated_at": now,
            },
            {
                "id": "script-logs",
                "name": "ì„œë¹„ìŠ¤ ë¡œê·¸ ì¡°íšŒ",
                "description": "íŠ¹ì • ì„œë¹„ìŠ¤ì˜ ë¡œê·¸ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.",
                "command": "docker compose -f {compose_file} -p newsinsight logs --tail {tail} {service}",
                "working_dir": str(self.project_root),
                "risk_level": RiskLevel.LOW,
                "estimated_duration": 10,
                "allowed_environments": ["zerotrust", "local", "production", "staging"],
                "required_role": UserRole.VIEWER,
                "parameters": [
                    ScriptParameter(
                        name="service",
                        param_type="string",
                        required=True,
                        description="ë¡œê·¸ë¥¼ ì¡°íšŒí•  ì„œë¹„ìŠ¤ ì´ë¦„",
                    ),
                    ScriptParameter(
                        name="tail",
                        param_type="number",
                        required=False,
                        default=100,
                        description="ì¶œë ¥í•  ë¡œê·¸ ì¤„ ìˆ˜",
                    ),
                ],
                "tags": ["docker", "logs"],
                "created_at": now,
                "updated_at": now,
            },
            {
                "id": "script-build-push",
                "name": "ì´ë¯¸ì§€ ë¹Œë“œ ë° í‘¸ì‹œ",
                "description": "Docker ì´ë¯¸ì§€ë¥¼ ë¹Œë“œí•˜ê³  ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— í‘¸ì‹œí•©ë‹ˆë‹¤.",
                "command": str(scripts_dir / "build-and-push.sh"),
                "working_dir": str(self.project_root),
                "risk_level": RiskLevel.MEDIUM,
                "estimated_duration": 300,
                "allowed_environments": ["production", "staging"],
                "required_role": UserRole.OPERATOR,
                "parameters": [
                    ScriptParameter(
                        name="tag",
                        param_type="string",
                        required=False,
                        default="latest",
                        description="ì´ë¯¸ì§€ íƒœê·¸",
                    ),
                ],
                "tags": ["docker", "build", "ci"],
                "created_at": now,
                "updated_at": now,
            },
            {
                "id": "script-health-check",
                "name": "í—¬ìŠ¤ì²´í¬",
                "description": "ëª¨ë“  ì„œë¹„ìŠ¤ì˜ í—¬ìŠ¤ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.",
                "command": """docker compose -f {compose_file} -p newsinsight ps --format json | \
python3 -c "import sys,json; [print(f'{json.loads(l).get(\"Name\")}: {json.loads(l).get(\"Health\", \"N/A\")}') for l in sys.stdin if l.strip()]" """,
                "working_dir": str(self.project_root),
                "risk_level": RiskLevel.LOW,
                "estimated_duration": 10,
                "allowed_environments": ["zerotrust", "local", "production", "staging"],
                "required_role": UserRole.VIEWER,
                "parameters": [],
                "tags": ["health", "monitoring"],
                "created_at": now,
                "updated_at": now,
            },
        ]

        for script_data in default_scripts:
            script = Script(**script_data)
            self.scripts[script.id] = script

        self._save_scripts()

    def _save_scripts(self) -> None:
        """ìŠ¤í¬ë¦½íŠ¸ ì„¤ì •ì„ íŒŒì¼ì— ì €ì¥"""
        self.config_dir.mkdir(parents=True, exist_ok=True)
        config_file = self.config_dir / "scripts.yaml"

        data = {
            "scripts": [
                script.model_dump(mode="json") for script in self.scripts.values()
            ]
        }

        with open(config_file, "w") as f:
            yaml.dump(data, f, default_flow_style=False, allow_unicode=True)

    def list_scripts(
        self,
        environment: Optional[str] = None,
        tag: Optional[str] = None,
        role: Optional[UserRole] = None,
    ) -> list[Script]:
        """ìŠ¤í¬ë¦½íŠ¸ ëª©ë¡ ì¡°íšŒ"""
        scripts = list(self.scripts.values())

        if environment:
            scripts = [
                s
                for s in scripts
                if not s.allowed_environments or environment in s.allowed_environments
            ]

        if tag:
            scripts = [s for s in scripts if tag in s.tags]

        if role:
            role_priority = {UserRole.VIEWER: 0, UserRole.OPERATOR: 1, UserRole.ADMIN: 2}
            user_level = role_priority.get(role, 0)
            scripts = [
                s for s in scripts if role_priority.get(s.required_role, 0) <= user_level
            ]

        return scripts

    def get_script(self, script_id: str) -> Optional[Script]:
        """ìŠ¤í¬ë¦½íŠ¸ ìƒì„¸ ì¡°íšŒ"""
        return self.scripts.get(script_id)

    def create_script(self, data: ScriptCreate) -> Script:
        """ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
        script_id = f"script-{uuid4().hex[:8]}"
        now = datetime.utcnow()

        script = Script(
            id=script_id,
            created_at=now,
            updated_at=now,
            **data.model_dump(),
        )

        self.scripts[script_id] = script
        self._save_scripts()
        return script

    def update_script(self, script_id: str, data: ScriptUpdate) -> Optional[Script]:
        """ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì •"""
        script = self.scripts.get(script_id)
        if not script:
            return None

        update_data = data.model_dump(exclude_unset=True)
        for key, value in update_data.items():
            setattr(script, key, value)

        script.updated_at = datetime.utcnow()
        self._save_scripts()
        return script

    def delete_script(self, script_id: str) -> bool:
        """ìŠ¤í¬ë¦½íŠ¸ ì‚­ì œ"""
        if script_id in self.scripts:
            del self.scripts[script_id]
            self._save_scripts()
            return True
        return False

    async def execute_script(
        self,
        script_id: str,
        environment_name: str,
        compose_file: str,
        parameters: dict[str, Any],
        executed_by: str,
    ) -> TaskExecution:
        """ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰"""
        script = self.scripts.get(script_id)
        if not script:
            raise ValueError(f"Script not found: {script_id}")

        execution_id = f"exec-{uuid4().hex[:8]}"
        now = datetime.utcnow()

        # ëª…ë ¹ì–´ í…œí”Œë¦¿ ì¹˜í™˜
        command = script.command.format(
            compose_file=compose_file,
            **parameters,
        )
        
        # ë³´ì•ˆ ê²€ì‚¬: ìœ„í—˜í•œ ëª…ë ¹ì–´ ì°¨ë‹¨
        try:
            validate_command_security(command)
        except CommandSecurityError as e:
            raise ValueError(f"ë³´ì•ˆ ìœ„ë°˜: {e.reason}")

        execution = TaskExecution(
            id=execution_id,
            script_id=script_id,
            script_name=script.name,
            environment_id=environment_name,
            environment_name=environment_name,
            status=TaskStatus.RUNNING,
            parameters=parameters,
            started_at=now,
            executed_by=executed_by,
        )

        self.executions[execution_id] = execution

        # ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
        asyncio.create_task(
            self._run_command(execution_id, command, script.working_dir)
        )

        return execution

    async def _run_command(
        self, execution_id: str, command: str, working_dir: Optional[str]
    ) -> None:
        """ëª…ë ¹ì–´ ì‹¤í–‰ (ë¹„ë™ê¸°)"""
        execution = self.executions.get(execution_id)
        if not execution:
            return

        try:
            process = await asyncio.create_subprocess_shell(
                command,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.STDOUT,
                cwd=working_dir,
            )

            self.running_processes[execution_id] = process

            # ì¶œë ¥ ìˆ˜ì§‘
            stdout, _ = await process.communicate()

            execution.exit_code = process.returncode
            execution.status = (
                TaskStatus.SUCCESS if process.returncode == 0 else TaskStatus.FAILED
            )
            execution.finished_at = datetime.utcnow()

            if process.returncode != 0:
                execution.error_message = stdout.decode() if stdout else "Unknown error"

        except Exception as e:
            execution.status = TaskStatus.FAILED
            execution.error_message = str(e)
            execution.finished_at = datetime.utcnow()
        finally:
            if execution_id in self.running_processes:
                del self.running_processes[execution_id]

    async def stream_execution_output(
        self,
        script_id: str,
        environment_name: str,
        compose_file: str,
        parameters: dict[str, Any],
        executed_by: str,
    ) -> AsyncGenerator[str, None]:
        """ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ë° ì¶œë ¥ ìŠ¤íŠ¸ë¦¬ë°"""
        script = self.scripts.get(script_id)
        if not script:
            yield f"Error: Script not found: {script_id}\n"
            return

        execution_id = f"exec-{uuid4().hex[:8]}"
        now = datetime.utcnow()

        # ëª…ë ¹ì–´ í…œí”Œë¦¿ ì¹˜í™˜
        command = script.command.format(
            compose_file=compose_file,
            **parameters,
        )
        
        # ë³´ì•ˆ ê²€ì‚¬: ìœ„í—˜í•œ ëª…ë ¹ì–´ ì°¨ë‹¨
        try:
            validate_command_security(command)
        except CommandSecurityError as e:
            yield f"[SECURITY ERROR] {e.reason}\n"
            yield f"[BLOCKED] ëª…ë ¹ì–´ê°€ ë³´ì•ˆ ì •ì±…ì— ì˜í•´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
            return

        execution = TaskExecution(
            id=execution_id,
            script_id=script_id,
            script_name=script.name,
            environment_id=environment_name,
            environment_name=environment_name,
            status=TaskStatus.RUNNING,
            parameters=parameters,
            started_at=now,
            executed_by=executed_by,
        )

        self.executions[execution_id] = execution

        yield f"[{now.isoformat()}] Starting: {script.name}\n"
        yield f"[{now.isoformat()}] Command: {command}\n"
        yield f"[{now.isoformat()}] Working dir: {script.working_dir}\n"
        yield "-" * 60 + "\n"

        try:
            process = await asyncio.create_subprocess_shell(
                command,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.STDOUT,
                cwd=script.working_dir,
            )

            self.running_processes[execution_id] = process

            # ì‹¤ì‹œê°„ ì¶œë ¥ ìŠ¤íŠ¸ë¦¬ë°
            async for line in process.stdout:
                yield line.decode()

            await process.wait()

            execution.exit_code = process.returncode
            execution.status = (
                TaskStatus.SUCCESS if process.returncode == 0 else TaskStatus.FAILED
            )
            execution.finished_at = datetime.utcnow()

            yield "-" * 60 + "\n"
            yield f"[{execution.finished_at.isoformat()}] Finished with exit code: {process.returncode}\n"
            yield f"[{execution.finished_at.isoformat()}] Status: {execution.status.value}\n"

        except Exception as e:
            execution.status = TaskStatus.FAILED
            execution.error_message = str(e)
            execution.finished_at = datetime.utcnow()
            yield f"[ERROR] {str(e)}\n"
        finally:
            if execution_id in self.running_processes:
                del self.running_processes[execution_id]

    def cancel_execution(self, execution_id: str) -> bool:
        """ì‹¤í–‰ ì¤‘ì¸ ì‘ì—… ì·¨ì†Œ"""
        process = self.running_processes.get(execution_id)
        if process:
            try:
                process.terminate()
                execution = self.executions.get(execution_id)
                if execution:
                    execution.status = TaskStatus.CANCELLED
                    execution.finished_at = datetime.utcnow()
                return True
            except Exception:
                return False
        return False

    def get_execution(self, execution_id: str) -> Optional[TaskExecution]:
        """ì‹¤í–‰ ì •ë³´ ì¡°íšŒ"""
        return self.executions.get(execution_id)

    def list_executions(
        self,
        script_id: Optional[str] = None,
        environment_id: Optional[str] = None,
        status: Optional[TaskStatus] = None,
        limit: int = 50,
    ) -> list[TaskExecution]:
        """ì‹¤í–‰ ì´ë ¥ ì¡°íšŒ"""
        executions = list(self.executions.values())

        if script_id:
            executions = [e for e in executions if e.script_id == script_id]

        if environment_id:
            executions = [e for e in executions if e.environment_id == environment_id]

        if status:
            executions = [e for e in executions if e.status == status]

        # ìµœì‹ ìˆœ ì •ë ¬
        executions.sort(key=lambda x: x.started_at, reverse=True)

        return executions[:limit]

```

---

## backend/admin-dashboard/docker-compose.yml

```yml
# Admin Dashboard Docker Compose
# ê°œë°œ ë° ìš´ì˜ í™˜ê²½ì—ì„œ Admin Dashboardë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

services:
  admin-api:
    build:
      context: .
      dockerfile: Dockerfile
    image: newsinsight/admin-dashboard:local
    container_name: newsinsight-admin-api
    restart: unless-stopped
    environment:
      - PORT=8889
      - PROJECT_ROOT=/workspace
      - ADMIN_CONFIG_DIR=/app/config
      - ADMIN_SECRET_KEY=${ADMIN_SECRET_KEY:-change-this-secret-key-in-production}
      - CORS_ORIGINS=http://localhost:3001,http://localhost:8889
    ports:
      - "8889:8889"
    volumes:
      # í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ë§ˆìš´íŠ¸í•˜ì—¬ docker compose ëª…ë ¹ ì‹¤í–‰ ê°€ëŠ¥
      - ../../:/workspace:ro
      # Docker ì†Œì¼“ ë§ˆìš´íŠ¸ (ì»¨í…Œì´ë„ˆ ê´€ë¦¬ìš©)
      - /var/run/docker.sock:/var/run/docker.sock
      # ì„¤ì • íŒŒì¼ ì˜ì†í™”
      - admin-config:/app/config
    networks:
      - admin-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8889/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  admin-web:
    image: node:20-alpine
    container_name: newsinsight-admin-web
    working_dir: /app
    volumes:
      - ./web:/app
      - admin-web-node-modules:/app/node_modules
    environment:
      - VITE_API_URL=http://admin-api:8889/api/v1/admin
    command: >
      sh -c "npm install && npm run dev -- --host 0.0.0.0"
    ports:
      - "3001:3001"
    depends_on:
      - admin-api
    networks:
      - admin-net

volumes:
  admin-config:
  admin-web-node-modules:

networks:
  admin-net:
    name: newsinsight-admin-net

```

---

## backend/api-gateway-service/bin/main/application.yml

```yml
spring:
  application:
    name: api-gateway
  
  config:
    # Consulì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸° (optional - Consul ì—†ì–´ë„ ì‹œì‘ ê°€ëŠ¥)
    import: ${SPRING_CONFIG_IMPORT:optional:consul:}
  
  cloud:
    consul:
      enabled: ${CONSUL_ENABLED:true}
      host: ${CONSUL_HOST:localhost}
      port: ${CONSUL_PORT:8500}
      config:
        enabled: ${CONSUL_CONFIG_ENABLED:true}
        # ê¸°ì¡´ Consul KV êµ¬ì¡°ì™€ ì¼ì¹˜
        prefix: config
        default-context: ${spring.application.name}
        format: PROPERTIES
        # Fail-Fast ë¹„í™œì„±í™”: Consul ì—°ê²° ì‹¤íŒ¨í•´ë„ ì‹œì‘
        fail-fast: ${CONSUL_FAIL_FAST:false}
      discovery:
        enabled: ${CONSUL_DISCOVERY_ENABLED:true}
        health-check-path: /actuator/health
        health-check-interval: 10s
        instance-id: ${spring.application.name}:${random.value}
        prefer-ip-address: true
        # ì„œë¹„ìŠ¤ ë“±ë¡ ì‹¤íŒ¨í•´ë„ ì‹œì‘
        fail-fast: false
    inetutils:
      preferred-networks:
        - 172.20

    gateway:
      routes:
        # ===========================================
        # Collector Service Routes
        # COLLECTOR_SERVICE_URL í™˜ê²½ë³€ìˆ˜ë¡œ ì§ì ‘ URL ì§€ì • ê°€ëŠ¥
        # Consul ì‚¬ìš© ì‹œ: lb://collector-service
        # Consul ë¯¸ì‚¬ìš© ì‹œ: http://collector-service:8081 (ê¸°ë³¸ê°’)
        # ===========================================
        
        # Collector ì„œë¹„ìŠ¤ - ë°ì´í„° API (DataController)
        - id: collector-data
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/data/**
          # NOTE: Redis RateLimiter ë¹„í™œì„±í™” - Redis ì—†ì´ë„ ë™ì‘í•˜ë„ë¡ ì„¤ì •
          # Redis ë„ì… ì‹œ ì•„ë˜ ì£¼ì„ í•´ì œ
          # filters:
          #   - name: RequestRateLimiter
          #     args:
          #       redis-rate-limiter.replenishRate: 100
          #       redis-rate-limiter.burstCapacity: 150

        # Collector ì„œë¹„ìŠ¤ - ì†ŒìŠ¤ ê´€ë¦¬ (SourceController)
        - id: collector-sources
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/sources/**

        # Collector ì„œë¹„ìŠ¤ - ìˆ˜ì§‘ ì‘ì—… (CollectionController)
        - id: collector-collections
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/collections/**

        # Articles & Analysis - í”„ë¡ íŠ¸ì—”ë“œ API
        - id: articles-root
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/articles

        - id: articles-alias
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/articles/**

        - id: analysis-root
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/analysis

        - id: analysis-alias
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/analysis/**
          metadata:
            response-timeout: 300000

        # Search API - í†µí•© ê²€ìƒ‰ (SSE ìŠ¤íŠ¸ë¦¬ë°)
        # NOTE: SSE ìŠ¤íŠ¸ë¦¬ë°ì€ RateLimiterì™€ í˜¸í™˜ì„± ë¬¸ì œê°€ ìˆì–´ ë¹„í™œì„±í™”
        - id: search-api
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/search/**

        # Search History API - ê²€ìƒ‰ ê¸°ë¡ ê´€ë¦¬
        - id: search-history
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/search-history/**
          metadata:
            # SSE ìŠ¤íŠ¸ë¦¼ ì—°ê²°ì„ ìœ„í•œ ê¸´ íƒ€ì„ì•„ì›ƒ (5ë¶„)
            response-timeout: 300000

        # Search Template API - SmartSearch ê²€ìƒ‰ í…œí”Œë¦¿ ê´€ë¦¬
        - id: search-templates
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/search-templates/**

        # Search Jobs API - ê²€ìƒ‰ ì‘ì—… ê´€ë¦¬ (SearchJobController)
        # SSE ìŠ¤íŠ¸ë¦¬ë° ì§€ì›: /api/v1/jobs/stream, /api/v1/jobs/{jobId}/stream
        - id: search-jobs
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/jobs,/api/v1/jobs/**
          metadata:
            # SSE ìŠ¤íŠ¸ë¦¼ ì—°ê²°ì„ ìœ„í•œ ê¸´ íƒ€ì„ì•„ì›ƒ (5ë¶„)
            response-timeout: 300000

        # AutoCrawl API - ìë™ í¬ë¡¤ë§ ê´€ë¦¬ (AutoCrawlController)
        - id: autocrawl
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/autocrawl/**

        # Config API - í”„ë¡ íŠ¸ì—”ë“œ ì„¤ì •
        # NOTE: Gateway ìì²´ FrontendConfigControllerê°€ ìš°ì„  ì²˜ë¦¬ë¨
        # ì´ ë¼ìš°íŠ¸ëŠ” fallbackìœ¼ë¡œ ìœ ì§€ (collector-serviceì—ë„ ConfigControllerê°€ ìˆì„ ê²½ìš°)
        - id: config-api
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/config/**

        # AI Orchestration - AI ë¶„ì„ ìš”ì²­ (collector-service AiOrchestrationController)
        - id: collector-ai
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/ai/**

        - id: collector-ml
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/ml/**

        # Dashboard Events - SSE ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ (collector-service DashboardEventsController)
        - id: collector-events
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/events/**
          metadata:
            # SSE ì—°ê²°ì„ ìœ„í•œ ê¸´ íƒ€ì„ì•„ì›ƒ
            response-timeout: 300000

        # Admin Dashboard API
        # NOTE: Using direct URL fallback if not found in Consul, assuming port 8888 from main.py
        - id: admin-dashboard-api
          uri: ${ADMIN_DASHBOARD_URL:http://localhost:8888}
          predicates:
            - Path=/api/v1/admin/**
          # No StripPrefix needed because the admin API uses /api/v1/admin prefix

        # Autonomous Crawler - í¬ë¡¤ë§ ì‘ì—… ê´€ë¦¬
        - id: autonomous-crawler-api
          uri: ${AUTONOMOUS_CRAWLER_API_URL:${AUTONOMOUS_CRAWLER_URL:http://autonomous-crawler:8030}}
          predicates:
            - Path=/api/v1/crawler/**
          filters:
            # /api/v1/crawler/** -> /** ë¡œ ì „ë‹¬ (3 segments: api, v1, crawler)
            - StripPrefix=3

        # Browser-Use API - AI ë¸Œë¼ìš°ì € ìë™í™”
        - id: browser-use-browse
          uri: ${BROWSER_USE_URL:http://localhost:8500}
          predicates:
            - Path=/api/browser-use/browse/**
          filters:
            # /api/browser-use/browse/** -> /browse/** ë¡œ ì „ë‹¬
            - StripPrefix=2

        - id: browser-use-jobs
          uri: ${BROWSER_USE_URL:http://localhost:8500}
          predicates:
            - Path=/api/browser-use/jobs/**
          filters:
            # /api/browser-use/jobs/** -> /jobs/** ë¡œ ì „ë‹¬
            - StripPrefix=2

        - id: browser-use-health
          uri: ${BROWSER_USE_URL:http://localhost:8500}
          predicates:
            - Path=/api/browser-use/health
          filters:
            # /api/browser-use/health -> /health ë¡œ ì „ë‹¬
            - StripPrefix=2

        # Browser-Use WebSocket
        - id: browser-use-websocket
          uri: ${BROWSER_USE_WS_URL:ws://localhost:8500}
          predicates:
            - Path=/api/browser-use/ws/**
          filters:
            # /api/browser-use/ws/** -> /ws/** ë¡œ ì „ë‹¬
            - StripPrefix=2

        # ML Add-ons - ê°ì • ë¶„ì„ (Sentiment Analysis)
        - id: ml-addon-sentiment
          uri: ${ML_ADDON_SENTIMENT_URL:http://sentiment-addon:8100}
          predicates:
            - Path=/api/ml-addons/sentiment/**
          filters:
            # /api/ml-addons/sentiment/** -> /** ë¡œ ì „ë‹¬
            - StripPrefix=3

        # Fact Check Chat - íŒ©íŠ¸ì²´í¬ ì±—ë´‡
        - id: factcheck-chat
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/factcheck-chat/**
          filters:
            - StripPrefix=0

        # ML Add-ons - íŒ©íŠ¸ì²´í¬ (Fact Check)
        - id: ml-addon-factcheck
          uri: ${ML_ADDON_FACTCHECK_URL:http://factcheck-addon:8101}
          predicates:
            - Path=/api/ml-addons/factcheck/**
          filters:
            # /api/ml-addons/factcheck/** -> /** ë¡œ ì „ë‹¬
            - StripPrefix=3

        # ML Add-ons - í¸í–¥ë„ ë¶„ì„ (Bias Analysis)
        - id: ml-addon-bias
          uri: ${ML_ADDON_BIAS_URL:http://bias-addon:8102}
          predicates:
            - Path=/api/ml-addons/bias/**
          filters:
            # /api/ml-addons/bias/** -> /** ë¡œ ì „ë‹¬
            - StripPrefix=3

        - id: ml-addon-bot-detector
          uri: ${ML_ADDON_BOT_DETECTOR_URL:http://bot-detector:8041}
          predicates:
            - Path=/api/ml-addons/bot-detector/**
          filters:
            - StripPrefix=3

        # ML Trainer Service - ëª¨ë¸ í•™ìŠµ ì„œë¹„ìŠ¤
        - id: ml-trainer
          uri: ${ML_TRAINER_URL:http://ml-trainer:8090}
          predicates:
            - Path=/api/ml-trainer/**
          filters:
            # /api/ml-trainer/** -> /** ë¡œ ì „ë‹¬
            - StripPrefix=2
          metadata:
            # í•™ìŠµ ì‘ì—…ì€ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê¸´ íƒ€ì„ì•„ì›ƒ
            response-timeout: 600000

        # ML Trainer SSE Stream - ì‹¤ì‹œê°„ í•™ìŠµ ì§„í–‰ ìƒí™©
        - id: ml-trainer-stream
          uri: ${ML_TRAINER_URL:http://ml-trainer:8090}
          predicates:
            - Path=/api/ml-trainer/jobs/*/stream
          filters:
            - StripPrefix=2
          metadata:
            # SSE ìŠ¤íŠ¸ë¦¼ì„ ìœ„í•œ ë§¤ìš° ê¸´ íƒ€ì„ì•„ì›ƒ (30ë¶„)
            response-timeout: 1800000

        # Health Check Rewrite - í”„ë¡ íŠ¸ì—”ë“œê°€ /api/actuator/health í˜¸ì¶œ ì‹œ ì²˜ë¦¬
        - id: gateway-health
          uri: http://localhost:${PORT:8000}
          predicates:
            - Path=/api/actuator/health,/api/actuator/health/**
          filters:
            # /api/actuator/health -> /actuator/health ë¡œ ì¬ì‘ì„±
            - RewritePath=/api/actuator/(?<segment>.*), /actuator/${segment}

      # ì „ì—­ CORS ì„¤ì •
      globalcors:
        cors-configurations:
          '[/**]':
            allowedOriginPatterns: "*"
            allowedMethods:
              - GET
              - POST
              - PUT
              - DELETE
              - PATCH
              - OPTIONS
            allowedHeaders: "*"
            allowCredentials: true
            maxAge: 3600
  
  # Redis ì„¤ì • (Rate Limiting ë° ì„¸ì…˜ ê´€ë¦¬ìš©)
  data:
    redis:
      host: ${REDIS_HOST:redis}
      port: ${REDIS_PORT:6379}
      password: ${REDIS_PASSWORD:}

server:
  port: ${PORT:8000}
  # Netty ì„¤ì • - SSE/WebSocket ì—°ê²°ì„ ìœ„í•œ íƒ€ì„ì•„ì›ƒ ì¦ê°€
  netty:
    # ì—°ê²° ìœ ì§€ ì‹œê°„ (5ë¶„)
    idle-timeout: 300000

# Spring Cloud Gateway HTTP Client ì„¤ì •
spring.cloud.gateway.httpclient:
  connect-timeout: 10000
  response-timeout: 300000
  pool:
    max-idle-time: 300000

management:
  endpoints:
    web:
      exposure:
        include: health,info,env,metrics,prometheus
  endpoint:
    health:
      show-details: always

logging:
  level:
    root: INFO
    org.springframework.cloud.gateway: DEBUG
    org.springframework.cloud.gateway.handler.RoutePredicateHandlerMapping: TRACE
    org.springframework.security: DEBUG
    com.newsinsight.gateway: DEBUG

```

---

## backend/api-gateway-service/build.gradle.kts

```kts
// API Gateway ëª¨ë“ˆ ë¹Œë“œ ì„¤ì •

plugins {
    java
    id("org.springframework.boot")
    id("io.spring.dependency-management")
}

dependencies {
    // Spring Cloud Gateway
    implementation("org.springframework.cloud:spring-cloud-starter-gateway")
    
    // Security (JWT ì¸ì¦)
    implementation("org.springframework.boot:spring-boot-starter-security")
    implementation("io.jsonwebtoken:jjwt-api:0.12.3")
    runtimeOnly("io.jsonwebtoken:jjwt-impl:0.12.3")
    runtimeOnly("io.jsonwebtoken:jjwt-jackson:0.12.3")
    
    // Redis for Rate Limiting
    implementation("org.springframework.boot:spring-boot-starter-data-redis-reactive")
    
    // WebFlux (GatewayëŠ” Reactive ìŠ¤íƒ)
    implementation("org.springframework.boot:spring-boot-starter-webflux")
    
    // Test
    testImplementation("org.springframework.security:spring-security-test")
    testImplementation("io.projectreactor:reactor-test")
}

tasks.named<org.springframework.boot.gradle.tasks.bundling.BootJar>("bootJar") {
    archiveBaseName.set("api-gateway")
    archiveVersion.set("1.0.0")
}

```

---

## backend/api-gateway-service/src/main/java/com/newsinsight/gateway/GatewayApplication.java

```java
package com.newsinsight.gateway;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.client.discovery.EnableDiscoveryClient;

/**
 * NewsInsight API Gateway Application
 * 
 * Spring Cloud Gateway ê¸°ë°˜ì˜ API Gateway ì„œë¹„ìŠ¤
 * - JWT ì¸ì¦/ì¸ê°€
 * - RBAC (Role-Based Access Control)
 * - Rate Limiting (Redis ê¸°ë°˜)
 * - Service Discovery (Consul)
 * - Dynamic Configuration (Consul KV)
 */
@SpringBootApplication
@EnableDiscoveryClient
public class GatewayApplication {

    public static void main(String[] args) {
        SpringApplication.run(GatewayApplication.class, args);
    }
}

```

---

## backend/api-gateway-service/src/main/java/com/newsinsight/gateway/config/SecurityConfig.java

```java
package com.newsinsight.gateway.config;

import org.springframework.beans.factory.annotation.Value;
import org.springframework.cloud.gateway.filter.ratelimit.KeyResolver;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.http.HttpMethod;
import org.springframework.security.config.annotation.web.reactive.EnableWebFluxSecurity;
import org.springframework.security.config.web.server.ServerHttpSecurity;
import org.springframework.security.web.server.SecurityWebFilterChain;
import org.springframework.web.cors.CorsConfiguration;
import org.springframework.web.cors.reactive.CorsConfigurationSource;
import org.springframework.web.cors.reactive.UrlBasedCorsConfigurationSource;
import reactor.core.publisher.Mono;

import java.util.Arrays;
import java.util.List;

/**
 * API Gateway Security Configuration
 * 
 * ê²½ë¡œë³„ ì¸ì¦ ì •ì±…:
 * - Public: í—¬ìŠ¤ì²´í¬, ì¸ì¦ ì—”ë“œí¬ì¸íŠ¸ (/api/v1/auth/**)
 * - Protected: ëŒ€ë¶€ë¶„ì˜ API ì—”ë“œí¬ì¸íŠ¸ (JWT ê²€ì¦ì€ downstream ì„œë¹„ìŠ¤ì—ì„œ ì²˜ë¦¬)
 * 
 * NOTE: GatewayëŠ” JWT í† í°ì„ downstream ì„œë¹„ìŠ¤ë¡œ ì „ë‹¬ë§Œ í•˜ê³ ,
 * ì‹¤ì œ ì¸ì¦/ì¸ê°€ëŠ” ê° ì„œë¹„ìŠ¤ì˜ SecurityConfigì—ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
 * Gatewayì—ì„œëŠ” ê¸°ë³¸ì ì¸ ê²½ë¡œ ê¸°ë°˜ ì ‘ê·¼ ì œì–´ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
 */
@Configuration
@EnableWebFluxSecurity
public class SecurityConfig {

    @Value("${security.gateway.enabled:true}")
    private boolean securityEnabled;

    @Bean
    public SecurityWebFilterChain securityWebFilterChain(ServerHttpSecurity http) {
        if (!securityEnabled) {
            // ê°œë°œ í™˜ê²½ì—ì„œë§Œ ì‚¬ìš© - í”„ë¡œë•ì…˜ì—ì„œëŠ” ì ˆëŒ€ ë¹„í™œì„±í™”í•˜ì§€ ë§ˆì„¸ìš”!
            return http
                    .csrf(ServerHttpSecurity.CsrfSpec::disable)
                    .cors(cors -> cors.configurationSource(corsConfigurationSource()))
                    .httpBasic(ServerHttpSecurity.HttpBasicSpec::disable)
                    .formLogin(ServerHttpSecurity.FormLoginSpec::disable)
                    .authorizeExchange(exchange -> exchange.anyExchange().permitAll())
                    .build();
        }

        return http
                .csrf(ServerHttpSecurity.CsrfSpec::disable)
                .cors(cors -> cors.configurationSource(corsConfigurationSource()))
                .httpBasic(ServerHttpSecurity.HttpBasicSpec::disable)
                .formLogin(ServerHttpSecurity.FormLoginSpec::disable)
                .authorizeExchange(exchange -> exchange
                        // ========================================
                        // Public Endpoints (ì¸ì¦ ë¶ˆí•„ìš”)
                        // ========================================
                        // Health checks & Actuator
                        .pathMatchers("/actuator/**").permitAll()
                        .pathMatchers("/api/actuator/**").permitAll()
                        
                        // Authentication endpoints (login, register, token)
                        .pathMatchers("/api/v1/auth/login").permitAll()
                        .pathMatchers("/api/v1/auth/register").permitAll()
                        .pathMatchers("/api/v1/auth/token").permitAll()
                        .pathMatchers("/api/v1/auth/send-verification").permitAll()
                        .pathMatchers("/api/v1/auth/verify-email").permitAll()
                        .pathMatchers("/api/v1/auth/resend-verification").permitAll()
                        .pathMatchers("/api/v1/auth/check-username/**").permitAll()
                        .pathMatchers("/api/v1/auth/check-email/**").permitAll()
                        
                        // CORS preflight
                        .pathMatchers(HttpMethod.OPTIONS, "/**").permitAll()
                        
                        // ========================================
                        // Protected Endpoints (ì¸ì¦ í•„ìš” - downstreamì—ì„œ ê²€ì¦)
                        // GatewayëŠ” í† í°ì„ ì „ë‹¬ë§Œ í•˜ê³ , ì‹¤ì œ ê²€ì¦ì€ ê° ì„œë¹„ìŠ¤ì—ì„œ ìˆ˜í–‰
                        // ========================================
                        // ëª¨ë“  ë‹¤ë¥¸ ìš”ì²­ì€ í†µê³¼ì‹œí‚¤ë˜, downstream ì„œë¹„ìŠ¤ê°€ ì¸ì¦ì„ ì²˜ë¦¬
                        .anyExchange().permitAll()
                )
                .build();
    }

    @Bean
    public CorsConfigurationSource corsConfigurationSource() {
        CorsConfiguration configuration = new CorsConfiguration();
        configuration.setAllowedOriginPatterns(List.of("*"));
        configuration.setAllowedMethods(Arrays.asList("GET", "POST", "PUT", "DELETE", "PATCH", "OPTIONS", "HEAD"));
        configuration.setAllowedHeaders(List.of("*"));
        configuration.setAllowCredentials(true);
        configuration.setMaxAge(3600L);
        
        UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource();
        source.registerCorsConfiguration("/**", configuration);
        return source;
    }

    /**
     * Rate Limiterìš© KeyResolver
     * IP ì£¼ì†Œ ê¸°ë°˜ìœ¼ë¡œ Rate Limit ì ìš©
     */
    @Bean
    public KeyResolver ipKeyResolver() {
        return exchange -> {
            var remoteAddress = exchange.getRequest().getRemoteAddress();
            String ip = remoteAddress != null
                    ? remoteAddress.getAddress().getHostAddress()
                    : "unknown";
            return Mono.just(ip);
        };
    }
}

```

---

## backend/api-gateway-service/src/main/java/com/newsinsight/gateway/controller/FrontendConfigController.java

```java
package com.newsinsight.gateway.controller;

import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Mono;

import java.util.HashMap;
import java.util.Map;
import java.util.Base64;

@RestController
@RequestMapping("/api/v1/config")
public class FrontendConfigController {

    @Value("${FRONTEND_API_BASE_URL:${API_GATEWAY_FRONTEND_API_BASE_URL:http://localhost:8112}}")
    private String frontendApiBaseUrl;

    @Value("${spring.cloud.consul.host:consul}")
    private String consulHost;

    @Value("${spring.cloud.consul.port:8500}")
    private int consulPort;

    private final WebClient webClient;

    public FrontendConfigController(WebClient.Builder webClientBuilder) {
        this.webClient = webClientBuilder.build();
    }

    @CrossOrigin(origins = "*")
    @GetMapping("/frontend")
    public ResponseEntity<Map<String, String>> getFrontendConfig() {
        Map<String, String> body = new HashMap<>();
        body.put("apiBaseUrl", frontendApiBaseUrl);
        return ResponseEntity.ok(body);
    }

    /**
     * Save AI/LLM settings to Consul KV store
     * PUT /api/v1/config/ai-settings
     */
    @CrossOrigin(origins = "*")
    @PutMapping("/ai-settings")
    public Mono<ResponseEntity<Map<String, Object>>> saveAISettings(@RequestBody Map<String, String> settings) {
        String consulUrl = "http://" + consulHost + ":" + consulPort;
        
        // Save each setting to Consul KV under config/autonomous-crawler/ prefix
        return Mono.when(
            settings.entrySet().stream()
                .map(entry -> {
                    String key = "config/autonomous-crawler/" + entry.getKey();
                    Object value = entry.getValue();
                    // Consul expects base64 encoded value for PUT
                    return webClient.put()
                        .uri(consulUrl + "/v1/kv/" + key)
                        .bodyValue(value.toString().isEmpty() ? "" : value)
                        .retrieve()
                        .bodyToMono(Boolean.class)
                        .onErrorReturn(false);
                })
                .toArray(Mono[]::new)
        ).then(Mono.fromCallable(() -> {
            Map<String, Object> response = new HashMap<>();
            response.put("success", true);
            response.put("message", "AI settings saved to Consul");
            response.put("keysCount", settings.size());
            return ResponseEntity.ok(response);
        }));
    }

    /**
     * Get AI/LLM settings from Consul KV store
     * GET /api/v1/config/ai-settings
     */
    @CrossOrigin(origins = "*")
    @GetMapping("/ai-settings")
    public Mono<ResponseEntity<Map<String, String>>> getAISettings() {
        String consulUrl = "http://" + consulHost + ":" + consulPort;
        
        return webClient.get()
            .uri(consulUrl + "/v1/kv/config/autonomous-crawler/?recurse=true")
            .retrieve()
            .bodyToMono(Object[].class)
            .map(entries -> {
                Map<String, String> settings = new HashMap<>();
                if (entries != null) {
                    for (Object entry : entries) {
                        if (entry instanceof Map) {
                            @SuppressWarnings("unchecked")
                            Map<String, Object> kvEntry = (Map<String, Object>) entry;
                            String fullKey = (String) kvEntry.get("Key");
                            String encodedValue = (String) kvEntry.get("Value");
                            
                            if (fullKey != null && encodedValue != null) {
                                // Remove prefix and decode value
                                String key = fullKey.replace("config/autonomous-crawler/", "");
                                String value = new String(Base64.getDecoder().decode(encodedValue));
                                settings.put(key, value);
                            }
                        }
                    }
                }
                return ResponseEntity.ok(settings);
            })
            .onErrorReturn(ResponseEntity.ok(new HashMap<>()));
    }
}

```

---

## backend/api-gateway-service/src/main/java/com/newsinsight/gateway/filter/JwtAuthenticationFilter.java

```java
package com.newsinsight.gateway.filter;

import io.jsonwebtoken.Claims;
import io.jsonwebtoken.Jwts;
import io.jsonwebtoken.security.Keys;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.cloud.gateway.filter.GatewayFilterChain;
import org.springframework.cloud.gateway.filter.GlobalFilter;
import org.springframework.core.Ordered;
import org.springframework.http.HttpCookie;
import org.springframework.http.HttpStatus;
import org.springframework.http.server.reactive.ServerHttpRequest;
import org.springframework.stereotype.Component;
import org.springframework.util.MultiValueMap;
import org.springframework.web.server.ServerWebExchange;
import reactor.core.publisher.Mono;

import javax.crypto.SecretKey;
import java.nio.charset.StandardCharsets;
import java.util.List;

/**
 * JWT ì¸ì¦ í•„í„°
 * 
 * Python FastAPIì˜ auth_middlewareì™€ ë™ì¼í•œ ê¸°ëŠ¥ êµ¬í˜„
 * - Authorization í—¤ë”ì—ì„œ Bearer í† í° ì¶”ì¶œ
 * - Cookieì—ì„œ access_token ì¶”ì¶œ (SSE/EventSource ì§€ì›)
 * - ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ì—ì„œ token ì¶”ì¶œ (fallback)
 * - JWT í† í° ê²€ì¦ (ì„œëª…, ë§Œë£Œ ì‹œê°„ ë“±)
 * - ì‚¬ìš©ì ì •ë³´ë¥¼ í—¤ë”ì— ì¶”ê°€í•˜ì—¬ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì„œë¹„ìŠ¤ë¡œ ì „ë‹¬
 */
@Slf4j
@Component
public class JwtAuthenticationFilter implements GlobalFilter, Ordered {
    
    // ì¸ì¦ ë¶ˆí•„ìš”í•œ ê³µê°œ ê²½ë¡œ
    private static final List<String> PUBLIC_PATHS = List.of(
        "/health",
        "/actuator",
        "/api/v1/auth",          // Public Auth API (register, login, check-username, check-email)
        "/api/v1/articles",
        "/api/v1/analysis",
        "/api/v1/ai",
        "/api/v1/config",
        "/api/v1/sources",
        "/api/v1/collections",
        "/api/v1/data",
        "/api/v1/search",
        "/api/v1/events",        // SSE ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ (EventSourceëŠ” í—¤ë” ì „ì†¡ ë¶ˆê°€)
        "/api/v1/search-history",
        "/api/v1/search-templates",
        "/api/v1/jobs",          // Search Jobs API (SSE ìŠ¤íŠ¸ë¦¼ í¬í•¨)
        "/api/v1/projects",      // Projects API (ìµëª… ì‚¬ìš©ì ì§€ì›)
        "/api/v1/ai",
        "/api/v1/ml",
        "/api/v1/llm-providers", // LLM Provider Settings (ì‚¬ìš©ìë³„ ì„¤ì • ì§€ì›)
        "/api/v1/admin",         // Admin Dashboard (ìì²´ ì¸ì¦ ì²˜ë¦¬)
        "/api/v1/crawler",       // Autonomous Crawler API
        "/api/v1/autocrawl",     // AutoCrawl API (ìë™ í¬ë¡¤ë§ ê´€ë¦¬)
        "/api/browser-use",      // Browser-Use API (gateway path)
        "/api/ml-addons",        // ML Add-ons API (sentiment, factcheck, bias)
        "/browse",               // Browser-Use API (direct path - legacy)
        "/jobs",                 // Browser-Use Jobs (direct path - legacy)
        "/ws"                    // WebSocket (direct path - legacy)
    );
    
    @Value("${JWT_SECRET_KEY:default-secret-key-please-change-in-consul}")
    private String jwtSecretKey;
    
    @Value("${JWT_ALGORITHM:HS256}")
    private String jwtAlgorithm;
    
    @Override
    public Mono<Void> filter(ServerWebExchange exchange, GatewayFilterChain chain) {
        String path = exchange.getRequest().getPath().value();
        
        // ê³µê°œ ì—”ë“œí¬ì¸íŠ¸ëŠ” ì¸ì¦ ìŠ¤í‚µí•˜ì§€ë§Œ ìµëª… ì‚¬ìš©ì í—¤ë”ëŠ” ì¶”ê°€
        if (PUBLIC_PATHS.stream().anyMatch(path::startsWith)) {
            log.debug("Public path: {}, adding anonymous user headers", path);
            return handleAnonymousUser(exchange, chain);
        }
        
        // í† í° ì¶”ì¶œ (ìš°ì„ ìˆœìœ„: Authorization í—¤ë” > Cookie > Query Parameter)
        String token = extractToken(exchange);
        
        if (token == null) {
            log.warn("No valid token found for path: {}", path);
            exchange.getResponse().setStatusCode(HttpStatus.UNAUTHORIZED);
            return exchange.getResponse().setComplete();
        }
        
        try {
            // JWT í† í° íŒŒì‹± ë° ê²€ì¦
            SecretKey key = Keys.hmacShaKeyFor(jwtSecretKey.getBytes(StandardCharsets.UTF_8));
            Claims claims = Jwts.parser()
                    .verifyWith(key)
                    .build()
                    .parseSignedClaims(token)
                    .getPayload();
            
            // ì‚¬ìš©ì ì •ë³´ë¥¼ í—¤ë”ì— ì¶”ê°€ (ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì„œë¹„ìŠ¤ì—ì„œ ì‚¬ìš©)
            ServerHttpRequest mutatedRequest = exchange.getRequest().mutate()
                    .header("X-User-Id", claims.getSubject())
                    .header("X-User-Role", claims.get("role", String.class))
                    .header("X-Username", claims.get("username", String.class))
                    .build();
            
            log.debug("Authenticated user: {} with role: {}", 
                    claims.get("username", String.class), 
                    claims.get("role", String.class));
            
            return chain.filter(exchange.mutate().request(mutatedRequest).build());
            
        } catch (Exception e) {
            log.error("JWT authentication failed: {}", e.getMessage());
            exchange.getResponse().setStatusCode(HttpStatus.UNAUTHORIZED);
            return exchange.getResponse().setComplete();
        }
    }
    
    /**
     * Handle anonymous user by generating unique user ID based on session
     * This prevents data leakage between different anonymous users
     */
    private Mono<Void> handleAnonymousUser(ServerWebExchange exchange, GatewayFilterChain chain) {
        // Extract session ID from headers (sent by frontend)
        String sessionId = exchange.getRequest().getHeaders().getFirst("X-Session-Id");
        String deviceId = exchange.getRequest().getHeaders().getFirst("X-Device-Id");
        
        // Generate unique anonymous user ID based on session
        String anonymousUserId;
        if (sessionId != null && !sessionId.isBlank()) {
            anonymousUserId = "user_anon_" + sessionId;
        } else {
            // Fallback: use device ID or generate random ID
            if (deviceId != null && !deviceId.isBlank()) {
                anonymousUserId = "user_anon_" + deviceId;
            } else {
                // Last resort: generate random ID (not ideal, but prevents null)
                anonymousUserId = "user_anon_" + System.currentTimeMillis();
            }
            log.warn("No session ID provided, using fallback anonymous user ID: {}", anonymousUserId);
        }
        
        // Add anonymous user headers for downstream services
        ServerHttpRequest mutatedRequest = exchange.getRequest().mutate()
                .header("X-User-Id", anonymousUserId)
                .header("X-User-Role", "anonymous")
                .header("X-Session-Id", sessionId != null ? sessionId : "")
                .header("X-Device-Id", deviceId != null ? deviceId : "")
                .build();
        
        log.debug("Anonymous user: userId={}, sessionId={}", anonymousUserId, sessionId);
        
        return chain.filter(exchange.mutate().request(mutatedRequest).build());
    }
    
    /**
     * ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ JWT í† í° ì¶”ì¶œ
     * ìš°ì„ ìˆœìœ„:
     * 1. Authorization í—¤ë” (Bearer token)
     * 2. Cookie (access_token)
     * 3. Query Parameter (token)
     */
    private String extractToken(ServerWebExchange exchange) {
        // 1. Authorization í—¤ë”ì—ì„œ ì¶”ì¶œ
        String authHeader = exchange.getRequest().getHeaders().getFirst("Authorization");
        if (authHeader != null && authHeader.startsWith("Bearer ")) {
            log.debug("Token extracted from Authorization header");
            return authHeader.substring(7);
        }
        
        // 2. Cookieì—ì„œ ì¶”ì¶œ (SSE/EventSource ì§€ì›)
        MultiValueMap<String, HttpCookie> cookies = exchange.getRequest().getCookies();
        HttpCookie accessTokenCookie = cookies.getFirst("access_token");
        if (accessTokenCookie != null && !accessTokenCookie.getValue().isEmpty()) {
            log.debug("Token extracted from access_token cookie");
            return accessTokenCookie.getValue();
        }
        
        // 3. Query Parameterì—ì„œ ì¶”ì¶œ (fallback)
        String queryToken = exchange.getRequest().getQueryParams().getFirst("token");
        if (queryToken != null && !queryToken.isEmpty()) {
            log.debug("Token extracted from query parameter");
            return queryToken;
        }
        
        return null;
    }
    
    @Override
    public int getOrder() {
        return -100; // ë†’ì€ ìš°ì„ ìˆœìœ„ (ë¨¼ì € ì‹¤í–‰)
    }
}

```

---

## backend/api-gateway-service/src/main/java/com/newsinsight/gateway/filter/RbacFilter.java

```java
package com.newsinsight.gateway.filter;

import lombok.extern.slf4j.Slf4j;
import org.springframework.cloud.gateway.filter.GatewayFilterChain;
import org.springframework.cloud.gateway.filter.GlobalFilter;
import org.springframework.core.Ordered;
import org.springframework.http.HttpMethod;
import org.springframework.http.HttpStatus;
import org.springframework.stereotype.Component;
import org.springframework.web.server.ServerWebExchange;
import reactor.core.publisher.Mono;

import java.util.List;
import java.util.Map;

/**
 * RBAC (Role-Based Access Control) í•„í„°
 * 
 * Python FastAPIì˜ rbac_middlewareì™€ ë™ì¼í•œ ê¸°ëŠ¥ êµ¬í˜„
 * - HTTP ë©”ì„œë“œì— ë”°ë¼ í•„ìš”í•œ ê¶Œí•œ í™•ì¸
 * - ì‚¬ìš©ì ì—­í• ì— ë”°ë¼ ì ‘ê·¼ ì œì–´
 */
@Slf4j
@Component
public class RbacFilter implements GlobalFilter, Ordered {
    
    // ì—­í• ë³„ ê¶Œí•œ ë§¤í•‘ (Pythonì˜ ROLE_PERMISSIONSì™€ ë™ì¼)
    private static final Map<String, List<String>> ROLE_PERMISSIONS = Map.of(
        "admin", List.of("READ", "WRITE", "DELETE", "ADMIN"),
        "analyst", List.of("READ", "WRITE"),
        "viewer", List.of("READ"),
        "system", List.of("READ", "WRITE", "DELETE"),
        "anonymous", List.of("READ", "WRITE"),  // ìµëª… ì‚¬ìš©ì - ê²€ìƒ‰ ë° ë¶„ì„ í—ˆìš©
        "user", List.of("READ", "WRITE")        // ì¼ë°˜ íšŒì› - ê²€ìƒ‰ ë° ë¶„ì„ í—ˆìš©
    );
    
    // HTTP ë©”ì„œë“œë³„ í•„ìš” ê¶Œí•œ
    private static final Map<HttpMethod, String> METHOD_PERMISSIONS = Map.of(
        HttpMethod.GET, "READ",
        HttpMethod.POST, "WRITE",
        HttpMethod.PUT, "WRITE",
        HttpMethod.PATCH, "WRITE",
        HttpMethod.DELETE, "DELETE"
    );
    
    @Override
    public Mono<Void> filter(ServerWebExchange exchange, GatewayFilterChain chain) {
        String userRole = exchange.getRequest().getHeaders().getFirst("X-User-Role");
        
        if (userRole == null) {
            // ì¸ì¦ë˜ì§€ ì•Šì€ ìš”ì²­ (public endpoint)
            log.debug("No user role found, allowing request to proceed");
            return chain.filter(exchange);
        }
        
        HttpMethod method = exchange.getRequest().getMethod();
        String requiredPermission = METHOD_PERMISSIONS.get(method);
        List<String> userPermissions = ROLE_PERMISSIONS.getOrDefault(userRole, List.of());
        
        if (requiredPermission != null && !userPermissions.contains(requiredPermission)) {
            log.warn("Access denied for role: {} on method: {}. Required permission: {}", 
                    userRole, method, requiredPermission);
            exchange.getResponse().setStatusCode(HttpStatus.FORBIDDEN);
            return exchange.getResponse().setComplete();
        }
        
        log.debug("Access granted for role: {} on method: {}", userRole, method);
        return chain.filter(exchange);
    }
    
    @Override
    public int getOrder() {
        return -90; // JWT í•„í„° ë‹¤ìŒ ì‹¤í–‰
    }
}

```

---

## backend/api-gateway-service/src/main/resources/application.yml

```yml
spring:
  application:
    name: api-gateway
  
  config:
    # Consulì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸° (optional - Consul ì—†ì–´ë„ ì‹œì‘ ê°€ëŠ¥)
    import: ${SPRING_CONFIG_IMPORT:optional:consul:}
  
  cloud:
    consul:
      enabled: ${CONSUL_ENABLED:true}
      host: ${CONSUL_HOST:localhost}
      port: ${CONSUL_PORT:8500}
      config:
        enabled: ${CONSUL_CONFIG_ENABLED:true}
        # ê¸°ì¡´ Consul KV êµ¬ì¡°ì™€ ì¼ì¹˜
        prefix: config
        default-context: ${spring.application.name}
        format: PROPERTIES
        # Fail-Fast ë¹„í™œì„±í™”: Consul ì—°ê²° ì‹¤íŒ¨í•´ë„ ì‹œì‘
        fail-fast: ${CONSUL_FAIL_FAST:false}
      discovery:
        enabled: ${CONSUL_DISCOVERY_ENABLED:true}
        health-check-path: /actuator/health
        health-check-interval: 10s
        instance-id: ${spring.application.name}:${random.value}
        prefer-ip-address: true
        # ì„œë¹„ìŠ¤ ë“±ë¡ ì‹¤íŒ¨í•´ë„ ì‹œì‘
        fail-fast: false
    inetutils:
      preferred-networks:
        - 172.20

    gateway:
      routes:
        # ===========================================
        # Collector Service Routes
        # COLLECTOR_SERVICE_URL í™˜ê²½ë³€ìˆ˜ë¡œ ì§ì ‘ URL ì§€ì • ê°€ëŠ¥
        # Consul ì‚¬ìš© ì‹œ: lb://collector-service
        # Consul ë¯¸ì‚¬ìš© ì‹œ: http://collector-service:8081 (ê¸°ë³¸ê°’)
        # ===========================================
        
        # Collector ì„œë¹„ìŠ¤ - ë°ì´í„° API (DataController)
        - id: collector-data
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/data/**
          # NOTE: Redis RateLimiter ë¹„í™œì„±í™” - Redis ì—†ì´ë„ ë™ì‘í•˜ë„ë¡ ì„¤ì •
          # Redis ë„ì… ì‹œ ì•„ë˜ ì£¼ì„ í•´ì œ
          # filters:
          #   - name: RequestRateLimiter
          #     args:
          #       redis-rate-limiter.replenishRate: 100
          #       redis-rate-limiter.burstCapacity: 150

        # Collector ì„œë¹„ìŠ¤ - ì†ŒìŠ¤ ê´€ë¦¬ (SourceController)
        - id: collector-sources
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/sources/**

        # Collector ì„œë¹„ìŠ¤ - ìˆ˜ì§‘ ì‘ì—… (CollectionController)
        - id: collector-collections
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/collections/**

        # Articles & Analysis - í”„ë¡ íŠ¸ì—”ë“œ API
        - id: articles-root
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/articles

        - id: articles-alias
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/articles/**

        - id: analysis-root
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/analysis

        - id: analysis-alias
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/analysis/**
          metadata:
            response-timeout: 300000

        # Search API - í†µí•© ê²€ìƒ‰ (SSE ìŠ¤íŠ¸ë¦¬ë°)
        # NOTE: SSE ìŠ¤íŠ¸ë¦¬ë°ì€ RateLimiterì™€ í˜¸í™˜ì„± ë¬¸ì œê°€ ìˆì–´ ë¹„í™œì„±í™”
        - id: search-api
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/search/**

        # Search History API - ê²€ìƒ‰ ê¸°ë¡ ê´€ë¦¬
        - id: search-history
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/search-history/**
          metadata:
            # SSE ìŠ¤íŠ¸ë¦¼ ì—°ê²°ì„ ìœ„í•œ ê¸´ íƒ€ì„ì•„ì›ƒ (5ë¶„)
            response-timeout: 300000

        # Search Template API - SmartSearch ê²€ìƒ‰ í…œí”Œë¦¿ ê´€ë¦¬
        - id: search-templates
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/search-templates/**

        # Reports API - PDF ë³´ê³ ì„œ ìƒì„± ë° ë‹¤ìš´ë¡œë“œ (ReportController)
        - id: reports
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/reports/**
          metadata:
            # PDF ìƒì„±ì— ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê¸´ íƒ€ì„ì•„ì›ƒ (3ë¶„)
            response-timeout: 180000

        # Search Jobs API - ê²€ìƒ‰ ì‘ì—… ê´€ë¦¬ (SearchJobController)
        # SSE ìŠ¤íŠ¸ë¦¬ë° ì§€ì›: /api/v1/jobs/stream, /api/v1/jobs/{jobId}/stream
        - id: search-jobs
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/jobs,/api/v1/jobs/**
          metadata:
            # SSE ìŠ¤íŠ¸ë¦¼ ì—°ê²°ì„ ìœ„í•œ ê¸´ íƒ€ì„ì•„ì›ƒ (5ë¶„)
            response-timeout: 300000

        # AutoCrawl API - ìë™ í¬ë¡¤ë§ ê´€ë¦¬ (AutoCrawlController)
        - id: autocrawl
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/autocrawl/**

        # Projects API - í”„ë¡œì íŠ¸ ê´€ë¦¬ (ProjectController)
        - id: projects
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/projects/**

        # Workspace Files API - íŒŒì¼ ì—…ë¡œë“œ/ë‹¤ìš´ë¡œë“œ (WorkspaceController)
        - id: workspace-files
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/workspace/**
          metadata:
            # íŒŒì¼ ì—…ë¡œë“œ/ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•œ ê¸´ íƒ€ì„ì•„ì›ƒ (5ë¶„)
            response-timeout: 300000

        # Config API - í”„ë¡ íŠ¸ì—”ë“œ ì„¤ì •
        # NOTE: Gateway ìì²´ FrontendConfigControllerê°€ ìš°ì„  ì²˜ë¦¬ë¨
        # ì´ ë¼ìš°íŠ¸ëŠ” fallbackìœ¼ë¡œ ìœ ì§€ (collector-serviceì—ë„ ConfigControllerê°€ ìˆì„ ê²½ìš°)
        - id: config-api
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/config/**

        # LLM Provider Settings - LLM ì œê³µì ì„¤ì • (collector-service LlmProviderSettingsController)
        - id: llm-providers
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/llm-providers/**

        # AI Orchestration - AI ë¶„ì„ ìš”ì²­ (collector-service AiOrchestrationController)
        - id: collector-ai
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/ai/**

        - id: collector-ml
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/ml/**

        # Dashboard Events - SSE ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ (collector-service DashboardEventsController)
        - id: collector-events
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/events/**
          metadata:
            # SSE ì—°ê²°ì„ ìœ„í•œ ê¸´ íƒ€ì„ì•„ì›ƒ
            response-timeout: 300000

        # Admin Dashboard API
        # NOTE: Using direct URL fallback if not found in Consul, assuming port 8888 from main.py
        - id: admin-dashboard-api
          uri: ${ADMIN_DASHBOARD_URL:http://localhost:8888}
          predicates:
            - Path=/api/v1/admin/**
          # No StripPrefix needed because the admin API uses /api/v1/admin prefix

        # Public Auth API - íšŒì›ê°€ì…, ë¡œê·¸ì¸ (Admin Dashboardì—ì„œ ì²˜ë¦¬)
        # /api/v1/auth/register, /api/v1/auth/login, /api/v1/auth/me ë“±
        - id: public-auth
          uri: ${ADMIN_DASHBOARD_URL:http://localhost:8888}
          predicates:
            - Path=/api/v1/auth/**
          # No StripPrefix needed because the admin API uses /api/v1/auth prefix

        # Autonomous Crawler - í¬ë¡¤ë§ ì‘ì—… ê´€ë¦¬
        - id: autonomous-crawler-api
          uri: ${AUTONOMOUS_CRAWLER_API_URL:${AUTONOMOUS_CRAWLER_URL:http://autonomous-crawler:8030}}
          predicates:
            - Path=/api/v1/crawler/**
          filters:
            # /api/v1/crawler/** -> /** ë¡œ ì „ë‹¬ (3 segments: api, v1, crawler)
            - StripPrefix=3

        # Browser-Use API - AI ë¸Œë¼ìš°ì € ìë™í™”
        - id: browser-use-browse
          uri: ${BROWSER_USE_URL:http://localhost:8500}
          predicates:
            - Path=/api/browser-use/browse/**
          filters:
            # /api/browser-use/browse/** -> /browse/** ë¡œ ì „ë‹¬
            - StripPrefix=2

        - id: browser-use-jobs
          uri: ${BROWSER_USE_URL:http://localhost:8500}
          predicates:
            - Path=/api/browser-use/jobs/**
          filters:
            # /api/browser-use/jobs/** -> /jobs/** ë¡œ ì „ë‹¬
            - StripPrefix=2

        - id: browser-use-health
          uri: ${BROWSER_USE_URL:http://localhost:8500}
          predicates:
            - Path=/api/browser-use/health
          filters:
            # /api/browser-use/health -> /health ë¡œ ì „ë‹¬
            - StripPrefix=2

        # Browser-Use WebSocket
        - id: browser-use-websocket
          uri: ${BROWSER_USE_WS_URL:ws://localhost:8500}
          predicates:
            - Path=/api/browser-use/ws/**
          filters:
            # /api/browser-use/ws/** -> /ws/** ë¡œ ì „ë‹¬
            - StripPrefix=2

        # ML Add-ons - ê°ì • ë¶„ì„ (Sentiment Analysis)
        - id: ml-addon-sentiment
          uri: ${ML_ADDON_SENTIMENT_URL:http://sentiment-addon:8100}
          predicates:
            - Path=/api/ml-addons/sentiment/**
          filters:
            # /api/ml-addons/sentiment/** -> /** ë¡œ ì „ë‹¬
            - StripPrefix=3

        # Fact Check Chat - íŒ©íŠ¸ì²´í¬ ì±—ë´‡
        - id: factcheck-chat
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/factcheck-chat/**
          filters:
            - StripPrefix=0

        # ML Add-ons - íŒ©íŠ¸ì²´í¬ (Fact Check)
        - id: ml-addon-factcheck
          uri: ${ML_ADDON_FACTCHECK_URL:http://factcheck-addon:8101}
          predicates:
            - Path=/api/ml-addons/factcheck/**
          filters:
            # /api/ml-addons/factcheck/** -> /** ë¡œ ì „ë‹¬
            - StripPrefix=3

        # ML Add-ons - í¸í–¥ë„ ë¶„ì„ (Bias Analysis)
        - id: ml-addon-bias
          uri: ${ML_ADDON_BIAS_URL:http://bias-addon:8102}
          predicates:
            - Path=/api/ml-addons/bias/**
          filters:
            # /api/ml-addons/bias/** -> /** ë¡œ ì „ë‹¬
            - StripPrefix=3

        - id: ml-addon-bot-detector
          uri: ${ML_ADDON_BOT_DETECTOR_URL:http://bot-detector:8041}
          predicates:
            - Path=/api/ml-addons/bot-detector/**
          filters:
            - StripPrefix=3

        # ML Trainer Service - ëª¨ë¸ í•™ìŠµ ì„œë¹„ìŠ¤
        - id: ml-trainer
          uri: ${ML_TRAINER_URL:http://ml-trainer:8090}
          predicates:
            - Path=/api/ml-trainer/**
          filters:
            # /api/ml-trainer/** -> /** ë¡œ ì „ë‹¬
            - StripPrefix=2
          metadata:
            # í•™ìŠµ ì‘ì—…ì€ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê¸´ íƒ€ì„ì•„ì›ƒ
            response-timeout: 600000

        # ML Trainer SSE Stream - ì‹¤ì‹œê°„ í•™ìŠµ ì§„í–‰ ìƒí™©
        - id: ml-trainer-stream
          uri: ${ML_TRAINER_URL:http://ml-trainer:8090}
          predicates:
            - Path=/api/ml-trainer/jobs/*/stream
          filters:
            - StripPrefix=2
          metadata:
            # SSE ìŠ¤íŠ¸ë¦¼ì„ ìœ„í•œ ë§¤ìš° ê¸´ íƒ€ì„ì•„ì›ƒ (30ë¶„)
            response-timeout: 1800000

        # Health Check Rewrite - í”„ë¡ íŠ¸ì—”ë“œê°€ /api/actuator/health í˜¸ì¶œ ì‹œ ì²˜ë¦¬
        - id: gateway-health
          uri: http://localhost:${PORT:8000}
          predicates:
            - Path=/api/actuator/health,/api/actuator/health/**
          filters:
            # /api/actuator/health -> /actuator/health ë¡œ ì¬ì‘ì„±
            - RewritePath=/api/actuator/(?<segment>.*), /actuator/${segment}

        # General Health Check - /api/health ê²½ë¡œ ì§€ì›
        - id: api-health
          uri: http://localhost:${PORT:8000}
          predicates:
            - Path=/api/health
          filters:
            - RewritePath=/api/health, /actuator/health

      # NOTE: CORS is handled by SecurityConfig.java - DO NOT enable globalcors
      # Having both causes duplicate Access-Control-Allow-Origin headers
      # globalcors:
      #   cors-configurations:
      #     '[/**]':
      #       allowedOriginPatterns: "*"
      #       allowedMethods:
      #         - GET
      #         - POST
      #         - PUT
      #         - DELETE
      #         - PATCH
      #         - OPTIONS
      #       allowedHeaders: "*"
      #       allowCredentials: true
      #       maxAge: 3600
  
  # Redis ì„¤ì • (Rate Limiting ë° ì„¸ì…˜ ê´€ë¦¬ìš©)
  data:
    redis:
      host: ${REDIS_HOST:redis}
      port: ${REDIS_PORT:6379}
      password: ${REDIS_PASSWORD:}

server:
  port: ${PORT:8000}
  # Netty ì„¤ì • - SSE/WebSocket ì—°ê²°ì„ ìœ„í•œ íƒ€ì„ì•„ì›ƒ ì¦ê°€
  netty:
    # ì—°ê²° ìœ ì§€ ì‹œê°„ (5ë¶„)
    idle-timeout: 300000

# Spring Cloud Gateway HTTP Client ì„¤ì •
spring.cloud.gateway.httpclient:
  connect-timeout: 10000
  response-timeout: 300000
  pool:
    max-idle-time: 300000

management:
  endpoints:
    web:
      exposure:
        include: health,info,env,metrics,prometheus
  endpoint:
    health:
      show-details: always

logging:
  level:
    root: INFO
    org.springframework.cloud.gateway: DEBUG
    org.springframework.cloud.gateway.handler.RoutePredicateHandlerMapping: TRACE
    org.springframework.security: DEBUG
    com.newsinsight.gateway: DEBUG

```

---

## backend/autonomous-crawler-service/crawl-worker/main.py

```py
import asyncio
import os
import sys
import time
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
import uuid
import logging

log = logging.getLogger(__name__)

# Add shared module to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))
try:
    from shared.prometheus_metrics import (
        setup_metrics,
        track_request_time,
        track_operation,
        track_error,
        track_item_processed,
        ServiceMetrics,
    )

    METRICS_AVAILABLE = True
except ImportError:
    METRICS_AVAILABLE = False

try:
    from shared.proxy_client import ProxyRotationClient, ProxyInfo

    PROXY_CLIENT_AVAILABLE = True
except ImportError:
    PROXY_CLIENT_AVAILABLE = False
    ProxyRotationClient = None
    ProxyInfo = None

try:
    from crawl4ai import AsyncWebCrawler  # type: ignore
except Exception:
    AsyncWebCrawler = None  # fallback for environments without crawl4ai


def detect_captcha_type(content: str) -> Optional[str]:
    text = content.lower()

    cloudflare_markers = [
        "cf-turnstile",
        "turnstile",
        "challenges.cloudflare.com",
        "challenge-running",
        "cf-browser-verification",
        "checking your browser",
        "just a moment",
    ]
    if any(m in text for m in cloudflare_markers):
        return "cloudflare"

    if "recaptcha" in text or "g-recaptcha" in text:
        return "recaptcha"

    if "hcaptcha" in text or "h-captcha" in text:
        return "hcaptcha"

    if "captcha" in text:
        return "captcha"

    return None

app = FastAPI(title="Crawl4AI Worker", version="0.3.0")

# Setup Prometheus metrics
SERVICE_NAME = "crawl-worker"
if METRICS_AVAILABLE:
    setup_metrics(app, SERVICE_NAME, version="0.3.0")
    service_metrics = ServiceMetrics(SERVICE_NAME)
    # Create service-specific metrics
    crawl_requests = service_metrics.create_counter(
        "crawl_requests_total",
        "Total crawl requests",
        ["status", "js_render", "proxy_used"],
    )
    crawl_latency = service_metrics.create_histogram(
        "crawl_latency_seconds",
        "Crawl request latency",
        ["status"],
        buckets=(0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0),
    )
    batch_crawls = service_metrics.create_counter(
        "batch_crawls_total", "Total batch crawl operations", ["status"]
    )
    concurrent_crawls_gauge = service_metrics.create_gauge(
        "concurrent_crawls", "Number of concurrent crawl operations"
    )
    proxy_usage = service_metrics.create_counter(
        "proxy_usage_total", "Proxy usage statistics", ["proxy_id", "status"]
    )
else:
    service_metrics = None

# In-memory storage for batch results
batch_results: Dict[str, Dict[str, Any]] = {}

# Semaphore for concurrent crawl limit
MAX_CONCURRENT_CRAWLS = int(os.environ.get("MAX_CONCURRENT_CRAWLS", "5"))
crawl_semaphore = asyncio.Semaphore(MAX_CONCURRENT_CRAWLS)

# Proxy rotation configuration
USE_PROXY_ROTATION = os.environ.get("USE_PROXY_ROTATION", "true").lower() == "true"
PROXY_ROTATION_URL = os.environ.get("PROXY_ROTATION_URL", "http://ip-rotation:8050")

# Initialize proxy client
proxy_client: Optional["ProxyRotationClient"] = None  # type: ignore
if PROXY_CLIENT_AVAILABLE and USE_PROXY_ROTATION:
    proxy_client = ProxyRotationClient(
        base_url=PROXY_ROTATION_URL,
        timeout=5.0,
        enabled=True,
    )
    log.info(f"Proxy rotation enabled, connecting to {PROXY_ROTATION_URL}")


class CrawlRequest(BaseModel):
    url: str
    js_render: bool = False
    wait_for: Optional[str] = None  # CSS selector to wait for (optional)
    use_proxy: bool = True  # Whether to use proxy rotation


class CrawlResponse(BaseModel):
    url: str
    markdown: Optional[str] = None
    html: Optional[str] = None
    status: str
    error: Optional[str] = None
    proxy_used: Optional[str] = None
    latency_ms: Optional[int] = None


class BatchCrawlRequest(BaseModel):
    """ë°°ì¹˜ í¬ë¡¤ë§ ìš”ì²­"""

    urls: List[str] = Field(
        ..., min_length=1, max_length=50, description="í¬ë¡¤ë§í•  URL ëª©ë¡ (ìµœëŒ€ 50ê°œ)"
    )
    js_render: bool = Field(default=False, description="JavaScript ë Œë”ë§ ì—¬ë¶€")
    wait_for: Optional[str] = Field(default=None, description="ëŒ€ê¸°í•  CSS ì…€ë ‰í„°")
    extract_links: bool = Field(default=False, description="ë§í¬ ì¶”ì¶œ ì—¬ë¶€")
    use_proxy: bool = Field(default=True, description="í”„ë¡ì‹œ ë¡œí…Œì´ì…˜ ì‚¬ìš© ì—¬ë¶€")


class BatchCrawlResult(BaseModel):
    """ë°°ì¹˜ í¬ë¡¤ë§ ê²°ê³¼"""

    batch_id: str
    total: int
    completed: int
    failed: int
    status: str  # "processing", "completed", "partial"
    results: List[CrawlResponse]


@app.get("/health")
@app.head("/health")
async def health():
    proxy_healthy = False
    if proxy_client:
        proxy_healthy = await proxy_client.health_check()

    return {
        "status": "ok",
        "service": "crawl-worker",
        "version": "0.3.0",
        "crawl4ai_available": AsyncWebCrawler is not None,
        "max_concurrent": MAX_CONCURRENT_CRAWLS,
        "proxy_rotation_enabled": USE_PROXY_ROTATION,
        "proxy_service_healthy": proxy_healthy,
    }


async def get_proxy_for_crawl(
    use_proxy: bool = True,
) -> tuple[Optional[str], Optional[str]]:
    """
    Get a proxy URL for crawling.

    Returns:
        Tuple of (proxy_url, proxy_id) or (None, None) if no proxy is used
    """
    if not use_proxy or not proxy_client:
        return None, None

    try:
        proxy_info = await proxy_client.get_next_proxy()
        if proxy_info:
            return proxy_info.get_proxy_url(), proxy_info.id
    except Exception as e:
        log.warning(f"Failed to get proxy: {e}")

    return None, None


async def record_proxy_result(
    proxy_id: Optional[str], success: bool, latency_ms: int = 0, error: str = ""
):
    """Record the result of a proxy-enabled request."""
    if not proxy_id or not proxy_client:
        return

    try:
        if success:
            await proxy_client.record_success(proxy_id, latency_ms)
        else:
            await proxy_client.record_failure(proxy_id, error)
    except Exception as e:
        log.debug(f"Failed to record proxy result: {e}")


async def crawl_single_url(
    url: str,
    js_render: bool = False,
    wait_for: Optional[str] = None,
    use_proxy: bool = True,
) -> CrawlResponse:
    """ë‹¨ì¼ URL í¬ë¡¤ë§ (ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì‹¤í–‰ ì œí•œ, í”„ë¡ì‹œ ë¡œí…Œì´ì…˜ ì§€ì›)"""
    async with crawl_semaphore:
        if AsyncWebCrawler is None:
            return CrawlResponse(url=url, status="FAILED", error="crawl4ai not available")

        max_attempts = 3 if use_proxy else 1
        last_error: str | None = None
        last_proxy_id: str | None = None

        for attempt in range(max_attempts):
            start_time = time.time()
            proxy_url, proxy_id = await get_proxy_for_crawl(use_proxy)
            if proxy_id:
                last_proxy_id = proxy_id

            try:
                # Prepare crawler configuration
                crawler_kwargs = {"verbose": False}
                if proxy_url:
                    crawler_kwargs["proxy"] = proxy_url
                    log.debug(f"Using proxy {proxy_id} for {url} (attempt={attempt + 1}/{max_attempts})")

                async with AsyncWebCrawler(**crawler_kwargs) as crawler:
                    result = await crawler.arun(
                        url=url,
                        js_code=wait_for if js_render else None,
                    )

                    latency_ms = int((time.time() - start_time) * 1000)

                    if not getattr(result, "success", False):
                        error_msg = getattr(result, "error_message", "Unknown error")
                        last_error = error_msg
                        await record_proxy_result(proxy_id, False, latency_ms, error_msg)
                        continue

                    html = getattr(result, "html", None) or ""
                    markdown = getattr(result, "markdown", None) or ""
                    captcha_type = detect_captcha_type(html) or detect_captcha_type(markdown)
                    if captcha_type:
                        last_error = f"CAPTCHA detected: {captcha_type}"
                        if proxy_id and proxy_client:
                            try:
                                await proxy_client.record_captcha(proxy_id, captcha_type=captcha_type)
                            except Exception:
                                pass
                        await record_proxy_result(proxy_id, False, latency_ms, last_error)
                        continue

                    await record_proxy_result(proxy_id, True, latency_ms)
                    return CrawlResponse(
                        url=getattr(result, "url", url),
                        markdown=getattr(result, "markdown", None),
                        html=getattr(result, "html", None),
                        status="SUCCESS",
                        proxy_used=proxy_id,
                        latency_ms=latency_ms,
                    )

            except Exception as e:
                latency_ms = int((time.time() - start_time) * 1000)
                last_error = str(e)
                await record_proxy_result(proxy_id, False, latency_ms, last_error)
                continue

        return CrawlResponse(
            url=url,
            status="FAILED",
            error=last_error or "Crawl failed",
            proxy_used=last_proxy_id,
        )


@app.post("/crawl", response_model=CrawlResponse)
async def crawl_url(request: CrawlRequest):
    """ë‹¨ì¼ URL í¬ë¡¤ë§"""
    if AsyncWebCrawler is None:
        raise HTTPException(
            status_code=500, detail="crawl4ai not available in this environment"
        )

    result = await crawl_single_url(
        request.url,
        request.js_render,
        request.wait_for,
        request.use_proxy,
    )

    if result.status == "FAILED":
        raise HTTPException(status_code=400, detail=result.error or "Crawl failed")

    return result


@app.post("/crawl/batch", response_model=BatchCrawlResult)
async def crawl_batch(request: BatchCrawlRequest):
    """
    ë°°ì¹˜ URL í¬ë¡¤ë§ (ë™ê¸°)

    ì—¬ëŸ¬ URLì„ ë³‘ë ¬ë¡œ í¬ë¡¤ë§í•˜ê³  ëª¨ë“  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ í¬ë¡¤ë§ ìˆ˜ë¥¼ ì œí•œí•©ë‹ˆë‹¤.
    ê° ìš”ì²­ì— ëŒ€í•´ í”„ë¡ì‹œ ë¡œí…Œì´ì…˜ì´ ì ìš©ë©ë‹ˆë‹¤.
    """
    if AsyncWebCrawler is None:
        raise HTTPException(
            status_code=500, detail="crawl4ai not available in this environment"
        )

    batch_id = str(uuid.uuid4())

    # ë³‘ë ¬ í¬ë¡¤ë§ ì‹¤í–‰
    tasks = [
        crawl_single_url(url, request.js_render, request.wait_for, request.use_proxy)
        for url in request.urls
    ]

    results = await asyncio.gather(*tasks, return_exceptions=True)

    # ê²°ê³¼ ì •ë¦¬
    crawl_results: List[CrawlResponse] = []
    completed = 0
    failed = 0

    for i, result in enumerate(results):
        if isinstance(result, Exception):
            crawl_results.append(
                CrawlResponse(url=request.urls[i], status="FAILED", error=str(result))
            )
            failed += 1
        elif isinstance(result, CrawlResponse):
            crawl_results.append(result)
            if result.status == "SUCCESS":
                completed += 1
            else:
                failed += 1
        else:
            crawl_results.append(
                CrawlResponse(
                    url=request.urls[i], status="FAILED", error="Unknown error"
                )
            )
            failed += 1

    status = "completed" if failed == 0 else ("partial" if completed > 0 else "failed")

    return BatchCrawlResult(
        batch_id=batch_id,
        total=len(request.urls),
        completed=completed,
        failed=failed,
        status=status,
        results=crawl_results,
    )


@app.post("/crawl/batch/async")
async def crawl_batch_async(
    request: BatchCrawlRequest, background_tasks: BackgroundTasks
):
    """
    ë¹„ë™ê¸° ë°°ì¹˜ í¬ë¡¤ë§

    ì¦‰ì‹œ batch_idë¥¼ ë°˜í™˜í•˜ê³  ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    ê²°ê³¼ëŠ” GET /crawl/batch/{batch_id}ë¡œ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    if AsyncWebCrawler is None:
        raise HTTPException(
            status_code=500, detail="crawl4ai not available in this environment"
        )

    batch_id = str(uuid.uuid4())

    # ì´ˆê¸° ìƒíƒœ ì €ì¥
    batch_results[batch_id] = {
        "batch_id": batch_id,
        "total": len(request.urls),
        "completed": 0,
        "failed": 0,
        "status": "processing",
        "results": [],
    }

    async def run_batch():
        tasks = [
            crawl_single_url(
                url, request.js_render, request.wait_for, request.use_proxy
            )
            for url in request.urls
        ]

        results = await asyncio.gather(*tasks, return_exceptions=True)

        crawl_results = []
        completed = 0
        failed = 0

        for i, result in enumerate(results):
            if isinstance(result, Exception):
                crawl_results.append(
                    {"url": request.urls[i], "status": "FAILED", "error": str(result)}
                )
                failed += 1
            elif isinstance(result, CrawlResponse):
                crawl_results.append(result.model_dump())
                if result.status == "SUCCESS":
                    completed += 1
                else:
                    failed += 1

        status = (
            "completed" if failed == 0 else ("partial" if completed > 0 else "failed")
        )

        batch_results[batch_id] = {
            "batch_id": batch_id,
            "total": len(request.urls),
            "completed": completed,
            "failed": failed,
            "status": status,
            "results": crawl_results,
        }

    background_tasks.add_task(run_batch)

    return {
        "batch_id": batch_id,
        "total": len(request.urls),
        "status": "processing",
        "message": "ë°°ì¹˜ í¬ë¡¤ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. GET /crawl/batch/{batch_id}ë¡œ ê²°ê³¼ë¥¼ ì¡°íšŒí•˜ì„¸ìš”.",
    }


@app.get("/crawl/batch/{batch_id}")
async def get_batch_result(batch_id: str):
    """ë°°ì¹˜ í¬ë¡¤ë§ ê²°ê³¼ ì¡°íšŒ"""
    if batch_id not in batch_results:
        raise HTTPException(status_code=404, detail="Batch not found")

    return batch_results[batch_id]


@app.delete("/crawl/batch/{batch_id}")
async def delete_batch_result(batch_id: str):
    """ë°°ì¹˜ í¬ë¡¤ë§ ê²°ê³¼ ì‚­ì œ"""
    if batch_id in batch_results:
        del batch_results[batch_id]
        return {"status": "deleted", "batch_id": batch_id}
    raise HTTPException(status_code=404, detail="Batch not found")


@app.get("/proxy/stats")
async def get_proxy_stats():
    """í”„ë¡ì‹œ í’€ í†µê³„ ì¡°íšŒ"""
    if not proxy_client:
        return {"error": "Proxy rotation not enabled"}

    stats = await proxy_client.get_pool_stats()
    return stats or {"error": "Failed to get stats"}


@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup on shutdown"""
    if proxy_client:
        await proxy_client.close()

```

---

## backend/autonomous-crawler-service/crawl-worker/state_store.py

```py
"""
State Storage Module for crawl-worker Service

Provides persistent storage for batch crawl results using Redis.
Falls back to in-memory storage if Redis is unavailable.
"""

import os
import json
import asyncio
from datetime import datetime
from typing import Optional, Dict, Any, List

# Redis configuration from environment
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/2")
REDIS_PREFIX = os.getenv("REDIS_PREFIX", "crawl_worker")
RESULT_TTL_HOURS = int(os.getenv("RESULT_TTL_HOURS", "24"))  # Results expire after 24 hours


class StateStore:
    """Persistent state storage with Redis backend."""
    
    def __init__(self):
        self._redis = None
        self._memory_store: Dict[str, Any] = {}
        self._using_redis = False
        self._lock = asyncio.Lock()
    
    async def connect(self) -> bool:
        """Connect to Redis."""
        try:
            import redis.asyncio as redis
            
            self._redis = redis.from_url(
                REDIS_URL,
                encoding="utf-8",
                decode_responses=True,
                socket_connect_timeout=5,
                socket_timeout=5,
            )
            
            await self._redis.ping()
            self._using_redis = True
            return True
            
        except ImportError:
            self._using_redis = False
            return False
        except Exception:
            self._using_redis = False
            return False
    
    async def disconnect(self):
        """Close Redis connection."""
        if self._redis:
            await self._redis.close()
            self._redis = None
            self._using_redis = False
    
    def _key(self, batch_id: str) -> str:
        """Generate Redis key."""
        return f"{REDIS_PREFIX}:batch:{batch_id}"
    
    async def save_batch(self, batch_id: str, data: Dict[str, Any]) -> bool:
        """Save batch result."""
        async with self._lock:
            try:
                data_json = json.dumps(data, default=str)
                self._memory_store[batch_id] = json.loads(data_json)
                
                if self._using_redis and self._redis:
                    ttl_seconds = RESULT_TTL_HOURS * 60 * 60
                    await self._redis.set(self._key(batch_id), data_json, ex=ttl_seconds)
                
                return True
            except Exception:
                return False
    
    async def load_batch(self, batch_id: str) -> Optional[Dict[str, Any]]:
        """Load batch result."""
        if batch_id in self._memory_store:
            return self._memory_store[batch_id]
        
        if self._using_redis and self._redis:
            try:
                data_json = await self._redis.get(self._key(batch_id))
                if data_json:
                    data = json.loads(data_json)
                    self._memory_store[batch_id] = data
                    return data
            except Exception:
                pass
        
        return None
    
    async def delete_batch(self, batch_id: str) -> bool:
        """Delete batch result."""
        async with self._lock:
            self._memory_store.pop(batch_id, None)
            if self._using_redis and self._redis:
                try:
                    await self._redis.delete(self._key(batch_id))
                except Exception:
                    pass
            return True
    
    @property
    def is_redis_connected(self) -> bool:
        return self._using_redis
    
    def get_memory_store(self) -> Dict[str, Any]:
        return self._memory_store


# Singleton instance
_store: Optional[StateStore] = None


async def get_state_store() -> StateStore:
    """Get or create the singleton StateStore instance."""
    global _store
    if _store is None:
        _store = StateStore()
        await _store.connect()
    return _store

```

---

## backend/autonomous-crawler-service/src/__init__.py

```py
"""Autonomous Crawler Service package."""

```

---

## backend/autonomous-crawler-service/src/api/__init__.py

```py
"""REST API module for autonomous-crawler-service."""

# Lazy imports to avoid circular dependencies
__all__ = ["app", "create_app", "SSEManager", "SSEEventType"]


def __getattr__(name):
    """Lazy import for circular dependency prevention."""
    if name == "app":
        from src.api.server import app

        return app
    elif name == "create_app":
        from src.api.server import create_app

        return create_app
    elif name == "SSEManager":
        from src.api.sse import SSEManager

        return SSEManager
    elif name == "SSEEventType":
        from src.api.sse import SSEEventType

        return SSEEventType
    raise AttributeError(f"module {__name__!r} has no attribute {name!r}")

```

---

## backend/autonomous-crawler-service/src/api/server.py

```py
"""
FastAPI REST Server for autonomous-crawler-service.

browser-agentì˜ REST API + SSE ê¸°ëŠ¥ì„ autonomous-crawler-serviceì— í†µí•©.
ê¸°ì¡´ Kafka ê¸°ë°˜ ì•„í‚¤í…ì²˜ì™€ ë³‘í–‰ ìš´ì˜ë©ë‹ˆë‹¤.
"""

import asyncio
import hashlib
import os
import uuid
from contextlib import asynccontextmanager
from datetime import datetime, timezone
from enum import Enum
from typing import Any, Dict, List, Optional

import httpx
import structlog
from fastapi import BackgroundTasks, Depends, FastAPI, HTTPException, Query, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field, HttpUrl

from src.api.sse import (
    SSEEvent,
    SSEEventType,
    SSEManager,
    get_sse_manager,
    sse_event_generator,
)
from src.config import Settings, get_settings
from src.crawler import AutonomousCrawlerAgent
from src.state.store import StateStore, get_state_store, close_state_store

# MCP Integration
from src.mcp.router import router as mcp_router

# ML Integration
from src.ml.router import router as ml_router
from src.ml.orchestrator import init_ml_orchestrator, get_ml_orchestrator

# Authentication
from src.auth.middleware import get_current_user, require_auth, require_admin, require_operator
from src.auth.jwt_utils import JWTPayload

logger = structlog.get_logger(__name__)


# ========================================
# Configuration
# ========================================


class LLMProvider(str, Enum):
    """ì§€ì›í•˜ëŠ” LLM Provider"""

    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    GOOGLE = "google"
    OPENROUTER = "openrouter"
    OLLAMA = "ollama"
    AZURE = "azure"
    CUSTOM = "custom"


class APIConfig:
    """API ì„œë²„ ì„¤ì •"""

    # í™˜ê²½ë³€ìˆ˜ì—ì„œ LLM API í‚¤ ë¡œë“œ (fallback)
    OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY", "")
    ANTHROPIC_API_KEY: str = os.getenv("ANTHROPIC_API_KEY", "")
    GOOGLE_API_KEY: str = os.getenv("GOOGLE_API_KEY", "")

    DEFAULT_LLM_PROVIDER: str = os.getenv("DEFAULT_LLM_PROVIDER", "openai")
    DEFAULT_MODEL: str = os.getenv("DEFAULT_MODEL", "gpt-4o")

    MAX_CONCURRENT_SESSIONS: int = int(os.getenv("MAX_CONCURRENT_SESSIONS", "3"))
    DEFAULT_TIMEOUT_SEC: int = int(os.getenv("DEFAULT_TIMEOUT_SEC", "120"))

    # DB ê¸°ë°˜ LLM Provider ì‚¬ìš© ì—¬ë¶€
    USE_DB_PROVIDERS: bool = os.getenv("USE_DB_PROVIDERS", "true").lower() == "true"
    COLLECTOR_SERVICE_URL: str = os.getenv("COLLECTOR_SERVICE_URL", "http://collector:8002")
    WEB_CRAWLER_URL: str = os.getenv("WEB_CRAWLER_URL", "http://web-crawler:11235")
    WEB_CRAWLER_API_TOKEN: str = os.getenv(
        "WEB_CRAWLER_API_TOKEN", os.getenv("CRAWL4AI_API_TOKEN", "")
    )


api_config = APIConfig()


# ========================================
# Request/Response Models (Pydantic)
# ========================================


class CrawlMethod(str, Enum):
    """í¬ë¡¤ë§ ë°©ì‹"""

    BROWSER_AGENT = "browser_agent"
    SIMPLE_FETCH = "simple_fetch"
    JS_RENDER = "js_render"


class AgentTask(BaseModel):
    """ë¸Œë¼ìš°ì € ì—ì´ì „íŠ¸ íƒœìŠ¤í¬ ìš”ì²­"""

    url: HttpUrl = Field(..., description="í¬ë¡¤ë§ ëŒ€ìƒ URL")
    task: str = Field(
        ...,
        description="ìì—°ì–´ íƒœìŠ¤í¬ (ì˜ˆ: 'ì´ í˜ì´ì§€ì—ì„œ ë‰´ìŠ¤ ì œëª©ê³¼ ë³¸ë¬¸ì„ ì¶”ì¶œí•´ì¤˜')",
        min_length=5,
        max_length=2000,
    )

    # LLM ì„¤ì •
    llm_provider: LLMProvider = Field(default=LLMProvider.OPENAI)
    model: Optional[str] = Field(default=None, description="ëª¨ë¸ëª… (ê¸°ë³¸: gpt-4o)")

    # í¬ë¡¤ë§ ì˜µì…˜
    screenshot: bool = Field(default=False, description="ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜ ì—¬ë¶€")
    extract_links: bool = Field(default=True, description="í˜ì´ì§€ ë‚´ ë§í¬ ì¶”ì¶œ")
    max_steps: int = Field(default=10, ge=1, le=50, description="ìµœëŒ€ ì—ì´ì „íŠ¸ ìŠ¤í…")
    timeout_sec: int = Field(default=120, ge=30, le=600)

    # URL ì €ì¥ ì˜µì…˜
    auto_save_url: bool = Field(default=True, description="ì¶”ì¶œëœ URL ìë™ ì €ì¥")
    source_category: str = Field(default="news", description="ì†ŒìŠ¤ ì¹´í…Œê³ ë¦¬")


class ExtractedLink(BaseModel):
    """ì¶”ì¶œëœ ë§í¬"""

    url: str
    text: Optional[str] = None
    context: Optional[str] = None


class AgentAction(BaseModel):
    """ì—ì´ì „íŠ¸ê°€ ìˆ˜í–‰í•œ ì•¡ì…˜"""

    step: int
    action_type: str
    target: Optional[str] = None
    value: Optional[str] = None
    timestamp: datetime


class AgentTaskResult(BaseModel):
    """ì—ì´ì „íŠ¸ íƒœìŠ¤í¬ ê²°ê³¼"""

    task_id: str
    url: str
    status: str  # success, failed, timeout

    # ì¶”ì¶œ ê²°ê³¼
    extracted_data: Optional[Dict[str, Any]] = None
    extracted_text: Optional[str] = None
    extracted_links: List[ExtractedLink] = []

    # ë©”íƒ€ë°ì´í„°
    content_hash: Optional[str] = None
    page_title: Optional[str] = None

    # ì—ì´ì „íŠ¸ ë©”íŠ¸ë¦­
    steps_taken: int = 0
    actions: List[AgentAction] = []
    tokens_used: Optional[int] = None
    duration_ms: int = 0

    # ìŠ¤í¬ë¦°ìƒ· (Base64)
    screenshot_base64: Optional[str] = None

    # ì—ëŸ¬ ì •ë³´
    error_message: Optional[str] = None


class BatchCrawlRequest(BaseModel):
    """ë°°ì¹˜ í¬ë¡¤ë§ ìš”ì²­"""

    urls: List[HttpUrl] = Field(..., min_length=1, max_length=20)
    task: str = Field(..., description="ê³µí†µ íƒœìŠ¤í¬")
    llm_provider: LLMProvider = Field(default=LLMProvider.OPENAI)
    model: Optional[str] = None
    auto_save_url: bool = True


class ChatMessage(BaseModel):
    """ì±„íŒ… ë©”ì‹œì§€"""

    role: str = Field(..., description="ë©”ì‹œì§€ ì—­í• : user, assistant, system")
    content: str = Field(..., description="ë©”ì‹œì§€ ë‚´ìš©")


class ChatRequest(BaseModel):
    """ì±„íŒ… ìš”ì²­"""

    messages: List[ChatMessage] = Field(..., description="ëŒ€í™” ì´ë ¥")
    llm_provider: LLMProvider = Field(default=LLMProvider.OPENAI)
    model: Optional[str] = Field(default=None, description="ì‚¬ìš©í•  ëª¨ë¸")
    stream: bool = Field(default=False, description="ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì—¬ë¶€")
    context_url: Optional[str] = Field(default=None, description="ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•  URL")


class ChatResponse(BaseModel):
    """ì±„íŒ… ì‘ë‹µ"""

    message: str
    provider: str
    model: str
    tokens_used: Optional[int] = None


# ========================================
# In-Memory Storage (deprecated - kept for backward compat, StateStore is primary)
# ========================================

# Note: task_results dict is now managed by StateStore with Redis persistence
# This variable is kept for quick reference but StateStore should be used
session_semaphore: Optional[asyncio.Semaphore] = None


# ========================================
# Provider Config Fetching (DB-based)
# ========================================

_provider_config_cache: Dict[str, dict] = {}
_provider_cache_ttl: Dict[str, datetime] = {}
PROVIDER_CACHE_TTL_SECONDS = 300


async def fetch_provider_config(provider_name: str) -> Optional[Dict[str, Any]]:
    """DBì—ì„œ LLM provider ì„¤ì • ê°€ì ¸ì˜¤ê¸° (ìºì‹œ ì ìš©)"""
    cache_key = provider_name.lower()
    now = datetime.now(timezone.utc)

    if cache_key in _provider_config_cache:
        cache_time = _provider_cache_ttl.get(cache_key)
        if cache_time and (now - cache_time).total_seconds() < PROVIDER_CACHE_TTL_SECONDS:
            return _provider_config_cache[cache_key]

    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(
                f"{api_config.COLLECTOR_SERVICE_URL}/api/v1/llm-providers/config/{provider_name}"
            )
            if response.status_code == 200:
                provider_config = response.json()
                _provider_config_cache[cache_key] = provider_config
                _provider_cache_ttl[cache_key] = now
                return provider_config
    except Exception as e:
        logger.warning("Failed to fetch provider config", provider=provider_name, error=str(e))

    return None


def clear_provider_cache():
    """Provider ìºì‹œ ì‚­ì œ"""
    _provider_config_cache.clear()
    _provider_cache_ttl.clear()


# ========================================
# LLM Factory
# ========================================


def get_llm_from_env(provider: LLMProvider, model: Optional[str] = None):
    """í™˜ê²½ë³€ìˆ˜/Settings ê¸°ë°˜ìœ¼ë¡œ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

    Note: SettingsëŠ” Consulì—ì„œ ë¡œë“œëœ ì„¤ì •ì„ í¬í•¨í•©ë‹ˆë‹¤.
    """
    from langchain_openai import ChatOpenAI
    from langchain_anthropic import ChatAnthropic
    from langchain_google_genai import ChatGoogleGenerativeAI

    # Get settings (includes Consul-loaded config)
    settings = get_settings()

    if provider == LLMProvider.OPENAI:
        return ChatOpenAI(
            model=model or settings.llm.openai_model or "gpt-4o",
            api_key=settings.llm.openai_api_key or api_config.OPENAI_API_KEY,
            temperature=0.1,
        )
    elif provider == LLMProvider.ANTHROPIC:
        return ChatAnthropic(
            model=model or settings.llm.anthropic_model or "claude-3-5-sonnet-20241022",
            api_key=settings.llm.anthropic_api_key or api_config.ANTHROPIC_API_KEY,
            temperature=0.1,
        )
    elif provider == LLMProvider.GOOGLE:
        return ChatGoogleGenerativeAI(
            model=model or "gemini-1.5-pro",
            google_api_key=api_config.GOOGLE_API_KEY,
            temperature=0.1,
        )
    elif provider == LLMProvider.OPENROUTER:
        return ChatOpenAI(
            model=model or settings.llm.openrouter_model or "openai/gpt-4o",
            api_key=settings.llm.openrouter_api_key,
            base_url=settings.llm.openrouter_base_url or "https://openrouter.ai/api/v1",
            temperature=0.1,
        )
    elif provider == LLMProvider.OLLAMA:
        try:
            from langchain_ollama import ChatOllama

            return ChatOllama(
                model=model or settings.llm.ollama_model or "llama3.1",
                base_url=settings.llm.ollama_base_url or "http://localhost:11434",
                temperature=0.1,
            )
        except ImportError:
            raise ValueError("Ollama support requires langchain-ollama package")
    elif provider == LLMProvider.AZURE:
        try:
            from langchain_openai import AzureChatOpenAI

            azure_endpoint = settings.llm.azure_endpoint or os.getenv("AZURE_OPENAI_ENDPOINT", "")
            azure_api_key = settings.llm.azure_api_key or os.getenv("AZURE_OPENAI_API_KEY", "")
            azure_deployment = (
                model
                or settings.llm.azure_deployment_name
                or os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME", "gpt-4o")
            )
            azure_api_version = settings.llm.azure_api_version or os.getenv(
                "AZURE_OPENAI_API_VERSION", "2024-02-15-preview"
            )

            if not azure_endpoint:
                raise ValueError(
                    "Azure OpenAI requires LLM_AZURE_ENDPOINT or AZURE_OPENAI_ENDPOINT"
                )
            if not azure_api_key:
                raise ValueError("Azure OpenAI requires LLM_AZURE_API_KEY or AZURE_OPENAI_API_KEY")

            return AzureChatOpenAI(
                azure_endpoint=azure_endpoint,
                azure_deployment=azure_deployment,
                api_key=azure_api_key,
                api_version=azure_api_version,
                temperature=0.1,
            )
        except ImportError:
            raise ValueError("Azure OpenAI support requires langchain-openai package")
    elif provider == LLMProvider.CUSTOM:
        custom_base_url = settings.llm.custom_base_url or os.getenv("CUSTOM_LLM_BASE_URL", "")
        if not custom_base_url:
            raise ValueError("Custom provider requires CUSTOM_LLM_BASE_URL or LLM_CUSTOM_BASE_URL")

        custom_request_format = settings.llm.custom_request_format
        custom_response_path = settings.llm.custom_response_path

        # If custom request format is provided, use CustomRESTAPIClient for non-OpenAI-compatible APIs
        if custom_request_format:
            from src.crawler.agent import CustomRESTAPIClient

            return CustomRESTAPIClient(
                base_url=custom_base_url,
                api_key=settings.llm.custom_api_key or os.getenv("CUSTOM_LLM_API_KEY", ""),
                model=model or settings.llm.custom_model or "default",
                request_format=custom_request_format,
                response_path=custom_response_path or "reply",
                custom_headers=settings.llm.custom_headers or "{}",
                temperature=0.1,
            )

        # Fallback to OpenAI-compatible format
        return ChatOpenAI(
            model=model or settings.llm.custom_model or "default",
            api_key=settings.llm.custom_api_key or os.getenv("CUSTOM_LLM_API_KEY", "none"),
            base_url=custom_base_url,
            temperature=0.1,
        )
    else:
        raise ValueError(f"Unsupported LLM provider: {provider}")


def get_llm_from_config(provider_config: Dict[str, Any], model_override: Optional[str] = None):
    """DB ì„¤ì • ê¸°ë°˜ìœ¼ë¡œ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    from langchain_openai import ChatOpenAI
    from langchain_anthropic import ChatAnthropic
    from langchain_google_genai import ChatGoogleGenerativeAI

    provider_type = provider_config.get("providerType", "").upper()
    api_key = provider_config.get("apiKey", "")
    base_url = provider_config.get("baseUrl")
    default_model = model_override or provider_config.get("defaultModel", "")
    extra_config = provider_config.get("config", {})
    temperature = extra_config.get("temperature", 0.1)

    if provider_type == "OPENAI":
        kwargs = {
            "model": default_model or "gpt-4o",
            "api_key": api_key,
            "temperature": temperature,
        }
        if base_url and base_url != "https://api.openai.com/v1":
            kwargs["base_url"] = base_url
        return ChatOpenAI(**kwargs)

    elif provider_type == "ANTHROPIC":
        return ChatAnthropic(
            model=default_model or "claude-3-5-sonnet-20241022",
            api_key=api_key,
            temperature=temperature,
        )

    elif provider_type == "GOOGLE":
        return ChatGoogleGenerativeAI(
            model=default_model or "gemini-1.5-pro",
            google_api_key=api_key,
            temperature=temperature,
        )

    elif provider_type == "OPENROUTER":
        return ChatOpenAI(
            model=default_model or "openai/gpt-4o",
            api_key=api_key,
            base_url=base_url or "https://openrouter.ai/api/v1",
            temperature=temperature,
        )

    elif provider_type == "OLLAMA":
        try:
            from langchain_ollama import ChatOllama

            return ChatOllama(
                model=default_model or "llama3.1",
                base_url=base_url or "http://localhost:11434",
                temperature=temperature,
            )
        except ImportError:
            raise ValueError("Ollama support requires langchain-ollama package")

    elif provider_type == "AZURE":
        try:
            from langchain_openai import AzureChatOpenAI

            azure_endpoint = extra_config.get("endpoint") or base_url
            deployment_name = extra_config.get("deployment_name") or default_model
            api_version = extra_config.get("api_version", "2024-02-15-preview")

            if not azure_endpoint:
                raise ValueError("Azure provider requires endpoint")
            if not deployment_name:
                raise ValueError("Azure provider requires deployment_name")

            return AzureChatOpenAI(
                azure_endpoint=azure_endpoint,
                azure_deployment=deployment_name,
                api_key=api_key,
                api_version=api_version,
                temperature=temperature,
            )
        except ImportError:
            raise ValueError("Azure OpenAI support requires langchain-openai package")

    elif provider_type == "CUSTOM":
        if not base_url:
            raise ValueError("Custom provider requires base_url")
        return ChatOpenAI(
            model=default_model or "default",
            api_key=api_key or "none",
            base_url=base_url,
            temperature=temperature,
        )

    else:
        raise ValueError(f"Unsupported provider type: {provider_type}")


async def get_llm(provider: LLMProvider, model: Optional[str] = None):
    """LLM ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸° - DB ìš°ì„ , í™˜ê²½ë³€ìˆ˜ fallback"""
    provider_name = provider.value

    if api_config.USE_DB_PROVIDERS:
        provider_config = await fetch_provider_config(provider_name)
        if provider_config:
            try:
                return get_llm_from_config(provider_config, model)
            except Exception as e:
                logger.warning(
                    "DB provider failed, using fallback",
                    provider=provider_name,
                    error=str(e),
                )

    return get_llm_from_env(provider, model)


# ========================================
# Core Agent Logic
# ========================================


async def run_browser_agent(
    request: AgentTask,
    settings: Settings,
    sse_manager: SSEManager,
) -> AgentTaskResult:
    """
    browser-use ì—ì´ì „íŠ¸ ì‹¤í–‰.

    ê¸°ì¡´ autonomous-crawler-serviceì˜ AutonomousCrawlerAgentë¥¼ í™œìš©í•˜ë©´ì„œ
    browser-agentì˜ API ì¸í„°í˜ì´ìŠ¤ì™€ SSE ì´ë²¤íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    """
    global session_semaphore

    task_id = str(uuid.uuid4())
    start_time = datetime.now(timezone.utc)
    actions_log: List[AgentAction] = []

    logger.info(
        "Agent task started",
        task_id=task_id,
        url=str(request.url),
        task=request.task[:100],
    )

    # SSE: ì—ì´ì „íŠ¸ ì‹œì‘ ì´ë²¤íŠ¸
    await sse_manager.send_agent_event(
        SSEEventType.AGENT_START,
        task_id=task_id,
        url=str(request.url),
        message=f"í¬ë¡¤ë§ ì‹œì‘: {request.task[:50]}...",
        provider=request.llm_provider.value,
    )

    try:
        if session_semaphore is None:
            session_semaphore = asyncio.Semaphore(api_config.MAX_CONCURRENT_SESSIONS)

        async with session_semaphore:
            # AutonomousCrawlerAgent ì‚¬ìš©
            agent = AutonomousCrawlerAgent(settings)

            # ë‹¨ìˆœ URL í¬ë¡¤ë§ + AI ë¶„ì„
            # smart_search ë©”ì„œë“œ í™œìš©
            crawl_result = await agent.crawl_with_camoufox(
                url=str(request.url),
                extract_content=True,
                wait_for_cloudflare=True,
            )

            if crawl_result.get("error"):
                raise Exception(crawl_result["error"])

            extracted_text = crawl_result.get("text", "")
            page_title = crawl_result.get("title", "")
            extracted_links: List[ExtractedLink] = []

            # ë§í¬ ì¶”ì¶œ (ì˜µì…˜)
            if request.extract_links:
                links = crawl_result.get("links", [])
                extracted_links = [
                    ExtractedLink(url=link.get("url", ""), text=link.get("text", ""))
                    for link in links[:50]
                    if link.get("url", "").startswith("http")
                ]

            content_hash = (
                hashlib.sha256((extracted_text or "").encode()).hexdigest()
                if extracted_text
                else None
            )

            duration_ms = int((datetime.now(timezone.utc) - start_time).total_seconds() * 1000)

            task_result = AgentTaskResult(
                task_id=task_id,
                url=str(request.url),
                status="success",
                extracted_text=extracted_text,
                extracted_links=extracted_links,
                content_hash=content_hash,
                page_title=page_title,
                steps_taken=1,
                actions=actions_log,
                duration_ms=duration_ms,
            )

            # ë¸Œë¼ìš°ì € ì •ë¦¬
            await agent.close()

            logger.info(
                "Agent task completed",
                task_id=task_id,
                duration_ms=duration_ms,
                links_count=len(extracted_links),
            )

            # SSE: ì—ì´ì „íŠ¸ ì™„ë£Œ ì´ë²¤íŠ¸
            await sse_manager.send_agent_event(
                SSEEventType.AGENT_COMPLETE,
                task_id=task_id,
                url=str(request.url),
                message=f"í¬ë¡¤ë§ ì™„ë£Œ: {len(extracted_links)}ê°œ ë§í¬ ë°œê²¬",
                duration_ms=duration_ms,
                links_count=len(extracted_links),
                has_content=bool(extracted_text),
            )

            return task_result

    except asyncio.TimeoutError:
        duration_ms = int((datetime.now(timezone.utc) - start_time).total_seconds() * 1000)
        logger.error("Agent task timeout", task_id=task_id, timeout=request.timeout_sec)

        await sse_manager.send_agent_event(
            SSEEventType.AGENT_ERROR,
            task_id=task_id,
            url=str(request.url),
            message=f"íƒ€ì„ì•„ì›ƒ: {request.timeout_sec}ì´ˆ ì´ˆê³¼",
            error_type="timeout",
        )

        return AgentTaskResult(
            task_id=task_id,
            url=str(request.url),
            status="timeout",
            error_message=f"íƒœìŠ¤í¬ê°€ {request.timeout_sec}ì´ˆ ë‚´ì— ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            duration_ms=duration_ms,
        )

    except Exception as e:
        duration_ms = int((datetime.now(timezone.utc) - start_time).total_seconds() * 1000)
        logger.error("Agent task failed", task_id=task_id, error=str(e))

        await sse_manager.send_agent_event(
            SSEEventType.AGENT_ERROR,
            task_id=task_id,
            url=str(request.url),
            message=f"ì—ëŸ¬: {str(e)[:100]}",
            error_type="exception",
            error_detail=str(e),
        )

        return AgentTaskResult(
            task_id=task_id,
            url=str(request.url),
            status="failed",
            error_message=str(e),
            duration_ms=duration_ms,
        )


# ========================================
# FastAPI App
# ========================================


@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ìˆ˜ëª… ì£¼ê¸° ê´€ë¦¬"""
    logger.info("Starting autonomous-crawler REST API server")

    # Initialize StateStore (connects to Redis)
    state_store = await get_state_store()
    app.state.state_store = state_store
    logger.info(
        "StateStore initialized",
        using_redis=state_store.is_redis_connected,
        task_count=state_store.task_count,
    )

    # Initialize ML Orchestrator
    try:
        ml_orchestrator = await init_ml_orchestrator()
        app.state.ml_orchestrator = ml_orchestrator
        logger.info("ML Orchestrator initialized")
    except Exception as e:
        logger.warning("ML Orchestrator initialization failed (non-critical)", error=str(e))
        app.state.ml_orchestrator = None

    yield

    # Cleanup ML Orchestrator
    if hasattr(app.state, "ml_orchestrator") and app.state.ml_orchestrator:
        await app.state.ml_orchestrator.close()
        logger.info("ML Orchestrator closed")

    # Cleanup StateStore
    logger.info("Shutting down autonomous-crawler REST API server")
    await close_state_store()


def create_app(settings: Settings | None = None) -> FastAPI:
    """FastAPI ì•± ìƒì„±"""
    if settings is None:
        settings = get_settings()

    app = FastAPI(
        title="Autonomous Crawler Service API",
        description="AI ê¸°ë°˜ ììœ¨ ë¸Œë¼ìš°ì € í¬ë¡¤ëŸ¬ - browser-agent í†µí•© REST API",
        version="0.2.0",
        lifespan=lifespan,
    )

    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # Store settings in app state
    app.state.settings = settings
    app.state.sse_manager = get_sse_manager()

    # Register routes
    register_routes(app)

    return app


def register_routes(app: FastAPI):
    """API ë¼ìš°íŠ¸ ë“±ë¡"""

    # MCP Add-on Router ë“±ë¡
    app.include_router(mcp_router)

    # ML Analysis Router ë“±ë¡
    app.include_router(ml_router)

    @app.get("/health")
    @app.head("/health")
    async def health(req: Request):
        """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
        state_store: StateStore = req.app.state.state_store
        store_stats = await state_store.get_stats()

        # Check ML orchestrator status
        ml_available = (
            hasattr(req.app.state, "ml_orchestrator") and req.app.state.ml_orchestrator is not None
        )

        return {
            "status": "ok",
            "service": "autonomous-crawler-service",
            "api_version": "0.2.0",
            "features": {
                "rest_api": True,
                "sse_events": True,
                "kafka_consumer": True,
                "camoufox": True,
                "captcha_bypass": True,
                "redis_persistence": state_store.is_redis_connected,
                "ml_analysis": ml_available,
            },
            "storage": store_stats,
            "active_sessions": state_store.task_count,
            "max_sessions": api_config.MAX_CONCURRENT_SESSIONS,
        }

    @app.get("/events")
    async def sse_events(request: Request):
        """
        SSE ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ ì—”ë“œí¬ì¸íŠ¸.

        ë¸Œë¼ìš°ì € ì—ì´ì „íŠ¸ì˜ ì‹¤ì‹œê°„ ìƒíƒœë¥¼ êµ¬ë…í•©ë‹ˆë‹¤.
        """
        sse_manager: SSEManager = request.app.state.sse_manager
        client_id, queue = await sse_manager.connect()

        return StreamingResponse(
            sse_event_generator(client_id, queue, sse_manager),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",
                "Access-Control-Allow-Origin": "*",
            },
        )

    @app.get("/events/clients")
    async def get_sse_clients(request: Request):
        """í˜„ì¬ ì—°ê²°ëœ SSE í´ë¼ì´ì–¸íŠ¸ ìˆ˜ ì¡°íšŒ"""
        sse_manager: SSEManager = request.app.state.sse_manager
        return {
            "connected_clients": sse_manager.client_count,
            "client_ids": sse_manager.client_ids,
        }

    @app.get("/providers")
    async def get_available_providers(user: JWTPayload = Depends(require_auth())):
        """ì‚¬ìš© ê°€ëŠ¥í•œ LLM Provider ëª©ë¡ ì¡°íšŒ (ì¸ì¦ í•„ìš” - API í‚¤ ì •ë³´ ë³´í˜¸)"""
        providers = []
        settings = get_settings()

        if settings.llm.openai_api_key or api_config.OPENAI_API_KEY:
            providers.append(
                {
                    "name": "openai",
                    "providerType": "OPENAI",
                    "defaultModel": settings.llm.openai_model or "gpt-4o",
                    "available": True,
                }
            )

        if settings.llm.anthropic_api_key or api_config.ANTHROPIC_API_KEY:
            providers.append(
                {
                    "name": "anthropic",
                    "providerType": "ANTHROPIC",
                    "defaultModel": settings.llm.anthropic_model or "claude-3-5-sonnet-20241022",
                    "available": True,
                }
            )

        if api_config.GOOGLE_API_KEY:
            providers.append(
                {
                    "name": "google",
                    "providerType": "GOOGLE",
                    "defaultModel": "gemini-1.5-pro",
                    "available": True,
                }
            )

        # OpenRouter (Settings ìš°ì„ )
        if settings.llm.openrouter_api_key or os.getenv("OPENROUTER_API_KEY"):
            providers.append(
                {
                    "name": "openrouter",
                    "providerType": "OPENROUTER",
                    "defaultModel": settings.llm.openrouter_model or "openai/gpt-4o",
                    "available": True,
                }
            )

        # Ollama (í•­ìƒ í‘œì‹œ, ë¡œì»¬ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥)
        providers.append(
            {
                "name": "ollama",
                "providerType": "OLLAMA",
                "defaultModel": settings.llm.ollama_model or "llama3.1",
                "available": True,
                "local": True,
            }
        )

        # Azure OpenAI (Settings ìš°ì„ )
        if (settings.llm.azure_api_key and settings.llm.azure_endpoint) or (
            os.getenv("AZURE_OPENAI_API_KEY") and os.getenv("AZURE_OPENAI_ENDPOINT")
        ):
            providers.append(
                {
                    "name": "azure",
                    "providerType": "AZURE",
                    "defaultModel": settings.llm.azure_deployment_name
                    or os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME", "gpt-4o"),
                    "available": True,
                }
            )

        # Custom (Settings ìš°ì„ )
        if settings.llm.custom_base_url or os.getenv("CUSTOM_LLM_BASE_URL"):
            providers.append(
                {
                    "name": "custom",
                    "providerType": "CUSTOM",
                    "defaultModel": settings.llm.custom_model or "default",
                    "available": True,
                }
            )

        return {
            "providers": providers,
            "defaultProvider": providers[0] if providers else None,
            "source": "environment",
            "supportedProviders": [p.value for p in LLMProvider],
        }

    @app.post("/providers/cache/clear")
    async def clear_providers_cache_endpoint():
        """Provider ì„¤ì • ìºì‹œ ì‚­ì œ"""
        clear_provider_cache()
        return {"status": "ok", "message": "Provider cache cleared"}

    @app.post("/providers/test")
    async def test_provider_connection(
        provider: LLMProvider = Query(..., description="í…ŒìŠ¤íŠ¸í•  Provider"),
        model: Optional[str] = Query(default=None, description="í…ŒìŠ¤íŠ¸í•  ëª¨ë¸"),
    ):
        """
        LLM Provider ì—°ê²° í…ŒìŠ¤íŠ¸.

        ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ë³´ë‚´ Providerê°€ ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        """
        import time

        start_time = time.time()

        try:
            llm = await get_llm(provider, model)

            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
            from langchain_core.messages import HumanMessage

            test_message = HumanMessage(content="Say 'Connection successful!' in one line.")
            response = await llm.ainvoke([test_message])

            elapsed_ms = int((time.time() - start_time) * 1000)

            return {
                "status": "success",
                "provider": provider.value,
                "model": model or "default",
                "response": response.content[:100] if response.content else "No response",
                "latency_ms": elapsed_ms,
                "message": "ì—°ê²° ì„±ê³µ",
            }

        except Exception as e:
            elapsed_ms = int((time.time() - start_time) * 1000)
            logger.error("Provider test failed", provider=provider.value, error=str(e))
            return {
                "status": "failed",
                "provider": provider.value,
                "model": model or "default",
                "error": str(e),
                "latency_ms": elapsed_ms,
                "message": f"ì—°ê²° ì‹¤íŒ¨: {str(e)[:100]}",
            }

    @app.get("/providers/{provider}/models")
    async def get_provider_models(
        provider: LLMProvider,
        api_key: Optional[str] = Query(
            default=None, description="API í‚¤ (ì˜µì…˜, ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)"
        ),
        base_url: Optional[str] = Query(default=None, description="Base URL (Ollama, Customìš©)"),
    ):
        """
        íŠ¹ì • LLM Providerì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ì„ ë™ì ìœ¼ë¡œ ì¡°íšŒí•©ë‹ˆë‹¤.

        - OpenAI: /v1/models API í˜¸ì¶œ
        - OpenRouter: /api/v1/models API í˜¸ì¶œ
        - Ollama: /api/tags API í˜¸ì¶œ
        - Anthropic, Google, Azure: ì •ì  ëª©ë¡ ë°˜í™˜ (ê³µì‹ API ì—†ìŒ)
        - Custom: /v1/models ë˜ëŠ” ì •ì  ëª©ë¡
        """
        try:
            async with httpx.AsyncClient(timeout=15.0) as client:
                if provider == LLMProvider.OPENAI:
                    key = api_key or os.getenv("OPENAI_API_KEY", "")
                    if not key:
                        return {
                            "provider": provider.value,
                            "models": _get_static_models("openai"),
                            "source": "static",
                            "message": "API í‚¤ê°€ ì—†ì–´ ì •ì  ëª©ë¡ ë°˜í™˜",
                        }

                    resp = await client.get(
                        "https://api.openai.com/v1/models",
                        headers={"Authorization": f"Bearer {key}"},
                    )
                    if resp.status_code == 200:
                        data = resp.json()
                        # GPT ëª¨ë¸ë§Œ í•„í„°ë§ (chat ëª¨ë¸)
                        models = [
                            {"id": m["id"], "name": m["id"], "owned_by": m.get("owned_by", "")}
                            for m in data.get("data", [])
                            if any(prefix in m["id"] for prefix in ["gpt-", "o1-", "chatgpt-"])
                        ]
                        # ì •ë ¬: gpt-4o ìš°ì„ 
                        models.sort(key=lambda x: (0 if "gpt-4o" in x["id"] else 1, x["id"]))
                        return {
                            "provider": provider.value,
                            "models": models[:20],  # ìµœëŒ€ 20ê°œ
                            "source": "api",
                            "total": len(models),
                        }
                    else:
                        return {
                            "provider": provider.value,
                            "models": _get_static_models("openai"),
                            "source": "static",
                            "error": f"API í˜¸ì¶œ ì‹¤íŒ¨: {resp.status_code}",
                        }

                elif provider == LLMProvider.OPENROUTER:
                    key = api_key or os.getenv("OPENROUTER_API_KEY", "")
                    headers = {}
                    if key:
                        headers["Authorization"] = f"Bearer {key}"

                    resp = await client.get(
                        "https://openrouter.ai/api/v1/models",
                        headers=headers,
                    )
                    if resp.status_code == 200:
                        data = resp.json()
                        all_models = data.get("data", [])

                        # ì£¼ìš” ëª¨ë¸ ì œê³µì ìš°ì„ ìˆœìœ„
                        priority_providers = [
                            "openai",
                            "anthropic",
                            "google",
                            "meta-llama",
                            "mistralai",
                            "deepseek",
                            "qwen",
                            "cohere",
                        ]

                        def get_provider_priority(model_id: str) -> int:
                            for i, provider in enumerate(priority_providers):
                                if model_id.startswith(provider):
                                    return i
                            return len(priority_providers)

                        # ì¸ê¸° ëª¨ë¸ ì •ë ¬ (ì œê³µì ìš°ì„ ìˆœìœ„ â†’ ì´ë¦„ìˆœ)
                        sorted_models = sorted(
                            all_models,
                            key=lambda m: (
                                get_provider_priority(m["id"]),
                                # ë¬´ë£Œ ëª¨ë¸ì€ í›„ìˆœìœ„
                                0 if m.get("pricing", {}).get("prompt", "0") != "0" else 1,
                                m["id"],
                            ),
                        )

                        models = [
                            {
                                "id": m["id"],
                                "name": m.get("name", m["id"]),
                                "context_length": m.get("context_length"),
                                "pricing": m.get("pricing"),
                            }
                            for m in sorted_models[:100]  # ìƒìœ„ 100ê°œ
                        ]
                        return {
                            "provider": provider.value,
                            "models": models,
                            "source": "api",
                            "total": len(all_models),
                        }
                    else:
                        return {
                            "provider": provider.value,
                            "models": _get_static_models("openrouter"),
                            "source": "static",
                            "error": f"API í˜¸ì¶œ ì‹¤íŒ¨: {resp.status_code}",
                        }

                elif provider == LLMProvider.OLLAMA:
                    ollama_url = base_url or os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
                    try:
                        resp = await client.get(f"{ollama_url}/api/tags")
                        if resp.status_code == 200:
                            data = resp.json()
                            models = [
                                {
                                    "id": m["name"],
                                    "name": m["name"],
                                    "size": m.get("size"),
                                    "modified_at": m.get("modified_at"),
                                }
                                for m in data.get("models", [])
                            ]
                            return {
                                "provider": provider.value,
                                "models": models,
                                "source": "api",
                                "ollama_url": ollama_url,
                            }
                        else:
                            return {
                                "provider": provider.value,
                                "models": _get_static_models("ollama"),
                                "source": "static",
                                "error": f"Ollama ì—°ê²° ì‹¤íŒ¨: {resp.status_code}",
                            }
                    except httpx.ConnectError:
                        return {
                            "provider": provider.value,
                            "models": _get_static_models("ollama"),
                            "source": "static",
                            "error": f"Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŒ: {ollama_url}",
                        }

                elif provider == LLMProvider.ANTHROPIC:
                    # Anthropicì€ ê³µì‹ ëª¨ë¸ ëª©ë¡ APIê°€ ì—†ìŒ
                    return {
                        "provider": provider.value,
                        "models": _get_static_models("anthropic"),
                        "source": "static",
                        "message": "Anthropicì€ ëª¨ë¸ ëª©ë¡ APIë¥¼ ì œê³µí•˜ì§€ ì•ŠìŒ",
                    }

                elif provider == LLMProvider.GOOGLE:
                    # Google AI - Generative Language APIë¡œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
                    key = api_key or os.getenv("GOOGLE_API_KEY", "")
                    if not key:
                        return {
                            "provider": provider.value,
                            "models": _get_static_models("google"),
                            "source": "static",
                            "message": "API í‚¤ê°€ ì—†ì–´ ì •ì  ëª©ë¡ ë°˜í™˜",
                        }

                    try:
                        resp = await client.get(
                            f"https://generativelanguage.googleapis.com/v1beta/models?key={key}",
                        )
                        if resp.status_code == 200:
                            data = resp.json()
                            # gemini ëª¨ë¸ë§Œ í•„í„°ë§ (generateContent ì§€ì› ëª¨ë¸)
                            models = [
                                {
                                    "id": m["name"].replace("models/", ""),
                                    "name": m.get("displayName", m["name"].replace("models/", "")),
                                    "context_length": m.get("inputTokenLimit"),
                                    "description": m.get("description", ""),
                                }
                                for m in data.get("models", [])
                                if "generateContent" in m.get("supportedGenerationMethods", [])
                                and "gemini" in m.get("name", "").lower()
                            ]
                            # ìµœì‹  ëª¨ë¸ ìš°ì„  ì •ë ¬
                            models.sort(
                                key=lambda x: (
                                    0
                                    if "2.5" in x["id"]
                                    else (
                                        1 if "2.0" in x["id"] else (2 if "1.5" in x["id"] else 3)
                                    ),
                                    0 if "pro" in x["id"].lower() else 1,
                                    x["id"],
                                )
                            )
                            return {
                                "provider": provider.value,
                                "models": models[:15],
                                "source": "api",
                                "total": len(models),
                            }
                        else:
                            return {
                                "provider": provider.value,
                                "models": _get_static_models("google"),
                                "source": "static",
                                "error": f"API í˜¸ì¶œ ì‹¤íŒ¨: {resp.status_code}",
                            }
                    except Exception as e:
                        return {
                            "provider": provider.value,
                            "models": _get_static_models("google"),
                            "source": "static",
                            "error": f"Google AI API ì˜¤ë¥˜: {str(e)[:50]}",
                        }

                elif provider == LLMProvider.AZURE:
                    # AzureëŠ” ë°°í¬ ê¸°ë°˜ì´ë¼ ë™ì  ì¡°íšŒ ë¶ˆê°€
                    return {
                        "provider": provider.value,
                        "models": _get_static_models("azure"),
                        "source": "static",
                        "message": "Azure OpenAIëŠ” ë°°í¬ ê¸°ë°˜ìœ¼ë¡œ ë™ì  ì¡°íšŒ ë¶ˆê°€",
                    }

                elif provider == LLMProvider.CUSTOM:
                    custom_url = base_url or os.getenv("CUSTOM_LLM_BASE_URL", "")
                    if custom_url:
                        try:
                            resp = await client.get(f"{custom_url}/v1/models")
                            if resp.status_code == 200:
                                data = resp.json()
                                models = [
                                    {"id": m["id"], "name": m.get("id", "")}
                                    for m in data.get("data", [])
                                ]
                                return {
                                    "provider": provider.value,
                                    "models": models,
                                    "source": "api",
                                    "base_url": custom_url,
                                }
                        except Exception:
                            pass

                    return {
                        "provider": provider.value,
                        "models": [{"id": "default", "name": "ê¸°ë³¸ ëª¨ë¸"}],
                        "source": "static",
                    }

                else:
                    return {
                        "provider": provider.value,
                        "models": [],
                        "source": "unknown",
                        "error": "ì•Œ ìˆ˜ ì—†ëŠ” Provider",
                    }

        except Exception as e:
            logger.error("Failed to fetch models", provider=provider.value, error=str(e))
            return {
                "provider": provider.value,
                "models": _get_static_models(provider.value),
                "source": "static",
                "error": str(e),
            }

    def _get_static_models(provider: str) -> List[Dict[str, Any]]:
        """ì •ì  ëª¨ë¸ ëª©ë¡ ë°˜í™˜ (fallback)"""
        static_models = {
            "openai": [
                {"id": "gpt-4o", "name": "GPT-4o (ì¶”ì²œ)"},
                {"id": "gpt-4o-mini", "name": "GPT-4o Mini (ë¹ ë¦„)"},
                {"id": "gpt-4.1", "name": "GPT-4.1"},
                {"id": "gpt-4.1-mini", "name": "GPT-4.1 Mini"},
                {"id": "gpt-4-turbo", "name": "GPT-4 Turbo"},
                {"id": "gpt-3.5-turbo", "name": "GPT-3.5 Turbo (ì €ë ´)"},
                {"id": "o1", "name": "o1 (ì¶”ë¡ )"},
                {"id": "o1-preview", "name": "o1-preview (ì¶”ë¡ )"},
                {"id": "o1-mini", "name": "o1-mini (ì¶”ë¡ , ë¹ ë¦„)"},
                {"id": "o3-mini", "name": "o3-mini (ìµœì‹  ì¶”ë¡ )"},
            ],
            "anthropic": [
                {"id": "claude-sonnet-4-20250514", "name": "Claude Sonnet 4 (ìµœì‹ )"},
                {"id": "claude-3-5-sonnet-20241022", "name": "Claude 3.5 Sonnet (ì¶”ì²œ)"},
                {"id": "claude-3-5-haiku-20241022", "name": "Claude 3.5 Haiku (ë¹ ë¦„)"},
                {"id": "claude-3-opus-20240229", "name": "Claude 3 Opus (ê°•ë ¥)"},
            ],
            "google": [
                {"id": "gemini-2.5-flash-preview-05-20", "name": "Gemini 2.5 Flash (ìµœì‹ )"},
                {"id": "gemini-2.5-pro-preview-05-06", "name": "Gemini 2.5 Pro (ìµœì‹ )"},
                {"id": "gemini-2.0-flash", "name": "Gemini 2.0 Flash"},
                {"id": "gemini-2.0-flash-lite", "name": "Gemini 2.0 Flash Lite (ë¹ ë¦„)"},
                {"id": "gemini-1.5-pro", "name": "Gemini 1.5 Pro"},
                {"id": "gemini-1.5-flash", "name": "Gemini 1.5 Flash"},
            ],
            "openrouter": [
                {"id": "openai/gpt-4o", "name": "GPT-4o (OpenAI)"},
                {"id": "openai/gpt-4.1", "name": "GPT-4.1 (OpenAI)"},
                {"id": "anthropic/claude-sonnet-4", "name": "Claude Sonnet 4 (Anthropic)"},
                {"id": "anthropic/claude-3.5-sonnet", "name": "Claude 3.5 Sonnet"},
                {"id": "google/gemini-2.5-flash-preview", "name": "Gemini 2.5 Flash (Google)"},
                {"id": "google/gemini-2.0-flash-001", "name": "Gemini 2.0 Flash (Google)"},
                {"id": "google/gemini-pro-1.5", "name": "Gemini 1.5 Pro"},
                {"id": "meta-llama/llama-3.3-70b-instruct", "name": "Llama 3.3 70B"},
                {"id": "meta-llama/llama-3.1-405b-instruct", "name": "Llama 3.1 405B"},
                {"id": "deepseek/deepseek-r1", "name": "DeepSeek R1 (ì¶”ë¡ )"},
                {"id": "deepseek/deepseek-chat", "name": "DeepSeek Chat"},
                {"id": "qwen/qwen-2.5-72b-instruct", "name": "Qwen 2.5 72B"},
            ],
            "ollama": [
                {"id": "llama3.3", "name": "Llama 3.3 (ìµœì‹ )"},
                {"id": "llama3.2", "name": "Llama 3.2"},
                {"id": "llama3.1", "name": "Llama 3.1"},
                {"id": "llama3.1:70b", "name": "Llama 3.1 70B"},
                {"id": "mistral", "name": "Mistral"},
                {"id": "mixtral", "name": "Mixtral"},
                {"id": "codellama", "name": "Code Llama"},
                {"id": "qwen2.5", "name": "Qwen 2.5"},
                {"id": "deepseek-r1", "name": "DeepSeek R1"},
                {"id": "gemma2", "name": "Gemma 2"},
            ],
            "azure": [
                {"id": "gpt-4o", "name": "GPT-4o"},
                {"id": "gpt-4-turbo", "name": "GPT-4 Turbo"},
                {"id": "gpt-35-turbo", "name": "GPT-3.5 Turbo"},
            ],
            "custom": [
                {"id": "default", "name": "ê¸°ë³¸ ëª¨ë¸"},
            ],
        }
        return static_models.get(provider, [])

    @app.post("/agent/crawl", response_model=AgentTaskResult)
    async def agent_crawl(
        request: AgentTask,
        req: Request,
        user: JWTPayload = Depends(require_auth()),
    ):
        """
        AI ì—ì´ì „íŠ¸ í¬ë¡¤ë§ ì‹¤í–‰ (ë™ê¸°).

        CAPTCHA ìš°íšŒ ë° ìŠ¤í…”ìŠ¤ ëª¨ë“œ ì§€ì›.
        ì¸ì¦ í•„ìš”: Bearer í† í° í•„ìˆ˜
        """
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store

        logger.info("Agent crawl requested", user=user.username, url=str(request.url))
        result = await run_browser_agent(request, settings, sse_manager)
        await state_store.save_task(result.task_id, result)
        return result

    @app.post("/agent/crawl/async")
    async def agent_crawl_async(
        request: AgentTask,
        req: Request,
        background_tasks: BackgroundTasks,
        user: JWTPayload = Depends(require_auth()),
    ):
        """AI ì—ì´ì „íŠ¸ í¬ë¡¤ë§ ë¹„ë™ê¸° ì‹¤í–‰ (ì¸ì¦ í•„ìš”)"""
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store
        task_id = str(uuid.uuid4())

        async def run_task():
            result = await run_browser_agent(request, settings, sse_manager)
            result.task_id = task_id
            await state_store.save_task(task_id, result)

        background_tasks.add_task(asyncio.create_task, run_task())

        return {
            "task_id": task_id,
            "status": "queued",
            "message": "íƒœìŠ¤í¬ê°€ íì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. GET /agent/task/{task_id}ë¡œ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.",
        }

    @app.get("/agent/task/{task_id}", response_model=AgentTaskResult)
    async def get_task_result(task_id: str, req: Request):
        """íƒœìŠ¤í¬ ê²°ê³¼ ì¡°íšŒ"""
        state_store: StateStore = req.app.state.state_store
        result = await state_store.load_task(task_id)
        if result is None:
            raise HTTPException(status_code=404, detail="íƒœìŠ¤í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return result

    @app.delete("/agent/task/{task_id}")
    async def delete_task(task_id: str, req: Request):
        """íƒœìŠ¤í¬ ê²°ê³¼ ì‚­ì œ"""
        state_store: StateStore = req.app.state.state_store
        result = await state_store.load_task(task_id)
        if result is not None:
            await state_store.delete_task(task_id)
            return {"status": "deleted", "task_id": task_id}
        raise HTTPException(status_code=404, detail="íƒœìŠ¤í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    @app.get("/agent/tasks")
    async def list_tasks(
        req: Request,
        limit: int = Query(default=20, le=100),
        status: Optional[str] = Query(default=None),
    ):
        """ìµœê·¼ íƒœìŠ¤í¬ ëª©ë¡ ì¡°íšŒ"""
        state_store: StateStore = req.app.state.state_store
        results = await state_store.list_tasks(status=status, limit=limit)

        # Sort by duration_ms (stored tasks are dicts, not Pydantic models)
        results = sorted(results, key=lambda x: x.get("duration_ms", 0), reverse=True)[:limit]

        return {"total": len(results), "tasks": results}

    @app.post("/agent/batch")
    async def batch_crawl(
        request: BatchCrawlRequest,
        req: Request,
        background_tasks: BackgroundTasks,
        user: JWTPayload = Depends(require_auth()),
    ):
        """ì—¬ëŸ¬ URL ë°°ì¹˜ í¬ë¡¤ë§ (ì¸ì¦ í•„ìš”)"""
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store
        batch_id = str(uuid.uuid4())
        task_ids = []

        tasks_to_run = []
        for url in request.urls:
            agent_task = AgentTask(
                url=url,
                task=request.task,
                llm_provider=request.llm_provider,
                model=request.model,
                auto_save_url=request.auto_save_url,
            )
            task_id = str(uuid.uuid4())
            task_ids.append(task_id)
            tasks_to_run.append((agent_task, task_id))

        async def run_batch():
            for agent_task, tid in tasks_to_run:
                try:
                    result = await run_browser_agent(agent_task, settings, sse_manager)
                    result.task_id = tid
                    await state_store.save_task(tid, result)
                except Exception as e:
                    error_result = AgentTaskResult(
                        task_id=tid,
                        url=str(agent_task.url),
                        status="failed",
                        error_message=str(e),
                        duration_ms=0,
                    )
                    await state_store.save_task(tid, error_result)

        background_tasks.add_task(run_batch)

        return {
            "batch_id": batch_id,
            "task_ids": task_ids,
            "total": len(task_ids),
            "message": "ë°°ì¹˜ í¬ë¡¤ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
        }

    # ========================================
    # ë‰´ìŠ¤ í”„ë¦¬ì…‹ ì—”ë“œí¬ì¸íŠ¸
    # ========================================

    @app.post("/agent/presets/extract-article")
    async def extract_article(url: HttpUrl, req: Request, auto_save: bool = True):
        """
        ë‰´ìŠ¤ ê¸°ì‚¬ ì¶”ì¶œ í”„ë¦¬ì…‹.

        URLì—ì„œ ì œëª©, ì‘ì„±ì, ë‚ ì§œ, ë³¸ë¬¸ì„ ìë™ ì¶”ì¶œ.
        """
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store

        request = AgentTask(
            url=url,
            task="""
            ì´ ë‰´ìŠ¤ ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:
            1. ê¸°ì‚¬ ì œëª©
            2. ì‘ì„±ì/ê¸°ìëª…
            3. ë°œí–‰ì¼
            4. ë³¸ë¬¸ ì „ì²´ í…ìŠ¤íŠ¸
            5. ê´€ë ¨ ê¸°ì‚¬ ë§í¬ë“¤
            
            JSON í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
            """,
            extract_links=True,
            auto_save_url=auto_save,
            source_category="news",
        )
        result = await run_browser_agent(request, settings, sse_manager)
        await state_store.save_task(result.task_id, result)
        return result

    @app.post("/agent/presets/extract-comments")
    async def extract_comments(
        url: HttpUrl,
        req: Request,
        max_scroll: int = 5,
    ):
        """
        ëŒ“ê¸€/ì—¬ë¡  ì¶”ì¶œ í”„ë¦¬ì…‹.

        ê²Œì‹œê¸€ì˜ ëŒ“ê¸€ë“¤ì„ ìˆ˜ì§‘.
        """
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store

        request = AgentTask(
            url=url,
            task=f"""
            ì´ í˜ì´ì§€ì—ì„œ ëŒ“ê¸€ë“¤ì„ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”:
            1. í˜ì´ì§€ë¥¼ {max_scroll}ë²ˆ ìŠ¤í¬ë¡¤í•˜ë©´ì„œ ëŒ“ê¸€ì„ ë¡œë“œí•´ì£¼ì„¸ìš”
            2. ê° ëŒ“ê¸€ì˜ ì‘ì„±ì, ë‚´ìš©, ì‘ì„±ì‹œê°„, ì¢‹ì•„ìš” ìˆ˜ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”
            3. "ë”ë³´ê¸°" ë²„íŠ¼ì´ ìˆìœ¼ë©´ í´ë¦­í•´ì„œ ë” ë§ì€ ëŒ“ê¸€ì„ ë¡œë“œí•´ì£¼ì„¸ìš”
            
            JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
            """,
            max_steps=20,
            auto_save_url=False,
            source_category="forum",
        )
        result = await run_browser_agent(request, settings, sse_manager)
        await state_store.save_task(result.task_id, result)
        return result

    @app.post("/agent/presets/site-structure")
    async def analyze_site_structure(url: HttpUrl, req: Request):
        """
        ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„ í”„ë¦¬ì…‹.

        ì‚¬ì´íŠ¸ì˜ ì£¼ìš” ì„¹ì…˜ê³¼ ë§í¬ êµ¬ì¡° íŒŒì•….
        """
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store

        request = AgentTask(
            url=url,
            task="""
            ì´ ì›¹ì‚¬ì´íŠ¸ì˜ êµ¬ì¡°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:
            1. ë©”ì¸ ë„¤ë¹„ê²Œì´ì…˜ ë©”ë‰´ í•­ëª©ë“¤
            2. ì£¼ìš” ì„¹ì…˜/ì¹´í…Œê³ ë¦¬
            3. RSS í”¼ë“œ ë§í¬ê°€ ìˆëŠ”ì§€
            4. API ì—”ë“œí¬ì¸íŠ¸ íŒíŠ¸ê°€ ìˆëŠ”ì§€
            5. ì‚¬ì´íŠ¸ë§µ ë§í¬
            
            í¬ë¡¤ë§ ì „ëµ ìˆ˜ë¦½ì— ë„ì›€ì´ ë˜ë„ë¡ ì •ë¦¬í•´ì£¼ì„¸ìš”.
            """,
            extract_links=True,
            auto_save_url=True,
            source_category="general",
        )
        result = await run_browser_agent(request, settings, sse_manager)
        await state_store.save_task(result.task_id, result)
        return result

    @app.post("/agent/presets/news-domain-crawl")
    async def news_domain_crawl(
        url: HttpUrl,
        req: Request,
        max_pages: int = Query(default=30, ge=1, le=100, description="ìµœëŒ€ ë°©ë¬¸ í˜ì´ì§€ ìˆ˜"),
        max_depth: int = Query(default=2, ge=1, le=5, description="ìµœëŒ€ íƒìƒ‰ ê¹Šì´"),
        focus_keywords: Optional[str] = Query(default=None, description="ì§‘ì¤‘ í‚¤ì›Œë“œ (ì‰¼í‘œ êµ¬ë¶„)"),
    ):
        """
        ë‰´ìŠ¤ ë„ë©”ì¸ ì „ì²´ í¬ë¡¤ë§ í”„ë¦¬ì…‹.

        ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì˜ ê¸°ì‚¬ë“¤ì„ ìë™ìœ¼ë¡œ íƒìƒ‰í•˜ê³  ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        NEWS_ONLY ì •ì±…ì„ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê¸°ì‚¬ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Args:
            url: ì‹œì‘ URL (ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ë©”ì¸ ë˜ëŠ” ì„¹ì…˜ í˜ì´ì§€)
            max_pages: ìµœëŒ€ ë°©ë¬¸í•  í˜ì´ì§€ ìˆ˜
            max_depth: ë§í¬ íƒìƒ‰ ìµœëŒ€ ê¹Šì´
            focus_keywords: íŠ¹ì • í‚¤ì›Œë“œì— ì§‘ì¤‘í•  ê²½ìš° ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥
        """
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store

        keyword_instruction = ""
        if focus_keywords:
            keyword_instruction = (
                f"\níŠ¹íˆ ë‹¤ìŒ í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ê¸°ì‚¬ì— ì§‘ì¤‘í•´ì£¼ì„¸ìš”: {focus_keywords}"
            )

        request = AgentTask(
            url=url,
            task=f"""
            ì´ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ ê¸°ì‚¬ë“¤ì„ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”:
            
            1. ë©”ì¸ í˜ì´ì§€ì—ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬ë“¤ì„ ì°¾ì•„ì£¼ì„¸ìš”
            2. ê° ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ì œëª©, ê¸°ìëª…, ë°œí–‰ì¼, ë³¸ë¬¸ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”
            3. ìµœëŒ€ {max_pages}ê°œ í˜ì´ì§€ë¥¼ ë°©ë¬¸í•˜ë©´ì„œ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”
            4. ìµœê·¼ ê¸°ì‚¬ë¥¼ ìš°ì„ ìœ¼ë¡œ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”
            5. ê´‘ê³ , ë¡œê·¸ì¸ í˜ì´ì§€, ë¹„ê¸°ì‚¬ ì½˜í…ì¸ ëŠ” ê±´ë„ˆë›°ì„¸ìš”
            {keyword_instruction}
            
            ê° ê¸°ì‚¬ëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:
            ---ARTICLE_START---
            URL: [ê¸°ì‚¬ URL]
            TITLE: [ì œëª©]
            AUTHOR: [ê¸°ìëª…]
            PUBLISHED_AT: [ë°œí–‰ì¼]
            CONTENT: [ë³¸ë¬¸]
            ---ARTICLE_END---
            """,
            max_steps=max_pages + 10,
            timeout_sec=min(max_pages * 15, 600),
            extract_links=True,
            auto_save_url=True,
            source_category="news",
        )
        result = await run_browser_agent(request, settings, sse_manager)
        await state_store.save_task(result.task_id, result)
        return result

    @app.post("/agent/presets/discover-rss")
    async def discover_rss(url: HttpUrl, req: Request):
        """
        RSS í”¼ë“œ ë°œê²¬ í”„ë¦¬ì…‹.

        ì›¹ì‚¬ì´íŠ¸ì—ì„œ RSS/Atom í”¼ë“œ URLì„ ì°¾ì•„ëƒ…ë‹ˆë‹¤.
        ë‰´ìŠ¤ ìˆ˜ì§‘ ìë™í™”ë¥¼ ìœ„í•œ í”¼ë“œ URL ë°œê²¬ì— ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store

        request = AgentTask(
            url=url,
            task="""
            ì´ ì›¹ì‚¬ì´íŠ¸ì—ì„œ RSS ë˜ëŠ” Atom í”¼ë“œë¥¼ ì°¾ì•„ì£¼ì„¸ìš”:
            
            1. í˜ì´ì§€ ì†ŒìŠ¤ì—ì„œ RSS/Atom ë§í¬ íƒœê·¸ í™•ì¸
               - <link rel="alternate" type="application/rss+xml" ...>
               - <link rel="alternate" type="application/atom+xml" ...>
            2. ì¼ë°˜ì ì¸ RSS ê²½ë¡œ í™•ì¸:
               - /feed, /rss, /feeds, /rss.xml, /feed.xml, /atom.xml
               - /news/rss, /blog/feed ë“±
            3. í˜ì´ì§€ í‘¸í„°ë‚˜ ì‚¬ì´ë“œë°”ì˜ RSS ì•„ì´ì½˜/ë§í¬ í™•ì¸
            4. sitemap.xmlì—ì„œ í”¼ë“œ ì •ë³´ í™•ì¸
            
            ë°œê²¬ëœ ëª¨ë“  í”¼ë“œë¥¼ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:
            {
                "feeds": [
                    {
                        "url": "í”¼ë“œ URL",
                        "type": "rss" ë˜ëŠ” "atom",
                        "title": "í”¼ë“œ ì œëª© (ìˆëŠ” ê²½ìš°)",
                        "category": "ì¹´í…Œê³ ë¦¬ (ìˆëŠ” ê²½ìš°)"
                    }
                ],
                "sitemap_url": "ì‚¬ì´íŠ¸ë§µ URL (ë°œê²¬ëœ ê²½ìš°)",
                "has_api_hints": true/false
            }
            """,
            max_steps=15,
            extract_links=True,
            auto_save_url=False,
            source_category="general",
        )
        result = await run_browser_agent(request, settings, sse_manager)
        await state_store.save_task(result.task_id, result)
        return result

    @app.post("/agent/presets/extract-news-list")
    async def extract_news_list(
        url: HttpUrl,
        req: Request,
        max_articles: int = Query(default=20, ge=1, le=50, description="ì¶”ì¶œí•  ìµœëŒ€ ê¸°ì‚¬ ìˆ˜"),
    ):
        """
        ë‰´ìŠ¤ ëª©ë¡ í˜ì´ì§€ ì¶”ì¶œ í”„ë¦¬ì…‹.

        ë‰´ìŠ¤ ì„¹ì…˜/ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ëª©ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        ê° ê¸°ì‚¬ì˜ ì œëª©, URL, ìš”ì•½, ë°œí–‰ì¼ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        """
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store

        request = AgentTask(
            url=url,
            task=f"""
            ì´ í˜ì´ì§€ì—ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”:
            
            1. ìµœëŒ€ {max_articles}ê°œì˜ ê¸°ì‚¬ ì •ë³´ë¥¼ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”
            2. ê° ê¸°ì‚¬ì— ëŒ€í•´ ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œ:
               - ê¸°ì‚¬ ì œëª©
               - ê¸°ì‚¬ URL (ì „ì²´ ë§í¬)
               - ìš”ì•½/ë¦¬ë“œ ë¬¸êµ¬ (ìˆëŠ” ê²½ìš°)
               - ë°œí–‰ì¼/ì‹œê°„ (ìˆëŠ” ê²½ìš°)
               - ì¸ë„¤ì¼ ì´ë¯¸ì§€ URL (ìˆëŠ” ê²½ìš°)
               - ì¹´í…Œê³ ë¦¬/ì„¹ì…˜ (ìˆëŠ” ê²½ìš°)
            3. ê´‘ê³ ë‚˜ í”„ë¡œëª¨ì…˜ ì½˜í…ì¸ ëŠ” ì œì™¸í•´ì£¼ì„¸ìš”
            4. ìµœì‹  ê¸°ì‚¬ ìˆœìœ¼ë¡œ ì •ë ¬í•´ì£¼ì„¸ìš”
            
            JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:
            [
                {{
                    "title": "ê¸°ì‚¬ ì œëª©",
                    "url": "ê¸°ì‚¬ URL",
                    "summary": "ìš”ì•½",
                    "published_at": "ë°œí–‰ì¼",
                    "thumbnail": "ì¸ë„¤ì¼ URL",
                    "category": "ì¹´í…Œê³ ë¦¬"
                }}
            ]
            """,
            max_steps=10,
            extract_links=True,
            auto_save_url=True,
            source_category="news",
        )
        result = await run_browser_agent(request, settings, sse_manager)
        await state_store.save_task(result.task_id, result)
        return result

    @app.post("/agent/presets/monitor-breaking-news")
    async def monitor_breaking_news(
        url: HttpUrl,
        req: Request,
        keywords: Optional[str] = Query(default=None, description="ëª¨ë‹ˆí„°ë§ í‚¤ì›Œë“œ (ì‰¼í‘œ êµ¬ë¶„)"),
    ):
        """
        ì†ë³´/ê¸´ê¸‰ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ í”„ë¦¬ì…‹.

        ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ ì†ë³´ë‚˜ ê¸´ê¸‰ ë‰´ìŠ¤ë¥¼ íƒì§€í•©ë‹ˆë‹¤.
        'ì†ë³´', 'ê¸´ê¸‰', 'Breaking' ë“±ì˜ ë¼ë²¨ì´ ë¶™ì€ ê¸°ì‚¬ë¥¼ ìš°ì„  ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        """
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store

        keyword_filter = ""
        if keywords:
            keyword_filter = f"\níŠ¹íˆ ë‹¤ìŒ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì†ë³´ì— ì£¼ëª©í•´ì£¼ì„¸ìš”: {keywords}"

        request = AgentTask(
            url=url,
            task=f"""
            ì´ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ ì†ë³´/ê¸´ê¸‰ ë‰´ìŠ¤ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”:
            
            1. ë‹¤ìŒ í‘œì‹œê°€ ìˆëŠ” ê¸°ì‚¬ë¥¼ ìš°ì„  íƒì§€:
               - "ì†ë³´", "Breaking", "ê¸´ê¸‰", "ë‹¨ë…", "flash"
               - ë¹¨ê°„ìƒ‰ ë˜ëŠ” ê°•ì¡°ëœ ë¼ë²¨
               - ìƒë‹¨ ê³ ì • ë˜ëŠ” íŠ¹ë³„ ì„¹ì…˜ì˜ ê¸°ì‚¬
            2. ìµœê·¼ 1ì‹œê°„ ì´ë‚´ ë°œí–‰ëœ ê¸°ì‚¬ ìš°ì„ 
            3. ê° ì†ë³´ ê¸°ì‚¬ì˜ ì „ì²´ ë‚´ìš© ì¶”ì¶œ
            {keyword_filter}
            
            ê²°ê³¼ë¥¼ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:
            {{
                "breaking_news": [
                    {{
                        "title": "ê¸°ì‚¬ ì œëª©",
                        "url": "ê¸°ì‚¬ URL",
                        "published_at": "ë°œí–‰ ì‹œê°„",
                        "label": "ì†ë³´/ê¸´ê¸‰/ë‹¨ë… ë“±",
                        "summary": "í•µì‹¬ ë‚´ìš© ìš”ì•½",
                        "full_content": "ì „ì²´ ë³¸ë¬¸"
                    }}
                ],
                "latest_update": "ë§ˆì§€ë§‰ í™•ì¸ ì‹œê°„",
                "total_found": ë°œê²¬ëœ ì†ë³´ ìˆ˜
            }}
            """,
            max_steps=15,
            timeout_sec=180,
            extract_links=True,
            auto_save_url=True,
            source_category="news",
        )
        result = await run_browser_agent(request, settings, sse_manager)
        await state_store.save_task(result.task_id, result)
        return result

    # ========================================
    # ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸
    # ========================================

    async def fetch_page_content_for_context(url: str, max_chars: int = 8000) -> Optional[str]:
        """
        URLì—ì„œ í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì™€ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©.

        crawl4ai ë˜ëŠ” ë‚´ë¶€ í¬ë¡¤ëŸ¬ë¥¼ í†µí•´ í˜ì´ì§€ ë‚´ìš©ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜.
        """
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                # 1ì°¨: crawl4ai ì„œë¹„ìŠ¤ ì‹œë„
                try:

                    def extract_content(payload: Any) -> Optional[str]:
                        if isinstance(payload, dict):
                            results = payload.get("results")
                            if isinstance(results, list) and results:
                                return extract_content(results[0])
                            result = payload.get("result")
                            if result is not None:
                                return extract_content(result)
                            for key in ("markdown", "text", "content"):
                                value = payload.get(key)
                                if isinstance(value, str) and value.strip():
                                    return value
                            return None
                        if isinstance(payload, str) and payload.strip():
                            return payload
                        return None

                    headers: dict[str, str] = {}
                    if api_config.WEB_CRAWLER_API_TOKEN:
                        headers["Authorization"] = f"Bearer {api_config.WEB_CRAWLER_API_TOKEN}"

                    base_url = api_config.WEB_CRAWLER_URL.rstrip("/")
                    endpoint = f"{base_url}/crawl"

                    crawl_response = await client.get(
                        endpoint, params={"url": url}, headers=headers
                    )
                    if crawl_response.status_code == 200:
                        try:
                            crawl_data = crawl_response.json()
                        except Exception:
                            crawl_data = crawl_response.text
                        content = extract_content(crawl_data)
                        if content:
                            return content[:max_chars]

                    crawl_response = await client.post(
                        endpoint,
                        json={"urls": [url], "priority": 10},
                        headers=headers,
                    )
                    if crawl_response.status_code == 200:
                        crawl_data = crawl_response.json()
                        content = extract_content(crawl_data)
                        if content:
                            return content[:max_chars]

                        task_id = crawl_data.get("task_id")
                        if task_id:
                            for status_path in (
                                f"{base_url}/task/{task_id}",
                                f"{base_url}/job/{task_id}",
                            ):
                                try:
                                    status_response = await client.get(status_path, headers=headers)
                                    if status_response.status_code == 200:
                                        status_data = status_response.json()
                                        status_content = extract_content(status_data)
                                        if status_content:
                                            return status_content[:max_chars]
                                except Exception:
                                    continue
                except Exception:
                    pass

                # 2ì°¨: ì§ì ‘ HTTP ìš”ì²­ fallback
                try:
                    response = await client.get(url, follow_redirects=True)
                    if response.status_code == 200:
                        from bs4 import BeautifulSoup

                        soup = BeautifulSoup(response.text, "html.parser")

                        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                        for tag in soup(["script", "style", "nav", "footer", "header", "aside"]):
                            tag.decompose()

                        # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        text = soup.get_text(separator="\n", strip=True)
                        return text[:max_chars] if text else None
                except Exception:
                    pass

        except Exception as e:
            logger.warning("Failed to fetch context URL", url=url, error=str(e))

        return None

    def convert_to_langchain_messages(messages: List[ChatMessage]):
        """ChatMessage ë¦¬ìŠ¤íŠ¸ë¥¼ LangChain ë©”ì‹œì§€ë¡œ ë³€í™˜"""
        from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

        langchain_messages = []
        for msg in messages:
            if msg.role == "user":
                langchain_messages.append(HumanMessage(content=msg.content))
            elif msg.role == "assistant":
                langchain_messages.append(AIMessage(content=msg.content))
            elif msg.role == "system":
                langchain_messages.append(SystemMessage(content=msg.content))
        return langchain_messages

    @app.post("/chat", response_model=ChatResponse)
    async def chat(request: ChatRequest):
        """
        AI ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸.

        LLMê³¼ ì§ì ‘ ëŒ€í™”í•˜ëŠ” ì¸í„°í˜ì´ìŠ¤.
        URL ì»¨í…ìŠ¤íŠ¸ê°€ ì œê³µë˜ë©´ í•´ë‹¹ í˜ì´ì§€ ë‚´ìš©ì„ ì°¸ì¡°í•˜ì—¬ ì‘ë‹µ.

        ì‚¬ìš© ì˜ˆì‹œ:
        ``\`json
        {
            "messages": [
                {"role": "user", "content": "ì´ ê¸°ì‚¬ì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ì¤˜"}
            ],
            "context_url": "https://news.example.com/article/123"
        }
        ``\`
        """
        try:
            llm = await get_llm(request.llm_provider, request.model)
            langchain_messages = convert_to_langchain_messages(request.messages)

            # URL ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ í˜ì´ì§€ ë‚´ìš© ì¶”ê°€
            if request.context_url:
                page_content = await fetch_page_content_for_context(request.context_url)
                if page_content:
                    from langchain_core.messages import SystemMessage

                    context_msg = SystemMessage(
                        content=f"ë‹¤ìŒì€ ì°¸ì¡°í•  ì›¹í˜ì´ì§€ ë‚´ìš©ì…ë‹ˆë‹¤:\n\n{page_content}"
                    )
                    langchain_messages.insert(0, context_msg)
                    logger.debug(
                        "Context URL content added",
                        url=request.context_url,
                        content_length=len(page_content),
                    )

            response = await llm.ainvoke(langchain_messages)

            return ChatResponse(
                message=response.content,
                provider=request.llm_provider.value,
                model=request.model or api_config.DEFAULT_MODEL,
                tokens_used=getattr(response, "usage_metadata", {}).get("total_tokens"),
            )

        except Exception as e:
            logger.error("Chat failed", error=str(e))
            raise HTTPException(status_code=500, detail=f"ì±„íŒ… ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

    @app.post("/chat/stream")
    async def chat_stream(request: ChatRequest):
        """
        AI ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° ì—”ë“œí¬ì¸íŠ¸ (SSE).

        LLM ì‘ë‹µì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.
        URL ì»¨í…ìŠ¤íŠ¸ê°€ ì œê³µë˜ë©´ í•´ë‹¹ í˜ì´ì§€ ë‚´ìš©ì„ ì°¸ì¡°í•˜ì—¬ ì‘ë‹µ.

        ì‚¬ìš© ì˜ˆì‹œ:
        ``\`json
        {
            "messages": [
                {"role": "user", "content": "íŒŒì´ì¬ì˜ ì¥ì ì„ ì„¤ëª…í•´ì¤˜"}
            ],
            "stream": true,
            "context_url": "https://docs.python.org/3/"
        }
        ``\`

        ì´ë²¤íŠ¸ íƒ€ì…:
        - chunk: ìŠ¤íŠ¸ë¦¬ë° í…ìŠ¤íŠ¸ ì¡°ê°
        - done: ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ
        - error: ì—ëŸ¬ ë°œìƒ
        """
        import json

        async def generate_stream():
            try:
                llm = await get_llm(request.llm_provider, request.model)
                langchain_messages = convert_to_langchain_messages(request.messages)

                # URL ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ í˜ì´ì§€ ë‚´ìš© ì¶”ê°€
                if request.context_url:
                    page_content = await fetch_page_content_for_context(request.context_url)
                    if page_content:
                        from langchain_core.messages import SystemMessage

                        context_msg = SystemMessage(
                            content=f"ë‹¤ìŒì€ ì°¸ì¡°í•  ì›¹í˜ì´ì§€ ë‚´ìš©ì…ë‹ˆë‹¤:\n\n{page_content}"
                        )
                        langchain_messages.insert(0, context_msg)
                        logger.debug(
                            "Context URL content added for streaming",
                            url=request.context_url,
                            content_length=len(page_content),
                        )

                # ìŠ¤íŠ¸ë¦¬ë° LLM í˜¸ì¶œ
                full_response = ""
                async for chunk in llm.astream(langchain_messages):
                    if hasattr(chunk, "content") and chunk.content:
                        content = chunk.content
                        full_response += content
                        yield f"data: {json.dumps({'content': content, 'type': 'chunk'})}\n\n"

                # ì™„ë£Œ ì´ë²¤íŠ¸
                yield f"data: {json.dumps({'type': 'done', 'provider': request.llm_provider.value, 'model': request.model or api_config.DEFAULT_MODEL, 'full_response': full_response})}\n\n"

                logger.info(
                    "Chat stream completed",
                    provider=request.llm_provider.value,
                    response_length=len(full_response),
                )

            except Exception as e:
                logger.error("Chat stream failed", error=str(e))
                yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"

        return StreamingResponse(
            generate_stream(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",
                "Access-Control-Allow-Origin": "*",
            },
        )


# Default app instance
app = create_app()


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "src.api.server:app",
        host="0.0.0.0",
        port=8030,
        reload=os.getenv("ENV", "development") == "development",
    )

```

---

## backend/autonomous-crawler-service/src/api/sse.py

```py
"""SSE (Server-Sent Events) Manager for real-time agent status updates."""

import asyncio
import json
import uuid
from datetime import datetime, timezone
from enum import Enum
from typing import Any, Dict

import structlog
from pydantic import BaseModel, Field

logger = structlog.get_logger(__name__)


class SSEEventType(str, Enum):
    """SSE ì´ë²¤íŠ¸ íƒ€ì…"""

    CONNECTED = "connected"
    AGENT_START = "agent_start"
    AGENT_STEP = "agent_step"
    AGENT_COMPLETE = "agent_complete"
    AGENT_ERROR = "agent_error"
    URL_DISCOVERED = "url_discovered"
    HEALTH_UPDATE = "health_update"
    CAPTCHA_DETECTED = "captcha_detected"
    CAPTCHA_SOLVED = "captcha_solved"
    # ìˆ˜ì§‘ ì‘ì—… ë¡œê·¸ ì´ë²¤íŠ¸
    COLLECTION_START = "collection_start"
    COLLECTION_PROGRESS = "collection_progress"
    COLLECTION_COMPLETE = "collection_complete"
    COLLECTION_ERROR = "collection_error"
    COLLECTION_LOG = "collection_log"


class SSEEvent(BaseModel):
    """SSE ì´ë²¤íŠ¸ ë°ì´í„°"""

    type: SSEEventType
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    timestamp: str = Field(default_factory=lambda: datetime.now(timezone.utc).isoformat())
    data: Dict[str, Any] = {}


class SSEManager:
    """
    SSE í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ê´€ë¦¬ì.

    ë¸Œë¼ìš°ì € ì—ì´ì „íŠ¸ì˜ ì‹¤ì‹œê°„ ìƒíƒœë¥¼ êµ¬ë…í•˜ëŠ” í´ë¼ì´ì–¸íŠ¸ë“¤ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
    """

    def __init__(self, max_queue_size: int = 100):
        self._clients: Dict[str, asyncio.Queue] = {}
        self._lock = asyncio.Lock()
        self._max_queue_size = max_queue_size

    @property
    def client_count(self) -> int:
        """í˜„ì¬ ì—°ê²°ëœ í´ë¼ì´ì–¸íŠ¸ ìˆ˜"""
        return len(self._clients)

    @property
    def client_ids(self) -> list[str]:
        """ì—°ê²°ëœ í´ë¼ì´ì–¸íŠ¸ ID ëª©ë¡"""
        return list(self._clients.keys())

    async def connect(self, client_id: str | None = None) -> tuple[str, asyncio.Queue]:
        """
        ìƒˆ í´ë¼ì´ì–¸íŠ¸ ì—°ê²°.

        Args:
            client_id: í´ë¼ì´ì–¸íŠ¸ ID (ì—†ìœ¼ë©´ ìë™ ìƒì„±)

        Returns:
            (client_id, queue) íŠœí”Œ
        """
        if client_id is None:
            client_id = str(uuid.uuid4())

        queue: asyncio.Queue = asyncio.Queue(maxsize=self._max_queue_size)

        async with self._lock:
            self._clients[client_id] = queue

        logger.info(
            "SSE client connected",
            client_id=client_id,
            total_clients=len(self._clients),
        )

        return client_id, queue

    async def disconnect(self, client_id: str) -> None:
        """í´ë¼ì´ì–¸íŠ¸ ì—°ê²° í•´ì œ"""
        async with self._lock:
            if client_id in self._clients:
                del self._clients[client_id]
                logger.info(
                    "SSE client disconnected",
                    client_id=client_id,
                    total_clients=len(self._clients),
                )

    async def broadcast(self, event: SSEEvent) -> None:
        """
        ëª¨ë“  ì—°ê²°ëœ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì´ë²¤íŠ¸ ë¸Œë¡œë“œìºìŠ¤íŠ¸.

        Args:
            event: ì „ì†¡í•  SSE ì´ë²¤íŠ¸
        """
        disconnected = []

        async with self._lock:
            for client_id, queue in self._clients.items():
                try:
                    queue.put_nowait(event)
                except asyncio.QueueFull:
                    logger.warning("SSE queue full", client_id=client_id)
                except Exception as e:
                    logger.warning(
                        "SSE broadcast error",
                        client_id=client_id,
                        error=str(e),
                    )
                    disconnected.append(client_id)

            # ì—°ê²° ëŠê¸´ í´ë¼ì´ì–¸íŠ¸ ì œê±°
            for client_id in disconnected:
                del self._clients[client_id]

    async def send_agent_event(
        self,
        event_type: SSEEventType,
        task_id: str,
        url: str,
        message: str,
        **kwargs,
    ) -> None:
        """
        ì—ì´ì „íŠ¸ ì´ë²¤íŠ¸ ì „ì†¡ í—¬í¼.

        Args:
            event_type: ì´ë²¤íŠ¸ íƒ€ì…
            task_id: íƒœìŠ¤í¬ ID
            url: ê´€ë ¨ URL
            message: ë©”ì‹œì§€
            **kwargs: ì¶”ê°€ ë°ì´í„°
        """
        event = SSEEvent(
            type=event_type,
            data={
                "task_id": task_id,
                "url": url,
                "message": message,
                **kwargs,
            },
        )
        await self.broadcast(event)

    async def send_collection_event(
        self,
        event_type: SSEEventType,
        source_name: str,
        message: str,
        level: str = "INFO",
        **kwargs,
    ) -> None:
        """
        ìˆ˜ì§‘ ì‘ì—… ë¡œê·¸ ì´ë²¤íŠ¸ ì „ì†¡ í—¬í¼.

        Args:
            event_type: ì´ë²¤íŠ¸ íƒ€ì… (COLLECTION_*)
            source_name: ë°ì´í„° ì†ŒìŠ¤ ì´ë¦„ (ì˜ˆ: ë””ì‹œì¸ì‚¬ì´ë“œ)
            message: ë¡œê·¸ ë©”ì‹œì§€
            level: ë¡œê·¸ ë ˆë²¨ (DEBUG, INFO, WARNING, ERROR)
            **kwargs: ì¶”ê°€ ë°ì´í„°
        """
        event = SSEEvent(
            type=event_type,
            data={
                "source": source_name,
                "message": message,
                "level": level,
                **kwargs,
            },
        )
        await self.broadcast(event)


async def sse_event_generator(
    client_id: str,
    queue: asyncio.Queue,
    manager: SSEManager,
    heartbeat_interval: float = 30.0,
):
    """
    SSE ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ ìƒì„±ê¸°.

    Args:
        client_id: í´ë¼ì´ì–¸íŠ¸ ID
        queue: ì´ë²¤íŠ¸ í
        manager: SSE ë§¤ë‹ˆì €
        heartbeat_interval: í•˜íŠ¸ë¹„íŠ¸ ê°„ê²© (ì´ˆ)

    Yields:
        SSE í˜•ì‹ì˜ ì´ë²¤íŠ¸ ë¬¸ìì—´
    """
    try:
        # ì—°ê²° í™•ì¸ ì´ë²¤íŠ¸
        connected_event = SSEEvent(
            type=SSEEventType.CONNECTED,
            data={
                "client_id": client_id,
                "message": "Autonomous Crawler SSE connected",
                "active_clients": manager.client_count,
            },
        )
        yield f"event: {connected_event.type.value}\ndata: {json.dumps(connected_event.model_dump())}\n\n"

        while True:
            try:
                # í•˜íŠ¸ë¹„íŠ¸ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì´ë²¤íŠ¸ ëŒ€ê¸°
                event = await asyncio.wait_for(queue.get(), timeout=heartbeat_interval)
                yield f"event: {event.type.value}\ndata: {json.dumps(event.model_dump())}\n\n"
            except asyncio.TimeoutError:
                # Heartbeat ì „ì†¡
                yield ": heartbeat\n\n"

    except asyncio.CancelledError:
        logger.info("SSE client stream cancelled", client_id=client_id)
    finally:
        await manager.disconnect(client_id)


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_sse_manager: SSEManager | None = None


def get_sse_manager() -> SSEManager:
    """ì‹±ê¸€í†¤ SSE ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _sse_manager
    if _sse_manager is None:
        _sse_manager = SSEManager()
    return _sse_manager

```

---

## backend/autonomous-crawler-service/src/auth/__init__.py

```py
"""
Authentication module for autonomous-crawler-service.
"""

from .middleware import AuthMiddleware, get_current_user, require_auth
from .jwt_utils import verify_jwt_token, JWTPayload

__all__ = [
    "AuthMiddleware",
    "get_current_user",
    "require_auth",
    "verify_jwt_token",
    "JWTPayload",
]

```

---

## backend/autonomous-crawler-service/src/auth/jwt_utils.py

```py
"""
JWT Utilities for autonomous-crawler-service.
"""

import os
from dataclasses import dataclass
from typing import Optional

import jwt
import structlog

logger = structlog.get_logger(__name__)

# Secret key for JWT verification (shared with admin-dashboard)
JWT_SECRET = os.getenv("ADMIN_SECRET_KEY", "your-secret-key-change-in-production")
JWT_ALGORITHM = "HS256"


@dataclass
class JWTPayload:
    """JWT Token Payload"""
    user_id: str
    username: str
    role: str
    exp: int
    iat: int


def verify_jwt_token(token: str) -> Optional[JWTPayload]:
    """
    Verify JWT token and return payload.
    
    Args:
        token: JWT token string (without 'Bearer ' prefix)
        
    Returns:
        JWTPayload if valid, None otherwise
    """
    try:
        payload = jwt.decode(
            token,
            JWT_SECRET,
            algorithms=[JWT_ALGORITHM],
        )
        
        return JWTPayload(
            user_id=payload.get("sub", ""),
            username=payload.get("username", ""),
            role=payload.get("role", "user"),
            exp=payload.get("exp", 0),
            iat=payload.get("iat", 0),
        )
        
    except jwt.ExpiredSignatureError:
        logger.warning("JWT token expired")
        return None
    except jwt.InvalidTokenError as e:
        logger.warning("Invalid JWT token", error=str(e))
        return None
    except Exception as e:
        logger.error("JWT verification error", error=str(e))
        return None

```

---

## backend/autonomous-crawler-service/src/auth/middleware.py

```py
"""
Authentication Middleware for autonomous-crawler-service.

Provides FastAPI dependencies for JWT-based authentication.
"""

import os
from functools import wraps
from typing import Callable, Optional

import structlog
from fastapi import Depends, HTTPException, Request, status
from fastapi.security import HTTPAuthorizationCredentials, HTTPBearer

from .jwt_utils import JWTPayload, verify_jwt_token

logger = structlog.get_logger(__name__)

# Security enabled flag - set to False only in development
SECURITY_ENABLED = os.getenv("SECURITY_ENABLED", "true").lower() == "true"

# HTTP Bearer scheme for extracting tokens
bearer_scheme = HTTPBearer(auto_error=False)


class AuthMiddleware:
    """
    Authentication middleware for FastAPI.
    
    Usage:
        app.add_middleware(AuthMiddleware)
    
    Or use the dependency injection approach with get_current_user.
    """
    
    # Endpoints that don't require authentication
    PUBLIC_PATHS = {
        "/health",
        "/",
        "/docs",
        "/openapi.json",
        "/redoc",
    }
    
    # Path prefixes that don't require authentication
    PUBLIC_PREFIXES = (
        "/health",
        "/docs",
        "/openapi",
        "/redoc",
    )

    def __init__(self, app):
        self.app = app

    async def __call__(self, scope, receive, send):
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return
            
        path = scope.get("path", "")
        
        # Skip auth for public paths
        if path in self.PUBLIC_PATHS or path.startswith(self.PUBLIC_PREFIXES):
            await self.app(scope, receive, send)
            return
            
        # Skip auth if disabled
        if not SECURITY_ENABLED:
            await self.app(scope, receive, send)
            return
            
        await self.app(scope, receive, send)


async def get_current_user(
    request: Request,
    credentials: Optional[HTTPAuthorizationCredentials] = Depends(bearer_scheme),
) -> Optional[JWTPayload]:
    """
    FastAPI dependency to get the current authenticated user.
    
    Returns None if:
    - Security is disabled
    - No token is provided
    - Token is invalid
    
    Usage:
        @app.get("/protected")
        async def protected_endpoint(user: JWTPayload = Depends(get_current_user)):
            if user is None:
                raise HTTPException(status_code=401)
            return {"user": user.username}
    """
    if not SECURITY_ENABLED:
        # Return a dummy user when security is disabled
        return JWTPayload(
            user_id="dev-user",
            username="developer",
            role="admin",
            exp=0,
            iat=0,
        )
    
    if credentials is None:
        return None
        
    token = credentials.credentials
    return verify_jwt_token(token)


def require_auth(
    roles: Optional[list[str]] = None,
) -> Callable:
    """
    FastAPI dependency factory that requires authentication.
    
    Args:
        roles: Optional list of required roles (e.g., ["admin", "operator"])
        
    Usage:
        @app.get("/admin-only")
        async def admin_endpoint(user: JWTPayload = Depends(require_auth(roles=["admin"]))):
            return {"message": "Admin access granted"}
            
        @app.get("/authenticated")
        async def auth_endpoint(user: JWTPayload = Depends(require_auth())):
            return {"message": f"Hello {user.username}"}
    """
    async def dependency(
        user: Optional[JWTPayload] = Depends(get_current_user),
    ) -> JWTPayload:
        if user is None:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Authentication required",
                headers={"WWW-Authenticate": "Bearer"},
            )
        
        if roles:
            user_role = user.role.lower()
            allowed_roles = [r.lower() for r in roles]
            
            # Admin has access to everything
            if user_role == "admin":
                return user
                
            if user_role not in allowed_roles:
                raise HTTPException(
                    status_code=status.HTTP_403_FORBIDDEN,
                    detail=f"Requires one of the following roles: {', '.join(roles)}",
                )
        
        return user
    
    return dependency


def require_admin() -> Callable:
    """Shorthand for require_auth(roles=["admin"])"""
    return require_auth(roles=["admin"])


def require_operator() -> Callable:
    """Shorthand for require_auth(roles=["admin", "operator"])"""
    return require_auth(roles=["admin", "operator"])

```

---

## backend/autonomous-crawler-service/src/captcha/__init__.py

```py
"""CAPTCHA solving module using open-source solutions."""

import asyncio
import base64
import os
import tempfile
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum
from typing import Any

import structlog

logger = structlog.get_logger(__name__)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Core Types (defined first to avoid circular imports)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class CaptchaType(str, Enum):
    """Types of CAPTCHAs."""

    RECAPTCHA_V2 = "recaptcha_v2"
    RECAPTCHA_V3 = "recaptcha_v3"
    HCAPTCHA = "hcaptcha"
    IMAGE = "image"
    AUDIO = "audio"
    CLOUDFLARE = "cloudflare"


@dataclass
class CaptchaSolution:
    """Result of CAPTCHA solving attempt."""

    success: bool
    token: str | None = None
    error: str | None = None
    solver_used: str | None = None
    time_ms: float = 0


class CaptchaSolver(ABC):
    """Abstract base class for CAPTCHA solvers."""

    @property
    @abstractmethod
    def name(self) -> str:
        """Solver name."""
        pass

    @abstractmethod
    async def solve(
        self,
        captcha_type: CaptchaType,
        **kwargs,
    ) -> CaptchaSolution:
        """Solve a CAPTCHA."""
        pass

    @abstractmethod
    async def health_check(self) -> bool:
        """Check if solver is available."""
        pass


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Import submodules (after core types are defined)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from src.captcha.stealth import (
    StealthConfig,
    EnhancedStealthConfig,
    apply_stealth_to_playwright_async,
    get_undetected_browser_args,
    get_nopecha_extension_path,
    get_stealth_browser_args_with_extensions,
)
from src.captcha.nopecha import (
    NopeCHAConfig,
    NopeCHAExtensionManager,
    NopeCHAAPI,
    solve_captcha_with_nopecha,
)
from src.captcha.undetected import (
    UndetectedConfig,
    AdvancedStealthPatcher,
    HumanBehaviorSimulator,
    create_undetected_driver,
    get_enhanced_browser_args,
)
from src.captcha.camoufox_driver import (
    CamoufoxConfig,
    CamoufoxHelper,
    create_camoufox_browser,
    create_camoufox_browser_sync,
    get_recommended_camoufox_config,
    is_camoufox_available,
)
from src.captcha.paid_solvers import (
    CapSolverClient,
    CapSolverConfig,
    TwoCaptchaClient,
    TwoCaptchaConfig,
    create_paid_solver,
)

# Re-export for convenience
__all__ = [
    # Core CAPTCHA types and solvers
    "CaptchaType",
    "CaptchaSolution",
    "CaptchaSolver",
    "CaptchaSolverOrchestrator",
    "AudioRecaptchaSolver",
    "HCaptchaChallenger",
    "CloudflareBypasser",
    # Stealth
    "StealthConfig",
    "EnhancedStealthConfig",
    "apply_stealth_to_playwright_async",
    "get_undetected_browser_args",
    "get_nopecha_extension_path",
    "get_stealth_browser_args_with_extensions",
    # NopeCHA
    "NopeCHAConfig",
    "NopeCHAExtensionManager",
    "NopeCHAAPI",
    "solve_captcha_with_nopecha",
    # Undetected ChromeDriver
    "UndetectedConfig",
    "AdvancedStealthPatcher",
    "HumanBehaviorSimulator",
    "create_undetected_driver",
    "get_enhanced_browser_args",
    # Camoufox
    "CamoufoxConfig",
    "CamoufoxHelper",
    "create_camoufox_browser",
    "create_camoufox_browser_sync",
    "get_recommended_camoufox_config",
    "is_camoufox_available",
    # Paid solvers
    "CapSolverClient",
    "CapSolverConfig",
    "TwoCaptchaClient",
    "TwoCaptchaConfig",
    "create_paid_solver",
]


class AudioRecaptchaSolver(CaptchaSolver):
    """
    reCAPTCHA solver using audio challenge (GoogleRecaptchaBypass approach).

    Uses speech recognition to solve audio challenges.
    Requires: speech_recognition, pydub, ffmpeg
    """

    def __init__(self):
        self._sr = None
        self._pydub = None

    @property
    def name(self) -> str:
        return "audio_recaptcha"

    async def _lazy_import(self):
        """Lazy import heavy dependencies."""
        if self._sr is None:
            try:
                import speech_recognition as sr
                from pydub import AudioSegment

                self._sr = sr
                self._pydub = AudioSegment
            except ImportError as e:
                logger.error(
                    "Missing dependencies for audio solver",
                    error=str(e),
                    hint="pip install SpeechRecognition pydub",
                )
                raise

    async def solve(
        self,
        captcha_type: CaptchaType,
        audio_data: bytes | None = None,
        audio_url: str | None = None,
        **kwargs,
    ) -> CaptchaSolution:
        """
        Solve reCAPTCHA using audio challenge.

        Args:
            captcha_type: Must be RECAPTCHA_V2 or AUDIO
            audio_data: Raw audio bytes
            audio_url: URL to download audio from
        """
        import time

        start = time.time()

        if captcha_type not in (CaptchaType.RECAPTCHA_V2, CaptchaType.AUDIO):
            return CaptchaSolution(
                success=False,
                error=f"Unsupported captcha type: {captcha_type}",
                solver_used=self.name,
            )

        try:
            await self._lazy_import()

            # Get audio data
            if audio_url and not audio_data:
                import httpx

                async with httpx.AsyncClient() as client:
                    resp = await client.get(audio_url)
                    audio_data = resp.content

            if not audio_data:
                return CaptchaSolution(
                    success=False,
                    error="No audio data provided",
                    solver_used=self.name,
                )

            # Convert and recognize
            text = await self._recognize_audio(audio_data)

            if text:
                return CaptchaSolution(
                    success=True,
                    token=text,
                    solver_used=self.name,
                    time_ms=(time.time() - start) * 1000,
                )
            else:
                return CaptchaSolution(
                    success=False,
                    error="Could not recognize audio",
                    solver_used=self.name,
                    time_ms=(time.time() - start) * 1000,
                )

        except Exception as e:
            return CaptchaSolution(
                success=False,
                error=str(e),
                solver_used=self.name,
                time_ms=(time.time() - start) * 1000,
            )

    async def _recognize_audio(self, audio_data: bytes) -> str | None:
        """Convert audio to text using speech recognition."""
        with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as f:
            f.write(audio_data)
            mp3_path = f.name

        wav_path = mp3_path.replace(".mp3", ".wav")

        try:
            # Convert MP3 to WAV
            audio = self._pydub.from_mp3(mp3_path)
            audio.export(wav_path, format="wav")

            # Recognize speech
            recognizer = self._sr.Recognizer()
            with self._sr.AudioFile(wav_path) as source:
                audio_data = recognizer.record(source)

            # Try Google Speech Recognition (free)
            try:
                text = recognizer.recognize_google(audio_data)
                return text
            except self._sr.UnknownValueError:
                logger.warning("Google Speech Recognition could not understand audio")
                return None

        finally:
            # Cleanup
            for path in [mp3_path, wav_path]:
                if os.path.exists(path):
                    os.remove(path)

    async def health_check(self) -> bool:
        """Check if dependencies are available."""
        try:
            await self._lazy_import()
            return True
        except Exception:
            return False


class HCaptchaChallenger(CaptchaSolver):
    """
    hCaptcha solver using hcaptcha-challenger library.

    Uses AI models (YOLO, ResNet) to solve image challenges.
    Requires: hcaptcha-challenger
    """

    def __init__(self, model_dir: str | None = None):
        self.model_dir = model_dir
        self._challenger = None

    @property
    def name(self) -> str:
        return "hcaptcha_challenger"

    async def _lazy_import(self):
        """Lazy import hcaptcha-challenger."""
        if self._challenger is None:
            try:
                from hcaptcha_challenger import AgentChallenger

                self._challenger = AgentChallenger
            except ImportError as e:
                logger.error(
                    "Missing hcaptcha-challenger",
                    error=str(e),
                    hint="pip install hcaptcha-challenger",
                )
                raise

    async def solve(
        self,
        captcha_type: CaptchaType,
        page: Any = None,  # Playwright page
        **kwargs,
    ) -> CaptchaSolution:
        """
        Solve hCaptcha on a Playwright page.

        Args:
            captcha_type: Must be HCAPTCHA
            page: Playwright page object
        """
        import time

        start = time.time()

        if captcha_type != CaptchaType.HCAPTCHA:
            return CaptchaSolution(
                success=False,
                error=f"Unsupported captcha type: {captcha_type}",
                solver_used=self.name,
            )

        if not page:
            return CaptchaSolution(
                success=False,
                error="Playwright page required",
                solver_used=self.name,
            )

        try:
            await self._lazy_import()

            challenger = self._challenger(page)
            result = await challenger.solve()

            return CaptchaSolution(
                success=result,
                solver_used=self.name,
                time_ms=(time.time() - start) * 1000,
            )

        except Exception as e:
            return CaptchaSolution(
                success=False,
                error=str(e),
                solver_used=self.name,
                time_ms=(time.time() - start) * 1000,
            )

    async def health_check(self) -> bool:
        """Check if library is available."""
        try:
            await self._lazy_import()
            return True
        except Exception:
            return False


class CloudflareBypasser(CaptchaSolver):
    """
    Cloudflare bypass using cloudscraper library.

    Handles Cloudflare's JavaScript challenges and Turnstile.
    Requires: cloudscraper
    """

    def __init__(self):
        self._scraper = None

    @property
    def name(self) -> str:
        return "cloudscraper"

    def _get_scraper(self):
        """Get or create cloudscraper instance."""
        if self._scraper is None:
            try:
                import cloudscraper

                self._scraper = cloudscraper.create_scraper(
                    browser={
                        "browser": "chrome",
                        "platform": "windows",
                        "mobile": False,
                    },
                    delay=10,
                )
            except ImportError as e:
                logger.error(
                    "Missing cloudscraper",
                    error=str(e),
                    hint="pip install cloudscraper",
                )
                raise
        return self._scraper

    async def solve(
        self,
        captcha_type: CaptchaType,
        url: str | None = None,
        **kwargs,
    ) -> CaptchaSolution:
        """
        Bypass Cloudflare protection.

        Args:
            captcha_type: Must be CLOUDFLARE
            url: URL to access through Cloudflare
        """
        import time

        start = time.time()

        if captcha_type != CaptchaType.CLOUDFLARE:
            return CaptchaSolution(
                success=False,
                error=f"Unsupported captcha type: {captcha_type}",
                solver_used=self.name,
            )

        if not url:
            return CaptchaSolution(
                success=False,
                error="URL required",
                solver_used=self.name,
            )

        try:
            scraper = self._get_scraper()

            # Execute in thread pool since cloudscraper is synchronous
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: scraper.get(url),
            )

            if response.status_code == 200:
                return CaptchaSolution(
                    success=True,
                    token=response.text[:100],  # First 100 chars as verification
                    solver_used=self.name,
                    time_ms=(time.time() - start) * 1000,
                )
            else:
                return CaptchaSolution(
                    success=False,
                    error=f"HTTP {response.status_code}",
                    solver_used=self.name,
                    time_ms=(time.time() - start) * 1000,
                )

        except Exception as e:
            return CaptchaSolution(
                success=False,
                error=str(e),
                solver_used=self.name,
                time_ms=(time.time() - start) * 1000,
            )

    async def get_session_cookies(self, url: str) -> dict[str, str]:
        """Get Cloudflare bypass cookies for a URL."""
        try:
            scraper = self._get_scraper()
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, lambda: scraper.get(url))
            return dict(scraper.cookies)
        except Exception as e:
            logger.error("Failed to get Cloudflare cookies", url=url, error=str(e))
            return {}

    async def health_check(self) -> bool:
        """Check if cloudscraper is available."""
        try:
            self._get_scraper()
            return True
        except Exception:
            return False


class CaptchaSolverOrchestrator:
    """
    Orchestrates multiple CAPTCHA solvers.

    Tries different solvers based on CAPTCHA type and availability.
    Supports both free and paid solvers, with paid solvers prioritized when configured.
    """

    def __init__(
        self,
        capsolver_api_key: str = "",
        twocaptcha_api_key: str = "",
        prefer_paid: bool = True,
        paid_timeout: float = 120.0,
    ):
        """
        Initialize CAPTCHA solver orchestrator.

        Args:
            capsolver_api_key: CapSolver API key (recommended for Turnstile)
            twocaptcha_api_key: 2Captcha API key
            prefer_paid: If True, try paid solvers first when available
            paid_timeout: Timeout for paid solver requests
        """
        self.prefer_paid = prefer_paid

        # Initialize free solvers
        free_solvers: dict[CaptchaType, list[CaptchaSolver]] = {
            CaptchaType.RECAPTCHA_V2: [AudioRecaptchaSolver()],
            CaptchaType.AUDIO: [AudioRecaptchaSolver()],
            CaptchaType.HCAPTCHA: [HCaptchaChallenger()],
            CaptchaType.CLOUDFLARE: [CloudflareBypasser()],
        }

        # Initialize paid solvers if API keys provided
        paid_solvers: dict[CaptchaType, list[CaptchaSolver]] = {}

        if capsolver_api_key:
            capsolver = CapSolverClient(
                CapSolverConfig(api_key=capsolver_api_key, timeout=paid_timeout)
            )
            # CapSolver supports all major CAPTCHA types
            for ctype in [
                CaptchaType.RECAPTCHA_V2,
                CaptchaType.RECAPTCHA_V3,
                CaptchaType.HCAPTCHA,
                CaptchaType.CLOUDFLARE,
            ]:
                if ctype not in paid_solvers:
                    paid_solvers[ctype] = []
                paid_solvers[ctype].append(capsolver)
            logger.info("CapSolver enabled for CAPTCHA solving")

        if twocaptcha_api_key:
            twocaptcha = TwoCaptchaClient(
                TwoCaptchaConfig(api_key=twocaptcha_api_key, timeout=paid_timeout)
            )
            # 2Captcha also supports all major types
            for ctype in [
                CaptchaType.RECAPTCHA_V2,
                CaptchaType.RECAPTCHA_V3,
                CaptchaType.HCAPTCHA,
                CaptchaType.CLOUDFLARE,
            ]:
                if ctype not in paid_solvers:
                    paid_solvers[ctype] = []
                paid_solvers[ctype].append(twocaptcha)
            logger.info("2Captcha enabled for CAPTCHA solving")

        # Combine solvers: paid first if prefer_paid, else free first
        self.solvers: dict[CaptchaType, list[CaptchaSolver]] = {}
        all_types = set(free_solvers.keys()) | set(paid_solvers.keys())

        for ctype in all_types:
            paid = paid_solvers.get(ctype, [])
            free = free_solvers.get(ctype, [])

            if prefer_paid:
                self.solvers[ctype] = paid + free
            else:
                self.solvers[ctype] = free + paid

        logger.info(
            "CAPTCHA solver orchestrator initialized",
            paid_solvers_enabled=bool(capsolver_api_key or twocaptcha_api_key),
            prefer_paid=prefer_paid,
            supported_types=list(self.solvers.keys()),
        )

    def add_solver(self, captcha_type: CaptchaType, solver: CaptchaSolver):
        """Add a solver for a captcha type."""
        if captcha_type not in self.solvers:
            self.solvers[captcha_type] = []
        self.solvers[captcha_type].append(solver)

    async def solve(
        self,
        captcha_type: CaptchaType,
        **kwargs,
    ) -> CaptchaSolution:
        """
        Try to solve CAPTCHA using available solvers.

        Args:
            captcha_type: Type of CAPTCHA
            **kwargs: Solver-specific arguments

        Returns:
            CaptchaSolution with result
        """
        solvers = self.solvers.get(captcha_type, [])

        if not solvers:
            return CaptchaSolution(
                success=False,
                error=f"No solver available for {captcha_type}",
            )

        errors = []
        for solver in solvers:
            try:
                # Check health first
                if not await solver.health_check():
                    errors.append(f"{solver.name}: not available")
                    continue

                result = await solver.solve(captcha_type, **kwargs)

                if result.success:
                    logger.info(
                        "CAPTCHA solved",
                        captcha_type=captcha_type.value,
                        solver=solver.name,
                        time_ms=result.time_ms,
                    )
                    return result
                else:
                    errors.append(f"{solver.name}: {result.error}")

            except Exception as e:
                errors.append(f"{solver.name}: {str(e)}")

        return CaptchaSolution(
            success=False,
            error=f"All solvers failed: {'; '.join(errors)}",
        )

    async def get_available_solvers(self) -> dict[str, list[str]]:
        """Get list of available solvers by captcha type."""
        available = {}
        for captcha_type, solvers in self.solvers.items():
            available[captcha_type.value] = []
            for solver in solvers:
                if await solver.health_check():
                    available[captcha_type.value].append(solver.name)
        return available

```

---

## backend/autonomous-crawler-service/src/captcha/camoufox_driver.py

```py
"""
Camoufox Firefox-based Anti-Detect Browser Integration.

Camoufox is a Firefox-based anti-detect browser that provides:
- Advanced fingerprint spoofing
- Human-like behavior simulation
- Cloudflare Turnstile bypass
- Compatible with Playwright API

Installation:
    pip install camoufox[geoip]
    python -m camoufox fetch
"""

import asyncio
from dataclasses import dataclass, field
from typing import Any

import structlog

logger = structlog.get_logger(__name__)


@dataclass
class CamoufoxConfig:
    """Configuration for Camoufox browser."""
    
    # Display mode
    headless: bool = True
    
    # Human-like behavior simulation
    humanize: bool = True
    humanize_level: int = 2  # 1-3, higher = more human-like
    
    # Fingerprint options
    os: str | None = None  # "windows", "macos", "linux" or None for random
    screen_width: int | None = None
    screen_height: int | None = None
    
    # Locale/timezone
    locale: str = "ko-KR"
    timezone: str = "Asia/Seoul"
    
    # Geolocation (requires geoip addon)
    geoip: bool = True
    
    # Proxy
    proxy: str | None = None
    
    # Browser settings
    block_images: bool = False
    block_webrtc: bool = True
    
    # Extra Firefox preferences
    firefox_prefs: dict[str, Any] = field(default_factory=dict)


def is_camoufox_available() -> bool:
    """Check if Camoufox is installed and available."""
    try:
        import camoufox
        return True
    except ImportError:
        return False


async def create_camoufox_browser(
    config: CamoufoxConfig | None = None,
) -> Any:
    """
    Create a Camoufox browser instance using async API.
    
    Args:
        config: Camoufox configuration
        
    Returns:
        Browser context or None if not available
    """
    if config is None:
        config = CamoufoxConfig()
    
    if not is_camoufox_available():
        logger.warning("Camoufox not installed. Install with: pip install camoufox[geoip]")
        return None
    
    try:
        from camoufox.async_api import AsyncCamoufox
        
        # Build kwargs
        kwargs = {
            "headless": config.headless,
            "humanize": config.humanize,
        }
        
        if config.os:
            kwargs["os"] = config.os
        
        if config.screen_width and config.screen_height:
            kwargs["screen"] = {"width": config.screen_width, "height": config.screen_height}
        
        if config.locale:
            kwargs["locale"] = config.locale
            
        if config.timezone:
            kwargs["timezone"] = config.timezone
        
        if config.geoip:
            kwargs["geoip"] = True
        
        if config.proxy:
            kwargs["proxy"] = {"server": config.proxy}
        
        if config.block_webrtc:
            kwargs["block_webrtc"] = True
        
        if config.block_images:
            kwargs["block_images"] = True
        
        # Apply extra Firefox preferences
        if config.firefox_prefs:
            kwargs["firefox_prefs"] = config.firefox_prefs
        
        # Create browser
        camoufox = AsyncCamoufox(**kwargs)
        browser = await camoufox.__aenter__()
        
        logger.info("Created Camoufox browser", headless=config.headless, humanize=config.humanize)
        return browser
        
    except Exception as e:
        logger.error("Failed to create Camoufox browser", error=str(e))
        return None


def create_camoufox_browser_sync(
    config: CamoufoxConfig | None = None,
) -> Any:
    """
    Create a Camoufox browser instance using sync API.
    
    Args:
        config: Camoufox configuration
        
    Returns:
        Browser context or None if not available
    """
    if config is None:
        config = CamoufoxConfig()
    
    if not is_camoufox_available():
        logger.warning("Camoufox not installed")
        return None
    
    try:
        from camoufox.sync_api import Camoufox
        
        kwargs = {
            "headless": config.headless,
            "humanize": config.humanize,
        }
        
        if config.os:
            kwargs["os"] = config.os
        
        if config.locale:
            kwargs["locale"] = config.locale
            
        if config.timezone:
            kwargs["timezone"] = config.timezone
        
        if config.geoip:
            kwargs["geoip"] = True
        
        if config.proxy:
            kwargs["proxy"] = {"server": config.proxy}
        
        if config.block_webrtc:
            kwargs["block_webrtc"] = True
        
        camoufox = Camoufox(**kwargs)
        browser = camoufox.__enter__()
        
        logger.info("Created Camoufox browser (sync)", headless=config.headless)
        return browser
        
    except Exception as e:
        logger.error("Failed to create Camoufox browser", error=str(e))
        return None


class CamoufoxHelper:
    """Helper utilities for Camoufox browser automation."""
    
    @staticmethod
    async def wait_for_cloudflare(page: Any, timeout: int = 30) -> bool:
        """
        Wait for Cloudflare challenge to complete.
        
        Camoufox handles Cloudflare automatically in most cases,
        but this provides explicit waiting if needed.
        
        Args:
            page: Camoufox page object
            timeout: Maximum wait time in seconds
            
        Returns:
            True if challenge passed, False if timeout
        """
        try:
            # Common Cloudflare challenge indicators
            challenge_selectors = [
                "#challenge-running",
                "#challenge-stage",
                ".cf-browser-verification",
                "#trk_jschal_js",
            ]
            
            start_time = asyncio.get_event_loop().time()
            
            while asyncio.get_event_loop().time() - start_time < timeout:
                # Check if any challenge elements are visible
                is_challenging = False
                
                for selector in challenge_selectors:
                    try:
                        element = await page.query_selector(selector)
                        if element:
                            is_visible = await element.is_visible()
                            if is_visible:
                                is_challenging = True
                                break
                    except Exception:
                        continue
                
                if not is_challenging:
                    # No challenge visible, likely passed
                    logger.debug("Cloudflare challenge completed")
                    return True
                
                await asyncio.sleep(0.5)
            
            logger.warning("Cloudflare challenge timeout")
            return False
            
        except Exception as e:
            logger.error("Error waiting for Cloudflare", error=str(e))
            return False
    
    @staticmethod
    async def solve_turnstile(page: Any, timeout: int = 30) -> bool:
        """
        Wait for Cloudflare Turnstile CAPTCHA to complete.
        
        Camoufox with humanize=True should handle most Turnstile
        challenges automatically.
        
        Args:
            page: Camoufox page object
            timeout: Maximum wait time in seconds
            
        Returns:
            True if solved, False if timeout
        """
        try:
            turnstile_selectors = [
                "iframe[src*='turnstile']",
                "#cf-turnstile",
                ".cf-turnstile",
            ]
            
            start_time = asyncio.get_event_loop().time()
            
            # First, wait for turnstile to appear
            turnstile_frame = None
            while asyncio.get_event_loop().time() - start_time < timeout / 2:
                for selector in turnstile_selectors:
                    try:
                        element = await page.query_selector(selector)
                        if element:
                            turnstile_frame = element
                            break
                    except Exception:
                        continue
                
                if turnstile_frame:
                    break
                    
                await asyncio.sleep(0.3)
            
            if not turnstile_frame:
                # No turnstile found, may not be needed
                logger.debug("No Turnstile CAPTCHA found")
                return True
            
            logger.debug("Turnstile CAPTCHA detected, waiting for auto-solve...")
            
            # Wait for turnstile to complete (it should auto-solve with humanize)
            # Check for success indicator or turnstile disappearing
            while asyncio.get_event_loop().time() - start_time < timeout:
                # Check if turnstile is still visible
                try:
                    is_visible = await turnstile_frame.is_visible()
                    if not is_visible:
                        logger.info("Turnstile CAPTCHA solved")
                        return True
                except Exception:
                    # Element may have been removed
                    return True
                
                # Check for success response in page
                try:
                    response = await page.evaluate("""
                        () => {
                            const input = document.querySelector('[name="cf-turnstile-response"]');
                            return input ? input.value : null;
                        }
                    """)
                    if response:
                        logger.info("Turnstile response received")
                        return True
                except Exception:
                    pass
                
                await asyncio.sleep(0.5)
            
            logger.warning("Turnstile solve timeout")
            return False
            
        except Exception as e:
            logger.error("Error solving Turnstile", error=str(e))
            return False
    
    @staticmethod
    async def extract_page_content(page: Any) -> dict[str, Any]:
        """
        Extract main content from a page.
        
        Args:
            page: Camoufox page object
            
        Returns:
            Dictionary with extracted content
        """
        try:
            content = await page.evaluate("""
                () => {
                    const result = {
                        title: document.title,
                        url: window.location.href,
                        text: '',
                        links: [],
                        images: [],
                    };
                    
                    // Get main text content
                    const article = document.querySelector('article') || 
                                   document.querySelector('main') || 
                                   document.body;
                    
                    if (article) {
                        result.text = article.innerText;
                    }
                    
                    // Get links
                    const links = document.querySelectorAll('a[href]');
                    links.forEach(link => {
                        if (link.href && link.href.startsWith('http')) {
                            result.links.push({
                                href: link.href,
                                text: link.innerText.trim().substring(0, 200)
                            });
                        }
                    });
                    
                    // Get images
                    const images = document.querySelectorAll('img[src]');
                    images.forEach(img => {
                        if (img.src && img.src.startsWith('http')) {
                            result.images.push({
                                src: img.src,
                                alt: img.alt || ''
                            });
                        }
                    });
                    
                    return result;
                }
            """)
            
            return content
            
        except Exception as e:
            logger.error("Failed to extract page content", error=str(e))
            return {"error": str(e)}


# Firefox-specific preferences for anti-detection
FIREFOX_ANTI_DETECT_PREFS = {
    # Disable WebRTC IP leak
    "media.peerconnection.enabled": False,
    "media.peerconnection.ice.no_host": True,
    "media.peerconnection.ice.default_address_only": True,
    
    # Disable tracking
    "privacy.trackingprotection.enabled": True,
    "privacy.trackingprotection.socialtracking.enabled": True,
    
    # Fingerprint resistance
    "privacy.resistFingerprinting": False,  # Camoufox handles this
    
    # Disable telemetry
    "toolkit.telemetry.enabled": False,
    "toolkit.telemetry.unified": False,
    "toolkit.telemetry.archive.enabled": False,
    
    # Disable crash reporter
    "breakpad.reportURL": "",
    "browser.crashReports.unsubmittedCheck.autoSubmit2": False,
    
    # Disable prefetch
    "network.prefetch-next": False,
    "network.dns.disablePrefetch": True,
    
    # Disable speculative connections
    "network.http.speculative-parallel-limit": 0,
    
    # Improve privacy
    "dom.battery.enabled": False,
    "geo.enabled": False,
    "media.navigator.enabled": False,
    
    # Performance
    "browser.cache.disk.enable": False,
    "browser.cache.memory.enable": True,
}


def get_recommended_camoufox_config(
    purpose: str = "general",
    headless: bool = True,
) -> CamoufoxConfig:
    """
    Get recommended Camoufox configuration for different purposes.
    
    Args:
        purpose: "general", "scraping", "turnstile", "cloudflare"
        headless: Whether to run headless
        
    Returns:
        Optimized CamoufoxConfig
    """
    base_config = CamoufoxConfig(
        headless=headless,
        humanize=True,
        humanize_level=2,
        locale="ko-KR",
        timezone="Asia/Seoul",
        geoip=True,
        block_webrtc=True,
    )
    
    if purpose == "scraping":
        base_config.block_images = True
        base_config.humanize_level = 1
    
    elif purpose == "turnstile":
        base_config.humanize_level = 3
        base_config.block_images = False
    
    elif purpose == "cloudflare":
        base_config.humanize_level = 3
        base_config.block_images = False
        base_config.firefox_prefs = FIREFOX_ANTI_DETECT_PREFS
    
    return base_config

```

---

## backend/autonomous-crawler-service/src/captcha/nopecha.py

```py
"""
NopeCHA CAPTCHA Solver Integration.

NopeCHA is a free/open-source CAPTCHA solving extension that supports:
- reCAPTCHA v2/v3
- hCaptcha
- FunCAPTCHA
- AWS WAF
- Turnstile
- Text CAPTCHA

Usage:
1. Browser extension (Chrome/Firefox)
2. API integration for headless automation
"""

import asyncio
import base64
import json
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Literal

import aiohttp
import structlog

logger = structlog.get_logger(__name__)

# NopeCHA Chrome Extension ID
NOPECHA_EXTENSION_ID = "dknlfmjaanfblgfdfebhijalfmhmjjjo"
NOPECHA_CRX_URL = f"https://clients2.google.com/service/update2/crx?response=redirect&prodversion=133&acceptformat=crx3&x=id%3D{NOPECHA_EXTENSION_ID}%26uc"


@dataclass
class NopeCHAConfig:
    """NopeCHA configuration."""
    
    # API key (optional, for faster solving via API)
    api_key: str = ""
    
    # Extension settings
    enabled: bool = True
    auto_solve: bool = True
    
    # Solve settings
    solve_delay_ms: int = 500
    max_retries: int = 3
    
    # Supported CAPTCHA types
    solve_recaptcha: bool = True
    solve_hcaptcha: bool = True
    solve_funcaptcha: bool = True
    solve_turnstile: bool = True
    solve_text: bool = True
    
    # Audio solving for accessibility (free reCAPTCHA bypass)
    use_audio_challenge: bool = True
    
    # Extension cache directory
    cache_dir: Path = field(default_factory=lambda: Path("/tmp/nopecha-extension"))


class NopeCHAExtensionManager:
    """Manage NopeCHA extension installation and configuration."""
    
    def __init__(self, config: NopeCHAConfig | None = None):
        self.config = config or NopeCHAConfig()
    
    async def download_extension(self) -> Path:
        """Download NopeCHA extension CRX file."""
        cache_dir = self.config.cache_dir
        cache_dir.mkdir(parents=True, exist_ok=True)
        
        crx_path = cache_dir / f"{NOPECHA_EXTENSION_ID}.crx"
        ext_dir = cache_dir / NOPECHA_EXTENSION_ID
        
        # Check if already downloaded and extracted
        if ext_dir.exists() and (ext_dir / "manifest.json").exists():
            logger.debug("NopeCHA extension already cached", path=str(ext_dir))
            return ext_dir
        
        # Download CRX
        logger.info("Downloading NopeCHA extension...")
        async with aiohttp.ClientSession() as session:
            async with session.get(NOPECHA_CRX_URL, allow_redirects=True) as resp:
                if resp.status == 200:
                    content = await resp.read()
                    with open(crx_path, 'wb') as f:
                        f.write(content)
                    logger.info("NopeCHA extension downloaded", path=str(crx_path))
                else:
                    raise Exception(f"Failed to download NopeCHA: HTTP {resp.status}")
        
        # Extract CRX
        await self._extract_crx(crx_path, ext_dir)
        
        # Apply configuration
        await self._configure_extension(ext_dir)
        
        return ext_dir
    
    async def _extract_crx(self, crx_path: Path, extract_dir: Path) -> None:
        """Extract CRX file to directory."""
        import zipfile
        import shutil
        
        if extract_dir.exists():
            shutil.rmtree(extract_dir)
        extract_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            with zipfile.ZipFile(crx_path, 'r') as zip_ref:
                zip_ref.extractall(extract_dir)
        except zipfile.BadZipFile:
            # CRX has a header, skip it
            with open(crx_path, 'rb') as f:
                magic = f.read(4)
                if magic != b'Cr24':
                    raise Exception("Invalid CRX format")
                
                version = int.from_bytes(f.read(4), 'little')
                if version == 2:
                    pubkey_len = int.from_bytes(f.read(4), 'little')
                    sig_len = int.from_bytes(f.read(4), 'little')
                    f.seek(16 + pubkey_len + sig_len)
                elif version == 3:
                    header_len = int.from_bytes(f.read(4), 'little')
                    f.seek(12 + header_len)
                
                zip_data = f.read()
            
            import tempfile
            with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as tmp:
                tmp.write(zip_data)
                tmp.flush()
                with zipfile.ZipFile(tmp.name, 'r') as zip_ref:
                    zip_ref.extractall(extract_dir)
                Path(tmp.name).unlink()
        
        logger.info("NopeCHA extension extracted", path=str(extract_dir))
    
    async def _configure_extension(self, ext_dir: Path) -> None:
        """Configure NopeCHA extension settings."""
        # Create settings file for extension
        settings = {
            "key": self.config.api_key,
            "enabled": self.config.enabled,
            "auto_solve": self.config.auto_solve,
            "delay": self.config.solve_delay_ms,
            "recaptcha": self.config.solve_recaptcha,
            "hcaptcha": self.config.solve_hcaptcha,
            "funcaptcha": self.config.solve_funcaptcha,
            "turnstile": self.config.solve_turnstile,
            "textcaptcha": self.config.solve_text,
            "audio": self.config.use_audio_challenge,
        }
        
        settings_path = ext_dir / "settings.json"
        with open(settings_path, 'w') as f:
            json.dump(settings, f)
        
        logger.debug("NopeCHA settings configured", settings=settings)
    
    def get_extension_path(self) -> str:
        """Get the path to the extracted extension directory."""
        ext_dir = self.config.cache_dir / NOPECHA_EXTENSION_ID
        if ext_dir.exists():
            return str(ext_dir)
        return ""


class NopeCHAAPI:
    """NopeCHA API client for programmatic CAPTCHA solving."""
    
    API_BASE = "https://api.nopecha.com"
    
    def __init__(self, api_key: str = ""):
        self.api_key = api_key
    
    async def solve_recaptcha(
        self,
        site_key: str,
        site_url: str,
        version: Literal["v2", "v3"] = "v2",
        action: str = "",
        invisible: bool = False,
    ) -> str | None:
        """
        Solve reCAPTCHA using NopeCHA API.
        
        Args:
            site_key: reCAPTCHA site key
            site_url: URL of the page with CAPTCHA
            version: reCAPTCHA version (v2 or v3)
            action: Action for v3 scoring
            invisible: Whether CAPTCHA is invisible
            
        Returns:
            Solution token or None if failed
        """
        if not self.api_key:
            logger.warning("NopeCHA API key not configured, using extension mode")
            return None
        
        payload = {
            "key": self.api_key,
            "type": "recaptcha2" if version == "v2" else "recaptcha3",
            "sitekey": site_key,
            "url": site_url,
        }
        
        if version == "v3" and action:
            payload["action"] = action
        if invisible:
            payload["invisible"] = True
        
        return await self._solve(payload)
    
    async def solve_hcaptcha(
        self,
        site_key: str,
        site_url: str,
    ) -> str | None:
        """Solve hCaptcha using NopeCHA API."""
        if not self.api_key:
            return None
        
        payload = {
            "key": self.api_key,
            "type": "hcaptcha",
            "sitekey": site_key,
            "url": site_url,
        }
        
        return await self._solve(payload)
    
    async def solve_turnstile(
        self,
        site_key: str,
        site_url: str,
    ) -> str | None:
        """Solve Cloudflare Turnstile using NopeCHA API."""
        if not self.api_key:
            return None
        
        payload = {
            "key": self.api_key,
            "type": "turnstile",
            "sitekey": site_key,
            "url": site_url,
        }
        
        return await self._solve(payload)
    
    async def solve_image_captcha(
        self,
        image_base64: str,
        captcha_type: str = "text",
    ) -> str | None:
        """Solve image-based CAPTCHA."""
        if not self.api_key:
            return None
        
        payload = {
            "key": self.api_key,
            "type": captcha_type,
            "image": image_base64,
        }
        
        return await self._solve(payload)
    
    async def _solve(self, payload: dict[str, Any]) -> str | None:
        """Send solve request to NopeCHA API."""
        try:
            async with aiohttp.ClientSession() as session:
                # Create task
                async with session.post(
                    f"{self.API_BASE}/",
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=30),
                ) as resp:
                    result = await resp.json()
                    
                    if "error" in result:
                        logger.error("NopeCHA API error", error=result.get("error"))
                        return None
                    
                    task_id = result.get("data")
                    if not task_id:
                        return None
                
                # Poll for result
                for _ in range(60):  # Max 60 seconds
                    await asyncio.sleep(1)
                    
                    async with session.get(
                        f"{self.API_BASE}/?key={self.api_key}&id={task_id}",
                        timeout=aiohttp.ClientTimeout(total=10),
                    ) as poll_resp:
                        poll_result = await poll_resp.json()
                        
                        if "error" in poll_result:
                            error = poll_result.get("error")
                            if error == "Incomplete job":
                                continue
                            logger.error("NopeCHA polling error", error=error)
                            return None
                        
                        if "data" in poll_result:
                            return poll_result["data"]
                
                logger.warning("NopeCHA solve timeout")
                return None
                
        except Exception as e:
            logger.error("NopeCHA API request failed", error=str(e))
            return None


# Helper function for quick CAPTCHA solving
async def solve_captcha_with_nopecha(
    captcha_type: Literal["recaptcha2", "recaptcha3", "hcaptcha", "turnstile"],
    site_key: str,
    site_url: str,
    api_key: str = "",
) -> str | None:
    """
    Quick helper to solve CAPTCHA using NopeCHA.
    
    Args:
        captcha_type: Type of CAPTCHA
        site_key: CAPTCHA site key
        site_url: URL of the page
        api_key: NopeCHA API key (optional)
        
    Returns:
        Solution token or None
    """
    client = NopeCHAAPI(api_key)
    
    if captcha_type == "recaptcha2":
        return await client.solve_recaptcha(site_key, site_url, "v2")
    elif captcha_type == "recaptcha3":
        return await client.solve_recaptcha(site_key, site_url, "v3")
    elif captcha_type == "hcaptcha":
        return await client.solve_hcaptcha(site_key, site_url)
    elif captcha_type == "turnstile":
        return await client.solve_turnstile(site_key, site_url)
    
    return None

```

---

## backend/autonomous-crawler-service/src/captcha/paid_solvers.py

```py
"""
Paid CAPTCHA Solver Integrations.

Integrates with reliable paid CAPTCHA solving services:
- CapSolver (https://capsolver.com) - Recommended, supports Turnstile
- 2Captcha (https://2captcha.com) - Widely used, reliable

These services are more reliable than free solutions for:
- reCAPTCHA v2/v3
- hCaptcha
- Cloudflare Turnstile
- FunCAPTCHA
- Image CAPTCHAs
"""

import asyncio
import time
from dataclasses import dataclass
from typing import Any, Literal

import httpx
import structlog

from src.captcha import CaptchaType, CaptchaSolution, CaptchaSolver

logger = structlog.get_logger(__name__)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CapSolver Integration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@dataclass
class CapSolverConfig:
    """CapSolver configuration."""

    api_key: str = ""
    base_url: str = "https://api.capsolver.com"
    timeout: float = 120.0
    poll_interval: float = 3.0


class CapSolverClient(CaptchaSolver):
    """
    CapSolver CAPTCHA solving service client.

    Supports:
    - reCAPTCHA v2/v3
    - hCaptcha
    - Cloudflare Turnstile
    - FunCAPTCHA
    - Image CAPTCHA

    Docs: https://docs.capsolver.com/
    """

    def __init__(self, config: CapSolverConfig | None = None):
        self.config = config or CapSolverConfig()
        self._client: httpx.AsyncClient | None = None

    @property
    def name(self) -> str:
        return "capsolver"

    async def _get_client(self) -> httpx.AsyncClient:
        if self._client is None:
            self._client = httpx.AsyncClient(
                base_url=self.config.base_url,
                timeout=self.config.timeout,
            )
        return self._client

    async def health_check(self) -> bool:
        """Check if CapSolver API is available and key is valid."""
        if not self.config.api_key:
            return False

        try:
            client = await self._get_client()
            resp = await client.post(
                "/getBalance",
                json={"clientKey": self.config.api_key},
            )
            data = resp.json()
            if data.get("errorId") == 0:
                balance = data.get("balance", 0)
                logger.debug("CapSolver balance check", balance=balance)
                return balance > 0
            return False
        except Exception as e:
            logger.debug("CapSolver health check failed", error=str(e))
            return False

    async def solve(
        self,
        captcha_type: CaptchaType,
        site_key: str | None = None,
        site_url: str | None = None,
        page: Any = None,
        **kwargs,
    ) -> CaptchaSolution:
        """
        Solve CAPTCHA using CapSolver API.

        Args:
            captcha_type: Type of CAPTCHA
            site_key: Site key for reCAPTCHA/hCaptcha
            site_url: URL of the page with CAPTCHA
            page: Playwright page (for extracting site_key if not provided)
        """
        start_time = time.time()

        if not self.config.api_key:
            return CaptchaSolution(
                success=False,
                error="CapSolver API key not configured",
                solver_used=self.name,
            )

        try:
            # Extract site_key from page if not provided
            if not site_key and page:
                site_key = await self._extract_site_key(page, captcha_type)
                site_url = page.url

            if not site_key:
                return CaptchaSolution(
                    success=False,
                    error="Site key not provided or could not be extracted",
                    solver_used=self.name,
                )

            # Map CAPTCHA type to CapSolver task type
            task_type = self._get_task_type(captcha_type)
            if not task_type:
                return CaptchaSolution(
                    success=False,
                    error=f"Unsupported CAPTCHA type: {captcha_type}",
                    solver_used=self.name,
                )

            # Create task
            task_data = {
                "type": task_type,
                "websiteURL": site_url,
                "websiteKey": site_key,
            }

            # Add type-specific parameters
            if captcha_type == CaptchaType.RECAPTCHA_V3:
                task_data["pageAction"] = kwargs.get("action", "verify")
                task_data["minScore"] = kwargs.get("min_score", 0.7)

            client = await self._get_client()

            # Create task
            create_resp = await client.post(
                "/createTask",
                json={
                    "clientKey": self.config.api_key,
                    "task": task_data,
                },
            )
            create_data = create_resp.json()

            if create_data.get("errorId") != 0:
                return CaptchaSolution(
                    success=False,
                    error=create_data.get("errorDescription", "Unknown error"),
                    solver_used=self.name,
                    time_ms=(time.time() - start_time) * 1000,
                )

            task_id = create_data.get("taskId")
            if not task_id:
                return CaptchaSolution(
                    success=False,
                    error="No task ID returned",
                    solver_used=self.name,
                    time_ms=(time.time() - start_time) * 1000,
                )

            # Poll for result
            token = await self._poll_result(task_id)

            if token:
                return CaptchaSolution(
                    success=True,
                    token=token,
                    solver_used=self.name,
                    time_ms=(time.time() - start_time) * 1000,
                )
            else:
                return CaptchaSolution(
                    success=False,
                    error="Failed to get solution within timeout",
                    solver_used=self.name,
                    time_ms=(time.time() - start_time) * 1000,
                )

        except Exception as e:
            logger.error("CapSolver error", error=str(e))
            return CaptchaSolution(
                success=False,
                error=str(e),
                solver_used=self.name,
                time_ms=(time.time() - start_time) * 1000,
            )

    async def _poll_result(self, task_id: str) -> str | None:
        """Poll for task result."""
        client = await self._get_client()
        max_attempts = int(self.config.timeout / self.config.poll_interval)

        for _ in range(max_attempts):
            await asyncio.sleep(self.config.poll_interval)

            resp = await client.post(
                "/getTaskResult",
                json={
                    "clientKey": self.config.api_key,
                    "taskId": task_id,
                },
            )
            data = resp.json()

            if data.get("errorId") != 0:
                logger.warning("CapSolver poll error", error=data.get("errorDescription"))
                return None

            status = data.get("status")
            if status == "ready":
                solution = data.get("solution", {})
                # Different CAPTCHA types return token in different fields
                return (
                    solution.get("gRecaptchaResponse")
                    or solution.get("token")
                    or solution.get("text")
                )
            elif status == "failed":
                logger.warning("CapSolver task failed", data=data)
                return None

        return None

    def _get_task_type(self, captcha_type: CaptchaType) -> str | None:
        """Map CaptchaType to CapSolver task type."""
        mapping = {
            CaptchaType.RECAPTCHA_V2: "ReCaptchaV2TaskProxyLess",
            CaptchaType.RECAPTCHA_V3: "ReCaptchaV3TaskProxyLess",
            CaptchaType.HCAPTCHA: "HCaptchaTaskProxyLess",
            CaptchaType.CLOUDFLARE: "AntiTurnstileTaskProxyLess",
        }
        return mapping.get(captcha_type)

    async def _extract_site_key(self, page: Any, captcha_type: CaptchaType) -> str | None:
        """Extract site key from page."""
        try:
            if captcha_type in (CaptchaType.RECAPTCHA_V2, CaptchaType.RECAPTCHA_V3):
                # reCAPTCHA site key extraction
                selectors = [
                    '[data-sitekey]',
                    '.g-recaptcha[data-sitekey]',
                    '#recaptcha[data-sitekey]',
                ]
                for selector in selectors:
                    element = await page.query_selector(selector)
                    if element:
                        return await element.get_attribute("data-sitekey")

                # Try script-based extraction
                site_key = await page.evaluate("""
                    () => {
                        const scripts = document.querySelectorAll('script');
                        for (const script of scripts) {
                            const match = script.src?.match(/render=([^&]+)/);
                            if (match) return match[1];
                        }
                        return window.___grecaptcha_cfg?.clients?.[0]?.N?.sitekey || null;
                    }
                """)
                if site_key:
                    return site_key

            elif captcha_type == CaptchaType.HCAPTCHA:
                element = await page.query_selector('[data-sitekey], .h-captcha[data-sitekey]')
                if element:
                    return await element.get_attribute("data-sitekey")

            elif captcha_type == CaptchaType.CLOUDFLARE:
                # Turnstile site key
                element = await page.query_selector('[data-sitekey], .cf-turnstile[data-sitekey]')
                if element:
                    return await element.get_attribute("data-sitekey")

        except Exception as e:
            logger.debug("Failed to extract site key", error=str(e))

        return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2Captcha Integration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@dataclass
class TwoCaptchaConfig:
    """2Captcha configuration."""

    api_key: str = ""
    base_url: str = "https://2captcha.com"
    timeout: float = 120.0
    poll_interval: float = 5.0


class TwoCaptchaClient(CaptchaSolver):
    """
    2Captcha CAPTCHA solving service client.

    Supports:
    - reCAPTCHA v2/v3
    - hCaptcha
    - Cloudflare Turnstile
    - FunCAPTCHA
    - Image CAPTCHA

    Docs: https://2captcha.com/api-docs
    """

    def __init__(self, config: TwoCaptchaConfig | None = None):
        self.config = config or TwoCaptchaConfig()

    @property
    def name(self) -> str:
        return "2captcha"

    async def health_check(self) -> bool:
        """Check if 2Captcha API is available and key is valid."""
        if not self.config.api_key:
            return False

        try:
            async with httpx.AsyncClient() as client:
                resp = await client.get(
                    f"{self.config.base_url}/res.php",
                    params={
                        "key": self.config.api_key,
                        "action": "getbalance",
                        "json": 1,
                    },
                )
                data = resp.json()
                if data.get("status") == 1:
                    balance = float(data.get("request", 0))
                    logger.debug("2Captcha balance check", balance=balance)
                    return balance > 0
                return False
        except Exception as e:
            logger.debug("2Captcha health check failed", error=str(e))
            return False

    async def solve(
        self,
        captcha_type: CaptchaType,
        site_key: str | None = None,
        site_url: str | None = None,
        page: Any = None,
        **kwargs,
    ) -> CaptchaSolution:
        """
        Solve CAPTCHA using 2Captcha API.

        Args:
            captcha_type: Type of CAPTCHA
            site_key: Site key for reCAPTCHA/hCaptcha
            site_url: URL of the page with CAPTCHA
            page: Playwright page (for extracting site_key if not provided)
        """
        start_time = time.time()

        if not self.config.api_key:
            return CaptchaSolution(
                success=False,
                error="2Captcha API key not configured",
                solver_used=self.name,
            )

        try:
            # Extract site_key from page if not provided
            if not site_key and page:
                site_key = await self._extract_site_key(page, captcha_type)
                site_url = page.url

            if not site_key:
                return CaptchaSolution(
                    success=False,
                    error="Site key not provided or could not be extracted",
                    solver_used=self.name,
                )

            # Build request parameters
            params = {
                "key": self.config.api_key,
                "json": 1,
                "pageurl": site_url,
            }

            # Add type-specific parameters
            if captcha_type in (CaptchaType.RECAPTCHA_V2, CaptchaType.RECAPTCHA_V3):
                params["method"] = "userrecaptcha"
                params["googlekey"] = site_key
                if captcha_type == CaptchaType.RECAPTCHA_V3:
                    params["version"] = "v3"
                    params["action"] = kwargs.get("action", "verify")
                    params["min_score"] = kwargs.get("min_score", 0.7)

            elif captcha_type == CaptchaType.HCAPTCHA:
                params["method"] = "hcaptcha"
                params["sitekey"] = site_key

            elif captcha_type == CaptchaType.CLOUDFLARE:
                params["method"] = "turnstile"
                params["sitekey"] = site_key

            else:
                return CaptchaSolution(
                    success=False,
                    error=f"Unsupported CAPTCHA type: {captcha_type}",
                    solver_used=self.name,
                )

            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                # Submit task
                submit_resp = await client.get(
                    f"{self.config.base_url}/in.php",
                    params=params,
                )
                submit_data = submit_resp.json()

                if submit_data.get("status") != 1:
                    return CaptchaSolution(
                        success=False,
                        error=submit_data.get("request", "Unknown error"),
                        solver_used=self.name,
                        time_ms=(time.time() - start_time) * 1000,
                    )

                task_id = submit_data.get("request")

                # Poll for result
                token = await self._poll_result(client, task_id)

                if token:
                    return CaptchaSolution(
                        success=True,
                        token=token,
                        solver_used=self.name,
                        time_ms=(time.time() - start_time) * 1000,
                    )
                else:
                    return CaptchaSolution(
                        success=False,
                        error="Failed to get solution within timeout",
                        solver_used=self.name,
                        time_ms=(time.time() - start_time) * 1000,
                    )

        except Exception as e:
            logger.error("2Captcha error", error=str(e))
            return CaptchaSolution(
                success=False,
                error=str(e),
                solver_used=self.name,
                time_ms=(time.time() - start_time) * 1000,
            )

    async def _poll_result(self, client: httpx.AsyncClient, task_id: str) -> str | None:
        """Poll for task result."""
        max_attempts = int(self.config.timeout / self.config.poll_interval)

        for _ in range(max_attempts):
            await asyncio.sleep(self.config.poll_interval)

            resp = await client.get(
                f"{self.config.base_url}/res.php",
                params={
                    "key": self.config.api_key,
                    "action": "get",
                    "id": task_id,
                    "json": 1,
                },
            )
            data = resp.json()

            if data.get("status") == 1:
                return data.get("request")
            elif data.get("request") == "CAPCHA_NOT_READY":
                continue
            else:
                logger.warning("2Captcha poll error", error=data.get("request"))
                return None

        return None

    async def _extract_site_key(self, page: Any, captcha_type: CaptchaType) -> str | None:
        """Extract site key from page (same logic as CapSolver)."""
        try:
            if captcha_type in (CaptchaType.RECAPTCHA_V2, CaptchaType.RECAPTCHA_V3):
                selectors = [
                    '[data-sitekey]',
                    '.g-recaptcha[data-sitekey]',
                    '#recaptcha[data-sitekey]',
                ]
                for selector in selectors:
                    element = await page.query_selector(selector)
                    if element:
                        return await element.get_attribute("data-sitekey")

            elif captcha_type == CaptchaType.HCAPTCHA:
                element = await page.query_selector('[data-sitekey], .h-captcha[data-sitekey]')
                if element:
                    return await element.get_attribute("data-sitekey")

            elif captcha_type == CaptchaType.CLOUDFLARE:
                element = await page.query_selector('[data-sitekey], .cf-turnstile[data-sitekey]')
                if element:
                    return await element.get_attribute("data-sitekey")

        except Exception as e:
            logger.debug("Failed to extract site key", error=str(e))

        return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Factory function
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def create_paid_solver(
    provider: Literal["capsolver", "2captcha"] = "capsolver",
    api_key: str = "",
) -> CaptchaSolver:
    """
    Create a paid CAPTCHA solver instance.

    Args:
        provider: Which service to use
        api_key: API key for the service

    Returns:
        CaptchaSolver instance
    """
    if provider == "capsolver":
        return CapSolverClient(CapSolverConfig(api_key=api_key))
    elif provider == "2captcha":
        return TwoCaptchaClient(TwoCaptchaConfig(api_key=api_key))
    else:
        raise ValueError(f"Unknown CAPTCHA solver provider: {provider}")

```

---

## backend/autonomous-crawler-service/src/captcha/stealth.py

```py
"""Stealth browser configuration for bot detection bypass."""

from dataclasses import dataclass, field
from pathlib import Path
from typing import Any

import structlog

logger = structlog.get_logger(__name__)


# Extension paths for CAPTCHA bypass
EXTENSION_CACHE_DIR = Path("/tmp/browser-extensions")


@dataclass
class StealthConfig:
    """Configuration for stealth browser mode."""
    
    # User agent rotation
    user_agents: list[str] = field(default_factory=lambda: [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:134.0) Gecko/20100101 Firefox/134.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.2 Safari/605.1.15",
    ])
    
    # Viewport sizes (common resolutions)
    viewports: list[dict[str, int]] = field(default_factory=lambda: [
        {"width": 1920, "height": 1080},
        {"width": 1366, "height": 768},
        {"width": 1536, "height": 864},
        {"width": 1440, "height": 900},
    ])
    
    # Timezone/locale
    timezone: str = "Asia/Seoul"
    locale: str = "ko-KR"
    
    # Extra args for Chromium
    extra_args: list[str] = field(default_factory=lambda: [
        "--disable-blink-features=AutomationControlled",
        "--disable-features=IsolateOrigins,site-per-process",
        "--disable-dev-shm-usage",
        "--disable-accelerated-2d-canvas",
        "--no-first-run",
        "--no-zygote",
        "--disable-gpu",
        "--hide-scrollbars",
        "--mute-audio",
    ])
    
    # Webdriver navigator override
    hide_webdriver: bool = True
    
    # Random delays (ms)
    min_delay: int = 100
    max_delay: int = 500


def apply_stealth_to_playwright(page: Any, config: StealthConfig | None = None) -> None:
    """
    Apply stealth settings to a Playwright page.
    
    Uses playwright_stealth if available, otherwise applies manual patches.
    """
    if config is None:
        config = StealthConfig()
    
    try:
        from playwright_stealth import stealth_sync
        stealth_sync(page)
        logger.debug("Applied playwright_stealth")
    except ImportError:
        logger.warning("playwright_stealth not available, using manual patches")
        _apply_manual_stealth(page, config)


async def apply_stealth_to_playwright_async(page: Any, config: StealthConfig | None = None) -> None:
    """Async version of stealth application."""
    if config is None:
        config = StealthConfig()
    
    try:
        from playwright_stealth import stealth_async
        await stealth_async(page)
        logger.debug("Applied playwright_stealth (async)")
    except ImportError:
        logger.warning("playwright_stealth not available, using manual patches")
        await _apply_manual_stealth_async(page, config)


def _apply_manual_stealth(page: Any, config: StealthConfig) -> None:
    """Apply manual stealth patches."""
    # Hide webdriver
    if config.hide_webdriver:
        page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
        """)
    
    # Override navigator properties
    page.add_init_script("""
        // Override plugins
        Object.defineProperty(navigator, 'plugins', {
            get: () => [1, 2, 3, 4, 5]
        });
        
        // Override languages
        Object.defineProperty(navigator, 'languages', {
            get: () => ['ko-KR', 'ko', 'en-US', 'en']
        });
        
        // Override platform
        Object.defineProperty(navigator, 'platform', {
            get: () => 'Win32'
        });
        
        // Chrome runtime
        window.chrome = {
            runtime: {}
        };
        
        // Permissions
        const originalQuery = window.navigator.permissions.query;
        window.navigator.permissions.query = (parameters) => (
            parameters.name === 'notifications' ?
                Promise.resolve({ state: Notification.permission }) :
                originalQuery(parameters)
        );
    """)


async def _apply_manual_stealth_async(page: Any, config: StealthConfig) -> None:
    """Apply manual stealth patches (async)."""
    # Same as sync but using await
    if config.hide_webdriver:
        await page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
        """)
    
    await page.add_init_script("""
        Object.defineProperty(navigator, 'plugins', {
            get: () => [1, 2, 3, 4, 5]
        });
        
        Object.defineProperty(navigator, 'languages', {
            get: () => ['ko-KR', 'ko', 'en-US', 'en']
        });
        
        Object.defineProperty(navigator, 'platform', {
            get: () => 'Win32'
        });
        
        window.chrome = { runtime: {} };
        
        const originalQuery = window.navigator.permissions.query;
        window.navigator.permissions.query = (parameters) => (
            parameters.name === 'notifications' ?
                Promise.resolve({ state: Notification.permission }) :
                originalQuery(parameters)
        );
    """)


def get_undetected_browser_args() -> list[str]:
    """Get Chrome args for undetected browsing."""
    return [
        "--disable-blink-features=AutomationControlled",
        "--disable-features=IsolateOrigins,site-per-process",
        "--disable-infobars",
        "--disable-background-networking",
        "--disable-background-timer-throttling",
        "--disable-backgrounding-occluded-windows",
        "--disable-breakpad",
        "--disable-component-extensions-with-background-pages",
        "--disable-component-update",
        "--disable-default-apps",
        "--disable-extensions",
        "--disable-hang-monitor",
        "--disable-ipc-flooding-protection",
        "--disable-popup-blocking",
        "--disable-prompt-on-repost",
        "--disable-renderer-backgrounding",
        "--disable-sync",
        "--enable-features=NetworkService,NetworkServiceInProcess",
        "--force-color-profile=srgb",
        "--metrics-recording-only",
        "--no-first-run",
        "--password-store=basic",
        "--use-mock-keychain",
        "--ignore-certificate-errors",
    ]


async def get_nopecha_extension_path(api_key: str = "") -> str | None:
    """
    Download and configure NopeCHA extension for browser use.
    
    Args:
        api_key: Optional NopeCHA API key for faster solving
        
    Returns:
        Path to the extracted extension directory, or None if failed
    """
    try:
        from src.captcha.nopecha import NopeCHAConfig, NopeCHAExtensionManager
        
        config = NopeCHAConfig(
            api_key=api_key,
            enabled=True,
            auto_solve=True,
            use_audio_challenge=True,
            cache_dir=EXTENSION_CACHE_DIR / "nopecha",
        )
        
        manager = NopeCHAExtensionManager(config)
        ext_path = await manager.download_extension()
        
        logger.info("NopeCHA extension ready", path=str(ext_path))
        return str(ext_path)
        
    except Exception as e:
        logger.error("Failed to setup NopeCHA extension", error=str(e))
        return None


def get_stealth_browser_args_with_extensions(
    extension_paths: list[str] | None = None,
    include_docker_args: bool = False,
) -> list[str]:
    """
    Get Chrome args for stealth browsing with extension support.
    
    Args:
        extension_paths: List of paths to unpacked extensions
        include_docker_args: Include Docker-specific args
        
    Returns:
        List of Chrome arguments
    """
    args = [
        "--disable-blink-features=AutomationControlled",
        "--disable-features=IsolateOrigins,site-per-process,AutomationControlled",
        "--disable-infobars",
        "--disable-popup-blocking",
        "--disable-notifications",
        "--no-first-run",
        "--no-default-browser-check",
        "--no-service-autorun",
        "--disable-background-networking",
        "--disable-background-timer-throttling",
        "--disable-backgrounding-occluded-windows",
        "--disable-breakpad",
        "--disable-component-update",
        "--disable-default-apps",
        "--disable-hang-monitor",
        "--disable-ipc-flooding-protection",
        "--disable-renderer-backgrounding",
        "--disable-sync",
        "--disable-client-side-phishing-detection",
        "--disable-domain-reliability",
        "--metrics-recording-only",
        "--safebrowsing-disable-auto-update",
        "--enable-webgl",
        "--enable-accelerated-2d-canvas",
        "--enable-features=NetworkService,NetworkServiceInProcess",
        "--ignore-certificate-errors",
        "--ignore-ssl-errors",
        "--allow-running-insecure-content",
        "--password-store=basic",
        "--use-mock-keychain",
        "--log-level=3",
    ]
    
    # Add extension paths (requires extensions to NOT be disabled)
    if extension_paths:
        # Remove --disable-extensions if present
        args = [a for a in args if a != "--disable-extensions"]
        
        # Add load-extension args
        for ext_path in extension_paths:
            if ext_path:
                args.append(f"--load-extension={ext_path}")
                
        # Also add to disable-extensions-except
        valid_paths = [p for p in extension_paths if p]
        if valid_paths:
            args.append(f"--disable-extensions-except={','.join(valid_paths)}")
    else:
        args.append("--disable-extensions")
    
    if include_docker_args:
        args.extend([
            "--no-sandbox",
            "--disable-dev-shm-usage",
            "--disable-gpu-sandbox",
            "--disable-setuid-sandbox",
            "--no-zygote",
            "--single-process",
        ])
    
    return args


@dataclass
class EnhancedStealthConfig(StealthConfig):
    """Enhanced stealth configuration with extension support."""
    
    # NopeCHA settings - ENABLED by default
    use_nopecha: bool = True
    nopecha_api_key: str = ""
    
    # Camoufox as alternative - can be enabled via settings
    use_camoufox: bool = False
    
    # Human-like behavior - ENABLED by default
    enable_human_simulation: bool = True
    
    # Extension paths
    extension_paths: list[str] = field(default_factory=list)
    
    async def setup_extensions(self) -> None:
        """Download and configure required extensions."""
        if self.use_nopecha:
            nopecha_path = await get_nopecha_extension_path(self.nopecha_api_key)
            if nopecha_path and nopecha_path not in self.extension_paths:
                self.extension_paths.append(nopecha_path)
    
    def get_browser_args(self, include_docker: bool = False) -> list[str]:
        """Get Chrome args with configured extensions."""
        return get_stealth_browser_args_with_extensions(
            extension_paths=self.extension_paths if self.extension_paths else None,
            include_docker_args=include_docker,
        )
    
    def get_random_user_agent(self) -> str:
        """Get a random user agent from the configured list."""
        import random
        return random.choice(self.user_agents)
    
    def get_random_viewport(self) -> dict[str, int]:
        """Get a random viewport size from the configured list."""
        import random
        return random.choice(self.viewports)

```

---

## backend/autonomous-crawler-service/src/captcha/undetected.py

```py
"""
Undetected ChromeDriver Integration for Bot Detection Bypass.

This module provides:
1. Undetected ChromeDriver - Patched ChromeDriver that bypasses detection
2. Advanced Stealth - JavaScript patches to hide automation fingerprints
3. Human-like behavior simulation

Supports:
- Cloudflare
- PerimeterX
- DataDome
- Incapsula
- reCAPTCHA detection
"""

import asyncio
import random
from dataclasses import dataclass
from typing import Any

import structlog

logger = structlog.get_logger(__name__)


@dataclass
class UndetectedConfig:
    """Configuration for undetected browser mode."""
    
    # Driver settings
    driver_executable_path: str | None = None
    browser_executable_path: str | None = None
    
    # Version matching
    version_main: int | None = None  # Chrome major version
    
    # User data
    user_data_dir: str | None = None
    use_subprocess: bool = True
    
    # Stealth options
    enable_cdp_events: bool = True
    suppress_welcome: bool = True
    log_level: int = 0
    
    # Headless mode (use new headless mode)
    headless: bool = False
    use_new_headless: bool = True  # Chrome 109+ new headless mode
    
    # Window size
    window_size: tuple[int, int] = (1920, 1080)
    
    # Proxy
    proxy: str | None = None


def get_undetected_chromedriver():
    """
    Get an undetected ChromeDriver instance.
    
    Uses undetected-chromedriver library if available,
    otherwise falls back to manual patching.
    """
    try:
        import undetected_chromedriver as uc
        return uc
    except ImportError:
        logger.warning("undetected-chromedriver not installed, using manual patches")
        return None


async def create_undetected_driver(
    config: UndetectedConfig | None = None,
) -> Any:
    """
    Create an undetected ChromeDriver instance.
    
    Args:
        config: Configuration options
        
    Returns:
        Undetected ChromeDriver instance or None
    """
    if config is None:
        config = UndetectedConfig()
    
    uc = get_undetected_chromedriver()
    if uc is None:
        return None
    
    options = uc.ChromeOptions()
    
    # Basic options
    options.add_argument(f"--window-size={config.window_size[0]},{config.window_size[1]}")
    
    if config.headless:
        if config.use_new_headless:
            options.add_argument("--headless=new")
        else:
            options.add_argument("--headless")
    
    if config.proxy:
        options.add_argument(f"--proxy-server={config.proxy}")
    
    # Additional stealth arguments
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-gpu")
    options.add_argument("--disable-infobars")
    
    # Create driver
    try:
        driver = uc.Chrome(
            options=options,
            driver_executable_path=config.driver_executable_path,
            browser_executable_path=config.browser_executable_path,
            version_main=config.version_main,
            user_data_dir=config.user_data_dir,
            use_subprocess=config.use_subprocess,
            enable_cdp_events=config.enable_cdp_events,
            suppress_welcome=config.suppress_welcome,
            log_level=config.log_level,
        )
        
        logger.info("Created undetected ChromeDriver")
        return driver
        
    except Exception as e:
        logger.error("Failed to create undetected ChromeDriver", error=str(e))
        return None


class AdvancedStealthPatcher:
    """
    Advanced JavaScript stealth patches for bot detection bypass.
    
    These patches are applied via CDP or page.evaluate to hide
    automation fingerprints that bot detection systems look for.
    """
    
    # Chrome properties patch
    CHROME_RUNTIME_PATCH = """
    (() => {
        // Add chrome.runtime if missing
        if (!window.chrome) {
            window.chrome = {};
        }
        if (!window.chrome.runtime) {
            window.chrome.runtime = {
                PlatformOs: { MAC: 'mac', WIN: 'win', ANDROID: 'android', CROS: 'cros', LINUX: 'linux', OPENBSD: 'openbsd' },
                PlatformArch: { ARM: 'arm', X86_32: 'x86-32', X86_64: 'x86-64' },
                PlatformNaclArch: { ARM: 'arm', X86_32: 'x86-32', X86_64: 'x86-64' },
                RequestUpdateCheckStatus: { THROTTLED: 'throttled', NO_UPDATE: 'no_update', UPDATE_AVAILABLE: 'update_available' },
                OnInstalledReason: { INSTALL: 'install', UPDATE: 'update', CHROME_UPDATE: 'chrome_update', SHARED_MODULE_UPDATE: 'shared_module_update' },
                OnRestartRequiredReason: { APP_UPDATE: 'app_update', OS_UPDATE: 'os_update', PERIODIC: 'periodic' }
            };
        }
        
        // Add chrome.csi if missing
        if (!window.chrome.csi) {
            window.chrome.csi = function() { return {}; };
        }
        
        // Add chrome.loadTimes if missing
        if (!window.chrome.loadTimes) {
            window.chrome.loadTimes = function() {
                return {
                    commitLoadTime: Date.now() / 1000,
                    connectionInfo: 'http/1.1',
                    finishDocumentLoadTime: Date.now() / 1000,
                    finishLoadTime: Date.now() / 1000,
                    firstPaintAfterLoadTime: 0,
                    firstPaintTime: Date.now() / 1000,
                    navigationType: 'Other',
                    npnNegotiatedProtocol: 'unknown',
                    requestTime: Date.now() / 1000,
                    startLoadTime: Date.now() / 1000,
                    wasAlternateProtocolAvailable: false,
                    wasFetchedViaSpdy: false,
                    wasNpnNegotiated: false
                };
            };
        }
    })();
    """
    
    # WebDriver property patch
    WEBDRIVER_PATCH = """
    (() => {
        // Remove webdriver property
        Object.defineProperty(navigator, 'webdriver', {
            get: () => undefined,
            configurable: true
        });
        
        // Also patch the prototype
        const originalQuery = window.Navigator.prototype.hasOwnProperty;
        Object.defineProperty(Navigator.prototype, 'webdriver', {
            get: () => undefined,
            configurable: true
        });
        
        // Delete if exists
        delete navigator.webdriver;
    })();
    """
    
    # Plugins patch
    PLUGINS_PATCH = """
    (() => {
        // Mock plugins array
        const mockPlugins = [
            {
                name: 'Chrome PDF Plugin',
                filename: 'internal-pdf-viewer',
                description: 'Portable Document Format',
                length: 1,
                0: { type: 'application/pdf', suffixes: 'pdf', description: 'Portable Document Format' }
            },
            {
                name: 'Chrome PDF Viewer',
                filename: 'mhjfbmdgcfjbbpaeojofohoefgiehjai',
                description: '',
                length: 1,
                0: { type: 'application/pdf', suffixes: 'pdf', description: '' }
            },
            {
                name: 'Native Client',
                filename: 'internal-nacl-plugin',
                description: '',
                length: 2,
                0: { type: 'application/x-nacl', suffixes: '', description: 'Native Client Executable' },
                1: { type: 'application/x-pnacl', suffixes: '', description: 'Portable Native Client Executable' }
            }
        ];
        
        // Create a proper PluginArray
        const pluginArray = Object.create(PluginArray.prototype);
        mockPlugins.forEach((plugin, i) => {
            pluginArray[i] = plugin;
        });
        pluginArray.length = mockPlugins.length;
        
        // Patch namedItem and item methods
        pluginArray.item = function(index) { return this[index]; };
        pluginArray.namedItem = function(name) {
            return mockPlugins.find(p => p.name === name) || null;
        };
        pluginArray.refresh = function() {};
        
        Object.defineProperty(navigator, 'plugins', {
            get: () => pluginArray,
            configurable: true
        });
    })();
    """
    
    # Languages patch
    LANGUAGES_PATCH = """
    (() => {
        Object.defineProperty(navigator, 'languages', {
            get: () => ['ko-KR', 'ko', 'en-US', 'en'],
            configurable: true
        });
        
        Object.defineProperty(navigator, 'language', {
            get: () => 'ko-KR',
            configurable: true
        });
    })();
    """
    
    # Hardware concurrency patch
    HARDWARE_PATCH = """
    (() => {
        Object.defineProperty(navigator, 'hardwareConcurrency', {
            get: () => 8,
            configurable: true
        });
        
        Object.defineProperty(navigator, 'deviceMemory', {
            get: () => 8,
            configurable: true
        });
    })();
    """
    
    # Permissions patch
    PERMISSIONS_PATCH = """
    (() => {
        const originalQuery = window.navigator.permissions.query;
        window.navigator.permissions.query = (parameters) => {
            if (parameters.name === 'notifications') {
                return Promise.resolve({ state: Notification.permission });
            }
            return originalQuery.call(window.navigator.permissions, parameters);
        };
    })();
    """
    
    # WebGL vendor/renderer patch
    WEBGL_PATCH = """
    (() => {
        const getParameter = WebGLRenderingContext.prototype.getParameter;
        WebGLRenderingContext.prototype.getParameter = function(parameter) {
            // UNMASKED_VENDOR_WEBGL
            if (parameter === 37445) {
                return 'Intel Inc.';
            }
            // UNMASKED_RENDERER_WEBGL
            if (parameter === 37446) {
                return 'Intel Iris OpenGL Engine';
            }
            return getParameter.call(this, parameter);
        };
        
        // Also patch WebGL2
        if (typeof WebGL2RenderingContext !== 'undefined') {
            const getParameter2 = WebGL2RenderingContext.prototype.getParameter;
            WebGL2RenderingContext.prototype.getParameter = function(parameter) {
                if (parameter === 37445) {
                    return 'Intel Inc.';
                }
                if (parameter === 37446) {
                    return 'Intel Iris OpenGL Engine';
                }
                return getParameter2.call(this, parameter);
            };
        }
    })();
    """
    
    # Iframe contentWindow patch
    IFRAME_PATCH = """
    (() => {
        // Prevent iframe detection
        try {
            if (window.top === window.self) {
                Object.defineProperty(window, 'frameElement', {
                    get: () => null,
                    configurable: true
                });
            }
        } catch (e) {}
    })();
    """
    
    # Console debug patch (hide console.debug modifications)
    CONSOLE_PATCH = """
    (() => {
        // Preserve original console methods
        const originalDebug = console.debug;
        console.debug = function(...args) {
            // Filter out automation-related debug messages
            const filtered = args.filter(arg => {
                if (typeof arg === 'string') {
                    const lower = arg.toLowerCase();
                    return !lower.includes('webdriver') && 
                           !lower.includes('automation') &&
                           !lower.includes('puppeteer') &&
                           !lower.includes('playwright');
                }
                return true;
            });
            if (filtered.length > 0) {
                originalDebug.apply(console, filtered);
            }
        };
    })();
    """
    
    @classmethod
    def get_all_patches(cls) -> str:
        """Get all stealth patches combined."""
        return "\n".join([
            cls.CHROME_RUNTIME_PATCH,
            cls.WEBDRIVER_PATCH,
            cls.PLUGINS_PATCH,
            cls.LANGUAGES_PATCH,
            cls.HARDWARE_PATCH,
            cls.PERMISSIONS_PATCH,
            cls.WEBGL_PATCH,
            cls.IFRAME_PATCH,
            cls.CONSOLE_PATCH,
        ])
    
    @classmethod
    async def apply_to_page(cls, page: Any) -> None:
        """Apply all stealth patches to a Playwright page."""
        try:
            await page.add_init_script(cls.get_all_patches())
            logger.debug("Applied advanced stealth patches to page")
        except Exception as e:
            logger.error("Failed to apply stealth patches", error=str(e))


class HumanBehaviorSimulator:
    """
    Simulate human-like behavior to avoid bot detection.
    
    Includes:
    - Random delays
    - Mouse movements
    - Scroll patterns
    - Typing patterns
    """
    
    @staticmethod
    def random_delay(min_ms: int = 100, max_ms: int = 500) -> float:
        """Get random delay in seconds."""
        return random.randint(min_ms, max_ms) / 1000
    
    @staticmethod
    async def human_type(page: Any, selector: str, text: str) -> None:
        """Type text with human-like delays."""
        element = await page.query_selector(selector)
        if not element:
            return
        
        await element.click()
        await asyncio.sleep(HumanBehaviorSimulator.random_delay(50, 150))
        
        for char in text:
            await page.keyboard.type(char)
            # Variable delay between keystrokes
            delay = random.uniform(0.05, 0.15)
            if char in " .,!?":
                delay += random.uniform(0.1, 0.2)
            await asyncio.sleep(delay)
    
    @staticmethod
    async def human_click(page: Any, selector: str) -> None:
        """Click with human-like behavior."""
        element = await page.query_selector(selector)
        if not element:
            return
        
        box = await element.bounding_box()
        if not box:
            await element.click()
            return
        
        # Click at random position within element
        x = box["x"] + random.uniform(5, box["width"] - 5)
        y = box["y"] + random.uniform(5, box["height"] - 5)
        
        # Move mouse first
        await page.mouse.move(x, y)
        await asyncio.sleep(HumanBehaviorSimulator.random_delay(50, 150))
        await page.mouse.click(x, y)
    
    @staticmethod
    async def human_scroll(page: Any, direction: str = "down", amount: int = 300) -> None:
        """Scroll with human-like patterns."""
        if direction == "down":
            delta = random.randint(amount - 50, amount + 50)
        else:
            delta = -random.randint(amount - 50, amount + 50)
        
        await page.mouse.wheel(0, delta)
        await asyncio.sleep(HumanBehaviorSimulator.random_delay(200, 400))
    
    @staticmethod
    async def random_mouse_movements(page: Any, count: int = 3) -> None:
        """Make random mouse movements."""
        viewport = page.viewport_size
        if not viewport:
            return
        
        for _ in range(count):
            x = random.randint(100, viewport["width"] - 100)
            y = random.randint(100, viewport["height"] - 100)
            await page.mouse.move(x, y)
            await asyncio.sleep(HumanBehaviorSimulator.random_delay(100, 300))


def get_enhanced_browser_args(
    include_docker: bool = False,
    include_stealth: bool = True,
) -> list[str]:
    """
    Get enhanced Chrome arguments for maximum undetectability.
    
    Args:
        include_docker: Include Docker-specific args
        include_stealth: Include stealth-related args
        
    Returns:
        List of Chrome arguments
    """
    args = [
        # Core anti-detection
        "--disable-blink-features=AutomationControlled",
        "--disable-features=IsolateOrigins,site-per-process,AutomationControlled",
        
        # Disable infobars and popups
        "--disable-infobars",
        "--disable-popup-blocking",
        "--disable-notifications",
        
        # Disable extensions welcome
        "--no-first-run",
        "--no-default-browser-check",
        "--no-service-autorun",
        
        # Performance
        "--disable-background-networking",
        "--disable-background-timer-throttling",
        "--disable-backgrounding-occluded-windows",
        "--disable-breakpad",
        "--disable-component-update",
        "--disable-default-apps",
        "--disable-hang-monitor",
        "--disable-ipc-flooding-protection",
        "--disable-renderer-backgrounding",
        "--disable-sync",
        
        # Privacy/security that helps with detection
        "--disable-client-side-phishing-detection",
        "--disable-domain-reliability",
        "--metrics-recording-only",
        "--safebrowsing-disable-auto-update",
        
        # WebRTC IP leak prevention
        "--disable-webrtc-apm-in-audio-service",
        "--disable-webrtc-encryption",
        "--disable-webrtc-hw-decoding",
        "--disable-webrtc-hw-encoding",
        "--force-webrtc-ip-handling-policy=disable_non_proxied_udp",
        
        # GPU (often checked by detection systems)
        "--enable-webgl",
        "--enable-accelerated-2d-canvas",
        "--enable-features=NetworkService,NetworkServiceInProcess",
        
        # Misc
        "--ignore-certificate-errors",
        "--ignore-ssl-errors",
        "--allow-running-insecure-content",
        "--password-store=basic",
        "--use-mock-keychain",
        "--log-level=3",
    ]
    
    if include_docker:
        args.extend([
            "--no-sandbox",
            "--disable-dev-shm-usage",
            "--disable-gpu-sandbox",
            "--disable-setuid-sandbox",
            "--no-zygote",
            "--single-process",
        ])
    
    return args

```

---

## backend/autonomous-crawler-service/src/config/__init__.py

```py
"""Configuration module for autonomous-crawler-service."""

from .settings import Settings, get_settings
from .consul import (
    load_config_from_consul,
    wait_for_consul,
    check_consul_health,
    CONSUL_ENABLED,
)

__all__ = [
    "Settings",
    "get_settings",
    "load_config_from_consul",
    "wait_for_consul",
    "check_consul_health",
    "CONSUL_ENABLED",
]

```

---

## backend/autonomous-crawler-service/src/config/consul.py

```py
"""Consul KV configuration loader for autonomous-crawler service.

Loads configuration with the following precedence:
1. Consul KV (highest priority) - key path: config/autonomous-crawler/{KEY}
2. Environment Variables - {KEY}
3. Error if required key not found

Usage:
    from src.config.consul import load_config_from_consul

    # Load all config from Consul and apply to environment
    consul_keys, env_keys = load_config_from_consul()
    
    # Then use Settings as normal
    from src.config import get_settings
    settings = get_settings()
"""

import base64
import os
from typing import Any

import httpx
import structlog

logger = structlog.get_logger(__name__)

# Consul configuration
CONSUL_HOST = os.getenv("CONSUL_HOST", "localhost")
CONSUL_PORT = os.getenv("CONSUL_PORT", "8500")
CONSUL_HTTP_TOKEN = os.getenv("CONSUL_HTTP_TOKEN", "")
CONSUL_ENABLED = os.getenv("CONSUL_ENABLED", "true").lower() == "true"
CONSUL_SERVICE_NAME = os.getenv("CONSUL_SERVICE_NAME", "autonomous-crawler")

# Consul KV prefix for this service
CONSUL_KV_PREFIX = f"config/{CONSUL_SERVICE_NAME}/"


def get_consul_url() -> str:
    """Get the Consul HTTP API URL."""
    return f"http://{CONSUL_HOST}:{CONSUL_PORT}"


def get_consul_headers() -> dict[str, str]:
    """Get headers for Consul API requests."""
    headers = {"Accept": "application/json"}
    if CONSUL_HTTP_TOKEN:
        headers["X-Consul-Token"] = CONSUL_HTTP_TOKEN
    return headers


async def fetch_consul_kv_async(key: str) -> str | None:
    """
    Fetch a single key from Consul KV (async version).
    
    Args:
        key: The key name (without prefix)
        
    Returns:
        The value as a string, or None if not found
    """
    url = f"{get_consul_url()}/v1/kv/{CONSUL_KV_PREFIX}{key}"
    
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(url, headers=get_consul_headers(), timeout=5.0)
            
            if response.status_code == 404:
                return None
            
            response.raise_for_status()
            data = response.json()
            
            if data and len(data) > 0:
                # Consul returns base64-encoded values
                value_b64 = data[0].get("Value")
                if value_b64:
                    return base64.b64decode(value_b64).decode("utf-8")
            
            return None
            
    except httpx.HTTPError as e:
        logger.warning("Failed to fetch Consul key", key=key, error=str(e))
        return None
    except Exception as e:
        logger.warning("Error fetching Consul key", key=key, error=str(e))
        return None


def fetch_consul_kv_sync(key: str) -> str | None:
    """
    Fetch a single key from Consul KV (sync version).
    
    Args:
        key: The key name (without prefix)
        
    Returns:
        The value as a string, or None if not found
    """
    url = f"{get_consul_url()}/v1/kv/{CONSUL_KV_PREFIX}{key}"
    
    try:
        with httpx.Client() as client:
            response = client.get(url, headers=get_consul_headers(), timeout=5.0)
            
            if response.status_code == 404:
                return None
            
            response.raise_for_status()
            data = response.json()
            
            if data and len(data) > 0:
                value_b64 = data[0].get("Value")
                if value_b64:
                    return base64.b64decode(value_b64).decode("utf-8")
            
            return None
            
    except httpx.HTTPError as e:
        logger.warning("Failed to fetch Consul key", key=key, error=str(e))
        return None
    except Exception as e:
        logger.warning("Error fetching Consul key", key=key, error=str(e))
        return None


def fetch_all_consul_keys_sync() -> dict[str, str]:
    """
    Fetch all keys under the service prefix from Consul KV.
    
    Returns:
        Dictionary of key-value pairs
    """
    url = f"{get_consul_url()}/v1/kv/{CONSUL_KV_PREFIX}?recurse=true"
    
    try:
        with httpx.Client() as client:
            response = client.get(url, headers=get_consul_headers(), timeout=10.0)
            
            if response.status_code == 404:
                logger.info("No keys found in Consul", prefix=CONSUL_KV_PREFIX)
                return {}
            
            response.raise_for_status()
            data = response.json()
            
            result = {}
            for item in data or []:
                full_key = item.get("Key", "")
                value_b64 = item.get("Value")
                
                # Extract key name (remove prefix)
                if full_key.startswith(CONSUL_KV_PREFIX):
                    key_name = full_key[len(CONSUL_KV_PREFIX):]
                    if value_b64 and key_name:
                        result[key_name] = base64.b64decode(value_b64).decode("utf-8")
            
            return result
            
    except httpx.HTTPError as e:
        logger.warning("Failed to fetch Consul keys", prefix=CONSUL_KV_PREFIX, error=str(e))
        return {}
    except Exception as e:
        logger.warning("Error fetching Consul keys", prefix=CONSUL_KV_PREFIX, error=str(e))
        return {}


def load_config_from_consul() -> tuple[list[str], list[str]]:
    """
    Load configuration from Consul KV and inject into environment variables.
    
    This should be called at application startup, before Settings are loaded.
    
    Returns:
        Tuple of (consul_loaded_keys, env_loaded_keys)
    """
    if not CONSUL_ENABLED:
        logger.info("Consul configuration disabled, using environment variables only")
        return [], []
    
    logger.info(
        "Loading configuration from Consul",
        consul_url=get_consul_url(),
        service_name=CONSUL_SERVICE_NAME,
        prefix=CONSUL_KV_PREFIX,
    )
    
    # Fetch all keys from Consul
    consul_config = fetch_all_consul_keys_sync()
    
    consul_loaded_keys = []
    env_loaded_keys = []
    
    # Inject Consul values into environment (they take precedence)
    for key, value in consul_config.items():
        os.environ[key] = value
        consul_loaded_keys.append(key)
        logger.debug("Loaded from Consul", key=key)
    
    # Track which keys came from existing environment variables
    # (These are keys that weren't in Consul but exist in env)
    # Note: We only track this for logging purposes
    env_only_keys = set(os.environ.keys()) - set(consul_loaded_keys)
    
    logger.info(
        "Configuration loaded",
        consul_keys_count=len(consul_loaded_keys),
        consul_keys=consul_loaded_keys,
    )
    
    return consul_loaded_keys, list(env_only_keys)


def check_consul_health() -> bool:
    """Check if Consul is reachable and healthy."""
    url = f"{get_consul_url()}/v1/status/leader"
    
    try:
        with httpx.Client() as client:
            response = client.get(url, headers=get_consul_headers(), timeout=5.0)
            return response.status_code == 200
    except Exception:
        return False


def wait_for_consul(max_attempts: int = 30, delay: float = 2.0) -> bool:
    """
    Wait for Consul to become available.
    
    Args:
        max_attempts: Maximum number of connection attempts
        delay: Delay between attempts in seconds
        
    Returns:
        True if Consul became available, False otherwise
    """
    import time
    
    logger.info("Waiting for Consul to be ready", consul_url=get_consul_url())
    
    for attempt in range(1, max_attempts + 1):
        if check_consul_health():
            logger.info("Consul is ready", attempts=attempt)
            return True
        
        logger.info(
            "Consul not ready, retrying",
            attempt=attempt,
            max_attempts=max_attempts,
        )
        time.sleep(delay)
    
    logger.error(
        "Consul did not become ready",
        max_attempts=max_attempts,
    )
    return False


# Type coercion utilities (matching ConsulConfigLoader pattern)
def coerce_bool(value: str | bool | None) -> bool:
    """Convert string value to boolean."""
    if isinstance(value, bool):
        return value
    if value is None:
        return False
    return value.lower() in ("true", "1", "yes", "on")


def coerce_int(value: str | int | None, default: int = 0) -> int:
    """Convert string value to integer."""
    if isinstance(value, int):
        return value
    if value is None:
        return default
    try:
        return int(value)
    except ValueError:
        return default


def coerce_float(value: str | float | None, default: float = 0.0) -> float:
    """Convert string value to float."""
    if isinstance(value, float):
        return value
    if value is None:
        return default
    try:
        return float(value)
    except ValueError:
        return default


def coerce_list(value: str | list | None, separator: str = ",") -> list[str]:
    """Convert comma-separated string to list."""
    if isinstance(value, list):
        return value
    if value is None or value == "":
        return []
    return [item.strip() for item in value.split(separator) if item.strip()]

```

---

## backend/autonomous-crawler-service/src/config/settings.py

```py
"""Application settings using Pydantic Settings."""

from functools import lru_cache
from typing import Literal

from pydantic import Field
from pydantic_settings import BaseSettings, SettingsConfigDict


class KafkaSettings(BaseSettings):
    """Kafka connection settings."""

    model_config = SettingsConfigDict(env_prefix="KAFKA_")

    bootstrap_servers: str = Field(
        default="localhost:9092",
        description="Kafka bootstrap servers",
    )
    consumer_group_id: str = Field(
        default="autonomous-crawler-group",
        description="Consumer group ID",
    )
    browser_task_topic: str = Field(
        default="newsinsight.crawl.browser.tasks",
        description="Topic for browser task messages",
    )
    crawl_result_topic: str = Field(
        default="newsinsight.crawl.results",
        description="Topic for crawl result messages",
    )
    auto_offset_reset: Literal["earliest", "latest"] = Field(
        default="earliest",
        description="Auto offset reset policy",
    )
    enable_auto_commit: bool = Field(
        default=False,
        description="Enable auto commit (disabled for manual acknowledgment)",
    )
    max_poll_records: int = Field(
        default=1,
        description="Maximum records per poll (1 for sequential processing)",
    )
    session_timeout_ms: int = Field(
        default=30000,
        description="Session timeout in milliseconds",
    )
    heartbeat_interval_ms: int = Field(
        default=10000,
        description="Heartbeat interval in milliseconds",
    )


class BrowserSettings(BaseSettings):
    """Browser and AI agent settings."""

    model_config = SettingsConfigDict(env_prefix="BROWSER_")

    headless: bool = Field(
        default=True,
        description="Run browser in headless mode",
    )
    max_concurrent_sessions: int = Field(
        default=2,
        description="Maximum concurrent browser sessions",
    )
    default_timeout_seconds: int = Field(
        default=300,
        description="Default timeout for browser tasks in seconds",
    )
    max_timeout_seconds: int = Field(
        default=600,
        description="Maximum allowed timeout in seconds",
    )
    screenshot_dir: str = Field(
        default="/tmp/crawler-screenshots",
        description="Directory for storing screenshots",
    )
    user_agent: str = Field(
        default="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
        description="User agent string for browser",
    )
    is_docker_env: bool = Field(
        default=False,
        description="Whether running in Docker environment (enables no-sandbox, etc.)",
    )
    # Browser backend selection
    backend: Literal["playwright", "camoufox"] = Field(
        default="playwright",
        description="Browser backend: 'playwright' (Chrome/Chromium) or 'camoufox' (Firefox anti-detect)",
    )


class LLMSettings(BaseSettings):
    """LLM provider settings.

    Supported providers:
    - openai: OpenAI API (default)
    - anthropic: Anthropic Claude API
    - openrouter: OpenRouter (access to multiple models via single API)
    - ollama: Local Ollama server
    - custom: Custom OpenAI-compatible REST API endpoint
    """

    model_config = SettingsConfigDict(env_prefix="LLM_")

    provider: Literal["openai", "anthropic", "openrouter", "ollama", "custom"] = Field(
        default="openai",
        description="LLM provider: openai, anthropic, openrouter, ollama, or custom",
    )

    # OpenAI settings
    openai_api_key: str = Field(
        default="",
        description="OpenAI API key",
    )
    openai_model: str = Field(
        default="gpt-4o",
        description="OpenAI model to use",
    )
    openai_base_url: str = Field(
        default="",
        description="Custom OpenAI API base URL (leave empty for default)",
    )

    # Anthropic settings
    anthropic_api_key: str = Field(
        default="",
        description="Anthropic API key",
    )
    anthropic_model: str = Field(
        default="claude-3-5-sonnet-20241022",
        description="Anthropic model to use",
    )

    # OpenRouter settings (https://openrouter.ai)
    openrouter_api_key: str = Field(
        default="",
        description="OpenRouter API key",
    )
    openrouter_model: str = Field(
        default="anthropic/claude-3.5-sonnet",
        description="OpenRouter model (e.g., anthropic/claude-3.5-sonnet, openai/gpt-4o, google/gemini-pro)",
    )
    openrouter_base_url: str = Field(
        default="https://openrouter.ai/api/v1",
        description="OpenRouter API base URL",
    )

    # Ollama settings (local LLM)
    ollama_base_url: str = Field(
        default="http://localhost:11434",
        description="Ollama server URL",
    )
    ollama_model: str = Field(
        default="llama3.2",
        description="Ollama model name",
    )

    # Azure OpenAI settings
    azure_api_key: str = Field(
        default="",
        description="Azure OpenAI API key",
    )
    azure_endpoint: str = Field(
        default="",
        description="Azure OpenAI endpoint URL (e.g., https://your-resource.openai.azure.com)",
    )
    azure_deployment_name: str = Field(
        default="gpt-4o",
        description="Azure OpenAI deployment name",
    )
    azure_api_version: str = Field(
        default="2024-02-15-preview",
        description="Azure OpenAI API version",
    )

    # Custom REST API settings (supports non-OpenAI-compatible APIs)
    custom_api_key: str = Field(
        default="",
        description="API key for custom endpoint (optional if API doesn't require auth)",
    )
    custom_base_url: str = Field(
        default="",
        description="Custom REST API endpoint URL (e.g., https://workflow.nodove.com/webhook/aidove)",
    )
    custom_model: str = Field(
        default="",
        description="Model name for custom endpoint (used in request if format includes it)",
    )
    custom_request_format: str = Field(
        default="",
        description="""JSON template for custom API request body.
Use placeholders: {prompt} for user message, {session_id} for session ID, {model} for model name.
Example for AI Dove: {"chatInput": "{prompt}", "sessionId": "{session_id}"}
Example for standard: {"message": "{prompt}", "model": "{model}"}
Leave empty for OpenAI-compatible format.""",
    )
    custom_response_path: str = Field(
        default="reply",
        description="""JSON path to extract response text from API response.
Use dot notation for nested fields (e.g., 'choices.0.message.content' for OpenAI format).
For AI Dove API, use 'reply'.""",
    )
    custom_headers: str = Field(
        default="",
        description="""Custom HTTP headers as JSON object.
Example: {"X-Custom-Header": "value", "Authorization": "Bearer {api_key}"}
Use {api_key} placeholder for API key substitution.""",
    )

    # Common settings
    temperature: float = Field(
        default=0.0,
        description="LLM temperature",
    )
    max_tokens: int = Field(
        default=4096,
        description="Maximum tokens for LLM response",
    )


class SearchSettings(BaseSettings):
    """Search provider settings."""

    model_config = SettingsConfigDict(env_prefix="SEARCH_")

    # API Keys
    brave_api_key: str = Field(
        default="",
        description="Brave Search API key",
    )
    tavily_api_key: str = Field(
        default="",
        description="Tavily Search API key",
    )
    perplexity_api_key: str = Field(
        default="",
        description="Perplexity API key",
    )

    # Configuration
    timeout: float = Field(
        default=30.0,
        description="Timeout for search requests in seconds",
    )
    max_results_per_provider: int = Field(
        default=10,
        description="Maximum results per search provider",
    )
    max_total_results: int = Field(
        default=30,
        description="Maximum total aggregated results",
    )
    enable_parallel: bool = Field(
        default=True,
        description="Enable parallel search across providers",
    )

    # RRF (Reciprocal Rank Fusion) Settings
    enable_rrf: bool = Field(
        default=True,
        description="Enable RRF-based multi-strategy search for improved accuracy",
    )
    rrf_k: int = Field(
        default=60,
        ge=1,
        le=1000,
        description="RRF constant k (higher = more weight to lower ranks, default: 60)",
    )
    enable_semantic_rrf: bool = Field(
        default=True,
        description="Enable semantic similarity scoring in RRF",
    )
    enable_query_expansion: bool = Field(
        default=True,
        description="Enable LLM-based query expansion for better search accuracy",
    )
    max_expanded_queries: int = Field(
        default=5,
        ge=1,
        le=10,
        description="Maximum number of expanded queries for multi-strategy search",
    )
    cache_query_analysis: bool = Field(
        default=True,
        description="Cache query analysis results to reduce LLM calls",
    )


class StealthSettings(BaseSettings):
    """Stealth/anti-detection settings."""

    model_config = SettingsConfigDict(env_prefix="STEALTH_")

    enabled: bool = Field(
        default=True,
        description="Enable stealth mode for browser",
    )
    hide_webdriver: bool = Field(
        default=True,
        description="Hide webdriver detection flags",
    )
    hide_automation: bool = Field(
        default=True,
        description="Hide automation flags",
    )
    mask_webgl: bool = Field(
        default=True,
        description="Mask WebGL vendor/renderer",
    )
    random_user_agent: bool = Field(
        default=False,
        description="Use random user agent on each session",
    )
    # NopeCHA CAPTCHA solver extension
    use_nopecha: bool = Field(
        default=True,
        description="Enable NopeCHA extension for automatic CAPTCHA solving",
    )
    nopecha_api_key: str = Field(
        default="",
        description="NopeCHA API key (optional, for faster solving)",
    )
    # Advanced stealth patches
    use_advanced_patches: bool = Field(
        default=True,
        description="Apply advanced JavaScript patches for bot detection bypass",
    )
    # Human behavior simulation
    simulate_human_behavior: bool = Field(
        default=True,
        description="Simulate human-like mouse movements and scrolling",
    )


class CamoufoxSettings(BaseSettings):
    """Camoufox Firefox-based anti-detect browser settings."""

    model_config = SettingsConfigDict(env_prefix="CAMOUFOX_")

    enabled: bool = Field(
        default=True,
        description="Enable Camoufox as alternative browser (when BROWSER_BACKEND=camoufox)",
    )
    humanize: bool = Field(
        default=True,
        description="Enable human-like behavior simulation",
    )
    humanize_level: int = Field(
        default=2,
        ge=1,
        le=3,
        description="Humanization level (1=low, 2=medium, 3=high)",
    )
    geoip: bool = Field(
        default=True,
        description="Enable GeoIP-based fingerprint matching",
    )
    block_webrtc: bool = Field(
        default=True,
        description="Block WebRTC to prevent IP leaks",
    )
    block_images: bool = Field(
        default=False,
        description="Block images for faster loading",
    )
    locale: str = Field(
        default="ko-KR",
        description="Browser locale",
    )
    timezone: str = Field(
        default="Asia/Seoul",
        description="Browser timezone",
    )
    os_type: Literal["windows", "macos", "linux", "random"] = Field(
        default="random",
        description="OS fingerprint type",
    )


class CaptchaSettings(BaseSettings):
    """CAPTCHA solving settings."""

    model_config = SettingsConfigDict(env_prefix="CAPTCHA_")

    enabled: bool = Field(
        default=True,
        description="Enable CAPTCHA solving",
    )
    prefer_audio: bool = Field(
        default=True,
        description="Prefer audio challenges for reCAPTCHA",
    )
    cloudflare_delay: int = Field(
        default=10,
        description="Delay for Cloudflare challenge (seconds)",
    )
    max_attempts: int = Field(
        default=3,
        description="Maximum CAPTCHA solve attempts",
    )
    # Paid CAPTCHA solver settings (more reliable for search portals)
    capsolver_api_key: str = Field(
        default="",
        description="CapSolver API key (https://capsolver.com) - Recommended for Turnstile",
    )
    twocaptcha_api_key: str = Field(
        default="",
        description="2Captcha API key (https://2captcha.com)",
    )
    prefer_paid_solver: bool = Field(
        default=True,
        description="Prefer paid solvers over free ones when API key is available",
    )
    paid_solver_timeout: float = Field(
        default=120.0,
        description="Timeout for paid CAPTCHA solving (seconds)",
    )


class RedisSettings(BaseSettings):
    """Redis connection settings for task state persistence."""

    model_config = SettingsConfigDict(env_prefix="REDIS_")

    enabled: bool = Field(
        default=True,
        description="Enable Redis for task state persistence",
    )
    url: str = Field(
        default="redis://localhost:6379/4",
        description="Redis connection URL",
    )
    prefix: str = Field(
        default="autonomous_crawler",
        description="Key prefix for Redis entries",
    )
    result_ttl_hours: int = Field(
        default=48,
        description="Task result TTL in hours",
    )
    socket_timeout: float = Field(
        default=5.0,
        description="Socket timeout in seconds",
    )
    connection_timeout: float = Field(
        default=5.0,
        description="Connection timeout in seconds",
    )
    max_connections: int = Field(
        default=10,
        description="Maximum Redis connections in pool",
    )
    retry_on_timeout: bool = Field(
        default=True,
        description="Retry operations on timeout",
    )


class MetricsSettings(BaseSettings):
    """Prometheus metrics settings."""

    model_config = SettingsConfigDict(env_prefix="METRICS_")

    enabled: bool = Field(
        default=True,
        description="Enable Prometheus metrics",
    )
    port: int = Field(
        default=9090,
        description="Metrics server port",
    )
    path: str = Field(
        default="/metrics",
        description="Metrics endpoint path",
    )


class Settings(BaseSettings):
    """Main application settings."""

    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        extra="ignore",
    )

    # Service settings
    service_name: str = Field(
        default="autonomous-crawler-service",
        description="Service name",
    )
    log_level: Literal["DEBUG", "INFO", "WARNING", "ERROR"] = Field(
        default="INFO",
        description="Logging level",
    )
    log_format: Literal["json", "console"] = Field(
        default="json",
        description="Log output format",
    )

    # Nested settings
    kafka: KafkaSettings = Field(default_factory=KafkaSettings)
    browser: BrowserSettings = Field(default_factory=BrowserSettings)
    llm: LLMSettings = Field(default_factory=LLMSettings)
    search: SearchSettings = Field(default_factory=SearchSettings)
    stealth: StealthSettings = Field(default_factory=StealthSettings)
    captcha: CaptchaSettings = Field(default_factory=CaptchaSettings)
    camoufox: CamoufoxSettings = Field(default_factory=CamoufoxSettings)
    redis: RedisSettings = Field(default_factory=RedisSettings)
    metrics: MetricsSettings = Field(default_factory=MetricsSettings)


@lru_cache
def get_settings() -> Settings:
    """Get cached application settings."""
    return Settings()

```

---

## backend/autonomous-crawler-service/src/crawler/__init__.py

```py
"""Crawler module for autonomous-crawler-service."""

from .agent import AutonomousCrawlerAgent
from .policies import CrawlPolicy, get_policy_prompt

__all__ = [
    "AutonomousCrawlerAgent",
    "CrawlPolicy",
    "get_policy_prompt",
]

```

---

## backend/autonomous-crawler-service/src/crawler/agent.py

```py
"""Autonomous crawler agent using browser-use with CAPTCHA bypass and proxy rotation."""

import asyncio
import json
import os
import re
import time
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Optional, TYPE_CHECKING

import httpx
import structlog
from browser_use.agent.service import Agent
from browser_use.browser.session import BrowserSession
from browser_use.browser.profile import BrowserProfile, ProxySettings
from browser_use.llm.openai.chat import ChatOpenAI
from browser_use.llm.anthropic.chat import ChatAnthropic
from pydantic import BaseModel

from src.config import Settings

# Proxy rotation client
try:
    import sys

    sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "..", ".."))
    from shared.proxy_client import ProxyRotationClient, ProxyInfo

    PROXY_CLIENT_AVAILABLE = True
except ImportError:
    PROXY_CLIENT_AVAILABLE = False
    ProxyRotationClient = None  # type: ignore
    ProxyInfo = None  # type: ignore
from src.crawler.policies import CrawlPolicy, get_policy_prompt
from src.kafka.messages import BrowserTaskMessage, CrawlResultMessage
from src.captcha.stealth import (
    StealthConfig,
    EnhancedStealthConfig,
    apply_stealth_to_playwright_async,
    get_undetected_browser_args,
    get_stealth_browser_args_with_extensions,
)
from src.captcha import (
    CaptchaSolverOrchestrator,
    CaptchaType,
    AdvancedStealthPatcher,
    HumanBehaviorSimulator,
    # Camoufox
    CamoufoxConfig,
    CamoufoxHelper,
    create_camoufox_browser,
    get_recommended_camoufox_config,
    is_camoufox_available,
)
from src.search.orchestrator import (
    ParallelSearchOrchestrator,
    RRFSearchOrchestrator,
    create_rrf_orchestrator,
)
from src.search.brave import BraveSearchProvider
from src.search.tavily import TavilySearchProvider
from src.search.perplexity import PerplexitySearchProvider
from src.search.query_analyzer import QueryAnalyzer

if TYPE_CHECKING:
    from browser_use.agent.service import Agent as BrowserUseAgent

logger = structlog.get_logger(__name__)


# =============================================================================
# Custom REST API Adapter for non-OpenAI-compatible APIs
# =============================================================================


class CustomRESTAPIClient:
    """
    Adapter for custom REST APIs with configurable request/response formats.

    Supports APIs like AI Dove that use non-standard request formats.
    Implements a minimal interface compatible with browser-use's LLM requirements.
    """

    def __init__(
        self,
        base_url: str,
        api_key: str = "",
        model: str = "",
        request_format: str = "",
        response_path: str = "reply",
        custom_headers: str = "",
        temperature: float = 0.0,
        timeout: float = 120.0,
    ):
        """
        Initialize the custom REST API client.

        Args:
            base_url: The API endpoint URL
            api_key: Optional API key for authentication
            model: Model name (used in request if format includes it)
            request_format: JSON template for request body with placeholders
            response_path: Dot-notation path to extract response from JSON
            custom_headers: JSON string of custom headers
            temperature: Temperature parameter (passed to API if supported)
            timeout: Request timeout in seconds
        """
        self.base_url = base_url.rstrip("/")
        self.api_key = api_key
        self.model = model
        self.request_format = request_format
        self.response_path = response_path
        self.custom_headers = custom_headers
        self.temperature = temperature
        self.timeout = timeout
        self._session_id = f"crawler_{int(time.time())}"

        # For browser-use compatibility
        self.provider = "custom"

    def _build_headers(self) -> dict[str, str]:
        """Build HTTP headers for the request."""
        headers = {"Content-Type": "application/json"}

        # Parse custom headers if provided
        if self.custom_headers:
            try:
                custom = json.loads(self.custom_headers)
                for key, value in custom.items():
                    # Replace {api_key} placeholder
                    if isinstance(value, str):
                        value = value.replace("{api_key}", self.api_key)
                    headers[key] = value
            except json.JSONDecodeError:
                logger.warning("Failed to parse custom_headers JSON", headers=self.custom_headers)

        # Add default Authorization header if API key is provided and not in custom headers
        if self.api_key and "Authorization" not in headers:
            headers["Authorization"] = f"Bearer {self.api_key}"

        return headers

    def _build_request_body(self, prompt: str) -> dict[str, Any]:
        """Build request body from template or default format."""
        if self.request_format:
            try:
                # Parse the template and substitute placeholders
                template = self.request_format
                template = template.replace("{prompt}", prompt)
                template = template.replace("{session_id}", self._session_id)
                template = template.replace("{model}", self.model)
                template = template.replace("{temperature}", str(self.temperature))
                return json.loads(template)
            except json.JSONDecodeError as e:
                logger.error(
                    "Failed to parse request_format JSON",
                    error=str(e),
                    template=self.request_format,
                )
                raise ValueError(f"Invalid custom_request_format JSON: {e}")

        # Default OpenAI-compatible format
        return {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": self.temperature,
        }

    def _extract_response(self, response_json: dict[str, Any]) -> str:
        """Extract response text using the configured path."""
        if not self.response_path:
            # Return full response as string
            return json.dumps(response_json)

        # Navigate the JSON path (supports dot notation and array indices)
        current = response_json
        for part in self.response_path.split("."):
            if part.isdigit():
                # Array index
                idx = int(part)
                if isinstance(current, list) and len(current) > idx:
                    current = current[idx]
                else:
                    raise KeyError(f"Array index {idx} not found in response")
            elif isinstance(current, dict):
                if part in current:
                    current = current[part]
                else:
                    raise KeyError(f"Key '{part}' not found in response: {list(current.keys())}")
            else:
                raise KeyError(f"Cannot navigate '{part}' in non-dict/list value")

        return str(current) if current is not None else ""

    async def ainvoke(self, messages: list[dict[str, Any]], **kwargs) -> Any:
        """
        Async invoke the custom API (compatible with LangChain interface).

        Args:
            messages: List of message dicts with 'role' and 'content' keys
            **kwargs: Additional arguments (ignored for custom API)

        Returns:
            Response object with 'content' attribute
        """
        # Extract the last user message as the prompt
        prompt = ""
        for msg in reversed(messages):
            if isinstance(msg, dict) and msg.get("role") == "user":
                prompt = msg.get("content", "")
                break
            elif hasattr(msg, "content"):
                prompt = msg.content
                break

        if not prompt:
            # Fallback: concatenate all messages
            prompt = "\n".join(
                msg.get("content", str(msg)) if isinstance(msg, dict) else str(msg)
                for msg in messages
            )

        headers = self._build_headers()
        body = self._build_request_body(prompt)

        logger.debug(
            "Custom API request",
            url=self.base_url,
            body_keys=list(body.keys()) if isinstance(body, dict) else "raw",
        )

        async with httpx.AsyncClient(timeout=self.timeout) as client:
            response = await client.post(
                self.base_url,
                headers=headers,
                json=body,
            )

            if response.status_code != 200:
                error_text = response.text[:500]
                logger.error(
                    "Custom API error",
                    status=response.status_code,
                    error=error_text,
                )
                raise RuntimeError(f"Custom API error {response.status_code}: {error_text}")

            response_json = response.json()
            content = self._extract_response(response_json)

            logger.debug(
                "Custom API response",
                response_keys=list(response_json.keys())
                if isinstance(response_json, dict)
                else "raw",
                content_length=len(content),
            )

        # Return an object with 'content' attribute for browser-use compatibility
        class Response:
            def __init__(self, text: str):
                self.content = text

        return Response(content)

    def invoke(self, messages: list[dict[str, Any]], **kwargs) -> Any:
        """Sync invoke - runs async version in event loop."""
        return asyncio.run(self.ainvoke(messages, **kwargs))


# =============================================================================
# CAPTCHA Detection Hook for browser-use Agent
# =============================================================================


async def create_captcha_detection_hook(
    crawler_agent: "AutonomousCrawlerAgent",
    on_captcha_detected: callable = None,
) -> callable:
    """
    Create a hook function for browser-use Agent that detects CAPTCHAs.

    This hook is called at the start of each step to check for CAPTCHAs
    and attempt to solve them before the agent takes action.

    Args:
        crawler_agent: The AutonomousCrawlerAgent instance
        on_captcha_detected: Optional callback when CAPTCHA is detected

    Returns:
        Async hook function compatible with browser-use Agent
    """

    async def on_step_start_hook(agent: "BrowserUseAgent") -> None:
        """Hook called at the start of each browser-use step."""
        try:
            # Get the current page from browser session
            browser_session = agent.browser_session
            if not browser_session:
                return

            # Access the current page
            page = None
            try:
                # browser-use stores pages internally
                if hasattr(browser_session, "_context") and browser_session._context:
                    pages = browser_session._context.pages
                    if pages:
                        page = pages[-1]  # Get the most recent page
            except Exception:
                pass

            if not page:
                return

            # Check for CAPTCHA indicators
            captcha_detected = await _quick_captcha_check(page)

            if captcha_detected:
                logger.info("CAPTCHA detected in browser-use step, attempting to solve...")

                if on_captcha_detected:
                    await on_captcha_detected(captcha_detected)

                # Try to solve the CAPTCHA
                solved = await crawler_agent._detect_and_handle_captcha(page)

                if solved:
                    logger.info("CAPTCHA solved successfully, continuing agent step")
                else:
                    logger.warning("CAPTCHA could not be solved, agent may fail on this step")

                    # Simulate human behavior to appear more legitimate
                    if crawler_agent._stealth_config.enable_human_simulation:
                        await crawler_agent._simulate_human_behavior(page)

        except Exception as e:
            logger.debug("Error in CAPTCHA detection hook", error=str(e))

    return on_step_start_hook


async def create_stealth_hook(crawler_agent: "AutonomousCrawlerAgent") -> callable:
    """
    Create a hook function that applies stealth patches after navigation.

    This hook is called at the end of each step to re-apply stealth patches
    if the page has navigated to a new URL.
    """
    _last_url = {"value": None}

    async def on_step_end_hook(agent: "BrowserUseAgent") -> None:
        """Hook called at the end of each browser-use step."""
        try:
            browser_session = agent.browser_session
            if not browser_session:
                return

            page = None
            try:
                if hasattr(browser_session, "_context") and browser_session._context:
                    pages = browser_session._context.pages
                    if pages:
                        page = pages[-1]
            except Exception:
                pass

            if not page:
                return

            current_url = page.url

            # Only apply patches if we navigated to a new URL
            if current_url != _last_url["value"]:
                _last_url["value"] = current_url

                # Re-apply stealth patches to the new page
                await AdvancedStealthPatcher.apply_to_page(page)

                # Brief human-like delay after navigation
                if crawler_agent._stealth_config.enable_human_simulation:
                    await asyncio.sleep(0.5)

        except Exception as e:
            logger.debug("Error in stealth hook", error=str(e))

    return on_step_end_hook


async def _quick_captcha_check(page) -> str | None:
    """
    Quick check for common CAPTCHA indicators on a page.

    Returns the CAPTCHA type if detected, None otherwise.
    """
    captcha_indicators = [
        # Cloudflare
        (
            "cloudflare",
            [
                "#challenge-running",
                ".cf-browser-verification",
                "iframe[src*='turnstile']",
                "#cf-turnstile",
                "div[class*='challenge']",
            ],
        ),
        # reCAPTCHA
        (
            "recaptcha",
            [
                "iframe[src*='recaptcha']",
                ".g-recaptcha",
                "#recaptcha",
                "div[class*='recaptcha']",
            ],
        ),
        # hCaptcha
        (
            "hcaptcha",
            [
                "iframe[src*='hcaptcha']",
                ".h-captcha",
                "div[class*='hcaptcha']",
            ],
        ),
        # Generic bot detection
        (
            "bot_detection",
            [
                "text=checking your browser",
                "text=please verify you are human",
                "text=access denied",
                "text=blocked",
            ],
        ),
    ]

    for captcha_type, selectors in captcha_indicators:
        for selector in selectors:
            try:
                if selector.startswith("text="):
                    # Text-based detection
                    text = selector[5:].lower()
                    page_text = await page.inner_text("body")
                    if text in page_text.lower():
                        return captcha_type
                else:
                    # Selector-based detection
                    element = await page.query_selector(selector)
                    if element:
                        is_visible = await element.is_visible()
                        if is_visible:
                            return captcha_type
            except Exception:
                continue

    return None


class ExtractedArticle(BaseModel):
    """Extracted article content from a page."""

    url: str
    title: str
    content: str
    published_at: str | None = None
    author: str | None = None
    summary: str | None = None
    extraction_time: datetime = field(default_factory=datetime.now)


@dataclass
class CrawlSession:
    """Tracks the state of a crawling session."""

    job_id: int
    source_id: int
    seed_url: str
    max_depth: int
    max_pages: int
    budget_seconds: int
    policy: CrawlPolicy
    focus_keywords: list[str]
    excluded_domains: list[str]

    # Runtime state
    visited_urls: set[str] = field(default_factory=set)
    extracted_articles: list[CrawlResultMessage] = field(default_factory=list)
    start_time: datetime | None = None
    end_time: datetime | None = None
    error: str | None = None

    # AutoCrawl metadata (passed from BrowserTaskMessage.metadata)
    metadata: dict[str, Any] | None = None

    def is_budget_exceeded(self) -> bool:
        """Check if time budget has been exceeded."""
        if not self.start_time:
            return False
        elapsed = (datetime.now() - self.start_time).total_seconds()
        return elapsed >= self.budget_seconds

    def is_page_limit_reached(self) -> bool:
        """Check if page limit has been reached."""
        return len(self.visited_urls) >= self.max_pages

    def can_continue(self) -> bool:
        """Check if crawling can continue."""
        return not self.is_budget_exceeded() and not self.is_page_limit_reached()


class AutonomousCrawlerAgent:
    """
    AI-driven autonomous web crawler using browser-use.

    Consumes BrowserTaskMessage from Kafka and produces CrawlResultMessage
    for each extracted article.

    Supports two browser backends:
    - Playwright (Chrome/Chromium) with stealth patches and NopeCHA extension
    - Camoufox (Firefox-based) anti-detect browser with built-in fingerprint spoofing

    Integrates with IP Rotation service to:
    - Automatically rotate proxies for each crawl session
    - Report CAPTCHA encounters to help weighted proxy selection
    - Retry with different proxies when CAPTCHA solving fails
    """

    def __init__(self, settings: Settings) -> None:
        self.settings = settings
        self._browser_session: BrowserSession | None = None
        self._camoufox_browser: Any = None  # Camoufox browser instance
        self._llm = self._create_llm()
        self._search_orchestrator: ParallelSearchOrchestrator | None = None
        self._rrf_search_orchestrator: RRFSearchOrchestrator | None = None
        self._query_analyzer: QueryAnalyzer | None = None
        self._captcha_solver: CaptchaSolverOrchestrator | None = None
        self._stealth_config = EnhancedStealthConfig(
            use_nopecha=getattr(settings.stealth, "use_nopecha", True),
            nopecha_api_key=getattr(settings.stealth, "nopecha_api_key", ""),
            use_camoufox=getattr(settings.browser, "backend", "playwright") == "camoufox",
            enable_human_simulation=getattr(settings.stealth, "simulate_human_behavior", True),
        )

        # Determine browser backend
        self._use_camoufox = getattr(settings.browser, "backend", "playwright") == "camoufox"
        if self._use_camoufox and not is_camoufox_available():
            logger.warning("Camoufox requested but not available, falling back to Playwright")
            self._use_camoufox = False

        # Proxy rotation integration
        self._proxy_client: Optional[Any] = None
        self._current_proxy: Optional[Any] = None  # Current ProxyInfo
        self._use_proxy_rotation = getattr(settings, "use_proxy_rotation", True)
        self._proxy_rotation_url = getattr(
            settings,
            "proxy_rotation_url",
            os.environ.get("PROXY_ROTATION_URL", "http://ip-rotation:8050"),
        )

        # Initialize proxy client if available
        if PROXY_CLIENT_AVAILABLE and self._use_proxy_rotation:
            self._proxy_client = ProxyRotationClient(
                base_url=self._proxy_rotation_url,
                timeout=5.0,
                enabled=True,
            )
            logger.info("Proxy rotation enabled", url=self._proxy_rotation_url)

    def _create_llm(self) -> ChatOpenAI | ChatAnthropic:
        """Create the LLM instance based on settings.

        Uses browser-use's LLM classes which implement the BaseChatModel protocol
        with the required .provider property.

        Supported providers:
        - openai: OpenAI API
        - anthropic: Anthropic Claude API
        - openrouter: OpenRouter (multiple models via single API)
        - ollama: Local Ollama server
        - custom: Custom OpenAI-compatible REST API
        """
        llm_settings = self.settings.llm
        provider = llm_settings.provider.lower()

        if provider == "anthropic":
            logger.info("Using Anthropic provider", model=llm_settings.anthropic_model)
            return ChatAnthropic(
                model=llm_settings.anthropic_model,
                api_key=llm_settings.anthropic_api_key,
                temperature=llm_settings.temperature,
                max_tokens=llm_settings.max_tokens,
            )

        elif provider == "openrouter":
            # OpenRouter uses OpenAI-compatible API
            logger.info("Using OpenRouter provider", model=llm_settings.openrouter_model)
            return ChatOpenAI(
                model=llm_settings.openrouter_model,
                api_key=llm_settings.openrouter_api_key,
                base_url=llm_settings.openrouter_base_url,
                temperature=llm_settings.temperature,
                max_completion_tokens=llm_settings.max_tokens,
                default_headers={
                    "HTTP-Referer": "https://newsinsight.app",
                    "X-Title": "NewsInsight Crawler",
                },
            )

        elif provider == "ollama":
            # Ollama uses OpenAI-compatible API
            logger.info(
                "Using Ollama provider",
                model=llm_settings.ollama_model,
                base_url=llm_settings.ollama_base_url,
            )
            return ChatOpenAI(
                model=llm_settings.ollama_model,
                api_key="ollama",  # Ollama doesn't require API key but field is required
                base_url=f"{llm_settings.ollama_base_url}/v1",
                temperature=llm_settings.temperature,
                max_completion_tokens=llm_settings.max_tokens,
            )

        elif provider == "custom":
            # Custom REST API endpoint (supports non-OpenAI-compatible APIs)
            if not llm_settings.custom_base_url:
                raise ValueError("LLM_CUSTOM_BASE_URL is required when using custom provider")

            # Check if custom request format is provided (non-OpenAI-compatible API)
            if llm_settings.custom_request_format:
                logger.info(
                    "Using custom REST API provider with custom format",
                    base_url=llm_settings.custom_base_url,
                    response_path=llm_settings.custom_response_path,
                )
                return CustomRESTAPIClient(
                    base_url=llm_settings.custom_base_url,
                    api_key=llm_settings.custom_api_key,
                    model=llm_settings.custom_model,
                    request_format=llm_settings.custom_request_format,
                    response_path=llm_settings.custom_response_path,
                    custom_headers=llm_settings.custom_headers,
                    temperature=llm_settings.temperature,
                )
            else:
                # Fallback to OpenAI-compatible format
                logger.info(
                    "Using custom provider (OpenAI-compatible)",
                    model=llm_settings.custom_model,
                    base_url=llm_settings.custom_base_url,
                )
                return ChatOpenAI(
                    model=llm_settings.custom_model,
                    api_key=llm_settings.custom_api_key or "not-required",
                    base_url=llm_settings.custom_base_url,
                    temperature=llm_settings.temperature,
                    max_completion_tokens=llm_settings.max_tokens,
                )

        else:
            # Default: OpenAI
            base_url = llm_settings.openai_base_url or None
            logger.info("Using OpenAI provider", model=llm_settings.openai_model, base_url=base_url)
            kwargs = {
                "model": llm_settings.openai_model,
                "api_key": llm_settings.openai_api_key,
                "temperature": llm_settings.temperature,
                "max_completion_tokens": llm_settings.max_tokens,
            }
            if base_url:
                kwargs["base_url"] = base_url
            return ChatOpenAI(**kwargs)

    def _get_search_orchestrator(self) -> ParallelSearchOrchestrator:
        """Get or create the search orchestrator with configured providers."""
        if self._search_orchestrator is None:
            providers = []
            search_settings = self.settings.search

            # Add Brave Search if API key is configured
            if search_settings.brave_api_key:
                providers.append(BraveSearchProvider(search_settings.brave_api_key))
                logger.info("Brave Search provider enabled")

            # Add Tavily if API key is configured
            if search_settings.tavily_api_key:
                providers.append(TavilySearchProvider(search_settings.tavily_api_key))
                logger.info("Tavily Search provider enabled")

            # Add Perplexity if API key is configured
            if search_settings.perplexity_api_key:
                providers.append(PerplexitySearchProvider(search_settings.perplexity_api_key))
                logger.info("Perplexity Search provider enabled")

            if not providers:
                logger.warning("No search providers configured - API search disabled")

            self._search_orchestrator = ParallelSearchOrchestrator(
                providers=providers,
                timeout=search_settings.timeout,
                deduplicate=True,
            )

        return self._search_orchestrator

    def _get_rrf_search_orchestrator(self) -> RRFSearchOrchestrator:
        """Get or create the RRF search orchestrator with query analysis."""
        if self._rrf_search_orchestrator is None:
            providers = []
            search_settings = self.settings.search

            # Add Brave Search if API key is configured
            if search_settings.brave_api_key:
                providers.append(BraveSearchProvider(search_settings.brave_api_key))

            # Add Tavily if API key is configured
            if search_settings.tavily_api_key:
                providers.append(TavilySearchProvider(search_settings.tavily_api_key))

            # Add Perplexity if API key is configured
            if search_settings.perplexity_api_key:
                providers.append(PerplexitySearchProvider(search_settings.perplexity_api_key))

            if not providers:
                logger.warning("No search providers configured - RRF search disabled")

            # Create query analyzer with the LLM
            self._query_analyzer = QueryAnalyzer(
                llm=self._llm,
                enable_expansion=True,
                max_expanded_queries=5,
                cache_results=True,
            )

            # Get RRF settings
            rrf_k = getattr(search_settings, "rrf_k", 60)
            enable_semantic = getattr(search_settings, "enable_semantic_rrf", True)

            self._rrf_search_orchestrator = create_rrf_orchestrator(
                providers=providers,
                llm=self._llm,
                timeout=search_settings.timeout,
                rrf_k=rrf_k,
                enable_semantic=enable_semantic,
            )

            logger.info(
                "RRF Search orchestrator initialized",
                providers=len(providers),
                rrf_k=rrf_k,
                semantic_enabled=enable_semantic,
            )

        return self._rrf_search_orchestrator

    def _get_captcha_solver(self) -> CaptchaSolverOrchestrator:
        """Get or create the CAPTCHA solver orchestrator with paid solver support."""
        if self._captcha_solver is None:
            # Get paid solver API keys from settings
            captcha_settings = getattr(self.settings, "captcha", None)
            capsolver_key = getattr(captcha_settings, "capsolver_api_key", "") if captcha_settings else ""
            twocaptcha_key = getattr(captcha_settings, "twocaptcha_api_key", "") if captcha_settings else ""
            prefer_paid = getattr(captcha_settings, "prefer_paid_solver", True) if captcha_settings else True
            paid_timeout = getattr(captcha_settings, "paid_solver_timeout", 120.0) if captcha_settings else 120.0
            
            self._captcha_solver = CaptchaSolverOrchestrator(
                capsolver_api_key=capsolver_key,
                twocaptcha_api_key=twocaptcha_key,
                prefer_paid=prefer_paid,
                paid_timeout=paid_timeout,
            )
        return self._captcha_solver

    async def _get_browser_session(self, force_new: bool = False) -> BrowserSession:
        """Get or create the browser session with enhanced stealth configuration.

        Args:
            force_new: If True, always create a new browser session (recommended for task isolation)
        """
        # Use Camoufox if configured
        if self._use_camoufox:
            return await self._get_camoufox_session()

        # Close existing session if force_new or if session is not healthy
        if force_new and self._browser_session is not None:
            try:
                await self._browser_session.stop()
            except Exception as e:
                logger.debug("Error closing existing browser session", error=str(e))
            self._browser_session = None

        # Use Playwright with stealth
        if self._browser_session is None:
            stealth_settings = self.settings.stealth

            # Setup extensions if NopeCHA is enabled
            if self._stealth_config.use_nopecha:
                await self._stealth_config.setup_extensions()
                logger.info("NopeCHA extension configured for CAPTCHA bypass")

            # Build browser args for stealth mode
            extra_args = []
            if stealth_settings.enabled:
                if self._stealth_config.extension_paths:
                    # Use enhanced args with extension support
                    extra_args = self._stealth_config.get_browser_args(
                        include_docker=getattr(self.settings.browser, "is_docker_env", False)
                    )
                else:
                    extra_args = get_undetected_browser_args()
                logger.info(
                    "Stealth mode enabled for browser session",
                    extensions_loaded=len(self._stealth_config.extension_paths),
                )

            # Get proxy from rotation service if available
            proxy_settings = None
            if self._proxy_client:
                try:
                    self._current_proxy = await self._proxy_client.get_next_proxy()
                    if self._current_proxy:
                        proxy_settings = ProxySettings(
                            server=self._current_proxy.address,
                            username=self._current_proxy.username,
                            password=self._current_proxy.password,
                        )
                        logger.info(
                            "Proxy assigned for browser session",
                            proxy_id=self._current_proxy.id,
                            proxy_address=self._current_proxy.address,
                        )
                except Exception as e:
                    logger.warning("Failed to get proxy from rotation service", error=str(e))

            # Detect Playwright browser path in Docker environment
            executable_path = None
            if getattr(self.settings.browser, "is_docker_env", False):
                # Look for installed Chromium in Playwright's cache
                playwright_path = os.environ.get(
                    "PLAYWRIGHT_BROWSERS_PATH", os.path.expanduser("~/.cache/ms-playwright")
                )
                # Find chromium executable
                import glob

                chromium_patterns = [
                    f"{playwright_path}/chromium-*/chrome-linux64/chrome",
                    f"{playwright_path}/chromium-*/chrome-linux/chrome",
                    f"{playwright_path}/chromium_headless_shell-*/chrome-linux64/headless_shell",
                    f"{playwright_path}/chromium_headless_shell-*/chrome-linux/headless_shell",
                ]
                for pattern in chromium_patterns:
                    matches = glob.glob(pattern)
                    if matches:
                        executable_path = sorted(matches)[-1]  # Use latest version
                        logger.debug("Found Playwright browser", executable_path=executable_path)
                        break
                if not executable_path:
                    logger.warning("No Playwright browser found, browser-use will try to install")

            profile = BrowserProfile(
                headless=self.settings.browser.headless,
                disable_security=True,  # Required for some sites
                extra_chromium_args=extra_args,
                proxy=proxy_settings,  # Apply proxy from rotation service
                executable_path=executable_path,  # Use pre-installed Playwright browser
            )
            self._browser_session = BrowserSession(browser_profile=profile)
        return self._browser_session

    async def _get_camoufox_session(self) -> Any:
        """Get or create Camoufox browser session."""
        if self._camoufox_browser is None:
            camoufox_settings = getattr(self.settings, "camoufox", None)

            # Build Camoufox config
            if camoufox_settings:
                config = CamoufoxConfig(
                    headless=self.settings.browser.headless,
                    humanize=camoufox_settings.humanize,
                    humanize_level=camoufox_settings.humanize_level,
                    locale=camoufox_settings.locale,
                    timezone=camoufox_settings.timezone,
                    geoip=camoufox_settings.geoip,
                    block_webrtc=camoufox_settings.block_webrtc,
                    block_images=camoufox_settings.block_images,
                    os=camoufox_settings.os_type if camoufox_settings.os_type != "random" else None,
                )
            else:
                # Use recommended config for Cloudflare bypass
                config = get_recommended_camoufox_config(
                    purpose="cloudflare",
                    headless=self.settings.browser.headless,
                )

            self._camoufox_browser = await create_camoufox_browser(config)

            if self._camoufox_browser:
                logger.info(
                    "Camoufox browser created",
                    headless=config.headless,
                    humanize=config.humanize,
                    humanize_level=config.humanize_level,
                )
            else:
                logger.error("Failed to create Camoufox browser, falling back to Playwright")
                self._use_camoufox = False
                return await self._get_browser_session()

        return self._camoufox_browser

    async def _get_camoufox_page(self) -> Any:
        """Get a new page from Camoufox browser."""
        browser = await self._get_camoufox_session()
        if browser:
            try:
                page = await browser.new_page()
                logger.debug("Created new Camoufox page")
                return page
            except Exception as e:
                logger.error("Failed to create Camoufox page", error=str(e))
        return None

    async def _apply_page_stealth(self, page) -> None:
        """Apply advanced stealth patches to a page."""
        # Apply playwright_stealth or manual patches
        await apply_stealth_to_playwright_async(page, self._stealth_config)

        # Apply advanced stealth patches from undetected module
        await AdvancedStealthPatcher.apply_to_page(page)

        logger.debug("Applied advanced stealth patches to page")

    async def search_before_crawl(
        self,
        query: str,
        max_results: int = 20,
        use_rrf: bool = True,
    ) -> list[str]:
        """
        Perform API-based search before browser crawling.

        Returns list of URLs to visit based on search results.
        Useful for bypassing search engine CAPTCHAs.

        Args:
            query: Search query
            max_results: Maximum number of URLs to return
            use_rrf: Use RRF-based multi-strategy search for better accuracy
        """
        if use_rrf:
            return await self._search_with_rrf(query, max_results)

        # Fallback to simple parallel search
        orchestrator = self._get_search_orchestrator()

        if not orchestrator.providers:
            logger.warning("No search providers available")
            return []

        try:
            result = await orchestrator.search_news(
                query=query,
                max_results_per_provider=self.settings.search.max_results_per_provider,
            )

            urls = [r.url for r in result.results[:max_results]]

            logger.info(
                "Search completed",
                query=query,
                results_count=len(urls),
                providers_used=result.providers_used,
            )

            return urls

        except Exception as e:
            logger.error("Search failed", query=query, error=str(e))
            return []

    async def _search_with_rrf(
        self,
        query: str,
        max_results: int = 20,
    ) -> list[str]:
        """
        Perform RRF-based multi-strategy search.

        This method:
        1. Analyzes the query to understand intent and extract keywords
        2. Expands the query into multiple semantically related queries
        3. Executes parallel searches across all providers for each query variant
        4. Merges results using Reciprocal Rank Fusion algorithm
        5. Returns URLs ranked by combined relevance
        """
        orchestrator = self._get_rrf_search_orchestrator()

        if not orchestrator.providers:
            logger.warning("No search providers available for RRF search")
            return []

        try:
            result = await orchestrator.search_news_with_rrf(
                query=query,
                max_results_per_strategy=self.settings.search.max_results_per_provider,
            )

            urls = [r.url for r in result.results[:max_results]]

            logger.info(
                "RRF search completed",
                query=query,
                results_count=len(urls),
                strategies_used=result.strategies_used,
                providers_used=result.providers_used,
                query_analysis=result.query_analysis,
            )

            return urls

        except Exception as e:
            logger.error(
                "RRF search failed, falling back to simple search", query=query, error=str(e)
            )
            # Fallback to simple search
            return await self.search_before_crawl(query, max_results, use_rrf=False)

    async def close(self) -> None:
        """Close the browser and cleanup resources."""
        if self._browser_session:
            await self._browser_session.stop()
            self._browser_session = None

        if self._camoufox_browser:
            try:
                await self._camoufox_browser.close()
            except Exception as e:
                logger.debug("Error closing Camoufox browser", error=str(e))
            self._camoufox_browser = None

        if self._search_orchestrator:
            await self._search_orchestrator.close_all()
            self._search_orchestrator = None

        # Close proxy client
        if self._proxy_client:
            try:
                await self._proxy_client.close()
            except Exception as e:
                logger.debug("Error closing proxy client", error=str(e))
            self._proxy_client = None
            self._current_proxy = None

    async def crawl_with_camoufox(
        self,
        url: str,
        extract_content: bool = True,
        wait_for_cloudflare: bool = True,
    ) -> dict[str, Any]:
        """
        Crawl a URL using Camoufox browser for maximum anti-detection.

        Args:
            url: URL to crawl
            extract_content: Whether to extract page content
            wait_for_cloudflare: Whether to wait for Cloudflare challenge

        Returns:
            Dictionary with page content and metadata
        """
        page = await self._get_camoufox_page()
        if not page:
            return {"error": "Failed to create Camoufox page"}

        try:
            # Navigate to URL
            await page.goto(url, wait_until="domcontentloaded")

            # Wait for Cloudflare challenge if needed
            if wait_for_cloudflare:
                passed = await CamoufoxHelper.wait_for_cloudflare(page, timeout=30)
                if not passed:
                    logger.warning("Cloudflare challenge may not have completed", url=url)

            # Simulate human behavior
            if self._stealth_config.enable_human_simulation:
                await asyncio.sleep(1)  # Brief pause

            # Extract content
            if extract_content:
                content = await CamoufoxHelper.extract_page_content(page)
                content["success"] = True
                return content

            return {
                "success": True,
                "url": url,
                "title": await page.title(),
            }

        except Exception as e:
            logger.error("Camoufox crawl failed", url=url, error=str(e))
            return {"error": str(e), "success": False}
        finally:
            try:
                await page.close()
            except Exception:
                pass

    async def _detect_and_handle_captcha(self, page) -> bool:
        """
        Detect and attempt to handle CAPTCHAs on a page.

        Args:
            page: Playwright page object

        Returns:
            True if CAPTCHA was detected and handled (or not detected),
            False if CAPTCHA was detected but could not be handled
        """
        try:
            # Check for common CAPTCHA indicators
            captcha_selectors = {
                CaptchaType.RECAPTCHA_V2: [
                    "iframe[src*='recaptcha']",
                    ".g-recaptcha",
                    "#recaptcha",
                ],
                CaptchaType.HCAPTCHA: [
                    "iframe[src*='hcaptcha']",
                    ".h-captcha",
                ],
                CaptchaType.CLOUDFLARE: [
                    "#challenge-running",
                    ".cf-browser-verification",
                    "iframe[src*='turnstile']",
                    "#cf-turnstile",
                ],
            }

            detected_type = None
            for captcha_type, selectors in captcha_selectors.items():
                for selector in selectors:
                    try:
                        element = await page.query_selector(selector)
                        if element:
                            is_visible = await element.is_visible()
                            if is_visible:
                                detected_type = captcha_type
                                logger.info(
                                    "CAPTCHA detected", type=captcha_type.value, selector=selector
                                )
                                break
                    except Exception:
                        continue
                if detected_type:
                    break

            if not detected_type:
                return True  # No CAPTCHA detected

            # Report CAPTCHA to IP rotation service for weighted proxy selection
            if self._proxy_client and self._current_proxy:
                try:
                    await self._proxy_client.record_captcha(
                        proxy_id=self._current_proxy.id,
                        captcha_type=detected_type.value,
                    )
                    logger.info(
                        "CAPTCHA reported to IP rotation service",
                        proxy_id=self._current_proxy.id,
                        captcha_type=detected_type.value,
                    )
                except Exception as e:
                    logger.debug("Failed to report CAPTCHA to IP rotation service", error=str(e))

            # Try to solve the CAPTCHA
            solver = self._get_captcha_solver()
            result = await solver.solve(detected_type, page=page)

            if result.success:
                logger.info(
                    "CAPTCHA solved successfully",
                    type=detected_type.value,
                    solver=result.solver_used,
                    time_ms=result.time_ms,
                )
                # Wait for page to update after CAPTCHA solve
                await asyncio.sleep(2)
                return True
            else:
                logger.warning("CAPTCHA solve failed", type=detected_type.value, error=result.error)
                return False

        except Exception as e:
            logger.error("Error in CAPTCHA detection/handling", error=str(e))
            return False

    async def _simulate_human_behavior(self, page) -> None:
        """Simulate human-like behavior on a page to avoid detection."""
        try:
            # Random mouse movements
            await HumanBehaviorSimulator.random_mouse_movements(page, count=2)

            # Random scroll
            await HumanBehaviorSimulator.human_scroll(page, "down", 200)
            await asyncio.sleep(HumanBehaviorSimulator.random_delay(500, 1000))

        except Exception as e:
            logger.debug("Human behavior simulation failed", error=str(e))

    async def smart_search(
        self,
        query: str,
        max_results: int = 20,
        use_browser_fallback: bool = True,
        use_rrf: bool = True,
    ) -> list[dict[str, Any]]:
        """
        Smart search with API-first strategy and browser fallback.

        Tries API-based search first to avoid CAPTCHA, then falls back
        to browser-based search with Camoufox if APIs fail.

        Now uses RRF (Reciprocal Rank Fusion) for improved accuracy by:
        1. Analyzing query intent and extracting semantic meaning
        2. Expanding query into multiple search strategies
        3. Merging results from multiple providers and strategies

        Args:
            query: Search query
            max_results: Maximum number of results
            use_browser_fallback: Whether to try browser search if API fails
            use_rrf: Use RRF-based multi-strategy search

        Returns:
            List of search results with url, title, snippet
        """
        results = []

        # Step 1: Try API-based search (no CAPTCHA)
        if use_rrf:
            logger.info("Attempting RRF-based API search", query=query)
            try:
                orchestrator = self._get_rrf_search_orchestrator()
                if orchestrator.providers:
                    rrf_result = await orchestrator.search_news_with_rrf(
                        query=query,
                        max_results_per_strategy=self.settings.search.max_results_per_provider,
                    )

                    if rrf_result.results:
                        logger.info(
                            "RRF API search successful",
                            query=query,
                            results_count=len(rrf_result.results),
                            strategies=rrf_result.strategies_used,
                            query_analysis=rrf_result.query_analysis,
                        )
                        return [
                            {
                                "url": r.url,
                                "title": r.title,
                                "snippet": r.snippet,
                                "source": f"rrf_{r.source_provider}",
                            }
                            for r in rrf_result.results[:max_results]
                        ]
            except Exception as e:
                logger.warning("RRF search failed", error=str(e))

        # Fallback to simple API search
        logger.info("Attempting simple API-based search", query=query)
        api_urls = await self.search_before_crawl(query, max_results, use_rrf=False)

        if api_urls:
            logger.info("API search successful", query=query, results_count=len(api_urls))
            results = [{"url": url, "source": "api"} for url in api_urls]
            return results

        if not use_browser_fallback:
            logger.warning("API search failed and browser fallback disabled")
            return results

        # Step 2: Try browser search with Camoufox (best anti-detection)
        if is_camoufox_available():
            logger.info("Trying Camoufox browser search", query=query)
            camoufox_results = await self._browser_search_with_camoufox(query, max_results)
            if camoufox_results:
                return camoufox_results

        # Step 3: Try browser search with enhanced Playwright stealth
        logger.info("Trying Playwright stealth browser search", query=query)
        playwright_results = await self._browser_search_with_stealth(query, max_results)

        return playwright_results

    async def _browser_search_with_camoufox(
        self,
        query: str,
        max_results: int = 20,
    ) -> list[dict[str, Any]]:
        """
        Perform browser search using Camoufox anti-detect browser.

        Uses DuckDuckGo HTML version which is less likely to show CAPTCHA.
        """
        results = []

        try:
            page = await self._get_camoufox_page()
            if not page:
                return results

            # Use DuckDuckGo HTML version (lighter, less detection)
            search_url = f"https://html.duckduckgo.com/html/?q={query.replace(' ', '+')}"

            await page.goto(search_url, wait_until="domcontentloaded")

            # Wait for Cloudflare if present
            await CamoufoxHelper.wait_for_cloudflare(page, timeout=15)

            # Simulate human behavior
            await asyncio.sleep(1)

            # Extract search results
            result_elements = await page.query_selector_all(".result")

            for element in result_elements[:max_results]:
                try:
                    link = await element.query_selector(".result__a")
                    snippet_el = await element.query_selector(".result__snippet")

                    if link:
                        url = await link.get_attribute("href")
                        title = await link.inner_text()
                        snippet = ""
                        if snippet_el:
                            snippet = await snippet_el.inner_text()

                        if url and title:
                            results.append(
                                {
                                    "url": url,
                                    "title": title.strip(),
                                    "snippet": snippet.strip(),
                                    "source": "camoufox_duckduckgo",
                                }
                            )
                except Exception:
                    continue

            await page.close()

            if results:
                logger.info("Camoufox search successful", query=query, results_count=len(results))

        except Exception as e:
            logger.error("Camoufox search failed", query=query, error=str(e))

        return results

    async def _browser_search_with_stealth(
        self,
        query: str,
        max_results: int = 20,
    ) -> list[dict[str, Any]]:
        """
        Perform browser search using Playwright with stealth patches.

        Tries multiple search engines with different strategies.
        Prioritizes engines that are less likely to show CAPTCHAs.
        """
        results = []

        # Search engines to try (in order of CAPTCHA likelihood - least likely first)
        search_engines = [
            {
                "name": "duckduckgo_html",
                "url": f"https://html.duckduckgo.com/html/?q={query.replace(' ', '+')}",
                "result_selector": ".result",
                "link_selector": ".result__a",
                "snippet_selector": ".result__snippet",
                "wait_selector": ".result",
            },
            {
                "name": "startpage",
                "url": f"https://www.startpage.com/do/search?q={query.replace(' ', '+')}",
                "result_selector": ".w-gl__result",
                "link_selector": "a.w-gl__result-title",
                "snippet_selector": ".w-gl__description",
                "wait_selector": ".w-gl__result",
            },
            {
                "name": "ecosia",
                "url": f"https://www.ecosia.org/search?q={query.replace(' ', '+')}",
                "result_selector": "[data-test-id='mainline-result-web']",
                "link_selector": "a[data-test-id='result-link']",
                "snippet_selector": "[data-test-id='result-snippet']",
                "wait_selector": "[data-test-id='mainline-result-web']",
            },
            {
                "name": "mojeek",  # Privacy-focused, rarely uses CAPTCHA
                "url": f"https://www.mojeek.com/search?q={query.replace(' ', '+')}",
                "result_selector": ".results-standard li",
                "link_selector": "a.title",
                "snippet_selector": ".s",
                "wait_selector": ".results-standard",
            },
        ]

        browser_session = None
        context = None
        page = None

        try:
            browser_session = await self._get_browser_session()

            # Get the underlying playwright browser to create isolated context
            if hasattr(browser_session, "_browser") and browser_session._browser:
                browser = browser_session._browser

                # Create isolated context with stealth args
                context = await browser.new_context(
                    user_agent=self._stealth_config.get_random_user_agent()
                    if hasattr(self._stealth_config, "get_random_user_agent")
                    else None,
                    locale="en-US",
                    timezone_id="America/New_York",
                )
                page = await context.new_page()

                # Apply stealth patches
                await self._apply_page_stealth(page)
            else:
                logger.warning("Could not access underlying browser for stealth search")
                return results

            for engine in search_engines:
                try:
                    logger.info("Trying search engine", engine=engine["name"], query=query)

                    # Navigate to search engine
                    await page.goto(engine["url"], wait_until="domcontentloaded", timeout=15000)

                    # Wait for results to load
                    try:
                        await page.wait_for_selector(
                            engine["wait_selector"], timeout=10000, state="visible"
                        )
                    except Exception:
                        # Check if we hit a CAPTCHA
                        captcha_type = await _quick_captcha_check(page)
                        if captcha_type:
                            logger.warning(
                                "CAPTCHA detected on search engine",
                                engine=engine["name"],
                                captcha_type=captcha_type,
                            )
                            # Try to solve it
                            solved = await self._detect_and_handle_captcha(page)
                            if not solved:
                                continue  # Try next engine
                            # Wait again for results after solving
                            try:
                                await page.wait_for_selector(engine["wait_selector"], timeout=5000)
                            except Exception:
                                continue
                        else:
                            logger.debug(
                                "Results not found, trying next engine", engine=engine["name"]
                            )
                            continue

                    # Simulate human behavior
                    if self._stealth_config.enable_human_simulation:
                        await HumanBehaviorSimulator.random_mouse_movements(page, count=1)
                        await asyncio.sleep(0.3)

                    # Extract search results
                    result_elements = await page.query_selector_all(engine["result_selector"])

                    for element in result_elements[:max_results]:
                        try:
                            link = await element.query_selector(engine["link_selector"])
                            snippet_el = await element.query_selector(engine["snippet_selector"])

                            if link:
                                url = await link.get_attribute("href")
                                title = await link.inner_text()
                                snippet = ""
                                if snippet_el:
                                    snippet = await snippet_el.inner_text()

                                # Clean up URL (some engines use redirect URLs)
                                if url and title:
                                    # Skip ad/sponsored results
                                    if "ad" in url.lower() or "sponsor" in title.lower():
                                        continue

                                    results.append(
                                        {
                                            "url": url,
                                            "title": title.strip(),
                                            "snippet": snippet.strip() if snippet else "",
                                            "source": f"stealth_{engine['name']}",
                                        }
                                    )
                        except Exception as e:
                            logger.debug("Failed to extract result", error=str(e))
                            continue

                    if results:
                        logger.info(
                            "Stealth search successful",
                            engine=engine["name"],
                            query=query,
                            results_count=len(results),
                        )
                        break  # Got results, stop trying other engines

                except Exception as e:
                    logger.debug("Search engine failed", engine=engine["name"], error=str(e))
                    continue

        except Exception as e:
            logger.error("Stealth browser search failed", query=query, error=str(e))
        finally:
            # Clean up
            if page:
                try:
                    await page.close()
                except Exception:
                    pass
            if context:
                try:
                    await context.close()
                except Exception:
                    pass

        return results

    async def handle_captcha_and_retry(
        self,
        page,
        action_func,
        max_retries: int = 3,
        switch_backend_on_failure: bool = True,
        rotate_proxy_on_failure: bool = True,
    ) -> Any:
        """
        Execute an action with CAPTCHA detection and retry logic.

        If CAPTCHA is detected and cannot be solved, optionally switches
        to a different browser backend or rotates to a new proxy and retries.

        Args:
            page: Current page object
            action_func: Async function to execute
            max_retries: Maximum retry attempts
            switch_backend_on_failure: Try different browser if CAPTCHA persists
            rotate_proxy_on_failure: Get a new proxy from rotation service on failure

        Returns:
            Result of action_func or None if all retries fail
        """
        for attempt in range(max_retries):
            try:
                # Check for CAPTCHA before action
                captcha_handled = await self._detect_and_handle_captcha(page)

                if not captcha_handled:
                    logger.warning(
                        "CAPTCHA detected but not solved",
                        attempt=attempt + 1,
                        max_retries=max_retries,
                    )

                    # Try rotating to a new proxy first
                    if rotate_proxy_on_failure and self._proxy_client:
                        try:
                            new_proxy = await self._proxy_client.get_next_proxy()
                            if new_proxy and (
                                not self._current_proxy or new_proxy.id != self._current_proxy.id
                            ):
                                self._current_proxy = new_proxy
                                logger.info(
                                    "Rotating to new proxy after CAPTCHA failure",
                                    proxy_id=new_proxy.id,
                                    proxy_address=new_proxy.address,
                                    attempt=attempt + 1,
                                )
                                # Recreate browser session with new proxy
                                if self._browser_session:
                                    await self._browser_session.stop()
                                    self._browser_session = None
                                # Get new session with new proxy
                                browser_session = await self._get_browser_session()
                                if (
                                    hasattr(browser_session, "_context")
                                    and browser_session._context
                                ):
                                    pages = browser_session._context.pages
                                    if pages:
                                        page = pages[-1]
                                        continue
                        except Exception as e:
                            logger.debug("Failed to rotate proxy", error=str(e))

                    # If Camoufox available and we're not already using it, switch
                    if (
                        switch_backend_on_failure
                        and not self._use_camoufox
                        and is_camoufox_available()
                    ):
                        logger.info("Switching to Camoufox browser for better CAPTCHA bypass")
                        self._use_camoufox = True

                        # Get new page from Camoufox
                        new_page = await self._get_camoufox_page()
                        if new_page:
                            page = new_page
                            continue

                    await asyncio.sleep(2**attempt)  # Exponential backoff
                    continue

                # Execute the action
                result = await action_func(page)

                # Check for CAPTCHA after action (might have triggered one)
                await self._detect_and_handle_captcha(page)

                return result

            except Exception as e:
                logger.error("Action failed", attempt=attempt + 1, error=str(e))
                await asyncio.sleep(2**attempt)

        logger.error("All retry attempts failed")
        return None

    async def execute_task(self, task: BrowserTaskMessage) -> list[CrawlResultMessage]:
        """
        Execute a browser crawling task.

        Args:
            task: The browser task message from Kafka

        Returns:
            List of extracted crawl results
        """
        # Parse policy
        try:
            policy = CrawlPolicy(task.policy.lower()) if task.policy else CrawlPolicy.NEWS_ONLY
        except ValueError:
            policy = CrawlPolicy.NEWS_ONLY

        # Create session with metadata from task (for AutoCrawl callback)
        session = CrawlSession(
            job_id=task.job_id,
            source_id=task.source_id,
            seed_url=task.seed_url,
            max_depth=task.max_depth or 2,
            max_pages=task.max_pages or 10,
            budget_seconds=min(
                task.budget_seconds or self.settings.browser.default_timeout_seconds,
                self.settings.browser.max_timeout_seconds,
            ),
            policy=policy,
            focus_keywords=task.get_focus_keywords_list(),
            excluded_domains=task.get_excluded_domains_list(),
            metadata=task.metadata,  # Pass metadata for AutoCrawl callback
        )

        # Track proxy used for this task
        task_proxy_id = None

        logger.info(
            "Starting crawl session",
            job_id=session.job_id,
            source_id=session.source_id,
            seed_url=session.seed_url,
            policy=policy.value,
            max_pages=session.max_pages,
            budget_seconds=session.budget_seconds,
        )

        session.start_time = datetime.now()

        try:
            # Generate the system prompt based on policy
            system_prompt = get_policy_prompt(
                policy=policy,
                focus_keywords=session.focus_keywords,
                custom_prompt=task.custom_prompt,
                excluded_domains=session.excluded_domains,
            )

            # Create the task prompt
            task_prompt = self._build_task_prompt(session)

            # Get browser session and create agent (this will also get proxy)
            # Use force_new=True to ensure a fresh browser for each task (avoids CDP issues)
            browser_session = await self._get_browser_session(force_new=True)

            # Track the proxy being used for this task
            if self._current_proxy:
                task_proxy_id = self._current_proxy.id
                logger.info(
                    "Task using proxy",
                    job_id=session.job_id,
                    proxy_id=task_proxy_id,
                )

            # Create CAPTCHA detection and stealth hooks for the agent
            captcha_hook = await create_captcha_detection_hook(
                crawler_agent=self,
                on_captcha_detected=lambda ct: logger.info(
                    "CAPTCHA detected during crawl",
                    captcha_type=ct,
                    job_id=session.job_id,
                ),
            )
            stealth_hook = await create_stealth_hook(self)

            agent = Agent(
                task=task_prompt,
                llm=self._llm,
                browser_session=browser_session,
                max_actions_per_step=5,
                extend_system_message=system_prompt,  # Add crawl policy to system prompt
            )

            # Run the agent with timeout and CAPTCHA/stealth hooks
            try:
                result = await asyncio.wait_for(
                    agent.run(
                        max_steps=session.max_pages * 3,  # Allow multiple steps per page
                        on_step_start=captcha_hook,  # CAPTCHA detection before each step
                        on_step_end=stealth_hook,  # Re-apply stealth after navigation
                    ),
                    timeout=session.budget_seconds,
                )

                # Parse the agent's output to extract articles
                session.extracted_articles = self._parse_agent_output(
                    result, session.job_id, session.source_id
                )

            except asyncio.TimeoutError:
                logger.warning(
                    "Crawl session timed out",
                    job_id=session.job_id,
                    elapsed_seconds=session.budget_seconds,
                )

        except Exception as e:
            session.error = str(e)
            logger.error(
                "Crawl session failed",
                job_id=session.job_id,
                error=str(e),
                exc_info=True,
            )

        finally:
            session.end_time = datetime.now()
            elapsed = (session.end_time - session.start_time).total_seconds()

            # Clean up browser session to prevent CDP issues on next task
            # Use kill() instead of stop() for more aggressive cleanup
            if self._browser_session:
                try:
                    await self._browser_session.kill()
                    logger.debug("Browser session killed successfully")
                except Exception as e:
                    logger.debug("Error killing browser session", error=str(e))
                self._browser_session = None

            # Small delay to ensure browser process fully terminates before next task
            await asyncio.sleep(0.5)

            # Record proxy usage result to IP rotation service
            if self._proxy_client and task_proxy_id:
                try:
                    if session.error:
                        await self._proxy_client.record_failure(
                            proxy_id=task_proxy_id,
                            reason=session.error[:200],  # Truncate error message
                        )
                    else:
                        await self._proxy_client.record_success(
                            proxy_id=task_proxy_id,
                            latency_ms=int(elapsed * 1000),
                        )
                    logger.debug(
                        "Proxy usage recorded",
                        proxy_id=task_proxy_id,
                        success=session.error is None,
                    )
                except Exception as e:
                    logger.debug("Failed to record proxy usage", error=str(e))

            # Send callback if configured
            if task.callback_url:
                await self._send_callback(task, session)

        logger.info(
            "Crawl session completed",
            job_id=session.job_id,
            articles_extracted=len(session.extracted_articles),
            elapsed_seconds=elapsed,
            error=session.error,
            proxy_id=task_proxy_id,
        )

        return session.extracted_articles

    def _build_task_prompt(self, session: CrawlSession) -> str:
        """Build the task prompt for the browser-use agent."""
        prompt_parts = [
            f"Navigate to {session.seed_url} and extract article content.",
            f"",
            f"## Constraints:",
            f"- Maximum pages to visit: {session.max_pages}",
            f"- Maximum link depth: {session.max_depth}",
            f"- Time budget: {session.budget_seconds} seconds",
            f"",
            f"## Output Format:",
            f"For each article you extract, output in this exact format:",
            f"---ARTICLE_START---",
            f"URL: [the page URL]",
            f"TITLE: [the article title]",
            f"PUBLISHED_AT: [publication date in ISO format, or 'unknown']",
            f"CONTENT: [the full article text]",
            f"---ARTICLE_END---",
            f"",
            f"Extract as many relevant articles as possible within the constraints.",
        ]

        if session.focus_keywords:
            prompt_parts.append(f"Focus on articles about: {', '.join(session.focus_keywords)}")

        return "\n".join(prompt_parts)

    def _is_invalid_agent_output(self, text: str) -> bool:
        """Check if text contains raw Python object representations that shouldn't be used as article content."""
        invalid_patterns = [
            "ActionResult(",
            "AgentHistoryList(",
            "AgentHistory(",
            "include_extracted_content_only_once=",
            "include_in_memory=",
            "metadata=None",
            "is_done=",
            "extracted_content=",
        ]
        return any(pattern in text for pattern in invalid_patterns)

    def _parse_agent_output(
        self, result: Any, job_id: int, source_id: int
    ) -> list[CrawlResultMessage]:
        """Parse the agent's output to extract article data."""
        articles = []

        # Get the final output from the agent
        output_text = ""
        if hasattr(result, "final_result"):
            try:
                final_result = result.final_result
                value = final_result() if callable(final_result) else final_result
                if isinstance(value, str) and value.strip():
                    # Validate that this is actual content, not raw Python objects
                    if not self._is_invalid_agent_output(value):
                        output_text = value
                    else:
                        logger.warning(
                            "final_result contains raw Python object representation, skipping",
                            job_id=job_id,
                        )
            except Exception:
                output_text = ""

        if not output_text and hasattr(result, "history") and result.history:
            # Get the last message content
            for item in reversed(result.history):
                if hasattr(item, "result") and item.result:
                    for r in reversed(item.result):
                        extracted_content = getattr(r, "extracted_content", None)
                        if isinstance(extracted_content, str) and extracted_content.strip():
                            # Validate extracted content is not raw debug output
                            if not self._is_invalid_agent_output(extracted_content):
                                output_text = extracted_content
                                break
                            else:
                                logger.debug(
                                    "Skipping invalid extracted_content",
                                    job_id=job_id,
                                    content_preview=extracted_content[:100],
                                )
                    if output_text:
                        break

        if not output_text:
            logger.warning("No valid output from agent", job_id=job_id)
            return articles

        # Parse articles from the output
        article_pattern = r"---ARTICLE_START---(.+?)---ARTICLE_END---"
        matches = re.findall(article_pattern, output_text, re.DOTALL)

        for match in matches:
            try:
                article = self._parse_article_block(match, job_id, source_id)
                if article:
                    articles.append(article)
            except Exception as e:
                logger.warning("Failed to parse article block", error=str(e))

        # If no structured output, try to extract from unstructured text
        if not articles:
            articles = self._extract_from_unstructured(output_text, job_id, source_id)

        return articles

    def _parse_article_block(
        self, block: str, job_id: int, source_id: int
    ) -> CrawlResultMessage | None:
        """Parse a single article block from agent output."""
        lines = block.strip().split("\n")
        data: dict[str, str] = {}

        current_key = None
        current_value = []

        for line in lines:
            if line.startswith("URL:"):
                if current_key:
                    data[current_key] = "\n".join(current_value).strip()
                current_key = "url"
                current_value = [line[4:].strip()]
            elif line.startswith("TITLE:"):
                if current_key:
                    data[current_key] = "\n".join(current_value).strip()
                current_key = "title"
                current_value = [line[6:].strip()]
            elif line.startswith("PUBLISHED_AT:"):
                if current_key:
                    data[current_key] = "\n".join(current_value).strip()
                current_key = "published_at"
                current_value = [line[13:].strip()]
            elif line.startswith("CONTENT:"):
                if current_key:
                    data[current_key] = "\n".join(current_value).strip()
                current_key = "content"
                current_value = [line[8:].strip()]
            elif current_key:
                current_value.append(line)

        # Don't forget the last field
        if current_key:
            data[current_key] = "\n".join(current_value).strip()

        # Validate required fields
        if not data.get("url") or not data.get("title") or not data.get("content"):
            return None

        # Handle "unknown" published_at
        published_at = data.get("published_at")
        if published_at and published_at.lower() == "unknown":
            published_at = None

        return CrawlResultMessage(
            job_id=job_id,
            source_id=source_id,
            url=data["url"],
            title=data["title"],
            content=data["content"],
            published_at=published_at,
            metadata_json=json.dumps({"source": "browser-agent"}),
        )

    def _extract_title_from_content(self, content: str) -> str | None:
        """Extract a meaningful title from content, or return None if not possible."""
        # Skip content that contains raw Python object representations
        if self._is_invalid_agent_output(content):
            return None

        # Try to find a headline-like first line
        lines = content.strip().split("\n")
        for line in lines[:3]:  # Check first 3 lines
            line = line.strip()
            # Skip empty lines and very short lines
            if len(line) < 10:
                continue
            # Skip lines that look like metadata or code
            if any(char in line for char in ["=", "(", ")", "{", "}", "[", "]"]):
                continue
            # Found a good candidate for title
            if len(line) <= 100:
                return line
            return line[:97] + "..."

        # Fallback: use first 100 chars if content looks valid
        clean_content = content.strip()
        if len(clean_content) >= 20:
            return clean_content[:97] + "..."
        return None

    def _extract_from_unstructured(
        self, text: str, job_id: int, source_id: int
    ) -> list[CrawlResultMessage]:
        """Try to extract articles from unstructured agent output."""
        # This is a fallback for when the agent doesn't follow the exact format
        # Look for URL patterns and try to associate content
        articles = []

        # First, validate the entire text is not invalid output
        if self._is_invalid_agent_output(text):
            logger.warning(
                "Skipping unstructured extraction - text contains invalid agent output",
                job_id=job_id,
                text_preview=text[:200] if len(text) > 200 else text,
            )
            return articles

        # Simple heuristic: split by URL patterns
        url_pattern = r"(https?://[^\s]+)"
        parts = re.split(url_pattern, text)

        current_url = None
        current_content = []

        for part in parts:
            if re.match(url_pattern, part):
                # Save previous article if exists
                if current_url and current_content:
                    content = " ".join(current_content).strip()
                    if len(content) > 100:  # Minimum content length
                        # Extract a valid title from content
                        title = self._extract_title_from_content(content)
                        if title:
                            articles.append(
                                CrawlResultMessage(
                                    job_id=job_id,
                                    source_id=source_id,
                                    url=current_url,
                                    title=title,
                                    content=content,
                                    published_at=None,
                                    metadata_json=json.dumps(
                                        {"source": "browser-agent", "extraction": "unstructured"}
                                    ),
                                )
                            )
                        else:
                            logger.debug(
                                "Skipping article - could not extract valid title",
                                job_id=job_id,
                                url=current_url,
                            )
                current_url = part
                current_content = []
            else:
                current_content.append(part)

        return articles

    async def _send_callback(self, task: BrowserTaskMessage, session: CrawlSession) -> None:
        """Send completion callback to the configured URL.

        The callback payload matches the schema expected by
        AutoCrawlController.handleCrawlerCallback() in data-collection-service:
        - targetId: CrawlTarget ID from metadata
        - urlHash: URL hash from metadata
        - success: boolean indicating success/failure
        - collectedDataId: (optional) ID of saved CollectedData
        - error: error message if failed
        """
        if not task.callback_url:
            return

        # Extract AutoCrawl metadata if available
        target_id = None
        url_hash = None
        if session.metadata:
            target_id = session.metadata.get("targetId")
            url_hash = session.metadata.get("urlHash")

        # Build callback payload matching Java's CrawlerCallbackRequest
        callback_data = {
            "targetId": int(target_id) if target_id else session.job_id,
            "urlHash": url_hash,
            "success": session.error is None,
            "collectedDataId": None,  # Will be set by CrawlResultConsumer
            "error": session.error,
            # Include additional stats for debugging/monitoring
            "articlesExtracted": len(session.extracted_articles),
            "pagesVisited": len(session.visited_urls),
            "elapsedSeconds": (
                (session.end_time - session.start_time).total_seconds()
                if session.end_time and session.start_time
                else 0
            ),
        }

        headers = {"Content-Type": "application/json"}
        if task.callback_token:
            headers["Authorization"] = f"Bearer {task.callback_token}"

        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    task.callback_url,
                    json=callback_data,
                    headers=headers,
                    timeout=30.0,
                )
                logger.info(
                    "Callback sent",
                    job_id=session.job_id,
                    target_id=target_id,
                    url_hash=url_hash[:16] if url_hash else None,
                    success=callback_data["success"],
                    callback_url=task.callback_url,
                    status_code=response.status_code,
                )
        except Exception as e:
            logger.error(
                "Failed to send callback",
                job_id=session.job_id,
                callback_url=task.callback_url,
                error=str(e),
            )

```

---

## backend/autonomous-crawler-service/src/crawler/policies.py

```py
"""Crawling policies and prompt generation."""

from enum import Enum


class CrawlPolicy(Enum):
    """Exploration policies for autonomous crawling."""

    # ê¸°ë³¸ ì •ì±…
    FOCUSED_TOPIC = "focused_topic"
    DOMAIN_WIDE = "domain_wide"
    NEWS_ONLY = "news_only"
    CROSS_DOMAIN = "cross_domain"
    SINGLE_PAGE = "single_page"

    # ë‰´ìŠ¤ íŠ¹í™” ì •ì±… (ì‹ ê·œ)
    NEWS_BREAKING = "news_breaking"  # ì†ë³´/ê¸´ê¸‰ ë‰´ìŠ¤ ìš°ì„ 
    NEWS_ARCHIVE = "news_archive"  # ê³¼ê±° ê¸°ì‚¬ ì•„ì¹´ì´ë¸Œ ìˆ˜ì§‘
    NEWS_OPINION = "news_opinion"  # ì˜¤í”¼ë‹ˆì–¸/ì¹¼ëŸ¼ ìˆ˜ì§‘
    NEWS_LOCAL = "news_local"  # ì§€ì—­ ë‰´ìŠ¤ íŠ¹í™”


# Base system prompt for all policies
BASE_SYSTEM_PROMPT = """You are an autonomous web crawler agent specialized in extracting news and article content.
Your goal is to navigate websites, identify valuable content, and extract structured information.

## Core Behaviors:
1. Navigate to the seed URL first
2. Identify and extract article content (title, body text, publication date, author)
3. Find relevant links to other articles/pages based on the policy
4. Avoid non-content pages (login, signup, ads, social media shares)
5. Respect the page budget and depth limits

## Content Extraction Guidelines:
- Extract the main article title (usually h1 or article header)
- Extract the full article body text, preserving paragraphs
- Look for publication date in meta tags, article headers, or bylines
- Skip navigation menus, footers, sidebars, and advertisements
- If a page is not an article, briefly note what type of page it is and move on

## Navigation Rules:
- Prioritize links that appear to be article links (news headlines, blog posts)
- Avoid external links unless specifically allowed by the policy
- Skip links to media files (images, PDFs, videos)
- Skip pagination if you've already seen the content pattern

## Output Format:
For each article extracted, use this format:
---ARTICLE_START---
URL: [article URL]
TITLE: [headline]
AUTHOR: [author name if found]
PUBLISHED_AT: [publication date in ISO format if found]
CATEGORY: [category/section if identified]
CONTENT: [full article text]
---ARTICLE_END---
"""

POLICY_PROMPTS = {
    CrawlPolicy.FOCUSED_TOPIC: """
## Policy: FOCUSED_TOPIC
Focus exclusively on content related to the specified keywords/topics.

### Specific Instructions:
- Only follow links that appear related to the focus keywords: {focus_keywords}
- Prioritize articles with titles containing the keywords
- Skip unrelated content even if it looks interesting
- Extract articles that discuss or mention the focus topics
- Look for related terms and synonyms of the focus keywords
""",
    CrawlPolicy.DOMAIN_WIDE: """
## Policy: DOMAIN_WIDE
Explore the entire domain broadly to discover all available content.

### Specific Instructions:
- Follow links to all content sections of the website
- Prioritize category/section pages that lead to more articles
- Create a broad coverage of the site's content
- Balance between depth and breadth of exploration
- Identify and visit major site sections (news, blog, articles, etc.)
""",
    CrawlPolicy.NEWS_ONLY: """
## Policy: NEWS_ONLY
Focus strictly on news articles and current events content.

### Specific Instructions:
- Only extract content that appears to be news articles
- Look for date indicators showing recent publication
- Prioritize breaking news, current events, and timely content
- Skip evergreen content, guides, and static pages
- Follow links from news sections, headlines, and latest articles
- Identify news patterns: bylines, datelines, news categories
""",
    CrawlPolicy.CROSS_DOMAIN: """
## Policy: CROSS_DOMAIN
Follow links across different domains to discover related content.

### Specific Instructions:
- You may follow external links to other websites
- Prioritize links that appear to lead to related news sources
- Respect the excluded domains list if provided
- Track which domains you've visited to ensure diversity
- Extract content from each domain you visit
- Be cautious of redirect chains and avoid loops
""",
    CrawlPolicy.SINGLE_PAGE: """
## Policy: SINGLE_PAGE
Extract content only from the seed URL without following any links.

### Specific Instructions:
- Do NOT navigate to any other pages
- Focus entirely on extracting content from the current page
- Extract all article content, metadata, and structured data
- Identify any embedded content or data on the page
- This is a single-page extraction task only
""",
    CrawlPolicy.NEWS_BREAKING: """
## Policy: NEWS_BREAKING
Priority collection of breaking news and urgent updates.

### Specific Instructions:
- Look for visual indicators of breaking news:
  - Labels: "ì†ë³´", "Breaking", "ê¸´ê¸‰", "ë‹¨ë…", "Flash", "Urgent"
  - Red or highlighted text, special formatting
  - Pinned or featured articles at the top
- Prioritize articles published in the last few hours
- Extract the FULL content of breaking news articles
- Note the exact publication time if available
- Skip older news and evergreen content
- Look for live update sections or real-time feeds
- Mark each article as breaking: true in metadata
""",
    CrawlPolicy.NEWS_ARCHIVE: """
## Policy: NEWS_ARCHIVE
Historical article collection from archives.

### Specific Instructions:
- Navigate through pagination and archive pages
- Look for "ì´ì „ ê¸°ì‚¬", "ë”ë³´ê¸°", "Load More" buttons
- Accept older publication dates (weeks, months, or years old)
- Follow links to category archives and date-based listings
- Collect articles systematically by date or category
- Note the original publication date accurately
- Skip duplicate or redirected articles
- Be patient with slower-loading archive pages
""",
    CrawlPolicy.NEWS_OPINION: """
## Policy: NEWS_OPINION
Focus on opinion pieces, editorials, and columns.

### Specific Instructions:
- Look for opinion/editorial sections:
  - "ì˜¤í”¼ë‹ˆì–¸", "ì¹¼ëŸ¼", "ì‚¬ì„¤", "Opinion", "Editorial", "Column"
  - Author-focused pages with byline photos
- Identify opinion content markers:
  - Personal pronouns and subjective language
  - Author bio sections
  - Regular column series
- Extract author information prominently
- Mark content as opinion: true in metadata
- Note if the author is a regular columnist
- Skip straight news reporting
""",
    CrawlPolicy.NEWS_LOCAL: """
## Policy: NEWS_LOCAL
Local and regional news collection.

### Specific Instructions:
- Focus on local news sections:
  - "ì§€ì—­", "Local", geographic region names
  - City or province-specific categories
- Look for location markers in articles:
  - City names, district names
  - Local government references
  - Regional business news
- Prioritize community-focused stories
- Note the geographic focus of each article
- Skip national or international news
- Include local events and announcements
""",
}


def get_policy_prompt(
    policy: CrawlPolicy | str,
    focus_keywords: list[str] | None = None,
    custom_prompt: str | None = None,
    excluded_domains: list[str] | None = None,
) -> str:
    """
    Generate the full system prompt for the crawler agent.

    Args:
        policy: The crawling policy to use
        focus_keywords: Keywords for FOCUSED_TOPIC policy
        custom_prompt: Optional custom instructions to append
        excluded_domains: Domains to exclude from crawling

    Returns:
        Complete system prompt for the browser-use agent
    """
    # Convert string to enum if needed
    if isinstance(policy, str):
        try:
            policy = CrawlPolicy(policy.lower())
        except ValueError:
            policy = CrawlPolicy.NEWS_ONLY  # Default fallback

    # Build the prompt
    prompt_parts = [BASE_SYSTEM_PROMPT]

    # Add policy-specific instructions
    policy_prompt = POLICY_PROMPTS.get(policy, POLICY_PROMPTS[CrawlPolicy.NEWS_ONLY])

    # Format with focus keywords if applicable
    if policy == CrawlPolicy.FOCUSED_TOPIC and focus_keywords:
        policy_prompt = policy_prompt.format(focus_keywords=", ".join(focus_keywords))
    else:
        policy_prompt = policy_prompt.replace("{focus_keywords}", "")

    prompt_parts.append(policy_prompt)

    # Add excluded domains if any
    if excluded_domains:
        prompt_parts.append(f"""
## Excluded Domains:
Do NOT visit or follow links to these domains:
{chr(10).join(f"- {d}" for d in excluded_domains)}
""")

    # Add custom instructions if provided
    if custom_prompt:
        prompt_parts.append(f"""
## Custom Instructions:
{custom_prompt}
""")

    return "\n".join(prompt_parts)


def get_extraction_prompt(url: str) -> str:
    """
    Generate the task prompt for extracting content from a specific page.

    Args:
        url: The URL to extract content from

    Returns:
        Task prompt for content extraction
    """
    return f"""
Extract the article content from this page: {url}

Return the extracted content in the following format:
1. TITLE: The main article headline
2. CONTENT: The full article text (preserve paragraph breaks)
3. PUBLISHED_AT: The publication date if found (ISO format preferred)
4. AUTHOR: The author name if found
5. SUMMARY: A brief 2-3 sentence summary of the article

If this is not an article page, indicate what type of page it is.
"""


def get_news_list_extraction_prompt(url: str, max_articles: int = 20) -> str:
    """
    Generate prompt for extracting article list from a news section page.

    Args:
        url: The news section/category URL
        max_articles: Maximum number of articles to extract

    Returns:
        Task prompt for list extraction
    """
    return f"""
Extract the list of news articles from this page: {url}

Find up to {max_articles} news articles and return each in this format:
---ARTICLE_LINK---
TITLE: [article headline]
URL: [full article URL]
SUMMARY: [brief description or lead text if visible]
PUBLISHED_AT: [date if shown]
THUMBNAIL: [image URL if present]
---END_LINK---

Focus on:
- Main content area article links
- Skip navigation, ads, and sidebar widgets
- Include only actual news article links
- Preserve the order as shown on the page
"""


def get_rss_discovery_prompt(url: str) -> str:
    """
    Generate prompt for discovering RSS/Atom feeds.

    Args:
        url: The website URL to search for feeds

    Returns:
        Task prompt for RSS discovery
    """
    return f"""
Find all RSS and Atom feeds available on this website: {url}

Search in these locations:
1. HTML head section:
   - <link rel="alternate" type="application/rss+xml" ...>
   - <link rel="alternate" type="application/atom+xml" ...>
2. Common feed paths:
   - /feed, /rss, /atom, /feeds
   - /rss.xml, /feed.xml, /atom.xml
   - /news/rss, /blog/feed
3. Page footer or sidebar RSS icons/links
4. sitemap.xml references

Return found feeds in JSON format:
{{
    "feeds": [
        {{
            "url": "feed URL",
            "type": "rss|atom",
            "title": "feed title if known",
            "category": "category if specified"
        }}
    ],
    "sitemap_url": "sitemap URL if found",
    "robots_txt_checked": true|false
}}
"""

```

---

## backend/autonomous-crawler-service/src/kafka/__init__.py

```py
"""Kafka module for autonomous-crawler-service."""

from .consumer import BrowserTaskConsumer
from .producer import CrawlResultProducer
from .messages import BrowserTaskMessage, CrawlResultMessage

__all__ = [
    "BrowserTaskConsumer",
    "CrawlResultProducer",
    "BrowserTaskMessage",
    "CrawlResultMessage",
]

```

---

## backend/autonomous-crawler-service/src/kafka/consumer.py

```py
"""Kafka consumer for browser task messages."""

import asyncio
import json
from typing import AsyncGenerator, Callable, Awaitable

import structlog
from aiokafka import AIOKafkaConsumer
from aiokafka.errors import KafkaError

from src.config import Settings
from src.kafka.messages import BrowserTaskMessage

logger = structlog.get_logger(__name__)


class BrowserTaskConsumer:
    """Async Kafka consumer for browser task messages."""

    def __init__(self, settings: Settings) -> None:
        self.settings = settings
        self._consumer: AIOKafkaConsumer | None = None
        self._running = False

    async def start(self) -> None:
        """Start the Kafka consumer with retry logic."""
        kafka_settings = self.settings.kafka

        self._consumer = AIOKafkaConsumer(
            kafka_settings.browser_task_topic,
            bootstrap_servers=kafka_settings.bootstrap_servers,
            group_id=kafka_settings.consumer_group_id,
            auto_offset_reset=kafka_settings.auto_offset_reset,
            enable_auto_commit=kafka_settings.enable_auto_commit,
            max_poll_records=kafka_settings.max_poll_records,
            session_timeout_ms=kafka_settings.session_timeout_ms,
            heartbeat_interval_ms=kafka_settings.heartbeat_interval_ms,
            value_deserializer=lambda m: json.loads(m.decode("utf-8")),
        )

        # Retry connection with exponential backoff
        max_retries = 10
        retry_delay = 2
        for attempt in range(max_retries):
            try:
                await self._consumer.start()
                self._running = True
                logger.info(
                    "Kafka consumer started",
                    topic=kafka_settings.browser_task_topic,
                    group_id=kafka_settings.consumer_group_id,
                )
                return
            except Exception as e:
                if attempt < max_retries - 1:
                    logger.warning(
                        "Failed to connect to Kafka, retrying...",
                        attempt=attempt + 1,
                        max_retries=max_retries,
                        retry_delay=retry_delay,
                        error=str(e),
                    )
                    await asyncio.sleep(retry_delay)
                    retry_delay = min(retry_delay * 2, 30)  # Max 30 seconds
                else:
                    logger.error(
                        "Failed to connect to Kafka after all retries",
                        error=str(e),
                    )
                    raise

    async def stop(self) -> None:
        """Stop the Kafka consumer."""
        self._running = False
        if self._consumer:
            await self._consumer.stop()
            logger.info("Kafka consumer stopped")

    async def consume(self) -> AsyncGenerator[BrowserTaskMessage, None]:
        """
        Consume messages from Kafka topic.

        Yields BrowserTaskMessage objects. Caller is responsible for
        committing offsets after successful processing.
        """
        if not self._consumer:
            raise RuntimeError("Consumer not started. Call start() first.")

        while self._running:
            try:
                # Get batch of messages (max_poll_records=1 means one at a time)
                result = await self._consumer.getmany(timeout_ms=1000)

                for topic_partition, messages in result.items():
                    for msg in messages:
                        try:
                            task = BrowserTaskMessage.model_validate(msg.value)
                            logger.info(
                                "Received browser task",
                                job_id=task.job_id,
                                source_id=task.source_id,
                                seed_url=task.seed_url,
                                offset=msg.offset,
                            )
                            yield task

                            # Commit after successful processing
                            await self._consumer.commit()
                            logger.debug(
                                "Committed offset",
                                offset=msg.offset,
                                partition=topic_partition.partition,
                            )

                        except Exception as e:
                            logger.error(
                                "Failed to parse browser task message",
                                error=str(e),
                                raw_value=msg.value,
                            )
                            # Still commit to avoid infinite retry on malformed messages
                            await self._consumer.commit()

            except KafkaError as e:
                logger.error("Kafka consumer error", error=str(e))
                await asyncio.sleep(1)  # Back off on errors

    async def run_with_handler(
        self,
        handler: Callable[[BrowserTaskMessage], Awaitable[None]],
    ) -> None:
        """
        Run consumer with a message handler callback.

        Args:
            handler: Async function to process each message
        """
        async for task in self.consume():
            try:
                await handler(task)
            except Exception as e:
                logger.error(
                    "Handler failed for task",
                    job_id=task.job_id,
                    error=str(e),
                    exc_info=True,
                )
                # Continue processing next messages

```

---

## backend/autonomous-crawler-service/src/kafka/messages.py

```py
"""Kafka message schemas matching Java DTOs."""

import json
from datetime import datetime
from typing import Any, Optional, Union

from pydantic import BaseModel, Field, field_validator


def parse_java_datetime(value: Any) -> datetime | None:
    """
    Parse datetime from various formats:
    - ISO-8601 string: "2025-12-17T11:29:39"
    - Java LocalDateTime array: [2025, 12, 17, 11, 29, 39, 532902301]
    - Python datetime object
    - None
    """
    if value is None:
        return None
    if isinstance(value, datetime):
        return value
    if isinstance(value, str):
        # ISO-8601 string format
        try:
            # Try with microseconds
            return datetime.fromisoformat(value.replace("Z", "+00:00"))
        except ValueError:
            # Try without timezone
            return datetime.fromisoformat(value)
    if isinstance(value, (list, tuple)) and len(value) >= 6:
        # Java LocalDateTime array format: [year, month, day, hour, minute, second, nano?]
        year, month, day, hour, minute, second = value[:6]
        microsecond = value[6] // 1000 if len(value) > 6 else 0  # nano to micro
        return datetime(year, month, day, hour, minute, second, microsecond)
    raise ValueError(f"Cannot parse datetime from: {value} (type: {type(value).__name__})")


class BrowserTaskMessage(BaseModel):
    """
    Kafka message for browser-based autonomous crawling tasks.
    Matches: com.newsinsight.collector.dto.BrowserTaskMessage
    """

    job_id: int = Field(..., alias="jobId", description="Unique job ID for tracking")
    source_id: int = Field(..., alias="sourceId", description="Data source ID")
    source_name: str | None = Field(
        default=None, alias="sourceName", description="Source name for logging/display"
    )
    seed_url: str = Field(..., alias="seedUrl", description="Seed URL to start exploration from")
    max_depth: int | None = Field(
        default=2, alias="maxDepth", description="Maximum link traversal depth"
    )
    max_pages: int | None = Field(
        default=10, alias="maxPages", description="Maximum pages to visit"
    )
    budget_seconds: int | None = Field(
        default=300, alias="budgetSeconds", description="Time budget in seconds"
    )
    policy: str | None = Field(
        default="NEWS_ONLY",
        description="Exploration policy (focused_topic, domain_wide, news_only, etc.)",
    )
    focus_keywords: str | None = Field(
        default=None, alias="focusKeywords", description="Focus keywords for FOCUSED_TOPIC policy"
    )
    custom_prompt: str | None = Field(
        default=None, alias="customPrompt", description="Custom prompt/instructions for AI agent"
    )
    capture_screenshots: bool | None = Field(
        default=False, alias="captureScreenshots", description="Whether to capture screenshots"
    )
    extract_structured: bool | None = Field(
        default=True, alias="extractStructured", description="Whether to extract structured data"
    )
    excluded_domains: str | None = Field(
        default=None, alias="excludedDomains", description="Domains to exclude"
    )
    callback_url: str | None = Field(
        default=None, alias="callbackUrl", description="Callback URL for session completion"
    )
    callback_token: str | None = Field(
        default=None, alias="callbackToken", description="Callback authentication token"
    )
    metadata: dict[str, Any] | None = Field(default=None, description="Additional metadata")
    created_at: datetime | None = Field(
        default=None, alias="createdAt", description="Task creation timestamp"
    )

    # Validator to handle Java LocalDateTime array format
    @field_validator("created_at", mode="before")
    @classmethod
    def parse_created_at(cls, value: Any) -> datetime | None:
        """Parse createdAt from Java LocalDateTime array or ISO string."""
        return parse_java_datetime(value)

    class Config:
        populate_by_name = True

    def get_excluded_domains_list(self) -> list[str]:
        """Parse excluded domains string into list."""
        if not self.excluded_domains:
            return []
        return [d.strip() for d in self.excluded_domains.split(",") if d.strip()]

    def get_focus_keywords_list(self) -> list[str]:
        """Parse focus keywords string into list."""
        if not self.focus_keywords:
            return []
        return [k.strip() for k in self.focus_keywords.split(",") if k.strip()]


class CrawlResultMessage(BaseModel):
    """
    Kafka message for crawl results.
    Matches: com.newsinsight.collector.dto.CrawlResultMessage

    ê¸°ë³¸ í•„ë“œëŠ” Java DTOì™€ í˜¸í™˜ë˜ë©°, ì¶”ê°€ ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„°ëŠ” metadata_jsonì— JSONìœ¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤.
    """

    job_id: int = Field(..., alias="jobId", description="Job ID this result belongs to")
    source_id: int = Field(..., alias="sourceId", description="Data source ID")
    title: str = Field(..., description="Article/page title")
    content: str = Field(..., description="Extracted content")
    url: str = Field(..., description="Page URL")
    published_at: str | None = Field(
        default=None, alias="publishedAt", description="Publication date as ISO string"
    )
    metadata_json: str | None = Field(
        default=None, alias="metadataJson", description="Additional metadata as JSON string"
    )

    class Config:
        populate_by_name = True

    def to_kafka_dict(self) -> dict[str, Any]:
        """Convert to Kafka-compatible dict with Java-style camelCase keys."""
        return {
            "jobId": self.job_id,
            "sourceId": self.source_id,
            "title": self.title,
            "content": self.content,
            "url": self.url,
            "publishedAt": self.published_at,
            "metadataJson": self.metadata_json,
        }


class NewsArticleMetadata(BaseModel):
    """
    ë‰´ìŠ¤ ê¸°ì‚¬ ì „ìš© ë©”íƒ€ë°ì´í„°.

    CrawlResultMessage.metadata_jsonì— JSONìœ¼ë¡œ ì§ë ¬í™”ë˜ì–´ ì €ì¥ë©ë‹ˆë‹¤.
    Java ì¸¡ì—ì„œëŠ” ì´ JSONì„ íŒŒì‹±í•˜ì—¬ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """

    # ê¸°ì/ì‘ì„±ì ì •ë³´
    authors: list[str] | None = Field(default=None, description="ê¸°ì‚¬ ì‘ì„±ì/ê¸°ì ëª©ë¡")
    author_email: str | None = Field(default=None, description="ê¸°ì ì´ë©”ì¼")

    # ë¶„ë¥˜ ì •ë³´
    category: str | None = Field(default=None, description="ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ (ì •ì¹˜, ê²½ì œ, ì‚¬íšŒ ë“±)")
    subcategory: str | None = Field(default=None, description="ì„¸ë¶€ ì¹´í…Œê³ ë¦¬")
    tags: list[str] | None = Field(default=None, description="ê¸°ì‚¬ íƒœê·¸/í‚¤ì›Œë“œ")

    # ì½˜í…ì¸  íŠ¹ì„±
    word_count: int | None = Field(default=None, description="ë³¸ë¬¸ ë‹¨ì–´ ìˆ˜")
    reading_time_minutes: float | None = Field(default=None, description="ì˜ˆìƒ ì½ê¸° ì‹œê°„ (ë¶„)")
    language: str | None = Field(default="ko", description="ê¸°ì‚¬ ì–¸ì–´ ì½”ë“œ")

    # ë‰´ìŠ¤ íŠ¹ì„±
    is_breaking: bool = Field(default=False, description="ì†ë³´ ì—¬ë¶€")
    is_exclusive: bool = Field(default=False, description="ë‹¨ë… ê¸°ì‚¬ ì—¬ë¶€")
    is_opinion: bool = Field(default=False, description="ì˜¤í”¼ë‹ˆì–¸/ì¹¼ëŸ¼ ì—¬ë¶€")
    has_paywall: bool = Field(default=False, description="ìœ ë£Œ êµ¬ë… í•„ìš” ì—¬ë¶€")

    # ë¯¸ë””ì–´ ì •ë³´
    thumbnail_url: str | None = Field(default=None, description="ëŒ€í‘œ ì´ë¯¸ì§€ URL")
    image_urls: list[str] | None = Field(default=None, description="ë³¸ë¬¸ ì´ë¯¸ì§€ URL ëª©ë¡")
    video_urls: list[str] | None = Field(default=None, description="ê´€ë ¨ ë™ì˜ìƒ URL ëª©ë¡")

    # ê´€ë ¨ ì½˜í…ì¸ 
    related_article_urls: list[str] | None = Field(default=None, description="ê´€ë ¨ ê¸°ì‚¬ URL ëª©ë¡")

    # ì†ŒìŠ¤ ì •ë³´
    source_name: str | None = Field(default=None, description="ì–¸ë¡ ì‚¬/ë§¤ì²´ëª…")
    source_bias: str | None = Field(default=None, description="ì •ì¹˜ ì„±í–¥ (ì•Œë ¤ì§„ ê²½ìš°)")

    # ì¶”ì¶œ í’ˆì§ˆ
    extraction_confidence: float | None = Field(default=None, description="ì¶”ì¶œ ì‹ ë¢°ë„ (0.0-1.0)")
    missing_fields: list[str] | None = Field(default=None, description="ì¶”ì¶œ ì‹¤íŒ¨í•œ í•„ë“œ ëª©ë¡")

    # ìˆ˜ì§‘ ì •ë³´
    crawled_at: str | None = Field(default=None, description="ìˆ˜ì§‘ ì‹œê°„ (ISO í˜•ì‹)")
    crawl_method: str | None = Field(default=None, description="ìˆ˜ì§‘ ë°©ë²• (ai_agent, rss, api)")

    def to_json(self) -> str:
        """JSON ë¬¸ìì—´ë¡œ ì§ë ¬í™”"""
        return self.model_dump_json(exclude_none=True)

    @classmethod
    def from_json(cls, json_str: str) -> "NewsArticleMetadata":
        """JSON ë¬¸ìì—´ì—ì„œ ë³µì›"""
        return cls.model_validate_json(json_str)


class EnhancedCrawlResultMessage(CrawlResultMessage):
    """
    í™•ì¥ëœ í¬ë¡¤ë§ ê²°ê³¼ ë©”ì‹œì§€.

    ê¸°ë³¸ CrawlResultMessageì™€ í˜¸í™˜ë˜ë©´ì„œ ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„°ë¥¼ í¸ë¦¬í•˜ê²Œ ë‹¤ë£° ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """

    def set_news_metadata(self, metadata: NewsArticleMetadata) -> None:
        """ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„° ì„¤ì •"""
        self.metadata_json = metadata.to_json()

    def get_news_metadata(self) -> Optional[NewsArticleMetadata]:
        """ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„° ì¡°íšŒ"""
        if not self.metadata_json:
            return None
        try:
            return NewsArticleMetadata.from_json(self.metadata_json)
        except Exception:
            return None

    @classmethod
    def create_news_result(
        cls,
        job_id: int,
        source_id: int,
        url: str,
        title: str,
        content: str,
        published_at: str | None = None,
        authors: list[str] | None = None,
        category: str | None = None,
        is_breaking: bool = False,
        thumbnail_url: str | None = None,
        source_name: str | None = None,
        word_count: int | None = None,
        **extra_metadata,
    ) -> "EnhancedCrawlResultMessage":
        """
        ë‰´ìŠ¤ ê¸°ì‚¬ ê²°ê³¼ ë©”ì‹œì§€ë¥¼ í¸ë¦¬í•˜ê²Œ ìƒì„±.

        Args:
            job_id: ì‘ì—… ID
            source_id: ì†ŒìŠ¤ ID
            url: ê¸°ì‚¬ URL
            title: ê¸°ì‚¬ ì œëª©
            content: ê¸°ì‚¬ ë³¸ë¬¸
            published_at: ë°œí–‰ì¼ (ISO í˜•ì‹)
            authors: ê¸°ì ëª©ë¡
            category: ì¹´í…Œê³ ë¦¬
            is_breaking: ì†ë³´ ì—¬ë¶€
            thumbnail_url: ì¸ë„¤ì¼ URL
            source_name: ì–¸ë¡ ì‚¬ëª…
            word_count: ë‹¨ì–´ ìˆ˜
            **extra_metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°

        Returns:
            EnhancedCrawlResultMessage ì¸ìŠ¤í„´ìŠ¤
        """
        # ë‹¨ì–´ ìˆ˜ ìë™ ê³„ì‚°
        if word_count is None and content:
            word_count = len(content.split())

        # ë©”íƒ€ë°ì´í„° ìƒì„±
        metadata = NewsArticleMetadata(
            authors=authors,
            category=category,
            is_breaking=is_breaking,
            thumbnail_url=thumbnail_url,
            source_name=source_name,
            word_count=word_count,
            crawled_at=datetime.utcnow().isoformat() + "Z",
            crawl_method="ai_agent",
            **extra_metadata,
        )

        # ê²°ê³¼ ë©”ì‹œì§€ ìƒì„±
        result = cls(
            jobId=job_id,
            sourceId=source_id,
            url=url,
            title=title,
            content=content,
            publishedAt=published_at,
        )
        result.set_news_metadata(metadata)

        return result


class CrawlSessionCallback(BaseModel):
    """
    í¬ë¡¤ë§ ì„¸ì…˜ ì™„ë£Œ ì½œë°± ë©”ì‹œì§€.

    autonomous-crawler-serviceê°€ í¬ë¡¤ë§ ì™„ë£Œ í›„ data-collection-serviceì— ì•Œë¦¼.
    """

    job_id: int = Field(..., alias="jobId", description="ì‘ì—… ID")
    source_id: int = Field(..., alias="sourceId", description="ì†ŒìŠ¤ ID")
    status: str = Field(..., description="ì™„ë£Œ ìƒíƒœ (COMPLETED, FAILED, TIMEOUT)")
    articles_extracted: int = Field(
        default=0, alias="articlesExtracted", description="ì¶”ì¶œëœ ê¸°ì‚¬ ìˆ˜"
    )
    pages_visited: int = Field(default=0, alias="pagesVisited", description="ë°©ë¬¸í•œ í˜ì´ì§€ ìˆ˜")
    elapsed_seconds: float = Field(default=0, alias="elapsedSeconds", description="ì†Œìš” ì‹œê°„ (ì´ˆ)")
    error: str | None = Field(default=None, description="ì—ëŸ¬ ë©”ì‹œì§€ (ì‹¤íŒ¨ ì‹œ)")
    captcha_encountered: bool = Field(
        default=False, alias="captchaEncountered", description="CAPTCHA ë°œê²¬ ì—¬ë¶€"
    )
    captcha_solved: bool = Field(
        default=False, alias="captchaSolved", description="CAPTCHA í•´ê²° ì—¬ë¶€"
    )

    class Config:
        populate_by_name = True

    def to_callback_dict(self) -> dict[str, Any]:
        """ì½œë°± API í˜¸ì¶œìš© ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "jobId": self.job_id,
            "sourceId": self.source_id,
            "status": self.status,
            "articlesExtracted": self.articles_extracted,
            "pagesVisited": self.pages_visited,
            "elapsedSeconds": self.elapsed_seconds,
            "error": self.error,
            "captchaEncountered": self.captcha_encountered,
            "captchaSolved": self.captcha_solved,
        }

```

---

## backend/autonomous-crawler-service/src/kafka/producer.py

```py
"""Kafka producer for crawl result messages."""

import json
from typing import Any

import structlog
from aiokafka import AIOKafkaProducer
from aiokafka.errors import KafkaError
from tenacity import retry, stop_after_attempt, wait_exponential

from src.config import Settings
from src.kafka.messages import CrawlResultMessage

logger = structlog.get_logger(__name__)


class CrawlResultProducer:
    """Async Kafka producer for crawl result messages."""

    def __init__(self, settings: Settings) -> None:
        self.settings = settings
        self._producer: AIOKafkaProducer | None = None

    async def start(self) -> None:
        """Start the Kafka producer."""
        self._producer = AIOKafkaProducer(
            bootstrap_servers=self.settings.kafka.bootstrap_servers,
            value_serializer=lambda v: json.dumps(v, default=str).encode("utf-8"),
            # Reliability settings
            acks="all",
        )

        await self._producer.start()
        logger.info(
            "Kafka producer started",
            bootstrap_servers=self.settings.kafka.bootstrap_servers,
        )

    async def stop(self) -> None:
        """Stop the Kafka producer."""
        if self._producer:
            await self._producer.stop()
            logger.info("Kafka producer stopped")

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=10),
    )
    async def send_result(self, result: CrawlResultMessage) -> None:
        """
        Send a crawl result to Kafka.

        Args:
            result: CrawlResultMessage to send
        """
        if not self._producer:
            raise RuntimeError("Producer not started. Call start() first.")

        topic = self.settings.kafka.crawl_result_topic
        value = result.to_kafka_dict()

        try:
            # Use job_id as key for partitioning (all results for same job go to same partition)
            key = str(result.job_id).encode("utf-8")

            await self._producer.send_and_wait(
                topic=topic,
                value=value,
                key=key,
            )

            logger.info(
                "Sent crawl result",
                job_id=result.job_id,
                source_id=result.source_id,
                url=result.url,
                title=result.title[:50] if result.title else None,
            )

        except KafkaError as e:
            logger.error(
                "Failed to send crawl result",
                job_id=result.job_id,
                error=str(e),
            )
            raise

    async def send_batch(self, results: list[CrawlResultMessage]) -> tuple[int, int]:
        """
        Send multiple crawl results to Kafka.

        Args:
            results: List of CrawlResultMessage objects

        Returns:
            Tuple of (successful_count, failed_count)
        """
        success_count = 0
        fail_count = 0

        for result in results:
            try:
                await self.send_result(result)
                success_count += 1
            except Exception as e:
                logger.error(
                    "Failed to send result in batch",
                    job_id=result.job_id,
                    url=result.url,
                    error=str(e),
                )
                fail_count += 1

        logger.info(
            "Batch send completed",
            success=success_count,
            failed=fail_count,
            total=len(results),
        )

        return success_count, fail_count

```

---

## backend/autonomous-crawler-service/src/main.py

```py
"""
Main entry point for autonomous-crawler-service.

Supports two modes:
1. Kafka mode (default): Consumes tasks from Kafka, produces results to Kafka
2. API mode: REST API + SSE for direct web UI access (browser-agent integration)
3. Hybrid mode: Both Kafka consumer and REST API running together

Usage:
    # Kafka mode (default)
    python -m src.main

    # API mode only
    python -m src.main --mode api

    # Hybrid mode (both Kafka + API)
    python -m src.main --mode hybrid

    # Or via environment variable
    SERVICE_MODE=hybrid python -m src.main
"""

import argparse
import asyncio
import logging
import os
import signal
import sys
from contextlib import asynccontextmanager
from enum import Enum
from typing import AsyncGenerator, Optional

import structlog
from prometheus_client import start_http_server

from src.config import Settings, get_settings
from src.config.consul import load_config_from_consul, wait_for_consul, CONSUL_ENABLED
from src.crawler import AutonomousCrawlerAgent
from src.kafka import BrowserTaskConsumer, CrawlResultProducer
from src.kafka.messages import BrowserTaskMessage
from src.api.sse import SSEEventType, SSEManager, get_sse_manager
from src.metrics import (
    ARTICLES_EXTRACTED,
    BROWSER_SESSIONS_ACTIVE,
    KAFKA_MESSAGES_CONSUMED,
    KAFKA_MESSAGES_PRODUCED,
    TASK_DURATION,
    TASKS_COMPLETED,
    TASKS_IN_PROGRESS,
    TASKS_RECEIVED,
    init_service_info,
)


class ServiceMode(str, Enum):
    """Service operation mode"""

    KAFKA = "kafka"  # Kafka consumer only (original)
    API = "api"  # REST API only (browser-agent style)
    HYBRID = "hybrid"  # Both Kafka + REST API


def configure_logging(settings: Settings) -> None:
    """Configure structured logging."""
    processors = [
        structlog.contextvars.merge_contextvars,
        structlog.processors.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
    ]

    if settings.log_format == "json":
        processors.append(structlog.processors.JSONRenderer())
    else:
        processors.append(structlog.dev.ConsoleRenderer(colors=True))

    # Map log level string to logging module level
    log_level = getattr(logging, settings.log_level.upper(), logging.INFO)

    structlog.configure(
        processors=processors,
        wrapper_class=structlog.make_filtering_bound_logger(log_level),
        context_class=dict,
        logger_factory=structlog.PrintLoggerFactory(),
        cache_logger_on_first_use=True,
    )


class CrawlerService:
    """Main service class orchestrating the crawler components."""

    def __init__(self, settings: Settings, sse_manager: SSEManager | None = None) -> None:
        self.settings = settings
        self.logger = structlog.get_logger(__name__)
        self.consumer = BrowserTaskConsumer(settings)
        self.producer = CrawlResultProducer(settings)
        self.agent = AutonomousCrawlerAgent(settings)
        self.sse_manager = sse_manager or get_sse_manager()
        self._shutdown_event = asyncio.Event()

    async def start(self) -> None:
        """Start all service components."""
        self.logger.info("Starting autonomous-crawler-service")

        # Start metrics server
        if self.settings.metrics.enabled:
            start_http_server(self.settings.metrics.port)
            self.logger.info(
                "Metrics server started",
                port=self.settings.metrics.port,
            )

        # Initialize service info metrics
        init_service_info(
            version="0.1.0",
            llm_provider=self.settings.llm.provider,
        )

        # Start Kafka components
        await self.consumer.start()
        await self.producer.start()

        BROWSER_SESSIONS_ACTIVE.set(0)

        self.logger.info("Service started successfully")

    async def stop(self) -> None:
        """Stop all service components."""
        self.logger.info("Stopping autonomous-crawler-service")

        self._shutdown_event.set()

        await self.agent.close()
        await self.consumer.stop()
        await self.producer.stop()

        self.logger.info("Service stopped")

    async def handle_task(self, task: BrowserTaskMessage) -> None:
        """
        Handle a single browser task.

        Args:
            task: The browser task to process
        """
        import time

        start_time = time.time()
        policy = task.policy or "news_only"
        source_name = (
            task.metadata.get("source_name", f"Source-{task.source_id}")
            if task.metadata
            else f"Source-{task.source_id}"
        )

        TASKS_RECEIVED.labels(policy=policy).inc()
        TASKS_IN_PROGRESS.inc()
        BROWSER_SESSIONS_ACTIVE.inc()
        KAFKA_MESSAGES_CONSUMED.labels(topic=self.settings.kafka.browser_task_topic).inc()

        self.logger.info(
            "Processing browser task",
            job_id=task.job_id,
            source_id=task.source_id,
            seed_url=task.seed_url,
            policy=policy,
        )

        # SSE: ìˆ˜ì§‘ ì‹œì‘ ì´ë²¤íŠ¸
        await self.sse_manager.send_collection_event(
            SSEEventType.COLLECTION_START,
            source_name=source_name,
            message=f"ìˆ˜ì§‘ ì‹œì‘: {task.seed_url}",
            level="INFO",
            job_id=task.job_id,
            source_id=task.source_id,
            seed_url=task.seed_url,
            policy=policy,
        )

        status = "success"
        try:
            # Execute the crawl task
            results = await self.agent.execute_task(task)

            # Send results to Kafka
            for i, result in enumerate(results):
                await self.producer.send_result(result)
                KAFKA_MESSAGES_PRODUCED.labels(topic=self.settings.kafka.crawl_result_topic).inc()
                ARTICLES_EXTRACTED.labels(source_id=str(task.source_id)).inc()

                # SSE: ìˆ˜ì§‘ ì§„í–‰ ì´ë²¤íŠ¸ (ë§¤ 5ê°œ ì•„í‹°í´ë§ˆë‹¤)
                if (i + 1) % 5 == 0 or i == len(results) - 1:
                    await self.sse_manager.send_collection_event(
                        SSEEventType.COLLECTION_PROGRESS,
                        source_name=source_name,
                        message=f"ì•„í‹°í´ {i + 1}/{len(results)} ìˆ˜ì§‘ ì™„ë£Œ",
                        level="INFO",
                        job_id=task.job_id,
                        progress=i + 1,
                        total=len(results),
                    )

            self.logger.info(
                "Task completed",
                job_id=task.job_id,
                articles_extracted=len(results),
            )

            # SSE: ìˆ˜ì§‘ ì™„ë£Œ ì´ë²¤íŠ¸
            duration = time.time() - start_time
            await self.sse_manager.send_collection_event(
                SSEEventType.COLLECTION_COMPLETE,
                source_name=source_name,
                message=f"ìˆ˜ì§‘ ì™„ë£Œ: {len(results)}ê°œ ì•„í‹°í´ ({duration:.1f}ì´ˆ)",
                level="INFO",
                job_id=task.job_id,
                articles_extracted=len(results),
                duration_seconds=duration,
            )

        except Exception as e:
            status = "error"
            self.logger.error(
                "Task failed",
                job_id=task.job_id,
                error=str(e),
                exc_info=True,
            )

            # SSE: ìˆ˜ì§‘ ì—ëŸ¬ ì´ë²¤íŠ¸
            await self.sse_manager.send_collection_event(
                SSEEventType.COLLECTION_ERROR,
                source_name=source_name,
                message=f"ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)[:100]}",
                level="ERROR",
                job_id=task.job_id,
                error=str(e),
            )

        finally:
            duration = time.time() - start_time
            TASK_DURATION.labels(policy=policy).observe(duration)
            TASKS_COMPLETED.labels(policy=policy, status=status).inc()
            TASKS_IN_PROGRESS.dec()
            BROWSER_SESSIONS_ACTIVE.dec()

    async def run(self) -> None:
        """Main run loop - consume and process tasks."""
        await self.start()

        try:
            await self.consumer.run_with_handler(self.handle_task)
        except asyncio.CancelledError:
            self.logger.info("Run loop cancelled")
        finally:
            await self.stop()


async def run_api_server(settings: Settings, port: int = 8030) -> None:
    """Run the FastAPI REST API server."""
    import uvicorn
    from src.api.server import create_app

    logger = structlog.get_logger(__name__)
    logger.info("Starting REST API server", port=port)

    app = create_app(settings)

    config = uvicorn.Config(
        app,
        host="0.0.0.0",
        port=port,
        log_level="info",
        access_log=True,
    )
    server = uvicorn.Server(config)
    await server.serve()


async def run_hybrid_mode(settings: Settings, api_port: int = 8030) -> None:
    """Run both Kafka consumer and REST API server concurrently."""
    logger = structlog.get_logger(__name__)
    logger.info(
        "Starting hybrid mode",
        api_port=api_port,
        kafka_enabled=True,
    )

    # Import here to avoid circular imports
    import uvicorn
    from src.api.server import create_app

    # Create services
    service = CrawlerService(settings)
    app = create_app(settings)

    # Create uvicorn config
    config = uvicorn.Config(
        app,
        host="0.0.0.0",
        port=api_port,
        log_level="info",
        access_log=True,
    )
    api_server = uvicorn.Server(config)

    # Setup signal handling
    shutdown_event = asyncio.Event()

    async def shutdown():
        logger.info("Shutting down hybrid mode...")
        shutdown_event.set()
        await service.stop()
        api_server.should_exit = True

    loop = asyncio.get_running_loop()
    for sig in (signal.SIGINT, signal.SIGTERM):
        loop.add_signal_handler(sig, lambda: asyncio.create_task(shutdown()))

    # Start Kafka service components (without the run loop)
    await service.start()

    # Run both concurrently
    async def kafka_consumer_loop():
        try:
            await service.consumer.run_with_handler(service.handle_task)
        except asyncio.CancelledError:
            logger.info("Kafka consumer loop cancelled")

    await asyncio.gather(
        kafka_consumer_loop(),
        api_server.serve(),
        return_exceptions=True,
    )


async def main_async(mode: ServiceMode = ServiceMode.KAFKA) -> None:
    """Async main function with mode selection."""
    # Load configuration from Consul (if enabled)
    consul_keys, env_keys = [], []
    if CONSUL_ENABLED:
        # Wait for Consul to be available
        if wait_for_consul(max_attempts=30, delay=2.0):
            consul_keys, env_keys = load_config_from_consul()
        else:
            print(
                "WARNING: Consul not available, using environment variables only", file=sys.stderr
            )

    settings = get_settings()
    configure_logging(settings)

    logger = structlog.get_logger(__name__)

    api_port = int(os.getenv("API_PORT", "8030"))

    logger.info(
        "Initializing autonomous-crawler-service",
        mode=mode.value,
        kafka_servers=settings.kafka.bootstrap_servers,
        llm_provider=settings.llm.provider,
        consul_enabled=CONSUL_ENABLED,
        consul_keys_loaded=len(consul_keys),
        api_port=api_port if mode in (ServiceMode.API, ServiceMode.HYBRID) else None,
    )

    if mode == ServiceMode.KAFKA:
        # Original Kafka-only mode
        service = CrawlerService(settings)

        loop = asyncio.get_running_loop()

        def signal_handler() -> None:
            logger.info("Received shutdown signal")
            asyncio.create_task(service.stop())

        for sig in (signal.SIGINT, signal.SIGTERM):
            loop.add_signal_handler(sig, signal_handler)

        await service.run()

    elif mode == ServiceMode.API:
        # REST API only mode (browser-agent style)
        await run_api_server(settings, api_port)

    elif mode == ServiceMode.HYBRID:
        # Both Kafka and REST API
        await run_hybrid_mode(settings, api_port)


def parse_args() -> argparse.Namespace:
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Autonomous Crawler Service - AI-driven browser crawler",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Modes:
  kafka   - Kafka consumer only (default, for internal job processing)
  api     - REST API only (browser-agent style, for direct web UI access)
  hybrid  - Both Kafka consumer and REST API running together

Examples:
  python -m src.main                    # Kafka mode (default)
  python -m src.main --mode api         # REST API only
  python -m src.main --mode hybrid      # Both Kafka + REST API
  SERVICE_MODE=hybrid python -m src.main  # Via environment variable
        """,
    )
    parser.add_argument(
        "--mode",
        "-m",
        type=str,
        choices=["kafka", "api", "hybrid"],
        default=os.getenv("SERVICE_MODE", "kafka"),
        help="Service operation mode (default: kafka, or SERVICE_MODE env var)",
    )
    parser.add_argument(
        "--port",
        "-p",
        type=int,
        default=int(os.getenv("API_PORT", "8030")),
        help="API server port (default: 8030, or API_PORT env var)",
    )
    return parser.parse_args()


def main() -> None:
    """Main entry point."""
    args = parse_args()

    # Convert string mode to enum
    mode_map = {
        "kafka": ServiceMode.KAFKA,
        "api": ServiceMode.API,
        "hybrid": ServiceMode.HYBRID,
    }
    mode = mode_map.get(args.mode, ServiceMode.KAFKA)

    # Set API_PORT env var so it's accessible in main_async
    os.environ["API_PORT"] = str(args.port)

    try:
        asyncio.run(main_async(mode))
    except KeyboardInterrupt:
        print("Interrupted")
        sys.exit(0)


if __name__ == "__main__":
    main()

```

---

## backend/autonomous-crawler-service/src/mcp/__init__.py

```py
"""
MCP (Model Context Protocol) Adapter Module

MCP ì„œë²„ë“¤ì„ ML Add-onìœ¼ë¡œ ë˜í•‘í•˜ì—¬ REST APIë¡œ ë…¸ì¶œí•©ë‹ˆë‹¤.
"""

from src.mcp.client import MCPClient
from src.mcp.adapter import MCPAdapter, get_mcp_adapter
from src.mcp.router import router as mcp_router

__all__ = ["MCPClient", "MCPAdapter", "get_mcp_adapter", "mcp_router"]

```

---

## backend/autonomous-crawler-service/src/mcp/adapter.py

```py
"""
MCP Adapter - MCP ì„œë²„ë“¤ì„ ML Add-onìœ¼ë¡œ ë˜í•‘í•˜ëŠ” ì–´ëŒ‘í„°

MCP ì„œë²„ì˜ toolë“¤ì„ REST API í˜•íƒœë¡œ ë…¸ì¶œí•˜ê³ ,
ML Add-on ì¸í„°í˜ì´ìŠ¤ì™€ í˜¸í™˜ë˜ëŠ” ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
"""

import os
from typing import Any, Dict, List, Optional
from datetime import datetime
from enum import Enum

import structlog
from pydantic import BaseModel, Field

from .client import (
    BiasMCPClient,
    FactcheckMCPClient,
    TopicMCPClient,
    HuggingFaceMCPClient,
    NewsInsightMCPClient,
    MCPClient,
)

logger = structlog.get_logger(__name__)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MCP ì„œë²„ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class MCPServerConfig:
    """MCP ì„œë²„ ì—°ê²° ì„¤ì •"""

    BIAS_MCP_URL = os.environ.get("BIAS_MCP_URL", "http://bias-mcp:5001")
    FACTCHECK_MCP_URL = os.environ.get("FACTCHECK_MCP_URL", "http://factcheck-mcp:5002")
    TOPIC_MCP_URL = os.environ.get("TOPIC_MCP_URL", "http://topic-mcp:5003")
    NEWSINSIGHT_MCP_URL = os.environ.get("NEWSINSIGHT_MCP_URL", "http://newsinsight-mcp:5000")
    HUGGINGFACE_MCP_URL = os.environ.get("HUGGINGFACE_MCP_URL", "http://huggingface-mcp:5011")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‘ë‹µ ëª¨ë¸ (ML Add-on í˜¸í™˜)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class MCPAddonCategory(str, Enum):
    """MCP Add-on ì¹´í…Œê³ ë¦¬"""

    BIAS = "BIAS_ANALYSIS"
    FACTCHECK = "FACTCHECK"
    TOPIC = "TOPIC_CLASSIFICATION"
    SENTIMENT = "SENTIMENT"
    SUMMARIZATION = "SUMMARIZATION"
    ENTITY = "ENTITY_EXTRACTION"


class MCPAddonResponse(BaseModel):
    """MCP Add-on í‘œì¤€ ì‘ë‹µ"""

    addon_key: str
    category: MCPAddonCategory
    success: bool
    data: Optional[Dict[str, Any]] = None
    report: Optional[str] = None
    error: Optional[str] = None
    latency_ms: int = 0
    generated_at: str = Field(default_factory=lambda: datetime.utcnow().isoformat())


class MCPAddonInfo(BaseModel):
    """MCP Add-on ì •ë³´"""

    addon_key: str
    name: str
    description: str
    category: MCPAddonCategory
    endpoint_url: str
    tools: List[str] = []
    enabled: bool = True
    health_status: str = "unknown"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MCP ì–´ëŒ‘í„° í´ë˜ìŠ¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class MCPAdapter:
    """MCP ì„œë²„ë“¤ì„ ML Add-onìœ¼ë¡œ ë˜í•‘í•˜ëŠ” ì–´ëŒ‘í„°"""

    def __init__(self):
        self.bias_client = BiasMCPClient(MCPServerConfig.BIAS_MCP_URL)
        self.factcheck_client = FactcheckMCPClient(MCPServerConfig.FACTCHECK_MCP_URL)
        self.topic_client = TopicMCPClient(MCPServerConfig.TOPIC_MCP_URL)
        self.newsinsight_client = NewsInsightMCPClient(MCPServerConfig.NEWSINSIGHT_MCP_URL)
        self.huggingface_client = HuggingFaceMCPClient(MCPServerConfig.HUGGINGFACE_MCP_URL)

        self._clients: Dict[str, MCPClient] = {
            "bias": self.bias_client,
            "factcheck": self.factcheck_client,
            "topic": self.topic_client,
            "newsinsight": self.newsinsight_client,
            "huggingface": self.huggingface_client,
        }

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Add-on ëª©ë¡ ë° ìƒíƒœ ì¡°íšŒ
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def list_addons(self) -> List[MCPAddonInfo]:
        """ë“±ë¡ëœ MCP Add-on ëª©ë¡ ë°˜í™˜"""
        addons = [
            MCPAddonInfo(
                addon_key="mcp-bias",
                name="í¸í–¥ë„ ë¶„ì„ (MCP)",
                description="ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì •ì¹˜ì /ì´ë…ì  í¸í–¥ë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤",
                category=MCPAddonCategory.BIAS,
                endpoint_url=MCPServerConfig.BIAS_MCP_URL,
                tools=["get_bias_raw", "get_bias_report", "get_source_bias_list"],
            ),
            MCPAddonInfo(
                addon_key="mcp-factcheck",
                name="íŒ©íŠ¸ì²´í¬/ì‹ ë¢°ë„ (MCP)",
                description="ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì‹ ë¢°ë„ì™€ íŒ©íŠ¸ì²´í¬ ìƒíƒœë¥¼ ë¶„ì„í•©ë‹ˆë‹¤",
                category=MCPAddonCategory.FACTCHECK,
                endpoint_url=MCPServerConfig.FACTCHECK_MCP_URL,
                tools=[
                    "get_factcheck_raw",
                    "get_factcheck_report",
                    "get_source_reliability_list",
                ],
            ),
            MCPAddonInfo(
                addon_key="mcp-topic",
                name="í† í”½ ë¶„ì„ (MCP)",
                description="ë‰´ìŠ¤ í† í”½, í‚¤ì›Œë“œ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤",
                category=MCPAddonCategory.TOPIC,
                endpoint_url=MCPServerConfig.TOPIC_MCP_URL,
                tools=[
                    "get_topic_raw",
                    "get_topic_report",
                    "get_trending_topics",
                    "get_category_list",
                ],
            ),
            MCPAddonInfo(
                addon_key="mcp-sentiment",
                name="ê°ì„± ë¶„ì„ (MCP)",
                description="ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ê°ì„±(ê¸ì •/ë¶€ì •/ì¤‘ë¦½)ì„ ë¶„ì„í•©ë‹ˆë‹¤",
                category=MCPAddonCategory.SENTIMENT,
                endpoint_url=MCPServerConfig.NEWSINSIGHT_MCP_URL,
                tools=[
                    "get_sentiment_raw",
                    "get_sentiment_report",
                    "get_article_list",
                ],
            ),
            MCPAddonInfo(
                addon_key="mcp-huggingface",
                name="HuggingFace NLP (MCP)",
                description="HuggingFace ëª¨ë¸ì„ í™œìš©í•œ NLP ë¶„ì„",
                category=MCPAddonCategory.SUMMARIZATION,
                endpoint_url=MCPServerConfig.HUGGINGFACE_MCP_URL,
                tools=[
                    "analyze_sentiment",
                    "summarize_article",
                    "extract_entities",
                    "extract_keywords",
                    "classify_news",
                ],
            ),
        ]
        return addons

    async def check_all_health(self) -> Dict[str, Dict[str, Any]]:
        """ëª¨ë“  MCP ì„œë²„ í—¬ìŠ¤ì²´í¬"""
        results = {}
        for name, client in self._clients.items():
            results[name] = await client.health_check()
        return results

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Bias Analysis
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def analyze_bias(
        self,
        keyword: str,
        days: int = 7,
        include_report: bool = False,
    ) -> MCPAddonResponse:
        """í¸í–¥ë„ ë¶„ì„ ì‹¤í–‰"""
        start_time = datetime.utcnow()

        try:
            result = await self.bias_client.get_bias_raw(keyword, days)

            if not result.get("success"):
                return MCPAddonResponse(
                    addon_key="mcp-bias",
                    category=MCPAddonCategory.BIAS,
                    success=False,
                    error=result.get("error", "Unknown error"),
                    latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
                )

            response = MCPAddonResponse(
                addon_key="mcp-bias",
                category=MCPAddonCategory.BIAS,
                success=True,
                data=result.get("data"),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

            # ë¦¬í¬íŠ¸ë„ í•¨ê»˜ ìš”ì²­ëœ ê²½ìš°
            if include_report:
                report_result = await self.bias_client.get_bias_report(keyword, days)
                if report_result.get("success"):
                    response.report = report_result.get("data")

            return response

        except Exception as e:
            logger.error("Bias analysis failed", error=str(e))
            return MCPAddonResponse(
                addon_key="mcp-bias",
                category=MCPAddonCategory.BIAS,
                success=False,
                error=str(e),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Factcheck Analysis
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def analyze_factcheck(
        self,
        keyword: str,
        days: int = 7,
        include_report: bool = False,
    ) -> MCPAddonResponse:
        """íŒ©íŠ¸ì²´í¬/ì‹ ë¢°ë„ ë¶„ì„ ì‹¤í–‰"""
        start_time = datetime.utcnow()

        try:
            result = await self.factcheck_client.get_factcheck_raw(keyword, days)

            if not result.get("success"):
                return MCPAddonResponse(
                    addon_key="mcp-factcheck",
                    category=MCPAddonCategory.FACTCHECK,
                    success=False,
                    error=result.get("error", "Unknown error"),
                    latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
                )

            response = MCPAddonResponse(
                addon_key="mcp-factcheck",
                category=MCPAddonCategory.FACTCHECK,
                success=True,
                data=result.get("data"),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

            if include_report:
                report_result = await self.factcheck_client.get_factcheck_report(keyword, days)
                if report_result.get("success"):
                    response.report = report_result.get("data")

            return response

        except Exception as e:
            logger.error("Factcheck analysis failed", error=str(e))
            return MCPAddonResponse(
                addon_key="mcp-factcheck",
                category=MCPAddonCategory.FACTCHECK,
                success=False,
                error=str(e),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Topic Analysis
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def analyze_topic(
        self,
        keyword: Optional[str] = None,
        days: int = 7,
        include_report: bool = False,
    ) -> MCPAddonResponse:
        """í† í”½ ë¶„ì„ ì‹¤í–‰"""
        start_time = datetime.utcnow()

        try:
            result = await self.topic_client.get_topic_raw(keyword, days)

            if not result.get("success"):
                return MCPAddonResponse(
                    addon_key="mcp-topic",
                    category=MCPAddonCategory.TOPIC,
                    success=False,
                    error=result.get("error", "Unknown error"),
                    latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
                )

            response = MCPAddonResponse(
                addon_key="mcp-topic",
                category=MCPAddonCategory.TOPIC,
                success=True,
                data=result.get("data"),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

            if include_report:
                report_result = await self.topic_client.get_topic_report(keyword, days)
                if report_result.get("success"):
                    response.report = report_result.get("data")

            return response

        except Exception as e:
            logger.error("Topic analysis failed", error=str(e))
            return MCPAddonResponse(
                addon_key="mcp-topic",
                category=MCPAddonCategory.TOPIC,
                success=False,
                error=str(e),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

    async def get_trending_topics(self, days: int = 1, limit: int = 10) -> MCPAddonResponse:
        """íŠ¸ë Œë”© í† í”½ ì¡°íšŒ"""
        start_time = datetime.utcnow()

        try:
            result = await self.topic_client.get_trending_topics(days, limit)

            return MCPAddonResponse(
                addon_key="mcp-topic",
                category=MCPAddonCategory.TOPIC,
                success=result.get("success", False),
                data=result.get("data"),
                error=result.get("error"),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

        except Exception as e:
            logger.error("Get trending topics failed", error=str(e))
            return MCPAddonResponse(
                addon_key="mcp-topic",
                category=MCPAddonCategory.TOPIC,
                success=False,
                error=str(e),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Sentiment Analysis
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def analyze_sentiment(
        self,
        keyword: str,
        days: int = 7,
        include_report: bool = False,
    ) -> MCPAddonResponse:
        """ê°ì„± ë¶„ì„ ì‹¤í–‰"""
        start_time = datetime.utcnow()

        try:
            result = await self.newsinsight_client.get_sentiment_raw(keyword, days)

            if not result.get("success"):
                return MCPAddonResponse(
                    addon_key="mcp-sentiment",
                    category=MCPAddonCategory.SENTIMENT,
                    success=False,
                    error=result.get("error", "Unknown error"),
                    latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
                )

            response = MCPAddonResponse(
                addon_key="mcp-sentiment",
                category=MCPAddonCategory.SENTIMENT,
                success=True,
                data=result.get("data"),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

            if include_report:
                report_result = await self.newsinsight_client.get_sentiment_report(keyword, days)
                if report_result.get("success"):
                    response.report = report_result.get("data")

            return response

        except Exception as e:
            logger.error("Sentiment analysis failed", error=str(e))
            return MCPAddonResponse(
                addon_key="mcp-sentiment",
                category=MCPAddonCategory.SENTIMENT,
                success=False,
                error=str(e),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # HuggingFace NLP
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def summarize_article(
        self,
        text: str,
        max_length: int = 150,
        min_length: int = 50,
    ) -> MCPAddonResponse:
        """ê¸°ì‚¬ ìš”ì•½"""
        start_time = datetime.utcnow()

        try:
            result = await self.huggingface_client.summarize_article(text, max_length, min_length)

            return MCPAddonResponse(
                addon_key="mcp-huggingface",
                category=MCPAddonCategory.SUMMARIZATION,
                success=result.get("success", False),
                data=result.get("data"),
                error=result.get("error"),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

        except Exception as e:
            logger.error("Summarization failed", error=str(e))
            return MCPAddonResponse(
                addon_key="mcp-huggingface",
                category=MCPAddonCategory.SUMMARIZATION,
                success=False,
                error=str(e),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

    async def extract_entities(self, text: str) -> MCPAddonResponse:
        """ê°œì²´ëª… ì¸ì‹"""
        start_time = datetime.utcnow()

        try:
            result = await self.huggingface_client.extract_entities(text)

            return MCPAddonResponse(
                addon_key="mcp-huggingface",
                category=MCPAddonCategory.ENTITY,
                success=result.get("success", False),
                data=result.get("data"),
                error=result.get("error"),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

        except Exception as e:
            logger.error("Entity extraction failed", error=str(e))
            return MCPAddonResponse(
                addon_key="mcp-huggingface",
                category=MCPAddonCategory.ENTITY,
                success=False,
                error=str(e),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_mcp_adapter: Optional[MCPAdapter] = None


def get_mcp_adapter() -> MCPAdapter:
    """MCP ì–´ëŒ‘í„° ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _mcp_adapter
    if _mcp_adapter is None:
        _mcp_adapter = MCPAdapter()
    return _mcp_adapter

```

---

## backend/autonomous-crawler-service/src/mcp/client.py

```py
"""
MCP Client - MCP ì„œë²„ JSON-RPC í˜¸ì¶œ í´ë¼ì´ì–¸íŠ¸

MCP ì„œë²„ë“¤ (bias, factcheck, topic, huggingface ë“±)ì—
JSON-RPC í˜•ì‹ìœ¼ë¡œ toolì„ í˜¸ì¶œí•˜ëŠ” í´ë¼ì´ì–¸íŠ¸ì…ë‹ˆë‹¤.
"""

import asyncio
import json
from typing import Any, Dict, List, Optional
from datetime import datetime

import httpx
import structlog

logger = structlog.get_logger(__name__)


class MCPClient:
    """MCP ì„œë²„ JSON-RPC í´ë¼ì´ì–¸íŠ¸"""

    def __init__(
        self,
        base_url: str,
        timeout: float = 60.0,
        health_check_path: str = "/health",
    ):
        """
        Args:
            base_url: MCP ì„œë²„ ë² ì´ìŠ¤ URL (ì˜ˆ: http://bias-mcp:5001)
            timeout: HTTP ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
            health_check_path: í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ ê²½ë¡œ
        """
        self.base_url = base_url.rstrip("/")
        self.timeout = timeout
        self.health_check_path = health_check_path
        self._mcp_path = "/mcp"

    async def health_check(self) -> Dict[str, Any]:
        """ì„œë²„ í—¬ìŠ¤ì²´í¬"""
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                resp = await client.get(f"{self.base_url}{self.health_check_path}")
                if resp.status_code == 200:
                    return {"status": "healthy", "data": resp.json()}
                return {"status": "unhealthy", "status_code": resp.status_code}
            except Exception as e:
                return {"status": "error", "error": str(e)}

    async def call_tool(
        self,
        tool_name: str,
        arguments: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        MCP ì„œë²„ì˜ toolì„ í˜¸ì¶œí•©ë‹ˆë‹¤.

        Args:
            tool_name: í˜¸ì¶œí•  tool ì´ë¦„ (ì˜ˆ: get_bias_raw)
            arguments: tool ì¸ì

        Returns:
            tool ì‹¤í–‰ ê²°ê³¼
        """
        # JSON-RPC 2.0 í˜•ì‹ì˜ ìš”ì²­ ìƒì„±
        request_payload = {
            "jsonrpc": "2.0",
            "method": "tools/call",
            "params": {
                "name": tool_name,
                "arguments": arguments or {},
            },
            "id": int(datetime.utcnow().timestamp() * 1000),
        }

        async with httpx.AsyncClient(timeout=self.timeout) as client:
            try:
                logger.debug(
                    "Calling MCP tool",
                    url=f"{self.base_url}{self._mcp_path}",
                    tool=tool_name,
                    arguments=arguments,
                )

                resp = await client.post(
                    f"{self.base_url}{self._mcp_path}",
                    json=request_payload,
                    headers={"Content-Type": "application/json"},
                )

                if resp.status_code != 200:
                    error_text = resp.text
                    logger.error(
                        "MCP call failed",
                        tool=tool_name,
                        status_code=resp.status_code,
                        error=error_text,
                    )
                    return {
                        "success": False,
                        "error": f"HTTP {resp.status_code}: {error_text}",
                    }

                result = resp.json()

                # JSON-RPC ì—ëŸ¬ ì²´í¬
                if "error" in result:
                    error = result["error"]
                    return {
                        "success": False,
                        "error": error.get("message", str(error)),
                        "code": error.get("code"),
                    }

                # ì„±ê³µ ì‘ë‹µ
                return {
                    "success": True,
                    "data": result.get("result"),
                }

            except httpx.TimeoutException:
                logger.error("MCP call timeout", tool=tool_name)
                return {"success": False, "error": "Request timeout"}
            except Exception as e:
                logger.error("MCP call exception", tool=tool_name, error=str(e))
                return {"success": False, "error": str(e)}

    async def list_tools(self) -> Dict[str, Any]:
        """MCP ì„œë²„ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ tool ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
        request_payload = {
            "jsonrpc": "2.0",
            "method": "tools/list",
            "params": {},
            "id": int(datetime.utcnow().timestamp() * 1000),
        }

        async with httpx.AsyncClient(timeout=self.timeout) as client:
            try:
                resp = await client.post(
                    f"{self.base_url}{self._mcp_path}",
                    json=request_payload,
                    headers={"Content-Type": "application/json"},
                )

                if resp.status_code != 200:
                    return {"success": False, "error": f"HTTP {resp.status_code}"}

                result = resp.json()
                return {
                    "success": True,
                    "tools": result.get("result", {}).get("tools", []),
                }
            except Exception as e:
                return {"success": False, "error": str(e)}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MCP ì„œë²„ë³„ íŠ¹í™” í´ë¼ì´ì–¸íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class BiasMCPClient(MCPClient):
    """Bias Analysis MCP í´ë¼ì´ì–¸íŠ¸"""

    async def get_bias_raw(self, keyword: str, days: int = 7) -> Dict[str, Any]:
        """í‚¤ì›Œë“œ ê´€ë ¨ í¸í–¥ë„ ë¶„ì„ ë°ì´í„° ì¡°íšŒ"""
        return await self.call_tool("get_bias_raw", {"keyword": keyword, "days": days})

    async def get_bias_report(
        self, keyword: str, days: int = 7, session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """í¸í–¥ë„ ë¶„ì„ ìì—°ì–´ ë¦¬í¬íŠ¸ ìƒì„±"""
        args = {"keyword": keyword, "days": days}
        if session_id:
            args["session_id"] = session_id
        return await self.call_tool("get_bias_report", args)

    async def get_source_bias_list(self) -> Dict[str, Any]:
        """ì–¸ë¡ ì‚¬ë³„ í¸í–¥ ì°¸ì¡° ë°ì´í„° ì¡°íšŒ"""
        return await self.call_tool("get_source_bias_list")


class FactcheckMCPClient(MCPClient):
    """Fact Check MCP í´ë¼ì´ì–¸íŠ¸"""

    async def get_factcheck_raw(self, keyword: str, days: int = 7) -> Dict[str, Any]:
        """í‚¤ì›Œë“œ ê´€ë ¨ ì‹ ë¢°ë„/íŒ©íŠ¸ì²´í¬ ë°ì´í„° ì¡°íšŒ"""
        return await self.call_tool("get_factcheck_raw", {"keyword": keyword, "days": days})

    async def get_factcheck_report(
        self, keyword: str, days: int = 7, session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """ì‹ ë¢°ë„ ë¶„ì„ ìì—°ì–´ ë¦¬í¬íŠ¸ ìƒì„±"""
        args = {"keyword": keyword, "days": days}
        if session_id:
            args["session_id"] = session_id
        return await self.call_tool("get_factcheck_report", args)

    async def get_source_reliability_list(self) -> Dict[str, Any]:
        """ì–¸ë¡ ì‚¬ë³„ ì‹ ë¢°ë„ ì°¸ì¡° ë°ì´í„° ì¡°íšŒ"""
        return await self.call_tool("get_source_reliability_list")


class TopicMCPClient(MCPClient):
    """Topic Analysis MCP í´ë¼ì´ì–¸íŠ¸"""

    async def get_topic_raw(self, keyword: Optional[str] = None, days: int = 7) -> Dict[str, Any]:
        """í† í”½ ë¶„ì„ ë°ì´í„° ì¡°íšŒ"""
        return await self.call_tool("get_topic_raw", {"keyword": keyword, "days": days})

    async def get_topic_report(
        self,
        keyword: Optional[str] = None,
        days: int = 7,
        session_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """í† í”½ ë¶„ì„ ìì—°ì–´ ë¦¬í¬íŠ¸ ìƒì„±"""
        args = {"keyword": keyword, "days": days}
        if session_id:
            args["session_id"] = session_id
        return await self.call_tool("get_topic_report", args)

    async def get_trending_topics(self, days: int = 1, limit: int = 10) -> Dict[str, Any]:
        """íŠ¸ë Œë”© í† í”½ ëª©ë¡ ì¡°íšŒ"""
        return await self.call_tool("get_trending_topics", {"days": days, "limit": limit})

    async def get_category_list(self) -> Dict[str, Any]:
        """ì¹´í…Œê³ ë¦¬ ëª©ë¡ ì¡°íšŒ"""
        return await self.call_tool("get_category_list")


class HuggingFaceMCPClient(MCPClient):
    """Hugging Face MCP í´ë¼ì´ì–¸íŠ¸"""

    async def analyze_sentiment(self, text: str, model_id: Optional[str] = None) -> Dict[str, Any]:
        """ê°ì„± ë¶„ì„"""
        args = {"text": text}
        if model_id:
            args["model_id"] = model_id
        return await self.call_tool("analyze_sentiment", args)

    async def summarize_article(
        self, text: str, max_length: int = 150, min_length: int = 50
    ) -> Dict[str, Any]:
        """ê¸°ì‚¬ ìš”ì•½"""
        return await self.call_tool(
            "summarize_article",
            {"text": text, "max_length": max_length, "min_length": min_length},
        )

    async def extract_entities(self, text: str) -> Dict[str, Any]:
        """ê°œì²´ëª… ì¸ì‹"""
        return await self.call_tool("extract_entities", {"text": text})

    async def extract_keywords(self, text: str, top_k: int = 10) -> Dict[str, Any]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        return await self.call_tool("extract_keywords", {"text": text, "top_k": top_k})

    async def classify_news(
        self, text: str, categories: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """ë‰´ìŠ¤ ë¶„ë¥˜"""
        args: Dict[str, Any] = {"text": text}
        if categories:
            args["categories"] = categories
        return await self.call_tool("classify_news", args)


class NewsInsightMCPClient(MCPClient):
    """NewsInsight MCP í´ë¼ì´ì–¸íŠ¸"""

    async def get_sentiment_raw(self, keyword: str, days: int = 7) -> Dict[str, Any]:
        """ê°ì„± ë¶„ì„ ë°ì´í„° ì¡°íšŒ"""
        return await self.call_tool("get_sentiment_raw", {"keyword": keyword, "days": days})

    async def get_sentiment_report(
        self, keyword: str, days: int = 7, session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """ê°ì„± ë¶„ì„ ìì—°ì–´ ë¦¬í¬íŠ¸ ìƒì„±"""
        args = {"keyword": keyword, "days": days}
        if session_id:
            args["session_id"] = session_id
        return await self.call_tool("get_sentiment_report", args)

    async def get_article_list(
        self, keyword: str, days: int = 7, limit: int = 50
    ) -> Dict[str, Any]:
        """ê¸°ì‚¬ ëª©ë¡ ì¡°íšŒ"""
        return await self.call_tool(
            "get_article_list", {"keyword": keyword, "days": days, "limit": limit}
        )

    async def get_discussion_summary(self, keyword: str, days: int = 7) -> Dict[str, Any]:
        """í† ë¡  ìš”ì•½ ì¡°íšŒ"""
        return await self.call_tool("get_discussion_summary", {"keyword": keyword, "days": days})

```

---

## backend/autonomous-crawler-service/src/mcp/router.py

```py
"""
MCP API Router - MCP Add-on REST API ë¼ìš°í„°

MCP ì–´ëŒ‘í„°ë¥¼ í†µí•´ MCP ì„œë²„ë“¤ì˜ ê¸°ëŠ¥ì„ REST APIë¡œ ë…¸ì¶œí•©ë‹ˆë‹¤.
"""

from typing import Any, Dict, List, Optional

import structlog
from fastapi import APIRouter, HTTPException, Query
from pydantic import BaseModel, Field

from .adapter import MCPAdapter, MCPAddonResponse, MCPAddonInfo, get_mcp_adapter

logger = structlog.get_logger(__name__)

router = APIRouter(prefix="/mcp", tags=["MCP Add-ons"])


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Request Models
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class KeywordAnalysisRequest(BaseModel):
    """í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„ ìš”ì²­"""

    keyword: str = Field(..., description="ë¶„ì„í•  í‚¤ì›Œë“œ", min_length=1, max_length=100)
    days: int = Field(default=7, ge=1, le=90, description="ë¶„ì„ ê¸°ê°„ (ì¼)")
    include_report: bool = Field(default=False, description="ìì—°ì–´ ë¦¬í¬íŠ¸ í¬í•¨ ì—¬ë¶€")


class TextAnalysisRequest(BaseModel):
    """í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶„ì„ ìš”ì²­"""

    text: str = Field(..., description="ë¶„ì„í•  í…ìŠ¤íŠ¸", min_length=10, max_length=50000)
    max_length: int = Field(default=150, ge=50, le=500, description="ìš”ì•½ ìµœëŒ€ ê¸¸ì´")
    min_length: int = Field(default=50, ge=20, le=200, description="ìš”ì•½ ìµœì†Œ ê¸¸ì´")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Add-on ê´€ë¦¬ ì—”ë“œí¬ì¸íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@router.get("/addons", response_model=List[MCPAddonInfo])
async def list_mcp_addons():
    """
    ë“±ë¡ëœ MCP Add-on ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    adapter = get_mcp_adapter()
    return await adapter.list_addons()


@router.get("/health")
async def check_mcp_health():
    """
    ëª¨ë“  MCP ì„œë²„ì˜ í—¬ìŠ¤ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    """
    adapter = get_mcp_adapter()
    results = await adapter.check_all_health()

    # ì „ì²´ ìƒíƒœ ìš”ì•½
    healthy_count = sum(1 for r in results.values() if r.get("status") == "healthy")
    total_count = len(results)

    return {
        "status": "healthy" if healthy_count == total_count else "degraded",
        "healthy": healthy_count,
        "total": total_count,
        "servers": results,
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Bias Analysis ì—”ë“œí¬ì¸íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@router.post("/bias/analyze", response_model=MCPAddonResponse)
async def analyze_bias(request: KeywordAnalysisRequest):
    """
    í‚¤ì›Œë“œ ê´€ë ¨ ë‰´ìŠ¤ì˜ í¸í–¥ë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

    - ì •ì¹˜ì /ì´ë…ì  í¸í–¥ ìŠ¤í™íŠ¸ëŸ¼ ë¶„ì„
    - ì–¸ë¡ ì‚¬ë³„ í¸í–¥ ë¶„í¬
    - ê°ê´€ì„± ì ìˆ˜
    """
    adapter = get_mcp_adapter()
    return await adapter.analyze_bias(
        keyword=request.keyword,
        days=request.days,
        include_report=request.include_report,
    )


@router.get("/bias/sources")
async def get_source_bias_list():
    """
    ì–¸ë¡ ì‚¬ë³„ ì¼ë°˜ì ì¸ í¸í–¥ ì„±í–¥ ì°¸ì¡° ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    adapter = get_mcp_adapter()
    result = await adapter.bias_client.get_source_bias_list()
    if not result.get("success"):
        raise HTTPException(status_code=500, detail=result.get("error"))
    return result.get("data")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Factcheck Analysis ì—”ë“œí¬ì¸íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@router.post("/factcheck/analyze", response_model=MCPAddonResponse)
async def analyze_factcheck(request: KeywordAnalysisRequest):
    """
    í‚¤ì›Œë“œ ê´€ë ¨ ë‰´ìŠ¤ì˜ ì‹ ë¢°ë„ ë° íŒ©íŠ¸ì²´í¬ ìƒíƒœë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

    - ì „ì²´ ì‹ ë¢°ë„ ì ìˆ˜
    - ì–¸ë¡ ì‚¬ë³„ ì‹ ë¢°ë„
    - ì£¼ì¥/ê²€ì¦ ë¹„ìœ¨
    - ì¸ìš© í’ˆì§ˆ ì ìˆ˜
    """
    adapter = get_mcp_adapter()
    return await adapter.analyze_factcheck(
        keyword=request.keyword,
        days=request.days,
        include_report=request.include_report,
    )


@router.get("/factcheck/sources")
async def get_source_reliability_list():
    """
    ì–¸ë¡ ì‚¬ë³„ ê¸°ë³¸ ì‹ ë¢°ë„ ì°¸ì¡° ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    adapter = get_mcp_adapter()
    result = await adapter.factcheck_client.get_source_reliability_list()
    if not result.get("success"):
        raise HTTPException(status_code=500, detail=result.get("error"))
    return result.get("data")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Topic Analysis ì—”ë“œí¬ì¸íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@router.post("/topics/analyze", response_model=MCPAddonResponse)
async def analyze_topic(request: KeywordAnalysisRequest):
    """
    í‚¤ì›Œë“œ ê´€ë ¨(ë˜ëŠ” ì „ì²´) ë‰´ìŠ¤ì˜ í† í”½ì„ ë¶„ì„í•©ë‹ˆë‹¤.

    - ì£¼ìš” í‚¤ì›Œë“œ/í† í”½ íŠ¸ë Œë“œ
    - ì¹´í…Œê³ ë¦¬ ë¶„í¬
    - íƒ€ì„ë¼ì¸ ë¶„ì„
    - ê´€ë ¨ ì—”í‹°í‹°
    """
    adapter = get_mcp_adapter()
    return await adapter.analyze_topic(
        keyword=request.keyword,
        days=request.days,
        include_report=request.include_report,
    )


@router.get("/topics/trending", response_model=MCPAddonResponse)
async def get_trending_topics(
    days: int = Query(default=1, ge=1, le=7, description="ë¶„ì„ ê¸°ê°„ (ì¼)"),
    limit: int = Query(default=10, ge=1, le=50, description="ë°˜í™˜í•  í† í”½ ìˆ˜"),
):
    """
    ìµœê·¼ Nì¼ê°„ íŠ¸ë Œë”© í† í”½ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.

    ëŒ€ì‹œë³´ë“œ ìœ„ì ¯ìš© APIì…ë‹ˆë‹¤.
    """
    adapter = get_mcp_adapter()
    return await adapter.get_trending_topics(days=days, limit=limit)


@router.get("/topics/categories")
async def get_category_list():
    """
    ì§€ì›í•˜ëŠ” ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    adapter = get_mcp_adapter()
    result = await adapter.topic_client.get_category_list()
    if not result.get("success"):
        raise HTTPException(status_code=500, detail=result.get("error"))
    return result.get("data")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Sentiment Analysis ì—”ë“œí¬ì¸íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@router.post("/sentiment/analyze", response_model=MCPAddonResponse)
async def analyze_sentiment(request: KeywordAnalysisRequest):
    """
    í‚¤ì›Œë“œ ê´€ë ¨ ë‰´ìŠ¤ì˜ ê°ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤.

    - ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ë¶„í¬
    - ê°ì„± íŠ¸ë Œë“œ
    - ì–¸ë¡ ì‚¬ë³„ ê°ì„± ì°¨ì´
    """
    adapter = get_mcp_adapter()
    return await adapter.analyze_sentiment(
        keyword=request.keyword,
        days=request.days,
        include_report=request.include_report,
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# HuggingFace NLP ì—”ë“œí¬ì¸íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@router.post("/nlp/summarize", response_model=MCPAddonResponse)
async def summarize_article(request: TextAnalysisRequest):
    """
    í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.

    HuggingFace ëª¨ë¸ì„ ì‚¬ìš©í•œ abstractive summarizationì…ë‹ˆë‹¤.
    """
    adapter = get_mcp_adapter()
    return await adapter.summarize_article(
        text=request.text,
        max_length=request.max_length,
        min_length=request.min_length,
    )


@router.post("/nlp/entities", response_model=MCPAddonResponse)
async def extract_entities(text: str = Query(..., description="ë¶„ì„í•  í…ìŠ¤íŠ¸", min_length=10)):
    """
    í…ìŠ¤íŠ¸ì—ì„œ ê°œì²´ëª…(ì¸ë¬¼, ê¸°ê´€, ì¥ì†Œ ë“±)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    adapter = get_mcp_adapter()
    return await adapter.extract_entities(text)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í†µí•© ë¶„ì„ ì—”ë“œí¬ì¸íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@router.post("/analyze/comprehensive")
async def comprehensive_analysis(request: KeywordAnalysisRequest):
    """
    í‚¤ì›Œë“œì— ëŒ€í•œ ì¢…í•© ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    í¸í–¥ë„, ì‹ ë¢°ë„, í† í”½, ê°ì„± ë¶„ì„ì„ ëª¨ë‘ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ í†µí•©í•©ë‹ˆë‹¤.
    """
    adapter = get_mcp_adapter()

    # ë³‘ë ¬ë¡œ ëª¨ë“  ë¶„ì„ ì‹¤í–‰
    import asyncio

    results = await asyncio.gather(
        adapter.analyze_bias(request.keyword, request.days),
        adapter.analyze_factcheck(request.keyword, request.days),
        adapter.analyze_topic(request.keyword, request.days),
        adapter.analyze_sentiment(request.keyword, request.days),
        return_exceptions=True,
    )

    # ê²°ê³¼ í†µí•©
    analysis_results = {
        "keyword": request.keyword,
        "days": request.days,
        "bias": (
            results[0].model_dump()
            if not isinstance(results[0], Exception)
            else {"error": str(results[0])}
        ),
        "factcheck": (
            results[1].model_dump()
            if not isinstance(results[1], Exception)
            else {"error": str(results[1])}
        ),
        "topic": (
            results[2].model_dump()
            if not isinstance(results[2], Exception)
            else {"error": str(results[2])}
        ),
        "sentiment": (
            results[3].model_dump()
            if not isinstance(results[3], Exception)
            else {"error": str(results[3])}
        ),
    }

    # ì„±ê³µë¥  ê³„ì‚°
    success_count = sum(1 for r in results if not isinstance(r, Exception) and r.success)

    return {
        "success": success_count > 0,
        "success_rate": success_count / 4,
        "results": analysis_results,
    }

```

---

## backend/autonomous-crawler-service/src/metrics/__init__.py

```py
"""Prometheus metrics for autonomous-crawler-service."""

from prometheus_client import Counter, Gauge, Histogram, Info

# Service info
SERVICE_INFO = Info("crawler_service", "Autonomous crawler service information")

# ========================================
# Task metrics
# ========================================

TASKS_RECEIVED = Counter(
    "crawler_tasks_received_total",
    "Total number of crawl tasks received from Kafka",
    ["policy"],
)

TASKS_COMPLETED = Counter(
    "crawler_tasks_completed_total",
    "Total number of crawl tasks completed",
    ["policy", "status"],
)

TASKS_IN_PROGRESS = Gauge(
    "crawler_tasks_in_progress",
    "Number of crawl tasks currently in progress",
)

# API-based task metrics (browser-agent compatibility)
API_CRAWL_TASKS = Counter(
    "crawler_api_tasks_total",
    "Total API-based crawl tasks",
    ["status", "llm_provider"],
)

# ========================================
# Article extraction metrics
# ========================================

ARTICLES_EXTRACTED = Counter(
    "crawler_articles_extracted_total",
    "Total number of articles extracted",
    ["source_id"],
)

PAGES_VISITED = Counter(
    "crawler_pages_visited_total",
    "Total number of pages visited",
    ["domain"],
)

URLS_DISCOVERED = Counter(
    "crawler_urls_discovered_total",
    "Total URLs discovered by agent",
    ["category"],
)

# ========================================
# Performance metrics
# ========================================

TASK_DURATION = Histogram(
    "crawler_task_duration_seconds",
    "Time spent processing a crawl task",
    ["policy"],
    buckets=[10, 30, 60, 120, 300, 600],
)

API_CRAWL_DURATION = Histogram(
    "crawler_api_crawl_duration_seconds",
    "API crawl task duration",
    ["status"],
    buckets=[5.0, 10.0, 30.0, 60.0, 120.0, 300.0, 600.0],
)

EXTRACTION_DURATION = Histogram(
    "crawler_extraction_duration_seconds",
    "Time spent extracting content from a single page",
    buckets=[1, 2, 5, 10, 30],
)

AGENT_STEPS = Histogram(
    "crawler_agent_steps",
    "Number of steps per agent task",
    ["status"],
    buckets=[1, 2, 5, 10, 20, 30, 50],
)

# ========================================
# Browser metrics
# ========================================

BROWSER_SESSIONS_ACTIVE = Gauge(
    "crawler_browser_sessions_active",
    "Number of active browser sessions",
)

BROWSER_ERRORS = Counter(
    "crawler_browser_errors_total",
    "Total number of browser errors",
    ["error_type"],
)

# ========================================
# CAPTCHA metrics
# ========================================

CAPTCHA_DETECTED = Counter(
    "crawler_captcha_detected_total",
    "Total number of CAPTCHAs detected",
    ["type"],
)

CAPTCHA_SOLVED = Counter(
    "crawler_captcha_solved_total",
    "Total number of CAPTCHAs successfully solved",
    ["type", "method"],
)

CAPTCHA_FAILED = Counter(
    "crawler_captcha_failed_total",
    "Total number of CAPTCHA solve failures",
    ["type", "reason"],
)

# ========================================
# Kafka metrics
# ========================================

KAFKA_MESSAGES_CONSUMED = Counter(
    "crawler_kafka_messages_consumed_total",
    "Total number of Kafka messages consumed",
    ["topic"],
)

KAFKA_MESSAGES_PRODUCED = Counter(
    "crawler_kafka_messages_produced_total",
    "Total number of Kafka messages produced",
    ["topic"],
)

KAFKA_CONSUMER_LAG = Gauge(
    "crawler_kafka_consumer_lag",
    "Kafka consumer lag (messages behind)",
    ["topic", "partition"],
)

# ========================================
# SSE metrics
# ========================================

SSE_CLIENTS_CONNECTED = Gauge(
    "crawler_sse_clients_connected",
    "Number of connected SSE clients",
)

SSE_EVENTS_SENT = Counter(
    "crawler_sse_events_sent_total",
    "Total number of SSE events sent",
    ["event_type"],
)

# ========================================
# Chat metrics
# ========================================

CHAT_REQUESTS = Counter(
    "crawler_chat_requests_total",
    "Total chat requests",
    ["provider", "streaming"],
)

CHAT_TOKENS_USED = Counter(
    "crawler_chat_tokens_used_total",
    "Total tokens used in chat requests",
    ["provider"],
)


def init_service_info(version: str = "0.1.0", llm_provider: str = "openai") -> None:
    """Initialize service info metrics."""
    SERVICE_INFO.info(
        {
            "version": version,
            "llm_provider": llm_provider,
        }
    )


def track_crawl_task(status: str, llm_provider: str, duration_seconds: float, steps: int = 0):
    """Track an API crawl task completion."""
    API_CRAWL_TASKS.labels(status=status, llm_provider=llm_provider).inc()
    API_CRAWL_DURATION.labels(status=status).observe(duration_seconds)
    if steps > 0:
        AGENT_STEPS.labels(status=status).observe(steps)


def track_url_discovery(category: str, count: int = 1):
    """Track URL discovery."""
    URLS_DISCOVERED.labels(category=category).inc(count)


def track_captcha(captcha_type: str, solved: bool, method: str = "unknown", reason: str = ""):
    """Track CAPTCHA detection and resolution."""
    CAPTCHA_DETECTED.labels(type=captcha_type).inc()
    if solved:
        CAPTCHA_SOLVED.labels(type=captcha_type, method=method).inc()
    else:
        CAPTCHA_FAILED.labels(type=captcha_type, reason=reason).inc()


def track_sse_event(event_type: str):
    """Track SSE event sent."""
    SSE_EVENTS_SENT.labels(event_type=event_type).inc()


def track_chat_request(provider: str, streaming: bool, tokens: int = 0):
    """Track chat request."""
    CHAT_REQUESTS.labels(provider=provider, streaming=str(streaming).lower()).inc()
    if tokens > 0:
        CHAT_TOKENS_USED.labels(provider=provider).inc(tokens)

```

---

## backend/autonomous-crawler-service/src/ml/__init__.py

```py
"""
ML Addon Integration for autonomous-crawler-service.

í¬ë¡¤ë§ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ ML ì• ë“œì˜¨ ë¶„ì„ì„ íŠ¸ë¦¬ê±°í•˜ëŠ” ëª¨ë“ˆ.
"""

from .orchestrator import (
    MLOrchestrator,
    MLAddonType,
    MLAddonConfig,
    MLAnalysisResult,
    BatchAnalysisResult,
    get_ml_orchestrator,
    init_ml_orchestrator,
)

__all__ = [
    "MLOrchestrator",
    "MLAddonType",
    "MLAddonConfig",
    "MLAnalysisResult",
    "BatchAnalysisResult",
    "get_ml_orchestrator",
    "init_ml_orchestrator",
]

```

---

## backend/autonomous-crawler-service/src/ml/orchestrator.py

```py
"""
ML Addon Orchestrator for autonomous-crawler-service.

í¬ë¡¤ë§ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ ML ì• ë“œì˜¨ ë¶„ì„ì„ íŠ¸ë¦¬ê±°í•˜ëŠ” ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°.
Sentiment, Factcheck, Bias ë¶„ì„ì„ ë³‘ë ¬ë¡œ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ DBì— ì €ì¥í•©ë‹ˆë‹¤.

Features:
- ë¹„ë™ê¸° HTTP í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•œ ML ì• ë“œì˜¨ í˜¸ì¶œ
- ë³‘ë ¬ ë¶„ì„ ì‹¤í–‰ (asyncio.gather)
- ê²°ê³¼ DB ì €ì¥ (article_analysis í…Œì´ë¸”)
- í—¬ìŠ¤ì²´í¬ ë° ì—°ê²° ìƒíƒœ ëª¨ë‹ˆí„°ë§
- ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ ë° í´ë°± ì²˜ë¦¬
"""

import os
import asyncio
import uuid
from typing import Any, Dict, List, Optional, Tuple
from datetime import datetime
from enum import Enum

import structlog
import httpx
from pydantic import BaseModel, Field

logger = structlog.get_logger(__name__)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Configuration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class MLAddonConfig:
    """ML Addon ì„œë²„ ì—°ê²° ì„¤ì •"""

    SENTIMENT_ADDON_URL = os.environ.get("SENTIMENT_ADDON_URL", "http://sentiment-addon:8100")
    FACTCHECK_ADDON_URL = os.environ.get("FACTCHECK_ADDON_URL", "http://factcheck-addon:8101")
    BIAS_ADDON_URL = os.environ.get("BIAS_ADDON_URL", "http://bias-addon:8102")

    # HTTP client settings
    TIMEOUT_SECONDS = int(os.environ.get("ML_ADDON_TIMEOUT", "60"))
    MAX_RETRIES = int(os.environ.get("ML_ADDON_MAX_RETRIES", "2"))
    RETRY_DELAY_SECONDS = float(os.environ.get("ML_ADDON_RETRY_DELAY", "1.0"))

    # Feature flags
    AUTO_ANALYSIS_ENABLED = os.environ.get("ML_AUTO_ANALYSIS_ENABLED", "true").lower() == "true"
    PARALLEL_ANALYSIS = os.environ.get("ML_PARALLEL_ANALYSIS", "true").lower() == "true"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Enums and Models
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class MLAddonType(str, Enum):
    """ML Addon íƒ€ì…"""

    SENTIMENT = "sentiment"
    FACTCHECK = "factcheck"
    BIAS = "bias"


class AddonHealthStatus(str, Enum):
    """Addon ìƒíƒœ"""

    HEALTHY = "healthy"
    UNHEALTHY = "unhealthy"
    UNKNOWN = "unknown"
    WARMING_UP = "warming_up"


class ArticleInput(BaseModel):
    """ë¶„ì„í•  ê¸°ì‚¬ ì…ë ¥"""

    id: Optional[int] = None
    title: Optional[str] = None
    content: Optional[str] = None
    url: Optional[str] = None
    source: Optional[str] = None
    published_at: Optional[str] = None


class MLAddonRequest(BaseModel):
    """ML Addon ë¶„ì„ ìš”ì²­"""

    request_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    addon_id: str
    task: str = "article_analysis"
    input_schema_version: str = "1.0"
    article: ArticleInput
    context: Optional[Dict[str, Any]] = None
    options: Optional[Dict[str, Any]] = None


class MLAnalysisResult(BaseModel):
    """ML ë¶„ì„ ê²°ê³¼"""

    addon_type: MLAddonType
    success: bool
    request_id: str
    results: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    latency_ms: int = 0
    analyzed_at: str = Field(default_factory=lambda: datetime.utcnow().isoformat())


class BatchAnalysisResult(BaseModel):
    """ë°°ì¹˜ ë¶„ì„ ê²°ê³¼"""

    article_id: int
    sentiment: Optional[MLAnalysisResult] = None
    factcheck: Optional[MLAnalysisResult] = None
    bias: Optional[MLAnalysisResult] = None
    total_latency_ms: int = 0
    success_count: int = 0
    failure_count: int = 0


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ML Addon Client
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class MLAddonClient:
    """ML Addon HTTP í´ë¼ì´ì–¸íŠ¸"""

    def __init__(
        self,
        addon_type: MLAddonType,
        base_url: str,
        timeout: float = MLAddonConfig.TIMEOUT_SECONDS,
    ):
        self.addon_type = addon_type
        self.base_url = base_url
        self.timeout = timeout
        self._client: Optional[httpx.AsyncClient] = None
        self._health_status = AddonHealthStatus.UNKNOWN

    async def _get_client(self) -> httpx.AsyncClient:
        """HTTP í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (lazy initialization)"""
        if self._client is None or self._client.is_closed:
            self._client = httpx.AsyncClient(
                base_url=self.base_url,
                timeout=httpx.Timeout(self.timeout),
                limits=httpx.Limits(max_connections=10, max_keepalive_connections=5),
            )
        return self._client

    async def close(self):
        """í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì¢…ë£Œ"""
        if self._client and not self._client.is_closed:
            await self._client.aclose()
            self._client = None

    async def health_check(self) -> Dict[str, Any]:
        """í—¬ìŠ¤ì²´í¬ ìˆ˜í–‰"""
        try:
            client = await self._get_client()
            response = await client.get("/health")

            if response.status_code == 200:
                data = response.json()
                status = data.get("status", "unknown")

                if status == "healthy":
                    self._health_status = AddonHealthStatus.HEALTHY
                elif data.get("warmup_complete") is False:
                    self._health_status = AddonHealthStatus.WARMING_UP
                else:
                    self._health_status = AddonHealthStatus.HEALTHY

                return {
                    "addon_type": self.addon_type.value,
                    "status": self._health_status.value,
                    "details": data,
                }
            else:
                self._health_status = AddonHealthStatus.UNHEALTHY
                return {
                    "addon_type": self.addon_type.value,
                    "status": AddonHealthStatus.UNHEALTHY.value,
                    "error": f"HTTP {response.status_code}",
                }

        except Exception as e:
            self._health_status = AddonHealthStatus.UNHEALTHY
            logger.warning(
                f"Health check failed for {self.addon_type.value}",
                error=str(e),
                url=self.base_url,
            )
            return {
                "addon_type": self.addon_type.value,
                "status": AddonHealthStatus.UNHEALTHY.value,
                "error": str(e),
            }

    async def analyze(
        self,
        article: ArticleInput,
        options: Optional[Dict[str, Any]] = None,
        retries: int = MLAddonConfig.MAX_RETRIES,
    ) -> MLAnalysisResult:
        """ê¸°ì‚¬ ë¶„ì„ ìˆ˜í–‰"""
        start_time = datetime.utcnow()
        request_id = str(uuid.uuid4())

        request_data = MLAddonRequest(
            request_id=request_id,
            addon_id=f"{self.addon_type.value}-addon",
            article=article,
            options=options or {},
        )

        for attempt in range(retries + 1):
            try:
                client = await self._get_client()
                response = await client.post(
                    "/analyze",
                    json=request_data.model_dump(),
                )

                latency_ms = int((datetime.utcnow() - start_time).total_seconds() * 1000)

                if response.status_code == 200:
                    data = response.json()
                    return MLAnalysisResult(
                        addon_type=self.addon_type,
                        success=data.get("status") == "success",
                        request_id=request_id,
                        results=data.get("results"),
                        error=data.get("error", {}).get("message") if data.get("error") else None,
                        latency_ms=latency_ms,
                    )
                else:
                    error_msg = f"HTTP {response.status_code}: {response.text[:200]}"
                    if attempt < retries:
                        logger.warning(
                            f"Retry {attempt + 1}/{retries} for {self.addon_type.value}",
                            error=error_msg,
                        )
                        await asyncio.sleep(MLAddonConfig.RETRY_DELAY_SECONDS)
                        continue

                    return MLAnalysisResult(
                        addon_type=self.addon_type,
                        success=False,
                        request_id=request_id,
                        error=error_msg,
                        latency_ms=latency_ms,
                    )

            except httpx.TimeoutException as e:
                latency_ms = int((datetime.utcnow() - start_time).total_seconds() * 1000)
                if attempt < retries:
                    logger.warning(
                        f"Timeout retry {attempt + 1}/{retries} for {self.addon_type.value}",
                        error=str(e),
                    )
                    await asyncio.sleep(MLAddonConfig.RETRY_DELAY_SECONDS)
                    continue

                return MLAnalysisResult(
                    addon_type=self.addon_type,
                    success=False,
                    request_id=request_id,
                    error=f"Timeout after {self.timeout}s",
                    latency_ms=latency_ms,
                )

            except Exception as e:
                latency_ms = int((datetime.utcnow() - start_time).total_seconds() * 1000)
                logger.error(
                    f"Analysis failed for {self.addon_type.value}",
                    error=str(e),
                    attempt=attempt + 1,
                )

                if attempt < retries:
                    await asyncio.sleep(MLAddonConfig.RETRY_DELAY_SECONDS)
                    continue

                return MLAnalysisResult(
                    addon_type=self.addon_type,
                    success=False,
                    request_id=request_id,
                    error=str(e),
                    latency_ms=latency_ms,
                )

        # Should not reach here
        return MLAnalysisResult(
            addon_type=self.addon_type,
            success=False,
            request_id=request_id,
            error="Max retries exceeded",
            latency_ms=0,
        )

    @property
    def is_healthy(self) -> bool:
        """Addonì´ ì •ìƒ ìƒíƒœì¸ì§€ í™•ì¸"""
        return self._health_status in [
            AddonHealthStatus.HEALTHY,
            AddonHealthStatus.WARMING_UP,
        ]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ML Orchestrator
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class MLOrchestrator:
    """
    ML Addon ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°.

    í¬ë¡¤ë§ëœ ê¸°ì‚¬ì— ëŒ€í•´ ìë™ìœ¼ë¡œ ML ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³ 
    ê²°ê³¼ë¥¼ DBì— ì €ì¥í•©ë‹ˆë‹¤.
    """

    def __init__(self, db_pool=None):
        """
        Args:
            db_pool: PostgreSQL ì—°ê²° í’€ (asyncpg)
        """
        self.db_pool = db_pool

        # Initialize addon clients
        self.sentiment_client = MLAddonClient(
            MLAddonType.SENTIMENT, MLAddonConfig.SENTIMENT_ADDON_URL
        )
        self.factcheck_client = MLAddonClient(
            MLAddonType.FACTCHECK, MLAddonConfig.FACTCHECK_ADDON_URL
        )
        self.bias_client = MLAddonClient(MLAddonType.BIAS, MLAddonConfig.BIAS_ADDON_URL)

        self._clients = {
            MLAddonType.SENTIMENT: self.sentiment_client,
            MLAddonType.FACTCHECK: self.factcheck_client,
            MLAddonType.BIAS: self.bias_client,
        }

        self._initialized = False

    async def initialize(self):
        """ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì´ˆê¸°í™” ë° í—¬ìŠ¤ì²´í¬"""
        if self._initialized:
            return

        logger.info("Initializing ML Orchestrator...")

        # Perform health checks
        health_results = await self.check_all_health()

        healthy_count = sum(
            1 for r in health_results.values() if r.get("status") in ["healthy", "warming_up"]
        )

        logger.info(
            f"ML Orchestrator initialized",
            healthy_addons=healthy_count,
            total_addons=len(self._clients),
            auto_analysis_enabled=MLAddonConfig.AUTO_ANALYSIS_ENABLED,
        )

        self._initialized = True

    async def close(self):
        """ëª¨ë“  í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì¢…ë£Œ"""
        for client in self._clients.values():
            await client.close()

    async def check_all_health(self) -> Dict[str, Dict[str, Any]]:
        """ëª¨ë“  ML Addon í—¬ìŠ¤ì²´í¬"""
        results = {}

        if MLAddonConfig.PARALLEL_ANALYSIS:
            health_tasks = [
                (addon_type, client.health_check()) for addon_type, client in self._clients.items()
            ]
            health_results = await asyncio.gather(
                *[task[1] for task in health_tasks], return_exceptions=True
            )

            for i, (addon_type, _) in enumerate(health_tasks):
                result = health_results[i]
                if isinstance(result, Exception):
                    results[addon_type.value] = {
                        "status": "error",
                        "error": str(result),
                    }
                else:
                    results[addon_type.value] = result
        else:
            for addon_type, client in self._clients.items():
                try:
                    results[addon_type.value] = await client.health_check()
                except Exception as e:
                    results[addon_type.value] = {
                        "status": "error",
                        "error": str(e),
                    }

        return results

    async def analyze_article(
        self,
        article_id: int,
        title: str,
        content: str,
        source: Optional[str] = None,
        url: Optional[str] = None,
        published_at: Optional[str] = None,
        addon_types: Optional[List[MLAddonType]] = None,
        save_to_db: bool = True,
    ) -> BatchAnalysisResult:
        """
        ê¸°ì‚¬ì— ëŒ€í•´ ML ë¶„ì„ ìˆ˜í–‰.

        Args:
            article_id: ê¸°ì‚¬ ID
            title: ê¸°ì‚¬ ì œëª©
            content: ê¸°ì‚¬ ë³¸ë¬¸
            source: ì–¸ë¡ ì‚¬
            url: ê¸°ì‚¬ URL
            published_at: ë°œí–‰ì¼
            addon_types: ì‹¤í–‰í•  ì• ë“œì˜¨ íƒ€ì… ëª©ë¡ (Noneì´ë©´ ëª¨ë‘ ì‹¤í–‰)
            save_to_db: ê²°ê³¼ë¥¼ DBì— ì €ì¥í• ì§€ ì—¬ë¶€

        Returns:
            BatchAnalysisResult: ë¶„ì„ ê²°ê³¼
        """
        if not MLAddonConfig.AUTO_ANALYSIS_ENABLED:
            logger.debug("ML auto-analysis is disabled")
            return BatchAnalysisResult(
                article_id=article_id,
                success_count=0,
                failure_count=0,
            )

        start_time = datetime.utcnow()

        # Prepare article input
        article = ArticleInput(
            id=article_id,
            title=title,
            content=content,
            source=source,
            url=url,
            published_at=published_at,
        )

        # Determine which addons to run
        if addon_types is None:
            addon_types = list(MLAddonType)

        # Filter to only healthy addons
        active_clients = {
            addon_type: self._clients[addon_type]
            for addon_type in addon_types
            if addon_type in self._clients
        }

        if not active_clients:
            logger.warning("No ML addons available for analysis")
            return BatchAnalysisResult(
                article_id=article_id,
                success_count=0,
                failure_count=len(addon_types),
            )

        # Execute analysis
        results: Dict[MLAddonType, MLAnalysisResult] = {}

        if MLAddonConfig.PARALLEL_ANALYSIS:
            # Parallel execution
            analysis_tasks = [
                (addon_type, client.analyze(article))
                for addon_type, client in active_clients.items()
            ]
            analysis_results = await asyncio.gather(
                *[task[1] for task in analysis_tasks], return_exceptions=True
            )

            for i, (addon_type, _) in enumerate(analysis_tasks):
                result = analysis_results[i]
                if isinstance(result, Exception):
                    results[addon_type] = MLAnalysisResult(
                        addon_type=addon_type,
                        success=False,
                        request_id="",
                        error=str(result),
                    )
                elif isinstance(result, MLAnalysisResult):
                    results[addon_type] = result
        else:
            # Sequential execution
            for addon_type, client in active_clients.items():
                try:
                    results[addon_type] = await client.analyze(article)
                except Exception as e:
                    results[addon_type] = MLAnalysisResult(
                        addon_type=addon_type,
                        success=False,
                        request_id="",
                        error=str(e),
                    )

        # Calculate totals
        total_latency = int((datetime.utcnow() - start_time).total_seconds() * 1000)
        success_count = sum(1 for r in results.values() if r.success)
        failure_count = len(results) - success_count

        batch_result = BatchAnalysisResult(
            article_id=article_id,
            sentiment=results.get(MLAddonType.SENTIMENT),
            factcheck=results.get(MLAddonType.FACTCHECK),
            bias=results.get(MLAddonType.BIAS),
            total_latency_ms=total_latency,
            success_count=success_count,
            failure_count=failure_count,
        )

        # Save to database
        if save_to_db and self.db_pool:
            await self._save_results_to_db(article_id, results)

        logger.info(
            f"ML analysis completed for article {article_id}",
            success=success_count,
            failure=failure_count,
            latency_ms=total_latency,
        )

        return batch_result

    async def analyze_batch(
        self,
        articles: List[Dict[str, Any]],
        addon_types: Optional[List[MLAddonType]] = None,
        save_to_db: bool = True,
        max_concurrent: int = 5,
    ) -> List[BatchAnalysisResult]:
        """
        ì—¬ëŸ¬ ê¸°ì‚¬ì— ëŒ€í•´ ë°°ì¹˜ ë¶„ì„ ìˆ˜í–‰.

        Args:
            articles: ê¸°ì‚¬ ëª©ë¡ (dict with id, title, content, source, url)
            addon_types: ì‹¤í–‰í•  ì• ë“œì˜¨ íƒ€ì…
            save_to_db: DB ì €ì¥ ì—¬ë¶€
            max_concurrent: ë™ì‹œ ì²˜ë¦¬ ê¸°ì‚¬ ìˆ˜

        Returns:
            List[BatchAnalysisResult]: ë¶„ì„ ê²°ê³¼ ëª©ë¡
        """
        if not articles:
            return []

        semaphore = asyncio.Semaphore(max_concurrent)

        async def analyze_with_semaphore(article: Dict[str, Any]) -> BatchAnalysisResult:
            async with semaphore:
                return await self.analyze_article(
                    article_id=article.get("id", 0),
                    title=article.get("title", ""),
                    content=article.get("content", ""),
                    source=article.get("source"),
                    url=article.get("url"),
                    published_at=article.get("published_at"),
                    addon_types=addon_types,
                    save_to_db=save_to_db,
                )

        results = await asyncio.gather(
            *[analyze_with_semaphore(a) for a in articles], return_exceptions=True
        )

        # Filter out exceptions
        valid_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(
                    f"Batch analysis failed for article",
                    article_id=articles[i].get("id"),
                    error=str(result),
                )
                valid_results.append(
                    BatchAnalysisResult(
                        article_id=articles[i].get("id", 0),
                        success_count=0,
                        failure_count=3,
                    )
                )
            else:
                valid_results.append(result)

        return valid_results

    async def _save_results_to_db(
        self,
        article_id: int,
        results: Dict[MLAddonType, MLAnalysisResult],
    ):
        """ë¶„ì„ ê²°ê³¼ë¥¼ DBì— ì €ì¥"""
        if not self.db_pool:
            logger.warning("No database pool configured, skipping DB save")
            return

        try:
            async with self.db_pool.acquire() as conn:
                for addon_type, result in results.items():
                    if not result.success:
                        continue

                    # Extract result data based on addon type
                    result_data = result.results or {}

                    if addon_type == MLAddonType.SENTIMENT:
                        sentiment_data = result_data.get("sentiment", {})
                        await conn.execute(
                            """
                            INSERT INTO article_analysis 
                                (article_id, addon_key, analysis_type, result_json, 
                                 score, confidence, created_at)
                            VALUES ($1, $2, $3, $4, $5, $6, NOW())
                            ON CONFLICT (article_id, addon_key) 
                            DO UPDATE SET 
                                result_json = EXCLUDED.result_json,
                                score = EXCLUDED.score,
                                confidence = EXCLUDED.confidence,
                                updated_at = NOW()
                            """,
                            article_id,
                            "sentiment-addon",
                            "sentiment",
                            result_data,
                            sentiment_data.get("score", 0),
                            sentiment_data.get("confidence", 0),
                        )

                    elif addon_type == MLAddonType.FACTCHECK:
                        factcheck_data = result_data.get("factcheck", {})
                        await conn.execute(
                            """
                            INSERT INTO article_analysis 
                                (article_id, addon_key, analysis_type, result_json, 
                                 score, confidence, created_at)
                            VALUES ($1, $2, $3, $4, $5, $6, NOW())
                            ON CONFLICT (article_id, addon_key) 
                            DO UPDATE SET 
                                result_json = EXCLUDED.result_json,
                                score = EXCLUDED.score,
                                confidence = EXCLUDED.confidence,
                                updated_at = NOW()
                            """,
                            article_id,
                            "factcheck-addon",
                            "factcheck",
                            result_data,
                            factcheck_data.get("overall_credibility", 0) / 100,
                            0.7,  # Default confidence for factcheck
                        )

                    elif addon_type == MLAddonType.BIAS:
                        bias_data = result_data.get("bias", {})
                        await conn.execute(
                            """
                            INSERT INTO article_analysis 
                                (article_id, addon_key, analysis_type, result_json, 
                                 score, confidence, created_at)
                            VALUES ($1, $2, $3, $4, $5, $6, NOW())
                            ON CONFLICT (article_id, addon_key) 
                            DO UPDATE SET 
                                result_json = EXCLUDED.result_json,
                                score = EXCLUDED.score,
                                confidence = EXCLUDED.confidence,
                                updated_at = NOW()
                            """,
                            article_id,
                            "bias-addon",
                            "bias",
                            result_data,
                            bias_data.get("overall_bias_score", 0),
                            bias_data.get("confidence", 0),
                        )

                logger.debug(f"Saved ML analysis results to DB for article {article_id}")

        except Exception as e:
            logger.error(
                f"Failed to save ML results to DB",
                article_id=article_id,
                error=str(e),
            )

    def get_addon_status(self) -> Dict[str, Any]:
        """í˜„ì¬ Addon ìƒíƒœ ë°˜í™˜"""
        return {
            "auto_analysis_enabled": MLAddonConfig.AUTO_ANALYSIS_ENABLED,
            "parallel_analysis": MLAddonConfig.PARALLEL_ANALYSIS,
            "addons": {
                addon_type.value: {
                    "url": client.base_url,
                    "healthy": client.is_healthy,
                    "status": client._health_status.value,
                }
                for addon_type, client in self._clients.items()
            },
        }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Singleton Instance
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_ml_orchestrator: Optional[MLOrchestrator] = None


def get_ml_orchestrator(db_pool=None) -> MLOrchestrator:
    """ML ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _ml_orchestrator
    if _ml_orchestrator is None:
        _ml_orchestrator = MLOrchestrator(db_pool=db_pool)
    elif db_pool is not None and _ml_orchestrator.db_pool is None:
        _ml_orchestrator.db_pool = db_pool
    return _ml_orchestrator


async def init_ml_orchestrator(db_pool=None) -> MLOrchestrator:
    """ML ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì´ˆê¸°í™” ë° ë°˜í™˜"""
    orchestrator = get_ml_orchestrator(db_pool)
    await orchestrator.initialize()
    return orchestrator

```

---

## backend/autonomous-crawler-service/src/ml/router.py

```py
"""
ML Analysis API Router for autonomous-crawler-service.

ML Addon ë¶„ì„ ê¸°ëŠ¥ì„ REST APIë¡œ ë…¸ì¶œí•©ë‹ˆë‹¤.
í¬ë¡¤ë§ëœ ê¸°ì‚¬ì— ëŒ€í•´ sentiment, factcheck, bias ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import os
from typing import Any, Dict, List, Optional

import structlog
from fastapi import APIRouter, HTTPException, Query, Request
from pydantic import BaseModel, Field, HttpUrl

from src.ml.orchestrator import (
    MLOrchestrator,
    MLAddonType,
    MLAddonConfig,
    MLAnalysisResult,
    BatchAnalysisResult,
    ArticleInput,
    get_ml_orchestrator,
    init_ml_orchestrator,
)

logger = structlog.get_logger(__name__)

router = APIRouter(prefix="/ml", tags=["ML Analysis"])


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Request/Response Models
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class MLAnalyzeRequest(BaseModel):
    """ML ë¶„ì„ ìš”ì²­"""

    article_id: int = Field(..., description="ê¸°ì‚¬ ID")
    title: str = Field(..., description="ê¸°ì‚¬ ì œëª©", min_length=1)
    content: str = Field(..., description="ê¸°ì‚¬ ë³¸ë¬¸", min_length=10)
    source: Optional[str] = Field(default=None, description="ì–¸ë¡ ì‚¬ëª…")
    url: Optional[str] = Field(default=None, description="ê¸°ì‚¬ URL")
    published_at: Optional[str] = Field(default=None, description="ë°œí–‰ì¼")

    # ë¶„ì„ ì˜µì…˜
    addons: Optional[List[str]] = Field(
        default=None,
        description="ì‹¤í–‰í•  ì• ë“œì˜¨ ëª©ë¡ (sentiment, factcheck, bias). Noneì´ë©´ ëª¨ë‘ ì‹¤í–‰",
    )
    save_to_db: bool = Field(default=True, description="ê²°ê³¼ë¥¼ DBì— ì €ì¥í• ì§€ ì—¬ë¶€")


class MLBatchAnalyzeRequest(BaseModel):
    """ML ë°°ì¹˜ ë¶„ì„ ìš”ì²­"""

    articles: List[MLAnalyzeRequest] = Field(..., min_length=1, max_length=50)
    addons: Optional[List[str]] = None
    save_to_db: bool = True
    max_concurrent: int = Field(default=5, ge=1, le=20)


class MLSimpleAnalyzeRequest(BaseModel):
    """ê°„ë‹¨í•œ ML ë¶„ì„ ìš”ì²­ (í…ìŠ¤íŠ¸ë§Œ)"""

    text: str = Field(..., description="ë¶„ì„í•  í…ìŠ¤íŠ¸", min_length=10)
    source: Optional[str] = Field(default=None, description="ì¶œì²˜/ì–¸ë¡ ì‚¬ëª…")
    addons: Optional[List[str]] = Field(default=None, description="ì‹¤í–‰í•  ì• ë“œì˜¨ ëª©ë¡")


class MLAddonInfo(BaseModel):
    """ML Addon ì •ë³´"""

    type: str
    url: str
    healthy: bool
    status: str


class MLStatusResponse(BaseModel):
    """ML ì‹œìŠ¤í…œ ìƒíƒœ"""

    auto_analysis_enabled: bool
    parallel_analysis: bool
    addons: Dict[str, MLAddonInfo]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Helper Functions
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def parse_addon_types(addons: Optional[List[str]]) -> Optional[List[MLAddonType]]:
    """ë¬¸ìì—´ ì• ë“œì˜¨ ëª©ë¡ì„ MLAddonType ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    if addons is None:
        return None

    addon_map = {
        "sentiment": MLAddonType.SENTIMENT,
        "factcheck": MLAddonType.FACTCHECK,
        "bias": MLAddonType.BIAS,
    }

    result = []
    for addon in addons:
        addon_type = addon_map.get(addon.lower())
        if addon_type:
            result.append(addon_type)

    return result if result else None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# API Endpoints
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@router.get("/health")
async def ml_health_check(request: Request):
    """
    ML ì‹œìŠ¤í…œ í—¬ìŠ¤ì²´í¬.

    ëª¨ë“  ML Addonì˜ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    """
    orchestrator = get_ml_orchestrator()
    health_results = await orchestrator.check_all_health()

    all_healthy = all(r.get("status") in ["healthy", "warming_up"] for r in health_results.values())

    return {
        "status": "healthy" if all_healthy else "degraded",
        "auto_analysis_enabled": MLAddonConfig.AUTO_ANALYSIS_ENABLED,
        "addons": health_results,
    }


@router.get("/status", response_model=MLStatusResponse)
async def ml_status(request: Request):
    """
    ML ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ.

    í˜„ì¬ ì„¤ì • ë° Addon ì—°ê²° ìƒíƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    orchestrator = get_ml_orchestrator()
    status = orchestrator.get_addon_status()

    return MLStatusResponse(
        auto_analysis_enabled=status["auto_analysis_enabled"],
        parallel_analysis=status["parallel_analysis"],
        addons={
            k: MLAddonInfo(
                type=k,
                url=v["url"],
                healthy=v["healthy"],
                status=v["status"],
            )
            for k, v in status["addons"].items()
        },
    )


@router.post("/analyze", response_model=BatchAnalysisResult)
async def analyze_article(
    request: MLAnalyzeRequest,
    req: Request,
):
    """
    ë‹¨ì¼ ê¸°ì‚¬ ML ë¶„ì„.

    ê¸°ì‚¬ì— ëŒ€í•´ sentiment, factcheck, bias ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    **ì‚¬ìš© ì˜ˆì‹œ:**
    ``\`json
    {
        "article_id": 12345,
        "title": "ë‰´ìŠ¤ ì œëª©",
        "content": "ë‰´ìŠ¤ ë³¸ë¬¸ ë‚´ìš©...",
        "source": "ì¡°ì„ ì¼ë³´",
        "addons": ["sentiment", "bias"]
    }
    ``\`
    """
    orchestrator = get_ml_orchestrator()
    addon_types = parse_addon_types(request.addons)

    result = await orchestrator.analyze_article(
        article_id=request.article_id,
        title=request.title,
        content=request.content,
        source=request.source,
        url=request.url,
        published_at=request.published_at,
        addon_types=addon_types,
        save_to_db=request.save_to_db,
    )

    return result


@router.post("/analyze/simple")
async def analyze_text_simple(
    request: MLSimpleAnalyzeRequest,
    req: Request,
):
    """
    ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ML ë¶„ì„.

    ê¸°ì‚¬ ID ì—†ì´ í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    ê²°ê³¼ëŠ” DBì— ì €ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

    **ì‚¬ìš© ì˜ˆì‹œ:**
    ``\`json
    {
        "text": "ë¶„ì„í•  ë‰´ìŠ¤ í…ìŠ¤íŠ¸...",
        "source": "í•œê²¨ë ˆ",
        "addons": ["sentiment"]
    }
    ``\`
    """
    orchestrator = get_ml_orchestrator()
    addon_types = parse_addon_types(request.addons)

    # ì„ì‹œ article_id ì‚¬ìš© (DB ì €ì¥ ì•ˆí•¨)
    result = await orchestrator.analyze_article(
        article_id=0,
        title="",
        content=request.text,
        source=request.source,
        addon_types=addon_types,
        save_to_db=False,
    )

    # ì‘ë‹µì—ì„œ ë¶ˆí•„ìš”í•œ í•„ë“œ ì œê±°
    return {
        "sentiment": result.sentiment.model_dump() if result.sentiment else None,
        "factcheck": result.factcheck.model_dump() if result.factcheck else None,
        "bias": result.bias.model_dump() if result.bias else None,
        "total_latency_ms": result.total_latency_ms,
        "success_count": result.success_count,
        "failure_count": result.failure_count,
    }


@router.post("/analyze/batch")
async def analyze_batch(
    request: MLBatchAnalyzeRequest,
    req: Request,
):
    """
    ë°°ì¹˜ ê¸°ì‚¬ ML ë¶„ì„.

    ì—¬ëŸ¬ ê¸°ì‚¬ë¥¼ í•œ ë²ˆì— ë¶„ì„í•©ë‹ˆë‹¤.

    **ì‚¬ìš© ì˜ˆì‹œ:**
    ``\`json
    {
        "articles": [
            {"article_id": 1, "title": "ì œëª©1", "content": "ë‚´ìš©1"},
            {"article_id": 2, "title": "ì œëª©2", "content": "ë‚´ìš©2"}
        ],
        "addons": ["sentiment", "factcheck", "bias"],
        "max_concurrent": 5
    }
    ``\`
    """
    orchestrator = get_ml_orchestrator()
    addon_types = parse_addon_types(request.addons)

    # ìš”ì²­ì„ dict ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    articles = [
        {
            "id": a.article_id,
            "title": a.title,
            "content": a.content,
            "source": a.source,
            "url": a.url,
            "published_at": a.published_at,
        }
        for a in request.articles
    ]

    results = await orchestrator.analyze_batch(
        articles=articles,
        addon_types=addon_types,
        save_to_db=request.save_to_db,
        max_concurrent=request.max_concurrent,
    )

    # í†µê³„ ê³„ì‚°
    total_success = sum(r.success_count for r in results)
    total_failure = sum(r.failure_count for r in results)

    return {
        "total_articles": len(results),
        "total_success": total_success,
        "total_failure": total_failure,
        "results": [
            {
                "article_id": r.article_id,
                "success_count": r.success_count,
                "failure_count": r.failure_count,
                "total_latency_ms": r.total_latency_ms,
            }
            for r in results
        ],
    }


@router.post("/analyze/url")
async def analyze_url(
    url: HttpUrl = Query(..., description="ë¶„ì„í•  URL"),
    req: Request = None,
):
    """
    URLì—ì„œ ê¸°ì‚¬ë¥¼ í¬ë¡¤ë§í•˜ê³  ML ë¶„ì„ ìˆ˜í–‰.

    URLì˜ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•œ í›„ sentiment, factcheck, bias ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    import httpx
    from bs4 import BeautifulSoup

    try:
        # ê°„ë‹¨í•œ URL í˜ì¹˜
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(str(url), follow_redirects=True)
            response.raise_for_status()

        # HTML íŒŒì‹±
        soup = BeautifulSoup(response.text, "html.parser")

        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for tag in soup(["script", "style", "nav", "footer", "header", "aside", "ad"]):
            tag.decompose()

        # ì œëª©ê³¼ ë³¸ë¬¸ ì¶”ì¶œ
        title = soup.title.string if soup.title else ""
        content = soup.get_text(separator="\n", strip=True)

        # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ì—ëŸ¬
        if len(content) < 100:
            raise HTTPException(
                status_code=400,
                detail="í˜ì´ì§€ì—ì„œ ì¶©ë¶„í•œ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            )

        # ML ë¶„ì„ ìˆ˜í–‰
        orchestrator = get_ml_orchestrator()
        result = await orchestrator.analyze_article(
            article_id=0,
            title=title[:500] if title else "",
            content=content[:10000],  # ìµœëŒ€ 10000ì
            url=str(url),
            save_to_db=False,
        )

        return {
            "url": str(url),
            "title": title[:200] if title else None,
            "content_length": len(content),
            "sentiment": result.sentiment.model_dump() if result.sentiment else None,
            "factcheck": result.factcheck.model_dump() if result.factcheck else None,
            "bias": result.bias.model_dump() if result.bias else None,
            "total_latency_ms": result.total_latency_ms,
        }

    except httpx.HTTPError as e:
        raise HTTPException(status_code=400, detail=f"URL ì ‘ê·¼ ì‹¤íŒ¨: {str(e)}")
    except Exception as e:
        logger.error("URL analysis failed", url=str(url), error=str(e))
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ì‹¤íŒ¨: {str(e)}")


@router.get("/addons")
async def list_addons():
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ ML Addon ëª©ë¡.

    ê° ì• ë“œì˜¨ì˜ ê¸°ëŠ¥ê³¼ í˜„ì¬ ìƒíƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    orchestrator = get_ml_orchestrator()
    health_results = await orchestrator.check_all_health()

    addons = [
        {
            "key": "sentiment",
            "name": "ê°ì„± ë¶„ì„ (Sentiment Analysis)",
            "description": "ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ê°ì •(ê¸ì •/ë¶€ì •/ì¤‘ë¦½)ì„ ë¶„ì„í•©ë‹ˆë‹¤. KoELECTRA ê¸°ë°˜ ML ëª¨ë¸ ì‚¬ìš©.",
            "endpoint": MLAddonConfig.SENTIMENT_ADDON_URL,
            "status": health_results.get("sentiment", {}).get("status", "unknown"),
            "features": ["sentiment_score", "emotion_detection", "tone_analysis"],
        },
        {
            "key": "factcheck",
            "name": "íŒ©íŠ¸ì²´í¬ (Fact-Check Analysis)",
            "description": "ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì‚¬ì‹¤ì„±ê³¼ ì‹ ë¢°ë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. ì£¼ì¥ ì¶”ì¶œ, í´ë¦­ë² ì´íŠ¸ íƒì§€, í—ˆìœ„ì •ë³´ ìœ„í—˜ë„ í‰ê°€.",
            "endpoint": MLAddonConfig.FACTCHECK_ADDON_URL,
            "status": health_results.get("factcheck", {}).get("status", "unknown"),
            "features": [
                "claim_extraction",
                "credibility_score",
                "clickbait_detection",
                "misinformation_risk",
            ],
        },
        {
            "key": "bias",
            "name": "í¸í–¥ ë¶„ì„ (Bias Analysis)",
            "description": "ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì •ì¹˜ì /ì´ë…ì  í¸í–¥ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤. ì–¸ë¡ ì‚¬ ì„±í–¥, í‚¤ì›Œë“œ ê¸°ë°˜, í”„ë ˆì´ë° ë¶„ì„.",
            "endpoint": MLAddonConfig.BIAS_ADDON_URL,
            "status": health_results.get("bias", {}).get("status", "unknown"),
            "features": [
                "political_lean",
                "source_bias",
                "framing_analysis",
                "objectivity_score",
            ],
        },
    ]

    return {
        "addons": addons,
        "total": len(addons),
        "auto_analysis_enabled": MLAddonConfig.AUTO_ANALYSIS_ENABLED,
    }


@router.post("/config/toggle")
async def toggle_auto_analysis(
    enabled: bool = Query(..., description="ìë™ ë¶„ì„ í™œì„±í™” ì—¬ë¶€"),
):
    """
    ìë™ ML ë¶„ì„ í† ê¸€.

    í¬ë¡¤ë§ í›„ ìë™ ML ë¶„ì„ ê¸°ëŠ¥ì„ í™œì„±í™”/ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.
    (ëŸ°íƒ€ì„ì—ë§Œ ì ìš©, í™˜ê²½ë³€ìˆ˜ ì„¤ì •ì€ ë³€ê²½ë˜ì§€ ì•ŠìŒ)
    """
    # Note: ì´ ì„¤ì •ì€ ëŸ°íƒ€ì„ì—ë§Œ ì ìš©ë¨
    # ì˜êµ¬ì ì¸ ë³€ê²½ì„ ìœ„í•´ì„œëŠ” í™˜ê²½ë³€ìˆ˜ ML_AUTO_ANALYSIS_ENABLED ìˆ˜ì • í•„ìš”
    MLAddonConfig.AUTO_ANALYSIS_ENABLED = enabled

    logger.info(f"ML auto-analysis toggled", enabled=enabled)

    return {
        "status": "ok",
        "auto_analysis_enabled": MLAddonConfig.AUTO_ANALYSIS_ENABLED,
        "message": f"ìë™ ML ë¶„ì„ì´ {'í™œì„±í™”' if enabled else 'ë¹„í™œì„±í™”'}ë˜ì—ˆìŠµë‹ˆë‹¤.",
    }

```

---

## backend/autonomous-crawler-service/src/search/__init__.py

```py
"""Search providers package with RRF-based multi-strategy search."""

from src.search.base import SearchResult, SearchProvider
from src.search.brave import BraveSearchProvider
from src.search.tavily import TavilySearchProvider
from src.search.perplexity import PerplexitySearchProvider
from src.search.orchestrator import (
    ParallelSearchOrchestrator,
    RRFSearchOrchestrator,
    RRFSearchResult,
    AggregatedSearchResult,
    create_rrf_orchestrator,
)
from src.search.rrf import (
    ReciprocalRankFusion,
    SemanticRRF,
    RRFConfig,
    RRFResult,
    RRFMergeResult,
    create_rrf_merger,
    create_semantic_rrf_merger,
)
from src.search.query_analyzer import (
    QueryAnalyzer,
    QueryAnalysis,
    MultiStrategyQueryExpander,
)

__all__ = [
    # Base classes
    "SearchResult",
    "SearchProvider",
    # Providers
    "BraveSearchProvider",
    "TavilySearchProvider",
    "PerplexitySearchProvider",
    # Orchestrators
    "ParallelSearchOrchestrator",
    "RRFSearchOrchestrator",
    "RRFSearchResult",
    "AggregatedSearchResult",
    "create_rrf_orchestrator",
    # RRF algorithm
    "ReciprocalRankFusion",
    "SemanticRRF",
    "RRFConfig",
    "RRFResult",
    "RRFMergeResult",
    "create_rrf_merger",
    "create_semantic_rrf_merger",
    # Query analysis
    "QueryAnalyzer",
    "QueryAnalysis",
    "MultiStrategyQueryExpander",
]

```

---

## backend/autonomous-crawler-service/src/search/base.py

```py
"""Base classes for search providers."""

from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any


@dataclass
class SearchResult:
    """Unified search result structure."""
    
    title: str
    url: str
    snippet: str
    source_provider: str  # brave, tavily, perplexity, browser
    
    # Optional fields
    published_date: str | None = None
    score: float | None = None
    raw_data: dict[str, Any] = field(default_factory=dict)
    
    # Metadata
    fetched_at: datetime = field(default_factory=datetime.now)
    
    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary."""
        return {
            "title": self.title,
            "url": self.url,
            "snippet": self.snippet,
            "source_provider": self.source_provider,
            "published_date": self.published_date,
            "score": self.score,
            "fetched_at": self.fetched_at.isoformat(),
        }


class SearchProvider(ABC):
    """Abstract base class for search providers."""
    
    @property
    @abstractmethod
    def name(self) -> str:
        """Provider name."""
        pass
    
    @abstractmethod
    async def search(
        self,
        query: str,
        max_results: int = 10,
        **kwargs,
    ) -> list[SearchResult]:
        """
        Execute search query.
        
        Args:
            query: Search query string
            max_results: Maximum number of results to return
            **kwargs: Provider-specific options
            
        Returns:
            List of SearchResult objects
        """
        pass
    
    @abstractmethod
    async def health_check(self) -> bool:
        """Check if the provider is healthy and accessible."""
        pass

```

---

## backend/autonomous-crawler-service/src/search/brave.py

```py
"""Brave Search API provider."""

import httpx
import structlog
from typing import Any

from src.search.base import SearchProvider, SearchResult

logger = structlog.get_logger(__name__)


class BraveSearchProvider(SearchProvider):
    """
    Brave Search API client.
    
    Docs: https://api.search.brave.com/app/documentation/web-search/get-started
    """
    
    BASE_URL = "https://api.search.brave.com/res/v1"
    
    def __init__(self, api_key: str, timeout: float = 30.0):
        self.api_key = api_key
        self.timeout = timeout
        self._client: httpx.AsyncClient | None = None
    
    @property
    def name(self) -> str:
        return "brave"
    
    async def _get_client(self) -> httpx.AsyncClient:
        """Get or create HTTP client."""
        if self._client is None or self._client.is_closed:
            self._client = httpx.AsyncClient(
                timeout=self.timeout,
                headers={
                    "Accept": "application/json",
                    "Accept-Encoding": "gzip",
                    "X-Subscription-Token": self.api_key,
                },
            )
        return self._client
    
    async def close(self) -> None:
        """Close the HTTP client."""
        if self._client and not self._client.is_closed:
            await self._client.aclose()
            self._client = None
    
    async def search(
        self,
        query: str,
        max_results: int = 10,
        country: str = "kr",
        search_lang: str = "ko",
        ui_lang: str = "ko-KR",
        freshness: str | None = None,  # pd (past day), pw (past week), pm (past month)
        **kwargs,
    ) -> list[SearchResult]:
        """
        Execute Brave web search.
        
        Args:
            query: Search query
            max_results: Max results (1-20 for free tier)
            country: Country code
            search_lang: Search language
            ui_lang: UI language
            freshness: Time filter (pd, pw, pm, py)
        """
        client = await self._get_client()
        
        params: dict[str, Any] = {
            "q": query,
            "count": min(max_results, 20),  # Brave max is 20
            "country": country,
            "search_lang": search_lang,
            "ui_lang": ui_lang,
        }
        
        if freshness:
            params["freshness"] = freshness
        
        try:
            response = await client.get(
                f"{self.BASE_URL}/web/search",
                params=params,
            )
            response.raise_for_status()
            data = response.json()
            
            results: list[SearchResult] = []
            
            # Parse web results
            web_results = data.get("web", {}).get("results", [])
            for item in web_results[:max_results]:
                results.append(SearchResult(
                    title=item.get("title", ""),
                    url=item.get("url", ""),
                    snippet=item.get("description", ""),
                    source_provider=self.name,
                    published_date=item.get("age"),  # e.g., "2 hours ago"
                    score=item.get("relevancy_score"),
                    raw_data=item,
                ))
            
            logger.info(
                "Brave search completed",
                query=query,
                results_count=len(results),
            )
            
            return results
            
        except httpx.HTTPStatusError as e:
            logger.error(
                "Brave search HTTP error",
                query=query,
                status_code=e.response.status_code,
                error=str(e),
            )
            return []
        except Exception as e:
            logger.error(
                "Brave search failed",
                query=query,
                error=str(e),
            )
            return []
    
    async def search_news(
        self,
        query: str,
        max_results: int = 10,
        country: str = "kr",
        **kwargs,
    ) -> list[SearchResult]:
        """Search news specifically."""
        client = await self._get_client()
        
        params = {
            "q": query,
            "count": min(max_results, 20),
            "country": country,
            "freshness": "pw",  # Past week for news
        }
        
        try:
            response = await client.get(
                f"{self.BASE_URL}/news/search",
                params=params,
            )
            response.raise_for_status()
            data = response.json()
            
            results: list[SearchResult] = []
            news_results = data.get("results", [])
            
            for item in news_results[:max_results]:
                results.append(SearchResult(
                    title=item.get("title", ""),
                    url=item.get("url", ""),
                    snippet=item.get("description", ""),
                    source_provider=f"{self.name}_news",
                    published_date=item.get("age"),
                    raw_data=item,
                ))
            
            return results
            
        except Exception as e:
            logger.error("Brave news search failed", query=query, error=str(e))
            return []
    
    async def health_check(self) -> bool:
        """Check API health."""
        try:
            results = await self.search("test", max_results=1)
            return len(results) > 0
        except Exception:
            return False

```

---

## backend/autonomous-crawler-service/src/search/orchestrator.py

```py
"""Parallel search orchestrator for multiple providers."""

import asyncio
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Optional
from urllib.parse import urlparse

import structlog

from src.search.base import SearchProvider, SearchResult
from src.search.rrf import (
    ReciprocalRankFusion,
    SemanticRRF,
    RRFConfig,
    RRFMergeResult,
)

logger = structlog.get_logger(__name__)


@dataclass
class AggregatedSearchResult:
    """Aggregated results from multiple search providers."""

    query: str
    results: list[SearchResult]
    providers_used: list[str]
    providers_failed: list[str]
    total_results: int
    unique_urls: int
    search_time_ms: float
    timestamp: datetime = field(default_factory=datetime.now)

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary."""
        return {
            "query": self.query,
            "results": [r.to_dict() for r in self.results],
            "providers_used": self.providers_used,
            "providers_failed": self.providers_failed,
            "total_results": self.total_results,
            "unique_urls": self.unique_urls,
            "search_time_ms": self.search_time_ms,
            "timestamp": self.timestamp.isoformat(),
        }


class ParallelSearchOrchestrator:
    """
    Orchestrates parallel searches across multiple providers.

    Features:
    - Parallel execution for speed
    - Deduplication by URL
    - Result ranking and merging
    - Provider health tracking
    - Fallback strategies
    """

    def __init__(
        self,
        providers: list[SearchProvider],
        timeout: float = 30.0,
        deduplicate: bool = True,
    ):
        """
        Initialize orchestrator.

        Args:
            providers: List of search providers to use
            timeout: Timeout for each provider search
            deduplicate: Whether to deduplicate results by URL
        """
        self.providers = providers
        self.timeout = timeout
        self.deduplicate = deduplicate
        self._provider_health: dict[str, bool] = {}

    async def search(
        self,
        query: str,
        max_results_per_provider: int = 10,
        max_total_results: int = 30,
        **kwargs,
    ) -> AggregatedSearchResult:
        """
        Execute parallel search across all providers.

        Args:
            query: Search query
            max_results_per_provider: Max results from each provider
            max_total_results: Max total results after aggregation
            **kwargs: Provider-specific options

        Returns:
            AggregatedSearchResult with merged, deduplicated results
        """
        start_time = datetime.now()

        # Create tasks for all providers
        tasks = []
        provider_names = []

        for provider in self.providers:
            task = asyncio.create_task(
                self._search_with_timeout(
                    provider,
                    query,
                    max_results_per_provider,
                    **kwargs,
                )
            )
            tasks.append(task)
            provider_names.append(provider.name)

        # Wait for all tasks to complete
        results_by_provider = await asyncio.gather(*tasks, return_exceptions=True)

        # Process results
        all_results: list[SearchResult] = []
        providers_used: list[str] = []
        providers_failed: list[str] = []

        for provider_name, result in zip(provider_names, results_by_provider):
            if isinstance(result, Exception):
                logger.error(
                    "Provider search failed",
                    provider=provider_name,
                    error=str(result),
                )
                providers_failed.append(provider_name)
                self._provider_health[provider_name] = False
            elif isinstance(result, list):
                all_results.extend(result)
                if result:
                    providers_used.append(provider_name)
                    self._provider_health[provider_name] = True
                else:
                    # Empty results but no error
                    providers_used.append(provider_name)

        # Deduplicate by URL
        if self.deduplicate:
            all_results = self._deduplicate_results(all_results)

        # Rank and limit results
        all_results = self._rank_results(all_results)[:max_total_results]

        # Calculate search time
        search_time_ms = (datetime.now() - start_time).total_seconds() * 1000

        logger.info(
            "Parallel search completed",
            query=query,
            total_results=len(all_results),
            providers_used=providers_used,
            providers_failed=providers_failed,
            search_time_ms=search_time_ms,
        )

        return AggregatedSearchResult(
            query=query,
            results=all_results,
            providers_used=providers_used,
            providers_failed=providers_failed,
            total_results=len(all_results),
            unique_urls=len(set(r.url for r in all_results)),
            search_time_ms=search_time_ms,
        )

    async def _search_with_timeout(
        self,
        provider: SearchProvider,
        query: str,
        max_results: int,
        **kwargs,
    ) -> list[SearchResult]:
        """Execute provider search with timeout."""
        try:
            return await asyncio.wait_for(
                provider.search(query, max_results, **kwargs),
                timeout=self.timeout,
            )
        except asyncio.TimeoutError:
            logger.warning(
                "Provider search timed out",
                provider=provider.name,
                timeout=self.timeout,
            )
            raise

    def _deduplicate_results(
        self,
        results: list[SearchResult],
    ) -> list[SearchResult]:
        """
        Deduplicate results by URL, keeping the first occurrence.

        Also merges information from duplicate entries.
        """
        seen_urls: dict[str, SearchResult] = {}

        for result in results:
            # Normalize URL for comparison
            normalized_url = self._normalize_url(result.url)

            if normalized_url not in seen_urls:
                seen_urls[normalized_url] = result
            else:
                # Merge: keep the one with more information
                existing = seen_urls[normalized_url]
                if len(result.snippet) > len(existing.snippet):
                    # Keep longer snippet
                    result.raw_data["merged_from"] = existing.source_provider
                    seen_urls[normalized_url] = result

        return list(seen_urls.values())

    def _normalize_url(self, url: str) -> str:
        """Normalize URL for deduplication."""
        try:
            parsed = urlparse(url)
            # Remove www. prefix and trailing slash
            netloc = parsed.netloc.lower().lstrip("www.")
            path = parsed.path.rstrip("/")
            return f"{netloc}{path}"
        except Exception:
            return url.lower()

    def _rank_results(self, results: list[SearchResult]) -> list[SearchResult]:
        """
        Rank results by relevance.

        Scoring factors:
        - Provider reliability
        - Relevance score (if available)
        - Content length
        - Recency (if date available)
        """

        def score_result(result: SearchResult) -> float:
            score = 0.0

            # Provider weight
            provider_weights = {
                "tavily": 1.0,
                "brave": 0.9,
                "perplexity": 0.85,
                "brave_news": 0.95,
            }
            score += provider_weights.get(result.source_provider, 0.5) * 10

            # Relevance score if available
            if result.score:
                score += result.score * 5

            # Content length (prefer informative snippets)
            score += min(len(result.snippet) / 100, 5)

            # Title quality
            if result.title and len(result.title) > 10:
                score += 2

            return score

        return sorted(results, key=score_result, reverse=True)

    async def search_news(
        self,
        query: str,
        max_results_per_provider: int = 10,
        days: int = 7,
        **kwargs,
    ) -> AggregatedSearchResult:
        """Search for news specifically."""
        # Add news-specific parameters
        kwargs["topic"] = "news"
        kwargs["days"] = days
        kwargs["freshness"] = "pw"  # Past week for Brave
        kwargs["search_recency_filter"] = "week"  # For Perplexity

        return await self.search(
            query=query,
            max_results_per_provider=max_results_per_provider,
            **kwargs,
        )

    async def health_check_all(self) -> dict[str, bool]:
        """Check health of all providers."""
        tasks = [(provider.name, provider.health_check()) for provider in self.providers]

        results = {}
        for name, task in tasks:
            try:
                results[name] = await asyncio.wait_for(task, timeout=10.0)
            except Exception:
                results[name] = False

        self._provider_health = results
        return results

    def get_healthy_providers(self) -> list[str]:
        """Get list of healthy provider names."""
        return [name for name, healthy in self._provider_health.items() if healthy]

    async def close_all(self) -> None:
        """Close all provider connections."""
        for provider in self.providers:
            if hasattr(provider, "close"):
                await provider.close()


@dataclass
class RRFSearchResult:
    """Result from RRF-based multi-strategy search."""

    original_query: str
    results: list[SearchResult]
    strategies_used: list[str]
    providers_used: list[str]
    total_results: int
    unique_urls: int
    search_time_ms: float
    rrf_merge_result: Optional[RRFMergeResult] = None
    query_analysis: Optional[dict[str, Any]] = None
    timestamp: datetime = field(default_factory=datetime.now)

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary."""
        return {
            "original_query": self.original_query,
            "results": [r.to_dict() for r in self.results],
            "strategies_used": self.strategies_used,
            "providers_used": self.providers_used,
            "total_results": self.total_results,
            "unique_urls": self.unique_urls,
            "search_time_ms": self.search_time_ms,
            "query_analysis": self.query_analysis,
            "timestamp": self.timestamp.isoformat(),
        }


class RRFSearchOrchestrator:
    """
    Advanced search orchestrator using RRF (Reciprocal Rank Fusion)
    to combine results from multiple search strategies.

    Features:
    - Query analysis and expansion using LLM
    - Multi-strategy parallel search
    - RRF-based result fusion
    - Semantic relevance scoring
    - Provider failover and health tracking

    Strategies:
    1. Original query search
    2. Keyword-extracted search
    3. Semantically expanded queries
    4. Cross-lingual search (for non-English queries)
    """

    def __init__(
        self,
        providers: list[SearchProvider],
        query_analyzer: Optional[Any] = None,  # QueryAnalyzer
        timeout: float = 30.0,
        rrf_config: Optional[RRFConfig] = None,
        enable_semantic_rrf: bool = True,
    ):
        """
        Initialize RRF Search Orchestrator.

        Args:
            providers: List of search providers
            query_analyzer: Optional QueryAnalyzer for query expansion
            timeout: Timeout per provider search
            rrf_config: RRF algorithm configuration
            enable_semantic_rrf: Use semantic similarity in RRF scoring
        """
        self.providers = providers
        self.query_analyzer = query_analyzer
        self.timeout = timeout
        self.rrf_config = rrf_config or RRFConfig()
        self._provider_health: dict[str, bool] = {}

        # Initialize RRF merger
        if enable_semantic_rrf:
            self._rrf_merger = SemanticRRF(self.rrf_config)
        else:
            self._rrf_merger = ReciprocalRankFusion(self.rrf_config)

    async def search_with_rrf(
        self,
        query: str,
        max_results_per_strategy: int = 10,
        max_total_results: int = 30,
        enable_query_expansion: bool = True,
        context: Optional[str] = None,
        **kwargs,
    ) -> RRFSearchResult:
        """
        Execute multi-strategy search with RRF fusion.

        Args:
            query: Original search query
            max_results_per_strategy: Max results per search strategy
            max_total_results: Max final results after RRF fusion
            enable_query_expansion: Whether to expand query using analyzer
            context: Optional context for query analysis
            **kwargs: Additional provider-specific options

        Returns:
            RRFSearchResult with fused results
        """
        start_time = datetime.now()

        # Step 1: Analyze and expand query
        search_queries = [query]  # Always include original
        query_analysis_dict = None

        if enable_query_expansion and self.query_analyzer:
            try:
                analysis = await self.query_analyzer.analyze(query, context)
                search_queries = analysis.get_all_search_queries()[:5]  # Limit strategies
                query_analysis_dict = {
                    "intent": analysis.intent,
                    "language": analysis.language,
                    "keywords": analysis.keywords,
                    "expanded_queries": analysis.expanded_queries,
                    "confidence": analysis.confidence,
                }
                logger.info(
                    "Query expanded for RRF search",
                    original=query,
                    expanded_count=len(search_queries),
                    intent=analysis.intent,
                )
            except Exception as e:
                logger.warning("Query expansion failed, using original", error=str(e))

        # Step 2: Execute parallel searches for each strategy
        ranked_lists: list[tuple[str, list[SearchResult]]] = []
        all_providers_used: set[str] = set()

        # Create search tasks for each query variant
        async def search_single_query(q: str, strategy_name: str) -> tuple[str, list[SearchResult]]:
            """Execute search for a single query across all providers."""
            results: list[SearchResult] = []

            tasks = [
                self._search_provider_with_timeout(provider, q, max_results_per_strategy, **kwargs)
                for provider in self.providers
            ]

            provider_results = await asyncio.gather(*tasks, return_exceptions=True)

            for provider, result in zip(self.providers, provider_results):
                if isinstance(result, Exception):
                    logger.debug(
                        "Provider search failed for strategy",
                        provider=provider.name,
                        strategy=strategy_name,
                        error=str(result),
                    )
                elif isinstance(result, list) and result:
                    results.extend(result)
                    all_providers_used.add(provider.name)

            return (strategy_name, results)

        # Execute all strategy searches in parallel
        strategy_tasks = [
            search_single_query(q, f"strategy_{i}" if i > 0 else "original")
            for i, q in enumerate(search_queries)
        ]

        ranked_lists = await asyncio.gather(*strategy_tasks)

        # Step 3: Apply RRF fusion
        strategy_weights = self._calculate_strategy_weights(search_queries)

        if isinstance(self._rrf_merger, SemanticRRF):
            # Use semantic RRF with query relevance
            keywords = query_analysis_dict.get("keywords", []) if query_analysis_dict else None
            rrf_result = self._rrf_merger.merge_with_query_relevance(
                query=query,
                ranked_lists=list(ranked_lists),
                query_keywords=keywords,
                weights=strategy_weights,
                max_results=max_total_results,
            )
        else:
            rrf_result = self._rrf_merger.merge(
                ranked_lists=list(ranked_lists),
                weights=strategy_weights,
                max_results=max_total_results,
            )

        # Extract final results
        final_results = rrf_result.get_search_results()

        search_time_ms = (datetime.now() - start_time).total_seconds() * 1000

        logger.info(
            "RRF search completed",
            query=query,
            strategies_used=len(search_queries),
            providers_used=list(all_providers_used),
            input_results=rrf_result.total_input_results,
            final_results=len(final_results),
            search_time_ms=round(search_time_ms, 2),
        )

        return RRFSearchResult(
            original_query=query,
            results=final_results,
            strategies_used=rrf_result.strategies_used,
            providers_used=list(all_providers_used),
            total_results=len(final_results),
            unique_urls=rrf_result.unique_results,
            search_time_ms=search_time_ms,
            rrf_merge_result=rrf_result,
            query_analysis=query_analysis_dict,
        )

    async def search_news_with_rrf(
        self,
        query: str,
        max_results_per_strategy: int = 10,
        days: int = 7,
        **kwargs,
    ) -> RRFSearchResult:
        """Search for news with RRF fusion."""
        kwargs["topic"] = "news"
        kwargs["days"] = days
        kwargs["freshness"] = "pw"
        kwargs["search_recency_filter"] = "week"

        return await self.search_with_rrf(
            query=query,
            max_results_per_strategy=max_results_per_strategy,
            **kwargs,
        )

    async def _search_provider_with_timeout(
        self,
        provider: SearchProvider,
        query: str,
        max_results: int,
        **kwargs,
    ) -> list[SearchResult]:
        """Execute provider search with timeout."""
        try:
            return await asyncio.wait_for(
                provider.search(query, max_results, **kwargs),
                timeout=self.timeout,
            )
        except asyncio.TimeoutError:
            logger.debug(
                "Provider search timed out",
                provider=provider.name,
                timeout=self.timeout,
            )
            raise
        except Exception as e:
            logger.debug(
                "Provider search failed",
                provider=provider.name,
                error=str(e),
            )
            raise

    def _calculate_strategy_weights(
        self,
        queries: list[str],
    ) -> dict[str, float]:
        """
        Calculate weights for each search strategy.

        Original query gets highest weight, expanded queries get decreasing weights.
        """
        weights = {}
        for i, _ in enumerate(queries):
            strategy_name = f"strategy_{i}" if i > 0 else "original"
            # Original: 1.0, then decreasing: 0.9, 0.8, 0.7...
            weight = max(1.0 - (i * 0.1), 0.5)
            weights[strategy_name] = weight
        return weights

    async def health_check_all(self) -> dict[str, bool]:
        """Check health of all providers."""
        results = {}
        for provider in self.providers:
            try:
                results[provider.name] = await asyncio.wait_for(
                    provider.health_check(),
                    timeout=10.0,
                )
            except Exception:
                results[provider.name] = False

        self._provider_health = results
        return results

    async def close_all(self) -> None:
        """Close all provider connections."""
        for provider in self.providers:
            if hasattr(provider, "close"):
                await provider.close()


def create_rrf_orchestrator(
    providers: list[SearchProvider],
    llm: Optional[Any] = None,
    timeout: float = 30.0,
    rrf_k: int = 60,
    enable_semantic: bool = True,
) -> RRFSearchOrchestrator:
    """
    Factory function to create an RRF Search Orchestrator.

    Args:
        providers: List of search providers
        llm: Optional LLM for query analysis
        timeout: Timeout per provider
        rrf_k: RRF constant
        enable_semantic: Enable semantic similarity in scoring

    Returns:
        Configured RRFSearchOrchestrator
    """
    from src.search.query_analyzer import QueryAnalyzer

    query_analyzer = None
    if llm:
        query_analyzer = QueryAnalyzer(llm)

    rrf_config = RRFConfig(
        k=rrf_k,
        boost_exact_matches=True,
        normalize_scores=True,
    )

    return RRFSearchOrchestrator(
        providers=providers,
        query_analyzer=query_analyzer,
        timeout=timeout,
        rrf_config=rrf_config,
        enable_semantic_rrf=enable_semantic,
    )

```

---

## backend/autonomous-crawler-service/src/search/perplexity.py

```py
"""Perplexity API provider."""

import httpx
import structlog
from typing import Any, Literal

from src.search.base import SearchProvider, SearchResult

logger = structlog.get_logger(__name__)


class PerplexitySearchProvider(SearchProvider):
    """
    Perplexity API client (Sonar models for search).
    
    Docs: https://docs.perplexity.ai/api-reference/chat-completions
    """
    
    BASE_URL = "https://api.perplexity.ai"
    
    # Available models
    MODELS = {
        "sonar": "sonar",  # Lightweight, fast
        "sonar-pro": "sonar-pro",  # More comprehensive
        "sonar-reasoning": "sonar-reasoning",  # With reasoning
        "sonar-reasoning-pro": "sonar-reasoning-pro",  # Best quality
    }
    
    def __init__(
        self,
        api_key: str,
        model: str = "sonar",
        timeout: float = 60.0,
    ):
        self.api_key = api_key
        self.model = self.MODELS.get(model, model)
        self.timeout = timeout
        self._client: httpx.AsyncClient | None = None
    
    @property
    def name(self) -> str:
        return "perplexity"
    
    async def _get_client(self) -> httpx.AsyncClient:
        """Get or create HTTP client."""
        if self._client is None or self._client.is_closed:
            self._client = httpx.AsyncClient(
                timeout=self.timeout,
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json",
                },
            )
        return self._client
    
    async def close(self) -> None:
        """Close the HTTP client."""
        if self._client and not self._client.is_closed:
            await self._client.aclose()
            self._client = None
    
    async def search(
        self,
        query: str,
        max_results: int = 10,
        search_recency_filter: Literal["month", "week", "day", "hour"] | None = None,
        search_domain_filter: list[str] | None = None,
        return_citations: bool = True,
        return_related_questions: bool = False,
        **kwargs,
    ) -> list[SearchResult]:
        """
        Execute Perplexity search using Sonar models.
        
        Args:
            query: Search query
            max_results: Not directly used (Perplexity returns variable citations)
            search_recency_filter: Filter by recency (month, week, day, hour)
            search_domain_filter: List of domains to restrict search
            return_citations: Return source citations
            return_related_questions: Return related questions
        """
        client = await self._get_client()
        
        # Build system prompt for search
        system_prompt = (
            "You are a search assistant. Return factual information with sources. "
            "Focus on finding relevant web pages and their content."
        )
        
        payload: dict[str, Any] = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": query},
            ],
            "return_citations": return_citations,
            "return_related_questions": return_related_questions,
        }
        
        # Add search filters if specified
        if search_recency_filter:
            payload["search_recency_filter"] = search_recency_filter
        if search_domain_filter:
            payload["search_domain_filter"] = search_domain_filter
        
        try:
            response = await client.post(
                f"{self.BASE_URL}/chat/completions",
                json=payload,
            )
            response.raise_for_status()
            data = response.json()
            
            results: list[SearchResult] = []
            
            # Extract citations as search results
            citations = data.get("citations", [])
            content = ""
            
            if data.get("choices"):
                content = data["choices"][0].get("message", {}).get("content", "")
            
            # Parse citations into results
            for i, citation_url in enumerate(citations[:max_results]):
                # Try to extract title from the content that references this citation
                # Citations are referenced as [1], [2], etc. in the content
                results.append(SearchResult(
                    title=f"Source {i + 1}",  # Perplexity doesn't provide titles directly
                    url=citation_url,
                    snippet=content[:500] if i == 0 else "",  # Full answer as snippet for first result
                    source_provider=self.name,
                    score=1.0 - (i * 0.1),  # Assume earlier citations are more relevant
                    raw_data={
                        "full_response": content,
                        "citation_index": i,
                        "model": self.model,
                    },
                ))
            
            logger.info(
                "Perplexity search completed",
                query=query,
                citations_count=len(citations),
                model=self.model,
            )
            
            return results
            
        except httpx.HTTPStatusError as e:
            logger.error(
                "Perplexity search HTTP error",
                query=query,
                status_code=e.response.status_code,
                error=str(e),
            )
            return []
        except Exception as e:
            logger.error(
                "Perplexity search failed",
                query=query,
                error=str(e),
            )
            return []
    
    async def search_with_context(
        self,
        query: str,
        context: str | None = None,
        focus: Literal["internet", "academic", "news"] = "internet",
        **kwargs,
    ) -> tuple[str, list[SearchResult]]:
        """
        Search with context and return both answer and citations.
        
        Args:
            query: Search query
            context: Additional context to consider
            focus: Search focus area
            
        Returns:
            Tuple of (AI answer, list of SearchResult)
        """
        client = await self._get_client()
        
        messages = []
        if context:
            messages.append({
                "role": "system",
                "content": f"Context: {context}\n\nProvide a comprehensive answer with citations.",
            })
        
        messages.append({"role": "user", "content": query})
        
        payload = {
            "model": self.model,
            "messages": messages,
            "return_citations": True,
        }
        
        try:
            response = await client.post(
                f"{self.BASE_URL}/chat/completions",
                json=payload,
            )
            response.raise_for_status()
            data = response.json()
            
            answer = ""
            if data.get("choices"):
                answer = data["choices"][0].get("message", {}).get("content", "")
            
            citations = data.get("citations", [])
            results = [
                SearchResult(
                    title=f"Citation {i + 1}",
                    url=url,
                    snippet="",
                    source_provider=self.name,
                    raw_data={"citation_index": i},
                )
                for i, url in enumerate(citations)
            ]
            
            return answer, results
            
        except Exception as e:
            logger.error("Perplexity context search failed", query=query, error=str(e))
            return "", []
    
    async def health_check(self) -> bool:
        """Check API health."""
        try:
            results = await self.search("test", max_results=1)
            return True
        except Exception:
            return False

```

---

## backend/autonomous-crawler-service/src/search/query_analyzer.py

```py
"""Query analyzer for semantic search enhancement using LLM."""

import asyncio
import re
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Callable, Optional

import structlog
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import HumanMessage, SystemMessage

logger = structlog.get_logger(__name__)


class SearchStrategy(Enum):
    """Search strategy types for fallback mechanisms."""

    FULL_QUERY = "full_query"
    KEYWORDS_AND = "keywords_and"
    KEYWORDS_OR = "keywords_or"
    PRIMARY_KEYWORD = "primary_keyword"
    SEMANTIC_VARIANT = "semantic_variant"
    RELATED_TOPIC = "related_topic"
    PARTIAL_MATCH = "partial_match"


@dataclass
class FallbackStrategy:
    """Represents a fallback search strategy."""

    strategy_type: SearchStrategy
    query: str
    priority: int
    description: str
    weight: float = 1.0


@dataclass
class QueryAnalysis:
    """Result of query analysis."""

    original_query: str
    intent: str  # search_intent: news, research, factcheck, general
    language: str  # detected language
    keywords: list[str]  # extracted keywords
    primary_keyword: str  # most important keyword
    entities: list[str]  # named entities (people, places, organizations)
    expanded_queries: list[str]  # semantically expanded queries
    synonyms: dict[str, list[str]]  # word -> synonyms mapping
    search_terms: list[str]  # optimized search terms for different strategies
    fallback_strategies: list[FallbackStrategy]  # ordered fallback strategies
    confidence: float  # analysis confidence score
    metadata: dict[str, Any] = field(default_factory=dict)

    def get_all_search_queries(self) -> list[str]:
        """Get all search queries for multi-strategy search."""
        queries = [self.original_query]
        queries.extend(self.expanded_queries)
        queries.extend(self.search_terms)
        # Deduplicate while preserving order
        seen = set()
        unique = []
        for q in queries:
            if q.lower() not in seen:
                seen.add(q.lower())
                unique.append(q)
        return unique

    def get_fallback_query(self, attempt_index: int) -> Optional[str]:
        """Get fallback query by attempt index."""
        if attempt_index < len(self.fallback_strategies):
            return self.fallback_strategies[attempt_index].query
        return None


class QueryAnalyzer:
    """
    Analyzes and expands search queries using LLM for better search accuracy.

    Features:
    - Intent detection (news, research, factcheck, general)
    - Keyword extraction
    - Named entity recognition
    - Query expansion with synonyms and related terms
    - Multi-language support
    - Search term optimization
    """

    ANALYSIS_PROMPT = """You are a search query analyzer. Analyze the given query and provide structured information to improve search accuracy.

For the query: "{query}"

Respond in the following JSON format ONLY (no additional text):
{{
    "intent": "<news|research|factcheck|opinion|general>",
    "language": "<detected language code, e.g., ko, en, ja>",
    "keywords": ["<key term 1>", "<key term 2>", ...],
    "entities": ["<named entity 1>", "<named entity 2>", ...],
    "expanded_queries": [
        "<semantically related query 1>",
        "<semantically related query 2>",
        "<semantically related query 3>"
    ],
    "synonyms": {{
        "<original term>": ["<synonym 1>", "<synonym 2>"]
    }},
    "search_terms": [
        "<optimized search term for web search>",
        "<optimized search term for news search>",
        "<english translation if non-english>"
    ],
    "confidence": <0.0 to 1.0>
}}

Guidelines:
1. For ambiguous or metaphorical queries (like "ë‘ë”ì§€ì˜ ê³µê²©ë ¥" which could be about moles' attack power, a game, or slang), generate multiple interpretations
2. Extract the core semantic meaning, not just literal keywords
3. For non-English queries, include English translations in search_terms
4. Identify if the query is about specific domains (politics, sports, tech, entertainment, etc.)
5. Generate expanded_queries that capture different aspects or interpretations of the query
6. Keep search_terms concise and optimized for search engines"""

    QUICK_KEYWORDS_PROMPT = """Extract key search terms from this query. Return ONLY a JSON array of strings, no explanation.
Query: "{query}"
Example output: ["term1", "term2", "term3"]"""

    def __init__(
        self,
        llm: BaseChatModel,
        enable_expansion: bool = True,
        max_expanded_queries: int = 5,
        cache_results: bool = True,
    ):
        """
        Initialize the query analyzer.

        Args:
            llm: Language model for query analysis
            enable_expansion: Whether to enable query expansion
            max_expanded_queries: Maximum number of expanded queries
            cache_results: Whether to cache analysis results
        """
        self.llm = llm
        self.enable_expansion = enable_expansion
        self.max_expanded_queries = max_expanded_queries
        self._cache: Optional[dict[str, QueryAnalysis]] = {} if cache_results else None

    async def analyze(
        self,
        query: str,
        context: Optional[str] = None,
        force_refresh: bool = False,
    ) -> QueryAnalysis:
        """
        Analyze a search query and generate expanded search terms.

        Args:
            query: The original search query
            context: Optional context about the search domain
            force_refresh: Force re-analysis even if cached

        Returns:
            QueryAnalysis with expanded queries and keywords
        """
        # Check cache
        cache_key = f"{query}:{context or ''}"
        if self._cache is not None and not force_refresh:
            if cache_key in self._cache:
                logger.debug("Using cached query analysis", query=query)
                return self._cache[cache_key]

        try:
            analysis = await self._analyze_with_llm(query, context)

            # Cache result
            if self._cache is not None:
                self._cache[cache_key] = analysis

            logger.info(
                "Query analyzed",
                query=query,
                intent=analysis.intent,
                keywords=analysis.keywords,
                expanded_count=len(analysis.expanded_queries),
            )

            return analysis

        except Exception as e:
            logger.error("Query analysis failed, using fallback", query=query, error=str(e))
            return self._fallback_analysis(query)

    async def _analyze_with_llm(
        self,
        query: str,
        context: Optional[str] = None,
    ) -> QueryAnalysis:
        """Perform LLM-based query analysis."""
        prompt = self.ANALYSIS_PROMPT.format(query=query)

        if context:
            prompt += f"\n\nAdditional context: {context}"

        messages = [
            SystemMessage(content="You are a search query analyzer. Respond only with valid JSON."),
            HumanMessage(content=prompt),
        ]

        response = await self.llm.ainvoke(messages)
        content = response.content.strip()

        # Parse JSON response
        import json

        # Try to extract JSON from the response
        json_match = re.search(r"\{[\s\S]*\}", content)
        if json_match:
            content = json_match.group()

        try:
            data = json.loads(content)
        except json.JSONDecodeError:
            logger.warning("Failed to parse LLM response as JSON", response=content[:200])
            return self._fallback_analysis(query)

        # Validate and extract fields with defaults
        keywords = data.get("keywords", [])[:10]
        primary_keyword = self._identify_primary_keyword(keywords, query)

        analysis = QueryAnalysis(
            original_query=query,
            intent=data.get("intent", "general"),
            language=data.get("language", "unknown"),
            keywords=keywords,
            primary_keyword=primary_keyword,
            entities=data.get("entities", [])[:5],
            expanded_queries=data.get("expanded_queries", [])[: self.max_expanded_queries],
            synonyms=data.get("synonyms", {}),
            search_terms=data.get("search_terms", [])[:5],
            fallback_strategies=[],  # Will be generated below
            confidence=min(max(data.get("confidence", 0.5), 0.0), 1.0),
            metadata={"context": context} if context else {},
        )

        # Generate fallback strategies
        analysis.fallback_strategies = self._generate_fallback_strategies(analysis)

        return analysis

    async def extract_keywords_quick(self, query: str) -> list[str]:
        """
        Quick keyword extraction without full analysis.

        Useful for simple queries or when speed is critical.
        """
        try:
            prompt = self.QUICK_KEYWORDS_PROMPT.format(query=query)
            messages = [HumanMessage(content=prompt)]

            response = await self.llm.ainvoke(messages)
            content = response.content.strip()

            # Parse JSON array
            import json

            # Try to extract JSON array
            array_match = re.search(r"\[[\s\S]*\]", content)
            if array_match:
                keywords = json.loads(array_match.group())
                if isinstance(keywords, list):
                    return [str(k) for k in keywords[:10]]

            return self._extract_keywords_rule_based(query)

        except Exception as e:
            logger.debug("Quick keyword extraction failed", error=str(e))
            return self._extract_keywords_rule_based(query)

    def _fallback_analysis(self, query: str) -> QueryAnalysis:
        """Fallback analysis when LLM fails."""
        keywords = self._extract_keywords_rule_based(query)
        primary_keyword = self._identify_primary_keyword(keywords, query)
        language = self._detect_language(query)

        analysis = QueryAnalysis(
            original_query=query,
            intent="general",
            language=language,
            keywords=keywords,
            primary_keyword=primary_keyword,
            entities=[],
            expanded_queries=[],
            synonyms={},
            search_terms=keywords[:3],
            fallback_strategies=[],  # Will be generated below
            confidence=0.3,
            metadata={"fallback": True},
        )

        # Generate fallback strategies
        analysis.fallback_strategies = self._generate_fallback_strategies(analysis)

        return analysis

    def _extract_keywords_rule_based(self, query: str) -> list[str]:
        """Rule-based keyword extraction as fallback."""
        # Remove common stop words (basic implementation)
        stop_words = {
            # English
            "the",
            "a",
            "an",
            "is",
            "are",
            "was",
            "were",
            "be",
            "been",
            "being",
            "have",
            "has",
            "had",
            "do",
            "does",
            "did",
            "will",
            "would",
            "could",
            "should",
            "may",
            "might",
            "must",
            "shall",
            "can",
            "need",
            "dare",
            "ought",
            "used",
            "to",
            "of",
            "in",
            "for",
            "on",
            "with",
            "at",
            "by",
            "from",
            "as",
            "into",
            "through",
            "during",
            "before",
            "after",
            "above",
            "below",
            "between",
            "under",
            "again",
            "further",
            "then",
            "once",
            "here",
            "there",
            "when",
            "where",
            "why",
            "how",
            "all",
            "each",
            "few",
            "more",
            "most",
            "other",
            "some",
            "such",
            "no",
            "nor",
            "not",
            "only",
            "own",
            "same",
            "so",
            "than",
            "too",
            "very",
            "just",
            "and",
            "but",
            "if",
            "or",
            "because",
            "until",
            "while",
            "although",
            # Korean particles and common words
            "ì˜",
            "ê°€",
            "ì´",
            "ì€",
            "ëŠ”",
            "ì„",
            "ë¥¼",
            "ì—",
            "ì—ì„œ",
            "ë¡œ",
            "ìœ¼ë¡œ",
            "ì™€",
            "ê³¼",
            "ë„",
            "ë§Œ",
            "ê¹Œì§€",
            "ë¶€í„°",
            "ì—ê²Œ",
            "í•œí…Œ",
            "ê»˜",
            "ì´ë‹¤",
            "ìˆë‹¤",
            "í•˜ë‹¤",
            "ë˜ë‹¤",
            "ì—†ë‹¤",
            "ì•„ë‹ˆë‹¤",
            "ê·¸",
            "ì´",
            "ì €",
            "ê²ƒ",
            "ìˆ˜",
            "ë“±",
            "ë“¤",
            "ë°",
            "ë”",
            "ë˜",
        }

        # Tokenize
        words = re.findall(r"[\wê°€-í£]+", query.lower())

        # Filter stop words and short words
        keywords = [w for w in words if w not in stop_words and len(w) > 1]

        return keywords[:10]

    def _detect_language(self, text: str) -> str:
        """Simple language detection based on character ranges."""
        # Count character types
        korean_count = len(re.findall(r"[ê°€-í£]", text))
        japanese_count = len(re.findall(r"[ã-ã‚“ã‚¡-ãƒ³]", text))
        chinese_count = len(re.findall(r"[\u4e00-\u9fff]", text))
        latin_count = len(re.findall(r"[a-zA-Z]", text))

        total = korean_count + japanese_count + chinese_count + latin_count
        if total == 0:
            return "unknown"

        if korean_count / total > 0.3:
            return "ko"
        elif japanese_count / total > 0.3:
            return "ja"
        elif chinese_count / total > 0.3:
            return "zh"
        else:
            return "en"

    def _identify_primary_keyword(self, keywords: list[str], original_query: str) -> str:
        """Identify the most important keyword from the list."""
        if not keywords:
            words = original_query.split()
            return words[0] if words else original_query

        # Score-based primary keyword selection
        scores = {}
        for keyword in keywords:
            score = 0.0

            # Length weight (longer keywords are more specific)
            score += min(len(keyword) / 10.0, 1.0) * 0.3

            # Position weight (earlier keywords are often more important)
            pos = original_query.lower().find(keyword.lower())
            if pos >= 0:
                score += (1.0 - pos / len(original_query)) * 0.3

            # Uppercase start (potential proper noun)
            if keyword[0].isupper():
                score += 0.2

            # Contains numbers (specific identifier)
            if any(c.isdigit() for c in keyword):
                score += 0.1

            scores[keyword] = score

        return max(scores.items(), key=lambda x: x[1])[0] if scores else keywords[0]

    def _generate_fallback_strategies(self, analysis: QueryAnalysis) -> list[FallbackStrategy]:
        """Generate ordered fallback search strategies."""
        strategies = []
        priority = 1

        # Strategy 1: Full query
        strategies.append(
            FallbackStrategy(
                strategy_type=SearchStrategy.FULL_QUERY,
                query=analysis.original_query,
                priority=priority,
                description="ì›ë³¸ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰"
                if analysis.language == "ko"
                else "Search with original query",
                weight=1.0,
            )
        )
        priority += 1

        # Strategy 2: Keywords AND
        if len(analysis.keywords) > 1:
            strategies.append(
                FallbackStrategy(
                    strategy_type=SearchStrategy.KEYWORDS_AND,
                    query=" ".join(analysis.keywords),
                    priority=priority,
                    description="ëª¨ë“  í‚¤ì›Œë“œë¡œ ê²€ìƒ‰"
                    if analysis.language == "ko"
                    else "Search with all keywords",
                    weight=0.9,
                )
            )
            priority += 1

        # Strategy 3: Primary keyword
        if analysis.primary_keyword:
            strategies.append(
                FallbackStrategy(
                    strategy_type=SearchStrategy.PRIMARY_KEYWORD,
                    query=analysis.primary_keyword,
                    priority=priority,
                    description="ì£¼ìš” í‚¤ì›Œë“œë§Œìœ¼ë¡œ ê²€ìƒ‰"
                    if analysis.language == "ko"
                    else "Search with primary keyword only",
                    weight=0.85,
                )
            )
            priority += 1

        # Strategy 4: Semantic variants (expanded queries)
        for i, expanded in enumerate(analysis.expanded_queries[:3]):
            if expanded.lower() != analysis.original_query.lower():
                strategies.append(
                    FallbackStrategy(
                        strategy_type=SearchStrategy.SEMANTIC_VARIANT,
                        query=expanded,
                        priority=priority,
                        description=f"ë³€í˜• ì¿¼ë¦¬ {i + 1}: {expanded}"
                        if analysis.language == "ko"
                        else f"Variant query {i + 1}: {expanded}",
                        weight=0.8 - (i * 0.1),
                    )
                )
                priority += 1

        # Strategy 5: Keywords OR (broader search)
        if len(analysis.keywords) > 1:
            or_query = " OR ".join(analysis.keywords[:5])
            strategies.append(
                FallbackStrategy(
                    strategy_type=SearchStrategy.KEYWORDS_OR,
                    query=or_query,
                    priority=priority,
                    description="í‚¤ì›Œë“œ OR ê²€ìƒ‰ (ë„“ì€ ê²€ìƒ‰)"
                    if analysis.language == "ko"
                    else "Keywords OR search (broader)",
                    weight=0.7,
                )
            )
            priority += 1

        # Strategy 6: Partial match (top 2 keywords)
        if len(analysis.keywords) >= 2:
            partial_query = f"{analysis.keywords[0]} {analysis.keywords[1]}"
            strategies.append(
                FallbackStrategy(
                    strategy_type=SearchStrategy.PARTIAL_MATCH,
                    query=partial_query,
                    priority=priority,
                    description="ìƒìœ„ í‚¤ì›Œë“œ ë¶€ë¶„ ë§¤ì¹­"
                    if analysis.language == "ko"
                    else "Top keywords partial match",
                    weight=0.65,
                )
            )
            priority += 1

        # Sort by priority
        strategies.sort(key=lambda s: s.priority)
        return strategies

    def build_no_result_message(self, analysis: QueryAnalysis) -> str:
        """Build a helpful message when no results are found."""
        if analysis.language == "ko":
            message = "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ê¸° ì–´ë ¤ì› ìŠµë‹ˆë‹¤. ë‹¤ìŒì„ ì‹œë„í•´ ë³´ì„¸ìš”:\n\n"
            message += "ì‹œë„í•œ ê²€ìƒ‰ì–´:\n"
            message += f"- {analysis.original_query}\n"
            message += f"- {analysis.primary_keyword}\n"

            message += "\nì¶”ì²œ ê²€ìƒ‰ ë°©ë²•:\n"
            message += "1. ê²€ìƒ‰ì–´ë¥¼ ë” êµ¬ì²´ì ìœ¼ë¡œ ë³€ê²½í•´ ë³´ì„¸ìš”\n"
            message += f"2. ë‹¤ë¥¸ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•´ ë³´ì„¸ìš”: {', '.join(analysis.keywords[:3])}\n"
            message += "3. ì‹œê°„ ë²”ìœ„ë¥¼ ì¡°ì •í•´ ë³´ì„¸ìš”\n"

            intent_desc = {
                "news": "ìµœì‹  ë‰´ìŠ¤",
                "research": "ì‹¬ì¸µ ë¶„ì„",
                "factcheck": "íŒ©íŠ¸ì²´í¬",
                "opinion": "ì—¬ë¡  ê²€ìƒ‰",
                "general": "ì¼ë°˜ ê²€ìƒ‰",
            }
            message += f"\në¶„ì„ëœ ì˜ë„: {intent_desc.get(analysis.intent, 'ì¼ë°˜ ê²€ìƒ‰')}"
        else:
            message = "Search results were difficult to find. Try the following:\n\n"
            message += "Queries attempted:\n"
            message += f"- {analysis.original_query}\n"
            message += f"- {analysis.primary_keyword}\n"

            message += "\nRecommended approaches:\n"
            message += "1. Try more specific keywords\n"
            message += f"2. Use alternative keywords: {', '.join(analysis.keywords[:3])}\n"
            message += "3. Adjust the time range\n"

            message += f"\nDetected intent: {analysis.intent}"

        return message

    def build_enhanced_task(self, analysis: QueryAnalysis, original_task: str) -> str:
        """Build an enhanced task with fallback instructions."""
        task = original_task + "\n\n"

        if analysis.language == "ko":
            task += "ê²€ìƒ‰ ì „ëµ (ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ìˆœì„œëŒ€ë¡œ ì‹œë„í•˜ì„¸ìš”):\n"
            for i, strategy in enumerate(analysis.fallback_strategies[:5], 1):
                task += f'{i}. {strategy.description}: "{strategy.query}"\n'
            task += "\nì¤‘ìš”: ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ë‹¤ê³  ë§í•˜ì§€ ë§ˆì„¸ìš”. ìœ„ì˜ ì „ëµì„ ëª¨ë‘ ì‹œë„í•˜ê³ ,\n"
            task += "ê´€ë ¨ëœ ì •ë³´ë¼ë„ ì°¾ì•„ì„œ ì œê³µí•˜ì„¸ìš”. ì™„ì „íˆ ì¼ì¹˜í•˜ì§€ ì•Šë”ë¼ë„\n"
            task += "ê°€ì¥ ê´€ë ¨ì„± ìˆëŠ” ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤."
        else:
            task += "Search strategies (try in order if no results):\n"
            for i, strategy in enumerate(analysis.fallback_strategies[:5], 1):
                task += f'{i}. {strategy.description}: "{strategy.query}"\n'
            task += (
                '\nIMPORTANT: Never say "not found" or "no results". Try ALL strategies above,\n'
            )
            task += "and provide whatever relevant information you can find. Even if not an exact match,\n"
            task += "providing the most relevant information is important."

        return task

    def clear_cache(self) -> None:
        """Clear the analysis cache."""
        if self._cache is not None:
            self._cache.clear()


class MultiStrategyQueryExpander:
    """
    Expands a single query into multiple search strategies.

    Strategies:
    1. Original query (exact match)
    2. Keyword-based query (extracted keywords)
    3. Semantic expansion (related concepts)
    4. Entity-focused query (named entities)
    5. Cross-lingual query (translations)
    """

    def __init__(self, analyzer: QueryAnalyzer):
        self.analyzer = analyzer

    async def expand(
        self,
        query: str,
        max_strategies: int = 5,
        context: Optional[str] = None,
    ) -> list[dict[str, Any]]:
        """
        Expand query into multiple search strategies.

        Returns:
            List of dicts with 'query', 'strategy', and 'weight' keys
        """
        analysis = await self.analyzer.analyze(query, context)

        strategies = []

        # Strategy 1: Original query (highest weight)
        strategies.append(
            {
                "query": analysis.original_query,
                "strategy": "original",
                "weight": 1.0,
            }
        )

        # Strategy 2: Keywords joined
        if analysis.keywords:
            keyword_query = " ".join(analysis.keywords[:5])
            if keyword_query.lower() != query.lower():
                strategies.append(
                    {
                        "query": keyword_query,
                        "strategy": "keywords",
                        "weight": 0.9,
                    }
                )

        # Strategy 3: Semantic expansions
        for i, expanded in enumerate(analysis.expanded_queries[:3]):
            strategies.append(
                {
                    "query": expanded,
                    "strategy": f"semantic_{i + 1}",
                    "weight": 0.8 - (i * 0.1),
                }
            )

        # Strategy 4: Entity-focused
        if analysis.entities:
            entity_query = " ".join(analysis.entities[:3])
            if entity_query.lower() != query.lower():
                strategies.append(
                    {
                        "query": entity_query,
                        "strategy": "entities",
                        "weight": 0.7,
                    }
                )

        # Strategy 5: Search terms (often includes translations)
        for i, term in enumerate(analysis.search_terms[:2]):
            if term.lower() != query.lower():
                strategies.append(
                    {
                        "query": term,
                        "strategy": f"search_term_{i + 1}",
                        "weight": 0.75 - (i * 0.1),
                    }
                )

        # Deduplicate and limit
        seen_queries = set()
        unique_strategies = []
        for s in strategies:
            q_lower = s["query"].lower()
            if q_lower not in seen_queries:
                seen_queries.add(q_lower)
                unique_strategies.append(s)

        return unique_strategies[:max_strategies]

```

---

## backend/autonomous-crawler-service/src/search/rrf.py

```py
"""Reciprocal Rank Fusion (RRF) for multi-strategy search result merging."""

from dataclasses import dataclass, field
from typing import Any, TypeVar, Callable, Optional
from collections import defaultdict

import structlog

from src.search.base import SearchResult

logger = structlog.get_logger(__name__)

T = TypeVar("T")


@dataclass
class RRFConfig:
    """Configuration for RRF algorithm."""

    k: int = 60  # RRF constant (default: 60, higher = more weight to lower ranks)
    min_score_threshold: float = 0.0  # Minimum RRF score to include result
    boost_exact_matches: bool = True  # Boost results that appear in multiple rankings
    multi_appearance_bonus: float = 0.1  # Bonus per additional appearance
    normalize_scores: bool = True  # Normalize final scores to 0-1 range


@dataclass
class RRFResult:
    """Result with RRF scoring information."""

    item: SearchResult
    rrf_score: float
    appearances: int  # Number of rankings this item appeared in
    rank_positions: list[int]  # Original positions in each ranking
    source_strategies: list[str]  # Which strategies found this result

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary."""
        return {
            "item": self.item.to_dict(),
            "rrf_score": self.rrf_score,
            "appearances": self.appearances,
            "rank_positions": self.rank_positions,
            "source_strategies": self.source_strategies,
        }


@dataclass
class RRFMergeResult:
    """Result of RRF merge operation."""

    results: list[RRFResult]
    total_input_results: int
    unique_results: int
    strategies_used: list[str]
    processing_time_ms: float
    config: RRFConfig = field(default_factory=RRFConfig)

    def get_search_results(self) -> list[SearchResult]:
        """Get just the SearchResult items, sorted by RRF score."""
        return [r.item for r in sorted(self.results, key=lambda x: x.rrf_score, reverse=True)]

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary."""
        return {
            "results": [r.to_dict() for r in self.results],
            "total_input_results": self.total_input_results,
            "unique_results": self.unique_results,
            "strategies_used": self.strategies_used,
            "processing_time_ms": self.processing_time_ms,
        }


class ReciprocalRankFusion:
    """
    Implements Reciprocal Rank Fusion (RRF) algorithm for combining
    multiple ranked lists into a single ranking.

    RRF Score = Î£ 1 / (k + rank_i) for each ranking list

    Features:
    - Combines results from multiple search strategies
    - Handles duplicates by URL normalization
    - Boosts results that appear in multiple rankings
    - Configurable k parameter and scoring thresholds

    Reference:
    Cormack, G. V., Clarke, C. L., & Buettcher, S. (2009).
    Reciprocal rank fusion outperforms condorcet and individual rank learning methods.
    """

    def __init__(self, config: Optional[RRFConfig] = None):
        """
        Initialize RRF merger.

        Args:
            config: RRF configuration options
        """
        self.config = config or RRFConfig()

    def merge(
        self,
        ranked_lists: list[tuple[str, list[SearchResult]]],
        weights: Optional[dict[str, float]] = None,
        max_results: int = 50,
    ) -> RRFMergeResult:
        """
        Merge multiple ranked lists using RRF.

        Args:
            ranked_lists: List of (strategy_name, results) tuples
            weights: Optional weights per strategy (default: equal weights)
            max_results: Maximum number of results to return

        Returns:
            RRFMergeResult with merged and scored results
        """
        import time

        start_time = time.time()

        if not ranked_lists:
            return RRFMergeResult(
                results=[],
                total_input_results=0,
                unique_results=0,
                strategies_used=[],
                processing_time_ms=0,
                config=self.config,
            )

        # Default equal weights
        if weights is None:
            weights = {name: 1.0 for name, _ in ranked_lists}

        # Track scores and metadata per unique URL
        url_scores: dict[str, float] = defaultdict(float)
        url_items: dict[str, SearchResult] = {}
        url_appearances: dict[str, int] = defaultdict(int)
        url_ranks: dict[str, list[int]] = defaultdict(list)
        url_strategies: dict[str, list[str]] = defaultdict(list)

        total_input = 0
        strategies_used = []

        for strategy_name, results in ranked_lists:
            if not results:
                continue

            strategies_used.append(strategy_name)
            strategy_weight = weights.get(strategy_name, 1.0)

            for rank, result in enumerate(results, start=1):
                total_input += 1

                # Normalize URL for deduplication
                normalized_url = self._normalize_url(result.url)

                # Calculate RRF score for this position
                rrf_score = strategy_weight / (self.config.k + rank)

                url_scores[normalized_url] += rrf_score
                url_appearances[normalized_url] += 1
                url_ranks[normalized_url].append(rank)
                url_strategies[normalized_url].append(strategy_name)

                # Keep the result with most information
                if normalized_url not in url_items:
                    url_items[normalized_url] = result
                else:
                    existing = url_items[normalized_url]
                    # Prefer result with longer snippet or more metadata
                    if len(result.snippet) > len(existing.snippet):
                        url_items[normalized_url] = result

        # Apply multi-appearance bonus
        if self.config.boost_exact_matches:
            for url, appearances in url_appearances.items():
                if appearances > 1:
                    bonus = self.config.multi_appearance_bonus * (appearances - 1)
                    url_scores[url] += bonus

        # Normalize scores if configured
        if self.config.normalize_scores and url_scores:
            max_score = max(url_scores.values())
            if max_score > 0:
                url_scores = {url: score / max_score for url, score in url_scores.items()}

        # Build final results
        rrf_results = []
        for url, score in url_scores.items():
            if score >= self.config.min_score_threshold:
                rrf_results.append(
                    RRFResult(
                        item=url_items[url],
                        rrf_score=score,
                        appearances=url_appearances[url],
                        rank_positions=url_ranks[url],
                        source_strategies=url_strategies[url],
                    )
                )

        # Sort by RRF score (descending)
        rrf_results.sort(key=lambda x: x.rrf_score, reverse=True)

        # Limit results
        rrf_results = rrf_results[:max_results]

        processing_time = (time.time() - start_time) * 1000

        logger.info(
            "RRF merge completed",
            total_input=total_input,
            unique_results=len(rrf_results),
            strategies=strategies_used,
            processing_time_ms=round(processing_time, 2),
        )

        return RRFMergeResult(
            results=rrf_results,
            total_input_results=total_input,
            unique_results=len(rrf_results),
            strategies_used=strategies_used,
            processing_time_ms=processing_time,
            config=self.config,
        )

    def merge_with_reranking(
        self,
        ranked_lists: list[tuple[str, list[SearchResult]]],
        rerank_fn: Callable[[SearchResult], float],
        rerank_weight: float = 0.3,
        **kwargs,
    ) -> RRFMergeResult:
        """
        Merge with additional reranking based on a custom scoring function.

        Args:
            ranked_lists: List of (strategy_name, results) tuples
            rerank_fn: Function that takes a SearchResult and returns a score (0-1)
            rerank_weight: Weight for the reranking score (RRF weight = 1 - rerank_weight)
            **kwargs: Additional arguments passed to merge()

        Returns:
            RRFMergeResult with combined RRF and reranking scores
        """
        # First do standard RRF merge
        rrf_result = self.merge(ranked_lists, **kwargs)

        # Apply reranking
        for result in rrf_result.results:
            try:
                rerank_score = rerank_fn(result.item)
                # Combine RRF score with rerank score
                combined = result.rrf_score * (1 - rerank_weight) + rerank_score * rerank_weight
                result.rrf_score = combined
            except Exception as e:
                logger.debug("Reranking failed for result", url=result.item.url, error=str(e))

        # Re-sort by new scores
        rrf_result.results.sort(key=lambda x: x.rrf_score, reverse=True)

        return rrf_result

    def _normalize_url(self, url: str) -> str:
        """Normalize URL for deduplication."""
        try:
            from urllib.parse import urlparse

            parsed = urlparse(url)
            # Remove www. prefix and trailing slash
            netloc = parsed.netloc.lower().lstrip("www.")
            path = parsed.path.rstrip("/")
            # Remove common tracking parameters
            return f"{netloc}{path}"
        except Exception:
            return url.lower()


class SemanticRRF(ReciprocalRankFusion):
    """
    Extended RRF with semantic similarity scoring.

    Combines RRF with semantic similarity between query and results
    for better relevance ranking.
    """

    def __init__(
        self,
        config: Optional[RRFConfig] = None,
        similarity_weight: float = 0.2,
    ):
        """
        Initialize Semantic RRF.

        Args:
            config: RRF configuration
            similarity_weight: Weight for semantic similarity score (0-1)
        """
        super().__init__(config)
        self.similarity_weight = similarity_weight

    def merge_with_query_relevance(
        self,
        query: str,
        ranked_lists: list[tuple[str, list[SearchResult]]],
        query_keywords: Optional[list[str]] = None,
        **kwargs,
    ) -> RRFMergeResult:
        """
        Merge with query relevance scoring.

        Args:
            query: Original search query
            ranked_lists: List of (strategy_name, results) tuples
            query_keywords: Pre-extracted keywords for matching
            **kwargs: Additional arguments passed to merge()
        """
        keywords = query_keywords or self._extract_keywords(query)

        def relevance_scorer(result: SearchResult) -> float:
            """Score result based on keyword presence."""
            text = f"{result.title} {result.snippet}".lower()

            if not keywords:
                return 0.5

            matches = sum(1 for kw in keywords if kw.lower() in text)
            return min(matches / len(keywords), 1.0)

        return self.merge_with_reranking(
            ranked_lists=ranked_lists,
            rerank_fn=relevance_scorer,
            rerank_weight=self.similarity_weight,
            **kwargs,
        )

    def _extract_keywords(self, query: str) -> list[str]:
        """Simple keyword extraction."""
        import re

        # Basic tokenization
        words = re.findall(r"[\wê°€-í£]+", query.lower())
        # Filter short words
        return [w for w in words if len(w) > 1]


def create_rrf_merger(
    k: int = 60,
    boost_duplicates: bool = True,
    normalize: bool = True,
) -> ReciprocalRankFusion:
    """
    Factory function to create an RRF merger with common configurations.

    Args:
        k: RRF constant (higher = more weight to lower ranks)
        boost_duplicates: Boost results appearing in multiple rankings
        normalize: Normalize final scores to 0-1

    Returns:
        Configured ReciprocalRankFusion instance
    """
    config = RRFConfig(
        k=k,
        boost_exact_matches=boost_duplicates,
        normalize_scores=normalize,
    )
    return ReciprocalRankFusion(config)


def create_semantic_rrf_merger(
    similarity_weight: float = 0.2,
    **kwargs,
) -> SemanticRRF:
    """
    Factory function to create a Semantic RRF merger.

    Args:
        similarity_weight: Weight for semantic similarity (0-1)
        **kwargs: Additional RRFConfig arguments

    Returns:
        Configured SemanticRRF instance
    """
    config = RRFConfig(**kwargs) if kwargs else None
    return SemanticRRF(config, similarity_weight=similarity_weight)

```

---

## backend/autonomous-crawler-service/src/search/tavily.py

```py
"""Tavily Search API provider."""

import httpx
import structlog
from typing import Any, Literal

from src.search.base import SearchProvider, SearchResult

logger = structlog.get_logger(__name__)


class TavilySearchProvider(SearchProvider):
    """
    Tavily Search API client.
    
    Docs: https://docs.tavily.com/docs/tavily-api/rest_api
    """
    
    BASE_URL = "https://api.tavily.com"
    
    def __init__(self, api_key: str, timeout: float = 60.0):
        self.api_key = api_key
        self.timeout = timeout
        self._client: httpx.AsyncClient | None = None
    
    @property
    def name(self) -> str:
        return "tavily"
    
    async def _get_client(self) -> httpx.AsyncClient:
        """Get or create HTTP client."""
        if self._client is None or self._client.is_closed:
            self._client = httpx.AsyncClient(
                timeout=self.timeout,
                headers={"Content-Type": "application/json"},
            )
        return self._client
    
    async def close(self) -> None:
        """Close the HTTP client."""
        if self._client and not self._client.is_closed:
            await self._client.aclose()
            self._client = None
    
    async def search(
        self,
        query: str,
        max_results: int = 10,
        search_depth: Literal["basic", "advanced"] = "basic",
        include_domains: list[str] | None = None,
        exclude_domains: list[str] | None = None,
        include_answer: bool = False,
        include_raw_content: bool = False,
        topic: Literal["general", "news"] = "general",
        days: int | None = None,  # For news topic, limit to N days
        **kwargs,
    ) -> list[SearchResult]:
        """
        Execute Tavily search.
        
        Args:
            query: Search query
            max_results: Maximum results (up to 10 for basic, 20 for advanced)
            search_depth: basic (faster) or advanced (more comprehensive)
            include_domains: List of domains to include
            exclude_domains: List of domains to exclude
            include_answer: Include AI-generated answer
            include_raw_content: Include raw HTML content
            topic: general or news
            days: For news, limit to past N days
        """
        client = await self._get_client()
        
        payload: dict[str, Any] = {
            "api_key": self.api_key,
            "query": query,
            "max_results": max_results,
            "search_depth": search_depth,
            "include_answer": include_answer,
            "include_raw_content": include_raw_content,
            "topic": topic,
        }
        
        if include_domains:
            payload["include_domains"] = include_domains
        if exclude_domains:
            payload["exclude_domains"] = exclude_domains
        if days and topic == "news":
            payload["days"] = days
        
        try:
            response = await client.post(
                f"{self.BASE_URL}/search",
                json=payload,
            )
            response.raise_for_status()
            data = response.json()
            
            results: list[SearchResult] = []
            
            for item in data.get("results", []):
                results.append(SearchResult(
                    title=item.get("title", ""),
                    url=item.get("url", ""),
                    snippet=item.get("content", ""),
                    source_provider=self.name,
                    published_date=item.get("published_date"),
                    score=item.get("score"),
                    raw_data={
                        "raw_content": item.get("raw_content"),
                        **item,
                    },
                ))
            
            # Add AI answer if available
            if include_answer and data.get("answer"):
                logger.info(
                    "Tavily AI answer generated",
                    query=query,
                    answer_preview=data["answer"][:100],
                )
            
            logger.info(
                "Tavily search completed",
                query=query,
                results_count=len(results),
                search_depth=search_depth,
            )
            
            return results
            
        except httpx.HTTPStatusError as e:
            logger.error(
                "Tavily search HTTP error",
                query=query,
                status_code=e.response.status_code,
                error=str(e),
            )
            return []
        except Exception as e:
            logger.error(
                "Tavily search failed",
                query=query,
                error=str(e),
            )
            return []
    
    async def search_news(
        self,
        query: str,
        max_results: int = 10,
        days: int = 7,
        **kwargs,
    ) -> list[SearchResult]:
        """Search news specifically."""
        return await self.search(
            query=query,
            max_results=max_results,
            topic="news",
            days=days,
            **kwargs,
        )
    
    async def extract_content(
        self,
        urls: list[str],
    ) -> list[dict[str, Any]]:
        """
        Extract content from URLs using Tavily Extract API.
        
        Args:
            urls: List of URLs to extract content from
            
        Returns:
            List of extracted content dictionaries
        """
        client = await self._get_client()
        
        try:
            response = await client.post(
                f"{self.BASE_URL}/extract",
                json={
                    "api_key": self.api_key,
                    "urls": urls,
                },
            )
            response.raise_for_status()
            data = response.json()
            
            return data.get("results", [])
            
        except Exception as e:
            logger.error("Tavily extract failed", urls=urls, error=str(e))
            return []
    
    async def health_check(self) -> bool:
        """Check API health."""
        try:
            results = await self.search("test", max_results=1)
            return True  # Tavily returns empty for simple queries but API works
        except Exception:
            return False

```

---

## backend/autonomous-crawler-service/src/state/__init__.py

```py
"""
State management module for autonomous-crawler-service.

Provides persistent storage for task results using Redis with in-memory fallback.
"""

from src.state.store import StateStore, get_state_store

__all__ = ["StateStore", "get_state_store"]

```

---

## backend/autonomous-crawler-service/src/state/store.py

```py
"""
State Storage Module for autonomous-crawler-service.

Provides persistent storage for task results using Redis.
Falls back to in-memory storage if Redis is unavailable.

Features:
- Redis backend with configurable TTL
- In-memory fallback for resilience
- Automatic state restoration on startup
- Async-safe with lock protection
"""

import asyncio
import json
from dataclasses import asdict
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional

import structlog

from src.config import get_settings

logger = structlog.get_logger(__name__)


class EnhancedJSONEncoder(json.JSONEncoder):
    """JSON encoder that handles dataclasses, enums, and datetime objects."""

    def default(self, o: Any) -> Any:
        if hasattr(o, "__dataclass_fields__"):
            return asdict(o)
        if hasattr(o, "model_dump"):
            # Pydantic v2 models
            return o.model_dump()
        if hasattr(o, "dict"):
            # Pydantic v1 models
            return o.dict()
        if isinstance(o, Enum):
            return o.value
        if isinstance(o, datetime):
            return o.isoformat()
        return super().default(o)


class StateStore:
    """
    Persistent state storage with Redis backend and in-memory fallback.

    Usage:
        store = StateStore()
        await store.connect()

        # Save task result
        await store.save_task("task-123", task_result)

        # Load task result
        result = await store.load_task("task-123")

        # List recent tasks
        tasks = await store.list_tasks(status="success", limit=10)

        # Cleanup
        await store.disconnect()
    """

    def __init__(self):
        self._redis = None
        self._memory_store: Dict[str, Any] = {}
        self._using_redis = False
        self._lock = asyncio.Lock()
        self._settings = get_settings().redis

    async def connect(self) -> bool:
        """
        Connect to Redis.

        Returns:
            True if Redis connection successful, False if falling back to memory.
        """
        if not self._settings.enabled:
            logger.info("Redis disabled in settings, using in-memory storage")
            return False

        try:
            import redis.asyncio as redis

            self._redis = redis.from_url(
                self._settings.url,
                encoding="utf-8",
                decode_responses=True,
                socket_connect_timeout=self._settings.connection_timeout,
                socket_timeout=self._settings.socket_timeout,
                max_connections=self._settings.max_connections,
                retry_on_timeout=self._settings.retry_on_timeout,
            )

            # Test connection
            await self._redis.ping()
            self._using_redis = True

            logger.info(
                "Connected to Redis",
                url=self._settings.url.split("@")[-1],  # Hide password if present
                prefix=self._settings.prefix,
            )

            # Load existing results from Redis into memory cache
            await self._load_existing_results()

            return True

        except ImportError:
            logger.warning(
                "redis package not installed, falling back to in-memory storage",
                hint="Install with: pip install redis>=5.0.0",
            )
            self._using_redis = False
            return False

        except Exception as e:
            logger.warning(
                "Failed to connect to Redis, falling back to in-memory storage",
                error=str(e),
                url=self._settings.url.split("@")[-1],
            )
            self._using_redis = False
            return False

    async def _load_existing_results(self):
        """Load existing results from Redis into memory on startup."""
        if not self._using_redis or not self._redis:
            return

        try:
            pattern = f"{self._settings.prefix}:task:*"
            cursor = 0
            loaded_count = 0

            while True:
                cursor, keys = await self._redis.scan(cursor, match=pattern, count=100)
                for key in keys:
                    task_id = key.split(":")[-1]
                    task_data = await self._redis.get(key)
                    if task_data:
                        self._memory_store[task_id] = json.loads(task_data)
                        loaded_count += 1

                if cursor == 0:
                    break

            if loaded_count > 0:
                logger.info(
                    "Restored tasks from Redis",
                    count=loaded_count,
                )

        except Exception as e:
            logger.warning(
                "Failed to load existing results from Redis",
                error=str(e),
            )

    async def disconnect(self):
        """Close Redis connection."""
        if self._redis:
            try:
                await self._redis.close()
                logger.info("Disconnected from Redis")
            except Exception as e:
                logger.warning("Error closing Redis connection", error=str(e))
            finally:
                self._redis = None
                self._using_redis = False

    def _key(self, task_id: str) -> str:
        """Generate Redis key for a task."""
        return f"{self._settings.prefix}:task:{task_id}"

    async def save_task(self, task_id: str, task_result: Any) -> bool:
        """
        Save task result to storage.

        Args:
            task_id: Unique task identifier
            task_result: Task result object (dict, dataclass, or Pydantic model)

        Returns:
            True if save successful
        """
        async with self._lock:
            try:
                # Convert to dict if needed
                if hasattr(task_result, "__dataclass_fields__"):
                    data = asdict(task_result)
                elif hasattr(task_result, "model_dump"):
                    data = task_result.model_dump()
                elif hasattr(task_result, "dict"):
                    data = task_result.dict()
                elif isinstance(task_result, dict):
                    data = task_result
                else:
                    data = task_result

                # Serialize to JSON
                data_json = json.dumps(data, cls=EnhancedJSONEncoder)

                # Store in memory cache (for fast reads)
                self._memory_store[task_id] = json.loads(data_json)

                # Store in Redis (for persistence)
                if self._using_redis and self._redis:
                    ttl_seconds = self._settings.result_ttl_hours * 60 * 60
                    await self._redis.set(self._key(task_id), data_json, ex=ttl_seconds)

                logger.debug(
                    "Task saved",
                    task_id=task_id,
                    redis=self._using_redis,
                )
                return True

            except Exception as e:
                logger.error(
                    "Failed to save task",
                    task_id=task_id,
                    error=str(e),
                )
                return False

    async def load_task(self, task_id: str) -> Optional[Dict[str, Any]]:
        """
        Load task result from storage.

        Args:
            task_id: Unique task identifier

        Returns:
            Task result dict or None if not found
        """
        # Check memory cache first (fast path)
        if task_id in self._memory_store:
            return self._memory_store[task_id]

        # Try Redis if available
        if self._using_redis and self._redis:
            try:
                data_json = await self._redis.get(self._key(task_id))
                if data_json:
                    data = json.loads(data_json)
                    # Cache in memory for faster subsequent access
                    self._memory_store[task_id] = data
                    return data
            except Exception as e:
                logger.warning(
                    "Failed to load task from Redis",
                    task_id=task_id,
                    error=str(e),
                )

        return None

    async def delete_task(self, task_id: str) -> bool:
        """
        Delete task result from storage.

        Args:
            task_id: Unique task identifier

        Returns:
            True if deletion successful
        """
        async with self._lock:
            # Remove from memory
            self._memory_store.pop(task_id, None)

            # Remove from Redis
            if self._using_redis and self._redis:
                try:
                    await self._redis.delete(self._key(task_id))
                except Exception as e:
                    logger.warning(
                        "Failed to delete task from Redis",
                        task_id=task_id,
                        error=str(e),
                    )

            logger.debug("Task deleted", task_id=task_id)
            return True

    async def list_tasks(
        self,
        status: Optional[str] = None,
        limit: int = 50,
    ) -> List[Dict[str, Any]]:
        """
        List tasks with optional filtering.

        Args:
            status: Filter by status (e.g., "success", "failed", "timeout")
            limit: Maximum number of tasks to return

        Returns:
            List of task result dicts
        """
        tasks = []
        for task_id, task_data in list(self._memory_store.items())[-limit * 2 :]:
            if status and task_data.get("status", "").lower() != status.lower():
                continue
            tasks.append(task_data)
            if len(tasks) >= limit:
                break

        return tasks

    async def get_stats(self) -> Dict[str, Any]:
        """
        Get storage statistics.

        Returns:
            Dict with storage stats
        """
        stats = {
            "using_redis": self._using_redis,
            "memory_count": len(self._memory_store),
            "redis_url": self._settings.url.split("@")[-1] if self._using_redis else None,
            "ttl_hours": self._settings.result_ttl_hours,
        }

        if self._using_redis and self._redis:
            try:
                pattern = f"{self._settings.prefix}:task:*"
                cursor = 0
                redis_count = 0
                while True:
                    cursor, keys = await self._redis.scan(cursor, match=pattern, count=100)
                    redis_count += len(keys)
                    if cursor == 0:
                        break
                stats["redis_count"] = redis_count
            except Exception:
                stats["redis_count"] = "unknown"

        return stats

    @property
    def is_redis_connected(self) -> bool:
        """Check if Redis is connected."""
        return self._using_redis

    @property
    def task_count(self) -> int:
        """Get number of tasks in memory cache."""
        return len(self._memory_store)

    def get_memory_store(self) -> Dict[str, Any]:
        """Get direct access to memory store (for backward compatibility)."""
        return self._memory_store


# Singleton instance
_store: Optional[StateStore] = None


async def get_state_store() -> StateStore:
    """
    Get or create the singleton StateStore instance.

    This ensures only one StateStore exists throughout the application lifecycle.

    Returns:
        Initialized StateStore instance
    """
    global _store
    if _store is None:
        _store = StateStore()
        await _store.connect()
    return _store


async def close_state_store():
    """Close the singleton StateStore instance."""
    global _store
    if _store is not None:
        await _store.disconnect()
        _store = None

```

---

## backend/data-collection-service/build.gradle.kts

```kts
// Collector Service ëª¨ë“ˆ ë¹Œë“œ ì„¤ì •

plugins {
    java
    id("org.springframework.boot")
    id("io.spring.dependency-management")
}

dependencies {
    // Spring Boot Web
    implementation("org.springframework.boot:spring-boot-starter-web")
    
    // Spring Security
    implementation("org.springframework.boot:spring-boot-starter-security")
    
    // JWT Support
    implementation("io.jsonwebtoken:jjwt-api:0.12.3")
    runtimeOnly("io.jsonwebtoken:jjwt-impl:0.12.3")
    runtimeOnly("io.jsonwebtoken:jjwt-jackson:0.12.3")
    
    // OpenAPI / Swagger
    implementation("org.springdoc:springdoc-openapi-starter-webmvc-ui:2.3.0")
    
    // Spring Data JPA
    implementation("org.springframework.boot:spring-boot-starter-data-jpa")
    
    // Redis
    implementation("org.springframework.boot:spring-boot-starter-data-redis")
    
    // WebClient for HTTP calls (web-crawler í†µí•©)
    implementation("org.springframework.boot:spring-boot-starter-webflux")
    
    // Database
    runtimeOnly("org.postgresql:postgresql")
    
    // Database Migration (ì„ íƒ)
    implementation("org.flywaydb:flyway-core:10.4.1")
    implementation("org.flywaydb:flyway-database-postgresql:10.4.1")
    
    // JSON Processing
    implementation("com.fasterxml.jackson.dataformat:jackson-dataformat-xml")
    implementation("com.fasterxml.jackson.datatype:jackson-datatype-jsr310")
    
    // HTML Parsing (ì›¹ í¬ë¡¤ë§ìš©)
    implementation("org.jsoup:jsoup:1.17.1")
    
    // MongoDB (AI ì‘ë‹µ ì €ì¥ìš©)
    implementation("org.springframework.boot:spring-boot-starter-data-mongodb")
    
    // RSS Feed Parsing
    implementation("com.rometools:rome:2.1.0")
    
    // Async Support
    implementation("org.springframework.boot:spring-boot-starter-aop")
    
    // Kafka (for AI integration and asynchronous messaging)
    implementation("org.springframework.kafka:spring-kafka")
    
    // PDF Generation - iText 7 (AGPL License)
    implementation("com.itextpdf:itext7-core:8.0.2")
    implementation("com.itextpdf:html2pdf:5.0.2")
    
    // Chart Generation - JFreeChart for server-side charts
    implementation("org.jfree:jfreechart:1.5.4")
    
    // Spring Boot Actuator (Health Check, Metrics)
    implementation("org.springframework.boot:spring-boot-starter-actuator")
    
    // Micrometer for Prometheus metrics
    implementation("io.micrometer:micrometer-registry-prometheus")
    
    // Caffeine Cache (Local cache fallback)
    implementation("com.github.ben-manes.caffeine:caffeine:3.1.8")
    
    // Spring Retry (ì¬ì‹œë„ ë¡œì§)
    implementation("org.springframework.retry:spring-retry")
    
    // Test
    testImplementation("com.h2database:h2")
    testImplementation("org.testcontainers:testcontainers:1.19.3")
    testImplementation("org.testcontainers:postgresql:1.19.3")
    testImplementation("org.testcontainers:junit-jupiter:1.19.3")
    testImplementation("org.testcontainers:mongodb:1.19.3")
}

tasks.named<org.springframework.boot.gradle.tasks.bundling.BootJar>("bootJar") {
    archiveBaseName.set("collector-service")
    archiveVersion.set("1.0.0")
}

```

---

## backend/data-collection-service/maigret-worker/main.py

```py
import asyncio
import json
import logging
import os
import subprocess
import tempfile
import uuid
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from contextlib import asynccontextmanager
import sys

from fastapi import FastAPI, HTTPException, BackgroundTasks, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

# Add shared module to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

# Try to import proxy client
try:
    from shared.proxy_client import ProxyRotationClient, ProxyInfo

    PROXY_CLIENT_AVAILABLE = True
except ImportError:
    PROXY_CLIENT_AVAILABLE = False
    ProxyRotationClient = None
    ProxyInfo = None

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


# --- Configuration ---
class AppConfig:
    """Application configuration from environment variables."""

    PORT = int(os.getenv("PORT", "8020"))
    LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")

    # Maigret configuration
    MAX_CONCURRENT_SCANS = int(os.getenv("MAX_CONCURRENT_SCANS", "3"))
    SCAN_TIMEOUT_SEC = int(os.getenv("SCAN_TIMEOUT_SEC", "300"))  # 5 minutes default
    REQUEST_DELAY_MS = int(os.getenv("REQUEST_DELAY_MS", "100"))

    # Output directory for reports
    REPORTS_DIR = Path(os.getenv("REPORTS_DIR", "/app/reports"))

    # Proxy configuration (optional)
    PROXY_URL = os.getenv("PROXY_URL", None)
    USE_TOR = os.getenv("USE_TOR", "false").lower() == "true"

    # Site filtering
    TOP_SITES_ONLY = os.getenv("TOP_SITES_ONLY", "false").lower() == "true"
    MAX_SITES = int(os.getenv("MAX_SITES", "500"))


config = AppConfig()

# Ensure reports directory exists
config.REPORTS_DIR.mkdir(parents=True, exist_ok=True)

# Semaphore for concurrent scan control
scan_semaphore = asyncio.Semaphore(config.MAX_CONCURRENT_SCANS)

# Proxy rotation configuration
USE_PROXY_ROTATION = os.getenv("USE_PROXY_ROTATION", "true").lower() == "true"
PROXY_ROTATION_URL = os.getenv("PROXY_ROTATION_URL", "http://ip-rotation:8050")

# Initialize proxy client (if available)
proxy_client = None
if PROXY_CLIENT_AVAILABLE and USE_PROXY_ROTATION:
    proxy_client = ProxyRotationClient(
        base_url=PROXY_ROTATION_URL,
        timeout=5.0,
        enabled=True,
    )
    logger.info(f"Proxy rotation enabled, connecting to {PROXY_ROTATION_URL}")

# Proxy rotation configuration
USE_PROXY_ROTATION = os.getenv("USE_PROXY_ROTATION", "true").lower() == "true"
PROXY_ROTATION_URL = os.getenv("PROXY_ROTATION_URL", "http://ip-rotation:8050")

# Initialize proxy client (if available)
proxy_client: "ProxyRotationClient | None" = None
if PROXY_CLIENT_AVAILABLE and USE_PROXY_ROTATION:
    proxy_client = ProxyRotationClient(
        base_url=PROXY_ROTATION_URL,
        timeout=5.0,
        enabled=True,
    )
    logger.info(f"Proxy rotation enabled, connecting to {PROXY_ROTATION_URL}")


# --- Enums and Models ---
class ScanStatus(str, Enum):
    PENDING = "PENDING"
    RUNNING = "RUNNING"
    COMPLETED = "COMPLETED"
    FAILED = "FAILED"
    TIMEOUT = "TIMEOUT"


class ScanRequest(BaseModel):
    """Request model for username scan."""

    username: str = Field(
        ..., min_length=1, max_length=100, description="Username to scan"
    )
    options: Optional[Dict[str, Any]] = Field(
        default=None, description="Additional Maigret options"
    )
    timeout_sec: Optional[int] = Field(
        default=None, ge=30, le=600, description="Scan timeout in seconds (30-600)"
    )
    top_sites_only: Optional[bool] = Field(
        default=None, description="Only scan top/popular sites for faster results"
    )


class AccountInfo(BaseModel):
    """Information about a discovered account."""

    site_name: str
    url: str
    username: str
    status: str = "claimed"
    tags: List[str] = Field(default_factory=list)


class ScanSummary(BaseModel):
    """Summary of scan results."""

    total_sites_checked: int
    accounts_found: int
    accounts_claimed: int
    accounts_available: int
    accounts_error: int
    scan_duration_ms: int


class ScanResult(BaseModel):
    """Complete scan result."""

    scan_id: str
    username: str
    status: ScanStatus
    summary: Optional[ScanSummary] = None
    accounts: List[AccountInfo] = Field(default_factory=list)
    raw_json_path: Optional[str] = None
    error_message: Optional[str] = None
    started_at: Optional[str] = None
    completed_at: Optional[str] = None


class ScanResponse(BaseModel):
    """Response model for scan endpoint."""

    status: str = "ok"
    scan_id: str
    message: str
    result: Optional[ScanResult] = None


class HealthResponse(BaseModel):
    """Health check response."""

    status: str
    version: str
    maigret_available: bool
    active_scans: int
    max_concurrent_scans: int
    proxy_rotation_enabled: bool = False
    proxy_service_healthy: bool = False


# --- In-memory scan tracking ---
# In production, this should be replaced with Redis or a database
active_scans: Dict[str, ScanResult] = {}


# --- Helper Functions ---
def sanitize_username(username: str) -> str:
    """Sanitize username to prevent command injection."""
    # Remove any shell-dangerous characters
    import re

    # Only allow alphanumeric, underscore, dash, dot
    sanitized = re.sub(r"[^a-zA-Z0-9_\-.]", "", username)
    if not sanitized:
        raise ValueError("Invalid username after sanitization")
    return sanitized


def build_maigret_command(
    username: str,
    output_dir: Path,
    options: Optional[Dict] = None,
    proxy_url: Optional[str] = None,
) -> List[str]:
    """Build Maigret CLI command with options.

    Args:
        username: Username to scan
        output_dir: Directory for output files
        options: Additional scan options
        proxy_url: Optional proxy URL from rotation service (takes priority)
    """
    cmd = [
        "maigret",
        username,
        "--json",
        "simple",
        "--folderoutput",
        str(output_dir),
    ]

    # Add timeout per site
    cmd.extend(["--timeout", "10"])

    # Proxy configuration (rotation proxy takes priority)
    if proxy_url:
        # Determine proxy type from URL
        if proxy_url.startswith("socks"):
            cmd.extend(["--tor-proxy", proxy_url])
        else:
            cmd.extend(["--proxy", proxy_url])
    elif config.PROXY_URL:
        cmd.extend(["--proxy", config.PROXY_URL])
    elif config.USE_TOR:
        cmd.extend(["--tor-proxy", "socks5://127.0.0.1:9050"])

    # Top sites only for faster scanning
    top_sites = (
        options.get("top_sites_only", config.TOP_SITES_ONLY)
        if options
        else config.TOP_SITES_ONLY
    )
    if top_sites:
        cmd.extend(["--top-sites", "50"])

    # Limit number of sites
    max_sites = (
        options.get("max_sites", config.MAX_SITES) if options else config.MAX_SITES
    )
    if max_sites and max_sites < 500:
        cmd.extend(["--top-sites", str(max_sites)])

    # No recursive search for basic scans
    cmd.append("--no-recursion")

    # Suppress color output and progress bar for cleaner parsing
    cmd.append("--no-color")
    cmd.append("--no-progressbar")

    return cmd


async def get_proxy_for_scan() -> tuple:
    """
    Get a proxy from the rotation service for scanning.

    Returns:
        Tuple of (proxy_url, proxy_id) or (None, None) if unavailable
    """
    if not proxy_client:
        return None, None

    try:
        proxy_info = await proxy_client.get_next_proxy()
        if proxy_info:
            return proxy_info.get_proxy_url(), proxy_info.id
    except Exception as e:
        logger.warning(f"Failed to get proxy from rotation service: {e}")

    return None, None


async def record_proxy_result(
    proxy_id: Optional[str], success: bool, latency_ms: int = 0, error: str = ""
):
    """Record the result of a proxy-enabled scan."""
    if not proxy_id or not proxy_client:
        return

    try:
        if success:
            await proxy_client.record_success(proxy_id, latency_ms)
        else:
            await proxy_client.record_failure(proxy_id, error[:200])
    except Exception as e:
        logger.debug(f"Failed to record proxy result: {e}")


def parse_maigret_json(json_path: Path) -> Dict[str, Any]:
    """Parse Maigret JSON output file."""
    try:
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data
    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse Maigret JSON: {e}")
        return {}
    except FileNotFoundError:
        logger.error(f"Maigret output file not found: {json_path}")
        return {}


def extract_accounts_from_result(
    raw_result: Dict[str, Any], username: str
) -> List[AccountInfo]:
    """Extract account information from Maigret JSON result."""
    accounts = []

    # Maigret JSON structure: { "SiteName": { "status": { "status": "Claimed" }, "url_user": "..." } }
    if isinstance(raw_result, dict):
        for site_name, site_data in raw_result.items():
            if isinstance(site_data, dict):
                # Get status from nested structure
                status_obj = site_data.get("status", {})
                if isinstance(status_obj, dict):
                    status = status_obj.get("status", "Unknown")
                    tags = status_obj.get("tags", [])
                else:
                    status = str(status_obj)
                    tags = []

                url = site_data.get("url_user", site_data.get("url", ""))

                # Only include claimed/found accounts
                if isinstance(status, str) and status.lower() in [
                    "claimed",
                    "found",
                    "detected",
                ]:
                    accounts.append(
                        AccountInfo(
                            site_name=site_name,
                            url=url or f"https://{site_name.lower()}.com/{username}",
                            username=username,
                            status="claimed",
                            tags=tags if isinstance(tags, list) else [],
                        )
                    )

    # Format 2: Array of results (fallback)
    elif isinstance(raw_result, list):
        for item in raw_result:
            if isinstance(item, dict):
                status_obj = item.get("status", {})
                if isinstance(status_obj, dict):
                    status = status_obj.get("status", "Unknown")
                else:
                    status = str(status_obj)

                if isinstance(status, str) and status.lower() in ["claimed", "found"]:
                    accounts.append(
                        AccountInfo(
                            site_name=item.get("site", item.get("name", "Unknown")),
                            url=item.get("url", ""),
                            username=username,
                            status="claimed",
                            tags=item.get("tags", []),
                        )
                    )

    return accounts


async def run_maigret_scan(
    scan_id: str, username: str, options: Optional[Dict], timeout: int
) -> ScanResult:
    """Execute Maigret scan as subprocess with proxy rotation support."""
    start_time = datetime.now()
    result = active_scans[scan_id]
    result.status = ScanStatus.RUNNING
    result.started_at = start_time.isoformat()

    # Create output directory for this scan
    scan_output_dir = config.REPORTS_DIR / f"scan_{scan_id}"
    scan_output_dir.mkdir(parents=True, exist_ok=True)

    # Get proxy from rotation service
    proxy_url, proxy_id = await get_proxy_for_scan()
    if proxy_id:
        logger.info(f"[{scan_id}] Using rotating proxy: {proxy_id}")

    try:
        # Sanitize username
        safe_username = sanitize_username(username)

        # Build command with optional proxy
        cmd = build_maigret_command(safe_username, scan_output_dir, options, proxy_url)
        logger.info(f"[{scan_id}] Running Maigret: {' '.join(cmd)}")

        # Run Maigret with timeout
        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
            cwd=str(scan_output_dir),
        )

        try:
            stdout, stderr = await asyncio.wait_for(
                process.communicate(), timeout=timeout
            )
        except asyncio.TimeoutError:
            process.kill()
            await process.wait()
            # Record proxy failure on timeout
            duration_ms = int((datetime.now() - start_time).total_seconds() * 1000)
            await record_proxy_result(proxy_id, False, duration_ms, "Scan timeout")
            result.status = ScanStatus.TIMEOUT
            result.error_message = f"Scan timed out after {timeout} seconds"
            result.completed_at = datetime.now().isoformat()
            return result

        # Log output for debugging
        stdout_text = stdout.decode("utf-8", errors="replace")
        stderr_text = stderr.decode("utf-8", errors="replace")
        if stdout_text:
            logger.info(f"[{scan_id}] Maigret stdout: {stdout_text[:500]}")
        if stderr_text:
            logger.warning(f"[{scan_id}] Maigret stderr: {stderr_text[:500]}")

        # Check for errors (but Maigret may exit non-zero even on partial success)
        if process.returncode != 0 and not any(scan_output_dir.glob("*.json")):
            logger.error(f"[{scan_id}] Maigret failed with no output: {stderr_text}")
            duration_ms = int((datetime.now() - start_time).total_seconds() * 1000)
            await record_proxy_result(proxy_id, False, duration_ms, stderr_text[:100])
            result.status = ScanStatus.FAILED
            result.error_message = (
                f"Maigret exited with code {process.returncode}: {stderr_text[:500]}"
            )
            result.completed_at = datetime.now().isoformat()
            return result

        # Find JSON output file - Maigret creates files like report_<username>_simple.json
        json_files = list(scan_output_dir.glob("*.json"))

        if json_files:
            output_path = json_files[0]  # Take the first JSON file
            logger.info(f"[{scan_id}] Found output file: {output_path}")

            raw_result = parse_maigret_json(output_path)
            accounts = extract_accounts_from_result(raw_result, safe_username)

            # Calculate summary
            end_time = datetime.now()
            duration_ms = int((end_time - start_time).total_seconds() * 1000)

            # Record proxy success
            await record_proxy_result(proxy_id, True, duration_ms)

            # Count stats from raw result
            total_checked = len(raw_result) if isinstance(raw_result, dict) else 0
            claimed = len(accounts)

            result.summary = ScanSummary(
                total_sites_checked=total_checked,
                accounts_found=claimed,
                accounts_claimed=claimed,
                accounts_available=0,
                accounts_error=0,
                scan_duration_ms=duration_ms,
            )
            result.accounts = accounts
            result.raw_json_path = str(output_path)
            result.status = ScanStatus.COMPLETED
            result.completed_at = end_time.isoformat()

            logger.info(
                f"[{scan_id}] Scan completed: {claimed} accounts found in {duration_ms}ms"
            )
        else:
            duration_ms = int((datetime.now() - start_time).total_seconds() * 1000)
            await record_proxy_result(proxy_id, False, duration_ms, "No output file")
            result.status = ScanStatus.FAILED
            result.error_message = "Maigret did not produce output file"
            result.completed_at = datetime.now().isoformat()

    except ValueError as e:
        duration_ms = int((datetime.now() - start_time).total_seconds() * 1000)
        await record_proxy_result(proxy_id, False, duration_ms, str(e))
        result.status = ScanStatus.FAILED
        result.error_message = f"Invalid input: {str(e)}"
        result.completed_at = datetime.now().isoformat()
    except Exception as e:
        logger.exception(f"[{scan_id}] Unexpected error during scan")
        duration_ms = int((datetime.now() - start_time).total_seconds() * 1000)
        await record_proxy_result(proxy_id, False, duration_ms, str(e)[:100])
        result.status = ScanStatus.FAILED
        result.error_message = f"Unexpected error: {str(e)}"
        result.completed_at = datetime.now().isoformat()

    return result


async def execute_scan_with_semaphore(
    scan_id: str, username: str, options: Optional[Dict], timeout: int
):
    """Execute scan with concurrency control."""
    async with scan_semaphore:
        await run_maigret_scan(scan_id, username, options, timeout)


# --- Lifespan Management ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan handler."""
    logger.info("Maigret Worker service starting up...")
    logger.info(f"Max concurrent scans: {config.MAX_CONCURRENT_SCANS}")
    logger.info(f"Default timeout: {config.SCAN_TIMEOUT_SEC}s")
    logger.info(f"Reports directory: {config.REPORTS_DIR}")
    logger.info(f"Proxy rotation enabled: {USE_PROXY_ROTATION}")

    # Verify Maigret is installed
    try:
        result = subprocess.run(
            ["maigret", "--version"], capture_output=True, text=True, timeout=10
        )
        logger.info(f"Maigret version: {result.stdout.strip()}")
    except Exception as e:
        logger.warning(f"Could not verify Maigret installation: {e}")

    yield

    # Cleanup proxy client on shutdown
    if proxy_client:
        await proxy_client.close()
    logger.info("Maigret Worker service shutting down...")


# --- FastAPI App ---
app = FastAPI(
    title="Maigret OSINT Username Scanner",
    description="Performs username OSINT scanning using Maigret for social media account discovery",
    version="1.0.0",
    lifespan=lifespan,
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# --- API Endpoints ---
@app.get("/health", response_model=HealthResponse)
@app.head("/health")
async def health_check():
    """Health check endpoint."""
    # Check if Maigret is available
    maigret_available = False
    try:
        result = subprocess.run(
            ["maigret", "--version"], capture_output=True, timeout=5
        )
        maigret_available = result.returncode == 0
    except Exception:
        pass

    # Count active scans
    active_count = sum(
        1
        for s in active_scans.values()
        if s.status in [ScanStatus.PENDING, ScanStatus.RUNNING]
    )

    # Check proxy service health
    proxy_healthy = False
    if proxy_client:
        proxy_healthy = await proxy_client.health_check()

    return HealthResponse(
        status="ok" if maigret_available else "degraded",
        version="1.1.0",
        maigret_available=maigret_available,
        active_scans=active_count,
        max_concurrent_scans=config.MAX_CONCURRENT_SCANS,
        proxy_rotation_enabled=USE_PROXY_ROTATION,
        proxy_service_healthy=proxy_healthy,
    )


@app.post("/scan", response_model=ScanResponse)
async def start_scan(
    request: ScanRequest, background_tasks: BackgroundTasks, req: Request
):
    """
    Start a username OSINT scan using Maigret.

    The scan runs asynchronously. Use GET /scan/{scan_id} to check status and results.
    """
    trace_id = req.headers.get("X-Trace-Id", req.headers.get("X-Request-Id", "unknown"))

    # Generate scan ID
    scan_id = str(uuid.uuid4())

    logger.info(
        f"[{trace_id}] Starting scan {scan_id} for username: {request.username}"
    )

    # Validate username early
    try:
        sanitize_username(request.username)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))

    # Determine timeout
    timeout = request.timeout_sec or config.SCAN_TIMEOUT_SEC

    # Build options
    options = request.options or {}
    if request.top_sites_only is not None:
        options["top_sites_only"] = request.top_sites_only

    # Create initial scan result
    result = ScanResult(
        scan_id=scan_id, username=request.username, status=ScanStatus.PENDING
    )
    active_scans[scan_id] = result

    # Start scan in background
    background_tasks.add_task(
        execute_scan_with_semaphore, scan_id, request.username, options, timeout
    )

    return ScanResponse(
        status="ok",
        scan_id=scan_id,
        message=f"Scan started for username '{request.username}'. Use GET /scan/{scan_id} to check status.",
        result=result,
    )


@app.post("/scan/sync", response_model=ScanResponse)
async def run_scan_sync(request: ScanRequest, req: Request):
    """
    Run a username scan synchronously and wait for results.

    This endpoint blocks until the scan completes or times out.
    Recommended for shorter scans (use top_sites_only=true for faster results).
    """
    trace_id = req.headers.get("X-Trace-Id", req.headers.get("X-Request-Id", "unknown"))

    scan_id = str(uuid.uuid4())
    logger.info(
        f"[{trace_id}] Starting sync scan {scan_id} for username: {request.username}"
    )

    # Validate username
    try:
        sanitize_username(request.username)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))

    # Determine timeout
    timeout = request.timeout_sec or config.SCAN_TIMEOUT_SEC

    # Build options - default to top_sites for sync
    options = request.options or {}
    if request.top_sites_only is not None:
        options["top_sites_only"] = request.top_sites_only
    else:
        options["top_sites_only"] = True  # Default to top sites for sync requests

    # Create scan result
    result = ScanResult(
        scan_id=scan_id, username=request.username, status=ScanStatus.PENDING
    )
    active_scans[scan_id] = result

    # Run scan with semaphore
    async with scan_semaphore:
        result = await run_maigret_scan(scan_id, request.username, options, timeout)

    if result.status == ScanStatus.COMPLETED:
        return ScanResponse(
            status="ok",
            scan_id=scan_id,
            message=f"Scan completed. Found {len(result.accounts)} accounts.",
            result=result,
        )
    else:
        return ScanResponse(
            status="error",
            scan_id=scan_id,
            message=result.error_message or "Scan failed",
            result=result,
        )


@app.get("/scan/{scan_id}", response_model=ScanResponse)
async def get_scan_status(scan_id: str):
    """Get the status and results of a scan."""
    result = active_scans.get(scan_id)

    if not result:
        raise HTTPException(status_code=404, detail=f"Scan not found: {scan_id}")

    return ScanResponse(
        status="ok",
        scan_id=scan_id,
        message=f"Scan status: {result.status.value}",
        result=result,
    )


@app.get("/scans", response_model=Dict[str, Any])
async def list_scans(status: Optional[ScanStatus] = None, limit: int = 50):
    """List recent scans, optionally filtered by status."""
    scans = list(active_scans.values())

    if status:
        scans = [s for s in scans if s.status == status]

    # Sort by started_at descending
    scans.sort(key=lambda x: x.started_at or "", reverse=True)

    return {"total": len(scans), "scans": scans[:limit]}


@app.delete("/scan/{scan_id}")
async def delete_scan(scan_id: str):
    """Delete a completed scan and its report file."""
    result = active_scans.get(scan_id)

    if not result:
        raise HTTPException(status_code=404, detail=f"Scan not found: {scan_id}")

    if result.status in [ScanStatus.PENDING, ScanStatus.RUNNING]:
        raise HTTPException(status_code=400, detail="Cannot delete a running scan")

    # Delete report file if exists
    if result.raw_json_path:
        try:
            Path(result.raw_json_path).unlink(missing_ok=True)
        except Exception as e:
            logger.warning(f"Failed to delete report file: {e}")

    del active_scans[scan_id]

    return {"status": "ok", "message": f"Scan {scan_id} deleted"}


@app.get("/report/{scan_id}")
async def get_raw_report(scan_id: str):
    """Get the raw JSON report for a completed scan."""
    result = active_scans.get(scan_id)

    if not result:
        raise HTTPException(status_code=404, detail=f"Scan not found: {scan_id}")

    if result.status != ScanStatus.COMPLETED:
        raise HTTPException(
            status_code=400, detail=f"Scan not completed: {result.status}"
        )

    if not result.raw_json_path or not Path(result.raw_json_path).exists():
        raise HTTPException(status_code=404, detail="Report file not found")

    with open(result.raw_json_path, "r", encoding="utf-8") as f:
        return json.load(f)


@app.get("/proxy/stats")
async def get_proxy_stats():
    """Get proxy pool statistics."""
    if not proxy_client:
        return {"error": "Proxy rotation not enabled", "enabled": False}

    stats = await proxy_client.get_pool_stats()
    return stats or {"error": "Failed to get stats", "enabled": True}


# --- Main Entry Point ---
if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=config.PORT,
        reload=os.getenv("ENV", "production") == "development",
    )

```

---

## backend/data-collection-service/maigret-worker/state_store.py

```py
"""
State Storage Module for maigret-worker Service

Provides persistent storage for OSINT scan results using Redis.
Falls back to in-memory storage if Redis is unavailable.
"""

import os
import json
import asyncio
from datetime import datetime
from typing import Optional, Dict, Any, List
from dataclasses import asdict
from enum import Enum

# Redis configuration from environment
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/3")
REDIS_PREFIX = os.getenv("REDIS_PREFIX", "maigret_worker")
SCAN_TTL_DAYS = int(os.getenv("SCAN_TTL_DAYS", "7"))  # Scans expire after 7 days


class EnhancedJSONEncoder(json.JSONEncoder):
    """JSON encoder that handles dataclasses and enums."""
    
    def default(self, o: Any) -> Any:
        if hasattr(o, '__dataclass_fields__'):
            return asdict(o)
        if isinstance(o, Enum):
            return o.value
        if isinstance(o, datetime):
            return o.isoformat()
        return super().default(o)


class StateStore:
    """Persistent state storage with Redis backend."""
    
    def __init__(self):
        self._redis = None
        self._memory_store: Dict[str, Any] = {}
        self._using_redis = False
        self._lock = asyncio.Lock()
    
    async def connect(self) -> bool:
        """Connect to Redis."""
        try:
            import redis.asyncio as redis
            
            self._redis = redis.from_url(
                REDIS_URL,
                encoding="utf-8",
                decode_responses=True,
                socket_connect_timeout=5,
                socket_timeout=5,
            )
            
            await self._redis.ping()
            self._using_redis = True
            
            # Load existing scans from Redis
            await self._load_existing_scans()
            
            return True
            
        except ImportError:
            self._using_redis = False
            return False
        except Exception:
            self._using_redis = False
            return False
    
    async def _load_existing_scans(self):
        """Load existing scans from Redis into memory on startup."""
        if not self._using_redis or not self._redis:
            return
        
        try:
            pattern = f"{REDIS_PREFIX}:scan:*"
            cursor = 0
            
            while True:
                cursor, keys = await self._redis.scan(cursor, match=pattern, count=100)
                for key in keys:
                    scan_id = key.split(":")[-1]
                    scan_data = await self._redis.get(key)
                    if scan_data:
                        self._memory_store[scan_id] = json.loads(scan_data)
                
                if cursor == 0:
                    break
        except Exception:
            pass
    
    async def disconnect(self):
        """Close Redis connection."""
        if self._redis:
            await self._redis.close()
            self._redis = None
            self._using_redis = False
    
    def _key(self, scan_id: str) -> str:
        """Generate Redis key."""
        return f"{REDIS_PREFIX}:scan:{scan_id}"
    
    async def save_scan(self, scan_id: str, scan_result: Any) -> bool:
        """Save scan result."""
        async with self._lock:
            try:
                # Convert to dict if needed
                if hasattr(scan_result, '__dataclass_fields__'):
                    data = asdict(scan_result)
                elif hasattr(scan_result, 'model_dump'):
                    data = scan_result.model_dump()
                elif hasattr(scan_result, 'dict'):
                    data = scan_result.dict()
                elif isinstance(scan_result, dict):
                    data = scan_result
                else:
                    data = scan_result
                
                data_json = json.dumps(data, cls=EnhancedJSONEncoder)
                self._memory_store[scan_id] = json.loads(data_json)
                
                if self._using_redis and self._redis:
                    ttl_seconds = SCAN_TTL_DAYS * 24 * 60 * 60
                    await self._redis.set(self._key(scan_id), data_json, ex=ttl_seconds)
                
                return True
            except Exception:
                return False
    
    async def load_scan(self, scan_id: str) -> Optional[Dict[str, Any]]:
        """Load scan result."""
        if scan_id in self._memory_store:
            return self._memory_store[scan_id]
        
        if self._using_redis and self._redis:
            try:
                data_json = await self._redis.get(self._key(scan_id))
                if data_json:
                    data = json.loads(data_json)
                    self._memory_store[scan_id] = data
                    return data
            except Exception:
                pass
        
        return None
    
    async def delete_scan(self, scan_id: str) -> bool:
        """Delete scan result."""
        async with self._lock:
            self._memory_store.pop(scan_id, None)
            if self._using_redis and self._redis:
                try:
                    await self._redis.delete(self._key(scan_id))
                except Exception:
                    pass
            return True
    
    async def list_scans(self, status: Optional[str] = None, limit: int = 50) -> List[Dict[str, Any]]:
        """List scans with optional filtering."""
        scans = []
        for scan_id, scan_data in list(self._memory_store.items())[-limit:]:
            if status and scan_data.get('status', '').lower() != status.lower():
                continue
            scans.append(scan_data)
        return scans
    
    @property
    def is_redis_connected(self) -> bool:
        return self._using_redis
    
    def get_memory_store(self) -> Dict[str, Any]:
        return self._memory_store


# Singleton instance
_store: Optional[StateStore] = None


async def get_state_store() -> StateStore:
    """Get or create the singleton StateStore instance."""
    global _store
    if _store is None:
        _store = StateStore()
        await _store.connect()
    return _store

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/CollectorApplication.java

```java
package com.newsinsight.collector;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.client.discovery.EnableDiscoveryClient;
import org.springframework.scheduling.annotation.EnableAsync;
import org.springframework.scheduling.annotation.EnableScheduling;

/**
 * NewsInsight Collector Service Application
 * 
 * Spring Boot ê¸°ë°˜ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ ì„œë¹„ìŠ¤
 * - ë‹¤ì–‘í•œ ì†ŒìŠ¤(RSS, Web Scraping, API)ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘
 * - ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ í†µí•œ íš¨ìœ¨ì ì¸ ìˆ˜ì§‘
 * - Consulì„ í†µí•œ ì„œë¹„ìŠ¤ ë””ìŠ¤ì»¤ë²„ë¦¬ ë° ì„¤ì • ê´€ë¦¬
 * - PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ë¥¼ í†µí•œ ë°ì´í„° ì €ì¥
 */
@SpringBootApplication
@EnableDiscoveryClient
@EnableAsync
@EnableScheduling
public class CollectorApplication {

    public static void main(String[] args) {
        SpringApplication.run(CollectorApplication.class, args);
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/client/AIDoveClient.java

```java
package com.newsinsight.collector.client;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import io.netty.channel.ChannelOption;
import io.netty.handler.timeout.ReadTimeoutHandler;
import io.netty.handler.timeout.WriteTimeoutHandler;
import jakarta.annotation.PostConstruct;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.MediaType;
import org.springframework.http.client.reactive.ReactorClientHttpConnector;
import org.springframework.stereotype.Component;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Mono;
import reactor.netty.http.client.HttpClient;

import java.time.Duration;
import java.util.Map;
import java.util.concurrent.TimeUnit;

/**
 * Client for AI Dove Agent API.
 * Provides AI-powered text analysis using the self-healing AI service.
 * 
 * API Endpoint: Configurable via COLLECTOR_AIDOVE_BASE_URL or Consul KV
 * 
 * Request:
 *   - chatInput: string (required) - The message/prompt
 *   - sessionId: string (optional) - Session ID for context continuity
 * 
 * Response:
 *   - reply: string - AI response
 *   - tokens_used: integer - Tokens consumed
 *   - model: string - Model used for generation
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class AIDoveClient {

    private final ObjectMapper objectMapper;

    @Value("${collector.ai-dove.base-url:${collector.aidove.base-url:${COLLECTOR_AIDOVE_BASE_URL:https://workflow.nodove.com/webhook/aidove}}}")
    private String baseUrl;

    @Value("${collector.ai-dove.timeout-seconds:${collector.aidove.timeout-seconds:180}}")
    private int timeoutSeconds;

    @Value("${collector.ai-dove.enabled:${collector.aidove.enabled:true}}")
    private boolean enabled;

    private WebClient aiDoveWebClient;

    @PostConstruct
    public void init() {
        // Create dedicated WebClient with extended timeout for AI operations
        HttpClient httpClient = HttpClient.create()
                .option(ChannelOption.CONNECT_TIMEOUT_MILLIS, 30000) // 30s connect timeout
                .responseTimeout(Duration.ofSeconds(timeoutSeconds))
                .doOnConnected(conn -> 
                    conn.addHandlerLast(new ReadTimeoutHandler(timeoutSeconds, TimeUnit.SECONDS))
                        .addHandlerLast(new WriteTimeoutHandler(timeoutSeconds, TimeUnit.SECONDS))
                )
                .followRedirect(true);

        this.aiDoveWebClient = WebClient.builder()
                .clientConnector(new ReactorClientHttpConnector(httpClient))
                .defaultHeader("User-Agent", "NewsInsight-AIDove/1.0")
                .build();
        
        log.info("AIDoveClient initialized with timeout: {}s, baseUrl: {}", timeoutSeconds, baseUrl);
    }

    /**
     * Check if AI Dove client is enabled
     */
    public boolean isEnabled() {
        return enabled;
    }

    /**
     * Send a prompt to AI Dove and get a response.
     * 
     * @param prompt The prompt to send
     * @param sessionId Optional session ID for context continuity
     * @return The AI response
     */
    public Mono<AIDoveResponse> chat(String prompt, String sessionId) {
        if (!enabled) {
            return Mono.error(new IllegalStateException("AI Dove client is disabled"));
        }

        Map<String, Object> payload = sessionId != null
                ? Map.of("chatInput", prompt, "sessionId", sessionId)
                : Map.of("chatInput", prompt);

        return aiDoveWebClient.post()
                .uri(baseUrl)
                .contentType(MediaType.APPLICATION_JSON)
                .bodyValue(payload)
                .retrieve()
                .bodyToMono(String.class)
                .timeout(Duration.ofSeconds(timeoutSeconds))
                .map(this::parseResponse)
                .doOnError(e -> log.error("AI Dove request failed: {}", e.getMessage()));
    }

    /**
     * Stream a response from AI Dove (simulated streaming by splitting response).
     * Note: AI Dove API doesn't support true streaming, so we simulate it.
     */
    public Flux<String> chatStream(String prompt, String sessionId) {
        return chat(prompt, sessionId)
                .flatMapMany(response -> {
                    if (response.reply() == null) {
                        return Flux.empty();
                    }
                    // Split response into chunks for simulated streaming
                    String[] sentences = response.reply().split("(?<=[.!?\\n])\\s*");
                    return Flux.fromArray(sentences)
                            .delayElements(Duration.ofMillis(50));
                })
                .onErrorResume(e -> {
                    log.error("AI Dove stream failed: {}", e.getMessage());
                    return Flux.just("AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: " + e.getMessage());
                });
    }

    private AIDoveResponse parseResponse(String json) {
        try {
            JsonNode node = objectMapper.readTree(json);
            return new AIDoveResponse(
                    node.has("reply") ? node.get("reply").asText() : null,
                    node.has("tokens_used") ? node.get("tokens_used").asInt() : 0,
                    node.has("model") ? node.get("model").asText() : "unknown"
            );
        } catch (Exception e) {
            log.error("Failed to parse AI Dove response: {}", e.getMessage());
            return new AIDoveResponse(json, 0, "unknown");
        }
    }

    /**
     * AI Dove API response
     */
    public record AIDoveResponse(
            String reply,
            int tokensUsed,
            String model
    ) {}
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/client/Crawl4aiClient.java

```java
package com.newsinsight.collector.client;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.jsoup.Jsoup;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.MediaType;
import org.springframework.stereotype.Component;
import org.springframework.web.reactive.function.client.ClientResponse;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Mono;

import java.net.URI;
import java.time.Duration;

/**
 * Lightweight client for the Crawl4AI service.
 * Tries to call /crawl at the configured base URL and extract text content.
 * If the API responds with JSON, attempts to read common fields like
 * "content", "markdown", "text", or "html". Falls back to plain text.
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class Crawl4aiClient {

    private final WebClient webClient;
    private final ObjectMapper objectMapper;

    @Value("${collector.crawler.base-url:http://web-crawler:11235}")
    private String baseUrl;

    @Value("${collector.http.timeout.read:30000}")
    private int readTimeoutMs;

    @Data
    @Builder
    @AllArgsConstructor
    public static class CrawlResult {
        private String title;
        private String content; // normalized text content
    }

    /**
     * Attempts to crawl the given URL via Crawl4AI. Returns null on failure.
     */
    public CrawlResult crawl(String targetUrl) {
        try {
            String endpoint = baseUrl.endsWith("/") ? baseUrl + "crawl" : baseUrl + "/crawl";

            Mono<CrawlResult> mono = webClient
                    .get()
                    .uri(uriBuilder -> {
                        URI uri = URI.create(endpoint);
                        return uriBuilder
                                .scheme(uri.getScheme())
                                .host(uri.getHost())
                                .port(uri.getPort())
                                .path(uri.getPath())
                                .queryParam("url", targetUrl)
                                .build();
                    })
                    .accept(MediaType.APPLICATION_JSON, MediaType.TEXT_PLAIN, MediaType.ALL)
                    .exchangeToMono(response -> handleResponse(response))
                    .timeout(Duration.ofMillis(Math.max(1000, readTimeoutMs)));

            return mono.onErrorResume(e -> {
                        log.warn("Crawl4AI request failed for {}: {}", targetUrl, e.toString());
                        return Mono.empty();
                    })
                    .block();
        } catch (Exception e) {
            log.warn("Crawl4AI client error for {}: {}", targetUrl, e.toString());
            return null;
        }
    }

    private Mono<CrawlResult> handleResponse(ClientResponse response) {
        MediaType ct = response.headers().contentType().orElse(MediaType.APPLICATION_JSON);
        if (ct.isCompatibleWith(MediaType.APPLICATION_JSON) || ct.getSubtype().contains("json")) {
            return response.bodyToMono(String.class).flatMap(body -> {
                try {
                    JsonNode node = objectMapper.readTree(body);
                    String title = textOf(node, "title");
                    String content = firstNonBlank(
                            textOf(node, "content"),
                            textOf(node, "markdown"),
                            textOf(node, "text"),
                            stripHtml(textOf(node, "html"))
                    );
                    if (isBlank(content)) return Mono.empty();
                    return Mono.just(CrawlResult.builder()
                            .title(title)
                            .content(normalize(content))
                            .build());
                } catch (Exception ex) {
                    log.debug("Failed to parse JSON from Crawl4AI: {}", ex.toString());
                    return Mono.empty();
                }
            });
        } else {
            return response.bodyToMono(String.class).map(body ->
                    CrawlResult.builder()
                            .title(null)
                            .content(normalize(stripHtml(body)))
                            .build()
            );
        }
    }

    private static boolean isBlank(String s) {
        return s == null || s.trim().isEmpty();
    }

    private static String firstNonBlank(String... values) {
        if (values == null) return null;
        for (String v : values) {
            if (!isBlank(v)) return v;
        }
        return null;
    }

    private static String normalize(String s) {
        if (s == null) return null;
        return s.replaceAll("\\s+", " ").trim();
    }

    private static String textOf(JsonNode node, String field) {
        if (node == null || node.isNull()) return null;
        JsonNode v = node.get(field);
        if (v == null || v.isNull()) return null;
        if (v.isTextual()) return v.asText();
        return v.toString();
    }

    private static String stripHtml(String html) {
        if (html == null) return null;
        try {
            return Jsoup.parse(html).text();
        } catch (Exception ignored) {
            return html;
        }
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/client/OpenAICompatibleClient.java

```java
package com.newsinsight.collector.client;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import io.netty.channel.ChannelOption;
import io.netty.handler.timeout.ReadTimeoutHandler;
import io.netty.handler.timeout.WriteTimeoutHandler;
import jakarta.annotation.PostConstruct;
import lombok.Getter;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.HttpHeaders;
import org.springframework.http.MediaType;
import org.springframework.http.client.reactive.ReactorClientHttpConnector;
import org.springframework.stereotype.Component;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Flux;
import reactor.netty.http.client.HttpClient;

import java.time.Duration;
import java.util.List;
import java.util.Map;
import java.util.concurrent.TimeUnit;

/**
 * OpenAI-compatible client that can connect to various LLM providers.
 * Supports: OpenAI, OpenRouter, Ollama, Azure OpenAI, and custom endpoints.
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class OpenAICompatibleClient {

    private final ObjectMapper objectMapper;
    private WebClient webClient;

    // OpenAI settings
    @Value("${LLM_OPENAI_API_KEY:${OPENAI_API_KEY:}}")
    private String openaiApiKey;

    @Value("${LLM_OPENAI_BASE_URL:https://api.openai.com/v1}")
    private String openaiBaseUrl;

    @Value("${LLM_OPENAI_MODEL:gpt-4o-mini}")
    private String openaiModel;

    // OpenRouter settings
    @Value("${LLM_OPENROUTER_API_KEY:${OPENROUTER_API_KEY:}}")
    private String openrouterApiKey;

    @Value("${LLM_OPENROUTER_BASE_URL:https://openrouter.ai/api/v1}")
    private String openrouterBaseUrl;

    @Value("${LLM_OPENROUTER_MODEL:anthropic/claude-3.5-sonnet}")
    private String openrouterModel;

    // Ollama settings
    @Value("${LLM_OLLAMA_BASE_URL:http://localhost:11434/v1}")
    private String ollamaBaseUrl;

    @Value("${LLM_OLLAMA_MODEL:llama3.2}")
    private String ollamaModel;

    // Azure OpenAI settings
    @Value("${LLM_AZURE_API_KEY:${AZURE_OPENAI_API_KEY:}}")
    private String azureApiKey;

    @Value("${LLM_AZURE_ENDPOINT:}")
    private String azureEndpoint;

    @Value("${LLM_AZURE_DEPLOYMENT:gpt-4o}")
    private String azureDeployment;

    @Value("${LLM_AZURE_API_VERSION:2024-02-15-preview}")
    private String azureApiVersion;

    // Custom endpoint settings
    @Value("${LLM_CUSTOM_BASE_URL:}")
    private String customBaseUrl;

    @Value("${LLM_CUSTOM_API_KEY:}")
    private String customApiKey;

    @Value("${LLM_CUSTOM_MODEL:}")
    private String customModel;

    @Value("${collector.openai.timeout-seconds:120}")
    private int timeoutSeconds;

    @Getter
    private ProviderStatus providerStatus;

    @PostConstruct
    public void init() {
        HttpClient httpClient = HttpClient.create()
                .option(ChannelOption.CONNECT_TIMEOUT_MILLIS, 30000)
                .responseTimeout(Duration.ofSeconds(timeoutSeconds))
                .doOnConnected(conn ->
                        conn.addHandlerLast(new ReadTimeoutHandler(timeoutSeconds, TimeUnit.SECONDS))
                            .addHandlerLast(new WriteTimeoutHandler(timeoutSeconds, TimeUnit.SECONDS))
                )
                .followRedirect(true);

        this.webClient = WebClient.builder()
                .clientConnector(new ReactorClientHttpConnector(httpClient))
                .defaultHeader("User-Agent", "NewsInsight-OpenAI/1.0")
                .codecs(configurer -> configurer.defaultCodecs().maxInMemorySize(10 * 1024 * 1024))
                .build();

        this.providerStatus = checkProviderStatus();
        log.info("OpenAICompatibleClient initialized - Available providers: {}", providerStatus);
    }

    /**
     * Check which providers are available
     */
    public ProviderStatus checkProviderStatus() {
        return new ProviderStatus(
                isNotBlank(openaiApiKey),
                isNotBlank(openrouterApiKey),
                true, // Ollama is always potentially available (local)
                isNotBlank(azureApiKey) && isNotBlank(azureEndpoint),
                isNotBlank(customBaseUrl)
        );
    }

    /**
     * Check if any OpenAI-compatible provider is enabled
     */
    public boolean isEnabled() {
        return providerStatus.openai() || providerStatus.openrouter() 
                || providerStatus.ollama() || providerStatus.azure() 
                || providerStatus.custom();
    }

    /**
     * Check if OpenAI is enabled
     */
    public boolean isOpenAIEnabled() {
        return isNotBlank(openaiApiKey);
    }

    /**
     * Check if OpenRouter is enabled
     */
    public boolean isOpenRouterEnabled() {
        return isNotBlank(openrouterApiKey);
    }

    /**
     * Check if Ollama is enabled (always returns true as it's local)
     */
    public boolean isOllamaEnabled() {
        return true;
    }

    /**
     * Check if Azure OpenAI is enabled
     */
    public boolean isAzureEnabled() {
        return isNotBlank(azureApiKey) && isNotBlank(azureEndpoint);
    }

    /**
     * Check if Custom endpoint is enabled
     */
    public boolean isCustomEnabled() {
        return isNotBlank(customBaseUrl);
    }

    /**
     * Stream completion from OpenAI
     */
    public Flux<String> streamFromOpenAI(String prompt) {
        if (!isOpenAIEnabled()) {
            return Flux.error(new IllegalStateException("OpenAI API key is not configured"));
        }
        return streamCompletion(openaiBaseUrl, openaiApiKey, openaiModel, prompt, "OpenAI");
    }

    /**
     * Stream completion from OpenRouter
     */
    public Flux<String> streamFromOpenRouter(String prompt) {
        if (!isOpenRouterEnabled()) {
            return Flux.error(new IllegalStateException("OpenRouter API key is not configured"));
        }
        return streamCompletion(openrouterBaseUrl, openrouterApiKey, openrouterModel, prompt, "OpenRouter");
    }

    /**
     * Stream completion from Ollama
     */
    public Flux<String> streamFromOllama(String prompt) {
        return streamCompletion(ollamaBaseUrl, null, ollamaModel, prompt, "Ollama");
    }

    /**
     * Stream completion from Azure OpenAI
     */
    public Flux<String> streamFromAzure(String prompt) {
        if (!isAzureEnabled()) {
            return Flux.error(new IllegalStateException("Azure OpenAI is not configured"));
        }
        String url = String.format("%s/openai/deployments/%s/chat/completions?api-version=%s",
                azureEndpoint, azureDeployment, azureApiVersion);
        return streamCompletionAzure(url, azureApiKey, prompt);
    }

    /**
     * Stream completion from Custom endpoint
     */
    public Flux<String> streamFromCustom(String prompt) {
        if (!isCustomEnabled()) {
            return Flux.error(new IllegalStateException("Custom endpoint is not configured"));
        }
        return streamCompletion(customBaseUrl, customApiKey, customModel, prompt, "Custom");
    }

    /**
     * Generic OpenAI-compatible streaming completion
     */
    private Flux<String> streamCompletion(String baseUrl, String apiKey, String model, String prompt, String providerName) {
        String url = baseUrl.endsWith("/") ? baseUrl + "chat/completions" : baseUrl + "/chat/completions";

        // System message to guide the AI to respond directly in report format
        String systemMessage = """
                ë‹¹ì‹ ì€ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìš”ì²­ì— ëŒ€í•´ ì§ì ‘ ë³´ê³ ì„œ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
                "ì•Œê² ìŠµë‹ˆë‹¤", "ë„¤", "ê²€ìƒ‰í•˜ê² ìŠµë‹ˆë‹¤" ë“±ì˜ ì„œë‘ ì—†ì´ ë°”ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
                ìš”ì²­ë°›ì€ í˜•ì‹(ë§ˆí¬ë‹¤ìš´ ë“±)ì„ ì •í™•íˆ ë”°ë¥´ì„¸ìš”.
                """;

        Map<String, Object> body = Map.of(
                "model", model,
                "stream", true,
                "messages", List.of(
                        Map.of("role", "system", "content", systemMessage),
                        Map.of("role", "user", "content", prompt)
                )
        );

        log.debug("Calling {} API: {} with model {}", providerName, url, model);

        WebClient.RequestBodySpec request = webClient.post()
                .uri(url)
                .contentType(MediaType.APPLICATION_JSON)
                .accept(MediaType.TEXT_EVENT_STREAM);

        if (apiKey != null && !apiKey.isBlank()) {
            request = request.header(HttpHeaders.AUTHORIZATION, "Bearer " + apiKey);
        }

        return request
                .bodyValue(body)
                .retrieve()
                .bodyToFlux(String.class)
                .timeout(Duration.ofSeconds(timeoutSeconds))
                .doOnSubscribe(s -> log.debug("Starting {} stream request", providerName))
                .filter(chunk -> chunk != null && !chunk.isBlank() && !chunk.equals("[DONE]"))
                .mapNotNull(this::extractContent)
                .doOnComplete(() -> log.debug("{} stream completed", providerName))
                .doOnError(e -> log.error("{} stream failed: {}", providerName, e.getMessage()));
    }

    /**
     * Azure-specific streaming (uses api-key header instead of Authorization)
     */
    private Flux<String> streamCompletionAzure(String url, String apiKey, String prompt) {
        // System message to guide the AI to respond directly in report format
        String systemMessage = """
                ë‹¹ì‹ ì€ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìš”ì²­ì— ëŒ€í•´ ì§ì ‘ ë³´ê³ ì„œ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
                "ì•Œê² ìŠµë‹ˆë‹¤", "ë„¤", "ê²€ìƒ‰í•˜ê² ìŠµë‹ˆë‹¤" ë“±ì˜ ì„œë‘ ì—†ì´ ë°”ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
                ìš”ì²­ë°›ì€ í˜•ì‹(ë§ˆí¬ë‹¤ìš´ ë“±)ì„ ì •í™•íˆ ë”°ë¥´ì„¸ìš”.
                """;

        Map<String, Object> body = Map.of(
                "stream", true,
                "messages", List.of(
                        Map.of("role", "system", "content", systemMessage),
                        Map.of("role", "user", "content", prompt)
                )
        );

        log.debug("Calling Azure OpenAI API: {}", url);

        return webClient.post()
                .uri(url)
                .header("api-key", apiKey)
                .contentType(MediaType.APPLICATION_JSON)
                .accept(MediaType.TEXT_EVENT_STREAM)
                .bodyValue(body)
                .retrieve()
                .bodyToFlux(String.class)
                .timeout(Duration.ofSeconds(timeoutSeconds))
                .doOnSubscribe(s -> log.debug("Starting Azure stream request"))
                .filter(chunk -> chunk != null && !chunk.isBlank() && !chunk.equals("[DONE]"))
                .mapNotNull(this::extractContent)
                .doOnComplete(() -> log.debug("Azure stream completed"))
                .doOnError(e -> log.error("Azure stream failed: {}", e.getMessage()));
    }

    /**
     * Extract content from SSE chunk
     */
    private String extractContent(String chunk) {
        try {
            // Handle SSE format: data: {...}
            String json = chunk.startsWith("data:") ? chunk.substring(5).trim() : chunk;
            if (json.isBlank() || json.equals("[DONE]")) {
                return null;
            }

            JsonNode node = objectMapper.readTree(json);
            JsonNode choices = node.get("choices");
            if (choices != null && choices.isArray() && !choices.isEmpty()) {
                JsonNode delta = choices.get(0).get("delta");
                if (delta != null && delta.has("content")) {
                    return delta.get("content").asText();
                }
            }
            return null;
        } catch (Exception e) {
            log.trace("Failed to parse chunk: {}", chunk);
            return null;
        }
    }

    private boolean isNotBlank(String str) {
        return str != null && !str.isBlank();
    }

    /**
     * Provider availability status
     */
    public record ProviderStatus(
            boolean openai,
            boolean openrouter,
            boolean ollama,
            boolean azure,
            boolean custom
    ) {
        @Override
        public String toString() {
            StringBuilder sb = new StringBuilder("[");
            if (openai) sb.append("OpenAI, ");
            if (openrouter) sb.append("OpenRouter, ");
            if (ollama) sb.append("Ollama, ");
            if (azure) sb.append("Azure, ");
            if (custom) sb.append("Custom, ");
            if (sb.length() > 1) {
                sb.setLength(sb.length() - 2);
            }
            sb.append("]");
            return sb.toString();
        }
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/client/PerplexityClient.java

```java
package com.newsinsight.collector.client;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import io.netty.channel.ChannelOption;
import io.netty.handler.timeout.ReadTimeoutHandler;
import io.netty.handler.timeout.WriteTimeoutHandler;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.HttpHeaders;
import org.springframework.http.MediaType;
import org.springframework.http.client.reactive.ReactorClientHttpConnector;
import org.springframework.stereotype.Component;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Flux;
import reactor.netty.http.client.HttpClient;

import jakarta.annotation.PostConstruct;
import java.time.Duration;
import java.util.List;
import java.util.Map;
import java.util.concurrent.TimeUnit;

@Component
@Slf4j
public class PerplexityClient {

    private final ObjectMapper objectMapper;
    private WebClient perplexityWebClient;

    @Value("${PERPLEXITY_API_KEY:}")
    private String apiKey;

    @Value("${PERPLEXITY_BASE_URL:https://api.perplexity.ai}")
    private String baseUrl;

    @Value("${PERPLEXITY_MODEL:llama-3.1-sonar-large-128k-online}")
    private String model;

    @Value("${collector.perplexity.timeout-seconds:120}")
    private int timeoutSeconds;

    public PerplexityClient(ObjectMapper objectMapper) {
        this.objectMapper = objectMapper;
    }

    @PostConstruct
    public void init() {
        // Create dedicated WebClient with longer timeout for AI streaming
        HttpClient httpClient = HttpClient.create()
                .option(ChannelOption.CONNECT_TIMEOUT_MILLIS, 30000)
                .responseTimeout(Duration.ofSeconds(timeoutSeconds))
                .doOnConnected(conn ->
                        conn.addHandlerLast(new ReadTimeoutHandler(timeoutSeconds, TimeUnit.SECONDS))
                            .addHandlerLast(new WriteTimeoutHandler(timeoutSeconds, TimeUnit.SECONDS))
                )
                .followRedirect(true);

        this.perplexityWebClient = WebClient.builder()
                .clientConnector(new ReactorClientHttpConnector(httpClient))
                .defaultHeader("User-Agent", "NewsInsight-Collector/1.0")
                .codecs(configurer -> configurer.defaultCodecs().maxInMemorySize(10 * 1024 * 1024))
                .build();

        log.info("PerplexityClient initialized with timeout: {}s, enabled: {}", timeoutSeconds, isEnabled());
    }

    /**
     * Check if Perplexity API is enabled (API key is configured)
     */
    public boolean isEnabled() {
        return apiKey != null && !apiKey.isBlank();
    }

    public Flux<String> streamCompletion(String prompt) {
        if (!isEnabled()) {
            return Flux.error(new IllegalStateException("Perplexity API key is not configured"));
        }

        String url = baseUrl.endsWith("/") ? baseUrl + "chat/completions" : baseUrl + "/chat/completions";

        // System message to guide the AI to respond directly in report format
        String systemMessage = """
                ë‹¹ì‹ ì€ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìš”ì²­ì— ëŒ€í•´ ì§ì ‘ ë³´ê³ ì„œ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
                "ì•Œê² ìŠµë‹ˆë‹¤", "ë„¤", "ê²€ìƒ‰í•˜ê² ìŠµë‹ˆë‹¤" ë“±ì˜ ì„œë‘ ì—†ì´ ë°”ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
                ìš”ì²­ë°›ì€ í˜•ì‹(ë§ˆí¬ë‹¤ìš´ ë“±)ì„ ì •í™•íˆ ë”°ë¥´ì„¸ìš”.
                """;

        Map<String, Object> body = Map.of(
                "model", model,
                "stream", true,
                "messages", List.of(
                        Map.of("role", "system", "content", systemMessage),
                        Map.of("role", "user", "content", prompt)
                )
        );

        log.debug("Calling Perplexity API: {} with timeout {}s", url, timeoutSeconds);

        return perplexityWebClient.post()
                .uri(url)
                .header(HttpHeaders.AUTHORIZATION, "Bearer " + apiKey)
                .contentType(MediaType.APPLICATION_JSON)
                .accept(MediaType.TEXT_EVENT_STREAM)
                .bodyValue(body)
                .retrieve()
                .bodyToFlux(String.class)
                .timeout(Duration.ofSeconds(timeoutSeconds))
                .doOnSubscribe(s -> log.debug("Starting Perplexity stream request"))
                .doOnNext(chunk -> log.debug("Perplexity raw chunk: {}", chunk))
                .doOnError(e -> log.error("Perplexity API error: {}", e.getMessage()))
                .doOnComplete(() -> log.debug("Perplexity stream completed"))
                .flatMap(this::extractTextFromChunk);
    }

    private Flux<String> extractTextFromChunk(String chunk) {
        if (chunk == null || chunk.isBlank()) {
            return Flux.empty();
        }

        String trimmed = chunk.trim();
        if ("[DONE]".equalsIgnoreCase(trimmed) || "data: [DONE]".equalsIgnoreCase(trimmed)) {
            return Flux.empty();
        }

        String json;
        if (trimmed.startsWith("data:")) {
            json = trimmed.substring(5).trim();
        } else {
            json = trimmed;
        }

        if (json.isEmpty()) {
            return Flux.empty();
        }

        try {
            JsonNode root = objectMapper.readTree(json);
            JsonNode choices = root.get("choices");
            if (choices == null || !choices.isArray() || choices.isEmpty()) {
                return Flux.empty();
            }

            JsonNode choice = choices.get(0);
            JsonNode delta = choice.get("delta");
            if (delta != null && delta.has("content")) {
                String text = delta.get("content").asText();
                if (text != null && !text.isEmpty()) {
                    return Flux.just(text);
                }
            }

            JsonNode message = choice.get("message");
            if (message != null && message.has("content")) {
                String text = message.get("content").asText();
                if (text != null && !text.isEmpty()) {
                    return Flux.just(text);
                }
            }
        } catch (Exception e) {
            log.warn("Failed to parse Perplexity chunk: {}", chunk, e);
        }

        return Flux.empty();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/AsyncConfig.java

```java
package com.newsinsight.collector.config;

import lombok.extern.slf4j.Slf4j;
import org.springframework.aop.interceptor.AsyncUncaughtExceptionHandler;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.scheduling.annotation.AsyncConfigurer;
import org.springframework.scheduling.annotation.EnableAsync;
import org.springframework.scheduling.annotation.EnableScheduling;
import org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor;

import java.util.concurrent.Executor;

@Configuration
@EnableAsync
@EnableScheduling
@Slf4j
public class AsyncConfig implements AsyncConfigurer {

    @Value("${async.executor.core-pool-size:5}")
    private int corePoolSize;

    @Value("${async.executor.max-pool-size:20}")
    private int maxPoolSize;

    @Value("${async.executor.queue-capacity:100}")
    private int queueCapacity;

    @Value("${async.chat-sync.core-pool-size:3}")
    private int chatSyncCorePoolSize;

    @Value("${async.chat-sync.max-pool-size:10}")
    private int chatSyncMaxPoolSize;

    @Value("${async.chat-sync.queue-capacity:50}")
    private int chatSyncQueueCapacity;

    /**
     * ê¸°ë³¸ ë¹„ë™ê¸° ì‘ì—… ì‹¤í–‰ì
     */
    @Bean(name = "taskExecutor")
    public Executor taskExecutor() {
        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
        executor.setCorePoolSize(corePoolSize);
        executor.setMaxPoolSize(maxPoolSize);
        executor.setQueueCapacity(queueCapacity);
        executor.setThreadNamePrefix("async-collection-");
        executor.setWaitForTasksToCompleteOnShutdown(true);
        executor.setAwaitTerminationSeconds(60);
        executor.setRejectedExecutionHandler((r, e) -> 
                log.warn("Task rejected from taskExecutor: {}", r.toString()));
        executor.initialize();
        return executor;
    }

    /**
     * ì±„íŒ… ë™ê¸°í™” ì „ìš© ì‹¤í–‰ì
     */
    @Bean(name = "chatSyncExecutor")
    public Executor chatSyncExecutor() {
        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
        executor.setCorePoolSize(chatSyncCorePoolSize);
        executor.setMaxPoolSize(chatSyncMaxPoolSize);
        executor.setQueueCapacity(chatSyncQueueCapacity);
        executor.setThreadNamePrefix("chat-sync-");
        executor.setWaitForTasksToCompleteOnShutdown(true);
        executor.setAwaitTerminationSeconds(120); // ë™ê¸°í™” ì™„ë£Œ ëŒ€ê¸° 2ë¶„
        executor.setRejectedExecutionHandler((r, e) -> 
                log.warn("Task rejected from chatSyncExecutor: {}", r.toString()));
        executor.initialize();
        return executor;
    }

    /**
     * ë²¡í„° ì„ë² ë”© ì „ìš© ì‹¤í–‰ì
     */
    @Bean(name = "embeddingExecutor")
    public Executor embeddingExecutor() {
        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
        executor.setCorePoolSize(2);
        executor.setMaxPoolSize(5);
        executor.setQueueCapacity(100);
        executor.setThreadNamePrefix("embedding-");
        executor.setWaitForTasksToCompleteOnShutdown(true);
        executor.setAwaitTerminationSeconds(180); // ì„ë² ë”© ì™„ë£Œ ëŒ€ê¸° 3ë¶„
        executor.setRejectedExecutionHandler((r, e) -> 
                log.warn("Task rejected from embeddingExecutor: {}", r.toString()));
        executor.initialize();
        return executor;
    }

    @Override
    public Executor getAsyncExecutor() {
        return taskExecutor();
    }

    @Override
    public AsyncUncaughtExceptionHandler getAsyncUncaughtExceptionHandler() {
        return (ex, method, params) -> {
            log.error("Uncaught async exception in method {}: {}", method.getName(), ex.getMessage(), ex);
        };
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/AutoCrawlInitializer.java

```java
package com.newsinsight.collector.config;

import com.newsinsight.collector.entity.autocrawl.CrawlTarget;
import com.newsinsight.collector.service.autocrawl.AutoCrawlDiscoveryService;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;
import org.springframework.boot.context.event.ApplicationReadyEvent;
import org.springframework.context.event.EventListener;
import org.springframework.stereotype.Component;

import java.util.Arrays;
import java.util.List;

/**
 * AutoCrawl ì´ˆê¸°í™” ì»´í¬ë„ŒíŠ¸.
 * 
 * ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ seed URLë“¤ì„ í¬ë¡¤ë§ íì— ìë™ìœ¼ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.
 * docker-compose ì‹¤í–‰ ì‹œ ì¦‰ì‹œ í¬ë¡¤ë§ì´ ì‹œì‘ë˜ë„ë¡ í•©ë‹ˆë‹¤.
 */
@Component
@RequiredArgsConstructor
@Slf4j
@ConditionalOnProperty(name = "autocrawl.enabled", havingValue = "true", matchIfMissing = false)
public class AutoCrawlInitializer {

    private final AutoCrawlDiscoveryService autoCrawlDiscoveryService;

    @Value("${autocrawl.seed.enabled:true}")
    private boolean seedEnabled;

    @Value("${autocrawl.seed.urls:}")
    private String seedUrlsConfig;

    @Value("${autocrawl.seed.keywords:ë‰´ìŠ¤,ì •ì¹˜,ê²½ì œ,ì‚¬íšŒ,IT,ê¸°ìˆ }")
    private String seedKeywords;

    @Value("${autocrawl.seed.priority:70}")
    private int seedPriority;

    /**
     * ê¸°ë³¸ seed URL ëª©ë¡ (í•œêµ­ ì£¼ìš” ë‰´ìŠ¤ í¬í„¸)
     */
    private static final List<String> DEFAULT_SEED_URLS = List.of(
            // ë„¤ì´ë²„ ë‰´ìŠ¤ ë©”ì¸
            "https://news.naver.com",
            "https://news.naver.com/section/100",  // ì •ì¹˜
            "https://news.naver.com/section/101",  // ê²½ì œ
            "https://news.naver.com/section/102",  // ì‚¬íšŒ
            "https://news.naver.com/section/103",  // ìƒí™œ/ë¬¸í™”
            "https://news.naver.com/section/104",  // ì„¸ê³„
            "https://news.naver.com/section/105",  // IT/ê³¼í•™
            
            // ë‹¤ìŒ ë‰´ìŠ¤ ë©”ì¸
            "https://news.daum.net",
            "https://news.daum.net/politics",
            "https://news.daum.net/economic",
            "https://news.daum.net/society",
            "https://news.daum.net/culture",
            "https://news.daum.net/digital",
            
            // ì£¼ìš” ì–¸ë¡ ì‚¬ ë©”ì¸
            "https://www.chosun.com",
            "https://www.donga.com",
            "https://www.joongang.co.kr",
            "https://www.hani.co.kr",
            "https://www.khan.co.kr",
            "https://www.yna.co.kr",
            
            // IT/ê¸°ìˆ  ë‰´ìŠ¤
            "https://www.etnews.com",
            "https://zdnet.co.kr",
            "https://www.bloter.net"
    );

    @EventListener(ApplicationReadyEvent.class)
    public void initializeSeedUrls() {
        if (!seedEnabled) {
            log.info("[AutoCrawl Init] Seed initialization is disabled. Set AUTOCRAWL_SEED_ENABLED=true to enable.");
            return;
        }

        log.info("[AutoCrawl Init] Starting seed URL initialization...");

        try {
            List<String> seedUrls = getSeedUrls();
            
            if (seedUrls.isEmpty()) {
                log.warn("[AutoCrawl Init] No seed URLs configured");
                return;
            }

            log.info("[AutoCrawl Init] Adding {} seed URLs to crawl queue", seedUrls.size());

            List<CrawlTarget> addedTargets = autoCrawlDiscoveryService.addManualTargets(
                    seedUrls,
                    seedKeywords,
                    seedPriority
            );

            log.info("[AutoCrawl Init] Successfully added {} seed URLs to crawl queue (skipped {} duplicates)",
                    addedTargets.size(),
                    seedUrls.size() - addedTargets.size());

            // ì¶”ê°€ëœ URL ë¡œê¹…
            if (!addedTargets.isEmpty() && log.isDebugEnabled()) {
                addedTargets.forEach(target -> 
                    log.debug("[AutoCrawl Init] Added: {} (priority={})", target.getUrl(), target.getPriority())
                );
            }

        } catch (Exception e) {
            log.error("[AutoCrawl Init] Failed to initialize seed URLs: {}", e.getMessage(), e);
        }
    }

    /**
     * Seed URL ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
     * í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •ëœ URLì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ URL ì‚¬ìš©
     */
    private List<String> getSeedUrls() {
        if (seedUrlsConfig != null && !seedUrlsConfig.isBlank()) {
            // í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •ëœ ì»¤ìŠ¤í…€ URL ì‚¬ìš© (ì½¤ë§ˆë¡œ êµ¬ë¶„)
            List<String> customUrls = Arrays.stream(seedUrlsConfig.split(","))
                    .map(String::trim)
                    .filter(url -> !url.isBlank())
                    .filter(url -> url.startsWith("http://") || url.startsWith("https://"))
                    .toList();
            
            if (!customUrls.isEmpty()) {
                log.info("[AutoCrawl Init] Using {} custom seed URLs from configuration", customUrls.size());
                return customUrls;
            }
        }

        // ê¸°ë³¸ seed URL ì‚¬ìš©
        log.info("[AutoCrawl Init] Using {} default seed URLs", DEFAULT_SEED_URLS.size());
        return DEFAULT_SEED_URLS;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/DataSourceInitializer.java

```java
package com.newsinsight.collector.config;

import com.newsinsight.collector.entity.DataSource;
import com.newsinsight.collector.entity.SourceType;
import com.newsinsight.collector.repository.DataSourceRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.boot.CommandLineRunner;
import org.springframework.stereotype.Component;
import org.springframework.transaction.annotation.Transactional;

import java.util.ArrayList;
import java.util.List;

/**
 * ì›¹ ê²€ìƒ‰ ì†ŒìŠ¤ ìë™ ì´ˆê¸°í™”.
 * 
 * ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ê¸°ë³¸ ì›¹ ê²€ìƒ‰ ì†ŒìŠ¤(ë„¤ì´ë²„, ë‹¤ìŒ, êµ¬ê¸€ ë‰´ìŠ¤)ë¥¼
 * DBì— ìë™ìœ¼ë¡œ ë“±ë¡í•©ë‹ˆë‹¤. ì´ë¯¸ ë“±ë¡ëœ ì†ŒìŠ¤ëŠ” ê±´ë„ˆëœë‹ˆë‹¤.
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class DataSourceInitializer implements CommandLineRunner {

    private final DataSourceRepository dataSourceRepository;

    @Override
    @Transactional
    public void run(String... args) {
        log.info("Initializing default web search sources...");
        
        List<DataSource> defaultSources = createDefaultWebSearchSources();
        int initialized = 0;
        
        for (DataSource source : defaultSources) {
            // ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ (ì´ë¦„ìœ¼ë¡œ)
            if (dataSourceRepository.findByName(source.getName()).isEmpty()) {
                dataSourceRepository.save(source);
                initialized++;
                log.info("Initialized web search source: {}", source.getName());
            } else {
                log.debug("Web search source already exists: {}", source.getName());
            }
        }
        
        if (initialized > 0) {
            log.info("Initialized {} new web search sources", initialized);
        } else {
            log.info("All default web search sources already exist");
        }
        
        // í˜„ì¬ í™œì„±í™”ëœ ì›¹ ê²€ìƒ‰ ì†ŒìŠ¤ ìˆ˜ ë¡œê¹…
        long activeCount = dataSourceRepository.findActiveWebSearchSources().size();
        log.info("Total active web search sources: {}", activeCount);
    }

    /**
     * ê¸°ë³¸ ì›¹ ê²€ìƒ‰ ì†ŒìŠ¤ ìƒì„±
     */
    private List<DataSource> createDefaultWebSearchSources() {
        List<DataSource> sources = new ArrayList<>();
        
        // 1. ë„¤ì´ë²„ ë‰´ìŠ¤ (ìµœê³  ìš°ì„ ìˆœìœ„)
        sources.add(DataSource.builder()
                .name("ë„¤ì´ë²„ ë‰´ìŠ¤")
                .url("https://news.naver.com")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://search.naver.com/search.naver?where=news&query={query}")
                .searchPriority(10)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"country\":\"KR\",\"language\":\"ko\",\"portal\":\"naver\"}")
                .build());
        
        // 2. ë‹¤ìŒ ë‰´ìŠ¤
        sources.add(DataSource.builder()
                .name("ë‹¤ìŒ ë‰´ìŠ¤")
                .url("https://news.daum.net")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://search.daum.net/search?w=news&q={query}")
                .searchPriority(20)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"country\":\"KR\",\"language\":\"ko\",\"portal\":\"daum\"}")
                .build());
        
        // 3. êµ¬ê¸€ ë‰´ìŠ¤ (í•œêµ­)
        sources.add(DataSource.builder()
                .name("êµ¬ê¸€ ë‰´ìŠ¤")
                .url("https://news.google.com")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://news.google.com/search?q={query}&hl=ko&gl=KR")
                .searchPriority(30)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"country\":\"KR\",\"language\":\"ko\",\"portal\":\"google\"}")
                .build());
        
        // 4. ë„¤ì´íŠ¸ ë‰´ìŠ¤ (ë¹„í™œì„±í™” ìƒíƒœë¡œ ì¶”ê°€ - ì‚¬ìš©ìê°€ í•„ìš”ì‹œ í™œì„±í™” ê°€ëŠ¥)
        sources.add(DataSource.builder()
                .name("ë„¤ì´íŠ¸ ë‰´ìŠ¤")
                .url("https://news.nate.com")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://search.nate.com/search/all.html?q={query}&csn=1")
                .searchPriority(40)
                .isActive(false)
                .collectionFrequency(3600)
                .metadataJson("{\"country\":\"KR\",\"language\":\"ko\",\"portal\":\"nate\"}")
                .build());
        
        // 5. ì¤Œ ë‰´ìŠ¤ (ë¹„í™œì„±í™” ìƒíƒœë¡œ ì¶”ê°€)
        sources.add(DataSource.builder()
                .name("ì¤Œ ë‰´ìŠ¤")
                .url("https://news.zum.com")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://search.zum.com/search.zum?method=news&query={query}")
                .searchPriority(50)
                .isActive(false)
                .collectionFrequency(3600)
                .metadataJson("{\"country\":\"KR\",\"language\":\"ko\",\"portal\":\"zum\"}")
                .build());
        
        return sources;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/DataSourceSeeder.java

```java
package com.newsinsight.collector.config;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.newsinsight.collector.entity.BrowserAgentConfig;
import com.newsinsight.collector.entity.DataSource;
import com.newsinsight.collector.entity.SourceType;
import com.newsinsight.collector.repository.DataSourceRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.boot.ApplicationArguments;
import org.springframework.boot.ApplicationRunner;
import org.springframework.context.annotation.Profile;
import org.springframework.stereotype.Component;
import org.springframework.transaction.annotation.Transactional;

import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.stream.Collectors;

/**
 * Seeds the database with default Korean news sources on application startup.
 * Only runs if the data_sources table is empty.
 * 
 * Sources can be configured via:
 * 1. application.yml (collector.data-sources.sources)
 * 2. Default hardcoded sources (if no external config provided)
 * 
 * Profiles:
 * - default: Runs automatically
 * - no-seed: Skip seeding (for production or when using external config)
 */
@Component
@Profile("!no-seed")
@RequiredArgsConstructor
@Slf4j
public class DataSourceSeeder implements ApplicationRunner {

    private final DataSourceRepository dataSourceRepository;
    private final DataSourcesConfig dataSourcesConfig;
    private final ObjectMapper objectMapper;

    @Override
    @Transactional
    public void run(ApplicationArguments args) {
        if (!dataSourcesConfig.isSeedEnabled()) {
            log.info("DataSource seeding is disabled via configuration.");
            return;
        }

        log.info("Seeding data sources...");
        
        List<DataSource> sources;
        
        // Check if external configuration is provided
        if (dataSourcesConfig.getSources() != null && !dataSourcesConfig.getSources().isEmpty()) {
            log.info("Using {} data sources from external configuration.", dataSourcesConfig.getSources().size());
            sources = dataSourcesConfig.getSources().stream()
                    .map(this::convertToDataSource)
                    .collect(Collectors.toList());
        } else {
            log.info("No external configuration found, using default Korean news sources.");
            sources = createDefaultSources();
        }

        int created = 0;
        int skipped = 0;
        for (DataSource desired : sources) {
            DataSource existing = dataSourceRepository
                    .findFirstByUrl(desired.getUrl())
                    .or(() -> dataSourceRepository.findByName(desired.getName()))
                    .orElse(null);

            if (existing == null) {
                dataSourceRepository.save(desired);
                created++;
                continue;
            }
            skipped++;
        }

        log.info(
                "Successfully seeded data sources. created={}, skipped={}, totalDesired={}",
                created,
                skipped,
                sources.size()
        );
    }

    /**
     * Convert external configuration entry to DataSource entity
     */
    private DataSource convertToDataSource(DataSourcesConfig.DataSourceEntry entry) {
        // Build metadata JSON from entry fields
        Map<String, String> metadata = new HashMap<>();
        if (entry.getRegion() != null) metadata.put("region", entry.getRegion());
        if (entry.getLanguage() != null) metadata.put("language", entry.getLanguage());
        if (entry.getReliability() != null) metadata.put("reliability", entry.getReliability());
        if (entry.getCategory() != null) metadata.put("category", entry.getCategory());
        if (entry.getStance() != null) metadata.put("stance", entry.getStance());
        
        // Merge with any additional metadata provided
        if (entry.getMetadata() != null) {
            metadata.putAll(entry.getMetadata());
        }
        
        String metadataJson;
        try {
            metadataJson = objectMapper.writeValueAsString(metadata);
        } catch (JsonProcessingException e) {
            log.warn("Failed to serialize metadata for source {}: {}", entry.getName(), e.getMessage());
            metadataJson = "{}";
        }
        
        DataSource.DataSourceBuilder builder = DataSource.builder()
                .name(entry.getName())
                .url(entry.getUrl())
                .sourceType(parseSourceType(entry.getSourceType()))
                .isActive(entry.isActive())
                .collectionFrequency(entry.getCollectionFrequency())
                .metadataJson(metadataJson);
        
        // Add search-related fields for WEB_SEARCH sources
        if (entry.getSearchUrlTemplate() != null) {
            builder.searchUrlTemplate(entry.getSearchUrlTemplate());
        }
        if (entry.getSearchPriority() != null) {
            builder.searchPriority(entry.getSearchPriority());
        }
        
        return builder.build();
    }

    private SourceType parseSourceType(String type) {
        if (type == null) return SourceType.RSS;
        try {
            return SourceType.valueOf(type.toUpperCase());
        } catch (IllegalArgumentException e) {
            log.warn("Unknown source type '{}', defaulting to RSS", type);
            return SourceType.RSS;
        }
    }

    private List<DataSource> createDefaultSources() {
        return List.of(
            // ========== Korean Major News (High Reliability) ==========
            DataSource.builder()
                .name("ì—°í•©ë‰´ìŠ¤ (Yonhap)")
                .url("https://www.yna.co.kr/rss/news.xml")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(1800) // 30 min
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"news_agency\"}")
                .build(),
                
            DataSource.builder()
                .name("KBS ë‰´ìŠ¤")
                .url("https://news.kbs.co.kr/rss/rss.xml")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(1800)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"broadcast\"}")
                .build(),
                
            DataSource.builder()
                .name("MBC ë‰´ìŠ¤")
                .url("https://imnews.imbc.com/rss/news.xml")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(1800)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"broadcast\"}")
                .build(),
                
            DataSource.builder()
                .name("SBS ë‰´ìŠ¤")
                .url("https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=01")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(1800)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"broadcast\"}")
                .build(),
                
            // ========== Korean Major Newspapers ==========
            DataSource.builder()
                .name("ì¡°ì„ ì¼ë³´")
                .url("https://www.chosun.com/arc/outboundfeeds/rss/?outputType=xml")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"newspaper\",\"stance\":\"conservative\"}")
                .build(),
                
            DataSource.builder()
                .name("ì¤‘ì•™ì¼ë³´")
                .url("https://rss.joins.com/joins_news_list.xml")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"newspaper\",\"stance\":\"center-right\"}")
                .build(),
                
            DataSource.builder()
                .name("ë™ì•„ì¼ë³´")
                .url("https://rss.donga.com/total.xml")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"newspaper\",\"stance\":\"conservative\"}")
                .build(),
                
            DataSource.builder()
                .name("í•œê²¨ë ˆ")
                .url("https://www.hani.co.kr/rss/")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"newspaper\",\"stance\":\"progressive\"}")
                .build(),
                
            DataSource.builder()
                .name("ê²½í–¥ì‹ ë¬¸")
                .url("https://www.khan.co.kr/rss/rssdata/total_news.xml")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"newspaper\",\"stance\":\"progressive\"}")
                .build(),
                
            // ========== Korean Business/Economy News ==========
            DataSource.builder()
                .name("ë§¤ì¼ê²½ì œ")
                .url("https://www.mk.co.kr/rss/30000001/")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"business\"}")
                .build(),
                
            DataSource.builder()
                .name("í•œêµ­ê²½ì œ")
                .url("https://www.hankyung.com/feed/all-news")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"business\"}")
                .build(),
                
            // ========== Korean IT/Tech News ==========
            DataSource.builder()
                .name("ZDNet Korea")
                .url("https://zdnet.co.kr/rss/")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"tech\"}")
                .build(),
                
            DataSource.builder()
                .name("ì „ìì‹ ë¬¸ (ETNews)")
                .url("https://www.etnews.com/rss")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"tech\"}")
                .build(),
                
            DataSource.builder()
                .name("ë¸”ë¡œí„° (Bloter)")
                .url("https://www.bloter.net/feed")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(7200)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"tech_startup\"}")
                .build(),
                
            // ========== International News (Korean Edition) ==========
            DataSource.builder()
                .name("BBC ì½”ë¦¬ì•„")
                .url("https://feeds.bbci.co.uk/korean/rss.xml")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(1800)
                .metadataJson("{\"region\":\"international\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"international\"}")
                .build(),
                
            DataSource.builder()
                .name("ë‰´ì‹œìŠ¤ (Newsis)")
                .url("https://newsis.com/RSS/")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(1800)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"news_agency\"}")
                .build(),
                
            DataSource.builder()
                .name("ë‰´ìŠ¤1")
                .url("https://www.news1.kr/rss/")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(1800)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"news_agency\"}")
                .build(),
                
            // ========== BROWSER_AGENT Sources (AI-based crawling) ==========
            // ë„¤ì´ë²„ ë‰´ìŠ¤ (Browser Agent)
            DataSource.builder()
                .name("ë„¤ì´ë²„ ë‰´ìŠ¤ (Browser Agent)")
                .url("https://news.naver.com/")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(true)
                .collectionFrequency(1800) // 30 min
                .browserAgentConfig(BrowserAgentConfig.forNewsExploration())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"portal\",\"crawler\":\"browser_agent\"}")
                .build(),
                
            // ë‹¤ìŒ ë‰´ìŠ¤ (Browser Agent)
            DataSource.builder()
                .name("ë‹¤ìŒ ë‰´ìŠ¤ (Browser Agent)")
                .url("https://news.daum.net/")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(true)
                .collectionFrequency(1800)
                .browserAgentConfig(BrowserAgentConfig.forNewsExploration())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"portal\",\"crawler\":\"browser_agent\"}")
                .build(),
                
            // êµ¬ê¸€ ë‰´ìŠ¤ í•œêµ­ (Browser Agent)
            DataSource.builder()
                .name("êµ¬ê¸€ ë‰´ìŠ¤ í•œêµ­ (Browser Agent)")
                .url("https://news.google.com/home?hl=ko&gl=KR&ceid=KR:ko")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(true)
                .collectionFrequency(3600)
                .browserAgentConfig(BrowserAgentConfig.forNewsExploration())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"aggregator\",\"crawler\":\"browser_agent\"}")
                .build(),
                
            // ë„¤ì´ë²„ ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ íŠ¸ë Œë“œ (Browser Agent - Breaking News)
            DataSource.builder()
                .name("ë„¤ì´ë²„ íŠ¸ë Œë“œ (Browser Agent)")
                .url("https://datalab.naver.com/keyword/realtimeList.naver")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(true)
                .collectionFrequency(900) // 15 min - íŠ¸ë Œë“œëŠ” ìì£¼ í™•ì¸
                .browserAgentConfig(BrowserAgentConfig.forBreakingNews())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"trending\",\"crawler\":\"browser_agent\"}")
                .build(),
                
            // ì¡°ì„ ì¼ë³´ (Browser Agent - Archive mode for non-RSS content)
            DataSource.builder()
                .name("ì¡°ì„ ì¼ë³´ (Browser Agent)")
                .url("https://www.chosun.com/")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(false) // RSSê°€ ìˆìœ¼ë¯€ë¡œ ê¸°ë³¸ ë¹„í™œì„±
                .collectionFrequency(7200)
                .browserAgentConfig(BrowserAgentConfig.forNewsArchive())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"newspaper\",\"crawler\":\"browser_agent\",\"stance\":\"conservative\"}")
                .build(),
                
            // í•œê²¨ë ˆ (Browser Agent)
            DataSource.builder()
                .name("í•œê²¨ë ˆ (Browser Agent)")
                .url("https://www.hani.co.kr/")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(false) // RSSê°€ ìˆìœ¼ë¯€ë¡œ ê¸°ë³¸ ë¹„í™œì„±
                .collectionFrequency(7200)
                .browserAgentConfig(BrowserAgentConfig.forNewsArchive())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"newspaper\",\"crawler\":\"browser_agent\",\"stance\":\"progressive\"}")
                .build(),
                
            // ========== WEB_SEARCH Sources (Portal Search Integration) ==========
            DataSource.builder()
                .name("ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰")
                .url("https://search.naver.com")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://search.naver.com/search.naver?where=news&query={query}")
                .searchPriority(1)
                .isActive(true)
                .collectionFrequency(0) // ê²€ìƒ‰ì€ ì£¼ê¸°ì  ìˆ˜ì§‘ ì—†ìŒ
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"portal_search\"}")
                .build(),
                
            DataSource.builder()
                .name("ë‹¤ìŒ ë‰´ìŠ¤ ê²€ìƒ‰")
                .url("https://search.daum.net")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://search.daum.net/search?w=news&q={query}")
                .searchPriority(2)
                .isActive(true)
                .collectionFrequency(0)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"portal_search\"}")
                .build(),
                
            DataSource.builder()
                .name("êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰ (í•œêµ­)")
                .url("https://news.google.com")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://news.google.com/search?q={query}&hl=ko&gl=KR")
                .searchPriority(3)
                .isActive(true)
                .collectionFrequency(0)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"aggregator_search\"}")
                .build(),
                
            DataSource.builder()
                .name("ë¹™ ë‰´ìŠ¤ ê²€ìƒ‰ (í•œêµ­)")
                .url("https://www.bing.com")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://www.bing.com/news/search?q={query}&cc=kr")
                .searchPriority(4)
                .isActive(true)
                .collectionFrequency(0)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"aggregator_search\"}")
                .build(),
                
            // ========== COMMUNITY Sources (ì»¤ë®¤ë‹ˆí‹° ì—¬ë¡  ìˆ˜ì§‘) ==========
            // DCInside (ë””ì‹œì¸ì‚¬ì´ë“œ)
            DataSource.builder()
                .name("ë””ì‹œì¸ì‚¬ì´ë“œ (Browser Agent)")
                .url("https://www.dcinside.com/")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(true)
                .collectionFrequency(3600)
                .browserAgentConfig(BrowserAgentConfig.forNewsExploration())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"low\",\"category\":\"community\",\"source_category\":\"community\",\"crawler\":\"browser_agent\"}")
                .build(),
                
            // Clien (í´ë¦¬ì•™)
            DataSource.builder()
                .name("í´ë¦¬ì•™ (Browser Agent)")
                .url("https://www.clien.net/service/")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(true)
                .collectionFrequency(3600)
                .browserAgentConfig(BrowserAgentConfig.forNewsExploration())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"community\",\"source_category\":\"community\",\"crawler\":\"browser_agent\"}")
                .build(),
                
            // Ruliweb (ë£¨ë¦¬ì›¹)
            DataSource.builder()
                .name("ë£¨ë¦¬ì›¹ (Browser Agent)")
                .url("https://bbs.ruliweb.com/")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(true)
                .collectionFrequency(3600)
                .browserAgentConfig(BrowserAgentConfig.forNewsExploration())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"low\",\"category\":\"community\",\"source_category\":\"community\",\"crawler\":\"browser_agent\"}")
                .build(),
                
            // Ppomppu (ë½ë¿Œ)
            DataSource.builder()
                .name("ë½ë¿Œ (Browser Agent)")
                .url("https://www.ppomppu.co.kr/")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(true)
                .collectionFrequency(3600)
                .browserAgentConfig(BrowserAgentConfig.forNewsExploration())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"low\",\"category\":\"community\",\"source_category\":\"community\",\"crawler\":\"browser_agent\"}")
                .build(),
                
            // TheQoo (ë”ì¿ )
            DataSource.builder()
                .name("ë”ì¿  (Browser Agent)")
                .url("https://theqoo.net/")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(true)
                .collectionFrequency(3600)
                .browserAgentConfig(BrowserAgentConfig.forNewsExploration())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"low\",\"category\":\"community\",\"source_category\":\"community\",\"crawler\":\"browser_agent\"}")
                .build(),
                
            // FMKorea (ì—í¨ì½”ë¦¬ì•„)
            DataSource.builder()
                .name("ì—í¨ì½”ë¦¬ì•„ (Browser Agent)")
                .url("https://www.fmkorea.com/")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(true)
                .collectionFrequency(3600)
                .browserAgentConfig(BrowserAgentConfig.forNewsExploration())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"low\",\"category\":\"community\",\"source_category\":\"community\",\"crawler\":\"browser_agent\"}")
                .build(),
                
            // MLB Park (ì— íŒ)
            DataSource.builder()
                .name("ì— ì—˜ë¹„íŒŒí¬ (Browser Agent)")
                .url("https://mlbpark.donga.com/")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(true)
                .collectionFrequency(3600)
                .browserAgentConfig(BrowserAgentConfig.forNewsExploration())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"low\",\"category\":\"community\",\"source_category\":\"community\",\"crawler\":\"browser_agent\"}")
                .build(),
                
            // Bobaedream (ë³´ë°°ë“œë¦¼ - ìë™ì°¨)
            DataSource.builder()
                .name("ë³´ë°°ë“œë¦¼ (Browser Agent)")
                .url("https://www.bobaedream.co.kr/")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(true)
                .collectionFrequency(7200)
                .browserAgentConfig(BrowserAgentConfig.forNewsExploration())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"low\",\"category\":\"community\",\"source_category\":\"community\",\"crawler\":\"browser_agent\"}")
                .build(),
                
            // ========== COMMUNITY Search Sources (ì»¤ë®¤ë‹ˆí‹° ê²€ìƒ‰) ==========
            DataSource.builder()
                .name("Reddit ê²€ìƒ‰")
                .url("https://www.reddit.com")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://www.reddit.com/search/?q={query}&type=link")
                .searchPriority(5)
                .isActive(true)
                .collectionFrequency(0)
                .metadataJson("{\"region\":\"global\",\"language\":\"en\",\"reliability\":\"medium\",\"category\":\"community_search\",\"source_category\":\"community\"}")
                .build(),
                
            DataSource.builder()
                .name("Twitter/X ê²€ìƒ‰")
                .url("https://twitter.com")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://twitter.com/search?q={query}&f=live")
                .searchPriority(6)
                .isActive(true)
                .collectionFrequency(0)
                .metadataJson("{\"region\":\"global\",\"language\":\"multi\",\"reliability\":\"low\",\"category\":\"social_search\",\"source_category\":\"community\"}")
                .build(),
                
            // ========== BLOG Sources (ë¸”ë¡œê·¸/ì˜ê²¬) ==========
            DataSource.builder()
                .name("ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰")
                .url("https://search.naver.com")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://search.naver.com/search.naver?where=blog&query={query}")
                .searchPriority(7)
                .isActive(true)
                .collectionFrequency(0)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"low\",\"category\":\"blog_search\",\"source_category\":\"blog\"}")
                .build(),
                
            DataSource.builder()
                .name("ë¸ŒëŸ°ì¹˜ ê²€ìƒ‰")
                .url("https://brunch.co.kr")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://brunch.co.kr/search?q={query}")
                .searchPriority(8)
                .isActive(true)
                .collectionFrequency(0)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"blog_search\",\"source_category\":\"blog\"}")
                .build()
        );
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/DataSourcesConfig.java

```java
package com.newsinsight.collector.config;

import lombok.Data;
import org.springframework.boot.context.properties.ConfigurationProperties;
import org.springframework.context.annotation.Configuration;

import java.util.ArrayList;
import java.util.List;
import java.util.Map;

/**
 * Configuration for externalized data sources.
 * 
 * Data sources can be configured via application.yml or environment variables.
 * This replaces hardcoded sources in DataSourceSeeder with configurable ones.
 */
@Configuration
@ConfigurationProperties(prefix = "collector.data-sources")
@Data
public class DataSourcesConfig {

    /**
     * Enable/disable automatic seeding of data sources
     */
    private boolean seedEnabled = true;

    /**
     * List of predefined data source configurations
     */
    private List<DataSourceEntry> sources = new ArrayList<>();

    @Data
    public static class DataSourceEntry {
        /**
         * Display name for the source
         */
        private String name;

        /**
         * URL for the data source (RSS feed, API endpoint, etc.)
         */
        private String url;

        /**
         * Type of source: RSS, API, WEB_SCRAPER, WEB_SEARCH, BROWSER_AGENT
         */
        private String sourceType = "RSS";

        /**
         * Whether this source is active and should be collected
         */
        private boolean active = true;

        /**
         * Collection frequency in seconds
         */
        private int collectionFrequency = 3600;

        /**
         * Search URL template for WEB_SEARCH sources.
         * Use {query} as placeholder for the encoded search query.
         * Example: "https://search.naver.com/search.naver?where=news&query={query}"
         */
        private String searchUrlTemplate;

        /**
         * Priority for web search sources (lower = higher priority).
         */
        private Integer searchPriority = 100;

        /**
         * Additional metadata as key-value pairs
         */
        private Map<String, String> metadata;

        /**
         * Region/country code (e.g., "korea", "international")
         */
        private String region;

        /**
         * Language code (e.g., "ko", "en")
         */
        private String language = "ko";

        /**
         * Reliability level: "high", "medium", "low"
         */
        private String reliability = "medium";

        /**
         * Category: "news_agency", "broadcast", "newspaper", "business", "tech", etc.
         */
        private String category;

        /**
         * Political stance (optional): "conservative", "progressive", "center", "center-right", "center-left"
         */
        private String stance;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/KafkaConfig.java

```java
package com.newsinsight.collector.config;

import com.newsinsight.collector.dto.AiRequestMessage;
import com.newsinsight.collector.dto.AiResponseMessage;
import com.newsinsight.collector.dto.AiTaskRequestMessage;
import com.newsinsight.collector.dto.BrowserTaskMessage;
import com.newsinsight.collector.dto.CrawlCommandMessage;
import com.newsinsight.collector.dto.CrawlResultMessage;
import com.newsinsight.collector.dto.SearchHistoryMessage;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.kafka.common.serialization.StringSerializer;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;
import org.springframework.kafka.core.*;
import org.springframework.kafka.listener.CommonErrorHandler;
import org.springframework.kafka.listener.ContainerProperties;
import org.springframework.kafka.listener.DeadLetterPublishingRecoverer;
import org.springframework.kafka.listener.DefaultErrorHandler;
import org.springframework.kafka.support.ExponentialBackOffWithMaxRetries;
import org.springframework.kafka.support.serializer.JsonDeserializer;
import org.springframework.kafka.support.serializer.JsonSerializer;

import java.util.HashMap;
import java.util.Map;

/**
 * Kafka Configuration with Production-grade reliability features:
 * - Dead Letter Queue (DLQ) for failed messages
 * - Exponential backoff retry with max attempts
 * - Producer reliability settings (acks=all, retries, idempotence)
 * - Manual acknowledgment mode for consumer reliability
 * - Centralized configuration to reduce duplication
 */
@Configuration
@Slf4j
public class KafkaConfig {

    // ========== Configuration Properties ==========
    
    @Value("${spring.kafka.bootstrap-servers:localhost:9092}")
    private String bootstrapServers;

    @Value("${spring.application.name:collector-service}")
    private String applicationName;

    // Producer reliability settings
    @Value("${spring.kafka.producer.acks:all}")
    private String producerAcks;

    @Value("${spring.kafka.producer.retries:3}")
    private int producerRetries;

    @Value("${spring.kafka.producer.retry-backoff-ms:1000}")
    private int producerRetryBackoffMs;

    @Value("${spring.kafka.producer.delivery-timeout-ms:120000}")
    private int producerDeliveryTimeoutMs;

    @Value("${spring.kafka.producer.enable-idempotence:true}")
    private boolean producerIdempotence;

    // Consumer reliability settings
    @Value("${spring.kafka.consumer.max-retry-attempts:3}")
    private int consumerMaxRetryAttempts;

    @Value("${spring.kafka.consumer.retry-backoff-ms:1000}")
    private long consumerRetryBackoffMs;

    @Value("${spring.kafka.consumer.retry-max-backoff-ms:30000}")
    private long consumerRetryMaxBackoffMs;

    @Value("${spring.kafka.consumer.concurrency:1}")
    private int consumerConcurrency;

    // DLQ suffix
    private static final String DLQ_SUFFIX = ".dlq";

    // ========== Common Producer Configuration ==========

    private Map<String, Object> buildProducerProps() {
        Map<String, Object> props = new HashMap<>();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, JsonSerializer.class);
        
        // Reliability settings
        props.put(ProducerConfig.ACKS_CONFIG, producerAcks);
        props.put(ProducerConfig.RETRIES_CONFIG, producerRetries);
        props.put(ProducerConfig.RETRY_BACKOFF_MS_CONFIG, producerRetryBackoffMs);
        props.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, producerDeliveryTimeoutMs);
        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, producerIdempotence);
        
        // Batching for throughput (can be tuned)
        props.put(ProducerConfig.LINGER_MS_CONFIG, 5);
        props.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384);
        
        return props;
    }

    // ========== Common Consumer Configuration ==========

    private Map<String, Object> buildConsumerProps(String groupIdSuffix) {
        Map<String, Object> props = new HashMap<>();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, applicationName + "-" + groupIdSuffix);
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, JsonDeserializer.class);
        props.put(JsonDeserializer.TRUSTED_PACKAGES, "com.newsinsight.collector.dto");
        
        // Reliability settings
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false); // Manual ack
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 100);
        props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, 300000); // 5 minutes
        
        return props;
    }

    // ========== DLQ Producer (Generic) ==========

    @Bean
    public ProducerFactory<String, Object> dlqProducerFactory() {
        return new DefaultKafkaProducerFactory<>(buildProducerProps());
    }

    @Bean
    public KafkaTemplate<String, Object> dlqKafkaTemplate() {
        return new KafkaTemplate<>(dlqProducerFactory());
    }

    /**
     * Dead Letter Publishing Recoverer - sends failed messages to DLQ topic.
     * Topic naming convention: original-topic.dlq
     */
    @Bean
    public DeadLetterPublishingRecoverer deadLetterPublishingRecoverer() {
        return new DeadLetterPublishingRecoverer(dlqKafkaTemplate(),
                (ConsumerRecord<?, ?> record, Exception ex) -> {
                    String dlqTopic = record.topic() + DLQ_SUFFIX;
                    log.error("Sending to DLQ: topic={}, key={}, offset={}, error={}",
                            dlqTopic, record.key(), record.offset(), ex.getMessage());
                    return new TopicPartition(dlqTopic, record.partition());
                });
    }

    /**
     * Common Error Handler with exponential backoff and DLQ.
     */
    @Bean
    public CommonErrorHandler kafkaErrorHandler(DeadLetterPublishingRecoverer recoverer) {
        ExponentialBackOffWithMaxRetries backOff = new ExponentialBackOffWithMaxRetries(consumerMaxRetryAttempts);
        backOff.setInitialInterval(consumerRetryBackoffMs);
        backOff.setMaxInterval(consumerRetryMaxBackoffMs);
        backOff.setMultiplier(2.0);

        DefaultErrorHandler errorHandler = new DefaultErrorHandler(recoverer, backOff);
        
        // Log retries
        errorHandler.setRetryListeners((record, ex, attempt) -> {
            log.warn("Retry attempt {} for record: topic={}, key={}, offset={}, error={}",
                    attempt, record.topic(), record.key(), record.offset(), ex.getMessage());
        });
        
        return errorHandler;
    }

    // ========== AI Request Producer ==========

    @Bean
    public ProducerFactory<String, AiRequestMessage> aiRequestProducerFactory() {
        return new DefaultKafkaProducerFactory<>(buildProducerProps());
    }

    @Bean
    public KafkaTemplate<String, AiRequestMessage> aiRequestKafkaTemplate() {
        KafkaTemplate<String, AiRequestMessage> template = new KafkaTemplate<>(aiRequestProducerFactory());
        template.setObservationEnabled(true); // Enable metrics
        return template;
    }

    // ========== Crawl Command Producer ==========

    @Bean
    public ProducerFactory<String, CrawlCommandMessage> crawlCommandProducerFactory() {
        return new DefaultKafkaProducerFactory<>(buildProducerProps());
    }

    @Bean
    public KafkaTemplate<String, CrawlCommandMessage> crawlCommandKafkaTemplate() {
        KafkaTemplate<String, CrawlCommandMessage> template = new KafkaTemplate<>(crawlCommandProducerFactory());
        template.setObservationEnabled(true);
        return template;
    }

    // ========== Crawl Result Producer ==========

    @Bean
    public ProducerFactory<String, CrawlResultMessage> crawlResultProducerFactory() {
        return new DefaultKafkaProducerFactory<>(buildProducerProps());
    }

    @Bean
    public KafkaTemplate<String, CrawlResultMessage> crawlResultKafkaTemplate() {
        KafkaTemplate<String, CrawlResultMessage> template = new KafkaTemplate<>(crawlResultProducerFactory());
        template.setObservationEnabled(true);
        return template;
    }

    // ========== AI Task Request Producer (for Orchestration) ==========

    @Bean
    public ProducerFactory<String, AiTaskRequestMessage> aiTaskRequestProducerFactory() {
        return new DefaultKafkaProducerFactory<>(buildProducerProps());
    }

    @Bean
    public KafkaTemplate<String, AiTaskRequestMessage> aiTaskRequestKafkaTemplate() {
        KafkaTemplate<String, AiTaskRequestMessage> template = new KafkaTemplate<>(aiTaskRequestProducerFactory());
        template.setObservationEnabled(true);
        return template;
    }

    // ========== Browser Task Producer (for autonomous browser crawling) ==========

    @Bean
    public ProducerFactory<String, BrowserTaskMessage> browserTaskProducerFactory() {
        return new DefaultKafkaProducerFactory<>(buildProducerProps());
    }

    @Bean
    public KafkaTemplate<String, BrowserTaskMessage> browserTaskKafkaTemplate() {
        KafkaTemplate<String, BrowserTaskMessage> template = new KafkaTemplate<>(browserTaskProducerFactory());
        template.setObservationEnabled(true);
        return template;
    }

    // ========== AI Response Consumer ==========

    @Bean
    public ConsumerFactory<String, AiResponseMessage> aiResponseConsumerFactory() {
        return new DefaultKafkaConsumerFactory<>(
                buildConsumerProps("ai"),
                new StringDeserializer(),
                new JsonDeserializer<>(AiResponseMessage.class)
        );
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, AiResponseMessage> aiResponseKafkaListenerContainerFactory(
            CommonErrorHandler kafkaErrorHandler) {
        ConcurrentKafkaListenerContainerFactory<String, AiResponseMessage> factory =
                new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(aiResponseConsumerFactory());
        factory.setConcurrency(consumerConcurrency);
        factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.RECORD);
        factory.setCommonErrorHandler(kafkaErrorHandler);
        return factory;
    }

    // ========== Crawl Command Consumer ==========

    @Bean
    public ConsumerFactory<String, CrawlCommandMessage> crawlCommandConsumerFactory() {
        return new DefaultKafkaConsumerFactory<>(
                buildConsumerProps("crawl"),
                new StringDeserializer(),
                new JsonDeserializer<>(CrawlCommandMessage.class)
        );
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, CrawlCommandMessage> crawlCommandKafkaListenerContainerFactory(
            CommonErrorHandler kafkaErrorHandler) {
        ConcurrentKafkaListenerContainerFactory<String, CrawlCommandMessage> factory =
                new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(crawlCommandConsumerFactory());
        factory.setConcurrency(consumerConcurrency);
        factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.RECORD);
        factory.setCommonErrorHandler(kafkaErrorHandler);
        return factory;
    }

    // ========== Crawl Result Consumer ==========

    @Bean
    public ConsumerFactory<String, CrawlResultMessage> crawlResultConsumerFactory() {
        return new DefaultKafkaConsumerFactory<>(
                buildConsumerProps("crawl-result"),
                new StringDeserializer(),
                new JsonDeserializer<>(CrawlResultMessage.class)
        );
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, CrawlResultMessage> crawlResultKafkaListenerContainerFactory(
            CommonErrorHandler kafkaErrorHandler) {
        ConcurrentKafkaListenerContainerFactory<String, CrawlResultMessage> factory =
                new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(crawlResultConsumerFactory());
        factory.setConcurrency(consumerConcurrency);
        factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.RECORD);
        factory.setCommonErrorHandler(kafkaErrorHandler);
        return factory;
    }

    // ========== Search History Producer (for async persistence) ==========

    @Bean
    public ProducerFactory<String, SearchHistoryMessage> searchHistoryProducerFactory() {
        return new DefaultKafkaProducerFactory<>(buildProducerProps());
    }

    @Bean
    public KafkaTemplate<String, SearchHistoryMessage> searchHistoryKafkaTemplate() {
        KafkaTemplate<String, SearchHistoryMessage> template = new KafkaTemplate<>(searchHistoryProducerFactory());
        template.setObservationEnabled(true);
        return template;
    }

    // ========== Search History Consumer ==========

    @Bean
    public ConsumerFactory<String, SearchHistoryMessage> searchHistoryConsumerFactory() {
        return new DefaultKafkaConsumerFactory<>(
                buildConsumerProps("search-history"),
                new StringDeserializer(),
                new JsonDeserializer<>(SearchHistoryMessage.class)
        );
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, SearchHistoryMessage> searchHistoryKafkaListenerContainerFactory(
            CommonErrorHandler kafkaErrorHandler) {
        ConcurrentKafkaListenerContainerFactory<String, SearchHistoryMessage> factory =
                new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(searchHistoryConsumerFactory());
        factory.setConcurrency(consumerConcurrency);
        factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.RECORD);
        factory.setCommonErrorHandler(kafkaErrorHandler);
        return factory;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/MlAddonSeeder.java

```java
package com.newsinsight.collector.config;

import com.newsinsight.collector.entity.addon.*;
import com.newsinsight.collector.repository.MlAddonRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.boot.ApplicationArguments;
import org.springframework.boot.ApplicationRunner;
import org.springframework.context.annotation.Profile;
import org.springframework.stereotype.Component;
import org.springframework.transaction.annotation.Transactional;

import java.util.List;
import java.util.Map;

/**
 * Seeds the database with default ML Add-ons on application startup.
 * Registers sentiment, factcheck, and bias analysis add-ons if not already present.
 * 
 * Profiles:
 * - default: Runs automatically
 * - no-seed: Skip seeding (for production or when using external config)
 */
@Component
@Profile("!no-seed")
@RequiredArgsConstructor
@Slf4j
public class MlAddonSeeder implements ApplicationRunner {

    private final MlAddonRepository mlAddonRepository;

    @Value("${ml.addon.sentiment.host:sentiment-addon}")
    private String sentimentHost;

    @Value("${ml.addon.sentiment.port:8100}")
    private int sentimentPort;

    @Value("${ml.addon.factcheck.host:factcheck-addon}")
    private String factcheckHost;

    @Value("${ml.addon.factcheck.port:8101}")
    private int factcheckPort;

    @Value("${ml.addon.bias.host:bias-addon}")
    private String biasHost;

    @Value("${ml.addon.bias.port:8102}")
    private int biasPort;

    @Value("${ml.addon.seed.enabled:true}")
    private boolean seedEnabled;

    @Override
    @Transactional
    public void run(ApplicationArguments args) {
        if (!seedEnabled) {
            log.info("ML Add-on seeding is disabled via configuration.");
            return;
        }

        log.info("Seeding ML Add-ons...");

        List<MlAddon> defaultAddons = createDefaultAddons();

        int created = 0;
        int skipped = 0;

        for (MlAddon addon : defaultAddons) {
            if (mlAddonRepository.existsByAddonKey(addon.getAddonKey())) {
                log.debug("ML Add-on '{}' already exists, skipping.", addon.getAddonKey());
                skipped++;
            } else {
                mlAddonRepository.save(addon);
                log.info("Created ML Add-on: {} ({})", addon.getName(), addon.getAddonKey());
                created++;
            }
        }

        log.info("Successfully seeded ML Add-ons. created={}, skipped={}, total={}", 
                created, skipped, defaultAddons.size());
    }

    private List<MlAddon> createDefaultAddons() {
        return List.of(
            // ========== Sentiment Analysis Add-on ==========
            MlAddon.builder()
                .addonKey("sentiment-v1")
                .name("í•œêµ­ì–´ ê°ì • ë¶„ì„")
                .description("í•œêµ­ì–´ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ê°ì •(ê¸ì •/ë¶€ì •/ì¤‘ë¦½)ì„ ë¶„ì„í•©ë‹ˆë‹¤. " +
                        "KoBERT/KoELECTRA ê¸°ë°˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ ê°ì • ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.")
                .category(AddonCategory.SENTIMENT)
                .invokeType(AddonInvokeType.HTTP_SYNC)
                .endpointUrl(String.format("http://%s:%d/analyze", sentimentHost, sentimentPort))
                .healthCheckUrl(String.format("http://%s:%d/health", sentimentHost, sentimentPort))
                .authType(AddonAuthType.NONE)
                .inputSchemaVersion("1.0")
                .outputSchemaVersion("1.0")
                .timeoutMs(30000)
                .maxQps(20)
                .maxRetries(3)
                .enabled(true)
                .priority(10)
                .healthStatus(AddonHealthStatus.UNKNOWN)
                .owner("system")
                .config(Map.of(
                    "model", "koelectra-sentiment",
                    "language", "ko",
                    "min_confidence", 0.5,
                    "include_emotions", true
                ))
                .build(),

            // ========== Fact Check Add-on ==========
            MlAddon.builder()
                .addonKey("factcheck-v1")
                .name("íŒ©íŠ¸ì²´í¬ ë¶„ì„")
                .description("ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì£¼ì¥ì„ ì¶”ì¶œí•˜ê³  ì‹ ë¢°ë„ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤. " +
                        "KoELECTRA, Sentence Transformers, KLUE BERTë¥¼ í™œìš©í•œ ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸” ë¶„ì„.")
                .category(AddonCategory.FACTCHECK)
                .invokeType(AddonInvokeType.HTTP_SYNC)
                .endpointUrl(String.format("http://%s:%d/analyze", factcheckHost, factcheckPort))
                .healthCheckUrl(String.format("http://%s:%d/health", factcheckHost, factcheckPort))
                .authType(AddonAuthType.NONE)
                .inputSchemaVersion("1.0")
                .outputSchemaVersion("1.0")
                .timeoutMs(60000) // Factcheck may take longer due to cross-reference
                .maxQps(10)
                .maxRetries(2)
                .enabled(true)
                .priority(20)
                .healthStatus(AddonHealthStatus.UNKNOWN)
                .owner("system")
                .config(Map.of(
                    "models", List.of("koelectra", "sentence-transformers", "klue-bert"),
                    "language", "ko",
                    "extract_claims", true,
                    "cross_reference", true,
                    "min_claim_confidence", 0.6
                ))
                .build(),

            // ========== Bias Analysis Add-on ==========
            MlAddon.builder()
                .addonKey("bias-v1")
                .name("í¸í–¥ë„ ë¶„ì„")
                .description("ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì •ì¹˜ì /ì´ë…ì  í¸í–¥ë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. " +
                        "ì¶œì²˜ ì‹ ë¢°ë„, ì–¸ì–´ íŒ¨í„´, í”„ë ˆì´ë° ë¶„ì„ì„ í†µí•œ ì¢…í•© í¸í–¥ ì ìˆ˜ ì œê³µ.")
                .category(AddonCategory.BIAS)
                .invokeType(AddonInvokeType.HTTP_SYNC)
                .endpointUrl(String.format("http://%s:%d/analyze", biasHost, biasPort))
                .healthCheckUrl(String.format("http://%s:%d/health", biasHost, biasPort))
                .authType(AddonAuthType.NONE)
                .inputSchemaVersion("1.0")
                .outputSchemaVersion("1.0")
                .timeoutMs(30000)
                .maxQps(15)
                .maxRetries(3)
                .enabled(true)
                .priority(30)
                .healthStatus(AddonHealthStatus.UNKNOWN)
                .owner("system")
                .config(Map.of(
                    "model", "bias-detector-ko",
                    "language", "ko",
                    "analyze_source", true,
                    "analyze_language", true,
                    "analyze_framing", true,
                    "political_spectrum", true
                ))
                .build(),

            // ========== Source Quality Add-on ==========
            MlAddon.builder()
                .addonKey("source-quality-v1")
                .name("ì¶œì²˜ ì‹ ë¢°ë„ ë¶„ì„")
                .description("ë‰´ìŠ¤ ì¶œì²˜ì˜ ì‹ ë¢°ë„ì™€ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤. " +
                        "ë¯¸ë””ì–´ ì¶œì²˜ ë°ì´í„°ë² ì´ìŠ¤ì™€ ì—­ì‚¬ì  ì •í™•ë„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„.")
                .category(AddonCategory.SOURCE_QUALITY)
                .invokeType(AddonInvokeType.HTTP_SYNC)
                .endpointUrl(String.format("http://%s:%d/analyze/source", biasHost, biasPort))
                .healthCheckUrl(String.format("http://%s:%d/health", biasHost, biasPort))
                .authType(AddonAuthType.NONE)
                .inputSchemaVersion("1.0")
                .outputSchemaVersion("1.0")
                .timeoutMs(15000)
                .maxQps(30)
                .maxRetries(2)
                .enabled(true)
                .priority(5) // Run early as other addons may depend on source info
                .healthStatus(AddonHealthStatus.UNKNOWN)
                .owner("system")
                .config(Map.of(
                    "include_history", true,
                    "check_domain_reputation", true,
                    "check_author", false
                ))
                .build(),

            // ========== Topic Classification Add-on ==========
            MlAddon.builder()
                .addonKey("topic-classifier-v1")
                .name("ì£¼ì œ ë¶„ë¥˜")
                .description("ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì •ì¹˜, ê²½ì œ, ì‚¬íšŒ, ë¬¸í™”, IT ë“±ì˜ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.")
                .category(AddonCategory.TOPIC_CLASSIFICATION)
                .invokeType(AddonInvokeType.HTTP_SYNC)
                .endpointUrl(String.format("http://%s:%d/analyze/topic", sentimentHost, sentimentPort))
                .healthCheckUrl(String.format("http://%s:%d/health", sentimentHost, sentimentPort))
                .authType(AddonAuthType.NONE)
                .inputSchemaVersion("1.0")
                .outputSchemaVersion("1.0")
                .timeoutMs(20000)
                .maxQps(25)
                .maxRetries(3)
                .enabled(false) // Disabled by default, enable when model is ready
                .priority(15)
                .healthStatus(AddonHealthStatus.UNKNOWN)
                .owner("system")
                .config(Map.of(
                    "model", "klue-ynat",
                    "language", "ko",
                    "categories", List.of("ì •ì¹˜", "ê²½ì œ", "ì‚¬íšŒ", "ë¬¸í™”", "ì„¸ê³„", "IT/ê³¼í•™", "ìŠ¤í¬ì¸ ", "ì—°ì˜ˆ"),
                    "multi_label", true
                ))
                .build(),

            // ========== Entity Extraction (NER) Add-on ==========
            MlAddon.builder()
                .addonKey("ner-v1")
                .name("ê°œì²´ëª… ì¸ì‹ (NER)")
                .description("ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ì¸ë¬¼, ì¡°ì§, ì¥ì†Œ, ë‚ ì§œ ë“±ì˜ ê°œì²´ëª…ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.")
                .category(AddonCategory.NER)
                .invokeType(AddonInvokeType.HTTP_SYNC)
                .endpointUrl(String.format("http://%s:%d/analyze/ner", factcheckHost, factcheckPort))
                .healthCheckUrl(String.format("http://%s:%d/health", factcheckHost, factcheckPort))
                .authType(AddonAuthType.NONE)
                .inputSchemaVersion("1.0")
                .outputSchemaVersion("1.0")
                .timeoutMs(25000)
                .maxQps(20)
                .maxRetries(3)
                .enabled(false) // Disabled by default
                .priority(8)
                .healthStatus(AddonHealthStatus.UNKNOWN)
                .owner("system")
                .config(Map.of(
                    "model", "klue-ner",
                    "language", "ko",
                    "entity_types", List.of("PERSON", "ORGANIZATION", "LOCATION", "DATE", "QUANTITY"),
                    "link_entities", true
                ))
                .build()
        );
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/RedisCacheConfig.java

```java
package com.newsinsight.collector.config;

import com.fasterxml.jackson.annotation.JsonTypeInfo;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.jsontype.impl.LaissezFaireSubTypeValidator;
import com.fasterxml.jackson.datatype.jsr310.JavaTimeModule;
import io.micrometer.core.instrument.MeterRegistry;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.cache.Cache;
import org.springframework.cache.CacheManager;
import org.springframework.cache.annotation.CachingConfigurer;
import org.springframework.cache.annotation.EnableCaching;
import org.springframework.cache.caffeine.CaffeineCacheManager;
import org.springframework.cache.interceptor.CacheErrorHandler;
import org.springframework.cache.support.CompositeCacheManager;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.Primary;
import org.springframework.data.redis.cache.RedisCacheConfiguration;
import org.springframework.data.redis.cache.RedisCacheManager;
import org.springframework.data.redis.connection.RedisConnectionFactory;
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.data.redis.serializer.GenericJackson2JsonRedisSerializer;
import org.springframework.data.redis.serializer.RedisSerializationContext;
import org.springframework.data.redis.serializer.StringRedisSerializer;

import java.time.Duration;
import java.util.Arrays;
import java.util.HashMap;
import java.util.Map;

/**
 * Redis ìºì‹± ì„¤ì •
 * 
 * íŒ©íŠ¸ì²´í¬ ì±—ë´‡ ì„¸ì…˜ì„ Redisì— ìºì‹±í•˜ì—¬ ë¹ ë¥¸ ì¡°íšŒë¥¼ ì§€ì›í•©ë‹ˆë‹¤.
 * 
 * ê°œì„ ì‚¬í•­:
 * - ìºì‹œ í‚¤ prefix ì¶”ê°€ (ì¶©ëŒ ë°©ì§€)
 * - ë¡œì»¬ ìºì‹œ í´ë°± (Caffeine)
 * - ìºì‹œ ì—ëŸ¬ í•¸ë“¤ëŸ¬
 * - ìºì‹œ í†µê³„ ë©”íŠ¸ë¦­
 * - ë‹¤ì–‘í•œ ìºì‹œ í”„ë¡œíŒŒì¼
 */
@Configuration
@EnableCaching
@Slf4j
public class RedisCacheConfig implements CachingConfigurer {

    @Value("${spring.application.name:newsinsight}")
    private String applicationName;

    @Value("${spring.data.redis.enabled:true}")
    private boolean redisEnabled;

    // ìºì‹œ TTL ì„¤ì •
    @Value("${cache.chat-sessions.ttl-hours:2}")
    private int chatSessionsTtlHours;

    @Value("${cache.chat-messages.ttl-minutes:30}")
    private int chatMessagesTtlMinutes;

    @Value("${cache.default.ttl-hours:24}")
    private int defaultTtlHours;

    // ë¡œì»¬ ìºì‹œ ì„¤ì •
    @Value("${cache.local.max-size:1000}")
    private int localCacheMaxSize;

    @Value("${cache.local.ttl-minutes:10}")
    private int localCacheTtlMinutes;

    private final MeterRegistry meterRegistry;

    public RedisCacheConfig(MeterRegistry meterRegistry) {
        this.meterRegistry = meterRegistry;
    }

    /**
     * ObjectMapper ì„¤ì • (íƒ€ì… ì •ë³´ í¬í•¨)
     */
    private ObjectMapper createCacheObjectMapper() {
        ObjectMapper mapper = new ObjectMapper();
        mapper.registerModule(new JavaTimeModule());
        mapper.activateDefaultTyping(
                LaissezFaireSubTypeValidator.instance,
                ObjectMapper.DefaultTyping.NON_FINAL,
                JsonTypeInfo.As.PROPERTY
        );
        return mapper;
    }

    /**
     * Redis ìºì‹œ ë§¤ë‹ˆì € ì„¤ì •
     */
    @Bean
    public RedisCacheManager redisCacheManager(RedisConnectionFactory connectionFactory) {
        // ìºì‹œ í‚¤ prefix ì„¤ì •
        String keyPrefix = applicationName + ":cache:";

        // ê¸°ë³¸ ìºì‹œ ì„¤ì •
        RedisCacheConfiguration defaultConfig = RedisCacheConfiguration.defaultCacheConfig()
                .entryTtl(Duration.ofHours(defaultTtlHours))
                .prefixCacheNameWith(keyPrefix)
                .serializeKeysWith(
                        RedisSerializationContext.SerializationPair.fromSerializer(
                                new StringRedisSerializer()
                        )
                )
                .serializeValuesWith(
                        RedisSerializationContext.SerializationPair.fromSerializer(
                                new GenericJackson2JsonRedisSerializer(createCacheObjectMapper())
                        )
                )
                // null ê°’ ìºì‹± ë¹„í™œì„±í™”
                .disableCachingNullValues();

        // ìºì‹œë³„ ì„¤ì •
        Map<String, RedisCacheConfiguration> cacheConfigurations = new HashMap<>();
        
        // ì±„íŒ… ì„¸ì…˜ ìºì‹œ: 2ì‹œê°„
        cacheConfigurations.put("chatSessions", 
                defaultConfig.entryTtl(Duration.ofHours(chatSessionsTtlHours)));
        
        // ì±„íŒ… ë©”ì‹œì§€ ìºì‹œ: 30ë¶„
        cacheConfigurations.put("chatMessages", 
                defaultConfig.entryTtl(Duration.ofMinutes(chatMessagesTtlMinutes)));
        
        // ì‚¬ìš©ì ì„¸ì…˜ ëª©ë¡ ìºì‹œ: 1ì‹œê°„
        cacheConfigurations.put("userSessions", 
                defaultConfig.entryTtl(Duration.ofHours(1)));
        
        // íŒ©íŠ¸ì²´í¬ ê²°ê³¼ ìºì‹œ: 6ì‹œê°„
        cacheConfigurations.put("factCheckResults", 
                defaultConfig.entryTtl(Duration.ofHours(6)));
        
        // ìœ ì‚¬ ì§ˆë¬¸ ê²€ìƒ‰ ìºì‹œ: 1ì‹œê°„
        cacheConfigurations.put("similarQuestions", 
                defaultConfig.entryTtl(Duration.ofHours(1)));
        
        // ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ: 5ë¶„ (ìì£¼ ì—…ë°ì´íŠ¸ë˜ëŠ” ë°ì´í„°)
        cacheConfigurations.put("searchResults", 
                defaultConfig.entryTtl(Duration.ofMinutes(5)));
        
        // DB ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ: 10ë¶„
        cacheConfigurations.put("dbSearchResults", 
                defaultConfig.entryTtl(Duration.ofMinutes(10)));

        RedisCacheManager cacheManager = RedisCacheManager.builder(connectionFactory)
                .cacheDefaults(defaultConfig)
                .withInitialCacheConfigurations(cacheConfigurations)
                .enableStatistics() // í†µê³„ í™œì„±í™”
                .build();

        log.info("Redis Cache Manager initialized with prefix: {}", keyPrefix);
        return cacheManager;
    }

    /**
     * ë¡œì»¬ ìºì‹œ ë§¤ë‹ˆì € (Caffeine) - í´ë°±ìš©
     */
    @Bean
    public CaffeineCacheManager caffeineCacheManager() {
        CaffeineCacheManager cacheManager = new CaffeineCacheManager();
        cacheManager.setCaffeine(
                com.github.benmanes.caffeine.cache.Caffeine.newBuilder()
                        .maximumSize(localCacheMaxSize)
                        .expireAfterWrite(Duration.ofMinutes(localCacheTtlMinutes))
                        .recordStats() // í†µê³„ í™œì„±í™”
        );
        cacheManager.setCacheNames(Arrays.asList(
                "chatSessions", 
                "chatMessages", 
                "userSessions",
                "factCheckResults",
                "similarQuestions",
                "searchResults",
                "dbSearchResults"
        ));

        log.info("Caffeine Cache Manager initialized (fallback)");
        return cacheManager;
    }

    /**
     * ë³µí•© ìºì‹œ ë§¤ë‹ˆì € (Redis ìš°ì„ , Caffeine í´ë°±)
     */
    @Bean
    @Primary
    @Override
    public CacheManager cacheManager() {
        CompositeCacheManager compositeCacheManager = new CompositeCacheManager();
        
        // Redisê°€ í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ Redis ìš°ì„  ì‚¬ìš©
        // ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ Caffeineë§Œ ì‚¬ìš©
        if (redisEnabled) {
            log.info("Using Redis as primary cache with Caffeine fallback");
        } else {
            log.info("Redis disabled, using Caffeine as primary cache");
        }
        
        compositeCacheManager.setFallbackToNoOpCache(false);
        return compositeCacheManager;
    }

    /**
     * Redis ìºì‹œ ë§¤ë‹ˆì €ë¥¼ Primaryë¡œ ì§ì ‘ ë°˜í™˜
     */
    @Bean("primaryCacheManager")
    public CacheManager primaryCacheManager(RedisConnectionFactory connectionFactory) {
        return redisCacheManager(connectionFactory);
    }

    /**
     * RedisTemplate ì„¤ì •
     */
    @Bean
    public RedisTemplate<String, Object> redisTemplate(RedisConnectionFactory connectionFactory) {
        RedisTemplate<String, Object> template = new RedisTemplate<>();
        template.setConnectionFactory(connectionFactory);
        
        // í‚¤ëŠ” Stringìœ¼ë¡œ ì§ë ¬í™”
        template.setKeySerializer(new StringRedisSerializer());
        template.setHashKeySerializer(new StringRedisSerializer());
        
        // ê°’ì€ JSONìœ¼ë¡œ ì§ë ¬í™”
        GenericJackson2JsonRedisSerializer jsonSerializer = 
                new GenericJackson2JsonRedisSerializer(createCacheObjectMapper());
        template.setValueSerializer(jsonSerializer);
        template.setHashValueSerializer(jsonSerializer);
        
        template.afterPropertiesSet();
        return template;
    }

    /**
     * ìºì‹œ ì—ëŸ¬ í•¸ë“¤ëŸ¬ - Redis ì¥ì•  ì‹œ ë¡œê¹…ë§Œ í•˜ê³  ê³„ì† ì§„í–‰
     */
    @Override
    public CacheErrorHandler errorHandler() {
        return new CacheErrorHandler() {
            @Override
            public void handleCacheGetError(RuntimeException exception, Cache cache, Object key) {
                log.warn("Cache GET error - cache: {}, key: {}, error: {}", 
                        cache.getName(), key, exception.getMessage());
                // ë©”íŠ¸ë¦­ ê¸°ë¡
                meterRegistry.counter("cache.error", 
                        "cache", cache.getName(), 
                        "operation", "get").increment();
            }

            @Override
            public void handleCachePutError(RuntimeException exception, Cache cache, Object key, Object value) {
                log.warn("Cache PUT error - cache: {}, key: {}, error: {}", 
                        cache.getName(), key, exception.getMessage());
                meterRegistry.counter("cache.error", 
                        "cache", cache.getName(), 
                        "operation", "put").increment();
            }

            @Override
            public void handleCacheEvictError(RuntimeException exception, Cache cache, Object key) {
                log.warn("Cache EVICT error - cache: {}, key: {}, error: {}", 
                        cache.getName(), key, exception.getMessage());
                meterRegistry.counter("cache.error", 
                        "cache", cache.getName(), 
                        "operation", "evict").increment();
            }

            @Override
            public void handleCacheClearError(RuntimeException exception, Cache cache) {
                log.warn("Cache CLEAR error - cache: {}, error: {}", 
                        cache.getName(), exception.getMessage());
                meterRegistry.counter("cache.error", 
                        "cache", cache.getName(), 
                        "operation", "clear").increment();
            }
        };
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/SecurityConfig.java

```java
package com.newsinsight.collector.config;

import io.jsonwebtoken.Claims;
import io.jsonwebtoken.Jwts;
import io.jsonwebtoken.security.Keys;
import jakarta.servlet.FilterChain;
import jakarta.servlet.ServletException;
import jakarta.servlet.http.HttpServletRequest;
import jakarta.servlet.http.HttpServletResponse;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.core.annotation.Order;
import org.springframework.http.HttpMethod;
import org.springframework.security.authentication.UsernamePasswordAuthenticationToken;
import org.springframework.security.config.annotation.method.configuration.EnableMethodSecurity;
import org.springframework.security.config.annotation.web.builders.HttpSecurity;
import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;
import org.springframework.security.config.annotation.web.configurers.AbstractHttpConfigurer;
import org.springframework.security.config.http.SessionCreationPolicy;
import org.springframework.security.core.authority.SimpleGrantedAuthority;
import org.springframework.security.core.context.SecurityContextHolder;
import org.springframework.security.web.SecurityFilterChain;
import org.springframework.security.web.authentication.UsernamePasswordAuthenticationFilter;
import org.springframework.security.web.authentication.WebAuthenticationDetailsSource;
import org.springframework.stereotype.Component;
import org.springframework.web.cors.CorsConfiguration;
import org.springframework.web.cors.CorsConfigurationSource;
import org.springframework.web.cors.UrlBasedCorsConfigurationSource;
import org.springframework.web.filter.OncePerRequestFilter;

import javax.crypto.SecretKey;
import java.io.IOException;
import java.nio.charset.StandardCharsets;
import java.util.Arrays;
import java.util.Collections;
import java.util.List;

/**
 * Security Configuration for Data Collection Service
 * 
 * ì—”ë“œí¬ì¸íŠ¸ë³„ ì¸ì¦/ì¸ê°€ ì •ì±…:
 * - Public: í—¬ìŠ¤ì²´í¬, Swagger, actuator
 * - Authenticated: ì¼ë°˜ API (search, data, analysis, reports ë“±)
 * - Admin Only: workspace/admin/*, ê´€ë¦¬ ì—”ë“œí¬ì¸íŠ¸
 */
@Configuration
@EnableWebSecurity
@EnableMethodSecurity(prePostEnabled = true)
public class SecurityConfig {

    private static final Logger log = LoggerFactory.getLogger(SecurityConfig.class);

    @Value("${security.jwt.secret:${ADMIN_SECRET_KEY:your-secret-key-change-in-production}}")
    private String jwtSecret;

    @Value("${security.enabled:true}")
    private boolean securityEnabled;

    @Value("${security.cors.enabled:true}")
    private boolean corsEnabled;

    @Bean
    public SecurityFilterChain securityFilterChain(HttpSecurity http, JwtAuthenticationFilter jwtFilter) throws Exception {
        if (!securityEnabled) {
            log.warn("Security is DISABLED. All endpoints are publicly accessible. DO NOT USE IN PRODUCTION!");
            var httpConfig = http
                    .csrf(AbstractHttpConfigurer::disable);
            
            // Only add CORS if enabled (disable when behind API Gateway to avoid duplicate headers)
            if (corsEnabled) {
                httpConfig.cors(cors -> cors.configurationSource(corsConfigurationSource()));
            } else {
                log.info("CORS is DISABLED on this service (handled by API Gateway)");
                httpConfig.cors(AbstractHttpConfigurer::disable);
            }
            
            return httpConfig
                    .authorizeHttpRequests(auth -> auth.anyRequest().permitAll())
                    .build();
        }

        var httpConfig = http
                .csrf(AbstractHttpConfigurer::disable);
        
        // Only add CORS if enabled (disable when behind API Gateway to avoid duplicate headers)
        if (corsEnabled) {
            httpConfig.cors(cors -> cors.configurationSource(corsConfigurationSource()));
        } else {
            log.info("CORS is DISABLED on this service (handled by API Gateway)");
            httpConfig.cors(AbstractHttpConfigurer::disable);
        }

        return httpConfig
                .sessionManagement(session -> session.sessionCreationPolicy(SessionCreationPolicy.STATELESS))
                .authorizeHttpRequests(auth -> auth
                        // ========================================
                        // Public Endpoints (ì¸ì¦ ë¶ˆí•„ìš”)
                        // ========================================
                        .requestMatchers("/actuator/**").permitAll()
                        .requestMatchers("/swagger-ui/**", "/v3/api-docs/**", "/swagger-resources/**").permitAll()
                        .requestMatchers(HttpMethod.OPTIONS, "/**").permitAll()
                        
                        // Health Check Endpoints (ê° ì„œë¹„ìŠ¤ë³„)
                        .requestMatchers("/api/v1/search/health").permitAll()
                        .requestMatchers("/api/v1/jobs/health").permitAll()
                        .requestMatchers("/api/v1/search-history/health").permitAll()
                        .requestMatchers("/api/v1/search-templates/health").permitAll()
                        .requestMatchers("/api/v1/projects/health").permitAll()
                        .requestMatchers("/api/v1/ai/health").permitAll()
                        .requestMatchers("/api/v1/analysis/deep/health").permitAll()
                        .requestMatchers("/api/v1/analysis/live/health").permitAll()
                        .requestMatchers("/api/v1/analysis/extract-claims/health").permitAll()
                        .requestMatchers("/api/v1/factcheck-chat/health").permitAll()
                        .requestMatchers("/api/v1/factcheck-chat/health/**").permitAll()
                        .requestMatchers("/api/v1/workspace/files/health").permitAll()
                        .requestMatchers("/api/v1/ml/status").permitAll()
                        
                        // ========================================
                        // SSE Endpoints (EventSourceëŠ” Authorization í—¤ë” ë¯¸ì§€ì›)
                        // ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ì€ ë³„ë„ ì¸ì¦ ì—†ì´ í—ˆìš©
                        // ========================================
                        .requestMatchers("/api/v1/jobs/stream").permitAll()
                        .requestMatchers("/api/v1/jobs/*/stream").permitAll()
                        .requestMatchers("/api/v1/events/stream").permitAll()
                        .requestMatchers("/api/v1/events/stats/stream").permitAll()
                        .requestMatchers("/api/v1/search/stream").permitAll()
                        .requestMatchers("/api/v1/search/deep/stream").permitAll()
                        .requestMatchers("/api/v1/search/jobs/*/stream").permitAll()
                        .requestMatchers("/api/v1/search/analysis/stream").permitAll()
                        .requestMatchers("/api/v1/search/analysis/stream/status").permitAll()
                        .requestMatchers("/api/v1/analysis/deep/*/stream").permitAll()
                        .requestMatchers("/api/v1/analysis/live").permitAll()
                        .requestMatchers("/api/v1/search-history/stream").permitAll()
                        // Factcheck Chat - ìµëª… ì„¸ì…˜ë„ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ì „ì²´ í—ˆìš©
                        .requestMatchers(HttpMethod.POST, "/api/v1/factcheck-chat/session").permitAll()
                        .requestMatchers(HttpMethod.GET, "/api/v1/factcheck-chat/session/**").permitAll()
                        .requestMatchers(HttpMethod.POST, "/api/v1/factcheck-chat/session/**").permitAll()
                        .requestMatchers("/api/v1/factcheck-chat/**").permitAll()
                        
                        // ========================================
                        // Public Report Export (ìµëª… ì„¸ì…˜ì—ì„œë„ PDF ë‹¤ìš´ë¡œë“œ í—ˆìš©)
                        // /api/v1/reports/unified-search/{jobId}/export ë“±
                        // ========================================
                        .requestMatchers("/api/v1/reports/*/export").permitAll()
                        .requestMatchers("/api/v1/reports/unified-search/**").permitAll()
                        .requestMatchers("/api/v1/reports/deep-search/**").permitAll()
                        .requestMatchers("/api/v1/reports/download/**").permitAll()
                        
                        // ========================================
                        // Admin Only Endpoints (ADMIN ê¶Œí•œ í•„ìš”)
                        // ========================================
                        .requestMatchers("/api/v1/workspace/admin/**").hasRole("ADMIN")
                        .requestMatchers("/api/v1/workspace/files/admin/**").hasRole("ADMIN")
                        .requestMatchers("/api/v1/admin/llm-providers/**").hasRole("ADMIN")
                        .requestMatchers("/api/v1/llm-providers/types").permitAll()
                        .requestMatchers("/api/v1/llm-providers/**").hasAnyRole("ADMIN", "OPERATOR")
                        
                        // ========================================
                        // Authenticated Endpoints (ë¡œê·¸ì¸ í•„ìš” X)
                        // ========================================
                        // Search API
                        .requestMatchers("/api/v1/search/**").permitAll()
                        .requestMatchers("/api/v1/search-history/**").authenticated()
                        .requestMatchers("/api/v1/search-templates/**").authenticated()
                        
                        // Data & Collections
                        .requestMatchers("/api/v1/data/**").permitAll()
                        .requestMatchers("/api/v1/collections/**").permitAll()
                        .requestMatchers("/api/v1/sources/**").permitAll()
                        
                        // Analysis & AI
                        .requestMatchers("/api/v1/analysis/**").permitAll()
                        .requestMatchers("/api/v1/ai/**").permitAll()
                        .requestMatchers("/api/v1/ml/**").permitAll()
                        
                        // Reports
                        .requestMatchers("/api/v1/reports/**").permitAll()
                        
                        // Projects & Workspace (ì¼ë°˜)
                        .requestMatchers("/api/v1/projects/**").permitAll()
                        .requestMatchers("/api/v1/workspace/**").permitAll()
                        
                        // Articles
                        .requestMatchers("/api/v1/articles/**").permitAll()
                        
                        // Jobs & AutoCrawl
                        .requestMatchers("/api/v1/jobs/**").permitAll()
                        .requestMatchers("/api/v1/autocrawl/**").permitAll()
                        
                        // Events (SSE)
                        .requestMatchers("/api/v1/events/**").permitAll()
                        
                        // Config (read-only for authenticated users)
                        .requestMatchers(HttpMethod.GET, "/api/v1/config/**").permitAll()
                        .requestMatchers("/api/v1/config/**").hasAnyRole("ADMIN", "OPERATOR")
                        
                        // Default: require authentication
                        .anyRequest().authenticated()
                )
                .addFilterBefore(jwtFilter, UsernamePasswordAuthenticationFilter.class)
                .build();
    }

    @Bean
    public CorsConfigurationSource corsConfigurationSource() {
        CorsConfiguration configuration = new CorsConfiguration();
        configuration.setAllowedOriginPatterns(List.of("*"));
        configuration.setAllowedMethods(Arrays.asList("GET", "POST", "PUT", "DELETE", "PATCH", "OPTIONS", "HEAD"));
        configuration.setAllowedHeaders(List.of("*"));
        configuration.setAllowCredentials(true);
        configuration.setMaxAge(3600L);

        UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource();
        source.registerCorsConfiguration("/**", configuration);
        return source;
    }

    /**
     * JWT Authentication Filter
     * Authorization í—¤ë”ì—ì„œ Bearer í† í°ì„ ì¶”ì¶œí•˜ê³  ê²€ì¦í•©ë‹ˆë‹¤.
     */
    @Component
    public static class JwtAuthenticationFilter extends OncePerRequestFilter {

        private static final Logger log = LoggerFactory.getLogger(JwtAuthenticationFilter.class);

        @Value("${security.jwt.secret:${ADMIN_SECRET_KEY:your-secret-key-change-in-production}}")
        private String jwtSecret;

        @Value("${security.enabled:true}")
        private boolean securityEnabled;

        @Override
        protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain)
                throws ServletException, IOException {

            if (!securityEnabled) {
                filterChain.doFilter(request, response);
                return;
            }

            // Try to extract token from multiple sources
            String token = extractToken(request);

            if (token == null) {
                filterChain.doFilter(request, response);
                return;
            }

            try {
                SecretKey key = Keys.hmacShaKeyFor(jwtSecret.getBytes(StandardCharsets.UTF_8));

                Claims claims = Jwts.parser()
                        .verifyWith(key)
                        .build()
                        .parseSignedClaims(token)
                        .getPayload();

                String userId = claims.getSubject();
                String username = claims.get("username", String.class);
                String role = claims.get("role", String.class);

                if (userId != null && SecurityContextHolder.getContext().getAuthentication() == null) {
                    List<SimpleGrantedAuthority> authorities;
                    
                    if (role != null) {
                        // roleì´ "admin", "operator", "user" í˜•íƒœë¡œ ì˜¬ ìˆ˜ ìˆìŒ
                        String normalizedRole = role.toUpperCase();
                        authorities = List.of(new SimpleGrantedAuthority("ROLE_" + normalizedRole));
                    } else {
                        authorities = List.of(new SimpleGrantedAuthority("ROLE_USER"));
                    }

                    UsernamePasswordAuthenticationToken authentication = new UsernamePasswordAuthenticationToken(
                            userId,
                            null,
                            authorities
                    );
                    authentication.setDetails(new WebAuthenticationDetailsSource().buildDetails(request));

                    SecurityContextHolder.getContext().setAuthentication(authentication);
                    
                    log.debug("JWT authenticated: user={}, role={}", username, role);
                }

            } catch (Exception e) {
                log.warn("JWT authentication failed: {}", e.getMessage());
                // ì¸ì¦ ì‹¤íŒ¨ ì‹œ SecurityContextë¥¼ ë¹„ì›Œë‘ê³  ê³„ì† ì§„í–‰
                // Spring Securityê°€ ì¸ì¦ë˜ì§€ ì•Šì€ ìš”ì²­ìœ¼ë¡œ ì²˜ë¦¬í•¨
            }

            filterChain.doFilter(request, response);
        }

        /**
         * Extract JWT token from multiple sources:
         * 1. Authorization header (Bearer token)
         * 2. Query parameter (token)
         * 3. Cookie (access_token)
         * 
         * This allows SSE/EventSource connections to be authenticated
         * since EventSource doesn't support custom headers.
         */
        private String extractToken(HttpServletRequest request) {
            // 1. Check Authorization header first (standard method)
            String authHeader = request.getHeader("Authorization");
            if (authHeader != null && authHeader.startsWith("Bearer ")) {
                return authHeader.substring(7);
            }

            // 2. Check query parameter (for SSE/EventSource)
            String tokenParam = request.getParameter("token");
            if (tokenParam != null && !tokenParam.isBlank()) {
                return tokenParam;
            }

            // 3. Check cookie (for SSE/EventSource)
            if (request.getCookies() != null) {
                for (jakarta.servlet.http.Cookie cookie : request.getCookies()) {
                    if ("access_token".equals(cookie.getName())) {
                        String cookieValue = cookie.getValue();
                        if (cookieValue != null && !cookieValue.isBlank()) {
                            return cookieValue;
                        }
                    }
                }
            }

            return null;
        }

        @Override
        protected boolean shouldNotFilter(HttpServletRequest request) {
            String path = request.getServletPath();
            // Public ì—”ë“œí¬ì¸íŠ¸ëŠ” í•„í„° ìŠ¤í‚µ (SSE ìŠ¤íŠ¸ë¦¼ì€ í† í° ì¸ì¦ì„ ìœ„í•´ í•„í„° í†µê³¼)
            return path.startsWith("/actuator") ||
                   path.startsWith("/swagger-ui") ||
                   path.startsWith("/v3/api-docs") ||
                   path.endsWith("/health") ||
                   path.contains("/health/");
            // SSE ìŠ¤íŠ¸ë¦¼ ì—”ë“œí¬ì¸íŠ¸ëŠ” ì´ì œ í•„í„°ë¥¼ í†µê³¼í•˜ì—¬ í† í° ì¸ì¦ ê°€ëŠ¥
        }
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/TrustScoreConfig.java

```java
package com.newsinsight.collector.config;

import lombok.Data;
import org.springframework.boot.context.properties.ConfigurationProperties;
import org.springframework.context.annotation.Configuration;

import java.util.HashMap;
import java.util.Map;

/**
 * Configuration for externalized trust scores.
 * 
 * Trust scores range from 0.0 to 1.0 where:
 * - 0.95+ : Very high trust (academic papers, official statistics)
 * - 0.90-0.94: High trust (encyclopedias, established fact-checkers)
 * - 0.80-0.89: Good trust (reputable news fact-check)
 * - 0.60-0.79: Moderate trust (community wikis, user-generated)
 * - 0.50 : Base trust (unknown sources)
 * - < 0.50: Low trust (unverified, suspicious)
 * 
 * Hierarchy: Academic > Official Statistics > Encyclopedia > News Fact Check
 */
@Configuration
@ConfigurationProperties(prefix = "collector.trust-scores")
@Data
public class TrustScoreConfig {

    /**
     * Trust scores for fact-check sources
     */
    private FactCheckSources factCheck = new FactCheckSources();

    /**
     * Trust scores for trusted reference sources (FactVerificationService)
     */
    private TrustedSources trusted = new TrustedSources();

    /**
     * Trust scores for collected data quality assessment
     */
    private DataQuality dataQuality = new DataQuality();

    /**
     * Additional custom source scores (can be configured dynamically)
     */
    private Map<String, Double> custom = new HashMap<>();

    @Data
    public static class FactCheckSources {
        /** CrossRef academic papers - highest trust */
        private double crossref = 0.95;
        
        /** OpenAlex academic database */
        private double openalex = 0.92;
        
        /** Wikipedia encyclopedia */
        private double wikipedia = 0.90;
        
        /** Google Fact Check verified results */
        private double googleFactCheck = 0.85;
    }

    @Data
    public static class TrustedSources {
        /** Korean Wikipedia */
        private double wikipediaKo = 0.90;
        
        /** English Wikipedia */
        private double wikipediaEn = 0.90;
        
        /** Britannica encyclopedia - very high trust */
        private double britannica = 0.95;
        
        /** Namu Wiki (community wiki - moderate trust) */
        private double namuWiki = 0.60;
        
        /** KOSIS Korean Statistics - official government data */
        private double kosis = 0.95;
        
        /** Google Scholar - academic search */
        private double googleScholar = 0.85;
    }

    @Data
    public static class DataQuality {
        /** Base score for unknown/unverified sources */
        private double baseScore = 0.50;
        
        /** Score for sources in domain whitelist */
        private double whitelistScore = 0.90;
        
        /** Bonus for successful HTTP connection */
        private double httpOkBonus = 0.10;
    }

    /**
     * Get trust score for a source by its key.
     * Falls back to custom map, then to base score.
     */
    public double getScoreForSource(String sourceKey) {
        if (sourceKey == null) return dataQuality.baseScore;
        
        String key = sourceKey.toLowerCase().replace("-", "_").replace(" ", "_");
        
        // Check fact-check sources
        if (key.contains("crossref")) return factCheck.crossref;
        if (key.contains("openalex")) return factCheck.openalex;
        if (key.contains("wikipedia")) {
            if (key.contains("en")) return trusted.wikipediaEn;
            if (key.contains("ko")) return trusted.wikipediaKo;
            return factCheck.wikipedia;
        }
        if (key.contains("google") && key.contains("fact")) return factCheck.googleFactCheck;
        
        // Check trusted sources
        if (key.contains("britannica")) return trusted.britannica;
        if (key.contains("namu")) return trusted.namuWiki;
        if (key.contains("kosis")) return trusted.kosis;
        if (key.contains("scholar")) return trusted.googleScholar;
        
        // Check custom sources
        if (custom.containsKey(key)) return custom.get(key);
        
        // Default
        return dataQuality.baseScore;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/WebClientConfig.java

```java
package com.newsinsight.collector.config;

import io.netty.channel.ChannelOption;
import io.netty.handler.timeout.ReadTimeoutHandler;
import io.netty.handler.timeout.WriteTimeoutHandler;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.http.client.SimpleClientHttpRequestFactory;
import org.springframework.http.client.reactive.ReactorClientHttpConnector;
import org.springframework.web.client.RestTemplate;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.netty.http.client.HttpClient;

import java.time.Duration;
import java.util.concurrent.TimeUnit;

@Configuration
public class WebClientConfig {

    @Value("${collector.http.user-agent:NewsInsight-Collector/1.0}")
    private String userAgent;

    @Value("${collector.http.timeout.connect:10000}")
    private int connectTimeout;

    @Value("${collector.http.timeout.read:30000}")
    private int readTimeout;

    @Bean
    public WebClient webClient() {
        HttpClient httpClient = HttpClient.create()
                .option(ChannelOption.CONNECT_TIMEOUT_MILLIS, connectTimeout)
                .responseTimeout(Duration.ofMillis(readTimeout))
                .doOnConnected(conn -> 
                    conn.addHandlerLast(new ReadTimeoutHandler(readTimeout, TimeUnit.MILLISECONDS))
                        .addHandlerLast(new WriteTimeoutHandler(readTimeout, TimeUnit.MILLISECONDS))
                )
                .followRedirect(true);

        return WebClient.builder()
                .clientConnector(new ReactorClientHttpConnector(httpClient))
                .defaultHeader("User-Agent", userAgent)
                .build();
    }

    @Bean
    public RestTemplate restTemplate() {
        SimpleClientHttpRequestFactory factory = new SimpleClientHttpRequestFactory();
        factory.setConnectTimeout(connectTimeout);
        factory.setReadTimeout(readTimeout);
        return new RestTemplate(factory);
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/AiOrchestrationController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.dto.AiJobDto;
import com.newsinsight.collector.dto.AiTaskCallbackRequest;
import com.newsinsight.collector.dto.DeepSearchRequest;
import com.newsinsight.collector.entity.ai.AiJobStatus;
import com.newsinsight.collector.entity.ai.AiProvider;
import com.newsinsight.collector.service.DeepOrchestrationService;
import jakarta.validation.Valid;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.data.domain.Page;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.List;
import java.util.Map;
import java.util.stream.Collectors;

/**
 * Controller for AI orchestration operations.
 * Provides endpoints for:
 * - Starting orchestrated AI analysis jobs
 * - Receiving callbacks from AI workers/n8n
 * - Managing job lifecycle
 */
@RestController
@RequestMapping("/api/v1/ai")
@RequiredArgsConstructor
@Slf4j
public class AiOrchestrationController {

    private final DeepOrchestrationService orchestrationService;

    @Value("${collector.ai.orchestration.callback-token:}")
    private String expectedCallbackToken;

    /**
     * Start a new orchestrated AI analysis job.
     * 
     * @param request The analysis request containing topic and optional base URL
     * @return 202 Accepted with job details
     */
    @PostMapping("/jobs")
    public ResponseEntity<AiJobDto> startAnalysis(
            @Valid @RequestBody DeepSearchRequest request,
            @RequestParam(required = false) List<String> providers
    ) {
        log.info("Starting orchestrated AI analysis for topic: {}", request.getTopic());

        List<AiProvider> providerList = null;
        if (providers != null && !providers.isEmpty()) {
            try {
                providerList = providers.stream()
                        .map(AiProvider::valueOf)
                        .collect(Collectors.toList());
            } catch (IllegalArgumentException e) {
                return ResponseEntity.badRequest()
                        .body(AiJobDto.builder()
                                .overallStatus("ERROR")
                                .errorMessage("Invalid provider: " + e.getMessage())
                                .build());
            }
        }

        AiJobDto job = orchestrationService.startDeepAnalysis(
                request.getTopic(),
                request.getBaseUrl(),
                providerList
        );

        return ResponseEntity.status(HttpStatus.ACCEPTED).body(job);
    }

    /**
     * Get the status of an AI job.
     * 
     * @param jobId The job ID
     * @return Job status details including sub-tasks
     */
    @GetMapping("/jobs/{jobId}")
    public ResponseEntity<AiJobDto> getJobStatus(@PathVariable String jobId) {
        try {
            AiJobDto job = orchestrationService.getJobStatus(jobId);
            return ResponseEntity.ok(job);
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    /**
     * List all AI jobs with optional filtering.
     * 
     * @param page Page number (0-based)
     * @param size Page size
     * @param status Optional status filter
     * @return Paginated list of jobs
     */
    @GetMapping("/jobs")
    public ResponseEntity<Page<AiJobDto>> listJobs(
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size,
            @RequestParam(required = false) String status
    ) {
        AiJobStatus statusFilter = null;
        if (status != null && !status.isBlank()) {
            try {
                statusFilter = AiJobStatus.valueOf(status.toUpperCase());
            } catch (IllegalArgumentException e) {
                log.warn("Invalid status filter: {}", status);
            }
        }

        Page<AiJobDto> jobs = orchestrationService.listJobs(page, size, statusFilter);
        return ResponseEntity.ok(jobs);
    }

    /**
     * Cancel a pending or in-progress job.
     * 
     * @param jobId The job ID to cancel
     * @return Updated job status
     */
    @PostMapping("/jobs/{jobId}/cancel")
    public ResponseEntity<AiJobDto> cancelJob(@PathVariable String jobId) {
        try {
            AiJobDto job = orchestrationService.cancelJob(jobId);
            return ResponseEntity.ok(job);
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    /**
     * Retry failed sub-tasks for a job.
     * 
     * @param jobId The job ID
     * @return Updated job status
     */
    @PostMapping("/jobs/{jobId}/retry")
    public ResponseEntity<AiJobDto> retryJob(@PathVariable String jobId) {
        try {
            AiJobDto job = orchestrationService.retryFailedTasks(jobId);
            return ResponseEntity.ok(job);
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    /**
     * Callback endpoint for AI workers/n8n to deliver results.
     * 
     * @param callbackToken Token for authentication (from header)
     * @param request The callback payload
     * @return Processing result
     */
    @PostMapping("/callback")
    public ResponseEntity<?> handleCallback(
            @RequestHeader(value = "X-Callback-Token", required = false) String callbackToken,
            @RequestBody AiTaskCallbackRequest request
    ) {
        log.info("Received AI callback: jobId={}, subTaskId={}, status={}", 
                request.jobId(), request.subTaskId(), request.status());

        try {
            // Validate callback token if configured
            if (expectedCallbackToken != null && !expectedCallbackToken.isBlank()) {
                String tokenToValidate = callbackToken != null ? callbackToken : request.callbackToken();
                if (!expectedCallbackToken.equals(tokenToValidate)) {
                    log.warn("Invalid callback token for job: {}", request.jobId());
                    return ResponseEntity.status(HttpStatus.UNAUTHORIZED)
                            .body(Map.of("error", "Invalid callback token"));
                }
            }

            orchestrationService.handleCallback(request);

            return ResponseEntity.ok(Map.of(
                    "status", "received",
                    "jobId", request.jobId(),
                    "subTaskId", request.subTaskId()
            ));

        } catch (Exception e) {
            log.error("Error processing AI callback", e);
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)
                    .body(Map.of("error", "Failed to process callback: " + e.getMessage()));
        }
    }

    /**
     * Get available AI providers.
     */
    @GetMapping("/providers")
    public ResponseEntity<List<Map<String, String>>> getProviders() {
        List<Map<String, String>> providers = java.util.Arrays.stream(AiProvider.values())
                .map(p -> Map.of(
                        "id", p.name(),
                        "workflowPath", p.getWorkflowPath(),
                        "description", p.getDescription(),
                        "external", String.valueOf(p.isExternal())
                ))
                .collect(Collectors.toList());
        
        return ResponseEntity.ok(providers);
    }

    /**
     * Health check for AI orchestration service.
     */
    @GetMapping("/health")
    public ResponseEntity<Map<String, Object>> healthCheck() {
        return ResponseEntity.ok(Map.of(
                "status", "UP",
                "service", "ai-orchestration",
                "providers", AiProvider.values().length
        ));
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/AnalysisController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.dto.AnalysisResponseDto;
import com.newsinsight.collector.dto.ArticlesResponseDto;
import com.newsinsight.collector.service.AnalysisService;
import lombok.RequiredArgsConstructor;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RequestParam;
import org.springframework.web.bind.annotation.RestController;

@RestController
@RequestMapping("/api/v1")
@RequiredArgsConstructor
public class AnalysisController {

    private final AnalysisService analysisService;

    @GetMapping("/analysis")
    public ResponseEntity<AnalysisResponseDto> getAnalysis(
            @RequestParam String query,
            @RequestParam(defaultValue = "7d") String window
    ) {
        return ResponseEntity.ok(analysisService.analyze(query, window));
    }

    @GetMapping("/articles")
    public ResponseEntity<ArticlesResponseDto> getArticles(
            @RequestParam String query,
            @RequestParam(defaultValue = "50") int limit
    ) {
        return ResponseEntity.ok(analysisService.searchArticles(query, limit));
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/AutoCrawlController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.entity.autocrawl.CrawlTarget;
import com.newsinsight.collector.entity.autocrawl.CrawlTargetStatus;
import com.newsinsight.collector.entity.autocrawl.DiscoverySource;
import com.newsinsight.collector.repository.CrawlTargetRepository;
import com.newsinsight.collector.service.autocrawl.AutoCrawlDiscoveryService;
import com.newsinsight.collector.service.autocrawl.CrawlQueueService;
import com.newsinsight.collector.scheduler.AutoCrawlScheduler;
import lombok.Builder;
import lombok.Data;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.PageRequest;
import org.springframework.data.domain.Sort;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.List;
import java.util.Map;

/**
 * ìë™ í¬ë¡¤ë§ ê´€ë¦¬ REST API.
 * 
 * URL ë°œê²¬, í ê´€ë¦¬, ìƒíƒœ ì¡°íšŒ, ìˆ˜ë™ ì œì–´ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
 */
@RestController
@RequestMapping("/api/v1/autocrawl")
@RequiredArgsConstructor
@Slf4j
public class AutoCrawlController {

    private final AutoCrawlDiscoveryService discoveryService;
    private final CrawlQueueService queueService;
    private final CrawlTargetRepository targetRepository;
    private final AutoCrawlScheduler autoCrawlScheduler;

    // ========================================
    // ìƒíƒœ ì¡°íšŒ
    // ========================================

    /**
     * í ìƒíƒœ ë° í†µê³„ ì¡°íšŒ
     */
    @GetMapping("/status")
    public ResponseEntity<AutoCrawlStatusResponse> getStatus() {
        CrawlQueueService.QueueStats stats = queueService.getQueueStats();
        Map<DiscoverySource, Long> discoveryStats = discoveryService.getDiscoveryStats();
        Map<String, Long> domainStats = queueService.getPendingCountByDomain();

        AutoCrawlStatusResponse response = AutoCrawlStatusResponse.builder()
                .pendingCount(stats.getPendingCount())
                .inProgressCount(stats.getInProgressCount())
                .completedCount(stats.getCompletedCount())
                .failedCount(stats.getFailedCount())
                .skippedCount(stats.getSkippedCount())
                .sessionDispatched(stats.getTotalDispatched())
                .sessionCompleted(stats.getTotalCompleted())
                .sessionFailed(stats.getTotalFailed())
                .discoveryStats(discoveryStats)
                .domainPendingStats(domainStats)
                .domainConcurrency(stats.getDomainConcurrency())
                .build();

        return ResponseEntity.ok(response);
    }

    /**
     * ëŒ€ê¸° ì¤‘ì¸ ëŒ€ìƒ ëª©ë¡ ì¡°íšŒ (í˜ì´ì§€ë„¤ì´ì…˜)
     */
    @GetMapping("/targets")
    public ResponseEntity<Page<CrawlTargetDto>> getTargets(
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size,
            @RequestParam(required = false) CrawlTargetStatus status,
            @RequestParam(required = false) DiscoverySource source) {

        PageRequest pageRequest = PageRequest.of(page, size, 
                Sort.by(Sort.Direction.DESC, "priority").and(Sort.by(Sort.Direction.ASC, "discoveredAt")));

        Page<CrawlTarget> targets;
        if (status != null) {
            targets = targetRepository.findByStatus(status, pageRequest);
        } else if (source != null) {
            targets = targetRepository.findByDiscoverySource(source, pageRequest);
        } else {
            targets = targetRepository.findAll(pageRequest);
        }

        Page<CrawlTargetDto> dtoPage = targets.map(this::toDto);
        return ResponseEntity.ok(dtoPage);
    }

    /**
     * ë‹¨ì¼ ëŒ€ìƒ ì¡°íšŒ
     */
    @GetMapping("/targets/{id}")
    public ResponseEntity<CrawlTargetDto> getTarget(@PathVariable Long id) {
        return targetRepository.findById(id)
                .map(this::toDto)
                .map(ResponseEntity::ok)
                .orElse(ResponseEntity.notFound().build());
    }

    // ========================================
    // URL ë°œê²¬ (ìˆ˜ë™)
    // ========================================

    /**
     * ìˆ˜ë™ìœ¼ë¡œ URL ì¶”ê°€
     */
    @PostMapping("/targets")
    public ResponseEntity<CrawlTargetDto> addTarget(@RequestBody AddTargetRequest request) {
        try {
            CrawlTarget target = discoveryService.addManualTarget(
                    request.getUrl(),
                    request.getKeywords(),
                    request.getPriority() != null ? request.getPriority() : 50
            );
            
            if (target == null) {
                return ResponseEntity.badRequest().build();
            }
            
            log.info("Manually added crawl target: url={}, priority={}", request.getUrl(), request.getPriority());
            return ResponseEntity.ok(toDto(target));
        } catch (IllegalArgumentException e) {
            return ResponseEntity.badRequest().build();
        }
    }

    /**
     * ì—¬ëŸ¬ URL ì¼ê´„ ì¶”ê°€
     */
    @PostMapping("/targets/batch")
    public ResponseEntity<BatchAddResponse> addTargetsBatch(@RequestBody BatchAddRequest request) {
        List<CrawlTarget> targets = discoveryService.addManualTargets(
                request.getUrls(),
                request.getKeywords(),
                request.getPriority() != null ? request.getPriority() : 50
        );

        BatchAddResponse response = BatchAddResponse.builder()
                .addedCount(targets.size())
                .requestedCount(request.getUrls().size())
                .build();

        log.info("Batch added {} crawl targets", targets.size());
        return ResponseEntity.ok(response);
    }

    /**
     * ê²€ìƒ‰ ê²°ê³¼ URLì—ì„œ ë°œê²¬
     */
    @PostMapping("/discover/search")
    public ResponseEntity<DiscoverResponse> discoverFromSearch(@RequestBody DiscoverSearchRequest request) {
        List<CrawlTarget> targets = discoveryService.discoverFromSearchUrls(
                request.getQuery(),
                request.getUrls()
        );

        DiscoverResponse response = DiscoverResponse.builder()
                .discoveredCount(targets.size())
                .source(DiscoverySource.SEARCH)
                .build();

        log.info("Discovered {} targets from search query: '{}'", targets.size(), request.getQuery());
        return ResponseEntity.ok(response);
    }

    // ========================================
    // í ì œì–´
    // ========================================

    /**
     * ìˆ˜ë™ìœ¼ë¡œ í ì²˜ë¦¬ íŠ¸ë¦¬ê±°
     */
    @PostMapping("/queue/process")
    public ResponseEntity<ProcessQueueResponse> processQueue(
            @RequestParam(defaultValue = "10") int batchSize) {
        int dispatched = autoCrawlScheduler.triggerQueueProcessing(batchSize);

        ProcessQueueResponse response = ProcessQueueResponse.builder()
                .dispatchedCount(dispatched)
                .batchSize(batchSize)
                .build();

        return ResponseEntity.ok(response);
    }

    /**
     * íŠ¹ì • ëŒ€ìƒ ì¦‰ì‹œ ë¶„ë°°
     */
    @PostMapping("/targets/{id}/dispatch")
    public ResponseEntity<Void> dispatchTarget(@PathVariable Long id) {
        boolean success = queueService.dispatchSingle(id);
        if (success) {
            log.info("Manually dispatched target: id={}", id);
            return ResponseEntity.ok().build();
        } else {
            return ResponseEntity.badRequest().build();
        }
    }

    /**
     * íŠ¹ì • í‚¤ì›Œë“œ ê´€ë ¨ ëŒ€ìƒ ìš°ì„ ìˆœìœ„ ë¶€ìŠ¤íŠ¸
     */
    @PostMapping("/queue/boost")
    public ResponseEntity<BoostResponse> boostKeyword(@RequestBody BoostRequest request) {
        int boosted = queueService.prioritizeKeyword(
                request.getKeyword(),
                request.getBoostAmount() != null ? request.getBoostAmount() : 20
        );

        BoostResponse response = BoostResponse.builder()
                .boostedCount(boosted)
                .keyword(request.getKeyword())
                .build();

        return ResponseEntity.ok(response);
    }

    /**
     * ëŒ€ìƒ ìƒíƒœ ë³€ê²½
     */
    @PutMapping("/targets/{id}/status")
    public ResponseEntity<Void> updateTargetStatus(
            @PathVariable Long id,
            @RequestBody UpdateStatusRequest request) {
        boolean success = queueService.updateTargetStatus(id, request.getStatus(), request.getReason());
        if (success) {
            log.info("Updated target status: id={}, newStatus={}", id, request.getStatus());
            return ResponseEntity.ok().build();
        } else {
            return ResponseEntity.notFound().build();
        }
    }

    // ========================================
    // ì •ë¦¬ ì‘ì—…
    // ========================================

    /**
     * ìˆ˜ë™ìœ¼ë¡œ ì •ë¦¬ íŠ¸ë¦¬ê±°
     */
    @PostMapping("/cleanup")
    public ResponseEntity<CleanupResponse> triggerCleanup(
            @RequestParam(defaultValue = "7") int daysOld) {
        int cleaned = queueService.cleanupOldTargets(daysOld);
        int expired = queueService.expireOldPendingTargets(daysOld);

        CleanupResponse response = CleanupResponse.builder()
                .cleanedCount(cleaned)
                .expiredCount(expired)
                .daysOld(daysOld)
                .build();

        log.info("Manual cleanup completed: cleaned={}, expired={}", cleaned, expired);
        return ResponseEntity.ok(response);
    }

    /**
     * ë©ˆì¶˜ ì‘ì—… ë³µêµ¬
     */
    @PostMapping("/queue/recover")
    public ResponseEntity<RecoverResponse> recoverStuck() {
        int recovered = queueService.recoverStuckTargets();

        RecoverResponse response = RecoverResponse.builder()
                .recoveredCount(recovered)
                .build();

        return ResponseEntity.ok(response);
    }

    // ========================================
    // í¬ë¡¤ëŸ¬ ì½œë°± (autonomous-crawler-serviceì—ì„œ í˜¸ì¶œ)
    // ========================================

    /**
     * í¬ë¡¤ë§ ì™„ë£Œ ì½œë°±
     */
    @PostMapping("/callback")
    public ResponseEntity<Void> handleCrawlerCallback(@RequestBody CrawlerCallbackRequest request) {
        log.debug("Received crawler callback: targetId={}, success={}", 
                request.getTargetId(), request.isSuccess());

        if (request.isSuccess()) {
            queueService.handleCrawlComplete(request.getUrlHash(), request.getCollectedDataId());
        } else {
            queueService.handleCrawlFailed(request.getUrlHash(), request.getError());
        }

        return ResponseEntity.ok().build();
    }

    // ========================================
    // DTO ë³€í™˜
    // ========================================

    private CrawlTargetDto toDto(CrawlTarget target) {
        return CrawlTargetDto.builder()
                .id(target.getId())
                .url(target.getUrl())
                .urlHash(target.getUrlHash().substring(0, 8) + "...") // ì¶•ì•½
                .discoverySource(target.getDiscoverySource())
                .discoveryContext(target.getDiscoveryContext())
                .priority(target.getPriority())
                .status(target.getStatus())
                .domain(target.getDomain())
                .expectedContentType(target.getExpectedContentType())
                .relatedKeywords(target.getRelatedKeywords())
                .retryCount(target.getRetryCount())
                .maxRetries(target.getMaxRetries())
                .lastError(target.getLastError())
                .discoveredAt(target.getDiscoveredAt() != null ? target.getDiscoveredAt().toString() : null)
                .lastAttemptAt(target.getLastAttemptAt() != null ? target.getLastAttemptAt().toString() : null)
                .completedAt(target.getCompletedAt() != null ? target.getCompletedAt().toString() : null)
                .collectedDataId(target.getCollectedDataId())
                .build();
    }

    // ========================================
    // Request/Response DTOs
    // ========================================

    @Data
    @Builder
    public static class AutoCrawlStatusResponse {
        private long pendingCount;
        private long inProgressCount;
        private long completedCount;
        private long failedCount;
        private long skippedCount;
        private int sessionDispatched;
        private int sessionCompleted;
        private int sessionFailed;
        private Map<DiscoverySource, Long> discoveryStats;
        private Map<String, Long> domainPendingStats;
        private Map<String, Integer> domainConcurrency;
    }

    @Data
    @Builder
    public static class CrawlTargetDto {
        private Long id;
        private String url;
        private String urlHash;
        private DiscoverySource discoverySource;
        private String discoveryContext;
        private Integer priority;
        private CrawlTargetStatus status;
        private String domain;
        private com.newsinsight.collector.entity.autocrawl.ContentType expectedContentType;
        private String relatedKeywords;
        private Integer retryCount;
        private Integer maxRetries;
        private String lastError;
        private String discoveredAt;
        private String lastAttemptAt;
        private String completedAt;
        private Long collectedDataId;
    }

    @Data
    public static class AddTargetRequest {
        private String url;
        private String keywords;
        private Integer priority;
    }

    @Data
    public static class BatchAddRequest {
        private List<String> urls;
        private String keywords;
        private Integer priority;
    }

    @Data
    @Builder
    public static class BatchAddResponse {
        private int addedCount;
        private int requestedCount;
    }

    @Data
    public static class DiscoverSearchRequest {
        private String query;
        private List<String> urls;
    }

    @Data
    @Builder
    public static class DiscoverResponse {
        private int discoveredCount;
        private DiscoverySource source;
    }

    @Data
    @Builder
    public static class ProcessQueueResponse {
        private int dispatchedCount;
        private int batchSize;
    }

    @Data
    public static class BoostRequest {
        private String keyword;
        private Integer boostAmount;
    }

    @Data
    @Builder
    public static class BoostResponse {
        private int boostedCount;
        private String keyword;
    }

    @Data
    public static class UpdateStatusRequest {
        private CrawlTargetStatus status;
        private String reason;
    }

    @Data
    @Builder
    public static class CleanupResponse {
        private int cleanedCount;
        private int expiredCount;
        private int daysOld;
    }

    @Data
    @Builder
    public static class RecoverResponse {
        private int recoveredCount;
    }

    @Data
    public static class CrawlerCallbackRequest {
        private Long targetId;
        private String urlHash;
        private boolean success;
        private Long collectedDataId;
        private String error;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/ChatHealthController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.service.ChatSyncService;
import com.newsinsight.collector.service.VectorEmbeddingService;
import io.micrometer.core.instrument.MeterRegistry;
import lombok.Builder;
import lombok.Data;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.boot.actuate.health.Health;
import org.springframework.boot.actuate.health.HealthIndicator;
import org.springframework.data.mongodb.core.MongoTemplate;
import org.springframework.data.redis.connection.RedisConnectionFactory;
import org.springframework.http.ResponseEntity;
import org.springframework.stereotype.Component;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;

import java.time.LocalDateTime;
import java.util.HashMap;
import java.util.Map;

/**
 * ì±„íŒ… ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬ ì»¨íŠ¸ë¡¤ëŸ¬
 * 
 * ì±„íŒ… ì„œë¹„ìŠ¤ì˜ ìƒíƒœì™€ ì˜ì¡´ ì„œë¹„ìŠ¤ë“¤ì˜ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
 */
@RestController
@RequestMapping("/api/v1/factcheck-chat/health")
@RequiredArgsConstructor
@Slf4j
public class ChatHealthController {

    private final MongoTemplate mongoTemplate;
    private final RedisConnectionFactory redisConnectionFactory;
    private final VectorEmbeddingService vectorEmbeddingService;
    private final ChatSyncService chatSyncService;
    private final MeterRegistry meterRegistry;

    /**
     * ì¢…í•© í—¬ìŠ¤ ì²´í¬
     */
    @GetMapping
    public ResponseEntity<HealthResponse> getHealth() {
        HealthResponse response = HealthResponse.builder()
                .status("UP")
                .timestamp(LocalDateTime.now())
                .mongodb(checkMongoHealth())
                .redis(checkRedisHealth())
                .vectorDb(checkVectorDbHealth())
                .sync(getSyncStatus())
                .build();

        // ì „ì²´ ìƒíƒœ ê²°ì •
        if (!response.getMongodb().isHealthy() || !response.getRedis().isHealthy()) {
            response.setStatus("DOWN");
        } else if (!response.getVectorDb().isHealthy()) {
            response.setStatus("DEGRADED");
        }

        return ResponseEntity.ok(response);
    }

    /**
     * MongoDB ìƒíƒœ í™•ì¸
     */
    @GetMapping("/mongodb")
    public ResponseEntity<ComponentHealth> getMongoHealth() {
        return ResponseEntity.ok(checkMongoHealth());
    }

    /**
     * Redis ìƒíƒœ í™•ì¸
     */
    @GetMapping("/redis")
    public ResponseEntity<ComponentHealth> getRedisHealth() {
        return ResponseEntity.ok(checkRedisHealth());
    }

    /**
     * ë²¡í„° DB ìƒíƒœ í™•ì¸
     */
    @GetMapping("/vector")
    public ResponseEntity<ComponentHealth> getVectorHealth() {
        return ResponseEntity.ok(checkVectorDbHealth());
    }

    /**
     * ë™ê¸°í™” ìƒíƒœ í™•ì¸
     */
    @GetMapping("/sync")
    public ResponseEntity<SyncHealthStatus> getSyncHealth() {
        return ResponseEntity.ok(getSyncStatus());
    }

    /**
     * ë©”íŠ¸ë¦­ ìš”ì•½
     */
    @GetMapping("/metrics")
    public ResponseEntity<Map<String, Object>> getMetrics() {
        Map<String, Object> metrics = new HashMap<>();
        
        // ì„¸ì…˜ ë©”íŠ¸ë¦­
        metrics.put("sessions", Map.of(
                "created", getCounterValue("factcheck.chat.sessions.created"),
                "closed", getCounterValue("factcheck.chat.sessions.closed"),
                "active", getGaugeValue("factcheck.chat.sessions.active")
        ));
        
        // ë©”ì‹œì§€ ë©”íŠ¸ë¦­
        metrics.put("messages", Map.of(
                "processed", getCounterValue("factcheck.chat.messages.processed")
        ));
        
        // íŒ©íŠ¸ì²´í¬ ë©”íŠ¸ë¦­
        metrics.put("factcheck", Map.of(
                "success", getCounterValue("factcheck.chat.factcheck.success"),
                "error", getCounterValue("factcheck.chat.factcheck.error")
        ));
        
        // ë™ê¸°í™” ë©”íŠ¸ë¦­
        metrics.put("sync", Map.of(
                "rdb_success", getCounterValue("chat.sync.rdb.success"),
                "rdb_error", getCounterValue("chat.sync.rdb.error"),
                "embedding_success", getCounterValue("chat.sync.embedding.success"),
                "embedding_error", getCounterValue("chat.sync.embedding.error"),
                "pending_sync", getGaugeValue("chat.sync.rdb.pending"),
                "pending_embedding", getGaugeValue("chat.sync.embedding.pending")
        ));
        
        // ìºì‹œ ì—ëŸ¬ ë©”íŠ¸ë¦­
        metrics.put("cache_errors", Map.of(
                "total", getCounterValue("cache.error")
        ));
        
        return ResponseEntity.ok(metrics);
    }

    private ComponentHealth checkMongoHealth() {
        try {
            mongoTemplate.executeCommand("{ ping: 1 }");
            return ComponentHealth.builder()
                    .name("MongoDB")
                    .healthy(true)
                    .message("Connected")
                    .build();
        } catch (Exception e) {
            log.error("MongoDB health check failed: {}", e.getMessage());
            return ComponentHealth.builder()
                    .name("MongoDB")
                    .healthy(false)
                    .message("Connection failed: " + e.getMessage())
                    .build();
        }
    }

    private ComponentHealth checkRedisHealth() {
        try {
            redisConnectionFactory.getConnection().ping();
            return ComponentHealth.builder()
                    .name("Redis")
                    .healthy(true)
                    .message("Connected")
                    .build();
        } catch (Exception e) {
            log.error("Redis health check failed: {}", e.getMessage());
            return ComponentHealth.builder()
                    .name("Redis")
                    .healthy(false)
                    .message("Connection failed: " + e.getMessage())
                    .build();
        }
    }

    private ComponentHealth checkVectorDbHealth() {
        VectorEmbeddingService.VectorServiceStatus status = vectorEmbeddingService.getStatus();
        
        if (!status.isEnabled()) {
            return ComponentHealth.builder()
                    .name("VectorDB")
                    .healthy(true) // disabledëŠ” ì—ëŸ¬ê°€ ì•„ë‹˜
                    .message("Disabled")
                    .build();
        }
        
        return ComponentHealth.builder()
                .name("VectorDB")
                .healthy(status.isVectorDbHealthy())
                .message(status.isVectorDbHealthy() ? "Connected" : "Connection failed")
                .details(Map.of(
                        "url", status.getVectorDbUrl(),
                        "collection", status.getCollectionName(),
                        "embeddingServiceHealthy", status.isEmbeddingServiceHealthy(),
                        "queueSize", status.getQueueSize()
                ))
                .build();
    }

    private SyncHealthStatus getSyncStatus() {
        ChatSyncService.SyncStats stats = chatSyncService.getSyncStats();
        
        return SyncHealthStatus.builder()
                .healthy(stats.getActiveSyncCount() < 10) // ë™ì‹œ ë™ê¸°í™” 10ê°œ ë¯¸ë§Œì´ë©´ ì •ìƒ
                .pendingSyncCount(stats.getPendingSyncCount())
                .pendingEmbeddingCount(stats.getPendingEmbeddingCount())
                .activeSyncCount(stats.getActiveSyncCount())
                .build();
    }

    private double getCounterValue(String name) {
        try {
            var counter = meterRegistry.find(name).counter();
            return counter != null ? counter.count() : 0;
        } catch (Exception e) {
            return 0;
        }
    }

    private double getGaugeValue(String name) {
        try {
            var gauge = meterRegistry.find(name).gauge();
            return gauge != null ? gauge.value() : 0;
        } catch (Exception e) {
            return 0;
        }
    }

    @Data
    @Builder
    public static class HealthResponse {
        private String status;
        private LocalDateTime timestamp;
        private ComponentHealth mongodb;
        private ComponentHealth redis;
        private ComponentHealth vectorDb;
        private SyncHealthStatus sync;
    }

    @Data
    @Builder
    public static class ComponentHealth {
        private String name;
        private boolean healthy;
        private String message;
        private Map<String, Object> details;
    }

    @Data
    @Builder
    public static class SyncHealthStatus {
        private boolean healthy;
        private long pendingSyncCount;
        private long pendingEmbeddingCount;
        private int activeSyncCount;
    }
}

/**
 * Spring Boot Actuator Health Indicator
 */
@Component
@RequiredArgsConstructor
@Slf4j
class ChatServiceHealthIndicator implements HealthIndicator {

    private final MongoTemplate mongoTemplate;
    private final RedisConnectionFactory redisConnectionFactory;
    private final VectorEmbeddingService vectorEmbeddingService;

    @Override
    public Health health() {
        Health.Builder builder = Health.up();
        
        // MongoDB ì²´í¬
        try {
            mongoTemplate.executeCommand("{ ping: 1 }");
            builder.withDetail("mongodb", "UP");
        } catch (Exception e) {
            builder.down().withDetail("mongodb", "DOWN: " + e.getMessage());
            return builder.build();
        }
        
        // Redis ì²´í¬
        try {
            redisConnectionFactory.getConnection().ping();
            builder.withDetail("redis", "UP");
        } catch (Exception e) {
            builder.down().withDetail("redis", "DOWN: " + e.getMessage());
            return builder.build();
        }
        
        // Vector DB ì²´í¬ (optional)
        VectorEmbeddingService.VectorServiceStatus vectorStatus = vectorEmbeddingService.getStatus();
        if (vectorStatus.isEnabled()) {
            if (vectorStatus.isVectorDbHealthy()) {
                builder.withDetail("vectorDb", "UP");
            } else {
                builder.withDetail("vectorDb", "DOWN");
                // Vector DBëŠ” optionalì´ë¯€ë¡œ degraded ìƒíƒœë¡œ
            }
        } else {
            builder.withDetail("vectorDb", "DISABLED");
        }
        
        return builder.build();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/ClaimExtractionController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.dto.ClaimExtractionRequest;
import com.newsinsight.collector.dto.ClaimExtractionResponse;
import com.newsinsight.collector.service.ClaimExtractionService;
import jakarta.validation.Valid;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;
import reactor.core.publisher.Mono;

import java.util.Map;

/**
 * Controller for claim extraction operations.
 * Extracts verifiable claims from URLs for fact-checking.
 */
@RestController
@RequestMapping("/api/v1/analysis")
@RequiredArgsConstructor
@Slf4j
public class ClaimExtractionController {

    private final ClaimExtractionService claimExtractionService;

    /**
     * Extract verifiable claims from a URL.
     * 
     * This endpoint:
     * 1. Crawls the given URL to extract page content
     * 2. Analyzes the content using AI to identify verifiable claims
     * 3. Returns structured claims with confidence scores
     * 
     * @param request The extraction request containing the URL
     * @return List of extracted claims with metadata
     */
    @PostMapping("/extract-claims")
    public Mono<ResponseEntity<ClaimExtractionResponse>> extractClaims(
            @Valid @RequestBody ClaimExtractionRequest request
    ) {
        log.info("Received claim extraction request for URL: {}", request.getUrl());

        return claimExtractionService.extractClaims(request)
                .map(response -> {
                    if (response == null) {
                        return ResponseEntity.internalServerError()
                                .body(ClaimExtractionResponse.builder()
                                        .url(request.getUrl())
                                        .message("ì¶”ì¶œ ì„œë¹„ìŠ¤ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                                        .build());
                    }

                    log.info("Extracted {} claims from URL: {}",
                            response.getClaims() != null ? response.getClaims().size() : 0,
                            request.getUrl());

                    return ResponseEntity.ok(response);
                })
                .onErrorResume(e -> {
                    log.error("Claim extraction failed for URL: {}", request.getUrl(), e);
                    return Mono.just(ResponseEntity.internalServerError()
                            .body(ClaimExtractionResponse.builder()
                                    .url(request.getUrl())
                                    .message("ì£¼ì¥ ì¶”ì¶œ ì‹¤íŒ¨: " + e.getMessage())
                                    .build()));
                });
    }

    /**
     * Health check for claim extraction service.
     */
    @GetMapping("/extract-claims/health")
    public ResponseEntity<Map<String, Object>> healthCheck() {
        return ResponseEntity.ok(Map.of(
                "service", "ClaimExtractionService",
                "status", "READY"
        ));
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/CollectionController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.dto.CollectionJobDTO;
import com.newsinsight.collector.dto.CollectionRequest;
import com.newsinsight.collector.dto.CollectionResponse;
import com.newsinsight.collector.dto.CollectionStatsDTO;
import com.newsinsight.collector.dto.PageResponse;
import com.newsinsight.collector.entity.CollectionJob;
import com.newsinsight.collector.entity.CollectionJob.JobStatus;
import com.newsinsight.collector.mapper.EntityMapper;
import com.newsinsight.collector.service.CollectionService;
import jakarta.validation.Valid;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.time.LocalDateTime;
import java.util.List;

@RestController
@RequestMapping("/api/v1/collections")
public class CollectionController {

    private final CollectionService collectionService;
    private final EntityMapper entityMapper;

    public CollectionController(CollectionService collectionService, EntityMapper entityMapper) {
        this.collectionService = collectionService;
        this.entityMapper = entityMapper;
    }

    /**
     * POST /api/v1/collections/start - ìˆ˜ì§‘ ì‘ì—… ì‹œì‘ (ì „ì²´ ë˜ëŠ” íŠ¹ì • ì†ŒìŠ¤)
     */
    @PostMapping("/start")
    public ResponseEntity<CollectionResponse> startCollection(
            @Valid @RequestBody CollectionRequest request) {

        List<CollectionJob> jobs;

        if (request.sourceIds().isEmpty()) {
            // í™œì„±í™”ëœ ëª¨ë“  ì†ŒìŠ¤ ëŒ€ìƒìœ¼ë¡œ ìˆ˜ì§‘
            jobs = collectionService.startCollectionForAllActive();
        } else {
            // ì§€ì •ëœ ì†ŒìŠ¤ë“¤ë§Œ ìˆ˜ì§‘
            jobs = collectionService.startCollectionForSources(request.sourceIds());
        }

        List<CollectionJobDTO> jobDTOs = jobs.stream()
                .map(entityMapper::toCollectionJobDTO)
                .toList();

        CollectionResponse response = new CollectionResponse(
                "Collection started for " + jobs.size() + " source(s)",
                jobDTOs,
                jobs.size(),
                LocalDateTime.now()
        );

        return ResponseEntity.status(HttpStatus.ACCEPTED).body(response);
    }

    /**
     * GET /api/v1/collections/jobs - ìˆ˜ì§‘ ì‘ì—… ëª©ë¡ ì¡°íšŒ (ìƒíƒœë³„ í•„í„°ë§)
     */
    @GetMapping("/jobs")
    public ResponseEntity<PageResponse<CollectionJobDTO>> listJobs(
            Pageable pageable,
            @RequestParam(required = false) JobStatus status) {

        Page<CollectionJob> jobs = (status != null)
                ? collectionService.getJobsByStatus(status, pageable)
                : collectionService.getAllJobs(pageable);

        Page<CollectionJobDTO> jobDTOs = jobs.map(entityMapper::toCollectionJobDTO);

        return ResponseEntity.ok(PageResponse.from(jobDTOs));
    }

    /**
     * GET /api/v1/collections/jobs/{id} - íŠ¹ì • ì‘ì—… ìƒì„¸ ì¡°íšŒ
     */
    @GetMapping("/jobs/{id}")
    public ResponseEntity<CollectionJobDTO> getJob(@PathVariable Long id) {
        return collectionService.getJobById(id)
                .map(entityMapper::toCollectionJobDTO)
                .map(ResponseEntity::ok)
                .orElse(ResponseEntity.notFound().build());
    }

    /**
     * POST /api/v1/collections/jobs/{id}/cancel - ìˆ˜ì§‘ ì‘ì—… ì·¨ì†Œ
     */
    @PostMapping("/jobs/{id}/cancel")
    public ResponseEntity<Void> cancelJob(@PathVariable Long id) {
        boolean cancelled = collectionService.cancelJob(id);
        return cancelled ? ResponseEntity.noContent().build() : ResponseEntity.notFound().build();
    }

    /**
     * GET /api/v1/collections/stats - ìˆ˜ì§‘ í†µê³„ ì¡°íšŒ
     */
    @GetMapping("/stats")
    public ResponseEntity<CollectionStatsDTO> getStats() {
        CollectionStatsDTO stats = collectionService.getStatistics();
        return ResponseEntity.ok(stats);
    }

    /**
     * DELETE /api/v1/collections/jobs/cleanup - ì˜¤ë˜ëœ ì‘ì—… ì •ë¦¬
     */
    @DeleteMapping("/jobs/cleanup")
    public ResponseEntity<String> cleanupOldJobs(
            @RequestParam(defaultValue = "30") int daysOld) {
        
        int cleaned = collectionService.cleanupOldJobs(daysOld);
        return ResponseEntity.ok("Cleaned up " + cleaned + " old jobs");
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/DashboardEventController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.dto.DashboardEventDto;
import com.newsinsight.collector.service.DashboardEventService;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.http.MediaType;
import org.springframework.http.codec.ServerSentEvent;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;
import reactor.core.publisher.Flux;

import java.time.Duration;

/**
 * ëŒ€ì‹œë³´ë“œ ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë° ì»¨íŠ¸ë¡¤ëŸ¬.
 * SSE(Server-Sent Events)ë¥¼ í†µí•´ í´ë¼ì´ì–¸íŠ¸ì— ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ë¥¼ í‘¸ì‹œí•©ë‹ˆë‹¤.
 */
@RestController
@RequestMapping("/api/v1/events")
@RequiredArgsConstructor
@Slf4j
public class DashboardEventController {

    private final DashboardEventService dashboardEventService;

    /**
     * ëŒ€ì‹œë³´ë“œ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼.
     * í´ë¼ì´ì–¸íŠ¸ëŠ” ì´ ì—”ë“œí¬ì¸íŠ¸ì— ì—°ê²°í•˜ì—¬ ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ë¥¼ ìˆ˜ì‹ í•©ë‹ˆë‹¤.
     * 
     * ì´ë²¤íŠ¸ íƒ€ì…:
     * - HEARTBEAT: ì—°ê²° ìœ ì§€ìš© (30ì´ˆë§ˆë‹¤)
     * - NEW_DATA: ìƒˆë¡œìš´ ë°ì´í„° ìˆ˜ì§‘ë¨
     * - SOURCE_UPDATED: ì†ŒìŠ¤ ìƒíƒœ ë³€ê²½
     * - STATS_UPDATED: í†µê³„ ê°±ì‹ 
     * 
     * @return SSE ìŠ¤íŠ¸ë¦¼
     */
    @GetMapping(value = "/stream", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
    public Flux<ServerSentEvent<DashboardEventDto>> streamEvents() {
        log.info("New SSE client connected to dashboard event stream");

        // ì—°ê²° í™•ì¸ ì´ë²¤íŠ¸ (ì¦‰ì‹œ ì „ì†¡)
        Flux<ServerSentEvent<DashboardEventDto>> connected = Flux.just(
                ServerSentEvent.<DashboardEventDto>builder()
                        .event("connected")
                        .data(DashboardEventDto.heartbeat())
                        .build()
        );

        // í•˜íŠ¸ë¹„íŠ¸ ìŠ¤íŠ¸ë¦¼ (ì¦‰ì‹œ ì‹œì‘, 30ì´ˆë§ˆë‹¤)
        Flux<ServerSentEvent<DashboardEventDto>> heartbeat = Flux.interval(Duration.ZERO, Duration.ofSeconds(30))
                .skip(1) // ì²« ë²ˆì§¸ëŠ” connected ì´ë²¤íŠ¸ë¡œ ëŒ€ì²´
                .map(tick -> ServerSentEvent.<DashboardEventDto>builder()
                        .event("heartbeat")
                        .data(DashboardEventDto.heartbeat())
                        .build());

        // ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼
        Flux<ServerSentEvent<DashboardEventDto>> events = dashboardEventService.getEventStream()
                .map(event -> ServerSentEvent.<DashboardEventDto>builder()
                        .event(event.getEventType().name().toLowerCase())
                        .data(event)
                        .build());

        // ì„¸ ìŠ¤íŠ¸ë¦¼ ë³‘í•© (connected ë¨¼ì €, ê·¸ ë‹¤ìŒ heartbeat + events)
        return Flux.concat(connected, Flux.merge(heartbeat, events))
                .doOnCancel(() -> log.info("SSE client disconnected from dashboard event stream"))
                .doOnError(e -> log.error("SSE stream error", e));
    }

    /**
     * ë°ì´í„° í†µê³„ ìŠ¤íŠ¸ë¦¼.
     * 5ì´ˆë§ˆë‹¤ ìµœì‹  í†µê³„ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤.
     * 
     * @return SSE ìŠ¤íŠ¸ë¦¼
     */
    @GetMapping(value = "/stats/stream", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
    public Flux<ServerSentEvent<DashboardEventDto>> streamStats() {
        log.debug("New SSE client connected to stats stream");

        return Flux.interval(Duration.ZERO, Duration.ofSeconds(5))
                .flatMap(tick -> dashboardEventService.getCurrentStats())
                .map(stats -> ServerSentEvent.<DashboardEventDto>builder()
                        .event("stats")
                        .data(stats)
                        .build())
                .doOnCancel(() -> log.debug("SSE client disconnected from stats stream"));
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/DataController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.dto.CollectedDataDTO;
import com.newsinsight.collector.entity.CollectedData;
import com.newsinsight.collector.mapper.EntityMapper;
import com.newsinsight.collector.service.CollectedDataService;
import lombok.RequiredArgsConstructor;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.PageRequest;
import org.springframework.data.domain.Pageable;
import org.springframework.data.domain.Sort;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

@RestController
@RequestMapping("/api/v1/data")
@RequiredArgsConstructor
public class DataController {

    private final CollectedDataService collectedDataService;
    private final EntityMapper entityMapper;

    /**
     * GET /api/v1/data - ìˆ˜ì§‘ëœ ë°ì´í„° ëª©ë¡ ì¡°íšŒ (ì†ŒìŠ¤/ì²˜ë¦¬ìƒíƒœ/ê²€ìƒ‰ í•„í„°ë§ ì§€ì›)
     */
    @GetMapping
    public ResponseEntity<Page<CollectedDataDTO>> listData(
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size,
            @RequestParam(required = false) Long sourceId,
            @RequestParam(required = false) Boolean processed,
            @RequestParam(required = false) String query) {
        
        Pageable pageable = PageRequest.of(page, size, Sort.by(Sort.Direction.DESC, "collectedAt"));
        
        Page<CollectedData> data;
        
        // ê²€ìƒ‰ì–´ê°€ ìˆëŠ” ê²½ìš°
        if (query != null && !query.isBlank()) {
            data = collectedDataService.searchWithFilter(query, processed, pageable);
        } else if (sourceId != null && processed != null) {
            // ì†ŒìŠ¤ + ì²˜ë¦¬ìƒíƒœ ë™ì‹œ í•„í„°ë§ì€ ë³„ë„ì˜ ì»¤ìŠ¤í…€ ì¿¼ë¦¬ í•„ìš” (í˜„ì¬ëŠ” ì†ŒìŠ¤ ê¸°ì¤€ í•„í„°ë§Œ ìˆ˜í–‰)
            data = collectedDataService.findBySourceId(sourceId, pageable);
        } else if (sourceId != null) {
            data = collectedDataService.findBySourceId(sourceId, pageable);
        } else if (Boolean.FALSE.equals(processed)) {
            data = collectedDataService.findUnprocessed(pageable);
        } else {
            data = collectedDataService.findAll(pageable);
        }
        
        Page<CollectedDataDTO> dataDTOs = data.map(entityMapper::toCollectedDataDTO);
        
        return ResponseEntity.ok(dataDTOs);
    }

    /**
     * GET /api/v1/data/unprocessed - ë¯¸ì²˜ë¦¬ ë°ì´í„° ëª©ë¡ ì¡°íšŒ
     */
    @GetMapping("/unprocessed")
    public ResponseEntity<Page<CollectedDataDTO>> listUnprocessedData(
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size) {
        
        Pageable pageable = PageRequest.of(page, size, Sort.by(Sort.Direction.DESC, "collectedAt"));
        Page<CollectedData> data = collectedDataService.findUnprocessed(pageable);
        Page<CollectedDataDTO> dataDTOs = data.map(entityMapper::toCollectedDataDTO);
        
        return ResponseEntity.ok(dataDTOs);
    }

    /**
     * GET /api/v1/data/{id} - ìˆ˜ì§‘ëœ ë°ì´í„° ë‹¨ê±´ ì¡°íšŒ (ID)
     */
    @GetMapping("/{id}")
    public ResponseEntity<CollectedDataDTO> getData(@PathVariable Long id) {
        return collectedDataService.findById(id)
                .map(entityMapper::toCollectedDataDTO)
                .map(ResponseEntity::ok)
                .orElse(ResponseEntity.notFound().build());
    }

    /**
     * POST /api/v1/data/{id}/processed - ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ ë§ˆí‚¹
     */
    @PostMapping("/{id}/processed")
    public ResponseEntity<Void> markAsProcessed(@PathVariable Long id) {
        boolean marked = collectedDataService.markAsProcessed(id);
        return marked ? ResponseEntity.noContent().build() : ResponseEntity.notFound().build();
    }

    /**
     * GET /api/v1/data/stats - ë°ì´í„° í†µê³„ ì¡°íšŒ (ì „ì²´/ë¯¸ì²˜ë¦¬/ì²˜ë¦¬ì™„ë£Œ)
     */
    @GetMapping("/stats")
    public ResponseEntity<DataStatsResponse> getDataStats() {
        long total = collectedDataService.countTotal();
        long unprocessed = collectedDataService.countUnprocessed();
        
        DataStatsResponse stats = new DataStatsResponse(total, unprocessed, total - unprocessed);
        return ResponseEntity.ok(stats);
    }

    /**
     * ê°„ë‹¨í•œ í†µê³„ ì‘ë‹µ êµ¬ì¡°ì²´
     */
    public record DataStatsResponse(long total, long unprocessed, long processed) {}
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/DeepAnalysisController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.dto.*;
import com.newsinsight.collector.entity.CrawlJobStatus;
import com.newsinsight.collector.service.DeepAnalysisService;
import com.newsinsight.collector.service.DeepSearchEventService;
import com.newsinsight.collector.service.IntegratedCrawlerService;
import jakarta.validation.Valid;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.data.domain.Page;
import org.springframework.http.HttpStatus;
import org.springframework.http.MediaType;
import org.springframework.http.ResponseEntity;
import org.springframework.http.codec.ServerSentEvent;
import org.springframework.web.bind.annotation.*;
import reactor.core.publisher.Flux;

import java.util.List;
import java.util.Map;

/**
 * Controller for deep AI search operations.
 * Provides endpoints for:
 * - Starting a new deep search
 * - Receiving callbacks from internal workers
 * - Retrieving search results
 * - Real-time SSE streaming of search progress
 * 
 * Uses IntegratedCrawlerService for multi-strategy crawling:
 * - Crawl4AI for JS-rendered pages
 * - Browser-Use API for complex interactions
 * - Direct HTTP for simple pages
 * - Search Engines for topic-based searches
 */
@RestController
@RequestMapping("/api/v1/analysis/deep")
@RequiredArgsConstructor
@Slf4j
public class DeepAnalysisController {

    private final DeepAnalysisService deepAnalysisService;
    private final DeepSearchEventService deepSearchEventService;
    private final IntegratedCrawlerService integratedCrawlerService;

    /**
     * Start a new deep AI search job.
     * 
     * @param request The search request containing topic and optional base URL
     * @return 202 Accepted with job details
     */
    @PostMapping
    public ResponseEntity<DeepSearchJobDto> startDeepSearch(
            @Valid @RequestBody DeepSearchRequest request
    ) {
        log.info("Starting deep search for topic: {}", request.getTopic());
        
        if (!integratedCrawlerService.isAvailable()) {
            return ResponseEntity.status(HttpStatus.SERVICE_UNAVAILABLE)
                    .body(DeepSearchJobDto.builder()
                            .status("UNAVAILABLE")
                            .errorMessage("Deep search service is not available. Please check crawler configuration.")
                            .build());
        }

        DeepSearchJobDto job = deepAnalysisService.startDeepSearch(
                request.getTopic(),
                request.getBaseUrl()
        );

        return ResponseEntity.status(HttpStatus.ACCEPTED).body(job);
    }

    /**
     * Get the status of a deep search job.
     * 
     * @param jobId The job ID
     * @return Job status details
     */
    @GetMapping("/{jobId}")
    public ResponseEntity<DeepSearchJobDto> getJobStatus(@PathVariable String jobId) {
        try {
            DeepSearchJobDto job = deepAnalysisService.getJobStatus(jobId);
            return ResponseEntity.ok(job);
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    /**
     * Get the full results of a completed deep search.
     * 
     * @param jobId The job ID
     * @return Full search results including evidence
     */
    @GetMapping("/{jobId}/result")
    public ResponseEntity<DeepSearchResultDto> getSearchResult(@PathVariable String jobId) {
        try {
            DeepSearchResultDto result = deepAnalysisService.getSearchResult(jobId);
            return ResponseEntity.ok(result);
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    /**
     * SSE stream for real-time job updates.
     * Clients can subscribe to this endpoint to receive live updates for a job.
     * 
     * Events:
     * - status: Job status changes (PENDING, IN_PROGRESS, COMPLETED, FAILED)
     * - progress: Progress updates (0-100%)
     * - evidence: New evidence found during the search
     * - complete: Job completed successfully
     * - error: Job failed with error
     * - heartbeat: Keep-alive ping every 15 seconds
     * 
     * @param jobId The job ID to subscribe to
     * @return SSE event stream
     */
    @GetMapping(value = "/{jobId}/stream", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
    public Flux<ServerSentEvent<Object>> streamJobUpdates(@PathVariable String jobId) {
        log.info("New SSE client subscribed to job: {}", jobId);
        
        // Validate job exists
        try {
            DeepSearchJobDto job = deepAnalysisService.getJobStatus(jobId);
            
            // If job is already completed or failed, send immediate result and close
            if ("COMPLETED".equals(job.getStatus()) || "FAILED".equals(job.getStatus()) 
                    || "CANCELLED".equals(job.getStatus()) || "TIMEOUT".equals(job.getStatus())) {
                log.info("Job {} already finished with status: {}, sending immediate result", jobId, job.getStatus());
                return Flux.just(ServerSentEvent.builder()
                        .event("complete")
                        .data(Map.of(
                                "jobId", jobId,
                                "job", job,
                                "timestamp", System.currentTimeMillis()
                        ))
                        .build());
            }
        } catch (IllegalArgumentException e) {
            log.warn("SSE subscription for unknown job: {}", jobId);
            return Flux.just(ServerSentEvent.builder()
                    .event("error")
                    .data(Map.of(
                            "jobId", jobId,
                            "error", "Job not found: " + jobId,
                            "timestamp", System.currentTimeMillis()
                    ))
                    .build());
        }

        return deepSearchEventService.getJobEventStream(jobId);
    }

    /**
     * List all deep search jobs with optional filtering.
     * 
     * @param page Page number (0-based)
     * @param size Page size
     * @param status Optional status filter
     * @return Paginated list of jobs
     */
    @GetMapping
    public ResponseEntity<Page<DeepSearchJobDto>> listJobs(
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size,
            @RequestParam(required = false) String status
    ) {
        CrawlJobStatus statusFilter = null;
        if (status != null && !status.isBlank()) {
            try {
                statusFilter = CrawlJobStatus.valueOf(status.toUpperCase());
            } catch (IllegalArgumentException e) {
                log.warn("Invalid status filter: {}", status);
            }
        }

        Page<DeepSearchJobDto> jobs = deepAnalysisService.listJobs(page, size, statusFilter);
        return ResponseEntity.ok(jobs);
    }

    /**
     * Cancel a pending or in-progress job.
     * 
     * @param jobId The job ID to cancel
     * @return Updated job status
     */
    @PostMapping("/{jobId}/cancel")
    public ResponseEntity<DeepSearchJobDto> cancelJob(@PathVariable String jobId) {
        try {
            DeepSearchJobDto job = deepAnalysisService.cancelJob(jobId);
            return ResponseEntity.ok(job);
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    /**
     * Callback endpoint for internal async workers to deliver results.
     * This can be used by future Kafka-based workers or other internal services.
     * 
     * @param callbackToken Token for authentication (from header)
     * @param payload The callback payload
     * @return Processing result
     */
    @PostMapping("/callback")
    public ResponseEntity<?> handleCallback(
            @RequestHeader(value = "X-Crawl-Callback-Token", required = false) String callbackToken,
            @RequestBody DeepSearchCallbackDto payload
    ) {
        log.info("Received internal callback for job: {}, status: {}", payload.getJobId(), payload.getStatus());

        try {
            // Convert DTO evidence to service format
            List<EvidenceDto> evidenceList = payload.getEvidence() != null 
                    ? payload.getEvidence().stream()
                            .map(e -> EvidenceDto.builder()
                                    .url(e.getUrl())
                                    .title(e.getTitle())
                                    .stance(e.getStance())
                                    .snippet(e.getSnippet())
                                    .source(e.getSource())
                                    .build())
                            .toList()
                    : List.of();

            DeepSearchResultDto result = deepAnalysisService.processInternalCallback(
                    callbackToken, 
                    payload.getJobId(),
                    payload.getStatus(),
                    evidenceList
            );
            
            return ResponseEntity.ok(Map.of(
                    "status", "received",
                    "jobId", result.getJobId(),
                    "evidenceCount", result.getEvidenceCount()
            ));

        } catch (SecurityException e) {
            log.warn("Callback authentication failed: {}", e.getMessage());
            return ResponseEntity.status(HttpStatus.UNAUTHORIZED)
                    .body(Map.of("error", "Invalid callback token"));

        } catch (IllegalArgumentException e) {
            log.warn("Callback for unknown job: {}", e.getMessage());
            return ResponseEntity.status(HttpStatus.NOT_FOUND)
                    .body(Map.of("error", e.getMessage()));

        } catch (Exception e) {
            log.error("Error processing callback", e);
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)
                    .body(Map.of("error", "Failed to process callback: " + e.getMessage()));
        }
    }

    /**
     * Health check for deep search service.
     */
    @GetMapping("/health")
    public ResponseEntity<Map<String, Object>> healthCheck() {
        boolean isAvailable = integratedCrawlerService.isAvailable();
        return ResponseEntity.ok(Map.of(
                "available", isAvailable,
                "service", "IntegratedCrawlerService",
                "status", isAvailable ? "READY" : "UNAVAILABLE"
        ));
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/FactCheckChatController.java

```java
package com.newsinsight.collector.controller;

import com.fasterxml.jackson.databind.ObjectMapper;
import com.newsinsight.collector.service.FactCheckChatService;
import lombok.Data;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.http.MediaType;
import org.springframework.http.codec.ServerSentEvent;
import org.springframework.web.bind.annotation.*;
import reactor.core.publisher.Flux;

import java.time.Duration;
import java.util.List;
import java.util.UUID;

/**
 * íŒ©íŠ¸ì²´í¬ ì±—ë´‡ ì»¨íŠ¸ë¡¤ëŸ¬
 * 
 * ì‚¬ìš©ìì™€ ëŒ€í™”í•˜ë©° ì‹¤ì‹œê°„ìœ¼ë¡œ íŒ©íŠ¸ì²´í¬ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
 * SSEë¥¼ í†µí•´ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‘ë‹µì„ ì „ì†¡í•©ë‹ˆë‹¤.
 * 
 * NOTE: CORS is handled by API Gateway - do not add @CrossOrigin here
 */
@RestController
@RequestMapping("/api/v1/factcheck-chat")
@RequiredArgsConstructor
@Slf4j
public class FactCheckChatController {

    private final FactCheckChatService factCheckChatService;
    private final ObjectMapper objectMapper;

    /**
     * íŒ©íŠ¸ì²´í¬ ì±—ë´‡ ì„¸ì…˜ ì‹œì‘
     * 
     * @param request ì´ˆê¸° ë©”ì‹œì§€ ìš”ì²­
     * @return ì„¸ì…˜ ID
     */
    @PostMapping("/session")
    public SessionResponse createSession(@RequestBody ChatRequest request) {
        String sessionId = UUID.randomUUID().toString();
        log.info("Created fact-check chat session: {}", sessionId);
        
        return SessionResponse.builder()
                .sessionId(sessionId)
                .message("íŒ©íŠ¸ì²´í¬ ì±—ë´‡ ì„¸ì…˜ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
                .build();
    }

    /**
     * íŒ©íŠ¸ì²´í¬ ì±—ë´‡ ë©”ì‹œì§€ ì „ì†¡ ë° SSE ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
     * 
     * @param sessionId ì„¸ì…˜ ID
     * @param request ì‚¬ìš©ì ë©”ì‹œì§€
     * @return SSE ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼
     */
    @PostMapping(value = "/session/{sessionId}/message", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
    public Flux<ServerSentEvent<String>> sendMessage(
            @PathVariable String sessionId,
            @RequestBody ChatRequest request
    ) {
        log.info("Received message for session {}: {}", sessionId, request.getMessage());

        return factCheckChatService.processMessage(sessionId, request.getMessage(), request.getClaims())
                .map(event -> {
                    try {
                        String data = objectMapper.writeValueAsString(event);
                        return ServerSentEvent.<String>builder()
                                .id(UUID.randomUUID().toString())
                                .event(event.getType())
                                .data(data)
                                .build();
                    } catch (Exception e) {
                        log.error("Failed to serialize chat event: {}", e.getMessage());
                        return ServerSentEvent.<String>builder()
                                .event("error")
                                .data("{\"error\": \"Serialization failed\"}")
                                .build();
                    }
                })
                .concatWith(Flux.just(
                        ServerSentEvent.<String>builder()
                                .event("done")
                                .data("{\"message\": \"Response completed\"}")
                                .build()
                ))
                .timeout(Duration.ofMinutes(3))
                .onErrorResume(e -> {
                    log.error("Error in chat stream for session {}: {}", sessionId, e.getMessage());
                    return Flux.just(
                            ServerSentEvent.<String>builder()
                                    .event("error")
                                    .data("{\"error\": \"" + e.getMessage() + "\"}")
                                    .build()
                    );
                });
    }

    /**
     * ì„¸ì…˜ ì¢…ë£Œ
     * 
     * @param sessionId ì„¸ì…˜ ID
     */
    @DeleteMapping("/session/{sessionId}")
    public void closeSession(@PathVariable String sessionId) {
        log.info("Closing fact-check chat session: {}", sessionId);
        factCheckChatService.closeSession(sessionId);
    }

    /**
     * ì„¸ì…˜ ì´ë ¥ ì¡°íšŒ
     * 
     * @param sessionId ì„¸ì…˜ ID
     * @return ëŒ€í™” ì´ë ¥
     */
    @GetMapping("/session/{sessionId}/history")
    public ChatHistoryResponse getHistory(@PathVariable String sessionId) {
        List<ChatMessage> history = factCheckChatService.getHistory(sessionId);
        return ChatHistoryResponse.builder()
                .sessionId(sessionId)
                .messages(history)
                .build();
    }

    // DTO Classes
    
    @Data
    public static class ChatRequest {
        private String message;
        private List<String> claims;
    }

    @Data
    @lombok.Builder
    public static class SessionResponse {
        private String sessionId;
        private String message;
    }

    @Data
    @lombok.Builder
    public static class ChatHistoryResponse {
        private String sessionId;
        private List<ChatMessage> messages;
    }

    @Data
    @lombok.Builder
    public static class ChatMessage {
        private String role; // user, assistant, system
        private String content;
        private Long timestamp;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/LiveAnalysisController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.client.AIDoveClient;
import com.newsinsight.collector.client.OpenAICompatibleClient;
import com.newsinsight.collector.client.PerplexityClient;
import com.newsinsight.collector.service.CrawlSearchService;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.http.MediaType;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RequestParam;
import org.springframework.web.bind.annotation.RestController;
import reactor.core.publisher.Flux;

import java.time.Duration;
import java.util.ArrayList;
import java.util.LinkedHashMap;
import java.util.List;
import java.util.Map;
import java.util.function.Supplier;

@RestController
@RequestMapping("/api/v1/analysis")
@RequiredArgsConstructor
@Slf4j
public class LiveAnalysisController {

    private final PerplexityClient perplexityClient;
    private final OpenAICompatibleClient openAICompatibleClient;
    private final AIDoveClient aiDoveClient;
    private final CrawlSearchService crawlSearchService;

    /**
     * Health check for live analysis service.
     * Returns whether the analysis APIs are configured and available.
     */
    @GetMapping("/live/health")
    public ResponseEntity<Map<String, Object>> liveAnalysisHealth() {
        List<String> availableProviders = getAvailableProviders();
        boolean anyEnabled = !availableProviders.isEmpty();

        String primaryProvider = availableProviders.isEmpty() ? "none" : availableProviders.get(0);
        String message = anyEnabled 
                ? "Live analysis is available (" + String.join(", ", availableProviders) + ")"
                : "Live analysis is disabled. No AI provider is configured.";

        Map<String, Object> response = new LinkedHashMap<>();
        response.put("enabled", anyEnabled);
        response.put("primaryProvider", primaryProvider);
        response.put("availableProviders", availableProviders);
        response.put("providerStatus", Map.of(
                "perplexity", perplexityClient.isEnabled(),
                "openai", openAICompatibleClient.isOpenAIEnabled(),
                "openrouter", openAICompatibleClient.isOpenRouterEnabled(),
                "azure", openAICompatibleClient.isAzureEnabled(),
                "aidove", aiDoveClient.isEnabled(),
                "ollama", true, // Ollama is always potentially available
                "custom", openAICompatibleClient.isCustomEnabled(),
                "crawl", crawlSearchService.isAvailable()
        ));
        response.put("message", message);

        return ResponseEntity.ok(response);
    }

    @GetMapping(value = "/live", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
    public Flux<String> streamLiveAnalysis(
            @RequestParam String query,
            @RequestParam(defaultValue = "7d") String window
    ) {
        String prompt = buildPrompt(query, window);
        log.info("Starting live analysis for query='{}', window='{}'", query, window);

        // Build provider chain and try with fallback
        List<ProviderAttempt> providers = buildProviderChain(prompt, query, window);
        
        if (providers.isEmpty()) {
            log.warn("Live analysis requested but no provider is available");
            return Flux.just(
                    "ì‹¤ì‹œê°„ ë¶„ì„ ê¸°ëŠ¥ì´ í˜„ì¬ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n" +
                    "ì„¤ì •ëœ AI ì œê³µìê°€ ì—†ìŠµë‹ˆë‹¤.\n" +
                    "ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.\n\n" +
                    "ëŒ€ì•ˆ: Deep AI Search ë˜ëŠ” Browser AI Agentë¥¼ ì‚¬ìš©í•´ ë³´ì„¸ìš”."
            );
        }

        log.info("Live analysis using fallback chain: {}", 
                providers.stream().map(ProviderAttempt::name).toList());

        return tryProvidersInSequence(providers, 0);
    }

    /**
     * Get list of available providers
     */
    private List<String> getAvailableProviders() {
        List<String> available = new ArrayList<>();
        
        if (perplexityClient.isEnabled()) available.add("Perplexity");
        if (openAICompatibleClient.isOpenAIEnabled()) available.add("OpenAI");
        if (openAICompatibleClient.isOpenRouterEnabled()) available.add("OpenRouter");
        if (openAICompatibleClient.isAzureEnabled()) available.add("Azure");
        if (aiDoveClient.isEnabled()) available.add("AI Dove");
        available.add("Ollama"); // Always potentially available
        if (openAICompatibleClient.isCustomEnabled()) available.add("Custom");
        if (crawlSearchService.isAvailable()) available.add("Crawl+AIDove");
        
        return available;
    }

    /**
     * Build provider chain for live analysis
     */
    private List<ProviderAttempt> buildProviderChain(String prompt, String query, String window) {
        List<ProviderAttempt> chain = new ArrayList<>();

        // 1. Perplexity - Best for news analysis with online search
        if (perplexityClient.isEnabled()) {
            chain.add(new ProviderAttempt("Perplexity", () -> perplexityClient.streamCompletion(prompt)));
        }

        // 2. OpenAI
        if (openAICompatibleClient.isOpenAIEnabled()) {
            chain.add(new ProviderAttempt("OpenAI", () -> openAICompatibleClient.streamFromOpenAI(prompt)));
        }

        // 3. OpenRouter
        if (openAICompatibleClient.isOpenRouterEnabled()) {
            chain.add(new ProviderAttempt("OpenRouter", () -> openAICompatibleClient.streamFromOpenRouter(prompt)));
        }

        // 4. Azure OpenAI
        if (openAICompatibleClient.isAzureEnabled()) {
            chain.add(new ProviderAttempt("Azure", () -> openAICompatibleClient.streamFromAzure(prompt)));
        }

        // 5. AI Dove
        if (aiDoveClient.isEnabled()) {
            chain.add(new ProviderAttempt("AI Dove", () -> aiDoveClient.chatStream(prompt, null)));
        }

        // 6. CrawlSearchService (Crawl4AI + AI Dove)
        if (crawlSearchService.isAvailable()) {
            chain.add(new ProviderAttempt("Crawl+AIDove", () -> crawlSearchService.searchAndAnalyze(query, window)));
        }

        // 7. Ollama - Local LLM
        chain.add(new ProviderAttempt("Ollama", () -> openAICompatibleClient.streamFromOllama(prompt)));

        // 8. Custom endpoint
        if (openAICompatibleClient.isCustomEnabled()) {
            chain.add(new ProviderAttempt("Custom", () -> openAICompatibleClient.streamFromCustom(prompt)));
        }

        return chain;
    }

    /**
     * Try providers in sequence until one succeeds
     */
    private Flux<String> tryProvidersInSequence(List<ProviderAttempt> providers, int index) {
        if (index >= providers.size()) {
            log.error("All AI providers failed for live analysis");
            return Flux.just("ëª¨ë“  AI ì œê³µì ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.");
        }

        ProviderAttempt current = providers.get(index);
        log.info("Trying AI provider: {} ({}/{})", current.name(), index + 1, providers.size());

        return current.streamSupplier().get()
                .timeout(Duration.ofSeconds(90))
                .onErrorResume(e -> {
                    log.warn("AI provider {} failed: {}. Trying next...", current.name(), e.getMessage());
                    return tryProvidersInSequence(providers, index + 1);
                })
                .switchIfEmpty(Flux.defer(() -> {
                    log.warn("AI provider {} returned empty. Trying next...", current.name());
                    return tryProvidersInSequence(providers, index + 1);
                }));
    }

    private String buildPrompt(String query, String window) {
        String normalizedQuery = (query == null || query.isBlank()) ? "ì§€ì •ëœ í‚¤ì›Œë“œ ì—†ìŒ" : query;

        String windowDescription;
        if ("1d".equals(window)) {
            windowDescription = "ìµœê·¼ 1ì¼";
        } else if ("30d".equals(window)) {
            windowDescription = "ìµœê·¼ 30ì¼";
        } else {
            windowDescription = "ìµœê·¼ 7ì¼";
        }

        return "ë‹¤ìŒ í‚¤ì›Œë“œ '" + normalizedQuery + "' ì— ëŒ€í•´ " + windowDescription +
                " ë™ì•ˆì˜ ì£¼ìš” ë‰´ìŠ¤ íë¦„ê³¼ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ìì„¸íˆ ìš”ì•½í•´ ì£¼ì„¸ìš”. " +
                "ê°€ëŠ¥í•˜ë©´ bullet í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•˜ê³ , ë§ˆì§€ë§‰ì— ì „ë°˜ì ì¸ ì˜ë¯¸ë¥¼ í•œ ë¬¸ë‹¨ìœ¼ë¡œ ì •ë¦¬í•´ ì£¼ì„¸ìš”.";
    }

    /**
     * Provider attempt wrapper
     */
    private record ProviderAttempt(
            String name,
            Supplier<Flux<String>> streamSupplier
    ) {}
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/LlmProviderSettingsController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.dto.llm.LlmProviderSettingsDto;
import com.newsinsight.collector.dto.llm.LlmProviderSettingsRequest;
import com.newsinsight.collector.dto.llm.LlmTestResult;
import com.newsinsight.collector.entity.settings.LlmProviderType;
import com.newsinsight.collector.service.LlmProviderSettingsService;
import jakarta.validation.Valid;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.Arrays;
import java.util.List;
import java.util.Map;
import java.util.stream.Collectors;

/**
 * LLM Provider ì„¤ì • API ì»¨íŠ¸ë¡¤ëŸ¬.
 * 
 * ê´€ë¦¬ì(ì „ì—­) ì„¤ì •ê³¼ ì‚¬ìš©ìë³„ ì„¤ì •ì„ ë¶„ë¦¬í•˜ì—¬ ê´€ë¦¬.
 * - /api/v1/admin/llm-providers: ê´€ë¦¬ì ì „ì—­ ì„¤ì •
 * - /api/v1/llm-providers: ì‚¬ìš©ìë³„ ì„¤ì •
 */
@RestController
@RequestMapping("/api/v1")
@RequiredArgsConstructor
@Slf4j
public class LlmProviderSettingsController {

    private final LlmProviderSettingsService settingsService;

    // ========== ê³µí†µ: Provider íƒ€ì… ëª©ë¡ ==========

    /**
     * ì§€ì›í•˜ëŠ” LLM Provider íƒ€ì… ëª©ë¡
     */
    @GetMapping("/llm-providers/types")
    public ResponseEntity<List<Map<String, String>>> getProviderTypes() {
        List<Map<String, String>> types = Arrays.stream(LlmProviderType.values())
                .map(type -> Map.of(
                        "value", type.name(),
                        "displayName", type.getDisplayName(),
                        "defaultBaseUrl", type.getDefaultBaseUrl() != null ? type.getDefaultBaseUrl() : ""
                ))
                .collect(Collectors.toList());
        return ResponseEntity.ok(types);
    }

    // ========== ê´€ë¦¬ì ì „ì—­ ì„¤ì • API ==========

    /**
     * ëª¨ë“  ì „ì—­ ì„¤ì • ì¡°íšŒ
     */
    @GetMapping("/admin/llm-providers")
    public ResponseEntity<List<LlmProviderSettingsDto>> getAllGlobalSettings() {
        return ResponseEntity.ok(settingsService.getAllGlobalSettings());
    }

    /**
     * íŠ¹ì • Providerì˜ ì „ì—­ ì„¤ì • ì¡°íšŒ
     */
    @GetMapping("/admin/llm-providers/{providerType}")
    public ResponseEntity<LlmProviderSettingsDto> getGlobalSetting(@PathVariable LlmProviderType providerType) {
        return settingsService.getGlobalSetting(providerType)
                .map(ResponseEntity::ok)
                .orElse(ResponseEntity.notFound().build());
    }

    /**
     * ì „ì—­ ì„¤ì • ìƒì„±/ì—…ë°ì´íŠ¸
     */
    @PutMapping("/admin/llm-providers")
    public ResponseEntity<LlmProviderSettingsDto> saveGlobalSetting(
            @Valid @RequestBody LlmProviderSettingsRequest request
    ) {
        LlmProviderSettingsDto saved = settingsService.saveGlobalSetting(request);
        return ResponseEntity.ok(saved);
    }

    /**
     * ì „ì—­ ì„¤ì • ì‚­ì œ
     */
    @DeleteMapping("/admin/llm-providers/{providerType}")
    public ResponseEntity<Map<String, String>> deleteGlobalSetting(@PathVariable LlmProviderType providerType) {
        settingsService.deleteGlobalSetting(providerType);
        return ResponseEntity.ok(Map.of(
                "status", "deleted",
                "provider", providerType.name()
        ));
    }

    /**
     * ì „ì—­ ì„¤ì • ì—°ê²° í…ŒìŠ¤íŠ¸
     */
    @PostMapping("/admin/llm-providers/{id}/test")
    public ResponseEntity<LlmTestResult> testGlobalConnection(@PathVariable Long id) {
        LlmTestResult result = settingsService.testConnection(id);
        return ResponseEntity.ok(result);
    }

    /**
     * ì „ì—­ ì„¤ì • í™œì„±í™”/ë¹„í™œì„±í™”
     */
    @PostMapping("/admin/llm-providers/{id}/toggle")
    public ResponseEntity<Map<String, Object>> toggleGlobalSetting(
            @PathVariable Long id,
            @RequestParam boolean enabled
    ) {
        settingsService.setEnabled(id, enabled);
        return ResponseEntity.ok(Map.of(
                "id", id,
                "enabled", enabled
        ));
    }

    // ========== ì‚¬ìš©ìë³„ ì„¤ì • API ==========

    /**
     * ì‚¬ìš©ìì˜ ìœ íš¨ ì„¤ì • ì¡°íšŒ (ì‚¬ìš©ì ì„¤ì • > ì „ì—­ ì„¤ì •)
     */
    @GetMapping("/llm-providers/effective")
    public ResponseEntity<List<LlmProviderSettingsDto>> getEffectiveSettings(
            @RequestHeader(value = "X-User-Id", required = false) String userId
    ) {
        return ResponseEntity.ok(settingsService.getEffectiveSettings(userId));
    }

    /**
     * ì‚¬ìš©ìì˜ í™œì„±í™”ëœ Provider ëª©ë¡ (Fallback ì²´ì¸ìš©)
     */
    @GetMapping("/llm-providers/enabled")
    public ResponseEntity<List<LlmProviderSettingsDto>> getEnabledProviders(
            @RequestHeader(value = "X-User-Id", required = false) String userId
    ) {
        return ResponseEntity.ok(settingsService.getEnabledProviders(userId));
    }

    /**
     * íŠ¹ì • Providerì˜ ìœ íš¨ ì„¤ì • ì¡°íšŒ
     */
    @GetMapping("/llm-providers/config/{providerType}")
    public ResponseEntity<LlmProviderSettingsDto> getEffectiveSetting(
            @PathVariable LlmProviderType providerType,
            @RequestHeader(value = "X-User-Id", required = false) String userId
    ) {
        return settingsService.getEffectiveSetting(userId, providerType)
                .map(ResponseEntity::ok)
                .orElse(ResponseEntity.notFound().build());
    }

    /**
     * ì‚¬ìš©ìì˜ ê°œì¸ ì„¤ì •ë§Œ ì¡°íšŒ
     */
    @GetMapping("/llm-providers/user")
    public ResponseEntity<List<LlmProviderSettingsDto>> getUserSettings(
            @RequestHeader("X-User-Id") String userId
    ) {
        return ResponseEntity.ok(settingsService.getUserSettings(userId));
    }

    /**
     * ì‚¬ìš©ì ì„¤ì • ìƒì„±/ì—…ë°ì´íŠ¸
     */
    @PutMapping("/llm-providers/user")
    public ResponseEntity<LlmProviderSettingsDto> saveUserSetting(
            @RequestHeader("X-User-Id") String userId,
            @Valid @RequestBody LlmProviderSettingsRequest request
    ) {
        LlmProviderSettingsDto saved = settingsService.saveUserSetting(userId, request);
        return ResponseEntity.ok(saved);
    }

    /**
     * ì‚¬ìš©ì ì„¤ì • ì‚­ì œ (ì „ì—­ ì„¤ì •ìœ¼ë¡œ í´ë°±)
     */
    @DeleteMapping("/llm-providers/user/{providerType}")
    public ResponseEntity<Map<String, String>> deleteUserSetting(
            @RequestHeader("X-User-Id") String userId,
            @PathVariable LlmProviderType providerType
    ) {
        settingsService.deleteUserSetting(userId, providerType);
        return ResponseEntity.ok(Map.of(
                "status", "deleted",
                "provider", providerType.name(),
                "message", "Falling back to global settings"
        ));
    }

    /**
     * ì‚¬ìš©ìì˜ ëª¨ë“  ê°œì¸ ì„¤ì • ì‚­ì œ
     */
    @DeleteMapping("/llm-providers/user")
    public ResponseEntity<Map<String, String>> deleteAllUserSettings(
            @RequestHeader("X-User-Id") String userId
    ) {
        settingsService.deleteAllUserSettings(userId);
        return ResponseEntity.ok(Map.of(
                "status", "deleted",
                "message", "All user settings deleted, falling back to global settings"
        ));
    }

    /**
     * ì‚¬ìš©ì ì„¤ì • ì—°ê²° í…ŒìŠ¤íŠ¸
     */
    @PostMapping("/llm-providers/user/{id}/test")
    public ResponseEntity<LlmTestResult> testUserConnection(@PathVariable Long id) {
        LlmTestResult result = settingsService.testConnection(id);
        return ResponseEntity.ok(result);
    }

    /**
     * ìƒˆ ì„¤ì •ìœ¼ë¡œ ì—°ê²° í…ŒìŠ¤íŠ¸ (ì €ì¥ ì „)
     */
    @PostMapping("/llm-providers/test")
    public ResponseEntity<LlmTestResult> testNewConnection(
            @Valid @RequestBody LlmProviderSettingsRequest request
    ) {
        LlmTestResult result = settingsService.testConnection(request);
        return ResponseEntity.ok(result);
    }

    // ========== ì˜ˆì™¸ ì²˜ë¦¬ ==========

    @ExceptionHandler(IllegalArgumentException.class)
    public ResponseEntity<Map<String, String>> handleIllegalArgument(IllegalArgumentException e) {
        return ResponseEntity.badRequest().body(Map.of("error", e.getMessage()));
    }

    @ExceptionHandler(Exception.class)
    public ResponseEntity<Map<String, String>> handleException(Exception e) {
        log.error("Unexpected error in LlmProviderSettingsController", e);
        return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)
                .body(Map.of("error", "Internal server error: " + e.getMessage()));
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/MlAddonController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.dto.addon.AddonResponse;
import com.newsinsight.collector.entity.addon.*;
import com.newsinsight.collector.repository.MlAddonExecutionRepository;
import com.newsinsight.collector.repository.MlAddonRepository;
import com.newsinsight.collector.service.AddonOrchestratorService;
import jakarta.validation.Valid;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.PageRequest;
import org.springframework.data.domain.Sort;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Map;
import java.util.concurrent.CompletableFuture;

/**
 * ML Add-on ê´€ë¦¬ ë° ë¶„ì„ ì‹¤í–‰ API.
 */
@RestController
@RequestMapping("/api/v1/ml")
@RequiredArgsConstructor
@Slf4j
public class MlAddonController {

    private final MlAddonRepository addonRepository;
    private final MlAddonExecutionRepository executionRepository;
    private final AddonOrchestratorService orchestratorService;

    // ========== Add-on Registry ê´€ë¦¬ ==========

    /**
     * ëª¨ë“  Add-on ëª©ë¡ ì¡°íšŒ
     */
    @GetMapping("/addons")
    public ResponseEntity<List<MlAddon>> listAddons(
            @RequestParam(required = false) AddonCategory category,
            @RequestParam(required = false) Boolean enabled
    ) {
        List<MlAddon> addons;
        if (category != null && enabled != null && enabled) {
            addons = addonRepository.findByCategoryAndEnabledTrue(category);
        } else if (category != null) {
            addons = addonRepository.findByCategory(category);
        } else if (enabled != null && enabled) {
            addons = addonRepository.findByEnabledTrue();
        } else {
            addons = addonRepository.findAll();
        }
        return ResponseEntity.ok(addons);
    }

    /**
     * íŠ¹ì • Add-on ì¡°íšŒ
     */
    @GetMapping("/addons/{addonKey}")
    public ResponseEntity<MlAddon> getAddon(@PathVariable String addonKey) {
        return addonRepository.findByAddonKey(addonKey)
                .map(ResponseEntity::ok)
                .orElse(ResponseEntity.notFound().build());
    }

    /**
     * Add-on ë“±ë¡
     */
    @PostMapping("/addons")
    public ResponseEntity<?> createAddon(@Valid @RequestBody MlAddon addon) {
        if (addonRepository.existsByAddonKey(addon.getAddonKey())) {
            return ResponseEntity.badRequest()
                    .body(Map.of("error", "Addon key already exists: " + addon.getAddonKey()));
        }

        MlAddon saved = addonRepository.save(addon);
        log.info("Created new addon: {}", addon.getAddonKey());
        return ResponseEntity.status(HttpStatus.CREATED).body(saved);
    }

    /**
     * Add-on ìˆ˜ì •
     */
    @PutMapping("/addons/{addonKey}")
    public ResponseEntity<?> updateAddon(
            @PathVariable String addonKey,
            @RequestBody MlAddon updates
    ) {
        return addonRepository.findByAddonKey(addonKey)
                .map(existing -> {
                    // ìˆ˜ì • ê°€ëŠ¥í•œ í•„ë“œë§Œ ì—…ë°ì´íŠ¸
                    if (updates.getName() != null) existing.setName(updates.getName());
                    if (updates.getDescription() != null) existing.setDescription(updates.getDescription());
                    if (updates.getEndpointUrl() != null) existing.setEndpointUrl(updates.getEndpointUrl());
                    if (updates.getTimeoutMs() != null) existing.setTimeoutMs(updates.getTimeoutMs());
                    if (updates.getMaxQps() != null) existing.setMaxQps(updates.getMaxQps());
                    if (updates.getMaxRetries() != null) existing.setMaxRetries(updates.getMaxRetries());
                    if (updates.getEnabled() != null) existing.setEnabled(updates.getEnabled());
                    if (updates.getPriority() != null) existing.setPriority(updates.getPriority());
                    if (updates.getConfig() != null) existing.setConfig(updates.getConfig());
                    if (updates.getDependsOn() != null) existing.setDependsOn(updates.getDependsOn());
                    if (updates.getAuthType() != null) existing.setAuthType(updates.getAuthType());
                    if (updates.getAuthCredentials() != null) existing.setAuthCredentials(updates.getAuthCredentials());
                    if (updates.getHealthCheckUrl() != null) existing.setHealthCheckUrl(updates.getHealthCheckUrl());

                    MlAddon saved = addonRepository.save(existing);
                    log.info("Updated addon: {}", addonKey);
                    return ResponseEntity.ok(saved);
                })
                .orElse(ResponseEntity.notFound().build());
    }

    /**
     * Add-on í™œì„±í™”/ë¹„í™œì„±í™”
     */
    @PostMapping("/addons/{addonKey}/toggle")
    public ResponseEntity<?> toggleAddon(@PathVariable String addonKey) {
        return addonRepository.findByAddonKey(addonKey)
                .map(addon -> {
                    addon.setEnabled(!addon.getEnabled());
                    MlAddon saved = addonRepository.save(addon);
                    log.info("Toggled addon {}: enabled={}", addonKey, saved.getEnabled());
                    return ResponseEntity.ok(Map.of(
                            "addonKey", addonKey,
                            "enabled", saved.getEnabled()
                    ));
                })
                .orElse(ResponseEntity.notFound().build());
    }

    /**
     * Add-on ì‚­ì œ
     */
    @DeleteMapping("/addons/{addonKey}")
    public ResponseEntity<?> deleteAddon(@PathVariable String addonKey) {
        return addonRepository.findByAddonKey(addonKey)
                .map(addon -> {
                    addonRepository.delete(addon);
                    log.info("Deleted addon: {}", addonKey);
                    return ResponseEntity.ok(Map.of("deleted", addonKey));
                })
                .orElse(ResponseEntity.notFound().build());
    }

    // ========== ë¶„ì„ ì‹¤í–‰ ==========

    /**
     * íŠ¹ì • Add-onìœ¼ë¡œ ì§ì ‘ ë¶„ì„ ì‹¤í–‰ (ì»¤ìŠ¤í…€ ì…ë ¥)
     * POST /api/v1/ml/addons/{addonKey}/analyze
     * 
     * í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì§ì ‘ íŠ¹ì • Add-onì„ í˜¸ì¶œí•˜ì—¬ ë¶„ì„ì„ ì‹¤í–‰í•  ë•Œ ì‚¬ìš©.
     * ê¸°ì‚¬ ID ì—†ì´ ì»¤ìŠ¤í…€ ë°ì´í„°ë¡œ ë¶„ì„ ê°€ëŠ¥.
     */
    @PostMapping("/addons/{addonKey}/analyze")
    public ResponseEntity<?> analyzeWithAddon(
            @PathVariable String addonKey,
            @RequestBody Map<String, Object> request
    ) {
        return addonRepository.findByAddonKey(addonKey)
                .map(addon -> {
                    if (!addon.getEnabled()) {
                        return ResponseEntity.badRequest()
                                .body(Map.of("error", "Addon is disabled: " + addonKey));
                    }
                    
                    try {
                        // ìš”ì²­ì—ì„œ article ì •ë³´ ì¶”ì¶œ
                        @SuppressWarnings("unchecked")
                        Map<String, Object> articleData = (Map<String, Object>) request.getOrDefault("article", Map.of());
                        
                        String requestId = java.util.UUID.randomUUID().toString();
                        String importance = (String) request.getOrDefault("importance", "batch");
                        
                        // Add-on ì§ì ‘ í˜¸ì¶œ
                        AddonResponse response = orchestratorService.executeAddonDirect(addon, articleData, requestId, importance);
                        
                        if (response == null) {
                            return ResponseEntity.status(HttpStatus.SERVICE_UNAVAILABLE)
                                    .body(Map.of("error", "Addon did not return a response"));
                        }
                        
                        return ResponseEntity.ok(response);
                    } catch (Exception e) {
                        log.error("Failed to execute addon {}: {}", addonKey, e.getMessage(), e);
                        return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)
                                .body(Map.of(
                                        "error", "Addon execution failed",
                                        "message", e.getMessage()
                                ));
                    }
                })
                .orElse(ResponseEntity.notFound().build());
    }

    /**
     * ë‹¨ì¼ ê¸°ì‚¬ ë¶„ì„ ì‹¤í–‰
     */
    @PostMapping("/analyze/{articleId}")
    public ResponseEntity<?> analyzeArticle(
            @PathVariable Long articleId,
            @RequestParam(defaultValue = "batch") String importance
    ) {
        try {
            CompletableFuture<String> future = orchestratorService.analyzeArticle(articleId, importance);
            String batchId = future.get();
            return ResponseEntity.accepted().body(Map.of(
                    "status", "accepted",
                    "articleId", articleId,
                    "batchId", batchId
            ));
        } catch (Exception e) {
            log.error("Failed to start analysis for article: {}", articleId, e);
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)
                    .body(Map.of("error", e.getMessage()));
        }
    }

    /**
     * ì—¬ëŸ¬ ê¸°ì‚¬ ì¼ê´„ ë¶„ì„
     */
    @PostMapping("/analyze/batch")
    public ResponseEntity<?> analyzeArticles(
            @RequestBody List<Long> articleIds,
            @RequestParam(defaultValue = "batch") String importance
    ) {
        CompletableFuture<String> future = orchestratorService.analyzeArticles(articleIds, importance);
        return ResponseEntity.accepted().body(Map.of(
                "status", "accepted",
                "articleCount", articleIds.size(),
                "batchId", future.join()
        ));
    }

    /**
     * íŠ¹ì • ì¹´í…Œê³ ë¦¬ Add-onë§Œ ì‹¤í–‰
     */
    @PostMapping("/analyze/{articleId}/category/{category}")
    public ResponseEntity<?> analyzeByCategory(
            @PathVariable Long articleId,
            @PathVariable AddonCategory category
    ) {
        try {
            CompletableFuture<AddonResponse> future = orchestratorService.executeCategory(articleId, category);
            AddonResponse response = future.get();
            return ResponseEntity.ok(response);
        } catch (Exception e) {
            log.error("Failed to analyze article {} with category {}", articleId, category, e);
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)
                    .body(Map.of("error", e.getMessage()));
        }
    }

    // ========== ì‹¤í–‰ ì´ë ¥ ==========

    /**
     * ì‹¤í–‰ ì´ë ¥ ì¡°íšŒ (status í•„í„° ì§€ì›)
     */
    @GetMapping("/executions")
    public ResponseEntity<Page<MlAddonExecution>> listExecutions(
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size,
            @RequestParam(required = false) ExecutionStatus status
    ) {
        PageRequest pageRequest = PageRequest.of(page, size, Sort.by(Sort.Direction.DESC, "createdAt"));
        Page<MlAddonExecution> executions;
        if (status != null) {
            executions = executionRepository.findByStatus(status, pageRequest);
        } else {
            executions = executionRepository.findAll(pageRequest);
        }
        return ResponseEntity.ok(executions);
    }

    /**
     * íŠ¹ì • ê¸°ì‚¬ì˜ ì‹¤í–‰ ì´ë ¥
     */
    @GetMapping("/executions/article/{articleId}")
    public ResponseEntity<List<MlAddonExecution>> getArticleExecutions(@PathVariable Long articleId) {
        return ResponseEntity.ok(executionRepository.findByArticleId(articleId));
    }

    // ========== ëª¨ë‹ˆí„°ë§ ==========

    /**
     * Add-on ìƒíƒœ ìš”ì•½
     * í”„ë¡ íŠ¸ì—”ë“œ MlAddonStatusSummary í˜•ì‹ì— ë§ì¶° ë°˜í™˜
     */
    @GetMapping("/status")
    public ResponseEntity<?> getStatus() {
        List<MlAddon> allAddons = addonRepository.findAll();
        long enabled = allAddons.stream().filter(MlAddon::getEnabled).count();
        long healthy = allAddons.stream()
                .filter(a -> a.getHealthStatus() == AddonHealthStatus.HEALTHY)
                .count();
        long unhealthy = allAddons.stream()
                .filter(a -> a.getHealthStatus() != AddonHealthStatus.HEALTHY && a.getHealthStatus() != AddonHealthStatus.UNKNOWN)
                .count();

        // ì˜¤ëŠ˜ì˜ ì‹¤í–‰ í†µê³„ ê³„ì‚°
        LocalDateTime todayStart = LocalDateTime.now().toLocalDate().atStartOfDay();
        List<MlAddonExecution> todayExecutions = executionRepository.findByCreatedAtAfter(todayStart);
        long totalExecutionsToday = todayExecutions.size();
        long successCount = todayExecutions.stream()
                .filter(e -> e.getStatus() == ExecutionStatus.SUCCESS)
                .count();
        double successRate = totalExecutionsToday > 0 
                ? (double) successCount / totalExecutionsToday * 100 
                : 0.0;
        
        // í‰ê·  ì§€ì—°ì‹œê°„ ê³„ì‚°
        double avgLatencyMs = todayExecutions.stream()
                .filter(e -> e.getLatencyMs() != null)
                .mapToLong(MlAddonExecution::getLatencyMs)
                .average()
                .orElse(0.0);
        
        // ì¹´í…Œê³ ë¦¬ë³„ addon ìˆ˜
        Map<String, Long> byCategory = allAddons.stream()
                .collect(java.util.stream.Collectors.groupingBy(
                        a -> a.getCategory().name(),
                        java.util.stream.Collectors.counting()
                ));
        
        return ResponseEntity.ok(Map.of(
                "totalAddons", allAddons.size(),
                "enabledAddons", enabled,
                "healthyAddons", healthy,
                "unhealthyAddons", unhealthy,
                "totalExecutionsToday", totalExecutionsToday,
                "successRate", Math.round(successRate * 100.0) / 100.0,
                "avgLatencyMs", Math.round(avgLatencyMs * 100.0) / 100.0,
                "byCategory", byCategory
        ));
    }

    /**
     * í—¬ìŠ¤ì²´í¬ ìˆ˜ë™ ì‹¤í–‰
     */
    @PostMapping("/health-check")
    public ResponseEntity<?> runHealthCheck() {
        orchestratorService.runHealthChecks();
        return ResponseEntity.ok(Map.of("status", "Health check started"));
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/ProjectController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.dto.PageResponse;
import com.newsinsight.collector.entity.project.*;
import com.newsinsight.collector.service.ProjectService;
import com.newsinsight.collector.service.ProjectService.*;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.data.domain.Page;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.List;
import java.util.Map;

/**
 * REST Controller for Project API.
 * Provides endpoints for project CRUD, members, items, and activities.
 */
@RestController
@RequestMapping("/api/v1/projects")
@RequiredArgsConstructor
@Slf4j
public class ProjectController {

    private final ProjectService projectService;

    // ============================================
    // Project CRUD
    // ============================================

    /**
     * Create a new project.
     */
    @PostMapping
    public ResponseEntity<Project> createProject(@RequestBody CreateProjectRequest request) {
        log.info("Creating project: name='{}', owner={}", request.getName(), request.getOwnerId());

        if (request.getName() == null || request.getName().isBlank()) {
            return ResponseEntity.badRequest().build();
        }
        if (request.getOwnerId() == null || request.getOwnerId().isBlank()) {
            return ResponseEntity.badRequest().build();
        }

        Project project = projectService.createProject(request);
        return ResponseEntity.status(HttpStatus.CREATED).body(project);
    }

    /**
     * Get project by ID.
     */
    @GetMapping("/{id}")
    public ResponseEntity<Project> getProject(
            @PathVariable Long id,
            @RequestParam(required = false) String userId
    ) {
        if (userId != null) {
            return projectService.getProjectWithAccess(id, userId)
                    .map(ResponseEntity::ok)
                    .orElse(ResponseEntity.notFound().build());
        }
        return projectService.getProject(id)
                .map(ResponseEntity::ok)
                .orElse(ResponseEntity.notFound().build());
    }

    /**
     * Update project.
     */
    @PutMapping("/{id}")
    public ResponseEntity<Project> updateProject(
            @PathVariable Long id,
            @RequestBody UpdateProjectRequest request,
            @RequestParam String userId
    ) {
        try {
            Project updated = projectService.updateProject(id, request, userId);
            return ResponseEntity.ok(updated);
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        } catch (IllegalStateException e) {
            return ResponseEntity.status(HttpStatus.FORBIDDEN).build();
        }
    }

    /**
     * Update project status.
     */
    @PutMapping("/{id}/status")
    public ResponseEntity<Project> updateProjectStatus(
            @PathVariable Long id,
            @RequestBody Map<String, String> body,
            @RequestParam String userId
    ) {
        String statusStr = body.get("status");
        if (statusStr == null) {
            return ResponseEntity.badRequest().build();
        }

        try {
            Project.ProjectStatus status = Project.ProjectStatus.valueOf(statusStr.toUpperCase());
            Project updated = projectService.updateProjectStatus(id, status, userId);
            return ResponseEntity.ok(updated);
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        } catch (IllegalStateException e) {
            return ResponseEntity.status(HttpStatus.FORBIDDEN).build();
        }
    }

    /**
     * Delete project.
     */
    @DeleteMapping("/{id}")
    public ResponseEntity<Void> deleteProject(
            @PathVariable Long id,
            @RequestParam String userId
    ) {
        try {
            projectService.deleteProject(id, userId);
            return ResponseEntity.noContent().build();
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        } catch (IllegalStateException e) {
            return ResponseEntity.status(HttpStatus.FORBIDDEN).build();
        }
    }

    /**
     * Get projects by owner.
     */
    @GetMapping
    public ResponseEntity<PageResponse<Project>> getProjects(
            @RequestParam String ownerId,
            @RequestParam(required = false) String status,
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size
    ) {
        Page<Project> result;

        if (status != null) {
            Project.ProjectStatus projectStatus = Project.ProjectStatus.valueOf(status.toUpperCase());
            result = projectService.getProjectsByOwnerAndStatus(ownerId, projectStatus, page, size);
        } else {
            result = projectService.getProjectsByOwner(ownerId, page, size);
        }

        PageResponse<Project> response = new PageResponse<>(
                result.getContent(),
                result.getNumber(),
                result.getSize(),
                result.getTotalElements(),
                result.getTotalPages(),
                result.isFirst(),
                result.isLast(),
                result.hasNext(),
                result.hasPrevious()
        );

        return ResponseEntity.ok(response);
    }

    /**
     * Search projects.
     */
    @GetMapping("/search")
    public ResponseEntity<PageResponse<Project>> searchProjects(
            @RequestParam String q,
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size
    ) {
        Page<Project> result = projectService.searchProjects(q, page, size);

        PageResponse<Project> response = new PageResponse<>(
                result.getContent(),
                result.getNumber(),
                result.getSize(),
                result.getTotalElements(),
                result.getTotalPages(),
                result.isFirst(),
                result.isLast(),
                result.hasNext(),
                result.hasPrevious()
        );

        return ResponseEntity.ok(response);
    }

    /**
     * Get or create default project.
     */
    @GetMapping("/default")
    public ResponseEntity<Project> getDefaultProject(@RequestParam String userId) {
        Project project = projectService.getOrCreateDefaultProject(userId);
        return ResponseEntity.ok(project);
    }

    /**
     * Get project statistics.
     */
    @GetMapping("/{id}/stats")
    public ResponseEntity<Map<String, Object>> getProjectStats(@PathVariable Long id) {
        try {
            Map<String, Object> stats = projectService.getProjectStats(id);
            return ResponseEntity.ok(stats);
        } catch (Exception e) {
            return ResponseEntity.notFound().build();
        }
    }

    // ============================================
    // Member Management
    // ============================================

    /**
     * Get project members.
     */
    @GetMapping("/{id}/members")
    public ResponseEntity<List<ProjectMember>> getMembers(@PathVariable Long id) {
        List<ProjectMember> members = projectService.getMembers(id);
        return ResponseEntity.ok(members);
    }

    /**
     * Get active members.
     */
    @GetMapping("/{id}/members/active")
    public ResponseEntity<List<ProjectMember>> getActiveMembers(@PathVariable Long id) {
        List<ProjectMember> members = projectService.getActiveMembers(id);
        return ResponseEntity.ok(members);
    }

    /**
     * Invite member.
     */
    @PostMapping("/{id}/members/invite")
    public ResponseEntity<ProjectMember> inviteMember(
            @PathVariable Long id,
            @RequestBody Map<String, String> body,
            @RequestParam String invitedBy
    ) {
        String userId = body.get("userId");
        String roleStr = body.get("role");

        if (userId == null || userId.isBlank()) {
            return ResponseEntity.badRequest().build();
        }

        ProjectMember.MemberRole role = roleStr != null 
                ? ProjectMember.MemberRole.valueOf(roleStr.toUpperCase())
                : ProjectMember.MemberRole.VIEWER;

        try {
            ProjectMember member = projectService.inviteMember(id, userId, role, invitedBy);
            return ResponseEntity.status(HttpStatus.CREATED).body(member);
        } catch (IllegalStateException e) {
            return ResponseEntity.status(HttpStatus.CONFLICT).build();
        }
    }

    /**
     * Accept invitation.
     */
    @PostMapping("/invitations/{token}/accept")
    public ResponseEntity<ProjectMember> acceptInvitation(
            @PathVariable String token,
            @RequestParam String userId
    ) {
        try {
            ProjectMember member = projectService.acceptInvitation(token, userId);
            return ResponseEntity.ok(member);
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        } catch (IllegalStateException e) {
            return ResponseEntity.status(HttpStatus.FORBIDDEN).build();
        }
    }

    /**
     * Remove member.
     */
    @DeleteMapping("/{id}/members/{userId}")
    public ResponseEntity<Void> removeMember(
            @PathVariable Long id,
            @PathVariable String userId,
            @RequestParam String removedBy
    ) {
        try {
            projectService.removeMember(id, userId, removedBy);
            return ResponseEntity.noContent().build();
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        } catch (IllegalStateException e) {
            return ResponseEntity.status(HttpStatus.FORBIDDEN).build();
        }
    }

    /**
     * Update member role.
     */
    @PutMapping("/{id}/members/{userId}/role")
    public ResponseEntity<ProjectMember> updateMemberRole(
            @PathVariable Long id,
            @PathVariable String userId,
            @RequestBody Map<String, String> body,
            @RequestParam String updatedBy
    ) {
        String roleStr = body.get("role");
        if (roleStr == null) {
            return ResponseEntity.badRequest().build();
        }

        try {
            ProjectMember.MemberRole role = ProjectMember.MemberRole.valueOf(roleStr.toUpperCase());
            ProjectMember member = projectService.updateMemberRole(id, userId, role, updatedBy);
            return ResponseEntity.ok(member);
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        } catch (IllegalStateException e) {
            return ResponseEntity.status(HttpStatus.FORBIDDEN).build();
        }
    }

    // ============================================
    // Item Management
    // ============================================

    /**
     * Add item to project.
     */
    @PostMapping("/{id}/items")
    public ResponseEntity<ProjectItem> addItem(
            @PathVariable Long id,
            @RequestBody AddItemRequest request,
            @RequestParam String userId
    ) {
        try {
            ProjectItem item = projectService.addItem(id, request, userId);
            return ResponseEntity.status(HttpStatus.CREATED).body(item);
        } catch (IllegalStateException e) {
            return ResponseEntity.status(HttpStatus.FORBIDDEN).build();
        }
    }

    /**
     * Get project items.
     */
    @GetMapping("/{id}/items")
    public ResponseEntity<PageResponse<ProjectItem>> getItems(
            @PathVariable Long id,
            @RequestParam(required = false) String type,
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size
    ) {
        Page<ProjectItem> result;

        if (type != null) {
            ProjectItem.ItemType itemType = ProjectItem.ItemType.valueOf(type.toUpperCase());
            result = projectService.getItemsByType(id, itemType, page, size);
        } else {
            result = projectService.getItems(id, page, size);
        }

        PageResponse<ProjectItem> response = new PageResponse<>(
                result.getContent(),
                result.getNumber(),
                result.getSize(),
                result.getTotalElements(),
                result.getTotalPages(),
                result.isFirst(),
                result.isLast(),
                result.hasNext(),
                result.hasPrevious()
        );

        return ResponseEntity.ok(response);
    }

    /**
     * Search items.
     */
    @GetMapping("/{id}/items/search")
    public ResponseEntity<PageResponse<ProjectItem>> searchItems(
            @PathVariable Long id,
            @RequestParam String q,
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size
    ) {
        Page<ProjectItem> result = projectService.searchItems(id, q, page, size);

        PageResponse<ProjectItem> response = new PageResponse<>(
                result.getContent(),
                result.getNumber(),
                result.getSize(),
                result.getTotalElements(),
                result.getTotalPages(),
                result.isFirst(),
                result.isLast(),
                result.hasNext(),
                result.hasPrevious()
        );

        return ResponseEntity.ok(response);
    }

    /**
     * Mark item as read.
     */
    @PostMapping("/{projectId}/items/{itemId}/read")
    public ResponseEntity<Void> markItemAsRead(
            @PathVariable Long projectId,
            @PathVariable Long itemId,
            @RequestParam String userId
    ) {
        projectService.markItemAsRead(itemId, userId);
        return ResponseEntity.ok().build();
    }

    /**
     * Toggle item bookmark.
     */
    @PostMapping("/{projectId}/items/{itemId}/bookmark")
    public ResponseEntity<Void> toggleItemBookmark(
            @PathVariable Long projectId,
            @PathVariable Long itemId,
            @RequestParam String userId
    ) {
        projectService.toggleItemBookmark(itemId, userId);
        return ResponseEntity.ok().build();
    }

    /**
     * Delete item.
     */
    @DeleteMapping("/{projectId}/items/{itemId}")
    public ResponseEntity<Void> deleteItem(
            @PathVariable Long projectId,
            @PathVariable Long itemId,
            @RequestParam String userId
    ) {
        try {
            projectService.deleteItem(projectId, itemId, userId);
            return ResponseEntity.noContent().build();
        } catch (IllegalStateException e) {
            return ResponseEntity.status(HttpStatus.FORBIDDEN).build();
        }
    }

    // ============================================
    // Activity Log
    // ============================================

    /**
     * Get project activity log.
     */
    @GetMapping("/{id}/activities")
    public ResponseEntity<PageResponse<ProjectActivityLog>> getActivityLog(
            @PathVariable Long id,
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size
    ) {
        Page<ProjectActivityLog> result = projectService.getActivityLog(id, page, size);

        PageResponse<ProjectActivityLog> response = new PageResponse<>(
                result.getContent(),
                result.getNumber(),
                result.getSize(),
                result.getTotalElements(),
                result.getTotalPages(),
                result.isFirst(),
                result.isLast(),
                result.hasNext(),
                result.hasPrevious()
        );

        return ResponseEntity.ok(response);
    }

    /**
     * Get recent activity.
     */
    @GetMapping("/{id}/activities/recent")
    public ResponseEntity<List<ProjectActivityLog>> getRecentActivity(@PathVariable Long id) {
        List<ProjectActivityLog> activities = projectService.getRecentActivity(id);
        return ResponseEntity.ok(activities);
    }

    // ============================================
    // Notifications
    // ============================================

    /**
     * Get user notifications.
     */
    @GetMapping("/notifications")
    public ResponseEntity<PageResponse<ProjectNotification>> getUserNotifications(
            @RequestParam String userId,
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size
    ) {
        Page<ProjectNotification> result = projectService.getUserNotifications(userId, page, size);

        PageResponse<ProjectNotification> response = new PageResponse<>(
                result.getContent(),
                result.getNumber(),
                result.getSize(),
                result.getTotalElements(),
                result.getTotalPages(),
                result.isFirst(),
                result.isLast(),
                result.hasNext(),
                result.hasPrevious()
        );

        return ResponseEntity.ok(response);
    }

    /**
     * Get unread notifications.
     */
    @GetMapping("/notifications/unread")
    public ResponseEntity<List<ProjectNotification>> getUnreadNotifications(@RequestParam String userId) {
        List<ProjectNotification> notifications = projectService.getUnreadNotifications(userId);
        return ResponseEntity.ok(notifications);
    }

    /**
     * Mark notification as read.
     */
    @PostMapping("/notifications/{notificationId}/read")
    public ResponseEntity<Void> markNotificationAsRead(@PathVariable Long notificationId) {
        projectService.markNotificationAsRead(notificationId);
        return ResponseEntity.ok().build();
    }

    /**
     * Mark all notifications as read.
     */
    @PostMapping("/notifications/read-all")
    public ResponseEntity<Void> markAllNotificationsAsRead(@RequestParam String userId) {
        projectService.markAllNotificationsAsRead(userId);
        return ResponseEntity.ok().build();
    }

    // ============================================
    // Health
    // ============================================

    /**
     * Health check.
     */
    @GetMapping("/health")
    public ResponseEntity<Map<String, Object>> healthCheck() {
        return ResponseEntity.ok(Map.of(
                "status", "available",
                "features", Map.of(
                        "projects", true,
                        "members", true,
                        "items", true,
                        "activities", true,
                        "notifications", true
                )
        ));
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/ReportController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.dto.report.ReportMetadata;
import com.newsinsight.collector.dto.report.ReportRequest;
import com.newsinsight.collector.service.report.ReportGenerationService;
import io.swagger.v3.oas.annotations.Operation;
import io.swagger.v3.oas.annotations.tags.Tag;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.http.HttpHeaders;
import org.springframework.http.HttpStatus;
import org.springframework.http.MediaType;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.io.IOException;
import java.net.URLEncoder;
import java.nio.charset.StandardCharsets;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;

/**
 * ë³´ê³ ì„œ ìƒì„± ë° ë‹¤ìš´ë¡œë“œ REST API ì»¨íŠ¸ë¡¤ëŸ¬
 */
@RestController
@RequestMapping("/api/v1/reports")
@RequiredArgsConstructor
@Slf4j
@Tag(name = "Reports", description = "PDF ë³´ê³ ì„œ ìƒì„± ë° ë‹¤ìš´ë¡œë“œ API")
public class ReportController {

    private final ReportGenerationService reportGenerationService;

    /**
     * í†µí•© ê²€ìƒ‰ ë³´ê³ ì„œ ìƒì„± ìš”ì²­ (ë¹„ë™ê¸°)
     * 
     * @param jobId í†µí•© ê²€ìƒ‰ Job ID
     * @param request ë³´ê³ ì„œ ìƒì„± ìš”ì²­
     * @return ë³´ê³ ì„œ ë©”íƒ€ë°ì´í„°
     */
    @PostMapping("/unified-search/{jobId}")
    @Operation(summary = "í†µí•© ê²€ìƒ‰ ë³´ê³ ì„œ ìƒì„± ìš”ì²­", description = "ë¹„ë™ê¸°ë¡œ PDF ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
    public ResponseEntity<ReportMetadata> requestUnifiedSearchReport(
            @PathVariable String jobId,
            @RequestBody ReportRequest request) {
        
        log.info("Report generation requested: jobId={}, query={}", jobId, request.getQuery());
        
        ReportMetadata metadata = reportGenerationService.requestUnifiedSearchReport(jobId, request);
        
        return ResponseEntity.accepted().body(metadata);
    }

    /**
     * í†µí•© ê²€ìƒ‰ ë³´ê³ ì„œ ì¦‰ì‹œ ë‹¤ìš´ë¡œë“œ (ë™ê¸°)
     * 
     * @param jobId í†µí•© ê²€ìƒ‰ Job ID
     * @param request ë³´ê³ ì„œ ìƒì„± ìš”ì²­
     * @return PDF íŒŒì¼
     */
    @PostMapping("/unified-search/{jobId}/export")
    @Operation(summary = "í†µí•© ê²€ìƒ‰ ë³´ê³ ì„œ ì¦‰ì‹œ ë‹¤ìš´ë¡œë“œ", description = "ë™ê¸°ë¡œ PDF ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ê³  ì¦‰ì‹œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.")
    public ResponseEntity<byte[]> exportUnifiedSearchReport(
            @PathVariable String jobId,
            @RequestBody ReportRequest request) {
        
        log.info("Report export requested: jobId={}, query={}", jobId, request.getQuery());
        
        try {
            byte[] pdfBytes = reportGenerationService.generateReportSync(jobId, request);
            
            String filename = generateFilename(request.getQuery(), "í†µí•©ê²€ìƒ‰");
            
            HttpHeaders headers = new HttpHeaders();
            headers.setContentType(MediaType.APPLICATION_PDF);
            headers.setContentDispositionFormData("attachment", filename);
            headers.setContentLength(pdfBytes.length);
            
            return ResponseEntity.ok()
                    .headers(headers)
                    .body(pdfBytes);
                    
        } catch (IllegalArgumentException e) {
            log.warn("Report export failed - not found: jobId={}, error={}", jobId, e.getMessage());
            return ResponseEntity.notFound().build();
        } catch (IOException e) {
            log.error("Report export failed - IO error: jobId={}, error={}", jobId, e.getMessage(), e);
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).build();
        }
    }

    /**
     * ë³´ê³ ì„œ ìƒíƒœ ì¡°íšŒ
     * 
     * @param reportId ë³´ê³ ì„œ ID
     * @return ë³´ê³ ì„œ ë©”íƒ€ë°ì´í„°
     */
    @GetMapping("/{reportId}")
    @Operation(summary = "ë³´ê³ ì„œ ìƒíƒœ ì¡°íšŒ", description = "ìƒì„± ì¤‘ì´ê±°ë‚˜ ì™„ë£Œëœ ë³´ê³ ì„œì˜ ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.")
    public ResponseEntity<ReportMetadata> getReportStatus(@PathVariable String reportId) {
        ReportMetadata metadata = reportGenerationService.getReportMetadata(reportId);
        
        if (metadata == null) {
            return ResponseEntity.notFound().build();
        }
        
        return ResponseEntity.ok(metadata);
    }

    /**
     * ìƒì„±ëœ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ
     * 
     * @param reportId ë³´ê³ ì„œ ID
     * @return PDF íŒŒì¼
     */
    @GetMapping("/{reportId}/download")
    @Operation(summary = "ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ", description = "ìƒì„±ëœ PDF ë³´ê³ ì„œë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.")
    public ResponseEntity<byte[]> downloadReport(@PathVariable String reportId) {
        ReportMetadata metadata = reportGenerationService.getReportMetadata(reportId);
        
        if (metadata == null) {
            return ResponseEntity.notFound().build();
        }
        
        if (metadata.getStatus() != ReportMetadata.ReportStatus.COMPLETED) {
            return ResponseEntity.status(HttpStatus.ACCEPTED)
                    .header("X-Report-Status", metadata.getStatus().name())
                    .build();
        }
        
        try {
            byte[] pdfBytes = reportGenerationService.downloadReport(reportId);
            
            String filename = generateFilename(metadata.getQuery(), "ë³´ê³ ì„œ");
            
            HttpHeaders headers = new HttpHeaders();
            headers.setContentType(MediaType.APPLICATION_PDF);
            headers.setContentDispositionFormData("attachment", filename);
            headers.setContentLength(pdfBytes.length);
            
            return ResponseEntity.ok()
                    .headers(headers)
                    .body(pdfBytes);
                    
        } catch (IllegalArgumentException e) {
            log.warn("Report download failed - not found: reportId={}", reportId);
            return ResponseEntity.notFound().build();
        }
    }

    /**
     * DeepSearch ë³´ê³ ì„œ ì¦‰ì‹œ ë‹¤ìš´ë¡œë“œ (ë™ê¸°)
     * 
     * @param jobId DeepSearch Job ID
     * @param request ë³´ê³ ì„œ ìƒì„± ìš”ì²­
     * @return PDF íŒŒì¼
     */
    @PostMapping("/deep-search/{jobId}/export")
    @Operation(summary = "DeepSearch ë³´ê³ ì„œ ì¦‰ì‹œ ë‹¤ìš´ë¡œë“œ", description = "DeepSearch ê²°ê³¼ë¥¼ PDF ë³´ê³ ì„œë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤.")
    public ResponseEntity<byte[]> exportDeepSearchReport(
            @PathVariable String jobId,
            @RequestBody ReportRequest request) {
        
        log.info("DeepSearch report export requested: jobId={}", jobId);
        
        // TODO: DeepSearch ì „ìš© ë³´ê³ ì„œ ìƒì„± ë¡œì§ êµ¬í˜„ í•„ìš”
        // í˜„ì¬ëŠ” í†µí•© ê²€ìƒ‰ ë³´ê³ ì„œë¡œ ëŒ€ì²´
        
        try {
            request = ReportRequest.builder()
                    .reportType(ReportRequest.ReportType.DEEP_SEARCH)
                    .targetId(jobId)
                    .query(request.getQuery())
                    .timeWindow(request.getTimeWindow())
                    .includeSections(request.getIncludeSections())
                    .chartImages(request.getChartImages())
                    .build();
            
            byte[] pdfBytes = reportGenerationService.generateReportSync(jobId, request);
            
            String filename = generateFilename(request.getQuery(), "DeepSearch");
            
            HttpHeaders headers = new HttpHeaders();
            headers.setContentType(MediaType.APPLICATION_PDF);
            headers.setContentDispositionFormData("attachment", filename);
            headers.setContentLength(pdfBytes.length);
            
            return ResponseEntity.ok()
                    .headers(headers)
                    .body(pdfBytes);
                    
        } catch (IllegalArgumentException e) {
            log.warn("DeepSearch report export failed - not found: jobId={}", jobId);
            return ResponseEntity.notFound().build();
        } catch (IOException e) {
            log.error("DeepSearch report export failed: jobId={}, error={}", jobId, e.getMessage(), e);
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).build();
        }
    }

    /**
     * ML ë¶„ì„ ë³´ê³ ì„œ ì¦‰ì‹œ ë‹¤ìš´ë¡œë“œ (ë™ê¸°)
     * 
     * @param articleId ê¸°ì‚¬ ID
     * @param request ë³´ê³ ì„œ ìƒì„± ìš”ì²­
     * @return PDF íŒŒì¼
     */
    @PostMapping("/ml-analysis/{articleId}/export")
    @Operation(summary = "ML ë¶„ì„ ë³´ê³ ì„œ ì¦‰ì‹œ ë‹¤ìš´ë¡œë“œ", description = "ê¸°ì‚¬ì˜ ML ë¶„ì„ ê²°ê³¼ë¥¼ PDF ë³´ê³ ì„œë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤.")
    public ResponseEntity<byte[]> exportMlAnalysisReport(
            @PathVariable Long articleId,
            @RequestBody ReportRequest request) {
        
        log.info("ML analysis report export requested: articleId={}", articleId);
        
        // TODO: ML ë¶„ì„ ì „ìš© ë³´ê³ ì„œ ìƒì„± ë¡œì§ êµ¬í˜„
        
        return ResponseEntity.status(HttpStatus.NOT_IMPLEMENTED)
                .header("X-Message", "ML analysis report is not yet implemented")
                .build();
    }

    // ===== í—¬í¼ ë©”ì„œë“œ =====

    /**
     * PDF íŒŒì¼ëª… ìƒì„±
     */
    private String generateFilename(String query, String type) {
        String dateStr = LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyyMMdd_HHmm"));
        String safeQuery = query != null ? query.replaceAll("[^ê°€-í£a-zA-Z0-9]", "_") : "report";
        if (safeQuery.length() > 30) {
            safeQuery = safeQuery.substring(0, 30);
        }
        
        String filename = String.format("NewsInsight_%s_%s_%s.pdf", type, safeQuery, dateStr);
        
        // URL ì¸ì½”ë”© (í•œê¸€ íŒŒì¼ëª… ì§€ì›)
        return URLEncoder.encode(filename, StandardCharsets.UTF_8)
                .replace("+", "%20");
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/SearchHistoryController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.dto.PageResponse;
import com.newsinsight.collector.dto.SearchHistoryDto;
import com.newsinsight.collector.dto.SearchHistoryMessage;
import com.newsinsight.collector.entity.search.SearchHistory;
import com.newsinsight.collector.entity.search.SearchType;
import com.newsinsight.collector.service.SearchHistoryEventService;
import com.newsinsight.collector.service.SearchHistoryService;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.data.domain.Page;
import org.springframework.http.HttpStatus;
import org.springframework.http.MediaType;
import org.springframework.http.ResponseEntity;
import org.springframework.http.codec.ServerSentEvent;
import org.springframework.web.bind.annotation.*;
import reactor.core.publisher.Flux;

import java.util.List;
import java.util.Map;

/**
 * REST Controller for Search History API.
 * Provides endpoints for saving, querying, and managing search history.
 */
@RestController
@RequestMapping("/api/v1/search-history")
@RequiredArgsConstructor
@Slf4j
public class SearchHistoryController {

    private final SearchHistoryService searchHistoryService;
    private final SearchHistoryEventService searchHistoryEventService;

    // ============================================
    // Create / Save
    // ============================================

    /**
     * Save search result asynchronously via Kafka.
     * This is the primary endpoint for saving search results.
     */
    @PostMapping
    public ResponseEntity<Map<String, Object>> saveSearchHistory(@RequestBody SearchHistoryDto request) {
        log.info("Saving search history: type={}, query='{}'", request.getSearchType(), request.getQuery());
        
        if (request.getQuery() == null || request.getQuery().isBlank()) {
            return ResponseEntity.badRequest().body(Map.of(
                    "error", "Query is required"
            ));
        }
        
        if (request.getSearchType() == null) {
            return ResponseEntity.badRequest().body(Map.of(
                    "error", "Search type is required"
            ));
        }

        // Convert to message and send to Kafka
        SearchHistoryMessage message = request.toMessage();
        searchHistoryService.sendToKafka(message);

        return ResponseEntity.status(HttpStatus.ACCEPTED).body(Map.of(
                "message", "Search history queued for saving",
                "externalId", message.getExternalId() != null ? message.getExternalId() : "",
                "searchType", message.getSearchType().name(),
                "query", message.getQuery()
        ));
    }

    /**
     * Save search result synchronously (for immediate persistence).
     */
    @PostMapping("/sync")
    public ResponseEntity<SearchHistoryDto> saveSearchHistorySync(@RequestBody SearchHistoryDto request) {
        log.info("Saving search history synchronously: type={}, query='{}'", 
                request.getSearchType(), request.getQuery());
        
        SearchHistoryMessage message = request.toMessage();
        SearchHistory saved = searchHistoryService.saveFromMessage(message);
        
        return ResponseEntity.status(HttpStatus.CREATED)
                .body(SearchHistoryDto.fromEntity(saved));
    }

    // ============================================
    // Read / Query
    // ============================================

    /**
     * Get search history by ID.
     */
    @GetMapping("/{id}")
    public ResponseEntity<SearchHistoryDto> getById(@PathVariable Long id) {
        return searchHistoryService.findById(id)
                .map(SearchHistoryDto::fromEntity)
                .map(ResponseEntity::ok)
                .orElse(ResponseEntity.notFound().build());
    }

    /**
     * Get search history by external ID (e.g., jobId).
     */
    @GetMapping("/external/{externalId}")
    public ResponseEntity<SearchHistoryDto> getByExternalId(@PathVariable String externalId) {
        return searchHistoryService.findByExternalId(externalId)
                .map(SearchHistoryDto::fromEntity)
                .map(ResponseEntity::ok)
                .orElse(ResponseEntity.notFound().build());
    }

    /**
     * Get paginated search history.
     */
    @GetMapping
    public ResponseEntity<PageResponse<SearchHistoryDto>> getAll(
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size,
            @RequestParam(defaultValue = "createdAt") String sortBy,
            @RequestParam(defaultValue = "DESC") String sortDirection,
            @RequestParam(required = false) String type,
            @RequestParam(required = false) String userId,
            @RequestHeader(value = "X-User-Id", required = false) String headerUserId,
            @RequestHeader(value = "X-Session-Id", required = false) String sessionId
    ) {
        // Use header userId if not provided in query param
        String effectiveUserId = userId != null ? userId : headerUserId;
        
        Page<SearchHistory> result;
        
        if (type != null && effectiveUserId != null) {
            SearchType searchType = SearchType.valueOf(type.toUpperCase());
            result = searchHistoryService.findByUserAndType(effectiveUserId, searchType, page, size);
        } else if (type != null) {
            SearchType searchType = SearchType.valueOf(type.toUpperCase());
            result = searchHistoryService.findByType(searchType, page, size);
        } else if (effectiveUserId != null) {
            result = searchHistoryService.findByUser(effectiveUserId, page, size);
        } else {
            result = searchHistoryService.findAll(page, size, sortBy, sortDirection);
        }

        PageResponse<SearchHistoryDto> response = new PageResponse<>(
                result.getContent().stream()
                        .map(SearchHistoryDto::fromEntity)
                        .toList(),
                result.getNumber(),
                result.getSize(),
                result.getTotalElements(),
                result.getTotalPages(),
                result.isFirst(),
                result.isLast(),
                result.hasNext(),
                result.hasPrevious()
        );

        return ResponseEntity.ok(response);
    }

    /**
     * Search history by query text.
     */
    @GetMapping("/search")
    public ResponseEntity<PageResponse<SearchHistoryDto>> searchByQuery(
            @RequestParam String q,
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size
    ) {
        Page<SearchHistory> result = searchHistoryService.searchByQuery(q, page, size);
        
        PageResponse<SearchHistoryDto> response = new PageResponse<>(
                result.getContent().stream()
                        .map(SearchHistoryDto::fromEntity)
                        .toList(),
                result.getNumber(),
                result.getSize(),
                result.getTotalElements(),
                result.getTotalPages(),
                result.isFirst(),
                result.isLast(),
                result.hasNext(),
                result.hasPrevious()
        );

        return ResponseEntity.ok(response);
    }

    /**
     * Get bookmarked searches.
     */
    @GetMapping("/bookmarked")
    public ResponseEntity<PageResponse<SearchHistoryDto>> getBookmarked(
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size
    ) {
        Page<SearchHistory> result = searchHistoryService.findBookmarked(page, size);
        
        PageResponse<SearchHistoryDto> response = new PageResponse<>(
                result.getContent().stream()
                        .map(SearchHistoryDto::fromEntity)
                        .toList(),
                result.getNumber(),
                result.getSize(),
                result.getTotalElements(),
                result.getTotalPages(),
                result.isFirst(),
                result.isLast(),
                result.hasNext(),
                result.hasPrevious()
        );

        return ResponseEntity.ok(response);
    }

    /**
     * Get derived searches from a parent.
     */
    @GetMapping("/{id}/derived")
    public ResponseEntity<List<SearchHistoryDto>> getDerivedSearches(@PathVariable Long id) {
        List<SearchHistory> derived = searchHistoryService.findDerivedSearches(id);
        List<SearchHistoryDto> response = derived.stream()
                .map(SearchHistoryDto::fromEntity)
                .toList();
        return ResponseEntity.ok(response);
    }

    /**
     * Get searches by session.
     */
    @GetMapping("/session/{sessionId}")
    public ResponseEntity<List<SearchHistoryDto>> getBySession(@PathVariable String sessionId) {
        List<SearchHistory> searches = searchHistoryService.findBySession(sessionId);
        List<SearchHistoryDto> response = searches.stream()
                .map(SearchHistoryDto::fromEntity)
                .toList();
        return ResponseEntity.ok(response);
    }

    // ============================================
    // Update
    // ============================================

    /**
     * Toggle bookmark status.
     */
    @PostMapping("/{id}/bookmark")
    public ResponseEntity<SearchHistoryDto> toggleBookmark(@PathVariable Long id) {
        try {
            SearchHistory updated = searchHistoryService.toggleBookmark(id);
            return ResponseEntity.ok(SearchHistoryDto.fromEntity(updated));
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    /**
     * Update tags.
     */
    @PutMapping("/{id}/tags")
    public ResponseEntity<SearchHistoryDto> updateTags(
            @PathVariable Long id,
            @RequestBody List<String> tags
    ) {
        try {
            SearchHistory updated = searchHistoryService.updateTags(id, tags);
            return ResponseEntity.ok(SearchHistoryDto.fromEntity(updated));
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    /**
     * Update notes.
     */
    @PutMapping("/{id}/notes")
    public ResponseEntity<SearchHistoryDto> updateNotes(
            @PathVariable Long id,
            @RequestBody Map<String, String> body
    ) {
        String notes = body.get("notes");
        try {
            SearchHistory updated = searchHistoryService.updateNotes(id, notes);
            return ResponseEntity.ok(SearchHistoryDto.fromEntity(updated));
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    // ============================================
    // Delete
    // ============================================

    /**
     * Delete search history by ID.
     */
    @DeleteMapping("/{id}")
    public ResponseEntity<Void> delete(@PathVariable Long id) {
        if (searchHistoryService.findById(id).isEmpty()) {
            return ResponseEntity.notFound().build();
        }
        searchHistoryService.delete(id);
        return ResponseEntity.noContent().build();
    }

    // ============================================
    // Derived Search (Drill-down)
    // ============================================

    /**
     * Create a derived search from a parent.
     * Used for drill-down functionality.
     */
    @PostMapping("/{parentId}/derive")
    public ResponseEntity<Map<String, Object>> createDerivedSearch(
            @PathVariable Long parentId,
            @RequestBody SearchHistoryDto request
    ) {
        log.info("Creating derived search from parent={}, query='{}'", parentId, request.getQuery());
        
        if (request.getQuery() == null || request.getQuery().isBlank()) {
            return ResponseEntity.badRequest().body(Map.of(
                    "error", "Query is required"
            ));
        }

        try {
            SearchHistoryMessage message = request.toMessage();
            SearchHistory derived = searchHistoryService.createDerivedSearch(parentId, message);
            
            return ResponseEntity.status(HttpStatus.CREATED).body(Map.of(
                    "id", derived.getId(),
                    "parentSearchId", parentId,
                    "depthLevel", derived.getDepthLevel(),
                    "query", derived.getQuery(),
                    "message", "Derived search created"
            ));
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    // ============================================
    // Statistics & Utilities
    // ============================================

    /**
     * Get search statistics.
     */
    @GetMapping("/stats")
    public ResponseEntity<Map<String, Object>> getStatistics(
            @RequestParam(defaultValue = "30") int days
    ) {
        return ResponseEntity.ok(searchHistoryService.getStatistics(days));
    }

    /**
     * Get recently discovered URLs.
     */
    @GetMapping("/discovered-urls")
    public ResponseEntity<List<String>> getDiscoveredUrls(
            @RequestParam(defaultValue = "7") int days,
            @RequestParam(defaultValue = "100") int limit
    ) {
        return ResponseEntity.ok(searchHistoryService.getRecentDiscoveredUrls(days, limit));
    }

    /**
     * Health check.
     */
    @GetMapping("/health")
    public ResponseEntity<Map<String, Object>> healthCheck() {
        return ResponseEntity.ok(Map.of(
                "status", "available",
                "features", Map.of(
                        "asyncSave", true,
                        "syncSave", true,
                        "derivedSearch", true,
                        "bookmarks", true,
                        "tags", true,
                        "statistics", true,
                        "sse", true
                ),
                "kafkaTopic", SearchHistoryService.SEARCH_HISTORY_TOPIC,
                "sseSubscribers", searchHistoryEventService.getSubscriberCount()
        ));
    }

    // ============================================
    // Continue Work Feature
    // ============================================

    /**
     * Get items for "Continue Work" feature.
     * Returns actionable searches: in-progress, failed, partial, draft, or unviewed completed.
     */
    @GetMapping("/continue-work")
    public ResponseEntity<Map<String, Object>> getContinueWorkItems(
            @RequestParam(required = false) String userId,
            @RequestParam(required = false) String sessionId,
            @RequestParam(required = false, defaultValue = "10") int limit,
            @RequestHeader(value = "X-User-Id", required = false) String headerUserId,
            @RequestHeader(value = "X-Session-Id", required = false) String headerSessionId
    ) {
        // Use headers if not provided in query params
        String effectiveUserId = userId != null ? userId : headerUserId;
        String effectiveSessionId = sessionId != null ? sessionId : headerSessionId;
        
        log.debug("Continue work request: userId={}, sessionId={}", effectiveUserId, effectiveSessionId);
        
        List<SearchHistory> items = searchHistoryService.findContinueWorkItems(
                effectiveUserId != null ? effectiveUserId : "", 
                effectiveSessionId != null ? effectiveSessionId : "", 
                limit
        );
        
        List<SearchHistoryDto> dtos = items.stream()
                .map(SearchHistoryDto::fromEntity)
                .toList();

        Map<String, Object> stats = searchHistoryService.getContinueWorkStats(
                effectiveUserId != null ? effectiveUserId : "", 
                effectiveSessionId != null ? effectiveSessionId : ""
        );

        return ResponseEntity.ok(Map.of(
                "items", dtos,
                "count", dtos.size(),
                "stats", stats
        ));
    }

    /**
     * Mark search as viewed.
     */
    @PostMapping("/{id}/viewed")
    public ResponseEntity<SearchHistoryDto> markAsViewed(@PathVariable Long id) {
        try {
            SearchHistory updated = searchHistoryService.markAsViewed(id);
            return ResponseEntity.ok(SearchHistoryDto.fromEntity(updated));
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    /**
     * Mark search as viewed by external ID.
     */
    @PostMapping("/external/{externalId}/viewed")
    public ResponseEntity<SearchHistoryDto> markAsViewedByExternalId(@PathVariable String externalId) {
        try {
            SearchHistory updated = searchHistoryService.markAsViewedByExternalId(externalId);
            return ResponseEntity.ok(SearchHistoryDto.fromEntity(updated));
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    /**
     * Update completion status.
     */
    @PutMapping("/{id}/status")
    public ResponseEntity<SearchHistoryDto> updateCompletionStatus(
            @PathVariable Long id,
            @RequestBody Map<String, String> body
    ) {
        String statusStr = body.get("status");
        if (statusStr == null || statusStr.isBlank()) {
            return ResponseEntity.badRequest().build();
        }

        try {
            SearchHistory.CompletionStatus status = SearchHistory.CompletionStatus.valueOf(statusStr.toUpperCase());
            SearchHistory updated = searchHistoryService.updateCompletionStatus(id, status);
            return ResponseEntity.ok(SearchHistoryDto.fromEntity(updated));
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    /**
     * Get searches by completion status.
     */
    @GetMapping("/status/{status}")
    public ResponseEntity<PageResponse<SearchHistoryDto>> getByCompletionStatus(
            @PathVariable String status,
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size
    ) {
        try {
            SearchHistory.CompletionStatus completionStatus = 
                    SearchHistory.CompletionStatus.valueOf(status.toUpperCase());
            
            Page<SearchHistory> result = searchHistoryService.findByCompletionStatus(completionStatus, page, size);
            
            PageResponse<SearchHistoryDto> response = new PageResponse<>(
                    result.getContent().stream()
                            .map(SearchHistoryDto::fromEntity)
                            .toList(),
                    result.getNumber(),
                    result.getSize(),
                    result.getTotalElements(),
                    result.getTotalPages(),
                    result.isFirst(),
                    result.isLast(),
                    result.hasNext(),
                    result.hasPrevious()
            );

            return ResponseEntity.ok(response);
        } catch (IllegalArgumentException e) {
            return ResponseEntity.badRequest().build();
        }
    }

    /**
     * Get searches by project ID.
     */
    @GetMapping("/project/{projectId}")
    public ResponseEntity<PageResponse<SearchHistoryDto>> getByProjectId(
            @PathVariable Long projectId,
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size
    ) {
        Page<SearchHistory> result = searchHistoryService.findByProjectId(projectId, page, size);
        
        PageResponse<SearchHistoryDto> response = new PageResponse<>(
                result.getContent().stream()
                        .map(SearchHistoryDto::fromEntity)
                        .toList(),
                result.getNumber(),
                result.getSize(),
                result.getTotalElements(),
                result.getTotalPages(),
                result.isFirst(),
                result.isLast(),
                result.hasNext(),
                result.hasPrevious()
        );

        return ResponseEntity.ok(response);
    }

    /**
     * Get failed searches for potential retry.
     */
    @GetMapping("/failed")
    public ResponseEntity<List<SearchHistoryDto>> getFailedSearches(
            @RequestParam(defaultValue = "7") int daysBack,
            @RequestParam(defaultValue = "20") int limit
    ) {
        List<SearchHistory> failed = searchHistoryService.findFailedSearches(daysBack, limit);
        List<SearchHistoryDto> response = failed.stream()
                .map(SearchHistoryDto::fromEntity)
                .toList();
        return ResponseEntity.ok(response);
    }

    // ============================================
    // SSE Real-time Stream
    // ============================================

    /**
     * SSE endpoint for real-time search history updates.
     * Clients can subscribe to receive notifications when:
     * - new_search: A new search was saved
     * - updated_search: An existing search was updated
     * - deleted_search: A search was deleted
     * - heartbeat: Keep-alive signal (every 30 seconds)
     */
    @GetMapping(value = "/stream", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
    public Flux<ServerSentEvent<SearchHistoryEventService.SearchHistoryEventDto>> streamSearchHistory() {
        log.info("New SSE client connected to search history stream");
        
        return searchHistoryEventService.getEventStream()
                .map(event -> ServerSentEvent.<SearchHistoryEventService.SearchHistoryEventDto>builder()
                        .id(String.valueOf(event.timestamp()))
                        .event(event.eventType())
                        .data(event)
                        .build());
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/SearchJobController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.entity.search.SearchType;
import com.newsinsight.collector.service.SearchJobQueueService;
import com.newsinsight.collector.service.SearchJobQueueService.*;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.http.HttpStatus;
import org.springframework.http.MediaType;
import org.springframework.http.ResponseEntity;
import org.springframework.http.codec.ServerSentEvent;
import org.springframework.web.bind.annotation.*;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Sinks;

import java.time.Duration;
import java.util.List;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;

/**
 * REST Controller for Search Job Queue API.
 * Enables concurrent search execution and real-time job monitoring.
 */
@RestController
@RequestMapping("/api/v1/jobs")
@RequiredArgsConstructor
@Slf4j
public class SearchJobController {

    private final SearchJobQueueService searchJobQueueService;

    // SSE sinks for job-specific streaming
    private final Map<String, Sinks.Many<SearchJobEvent>> jobSinks = new ConcurrentHashMap<>();

    // ============================================
    // Job Creation
    // ============================================

    /**
     * Start a new search job.
     * Supports concurrent execution of multiple job types.
     */
    @PostMapping
    public ResponseEntity<Map<String, Object>> startJob(
            @RequestBody JobStartRequest request,
            @RequestHeader(value = "X-User-Id", required = false) String userId,
            @RequestHeader(value = "X-Session-Id", required = false) String sessionId
    ) {
        log.info("Starting new search job: type={}, query='{}', userId={}, sessionId={}", 
                request.type(), request.query(), userId, sessionId);

        if (request.query() == null || request.query().isBlank()) {
            return ResponseEntity.badRequest().body(Map.of(
                    "error", "Query is required"
            ));
        }

        if (request.type() == null) {
            return ResponseEntity.badRequest().body(Map.of(
                    "error", "Search type is required"
            ));
        }

        // Use headers if request doesn't specify userId/sessionId
        String effectiveUserId = request.userId() != null ? request.userId() : userId;
        String effectiveSessionId = request.sessionId() != null ? request.sessionId() : sessionId;

        SearchJobRequest jobRequest = SearchJobRequest.builder()
                .type(request.type())
                .query(request.query())
                .timeWindow(request.timeWindow() != null ? request.timeWindow() : "7d")
                .userId(effectiveUserId)
                .sessionId(effectiveSessionId)
                .projectId(request.projectId())
                .options(request.options())
                .build();

        String jobId = searchJobQueueService.startJob(jobRequest);

        return ResponseEntity.status(HttpStatus.ACCEPTED).body(Map.of(
                "jobId", jobId,
                "type", request.type().name(),
                "query", request.query(),
                "status", "PENDING",
                "message", "ê²€ìƒ‰ ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤"
        ));
    }

    /**
     * Start multiple search jobs concurrently.
     * Enables running Unified Search, Deep Search, etc. at the same time.
     */
    @PostMapping("/batch")
    public ResponseEntity<Map<String, Object>> startBatchJobs(@RequestBody List<JobStartRequest> requests) {
        log.info("Starting batch jobs: count={}", requests.size());

        if (requests.isEmpty()) {
            return ResponseEntity.badRequest().body(Map.of(
                    "error", "At least one job request is required"
            ));
        }

        List<Map<String, Object>> startedJobs = requests.stream()
                .map(request -> {
                    SearchJobRequest jobRequest = SearchJobRequest.builder()
                            .type(request.type())
                            .query(request.query())
                            .timeWindow(request.timeWindow() != null ? request.timeWindow() : "7d")
                            .userId(request.userId())
                            .sessionId(request.sessionId())
                            .projectId(request.projectId())
                            .options(request.options())
                            .build();

                    String jobId = searchJobQueueService.startJob(jobRequest);

                    return Map.<String, Object>of(
                            "jobId", jobId,
                            "type", request.type().name(),
                            "query", request.query(),
                            "status", "PENDING"
                    );
                })
                .toList();

        return ResponseEntity.status(HttpStatus.ACCEPTED).body(Map.of(
                "jobs", startedJobs,
                "count", startedJobs.size(),
                "message", String.format("%dê°œì˜ ê²€ìƒ‰ ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤", startedJobs.size())
        ));
    }

    // ============================================
    // Job Status & Query
    // ============================================

    /**
     * Get status of a specific job.
     */
    @GetMapping("/{jobId}")
    public ResponseEntity<SearchJob> getJobStatus(@PathVariable String jobId) {
        return searchJobQueueService.getJobStatus(jobId)
                .map(ResponseEntity::ok)
                .orElse(ResponseEntity.notFound().build());
    }

    /**
     * Get active jobs for user.
     */
    @GetMapping("/active")
    public ResponseEntity<List<SearchJob>> getActiveJobs(
            @RequestParam(required = false) String userId,
            @RequestHeader(value = "X-User-Id", required = false) String headerUserId
    ) {
        // Use header userId if not provided in query param
        String effectiveUserId = userId != null ? userId : headerUserId;
        List<SearchJob> activeJobs = searchJobQueueService.getActiveJobs(effectiveUserId);
        return ResponseEntity.ok(activeJobs);
    }

    /**
     * Get all jobs for user (with limit).
     */
    @GetMapping
    public ResponseEntity<List<SearchJob>> getAllJobs(
            @RequestParam(required = false) String userId,
            @RequestParam(required = false, defaultValue = "20") int limit,
            @RequestHeader(value = "X-User-Id", required = false) String headerUserId
    ) {
        // Use header userId if not provided in query param
        String effectiveUserId = userId != null ? userId : headerUserId;
        List<SearchJob> jobs = searchJobQueueService.getAllJobs(effectiveUserId, limit);
        return ResponseEntity.ok(jobs);
    }

    // ============================================
    // Job Control
    // ============================================

    /**
     * Cancel a running job.
     */
    @PostMapping("/{jobId}/cancel")
    public ResponseEntity<Map<String, Object>> cancelJob(@PathVariable String jobId) {
        log.info("Cancelling job: jobId={}", jobId);

        boolean cancelled = searchJobQueueService.cancelJob(jobId);

        if (cancelled) {
            return ResponseEntity.ok(Map.of(
                    "jobId", jobId,
                    "status", "CANCELLED",
                    "message", "ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤"
            ));
        } else {
            return ResponseEntity.badRequest().body(Map.of(
                    "jobId", jobId,
                    "error", "ì‘ì—…ì„ ì·¨ì†Œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ì´ë¯¸ ì™„ë£Œë˜ì—ˆê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŒ)"
            ));
        }
    }

    // ============================================
    // SSE Real-time Job Streaming
    // ============================================

    /**
     * SSE endpoint for real-time job updates.
     * Stream updates for a specific job.
     */
    @GetMapping(value = "/{jobId}/stream", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
    public Flux<ServerSentEvent<SearchJobEvent>> streamJobUpdates(@PathVariable String jobId) {
        log.info("New SSE client connected to job stream: jobId={}", jobId);

        // Create or get sink for this job
        Sinks.Many<SearchJobEvent> sink = jobSinks.computeIfAbsent(
                jobId,
                id -> Sinks.many().multicast().onBackpressureBuffer()
        );

        // Register listener with the service
        searchJobQueueService.registerListener(jobId, event -> {
            sink.tryEmitNext(event);

            // Cleanup on completion
            if ("completed".equals(event.getEventType()) ||
                    "failed".equals(event.getEventType()) ||
                    "cancelled".equals(event.getEventType())) {
                // Emit complete signal after a delay
                sink.tryEmitComplete();
                jobSinks.remove(jobId);
            }
        });

        // Add heartbeat to keep connection alive
        Flux<ServerSentEvent<SearchJobEvent>> heartbeat = Flux.interval(Duration.ofSeconds(15))
                .map(i -> ServerSentEvent.<SearchJobEvent>builder()
                        .id(String.valueOf(System.currentTimeMillis()))
                        .event("heartbeat")
                        .data(SearchJobEvent.builder()
                                .jobId(jobId)
                                .eventType("heartbeat")
                                .timestamp(System.currentTimeMillis())
                                .build())
                        .build());

        Flux<ServerSentEvent<SearchJobEvent>> events = sink.asFlux()
                .map(event -> ServerSentEvent.<SearchJobEvent>builder()
                        .id(String.valueOf(event.getTimestamp()))
                        .event(event.getEventType())
                        .data(event)
                        .build())
                .doOnCancel(() -> {
                    searchJobQueueService.unregisterListener(jobId);
                    jobSinks.remove(jobId);
                });

        return Flux.merge(events, heartbeat)
                .doFinally(signal -> {
                    searchJobQueueService.unregisterListener(jobId);
                    jobSinks.remove(jobId);
                });
    }

    /**
     * SSE endpoint for all active jobs of a user.
     * Stream updates for all active jobs.
     */
    @GetMapping(value = "/stream", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
    public Flux<ServerSentEvent<Map<String, Object>>> streamAllJobs(
            @RequestParam(required = false, defaultValue = "anonymous") String userId
    ) {
        log.info("New SSE client connected to all-jobs stream: userId={}", userId);

        // Poll for job updates every 2 seconds
        return Flux.interval(Duration.ofSeconds(2))
                .map(i -> {
                    List<SearchJob> activeJobs = searchJobQueueService.getActiveJobs(userId);
                    return ServerSentEvent.<Map<String, Object>>builder()
                            .id(String.valueOf(System.currentTimeMillis()))
                            .event("jobs_update")
                            .data(Map.of(
                                    "jobs", activeJobs,
                                    "count", activeJobs.size(),
                                    "timestamp", System.currentTimeMillis()
                            ))
                            .build();
                })
                .takeUntilOther(Flux.never()); // Keep alive until client disconnects
    }

    // ============================================
    // Health & Stats
    // ============================================

    /**
     * Health check endpoint.
     */
    @GetMapping("/health")
    public ResponseEntity<Map<String, Object>> healthCheck() {
        return ResponseEntity.ok(Map.of(
                "status", "available",
                "features", Map.of(
                        "concurrentJobs", true,
                        "batchStart", true,
                        "jobCancellation", true,
                        "sseStreaming", true
                ),
                "supportedTypes", List.of(
                        SearchType.UNIFIED.name(),
                        SearchType.DEEP_SEARCH.name(),
                        SearchType.FACT_CHECK.name(),
                        SearchType.BROWSER_AGENT.name()
                )
        ));
    }

    // ============================================
    // DTOs
    // ============================================

    /**
     * Request DTO for starting a job.
     */
    public record JobStartRequest(
            SearchType type,
            String query,
            String timeWindow,
            String userId,
            String sessionId,
            Long projectId,
            Map<String, Object> options
    ) {}
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/SearchTemplateController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.dto.PageResponse;
import com.newsinsight.collector.dto.SearchTemplateDto;
import com.newsinsight.collector.entity.search.SearchTemplate;
import com.newsinsight.collector.service.SearchTemplateService;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.data.domain.Page;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.List;
import java.util.Map;

/**
 * REST Controller for Search Template API.
 * Provides endpoints for managing search templates (SmartSearch feature).
 */
@RestController
@RequestMapping("/api/v1/search-templates")
@RequiredArgsConstructor
@Slf4j
public class SearchTemplateController {

    private final SearchTemplateService searchTemplateService;

    // ============================================
    // Create
    // ============================================

    /**
     * Create a new search template
     */
    @PostMapping
    public ResponseEntity<?> createTemplate(@RequestBody SearchTemplateDto request) {
        log.info("Creating template: name='{}', mode={}, userId={}", 
                request.getName(), request.getMode(), request.getUserId());

        try {
            SearchTemplate created = searchTemplateService.create(request);
            return ResponseEntity.status(HttpStatus.CREATED)
                    .body(SearchTemplateDto.fromEntity(created));
        } catch (IllegalArgumentException e) {
            return ResponseEntity.badRequest().body(Map.of("error", e.getMessage()));
        }
    }

    // ============================================
    // Read
    // ============================================

    /**
     * Get template by ID
     */
    @GetMapping("/{id}")
    public ResponseEntity<SearchTemplateDto> getById(@PathVariable Long id) {
        return searchTemplateService.findById(id)
                .map(SearchTemplateDto::fromEntity)
                .map(ResponseEntity::ok)
                .orElse(ResponseEntity.notFound().build());
    }

    /**
     * Get paginated templates with optional filtering
     */
    @GetMapping
    public ResponseEntity<PageResponse<SearchTemplateDto>> getAll(
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size,
            @RequestParam(defaultValue = "createdAt") String sortBy,
            @RequestParam(defaultValue = "DESC") String sortDirection,
            @RequestParam(required = false) String userId,
            @RequestParam(required = false) String mode
    ) {
        Page<SearchTemplate> result;

        if (userId != null && mode != null) {
            result = searchTemplateService.findByUserAndMode(userId, mode, page, size);
        } else if (userId != null) {
            result = searchTemplateService.findByUser(userId, page, size);
        } else {
            result = searchTemplateService.findAll(page, size, sortBy, sortDirection);
        }

        PageResponse<SearchTemplateDto> response = new PageResponse<>(
                result.getContent().stream()
                        .map(SearchTemplateDto::fromEntity)
                        .toList(),
                result.getNumber(),
                result.getSize(),
                result.getTotalElements(),
                result.getTotalPages(),
                result.isFirst(),
                result.isLast(),
                result.hasNext(),
                result.hasPrevious()
        );

        return ResponseEntity.ok(response);
    }

    /**
     * Get all templates for a user (list format, no pagination)
     */
    @GetMapping("/user/{userId}")
    public ResponseEntity<List<SearchTemplateDto>> getAllByUser(@PathVariable String userId) {
        List<SearchTemplate> templates = searchTemplateService.findAllByUser(userId);
        List<SearchTemplateDto> response = templates.stream()
                .map(SearchTemplateDto::fromEntity)
                .toList();
        return ResponseEntity.ok(response);
    }

    /**
     * Get favorite templates for a user
     */
    @GetMapping("/user/{userId}/favorites")
    public ResponseEntity<List<SearchTemplateDto>> getFavorites(@PathVariable String userId) {
        List<SearchTemplate> templates = searchTemplateService.findFavoritesByUser(userId);
        List<SearchTemplateDto> response = templates.stream()
                .map(SearchTemplateDto::fromEntity)
                .toList();
        return ResponseEntity.ok(response);
    }

    /**
     * Get most used templates for a user
     */
    @GetMapping("/user/{userId}/most-used")
    public ResponseEntity<List<SearchTemplateDto>> getMostUsed(
            @PathVariable String userId,
            @RequestParam(defaultValue = "10") int limit
    ) {
        List<SearchTemplate> templates = searchTemplateService.findMostUsed(userId, limit);
        List<SearchTemplateDto> response = templates.stream()
                .map(SearchTemplateDto::fromEntity)
                .toList();
        return ResponseEntity.ok(response);
    }

    /**
     * Get recently used templates for a user
     */
    @GetMapping("/user/{userId}/recent")
    public ResponseEntity<List<SearchTemplateDto>> getRecentlyUsed(
            @PathVariable String userId,
            @RequestParam(defaultValue = "10") int limit
    ) {
        List<SearchTemplate> templates = searchTemplateService.findRecentlyUsed(userId, limit);
        List<SearchTemplateDto> response = templates.stream()
                .map(SearchTemplateDto::fromEntity)
                .toList();
        return ResponseEntity.ok(response);
    }

    /**
     * Search templates by name
     */
    @GetMapping("/search")
    public ResponseEntity<PageResponse<SearchTemplateDto>> searchByName(
            @RequestParam String q,
            @RequestParam(required = false) String userId,
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size
    ) {
        Page<SearchTemplate> result = searchTemplateService.searchByName(q, userId, page, size);

        PageResponse<SearchTemplateDto> response = new PageResponse<>(
                result.getContent().stream()
                        .map(SearchTemplateDto::fromEntity)
                        .toList(),
                result.getNumber(),
                result.getSize(),
                result.getTotalElements(),
                result.getTotalPages(),
                result.isFirst(),
                result.isLast(),
                result.hasNext(),
                result.hasPrevious()
        );

        return ResponseEntity.ok(response);
    }

    // ============================================
    // Update
    // ============================================

    /**
     * Update a template
     */
    @PutMapping("/{id}")
    public ResponseEntity<?> updateTemplate(
            @PathVariable Long id,
            @RequestBody SearchTemplateDto request
    ) {
        try {
            SearchTemplate updated = searchTemplateService.update(id, request);
            return ResponseEntity.ok(SearchTemplateDto.fromEntity(updated));
        } catch (IllegalArgumentException e) {
            if (e.getMessage().contains("not found")) {
                return ResponseEntity.notFound().build();
            }
            return ResponseEntity.badRequest().body(Map.of("error", e.getMessage()));
        }
    }

    /**
     * Toggle favorite status
     */
    @PostMapping("/{id}/favorite")
    public ResponseEntity<?> toggleFavorite(@PathVariable Long id) {
        try {
            SearchTemplate updated = searchTemplateService.toggleFavorite(id);
            return ResponseEntity.ok(SearchTemplateDto.fromEntity(updated));
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    /**
     * Record template usage (when user loads a template)
     */
    @PostMapping("/{id}/use")
    public ResponseEntity<Map<String, Object>> recordUsage(@PathVariable Long id) {
        if (searchTemplateService.findById(id).isEmpty()) {
            return ResponseEntity.notFound().build();
        }
        searchTemplateService.recordUsage(id);
        return ResponseEntity.ok(Map.of(
                "message", "Usage recorded",
                "templateId", id
        ));
    }

    /**
     * Duplicate a template
     */
    @PostMapping("/{id}/duplicate")
    public ResponseEntity<?> duplicateTemplate(
            @PathVariable Long id,
            @RequestParam(required = false) String newName,
            @RequestParam(required = false) String userId
    ) {
        try {
            SearchTemplate duplicated = searchTemplateService.duplicate(id, newName, userId);
            return ResponseEntity.status(HttpStatus.CREATED)
                    .body(SearchTemplateDto.fromEntity(duplicated));
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    // ============================================
    // Delete
    // ============================================

    /**
     * Delete a template
     */
    @DeleteMapping("/{id}")
    public ResponseEntity<Void> deleteTemplate(@PathVariable Long id) {
        try {
            searchTemplateService.delete(id);
            return ResponseEntity.noContent().build();
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    // ============================================
    // Statistics
    // ============================================

    /**
     * Get template statistics
     */
    @GetMapping("/stats")
    public ResponseEntity<Map<String, Object>> getStatistics(
            @RequestParam(required = false) String userId
    ) {
        return ResponseEntity.ok(searchTemplateService.getStatistics(userId));
    }

    /**
     * Health check
     */
    @GetMapping("/health")
    public ResponseEntity<Map<String, Object>> healthCheck() {
        return ResponseEntity.ok(Map.of(
                "service", "SearchTemplateService",
                "status", "available",
                "features", Map.of(
                        "create", true,
                        "favorites", true,
                        "duplicate", true,
                        "usageTracking", true
                )
        ));
    }
}

```
