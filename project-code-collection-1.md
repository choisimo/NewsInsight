# Project Code Snapshot

Generated at 2025-12-21T10:30:23.495Z

---

## backend/admin-dashboard/api/dependencies.py

```py
"""
FastAPI Dependencies - ì˜ì¡´ì„± ì£¼ì…
"""

import os
from functools import lru_cache
from typing import Callable

from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer

from .models.schemas import User, UserRole
from .services.audit_service import AuditService
from .services.auth_service import AuthService
from .services.document_service import DocumentService
from .services.environment_service import EnvironmentService
from .services.script_service import ScriptService
from .services.health_service import HealthService
from .services.data_source_service import DataSourceService
from .services.database_service import DatabaseService
from .services.kafka_service import KafkaService

# OAuth2 ìŠ¤í‚´
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="/api/v1/admin/auth/token")

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = os.environ.get(
    "PROJECT_ROOT",
    os.path.dirname(
        os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
    ),
)
CONFIG_DIR = os.environ.get(
    "ADMIN_CONFIG_DIR",
    os.path.join(os.path.dirname(os.path.dirname(__file__)), "config"),
)
# SECRET_KEYëŠ” ë°˜ë“œì‹œ í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤
# í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” ê°•ë ¥í•œ ëœë¤ í‚¤ë¥¼ ì‚¬ìš©í•˜ì„¸ìš” (ì˜ˆ: openssl rand -hex 32)
_default_secret = "your-secret-key-change-in-production"
SECRET_KEY = os.environ.get("ADMIN_SECRET_KEY", _default_secret)

# í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ê¸°ë³¸ ì‹œí¬ë¦¿ í‚¤ ì‚¬ìš© ì‹œ ê²½ê³ 
if SECRET_KEY == _default_secret:
    import warnings
    warnings.warn(
        "ğŸ”´ SECURITY WARNING: Using default SECRET_KEY! "
        "Set ADMIN_SECRET_KEY environment variable in production. "
        "Generate a secure key with: openssl rand -hex 32",
        UserWarning,
    )


@lru_cache()
def get_auth_service() -> AuthService:
    """ì¸ì¦ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤"""
    return AuthService(
        config_dir=CONFIG_DIR,
        secret_key=SECRET_KEY,
    )


@lru_cache()
def get_environment_service() -> EnvironmentService:
    """í™˜ê²½ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤"""
    return EnvironmentService(
        project_root=PROJECT_ROOT,
        config_dir=CONFIG_DIR,
    )


@lru_cache()
def get_script_service() -> ScriptService:
    """ìŠ¤í¬ë¦½íŠ¸ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤"""
    return ScriptService(
        project_root=PROJECT_ROOT,
        config_dir=CONFIG_DIR,
    )


@lru_cache()
def get_document_service() -> DocumentService:
    """ë¬¸ì„œ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤"""
    return DocumentService(
        project_root=PROJECT_ROOT,
        config_dir=CONFIG_DIR,
    )


@lru_cache()
def get_audit_service() -> AuditService:
    """ê°ì‚¬ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤"""
    return AuditService(config_dir=CONFIG_DIR)


@lru_cache()
def get_health_service() -> HealthService:
    """í—¬ìŠ¤ ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤"""
    return HealthService(
        project_root=PROJECT_ROOT,
        config_dir=CONFIG_DIR,
    )


@lru_cache()
def get_data_source_service() -> DataSourceService:
    """ë°ì´í„° ì†ŒìŠ¤ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤"""
    return DataSourceService(
        project_root=PROJECT_ROOT,
        config_dir=CONFIG_DIR,
    )


@lru_cache()
def get_database_service() -> DatabaseService:
    """ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤"""
    return DatabaseService(
        project_root=PROJECT_ROOT,
        config_dir=CONFIG_DIR,
    )


@lru_cache()
def get_kafka_service() -> KafkaService:
    """Kafka/Redpanda ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤"""
    return KafkaService(
        project_root=PROJECT_ROOT,
        config_dir=CONFIG_DIR,
    )


async def get_current_user(
    token: str = Depends(oauth2_scheme),
    auth_service: AuthService = Depends(get_auth_service),
) -> User:
    """í˜„ì¬ ì¸ì¦ëœ ì‚¬ìš©ì ì¡°íšŒ"""
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )

    token_data = auth_service.verify_token(token)
    if not token_data:
        raise credentials_exception

    user = auth_service.get_user(token_data.user_id)
    if not user:
        raise credentials_exception

    if not user.is_active:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="User account is disabled",
        )

    return user


def require_role(required_role: UserRole) -> Callable:
    """íŠ¹ì • ì—­í•  ì´ìƒ ê¶Œí•œ ìš”êµ¬"""

    async def role_checker(
        current_user: User = Depends(get_current_user),
        auth_service: AuthService = Depends(get_auth_service),
    ) -> User:
        if not auth_service.check_permission(current_user.role, required_role):
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail=f"Requires {required_role.value} permission or higher",
            )
        return current_user

    return role_checker

```

---

## backend/admin-dashboard/api/main.py

```py
"""
Admin Dashboard API - FastAPI ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
"""

import os
from contextlib import asynccontextmanager
from datetime import datetime

from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from fastapi.staticfiles import StaticFiles

from .models.schemas import HealthCheck
from .routers import (
    auth,
    audit,
    documents,
    environments,
    scripts,
    public_auth,
    llm_providers,
    health_monitor,
    data_sources,
    ml_addons,
    ml_training,
    databases,
    kafka,
)

# ë²„ì „ ì •ë³´
VERSION = "1.0.0"


@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬"""
    # ì‹œì‘ ì‹œ
    print(f"ğŸš€ Admin Dashboard API v{VERSION} starting...")
    yield
    # ì¢…ë£Œ ì‹œ
    print("ğŸ‘‹ Admin Dashboard API shutting down...")


# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="NewsInsight Admin Dashboard API",
    description="í†µí•© TUI/Web Admin ëŒ€ì‹œë³´ë“œ API",
    version=VERSION,
    docs_url="/api/v1/admin/docs",
    redoc_url="/api/v1/admin/redoc",
    openapi_url="/api/v1/admin/openapi.json",
    lifespan=lifespan,
)

# CORS ì„¤ì •
CORS_ORIGINS = os.environ.get(
    "CORS_ORIGINS", "http://localhost:3000,http://localhost:5173,http://localhost:8080"
).split(",")

app.add_middleware(
    CORSMiddleware,
    allow_origins=CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ì „ì—­ ì˜ˆì™¸ í•¸ë“¤ëŸ¬
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """ì „ì—­ ì˜ˆì™¸ ì²˜ë¦¬"""
    return JSONResponse(
        status_code=500,
        content={
            "detail": str(exc),
            "type": type(exc).__name__,
        },
    )


# API ë¼ìš°í„° ë“±ë¡
API_PREFIX = "/api/v1/admin"
PUBLIC_API_PREFIX = "/api/v1"

# Admin ì „ìš© ë¼ìš°í„° (/api/v1/admin/...)
app.include_router(auth.router, prefix=API_PREFIX)
app.include_router(environments.router, prefix=API_PREFIX)
app.include_router(scripts.router, prefix=API_PREFIX)
app.include_router(documents.router, prefix=API_PREFIX)
app.include_router(audit.router, prefix=API_PREFIX)
app.include_router(llm_providers.router, prefix=API_PREFIX)
app.include_router(health_monitor.router, prefix=API_PREFIX)
app.include_router(data_sources.router, prefix=API_PREFIX)
app.include_router(ml_addons.router, prefix=API_PREFIX)
app.include_router(ml_training.router, prefix=API_PREFIX)
app.include_router(databases.router, prefix=API_PREFIX)
app.include_router(kafka.router, prefix=API_PREFIX)

# ê³µê°œ ë¼ìš°í„° (/api/v1/auth/...)
app.include_router(public_auth.router, prefix=PUBLIC_API_PREFIX)


# í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
@app.get("/health", response_model=HealthCheck, tags=["Health"])
@app.get(f"{API_PREFIX}/health", response_model=HealthCheck, tags=["Health"])
async def health_check():
    """í—¬ìŠ¤ì²´í¬"""
    return HealthCheck(
        status="healthy",
        version=VERSION,
        timestamp=datetime.utcnow(),
    )


# ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸
@app.get("/", tags=["Root"])
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "name": "NewsInsight Admin Dashboard API",
        "version": VERSION,
        "docs": "/api/v1/admin/docs",
        "health": "/health",
    }


# ì •ì  íŒŒì¼ ì„œë¹™ (Web UI)
WEB_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), "web", "dist")
if os.path.exists(WEB_DIR):
    app.mount("/", StaticFiles(directory=WEB_DIR, html=True), name="static")


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "api.main:app",
        host="0.0.0.0",
        port=int(os.environ.get("PORT", 8888)),
        reload=True,
    )

```

---

## backend/admin-dashboard/api/models/__init__.py

```py
# Admin Dashboard Models

```

---

## backend/admin-dashboard/api/models/schemas.py

```py
"""
Admin Dashboard - Pydantic Schemas
í™˜ê²½, ìŠ¤í¬ë¦½íŠ¸, ë¬¸ì„œ, ê°ì‚¬ ë¡œê·¸ ë“±ì˜ ë°ì´í„° ëª¨ë¸ ì •ì˜
"""

from datetime import datetime
from enum import Enum
from typing import Any, Optional

from pydantic import BaseModel, Field


# ============================================================================
# Enums
# ============================================================================
class EnvironmentType(str, Enum):
    ZEROTRUST = "zerotrust"
    LOCAL = "local"
    GCP = "gcp"
    AWS = "aws"
    PRODUCTION = "production"
    STAGING = "staging"


class RiskLevel(str, Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class TaskStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    SUCCESS = "success"
    FAILED = "failed"
    CANCELLED = "cancelled"


class UserRole(str, Enum):
    USER = "user"  # ì¼ë°˜ ì‚¬ìš©ì (íšŒì›ê°€ì…)
    VIEWER = "viewer"  # ê´€ë¦¬ììš© - ì½ê¸° ì „ìš©
    OPERATOR = "operator"  # ê´€ë¦¬ììš© - ìš´ì˜ì
    ADMIN = "admin"  # ê´€ë¦¬ììš© - ìµœê³  ê´€ë¦¬ì


class ServiceStatus(str, Enum):
    UP = "up"
    DOWN = "down"
    STARTING = "starting"
    STOPPING = "stopping"
    UNKNOWN = "unknown"


# ============================================================================
# Environment / Profile Models
# ============================================================================
class EnvironmentBase(BaseModel):
    name: str = Field(..., description="í™˜ê²½ ì´ë¦„ (ì˜ˆ: zerotrust, local)")
    env_type: EnvironmentType = Field(..., description="í™˜ê²½ íƒ€ì…")
    description: Optional[str] = Field(None, description="í™˜ê²½ ì„¤ëª…")
    compose_file: str = Field(..., description="Docker Compose íŒŒì¼ ê²½ë¡œ")
    env_file: Optional[str] = Field(None, description="í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ ê²½ë¡œ")
    is_active: bool = Field(True, description="í™œì„±í™” ì—¬ë¶€")
    priority: int = Field(0, description="ìš°ì„ ìˆœìœ„ (ë†’ì„ìˆ˜ë¡ ë¨¼ì € í‘œì‹œ)")


class EnvironmentCreate(EnvironmentBase):
    pass


class EnvironmentUpdate(BaseModel):
    name: Optional[str] = None
    description: Optional[str] = None
    compose_file: Optional[str] = None
    env_file: Optional[str] = None
    is_active: Optional[bool] = None
    priority: Optional[int] = None


class Environment(EnvironmentBase):
    id: str = Field(..., description="í™˜ê²½ ID")
    created_at: datetime
    updated_at: datetime

    class Config:
        from_attributes = True


# ============================================================================
# Environment Variable Models
# ============================================================================
class EnvVariableBase(BaseModel):
    key: str = Field(..., description="í™˜ê²½ ë³€ìˆ˜ í‚¤")
    value: str = Field(..., description="í™˜ê²½ ë³€ìˆ˜ ê°’")
    is_secret: bool = Field(False, description="ë¯¼ê° ì •ë³´ ì—¬ë¶€")
    description: Optional[str] = Field(None, description="ë³€ìˆ˜ ì„¤ëª…")


class EnvVariableCreate(EnvVariableBase):
    environment_id: str


class EnvVariableUpdate(BaseModel):
    value: Optional[str] = None
    is_secret: Optional[bool] = None
    description: Optional[str] = None
    comment: Optional[str] = Field(None, description="ë³€ê²½ ì‚¬ìœ ")


class EnvVariable(EnvVariableBase):
    id: str
    environment_id: str
    masked_value: str = Field(..., description="ë§ˆìŠ¤í‚¹ëœ ê°’")
    created_at: datetime
    updated_at: datetime

    class Config:
        from_attributes = True


class EnvVariableHistory(BaseModel):
    id: str
    variable_id: str
    old_value: str
    new_value: str
    changed_by: str
    comment: Optional[str]
    changed_at: datetime


# ============================================================================
# Script / Task Models
# ============================================================================
class ScriptParameter(BaseModel):
    name: str = Field(..., description="íŒŒë¼ë¯¸í„° ì´ë¦„")
    param_type: str = Field(
        "string", description="íŒŒë¼ë¯¸í„° íƒ€ì… (string, boolean, number)"
    )
    required: bool = Field(False, description="í•„ìˆ˜ ì—¬ë¶€")
    default: Optional[Any] = Field(None, description="ê¸°ë³¸ê°’")
    description: Optional[str] = Field(None, description="íŒŒë¼ë¯¸í„° ì„¤ëª…")


class ScriptBase(BaseModel):
    name: str = Field(..., description="ìŠ¤í¬ë¦½íŠ¸ ì´ë¦„")
    description: Optional[str] = Field(None, description="ìŠ¤í¬ë¦½íŠ¸ ì„¤ëª…")
    command: str = Field(..., description="ì‹¤í–‰í•  ëª…ë ¹ì–´")
    working_dir: Optional[str] = Field(None, description="ì‘ì—… ë””ë ‰í† ë¦¬")
    risk_level: RiskLevel = Field(RiskLevel.LOW, description="ìœ„í—˜ë„")
    estimated_duration: Optional[int] = Field(None, description="ì˜ˆìƒ ì†Œìš” ì‹œê°„(ì´ˆ)")
    allowed_environments: list[str] = Field(
        default_factory=list, description="í—ˆìš©ëœ í™˜ê²½ ëª©ë¡"
    )
    required_role: UserRole = Field(UserRole.OPERATOR, description="í•„ìš” ê¶Œí•œ")
    parameters: list[ScriptParameter] = Field(
        default_factory=list, description="íŒŒë¼ë¯¸í„° ìŠ¤í‚¤ë§ˆ"
    )
    pre_hooks: list[str] = Field(default_factory=list, description="ì‹¤í–‰ ì „ í›„í¬")
    post_hooks: list[str] = Field(default_factory=list, description="ì‹¤í–‰ í›„ í›„í¬")
    tags: list[str] = Field(default_factory=list, description="íƒœê·¸")


class ScriptCreate(ScriptBase):
    pass


class ScriptUpdate(BaseModel):
    name: Optional[str] = None
    description: Optional[str] = None
    command: Optional[str] = None
    working_dir: Optional[str] = None
    risk_level: Optional[RiskLevel] = None
    estimated_duration: Optional[int] = None
    allowed_environments: Optional[list[str]] = None
    required_role: Optional[UserRole] = None
    parameters: Optional[list[ScriptParameter]] = None
    pre_hooks: Optional[list[str]] = None
    post_hooks: Optional[list[str]] = None
    tags: Optional[list[str]] = None


class Script(ScriptBase):
    id: str
    created_at: datetime
    updated_at: datetime

    class Config:
        from_attributes = True


# ============================================================================
# Task Execution Models
# ============================================================================
class TaskExecutionRequest(BaseModel):
    script_id: str = Field(..., description="ì‹¤í–‰í•  ìŠ¤í¬ë¦½íŠ¸ ID")
    environment_id: str = Field(..., description="ëŒ€ìƒ í™˜ê²½ ID")
    parameters: dict[str, Any] = Field(
        default_factory=dict, description="ì‹¤í–‰ íŒŒë¼ë¯¸í„°"
    )


class TaskExecution(BaseModel):
    id: str = Field(..., description="ì‹¤í–‰ ID")
    script_id: str
    script_name: str
    environment_id: str
    environment_name: str
    status: TaskStatus
    parameters: dict[str, Any]
    started_at: datetime
    finished_at: Optional[datetime] = None
    executed_by: str
    exit_code: Optional[int] = None
    error_message: Optional[str] = None


class TaskLog(BaseModel):
    execution_id: str
    timestamp: datetime
    level: str  # INFO, WARN, ERROR
    message: str


# ============================================================================
# Service Status Models
# ============================================================================
class ContainerInfo(BaseModel):
    name: str
    image: str
    status: ServiceStatus
    health: Optional[str] = None
    ports: list[str] = Field(default_factory=list)
    created_at: Optional[datetime] = None
    started_at: Optional[datetime] = None


class EnvironmentStatus(BaseModel):
    environment_id: str
    environment_name: str
    containers: list[ContainerInfo]
    total_containers: int
    running_containers: int
    last_deployment: Optional[datetime] = None
    deployed_by: Optional[str] = None


# ============================================================================
# Document Models
# ============================================================================
class DocumentCategory(str, Enum):
    DEPLOYMENT = "deployment"
    TROUBLESHOOTING = "troubleshooting"
    ARCHITECTURE = "architecture"
    RUNBOOK = "runbook"
    GENERAL = "general"


class DocumentBase(BaseModel):
    title: str = Field(..., description="ë¬¸ì„œ ì œëª©")
    file_path: str = Field(..., description="íŒŒì¼ ê²½ë¡œ")
    category: DocumentCategory = Field(DocumentCategory.GENERAL, description="ì¹´í…Œê³ ë¦¬")
    tags: list[str] = Field(default_factory=list, description="íƒœê·¸")
    related_environments: list[str] = Field(
        default_factory=list, description="ê´€ë ¨ í™˜ê²½"
    )
    related_scripts: list[str] = Field(
        default_factory=list, description="ê´€ë ¨ ìŠ¤í¬ë¦½íŠ¸"
    )


class Document(DocumentBase):
    id: str
    content: Optional[str] = Field(None, description="Markdown ë‚´ìš©")
    last_modified: datetime

    class Config:
        from_attributes = True


# ============================================================================
# Audit Log Models
# ============================================================================
class AuditAction(str, Enum):
    LOGIN = "login"
    LOGOUT = "logout"
    VIEW = "view"
    CREATE = "create"
    UPDATE = "update"
    DELETE = "delete"
    EXECUTE = "execute"
    DEPLOY = "deploy"
    ROLLBACK = "rollback"


class AuditLog(BaseModel):
    id: str
    user_id: str
    username: str
    action: AuditAction
    resource_type: str  # environment, script, variable, etc.
    resource_id: Optional[str] = None
    resource_name: Optional[str] = None
    environment_id: Optional[str] = None
    environment_name: Optional[str] = None
    details: dict[str, Any] = Field(default_factory=dict)
    ip_address: Optional[str] = None
    user_agent: Optional[str] = None
    timestamp: datetime
    success: bool = True
    error_message: Optional[str] = None


class AuditLogFilter(BaseModel):
    user_id: Optional[str] = None
    action: Optional[AuditAction] = None
    resource_type: Optional[str] = None
    environment_id: Optional[str] = None
    start_date: Optional[datetime] = None
    end_date: Optional[datetime] = None
    success: Optional[bool] = None


# ============================================================================
# User / Auth Models
# ============================================================================
class UserBase(BaseModel):
    username: str
    email: Optional[str] = None
    role: UserRole = Field(UserRole.USER)
    is_active: bool = True


class UserCreate(UserBase):
    password: str


class UserRegister(BaseModel):
    """ì¼ë°˜ ì‚¬ìš©ì íšŒì›ê°€ì…ìš© ìŠ¤í‚¤ë§ˆ"""

    username: str = Field(
        ..., min_length=3, max_length=50, description="ì‚¬ìš©ìëª… (3-50ì)"
    )
    email: str = Field(..., description="ì´ë©”ì¼ ì£¼ì†Œ")
    password: str = Field(..., min_length=8, description="ë¹„ë°€ë²ˆí˜¸ (8ì ì´ìƒ)")


class User(UserBase):
    id: str
    created_at: datetime
    last_login: Optional[datetime] = None
    password_change_required: bool = Field(False, description="ë¹„ë°€ë²ˆí˜¸ ë³€ê²½ í•„ìš” ì—¬ë¶€")

    class Config:
        from_attributes = True


class SetupStatus(BaseModel):
    """ì´ˆê¸° ì„¤ì • ìƒíƒœ"""

    setup_required: bool = Field(..., description="ì´ˆê¸° ì„¤ì • í•„ìš” ì—¬ë¶€")
    has_users: bool = Field(..., description="ì‚¬ìš©ì ì¡´ì¬ ì—¬ë¶€")
    is_default_admin: bool = Field(False, description="ê¸°ë³¸ ê´€ë¦¬ì ê³„ì • ì‚¬ìš© ì—¬ë¶€")


class Token(BaseModel):
    access_token: str
    token_type: str = "bearer"
    expires_in: int


class TokenData(BaseModel):
    user_id: str
    username: str
    role: UserRole
    exp: datetime


# ============================================================================
# Response Models
# ============================================================================
class PaginatedResponse(BaseModel):
    items: list[Any]
    total: int
    page: int
    page_size: int
    total_pages: int


class HealthCheck(BaseModel):
    status: str = "healthy"
    version: str
    timestamp: datetime


# ============================================================================
# Service Health Monitoring Models
# ============================================================================
class ServiceHealthStatus(str, Enum):
    HEALTHY = "healthy"
    UNHEALTHY = "unhealthy"
    DEGRADED = "degraded"
    UNREACHABLE = "unreachable"
    UNKNOWN = "unknown"


class ServiceHealth(BaseModel):
    service_id: str = Field(..., description="ì„œë¹„ìŠ¤ ID")
    name: str = Field(..., description="ì„œë¹„ìŠ¤ ì´ë¦„")
    status: ServiceHealthStatus = Field(..., description="í—¬ìŠ¤ ìƒíƒœ")
    message: Optional[str] = Field(default=None, description="ìƒíƒœ ë©”ì‹œì§€")
    response_time_ms: Optional[float] = Field(default=None, description="ì‘ë‹µ ì‹œê°„(ms)")
    url: Optional[str] = Field(default=None, description="í—¬ìŠ¤ì²´í¬ URL")
    checked_at: datetime = Field(..., description="ì²´í¬ ì‹œê°„")
    details: Optional[dict[str, Any]] = Field(default=None, description="ìƒì„¸ ì •ë³´")

    class Config:
        from_attributes = True


class InfrastructureHealth(BaseModel):
    service_id: str = Field(..., description="ì¸í”„ë¼ ì„œë¹„ìŠ¤ ID")
    name: str = Field(..., description="ì¸í”„ë¼ ì´ë¦„")
    status: ServiceHealthStatus = Field(..., description="í—¬ìŠ¤ ìƒíƒœ")
    message: Optional[str] = Field(default=None, description="ìƒíƒœ ë©”ì‹œì§€")
    port: Optional[int] = Field(default=None, description="í¬íŠ¸")
    checked_at: datetime = Field(..., description="ì²´í¬ ì‹œê°„")
    details: Optional[dict[str, Any]] = Field(default=None, description="ìƒì„¸ ì •ë³´")

    class Config:
        from_attributes = True


class OverallSystemHealth(BaseModel):
    status: ServiceHealthStatus = Field(..., description="ì „ì²´ ì‹œìŠ¤í…œ ìƒíƒœ")
    total_services: int = Field(..., description="ì „ì²´ ì„œë¹„ìŠ¤ ìˆ˜")
    healthy_services: int = Field(..., description="ì •ìƒ ì„œë¹„ìŠ¤ ìˆ˜")
    unhealthy_services: int = Field(..., description="ë¹„ì •ìƒ ì„œë¹„ìŠ¤ ìˆ˜")
    degraded_services: int = Field(..., description="ì €í•˜ ì„œë¹„ìŠ¤ ìˆ˜")
    total_infrastructure: int = Field(..., description="ì „ì²´ ì¸í”„ë¼ ìˆ˜")
    healthy_infrastructure: int = Field(..., description="ì •ìƒ ì¸í”„ë¼ ìˆ˜")
    average_response_time_ms: Optional[float] = Field(
        None, description="í‰ê·  ì‘ë‹µ ì‹œê°„"
    )
    services: list[ServiceHealth] = Field(
        default_factory=list, description="ì„œë¹„ìŠ¤ í—¬ìŠ¤ ëª©ë¡"
    )
    infrastructure: list[InfrastructureHealth] = Field(
        default_factory=list, description="ì¸í”„ë¼ í—¬ìŠ¤ ëª©ë¡"
    )
    checked_at: datetime = Field(..., description="ì²´í¬ ì‹œê°„")

    class Config:
        from_attributes = True


class ServiceMetrics(BaseModel):
    service_id: str
    cpu_usage_percent: Optional[float] = None
    memory_usage_mb: Optional[float] = None
    memory_limit_mb: Optional[float] = None
    request_count: Optional[int] = None
    error_count: Optional[int] = None
    avg_response_time_ms: Optional[float] = None
    collected_at: datetime

    class Config:
        from_attributes = True


class ServiceInfo(BaseModel):
    id: str = Field(..., description="ì„œë¹„ìŠ¤ ID")
    name: str = Field(..., description="ì„œë¹„ìŠ¤ ì´ë¦„")
    description: Optional[str] = Field(None, description="ì„¤ëª…")
    port: Optional[int] = Field(None, description="í¬íŠ¸")
    healthcheck: str = Field("/health", description="í—¬ìŠ¤ì²´í¬ ê²½ë¡œ")
    hostname: str = Field(..., description="í˜¸ìŠ¤íŠ¸ëª…")
    type: str = Field(..., description="ì„œë¹„ìŠ¤ íƒ€ì…")
    tags: list[str] = Field(default_factory=list, description="íƒœê·¸")


# ============================================================================
# Data Source Management Models
# ============================================================================
class DataSourceType(str, Enum):
    RSS = "rss"
    WEB = "web"
    API = "api"
    SOCIAL = "social"


class DataSourceStatus(str, Enum):
    ACTIVE = "active"
    INACTIVE = "inactive"
    ERROR = "error"
    TESTING = "testing"


class DataSourceBase(BaseModel):
    name: str = Field(..., description="ì†ŒìŠ¤ ì´ë¦„")
    source_type: DataSourceType = Field(..., description="ì†ŒìŠ¤ íƒ€ì…")
    url: str = Field(..., description="ì†ŒìŠ¤ URL")
    description: Optional[str] = Field(None, description="ì„¤ëª…")
    category: Optional[str] = Field(None, description="ì¹´í…Œê³ ë¦¬")
    language: str = Field("ko", description="ì–¸ì–´")
    is_active: bool = Field(True, description="í™œì„±í™” ì—¬ë¶€")
    crawl_interval_minutes: int = Field(60, description="ìˆ˜ì§‘ ì£¼ê¸°(ë¶„)")
    priority: int = Field(0, description="ìš°ì„ ìˆœìœ„")
    config: dict[str, Any] = Field(default_factory=dict, description="ì¶”ê°€ ì„¤ì •")


class DataSourceCreate(DataSourceBase):
    pass


class DataSourceUpdate(BaseModel):
    name: Optional[str] = None
    description: Optional[str] = None
    url: Optional[str] = None
    category: Optional[str] = None
    is_active: Optional[bool] = None
    crawl_interval_minutes: Optional[int] = None
    priority: Optional[int] = None
    config: Optional[dict[str, Any]] = None


class DataSource(DataSourceBase):
    id: str = Field(..., description="ì†ŒìŠ¤ ID")
    status: DataSourceStatus = Field(
        default=DataSourceStatus.ACTIVE, description="ìƒíƒœ"
    )
    last_crawled_at: Optional[datetime] = Field(
        default=None, description="ë§ˆì§€ë§‰ ìˆ˜ì§‘ ì‹œê°„"
    )
    total_articles: int = Field(default=0, description="ì´ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜")
    success_rate: float = Field(default=100.0, description="ì„±ê³µë¥ ")
    created_at: datetime
    updated_at: datetime

    class Config:
        from_attributes = True


class DataSourceStats(BaseModel):
    source_id: str
    total_crawls: int = 0
    successful_crawls: int = 0
    failed_crawls: int = 0
    total_articles: int = 0
    avg_articles_per_crawl: float = 0.0
    last_error: Optional[str] = None
    last_error_at: Optional[datetime] = None


class DataSourceTestResult(BaseModel):
    source_id: str
    success: bool
    message: str
    response_time_ms: Optional[float] = None
    sample_data: Optional[dict[str, Any]] = None
    tested_at: datetime


# ============================================================================
# Database Management Models
# ============================================================================
class DatabaseType(str, Enum):
    POSTGRESQL = "postgresql"
    MONGODB = "mongodb"
    REDIS = "redis"


class DatabaseInfo(BaseModel):
    db_type: DatabaseType
    name: str
    host: str
    port: int
    status: ServiceHealthStatus
    version: Optional[str] = None
    size_bytes: Optional[int] = None
    size_human: Optional[str] = None
    connection_count: Optional[int] = None
    max_connections: Optional[int] = None
    uptime_seconds: Optional[int] = None
    checked_at: datetime


class PostgresTableInfo(BaseModel):
    schema_name: str
    table_name: str
    row_count: int
    size_bytes: int
    size_human: str
    index_size_bytes: Optional[int] = None
    last_vacuum: Optional[datetime] = None
    last_analyze: Optional[datetime] = None


class PostgresDatabaseStats(BaseModel):
    database_name: str
    size_bytes: int
    size_human: str
    tables: list[PostgresTableInfo]
    total_tables: int
    total_rows: int
    connection_count: int
    max_connections: int
    checked_at: datetime


class MongoCollectionInfo(BaseModel):
    collection_name: str
    document_count: int
    size_bytes: int
    size_human: str
    avg_document_size_bytes: Optional[int] = None
    index_count: int
    total_index_size_bytes: Optional[int] = None


class MongoDatabaseStats(BaseModel):
    database_name: str
    size_bytes: int
    size_human: str
    collections: list[MongoCollectionInfo]
    total_collections: int
    total_documents: int
    checked_at: datetime


class RedisStats(BaseModel):
    used_memory_bytes: int
    used_memory_human: str
    max_memory_bytes: Optional[int] = None
    connected_clients: int
    total_keys: int
    expired_keys: int
    keyspace_hits: int
    keyspace_misses: int
    hit_rate: float
    uptime_seconds: int
    checked_at: datetime


# ============================================================================
# Kafka/Redpanda Management Models
# ============================================================================
class KafkaTopicInfo(BaseModel):
    name: str
    partition_count: int
    replication_factor: int
    message_count: Optional[int] = None
    size_bytes: Optional[int] = None
    retention_ms: Optional[int] = None
    is_internal: bool = False


class KafkaConsumerGroupInfo(BaseModel):
    group_id: str
    state: str
    members_count: int
    topics: list[str]
    total_lag: int
    lag_per_partition: dict[str, int]


class KafkaClusterInfo(BaseModel):
    broker_count: int
    controller_id: Optional[int] = None
    cluster_id: Optional[str] = None
    topics: list[KafkaTopicInfo]
    consumer_groups: list[KafkaConsumerGroupInfo]
    total_topics: int
    total_partitions: int
    total_messages: Optional[int] = None
    checked_at: datetime

```

---

## backend/admin-dashboard/api/routers/__init__.py

```py
# Admin Dashboard Routers
from . import (
    auth,
    audit,
    documents,
    environments,
    scripts,
    public_auth,
    llm_providers,
    health_monitor,
    data_sources,
    ml_addons,
    ml_training,
    databases,
    kafka,
)

```

---

## backend/admin-dashboard/api/routers/audit.py

```py
"""
Audit Router - ê°ì‚¬ ë¡œê·¸ API ì—”ë“œí¬ì¸íŠ¸
"""

import asyncio
import json
from datetime import datetime
from typing import AsyncGenerator, Optional

from fastapi import APIRouter, Depends, HTTPException, Query
from fastapi.responses import StreamingResponse

from ..models.schemas import AuditAction, AuditLog, AuditLogFilter, UserRole
from ..dependencies import get_audit_service, get_current_user, require_role

router = APIRouter(prefix="/audit", tags=["Audit Logs"])


# ============================================
# SSE Event Stream for Real-time Activity
# ============================================


async def activity_event_generator(
    audit_service,
    last_timestamp: Optional[datetime] = None,
) -> AsyncGenerator[str, None]:
    """
    Server-Sent Events generator for real-time activity stream.
    Polls for new audit logs and sends them as events.
    """
    poll_interval = 5  # seconds
    seen_ids = set()

    # Initialize with recent logs if no timestamp provided
    if last_timestamp is None:
        recent_logs, _ = audit_service.get_logs(page=1, page_size=20)
        for log in recent_logs:
            seen_ids.add(log.id)

    while True:
        try:
            # Get recent logs
            logs, _ = audit_service.get_logs(page=1, page_size=50)

            # Find new logs
            new_logs = []
            for log in logs:
                if log.id not in seen_ids:
                    new_logs.append(log)
                    seen_ids.add(log.id)

            # Send new logs as events
            for log in reversed(new_logs):  # Send oldest first
                event_data = {
                    "eventType": "activity",
                    "timestamp": log.timestamp.isoformat(),
                    "data": {
                        "id": log.id,
                        "userId": log.user_id,
                        "username": log.username,
                        "action": log.action.value,
                        "resourceType": log.resource_type,
                        "resourceId": log.resource_id,
                        "resourceName": log.resource_name,
                        "environmentId": log.environment_id,
                        "environmentName": log.environment_name,
                        "success": log.success,
                        "errorMessage": log.error_message,
                        "timestamp": log.timestamp.isoformat(),
                    },
                }
                yield f"event: activity\ndata: {json.dumps(event_data, ensure_ascii=False)}\n\n"

            # Send heartbeat
            heartbeat_data = {
                "eventType": "heartbeat",
                "timestamp": datetime.utcnow().isoformat(),
            }
            yield f"event: heartbeat\ndata: {json.dumps(heartbeat_data)}\n\n"

            # Limit seen_ids to prevent memory growth
            if len(seen_ids) > 1000:
                seen_ids = set(list(seen_ids)[-500:])

            await asyncio.sleep(poll_interval)

        except asyncio.CancelledError:
            break
        except Exception as e:
            error_data = {"eventType": "error", "message": str(e)}
            yield f"event: error\ndata: {json.dumps(error_data)}\n\n"
            await asyncio.sleep(poll_interval)


@router.get("/activity/stream")
async def stream_activity_events(
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """
    SSE endpoint for real-time activity events.

    Returns a Server-Sent Events stream with:
    - activity: New audit log entries
    - heartbeat: Keep-alive signal (every 5 seconds)
    - error: Error notifications

    Requires: OPERATOR role or higher
    """
    return StreamingResponse(
        activity_event_generator(audit_service),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # Disable nginx buffering
        },
    )


@router.get("/activity/recent")
async def get_recent_activity(
    limit: int = Query(20, ge=1, le=100, description="ìµœê·¼ í™œë™ ê°œìˆ˜"),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """
    ìµœê·¼ í™œë™ ëª©ë¡ ì¡°íšŒ (SSE ëŒ€ì•ˆ).
    í´ë§ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    Requires: OPERATOR role or higher
    """
    logs, total = audit_service.get_logs(page=1, page_size=limit)

    return {
        "activities": [
            {
                "id": log.id,
                "userId": log.user_id,
                "username": log.username,
                "action": log.action.value,
                "resourceType": log.resource_type,
                "resourceId": log.resource_id,
                "resourceName": log.resource_name,
                "environmentId": log.environment_id,
                "environmentName": log.environment_name,
                "success": log.success,
                "errorMessage": log.error_message,
                "timestamp": log.timestamp.isoformat(),
            }
            for log in logs
        ],
        "total": total,
    }


# ============================================
# Original Audit Endpoints
# ============================================


@router.get("/logs", response_model=list[AuditLog])
async def list_audit_logs(
    user_id: Optional[str] = Query(None, description="ì‚¬ìš©ì ID í•„í„°"),
    action: Optional[AuditAction] = Query(None, description="ì•¡ì…˜ í•„í„°"),
    resource_type: Optional[str] = Query(None, description="ë¦¬ì†ŒìŠ¤ íƒ€ì… í•„í„°"),
    environment_id: Optional[str] = Query(None, description="í™˜ê²½ ID í•„í„°"),
    start_date: Optional[datetime] = Query(None, description="ì‹œì‘ ë‚ ì§œ"),
    end_date: Optional[datetime] = Query(None, description="ì¢…ë£Œ ë‚ ì§œ"),
    success: Optional[bool] = Query(None, description="ì„±ê³µ/ì‹¤íŒ¨ í•„í„°"),
    page: int = Query(1, ge=1, description="í˜ì´ì§€ ë²ˆí˜¸"),
    page_size: int = Query(50, ge=1, le=200, description="í˜ì´ì§€ í¬ê¸°"),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """ê°ì‚¬ ë¡œê·¸ ì¡°íšŒ (Operator ì´ìƒ ê¶Œí•œ í•„ìš”)"""
    filter_params = AuditLogFilter(
        user_id=user_id,
        action=action,
        resource_type=resource_type,
        environment_id=environment_id,
        start_date=start_date,
        end_date=end_date,
        success=success,
    )

    logs, total = audit_service.get_logs(
        filter_params=filter_params,
        page=page,
        page_size=page_size,
    )

    return logs


@router.get("/logs/count")
async def get_audit_logs_count(
    user_id: Optional[str] = Query(None),
    action: Optional[AuditAction] = Query(None),
    resource_type: Optional[str] = Query(None),
    environment_id: Optional[str] = Query(None),
    start_date: Optional[datetime] = Query(None),
    end_date: Optional[datetime] = Query(None),
    success: Optional[bool] = Query(None),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """ê°ì‚¬ ë¡œê·¸ ì´ ê°œìˆ˜ ì¡°íšŒ"""
    filter_params = AuditLogFilter(
        user_id=user_id,
        action=action,
        resource_type=resource_type,
        environment_id=environment_id,
        start_date=start_date,
        end_date=end_date,
        success=success,
    )

    _, total = audit_service.get_logs(
        filter_params=filter_params,
        page=1,
        page_size=1,
    )

    return {"total": total}


@router.get("/logs/{log_id}", response_model=AuditLog)
async def get_audit_log(
    log_id: str,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """ê°ì‚¬ ë¡œê·¸ ìƒì„¸ ì¡°íšŒ"""
    log = audit_service.get_log_by_id(log_id)
    if not log:
        raise HTTPException(status_code=404, detail="Audit log not found")
    return log


@router.get("/users/{user_id}/activity", response_model=list[AuditLog])
async def get_user_activity(
    user_id: str,
    limit: int = Query(100, ge=1, le=500),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """íŠ¹ì • ì‚¬ìš©ì í™œë™ ì´ë ¥ ì¡°íšŒ (Admin ê¶Œí•œ í•„ìš”)"""
    return audit_service.get_user_activity(user_id, limit=limit)


@router.get(
    "/resources/{resource_type}/{resource_id}/history", response_model=list[AuditLog]
)
async def get_resource_history(
    resource_type: str,
    resource_id: str,
    limit: int = Query(100, ge=1, le=500),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """ë¦¬ì†ŒìŠ¤ ë³€ê²½ ì´ë ¥ ì¡°íšŒ"""
    return audit_service.get_resource_history(resource_type, resource_id, limit=limit)


@router.get("/statistics")
async def get_audit_statistics(
    start_date: Optional[datetime] = Query(None, description="ì‹œì‘ ë‚ ì§œ"),
    end_date: Optional[datetime] = Query(None, description="ì¢…ë£Œ ë‚ ì§œ"),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ê°ì‚¬ ë¡œê·¸ í†µê³„ (Admin ê¶Œí•œ í•„ìš”)"""
    return audit_service.get_statistics(start_date=start_date, end_date=end_date)


@router.delete("/logs/cleanup")
async def cleanup_old_logs(
    days: int = Query(90, ge=30, le=365, description="ë³´ê´€ ê¸°ê°„ (ì¼)"),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ì˜¤ë˜ëœ ë¡œê·¸ ì •ë¦¬ (Admin ê¶Œí•œ í•„ìš”)"""
    deleted_count = audit_service.clear_old_logs(days=days)
    return {
        "success": True,
        "message": f"Deleted {deleted_count} old logs (older than {days} days)",
        "deleted_count": deleted_count,
    }

```

---

## backend/admin-dashboard/api/routers/auth.py

```py
"""
Auth Router - ì¸ì¦/ê¶Œí•œ API ì—”ë“œí¬ì¸íŠ¸
"""

from typing import Optional

from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.security import OAuth2PasswordRequestForm
from pydantic import BaseModel

from ..models.schemas import AuditAction, Token, User, UserCreate, UserRole, SetupStatus
from ..dependencies import (
    get_audit_service,
    get_auth_service,
    get_current_user,
    require_role,
)

router = APIRouter(prefix="/auth", tags=["Authentication"])


class LoginRequest(BaseModel):
    username: str
    password: str


class ChangePasswordRequest(BaseModel):
    old_password: str
    new_password: str


class ResetPasswordRequest(BaseModel):
    new_password: str


class UpdateUserRequest(BaseModel):
    email: Optional[str] = None
    role: Optional[UserRole] = None
    is_active: Optional[bool] = None


@router.post("/login", response_model=Token)
async def login(
    form_data: OAuth2PasswordRequestForm = Depends(),
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """ë¡œê·¸ì¸"""
    user = auth_service.authenticate(form_data.username, form_data.password)

    if not user:
        # ì‹¤íŒ¨ ë¡œê·¸
        audit_service.log(
            user_id="unknown",
            username=form_data.username,
            action=AuditAction.LOGIN,
            resource_type="auth",
            success=False,
            error_message="Invalid credentials",
        )
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid username or password",
            headers={"WWW-Authenticate": "Bearer"},
        )

    token = auth_service.create_access_token(user)

    # ì„±ê³µ ë¡œê·¸
    audit_service.log(
        user_id=user.id,
        username=user.username,
        action=AuditAction.LOGIN,
        resource_type="auth",
        success=True,
    )

    return token


@router.post("/token", response_model=Token)
async def login_for_access_token(
    form_data: OAuth2PasswordRequestForm = Depends(),
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """OAuth2 í˜¸í™˜ í† í° ì—”ë“œí¬ì¸íŠ¸"""
    return await login(form_data, auth_service, audit_service)


@router.get("/me", response_model=User)
async def get_current_user_info(
    current_user=Depends(get_current_user),
):
    """í˜„ì¬ ë¡œê·¸ì¸í•œ ì‚¬ìš©ì ì •ë³´"""
    return current_user


@router.post("/logout")
async def logout(
    current_user=Depends(get_current_user),
    audit_service=Depends(get_audit_service),
):
    """ë¡œê·¸ì•„ì›ƒ"""
    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.LOGOUT,
        resource_type="auth",
    )

    return {"success": True, "message": "Logged out successfully"}


@router.post("/change-password")
async def change_password(
    request: ChangePasswordRequest,
    current_user=Depends(get_current_user),
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """ë¹„ë°€ë²ˆí˜¸ ë³€ê²½"""
    success = auth_service.change_password(
        user_id=current_user.id,
        old_password=request.old_password,
        new_password=request.new_password,
    )

    if not success:
        audit_service.log(
            user_id=current_user.id,
            username=current_user.username,
            action=AuditAction.UPDATE,
            resource_type="password",
            success=False,
            error_message="Invalid old password",
        )
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid old password",
        )

    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="password",
        success=True,
    )

    return {"success": True, "message": "Password changed successfully"}


# ============================================================================
# User Management (Admin only)
# ============================================================================
@router.get("/users", response_model=list[User])
async def list_users(
    active_only: bool = False,
    auth_service=Depends(get_auth_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ì‚¬ìš©ì ëª©ë¡ ì¡°íšŒ (Admin ê¶Œí•œ í•„ìš”)"""
    return auth_service.list_users(active_only=active_only)


@router.get("/users/{user_id}", response_model=User)
async def get_user(
    user_id: str,
    auth_service=Depends(get_auth_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ì‚¬ìš©ì ì¡°íšŒ (Admin ê¶Œí•œ í•„ìš”)"""
    user = auth_service.get_user(user_id)
    if not user:
        raise HTTPException(status_code=404, detail="User not found")
    return user


@router.post("/users", response_model=User, status_code=status.HTTP_201_CREATED)
async def create_user(
    data: UserCreate,
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ì‚¬ìš©ì ìƒì„± (Admin ê¶Œí•œ í•„ìš”)"""
    try:
        user = auth_service.create_user(data)

        audit_service.log(
            user_id=current_user.id,
            username=current_user.username,
            action=AuditAction.CREATE,
            resource_type="user",
            resource_id=user.id,
            resource_name=user.username,
            details={"role": data.role.value},
        )

        return user
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.patch("/users/{user_id}", response_model=User)
async def update_user(
    user_id: str,
    data: UpdateUserRequest,
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ì‚¬ìš©ì ì •ë³´ ìˆ˜ì • (Admin ê¶Œí•œ í•„ìš”)"""
    user = auth_service.update_user(
        user_id=user_id,
        email=data.email,
        role=data.role,
        is_active=data.is_active,
    )

    if not user:
        raise HTTPException(status_code=404, detail="User not found")

    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="user",
        resource_id=user_id,
        resource_name=user.username,
        details=data.model_dump(exclude_unset=True),
    )

    return user


@router.post("/users/{user_id}/reset-password")
async def reset_user_password(
    user_id: str,
    request: ResetPasswordRequest,
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ì‚¬ìš©ì ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™” (Admin ê¶Œí•œ í•„ìš”)"""
    user = auth_service.get_user(user_id)
    if not user:
        raise HTTPException(status_code=404, detail="User not found")

    success = auth_service.reset_password(user_id, request.new_password)

    if not success:
        raise HTTPException(status_code=500, detail="Failed to reset password")

    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="user",
        resource_id=user_id,
        resource_name=user.username,
        details={"action": "password_reset"},
    )

    return {"success": True, "message": "Password reset successfully"}


@router.delete("/users/{user_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_user(
    user_id: str,
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ì‚¬ìš©ì ì‚­ì œ (Admin ê¶Œí•œ í•„ìš”)"""
    # ìê¸° ìì‹ ì€ ì‚­ì œ ë¶ˆê°€
    if user_id == current_user.id:
        raise HTTPException(status_code=400, detail="Cannot delete yourself")

    user = auth_service.get_user(user_id)
    if not user:
        raise HTTPException(status_code=404, detail="User not found")

    if not auth_service.delete_user(user_id):
        raise HTTPException(status_code=500, detail="Failed to delete user")

    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DELETE,
        resource_type="user",
        resource_id=user_id,
        resource_name=user.username,
    )


# ============================================================================
# Setup Status (Public - no auth required)
# ============================================================================
@router.get("/setup-status", response_model=SetupStatus)
async def get_setup_status(
    auth_service=Depends(get_auth_service),
):
    """ì´ˆê¸° ì„¤ì • ìƒíƒœ í™•ì¸ (ì¸ì¦ ë¶ˆí•„ìš”)

    ì‹œìŠ¤í…œì´ ì´ˆê¸° ì„¤ì •ì´ í•„ìš”í•œ ìƒíƒœì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    - setup_required: ì´ˆê¸° ì„¤ì •ì´ í•„ìš”í•œì§€ ì—¬ë¶€
    - has_users: ì‚¬ìš©ìê°€ ì¡´ì¬í•˜ëŠ”ì§€ ì—¬ë¶€
    - is_default_admin: ê¸°ë³¸ ê´€ë¦¬ì ê³„ì •(admin/admin123)ì„ ì‚¬ìš© ì¤‘ì¸ì§€ ì—¬ë¶€
    """
    return auth_service.get_setup_status()

```

---

## backend/admin-dashboard/api/routers/data_sources.py

```py
"""
Data Sources Router
ë°ì´í„° ì†ŒìŠ¤ ê´€ë¦¬ API
"""

from typing import Optional
from fastapi import APIRouter, Depends, HTTPException, Query

from ..models.schemas import (
    DataSource,
    DataSourceCreate,
    DataSourceUpdate,
    DataSourceType,
    DataSourceStatus,
    DataSourceTestResult,
    UserRole,
    AuditAction,
)
from ..dependencies import (
    get_current_user,
    require_role,
    get_data_source_service,
    get_audit_service,
)
from ..services.data_source_service import DataSourceService
from ..services.audit_service import AuditService

router = APIRouter(prefix="/data-sources", tags=["Data Sources"])


@router.get("", response_model=list[DataSource])
async def list_data_sources(
    source_type: Optional[str] = Query(None, description="ì†ŒìŠ¤ íƒ€ì… í•„í„°"),
    status: Optional[str] = Query(None, description="ìƒíƒœ í•„í„°"),
    category: Optional[str] = Query(None, description="ì¹´í…Œê³ ë¦¬ í•„í„°"),
    is_active: Optional[bool] = Query(None, description="í™œì„±í™” ìƒíƒœ í•„í„°"),
    service: DataSourceService = Depends(get_data_source_service),
    current_user=Depends(get_current_user),
):
    """ë°ì´í„° ì†ŒìŠ¤ ëª©ë¡ ì¡°íšŒ"""
    type_filter = DataSourceType(source_type) if source_type else None
    status_filter = DataSourceStatus(status) if status else None

    return service.list_sources(
        source_type=type_filter,
        status=status_filter,
        category=category,
        is_active=is_active,
    )


@router.get("/categories", response_model=list[str])
async def get_categories(
    service: DataSourceService = Depends(get_data_source_service),
    current_user=Depends(get_current_user),
):
    """ëª¨ë“  ì¹´í…Œê³ ë¦¬ ëª©ë¡ ì¡°íšŒ"""
    return service.get_categories()


@router.get("/stats", response_model=dict)
async def get_stats(
    service: DataSourceService = Depends(get_data_source_service),
    current_user=Depends(get_current_user),
):
    """ë°ì´í„° ì†ŒìŠ¤ í†µê³„ ì¡°íšŒ"""
    return service.get_stats()


@router.get("/{source_id}", response_model=DataSource)
async def get_data_source(
    source_id: str,
    service: DataSourceService = Depends(get_data_source_service),
    current_user=Depends(get_current_user),
):
    """íŠ¹ì • ë°ì´í„° ì†ŒìŠ¤ ì¡°íšŒ"""
    source = service.get_source(source_id)
    if not source:
        raise HTTPException(status_code=404, detail="Data source not found")
    return source


@router.post("", response_model=DataSource, status_code=201)
async def create_data_source(
    data: DataSourceCreate,
    service: DataSourceService = Depends(get_data_source_service),
    audit_service: AuditService = Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """ìƒˆ ë°ì´í„° ì†ŒìŠ¤ ìƒì„±"""
    source = service.create_source(data)

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.CREATE,
        resource_type="data_source",
        resource_id=source.id,
        resource_name=source.name,
        details={"url": source.url, "type": source.source_type.value},
    )

    return source


@router.patch("/{source_id}", response_model=DataSource)
async def update_data_source(
    source_id: str,
    data: DataSourceUpdate,
    service: DataSourceService = Depends(get_data_source_service),
    audit_service: AuditService = Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """ë°ì´í„° ì†ŒìŠ¤ ìˆ˜ì •"""
    source = service.update_source(source_id, data)
    if not source:
        raise HTTPException(status_code=404, detail="Data source not found")

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="data_source",
        resource_id=source.id,
        resource_name=source.name,
        details=data.model_dump(exclude_unset=True),
    )

    return source


@router.delete("/{source_id}", status_code=204)
async def delete_data_source(
    source_id: str,
    service: DataSourceService = Depends(get_data_source_service),
    audit_service: AuditService = Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ë°ì´í„° ì†ŒìŠ¤ ì‚­ì œ"""
    source = service.get_source(source_id)
    if not source:
        raise HTTPException(status_code=404, detail="Data source not found")

    service.delete_source(source_id)

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DELETE,
        resource_type="data_source",
        resource_id=source_id,
        resource_name=source.name,
        details={},
    )


@router.post("/{source_id}/test", response_model=DataSourceTestResult)
async def test_data_source(
    source_id: str,
    service: DataSourceService = Depends(get_data_source_service),
    audit_service: AuditService = Depends(get_audit_service),
    current_user=Depends(get_current_user),
):
    """ë°ì´í„° ì†ŒìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    result = await service.test_source(source_id)

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.EXECUTE,
        resource_type="data_source",
        resource_id=source_id,
        details={"action": "test", "success": result.success},
        success=result.success,
        error_message=None if result.success else result.message,
    )

    return result


@router.post("/{source_id}/crawl")
async def trigger_crawl(
    source_id: str,
    service: DataSourceService = Depends(get_data_source_service),
    audit_service: AuditService = Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """ë°ì´í„° ìˆ˜ì§‘ íŠ¸ë¦¬ê±°"""
    result = await service.trigger_crawl(source_id)

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.EXECUTE,
        resource_type="data_source",
        resource_id=source_id,
        details={"action": "crawl", "success": result["success"]},
        success=result["success"],
        error_message=result.get("message") if not result["success"] else None,
    )

    return result


@router.post("/bulk/toggle-active")
async def bulk_toggle_active(
    source_ids: list[str],
    is_active: bool = Query(..., description="í™œì„±í™” ì—¬ë¶€"),
    service: DataSourceService = Depends(get_data_source_service),
    audit_service: AuditService = Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """ì—¬ëŸ¬ ë°ì´í„° ì†ŒìŠ¤ ì¼ê´„ í™œì„±í™”/ë¹„í™œì„±í™”"""
    updated = service.bulk_toggle_active(source_ids, is_active)

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="data_source",
        details={
            "action": "bulk_toggle_active",
            "source_ids": source_ids,
            "is_active": is_active,
            "updated_count": updated,
        },
    )

    return {"updated": updated}

```

---

## backend/admin-dashboard/api/routers/databases.py

```py
"""
Database Management Router
PostgreSQL, MongoDB, Redis ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ API
"""

from fastapi import APIRouter, Depends, HTTPException

from ..models.schemas import (
    DatabaseType,
    DatabaseInfo,
    PostgresDatabaseStats,
    MongoDatabaseStats,
    RedisStats,
    UserRole,
)
from ..dependencies import get_current_user, require_role, get_database_service
from ..services.database_service import DatabaseService

router = APIRouter(prefix="/databases", tags=["Database Management"])


@router.get("", response_model=list[DatabaseInfo])
async def list_databases(
    service: DatabaseService = Depends(get_database_service),
    current_user=Depends(get_current_user),
):
    """ëª¨ë“  ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ ì¡°íšŒ"""
    return await service.get_all_databases()


@router.get("/postgres/health", response_model=DatabaseInfo)
async def check_postgres_health(
    service: DatabaseService = Depends(get_database_service),
    current_user=Depends(get_current_user),
):
    """PostgreSQL í—¬ìŠ¤ ì²´í¬"""
    return await service.get_postgres_health()


@router.get("/mongo/health", response_model=DatabaseInfo)
async def check_mongo_health(
    service: DatabaseService = Depends(get_database_service),
    current_user=Depends(get_current_user),
):
    """MongoDB í—¬ìŠ¤ ì²´í¬"""
    return await service.get_mongo_health()


@router.get("/redis/health", response_model=DatabaseInfo)
async def check_redis_health(
    service: DatabaseService = Depends(get_database_service),
    current_user=Depends(get_current_user),
):
    """Redis í—¬ìŠ¤ ì²´í¬"""
    return await service.get_redis_health()


@router.get("/{db_type}/health", response_model=DatabaseInfo)
async def check_database_health(
    db_type: str,
    service: DatabaseService = Depends(get_database_service),
    current_user=Depends(get_current_user),
):
    """íŠ¹ì • ë°ì´í„°ë² ì´ìŠ¤ í—¬ìŠ¤ ì²´í¬"""
    db_type_lower = db_type.lower()

    if db_type_lower in ("postgres", "postgresql"):
        return await service.get_postgres_health()
    elif db_type_lower in ("mongo", "mongodb"):
        return await service.get_mongo_health()
    elif db_type_lower == "redis":
        return await service.get_redis_health()
    else:
        raise HTTPException(
            status_code=400,
            detail=f"Unknown database type: {db_type}. Supported: postgres, mongo, redis",
        )


@router.get("/postgres/stats", response_model=PostgresDatabaseStats)
async def get_postgres_stats(
    service: DatabaseService = Depends(get_database_service),
    current_user=Depends(get_current_user),
):
    """PostgreSQL ìƒì„¸ í†µê³„"""
    return await service.get_postgres_stats()


@router.get("/mongo/stats", response_model=MongoDatabaseStats)
async def get_mongo_stats(
    service: DatabaseService = Depends(get_database_service),
    current_user=Depends(get_current_user),
):
    """MongoDB ìƒì„¸ í†µê³„"""
    return await service.get_mongo_stats()


@router.get("/redis/stats", response_model=RedisStats)
async def get_redis_stats(
    service: DatabaseService = Depends(get_database_service),
    current_user=Depends(get_current_user),
):
    """Redis ìƒì„¸ í†µê³„"""
    return await service.get_redis_stats()

```

---

## backend/admin-dashboard/api/routers/documents.py

```py
"""
Document Router - ë¬¸ì„œ ê´€ë¦¬ API ì—”ë“œí¬ì¸íŠ¸
"""
from typing import Optional

from fastapi import APIRouter, Depends, HTTPException, Query

from ..models.schemas import Document, DocumentCategory, UserRole
from ..dependencies import get_current_user, get_document_service, require_role

router = APIRouter(prefix="/documents", tags=["Documents"])


@router.get("", response_model=list[Document])
async def list_documents(
    category: Optional[DocumentCategory] = Query(None, description="ì¹´í…Œê³ ë¦¬ í•„í„°"),
    tag: Optional[str] = Query(None, description="íƒœê·¸ í•„í„°"),
    environment: Optional[str] = Query(None, description="í™˜ê²½ í•„í„°"),
    search: Optional[str] = Query(None, description="ê²€ìƒ‰ì–´"),
    doc_service=Depends(get_document_service),
    current_user=Depends(get_current_user),
):
    """ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
    docs = doc_service.list_documents(
        category=category,
        tag=tag,
        environment=environment,
        search=search,
    )
    # ëª©ë¡ì—ì„œëŠ” content ì œì™¸
    for doc in docs:
        doc.content = None
    return docs


@router.get("/categories")
async def get_categories_summary(
    doc_service=Depends(get_document_service),
    current_user=Depends(get_current_user),
):
    """ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ ìˆ˜ ìš”ì•½"""
    return doc_service.get_categories_summary()


@router.get("/tags")
async def get_tags_summary(
    doc_service=Depends(get_document_service),
    current_user=Depends(get_current_user),
):
    """íƒœê·¸ë³„ ë¬¸ì„œ ìˆ˜ ìš”ì•½"""
    return doc_service.get_tags_summary()


@router.get("/related")
async def get_related_documents(
    environment: Optional[str] = Query(None, description="í™˜ê²½ ì´ë¦„"),
    script_id: Optional[str] = Query(None, description="ìŠ¤í¬ë¦½íŠ¸ ID"),
    doc_service=Depends(get_document_service),
    current_user=Depends(get_current_user),
):
    """ê´€ë ¨ ë¬¸ì„œ ì¡°íšŒ"""
    if not environment and not script_id:
        raise HTTPException(
            status_code=400,
            detail="At least one of environment or script_id is required",
        )

    docs = doc_service.get_related_documents(
        environment=environment,
        script_id=script_id,
    )
    # ëª©ë¡ì—ì„œëŠ” content ì œì™¸
    for doc in docs:
        doc.content = None
    return docs


@router.get("/{doc_id}", response_model=Document)
async def get_document(
    doc_id: str,
    doc_service=Depends(get_document_service),
    current_user=Depends(get_current_user),
):
    """ë¬¸ì„œ ìƒì„¸ ì¡°íšŒ (ë‚´ìš© í¬í•¨)"""
    doc = doc_service.get_document(doc_id)
    if not doc:
        raise HTTPException(status_code=404, detail="Document not found")
    return doc


@router.patch("/{doc_id}", response_model=Document)
async def update_document_metadata(
    doc_id: str,
    title: Optional[str] = None,
    category: Optional[DocumentCategory] = None,
    tags: Optional[list[str]] = Query(None),
    related_environments: Optional[list[str]] = Query(None),
    related_scripts: Optional[list[str]] = Query(None),
    doc_service=Depends(get_document_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìˆ˜ì • (Admin ê¶Œí•œ í•„ìš”)"""
    doc = doc_service.update_document_metadata(
        doc_id=doc_id,
        title=title,
        category=category,
        tags=tags,
        related_environments=related_environments,
        related_scripts=related_scripts,
    )
    if not doc:
        raise HTTPException(status_code=404, detail="Document not found")
    return doc


@router.post("/refresh")
async def refresh_documents(
    doc_service=Depends(get_document_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ë¬¸ì„œ ëª©ë¡ ìƒˆë¡œê³ ì¹¨ (Admin ê¶Œí•œ í•„ìš”)"""
    diff = doc_service.refresh_documents()
    return {
        "success": True,
        "message": f"Documents refreshed. {diff:+d} documents changed.",
        "total": len(doc_service.documents),
    }

```

---

## backend/admin-dashboard/api/routers/environments.py

```py
"""
Environment Router - í™˜ê²½ ê´€ë¦¬ API ì—”ë“œí¬ì¸íŠ¸
"""
from typing import Optional

from fastapi import APIRouter, Depends, HTTPException, Query, status
from fastapi.responses import StreamingResponse

from ..models.schemas import (
    AuditAction,
    Environment,
    EnvironmentCreate,
    EnvironmentStatus,
    EnvironmentUpdate,
    UserRole,
)
from ..dependencies import (
    get_audit_service,
    get_current_user,
    get_environment_service,
    require_role,
)

router = APIRouter(prefix="/environments", tags=["Environments"])


@router.get("", response_model=list[Environment])
async def list_environments(
    active_only: bool = Query(False, description="í™œì„± í™˜ê²½ë§Œ ì¡°íšŒ"),
    env_service=Depends(get_environment_service),
    current_user=Depends(get_current_user),
):
    """í™˜ê²½ ëª©ë¡ ì¡°íšŒ"""
    return env_service.list_environments(active_only=active_only)


@router.get("/{env_id}", response_model=Environment)
async def get_environment(
    env_id: str,
    env_service=Depends(get_environment_service),
    current_user=Depends(get_current_user),
):
    """í™˜ê²½ ìƒì„¸ ì¡°íšŒ"""
    env = env_service.get_environment(env_id)
    if not env:
        raise HTTPException(status_code=404, detail="Environment not found")
    return env


@router.post("", response_model=Environment, status_code=status.HTTP_201_CREATED)
async def create_environment(
    data: EnvironmentCreate,
    env_service=Depends(get_environment_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """í™˜ê²½ ìƒì„± (Admin ê¶Œí•œ í•„ìš”)"""
    env = env_service.create_environment(data)

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.CREATE,
        resource_type="environment",
        resource_id=env.id,
        resource_name=env.name,
        details={"data": data.model_dump()},
    )

    return env


@router.patch("/{env_id}", response_model=Environment)
async def update_environment(
    env_id: str,
    data: EnvironmentUpdate,
    env_service=Depends(get_environment_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """í™˜ê²½ ìˆ˜ì • (Admin ê¶Œí•œ í•„ìš”)"""
    env = env_service.update_environment(env_id, data)
    if not env:
        raise HTTPException(status_code=404, detail="Environment not found")

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="environment",
        resource_id=env.id,
        resource_name=env.name,
        details={"changes": data.model_dump(exclude_unset=True)},
    )

    return env


@router.delete("/{env_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_environment(
    env_id: str,
    env_service=Depends(get_environment_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """í™˜ê²½ ì‚­ì œ (Admin ê¶Œí•œ í•„ìš”)"""
    env = env_service.get_environment(env_id)
    if not env:
        raise HTTPException(status_code=404, detail="Environment not found")

    if not env_service.delete_environment(env_id):
        raise HTTPException(status_code=500, detail="Failed to delete environment")

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DELETE,
        resource_type="environment",
        resource_id=env_id,
        resource_name=env.name,
    )


@router.get("/{env_id}/status", response_model=EnvironmentStatus)
async def get_environment_status(
    env_id: str,
    env_service=Depends(get_environment_service),
    current_user=Depends(get_current_user),
):
    """í™˜ê²½ ìƒíƒœ ì¡°íšŒ (ì»¨í…Œì´ë„ˆ ìƒíƒœ)"""
    status = env_service.get_environment_status(env_id)
    if not status:
        raise HTTPException(status_code=404, detail="Environment not found")
    return status


@router.post("/{env_id}/up")
async def docker_compose_up(
    env_id: str,
    build: bool = Query(True, description="ì´ë¯¸ì§€ ë¹Œë“œ ì—¬ë¶€"),
    env_service=Depends(get_environment_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """Docker Compose Up ì‹¤í–‰"""
    env = env_service.get_environment(env_id)
    if not env:
        raise HTTPException(status_code=404, detail="Environment not found")

    success, output = await env_service.docker_compose_up(env_id, build=build)

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DEPLOY,
        resource_type="environment",
        resource_id=env_id,
        resource_name=env.name,
        environment_id=env_id,
        environment_name=env.name,
        details={"build": build},
        success=success,
        error_message=output if not success else None,
    )

    if not success:
        raise HTTPException(status_code=500, detail=output)

    return {"success": True, "message": "Services started successfully", "output": output}


@router.post("/{env_id}/down")
async def docker_compose_down(
    env_id: str,
    volumes: bool = Query(False, description="ë³¼ë¥¨ë„ ì‚­ì œí• ì§€ ì—¬ë¶€"),
    env_service=Depends(get_environment_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """Docker Compose Down ì‹¤í–‰"""
    env = env_service.get_environment(env_id)
    if not env:
        raise HTTPException(status_code=404, detail="Environment not found")

    # ë³¼ë¥¨ ì‚­ì œëŠ” Admin ê¶Œí•œ í•„ìš”
    if volumes and current_user.role != UserRole.ADMIN:
        raise HTTPException(
            status_code=403,
            detail="Admin permission required to delete volumes",
        )

    success, output = await env_service.docker_compose_down(env_id, volumes=volumes)

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.EXECUTE,
        resource_type="environment",
        resource_id=env_id,
        resource_name=env.name,
        environment_id=env_id,
        environment_name=env.name,
        details={"action": "down", "volumes": volumes},
        success=success,
        error_message=output if not success else None,
    )

    if not success:
        raise HTTPException(status_code=500, detail=output)

    return {"success": True, "message": "Services stopped successfully", "output": output}


@router.post("/{env_id}/restart")
async def docker_compose_restart(
    env_id: str,
    service: Optional[str] = Query(None, description="ì¬ì‹œì‘í•  ì„œë¹„ìŠ¤ ì´ë¦„"),
    env_service=Depends(get_environment_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """Docker Compose Restart ì‹¤í–‰"""
    env = env_service.get_environment(env_id)
    if not env:
        raise HTTPException(status_code=404, detail="Environment not found")

    success, output = await env_service.docker_compose_restart(env_id, service=service)

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.EXECUTE,
        resource_type="environment",
        resource_id=env_id,
        resource_name=env.name,
        environment_id=env_id,
        environment_name=env.name,
        details={"action": "restart", "service": service},
        success=success,
        error_message=output if not success else None,
    )

    if not success:
        raise HTTPException(status_code=500, detail=output)

    return {"success": True, "message": "Services restarted successfully", "output": output}


@router.get("/{env_id}/logs/{service}")
async def get_service_logs(
    env_id: str,
    service: str,
    tail: int = Query(100, ge=1, le=1000, description="ì¶œë ¥í•  ë¡œê·¸ ì¤„ ìˆ˜"),
    env_service=Depends(get_environment_service),
    current_user=Depends(get_current_user),
):
    """ì„œë¹„ìŠ¤ ë¡œê·¸ ì¡°íšŒ"""
    env = env_service.get_environment(env_id)
    if not env:
        raise HTTPException(status_code=404, detail="Environment not found")

    success, output = await env_service.get_service_logs(env_id, service, tail=tail)

    if not success:
        raise HTTPException(status_code=500, detail=output)

    return {"service": service, "logs": output}

```

---

## backend/admin-dashboard/api/routers/health_monitor.py

```py
"""
Service Health Monitoring Router
ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬ ë° ì‹œìŠ¤í…œ ìƒíƒœ ëª¨ë‹ˆí„°ë§ API
"""

from fastapi import APIRouter, Depends, HTTPException
from fastapi.responses import StreamingResponse
import asyncio
import json
from datetime import datetime

from ..models.schemas import (
    ServiceHealth,
    InfrastructureHealth,
    OverallSystemHealth,
    ServiceInfo,
    UserRole,
)
from ..dependencies import get_current_user, require_role, get_health_service
from ..services.health_service import HealthService

router = APIRouter(prefix="/health-monitor", tags=["Health Monitor"])


@router.get("/services", response_model=list[ServiceInfo])
async def list_services(
    service: HealthService = Depends(get_health_service),
    current_user=Depends(get_current_user),
):
    """ë“±ë¡ëœ ëª¨ë“  ì„œë¹„ìŠ¤ ëª©ë¡ ì¡°íšŒ"""
    services = service.get_all_services()
    return [
        ServiceInfo(
            id=s["id"],
            name=s["name"],
            description=s.get("description"),
            port=s.get("port"),
            healthcheck=s.get("healthcheck", "/health"),
            hostname=s["hostname"],
            type=s["type"],
            tags=s.get("tags", []),
        )
        for s in services
    ]


@router.get("/infrastructure", response_model=list[dict])
async def list_infrastructure(
    service: HealthService = Depends(get_health_service),
    current_user=Depends(get_current_user),
):
    """ì¸í”„ë¼ ì„œë¹„ìŠ¤ ëª©ë¡ ì¡°íšŒ"""
    return service.get_infrastructure_services()


@router.get("/check/{service_id}", response_model=ServiceHealth)
async def check_service(
    service_id: str,
    service: HealthService = Depends(get_health_service),
    current_user=Depends(get_current_user),
):
    """íŠ¹ì • ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬"""
    health = await service.check_service_health(service_id)
    return health


@router.get("/check-all", response_model=list[ServiceHealth])
async def check_all_services(
    service: HealthService = Depends(get_health_service),
    current_user=Depends(get_current_user),
):
    """ëª¨ë“  ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬ (ë³‘ë ¬ ì‹¤í–‰)"""
    return await service.check_all_services_health()


@router.get("/check-infrastructure", response_model=list[InfrastructureHealth])
async def check_infrastructure(
    service: HealthService = Depends(get_health_service),
    current_user=Depends(get_current_user),
):
    """ëª¨ë“  ì¸í”„ë¼ ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬"""
    return await service.check_infrastructure_health()


@router.get("/overall", response_model=OverallSystemHealth)
async def get_overall_health(
    service: HealthService = Depends(get_health_service),
    current_user=Depends(get_current_user),
):
    """ì „ì²´ ì‹œìŠ¤í…œ í—¬ìŠ¤ ìƒíƒœ ìš”ì•½"""
    return await service.get_overall_health()


@router.get("/stream")
async def stream_health_updates(
    interval: int = 10,
    service: HealthService = Depends(get_health_service),
    current_user=Depends(get_current_user),
):
    """ì‹¤ì‹œê°„ í—¬ìŠ¤ ìƒíƒœ ìŠ¤íŠ¸ë¦¬ë° (SSE)

    Args:
        interval: ì—…ë°ì´íŠ¸ ê°„ê²© (ì´ˆ, ê¸°ë³¸ 10ì´ˆ, ìµœì†Œ 5ì´ˆ)
    """
    interval = max(5, min(interval, 60))  # 5-60ì´ˆ ë²”ìœ„ ì œí•œ

    async def generate():
        while True:
            try:
                health = await service.get_overall_health()
                data = health.model_dump_json()
                yield f"data: {data}\n\n"
                await asyncio.sleep(interval)
            except Exception as e:
                error_data = json.dumps(
                    {"error": str(e), "timestamp": datetime.utcnow().isoformat()}
                )
                yield f"data: {error_data}\n\n"
                await asyncio.sleep(interval)

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )


@router.get("/last-check/{service_id}", response_model=ServiceHealth)
async def get_last_check(
    service_id: str,
    service: HealthService = Depends(get_health_service),
    current_user=Depends(get_current_user),
):
    """ë§ˆì§€ë§‰ í—¬ìŠ¤ ì²´í¬ ê²°ê³¼ ì¡°íšŒ (ìºì‹œëœ ê²°ê³¼)"""
    result = service.get_last_check(service_id)
    if not result:
        raise HTTPException(
            status_code=404,
            detail=f"No health check result found for service: {service_id}",
        )
    return result

```

---

## backend/admin-dashboard/api/routers/kafka.py

```py
"""
Kafka/Redpanda Monitoring Router
Kafka/Redpanda í´ëŸ¬ìŠ¤í„° ëª¨ë‹ˆí„°ë§ API
"""

from fastapi import APIRouter, Depends, HTTPException

from ..models.schemas import (
    KafkaTopicInfo,
    KafkaConsumerGroupInfo,
    KafkaClusterInfo,
    UserRole,
)
from ..dependencies import get_current_user, require_role, get_kafka_service
from ..services.kafka_service import KafkaService

router = APIRouter(prefix="/kafka", tags=["Kafka/Redpanda Monitoring"])


@router.get("/cluster", response_model=KafkaClusterInfo)
async def get_cluster_info(
    service: KafkaService = Depends(get_kafka_service),
    current_user=Depends(get_current_user),
):
    """Kafka/Redpanda í´ëŸ¬ìŠ¤í„° ì „ì²´ ì •ë³´ ì¡°íšŒ"""
    return await service.get_cluster_info()


@router.get("/topics", response_model=list[KafkaTopicInfo])
async def list_topics(
    service: KafkaService = Depends(get_kafka_service),
    current_user=Depends(get_current_user),
):
    """ëª¨ë“  í† í”½ ëª©ë¡ ì¡°íšŒ"""
    return await service.list_topics()


@router.get("/topics/{topic_name}", response_model=KafkaTopicInfo)
async def get_topic(
    topic_name: str,
    service: KafkaService = Depends(get_kafka_service),
    current_user=Depends(get_current_user),
):
    """íŠ¹ì • í† í”½ ìƒì„¸ ì •ë³´"""
    topic = await service.get_topic_detail(topic_name)
    if not topic:
        raise HTTPException(status_code=404, detail=f"Topic not found: {topic_name}")
    return topic


@router.get("/consumer-groups", response_model=list[KafkaConsumerGroupInfo])
async def list_consumer_groups(
    service: KafkaService = Depends(get_kafka_service),
    current_user=Depends(get_current_user),
):
    """ëª¨ë“  ì»¨ìŠˆë¨¸ ê·¸ë£¹ ì¡°íšŒ"""
    return await service.list_consumer_groups()


@router.get("/consumer-groups/{group_id}", response_model=KafkaConsumerGroupInfo)
async def get_consumer_group(
    group_id: str,
    service: KafkaService = Depends(get_kafka_service),
    current_user=Depends(get_current_user),
):
    """íŠ¹ì • ì»¨ìŠˆë¨¸ ê·¸ë£¹ ìƒì„¸ ì •ë³´"""
    group = await service.get_consumer_group_detail(group_id)
    if not group:
        raise HTTPException(
            status_code=404, detail=f"Consumer group not found: {group_id}"
        )
    return group


@router.get("/health")
async def check_health(
    service: KafkaService = Depends(get_kafka_service),
    current_user=Depends(get_current_user),
):
    """Kafka/Redpanda í—¬ìŠ¤ ì²´í¬"""
    return await service.check_health()

```

---

## backend/admin-dashboard/api/routers/llm_providers.py

```py
"""
LLM Provider Settings Router - ê´€ë¦¬ì ì „ì—­ LLM ì„¤ì • API ì—”ë“œí¬ì¸íŠ¸
"""

import os
from enum import Enum
from typing import Optional
from datetime import datetime

import httpx
from fastapi import APIRouter, Depends, HTTPException, Query, status
from pydantic import BaseModel, Field

from ..models.schemas import (
    AuditAction,
    UserRole,
)
from ..dependencies import (
    get_audit_service,
    get_current_user,
    require_role,
)

router = APIRouter(prefix="/llm-providers", tags=["LLM Providers"])


# ============================================================================
# Schemas
# ============================================================================


class LlmProviderType(str, Enum):
    OPENAI = "OPENAI"
    ANTHROPIC = "ANTHROPIC"
    GOOGLE = "GOOGLE"
    OPENROUTER = "OPENROUTER"
    OLLAMA = "OLLAMA"
    AZURE_OPENAI = "AZURE_OPENAI"
    CUSTOM = "CUSTOM"


class LlmProviderTypeInfo(BaseModel):
    value: LlmProviderType
    displayName: str
    description: str
    requiresApiKey: bool
    defaultBaseUrl: Optional[str] = None


class LlmProviderSettingsRequest(BaseModel):
    providerType: LlmProviderType
    apiKey: Optional[str] = Field(None, description="API í‚¤ (ë¹„ìš°ë©´ ê¸°ì¡´ ê°’ ìœ ì§€)")
    defaultModel: str = Field(..., description="ê¸°ë³¸ ëª¨ë¸")
    baseUrl: Optional[str] = Field(None, description="Base URL (Ollama/Customìš©)")
    enabled: bool = Field(True, description="í™œì„±í™” ì—¬ë¶€")
    priority: int = Field(100, ge=1, le=999, description="ìš°ì„ ìˆœìœ„")
    maxTokens: int = Field(4096, ge=1, le=128000, description="ìµœëŒ€ í† í°")
    temperature: float = Field(0.7, ge=0, le=2, description="Temperature")
    timeoutMs: int = Field(60000, ge=1000, le=300000, description="íƒ€ì„ì•„ì›ƒ (ms)")
    azureDeploymentName: Optional[str] = Field(
        None, description="Azure deployment name"
    )
    azureApiVersion: Optional[str] = Field(None, description="Azure API version")


class LlmProviderSettings(BaseModel):
    id: int
    providerType: LlmProviderType
    userId: Optional[str] = None  # null = global setting
    hasApiKey: bool
    maskedApiKey: Optional[str] = None
    defaultModel: str
    baseUrl: Optional[str] = None
    enabled: bool
    priority: int
    maxTokens: int
    temperature: float
    timeoutMs: int
    azureDeploymentName: Optional[str] = None
    azureApiVersion: Optional[str] = None
    lastTestedAt: Optional[datetime] = None
    lastTestSuccess: Optional[bool] = None
    createdAt: datetime
    updatedAt: datetime


class LlmTestResult(BaseModel):
    providerType: LlmProviderType
    success: bool
    message: str
    latencyMs: Optional[int] = None
    testedAt: datetime


# Provider metadata
LLM_PROVIDER_TYPES = [
    LlmProviderTypeInfo(
        value=LlmProviderType.OPENAI,
        displayName="OpenAI",
        description="GPT-4, GPT-3.5 Turbo",
        requiresApiKey=True,
    ),
    LlmProviderTypeInfo(
        value=LlmProviderType.ANTHROPIC,
        displayName="Anthropic",
        description="Claude 3.5 Sonnet, Claude 3 Opus",
        requiresApiKey=True,
    ),
    LlmProviderTypeInfo(
        value=LlmProviderType.GOOGLE,
        displayName="Google AI",
        description="Gemini 1.5 Pro, Gemini 1.5 Flash",
        requiresApiKey=True,
    ),
    LlmProviderTypeInfo(
        value=LlmProviderType.OPENROUTER,
        displayName="OpenRouter",
        description="ë‹¤ì–‘í•œ ëª¨ë¸ì„ í†µí•© APIë¡œ ì œê³µ",
        requiresApiKey=True,
    ),
    LlmProviderTypeInfo(
        value=LlmProviderType.OLLAMA,
        displayName="Ollama",
        description="ë¡œì»¬ LLM ì‹¤í–‰",
        requiresApiKey=False,
        defaultBaseUrl="http://localhost:11434",
    ),
    LlmProviderTypeInfo(
        value=LlmProviderType.AZURE_OPENAI,
        displayName="Azure OpenAI",
        description="Azureì—ì„œ í˜¸ìŠ¤íŒ…í•˜ëŠ” OpenAI ëª¨ë¸",
        requiresApiKey=True,
    ),
    LlmProviderTypeInfo(
        value=LlmProviderType.CUSTOM,
        displayName="Custom API",
        description="OpenAI í˜¸í™˜ ì»¤ìŠ¤í…€ API",
        requiresApiKey=False,
    ),
]


# Backend service URL for data-collection-service
COLLECTOR_SERVICE_URL = os.environ.get("COLLECTOR_SERVICE_URL", "http://localhost:8081")


# ============================================================================
# Helper functions
# ============================================================================


async def call_collector_service(
    method: str,
    path: str,
    params: Optional[dict] = None,
    json_data: Optional[dict] = None,
) -> dict:
    """Call the data-collection-service API"""
    url = f"{COLLECTOR_SERVICE_URL}{path}"

    async with httpx.AsyncClient(timeout=30.0) as client:
        try:
            response = await client.request(
                method=method,
                url=url,
                params=params,
                json=json_data,
            )

            if response.status_code >= 400:
                detail = (
                    response.json().get("message", response.text)
                    if response.text
                    else "Unknown error"
                )
                raise HTTPException(
                    status_code=response.status_code,
                    detail=detail,
                )

            if response.status_code == 204:
                return {}

            return response.json()
        except httpx.RequestError as e:
            raise HTTPException(
                status_code=503,
                detail=f"Collector service unavailable: {str(e)}",
            )


# ============================================================================
# Endpoints
# ============================================================================


@router.get("/types", response_model=list[LlmProviderTypeInfo])
async def list_provider_types(
    current_user=Depends(get_current_user),
):
    """LLM Provider íƒ€ì… ëª©ë¡ ì¡°íšŒ"""
    return LLM_PROVIDER_TYPES


@router.get("/global", response_model=list[LlmProviderSettings])
async def list_global_settings(
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ì „ì—­ LLM ì„¤ì • ëª©ë¡ ì¡°íšŒ (Admin ê¶Œí•œ í•„ìš”)"""
    result = await call_collector_service("GET", "/api/v1/admin/llm-providers")
    return result


@router.get("/global/{provider_type}", response_model=LlmProviderSettings)
async def get_global_setting(
    provider_type: LlmProviderType,
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """íŠ¹ì • Providerì˜ ì „ì—­ ì„¤ì • ì¡°íšŒ (Admin ê¶Œí•œ í•„ìš”)"""
    result = await call_collector_service(
        "GET", f"/api/v1/admin/llm-providers/{provider_type.value}"
    )
    return result


@router.put("/global/{provider_type}", response_model=LlmProviderSettings)
async def save_global_setting(
    provider_type: LlmProviderType,
    data: LlmProviderSettingsRequest,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ì „ì—­ LLM ì„¤ì • ì €ì¥/ìˆ˜ì • (Admin ê¶Œí•œ í•„ìš”)"""
    # Ensure provider type matches
    if data.providerType != provider_type:
        raise HTTPException(
            status_code=400,
            detail="Provider type in path must match request body",
        )

    result = await call_collector_service(
        "PUT",
        f"/api/v1/admin/llm-providers/{provider_type.value}",
        json_data=data.model_dump(exclude_none=True),
    )

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="llm_provider",
        resource_id=provider_type.value,
        resource_name=f"Global {provider_type.value} Settings",
        details={
            "provider": provider_type.value,
            "enabled": data.enabled,
            "model": data.defaultModel,
        },
    )

    return result


@router.delete("/global/{provider_type}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_global_setting(
    provider_type: LlmProviderType,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ì „ì—­ LLM ì„¤ì • ì‚­ì œ (Admin ê¶Œí•œ í•„ìš”)"""
    await call_collector_service(
        "DELETE", f"/api/v1/admin/llm-providers/{provider_type.value}"
    )

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DELETE,
        resource_type="llm_provider",
        resource_id=provider_type.value,
        resource_name=f"Global {provider_type.value} Settings",
    )


@router.post("/test", response_model=LlmTestResult)
async def test_connection(
    provider_type: LlmProviderType = Query(..., description="Provider íƒ€ì…"),
    model: Optional[str] = Query(None, description="í…ŒìŠ¤íŠ¸í•  ëª¨ë¸"),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """LLM Provider ì—°ê²° í…ŒìŠ¤íŠ¸ (Admin ê¶Œí•œ í•„ìš”)"""
    params = {"providerType": provider_type.value}
    if model:
        params["model"] = model

    result = await call_collector_service(
        "POST", "/api/v1/llm-providers/test", params=params
    )
    return result


@router.get("/effective", response_model=list[LlmProviderSettings])
async def get_effective_settings(
    user_id: Optional[str] = Query(None, description="ì‚¬ìš©ì ID (ì—†ìœ¼ë©´ ì „ì—­ë§Œ)"),
    current_user=Depends(get_current_user),
):
    """ìœ íš¨ LLM ì„¤ì • ì¡°íšŒ (ì‚¬ìš©ì ì„¤ì • + ì „ì—­ fallback)"""
    params = {}
    if user_id:
        params["userId"] = user_id

    result = await call_collector_service(
        "GET", "/api/v1/llm-providers/effective", params=params
    )
    return result


@router.get("/enabled", response_model=list[LlmProviderSettings])
async def get_enabled_providers(
    user_id: Optional[str] = Query(None, description="ì‚¬ìš©ì ID"),
    current_user=Depends(get_current_user),
):
    """í™œì„±í™”ëœ LLM Provider ëª©ë¡ ì¡°íšŒ"""
    params = {}
    if user_id:
        params["userId"] = user_id

    result = await call_collector_service(
        "GET", "/api/v1/llm-providers/enabled", params=params
    )
    return result

```

---

## backend/admin-dashboard/api/routers/ml_addons.py

```py
"""
ML Addons Router - ML ì• ë“œì˜¨ ê´€ë¦¬ API ì—”ë“œí¬ì¸íŠ¸
"""

import os
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional

import httpx
from fastapi import APIRouter, Depends, HTTPException, Query, status
from pydantic import BaseModel, Field

from ..models.schemas import (
    AuditAction,
    UserRole,
)
from ..dependencies import (
    get_audit_service,
    get_current_user,
    require_role,
)

router = APIRouter(prefix="/ml-addons", tags=["ML Addons"])


# ============================================================================
# Configuration
# ============================================================================

CRAWLER_SERVICE_URL = os.environ.get(
    "CRAWLER_SERVICE_URL", "http://autonomous-crawler:8030"
)


# ============================================================================
# Schemas
# ============================================================================


class MLAddonType(str, Enum):
    """ML Addon íƒ€ì…"""

    SENTIMENT = "sentiment"
    FACTCHECK = "factcheck"
    BIAS = "bias"


class MLAddonStatus(str, Enum):
    """ML Addon ìƒíƒœ"""

    HEALTHY = "healthy"
    UNHEALTHY = "unhealthy"
    UNKNOWN = "unknown"
    WARMING_UP = "warming_up"


class MLAddonInfo(BaseModel):
    """ML Addon ì •ë³´"""

    key: str
    name: str
    description: str
    endpoint: str
    status: MLAddonStatus
    features: List[str]


class MLAddonHealthResponse(BaseModel):
    """ML Addon í—¬ìŠ¤ ì‘ë‹µ"""

    status: str
    auto_analysis_enabled: bool
    addons: Dict[str, Dict[str, Any]]


class MLAddonStatusResponse(BaseModel):
    """ML Addon ìƒíƒœ ì‘ë‹µ"""

    auto_analysis_enabled: bool
    parallel_analysis: bool
    addons: Dict[str, Any]


class MLAnalyzeRequest(BaseModel):
    """ML ë¶„ì„ ìš”ì²­"""

    article_id: int = Field(..., description="ê¸°ì‚¬ ID")
    title: str = Field(..., description="ê¸°ì‚¬ ì œëª©", min_length=1)
    content: str = Field(..., description="ê¸°ì‚¬ ë³¸ë¬¸", min_length=10)
    source: Optional[str] = Field(default=None, description="ì–¸ë¡ ì‚¬ëª…")
    url: Optional[str] = Field(default=None, description="ê¸°ì‚¬ URL")
    published_at: Optional[str] = Field(default=None, description="ë°œí–‰ì¼")
    addons: Optional[List[str]] = Field(
        default=None,
        description="ì‹¤í–‰í•  ì• ë“œì˜¨ ëª©ë¡ (sentiment, factcheck, bias). Noneì´ë©´ ëª¨ë‘ ì‹¤í–‰",
    )
    save_to_db: bool = Field(default=True, description="ê²°ê³¼ë¥¼ DBì— ì €ì¥í• ì§€ ì—¬ë¶€")


class MLBatchAnalyzeRequest(BaseModel):
    """ML ë°°ì¹˜ ë¶„ì„ ìš”ì²­"""

    articles: List[MLAnalyzeRequest] = Field(..., min_length=1, max_length=50)
    addons: Optional[List[str]] = None
    save_to_db: bool = True
    max_concurrent: int = Field(default=5, ge=1, le=20)


class MLAnalysisResult(BaseModel):
    """ML ë¶„ì„ ê²°ê³¼"""

    addon_type: str
    success: bool
    request_id: str
    results: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    latency_ms: int = 0
    analyzed_at: str


class BatchAnalysisResult(BaseModel):
    """ë°°ì¹˜ ë¶„ì„ ê²°ê³¼"""

    article_id: int
    sentiment: Optional[MLAnalysisResult] = None
    factcheck: Optional[MLAnalysisResult] = None
    bias: Optional[MLAnalysisResult] = None
    total_latency_ms: int = 0
    success_count: int = 0
    failure_count: int = 0


class MLAddonDBEntry(BaseModel):
    """DBì— ì €ì¥ëœ ML Addon ì •ë³´"""

    id: int
    addon_key: str
    name: str
    description: Optional[str] = None
    endpoint_url: str
    version: Optional[str] = None
    status: str
    config: Optional[Dict[str, Any]] = None
    created_at: datetime
    updated_at: datetime


class MLAddonCreateRequest(BaseModel):
    """ML Addon ìƒì„± ìš”ì²­"""

    addon_key: str = Field(..., description="ê³ ìœ  í‚¤ (ì˜ˆ: sentiment-addon)")
    name: str = Field(..., description="í‘œì‹œ ì´ë¦„")
    description: Optional[str] = Field(None, description="ì„¤ëª…")
    endpoint_url: str = Field(..., description="ì„œë¹„ìŠ¤ URL")
    version: Optional[str] = Field(None, description="ë²„ì „")
    config: Optional[Dict[str, Any]] = Field(None, description="ì¶”ê°€ ì„¤ì •")


class MLAddonUpdateRequest(BaseModel):
    """ML Addon ìˆ˜ì • ìš”ì²­"""

    name: Optional[str] = None
    description: Optional[str] = None
    endpoint_url: Optional[str] = None
    version: Optional[str] = None
    status: Optional[str] = None
    config: Optional[Dict[str, Any]] = None


# ============================================================================
# Helper functions
# ============================================================================


async def call_crawler_service(
    method: str,
    path: str,
    params: Optional[dict] = None,
    json_data: Optional[dict] = None,
    timeout: float = 60.0,
) -> dict:
    """Call the autonomous-crawler-service API"""
    url = f"{CRAWLER_SERVICE_URL}{path}"

    async with httpx.AsyncClient(timeout=timeout) as client:
        try:
            response = await client.request(
                method=method,
                url=url,
                params=params,
                json=json_data,
            )

            if response.status_code >= 400:
                detail = (
                    response.json().get("detail", response.text)
                    if response.text
                    else "Unknown error"
                )
                raise HTTPException(
                    status_code=response.status_code,
                    detail=detail,
                )

            if response.status_code == 204:
                return {}

            return response.json()
        except httpx.RequestError as e:
            raise HTTPException(
                status_code=503,
                detail=f"Crawler service unavailable: {str(e)}",
            )


# ============================================================================
# Endpoints - Health & Status
# ============================================================================


@router.get("/health", response_model=MLAddonHealthResponse)
async def ml_health_check(
    current_user=Depends(get_current_user),
):
    """
    ML ì‹œìŠ¤í…œ í—¬ìŠ¤ì²´í¬.

    ëª¨ë“  ML Addonì˜ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    """
    result = await call_crawler_service("GET", "/ml/health")
    return result


@router.get("/status", response_model=MLAddonStatusResponse)
async def ml_status(
    current_user=Depends(get_current_user),
):
    """
    ML ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ.

    í˜„ì¬ ì„¤ì • ë° Addon ì—°ê²° ìƒíƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    result = await call_crawler_service("GET", "/ml/status")
    return result


@router.get("/list", response_model=Dict[str, Any])
async def list_addons(
    current_user=Depends(get_current_user),
):
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ ML Addon ëª©ë¡.

    ê° ì• ë“œì˜¨ì˜ ê¸°ëŠ¥ê³¼ í˜„ì¬ ìƒíƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    result = await call_crawler_service("GET", "/ml/addons")
    return result


# ============================================================================
# Endpoints - Analysis
# ============================================================================


@router.post("/analyze", response_model=BatchAnalysisResult)
async def analyze_article(
    request: MLAnalyzeRequest,
    current_user=Depends(get_current_user),
):
    """
    ë‹¨ì¼ ê¸°ì‚¬ ML ë¶„ì„.

    ê¸°ì‚¬ì— ëŒ€í•´ sentiment, factcheck, bias ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    result = await call_crawler_service(
        "POST",
        "/ml/analyze",
        json_data=request.model_dump(exclude_none=True),
        timeout=120.0,
    )
    return result


@router.post("/analyze/simple")
async def analyze_text_simple(
    text: str = Query(..., min_length=10, description="ë¶„ì„í•  í…ìŠ¤íŠ¸"),
    source: Optional[str] = Query(None, description="ì¶œì²˜"),
    addons: Optional[str] = Query(None, description="ì• ë“œì˜¨ (ì‰¼í‘œ êµ¬ë¶„)"),
    current_user=Depends(get_current_user),
):
    """
    ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ML ë¶„ì„.

    ê¸°ì‚¬ ID ì—†ì´ í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    ê²°ê³¼ëŠ” DBì— ì €ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """
    addon_list = addons.split(",") if addons else None

    result = await call_crawler_service(
        "POST",
        "/ml/analyze/simple",
        json_data={
            "text": text,
            "source": source,
            "addons": addon_list,
        },
        timeout=120.0,
    )
    return result


@router.post("/analyze/batch")
async def analyze_batch(
    request: MLBatchAnalyzeRequest,
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    ë°°ì¹˜ ê¸°ì‚¬ ML ë¶„ì„.

    ì—¬ëŸ¬ ê¸°ì‚¬ë¥¼ í•œ ë²ˆì— ë¶„ì„í•©ë‹ˆë‹¤. (Admin ê¶Œí•œ í•„ìš”)
    """
    result = await call_crawler_service(
        "POST",
        "/ml/analyze/batch",
        json_data=request.model_dump(exclude_none=True),
        timeout=300.0,
    )
    return result


@router.post("/analyze/url")
async def analyze_url(
    url: str = Query(..., description="ë¶„ì„í•  URL"),
    current_user=Depends(get_current_user),
):
    """
    URLì—ì„œ ê¸°ì‚¬ë¥¼ í¬ë¡¤ë§í•˜ê³  ML ë¶„ì„ ìˆ˜í–‰.

    URLì˜ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•œ í›„ sentiment, factcheck, bias ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    result = await call_crawler_service(
        "POST",
        "/ml/analyze/url",
        params={"url": url},
        timeout=120.0,
    )
    return result


# ============================================================================
# Endpoints - Configuration
# ============================================================================


@router.post("/config/toggle")
async def toggle_auto_analysis(
    enabled: bool = Query(..., description="ìë™ ë¶„ì„ í™œì„±í™” ì—¬ë¶€"),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    ìë™ ML ë¶„ì„ í† ê¸€.

    í¬ë¡¤ë§ í›„ ìë™ ML ë¶„ì„ ê¸°ëŠ¥ì„ í™œì„±í™”/ë¹„í™œì„±í™”í•©ë‹ˆë‹¤. (Admin ê¶Œí•œ í•„ìš”)
    """
    result = await call_crawler_service(
        "POST",
        "/ml/config/toggle",
        params={"enabled": enabled},
    )

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="ml_config",
        resource_id="auto_analysis",
        resource_name="ML Auto Analysis",
        details={"enabled": enabled},
    )

    return result


# ============================================================================
# Endpoints - CRUD for ml_addon table (via collector service)
# ============================================================================

COLLECTOR_SERVICE_URL_DB = os.environ.get(
    "COLLECTOR_SERVICE_URL", "http://localhost:8081"
)


async def call_collector_service_db(
    method: str,
    path: str,
    params: Optional[dict] = None,
    json_data: Optional[dict] = None,
) -> dict:
    """Call the data-collection-service API for DB operations"""
    url = f"{COLLECTOR_SERVICE_URL_DB}{path}"

    async with httpx.AsyncClient(timeout=30.0) as client:
        try:
            response = await client.request(
                method=method,
                url=url,
                params=params,
                json=json_data,
            )

            if response.status_code >= 400:
                detail = (
                    response.json().get("message", response.text)
                    if response.text
                    else "Unknown error"
                )
                raise HTTPException(
                    status_code=response.status_code,
                    detail=detail,
                )

            if response.status_code == 204:
                return {}

            return response.json()
        except httpx.RequestError as e:
            raise HTTPException(
                status_code=503,
                detail=f"Collector service unavailable: {str(e)}",
            )


@router.get("/registered", response_model=List[MLAddonDBEntry])
async def list_registered_addons(
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    DBì— ë“±ë¡ëœ ML Addon ëª©ë¡ ì¡°íšŒ (Admin ê¶Œí•œ í•„ìš”)
    """
    result = await call_collector_service_db("GET", "/api/v1/admin/ml-addons")
    return result


@router.get("/registered/{addon_id}", response_model=MLAddonDBEntry)
async def get_registered_addon(
    addon_id: int,
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    íŠ¹ì • ML Addon ì¡°íšŒ (Admin ê¶Œí•œ í•„ìš”)
    """
    result = await call_collector_service_db(
        "GET", f"/api/v1/admin/ml-addons/{addon_id}"
    )
    return result


@router.post(
    "/registered", response_model=MLAddonDBEntry, status_code=status.HTTP_201_CREATED
)
async def create_addon(
    request: MLAddonCreateRequest,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    ìƒˆ ML Addon ë“±ë¡ (Admin ê¶Œí•œ í•„ìš”)
    """
    result = await call_collector_service_db(
        "POST",
        "/api/v1/admin/ml-addons",
        json_data=request.model_dump(exclude_none=True),
    )

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.CREATE,
        resource_type="ml_addon",
        resource_id=request.addon_key,
        resource_name=request.name,
        details={"endpoint": request.endpoint_url},
    )

    return result


@router.put("/registered/{addon_id}", response_model=MLAddonDBEntry)
async def update_addon(
    addon_id: int,
    request: MLAddonUpdateRequest,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    ML Addon ìˆ˜ì • (Admin ê¶Œí•œ í•„ìš”)
    """
    result = await call_collector_service_db(
        "PUT",
        f"/api/v1/admin/ml-addons/{addon_id}",
        json_data=request.model_dump(exclude_none=True),
    )

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="ml_addon",
        resource_id=str(addon_id),
        resource_name=result.get("name", f"Addon {addon_id}"),
        details=request.model_dump(exclude_none=True),
    )

    return result


@router.delete("/registered/{addon_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_addon(
    addon_id: int,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    ML Addon ì‚­ì œ (Admin ê¶Œí•œ í•„ìš”)
    """
    await call_collector_service_db("DELETE", f"/api/v1/admin/ml-addons/{addon_id}")

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DELETE,
        resource_type="ml_addon",
        resource_id=str(addon_id),
        resource_name=f"Addon {addon_id}",
    )


@router.post("/registered/{addon_id}/test")
async def test_addon(
    addon_id: int,
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    ML Addon ì—°ê²° í…ŒìŠ¤íŠ¸ (Admin ê¶Œí•œ í•„ìš”)
    """
    result = await call_collector_service_db(
        "POST", f"/api/v1/admin/ml-addons/{addon_id}/test"
    )
    return result

```

---

## backend/admin-dashboard/api/routers/ml_training.py

```py
"""
ML Training Router - ML ëª¨ë¸ í•™ìŠµ ê´€ë¦¬ API ì—”ë“œí¬ì¸íŠ¸
"""

import os
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional

import httpx
from fastapi import APIRouter, Depends, HTTPException, Query, status
from pydantic import BaseModel, Field

from ..models.schemas import (
    AuditAction,
    UserRole,
)
from ..dependencies import (
    get_audit_service,
    get_current_user,
    require_role,
)

router = APIRouter(prefix="/ml-training", tags=["ML Training"])


# ============================================================================
# Configuration
# ============================================================================

ML_TRAINER_URL = os.environ.get("ML_TRAINER_URL", "http://ml-trainer:8103")


# ============================================================================
# Schemas
# ============================================================================


class TrainingJobStatus(str, Enum):
    """í•™ìŠµ ì‘ì—… ìƒíƒœ"""

    PENDING = "pending"
    PREPARING = "preparing"
    TRAINING = "training"
    EVALUATING = "evaluating"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


class ModelType(str, Enum):
    """ëª¨ë¸ íƒ€ì…"""

    SENTIMENT = "sentiment"
    FACTCHECK = "factcheck"
    BIAS = "bias"
    CUSTOM = "custom"


class DatasetSource(str, Enum):
    """ë°ì´í„°ì…‹ ì†ŒìŠ¤"""

    HUGGINGFACE = "huggingface"
    LOCAL = "local"
    URL = "url"
    DATABASE = "database"


class TrainingJobCreate(BaseModel):
    """í•™ìŠµ ì‘ì—… ìƒì„± ìš”ì²­"""

    name: str = Field(..., description="ì‘ì—… ì´ë¦„")
    description: Optional[str] = Field(None, description="ì„¤ëª…")
    model_type: ModelType = Field(..., description="ëª¨ë¸ íƒ€ì…")
    base_model: str = Field(
        ..., description="ë² ì´ìŠ¤ ëª¨ë¸ (ì˜ˆ: monologg/koelectra-base-v3-discriminator)"
    )

    # Dataset configuration
    dataset_source: DatasetSource = Field(
        DatasetSource.HUGGINGFACE, description="ë°ì´í„°ì…‹ ì†ŒìŠ¤"
    )
    dataset_name: Optional[str] = Field(
        None, description="ë°ì´í„°ì…‹ ì´ë¦„ (HuggingFace ë˜ëŠ” ë¡œì»¬ ê²½ë¡œ)"
    )
    dataset_split: str = Field("train", description="ë°ì´í„°ì…‹ ë¶„í• ")
    text_column: str = Field("text", description="í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…")
    label_column: str = Field("label", description="ë¼ë²¨ ì»¬ëŸ¼ëª…")

    # Training hyperparameters
    num_epochs: int = Field(3, ge=1, le=100, description="ì—í¬í¬ ìˆ˜")
    batch_size: int = Field(16, ge=1, le=256, description="ë°°ì¹˜ í¬ê¸°")
    learning_rate: float = Field(2e-5, ge=1e-7, le=1e-2, description="í•™ìŠµë¥ ")
    warmup_ratio: float = Field(0.1, ge=0, le=1, description="ì›œì—… ë¹„ìœ¨")
    weight_decay: float = Field(0.01, ge=0, le=1, description="ê°€ì¤‘ì¹˜ ê°ì‡ ")
    max_seq_length: int = Field(512, ge=32, le=2048, description="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´")

    # Output configuration
    output_dir: Optional[str] = Field(None, description="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    save_steps: int = Field(500, description="ì €ì¥ ìŠ¤í… ê°„ê²©")
    eval_steps: int = Field(500, description="í‰ê°€ ìŠ¤í… ê°„ê²©")

    # Additional options
    fp16: bool = Field(False, description="FP16 í•™ìŠµ ì‚¬ìš©")
    gradient_accumulation_steps: int = Field(
        1, ge=1, le=64, description="ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…"
    )


class TrainingJob(BaseModel):
    """í•™ìŠµ ì‘ì—…"""

    id: str
    name: str
    description: Optional[str] = None
    model_type: ModelType
    base_model: str
    status: TrainingJobStatus

    # Progress
    progress: float = Field(0.0, description="ì§„í–‰ë¥  (0-100)")
    current_epoch: int = 0
    total_epochs: int = 0
    current_step: int = 0
    total_steps: int = 0

    # Metrics
    train_loss: Optional[float] = None
    eval_loss: Optional[float] = None
    eval_accuracy: Optional[float] = None
    eval_f1: Optional[float] = None

    # Timing
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    elapsed_seconds: int = 0
    estimated_remaining_seconds: Optional[int] = None

    # Output
    output_path: Optional[str] = None
    model_size_mb: Optional[float] = None

    # Error handling
    error_message: Optional[str] = None

    created_at: datetime
    updated_at: datetime


class TrainingJobList(BaseModel):
    """í•™ìŠµ ì‘ì—… ëª©ë¡"""

    jobs: List[TrainingJob]
    total: int
    pending: int
    running: int
    completed: int
    failed: int


class ModelInfo(BaseModel):
    """í•™ìŠµëœ ëª¨ë¸ ì •ë³´"""

    id: str
    name: str
    model_type: ModelType
    base_model: str
    training_job_id: str

    # Performance metrics
    accuracy: Optional[float] = None
    f1_score: Optional[float] = None
    precision: Optional[float] = None
    recall: Optional[float] = None

    # Model details
    model_path: str
    size_mb: float
    num_labels: int
    label_mapping: Optional[Dict[str, int]] = None

    # Deployment
    is_deployed: bool = False
    deployed_at: Optional[datetime] = None
    addon_key: Optional[str] = None

    created_at: datetime


class ModelList(BaseModel):
    """í•™ìŠµëœ ëª¨ë¸ ëª©ë¡"""

    models: List[ModelInfo]
    total: int


class DeployRequest(BaseModel):
    """ëª¨ë¸ ë°°í¬ ìš”ì²­"""

    addon_key: str = Field(..., description="ë°°í¬í•  ì• ë“œì˜¨ í‚¤ (ì˜ˆ: sentiment-addon)")
    replace_current: bool = Field(True, description="í˜„ì¬ ëª¨ë¸ êµì²´ ì—¬ë¶€")


class TrainingMetrics(BaseModel):
    """í•™ìŠµ ë©”íŠ¸ë¦­"""

    job_id: str
    step: int
    epoch: float
    train_loss: Optional[float] = None
    eval_loss: Optional[float] = None
    eval_accuracy: Optional[float] = None
    eval_f1: Optional[float] = None
    learning_rate: Optional[float] = None
    timestamp: datetime


# ============================================================================
# Helper functions
# ============================================================================


async def call_trainer_service(
    method: str,
    path: str,
    params: Optional[dict] = None,
    json_data: Optional[dict] = None,
    timeout: float = 30.0,
) -> dict:
    """Call the ml-trainer service API"""
    url = f"{ML_TRAINER_URL}{path}"

    async with httpx.AsyncClient(timeout=timeout) as client:
        try:
            response = await client.request(
                method=method,
                url=url,
                params=params,
                json=json_data,
            )

            if response.status_code >= 400:
                detail = (
                    response.json().get("detail", response.text)
                    if response.text
                    else "Unknown error"
                )
                raise HTTPException(
                    status_code=response.status_code,
                    detail=detail,
                )

            if response.status_code == 204:
                return {}

            return response.json()
        except httpx.RequestError as e:
            raise HTTPException(
                status_code=503,
                detail=f"ML Trainer service unavailable: {str(e)}",
            )


# ============================================================================
# Endpoints - Health & Status
# ============================================================================


@router.get("/health")
async def trainer_health_check(
    current_user=Depends(get_current_user),
):
    """
    ML Trainer ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬.
    """
    try:
        result = await call_trainer_service("GET", "/health")
        return result
    except HTTPException as e:
        if e.status_code == 503:
            return {
                "status": "unhealthy",
                "message": "ML Trainer service is unavailable",
            }
        raise


@router.get("/status")
async def trainer_status(
    current_user=Depends(get_current_user),
):
    """
    ML Trainer ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ.

    í˜„ì¬ í•™ìŠµ ì‘ì—… ìƒíƒœ ë° ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    result = await call_trainer_service("GET", "/status")
    return result


# ============================================================================
# Endpoints - Training Jobs
# ============================================================================


@router.get("/jobs", response_model=TrainingJobList)
async def list_training_jobs(
    status: Optional[TrainingJobStatus] = Query(None, description="ìƒíƒœ í•„í„°"),
    model_type: Optional[ModelType] = Query(None, description="ëª¨ë¸ íƒ€ì… í•„í„°"),
    limit: int = Query(20, ge=1, le=100, description="ìµœëŒ€ ê°œìˆ˜"),
    offset: int = Query(0, ge=0, description="ì˜¤í”„ì…‹"),
    current_user=Depends(get_current_user),
):
    """
    í•™ìŠµ ì‘ì—… ëª©ë¡ ì¡°íšŒ.
    """
    params = {"limit": limit, "offset": offset}
    if status:
        params["status"] = status.value
    if model_type:
        params["model_type"] = model_type.value

    result = await call_trainer_service("GET", "/jobs", params=params)
    return result


@router.get("/jobs/{job_id}", response_model=TrainingJob)
async def get_training_job(
    job_id: str,
    current_user=Depends(get_current_user),
):
    """
    íŠ¹ì • í•™ìŠµ ì‘ì—… ì¡°íšŒ.
    """
    result = await call_trainer_service("GET", f"/jobs/{job_id}")
    return result


@router.post("/jobs", response_model=TrainingJob, status_code=status.HTTP_201_CREATED)
async def create_training_job(
    request: TrainingJobCreate,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    ìƒˆ í•™ìŠµ ì‘ì—… ìƒì„±. (Admin ê¶Œí•œ í•„ìš”)

    í•™ìŠµ ì‘ì—…ì„ ìƒì„±í•˜ê³  íì— ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    result = await call_trainer_service(
        "POST",
        "/jobs",
        json_data=request.model_dump(exclude_none=True),
    )

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.CREATE,
        resource_type="training_job",
        resource_id=result.get("id", "unknown"),
        resource_name=request.name,
        details={
            "model_type": request.model_type.value,
            "base_model": request.base_model,
            "epochs": request.num_epochs,
        },
    )

    return result


@router.post("/jobs/{job_id}/start")
async def start_training_job(
    job_id: str,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    í•™ìŠµ ì‘ì—… ì‹œì‘. (Admin ê¶Œí•œ í•„ìš”)
    """
    result = await call_trainer_service("POST", f"/jobs/{job_id}/start", timeout=60.0)

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="training_job",
        resource_id=job_id,
        resource_name=f"Training Job {job_id}",
        details={"action": "start"},
    )

    return result


@router.post("/jobs/{job_id}/cancel")
async def cancel_training_job(
    job_id: str,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    í•™ìŠµ ì‘ì—… ì·¨ì†Œ. (Admin ê¶Œí•œ í•„ìš”)
    """
    result = await call_trainer_service("POST", f"/jobs/{job_id}/cancel")

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="training_job",
        resource_id=job_id,
        resource_name=f"Training Job {job_id}",
        details={"action": "cancel"},
    )

    return result


@router.delete("/jobs/{job_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_training_job(
    job_id: str,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    í•™ìŠµ ì‘ì—… ì‚­ì œ. (Admin ê¶Œí•œ í•„ìš”)

    ì™„ë£Œë˜ê±°ë‚˜ ì‹¤íŒ¨í•œ ì‘ì—…ë§Œ ì‚­ì œ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    """
    await call_trainer_service("DELETE", f"/jobs/{job_id}")

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DELETE,
        resource_type="training_job",
        resource_id=job_id,
        resource_name=f"Training Job {job_id}",
    )


@router.get("/jobs/{job_id}/metrics", response_model=List[TrainingMetrics])
async def get_training_metrics(
    job_id: str,
    limit: int = Query(100, ge=1, le=1000, description="ìµœëŒ€ ê°œìˆ˜"),
    current_user=Depends(get_current_user),
):
    """
    í•™ìŠµ ì‘ì—…ì˜ ë©”íŠ¸ë¦­ ì¡°íšŒ.

    í•™ìŠµ ì§„í–‰ ì¤‘ ê¸°ë¡ëœ loss, accuracy ë“±ì˜ ë©”íŠ¸ë¦­ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    result = await call_trainer_service(
        "GET", f"/jobs/{job_id}/metrics", params={"limit": limit}
    )
    return result


@router.get("/jobs/{job_id}/logs")
async def get_training_logs(
    job_id: str,
    lines: int = Query(100, ge=1, le=1000, description="ë¡œê·¸ ë¼ì¸ ìˆ˜"),
    current_user=Depends(get_current_user),
):
    """
    í•™ìŠµ ì‘ì—…ì˜ ë¡œê·¸ ì¡°íšŒ.
    """
    result = await call_trainer_service(
        "GET", f"/jobs/{job_id}/logs", params={"lines": lines}
    )
    return result


# ============================================================================
# Endpoints - Models
# ============================================================================


@router.get("/models", response_model=ModelList)
async def list_models(
    model_type: Optional[ModelType] = Query(None, description="ëª¨ë¸ íƒ€ì… í•„í„°"),
    deployed_only: bool = Query(False, description="ë°°í¬ëœ ëª¨ë¸ë§Œ"),
    limit: int = Query(20, ge=1, le=100, description="ìµœëŒ€ ê°œìˆ˜"),
    offset: int = Query(0, ge=0, description="ì˜¤í”„ì…‹"),
    current_user=Depends(get_current_user),
):
    """
    í•™ìŠµëœ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ.
    """
    params = {"limit": limit, "offset": offset, "deployed_only": deployed_only}
    if model_type:
        params["model_type"] = model_type.value

    result = await call_trainer_service("GET", "/models", params=params)
    return result


@router.get("/models/{model_id}", response_model=ModelInfo)
async def get_model(
    model_id: str,
    current_user=Depends(get_current_user),
):
    """
    íŠ¹ì • ëª¨ë¸ ì¡°íšŒ.
    """
    result = await call_trainer_service("GET", f"/models/{model_id}")
    return result


@router.post("/models/{model_id}/deploy")
async def deploy_model(
    model_id: str,
    request: DeployRequest,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    ëª¨ë¸ì„ ML Addonì— ë°°í¬. (Admin ê¶Œí•œ í•„ìš”)

    í•™ìŠµëœ ëª¨ë¸ì„ ì§€ì •ëœ ML Addon ì„œë¹„ìŠ¤ì— ë°°í¬í•©ë‹ˆë‹¤.
    """
    result = await call_trainer_service(
        "POST",
        f"/models/{model_id}/deploy",
        json_data=request.model_dump(),
        timeout=120.0,
    )

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="ml_model",
        resource_id=model_id,
        resource_name=f"Model {model_id}",
        details={
            "action": "deploy",
            "addon_key": request.addon_key,
            "replace_current": request.replace_current,
        },
    )

    return result


@router.delete("/models/{model_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_model(
    model_id: str,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    í•™ìŠµëœ ëª¨ë¸ ì‚­ì œ. (Admin ê¶Œí•œ í•„ìš”)

    ë°°í¬ë˜ì§€ ì•Šì€ ëª¨ë¸ë§Œ ì‚­ì œ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    """
    await call_trainer_service("DELETE", f"/models/{model_id}")

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DELETE,
        resource_type="ml_model",
        resource_id=model_id,
        resource_name=f"Model {model_id}",
    )


# ============================================================================
# Endpoints - Datasets
# ============================================================================


@router.get("/datasets")
async def list_datasets(
    source: Optional[DatasetSource] = Query(None, description="ë°ì´í„°ì…‹ ì†ŒìŠ¤ í•„í„°"),
    current_user=Depends(get_current_user),
):
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡ ì¡°íšŒ.
    """
    params = {}
    if source:
        params["source"] = source.value

    result = await call_trainer_service("GET", "/datasets", params=params)
    return result


@router.post("/datasets/search")
async def search_huggingface_datasets(
    query: str = Query(..., description="ê²€ìƒ‰ ì¿¼ë¦¬"),
    task: Optional[str] = Query(
        None, description="íƒœìŠ¤í¬ íƒ€ì… (text-classification ë“±)"
    ),
    language: Optional[str] = Query("ko", description="ì–¸ì–´"),
    limit: int = Query(10, ge=1, le=50),
    current_user=Depends(get_current_user),
):
    """
    HuggingFace ë°ì´í„°ì…‹ ê²€ìƒ‰.
    """
    params = {"query": query, "limit": limit}
    if task:
        params["task"] = task
    if language:
        params["language"] = language

    result = await call_trainer_service("POST", "/datasets/search", params=params)
    return result


@router.get("/datasets/{dataset_name}/preview")
async def preview_dataset(
    dataset_name: str,
    split: str = Query("train", description="ë°ì´í„°ì…‹ ë¶„í• "),
    num_samples: int = Query(5, ge=1, le=20, description="ìƒ˜í”Œ ìˆ˜"),
    current_user=Depends(get_current_user),
):
    """
    ë°ì´í„°ì…‹ ë¯¸ë¦¬ë³´ê¸°.

    ë°ì´í„°ì…‹ì˜ ìƒ˜í”Œ ë°ì´í„°ì™€ ìŠ¤í‚¤ë§ˆë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    result = await call_trainer_service(
        "GET",
        f"/datasets/{dataset_name}/preview",
        params={"split": split, "num_samples": num_samples},
    )
    return result


# ============================================================================
# Endpoints - Presets
# ============================================================================


@router.get("/presets")
async def list_training_presets(
    current_user=Depends(get_current_user),
):
    """
    í•™ìŠµ í”„ë¦¬ì…‹ ëª©ë¡.

    ì¼ë°˜ì ì¸ í•™ìŠµ ì„¤ì • í”„ë¦¬ì…‹ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    presets = [
        {
            "id": "korean-sentiment-koelectra",
            "name": "í•œêµ­ì–´ ê°ì„±ë¶„ì„ (KoELECTRA)",
            "model_type": "sentiment",
            "base_model": "monologg/koelectra-base-v3-discriminator",
            "recommended_datasets": [
                "nsmc",
                "klue/ner",
            ],
            "default_config": {
                "num_epochs": 3,
                "batch_size": 32,
                "learning_rate": 2e-5,
                "max_seq_length": 128,
            },
        },
        {
            "id": "korean-bias-kcbert",
            "name": "í•œêµ­ì–´ í¸í–¥ë¶„ì„ (KcBERT)",
            "model_type": "bias",
            "base_model": "beomi/KcBERT-base",
            "recommended_datasets": [],
            "default_config": {
                "num_epochs": 5,
                "batch_size": 16,
                "learning_rate": 3e-5,
                "max_seq_length": 256,
            },
        },
        {
            "id": "multilingual-factcheck",
            "name": "ë‹¤êµ­ì–´ íŒ©íŠ¸ì²´í¬ (mBERT)",
            "model_type": "factcheck",
            "base_model": "bert-base-multilingual-cased",
            "recommended_datasets": [],
            "default_config": {
                "num_epochs": 5,
                "batch_size": 16,
                "learning_rate": 2e-5,
                "max_seq_length": 512,
            },
        },
    ]
    return {"presets": presets, "total": len(presets)}


@router.post("/presets/{preset_id}/apply", response_model=TrainingJobCreate)
async def apply_preset(
    preset_id: str,
    dataset_name: Optional[str] = Query(
        None, description="ë°ì´í„°ì…‹ ì´ë¦„ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)"
    ),
    current_user=Depends(get_current_user),
):
    """
    í”„ë¦¬ì…‹ ì ìš©.

    ì„ íƒí•œ í”„ë¦¬ì…‹ì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµ ì„¤ì •ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    result = await call_trainer_service(
        "POST",
        f"/presets/{preset_id}/apply",
        params={"dataset_name": dataset_name} if dataset_name else None,
    )
    return result

```

---

## backend/admin-dashboard/api/routers/public_auth.py

```py
"""
Public Auth Router - ì¼ë°˜ ì‚¬ìš©ììš© ì¸ì¦ API (ê³µê°œ ì—”ë“œí¬ì¸íŠ¸)

- íšŒì›ê°€ì…: ì¸ì¦ ë¶ˆí•„ìš”
- ë¡œê·¸ì¸: ì¸ì¦ ë¶ˆí•„ìš”
- ë‚´ ì •ë³´: ì¸ì¦ í•„ìš”
- ë¡œê·¸ì•„ì›ƒ: ì¸ì¦ í•„ìš”
- ë¹„ë°€ë²ˆí˜¸ ë³€ê²½: ì¸ì¦ í•„ìš”
"""

from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.security import OAuth2PasswordRequestForm
from pydantic import BaseModel, EmailStr, Field

from ..models.schemas import AuditAction, Token, User, UserRegister, UserRole
from ..dependencies import (
    get_audit_service,
    get_auth_service,
    get_current_user,
)

router = APIRouter(prefix="/auth", tags=["Public Authentication"])


class RegisterRequest(BaseModel):
    """íšŒì›ê°€ì… ìš”ì²­"""

    username: str = Field(..., min_length=3, max_length=50, description="ì‚¬ìš©ìëª…")
    email: EmailStr = Field(..., description="ì´ë©”ì¼ ì£¼ì†Œ")
    password: str = Field(..., min_length=8, description="ë¹„ë°€ë²ˆí˜¸ (8ì ì´ìƒ)")


class ChangePasswordRequest(BaseModel):
    """ë¹„ë°€ë²ˆí˜¸ ë³€ê²½ ìš”ì²­"""

    old_password: str
    new_password: str = Field(..., min_length=8, description="ìƒˆ ë¹„ë°€ë²ˆí˜¸ (8ì ì´ìƒ)")


class UserPublicResponse(BaseModel):
    """ì¼ë°˜ ì‚¬ìš©ì ì‘ë‹µ (ë¯¼ê° ì •ë³´ ì œì™¸)"""

    id: str
    username: str
    email: str | None
    role: UserRole
    created_at: str


# ============================================================================
# Public Endpoints (No Auth Required)
# ============================================================================
@router.post("/register", response_model=Token, status_code=status.HTTP_201_CREATED)
async def register(
    request: RegisterRequest,
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """
    íšŒì›ê°€ì… (ê³µê°œ API)

    - ì‚¬ìš©ìëª…ì€ 3-50ì
    - ë¹„ë°€ë²ˆí˜¸ëŠ” 8ì ì´ìƒ
    - ì´ë©”ì¼ì€ ìœ íš¨í•œ í˜•ì‹ì´ì–´ì•¼ í•¨
    - ê°€ì… ì¦‰ì‹œ ë¡œê·¸ì¸ í† í° ë°˜í™˜
    """
    try:
        # ì¼ë°˜ ì‚¬ìš©ìë¡œ ê°€ì… (role: USER)
        user = auth_service.register_user(
            username=request.username,
            email=request.email,
            password=request.password,
        )

        # ê°€ì… ì„±ê³µ ì‹œ ë°”ë¡œ í† í° ë°œê¸‰
        token = auth_service.create_access_token(user)

        # ê°ì‚¬ ë¡œê·¸
        audit_service.log(
            user_id=user.id,
            username=user.username,
            action=AuditAction.CREATE,
            resource_type="user_registration",
            resource_id=user.id,
            success=True,
            details={"email": request.email},
        )

        return token

    except ValueError as e:
        # ì¤‘ë³µ ì‚¬ìš©ìëª…/ì´ë©”ì¼ ë“±
        audit_service.log(
            user_id="unknown",
            username=request.username,
            action=AuditAction.CREATE,
            resource_type="user_registration",
            success=False,
            error_message=str(e),
        )
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e),
        )


@router.post("/login", response_model=Token)
async def login(
    form_data: OAuth2PasswordRequestForm = Depends(),
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """
    ë¡œê·¸ì¸ (ê³µê°œ API)

    ì¼ë°˜ ì‚¬ìš©ìì™€ ê´€ë¦¬ì ëª¨ë‘ ì‚¬ìš© ê°€ëŠ¥
    """
    user = auth_service.authenticate(form_data.username, form_data.password)

    if not user:
        audit_service.log(
            user_id="unknown",
            username=form_data.username,
            action=AuditAction.LOGIN,
            resource_type="auth",
            success=False,
            error_message="Invalid credentials",
        )
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="ì•„ì´ë”” ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤",
            headers={"WWW-Authenticate": "Bearer"},
        )

    token = auth_service.create_access_token(user)

    audit_service.log(
        user_id=user.id,
        username=user.username,
        action=AuditAction.LOGIN,
        resource_type="auth",
        success=True,
    )

    return token


@router.post("/token", response_model=Token)
async def login_for_access_token(
    form_data: OAuth2PasswordRequestForm = Depends(),
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """OAuth2 í˜¸í™˜ í† í° ì—”ë“œí¬ì¸íŠ¸"""
    return await login(form_data, auth_service, audit_service)


# ============================================================================
# Protected Endpoints (Auth Required)
# ============================================================================
@router.get("/me", response_model=User)
async def get_current_user_info(
    current_user=Depends(get_current_user),
):
    """í˜„ì¬ ë¡œê·¸ì¸í•œ ì‚¬ìš©ì ì •ë³´"""
    return current_user


@router.post("/logout")
async def logout(
    current_user=Depends(get_current_user),
    audit_service=Depends(get_audit_service),
):
    """ë¡œê·¸ì•„ì›ƒ"""
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.LOGOUT,
        resource_type="auth",
    )

    return {"success": True, "message": "ë¡œê·¸ì•„ì›ƒë˜ì—ˆìŠµë‹ˆë‹¤"}


@router.post("/change-password")
async def change_password(
    request: ChangePasswordRequest,
    current_user=Depends(get_current_user),
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """ë¹„ë°€ë²ˆí˜¸ ë³€ê²½"""
    success = auth_service.change_password(
        user_id=current_user.id,
        old_password=request.old_password,
        new_password=request.new_password,
    )

    if not success:
        audit_service.log(
            user_id=current_user.id,
            username=current_user.username,
            action=AuditAction.UPDATE,
            resource_type="password",
            success=False,
            error_message="Invalid old password",
        )
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="í˜„ì¬ ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤",
        )

    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="password",
        success=True,
    )

    return {"success": True, "message": "ë¹„ë°€ë²ˆí˜¸ê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤"}


@router.delete("/me")
async def delete_my_account(
    current_user=Depends(get_current_user),
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """
    ë‚´ ê³„ì • ì‚­ì œ (íšŒì›íƒˆí‡´)

    ì£¼ì˜: ì´ ì‘ì—…ì€ ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
    """
    # ê´€ë¦¬ì ê³„ì •ì€ ìê¸° ì‚­ì œ ë¶ˆê°€
    if current_user.role == UserRole.ADMIN:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="ê´€ë¦¬ì ê³„ì •ì€ ì§ì ‘ ì‚­ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
        )

    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DELETE,
        resource_type="user_account",
        resource_id=current_user.id,
        success=True,
    )

    auth_service.delete_user(current_user.id)

    return {"success": True, "message": "ê³„ì •ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤"}


# ============================================================================
# Username/Email Availability Check (Public)
# ============================================================================
@router.get("/check-username/{username}")
async def check_username_availability(
    username: str,
    auth_service=Depends(get_auth_service),
):
    """ì‚¬ìš©ìëª… ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    exists = auth_service.get_user_by_username(username) is not None
    return {
        "username": username,
        "available": not exists,
    }


@router.get("/check-email/{email}")
async def check_email_availability(
    email: str,
    auth_service=Depends(get_auth_service),
):
    """ì´ë©”ì¼ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    exists = auth_service.get_user_by_email(email) is not None
    return {
        "email": email,
        "available": not exists,
    }


# ============================================================================
# Email Verification Endpoints (for Registration)
# ============================================================================
class SendVerificationRequest(BaseModel):
    """ì´ë©”ì¼ ì¸ì¦ ìš”ì²­"""
    username: str = Field(..., min_length=3, max_length=50, description="ì‚¬ìš©ìëª…")
    email: EmailStr = Field(..., description="ì´ë©”ì¼ ì£¼ì†Œ")
    password: str = Field(..., min_length=8, description="ë¹„ë°€ë²ˆí˜¸ (8ì ì´ìƒ)")


class VerifyEmailRequest(BaseModel):
    """ì´ë©”ì¼ ì¸ì¦ ì½”ë“œ ê²€ì¦ ìš”ì²­"""
    email: EmailStr = Field(..., description="ì´ë©”ì¼ ì£¼ì†Œ")
    code: str = Field(..., min_length=6, max_length=6, description="6ìë¦¬ ì¸ì¦ ì½”ë“œ")


class ResendVerificationRequest(BaseModel):
    """ì¸ì¦ ì½”ë“œ ì¬ë°œì†¡ ìš”ì²­"""
    email: EmailStr = Field(..., description="ì´ë©”ì¼ ì£¼ì†Œ")


@router.post("/send-verification")
async def send_verification_code(
    request: SendVerificationRequest,
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """
    ì´ë©”ì¼ ì¸ì¦ ì½”ë“œ ë°œì†¡ (íšŒì›ê°€ì… 1ë‹¨ê³„)
    
    - ì‚¬ìš©ìëª…, ì´ë©”ì¼, ë¹„ë°€ë²ˆí˜¸ë¥¼ ì €ì¥í•˜ê³  ì¸ì¦ ì½”ë“œ ìƒì„±
    - ì‹¤ì œ ì´ë©”ì¼ ë°œì†¡ì€ ë³„ë„ ì„œë¹„ìŠ¤ ì—°ë™ í•„ìš”
    - 10ë¶„ê°„ ìœ íš¨í•œ 6ìë¦¬ ì¸ì¦ ì½”ë“œ ë°˜í™˜
    """
    try:
        code = auth_service.create_email_verification(
            email=request.email,
            username=request.username,
            password=request.password,
        )
        
        # TODO: ì‹¤ì œ ì´ë©”ì¼ ë°œì†¡ ë¡œì§ (SMTP, SendGrid, AWS SES ë“±)
        # await send_email(
        #     to=request.email,
        #     subject="[NewsInsight] ì´ë©”ì¼ ì¸ì¦ ì½”ë“œ",
        #     body=f"ì¸ì¦ ì½”ë“œ: {code}\n\n10ë¶„ ì´ë‚´ì— ì…ë ¥í•´ì£¼ì„¸ìš”."
        # )
        
        audit_service.log(
            user_id="unknown",
            username=request.username,
            action=AuditAction.CREATE,
            resource_type="email_verification",
            success=True,
            details={"email": request.email},
        )
        
        # NOTE: ì¸ì¦ ì½”ë“œëŠ” ë³´ì•ˆìƒ ì‘ë‹µì— í¬í•¨í•˜ì§€ ì•ŠìŒ
        # ê°œë°œ/í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œëŠ” ë¡œê·¸ë¡œ í™•ì¸í•˜ê±°ë‚˜ DEBUG_EMAIL_CODE í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
        import os
        response = {
            "success": True,
            "message": "ì¸ì¦ ì½”ë“œê°€ ì´ë©”ì¼ë¡œ ë°œì†¡ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "email": request.email,
            "expires_in": 600,  # 10ë¶„
        }
        
        # ê°œë°œ í™˜ê²½ì—ì„œë§Œ ì½”ë“œ ë°˜í™˜ (DEBUG_EMAIL_CODE=true ì„¤ì • í•„ìš”)
        if os.getenv("DEBUG_EMAIL_CODE", "false").lower() == "true":
            response["code"] = code
            
        return response
        
    except ValueError as e:
        audit_service.log(
            user_id="unknown",
            username=request.username,
            action=AuditAction.CREATE,
            resource_type="email_verification",
            success=False,
            error_message=str(e),
        )
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e),
        )


@router.post("/verify-email", response_model=Token)
async def verify_email_code(
    request: VerifyEmailRequest,
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """
    ì´ë©”ì¼ ì¸ì¦ ì½”ë“œ ê²€ì¦ ë° íšŒì›ê°€ì… ì™„ë£Œ (íšŒì›ê°€ì… 2ë‹¨ê³„)
    
    - ì˜¬ë°”ë¥¸ ì¸ì¦ ì½”ë“œ ì…ë ¥ ì‹œ íšŒì›ê°€ì… ì™„ë£Œ
    - ê°€ì… ì„±ê³µ ì‹œ ë¡œê·¸ì¸ í† í° ë°˜í™˜
    - ìµœëŒ€ 5íšŒ ì‹œë„ ê°€ëŠ¥
    """
    try:
        user = auth_service.verify_email_code(
            email=request.email,
            code=request.code,
        )
        
        # í† í° ë°œê¸‰
        token = auth_service.create_access_token(user)
        
        audit_service.log(
            user_id=user.id,
            username=user.username,
            action=AuditAction.CREATE,
            resource_type="user_registration",
            resource_id=user.id,
            success=True,
            details={"email": request.email, "verified": True},
        )
        
        return token
        
    except ValueError as e:
        audit_service.log(
            user_id="unknown",
            username="unknown",
            action=AuditAction.UPDATE,
            resource_type="email_verification",
            success=False,
            error_message=str(e),
            details={"email": request.email},
        )
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e),
        )


@router.post("/resend-verification")
async def resend_verification_code(
    request: ResendVerificationRequest,
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """
    ì¸ì¦ ì½”ë“œ ì¬ë°œì†¡
    
    - ê¸°ì¡´ ì¸ì¦ ìš”ì²­ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ê°€ëŠ¥
    - ìƒˆë¡œìš´ 6ìë¦¬ ì½”ë“œ ìƒì„± ë° ìœ íš¨ ì‹œê°„ ì´ˆê¸°í™”
    """
    try:
        code = auth_service.resend_verification_code(request.email)
        
        # TODO: ì‹¤ì œ ì´ë©”ì¼ ë°œì†¡ ë¡œì§
        
        audit_service.log(
            user_id="unknown",
            username="unknown",
            action=AuditAction.UPDATE,
            resource_type="email_verification",
            success=True,
            details={"email": request.email, "action": "resend"},
        )
        
        # NOTE: ì¸ì¦ ì½”ë“œëŠ” ë³´ì•ˆìƒ ì‘ë‹µì— í¬í•¨í•˜ì§€ ì•ŠìŒ
        import os
        response = {
            "success": True,
            "message": "ì¸ì¦ ì½”ë“œê°€ ì¬ë°œì†¡ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "email": request.email,
            "expires_in": 600,
        }
        
        # ê°œë°œ í™˜ê²½ì—ì„œë§Œ ì½”ë“œ ë°˜í™˜ (DEBUG_EMAIL_CODE=true ì„¤ì • í•„ìš”)
        if os.getenv("DEBUG_EMAIL_CODE", "false").lower() == "true":
            response["code"] = code
            
        return response
        
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e),
        )

```

---

## backend/admin-dashboard/api/routers/scripts.py

```py
"""
Script Router - ìŠ¤í¬ë¦½íŠ¸/ì‘ì—… ê´€ë¦¬ API ì—”ë“œí¬ì¸íŠ¸
"""
from typing import Any, Optional

from fastapi import APIRouter, Depends, HTTPException, Query, status
from fastapi.responses import StreamingResponse

from ..models.schemas import (
    AuditAction,
    Script,
    ScriptCreate,
    ScriptUpdate,
    TaskExecution,
    TaskExecutionRequest,
    TaskStatus,
    UserRole,
)
from ..dependencies import (
    get_audit_service,
    get_current_user,
    get_environment_service,
    get_script_service,
    require_role,
)

router = APIRouter(prefix="/scripts", tags=["Scripts"])


@router.get("", response_model=list[Script])
async def list_scripts(
    environment: Optional[str] = Query(None, description="í™˜ê²½ ì´ë¦„ìœ¼ë¡œ í•„í„°"),
    tag: Optional[str] = Query(None, description="íƒœê·¸ë¡œ í•„í„°"),
    script_service=Depends(get_script_service),
    current_user=Depends(get_current_user),
):
    """ìŠ¤í¬ë¦½íŠ¸ ëª©ë¡ ì¡°íšŒ (ì‚¬ìš©ì ê¶Œí•œì— ë”°ë¼ í•„í„°ë§)"""
    return script_service.list_scripts(
        environment=environment,
        tag=tag,
        role=current_user.role,
    )


@router.get("/{script_id}", response_model=Script)
async def get_script(
    script_id: str,
    script_service=Depends(get_script_service),
    current_user=Depends(get_current_user),
):
    """ìŠ¤í¬ë¦½íŠ¸ ìƒì„¸ ì¡°íšŒ"""
    script = script_service.get_script(script_id)
    if not script:
        raise HTTPException(status_code=404, detail="Script not found")
    return script


@router.post("", response_model=Script, status_code=status.HTTP_201_CREATED)
async def create_script(
    data: ScriptCreate,
    script_service=Depends(get_script_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ìŠ¤í¬ë¦½íŠ¸ ìƒì„± (Admin ê¶Œí•œ í•„ìš”)"""
    script = script_service.create_script(data)

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.CREATE,
        resource_type="script",
        resource_id=script.id,
        resource_name=script.name,
        details={"data": data.model_dump()},
    )

    return script


@router.patch("/{script_id}", response_model=Script)
async def update_script(
    script_id: str,
    data: ScriptUpdate,
    script_service=Depends(get_script_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì • (Admin ê¶Œí•œ í•„ìš”)"""
    script = script_service.update_script(script_id, data)
    if not script:
        raise HTTPException(status_code=404, detail="Script not found")

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="script",
        resource_id=script.id,
        resource_name=script.name,
        details={"changes": data.model_dump(exclude_unset=True)},
    )

    return script


@router.delete("/{script_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_script(
    script_id: str,
    script_service=Depends(get_script_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ìŠ¤í¬ë¦½íŠ¸ ì‚­ì œ (Admin ê¶Œí•œ í•„ìš”)"""
    script = script_service.get_script(script_id)
    if not script:
        raise HTTPException(status_code=404, detail="Script not found")

    if not script_service.delete_script(script_id):
        raise HTTPException(status_code=500, detail="Failed to delete script")

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DELETE,
        resource_type="script",
        resource_id=script_id,
        resource_name=script.name,
    )


@router.post("/execute", response_model=TaskExecution)
async def execute_script(
    request: TaskExecutionRequest,
    script_service=Depends(get_script_service),
    env_service=Depends(get_environment_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(get_current_user),
):
    """ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰"""
    # ìŠ¤í¬ë¦½íŠ¸ ì¡°íšŒ
    script = script_service.get_script(request.script_id)
    if not script:
        raise HTTPException(status_code=404, detail="Script not found")

    # í™˜ê²½ ì¡°íšŒ
    env = env_service.get_environment(request.environment_id)
    if not env:
        raise HTTPException(status_code=404, detail="Environment not found")

    # ê¶Œí•œ í™•ì¸
    from ..services.auth_service import AuthService

    if not AuthService.check_permission(
        AuthService, current_user.role, script.required_role
    ):
        raise HTTPException(
            status_code=403,
            detail=f"Requires {script.required_role.value} permission",
        )

    # í™˜ê²½ í—ˆìš© í™•ì¸
    if script.allowed_environments and env.name not in script.allowed_environments:
        raise HTTPException(
            status_code=400,
            detail=f"Script not allowed for environment: {env.name}",
        )

    try:
        execution = await script_service.execute_script(
            script_id=request.script_id,
            environment_name=env.name,
            compose_file=env.compose_file,
            parameters=request.parameters,
            executed_by=current_user.username,
        )

        # ê°ì‚¬ ë¡œê·¸
        audit_service.log(
            user_id=current_user.id,
            username=current_user.username,
            action=AuditAction.EXECUTE,
            resource_type="script",
            resource_id=script.id,
            resource_name=script.name,
            environment_id=env.id,
            environment_name=env.name,
            details={
                "execution_id": execution.id,
                "parameters": request.parameters,
            },
        )

        return execution

    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.post("/execute/stream")
async def execute_script_stream(
    request: TaskExecutionRequest,
    script_service=Depends(get_script_service),
    env_service=Depends(get_environment_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(get_current_user),
):
    """ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ (ì‹¤ì‹œê°„ ì¶œë ¥ ìŠ¤íŠ¸ë¦¬ë°)"""
    # ìŠ¤í¬ë¦½íŠ¸ ì¡°íšŒ
    script = script_service.get_script(request.script_id)
    if not script:
        raise HTTPException(status_code=404, detail="Script not found")

    # í™˜ê²½ ì¡°íšŒ
    env = env_service.get_environment(request.environment_id)
    if not env:
        raise HTTPException(status_code=404, detail="Environment not found")

    # ê¶Œí•œ í™•ì¸
    role_priority = {UserRole.VIEWER: 0, UserRole.OPERATOR: 1, UserRole.ADMIN: 2}
    if role_priority.get(current_user.role, 0) < role_priority.get(
        script.required_role, 0
    ):
        raise HTTPException(
            status_code=403,
            detail=f"Requires {script.required_role.value} permission",
        )

    # í™˜ê²½ í—ˆìš© í™•ì¸
    if script.allowed_environments and env.name not in script.allowed_environments:
        raise HTTPException(
            status_code=400,
            detail=f"Script not allowed for environment: {env.name}",
        )

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.EXECUTE,
        resource_type="script",
        resource_id=script.id,
        resource_name=script.name,
        environment_id=env.id,
        environment_name=env.name,
        details={"parameters": request.parameters, "streaming": True},
    )

    async def generate():
        async for line in script_service.stream_execution_output(
            script_id=request.script_id,
            environment_name=env.name,
            compose_file=env.compose_file,
            parameters=request.parameters,
            executed_by=current_user.username,
        ):
            yield line

    return StreamingResponse(
        generate(),
        media_type="text/plain",
        headers={"X-Content-Type-Options": "nosniff"},
    )


@router.get("/executions", response_model=list[TaskExecution])
async def list_executions(
    script_id: Optional[str] = Query(None, description="ìŠ¤í¬ë¦½íŠ¸ IDë¡œ í•„í„°"),
    environment_id: Optional[str] = Query(None, description="í™˜ê²½ IDë¡œ í•„í„°"),
    status: Optional[TaskStatus] = Query(None, description="ìƒíƒœë¡œ í•„í„°"),
    limit: int = Query(50, ge=1, le=200, description="ì¡°íšŒ ê°œìˆ˜"),
    script_service=Depends(get_script_service),
    current_user=Depends(get_current_user),
):
    """ì‹¤í–‰ ì´ë ¥ ì¡°íšŒ"""
    return script_service.list_executions(
        script_id=script_id,
        environment_id=environment_id,
        status=status,
        limit=limit,
    )


@router.get("/executions/{execution_id}", response_model=TaskExecution)
async def get_execution(
    execution_id: str,
    script_service=Depends(get_script_service),
    current_user=Depends(get_current_user),
):
    """ì‹¤í–‰ ìƒì„¸ ì¡°íšŒ"""
    execution = script_service.get_execution(execution_id)
    if not execution:
        raise HTTPException(status_code=404, detail="Execution not found")
    return execution


@router.post("/executions/{execution_id}/cancel")
async def cancel_execution(
    execution_id: str,
    script_service=Depends(get_script_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """ì‹¤í–‰ ì¤‘ì¸ ì‘ì—… ì·¨ì†Œ"""
    execution = script_service.get_execution(execution_id)
    if not execution:
        raise HTTPException(status_code=404, detail="Execution not found")

    if execution.status != TaskStatus.RUNNING:
        raise HTTPException(status_code=400, detail="Execution is not running")

    if not script_service.cancel_execution(execution_id):
        raise HTTPException(status_code=500, detail="Failed to cancel execution")

    # ê°ì‚¬ ë¡œê·¸
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.EXECUTE,
        resource_type="execution",
        resource_id=execution_id,
        details={"action": "cancel"},
    )

    return {"success": True, "message": "Execution cancelled"}

```

---

## backend/admin-dashboard/api/services/__init__.py

```py
# Admin Dashboard Services

```

---

## backend/admin-dashboard/api/services/audit_service.py

```py
"""
Audit Service - ê°ì‚¬ ë¡œê·¸ ê´€ë¦¬ ì„œë¹„ìŠ¤
"""
import json
from datetime import datetime
from pathlib import Path
from typing import Any, Optional
from uuid import uuid4

from ..models.schemas import AuditAction, AuditLog, AuditLogFilter


class AuditService:
    """ê°ì‚¬ ë¡œê·¸ ì„œë¹„ìŠ¤"""

    def __init__(self, config_dir: str, max_logs: int = 10000):
        self.config_dir = Path(config_dir)
        self.logs_file = self.config_dir / "audit_logs.jsonl"
        self.max_logs = max_logs
        self._ensure_log_file()

    def _ensure_log_file(self) -> None:
        """ë¡œê·¸ íŒŒì¼ ì¡´ì¬ í™•ì¸"""
        self.config_dir.mkdir(parents=True, exist_ok=True)
        if not self.logs_file.exists():
            self.logs_file.touch()

    def log(
        self,
        user_id: str,
        username: str,
        action: AuditAction,
        resource_type: str,
        resource_id: Optional[str] = None,
        resource_name: Optional[str] = None,
        environment_id: Optional[str] = None,
        environment_name: Optional[str] = None,
        details: Optional[dict[str, Any]] = None,
        ip_address: Optional[str] = None,
        user_agent: Optional[str] = None,
        success: bool = True,
        error_message: Optional[str] = None,
    ) -> AuditLog:
        """ê°ì‚¬ ë¡œê·¸ ê¸°ë¡"""
        log_entry = AuditLog(
            id=f"audit-{uuid4().hex[:12]}",
            user_id=user_id,
            username=username,
            action=action,
            resource_type=resource_type,
            resource_id=resource_id,
            resource_name=resource_name,
            environment_id=environment_id,
            environment_name=environment_name,
            details=details or {},
            ip_address=ip_address,
            user_agent=user_agent,
            timestamp=datetime.utcnow(),
            success=success,
            error_message=error_message,
        )

        # JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥
        with open(self.logs_file, "a") as f:
            f.write(log_entry.model_dump_json() + "\n")

        # ë¡œê·¸ íŒŒì¼ í¬ê¸° ê´€ë¦¬
        self._rotate_if_needed()

        return log_entry

    def _rotate_if_needed(self) -> None:
        """ë¡œê·¸ íŒŒì¼ í¬ê¸° ê´€ë¦¬"""
        try:
            with open(self.logs_file, "r") as f:
                lines = f.readlines()

            if len(lines) > self.max_logs:
                # ì˜¤ë˜ëœ ë¡œê·¸ ì‚­ì œ (ìµœì‹  max_logsê°œë§Œ ìœ ì§€)
                with open(self.logs_file, "w") as f:
                    f.writelines(lines[-self.max_logs :])
        except Exception:
            pass

    def get_logs(
        self,
        filter_params: Optional[AuditLogFilter] = None,
        page: int = 1,
        page_size: int = 50,
    ) -> tuple[list[AuditLog], int]:
        """ê°ì‚¬ ë¡œê·¸ ì¡°íšŒ"""
        logs = []

        try:
            with open(self.logs_file, "r") as f:
                for line in f:
                    if line.strip():
                        try:
                            data = json.loads(line)
                            log = AuditLog(**data)
                            logs.append(log)
                        except (json.JSONDecodeError, Exception):
                            continue
        except FileNotFoundError:
            return [], 0

        # í•„í„° ì ìš©
        if filter_params:
            logs = self._apply_filter(logs, filter_params)

        # ìµœì‹ ìˆœ ì •ë ¬
        logs.sort(key=lambda x: x.timestamp, reverse=True)

        total = len(logs)

        # í˜ì´ì§€ë„¤ì´ì…˜
        start = (page - 1) * page_size
        end = start + page_size
        paginated_logs = logs[start:end]

        return paginated_logs, total

    def _apply_filter(
        self, logs: list[AuditLog], filter_params: AuditLogFilter
    ) -> list[AuditLog]:
        """í•„í„° ì ìš©"""
        filtered = logs

        if filter_params.user_id:
            filtered = [l for l in filtered if l.user_id == filter_params.user_id]

        if filter_params.action:
            filtered = [l for l in filtered if l.action == filter_params.action]

        if filter_params.resource_type:
            filtered = [
                l for l in filtered if l.resource_type == filter_params.resource_type
            ]

        if filter_params.environment_id:
            filtered = [
                l for l in filtered if l.environment_id == filter_params.environment_id
            ]

        if filter_params.start_date:
            filtered = [
                l for l in filtered if l.timestamp >= filter_params.start_date
            ]

        if filter_params.end_date:
            filtered = [l for l in filtered if l.timestamp <= filter_params.end_date]

        if filter_params.success is not None:
            filtered = [l for l in filtered if l.success == filter_params.success]

        return filtered

    def get_log_by_id(self, log_id: str) -> Optional[AuditLog]:
        """íŠ¹ì • ë¡œê·¸ ì¡°íšŒ"""
        try:
            with open(self.logs_file, "r") as f:
                for line in f:
                    if line.strip():
                        try:
                            data = json.loads(line)
                            if data.get("id") == log_id:
                                return AuditLog(**data)
                        except (json.JSONDecodeError, Exception):
                            continue
        except FileNotFoundError:
            pass

        return None

    def get_user_activity(
        self, user_id: str, limit: int = 100
    ) -> list[AuditLog]:
        """ì‚¬ìš©ì í™œë™ ì´ë ¥ ì¡°íšŒ"""
        logs, _ = self.get_logs(
            filter_params=AuditLogFilter(user_id=user_id),
            page=1,
            page_size=limit,
        )
        return logs

    def get_resource_history(
        self, resource_type: str, resource_id: str, limit: int = 100
    ) -> list[AuditLog]:
        """ë¦¬ì†ŒìŠ¤ ë³€ê²½ ì´ë ¥ ì¡°íšŒ"""
        logs = []

        try:
            with open(self.logs_file, "r") as f:
                for line in f:
                    if line.strip():
                        try:
                            data = json.loads(line)
                            if (
                                data.get("resource_type") == resource_type
                                and data.get("resource_id") == resource_id
                            ):
                                logs.append(AuditLog(**data))
                        except (json.JSONDecodeError, Exception):
                            continue
        except FileNotFoundError:
            pass

        logs.sort(key=lambda x: x.timestamp, reverse=True)
        return logs[:limit]

    def get_statistics(
        self, start_date: Optional[datetime] = None, end_date: Optional[datetime] = None
    ) -> dict[str, Any]:
        """ê°ì‚¬ ë¡œê·¸ í†µê³„"""
        logs, total = self.get_logs(
            filter_params=AuditLogFilter(start_date=start_date, end_date=end_date),
            page=1,
            page_size=self.max_logs,
        )

        # ì•¡ì…˜ë³„ í†µê³„
        action_counts = {}
        for log in logs:
            action = log.action.value
            action_counts[action] = action_counts.get(action, 0) + 1

        # ì‚¬ìš©ìë³„ í†µê³„
        user_counts = {}
        for log in logs:
            user = log.username
            user_counts[user] = user_counts.get(user, 0) + 1

        # ë¦¬ì†ŒìŠ¤ íƒ€ì…ë³„ í†µê³„
        resource_counts = {}
        for log in logs:
            resource = log.resource_type
            resource_counts[resource] = resource_counts.get(resource, 0) + 1

        # ì„±ê³µ/ì‹¤íŒ¨ í†µê³„
        success_count = sum(1 for l in logs if l.success)
        failure_count = total - success_count

        return {
            "total_logs": total,
            "action_counts": action_counts,
            "user_counts": user_counts,
            "resource_counts": resource_counts,
            "success_count": success_count,
            "failure_count": failure_count,
        }

    def clear_old_logs(self, days: int = 90) -> int:
        """ì˜¤ë˜ëœ ë¡œê·¸ ì‚­ì œ"""
        cutoff = datetime.utcnow().replace(
            hour=0, minute=0, second=0, microsecond=0
        )
        from datetime import timedelta

        cutoff = cutoff - timedelta(days=days)

        kept_logs = []
        deleted_count = 0

        try:
            with open(self.logs_file, "r") as f:
                for line in f:
                    if line.strip():
                        try:
                            data = json.loads(line)
                            log_time = datetime.fromisoformat(
                                data.get("timestamp", "").replace("Z", "+00:00")
                            )
                            if log_time >= cutoff:
                                kept_logs.append(line)
                            else:
                                deleted_count += 1
                        except (json.JSONDecodeError, Exception):
                            kept_logs.append(line)

            with open(self.logs_file, "w") as f:
                f.writelines(kept_logs)

        except FileNotFoundError:
            pass

        return deleted_count

```

---

## backend/admin-dashboard/api/services/auth_service.py

```py
"""
Auth Service - ì¸ì¦/ê¶Œí•œ ê´€ë¦¬ ì„œë¹„ìŠ¤
"""

import hashlib
import secrets
import random
import string
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional
from uuid import uuid4

import yaml
from jose import JWTError, jwt

from ..models.schemas import Token, TokenData, User, UserCreate, UserRole, SetupStatus


class AuthService:
    """ì¸ì¦/ê¶Œí•œ ì„œë¹„ìŠ¤"""

    def __init__(
        self,
        config_dir: str,
        secret_key: Optional[str] = None,
        algorithm: str = "HS256",
        access_token_expire_minutes: int = 60,
    ):
        self.config_dir = Path(config_dir)
        self.secret_key = secret_key or secrets.token_urlsafe(32)
        self.algorithm = algorithm
        self.access_token_expire_minutes = access_token_expire_minutes
        self.users: dict[str, dict] = {}  # user_id -> user_data (with password hash)
        self.email_verifications: dict[str, dict] = {}  # email -> verification data
        self._load_users()

    def _load_users(self) -> None:
        """ì‚¬ìš©ì ì •ë³´ ë¡œë“œ"""
        users_file = self.config_dir / "users.yaml"
        if users_file.exists():
            with open(users_file) as f:
                data = yaml.safe_load(f) or {}
                self.users = data.get("users", {})
        else:
            # ê¸°ë³¸ ê´€ë¦¬ì ê³„ì • ìƒì„±
            self._create_default_admin()

    def _create_default_admin(self) -> None:
        """ê¸°ë³¸ ê´€ë¦¬ì ê³„ì • ìƒì„±"""
        admin_id = f"user-{uuid4().hex[:8]}"
        now = datetime.utcnow()

        # ê¸°ë³¸ ë¹„ë°€ë²ˆí˜¸: admin123 (ìš´ì˜ ì‹œ ë°˜ë“œì‹œ ë³€ê²½!)
        password_hash = self._hash_password("admin123")

        self.users[admin_id] = {
            "id": admin_id,
            "username": "admin",
            "email": "admin@localhost",
            "password_hash": password_hash,
            "role": UserRole.ADMIN.value,
            "is_active": True,
            "created_at": now.isoformat(),
            "last_login": None,
            "password_change_required": True,  # ì´ˆê¸° ì„¤ì • ì‹œ ë¹„ë°€ë²ˆí˜¸ ë³€ê²½ í•„ìš”
        }

        self._save_users()

    def _save_users(self) -> None:
        """ì‚¬ìš©ì ì •ë³´ ì €ì¥"""
        self.config_dir.mkdir(parents=True, exist_ok=True)
        users_file = self.config_dir / "users.yaml"

        data = {"users": self.users}

        with open(users_file, "w") as f:
            yaml.dump(data, f, default_flow_style=False, allow_unicode=True)

    def _hash_password(self, password: str) -> str:
        """ë¹„ë°€ë²ˆí˜¸ í•´ì‹œ"""
        # ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” bcrypt ë“± ì‚¬ìš© ê¶Œì¥
        return hashlib.sha256(password.encode()).hexdigest()

    def _verify_password(self, password: str, password_hash: str) -> bool:
        """ë¹„ë°€ë²ˆí˜¸ ê²€ì¦"""
        return self._hash_password(password) == password_hash

    def authenticate(self, username: str, password: str) -> Optional[User]:
        """ì‚¬ìš©ì ì¸ì¦"""
        for user_data in self.users.values():
            if user_data.get("username") == username:
                if not user_data.get("is_active", False):
                    return None

                if self._verify_password(password, user_data.get("password_hash", "")):
                    # ë§ˆì§€ë§‰ ë¡œê·¸ì¸ ì‹œê°„ ì—…ë°ì´íŠ¸
                    user_data["last_login"] = datetime.utcnow().isoformat()
                    self._save_users()

                    return User(
                        id=user_data["id"],
                        username=user_data["username"],
                        email=user_data.get("email"),
                        role=UserRole(user_data["role"]),
                        is_active=user_data["is_active"],
                        created_at=datetime.fromisoformat(user_data["created_at"]),
                        last_login=datetime.fromisoformat(user_data["last_login"])
                        if user_data.get("last_login")
                        else None,
                        password_change_required=user_data.get(
                            "password_change_required", False
                        ),
                    )

        return None

    def create_access_token(self, user: User) -> Token:
        """ì•¡ì„¸ìŠ¤ í† í° ìƒì„±"""
        expire = datetime.utcnow() + timedelta(minutes=self.access_token_expire_minutes)

        payload = {
            "sub": user.id,
            "username": user.username,
            "role": user.role.value,
            "exp": expire,
        }

        token = jwt.encode(payload, self.secret_key, algorithm=self.algorithm)

        return Token(
            access_token=token,
            token_type="bearer",
            expires_in=self.access_token_expire_minutes * 60,
        )

    def verify_token(self, token: str) -> Optional[TokenData]:
        """í† í° ê²€ì¦"""
        try:
            payload = jwt.decode(token, self.secret_key, algorithms=[self.algorithm])

            user_id = payload.get("sub")
            username = payload.get("username")
            role = payload.get("role")
            exp = payload.get("exp")

            if not all([user_id, username, role, exp]):
                return None

            return TokenData(
                user_id=user_id,
                username=username,
                role=UserRole(role),
                exp=datetime.fromtimestamp(exp),
            )

        except JWTError:
            return None

    def get_user(self, user_id: str) -> Optional[User]:
        """ì‚¬ìš©ì ì¡°íšŒ"""
        user_data = self.users.get(user_id)
        if not user_data:
            return None

        return User(
            id=user_data["id"],
            username=user_data["username"],
            email=user_data.get("email"),
            role=UserRole(user_data["role"]),
            is_active=user_data["is_active"],
            created_at=datetime.fromisoformat(user_data["created_at"]),
            last_login=datetime.fromisoformat(user_data["last_login"])
            if user_data.get("last_login")
            else None,
            password_change_required=user_data.get("password_change_required", False),
        )

    def get_user_by_username(self, username: str) -> Optional[User]:
        """ì‚¬ìš©ìëª…ìœ¼ë¡œ ì¡°íšŒ"""
        for user_data in self.users.values():
            if user_data.get("username") == username:
                return self.get_user(user_data["id"])
        return None

    def get_user_by_email(self, email: str) -> Optional[User]:
        """ì´ë©”ì¼ë¡œ ì¡°íšŒ"""
        for user_data in self.users.values():
            if user_data.get("email") == email:
                return self.get_user(user_data["id"])
        return None

    def list_users(self, active_only: bool = False) -> list[User]:
        """ì‚¬ìš©ì ëª©ë¡ ì¡°íšŒ"""
        users = []
        for user_data in self.users.values():
            if active_only and not user_data.get("is_active", False):
                continue

            users.append(
                User(
                    id=user_data["id"],
                    username=user_data["username"],
                    email=user_data.get("email"),
                    role=UserRole(user_data["role"]),
                    is_active=user_data["is_active"],
                    created_at=datetime.fromisoformat(user_data["created_at"]),
                    last_login=datetime.fromisoformat(user_data["last_login"])
                    if user_data.get("last_login")
                    else None,
                    password_change_required=user_data.get(
                        "password_change_required", False
                    ),
                )
            )

        return users

    def create_user(self, data: UserCreate) -> User:
        """ì‚¬ìš©ì ìƒì„±"""
        # ì¤‘ë³µ í™•ì¸
        if self.get_user_by_username(data.username):
            raise ValueError(f"Username already exists: {data.username}")

        user_id = f"user-{uuid4().hex[:8]}"
        now = datetime.utcnow()

        self.users[user_id] = {
            "id": user_id,
            "username": data.username,
            "email": data.email,
            "password_hash": self._hash_password(data.password),
            "role": data.role.value,
            "is_active": data.is_active,
            "created_at": now.isoformat(),
            "last_login": None,
            "password_change_required": False,  # ê´€ë¦¬ìê°€ ìƒì„±í•œ ê³„ì •ì€ ë³€ê²½ ë¶ˆí•„ìš”
        }

        self._save_users()

        return User(
            id=user_id,
            username=data.username,
            email=data.email,
            role=data.role,
            is_active=data.is_active,
            created_at=now,
            last_login=None,
            password_change_required=False,
        )

    def register_user(self, username: str, email: str, password: str) -> User:
        """ì¼ë°˜ ì‚¬ìš©ì íšŒì›ê°€ì…

        - ì‚¬ìš©ìëª… ì¤‘ë³µ ì²´í¬
        - ì´ë©”ì¼ ì¤‘ë³µ ì²´í¬
        - roleì€ í•­ìƒ USERë¡œ ê³ ì •
        """
        # ì‚¬ìš©ìëª… ì¤‘ë³µ í™•ì¸
        if self.get_user_by_username(username):
            raise ValueError(f"ì´ë¯¸ ì‚¬ìš© ì¤‘ì¸ ì‚¬ìš©ìëª…ì…ë‹ˆë‹¤: {username}")

        # ì´ë©”ì¼ ì¤‘ë³µ í™•ì¸
        if self.get_user_by_email(email):
            raise ValueError(f"ì´ë¯¸ ì‚¬ìš© ì¤‘ì¸ ì´ë©”ì¼ì…ë‹ˆë‹¤: {email}")

        user_id = f"user-{uuid4().hex[:8]}"
        now = datetime.utcnow()

        self.users[user_id] = {
            "id": user_id,
            "username": username,
            "email": email,
            "password_hash": self._hash_password(password),
            "role": UserRole.USER.value,  # í•­ìƒ ì¼ë°˜ ì‚¬ìš©ì
            "is_active": True,
            "created_at": now.isoformat(),
            "last_login": None,
            "password_change_required": False,
        }

        self._save_users()

        return User(
            id=user_id,
            username=username,
            email=email,
            role=UserRole.USER,
            is_active=True,
            created_at=now,
            last_login=None,
            password_change_required=False,
        )

    def update_user(
        self,
        user_id: str,
        email: Optional[str] = None,
        role: Optional[UserRole] = None,
        is_active: Optional[bool] = None,
    ) -> Optional[User]:
        """ì‚¬ìš©ì ì •ë³´ ìˆ˜ì •"""
        user_data = self.users.get(user_id)
        if not user_data:
            return None

        if email is not None:
            user_data["email"] = email
        if role is not None:
            user_data["role"] = role.value
        if is_active is not None:
            user_data["is_active"] = is_active

        self._save_users()
        return self.get_user(user_id)

    def change_password(
        self, user_id: str, old_password: str, new_password: str
    ) -> bool:
        """ë¹„ë°€ë²ˆí˜¸ ë³€ê²½"""
        user_data = self.users.get(user_id)
        if not user_data:
            return False

        if not self._verify_password(old_password, user_data.get("password_hash", "")):
            return False

        user_data["password_hash"] = self._hash_password(new_password)
        user_data["password_change_required"] = False  # ë¹„ë°€ë²ˆí˜¸ ë³€ê²½ í›„ í”Œë˜ê·¸ í•´ì œ
        self._save_users()
        return True

    def reset_password(self, user_id: str, new_password: str) -> bool:
        """ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™” (ê´€ë¦¬ììš©)"""
        user_data = self.users.get(user_id)
        if not user_data:
            return False

        user_data["password_hash"] = self._hash_password(new_password)
        user_data["password_change_required"] = True  # ì´ˆê¸°í™” í›„ ë³€ê²½ í•„ìš”
        self._save_users()
        return True

    def delete_user(self, user_id: str) -> bool:
        """ì‚¬ìš©ì ì‚­ì œ"""
        if user_id in self.users:
            del self.users[user_id]
            self._save_users()
            return True
        return False

    def check_permission(self, user_role: UserRole, required_role: UserRole) -> bool:
        """ê¶Œí•œ í™•ì¸"""
        role_priority = {
            UserRole.VIEWER: 0,
            UserRole.OPERATOR: 1,
            UserRole.ADMIN: 2,
        }

        user_level = role_priority.get(user_role, 0)
        required_level = role_priority.get(required_role, 0)

        return user_level >= required_level

    def get_setup_status(self) -> SetupStatus:
        """ì´ˆê¸° ì„¤ì • ìƒíƒœ í™•ì¸"""
        has_users = len(self.users) > 0

        # ê¸°ë³¸ ê´€ë¦¬ì ê³„ì •ë§Œ ì¡´ì¬í•˜ê³ , ë¹„ë°€ë²ˆí˜¸ ë³€ê²½ì´ í•„ìš”í•œ ê²½ìš°
        is_default_admin = False
        setup_required = False

        if has_users:
            # admin ê³„ì •ì´ ìˆê³  password_change_requiredê°€ Trueì¸ì§€ í™•ì¸
            for user_data in self.users.values():
                if user_data.get("username") == "admin" and user_data.get(
                    "password_change_required", False
                ):
                    is_default_admin = True
                    setup_required = True
                    break
        else:
            # ì‚¬ìš©ìê°€ ì—†ìœ¼ë©´ ì„¤ì •ì´ í•„ìš”
            setup_required = True

        return SetupStatus(
            setup_required=setup_required,
            has_users=has_users,
            is_default_admin=is_default_admin,
        )

    # ============================================================================
    # Email Verification Methods
    # ============================================================================

    def generate_verification_code(self) -> str:
        """6ìë¦¬ ì¸ì¦ ì½”ë“œ ìƒì„±"""
        return ''.join(random.choices(string.digits, k=6))

    def create_email_verification(self, email: str, username: str, password: str) -> str:
        """ì´ë©”ì¼ ì¸ì¦ ìš”ì²­ ìƒì„± (íšŒì›ê°€ì… ì „ ë‹¨ê³„)
        
        Returns:
            verification_code: 6ìë¦¬ ì¸ì¦ ì½”ë“œ
        """
        # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì´ë©”ì¼ì¸ì§€ í™•ì¸
        if self.get_user_by_email(email):
            raise ValueError(f"ì´ë¯¸ ì‚¬ìš© ì¤‘ì¸ ì´ë©”ì¼ì…ë‹ˆë‹¤: {email}")
        
        # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì‚¬ìš©ìëª…ì¸ì§€ í™•ì¸
        if self.get_user_by_username(username):
            raise ValueError(f"ì´ë¯¸ ì‚¬ìš© ì¤‘ì¸ ì‚¬ìš©ìëª…ì…ë‹ˆë‹¤: {username}")

        code = self.generate_verification_code()
        expires_at = datetime.utcnow() + timedelta(minutes=10)  # 10ë¶„ ìœ íš¨

        self.email_verifications[email] = {
            "code": code,
            "username": username,
            "password_hash": self._hash_password(password),
            "expires_at": expires_at.isoformat(),
            "created_at": datetime.utcnow().isoformat(),
            "attempts": 0,
        }

        return code

    def verify_email_code(self, email: str, code: str) -> User:
        """ì´ë©”ì¼ ì¸ì¦ ì½”ë“œ ê²€ì¦ ë° íšŒì›ê°€ì… ì™„ë£Œ
        
        Args:
            email: ì´ë©”ì¼ ì£¼ì†Œ
            code: 6ìë¦¬ ì¸ì¦ ì½”ë“œ
            
        Returns:
            User: ìƒì„±ëœ ì‚¬ìš©ì ê°ì²´
            
        Raises:
            ValueError: ìœ íš¨í•˜ì§€ ì•Šì€ ì¸ì¦ ì½”ë“œ ë˜ëŠ” ë§Œë£Œëœ ê²½ìš°
        """
        verification = self.email_verifications.get(email)
        
        if not verification:
            raise ValueError("ì¸ì¦ ìš”ì²­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        
        # ì‹œë„ íšŸìˆ˜ ì¦ê°€
        verification["attempts"] += 1
        
        # ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ì´ˆê³¼
        if verification["attempts"] > 5:
            del self.email_verifications[email]
            raise ValueError("ì¸ì¦ ì‹œë„ íšŸìˆ˜ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        
        # ë§Œë£Œ í™•ì¸
        expires_at = datetime.fromisoformat(verification["expires_at"])
        if datetime.utcnow() > expires_at:
            del self.email_verifications[email]
            raise ValueError("ì¸ì¦ ì½”ë“œê°€ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        
        # ì½”ë“œ í™•ì¸
        if verification["code"] != code:
            raise ValueError(f"ì˜ëª»ëœ ì¸ì¦ ì½”ë“œì…ë‹ˆë‹¤. (ë‚¨ì€ ì‹œë„: {5 - verification['attempts']}íšŒ)")
        
        # ì¸ì¦ ì„±ê³µ - íšŒì›ê°€ì… ì™„ë£Œ
        username = verification["username"]
        password_hash = verification["password_hash"]
        
        # ìµœì¢… ì¤‘ë³µ í™•ì¸
        if self.get_user_by_email(email):
            del self.email_verifications[email]
            raise ValueError(f"ì´ë¯¸ ì‚¬ìš© ì¤‘ì¸ ì´ë©”ì¼ì…ë‹ˆë‹¤: {email}")
        
        if self.get_user_by_username(username):
            del self.email_verifications[email]
            raise ValueError(f"ì´ë¯¸ ì‚¬ìš© ì¤‘ì¸ ì‚¬ìš©ìëª…ì…ë‹ˆë‹¤: {username}")
        
        # ì‚¬ìš©ì ìƒì„±
        user_id = f"user-{uuid4().hex[:8]}"
        now = datetime.utcnow()

        self.users[user_id] = {
            "id": user_id,
            "username": username,
            "email": email,
            "password_hash": password_hash,
            "role": UserRole.USER.value,
            "is_active": True,
            "created_at": now.isoformat(),
            "last_login": None,
            "password_change_required": False,
            "email_verified": True,
        }

        self._save_users()
        
        # ì¸ì¦ ì •ë³´ ì‚­ì œ
        del self.email_verifications[email]

        return User(
            id=user_id,
            username=username,
            email=email,
            role=UserRole.USER,
            is_active=True,
            created_at=now,
            last_login=None,
            password_change_required=False,
        )

    def resend_verification_code(self, email: str) -> str:
        """ì¸ì¦ ì½”ë“œ ì¬ë°œì†¡
        
        Returns:
            ìƒˆë¡œìš´ ì¸ì¦ ì½”ë“œ
        """
        verification = self.email_verifications.get(email)
        
        if not verification:
            raise ValueError("ì¸ì¦ ìš”ì²­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        
        # ìƒˆ ì½”ë“œ ìƒì„±
        code = self.generate_verification_code()
        verification["code"] = code
        verification["expires_at"] = (datetime.utcnow() + timedelta(minutes=10)).isoformat()
        verification["attempts"] = 0
        
        return code

```

---

## backend/admin-dashboard/api/services/data_source_service.py

```py
"""
Data Source Management Service
ë°ì´í„° ì†ŒìŠ¤ CRUD ë° ê´€ë¦¬ ì„œë¹„ìŠ¤
"""

import os
import uuid
from datetime import datetime
from pathlib import Path
from typing import Optional
import json
import yaml

try:
    import httpx
except ImportError:
    httpx = None  # type: ignore

from ..models.schemas import (
    DataSource,
    DataSourceCreate,
    DataSourceUpdate,
    DataSourceType,
    DataSourceStatus,
    DataSourceStats,
    DataSourceTestResult,
)


class DataSourceService:
    """ë°ì´í„° ì†ŒìŠ¤ ê´€ë¦¬ ì„œë¹„ìŠ¤"""

    def __init__(
        self,
        project_root: str,
        config_dir: str,
        collector_service_url: Optional[str] = None,
    ):
        self.project_root = Path(project_root)
        self.config_dir = Path(config_dir)
        self.config_file = self.config_dir / "data_sources.yaml"
        self._sources: dict[str, DataSource] = {}
        self.collector_service_url = collector_service_url or os.environ.get(
            "COLLECTOR_SERVICE_URL", "http://collector-service:8081"
        )
        self._load_sources()

    def _load_sources(self) -> None:
        """ì„¤ì • íŒŒì¼ì—ì„œ ë°ì´í„° ì†ŒìŠ¤ ë¡œë“œ"""
        if self.config_file.exists():
            with open(self.config_file, "r", encoding="utf-8") as f:
                data = yaml.safe_load(f) or {}
                for source_id, source_data in data.get("sources", {}).items():
                    try:
                        # Enum ë³€í™˜
                        source_data["source_type"] = DataSourceType(
                            source_data.get("source_type", "rss")
                        )
                        source_data["status"] = DataSourceStatus(
                            source_data.get("status", "active")
                        )
                        # datetime ë³€í™˜
                        for dt_field in ["created_at", "updated_at", "last_crawled_at"]:
                            if source_data.get(dt_field) and isinstance(
                                source_data[dt_field], str
                            ):
                                source_data[dt_field] = datetime.fromisoformat(
                                    source_data[dt_field]
                                )

                        self._sources[source_id] = DataSource(
                            id=source_id, **source_data
                        )
                    except Exception as e:
                        print(f"Error loading source {source_id}: {e}")

    def _save_sources(self) -> None:
        """ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì„¤ì • íŒŒì¼ì— ì €ì¥"""
        self.config_dir.mkdir(parents=True, exist_ok=True)

        data = {"sources": {}}
        for source_id, source in self._sources.items():
            source_dict = source.model_dump()
            # Enumì„ ë¬¸ìì—´ë¡œ ë³€í™˜
            source_dict["source_type"] = (
                source_dict["source_type"].value
                if hasattr(source_dict["source_type"], "value")
                else source_dict["source_type"]
            )
            source_dict["status"] = (
                source_dict["status"].value
                if hasattr(source_dict["status"], "value")
                else source_dict["status"]
            )
            # datetimeì„ ISO ë¬¸ìì—´ë¡œ ë³€í™˜
            for dt_field in ["created_at", "updated_at", "last_crawled_at"]:
                if source_dict.get(dt_field):
                    source_dict[dt_field] = (
                        source_dict[dt_field].isoformat()
                        if hasattr(source_dict[dt_field], "isoformat")
                        else source_dict[dt_field]
                    )
            # IDëŠ” í‚¤ë¡œ ì‚¬ìš©í•˜ë¯€ë¡œ ì œê±°
            del source_dict["id"]
            data["sources"][source_id] = source_dict

        with open(self.config_file, "w", encoding="utf-8") as f:
            yaml.dump(data, f, allow_unicode=True, default_flow_style=False)

    def list_sources(
        self,
        source_type: Optional[DataSourceType] = None,
        status: Optional[DataSourceStatus] = None,
        category: Optional[str] = None,
        is_active: Optional[bool] = None,
    ) -> list[DataSource]:
        """ë°ì´í„° ì†ŒìŠ¤ ëª©ë¡ ì¡°íšŒ"""
        sources = list(self._sources.values())

        if source_type:
            sources = [s for s in sources if s.source_type == source_type]
        if status:
            sources = [s for s in sources if s.status == status]
        if category:
            sources = [s for s in sources if s.category == category]
        if is_active is not None:
            sources = [s for s in sources if s.is_active == is_active]

        return sorted(sources, key=lambda x: (-x.priority, x.name))

    def get_source(self, source_id: str) -> Optional[DataSource]:
        """íŠ¹ì • ë°ì´í„° ì†ŒìŠ¤ ì¡°íšŒ"""
        return self._sources.get(source_id)

    def create_source(self, data: DataSourceCreate) -> DataSource:
        """ìƒˆ ë°ì´í„° ì†ŒìŠ¤ ìƒì„±"""
        source_id = str(uuid.uuid4())[:8]
        now = datetime.utcnow()

        source = DataSource(
            id=source_id,
            name=data.name,
            source_type=data.source_type,
            url=data.url,
            description=data.description,
            category=data.category,
            language=data.language,
            is_active=data.is_active,
            crawl_interval_minutes=data.crawl_interval_minutes,
            priority=data.priority,
            config=data.config,
            status=DataSourceStatus.ACTIVE
            if data.is_active
            else DataSourceStatus.INACTIVE,
            created_at=now,
            updated_at=now,
        )

        self._sources[source_id] = source
        self._save_sources()
        return source

    def update_source(
        self, source_id: str, data: DataSourceUpdate
    ) -> Optional[DataSource]:
        """ë°ì´í„° ì†ŒìŠ¤ ìˆ˜ì •"""
        source = self._sources.get(source_id)
        if not source:
            return None

        update_data = data.model_dump(exclude_unset=True)
        update_data["updated_at"] = datetime.utcnow()

        # is_active ë³€ê²½ ì‹œ statusë„ ì—…ë°ì´íŠ¸
        if "is_active" in update_data:
            if update_data["is_active"]:
                update_data["status"] = DataSourceStatus.ACTIVE
            else:
                update_data["status"] = DataSourceStatus.INACTIVE

        for key, value in update_data.items():
            if hasattr(source, key):
                setattr(source, key, value)

        self._save_sources()
        return source

    def delete_source(self, source_id: str) -> bool:
        """ë°ì´í„° ì†ŒìŠ¤ ì‚­ì œ"""
        if source_id in self._sources:
            del self._sources[source_id]
            self._save_sources()
            return True
        return False

    async def test_source(self, source_id: str) -> DataSourceTestResult:
        """ë°ì´í„° ì†ŒìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        source = self._sources.get(source_id)
        if not source:
            return DataSourceTestResult(
                source_id=source_id,
                success=False,
                message="Source not found",
                tested_at=datetime.utcnow(),
            )

        if httpx is None:
            return DataSourceTestResult(
                source_id=source_id,
                success=False,
                message="httpx not installed",
                tested_at=datetime.utcnow(),
            )

        # ì†ŒìŠ¤ URLì— ì§ì ‘ ìš”ì²­
        try:
            start_time = datetime.utcnow()
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(source.url)
                response_time = (datetime.utcnow() - start_time).total_seconds() * 1000

                if response.status_code == 200:
                    # ì†ŒìŠ¤ íƒ€ì…ì— ë”°ë¥¸ ìƒ˜í”Œ ë°ì´í„° íŒŒì‹±
                    sample_data = None
                    if source.source_type == DataSourceType.RSS:
                        sample_data = {
                            "content_type": response.headers.get(
                                "content-type", "unknown"
                            )
                        }
                    elif source.source_type == DataSourceType.API:
                        try:
                            sample_data = response.json()
                        except Exception:
                            sample_data = {"raw_length": len(response.text)}

                    # í…ŒìŠ¤íŠ¸ ì„±ê³µ ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸
                    source.status = DataSourceStatus.ACTIVE
                    self._save_sources()

                    return DataSourceTestResult(
                        source_id=source_id,
                        success=True,
                        message="Connection successful",
                        response_time_ms=response_time,
                        sample_data=sample_data,
                        tested_at=datetime.utcnow(),
                    )
                else:
                    return DataSourceTestResult(
                        source_id=source_id,
                        success=False,
                        message=f"HTTP {response.status_code}",
                        response_time_ms=response_time,
                        tested_at=datetime.utcnow(),
                    )
        except Exception as e:
            # í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸
            source.status = DataSourceStatus.ERROR
            self._save_sources()

            return DataSourceTestResult(
                source_id=source_id,
                success=False,
                message=f"Error: {str(e)}",
                tested_at=datetime.utcnow(),
            )

    async def trigger_crawl(self, source_id: str) -> dict:
        """ë°ì´í„° ìˆ˜ì§‘ íŠ¸ë¦¬ê±°"""
        source = self._sources.get(source_id)
        if not source:
            return {"success": False, "message": "Source not found"}

        if not source.is_active:
            return {"success": False, "message": "Source is not active"}

        if httpx is None:
            return {"success": False, "message": "httpx not installed"}

        # Collector Serviceì— ìˆ˜ì§‘ ìš”ì²­
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    f"{self.collector_service_url}/api/v1/crawl/trigger",
                    json={
                        "source_id": source_id,
                        "source_url": source.url,
                        "source_type": source.source_type.value,
                    },
                )

                if response.status_code in [200, 202]:
                    source.last_crawled_at = datetime.utcnow()
                    self._save_sources()
                    return {"success": True, "message": "Crawl triggered successfully"}
                else:
                    return {
                        "success": False,
                        "message": f"Failed: HTTP {response.status_code}",
                    }
        except Exception as e:
            return {"success": False, "message": f"Error: {str(e)}"}

    def get_categories(self) -> list[str]:
        """ëª¨ë“  ì¹´í…Œê³ ë¦¬ ëª©ë¡ ì¡°íšŒ"""
        categories = set()
        for source in self._sources.values():
            if source.category:
                categories.add(source.category)
        return sorted(categories)

    def get_stats(self) -> dict:
        """ë°ì´í„° ì†ŒìŠ¤ í†µê³„"""
        total = len(self._sources)
        active = sum(1 for s in self._sources.values() if s.is_active)
        by_type = {}
        by_status = {}

        for source in self._sources.values():
            type_key = source.source_type.value
            by_type[type_key] = by_type.get(type_key, 0) + 1

            status_key = source.status.value
            by_status[status_key] = by_status.get(status_key, 0) + 1

        total_articles = sum(s.total_articles for s in self._sources.values())

        return {
            "total_sources": total,
            "active_sources": active,
            "inactive_sources": total - active,
            "by_type": by_type,
            "by_status": by_status,
            "total_articles": total_articles,
        }

    def bulk_toggle_active(self, source_ids: list[str], is_active: bool) -> int:
        """ì—¬ëŸ¬ ì†ŒìŠ¤ í™œì„±í™”/ë¹„í™œì„±í™”"""
        updated = 0
        for source_id in source_ids:
            source = self._sources.get(source_id)
            if source:
                source.is_active = is_active
                source.status = (
                    DataSourceStatus.ACTIVE if is_active else DataSourceStatus.INACTIVE
                )
                source.updated_at = datetime.utcnow()
                updated += 1

        if updated > 0:
            self._save_sources()

        return updated

```

---

## backend/admin-dashboard/api/services/database_service.py

```py
"""
Database Management Service
PostgreSQL, MongoDB, Redis ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ ì„œë¹„ìŠ¤
"""

import os
from datetime import datetime
from pathlib import Path
from typing import Optional
import json

try:
    import httpx
except ImportError:
    httpx = None  # type: ignore

from ..models.schemas import (
    DatabaseType,
    DatabaseInfo,
    PostgresDatabaseStats,
    PostgresTableInfo,
    MongoDatabaseStats,
    MongoCollectionInfo,
    RedisStats,
    ServiceHealthStatus,
)


def format_bytes(size_bytes: int) -> str:
    """ë°”ì´íŠ¸ë¥¼ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    for unit in ["B", "KB", "MB", "GB", "TB"]:
        if size_bytes < 1024.0:
            return f"{size_bytes:.2f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.2f} PB"


class DatabaseService:
    """ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ ì„œë¹„ìŠ¤"""

    def __init__(self, project_root: str, config_dir: str):
        self.project_root = Path(project_root)
        self.config_dir = Path(config_dir)

        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë³´
        self.postgres_host = os.environ.get("POSTGRES_HOST", "postgres")
        self.postgres_port = int(os.environ.get("POSTGRES_PORT", "5432"))
        self.postgres_db = os.environ.get("POSTGRES_DB", "newsinsight")
        self.postgres_user = os.environ.get("POSTGRES_USER", "postgres")
        self.postgres_password = os.environ.get("POSTGRES_PASSWORD", "postgres")

        self.mongo_host = os.environ.get("MONGO_HOST", "mongo")
        self.mongo_port = int(os.environ.get("MONGO_PORT", "27017"))
        self.mongo_db = os.environ.get("MONGO_DB", "newsinsight")

        self.redis_host = os.environ.get("REDIS_HOST", "redis")
        self.redis_port = int(os.environ.get("REDIS_PORT", "6379"))

        self.timeout = 5.0

    async def get_all_databases(self) -> list[DatabaseInfo]:
        """ëª¨ë“  ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ ì¡°íšŒ"""
        databases = []

        # PostgreSQL
        postgres_info = await self.get_postgres_health()
        databases.append(postgres_info)

        # MongoDB
        mongo_info = await self.get_mongo_health()
        databases.append(mongo_info)

        # Redis
        redis_info = await self.get_redis_health()
        databases.append(redis_info)

        return databases

    async def get_postgres_health(self) -> DatabaseInfo:
        """PostgreSQL í—¬ìŠ¤ ì •ë³´"""
        try:
            # psycopg2 ì—†ì´ TCP ì—°ê²°ë¡œë§Œ ì²´í¬
            import asyncio

            reader, writer = await asyncio.wait_for(
                asyncio.open_connection(self.postgres_host, self.postgres_port),
                timeout=self.timeout,
            )
            writer.close()
            await writer.wait_closed()

            return DatabaseInfo(
                db_type=DatabaseType.POSTGRESQL,
                name=self.postgres_db,
                host=self.postgres_host,
                port=self.postgres_port,
                status=ServiceHealthStatus.HEALTHY,
                version="15.x",  # ì‹¤ì œë¡œëŠ” ì¿¼ë¦¬ë¡œ í™•ì¸ í•„ìš”
                checked_at=datetime.utcnow(),
            )
        except Exception as e:
            return DatabaseInfo(
                db_type=DatabaseType.POSTGRESQL,
                name=self.postgres_db,
                host=self.postgres_host,
                port=self.postgres_port,
                status=ServiceHealthStatus.UNREACHABLE,
                checked_at=datetime.utcnow(),
            )

    async def get_mongo_health(self) -> DatabaseInfo:
        """MongoDB í—¬ìŠ¤ ì •ë³´"""
        try:
            import asyncio

            reader, writer = await asyncio.wait_for(
                asyncio.open_connection(self.mongo_host, self.mongo_port),
                timeout=self.timeout,
            )
            writer.close()
            await writer.wait_closed()

            return DatabaseInfo(
                db_type=DatabaseType.MONGODB,
                name=self.mongo_db,
                host=self.mongo_host,
                port=self.mongo_port,
                status=ServiceHealthStatus.HEALTHY,
                version="7.x",
                checked_at=datetime.utcnow(),
            )
        except Exception as e:
            return DatabaseInfo(
                db_type=DatabaseType.MONGODB,
                name=self.mongo_db,
                host=self.mongo_host,
                port=self.mongo_port,
                status=ServiceHealthStatus.UNREACHABLE,
                checked_at=datetime.utcnow(),
            )

    async def get_redis_health(self) -> DatabaseInfo:
        """Redis í—¬ìŠ¤ ì •ë³´"""
        try:
            import asyncio

            reader, writer = await asyncio.wait_for(
                asyncio.open_connection(self.redis_host, self.redis_port),
                timeout=self.timeout,
            )

            # PING ëª…ë ¹
            writer.write(b"PING\r\n")
            await writer.drain()
            response = await asyncio.wait_for(reader.readline(), timeout=2.0)
            writer.close()
            await writer.wait_closed()

            is_healthy = b"+PONG" in response

            return DatabaseInfo(
                db_type=DatabaseType.REDIS,
                name="redis",
                host=self.redis_host,
                port=self.redis_port,
                status=ServiceHealthStatus.HEALTHY
                if is_healthy
                else ServiceHealthStatus.DEGRADED,
                version="7.x",
                checked_at=datetime.utcnow(),
            )
        except Exception as e:
            return DatabaseInfo(
                db_type=DatabaseType.REDIS,
                name="redis",
                host=self.redis_host,
                port=self.redis_port,
                status=ServiceHealthStatus.UNREACHABLE,
                checked_at=datetime.utcnow(),
            )

    async def get_postgres_stats(self) -> PostgresDatabaseStats:
        """PostgreSQL ìƒì„¸ í†µê³„ (ì‹¤ì œ ì—°ê²° í•„ìš”)"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” psycopg2 ë˜ëŠ” asyncpg ì‚¬ìš©
        # ì—¬ê¸°ì„œëŠ” ìƒ˜í”Œ ë°ì´í„° ë°˜í™˜
        return PostgresDatabaseStats(
            database_name=self.postgres_db,
            size_bytes=0,
            size_human="N/A",
            tables=[
                PostgresTableInfo(
                    schema_name="public",
                    table_name="news_articles",
                    row_count=0,
                    size_bytes=0,
                    size_human="N/A",
                ),
                PostgresTableInfo(
                    schema_name="public",
                    table_name="news_sources",
                    row_count=0,
                    size_bytes=0,
                    size_human="N/A",
                ),
            ],
            total_tables=0,
            total_rows=0,
            connection_count=0,
            max_connections=100,
            checked_at=datetime.utcnow(),
        )

    async def get_mongo_stats(self) -> MongoDatabaseStats:
        """MongoDB ìƒì„¸ í†µê³„"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” pymongo ì‚¬ìš©
        return MongoDatabaseStats(
            database_name=self.mongo_db,
            size_bytes=0,
            size_human="N/A",
            collections=[
                MongoCollectionInfo(
                    collection_name="ai_responses",
                    document_count=0,
                    size_bytes=0,
                    size_human="N/A",
                    index_count=1,
                ),
            ],
            total_collections=0,
            total_documents=0,
            checked_at=datetime.utcnow(),
        )

    async def get_redis_stats(self) -> RedisStats:
        """Redis ìƒì„¸ í†µê³„"""
        try:
            import asyncio

            reader, writer = await asyncio.wait_for(
                asyncio.open_connection(self.redis_host, self.redis_port),
                timeout=self.timeout,
            )

            # INFO ëª…ë ¹
            writer.write(b"INFO\r\n")
            await writer.drain()

            # ì‘ë‹µ ì½ê¸° (bulk string)
            response_lines = []
            while True:
                line = await asyncio.wait_for(reader.readline(), timeout=2.0)
                if not line or line == b"\r\n":
                    break
                response_lines.append(line.decode("utf-8", errors="ignore").strip())

            writer.close()
            await writer.wait_closed()

            # íŒŒì‹±
            info = {}
            for line in response_lines:
                if ":" in line and not line.startswith("#"):
                    key, value = line.split(":", 1)
                    info[key] = value

            used_memory = int(info.get("used_memory", 0))
            keyspace_hits = int(info.get("keyspace_hits", 0))
            keyspace_misses = int(info.get("keyspace_misses", 0))
            total_requests = keyspace_hits + keyspace_misses
            hit_rate = (
                (keyspace_hits / total_requests * 100) if total_requests > 0 else 0.0
            )

            # DB0ì—ì„œ í‚¤ ìˆ˜ ì¶”ì¶œ
            db0_info = info.get("db0", "")
            total_keys = 0
            if db0_info:
                for part in db0_info.split(","):
                    if part.startswith("keys="):
                        total_keys = int(part.split("=")[1])
                        break

            return RedisStats(
                used_memory_bytes=used_memory,
                used_memory_human=format_bytes(used_memory),
                max_memory_bytes=int(info.get("maxmemory", 0)) or None,
                connected_clients=int(info.get("connected_clients", 0)),
                total_keys=total_keys,
                expired_keys=int(info.get("expired_keys", 0)),
                keyspace_hits=keyspace_hits,
                keyspace_misses=keyspace_misses,
                hit_rate=hit_rate,
                uptime_seconds=int(info.get("uptime_in_seconds", 0)),
                checked_at=datetime.utcnow(),
            )
        except Exception as e:
            return RedisStats(
                used_memory_bytes=0,
                used_memory_human="N/A",
                connected_clients=0,
                total_keys=0,
                expired_keys=0,
                keyspace_hits=0,
                keyspace_misses=0,
                hit_rate=0.0,
                uptime_seconds=0,
                checked_at=datetime.utcnow(),
            )

```

---

## backend/admin-dashboard/api/services/document_service.py

```py
"""
Document Service - Markdown ë¬¸ì„œ ê´€ë¦¬ ì„œë¹„ìŠ¤
"""
import os
from datetime import datetime
from pathlib import Path
from typing import Optional
from uuid import uuid4

import yaml

from ..models.schemas import Document, DocumentBase, DocumentCategory


class DocumentService:
    """ë¬¸ì„œ ê´€ë¦¬ ì„œë¹„ìŠ¤"""

    def __init__(self, project_root: str, config_dir: str):
        self.project_root = Path(project_root)
        self.config_dir = Path(config_dir)
        self.docs_dirs = [
            self.project_root / "docs",
            self.project_root / "etc" / "infra-guides",
        ]
        self.documents: dict[str, Document] = {}
        self._scan_documents()

    def _scan_documents(self) -> None:
        """ë¬¸ì„œ ë””ë ‰í† ë¦¬ ìŠ¤ìº”"""
        config_file = self.config_dir / "documents.yaml"

        # ê¸°ì¡´ ì„¤ì • ë¡œë“œ
        existing_config = {}
        if config_file.exists():
            with open(config_file) as f:
                data = yaml.safe_load(f) or {}
                for doc_data in data.get("documents", []):
                    existing_config[doc_data.get("file_path")] = doc_data

        # ë¬¸ì„œ ìŠ¤ìº”
        for docs_dir in self.docs_dirs:
            if not docs_dir.exists():
                continue

            for md_file in docs_dir.rglob("*.md"):
                file_path = str(md_file.relative_to(self.project_root))
                abs_path = str(md_file)

                # ê¸°ì¡´ ì„¤ì •ì´ ìˆìœ¼ë©´ ì‚¬ìš©
                if abs_path in existing_config:
                    doc_data = existing_config[abs_path]
                    doc = Document(**doc_data)
                else:
                    # ìƒˆ ë¬¸ì„œ ìƒì„±
                    doc = self._create_document_from_file(md_file)

                self.documents[doc.id] = doc

        self._save_documents()

    def _create_document_from_file(self, file_path: Path) -> Document:
        """íŒŒì¼ì—ì„œ ë¬¸ì„œ ì •ë³´ ìƒì„±"""
        doc_id = f"doc-{uuid4().hex[:8]}"
        rel_path = str(file_path.relative_to(self.project_root))

        # íŒŒì¼ëª…ì—ì„œ ì œëª© ì¶”ì¶œ
        title = file_path.stem.replace("_", " ").replace("-", " ").title()

        # ì¹´í…Œê³ ë¦¬ ì¶”ë¡ 
        category = self._infer_category(file_path)

        # íƒœê·¸ ì¶”ë¡ 
        tags = self._infer_tags(file_path)

        # ê´€ë ¨ í™˜ê²½ ì¶”ë¡ 
        related_envs = self._infer_environments(file_path)

        # ìˆ˜ì • ì‹œê°„
        stat = file_path.stat()
        last_modified = datetime.fromtimestamp(stat.st_mtime)

        return Document(
            id=doc_id,
            title=title,
            file_path=str(file_path),
            category=category,
            tags=tags,
            related_environments=related_envs,
            related_scripts=[],
            last_modified=last_modified,
        )

    def _infer_category(self, file_path: Path) -> DocumentCategory:
        """íŒŒì¼ ê²½ë¡œì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ë¡ """
        path_str = str(file_path).lower()

        if "deploy" in path_str or "deployment" in path_str:
            return DocumentCategory.DEPLOYMENT
        elif "troubleshoot" in path_str or "debug" in path_str:
            return DocumentCategory.TROUBLESHOOTING
        elif "architecture" in path_str or "overview" in path_str:
            return DocumentCategory.ARCHITECTURE
        elif "runbook" in path_str or "guide" in path_str:
            return DocumentCategory.RUNBOOK
        else:
            return DocumentCategory.GENERAL

    def _infer_tags(self, file_path: Path) -> list[str]:
        """íŒŒì¼ ê²½ë¡œì—ì„œ íƒœê·¸ ì¶”ë¡ """
        tags = []
        path_str = str(file_path).lower()

        tag_keywords = [
            "docker",
            "kubernetes",
            "k8s",
            "consul",
            "cloudflare",
            "gcp",
            "aws",
            "api",
            "frontend",
            "backend",
            "database",
            "redis",
            "postgres",
            "mongo",
            "kafka",
        ]

        for keyword in tag_keywords:
            if keyword in path_str:
                tags.append(keyword)

        return tags

    def _infer_environments(self, file_path: Path) -> list[str]:
        """íŒŒì¼ ê²½ë¡œì—ì„œ ê´€ë ¨ í™˜ê²½ ì¶”ë¡ """
        envs = []
        path_str = str(file_path).lower()

        env_keywords = {
            "zerotrust": "zerotrust",
            "local": "local",
            "gcp": "gcp",
            "aws": "aws",
            "production": "production",
            "staging": "staging",
            "pmx": "production",
        }

        for keyword, env in env_keywords.items():
            if keyword in path_str and env not in envs:
                envs.append(env)

        return envs

    def _save_documents(self) -> None:
        """ë¬¸ì„œ ì„¤ì •ì„ íŒŒì¼ì— ì €ì¥"""
        self.config_dir.mkdir(parents=True, exist_ok=True)
        config_file = self.config_dir / "documents.yaml"

        data = {
            "documents": [doc.model_dump(mode="json") for doc in self.documents.values()]
        }

        # content í•„ë“œëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ
        for doc_data in data["documents"]:
            doc_data.pop("content", None)

        with open(config_file, "w") as f:
            yaml.dump(data, f, default_flow_style=False, allow_unicode=True)

    def list_documents(
        self,
        category: Optional[DocumentCategory] = None,
        tag: Optional[str] = None,
        environment: Optional[str] = None,
        search: Optional[str] = None,
    ) -> list[Document]:
        """ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
        docs = list(self.documents.values())

        if category:
            docs = [d for d in docs if d.category == category]

        if tag:
            docs = [d for d in docs if tag in d.tags]

        if environment:
            docs = [d for d in docs if environment in d.related_environments]

        if search:
            search_lower = search.lower()
            docs = [
                d
                for d in docs
                if search_lower in d.title.lower()
                or any(search_lower in t.lower() for t in d.tags)
            ]

        return sorted(docs, key=lambda x: x.title)

    def get_document(self, doc_id: str) -> Optional[Document]:
        """ë¬¸ì„œ ìƒì„¸ ì¡°íšŒ (ë‚´ìš© í¬í•¨)"""
        doc = self.documents.get(doc_id)
        if not doc:
            return None

        # íŒŒì¼ ë‚´ìš© ì½ê¸°
        file_path = Path(doc.file_path)
        if file_path.exists():
            try:
                with open(file_path, encoding="utf-8") as f:
                    doc.content = f.read()
            except Exception:
                doc.content = "Error reading file content"

        return doc

    def get_document_by_path(self, file_path: str) -> Optional[Document]:
        """íŒŒì¼ ê²½ë¡œë¡œ ë¬¸ì„œ ì¡°íšŒ"""
        for doc in self.documents.values():
            if doc.file_path == file_path:
                return self.get_document(doc.id)
        return None

    def update_document_metadata(
        self,
        doc_id: str,
        title: Optional[str] = None,
        category: Optional[DocumentCategory] = None,
        tags: Optional[list[str]] = None,
        related_environments: Optional[list[str]] = None,
        related_scripts: Optional[list[str]] = None,
    ) -> Optional[Document]:
        """ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìˆ˜ì •"""
        doc = self.documents.get(doc_id)
        if not doc:
            return None

        if title is not None:
            doc.title = title
        if category is not None:
            doc.category = category
        if tags is not None:
            doc.tags = tags
        if related_environments is not None:
            doc.related_environments = related_environments
        if related_scripts is not None:
            doc.related_scripts = related_scripts

        self._save_documents()
        return doc

    def refresh_documents(self) -> int:
        """ë¬¸ì„œ ëª©ë¡ ìƒˆë¡œê³ ì¹¨"""
        old_count = len(self.documents)
        self.documents.clear()
        self._scan_documents()
        return len(self.documents) - old_count

    def get_related_documents(
        self, environment: Optional[str] = None, script_id: Optional[str] = None
    ) -> list[Document]:
        """ê´€ë ¨ ë¬¸ì„œ ì¡°íšŒ"""
        docs = []

        if environment:
            docs.extend(
                [d for d in self.documents.values() if environment in d.related_environments]
            )

        if script_id:
            docs.extend(
                [d for d in self.documents.values() if script_id in d.related_scripts]
            )

        # ì¤‘ë³µ ì œê±°
        seen = set()
        unique_docs = []
        for doc in docs:
            if doc.id not in seen:
                seen.add(doc.id)
                unique_docs.append(doc)

        return unique_docs

    def get_categories_summary(self) -> dict[str, int]:
        """ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ ìˆ˜ ìš”ì•½"""
        summary = {}
        for doc in self.documents.values():
            cat = doc.category.value
            summary[cat] = summary.get(cat, 0) + 1
        return summary

    def get_tags_summary(self) -> dict[str, int]:
        """íƒœê·¸ë³„ ë¬¸ì„œ ìˆ˜ ìš”ì•½"""
        summary = {}
        for doc in self.documents.values():
            for tag in doc.tags:
                summary[tag] = summary.get(tag, 0) + 1
        return summary

```

---

## backend/admin-dashboard/api/services/environment_service.py

```py
"""
Environment Service - í™˜ê²½/í”„ë¡œí•„ ê´€ë¦¬ ì„œë¹„ìŠ¤
"""
import os
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Optional
from uuid import uuid4

import yaml

from ..models.schemas import (
    ContainerInfo,
    Environment,
    EnvironmentCreate,
    EnvironmentStatus,
    EnvironmentType,
    EnvironmentUpdate,
    ServiceStatus,
)


class EnvironmentService:
    """í™˜ê²½ ê´€ë¦¬ ì„œë¹„ìŠ¤"""

    def __init__(self, project_root: str, config_dir: str):
        self.project_root = Path(project_root)
        self.config_dir = Path(config_dir)
        self.environments: dict[str, Environment] = {}
        self._load_environments()

    def _load_environments(self) -> None:
        """ì„¤ì • íŒŒì¼ì—ì„œ í™˜ê²½ ì •ë³´ ë¡œë“œ"""
        config_file = self.config_dir / "environments.yaml"
        if config_file.exists():
            with open(config_file) as f:
                data = yaml.safe_load(f) or {}
                for env_data in data.get("environments", []):
                    env = Environment(**env_data)
                    self.environments[env.id] = env
        else:
            # ê¸°ë³¸ í™˜ê²½ ì„¤ì • ìƒì„±
            self._create_default_environments()

    def _create_default_environments(self) -> None:
        """ê¸°ë³¸ í™˜ê²½ ì„¤ì • ìƒì„±"""
        docker_dir = self.project_root / "etc" / "docker"
        configs_dir = self.project_root / "etc" / "configs"

        default_envs = [
            {
                "id": "env-zerotrust",
                "name": "zerotrust",
                "env_type": EnvironmentType.ZEROTRUST,
                "description": "Cloudflare Zero Trust ê¸°ë°˜ ë³´ì•ˆ í™˜ê²½",
                "compose_file": str(docker_dir / "docker-compose.zerotrust.yml"),
                "env_file": str(docker_dir / ".env"),
                "is_active": True,
                "priority": 100,
                "created_at": datetime.utcnow(),
                "updated_at": datetime.utcnow(),
            },
            {
                "id": "env-production",
                "name": "production",
                "env_type": EnvironmentType.PRODUCTION,
                "description": "í”„ë¡œë•ì…˜ í™˜ê²½",
                "compose_file": str(docker_dir / "docker-compose.production.yml"),
                "env_file": str(configs_dir / "production.env"),
                "is_active": True,
                "priority": 90,
                "created_at": datetime.utcnow(),
                "updated_at": datetime.utcnow(),
            },
            {
                "id": "env-staging",
                "name": "staging",
                "env_type": EnvironmentType.STAGING,
                "description": "ìŠ¤í…Œì´ì§• í™˜ê²½",
                "compose_file": str(docker_dir / "docker-compose.consul.yml"),
                "env_file": str(configs_dir / "staging.env"),
                "is_active": True,
                "priority": 80,
                "created_at": datetime.utcnow(),
                "updated_at": datetime.utcnow(),
            },
            {
                "id": "env-local",
                "name": "local",
                "env_type": EnvironmentType.LOCAL,
                "description": "ë¡œì»¬ ê°œë°œ í™˜ê²½",
                "compose_file": str(docker_dir / "docker-compose.consul.yml"),
                "env_file": str(configs_dir / "development.env"),
                "is_active": True,
                "priority": 70,
                "created_at": datetime.utcnow(),
                "updated_at": datetime.utcnow(),
            },
        ]

        for env_data in default_envs:
            env = Environment(**env_data)
            self.environments[env.id] = env

        self._save_environments()

    def _save_environments(self) -> None:
        """í™˜ê²½ ì„¤ì •ì„ íŒŒì¼ì— ì €ì¥"""
        self.config_dir.mkdir(parents=True, exist_ok=True)
        config_file = self.config_dir / "environments.yaml"

        data = {
            "environments": [
                env.model_dump(mode="json") for env in self.environments.values()
            ]
        }

        with open(config_file, "w") as f:
            yaml.dump(data, f, default_flow_style=False, allow_unicode=True)

    def list_environments(self, active_only: bool = False) -> list[Environment]:
        """í™˜ê²½ ëª©ë¡ ì¡°íšŒ"""
        envs = list(self.environments.values())
        if active_only:
            envs = [e for e in envs if e.is_active]
        return sorted(envs, key=lambda x: -x.priority)

    def get_environment(self, env_id: str) -> Optional[Environment]:
        """í™˜ê²½ ìƒì„¸ ì¡°íšŒ"""
        return self.environments.get(env_id)

    def get_environment_by_name(self, name: str) -> Optional[Environment]:
        """ì´ë¦„ìœ¼ë¡œ í™˜ê²½ ì¡°íšŒ"""
        for env in self.environments.values():
            if env.name == name:
                return env
        return None

    def create_environment(self, data: EnvironmentCreate) -> Environment:
        """í™˜ê²½ ìƒì„±"""
        env_id = f"env-{uuid4().hex[:8]}"
        now = datetime.utcnow()

        env = Environment(
            id=env_id,
            created_at=now,
            updated_at=now,
            **data.model_dump(),
        )

        self.environments[env_id] = env
        self._save_environments()
        return env

    def update_environment(
        self, env_id: str, data: EnvironmentUpdate
    ) -> Optional[Environment]:
        """í™˜ê²½ ìˆ˜ì •"""
        env = self.environments.get(env_id)
        if not env:
            return None

        update_data = data.model_dump(exclude_unset=True)
        for key, value in update_data.items():
            setattr(env, key, value)

        env.updated_at = datetime.utcnow()
        self._save_environments()
        return env

    def delete_environment(self, env_id: str) -> bool:
        """í™˜ê²½ ì‚­ì œ"""
        if env_id in self.environments:
            del self.environments[env_id]
            self._save_environments()
            return True
        return False

    def get_environment_status(self, env_id: str) -> Optional[EnvironmentStatus]:
        """í™˜ê²½ì˜ ì»¨í…Œì´ë„ˆ ìƒíƒœ ì¡°íšŒ"""
        env = self.environments.get(env_id)
        if not env:
            return None

        containers = self._get_docker_containers(env)

        return EnvironmentStatus(
            environment_id=env.id,
            environment_name=env.name,
            containers=containers,
            total_containers=len(containers),
            running_containers=sum(
                1 for c in containers if c.status == ServiceStatus.UP
            ),
        )

    def _get_docker_containers(self, env: Environment) -> list[ContainerInfo]:
        """Docker ì»¨í…Œì´ë„ˆ ìƒíƒœ ì¡°íšŒ"""
        containers = []

        if not Path(env.compose_file).exists():
            return containers

        try:
            # docker compose ps ì‹¤í–‰
            result = subprocess.run(
                [
                    "docker",
                    "compose",
                    "-f",
                    env.compose_file,
                    "-p",
                    "newsinsight",
                    "ps",
                    "--format",
                    "json",
                ],
                capture_output=True,
                text=True,
                timeout=30,
            )

            if result.returncode == 0 and result.stdout.strip():
                import json

                # ê° ì¤„ì´ JSON ê°ì²´ì¼ ìˆ˜ ìˆìŒ
                for line in result.stdout.strip().split("\n"):
                    if line.strip():
                        try:
                            container_data = json.loads(line)
                            status = self._parse_container_status(
                                container_data.get("State", "")
                            )
                            containers.append(
                                ContainerInfo(
                                    name=container_data.get("Name", "unknown"),
                                    image=container_data.get("Image", "unknown"),
                                    status=status,
                                    health=container_data.get("Health", None),
                                    ports=self._parse_ports(
                                        container_data.get("Ports", "")
                                    ),
                                )
                            )
                        except json.JSONDecodeError:
                            continue

        except subprocess.TimeoutExpired:
            pass
        except FileNotFoundError:
            pass

        return containers

    def _parse_container_status(self, state: str) -> ServiceStatus:
        """ì»¨í…Œì´ë„ˆ ìƒíƒœ íŒŒì‹±"""
        state_lower = state.lower()
        if "running" in state_lower:
            return ServiceStatus.UP
        elif "exited" in state_lower or "dead" in state_lower:
            return ServiceStatus.DOWN
        elif "starting" in state_lower or "created" in state_lower:
            return ServiceStatus.STARTING
        elif "stopping" in state_lower or "removing" in state_lower:
            return ServiceStatus.STOPPING
        return ServiceStatus.UNKNOWN

    def _parse_ports(self, ports_str: str) -> list[str]:
        """í¬íŠ¸ ë¬¸ìì—´ íŒŒì‹±"""
        if not ports_str:
            return []
        return [p.strip() for p in ports_str.split(",") if p.strip()]

    async def docker_compose_up(
        self, env_id: str, build: bool = True, detach: bool = True
    ) -> tuple[bool, str]:
        """Docker Compose Up ì‹¤í–‰"""
        env = self.environments.get(env_id)
        if not env:
            return False, "Environment not found"

        cmd = ["docker", "compose", "-f", env.compose_file, "-p", "newsinsight"]

        if build:
            cmd.extend(["up", "--build"])
        else:
            cmd.extend(["up"])

        if detach:
            cmd.append("-d")

        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=600,  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
                cwd=str(self.project_root),
            )
            return result.returncode == 0, result.stdout + result.stderr
        except subprocess.TimeoutExpired:
            return False, "Command timed out"
        except Exception as e:
            return False, str(e)

    async def docker_compose_down(
        self, env_id: str, volumes: bool = False
    ) -> tuple[bool, str]:
        """Docker Compose Down ì‹¤í–‰"""
        env = self.environments.get(env_id)
        if not env:
            return False, "Environment not found"

        cmd = ["docker", "compose", "-f", env.compose_file, "-p", "newsinsight", "down"]

        if volumes:
            cmd.append("-v")

        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=120,
                cwd=str(self.project_root),
            )
            return result.returncode == 0, result.stdout + result.stderr
        except subprocess.TimeoutExpired:
            return False, "Command timed out"
        except Exception as e:
            return False, str(e)

    async def docker_compose_restart(
        self, env_id: str, service: Optional[str] = None
    ) -> tuple[bool, str]:
        """Docker Compose Restart ì‹¤í–‰"""
        env = self.environments.get(env_id)
        if not env:
            return False, "Environment not found"

        cmd = [
            "docker",
            "compose",
            "-f",
            env.compose_file,
            "-p",
            "newsinsight",
            "restart",
        ]

        if service:
            cmd.append(service)

        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=120,
                cwd=str(self.project_root),
            )
            return result.returncode == 0, result.stdout + result.stderr
        except subprocess.TimeoutExpired:
            return False, "Command timed out"
        except Exception as e:
            return False, str(e)

    async def get_service_logs(
        self, env_id: str, service: str, tail: int = 100
    ) -> tuple[bool, str]:
        """ì„œë¹„ìŠ¤ ë¡œê·¸ ì¡°íšŒ"""
        env = self.environments.get(env_id)
        if not env:
            return False, "Environment not found"

        cmd = [
            "docker",
            "compose",
            "-f",
            env.compose_file,
            "-p",
            "newsinsight",
            "logs",
            "--tail",
            str(tail),
            service,
        ]

        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=30,
                cwd=str(self.project_root),
            )
            return result.returncode == 0, result.stdout + result.stderr
        except subprocess.TimeoutExpired:
            return False, "Command timed out"
        except Exception as e:
            return False, str(e)

```

---

## backend/admin-dashboard/api/services/health_service.py

```py
"""
Service Health Monitoring Service
ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬ ë° ìƒíƒœ ëª¨ë‹ˆí„°ë§
"""

import asyncio
import os
from datetime import datetime
from pathlib import Path
from typing import Optional
import json

try:
    import httpx
except ImportError:
    httpx = None  # type: ignore

from ..models.schemas import (
    ServiceHealthStatus,
    ServiceHealth,
    InfrastructureHealth,
    OverallSystemHealth,
)


class HealthService:
    """ì„œë¹„ìŠ¤ í—¬ìŠ¤ ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤"""

    def __init__(self, project_root: str, config_dir: str):
        self.project_root = Path(project_root)
        self.config_dir = Path(config_dir)
        self.services_config_path = (
            self.project_root / "etc" / "configs" / "services.json"
        )
        self._services_config: Optional[dict] = None
        self._last_check: dict[str, ServiceHealth] = {}
        self.timeout = 5.0  # í—¬ìŠ¤ì²´í¬ íƒ€ì„ì•„ì›ƒ (ì´ˆ)

    def _load_services_config(self) -> dict:
        """ì„œë¹„ìŠ¤ ì„¤ì • ë¡œë“œ"""
        if self._services_config is None:
            if self.services_config_path.exists():
                with open(self.services_config_path, "r") as f:
                    self._services_config = json.load(f)
            else:
                self._services_config = {
                    "services": {},
                    "infrastructure": {},
                    "service_urls": {},
                }
        return self._services_config or {}

    def get_all_services(self) -> list[dict]:
        """ëª¨ë“  ì„œë¹„ìŠ¤ ëª©ë¡ ì¡°íšŒ"""
        config = self._load_services_config()
        services = []

        # ë©”ì¸ ì„œë¹„ìŠ¤
        for service_id, service_info in config.get("services", {}).items():
            services.append(
                {
                    "id": service_id,
                    "name": service_info.get("name", service_id),
                    "description": service_info.get("description", ""),
                    "port": service_info.get("port"),
                    "healthcheck": service_info.get("healthcheck", "/health"),
                    "hostname": service_info.get("hostname", service_id),
                    "type": "service",
                    "tags": service_info.get("consul", {}).get("tags", []),
                }
            )

        # ML ì• ë“œì˜¨
        for addon_id, addon_info in config.get("ml-addons", {}).items():
            services.append(
                {
                    "id": addon_id,
                    "name": addon_info.get("name", addon_id),
                    "description": f"ML Addon - {addon_info.get('name', addon_id)}",
                    "port": addon_info.get("port"),
                    "healthcheck": addon_info.get("healthcheck", "/health"),
                    "hostname": addon_id,
                    "type": "ml-addon",
                    "tags": ["ml", "addon"],
                }
            )

        return services

    def get_infrastructure_services(self) -> list[dict]:
        """ì¸í”„ë¼ ì„œë¹„ìŠ¤ ëª©ë¡ ì¡°íšŒ"""
        config = self._load_services_config()
        infra_services = []

        for infra_id, infra_info in config.get("infrastructure", {}).items():
            infra_services.append(
                {
                    "id": infra_id,
                    "name": infra_id.capitalize(),
                    "port": infra_info.get("port"),
                    "image": infra_info.get("image"),
                    "healthcheck": infra_info.get("healthcheck"),
                    "type": "infrastructure",
                }
            )

        return infra_services

    async def check_service_health(self, service_id: str) -> ServiceHealth:
        """ë‹¨ì¼ ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬"""
        config = self._load_services_config()
        service_urls = config.get("service_urls", {})

        # ì„œë¹„ìŠ¤ ì •ë³´ ì°¾ê¸°
        service_info = None
        for services_dict in [config.get("services", {}), config.get("ml-addons", {})]:
            if service_id in services_dict:
                service_info = services_dict[service_id]
                break

        if not service_info:
            return ServiceHealth(
                service_id=service_id,
                name=service_id,
                status=ServiceHealthStatus.UNKNOWN,
                message="Service not found in configuration",
                checked_at=datetime.utcnow(),
            )

        # ì„œë¹„ìŠ¤ URL ê²°ì •
        base_url = service_urls.get(service_id)
        if not base_url:
            hostname = service_info.get("hostname", service_id)
            port = service_info.get("port", service_info.get("api_port"))
            base_url = f"http://{hostname}:{port}"

        healthcheck_path = service_info.get("healthcheck", "/health")
        health_url = f"{base_url}{healthcheck_path}"

        if httpx is None:
            return ServiceHealth(
                service_id=service_id,
                name=service_info.get("name", service_id),
                status=ServiceHealthStatus.UNKNOWN,
                message="httpx not installed",
                url=health_url,
                checked_at=datetime.utcnow(),
            )

        # í—¬ìŠ¤ ì²´í¬ ìˆ˜í–‰
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                start_time = datetime.utcnow()
                response = await client.get(health_url)
                response_time_ms = (
                    datetime.utcnow() - start_time
                ).total_seconds() * 1000

                if response.status_code == 200:
                    status = ServiceHealthStatus.HEALTHY
                    message = "Service is healthy"
                    try:
                        health_data = response.json()
                    except Exception:
                        health_data = None
                elif response.status_code >= 500:
                    status = ServiceHealthStatus.UNHEALTHY
                    message = f"Server error: {response.status_code}"
                    health_data = None
                else:
                    status = ServiceHealthStatus.DEGRADED
                    message = f"Unexpected status: {response.status_code}"
                    health_data = None

                health = ServiceHealth(
                    service_id=service_id,
                    name=service_info.get("name", service_id),
                    status=status,
                    message=message,
                    response_time_ms=response_time_ms,
                    url=health_url,
                    checked_at=datetime.utcnow(),
                    details=health_data,
                )

        except Exception as e:
            error_type = type(e).__name__
            if "Timeout" in error_type:
                status = ServiceHealthStatus.UNHEALTHY
                message = "Connection timeout"
            elif "Connect" in error_type:
                status = ServiceHealthStatus.UNREACHABLE
                message = "Connection refused - service may be down"
            else:
                status = ServiceHealthStatus.UNKNOWN
                message = f"Error: {str(e)}"

            health = ServiceHealth(
                service_id=service_id,
                name=service_info.get("name", service_id),
                status=status,
                message=message,
                url=health_url,
                checked_at=datetime.utcnow(),
            )

        self._last_check[service_id] = health
        return health

    async def check_all_services_health(self) -> list[ServiceHealth]:
        """ëª¨ë“  ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬ (ë³‘ë ¬)"""
        services = self.get_all_services()

        # ë³‘ë ¬ë¡œ í—¬ìŠ¤ ì²´í¬ ìˆ˜í–‰
        tasks = [self.check_service_health(service["id"]) for service in services]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        health_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                health_results.append(
                    ServiceHealth(
                        service_id=services[i]["id"],
                        name=services[i]["name"],
                        status=ServiceHealthStatus.UNKNOWN,
                        message=f"Error: {str(result)}",
                        checked_at=datetime.utcnow(),
                    )
                )
            else:
                health_results.append(result)

        return health_results

    async def check_infrastructure_health(self) -> list[InfrastructureHealth]:
        """ì¸í”„ë¼ ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬"""
        infra_services = self.get_infrastructure_services()
        results = []

        for infra in infra_services:
            infra_id = infra["id"]
            port = infra["port"]

            # ì¸í”„ë¼ë³„ í—¬ìŠ¤ì²´í¬ ìˆ˜í–‰
            if infra_id == "postgres":
                health = await self._check_postgres(port)
            elif infra_id == "mongo":
                health = await self._check_mongo(port)
            elif infra_id == "redis":
                health = await self._check_redis(port)
            elif infra_id == "consul":
                health = await self._check_consul(port)
            elif infra_id == "redpanda":
                health = await self._check_redpanda(port)
            else:
                health = InfrastructureHealth(
                    service_id=infra_id,
                    name=infra["name"],
                    status=ServiceHealthStatus.UNKNOWN,
                    message="Unknown infrastructure type",
                    checked_at=datetime.utcnow(),
                )

            results.append(health)

        return results

    async def _check_postgres(self, port: int) -> InfrastructureHealth:
        """PostgreSQL í—¬ìŠ¤ ì²´í¬"""
        try:
            reader, writer = await asyncio.wait_for(
                asyncio.open_connection("postgres", port), timeout=self.timeout
            )
            writer.close()
            await writer.wait_closed()

            return InfrastructureHealth(
                service_id="postgres",
                name="PostgreSQL",
                status=ServiceHealthStatus.HEALTHY,
                message="PostgreSQL is accepting connections",
                port=port,
                checked_at=datetime.utcnow(),
            )
        except asyncio.TimeoutError:
            return InfrastructureHealth(
                service_id="postgres",
                name="PostgreSQL",
                status=ServiceHealthStatus.UNHEALTHY,
                message="Connection timeout",
                port=port,
                checked_at=datetime.utcnow(),
            )
        except Exception as e:
            return InfrastructureHealth(
                service_id="postgres",
                name="PostgreSQL",
                status=ServiceHealthStatus.UNREACHABLE,
                message=f"Connection failed: {str(e)}",
                port=port,
                checked_at=datetime.utcnow(),
            )

    async def _check_mongo(self, port: int) -> InfrastructureHealth:
        """MongoDB í—¬ìŠ¤ ì²´í¬"""
        try:
            reader, writer = await asyncio.wait_for(
                asyncio.open_connection("mongo", port), timeout=self.timeout
            )
            writer.close()
            await writer.wait_closed()

            return InfrastructureHealth(
                service_id="mongo",
                name="MongoDB",
                status=ServiceHealthStatus.HEALTHY,
                message="MongoDB is accepting connections",
                port=port,
                checked_at=datetime.utcnow(),
            )
        except Exception as e:
            return InfrastructureHealth(
                service_id="mongo",
                name="MongoDB",
                status=ServiceHealthStatus.UNREACHABLE,
                message=f"Connection failed: {str(e)}",
                port=port,
                checked_at=datetime.utcnow(),
            )

    async def _check_redis(self, port: int) -> InfrastructureHealth:
        """Redis í—¬ìŠ¤ ì²´í¬"""
        try:
            reader, writer = await asyncio.wait_for(
                asyncio.open_connection("redis", port), timeout=self.timeout
            )
            # Redis PING ëª…ë ¹
            writer.write(b"PING\r\n")
            await writer.drain()
            response = await asyncio.wait_for(reader.readline(), timeout=2.0)
            writer.close()
            await writer.wait_closed()

            if b"+PONG" in response:
                return InfrastructureHealth(
                    service_id="redis",
                    name="Redis",
                    status=ServiceHealthStatus.HEALTHY,
                    message="Redis is responding to PING",
                    port=port,
                    checked_at=datetime.utcnow(),
                )
            else:
                return InfrastructureHealth(
                    service_id="redis",
                    name="Redis",
                    status=ServiceHealthStatus.DEGRADED,
                    message="Redis connected but unexpected response",
                    port=port,
                    checked_at=datetime.utcnow(),
                )
        except Exception as e:
            return InfrastructureHealth(
                service_id="redis",
                name="Redis",
                status=ServiceHealthStatus.UNREACHABLE,
                message=f"Connection failed: {str(e)}",
                port=port,
                checked_at=datetime.utcnow(),
            )

    async def _check_consul(self, port: int) -> InfrastructureHealth:
        """Consul í—¬ìŠ¤ ì²´í¬"""
        if httpx is None:
            return InfrastructureHealth(
                service_id="consul",
                name="Consul",
                status=ServiceHealthStatus.UNKNOWN,
                message="httpx not installed",
                port=port,
                checked_at=datetime.utcnow(),
            )

        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.get(f"http://consul:{port}/v1/status/leader")

                if response.status_code == 200:
                    leader = response.text.strip('"')
                    return InfrastructureHealth(
                        service_id="consul",
                        name="Consul",
                        status=ServiceHealthStatus.HEALTHY,
                        message=f"Consul leader: {leader}",
                        port=port,
                        checked_at=datetime.utcnow(),
                        details={"leader": leader},
                    )
                else:
                    return InfrastructureHealth(
                        service_id="consul",
                        name="Consul",
                        status=ServiceHealthStatus.DEGRADED,
                        message=f"Consul returned {response.status_code}",
                        port=port,
                        checked_at=datetime.utcnow(),
                    )
        except Exception as e:
            return InfrastructureHealth(
                service_id="consul",
                name="Consul",
                status=ServiceHealthStatus.UNREACHABLE,
                message=f"Connection failed: {str(e)}",
                port=port,
                checked_at=datetime.utcnow(),
            )

    async def _check_redpanda(self, port: int) -> InfrastructureHealth:
        """Redpanda/Kafka í—¬ìŠ¤ ì²´í¬"""
        if httpx is None:
            return InfrastructureHealth(
                service_id="redpanda",
                name="Redpanda",
                status=ServiceHealthStatus.UNKNOWN,
                message="httpx not installed",
                port=port,
                checked_at=datetime.utcnow(),
            )

        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.get("http://redpanda:9644/v1/status/ready")

                if response.status_code == 200:
                    return InfrastructureHealth(
                        service_id="redpanda",
                        name="Redpanda",
                        status=ServiceHealthStatus.HEALTHY,
                        message="Redpanda is ready",
                        port=port,
                        checked_at=datetime.utcnow(),
                    )
                else:
                    return InfrastructureHealth(
                        service_id="redpanda",
                        name="Redpanda",
                        status=ServiceHealthStatus.DEGRADED,
                        message=f"Redpanda returned {response.status_code}",
                        port=port,
                        checked_at=datetime.utcnow(),
                    )
        except Exception as e:
            return InfrastructureHealth(
                service_id="redpanda",
                name="Redpanda",
                status=ServiceHealthStatus.UNREACHABLE,
                message=f"Connection failed: {str(e)}",
                port=port,
                checked_at=datetime.utcnow(),
            )

    async def get_overall_health(self) -> OverallSystemHealth:
        """ì „ì²´ ì‹œìŠ¤í…œ í—¬ìŠ¤ ìƒíƒœ ìš”ì•½"""
        services_health = await self.check_all_services_health()
        infra_health = await self.check_infrastructure_health()

        # í†µê³„ ê³„ì‚°
        total_services = len(services_health)
        healthy_services = sum(
            1 for s in services_health if s.status == ServiceHealthStatus.HEALTHY
        )
        unhealthy_services = sum(
            1
            for s in services_health
            if s.status
            in [ServiceHealthStatus.UNHEALTHY, ServiceHealthStatus.UNREACHABLE]
        )
        degraded_services = sum(
            1 for s in services_health if s.status == ServiceHealthStatus.DEGRADED
        )

        total_infra = len(infra_health)
        healthy_infra = sum(
            1 for i in infra_health if i.status == ServiceHealthStatus.HEALTHY
        )

        # ì „ì²´ ìƒíƒœ ê²°ì •
        if unhealthy_services > 0 or healthy_infra < total_infra:
            if healthy_services == 0:
                overall_status = ServiceHealthStatus.UNHEALTHY
            else:
                overall_status = ServiceHealthStatus.DEGRADED
        elif degraded_services > 0:
            overall_status = ServiceHealthStatus.DEGRADED
        else:
            overall_status = ServiceHealthStatus.HEALTHY

        # í‰ê·  ì‘ë‹µ ì‹œê°„ ê³„ì‚°
        response_times = [
            s.response_time_ms
            for s in services_health
            if s.response_time_ms is not None
        ]
        avg_response_time = (
            sum(response_times) / len(response_times) if response_times else None
        )

        return OverallSystemHealth(
            status=overall_status,
            total_services=total_services,
            healthy_services=healthy_services,
            unhealthy_services=unhealthy_services,
            degraded_services=degraded_services,
            total_infrastructure=total_infra,
            healthy_infrastructure=healthy_infra,
            average_response_time_ms=avg_response_time,
            services=services_health,
            infrastructure=infra_health,
            checked_at=datetime.utcnow(),
        )

    def get_last_check(self, service_id: str) -> Optional[ServiceHealth]:
        """ë§ˆì§€ë§‰ í—¬ìŠ¤ ì²´í¬ ê²°ê³¼ ì¡°íšŒ"""
        return self._last_check.get(service_id)

```

---

## backend/admin-dashboard/api/services/kafka_service.py

```py
"""
Kafka/Redpanda Monitoring Service
Kafka/Redpanda í´ëŸ¬ìŠ¤í„° ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤
"""

import os
from datetime import datetime
from pathlib import Path
from typing import Optional

try:
    import httpx
except ImportError:
    httpx = None  # type: ignore

from ..models.schemas import (
    KafkaTopicInfo,
    KafkaConsumerGroupInfo,
    KafkaClusterInfo,
)


class KafkaService:
    """Kafka/Redpanda ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤"""

    def __init__(self, project_root: str, config_dir: str):
        self.project_root = Path(project_root)
        self.config_dir = Path(config_dir)

        # Redpanda Admin API ì„¤ì • (Kafka-compatible)
        self.redpanda_host = os.environ.get("REDPANDA_HOST", "redpanda")
        self.redpanda_admin_port = int(os.environ.get("REDPANDA_ADMIN_PORT", "9644"))
        self.redpanda_kafka_port = int(os.environ.get("REDPANDA_KAFKA_PORT", "9092"))

        self.admin_api_base = f"http://{self.redpanda_host}:{self.redpanda_admin_port}"
        self.timeout = 10.0

    async def get_cluster_info(self) -> KafkaClusterInfo:
        """í´ëŸ¬ìŠ¤í„° ì „ì²´ ì •ë³´ ì¡°íšŒ"""
        topics = await self.list_topics()
        consumer_groups = await self.list_consumer_groups()

        total_partitions = sum(t.partition_count for t in topics)
        total_messages = None

        # ë¸Œë¡œì»¤ ì •ë³´ ì¡°íšŒ ì‹œë„
        broker_count = 1  # ê¸°ë³¸ê°’
        controller_id = None
        cluster_id = None

        try:
            if httpx:
                async with httpx.AsyncClient(timeout=self.timeout) as client:
                    # Redpanda Admin API - brokers
                    response = await client.get(f"{self.admin_api_base}/v1/brokers")
                    if response.status_code == 200:
                        brokers = response.json()
                        broker_count = len(brokers) if isinstance(brokers, list) else 1

                    # Redpanda Admin API - cluster health
                    response = await client.get(
                        f"{self.admin_api_base}/v1/cluster/health_overview"
                    )
                    if response.status_code == 200:
                        health = response.json()
                        controller_id = health.get("controller_id")
        except Exception:
            pass

        return KafkaClusterInfo(
            broker_count=broker_count,
            controller_id=controller_id,
            cluster_id=cluster_id,
            topics=topics,
            consumer_groups=consumer_groups,
            total_topics=len(topics),
            total_partitions=total_partitions,
            total_messages=total_messages,
            checked_at=datetime.utcnow(),
        )

    async def list_topics(self) -> list[KafkaTopicInfo]:
        """ëª¨ë“  í† í”½ ëª©ë¡ ì¡°íšŒ"""
        topics = []

        try:
            if httpx:
                async with httpx.AsyncClient(timeout=self.timeout) as client:
                    # Redpanda Admin API - topics
                    response = await client.get(f"{self.admin_api_base}/v1/topics")
                    if response.status_code == 200:
                        topic_list = response.json()
                        for topic_data in topic_list:
                            if isinstance(topic_data, dict):
                                topics.append(
                                    KafkaTopicInfo(
                                        name=topic_data.get(
                                            "topic", topic_data.get("name", "unknown")
                                        ),
                                        partition_count=topic_data.get(
                                            "partition_count", 1
                                        ),
                                        replication_factor=topic_data.get(
                                            "replication_factor", 1
                                        ),
                                        message_count=topic_data.get("message_count"),
                                        size_bytes=topic_data.get("size_bytes"),
                                        retention_ms=topic_data.get("retention_ms"),
                                        is_internal=topic_data.get(
                                            "is_internal", False
                                        ),
                                    )
                                )
                            elif isinstance(topic_data, str):
                                # í† í”½ ì´ë¦„ë§Œ ë°˜í™˜ë˜ëŠ” ê²½ìš°
                                topic_detail = await self.get_topic_detail(topic_data)
                                if topic_detail:
                                    topics.append(topic_detail)
        except Exception as e:
            # ì—°ê²° ì‹¤íŒ¨ ì‹œ ìƒ˜í”Œ ë°ì´í„° ë°˜í™˜
            topics = [
                KafkaTopicInfo(
                    name="news-raw",
                    partition_count=3,
                    replication_factor=1,
                    is_internal=False,
                ),
                KafkaTopicInfo(
                    name="news-processed",
                    partition_count=3,
                    replication_factor=1,
                    is_internal=False,
                ),
                KafkaTopicInfo(
                    name="crawl-jobs",
                    partition_count=1,
                    replication_factor=1,
                    is_internal=False,
                ),
            ]

        return topics

    async def get_topic_detail(self, topic_name: str) -> Optional[KafkaTopicInfo]:
        """íŠ¹ì • í† í”½ ìƒì„¸ ì •ë³´"""
        try:
            if httpx:
                async with httpx.AsyncClient(timeout=self.timeout) as client:
                    response = await client.get(
                        f"{self.admin_api_base}/v1/topics/{topic_name}"
                    )
                    if response.status_code == 200:
                        data = response.json()
                        return KafkaTopicInfo(
                            name=data.get("topic", topic_name),
                            partition_count=len(data.get("partitions", []))
                            or data.get("partition_count", 1),
                            replication_factor=data.get("replication_factor", 1),
                            message_count=data.get("message_count"),
                            size_bytes=data.get("size_bytes"),
                            retention_ms=data.get("retention_ms"),
                            is_internal=data.get(
                                "is_internal", topic_name.startswith("_")
                            ),
                        )
        except Exception:
            pass

        return KafkaTopicInfo(
            name=topic_name,
            partition_count=1,
            replication_factor=1,
            is_internal=topic_name.startswith("_"),
        )

    async def list_consumer_groups(self) -> list[KafkaConsumerGroupInfo]:
        """ëª¨ë“  ì»¨ìŠˆë¨¸ ê·¸ë£¹ ì¡°íšŒ"""
        groups = []

        try:
            if httpx:
                async with httpx.AsyncClient(timeout=self.timeout) as client:
                    # Redpanda Admin API - consumer groups
                    response = await client.get(
                        f"{self.admin_api_base}/v1/consumer_groups"
                    )
                    if response.status_code == 200:
                        group_list = response.json()
                        for group_data in group_list:
                            if isinstance(group_data, dict):
                                groups.append(
                                    KafkaConsumerGroupInfo(
                                        group_id=group_data.get(
                                            "group_id",
                                            group_data.get("name", "unknown"),
                                        ),
                                        state=group_data.get("state", "Unknown"),
                                        members_count=group_data.get(
                                            "members_count",
                                            len(group_data.get("members", [])),
                                        ),
                                        topics=group_data.get("topics", []),
                                        total_lag=group_data.get("total_lag", 0),
                                        lag_per_partition=group_data.get(
                                            "lag_per_partition", {}
                                        ),
                                    )
                                )
                            elif isinstance(group_data, str):
                                # ê·¸ë£¹ IDë§Œ ë°˜í™˜ë˜ëŠ” ê²½ìš°
                                group_detail = await self.get_consumer_group_detail(
                                    group_data
                                )
                                if group_detail:
                                    groups.append(group_detail)
        except Exception as e:
            # ì—°ê²° ì‹¤íŒ¨ ì‹œ ìƒ˜í”Œ ë°ì´í„° ë°˜í™˜
            groups = [
                KafkaConsumerGroupInfo(
                    group_id="news-processor",
                    state="Stable",
                    members_count=2,
                    topics=["news-raw"],
                    total_lag=0,
                    lag_per_partition={},
                ),
                KafkaConsumerGroupInfo(
                    group_id="crawler-consumer",
                    state="Stable",
                    members_count=1,
                    topics=["crawl-jobs"],
                    total_lag=0,
                    lag_per_partition={},
                ),
            ]

        return groups

    async def get_consumer_group_detail(
        self, group_id: str
    ) -> Optional[KafkaConsumerGroupInfo]:
        """íŠ¹ì • ì»¨ìŠˆë¨¸ ê·¸ë£¹ ìƒì„¸ ì •ë³´"""
        try:
            if httpx:
                async with httpx.AsyncClient(timeout=self.timeout) as client:
                    response = await client.get(
                        f"{self.admin_api_base}/v1/consumer_groups/{group_id}"
                    )
                    if response.status_code == 200:
                        data = response.json()
                        return KafkaConsumerGroupInfo(
                            group_id=data.get("group_id", group_id),
                            state=data.get("state", "Unknown"),
                            members_count=len(data.get("members", [])),
                            topics=data.get("topics", []),
                            total_lag=data.get("total_lag", 0),
                            lag_per_partition=data.get("lag_per_partition", {}),
                        )
        except Exception:
            pass

        return KafkaConsumerGroupInfo(
            group_id=group_id,
            state="Unknown",
            members_count=0,
            topics=[],
            total_lag=0,
            lag_per_partition={},
        )

    async def check_health(self) -> dict:
        """Kafka/Redpanda í—¬ìŠ¤ ì²´í¬"""
        import asyncio

        try:
            # TCP ì—°ê²° ì²´í¬
            reader, writer = await asyncio.wait_for(
                asyncio.open_connection(self.redpanda_host, self.redpanda_kafka_port),
                timeout=5.0,
            )
            writer.close()
            await writer.wait_closed()

            return {
                "status": "healthy",
                "host": self.redpanda_host,
                "kafka_port": self.redpanda_kafka_port,
                "admin_port": self.redpanda_admin_port,
                "checked_at": datetime.utcnow().isoformat(),
            }
        except Exception as e:
            return {
                "status": "unreachable",
                "host": self.redpanda_host,
                "kafka_port": self.redpanda_kafka_port,
                "admin_port": self.redpanda_admin_port,
                "error": str(e),
                "checked_at": datetime.utcnow().isoformat(),
            }

```

---

## backend/admin-dashboard/api/services/script_service.py

```py
"""
Script Service - ìŠ¤í¬ë¦½íŠ¸/ì‘ì—… ê´€ë¦¬ ë° ì‹¤í–‰ ì„œë¹„ìŠ¤
"""
import asyncio
import os
import signal
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Any, AsyncGenerator, Optional
from uuid import uuid4

import yaml

from ..models.schemas import (
    RiskLevel,
    Script,
    ScriptCreate,
    ScriptParameter,
    ScriptUpdate,
    TaskExecution,
    TaskLog,
    TaskStatus,
    UserRole,
)


class ScriptService:
    """ìŠ¤í¬ë¦½íŠ¸ ê´€ë¦¬ ë° ì‹¤í–‰ ì„œë¹„ìŠ¤"""

    def __init__(self, project_root: str, config_dir: str):
        self.project_root = Path(project_root)
        self.config_dir = Path(config_dir)
        self.scripts: dict[str, Script] = {}
        self.executions: dict[str, TaskExecution] = {}
        self.running_processes: dict[str, subprocess.Popen] = {}
        self._load_scripts()

    def _load_scripts(self) -> None:
        """ì„¤ì • íŒŒì¼ì—ì„œ ìŠ¤í¬ë¦½íŠ¸ ì •ë³´ ë¡œë“œ"""
        config_file = self.config_dir / "scripts.yaml"
        if config_file.exists():
            with open(config_file) as f:
                data = yaml.safe_load(f) or {}
                for script_data in data.get("scripts", []):
                    # parametersë¥¼ ScriptParameter ê°ì²´ë¡œ ë³€í™˜
                    if "parameters" in script_data:
                        script_data["parameters"] = [
                            ScriptParameter(**p) if isinstance(p, dict) else p
                            for p in script_data["parameters"]
                        ]
                    script = Script(**script_data)
                    self.scripts[script.id] = script
        else:
            self._create_default_scripts()

    def _create_default_scripts(self) -> None:
        """ê¸°ë³¸ ìŠ¤í¬ë¦½íŠ¸ ì„¤ì • ìƒì„±"""
        scripts_dir = self.project_root / "scripts"
        now = datetime.utcnow()

        default_scripts = [
            {
                "id": "script-start",
                "name": "ì„œë¹„ìŠ¤ ì‹œì‘",
                "description": "ì„ íƒí•œ í™˜ê²½ì˜ Docker Compose ì„œë¹„ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.",
                "command": "docker compose -f {compose_file} -p newsinsight up -d",
                "working_dir": str(self.project_root),
                "risk_level": RiskLevel.LOW,
                "estimated_duration": 120,
                "allowed_environments": ["zerotrust", "local", "production", "staging"],
                "required_role": UserRole.OPERATOR,
                "parameters": [
                    ScriptParameter(
                        name="build",
                        param_type="boolean",
                        required=False,
                        default=True,
                        description="ì´ë¯¸ì§€ ë¹Œë“œ ì—¬ë¶€",
                    ),
                ],
                "tags": ["docker", "deploy"],
                "created_at": now,
                "updated_at": now,
            },
            {
                "id": "script-stop",
                "name": "ì„œë¹„ìŠ¤ ì¤‘ì§€",
                "description": "ì„ íƒí•œ í™˜ê²½ì˜ Docker Compose ì„œë¹„ìŠ¤ë¥¼ ì¤‘ì§€í•©ë‹ˆë‹¤.",
                "command": "docker compose -f {compose_file} -p newsinsight down",
                "working_dir": str(self.project_root),
                "risk_level": RiskLevel.MEDIUM,
                "estimated_duration": 60,
                "allowed_environments": ["zerotrust", "local", "production", "staging"],
                "required_role": UserRole.OPERATOR,
                "parameters": [],
                "tags": ["docker", "stop"],
                "created_at": now,
                "updated_at": now,
            },
            {
                "id": "script-restart",
                "name": "ì„œë¹„ìŠ¤ ì¬ì‹œì‘",
                "description": "ì„ íƒí•œ í™˜ê²½ì˜ Docker Compose ì„œë¹„ìŠ¤ë¥¼ ì¬ì‹œì‘í•©ë‹ˆë‹¤.",
                "command": "docker compose -f {compose_file} -p newsinsight restart {service}",
                "working_dir": str(self.project_root),
                "risk_level": RiskLevel.MEDIUM,
                "estimated_duration": 60,
                "allowed_environments": ["zerotrust", "local", "production", "staging"],
                "required_role": UserRole.OPERATOR,
                "parameters": [
                    ScriptParameter(
                        name="service",
                        param_type="string",
                        required=False,
                        default="",
                        description="ì¬ì‹œì‘í•  ì„œë¹„ìŠ¤ ì´ë¦„ (ë¹„ì›Œë‘ë©´ ì „ì²´ ì¬ì‹œì‘)",
                    ),
                ],
                "tags": ["docker", "restart"],
                "created_at": now,
                "updated_at": now,
            },
            {
                "id": "script-full-cleanup",
                "name": "ì „ì²´ ì •ë¦¬ (Full Cleanup)",
                "description": "ì»¨í…Œì´ë„ˆ, ë³¼ë¥¨, ì´ë¯¸ì§€, ìºì‹œë¥¼ ëª¨ë‘ ì •ë¦¬í•©ë‹ˆë‹¤. âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ ë³¼ë¥¨ë„ ì‚­ì œë©ë‹ˆë‹¤!",
                "command": """docker compose -f {compose_file} -p newsinsight down -v && \
docker builder prune -f && \
docker image prune -f && \
docker volume prune -f""",
                "working_dir": str(self.project_root),
                "risk_level": RiskLevel.CRITICAL,
                "estimated_duration": 180,
                "allowed_environments": ["local", "staging"],
                "required_role": UserRole.ADMIN,
                "parameters": [],
                "tags": ["docker", "cleanup", "dangerous"],
                "created_at": now,
                "updated_at": now,
            },
            {
                "id": "script-status",
                "name": "ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸",
                "description": "í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ì»¨í…Œì´ë„ˆ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.",
                "command": "docker compose -f {compose_file} -p newsinsight ps -a",
                "working_dir": str(self.project_root),
                "risk_level": RiskLevel.LOW,
                "estimated_duration": 5,
                "allowed_environments": ["zerotrust", "local", "production", "staging"],
                "required_role": UserRole.VIEWER,
                "parameters": [],
                "tags": ["docker", "status"],
                "created_at": now,
                "updated_at": now,
            },
            {
                "id": "script-logs",
                "name": "ì„œë¹„ìŠ¤ ë¡œê·¸ ì¡°íšŒ",
                "description": "íŠ¹ì • ì„œë¹„ìŠ¤ì˜ ë¡œê·¸ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.",
                "command": "docker compose -f {compose_file} -p newsinsight logs --tail {tail} {service}",
                "working_dir": str(self.project_root),
                "risk_level": RiskLevel.LOW,
                "estimated_duration": 10,
                "allowed_environments": ["zerotrust", "local", "production", "staging"],
                "required_role": UserRole.VIEWER,
                "parameters": [
                    ScriptParameter(
                        name="service",
                        param_type="string",
                        required=True,
                        description="ë¡œê·¸ë¥¼ ì¡°íšŒí•  ì„œë¹„ìŠ¤ ì´ë¦„",
                    ),
                    ScriptParameter(
                        name="tail",
                        param_type="number",
                        required=False,
                        default=100,
                        description="ì¶œë ¥í•  ë¡œê·¸ ì¤„ ìˆ˜",
                    ),
                ],
                "tags": ["docker", "logs"],
                "created_at": now,
                "updated_at": now,
            },
            {
                "id": "script-build-push",
                "name": "ì´ë¯¸ì§€ ë¹Œë“œ ë° í‘¸ì‹œ",
                "description": "Docker ì´ë¯¸ì§€ë¥¼ ë¹Œë“œí•˜ê³  ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— í‘¸ì‹œí•©ë‹ˆë‹¤.",
                "command": str(scripts_dir / "build-and-push.sh"),
                "working_dir": str(self.project_root),
                "risk_level": RiskLevel.MEDIUM,
                "estimated_duration": 300,
                "allowed_environments": ["production", "staging"],
                "required_role": UserRole.OPERATOR,
                "parameters": [
                    ScriptParameter(
                        name="tag",
                        param_type="string",
                        required=False,
                        default="latest",
                        description="ì´ë¯¸ì§€ íƒœê·¸",
                    ),
                ],
                "tags": ["docker", "build", "ci"],
                "created_at": now,
                "updated_at": now,
            },
            {
                "id": "script-health-check",
                "name": "í—¬ìŠ¤ì²´í¬",
                "description": "ëª¨ë“  ì„œë¹„ìŠ¤ì˜ í—¬ìŠ¤ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.",
                "command": """docker compose -f {compose_file} -p newsinsight ps --format json | \
python3 -c "import sys,json; [print(f'{json.loads(l).get(\"Name\")}: {json.loads(l).get(\"Health\", \"N/A\")}') for l in sys.stdin if l.strip()]" """,
                "working_dir": str(self.project_root),
                "risk_level": RiskLevel.LOW,
                "estimated_duration": 10,
                "allowed_environments": ["zerotrust", "local", "production", "staging"],
                "required_role": UserRole.VIEWER,
                "parameters": [],
                "tags": ["health", "monitoring"],
                "created_at": now,
                "updated_at": now,
            },
        ]

        for script_data in default_scripts:
            script = Script(**script_data)
            self.scripts[script.id] = script

        self._save_scripts()

    def _save_scripts(self) -> None:
        """ìŠ¤í¬ë¦½íŠ¸ ì„¤ì •ì„ íŒŒì¼ì— ì €ì¥"""
        self.config_dir.mkdir(parents=True, exist_ok=True)
        config_file = self.config_dir / "scripts.yaml"

        data = {
            "scripts": [
                script.model_dump(mode="json") for script in self.scripts.values()
            ]
        }

        with open(config_file, "w") as f:
            yaml.dump(data, f, default_flow_style=False, allow_unicode=True)

    def list_scripts(
        self,
        environment: Optional[str] = None,
        tag: Optional[str] = None,
        role: Optional[UserRole] = None,
    ) -> list[Script]:
        """ìŠ¤í¬ë¦½íŠ¸ ëª©ë¡ ì¡°íšŒ"""
        scripts = list(self.scripts.values())

        if environment:
            scripts = [
                s
                for s in scripts
                if not s.allowed_environments or environment in s.allowed_environments
            ]

        if tag:
            scripts = [s for s in scripts if tag in s.tags]

        if role:
            role_priority = {UserRole.VIEWER: 0, UserRole.OPERATOR: 1, UserRole.ADMIN: 2}
            user_level = role_priority.get(role, 0)
            scripts = [
                s for s in scripts if role_priority.get(s.required_role, 0) <= user_level
            ]

        return scripts

    def get_script(self, script_id: str) -> Optional[Script]:
        """ìŠ¤í¬ë¦½íŠ¸ ìƒì„¸ ì¡°íšŒ"""
        return self.scripts.get(script_id)

    def create_script(self, data: ScriptCreate) -> Script:
        """ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
        script_id = f"script-{uuid4().hex[:8]}"
        now = datetime.utcnow()

        script = Script(
            id=script_id,
            created_at=now,
            updated_at=now,
            **data.model_dump(),
        )

        self.scripts[script_id] = script
        self._save_scripts()
        return script

    def update_script(self, script_id: str, data: ScriptUpdate) -> Optional[Script]:
        """ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì •"""
        script = self.scripts.get(script_id)
        if not script:
            return None

        update_data = data.model_dump(exclude_unset=True)
        for key, value in update_data.items():
            setattr(script, key, value)

        script.updated_at = datetime.utcnow()
        self._save_scripts()
        return script

    def delete_script(self, script_id: str) -> bool:
        """ìŠ¤í¬ë¦½íŠ¸ ì‚­ì œ"""
        if script_id in self.scripts:
            del self.scripts[script_id]
            self._save_scripts()
            return True
        return False

    async def execute_script(
        self,
        script_id: str,
        environment_name: str,
        compose_file: str,
        parameters: dict[str, Any],
        executed_by: str,
    ) -> TaskExecution:
        """ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰"""
        script = self.scripts.get(script_id)
        if not script:
            raise ValueError(f"Script not found: {script_id}")

        execution_id = f"exec-{uuid4().hex[:8]}"
        now = datetime.utcnow()

        # ëª…ë ¹ì–´ í…œí”Œë¦¿ ì¹˜í™˜
        command = script.command.format(
            compose_file=compose_file,
            **parameters,
        )

        execution = TaskExecution(
            id=execution_id,
            script_id=script_id,
            script_name=script.name,
            environment_id=environment_name,
            environment_name=environment_name,
            status=TaskStatus.RUNNING,
            parameters=parameters,
            started_at=now,
            executed_by=executed_by,
        )

        self.executions[execution_id] = execution

        # ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
        asyncio.create_task(
            self._run_command(execution_id, command, script.working_dir)
        )

        return execution

    async def _run_command(
        self, execution_id: str, command: str, working_dir: Optional[str]
    ) -> None:
        """ëª…ë ¹ì–´ ì‹¤í–‰ (ë¹„ë™ê¸°)"""
        execution = self.executions.get(execution_id)
        if not execution:
            return

        try:
            process = await asyncio.create_subprocess_shell(
                command,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.STDOUT,
                cwd=working_dir,
            )

            self.running_processes[execution_id] = process

            # ì¶œë ¥ ìˆ˜ì§‘
            stdout, _ = await process.communicate()

            execution.exit_code = process.returncode
            execution.status = (
                TaskStatus.SUCCESS if process.returncode == 0 else TaskStatus.FAILED
            )
            execution.finished_at = datetime.utcnow()

            if process.returncode != 0:
                execution.error_message = stdout.decode() if stdout else "Unknown error"

        except Exception as e:
            execution.status = TaskStatus.FAILED
            execution.error_message = str(e)
            execution.finished_at = datetime.utcnow()
        finally:
            if execution_id in self.running_processes:
                del self.running_processes[execution_id]

    async def stream_execution_output(
        self,
        script_id: str,
        environment_name: str,
        compose_file: str,
        parameters: dict[str, Any],
        executed_by: str,
    ) -> AsyncGenerator[str, None]:
        """ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ë° ì¶œë ¥ ìŠ¤íŠ¸ë¦¬ë°"""
        script = self.scripts.get(script_id)
        if not script:
            yield f"Error: Script not found: {script_id}\n"
            return

        execution_id = f"exec-{uuid4().hex[:8]}"
        now = datetime.utcnow()

        # ëª…ë ¹ì–´ í…œí”Œë¦¿ ì¹˜í™˜
        command = script.command.format(
            compose_file=compose_file,
            **parameters,
        )

        execution = TaskExecution(
            id=execution_id,
            script_id=script_id,
            script_name=script.name,
            environment_id=environment_name,
            environment_name=environment_name,
            status=TaskStatus.RUNNING,
            parameters=parameters,
            started_at=now,
            executed_by=executed_by,
        )

        self.executions[execution_id] = execution

        yield f"[{now.isoformat()}] Starting: {script.name}\n"
        yield f"[{now.isoformat()}] Command: {command}\n"
        yield f"[{now.isoformat()}] Working dir: {script.working_dir}\n"
        yield "-" * 60 + "\n"

        try:
            process = await asyncio.create_subprocess_shell(
                command,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.STDOUT,
                cwd=script.working_dir,
            )

            self.running_processes[execution_id] = process

            # ì‹¤ì‹œê°„ ì¶œë ¥ ìŠ¤íŠ¸ë¦¬ë°
            async for line in process.stdout:
                yield line.decode()

            await process.wait()

            execution.exit_code = process.returncode
            execution.status = (
                TaskStatus.SUCCESS if process.returncode == 0 else TaskStatus.FAILED
            )
            execution.finished_at = datetime.utcnow()

            yield "-" * 60 + "\n"
            yield f"[{execution.finished_at.isoformat()}] Finished with exit code: {process.returncode}\n"
            yield f"[{execution.finished_at.isoformat()}] Status: {execution.status.value}\n"

        except Exception as e:
            execution.status = TaskStatus.FAILED
            execution.error_message = str(e)
            execution.finished_at = datetime.utcnow()
            yield f"[ERROR] {str(e)}\n"
        finally:
            if execution_id in self.running_processes:
                del self.running_processes[execution_id]

    def cancel_execution(self, execution_id: str) -> bool:
        """ì‹¤í–‰ ì¤‘ì¸ ì‘ì—… ì·¨ì†Œ"""
        process = self.running_processes.get(execution_id)
        if process:
            try:
                process.terminate()
                execution = self.executions.get(execution_id)
                if execution:
                    execution.status = TaskStatus.CANCELLED
                    execution.finished_at = datetime.utcnow()
                return True
            except Exception:
                return False
        return False

    def get_execution(self, execution_id: str) -> Optional[TaskExecution]:
        """ì‹¤í–‰ ì •ë³´ ì¡°íšŒ"""
        return self.executions.get(execution_id)

    def list_executions(
        self,
        script_id: Optional[str] = None,
        environment_id: Optional[str] = None,
        status: Optional[TaskStatus] = None,
        limit: int = 50,
    ) -> list[TaskExecution]:
        """ì‹¤í–‰ ì´ë ¥ ì¡°íšŒ"""
        executions = list(self.executions.values())

        if script_id:
            executions = [e for e in executions if e.script_id == script_id]

        if environment_id:
            executions = [e for e in executions if e.environment_id == environment_id]

        if status:
            executions = [e for e in executions if e.status == status]

        # ìµœì‹ ìˆœ ì •ë ¬
        executions.sort(key=lambda x: x.started_at, reverse=True)

        return executions[:limit]

```

---

## backend/admin-dashboard/docker-compose.yml

```yml
# Admin Dashboard Docker Compose
# ê°œë°œ ë° ìš´ì˜ í™˜ê²½ì—ì„œ Admin Dashboardë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

services:
  admin-api:
    build:
      context: .
      dockerfile: Dockerfile
    image: newsinsight/admin-dashboard:local
    container_name: newsinsight-admin-api
    restart: unless-stopped
    environment:
      - PORT=8889
      - PROJECT_ROOT=/workspace
      - ADMIN_CONFIG_DIR=/app/config
      - ADMIN_SECRET_KEY=${ADMIN_SECRET_KEY:-change-this-secret-key-in-production}
      - CORS_ORIGINS=http://localhost:3001,http://localhost:8889
    ports:
      - "8889:8889"
    volumes:
      # í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ë§ˆìš´íŠ¸í•˜ì—¬ docker compose ëª…ë ¹ ì‹¤í–‰ ê°€ëŠ¥
      - ../../:/workspace:ro
      # Docker ì†Œì¼“ ë§ˆìš´íŠ¸ (ì»¨í…Œì´ë„ˆ ê´€ë¦¬ìš©)
      - /var/run/docker.sock:/var/run/docker.sock
      # ì„¤ì • íŒŒì¼ ì˜ì†í™”
      - admin-config:/app/config
    networks:
      - admin-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8889/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  admin-web:
    image: node:20-alpine
    container_name: newsinsight-admin-web
    working_dir: /app
    volumes:
      - ./web:/app
      - admin-web-node-modules:/app/node_modules
    environment:
      - VITE_API_URL=http://admin-api:8889/api/v1/admin
    command: >
      sh -c "npm install && npm run dev -- --host 0.0.0.0"
    ports:
      - "3001:3001"
    depends_on:
      - admin-api
    networks:
      - admin-net

volumes:
  admin-config:
  admin-web-node-modules:

networks:
  admin-net:
    name: newsinsight-admin-net

```

---

## backend/admin-dashboard/web/eslint.config.js

```js
import js from '@eslint/js'
import globals from 'globals'
import reactHooks from 'eslint-plugin-react-hooks'
import reactRefresh from 'eslint-plugin-react-refresh'
import tseslint from 'typescript-eslint'
import { defineConfig, globalIgnores } from 'eslint/config'

export default defineConfig([
  globalIgnores(['dist']),
  {
    files: ['**/*.{ts,tsx}'],
    extends: [
      js.configs.recommended,
      tseslint.configs.recommended,
      reactHooks.configs.flat.recommended,
      reactRefresh.configs.vite,
    ],
    languageOptions: {
      ecmaVersion: 2020,
      globals: globals.browser,
    },
  },
])

```

---

## backend/admin-dashboard/web/index.html

```html
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/vite.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>web</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.tsx"></script>
  </body>
</html>

```

---

## backend/admin-dashboard/web/package-lock.json

```json
{
  "name": "admin-dashboard-web",
  "version": "1.0.0",
  "lockfileVersion": 3,
  "requires": true,
  "packages": {
    "": {
      "name": "admin-dashboard-web",
      "version": "1.0.0",
      "dependencies": {
        "@tanstack/react-query": "^5.80.6",
        "clsx": "^2.1.1",
        "date-fns": "^4.1.0",
        "lucide-react": "^0.511.0",
        "react": "^19.2.0",
        "react-dom": "^19.2.0",
        "react-markdown": "^10.1.0",
        "react-router-dom": "^7.6.1",
        "react-syntax-highlighter": "^15.6.1"
      },
      "devDependencies": {
        "@eslint/js": "^9.39.1",
        "@types/node": "^24.10.1",
        "@types/react": "^19.2.7",
        "@types/react-dom": "^19.2.3",
        "@vitejs/plugin-react": "^5.1.1",
        "eslint": "^9.39.1",
        "eslint-plugin-react-hooks": "^7.0.1",
        "eslint-plugin-react-refresh": "^0.4.24",
        "globals": "^16.5.0",
        "typescript": "~5.9.3",
        "typescript-eslint": "^8.46.4",
        "vite": "^7.2.4"
      }
    },
    "node_modules/@babel/code-frame": {
      "version": "7.27.1",
      "resolved": "https://registry.npmjs.org/@babel/code-frame/-/code-frame-7.27.1.tgz",
      "integrity": "sha512-cjQ7ZlQ0Mv3b47hABuTevyTuYN4i+loJKGeV9flcCgIK37cCXRh+L1bd3iBHlynerhQ7BhCkn2BPbQUL+rGqFg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-validator-identifier": "^7.27.1",
        "js-tokens": "^4.0.0",
        "picocolors": "^1.1.1"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/compat-data": {
      "version": "7.28.5",
      "resolved": "https://registry.npmjs.org/@babel/compat-data/-/compat-data-7.28.5.tgz",
      "integrity": "sha512-6uFXyCayocRbqhZOB+6XcuZbkMNimwfVGFji8CTZnCzOHVGvDqzvitu1re2AU5LROliz7eQPhB8CpAMvnx9EjA==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/core": {
      "version": "7.28.5",
      "resolved": "https://registry.npmjs.org/@babel/core/-/core-7.28.5.tgz",
      "integrity": "sha512-e7jT4DxYvIDLk1ZHmU/m/mB19rex9sv0c2ftBtjSBv+kVM/902eh0fINUzD7UwLLNR+jU585GxUJ8/EBfAM5fw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/code-frame": "^7.27.1",
        "@babel/generator": "^7.28.5",
        "@babel/helper-compilation-targets": "^7.27.2",
        "@babel/helper-module-transforms": "^7.28.3",
        "@babel/helpers": "^7.28.4",
        "@babel/parser": "^7.28.5",
        "@babel/template": "^7.27.2",
        "@babel/traverse": "^7.28.5",
        "@babel/types": "^7.28.5",
        "@jridgewell/remapping": "^2.3.5",
        "convert-source-map": "^2.0.0",
        "debug": "^4.1.0",
        "gensync": "^1.0.0-beta.2",
        "json5": "^2.2.3",
        "semver": "^6.3.1"
      },
      "engines": {
        "node": ">=6.9.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/babel"
      }
    },
    "node_modules/@babel/generator": {
      "version": "7.28.5",
      "resolved": "https://registry.npmjs.org/@babel/generator/-/generator-7.28.5.tgz",
      "integrity": "sha512-3EwLFhZ38J4VyIP6WNtt2kUdW9dokXA9Cr4IVIFHuCpZ3H8/YFOl5JjZHisrn1fATPBmKKqXzDFvh9fUwHz6CQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/parser": "^7.28.5",
        "@babel/types": "^7.28.5",
        "@jridgewell/gen-mapping": "^0.3.12",
        "@jridgewell/trace-mapping": "^0.3.28",
        "jsesc": "^3.0.2"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helper-compilation-targets": {
      "version": "7.27.2",
      "resolved": "https://registry.npmjs.org/@babel/helper-compilation-targets/-/helper-compilation-targets-7.27.2.tgz",
      "integrity": "sha512-2+1thGUUWWjLTYTHZWK1n8Yga0ijBz1XAhUXcKy81rd5g6yh7hGqMp45v7cadSbEHc9G3OTv45SyneRN3ps4DQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/compat-data": "^7.27.2",
        "@babel/helper-validator-option": "^7.27.1",
        "browserslist": "^4.24.0",
        "lru-cache": "^5.1.1",
        "semver": "^6.3.1"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helper-globals": {
      "version": "7.28.0",
      "resolved": "https://registry.npmjs.org/@babel/helper-globals/-/helper-globals-7.28.0.tgz",
      "integrity": "sha512-+W6cISkXFa1jXsDEdYA8HeevQT/FULhxzR99pxphltZcVaugps53THCeiWA8SguxxpSp3gKPiuYfSWopkLQ4hw==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helper-module-imports": {
      "version": "7.27.1",
      "resolved": "https://registry.npmjs.org/@babel/helper-module-imports/-/helper-module-imports-7.27.1.tgz",
      "integrity": "sha512-0gSFWUPNXNopqtIPQvlD5WgXYI5GY2kP2cCvoT8kczjbfcfuIljTbcWrulD1CIPIX2gt1wghbDy08yE1p+/r3w==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/traverse": "^7.27.1",
        "@babel/types": "^7.27.1"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helper-module-transforms": {
      "version": "7.28.3",
      "resolved": "https://registry.npmjs.org/@babel/helper-module-transforms/-/helper-module-transforms-7.28.3.tgz",
      "integrity": "sha512-gytXUbs8k2sXS9PnQptz5o0QnpLL51SwASIORY6XaBKF88nsOT0Zw9szLqlSGQDP/4TljBAD5y98p2U1fqkdsw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-module-imports": "^7.27.1",
        "@babel/helper-validator-identifier": "^7.27.1",
        "@babel/traverse": "^7.28.3"
      },
      "engines": {
        "node": ">=6.9.0"
      },
      "peerDependencies": {
        "@babel/core": "^7.0.0"
      }
    },
    "node_modules/@babel/helper-plugin-utils": {
      "version": "7.27.1",
      "resolved": "https://registry.npmjs.org/@babel/helper-plugin-utils/-/helper-plugin-utils-7.27.1.tgz",
      "integrity": "sha512-1gn1Up5YXka3YYAHGKpbideQ5Yjf1tDa9qYcgysz+cNCXukyLl6DjPXhD3VRwSb8c0J9tA4b2+rHEZtc6R0tlw==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helper-string-parser": {
      "version": "7.27.1",
      "resolved": "https://registry.npmjs.org/@babel/helper-string-parser/-/helper-string-parser-7.27.1.tgz",
      "integrity": "sha512-qMlSxKbpRlAridDExk92nSobyDdpPijUq2DW6oDnUqd0iOGxmQjyqhMIihI9+zv4LPyZdRje2cavWPbCbWm3eA==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helper-validator-identifier": {
      "version": "7.28.5",
      "resolved": "https://registry.npmjs.org/@babel/helper-validator-identifier/-/helper-validator-identifier-7.28.5.tgz",
      "integrity": "sha512-qSs4ifwzKJSV39ucNjsvc6WVHs6b7S03sOh2OcHF9UHfVPqWWALUsNUVzhSBiItjRZoLHx7nIarVjqKVusUZ1Q==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helper-validator-option": {
      "version": "7.27.1",
      "resolved": "https://registry.npmjs.org/@babel/helper-validator-option/-/helper-validator-option-7.27.1.tgz",
      "integrity": "sha512-YvjJow9FxbhFFKDSuFnVCe2WxXk1zWc22fFePVNEaWJEu8IrZVlda6N0uHwzZrUM1il7NC9Mlp4MaJYbYd9JSg==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helpers": {
      "version": "7.28.4",
      "resolved": "https://registry.npmjs.org/@babel/helpers/-/helpers-7.28.4.tgz",
      "integrity": "sha512-HFN59MmQXGHVyYadKLVumYsA9dBFun/ldYxipEjzA4196jpLZd8UjEEBLkbEkvfYreDqJhZxYAWFPtrfhNpj4w==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/template": "^7.27.2",
        "@babel/types": "^7.28.4"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/parser": {
      "version": "7.28.5",
      "resolved": "https://registry.npmjs.org/@babel/parser/-/parser-7.28.5.tgz",
      "integrity": "sha512-KKBU1VGYR7ORr3At5HAtUQ+TV3SzRCXmA/8OdDZiLDBIZxVyzXuztPjfLd3BV1PRAQGCMWWSHYhL0F8d5uHBDQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/types": "^7.28.5"
      },
      "bin": {
        "parser": "bin/babel-parser.js"
      },
      "engines": {
        "node": ">=6.0.0"
      }
    },
    "node_modules/@babel/plugin-transform-react-jsx-self": {
      "version": "7.27.1",
      "resolved": "https://registry.npmjs.org/@babel/plugin-transform-react-jsx-self/-/plugin-transform-react-jsx-self-7.27.1.tgz",
      "integrity": "sha512-6UzkCs+ejGdZ5mFFC/OCUrv028ab2fp1znZmCZjAOBKiBK2jXD1O+BPSfX8X2qjJ75fZBMSnQn3Rq2mrBJK2mw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-plugin-utils": "^7.27.1"
      },
      "engines": {
        "node": ">=6.9.0"
      },
      "peerDependencies": {
        "@babel/core": "^7.0.0-0"
      }
    },
    "node_modules/@babel/plugin-transform-react-jsx-source": {
      "version": "7.27.1",
      "resolved": "https://registry.npmjs.org/@babel/plugin-transform-react-jsx-source/-/plugin-transform-react-jsx-source-7.27.1.tgz",
      "integrity": "sha512-zbwoTsBruTeKB9hSq73ha66iFeJHuaFkUbwvqElnygoNbj/jHRsSeokowZFN3CZ64IvEqcmmkVe89OPXc7ldAw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-plugin-utils": "^7.27.1"
      },
      "engines": {
        "node": ">=6.9.0"
      },
      "peerDependencies": {
        "@babel/core": "^7.0.0-0"
      }
    },
    "node_modules/@babel/runtime": {
      "version": "7.28.4",
      "resolved": "https://registry.npmjs.org/@babel/runtime/-/runtime-7.28.4.tgz",
      "integrity": "sha512-Q/N6JNWvIvPnLDvjlE1OUBLPQHH6l3CltCEsHIujp45zQUSSh8K+gHnaEX45yAT1nyngnINhvWtzN+Nb9D8RAQ==",
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/template": {
      "version": "7.27.2",
      "resolved": "https://registry.npmjs.org/@babel/template/-/template-7.27.2.tgz",
      "integrity": "sha512-LPDZ85aEJyYSd18/DkjNh4/y1ntkE5KwUHWTiqgRxruuZL2F1yuHligVHLvcHY2vMHXttKFpJn6LwfI7cw7ODw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/code-frame": "^7.27.1",
        "@babel/parser": "^7.27.2",
        "@babel/types": "^7.27.1"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/traverse": {
      "version": "7.28.5",
      "resolved": "https://registry.npmjs.org/@babel/traverse/-/traverse-7.28.5.tgz",
      "integrity": "sha512-TCCj4t55U90khlYkVV/0TfkJkAkUg3jZFA3Neb7unZT8CPok7iiRfaX0F+WnqWqt7OxhOn0uBKXCw4lbL8W0aQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/code-frame": "^7.27.1",
        "@babel/generator": "^7.28.5",
        "@babel/helper-globals": "^7.28.0",
        "@babel/parser": "^7.28.5",
        "@babel/template": "^7.27.2",
        "@babel/types": "^7.28.5",
        "debug": "^4.3.1"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/types": {
      "version": "7.28.5",
      "resolved": "https://registry.npmjs.org/@babel/types/-/types-7.28.5.tgz",
      "integrity": "sha512-qQ5m48eI/MFLQ5PxQj4PFaprjyCTLI37ElWMmNs0K8Lk3dVeOdNpB3ks8jc7yM5CDmVC73eMVk/trk3fgmrUpA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-string-parser": "^7.27.1",
        "@babel/helper-validator-identifier": "^7.28.5"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@esbuild/aix-ppc64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/aix-ppc64/-/aix-ppc64-0.25.12.tgz",
      "integrity": "sha512-Hhmwd6CInZ3dwpuGTF8fJG6yoWmsToE+vYgD4nytZVxcu1ulHpUQRAB1UJ8+N1Am3Mz4+xOByoQoSZf4D+CpkA==",
      "cpu": [
        "ppc64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "aix"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/android-arm": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/android-arm/-/android-arm-0.25.12.tgz",
      "integrity": "sha512-VJ+sKvNA/GE7Ccacc9Cha7bpS8nyzVv0jdVgwNDaR4gDMC/2TTRc33Ip8qrNYUcpkOHUT5OZ0bUcNNVZQ9RLlg==",
      "cpu": [
        "arm"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "android"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/android-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/android-arm64/-/android-arm64-0.25.12.tgz",
      "integrity": "sha512-6AAmLG7zwD1Z159jCKPvAxZd4y/VTO0VkprYy+3N2FtJ8+BQWFXU+OxARIwA46c5tdD9SsKGZ/1ocqBS/gAKHg==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "android"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/android-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/android-x64/-/android-x64-0.25.12.tgz",
      "integrity": "sha512-5jbb+2hhDHx5phYR2By8GTWEzn6I9UqR11Kwf22iKbNpYrsmRB18aX/9ivc5cabcUiAT/wM+YIZ6SG9QO6a8kg==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "android"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/darwin-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/darwin-arm64/-/darwin-arm64-0.25.12.tgz",
      "integrity": "sha512-N3zl+lxHCifgIlcMUP5016ESkeQjLj/959RxxNYIthIg+CQHInujFuXeWbWMgnTo4cp5XVHqFPmpyu9J65C1Yg==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "darwin"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/darwin-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/darwin-x64/-/darwin-x64-0.25.12.tgz",
      "integrity": "sha512-HQ9ka4Kx21qHXwtlTUVbKJOAnmG1ipXhdWTmNXiPzPfWKpXqASVcWdnf2bnL73wgjNrFXAa3yYvBSd9pzfEIpA==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "darwin"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/freebsd-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/freebsd-arm64/-/freebsd-arm64-0.25.12.tgz",
      "integrity": "sha512-gA0Bx759+7Jve03K1S0vkOu5Lg/85dou3EseOGUes8flVOGxbhDDh/iZaoek11Y8mtyKPGF3vP8XhnkDEAmzeg==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "freebsd"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/freebsd-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/freebsd-x64/-/freebsd-x64-0.25.12.tgz",
      "integrity": "sha512-TGbO26Yw2xsHzxtbVFGEXBFH0FRAP7gtcPE7P5yP7wGy7cXK2oO7RyOhL5NLiqTlBh47XhmIUXuGciXEqYFfBQ==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "freebsd"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-arm": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-arm/-/linux-arm-0.25.12.tgz",
      "integrity": "sha512-lPDGyC1JPDou8kGcywY0YILzWlhhnRjdof3UlcoqYmS9El818LLfJJc3PXXgZHrHCAKs/Z2SeZtDJr5MrkxtOw==",
      "cpu": [
        "arm"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-arm64/-/linux-arm64-0.25.12.tgz",
      "integrity": "sha512-8bwX7a8FghIgrupcxb4aUmYDLp8pX06rGh5HqDT7bB+8Rdells6mHvrFHHW2JAOPZUbnjUpKTLg6ECyzvas2AQ==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-ia32": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-ia32/-/linux-ia32-0.25.12.tgz",
      "integrity": "sha512-0y9KrdVnbMM2/vG8KfU0byhUN+EFCny9+8g202gYqSSVMonbsCfLjUO+rCci7pM0WBEtz+oK/PIwHkzxkyharA==",
      "cpu": [
        "ia32"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-loong64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-loong64/-/linux-loong64-0.25.12.tgz",
      "integrity": "sha512-h///Lr5a9rib/v1GGqXVGzjL4TMvVTv+s1DPoxQdz7l/AYv6LDSxdIwzxkrPW438oUXiDtwM10o9PmwS/6Z0Ng==",
      "cpu": [
        "loong64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-mips64el": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-mips64el/-/linux-mips64el-0.25.12.tgz",
      "integrity": "sha512-iyRrM1Pzy9GFMDLsXn1iHUm18nhKnNMWscjmp4+hpafcZjrr2WbT//d20xaGljXDBYHqRcl8HnxbX6uaA/eGVw==",
      "cpu": [
        "mips64el"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-ppc64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-ppc64/-/linux-ppc64-0.25.12.tgz",
      "integrity": "sha512-9meM/lRXxMi5PSUqEXRCtVjEZBGwB7P/D4yT8UG/mwIdze2aV4Vo6U5gD3+RsoHXKkHCfSxZKzmDssVlRj1QQA==",
      "cpu": [
        "ppc64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-riscv64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-riscv64/-/linux-riscv64-0.25.12.tgz",
      "integrity": "sha512-Zr7KR4hgKUpWAwb1f3o5ygT04MzqVrGEGXGLnj15YQDJErYu/BGg+wmFlIDOdJp0PmB0lLvxFIOXZgFRrdjR0w==",
      "cpu": [
        "riscv64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-s390x": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-s390x/-/linux-s390x-0.25.12.tgz",
      "integrity": "sha512-MsKncOcgTNvdtiISc/jZs/Zf8d0cl/t3gYWX8J9ubBnVOwlk65UIEEvgBORTiljloIWnBzLs4qhzPkJcitIzIg==",
      "cpu": [
        "s390x"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-x64/-/linux-x64-0.25.12.tgz",
      "integrity": "sha512-uqZMTLr/zR/ed4jIGnwSLkaHmPjOjJvnm6TVVitAa08SLS9Z0VM8wIRx7gWbJB5/J54YuIMInDquWyYvQLZkgw==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/netbsd-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/netbsd-arm64/-/netbsd-arm64-0.25.12.tgz",
      "integrity": "sha512-xXwcTq4GhRM7J9A8Gv5boanHhRa/Q9KLVmcyXHCTaM4wKfIpWkdXiMog/KsnxzJ0A1+nD+zoecuzqPmCRyBGjg==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "netbsd"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/netbsd-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/netbsd-x64/-/netbsd-x64-0.25.12.tgz",
      "integrity": "sha512-Ld5pTlzPy3YwGec4OuHh1aCVCRvOXdH8DgRjfDy/oumVovmuSzWfnSJg+VtakB9Cm0gxNO9BzWkj6mtO1FMXkQ==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "netbsd"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/openbsd-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/openbsd-arm64/-/openbsd-arm64-0.25.12.tgz",
      "integrity": "sha512-fF96T6KsBo/pkQI950FARU9apGNTSlZGsv1jZBAlcLL1MLjLNIWPBkj5NlSz8aAzYKg+eNqknrUJ24QBybeR5A==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "openbsd"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/openbsd-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/openbsd-x64/-/openbsd-x64-0.25.12.tgz",
      "integrity": "sha512-MZyXUkZHjQxUvzK7rN8DJ3SRmrVrke8ZyRusHlP+kuwqTcfWLyqMOE3sScPPyeIXN/mDJIfGXvcMqCgYKekoQw==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "openbsd"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/openharmony-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/openharmony-arm64/-/openharmony-arm64-0.25.12.tgz",
      "integrity": "sha512-rm0YWsqUSRrjncSXGA7Zv78Nbnw4XL6/dzr20cyrQf7ZmRcsovpcRBdhD43Nuk3y7XIoW2OxMVvwuRvk9XdASg==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "openharmony"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/sunos-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/sunos-x64/-/sunos-x64-0.25.12.tgz",
      "integrity": "sha512-3wGSCDyuTHQUzt0nV7bocDy72r2lI33QL3gkDNGkod22EsYl04sMf0qLb8luNKTOmgF/eDEDP5BFNwoBKH441w==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "sunos"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/win32-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/win32-arm64/-/win32-arm64-0.25.12.tgz",
      "integrity": "sha512-rMmLrur64A7+DKlnSuwqUdRKyd3UE7oPJZmnljqEptesKM8wx9J8gx5u0+9Pq0fQQW8vqeKebwNXdfOyP+8Bsg==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/win32-ia32": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/win32-ia32/-/win32-ia32-0.25.12.tgz",
      "integrity": "sha512-HkqnmmBoCbCwxUKKNPBixiWDGCpQGVsrQfJoVGYLPT41XWF8lHuE5N6WhVia2n4o5QK5M4tYr21827fNhi4byQ==",
      "cpu": [
        "ia32"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/win32-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/win32-x64/-/win32-x64-0.25.12.tgz",
      "integrity": "sha512-alJC0uCZpTFrSL0CCDjcgleBXPnCrEAhTBILpeAp7M/OFgoqtAetfBzX0xM00MUsVVPpVjlPuMbREqnZCXaTnA==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@eslint-community/eslint-utils": {
      "version": "4.9.0",
      "resolved": "https://registry.npmjs.org/@eslint-community/eslint-utils/-/eslint-utils-4.9.0.tgz",
      "integrity": "sha512-ayVFHdtZ+hsq1t2Dy24wCmGXGe4q9Gu3smhLYALJrr473ZH27MsnSL+LKUlimp4BWJqMDMLmPpx/Q9R3OAlL4g==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "eslint-visitor-keys": "^3.4.3"
      },
      "engines": {
        "node": "^12.22.0 || ^14.17.0 || >=16.0.0"
      },
      "funding": {
        "url": "https://opencollective.com/eslint"
      },
      "peerDependencies": {
        "eslint": "^6.0.0 || ^7.0.0 || >=8.0.0"
      }
    },
    "node_modules/@eslint-community/eslint-utils/node_modules/eslint-visitor-keys": {
      "version": "3.4.3",
      "resolved": "https://registry.npmjs.org/eslint-visitor-keys/-/eslint-visitor-keys-3.4.3.tgz",
      "integrity": "sha512-wpc+LXeiyiisxPlEkUzU6svyS1frIO3Mgxj1fdy7Pm8Ygzguax2N3Fa/D/ag1WqbOprdI+uY6wMUl8/a2G+iag==",
      "dev": true,
      "license": "Apache-2.0",
      "engines": {
        "node": "^12.22.0 || ^14.17.0 || >=16.0.0"
      },
      "funding": {
        "url": "https://opencollective.com/eslint"
      }
    },
    "node_modules/@eslint-community/regexpp": {
      "version": "4.12.2",
      "resolved": "https://registry.npmjs.org/@eslint-community/regexpp/-/regexpp-4.12.2.tgz",
      "integrity": "sha512-EriSTlt5OC9/7SXkRSCAhfSxxoSUgBm33OH+IkwbdpgoqsSsUg7y3uh+IICI/Qg4BBWr3U2i39RpmycbxMq4ew==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": "^12.0.0 || ^14.0.0 || >=16.0.0"
      }
    },
    "node_modules/@eslint/config-array": {
      "version": "0.21.1",
      "resolved": "https://registry.npmjs.org/@eslint/config-array/-/config-array-0.21.1.tgz",
      "integrity": "sha512-aw1gNayWpdI/jSYVgzN5pL0cfzU02GT3NBpeT/DXbx1/1x7ZKxFPd9bwrzygx/qiwIQiJ1sw/zD8qY/kRvlGHA==",
      "dev": true,
      "license": "Apache-2.0",
      "dependencies": {
        "@eslint/object-schema": "^2.1.7",
        "debug": "^4.3.1",
        "minimatch": "^3.1.2"
      },
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      }
    },
    "node_modules/@eslint/config-helpers": {
      "version": "0.4.2",
      "resolved": "https://registry.npmjs.org/@eslint/config-helpers/-/config-helpers-0.4.2.tgz",
      "integrity": "sha512-gBrxN88gOIf3R7ja5K9slwNayVcZgK6SOUORm2uBzTeIEfeVaIhOpCtTox3P6R7o2jLFwLFTLnC7kU/RGcYEgw==",
      "dev": true,
      "license": "Apache-2.0",
      "dependencies": {
        "@eslint/core": "^0.17.0"
      },
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      }
    },
    "node_modules/@eslint/core": {
      "version": "0.17.0",
      "resolved": "https://registry.npmjs.org/@eslint/core/-/core-0.17.0.tgz",
      "integrity": "sha512-yL/sLrpmtDaFEiUj1osRP4TI2MDz1AddJL+jZ7KSqvBuliN4xqYY54IfdN8qD8Toa6g1iloph1fxQNkjOxrrpQ==",
      "dev": true,
      "license": "Apache-2.0",
      "dependencies": {
        "@types/json-schema": "^7.0.15"
      },
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      }
    },
    "node_modules/@eslint/eslintrc": {
      "version": "3.3.3",
      "resolved": "https://registry.npmjs.org/@eslint/eslintrc/-/eslintrc-3.3.3.tgz",
      "integrity": "sha512-Kr+LPIUVKz2qkx1HAMH8q1q6azbqBAsXJUxBl/ODDuVPX45Z9DfwB8tPjTi6nNZ8BuM3nbJxC5zCAg5elnBUTQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "ajv": "^6.12.4",
        "debug": "^4.3.2",
        "espree": "^10.0.1",
        "globals": "^14.0.0",
        "ignore": "^5.2.0",
        "import-fresh": "^3.2.1",
        "js-yaml": "^4.1.1",
        "minimatch": "^3.1.2",
        "strip-json-comments": "^3.1.1"
      },
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      },
      "funding": {
        "url": "https://opencollective.com/eslint"
      }
    },
    "node_modules/@eslint/eslintrc/node_modules/globals": {
      "version": "14.0.0",
      "resolved": "https://registry.npmjs.org/globals/-/globals-14.0.0.tgz",
      "integrity": "sha512-oahGvuMGQlPw/ivIYBjVSrWAfWLBeku5tpPE2fOPLi+WHffIWbuh2tCjhyQhTBPMf5E9jDEH4FOmTYgYwbKwtQ==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=18"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/@eslint/js": {
      "version": "9.39.1",
      "resolved": "https://registry.npmjs.org/@eslint/js/-/js-9.39.1.tgz",
      "integrity": "sha512-S26Stp4zCy88tH94QbBv3XCuzRQiZ9yXofEILmglYTh/Ug/a9/umqvgFtYBAo3Lp0nsI/5/qH1CCrbdK3AP1Tw==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      },
      "funding": {
        "url": "https://eslint.org/donate"
      }
    },
    "node_modules/@eslint/object-schema": {
      "version": "2.1.7",
      "resolved": "https://registry.npmjs.org/@eslint/object-schema/-/object-schema-2.1.7.tgz",
      "integrity": "sha512-VtAOaymWVfZcmZbp6E2mympDIHvyjXs/12LqWYjVw6qjrfF+VK+fyG33kChz3nnK+SU5/NeHOqrTEHS8sXO3OA==",
      "dev": true,
      "license": "Apache-2.0",
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      }
    },
    "node_modules/@eslint/plugin-kit": {
      "version": "0.4.1",
      "resolved": "https://registry.npmjs.org/@eslint/plugin-kit/-/plugin-kit-0.4.1.tgz",
      "integrity": "sha512-43/qtrDUokr7LJqoF2c3+RInu/t4zfrpYdoSDfYyhg52rwLV6TnOvdG4fXm7IkSB3wErkcmJS9iEhjVtOSEjjA==",
      "dev": true,
      "license": "Apache-2.0",
      "dependencies": {
        "@eslint/core": "^0.17.0",
        "levn": "^0.4.1"
      },
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      }
    },
    "node_modules/@humanfs/core": {
      "version": "0.19.1",
      "resolved": "https://registry.npmjs.org/@humanfs/core/-/core-0.19.1.tgz",
      "integrity": "sha512-5DyQ4+1JEUzejeK1JGICcideyfUbGixgS9jNgex5nqkW+cY7WZhxBigmieN5Qnw9ZosSNVC9KQKyb+GUaGyKUA==",
      "dev": true,
      "license": "Apache-2.0",
      "engines": {
        "node": ">=18.18.0"
      }
    },
    "node_modules/@humanfs/node": {
      "version": "0.16.7",
      "resolved": "https://registry.npmjs.org/@humanfs/node/-/node-0.16.7.tgz",
      "integrity": "sha512-/zUx+yOsIrG4Y43Eh2peDeKCxlRt/gET6aHfaKpuq267qXdYDFViVHfMaLyygZOnl0kGWxFIgsBy8QFuTLUXEQ==",
      "dev": true,
      "license": "Apache-2.0",
      "dependencies": {
        "@humanfs/core": "^0.19.1",
        "@humanwhocodes/retry": "^0.4.0"
      },
      "engines": {
        "node": ">=18.18.0"
      }
    },
    "node_modules/@humanwhocodes/module-importer": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/@humanwhocodes/module-importer/-/module-importer-1.0.1.tgz",
      "integrity": "sha512-bxveV4V8v5Yb4ncFTT3rPSgZBOpCkjfK0y4oVVVJwIuDVBRMDXrPyXRL988i5ap9m9bnyEEjWfm5WkBmtffLfA==",
      "dev": true,
      "license": "Apache-2.0",
      "engines": {
        "node": ">=12.22"
      },
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/nzakas"
      }
    },
    "node_modules/@humanwhocodes/retry": {
      "version": "0.4.3",
      "resolved": "https://registry.npmjs.org/@humanwhocodes/retry/-/retry-0.4.3.tgz",
      "integrity": "sha512-bV0Tgo9K4hfPCek+aMAn81RppFKv2ySDQeMoSZuvTASywNTnVJCArCZE2FWqpvIatKu7VMRLWlR1EazvVhDyhQ==",
      "dev": true,
      "license": "Apache-2.0",
      "engines": {
        "node": ">=18.18"
      },
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/nzakas"
      }
    },
    "node_modules/@jridgewell/gen-mapping": {
      "version": "0.3.13",
      "resolved": "https://registry.npmjs.org/@jridgewell/gen-mapping/-/gen-mapping-0.3.13.tgz",
      "integrity": "sha512-2kkt/7niJ6MgEPxF0bYdQ6etZaA+fQvDcLKckhy1yIQOzaoKjBBjSj63/aLVjYE3qhRt5dvM+uUyfCg6UKCBbA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jridgewell/sourcemap-codec": "^1.5.0",
        "@jridgewell/trace-mapping": "^0.3.24"
      }
    },
    "node_modules/@jridgewell/remapping": {
      "version": "2.3.5",
      "resolved": "https://registry.npmjs.org/@jridgewell/remapping/-/remapping-2.3.5.tgz",
      "integrity": "sha512-LI9u/+laYG4Ds1TDKSJW2YPrIlcVYOwi2fUC6xB43lueCjgxV4lffOCZCtYFiH6TNOX+tQKXx97T4IKHbhyHEQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jridgewell/gen-mapping": "^0.3.5",
        "@jridgewell/trace-mapping": "^0.3.24"
      }
    },
    "node_modules/@jridgewell/resolve-uri": {
      "version": "3.1.2",
      "resolved": "https://registry.npmjs.org/@jridgewell/resolve-uri/-/resolve-uri-3.1.2.tgz",
      "integrity": "sha512-bRISgCIjP20/tbWSPWMEi54QVPRZExkuD9lJL+UIxUKtwVJA8wW1Trb1jMs1RFXo1CBTNZ/5hpC9QvmKWdopKw==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.0.0"
      }
    },
    "node_modules/@jridgewell/sourcemap-codec": {
      "version": "1.5.5",
      "resolved": "https://registry.npmjs.org/@jridgewell/sourcemap-codec/-/sourcemap-codec-1.5.5.tgz",
      "integrity": "sha512-cYQ9310grqxueWbl+WuIUIaiUaDcj7WOq5fVhEljNVgRfOUhY9fy2zTvfoqWsnebh8Sl70VScFbICvJnLKB0Og==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@jridgewell/trace-mapping": {
      "version": "0.3.31",
      "resolved": "https://registry.npmjs.org/@jridgewell/trace-mapping/-/trace-mapping-0.3.31.tgz",
      "integrity": "sha512-zzNR+SdQSDJzc8joaeP8QQoCQr8NuYx2dIIytl1QeBEZHJ9uW6hebsrYgbz8hJwUQao3TWCMtmfV8Nu1twOLAw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jridgewell/resolve-uri": "^3.1.0",
        "@jridgewell/sourcemap-codec": "^1.4.14"
      }
    },
    "node_modules/@rolldown/pluginutils": {
      "version": "1.0.0-beta.53",
      "resolved": "https://registry.npmjs.org/@rolldown/pluginutils/-/pluginutils-1.0.0-beta.53.tgz",
      "integrity": "sha512-vENRlFU4YbrwVqNDZ7fLvy+JR1CRkyr01jhSiDpE1u6py3OMzQfztQU2jxykW3ALNxO4kSlqIDeYyD0Y9RcQeQ==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@rollup/rollup-android-arm-eabi": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-android-arm-eabi/-/rollup-android-arm-eabi-4.53.3.tgz",
      "integrity": "sha512-mRSi+4cBjrRLoaal2PnqH82Wqyb+d3HsPUN/W+WslCXsZsyHa9ZeQQX/pQsZaVIWDkPcpV6jJ+3KLbTbgnwv8w==",
      "cpu": [
        "arm"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "android"
      ]
    },
    "node_modules/@rollup/rollup-android-arm64": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-android-arm64/-/rollup-android-arm64-4.53.3.tgz",
      "integrity": "sha512-CbDGaMpdE9sh7sCmTrTUyllhrg65t6SwhjlMJsLr+J8YjFuPmCEjbBSx4Z/e4SmDyH3aB5hGaJUP2ltV/vcs4w==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "android"
      ]
    },
    "node_modules/@rollup/rollup-darwin-arm64": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-darwin-arm64/-/rollup-darwin-arm64-4.53.3.tgz",
      "integrity": "sha512-Nr7SlQeqIBpOV6BHHGZgYBuSdanCXuw09hon14MGOLGmXAFYjx1wNvquVPmpZnl0tLjg25dEdr4IQ6GgyToCUA==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "darwin"
      ]
    },
    "node_modules/@rollup/rollup-darwin-x64": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-darwin-x64/-/rollup-darwin-x64-4.53.3.tgz",
      "integrity": "sha512-DZ8N4CSNfl965CmPktJ8oBnfYr3F8dTTNBQkRlffnUarJ2ohudQD17sZBa097J8xhQ26AwhHJ5mvUyQW8ddTsQ==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "darwin"
      ]
    },
    "node_modules/@rollup/rollup-freebsd-arm64": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-freebsd-arm64/-/rollup-freebsd-arm64-4.53.3.tgz",
      "integrity": "sha512-yMTrCrK92aGyi7GuDNtGn2sNW+Gdb4vErx4t3Gv/Tr+1zRb8ax4z8GWVRfr3Jw8zJWvpGHNpss3vVlbF58DZ4w==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "freebsd"
      ]
    },
    "node_modules/@rollup/rollup-freebsd-x64": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-freebsd-x64/-/rollup-freebsd-x64-4.53.3.tgz",
      "integrity": "sha512-lMfF8X7QhdQzseM6XaX0vbno2m3hlyZFhwcndRMw8fbAGUGL3WFMBdK0hbUBIUYcEcMhVLr1SIamDeuLBnXS+Q==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "freebsd"
      ]
    },
    "node_modules/@rollup/rollup-linux-arm-gnueabihf": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-arm-gnueabihf/-/rollup-linux-arm-gnueabihf-4.53.3.tgz",
      "integrity": "sha512-k9oD15soC/Ln6d2Wv/JOFPzZXIAIFLp6B+i14KhxAfnq76ajt0EhYc5YPeX6W1xJkAdItcVT+JhKl1QZh44/qw==",
      "cpu": [
        "arm"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-arm-musleabihf": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-arm-musleabihf/-/rollup-linux-arm-musleabihf-4.53.3.tgz",
      "integrity": "sha512-vTNlKq+N6CK/8UktsrFuc+/7NlEYVxgaEgRXVUVK258Z5ymho29skzW1sutgYjqNnquGwVUObAaxae8rZ6YMhg==",
      "cpu": [
        "arm"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-arm64-gnu": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-arm64-gnu/-/rollup-linux-arm64-gnu-4.53.3.tgz",
      "integrity": "sha512-RGrFLWgMhSxRs/EWJMIFM1O5Mzuz3Xy3/mnxJp/5cVhZ2XoCAxJnmNsEyeMJtpK+wu0FJFWz+QF4mjCA7AUQ3w==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-arm64-musl": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-arm64-musl/-/rollup-linux-arm64-musl-4.53.3.tgz",
      "integrity": "sha512-kASyvfBEWYPEwe0Qv4nfu6pNkITLTb32p4yTgzFCocHnJLAHs+9LjUu9ONIhvfT/5lv4YS5muBHyuV84epBo/A==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-loong64-gnu": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-loong64-gnu/-/rollup-linux-loong64-gnu-4.53.3.tgz",
      "integrity": "sha512-JiuKcp2teLJwQ7vkJ95EwESWkNRFJD7TQgYmCnrPtlu50b4XvT5MOmurWNrCj3IFdyjBQ5p9vnrX4JM6I8OE7g==",
      "cpu": [
        "loong64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-ppc64-gnu": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-ppc64-gnu/-/rollup-linux-ppc64-gnu-4.53.3.tgz",
      "integrity": "sha512-EoGSa8nd6d3T7zLuqdojxC20oBfNT8nexBbB/rkxgKj5T5vhpAQKKnD+h3UkoMuTyXkP5jTjK/ccNRmQrPNDuw==",
      "cpu": [
        "ppc64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-riscv64-gnu": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-riscv64-gnu/-/rollup-linux-riscv64-gnu-4.53.3.tgz",
      "integrity": "sha512-4s+Wped2IHXHPnAEbIB0YWBv7SDohqxobiiPA1FIWZpX+w9o2i4LezzH/NkFUl8LRci/8udci6cLq+jJQlh+0g==",
      "cpu": [
        "riscv64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-riscv64-musl": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-riscv64-musl/-/rollup-linux-riscv64-musl-4.53.3.tgz",
      "integrity": "sha512-68k2g7+0vs2u9CxDt5ktXTngsxOQkSEV/xBbwlqYcUrAVh6P9EgMZvFsnHy4SEiUl46Xf0IObWVbMvPrr2gw8A==",
      "cpu": [
        "riscv64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-s390x-gnu": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-s390x-gnu/-/rollup-linux-s390x-gnu-4.53.3.tgz",
      "integrity": "sha512-VYsFMpULAz87ZW6BVYw3I6sWesGpsP9OPcyKe8ofdg9LHxSbRMd7zrVrr5xi/3kMZtpWL/wC+UIJWJYVX5uTKg==",
      "cpu": [
        "s390x"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-x64-gnu": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-x64-gnu/-/rollup-linux-x64-gnu-4.53.3.tgz",
      "integrity": "sha512-3EhFi1FU6YL8HTUJZ51imGJWEX//ajQPfqWLI3BQq4TlvHy4X0MOr5q3D2Zof/ka0d5FNdPwZXm3Yyib/UEd+w==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-x64-musl": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-x64-musl/-/rollup-linux-x64-musl-4.53.3.tgz",
      "integrity": "sha512-eoROhjcc6HbZCJr+tvVT8X4fW3/5g/WkGvvmwz/88sDtSJzO7r/blvoBDgISDiCjDRZmHpwud7h+6Q9JxFwq1Q==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-openharmony-arm64": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-openharmony-arm64/-/rollup-openharmony-arm64-4.53.3.tgz",
      "integrity": "sha512-OueLAWgrNSPGAdUdIjSWXw+u/02BRTcnfw9PN41D2vq/JSEPnJnVuBgw18VkN8wcd4fjUs+jFHVM4t9+kBSNLw==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "openharmony"
      ]
    },
    "node_modules/@rollup/rollup-win32-arm64-msvc": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-win32-arm64-msvc/-/rollup-win32-arm64-msvc-4.53.3.tgz",
      "integrity": "sha512-GOFuKpsxR/whszbF/bzydebLiXIHSgsEUp6M0JI8dWvi+fFa1TD6YQa4aSZHtpmh2/uAlj/Dy+nmby3TJ3pkTw==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ]
    },
    "node_modules/@rollup/rollup-win32-ia32-msvc": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-win32-ia32-msvc/-/rollup-win32-ia32-msvc-4.53.3.tgz",
      "integrity": "sha512-iah+THLcBJdpfZ1TstDFbKNznlzoxa8fmnFYK4V67HvmuNYkVdAywJSoteUszvBQ9/HqN2+9AZghbajMsFT+oA==",
      "cpu": [
        "ia32"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ]
    },
    "node_modules/@rollup/rollup-win32-x64-gnu": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-win32-x64-gnu/-/rollup-win32-x64-gnu-4.53.3.tgz",
      "integrity": "sha512-J9QDiOIZlZLdcot5NXEepDkstocktoVjkaKUtqzgzpt2yWjGlbYiKyp05rWwk4nypbYUNoFAztEgixoLaSETkg==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ]
    },
    "node_modules/@rollup/rollup-win32-x64-msvc": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-win32-x64-msvc/-/rollup-win32-x64-msvc-4.53.3.tgz",
      "integrity": "sha512-UhTd8u31dXadv0MopwGgNOBpUVROFKWVQgAg5N1ESyCz8AuBcMqm4AuTjrwgQKGDfoFuz02EuMRHQIw/frmYKQ==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ]
    },
    "node_modules/@tanstack/query-core": {
      "version": "5.90.12",
      "resolved": "https://registry.npmjs.org/@tanstack/query-core/-/query-core-5.90.12.tgz",
      "integrity": "sha512-T1/8t5DhV/SisWjDnaiU2drl6ySvsHj1bHBCWNXd+/T+Hh1cf6JodyEYMd5sgwm+b/mETT4EV3H+zCVczCU5hg==",
      "license": "MIT",
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/tannerlinsley"
      }
    },
    "node_modules/@tanstack/react-query": {
      "version": "5.90.12",
      "resolved": "https://registry.npmjs.org/@tanstack/react-query/-/react-query-5.90.12.tgz",
      "integrity": "sha512-graRZspg7EoEaw0a8faiUASCyJrqjKPdqJ9EwuDRUF9mEYJ1YPczI9H+/agJ0mOJkPCJDk0lsz5QTrLZ/jQ2rg==",
      "license": "MIT",
      "dependencies": {
        "@tanstack/query-core": "5.90.12"
      },
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/tannerlinsley"
      },
      "peerDependencies": {
        "react": "^18 || ^19"
      }
    },
    "node_modules/@types/babel__core": {
      "version": "7.20.5",
      "resolved": "https://registry.npmjs.org/@types/babel__core/-/babel__core-7.20.5.tgz",
      "integrity": "sha512-qoQprZvz5wQFJwMDqeseRXWv3rqMvhgpbXFfVyWhbx9X47POIA6i/+dXefEmZKoAgOaTdaIgNSMqMIU61yRyzA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/parser": "^7.20.7",
        "@babel/types": "^7.20.7",
        "@types/babel__generator": "*",
        "@types/babel__template": "*",
        "@types/babel__traverse": "*"
      }
    },
    "node_modules/@types/babel__generator": {
      "version": "7.27.0",
      "resolved": "https://registry.npmjs.org/@types/babel__generator/-/babel__generator-7.27.0.tgz",
      "integrity": "sha512-ufFd2Xi92OAVPYsy+P4n7/U7e68fex0+Ee8gSG9KX7eo084CWiQ4sdxktvdl0bOPupXtVJPY19zk6EwWqUQ8lg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/types": "^7.0.0"
      }
    },
    "node_modules/@types/babel__template": {
      "version": "7.4.4",
      "resolved": "https://registry.npmjs.org/@types/babel__template/-/babel__template-7.4.4.tgz",
      "integrity": "sha512-h/NUaSyG5EyxBIp8YRxo4RMe2/qQgvyowRwVMzhYhBCONbW8PUsg4lkFMrhgZhUe5z3L3MiLDuvyJ/CaPa2A8A==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/parser": "^7.1.0",
        "@babel/types": "^7.0.0"
      }
    },
    "node_modules/@types/babel__traverse": {
      "version": "7.28.0",
      "resolved": "https://registry.npmjs.org/@types/babel__traverse/-/babel__traverse-7.28.0.tgz",
      "integrity": "sha512-8PvcXf70gTDZBgt9ptxJ8elBeBjcLOAcOtoO/mPJjtji1+CdGbHgm77om1GrsPxsiE+uXIpNSK64UYaIwQXd4Q==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/types": "^7.28.2"
      }
    },
    "node_modules/@types/debug": {
      "version": "4.1.12",
      "resolved": "https://registry.npmjs.org/@types/debug/-/debug-4.1.12.tgz",
      "integrity": "sha512-vIChWdVG3LG1SMxEvI/AK+FWJthlrqlTu7fbrlywTkkaONwk/UAGaULXRlf8vkzFBLVm0zkMdCquhL5aOjhXPQ==",
      "license": "MIT",
      "dependencies": {
        "@types/ms": "*"
      }
    },
    "node_modules/@types/estree": {
      "version": "1.0.8",
      "resolved": "https://registry.npmjs.org/@types/estree/-/estree-1.0.8.tgz",
      "integrity": "sha512-dWHzHa2WqEXI/O1E9OjrocMTKJl2mSrEolh1Iomrv6U+JuNwaHXsXx9bLu5gG7BUWFIN0skIQJQ/L1rIex4X6w==",
      "license": "MIT"
    },
    "node_modules/@types/estree-jsx": {
      "version": "1.0.5",
      "resolved": "https://registry.npmjs.org/@types/estree-jsx/-/estree-jsx-1.0.5.tgz",
      "integrity": "sha512-52CcUVNFyfb1A2ALocQw/Dd1BQFNmSdkuC3BkZ6iqhdMfQz7JWOFRuJFloOzjk+6WijU56m9oKXFAXc7o3Towg==",
      "license": "MIT",
      "dependencies": {
        "@types/estree": "*"
      }
    },
    "node_modules/@types/hast": {
      "version": "3.0.4",
      "resolved": "https://registry.npmjs.org/@types/hast/-/hast-3.0.4.tgz",
      "integrity": "sha512-WPs+bbQw5aCj+x6laNGWLH3wviHtoCv/P3+otBhbOhJgG8qtpdAMlTCxLtsTWA7LH1Oh/bFCHsBn0TPS5m30EQ==",
      "license": "MIT",
      "dependencies": {
        "@types/unist": "*"
      }
    },
    "node_modules/@types/json-schema": {
      "version": "7.0.15",
      "resolved": "https://registry.npmjs.org/@types/json-schema/-/json-schema-7.0.15.tgz",
      "integrity": "sha512-5+fP8P8MFNC+AyZCDxrB2pkZFPGzqQWUzpSeuuVLvm8VMcorNYavBqoFcxK8bQz4Qsbn4oUEEem4wDLfcysGHA==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@types/mdast": {
      "version": "4.0.4",
      "resolved": "https://registry.npmjs.org/@types/mdast/-/mdast-4.0.4.tgz",
      "integrity": "sha512-kGaNbPh1k7AFzgpud/gMdvIm5xuECykRR+JnWKQno9TAXVa6WIVCGTPvYGekIDL4uwCZQSYbUxNBSb1aUo79oA==",
      "license": "MIT",
      "dependencies": {
        "@types/unist": "*"
      }
    },
    "node_modules/@types/ms": {
      "version": "2.1.0",
      "resolved": "https://registry.npmjs.org/@types/ms/-/ms-2.1.0.tgz",
      "integrity": "sha512-GsCCIZDE/p3i96vtEqx+7dBUGXrc7zeSK3wwPHIaRThS+9OhWIXRqzs4d6k1SVU8g91DrNRWxWUGhp5KXQb2VA==",
      "license": "MIT"
    },
    "node_modules/@types/node": {
      "version": "24.10.2",
      "resolved": "https://registry.npmjs.org/@types/node/-/node-24.10.2.tgz",
      "integrity": "sha512-WOhQTZ4G8xZ1tjJTvKOpyEVSGgOTvJAfDK3FNFgELyaTpzhdgHVHeqW8V+UJvzF5BT+/B54T/1S2K6gd9c7bbA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "undici-types": "~7.16.0"
      }
    },
    "node_modules/@types/react": {
      "version": "19.2.7",
      "resolved": "https://registry.npmjs.org/@types/react/-/react-19.2.7.tgz",
      "integrity": "sha512-MWtvHrGZLFttgeEj28VXHxpmwYbor/ATPYbBfSFZEIRK0ecCFLl2Qo55z52Hss+UV9CRN7trSeq1zbgx7YDWWg==",
      "license": "MIT",
      "dependencies": {
        "csstype": "^3.2.2"
      }
    },
    "node_modules/@types/react-dom": {
      "version": "19.2.3",
      "resolved": "https://registry.npmjs.org/@types/react-dom/-/react-dom-19.2.3.tgz",
      "integrity": "sha512-jp2L/eY6fn+KgVVQAOqYItbF0VY/YApe5Mz2F0aykSO8gx31bYCZyvSeYxCHKvzHG5eZjc+zyaS5BrBWya2+kQ==",
      "dev": true,
      "license": "MIT",
      "peerDependencies": {
        "@types/react": "^19.2.0"
      }
    },
    "node_modules/@types/unist": {
      "version": "3.0.3",
      "resolved": "https://registry.npmjs.org/@types/unist/-/unist-3.0.3.tgz",
      "integrity": "sha512-ko/gIFJRv177XgZsZcBwnqJN5x/Gien8qNOn0D5bQU/zAzVf9Zt3BlcUiLqhV9y4ARk0GbT3tnUiPNgnTXzc/Q==",
      "license": "MIT"
    },
    "node_modules/@typescript-eslint/eslint-plugin": {
      "version": "8.49.0",
      "resolved": "https://registry.npmjs.org/@typescript-eslint/eslint-plugin/-/eslint-plugin-8.49.0.tgz",
      "integrity": "sha512-JXij0vzIaTtCwu6SxTh8qBc66kmf1xs7pI4UOiMDFVct6q86G0Zs7KRcEoJgY3Cav3x5Tq0MF5jwgpgLqgKG3A==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@eslint-community/regexpp": "^4.10.0",
        "@typescript-eslint/scope-manager": "8.49.0",
        "@typescript-eslint/type-utils": "8.49.0",
        "@typescript-eslint/utils": "8.49.0",
        "@typescript-eslint/visitor-keys": "8.49.0",
        "ignore": "^7.0.0",
        "natural-compare": "^1.4.0",
        "ts-api-utils": "^2.1.0"
      },
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/typescript-eslint"
      },
      "peerDependencies": {
        "@typescript-eslint/parser": "^8.49.0",
        "eslint": "^8.57.0 || ^9.0.0",
        "typescript": ">=4.8.4 <6.0.0"
      }
    },
    "node_modules/@typescript-eslint/eslint-plugin/node_modules/ignore": {
      "version": "7.0.5",
      "resolved": "https://registry.npmjs.org/ignore/-/ignore-7.0.5.tgz",
      "integrity": "sha512-Hs59xBNfUIunMFgWAbGX5cq6893IbWg4KnrjbYwX3tx0ztorVgTDA6B2sxf8ejHJ4wz8BqGUMYlnzNBer5NvGg==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">= 4"
      }
    },
    "node_modules/@typescript-eslint/parser": {
      "version": "8.49.0",
      "resolved": "https://registry.npmjs.org/@typescript-eslint/parser/-/parser-8.49.0.tgz",
      "integrity": "sha512-N9lBGA9o9aqb1hVMc9hzySbhKibHmB+N3IpoShyV6HyQYRGIhlrO5rQgttypi+yEeKsKI4idxC8Jw6gXKD4THA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@typescript-eslint/scope-manager": "8.49.0",
        "@typescript-eslint/types": "8.49.0",
        "@typescript-eslint/typescript-estree": "8.49.0",
        "@typescript-eslint/visitor-keys": "8.49.0",
        "debug": "^4.3.4"
      },
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/typescript-eslint"
      },
      "peerDependencies": {
        "eslint": "^8.57.0 || ^9.0.0",
        "typescript": ">=4.8.4 <6.0.0"
      }
    },
    "node_modules/@typescript-eslint/project-service": {
      "version": "8.49.0",
      "resolved": "https://registry.npmjs.org/@typescript-eslint/project-service/-/project-service-8.49.0.tgz",
      "integrity": "sha512-/wJN0/DKkmRUMXjZUXYZpD1NEQzQAAn9QWfGwo+Ai8gnzqH7tvqS7oNVdTjKqOcPyVIdZdyCMoqN66Ia789e7g==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@typescript-eslint/tsconfig-utils": "^8.49.0",
        "@typescript-eslint/types": "^8.49.0",
        "debug": "^4.3.4"
      },
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/typescript-eslint"
      },
      "peerDependencies": {
        "typescript": ">=4.8.4 <6.0.0"
      }
    },
    "node_modules/@typescript-eslint/scope-manager": {
      "version": "8.49.0",
      "resolved": "https://registry.npmjs.org/@typescript-eslint/scope-manager/-/scope-manager-8.49.0.tgz",
      "integrity": "sha512-npgS3zi+/30KSOkXNs0LQXtsg9ekZ8OISAOLGWA/ZOEn0ZH74Ginfl7foziV8DT+D98WfQ5Kopwqb/PZOaIJGg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@typescript-eslint/types": "8.49.0",
        "@typescript-eslint/visitor-keys": "8.49.0"
      },
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/typescript-eslint"
      }
    },
    "node_modules/@typescript-eslint/tsconfig-utils": {
      "version": "8.49.0",
      "resolved": "https://registry.npmjs.org/@typescript-eslint/tsconfig-utils/-/tsconfig-utils-8.49.0.tgz",
      "integrity": "sha512-8prixNi1/6nawsRYxet4YOhnbW+W9FK/bQPxsGB1D3ZrDzbJ5FXw5XmzxZv82X3B+ZccuSxo/X8q9nQ+mFecWA==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/typescript-eslint"
      },
      "peerDependencies": {
        "typescript": ">=4.8.4 <6.0.0"
      }
    },
    "node_modules/@typescript-eslint/type-utils": {
      "version": "8.49.0",
      "resolved": "https://registry.npmjs.org/@typescript-eslint/type-utils/-/type-utils-8.49.0.tgz",
      "integrity": "sha512-KTExJfQ+svY8I10P4HdxKzWsvtVnsuCifU5MvXrRwoP2KOlNZ9ADNEWWsQTJgMxLzS5VLQKDjkCT/YzgsnqmZg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@typescript-eslint/types": "8.49.0",
        "@typescript-eslint/typescript-estree": "8.49.0",
        "@typescript-eslint/utils": "8.49.0",
        "debug": "^4.3.4",
        "ts-api-utils": "^2.1.0"
      },
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/typescript-eslint"
      },
      "peerDependencies": {
        "eslint": "^8.57.0 || ^9.0.0",
        "typescript": ">=4.8.4 <6.0.0"
      }
    },
    "node_modules/@typescript-eslint/types": {
      "version": "8.49.0",
      "resolved": "https://registry.npmjs.org/@typescript-eslint/types/-/types-8.49.0.tgz",
      "integrity": "sha512-e9k/fneezorUo6WShlQpMxXh8/8wfyc+biu6tnAqA81oWrEic0k21RHzP9uqqpyBBeBKu4T+Bsjy9/b8u7obXQ==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/typescript-eslint"
      }
    },
    "node_modules/@typescript-eslint/typescript-estree": {
      "version": "8.49.0",
      "resolved": "https://registry.npmjs.org/@typescript-eslint/typescript-estree/-/typescript-estree-8.49.0.tgz",
      "integrity": "sha512-jrLdRuAbPfPIdYNppHJ/D0wN+wwNfJ32YTAm10eJVsFmrVpXQnDWBn8niCSMlWjvml8jsce5E/O+86IQtTbJWA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@typescript-eslint/project-service": "8.49.0",
        "@typescript-eslint/tsconfig-utils": "8.49.0",
        "@typescript-eslint/types": "8.49.0",
        "@typescript-eslint/visitor-keys": "8.49.0",
        "debug": "^4.3.4",
        "minimatch": "^9.0.4",
        "semver": "^7.6.0",
        "tinyglobby": "^0.2.15",
        "ts-api-utils": "^2.1.0"
      },
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/typescript-eslint"
      },
      "peerDependencies": {
        "typescript": ">=4.8.4 <6.0.0"
      }
    },
    "node_modules/@typescript-eslint/typescript-estree/node_modules/brace-expansion": {
      "version": "2.0.2",
      "resolved": "https://registry.npmjs.org/brace-expansion/-/brace-expansion-2.0.2.tgz",
      "integrity": "sha512-Jt0vHyM+jmUBqojB7E1NIYadt0vI0Qxjxd2TErW94wDz+E2LAm5vKMXXwg6ZZBTHPuUlDgQHKXvjGBdfcF1ZDQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "balanced-match": "^1.0.0"
      }
    },
    "node_modules/@typescript-eslint/typescript-estree/node_modules/minimatch": {
      "version": "9.0.5",
      "resolved": "https://registry.npmjs.org/minimatch/-/minimatch-9.0.5.tgz",
      "integrity": "sha512-G6T0ZX48xgozx7587koeX9Ys2NYy6Gmv//P89sEte9V9whIapMNF4idKxnW2QtCcLiTWlb/wfCabAtAFWhhBow==",
      "dev": true,
      "license": "ISC",
      "dependencies": {
        "brace-expansion": "^2.0.1"
      },
      "engines": {
        "node": ">=16 || 14 >=14.17"
      },
      "funding": {
        "url": "https://github.com/sponsors/isaacs"
      }
    },
    "node_modules/@typescript-eslint/typescript-estree/node_modules/semver": {
      "version": "7.7.3",
      "resolved": "https://registry.npmjs.org/semver/-/semver-7.7.3.tgz",
      "integrity": "sha512-SdsKMrI9TdgjdweUSR9MweHA4EJ8YxHn8DFaDisvhVlUOe4BF1tLD7GAj0lIqWVl+dPb/rExr0Btby5loQm20Q==",
      "dev": true,
      "license": "ISC",
      "bin": {
        "semver": "bin/semver.js"
      },
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/@typescript-eslint/utils": {
      "version": "8.49.0",
      "resolved": "https://registry.npmjs.org/@typescript-eslint/utils/-/utils-8.49.0.tgz",
      "integrity": "sha512-N3W7rJw7Rw+z1tRsHZbK395TWSYvufBXumYtEGzypgMUthlg0/hmCImeA8hgO2d2G4pd7ftpxxul2J8OdtdaFA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@eslint-community/eslint-utils": "^4.7.0",
        "@typescript-eslint/scope-manager": "8.49.0",
        "@typescript-eslint/types": "8.49.0",
        "@typescript-eslint/typescript-estree": "8.49.0"
      },
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/typescript-eslint"
      },
      "peerDependencies": {
        "eslint": "^8.57.0 || ^9.0.0",
        "typescript": ">=4.8.4 <6.0.0"
      }
    },
    "node_modules/@typescript-eslint/visitor-keys": {
      "version": "8.49.0",
      "resolved": "https://registry.npmjs.org/@typescript-eslint/visitor-keys/-/visitor-keys-8.49.0.tgz",
      "integrity": "sha512-LlKaciDe3GmZFphXIc79THF/YYBugZ7FS1pO581E/edlVVNbZKDy93evqmrfQ9/Y4uN0vVhX4iuchq26mK/iiA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@typescript-eslint/types": "8.49.0",
        "eslint-visitor-keys": "^4.2.1"
      },
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/typescript-eslint"
      }
    },
    "node_modules/@ungap/structured-clone": {
      "version": "1.3.0",
      "resolved": "https://registry.npmjs.org/@ungap/structured-clone/-/structured-clone-1.3.0.tgz",
      "integrity": "sha512-WmoN8qaIAo7WTYWbAZuG8PYEhn5fkz7dZrqTBZ7dtt//lL2Gwms1IcnQ5yHqjDfX8Ft5j4YzDM23f87zBfDe9g==",
      "license": "ISC"
    },
    "node_modules/@vitejs/plugin-react": {
      "version": "5.1.2",
      "resolved": "https://registry.npmjs.org/@vitejs/plugin-react/-/plugin-react-5.1.2.tgz",
      "integrity": "sha512-EcA07pHJouywpzsoTUqNh5NwGayl2PPVEJKUSinGGSxFGYn+shYbqMGBg6FXDqgXum9Ou/ecb+411ssw8HImJQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/core": "^7.28.5",
        "@babel/plugin-transform-react-jsx-self": "^7.27.1",
        "@babel/plugin-transform-react-jsx-source": "^7.27.1",
        "@rolldown/pluginutils": "1.0.0-beta.53",
        "@types/babel__core": "^7.20.5",
        "react-refresh": "^0.18.0"
      },
      "engines": {
        "node": "^20.19.0 || >=22.12.0"
      },
      "peerDependencies": {
        "vite": "^4.2.0 || ^5.0.0 || ^6.0.0 || ^7.0.0"
      }
    },
    "node_modules/acorn": {
      "version": "8.15.0",
      "resolved": "https://registry.npmjs.org/acorn/-/acorn-8.15.0.tgz",
      "integrity": "sha512-NZyJarBfL7nWwIq+FDL6Zp/yHEhePMNnnJ0y3qfieCrmNvYct8uvtiV41UvlSe6apAfk0fY1FbWx+NwfmpvtTg==",
      "dev": true,
      "license": "MIT",
      "bin": {
        "acorn": "bin/acorn"
      },
      "engines": {
        "node": ">=0.4.0"
      }
    },
    "node_modules/acorn-jsx": {
      "version": "5.3.2",
      "resolved": "https://registry.npmjs.org/acorn-jsx/-/acorn-jsx-5.3.2.tgz",
      "integrity": "sha512-rq9s+JNhf0IChjtDXxllJ7g41oZk5SlXtp0LHwyA5cejwn7vKmKp4pPri6YEePv2PU65sAsegbXtIinmDFDXgQ==",
      "dev": true,
      "license": "MIT",
      "peerDependencies": {
        "acorn": "^6.0.0 || ^7.0.0 || ^8.0.0"
      }
    },
    "node_modules/ajv": {
      "version": "6.12.6",
      "resolved": "https://registry.npmjs.org/ajv/-/ajv-6.12.6.tgz",
      "integrity": "sha512-j3fVLgvTo527anyYyJOGTYJbG+vnnQYvE0m5mmkc1TK+nxAppkCLMIL0aZ4dblVCNoGShhm+kzE4ZUykBoMg4g==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "fast-deep-equal": "^3.1.1",
        "fast-json-stable-stringify": "^2.0.0",
        "json-schema-traverse": "^0.4.1",
        "uri-js": "^4.2.2"
      },
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/epoberezkin"
      }
    },
    "node_modules/ansi-styles": {
      "version": "4.3.0",
      "resolved": "https://registry.npmjs.org/ansi-styles/-/ansi-styles-4.3.0.tgz",
      "integrity": "sha512-zbB9rCJAT1rbjiVDb2hqKFHNYLxgtk8NURxZ3IZwD3F6NtxbXZQCnnSi1Lkx+IDohdPlFp222wVALIheZJQSEg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "color-convert": "^2.0.1"
      },
      "engines": {
        "node": ">=8"
      },
      "funding": {
        "url": "https://github.com/chalk/ansi-styles?sponsor=1"
      }
    },
    "node_modules/argparse": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/argparse/-/argparse-2.0.1.tgz",
      "integrity": "sha512-8+9WqebbFzpX9OR+Wa6O29asIogeRMzcGtAINdpMHHyAg10f05aSFVBbcEqGf/PXw1EjAZ+q2/bEBg3DvurK3Q==",
      "dev": true,
      "license": "Python-2.0"
    },
    "node_modules/bail": {
      "version": "2.0.2",
      "resolved": "https://registry.npmjs.org/bail/-/bail-2.0.2.tgz",
      "integrity": "sha512-0xO6mYd7JB2YesxDKplafRpsiOzPt9V02ddPCLbY1xYGPOX24NTyN50qnUxgCPcSoYMhKpAuBTjQoRZCAkUDRw==",
      "license": "MIT",
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/balanced-match": {
      "version": "1.0.2",
      "resolved": "https://registry.npmjs.org/balanced-match/-/balanced-match-1.0.2.tgz",
      "integrity": "sha512-3oSeUO0TMV67hN1AmbXsK4yaqU7tjiHlbxRDZOpH0KW9+CeX4bRAaX0Anxt0tx2MrpRpWwQaPwIlISEJhYU5Pw==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/baseline-browser-mapping": {
      "version": "2.9.5",
      "resolved": "https://registry.npmjs.org/baseline-browser-mapping/-/baseline-browser-mapping-2.9.5.tgz",
      "integrity": "sha512-D5vIoztZOq1XM54LUdttJVc96ggEsIfju2JBvht06pSzpckp3C7HReun67Bghzrtdsq9XdMGbSSB3v3GhMNmAA==",
      "dev": true,
      "license": "Apache-2.0",
      "bin": {
        "baseline-browser-mapping": "dist/cli.js"
      }
    },
    "node_modules/brace-expansion": {
      "version": "1.1.12",
      "resolved": "https://registry.npmjs.org/brace-expansion/-/brace-expansion-1.1.12.tgz",
      "integrity": "sha512-9T9UjW3r0UW5c1Q7GTwllptXwhvYmEzFhzMfZ9H7FQWt+uZePjZPjBP/W1ZEyZ1twGWom5/56TF4lPcqjnDHcg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "balanced-match": "^1.0.0",
        "concat-map": "0.0.1"
      }
    },
    "node_modules/browserslist": {
      "version": "4.28.1",
      "resolved": "https://registry.npmjs.org/browserslist/-/browserslist-4.28.1.tgz",
      "integrity": "sha512-ZC5Bd0LgJXgwGqUknZY/vkUQ04r8NXnJZ3yYi4vDmSiZmC/pdSN0NbNRPxZpbtO4uAfDUAFffO8IZoM3Gj8IkA==",
      "dev": true,
      "funding": [
        {
          "type": "opencollective",
          "url": "https://opencollective.com/browserslist"
        },
        {
          "type": "tidelift",
          "url": "https://tidelift.com/funding/github/npm/browserslist"
        },
        {
          "type": "github",
          "url": "https://github.com/sponsors/ai"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "baseline-browser-mapping": "^2.9.0",
        "caniuse-lite": "^1.0.30001759",
        "electron-to-chromium": "^1.5.263",
        "node-releases": "^2.0.27",
        "update-browserslist-db": "^1.2.0"
      },
      "bin": {
        "browserslist": "cli.js"
      },
      "engines": {
        "node": "^6 || ^7 || ^8 || ^9 || ^10 || ^11 || ^12 || >=13.7"
      }
    },
    "node_modules/callsites": {
      "version": "3.1.0",
      "resolved": "https://registry.npmjs.org/callsites/-/callsites-3.1.0.tgz",
      "integrity": "sha512-P8BjAsXvZS+VIDUI11hHCQEv74YT67YUi5JJFNWIqL235sBmjX4+qx9Muvls5ivyNENctx46xQLQ3aTuE7ssaQ==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/caniuse-lite": {
      "version": "1.0.30001759",
      "resolved": "https://registry.npmjs.org/caniuse-lite/-/caniuse-lite-1.0.30001759.tgz",
      "integrity": "sha512-Pzfx9fOKoKvevQf8oCXoyNRQ5QyxJj+3O0Rqx2V5oxT61KGx8+n6hV/IUyJeifUci2clnmmKVpvtiqRzgiWjSw==",
      "dev": true,
      "funding": [
        {
          "type": "opencollective",
          "url": "https://opencollective.com/browserslist"
        },
        {
          "type": "tidelift",
          "url": "https://tidelift.com/funding/github/npm/caniuse-lite"
        },
        {
          "type": "github",
          "url": "https://github.com/sponsors/ai"
        }
      ],
      "license": "CC-BY-4.0"
    },
    "node_modules/ccount": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/ccount/-/ccount-2.0.1.tgz",
      "integrity": "sha512-eyrF0jiFpY+3drT6383f1qhkbGsLSifNAjA61IUjZjmLCWjItY6LB9ft9YhoDgwfmclB2zhu51Lc7+95b8NRAg==",
      "license": "MIT",
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/chalk": {
      "version": "4.1.2",
      "resolved": "https://registry.npmjs.org/chalk/-/chalk-4.1.2.tgz",
      "integrity": "sha512-oKnbhFyRIXpUuez8iBMmyEa4nbj4IOQyuhc/wy9kY7/WVPcwIO9VA668Pu8RkO7+0G76SLROeyw9CpQ061i4mA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "ansi-styles": "^4.1.0",
        "supports-color": "^7.1.0"
      },
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/chalk/chalk?sponsor=1"
      }
    },
    "node_modules/character-entities": {
      "version": "2.0.2",
      "resolved": "https://registry.npmjs.org/character-entities/-/character-entities-2.0.2.tgz",
      "integrity": "sha512-shx7oQ0Awen/BRIdkjkvz54PnEEI/EjwXDSIZp86/KKdbafHh1Df/RYGBhn4hbe2+uKC9FnT5UCEdyPz3ai9hQ==",
      "license": "MIT",
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/character-entities-html4": {
      "version": "2.1.0",
      "resolved": "https://registry.npmjs.org/character-entities-html4/-/character-entities-html4-2.1.0.tgz",
      "integrity": "sha512-1v7fgQRj6hnSwFpq1Eu0ynr/CDEw0rXo2B61qXrLNdHZmPKgb7fqS1a2JwF0rISo9q77jDI8VMEHoApn8qDoZA==",
      "license": "MIT",
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/character-entities-legacy": {
      "version": "3.0.0",
      "resolved": "https://registry.npmjs.org/character-entities-legacy/-/character-entities-legacy-3.0.0.tgz",
      "integrity": "sha512-RpPp0asT/6ufRm//AJVwpViZbGM/MkjQFxJccQRHmISF/22NBtsHqAWmL+/pmkPWoIUJdWyeVleTl1wydHATVQ==",
      "license": "MIT",
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/character-reference-invalid": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/character-reference-invalid/-/character-reference-invalid-2.0.1.tgz",
      "integrity": "sha512-iBZ4F4wRbyORVsu0jPV7gXkOsGYjGHPmAyv+HiHG8gi5PtC9KI2j1+v8/tlibRvjoWX027ypmG/n0HtO5t7unw==",
      "license": "MIT",
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/clsx": {
      "version": "2.1.1",
      "resolved": "https://registry.npmjs.org/clsx/-/clsx-2.1.1.tgz",
      "integrity": "sha512-eYm0QWBtUrBWZWG0d386OGAw16Z995PiOVo2B7bjWSbHedGl5e0ZWaq65kOGgUSNesEIDkB9ISbTg/JK9dhCZA==",
      "license": "MIT",
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/color-convert": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/color-convert/-/color-convert-2.0.1.tgz",
      "integrity": "sha512-RRECPsj7iu/xb5oKYcsFHSppFNnsj/52OVTRKb4zP5onXwVF3zVmmToNcOfGC+CRDpfK/U584fMg38ZHCaElKQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "color-name": "~1.1.4"
      },
      "engines": {
        "node": ">=7.0.0"
      }
    },
    "node_modules/color-name": {
      "version": "1.1.4",
      "resolved": "https://registry.npmjs.org/color-name/-/color-name-1.1.4.tgz",
      "integrity": "sha512-dOy+3AuW3a2wNbZHIuMZpTcgjGuLU/uBL/ubcZF9OXbDo8ff4O8yVp5Bf0efS8uEoYo5q4Fx7dY9OgQGXgAsQA==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/comma-separated-tokens": {
      "version": "2.0.3",
      "resolved": "https://registry.npmjs.org/comma-separated-tokens/-/comma-separated-tokens-2.0.3.tgz",
      "integrity": "sha512-Fu4hJdvzeylCfQPp9SGWidpzrMs7tTrlu6Vb8XGaRGck8QSNZJJp538Wrb60Lax4fPwR64ViY468OIUTbRlGZg==",
      "license": "MIT",
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/concat-map": {
      "version": "0.0.1",
      "resolved": "https://registry.npmjs.org/concat-map/-/concat-map-0.0.1.tgz",
      "integrity": "sha512-/Srv4dswyQNBfohGpz9o6Yb3Gz3SrUDqBH5rTuhGR7ahtlbYKnVxw2bCFMRljaA7EXHaXZ8wsHdodFvbkhKmqg==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/convert-source-map": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/convert-source-map/-/convert-source-map-2.0.0.tgz",
      "integrity": "sha512-Kvp459HrV2FEJ1CAsi1Ku+MY3kasH19TFykTz2xWmMeq6bk2NU3XXvfJ+Q61m0xktWwt+1HSYf3JZsTms3aRJg==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/cookie": {
      "version": "1.1.1",
      "resolved": "https://registry.npmjs.org/cookie/-/cookie-1.1.1.tgz",
      "integrity": "sha512-ei8Aos7ja0weRpFzJnEA9UHJ/7XQmqglbRwnf2ATjcB9Wq874VKH9kfjjirM6UhU2/E5fFYadylyhFldcqSidQ==",
      "license": "MIT",
      "engines": {
        "node": ">=18"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/express"
      }
    },
    "node_modules/cross-spawn": {
      "version": "7.0.6",
      "resolved": "https://registry.npmjs.org/cross-spawn/-/cross-spawn-7.0.6.tgz",
      "integrity": "sha512-uV2QOWP2nWzsy2aMp8aRibhi9dlzF5Hgh5SHaB9OiTGEyDTiJJyx0uy51QXdyWbtAHNua4XJzUKca3OzKUd3vA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "path-key": "^3.1.0",
        "shebang-command": "^2.0.0",
        "which": "^2.0.1"
      },
      "engines": {
        "node": ">= 8"
      }
    },
    "node_modules/csstype": {
      "version": "3.2.3",
      "resolved": "https://registry.npmjs.org/csstype/-/csstype-3.2.3.tgz",
      "integrity": "sha512-z1HGKcYy2xA8AGQfwrn0PAy+PB7X/GSj3UVJW9qKyn43xWa+gl5nXmU4qqLMRzWVLFC8KusUX8T/0kCiOYpAIQ==",
      "license": "MIT"
    },
    "node_modules/date-fns": {
      "version": "4.1.0",
      "resolved": "https://registry.npmjs.org/date-fns/-/date-fns-4.1.0.tgz",
      "integrity": "sha512-Ukq0owbQXxa/U3EGtsdVBkR1w7KOQ5gIBqdH2hkvknzZPYvBxb/aa6E8L7tmjFtkwZBu3UXBbjIgPo/Ez4xaNg==",
      "license": "MIT",
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/kossnocorp"
      }
    },
    "node_modules/debug": {
      "version": "4.4.3",
      "resolved": "https://registry.npmjs.org/debug/-/debug-4.4.3.tgz",
      "integrity": "sha512-RGwwWnwQvkVfavKVt22FGLw+xYSdzARwm0ru6DhTVA3umU5hZc28V3kO4stgYryrTlLpuvgI9GiijltAjNbcqA==",
      "license": "MIT",
      "dependencies": {
        "ms": "^2.1.3"
      },
      "engines": {
        "node": ">=6.0"
      },
      "peerDependenciesMeta": {
        "supports-color": {
          "optional": true
        }
      }
    },
    "node_modules/decode-named-character-reference": {
      "version": "1.2.0",
      "resolved": "https://registry.npmjs.org/decode-named-character-reference/-/decode-named-character-reference-1.2.0.tgz",
      "integrity": "sha512-c6fcElNV6ShtZXmsgNgFFV5tVX2PaV4g+MOAkb8eXHvn6sryJBrZa9r0zV6+dtTyoCKxtDy5tyQ5ZwQuidtd+Q==",
      "license": "MIT",
      "dependencies": {
        "character-entities": "^2.0.0"
      },
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/deep-is": {
      "version": "0.1.4",
      "resolved": "https://registry.npmjs.org/deep-is/-/deep-is-0.1.4.tgz",
      "integrity": "sha512-oIPzksmTg4/MriiaYGO+okXDT7ztn/w3Eptv/+gSIdMdKsJo0u4CfYNFJPy+4SKMuCqGw2wxnA+URMg3t8a/bQ==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/dequal": {
      "version": "2.0.3",
      "resolved": "https://registry.npmjs.org/dequal/-/dequal-2.0.3.tgz",
      "integrity": "sha512-0je+qPKHEMohvfRTCEo3CrPG6cAzAYgmzKyxRiYSSDkS6eGJdyVJm7WaYA5ECaAD9wLB2T4EEeymA5aFVcYXCA==",
      "license": "MIT",
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/devlop": {
      "version": "1.1.0",
      "resolved": "https://registry.npmjs.org/devlop/-/devlop-1.1.0.tgz",
      "integrity": "sha512-RWmIqhcFf1lRYBvNmr7qTNuyCt/7/ns2jbpp1+PalgE/rDQcBT0fioSMUpJ93irlUhC5hrg4cYqe6U+0ImW0rA==",
      "license": "MIT",
      "dependencies": {
        "dequal": "^2.0.0"
      },
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/electron-to-chromium": {
      "version": "1.5.267",
      "resolved": "https://registry.npmjs.org/electron-to-chromium/-/electron-to-chromium-1.5.267.tgz",
      "integrity": "sha512-0Drusm6MVRXSOJpGbaSVgcQsuB4hEkMpHXaVstcPmhu5LIedxs1xNK/nIxmQIU/RPC0+1/o0AVZfBTkTNJOdUw==",
      "dev": true,
      "license": "ISC"
    },
    "node_modules/esbuild": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/esbuild/-/esbuild-0.25.12.tgz",
      "integrity": "sha512-bbPBYYrtZbkt6Os6FiTLCTFxvq4tt3JKall1vRwshA3fdVztsLAatFaZobhkBC8/BrPetoa0oksYoKXoG4ryJg==",
      "dev": true,
      "hasInstallScript": true,
      "license": "MIT",
      "bin": {
        "esbuild": "bin/esbuild"
      },
      "engines": {
        "node": ">=18"
      },
      "optionalDependencies": {
        "@esbuild/aix-ppc64": "0.25.12",
        "@esbuild/android-arm": "0.25.12",
        "@esbuild/android-arm64": "0.25.12",
        "@esbuild/android-x64": "0.25.12",
        "@esbuild/darwin-arm64": "0.25.12",
        "@esbuild/darwin-x64": "0.25.12",
        "@esbuild/freebsd-arm64": "0.25.12",
        "@esbuild/freebsd-x64": "0.25.12",
        "@esbuild/linux-arm": "0.25.12",
        "@esbuild/linux-arm64": "0.25.12",
        "@esbuild/linux-ia32": "0.25.12",
        "@esbuild/linux-loong64": "0.25.12",
        "@esbuild/linux-mips64el": "0.25.12",
        "@esbuild/linux-ppc64": "0.25.12",
        "@esbuild/linux-riscv64": "0.25.12",
        "@esbuild/linux-s390x": "0.25.12",
        "@esbuild/linux-x64": "0.25.12",
        "@esbuild/netbsd-arm64": "0.25.12",
        "@esbuild/netbsd-x64": "0.25.12",
        "@esbuild/openbsd-arm64": "0.25.12",
        "@esbuild/openbsd-x64": "0.25.12",
        "@esbuild/openharmony-arm64": "0.25.12",
        "@esbuild/sunos-x64": "0.25.12",
        "@esbuild/win32-arm64": "0.25.12",
        "@esbuild/win32-ia32": "0.25.12",
        "@esbuild/win32-x64": "0.25.12"
      }
    },
    "node_modules/escalade": {
      "version": "3.2.0",
      "resolved": "https://registry.npmjs.org/escalade/-/escalade-3.2.0.tgz",
      "integrity": "sha512-WUj2qlxaQtO4g6Pq5c29GTcWGDyd8itL8zTlipgECz3JesAiiOKotd8JU6otB3PACgG6xkJUyVhboMS+bje/jA==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/escape-string-regexp": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/escape-string-regexp/-/escape-string-regexp-4.0.0.tgz",
      "integrity": "sha512-TtpcNJ3XAzx3Gq8sWRzJaVajRs0uVxA2YAkdb1jm2YkPz4G6egUFAyA3n5vtEIZefPk5Wa4UXbKuS5fKkJWdgA==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/eslint": {
      "version": "9.39.1",
      "resolved": "https://registry.npmjs.org/eslint/-/eslint-9.39.1.tgz",
      "integrity": "sha512-BhHmn2yNOFA9H9JmmIVKJmd288g9hrVRDkdoIgRCRuSySRUHH7r/DI6aAXW9T1WwUuY3DFgrcaqB+deURBLR5g==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@eslint-community/eslint-utils": "^4.8.0",
        "@eslint-community/regexpp": "^4.12.1",
        "@eslint/config-array": "^0.21.1",
        "@eslint/config-helpers": "^0.4.2",
        "@eslint/core": "^0.17.0",
        "@eslint/eslintrc": "^3.3.1",
        "@eslint/js": "9.39.1",
        "@eslint/plugin-kit": "^0.4.1",
        "@humanfs/node": "^0.16.6",
        "@humanwhocodes/module-importer": "^1.0.1",
        "@humanwhocodes/retry": "^0.4.2",
        "@types/estree": "^1.0.6",
        "ajv": "^6.12.4",
        "chalk": "^4.0.0",
        "cross-spawn": "^7.0.6",
        "debug": "^4.3.2",
        "escape-string-regexp": "^4.0.0",
        "eslint-scope": "^8.4.0",
        "eslint-visitor-keys": "^4.2.1",
        "espree": "^10.4.0",
        "esquery": "^1.5.0",
        "esutils": "^2.0.2",
        "fast-deep-equal": "^3.1.3",
        "file-entry-cache": "^8.0.0",
        "find-up": "^5.0.0",
        "glob-parent": "^6.0.2",
        "ignore": "^5.2.0",
        "imurmurhash": "^0.1.4",
        "is-glob": "^4.0.0",
        "json-stable-stringify-without-jsonify": "^1.0.1",
        "lodash.merge": "^4.6.2",
        "minimatch": "^3.1.2",
        "natural-compare": "^1.4.0",
        "optionator": "^0.9.3"
      },
      "bin": {
        "eslint": "bin/eslint.js"
      },
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      },
      "funding": {
        "url": "https://eslint.org/donate"
      },
      "peerDependencies": {
        "jiti": "*"
      },
      "peerDependenciesMeta": {
        "jiti": {
          "optional": true
        }
      }
    },
    "node_modules/eslint-plugin-react-hooks": {
      "version": "7.0.1",
      "resolved": "https://registry.npmjs.org/eslint-plugin-react-hooks/-/eslint-plugin-react-hooks-7.0.1.tgz",
      "integrity": "sha512-O0d0m04evaNzEPoSW+59Mezf8Qt0InfgGIBJnpC0h3NH/WjUAR7BIKUfysC6todmtiZ/A0oUVS8Gce0WhBrHsA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/core": "^7.24.4",
        "@babel/parser": "^7.24.4",
        "hermes-parser": "^0.25.1",
        "zod": "^3.25.0 || ^4.0.0",
        "zod-validation-error": "^3.5.0 || ^4.0.0"
      },
      "engines": {
        "node": ">=18"
      },
      "peerDependencies": {
        "eslint": "^3.0.0 || ^4.0.0 || ^5.0.0 || ^6.0.0 || ^7.0.0 || ^8.0.0-0 || ^9.0.0"
      }
    },
    "node_modules/eslint-plugin-react-refresh": {
      "version": "0.4.24",
      "resolved": "https://registry.npmjs.org/eslint-plugin-react-refresh/-/eslint-plugin-react-refresh-0.4.24.tgz",
      "integrity": "sha512-nLHIW7TEq3aLrEYWpVaJ1dRgFR+wLDPN8e8FpYAql/bMV2oBEfC37K0gLEGgv9fy66juNShSMV8OkTqzltcG/w==",
      "dev": true,
      "license": "MIT",
      "peerDependencies": {
        "eslint": ">=8.40"
      }
    },
    "node_modules/eslint-scope": {
      "version": "8.4.0",
      "resolved": "https://registry.npmjs.org/eslint-scope/-/eslint-scope-8.4.0.tgz",
      "integrity": "sha512-sNXOfKCn74rt8RICKMvJS7XKV/Xk9kA7DyJr8mJik3S7Cwgy3qlkkmyS2uQB3jiJg6VNdZd/pDBJu0nvG2NlTg==",
      "dev": true,
      "license": "BSD-2-Clause",
      "dependencies": {
        "esrecurse": "^4.3.0",
        "estraverse": "^5.2.0"
      },
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      },
      "funding": {
        "url": "https://opencollective.com/eslint"
      }
    },
    "node_modules/eslint-visitor-keys": {
      "version": "4.2.1",
      "resolved": "https://registry.npmjs.org/eslint-visitor-keys/-/eslint-visitor-keys-4.2.1.tgz",
      "integrity": "sha512-Uhdk5sfqcee/9H/rCOJikYz67o0a2Tw2hGRPOG2Y1R2dg7brRe1uG0yaNQDHu+TO/uQPF/5eCapvYSmHUjt7JQ==",
      "dev": true,
      "license": "Apache-2.0",
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      },
      "funding": {
        "url": "https://opencollective.com/eslint"
      }
    },
    "node_modules/espree": {
      "version": "10.4.0",
      "resolved": "https://registry.npmjs.org/espree/-/espree-10.4.0.tgz",
      "integrity": "sha512-j6PAQ2uUr79PZhBjP5C5fhl8e39FmRnOjsD5lGnWrFU8i2G776tBK7+nP8KuQUTTyAZUwfQqXAgrVH5MbH9CYQ==",
      "dev": true,
      "license": "BSD-2-Clause",
      "dependencies": {
        "acorn": "^8.15.0",
        "acorn-jsx": "^5.3.2",
        "eslint-visitor-keys": "^4.2.1"
      },
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      },
      "funding": {
        "url": "https://opencollective.com/eslint"
      }
    },
    "node_modules/esquery": {
      "version": "1.6.0",
      "resolved": "https://registry.npmjs.org/esquery/-/esquery-1.6.0.tgz",
      "integrity": "sha512-ca9pw9fomFcKPvFLXhBKUK90ZvGibiGOvRJNbjljY7s7uq/5YO4BOzcYtJqExdx99rF6aAcnRxHmcUHcz6sQsg==",
      "dev": true,
      "license": "BSD-3-Clause",
      "dependencies": {
        "estraverse": "^5.1.0"
      },
      "engines": {
        "node": ">=0.10"
      }
    },
    "node_modules/esrecurse": {
      "version": "4.3.0",
      "resolved": "https://registry.npmjs.org/esrecurse/-/esrecurse-4.3.0.tgz",
      "integrity": "sha512-KmfKL3b6G+RXvP8N1vr3Tq1kL/oCFgn2NYXEtqP8/L3pKapUA4G8cFVaoF3SU323CD4XypR/ffioHmkti6/Tag==",
      "dev": true,
      "license": "BSD-2-Clause",
      "dependencies": {
        "estraverse": "^5.2.0"
      },
      "engines": {
        "node": ">=4.0"
      }
    },
    "node_modules/estraverse": {
      "version": "5.3.0",
      "resolved": "https://registry.npmjs.org/estraverse/-/estraverse-5.3.0.tgz",
      "integrity": "sha512-MMdARuVEQziNTeJD8DgMqmhwR11BRQ/cBP+pLtYdSTnf3MIO8fFeiINEbX36ZdNlfU/7A9f3gUw49B3oQsvwBA==",
      "dev": true,
      "license": "BSD-2-Clause",
      "engines": {
        "node": ">=4.0"
      }
    },
    "node_modules/estree-util-is-identifier-name": {
      "version": "3.0.0",
      "resolved": "https://registry.npmjs.org/estree-util-is-identifier-name/-/estree-util-is-identifier-name-3.0.0.tgz",
      "integrity": "sha512-hFtqIDZTIUZ9BXLb8y4pYGyk6+wekIivNVTcmvk8NoOh+VeRn5y6cEHzbURrWbfp1fIqdVipilzj+lfaadNZmg==",
      "license": "MIT",
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/unified"
      }
    },
    "node_modules/esutils": {
      "version": "2.0.3",
      "resolved": "https://registry.npmjs.org/esutils/-/esutils-2.0.3.tgz",
      "integrity": "sha512-kVscqXk4OCp68SZ0dkgEKVi6/8ij300KBWTJq32P/dYeWTSwK41WyTxalN1eRmA5Z9UU/LX9D7FWSmV9SAYx6g==",
      "dev": true,
      "license": "BSD-2-Clause",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/extend": {
      "version": "3.0.2",
      "resolved": "https://registry.npmjs.org/extend/-/extend-3.0.2.tgz",
      "integrity": "sha512-fjquC59cD7CyW6urNXK0FBufkZcoiGG80wTuPujX590cB5Ttln20E2UB4S/WARVqhXffZl2LNgS+gQdPIIim/g==",
      "license": "MIT"
    },
    "node_modules/fast-deep-equal": {
      "version": "3.1.3",
      "resolved": "https://registry.npmjs.org/fast-deep-equal/-/fast-deep-equal-3.1.3.tgz",
      "integrity": "sha512-f3qQ9oQy9j2AhBe/H9VC91wLmKBCCU/gDOnKNAYG5hswO7BLKj09Hc5HYNz9cGI++xlpDCIgDaitVs03ATR84Q==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/fast-json-stable-stringify": {
      "version": "2.1.0",
      "resolved": "https://registry.npmjs.org/fast-json-stable-stringify/-/fast-json-stable-stringify-2.1.0.tgz",
      "integrity": "sha512-lhd/wF+Lk98HZoTCtlVraHtfh5XYijIjalXck7saUtuanSDyLMxnHhSXEDJqHxD7msR8D0uCmqlkwjCV8xvwHw==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/fast-levenshtein": {
      "version": "2.0.6",
      "resolved": "https://registry.npmjs.org/fast-levenshtein/-/fast-levenshtein-2.0.6.tgz",
      "integrity": "sha512-DCXu6Ifhqcks7TZKY3Hxp3y6qphY5SJZmrWMDrKcERSOXWQdMhU9Ig/PYrzyw/ul9jOIyh0N4M0tbC5hodg8dw==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/fault": {
      "version": "1.0.4",
      "resolved": "https://registry.npmjs.org/fault/-/fault-1.0.4.tgz",
      "integrity": "sha512-CJ0HCB5tL5fYTEA7ToAq5+kTwd++Borf1/bifxd9iT70QcXr4MRrO3Llf8Ifs70q+SJcGHFtnIE/Nw6giCtECA==",
      "license": "MIT",
      "dependencies": {
        "format": "^0.2.0"
      },
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/fdir": {
      "version": "6.5.0",
      "resolved": "https://registry.npmjs.org/fdir/-/fdir-6.5.0.tgz",
      "integrity": "sha512-tIbYtZbucOs0BRGqPJkshJUYdL+SDH7dVM8gjy+ERp3WAUjLEFJE+02kanyHtwjWOnwrKYBiwAmM0p4kLJAnXg==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=12.0.0"
      },
      "peerDependencies": {
        "picomatch": "^3 || ^4"
      },
      "peerDependenciesMeta": {
        "picomatch": {
          "optional": true
        }
      }
    },
    "node_modules/file-entry-cache": {
      "version": "8.0.0",
      "resolved": "https://registry.npmjs.org/file-entry-cache/-/file-entry-cache-8.0.0.tgz",
      "integrity": "sha512-XXTUwCvisa5oacNGRP9SfNtYBNAMi+RPwBFmblZEF7N7swHYQS6/Zfk7SRwx4D5j3CH211YNRco1DEMNVfZCnQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "flat-cache": "^4.0.0"
      },
      "engines": {
        "node": ">=16.0.0"
      }
    },
    "node_modules/find-up": {
      "version": "5.0.0",
      "resolved": "https://registry.npmjs.org/find-up/-/find-up-5.0.0.tgz",
      "integrity": "sha512-78/PXT1wlLLDgTzDs7sjq9hzz0vXD+zn+7wypEe4fXQxCmdmqfGsEPQxmiCSQI3ajFV91bVSsvNtrJRiW6nGng==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "locate-path": "^6.0.0",
        "path-exists": "^4.0.0"
      },
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/flat-cache": {
      "version": "4.0.1",
      "resolved": "https://registry.npmjs.org/flat-cache/-/flat-cache-4.0.1.tgz",
      "integrity": "sha512-f7ccFPK3SXFHpx15UIGyRJ/FJQctuKZ0zVuN3frBo4HnK3cay9VEW0R6yPYFHC0AgqhukPzKjq22t5DmAyqGyw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "flatted": "^3.2.9",
        "keyv": "^4.5.4"
      },
      "engines": {
        "node": ">=16"
      }
    },
    "node_modules/flatted": {
      "version": "3.3.3",
      "resolved": "https://registry.npmjs.org/flatted/-/flatted-3.3.3.tgz",
      "integrity": "sha512-GX+ysw4PBCz0PzosHDepZGANEuFCMLrnRTiEy9McGjmkCQYwRq4A/X786G/fjM/+OjsWSU1ZrY5qyARZmO/uwg==",
      "dev": true,
      "license": "ISC"
    },
    "node_modules/format": {
      "version": "0.2.2",
      "resolved": "https://registry.npmjs.org/format/-/format-0.2.2.tgz",
      "integrity": "sha512-wzsgA6WOq+09wrU1tsJ09udeR/YZRaeArL9e1wPbFg3GG2yDnC2ldKpxs4xunpFF9DgqCqOIra3bc1HWrJ37Ww==",
      "engines": {
        "node": ">=0.4.x"
      }
    },
    "node_modules/fsevents": {
      "version": "2.3.3",
      "resolved": "https://registry.npmjs.org/fsevents/-/fsevents-2.3.3.tgz",
      "integrity": "sha512-5xoDfX+fL7faATnagmWPpbFtwh/R77WmMMqqHGS65C3vvB0YHrgF+B1YmZ3441tMj5n63k0212XNoJwzlhffQw==",
      "dev": true,
      "hasInstallScript": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "darwin"
      ],
      "engines": {
        "node": "^8.16.0 || ^10.6.0 || >=11.0.0"
      }
    },
    "node_modules/gensync": {
      "version": "1.0.0-beta.2",
      "resolved": "https://registry.npmjs.org/gensync/-/gensync-1.0.0-beta.2.tgz",
      "integrity": "sha512-3hN7NaskYvMDLQY55gnW3NQ+mesEAepTqlg+VEbj7zzqEMBVNhzcGYYeqFo/TlYz6eQiFcp1HcsCZO+nGgS8zg==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/glob-parent": {
      "version": "6.0.2",
      "resolved": "https://registry.npmjs.org/glob-parent/-/glob-parent-6.0.2.tgz",
      "integrity": "sha512-XxwI8EOhVQgWp6iDL+3b0r86f4d6AX6zSU55HfB4ydCEuXLXc5FcYeOu+nnGftS4TEju/11rt4KJPTMgbfmv4A==",
      "dev": true,
      "license": "ISC",
      "dependencies": {
        "is-glob": "^4.0.3"
      },
      "engines": {
        "node": ">=10.13.0"
      }
    },
    "node_modules/globals": {
      "version": "16.5.0",
      "resolved": "https://registry.npmjs.org/globals/-/globals-16.5.0.tgz",
      "integrity": "sha512-c/c15i26VrJ4IRt5Z89DnIzCGDn9EcebibhAOjw5ibqEHsE1wLUgkPn9RDmNcUKyU87GeaL633nyJ+pplFR2ZQ==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=18"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/has-flag": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/has-flag/-/has-flag-4.0.0.tgz",
      "integrity": "sha512-EykJT/Q1KjTWctppgIAgfSO0tKVuZUjhgMr17kqTumMl6Afv3EISleU7qZUzoXDFTAHTDC4NOoG/ZxU3EvlMPQ==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/hast-util-parse-selector": {
      "version": "2.2.5",
      "resolved": "https://registry.npmjs.org/hast-util-parse-selector/-/hast-util-parse-selector-2.2.5.tgz",
      "integrity": "sha512-7j6mrk/qqkSehsM92wQjdIgWM2/BW61u/53G6xmC8i1OmEdKLHbk419QKQUjz6LglWsfqoiHmyMRkP1BGjecNQ==",
      "license": "MIT",
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/unified"
      }
    },
    "node_modules/hast-util-to-jsx-runtime": {
      "version": "2.3.6",
      "resolved": "https://registry.npmjs.org/hast-util-to-jsx-runtime/-/hast-util-to-jsx-runtime-2.3.6.tgz",
      "integrity": "sha512-zl6s8LwNyo1P9uw+XJGvZtdFF1GdAkOg8ujOw+4Pyb76874fLps4ueHXDhXWdk6YHQ6OgUtinliG7RsYvCbbBg==",
      "license": "MIT",
      "dependencies": {
        "@types/estree": "^1.0.0",
        "@types/hast": "^3.0.0",
        "@types/unist": "^3.0.0",
        "comma-separated-tokens": "^2.0.0",
        "devlop": "^1.0.0",
        "estree-util-is-identifier-name": "^3.0.0",
        "hast-util-whitespace": "^3.0.0",
        "mdast-util-mdx-expression": "^2.0.0",
        "mdast-util-mdx-jsx": "^3.0.0",
        "mdast-util-mdxjs-esm": "^2.0.0",
        "property-information": "^7.0.0",
        "space-separated-tokens": "^2.0.0",
        "style-to-js": "^1.0.0",
        "unist-util-position": "^5.0.0",
        "vfile-message": "^4.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/unified"
      }
    },
    "node_modules/hast-util-whitespace": {
      "version": "3.0.0",
      "resolved": "https://registry.npmjs.org/hast-util-whitespace/-/hast-util-whitespace-3.0.0.tgz",
      "integrity": "sha512-88JUN06ipLwsnv+dVn+OIYOvAuvBMy/Qoi6O7mQHxdPXpjy+Cd6xRkWwux7DKO+4sYILtLBRIKgsdpS2gQc7qw==",
      "license": "MIT",
      "dependencies": {
        "@types/hast": "^3.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/unified"
      }
    },
    "node_modules/hastscript": {
      "version": "6.0.0",
      "resolved": "https://registry.npmjs.org/hastscript/-/hastscript-6.0.0.tgz",
      "integrity": "sha512-nDM6bvd7lIqDUiYEiu5Sl/+6ReP0BMk/2f4U/Rooccxkj0P5nm+acM5PrGJ/t5I8qPGiqZSE6hVAwZEdZIvP4w==",
      "license": "MIT",
      "dependencies": {
        "@types/hast": "^2.0.0",
        "comma-separated-tokens": "^1.0.0",
        "hast-util-parse-selector": "^2.0.0",
        "property-information": "^5.0.0",
        "space-separated-tokens": "^1.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/unified"
      }
    },
    "node_modules/hastscript/node_modules/@types/hast": {
      "version": "2.3.10",
      "resolved": "https://registry.npmjs.org/@types/hast/-/hast-2.3.10.tgz",
      "integrity": "sha512-McWspRw8xx8J9HurkVBfYj0xKoE25tOFlHGdx4MJ5xORQrMGZNqJhVQWaIbm6Oyla5kYOXtDiopzKRJzEOkwJw==",
      "license": "MIT",
      "dependencies": {
        "@types/unist": "^2"
      }
    },
    "node_modules/hastscript/node_modules/@types/unist": {
      "version": "2.0.11",
      "resolved": "https://registry.npmjs.org/@types/unist/-/unist-2.0.11.tgz",
      "integrity": "sha512-CmBKiL6NNo/OqgmMn95Fk9Whlp2mtvIv+KNpQKN2F4SjvrEesubTRWGYSg+BnWZOnlCaSTU1sMpsBOzgbYhnsA==",
      "license": "MIT"
    },
    "node_modules/hastscript/node_modules/comma-separated-tokens": {
      "version": "1.0.8",
      "resolved": "https://registry.npmjs.org/comma-separated-tokens/-/comma-separated-tokens-1.0.8.tgz",
      "integrity": "sha512-GHuDRO12Sypu2cV70d1dkA2EUmXHgntrzbpvOB+Qy+49ypNfGgFQIC2fhhXbnyrJRynDCAARsT7Ou0M6hirpfw==",
      "license": "MIT",
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/hastscript/node_modules/property-information": {
      "version": "5.6.0",
      "resolved": "https://registry.npmjs.org/property-information/-/property-information-5.6.0.tgz",
      "integrity": "sha512-YUHSPk+A30YPv+0Qf8i9Mbfe/C0hdPXk1s1jPVToV8pk8BQtpw10ct89Eo7OWkutrwqvT0eicAxlOg3dOAu8JA==",
      "license": "MIT",
      "dependencies": {
        "xtend": "^4.0.0"
      },
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/hastscript/node_modules/space-separated-tokens": {
      "version": "1.1.5",
      "resolved": "https://registry.npmjs.org/space-separated-tokens/-/space-separated-tokens-1.1.5.tgz",
      "integrity": "sha512-q/JSVd1Lptzhf5bkYm4ob4iWPjx0KiRe3sRFBNrVqbJkFaBm5vbbowy1mymoPNLRa52+oadOhJ+K49wsSeSjTA==",
      "license": "MIT",
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/hermes-estree": {
      "version": "0.25.1",
      "resolved": "https://registry.npmjs.org/hermes-estree/-/hermes-estree-0.25.1.tgz",
      "integrity": "sha512-0wUoCcLp+5Ev5pDW2OriHC2MJCbwLwuRx+gAqMTOkGKJJiBCLjtrvy4PWUGn6MIVefecRpzoOZ/UV6iGdOr+Cw==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/hermes-parser": {
      "version": "0.25.1",
      "resolved": "https://registry.npmjs.org/hermes-parser/-/hermes-parser-0.25.1.tgz",
      "integrity": "sha512-6pEjquH3rqaI6cYAXYPcz9MS4rY6R4ngRgrgfDshRptUZIc3lw0MCIJIGDj9++mfySOuPTHB4nrSW99BCvOPIA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "hermes-estree": "0.25.1"
      }
    },
    "node_modules/highlight.js": {
      "version": "10.7.3",
      "resolved": "https://registry.npmjs.org/highlight.js/-/highlight.js-10.7.3.tgz",
      "integrity": "sha512-tzcUFauisWKNHaRkN4Wjl/ZA07gENAjFl3J/c480dprkGTg5EQstgaNFqBfUqCq54kZRIEcreTsAgF/m2quD7A==",
      "license": "BSD-3-Clause",
      "engines": {
        "node": "*"
      }
    },
    "node_modules/highlightjs-vue": {
      "version": "1.0.0",
      "resolved": "https://registry.npmjs.org/highlightjs-vue/-/highlightjs-vue-1.0.0.tgz",
      "integrity": "sha512-PDEfEF102G23vHmPhLyPboFCD+BkMGu+GuJe2d9/eH4FsCwvgBpnc9n0pGE+ffKdph38s6foEZiEjdgHdzp+IA==",
      "license": "CC0-1.0"
    },
    "node_modules/html-url-attributes": {
      "version": "3.0.1",
      "resolved": "https://registry.npmjs.org/html-url-attributes/-/html-url-attributes-3.0.1.tgz",
      "integrity": "sha512-ol6UPyBWqsrO6EJySPz2O7ZSr856WDrEzM5zMqp+FJJLGMW35cLYmmZnl0vztAZxRUoNZJFTCohfjuIJ8I4QBQ==",
      "license": "MIT",
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/unified"
      }
    },
    "node_modules/ignore": {
      "version": "5.3.2",
      "resolved": "https://registry.npmjs.org/ignore/-/ignore-5.3.2.tgz",
      "integrity": "sha512-hsBTNUqQTDwkWtcdYI2i06Y/nUBEsNEDJKjWdigLvegy8kDuJAS8uRlpkkcQpyEXL0Z/pjDy5HBmMjRCJ2gq+g==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">= 4"
      }
    },
    "node_modules/import-fresh": {
      "version": "3.3.1",
      "resolved": "https://registry.npmjs.org/import-fresh/-/import-fresh-3.3.1.tgz",
      "integrity": "sha512-TR3KfrTZTYLPB6jUjfx6MF9WcWrHL9su5TObK4ZkYgBdWKPOFoSoQIdEuTuR82pmtxH2spWG9h6etwfr1pLBqQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "parent-module": "^1.0.0",
        "resolve-from": "^4.0.0"
      },
      "engines": {
        "node": ">=6"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/imurmurhash": {
      "version": "0.1.4",
      "resolved": "https://registry.npmjs.org/imurmurhash/-/imurmurhash-0.1.4.tgz",
      "integrity": "sha512-JmXMZ6wuvDmLiHEml9ykzqO6lwFbof0GG4IkcGaENdCRDDmMVnny7s5HsIgHCbaq0w2MyPhDqkhTUgS2LU2PHA==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=0.8.19"
      }
    },
    "node_modules/inline-style-parser": {
      "version": "0.2.7",
      "resolved": "https://registry.npmjs.org/inline-style-parser/-/inline-style-parser-0.2.7.tgz",
      "integrity": "sha512-Nb2ctOyNR8DqQoR0OwRG95uNWIC0C1lCgf5Naz5H6Ji72KZ8OcFZLz2P5sNgwlyoJ8Yif11oMuYs5pBQa86csA==",
      "license": "MIT"
    },
    "node_modules/is-alphabetical": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/is-alphabetical/-/is-alphabetical-2.0.1.tgz",
      "integrity": "sha512-FWyyY60MeTNyeSRpkM2Iry0G9hpr7/9kD40mD/cGQEuilcZYS4okz8SN2Q6rLCJ8gbCt6fN+rC+6tMGS99LaxQ==",
      "license": "MIT",
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/is-alphanumerical": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/is-alphanumerical/-/is-alphanumerical-2.0.1.tgz",
      "integrity": "sha512-hmbYhX/9MUMF5uh7tOXyK/n0ZvWpad5caBA17GsC6vyuCqaWliRG5K1qS9inmUhEMaOBIW7/whAnSwveW/LtZw==",
      "license": "MIT",
      "dependencies": {
        "is-alphabetical": "^2.0.0",
        "is-decimal": "^2.0.0"
      },
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/is-decimal": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/is-decimal/-/is-decimal-2.0.1.tgz",
      "integrity": "sha512-AAB9hiomQs5DXWcRB1rqsxGUstbRroFOPPVAomNk/3XHR5JyEZChOyTWe2oayKnsSsr/kcGqF+z6yuH6HHpN0A==",
      "license": "MIT",
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/is-extglob": {
      "version": "2.1.1",
      "resolved": "https://registry.npmjs.org/is-extglob/-/is-extglob-2.1.1.tgz",
      "integrity": "sha512-SbKbANkN603Vi4jEZv49LeVJMn4yGwsbzZworEoyEiutsN3nJYdbO36zfhGJ6QEDpOZIFkDtnq5JRxmvl3jsoQ==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/is-glob": {
      "version": "4.0.3",
      "resolved": "https://registry.npmjs.org/is-glob/-/is-glob-4.0.3.tgz",
      "integrity": "sha512-xelSayHH36ZgE7ZWhli7pW34hNbNl8Ojv5KVmkJD4hBdD3th8Tfk9vYasLM+mXWOZhFkgZfxhLSnrwRr4elSSg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "is-extglob": "^2.1.1"
      },
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/is-hexadecimal": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/is-hexadecimal/-/is-hexadecimal-2.0.1.tgz",
      "integrity": "sha512-DgZQp241c8oO6cA1SbTEWiXeoxV42vlcJxgH+B3hi1AiqqKruZR3ZGF8In3fj4+/y/7rHvlOZLZtgJ/4ttYGZg==",
      "license": "MIT",
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/is-plain-obj": {
      "version": "4.1.0",
      "resolved": "https://registry.npmjs.org/is-plain-obj/-/is-plain-obj-4.1.0.tgz",
      "integrity": "sha512-+Pgi+vMuUNkJyExiMBt5IlFoMyKnr5zhJ4Uspz58WOhBF5QoIZkFyNHIbBAtHwzVAgk5RtndVNsDRN61/mmDqg==",
      "license": "MIT",
      "engines": {
        "node": ">=12"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/isexe": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/isexe/-/isexe-2.0.0.tgz",
      "integrity": "sha512-RHxMLp9lnKHGHRng9QFhRCMbYAcVpn69smSGcq3f36xjgVVWThj4qqLbTLlq7Ssj8B+fIQ1EuCEGI2lKsyQeIw==",
      "dev": true,
      "license": "ISC"
    },
    "node_modules/js-tokens": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/js-tokens/-/js-tokens-4.0.0.tgz",
      "integrity": "sha512-RdJUflcE3cUzKiMqQgsCu06FPu9UdIJO0beYbPhHN4k6apgJtifcoCtT9bcxOpYBtpD2kCM6Sbzg4CausW/PKQ==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/js-yaml": {
      "version": "4.1.1",
      "resolved": "https://registry.npmjs.org/js-yaml/-/js-yaml-4.1.1.tgz",
      "integrity": "sha512-qQKT4zQxXl8lLwBtHMWwaTcGfFOZviOJet3Oy/xmGk2gZH677CJM9EvtfdSkgWcATZhj/55JZ0rmy3myCT5lsA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "argparse": "^2.0.1"
      },
      "bin": {
        "js-yaml": "bin/js-yaml.js"
      }
    },
    "node_modules/jsesc": {
      "version": "3.1.0",
      "resolved": "https://registry.npmjs.org/jsesc/-/jsesc-3.1.0.tgz",
      "integrity": "sha512-/sM3dO2FOzXjKQhJuo0Q173wf2KOo8t4I8vHy6lF9poUp7bKT0/NHE8fPX23PwfhnykfqnC2xRxOnVw5XuGIaA==",
      "dev": true,
      "license": "MIT",
      "bin": {
        "jsesc": "bin/jsesc"
      },
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/json-buffer": {
      "version": "3.0.1",
      "resolved": "https://registry.npmjs.org/json-buffer/-/json-buffer-3.0.1.tgz",
      "integrity": "sha512-4bV5BfR2mqfQTJm+V5tPPdf+ZpuhiIvTuAB5g8kcrXOZpTT/QwwVRWBywX1ozr6lEuPdbHxwaJlm9G6mI2sfSQ==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/json-schema-traverse": {
      "version": "0.4.1",
      "resolved": "https://registry.npmjs.org/json-schema-traverse/-/json-schema-traverse-0.4.1.tgz",
      "integrity": "sha512-xbbCH5dCYU5T8LcEhhuh7HJ88HXuW3qsI3Y0zOZFKfZEHcpWiHU/Jxzk629Brsab/mMiHQti9wMP+845RPe3Vg==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/json-stable-stringify-without-jsonify": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/json-stable-stringify-without-jsonify/-/json-stable-stringify-without-jsonify-1.0.1.tgz",
      "integrity": "sha512-Bdboy+l7tA3OGW6FjyFHWkP5LuByj1Tk33Ljyq0axyzdk9//JSi2u3fP1QSmd1KNwq6VOKYGlAu87CisVir6Pw==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/json5": {
      "version": "2.2.3",
      "resolved": "https://registry.npmjs.org/json5/-/json5-2.2.3.tgz",
      "integrity": "sha512-XmOWe7eyHYH14cLdVPoyg+GOH3rYX++KpzrylJwSW98t3Nk+U8XOl8FWKOgwtzdb8lXGf6zYwDUzeHMWfxasyg==",
      "dev": true,
      "license": "MIT",
      "bin": {
        "json5": "lib/cli.js"
      },
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/keyv": {
      "version": "4.5.4",
      "resolved": "https://registry.npmjs.org/keyv/-/keyv-4.5.4.tgz",
      "integrity": "sha512-oxVHkHR/EJf2CNXnWxRLW6mg7JyCCUcG0DtEGmL2ctUo1PNTin1PUil+r/+4r5MpVgC/fn1kjsx7mjSujKqIpw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "json-buffer": "3.0.1"
      }
    },
    "node_modules/levn": {
      "version": "0.4.1",
      "resolved": "https://registry.npmjs.org/levn/-/levn-0.4.1.tgz",
      "integrity": "sha512-+bT2uH4E5LGE7h/n3evcS/sQlJXCpIp6ym8OWJ5eV6+67Dsql/LaaT7qJBAt2rzfoa/5QBGBhxDix1dMt2kQKQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "prelude-ls": "^1.2.1",
        "type-check": "~0.4.0"
      },
      "engines": {
        "node": ">= 0.8.0"
      }
    },
    "node_modules/locate-path": {
      "version": "6.0.0",
      "resolved": "https://registry.npmjs.org/locate-path/-/locate-path-6.0.0.tgz",
      "integrity": "sha512-iPZK6eYjbxRu3uB4/WZ3EsEIMJFMqAoopl3R+zuq0UjcAm/MO6KCweDgPfP3elTztoKP3KtnVHxTn2NHBSDVUw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "p-locate": "^5.0.0"
      },
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/lodash.merge": {
      "version": "4.6.2",
      "resolved": "https://registry.npmjs.org/lodash.merge/-/lodash.merge-4.6.2.tgz",
      "integrity": "sha512-0KpjqXRVvrYyCsX1swR/XTK0va6VQkQM6MNo7PqW77ByjAhoARA8EfrP1N4+KlKj8YS0ZUCtRT/YUuhyYDujIQ==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/longest-streak": {
      "version": "3.1.0",
      "resolved": "https://registry.npmjs.org/longest-streak/-/longest-streak-3.1.0.tgz",
      "integrity": "sha512-9Ri+o0JYgehTaVBBDoMqIl8GXtbWg711O3srftcHhZ0dqnETqLaoIK0x17fUw9rFSlK/0NlsKe0Ahhyl5pXE2g==",
      "license": "MIT",
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/lowlight": {
      "version": "1.20.0",
      "resolved": "https://registry.npmjs.org/lowlight/-/lowlight-1.20.0.tgz",
      "integrity": "sha512-8Ktj+prEb1RoCPkEOrPMYUN/nCggB7qAWe3a7OpMjWQkh3l2RD5wKRQ+o8Q8YuI9RG/xs95waaI/E6ym/7NsTw==",
      "license": "MIT",
      "dependencies": {
        "fault": "^1.0.0",
        "highlight.js": "~10.7.0"
      },
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/lru-cache": {
      "version": "5.1.1",
      "resolved": "https://registry.npmjs.org/lru-cache/-/lru-cache-5.1.1.tgz",
      "integrity": "sha512-KpNARQA3Iwv+jTA0utUVVbrh+Jlrr1Fv0e56GGzAFOXN7dk/FviaDW8LHmK52DlcH4WP2n6gI8vN1aesBFgo9w==",
      "dev": true,
      "license": "ISC",
      "dependencies": {
        "yallist": "^3.0.2"
      }
    },
    "node_modules/lucide-react": {
      "version": "0.511.0",
      "resolved": "https://registry.npmjs.org/lucide-react/-/lucide-react-0.511.0.tgz",
      "integrity": "sha512-VK5a2ydJ7xm8GvBeKLS9mu1pVK6ucef9780JVUjw6bAjJL/QXnd4Y0p7SPeOUMC27YhzNCZvm5d/QX0Tp3rc0w==",
      "license": "ISC",
      "peerDependencies": {
        "react": "^16.5.1 || ^17.0.0 || ^18.0.0 || ^19.0.0"
      }
    },
    "node_modules/mdast-util-from-markdown": {
      "version": "2.0.2",
      "resolved": "https://registry.npmjs.org/mdast-util-from-markdown/-/mdast-util-from-markdown-2.0.2.tgz",
      "integrity": "sha512-uZhTV/8NBuw0WHkPTrCqDOl0zVe1BIng5ZtHoDk49ME1qqcjYmmLmOf0gELgcRMxN4w2iuIeVso5/6QymSrgmA==",
      "license": "MIT",
      "dependencies": {
        "@types/mdast": "^4.0.0",
        "@types/unist": "^3.0.0",
        "decode-named-character-reference": "^1.0.0",
        "devlop": "^1.0.0",
        "mdast-util-to-string": "^4.0.0",
        "micromark": "^4.0.0",
        "micromark-util-decode-numeric-character-reference": "^2.0.0",
        "micromark-util-decode-string": "^2.0.0",
        "micromark-util-normalize-identifier": "^2.0.0",
        "micromark-util-symbol": "^2.0.0",
        "micromark-util-types": "^2.0.0",
        "unist-util-stringify-position": "^4.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/unified"
      }
    },
    "node_modules/mdast-util-mdx-expression": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/mdast-util-mdx-expression/-/mdast-util-mdx-expression-2.0.1.tgz",
      "integrity": "sha512-J6f+9hUp+ldTZqKRSg7Vw5V6MqjATc+3E4gf3CFNcuZNWD8XdyI6zQ8GqH7f8169MM6P7hMBRDVGnn7oHB9kXQ==",
      "license": "MIT",
      "dependencies": {
        "@types/estree-jsx": "^1.0.0",
        "@types/hast": "^3.0.0",
        "@types/mdast": "^4.0.0",
        "devlop": "^1.0.0",
        "mdast-util-from-markdown": "^2.0.0",
        "mdast-util-to-markdown": "^2.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/unified"
      }
    },
    "node_modules/mdast-util-mdx-jsx": {
      "version": "3.2.0",
      "resolved": "https://registry.npmjs.org/mdast-util-mdx-jsx/-/mdast-util-mdx-jsx-3.2.0.tgz",
      "integrity": "sha512-lj/z8v0r6ZtsN/cGNNtemmmfoLAFZnjMbNyLzBafjzikOM+glrjNHPlf6lQDOTccj9n5b0PPihEBbhneMyGs1Q==",
      "license": "MIT",
      "dependencies": {
        "@types/estree-jsx": "^1.0.0",
        "@types/hast": "^3.0.0",
        "@types/mdast": "^4.0.0",
        "@types/unist": "^3.0.0",
        "ccount": "^2.0.0",
        "devlop": "^1.1.0",
        "mdast-util-from-markdown": "^2.0.0",
        "mdast-util-to-markdown": "^2.0.0",
        "parse-entities": "^4.0.0",
        "stringify-entities": "^4.0.0",
        "unist-util-stringify-position": "^4.0.0",
        "vfile-message": "^4.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/unified"
      }
    },
    "node_modules/mdast-util-mdxjs-esm": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/mdast-util-mdxjs-esm/-/mdast-util-mdxjs-esm-2.0.1.tgz",
      "integrity": "sha512-EcmOpxsZ96CvlP03NghtH1EsLtr0n9Tm4lPUJUBccV9RwUOneqSycg19n5HGzCf+10LozMRSObtVr3ee1WoHtg==",
      "license": "MIT",
      "dependencies": {
        "@types/estree-jsx": "^1.0.0",
        "@types/hast": "^3.0.0",
        "@types/mdast": "^4.0.0",
        "devlop": "^1.0.0",
        "mdast-util-from-markdown": "^2.0.0",
        "mdast-util-to-markdown": "^2.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/unified"
      }
    },
    "node_modules/mdast-util-phrasing": {
      "version": "4.1.0",
      "resolved": "https://registry.npmjs.org/mdast-util-phrasing/-/mdast-util-phrasing-4.1.0.tgz",
      "integrity": "sha512-TqICwyvJJpBwvGAMZjj4J2n0X8QWp21b9l0o7eXyVJ25YNWYbJDVIyD1bZXE6WtV6RmKJVYmQAKWa0zWOABz2w==",
      "license": "MIT",
      "dependencies": {
        "@types/mdast": "^4.0.0",
        "unist-util-is": "^6.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/unified"
      }
    },
    "node_modules/mdast-util-to-hast": {
      "version": "13.2.1",
      "resolved": "https://registry.npmjs.org/mdast-util-to-hast/-/mdast-util-to-hast-13.2.1.tgz",
      "integrity": "sha512-cctsq2wp5vTsLIcaymblUriiTcZd0CwWtCbLvrOzYCDZoWyMNV8sZ7krj09FSnsiJi3WVsHLM4k6Dq/yaPyCXA==",
      "license": "MIT",
      "dependencies": {
        "@types/hast": "^3.0.0",
        "@types/mdast": "^4.0.0",
        "@ungap/structured-clone": "^1.0.0",
        "devlop": "^1.0.0",
        "micromark-util-sanitize-uri": "^2.0.0",
        "trim-lines": "^3.0.0",
        "unist-util-position": "^5.0.0",
        "unist-util-visit": "^5.0.0",
        "vfile": "^6.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/unified"
      }
    },
    "node_modules/mdast-util-to-markdown": {
      "version": "2.1.2",
      "resolved": "https://registry.npmjs.org/mdast-util-to-markdown/-/mdast-util-to-markdown-2.1.2.tgz",
      "integrity": "sha512-xj68wMTvGXVOKonmog6LwyJKrYXZPvlwabaryTjLh9LuvovB/KAH+kvi8Gjj+7rJjsFi23nkUxRQv1KqSroMqA==",
      "license": "MIT",
      "dependencies": {
        "@types/mdast": "^4.0.0",
        "@types/unist": "^3.0.0",
        "longest-streak": "^3.0.0",
        "mdast-util-phrasing": "^4.0.0",
        "mdast-util-to-string": "^4.0.0",
        "micromark-util-classify-character": "^2.0.0",
        "micromark-util-decode-string": "^2.0.0",
        "unist-util-visit": "^5.0.0",
        "zwitch": "^2.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/unified"
      }
    },
    "node_modules/mdast-util-to-string": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/mdast-util-to-string/-/mdast-util-to-string-4.0.0.tgz",
      "integrity": "sha512-0H44vDimn51F0YwvxSJSm0eCDOJTRlmN0R1yBh4HLj9wiV1Dn0QoXGbvFAWj2hSItVTlCmBF1hqKlIyUBVFLPg==",
      "license": "MIT",
      "dependencies": {
        "@types/mdast": "^4.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/unified"
      }
    },
    "node_modules/micromark": {
      "version": "4.0.2",
      "resolved": "https://registry.npmjs.org/micromark/-/micromark-4.0.2.tgz",
      "integrity": "sha512-zpe98Q6kvavpCr1NPVSCMebCKfD7CA2NqZ+rykeNhONIJBpc1tFKt9hucLGwha3jNTNI8lHpctWJWoimVF4PfA==",
      "funding": [
        {
          "type": "GitHub Sponsors",
          "url": "https://github.com/sponsors/unifiedjs"
        },
        {
          "type": "OpenCollective",
          "url": "https://opencollective.com/unified"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "@types/debug": "^4.0.0",
        "debug": "^4.0.0",
        "decode-named-character-reference": "^1.0.0",
        "devlop": "^1.0.0",
        "micromark-core-commonmark": "^2.0.0",
        "micromark-factory-space": "^2.0.0",
        "micromark-util-character": "^2.0.0",
        "micromark-util-chunked": "^2.0.0",
        "micromark-util-combine-extensions": "^2.0.0",
        "micromark-util-decode-numeric-character-reference": "^2.0.0",
        "micromark-util-encode": "^2.0.0",
        "micromark-util-normalize-identifier": "^2.0.0",
        "micromark-util-resolve-all": "^2.0.0",
        "micromark-util-sanitize-uri": "^2.0.0",
        "micromark-util-subtokenize": "^2.0.0",
        "micromark-util-symbol": "^2.0.0",
        "micromark-util-types": "^2.0.0"
      }
    },
    "node_modules/micromark-core-commonmark": {
      "version": "2.0.3",
      "resolved": "https://registry.npmjs.org/micromark-core-commonmark/-/micromark-core-commonmark-2.0.3.tgz",
      "integrity": "sha512-RDBrHEMSxVFLg6xvnXmb1Ayr2WzLAWjeSATAoxwKYJV94TeNavgoIdA0a9ytzDSVzBy2YKFK+emCPOEibLeCrg==",
      "funding": [
        {
          "type": "GitHub Sponsors",
          "url": "https://github.com/sponsors/unifiedjs"
        },
        {
          "type": "OpenCollective",
          "url": "https://opencollective.com/unified"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "decode-named-character-reference": "^1.0.0",
        "devlop": "^1.0.0",
        "micromark-factory-destination": "^2.0.0",
        "micromark-factory-label": "^2.0.0",
        "micromark-factory-space": "^2.0.0",
        "micromark-factory-title": "^2.0.0",
        "micromark-factory-whitespace": "^2.0.0",
        "micromark-util-character": "^2.0.0",
        "micromark-util-chunked": "^2.0.0",
        "micromark-util-classify-character": "^2.0.0",
        "micromark-util-html-tag-name": "^2.0.0",
        "micromark-util-normalize-identifier": "^2.0.0",
        "micromark-util-resolve-all": "^2.0.0",
        "micromark-util-subtokenize": "^2.0.0",
        "micromark-util-symbol": "^2.0.0",
        "micromark-util-types": "^2.0.0"
      }
    },
    "node_modules/micromark-factory-destination": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/micromark-factory-destination/-/micromark-factory-destination-2.0.1.tgz",
      "integrity": "sha512-Xe6rDdJlkmbFRExpTOmRj9N3MaWmbAgdpSrBQvCFqhezUn4AHqJHbaEnfbVYYiexVSs//tqOdY/DxhjdCiJnIA==",
      "funding": [
        {
          "type": "GitHub Sponsors",
          "url": "https://github.com/sponsors/unifiedjs"
        },
        {
          "type": "OpenCollective",
          "url": "https://opencollective.com/unified"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "micromark-util-character": "^2.0.0",
        "micromark-util-symbol": "^2.0.0",
        "micromark-util-types": "^2.0.0"
      }
    },
    "node_modules/micromark-factory-label": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/micromark-factory-label/-/micromark-factory-label-2.0.1.tgz",
      "integrity": "sha512-VFMekyQExqIW7xIChcXn4ok29YE3rnuyveW3wZQWWqF4Nv9Wk5rgJ99KzPvHjkmPXF93FXIbBp6YdW3t71/7Vg==",
      "funding": [
        {
          "type": "GitHub Sponsors",
          "url": "https://github.com/sponsors/unifiedjs"
        },
        {
          "type": "OpenCollective",
          "url": "https://opencollective.com/unified"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "devlop": "^1.0.0",
        "micromark-util-character": "^2.0.0",
        "micromark-util-symbol": "^2.0.0",
        "micromark-util-types": "^2.0.0"
      }
    },
    "node_modules/micromark-factory-space": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/micromark-factory-space/-/micromark-factory-space-2.0.1.tgz",
      "integrity": "sha512-zRkxjtBxxLd2Sc0d+fbnEunsTj46SWXgXciZmHq0kDYGnck/ZSGj9/wULTV95uoeYiK5hRXP2mJ98Uo4cq/LQg==",
      "funding": [
        {
          "type": "GitHub Sponsors",
          "url": "https://github.com/sponsors/unifiedjs"
        },
        {
          "type": "OpenCollective",
          "url": "https://opencollective.com/unified"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "micromark-util-character": "^2.0.0",
        "micromark-util-types": "^2.0.0"
      }
    },
    "node_modules/micromark-factory-title": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/micromark-factory-title/-/micromark-factory-title-2.0.1.tgz",
      "integrity": "sha512-5bZ+3CjhAd9eChYTHsjy6TGxpOFSKgKKJPJxr293jTbfry2KDoWkhBb6TcPVB4NmzaPhMs1Frm9AZH7OD4Cjzw==",
      "funding": [
        {
          "type": "GitHub Sponsors",
          "url": "https://github.com/sponsors/unifiedjs"
        },
        {
          "type": "OpenCollective",
          "url": "https://opencollective.com/unified"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "micromark-factory-space": "^2.0.0",
        "micromark-util-character": "^2.0.0",
        "micromark-util-symbol": "^2.0.0",
        "micromark-util-types": "^2.0.0"
      }
    },
    "node_modules/micromark-factory-whitespace": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/micromark-factory-whitespace/-/micromark-factory-whitespace-2.0.1.tgz",
      "integrity": "sha512-Ob0nuZ3PKt/n0hORHyvoD9uZhr+Za8sFoP+OnMcnWK5lngSzALgQYKMr9RJVOWLqQYuyn6ulqGWSXdwf6F80lQ==",
      "funding": [
        {
          "type": "GitHub Sponsors",
          "url": "https://github.com/sponsors/unifiedjs"
        },
        {
          "type": "OpenCollective",
          "url": "https://opencollective.com/unified"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "micromark-factory-space": "^2.0.0",
        "micromark-util-character": "^2.0.0",
        "micromark-util-symbol": "^2.0.0",
        "micromark-util-types": "^2.0.0"
      }
    },
    "node_modules/micromark-util-character": {
      "version": "2.1.1",
      "resolved": "https://registry.npmjs.org/micromark-util-character/-/micromark-util-character-2.1.1.tgz",
      "integrity": "sha512-wv8tdUTJ3thSFFFJKtpYKOYiGP2+v96Hvk4Tu8KpCAsTMs6yi+nVmGh1syvSCsaxz45J6Jbw+9DD6g97+NV67Q==",
      "funding": [
        {
          "type": "GitHub Sponsors",
          "url": "https://github.com/sponsors/unifiedjs"
        },
        {
          "type": "OpenCollective",
          "url": "https://opencollective.com/unified"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "micromark-util-symbol": "^2.0.0",
        "micromark-util-types": "^2.0.0"
      }
    },
    "node_modules/micromark-util-chunked": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/micromark-util-chunked/-/micromark-util-chunked-2.0.1.tgz",
      "integrity": "sha512-QUNFEOPELfmvv+4xiNg2sRYeS/P84pTW0TCgP5zc9FpXetHY0ab7SxKyAQCNCc1eK0459uoLI1y5oO5Vc1dbhA==",
      "funding": [
        {
          "type": "GitHub Sponsors",
          "url": "https://github.com/sponsors/unifiedjs"
        },
        {
          "type": "OpenCollective",
          "url": "https://opencollective.com/unified"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "micromark-util-symbol": "^2.0.0"
      }
    },
    "node_modules/micromark-util-classify-character": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/micromark-util-classify-character/-/micromark-util-classify-character-2.0.1.tgz",
      "integrity": "sha512-K0kHzM6afW/MbeWYWLjoHQv1sgg2Q9EccHEDzSkxiP/EaagNzCm7T/WMKZ3rjMbvIpvBiZgwR3dKMygtA4mG1Q==",
      "funding": [
        {
          "type": "GitHub Sponsors",
          "url": "https://github.com/sponsors/unifiedjs"
        },
        {
          "type": "OpenCollective",
          "url": "https://opencollective.com/unified"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "micromark-util-character": "^2.0.0",
        "micromark-util-symbol": "^2.0.0",
        "micromark-util-types": "^2.0.0"
      }
    },
    "node_modules/micromark-util-combine-extensions": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/micromark-util-combine-extensions/-/micromark-util-combine-extensions-2.0.1.tgz",
      "integrity": "sha512-OnAnH8Ujmy59JcyZw8JSbK9cGpdVY44NKgSM7E9Eh7DiLS2E9RNQf0dONaGDzEG9yjEl5hcqeIsj4hfRkLH/Bg==",
      "funding": [
        {
          "type": "GitHub Sponsors",
          "url": "https://github.com/sponsors/unifiedjs"
        },
        {
          "type": "OpenCollective",
          "url": "https://opencollective.com/unified"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "micromark-util-chunked": "^2.0.0",
        "micromark-util-types": "^2.0.0"
      }
    },
    "node_modules/micromark-util-decode-numeric-character-reference": {
      "version": "2.0.2",
      "resolved": "https://registry.npmjs.org/micromark-util-decode-numeric-character-reference/-/micromark-util-decode-numeric-character-reference-2.0.2.tgz",
      "integrity": "sha512-ccUbYk6CwVdkmCQMyr64dXz42EfHGkPQlBj5p7YVGzq8I7CtjXZJrubAYezf7Rp+bjPseiROqe7G6foFd+lEuw==",
      "funding": [
        {
          "type": "GitHub Sponsors",
          "url": "https://github.com/sponsors/unifiedjs"
        },
        {
          "type": "OpenCollective",
          "url": "https://opencollective.com/unified"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "micromark-util-symbol": "^2.0.0"
      }
    },
    "node_modules/micromark-util-decode-string": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/micromark-util-decode-string/-/micromark-util-decode-string-2.0.1.tgz",
      "integrity": "sha512-nDV/77Fj6eH1ynwscYTOsbK7rR//Uj0bZXBwJZRfaLEJ1iGBR6kIfNmlNqaqJf649EP0F3NWNdeJi03elllNUQ==",
      "funding": [
        {
          "type": "GitHub Sponsors",
          "url": "https://github.com/sponsors/unifiedjs"
        },
        {
          "type": "OpenCollective",
          "url": "https://opencollective.com/unified"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "decode-named-character-reference": "^1.0.0",
        "micromark-util-character": "^2.0.0",
        "micromark-util-decode-numeric-character-reference": "^2.0.0",
        "micromark-util-symbol": "^2.0.0"
      }
    },
    "node_modules/micromark-util-encode": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/micromark-util-encode/-/micromark-util-encode-2.0.1.tgz",
      "integrity": "sha512-c3cVx2y4KqUnwopcO9b/SCdo2O67LwJJ/UyqGfbigahfegL9myoEFoDYZgkT7f36T0bLrM9hZTAaAyH+PCAXjw==",
      "funding": [
        {
          "type": "GitHub Sponsors",
          "url": "https://github.com/sponsors/unifiedjs"
        },
        {
          "type": "OpenCollective",
          "url": "https://opencollective.com/unified"
        }
      ],
      "license": "MIT"
    },
    "node_modules/micromark-util-html-tag-name": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/micromark-util-html-tag-name/-/micromark-util-html-tag-name-2.0.1.tgz",
      "integrity": "sha512-2cNEiYDhCWKI+Gs9T0Tiysk136SnR13hhO8yW6BGNyhOC4qYFnwF1nKfD3HFAIXA5c45RrIG1ub11GiXeYd1xA==",
      "funding": [
        {
          "type": "GitHub Sponsors",
          "url": "https://github.com/sponsors/unifiedjs"
        },
        {
          "type": "OpenCollective",
          "url": "https://opencollective.com/unified"
        }
      ],
      "license": "MIT"
    },
    "node_modules/micromark-util-normalize-identifier": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/micromark-util-normalize-identifier/-/micromark-util-normalize-identifier-2.0.1.tgz",
      "integrity": "sha512-sxPqmo70LyARJs0w2UclACPUUEqltCkJ6PhKdMIDuJ3gSf/Q+/GIe3WKl0Ijb/GyH9lOpUkRAO2wp0GVkLvS9Q==",
      "funding": [
        {
          "type": "GitHub Sponsors",
          "url": "https://github.com/sponsors/unifiedjs"
        },
        {
          "type": "OpenCollective",
          "url": "https://opencollective.com/unified"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "micromark-util-symbol": "^2.0.0"
      }
    },
    "node_modules/micromark-util-resolve-all": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/micromark-util-resolve-all/-/micromark-util-resolve-all-2.0.1.tgz",
      "integrity": "sha512-VdQyxFWFT2/FGJgwQnJYbe1jjQoNTS4RjglmSjTUlpUMa95Htx9NHeYW4rGDJzbjvCsl9eLjMQwGeElsqmzcHg==",
      "funding": [
        {
          "type": "GitHub Sponsors",
          "url": "https://github.com/sponsors/unifiedjs"
        },
        {
          "type": "OpenCollective",
          "url": "https://opencollective.com/unified"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "micromark-util-types": "^2.0.0"
      }
    },
    "node_modules/micromark-util-sanitize-uri": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/micromark-util-sanitize-uri/-/micromark-util-sanitize-uri-2.0.1.tgz",
      "integrity": "sha512-9N9IomZ/YuGGZZmQec1MbgxtlgougxTodVwDzzEouPKo3qFWvymFHWcnDi2vzV1ff6kas9ucW+o3yzJK9YB1AQ==",
      "funding": [
        {
          "type": "GitHub Sponsors",
          "url": "https://github.com/sponsors/unifiedjs"
        },
        {
          "type": "OpenCollective",
          "url": "https://opencollective.com/unified"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "micromark-util-character": "^2.0.0",
        "micromark-util-encode": "^2.0.0",
        "micromark-util-symbol": "^2.0.0"
      }
    },
    "node_modules/micromark-util-subtokenize": {
      "version": "2.1.0",
      "resolved": "https://registry.npmjs.org/micromark-util-subtokenize/-/micromark-util-subtokenize-2.1.0.tgz",
      "integrity": "sha512-XQLu552iSctvnEcgXw6+Sx75GflAPNED1qx7eBJ+wydBb2KCbRZe+NwvIEEMM83uml1+2WSXpBAcp9IUCgCYWA==",
      "funding": [
        {
          "type": "GitHub Sponsors",
          "url": "https://github.com/sponsors/unifiedjs"
        },
        {
          "type": "OpenCollective",
          "url": "https://opencollective.com/unified"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "devlop": "^1.0.0",
        "micromark-util-chunked": "^2.0.0",
        "micromark-util-symbol": "^2.0.0",
        "micromark-util-types": "^2.0.0"
      }
    },
    "node_modules/micromark-util-symbol": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/micromark-util-symbol/-/micromark-util-symbol-2.0.1.tgz",
      "integrity": "sha512-vs5t8Apaud9N28kgCrRUdEed4UJ+wWNvicHLPxCa9ENlYuAY31M0ETy5y1vA33YoNPDFTghEbnh6efaE8h4x0Q==",
      "funding": [
        {
          "type": "GitHub Sponsors",
          "url": "https://github.com/sponsors/unifiedjs"
        },
        {
          "type": "OpenCollective",
          "url": "https://opencollective.com/unified"
        }
      ],
      "license": "MIT"
    },
    "node_modules/micromark-util-types": {
      "version": "2.0.2",
      "resolved": "https://registry.npmjs.org/micromark-util-types/-/micromark-util-types-2.0.2.tgz",
      "integrity": "sha512-Yw0ECSpJoViF1qTU4DC6NwtC4aWGt1EkzaQB8KPPyCRR8z9TWeV0HbEFGTO+ZY1wB22zmxnJqhPyTpOVCpeHTA==",
      "funding": [
        {
          "type": "GitHub Sponsors",
          "url": "https://github.com/sponsors/unifiedjs"
        },
        {
          "type": "OpenCollective",
          "url": "https://opencollective.com/unified"
        }
      ],
      "license": "MIT"
    },
    "node_modules/minimatch": {
      "version": "3.1.2",
      "resolved": "https://registry.npmjs.org/minimatch/-/minimatch-3.1.2.tgz",
      "integrity": "sha512-J7p63hRiAjw1NDEww1W7i37+ByIrOWO5XQQAzZ3VOcL0PNybwpfmV/N05zFAzwQ9USyEcX6t3UO+K5aqBQOIHw==",
      "dev": true,
      "license": "ISC",
      "dependencies": {
        "brace-expansion": "^1.1.7"
      },
      "engines": {
        "node": "*"
      }
    },
    "node_modules/ms": {
      "version": "2.1.3",
      "resolved": "https://registry.npmjs.org/ms/-/ms-2.1.3.tgz",
      "integrity": "sha512-6FlzubTLZG3J2a/NVCAleEhjzq5oxgHyaCU9yYXvcLsvoVaHJq/s5xXI6/XXP6tz7R9xAOtHnSO/tXtF3WRTlA==",
      "license": "MIT"
    },
    "node_modules/nanoid": {
      "version": "3.3.11",
      "resolved": "https://registry.npmjs.org/nanoid/-/nanoid-3.3.11.tgz",
      "integrity": "sha512-N8SpfPUnUp1bK+PMYW8qSWdl9U+wwNWI4QKxOYDy9JAro3WMX7p2OeVRF9v+347pnakNevPmiHhNmZ2HbFA76w==",
      "dev": true,
      "funding": [
        {
          "type": "github",
          "url": "https://github.com/sponsors/ai"
        }
      ],
      "license": "MIT",
      "bin": {
        "nanoid": "bin/nanoid.cjs"
      },
      "engines": {
        "node": "^10 || ^12 || ^13.7 || ^14 || >=15.0.1"
      }
    },
    "node_modules/natural-compare": {
      "version": "1.4.0",
      "resolved": "https://registry.npmjs.org/natural-compare/-/natural-compare-1.4.0.tgz",
      "integrity": "sha512-OWND8ei3VtNC9h7V60qff3SVobHr996CTwgxubgyQYEpg290h9J0buyECNNJexkFm5sOajh5G116RYA1c8ZMSw==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/node-releases": {
      "version": "2.0.27",
      "resolved": "https://registry.npmjs.org/node-releases/-/node-releases-2.0.27.tgz",
      "integrity": "sha512-nmh3lCkYZ3grZvqcCH+fjmQ7X+H0OeZgP40OierEaAptX4XofMh5kwNbWh7lBduUzCcV/8kZ+NDLCwm2iorIlA==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/optionator": {
      "version": "0.9.4",
      "resolved": "https://registry.npmjs.org/optionator/-/optionator-0.9.4.tgz",
      "integrity": "sha512-6IpQ7mKUxRcZNLIObR0hz7lxsapSSIYNZJwXPGeF0mTVqGKFIXj1DQcMoT22S3ROcLyY/rz0PWaWZ9ayWmad9g==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "deep-is": "^0.1.3",
        "fast-levenshtein": "^2.0.6",
        "levn": "^0.4.1",
        "prelude-ls": "^1.2.1",
        "type-check": "^0.4.0",
        "word-wrap": "^1.2.5"
      },
      "engines": {
        "node": ">= 0.8.0"
      }
    },
    "node_modules/p-limit": {
      "version": "3.1.0",
      "resolved": "https://registry.npmjs.org/p-limit/-/p-limit-3.1.0.tgz",
      "integrity": "sha512-TYOanM3wGwNGsZN2cVTYPArw454xnXj5qmWF1bEoAc4+cU/ol7GVh7odevjp1FNHduHc3KZMcFduxU5Xc6uJRQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "yocto-queue": "^0.1.0"
      },
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/p-locate": {
      "version": "5.0.0",
      "resolved": "https://registry.npmjs.org/p-locate/-/p-locate-5.0.0.tgz",
      "integrity": "sha512-LaNjtRWUBY++zB5nE/NwcaoMylSPk+S+ZHNB1TzdbMJMny6dynpAGt7X/tl/QYq3TIeE6nxHppbo2LGymrG5Pw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "p-limit": "^3.0.2"
      },
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/parent-module": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/parent-module/-/parent-module-1.0.1.tgz",
      "integrity": "sha512-GQ2EWRpQV8/o+Aw8YqtfZZPfNRWZYkbidE9k5rpl/hC3vtHHBfGm2Ifi6qWV+coDGkrUKZAxE3Lot5kcsRlh+g==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "callsites": "^3.0.0"
      },
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/parse-entities": {
      "version": "4.0.2",
      "resolved": "https://registry.npmjs.org/parse-entities/-/parse-entities-4.0.2.tgz",
      "integrity": "sha512-GG2AQYWoLgL877gQIKeRPGO1xF9+eG1ujIb5soS5gPvLQ1y2o8FL90w2QWNdf9I361Mpp7726c+lj3U0qK1uGw==",
      "license": "MIT",
      "dependencies": {
        "@types/unist": "^2.0.0",
        "character-entities-legacy": "^3.0.0",
        "character-reference-invalid": "^2.0.0",
        "decode-named-character-reference": "^1.0.0",
        "is-alphanumerical": "^2.0.0",
        "is-decimal": "^2.0.0",
        "is-hexadecimal": "^2.0.0"
      },
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/parse-entities/node_modules/@types/unist": {
      "version": "2.0.11",
      "resolved": "https://registry.npmjs.org/@types/unist/-/unist-2.0.11.tgz",
      "integrity": "sha512-CmBKiL6NNo/OqgmMn95Fk9Whlp2mtvIv+KNpQKN2F4SjvrEesubTRWGYSg+BnWZOnlCaSTU1sMpsBOzgbYhnsA==",
      "license": "MIT"
    },
    "node_modules/path-exists": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/path-exists/-/path-exists-4.0.0.tgz",
      "integrity": "sha512-ak9Qy5Q7jYb2Wwcey5Fpvg2KoAc/ZIhLSLOSBmRmygPsGwkVVt0fZa0qrtMz+m6tJTAHfZQ8FnmB4MG4LWy7/w==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/path-key": {
      "version": "3.1.1",
      "resolved": "https://registry.npmjs.org/path-key/-/path-key-3.1.1.tgz",
      "integrity": "sha512-ojmeN0qd+y0jszEtoY48r0Peq5dwMEkIlCOu6Q5f41lfkswXuKtYrhgoTpLnyIcHm24Uhqx+5Tqm2InSwLhE6Q==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/picocolors": {
      "version": "1.1.1",
      "resolved": "https://registry.npmjs.org/picocolors/-/picocolors-1.1.1.tgz",
      "integrity": "sha512-xceH2snhtb5M9liqDsmEw56le376mTZkEX/jEb/RxNFyegNul7eNslCXP9FDj/Lcu0X8KEyMceP2ntpaHrDEVA==",
      "dev": true,
      "license": "ISC"
    },
    "node_modules/picomatch": {
      "version": "4.0.3",
      "resolved": "https://registry.npmjs.org/picomatch/-/picomatch-4.0.3.tgz",
      "integrity": "sha512-5gTmgEY/sqK6gFXLIsQNH19lWb4ebPDLA4SdLP7dsWkIXHWlG66oPuVvXSGFPppYZz8ZDZq0dYYrbHfBCVUb1Q==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=12"
      },
      "funding": {
        "url": "https://github.com/sponsors/jonschlinkert"
      }
    },
    "node_modules/postcss": {
      "version": "8.5.6",
      "resolved": "https://registry.npmjs.org/postcss/-/postcss-8.5.6.tgz",
      "integrity": "sha512-3Ybi1tAuwAP9s0r1UQ2J4n5Y0G05bJkpUIO0/bI9MhwmD70S5aTWbXGBwxHrelT+XM1k6dM0pk+SwNkpTRN7Pg==",
      "dev": true,
      "funding": [
        {
          "type": "opencollective",
          "url": "https://opencollective.com/postcss/"
        },
        {
          "type": "tidelift",
          "url": "https://tidelift.com/funding/github/npm/postcss"
        },
        {
          "type": "github",
          "url": "https://github.com/sponsors/ai"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "nanoid": "^3.3.11",
        "picocolors": "^1.1.1",
        "source-map-js": "^1.2.1"
      },
      "engines": {
        "node": "^10 || ^12 || >=14"
      }
    },
    "node_modules/prelude-ls": {
      "version": "1.2.1",
      "resolved": "https://registry.npmjs.org/prelude-ls/-/prelude-ls-1.2.1.tgz",
      "integrity": "sha512-vkcDPrRZo1QZLbn5RLGPpg/WmIQ65qoWWhcGKf/b5eplkkarX0m9z8ppCat4mlOqUsWpyNuYgO3VRyrYHSzX5g==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">= 0.8.0"
      }
    },
    "node_modules/prismjs": {
      "version": "1.30.0",
      "resolved": "https://registry.npmjs.org/prismjs/-/prismjs-1.30.0.tgz",
      "integrity": "sha512-DEvV2ZF2r2/63V+tK8hQvrR2ZGn10srHbXviTlcv7Kpzw8jWiNTqbVgjO3IY8RxrrOUF8VPMQQFysYYYv0YZxw==",
      "license": "MIT",
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/property-information": {
      "version": "7.1.0",
      "resolved": "https://registry.npmjs.org/property-information/-/property-information-7.1.0.tgz",
      "integrity": "sha512-TwEZ+X+yCJmYfL7TPUOcvBZ4QfoT5YenQiJuX//0th53DE6w0xxLEtfK3iyryQFddXuvkIk51EEgrJQ0WJkOmQ==",
      "license": "MIT",
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/punycode": {
      "version": "2.3.1",
      "resolved": "https://registry.npmjs.org/punycode/-/punycode-2.3.1.tgz",
      "integrity": "sha512-vYt7UD1U9Wg6138shLtLOvdAu+8DsC/ilFtEVHcH+wydcSpNE20AfSOduf6MkRFahL5FY7X1oU7nKVZFtfq8Fg==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/react": {
      "version": "19.2.1",
      "resolved": "https://registry.npmjs.org/react/-/react-19.2.1.tgz",
      "integrity": "sha512-DGrYcCWK7tvYMnWh79yrPHt+vdx9tY+1gPZa7nJQtO/p8bLTDaHp4dzwEhQB7pZ4Xe3ok4XKuEPrVuc+wlpkmw==",
      "license": "MIT",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/react-dom": {
      "version": "19.2.1",
      "resolved": "https://registry.npmjs.org/react-dom/-/react-dom-19.2.1.tgz",
      "integrity": "sha512-ibrK8llX2a4eOskq1mXKu/TGZj9qzomO+sNfO98M6d9zIPOEhlBkMkBUBLd1vgS0gQsLDBzA+8jJBVXDnfHmJg==",
      "license": "MIT",
      "dependencies": {
        "scheduler": "^0.27.0"
      },
      "peerDependencies": {
        "react": "^19.2.1"
      }
    },
    "node_modules/react-markdown": {
      "version": "10.1.0",
      "resolved": "https://registry.npmjs.org/react-markdown/-/react-markdown-10.1.0.tgz",
      "integrity": "sha512-qKxVopLT/TyA6BX3Ue5NwabOsAzm0Q7kAPwq6L+wWDwisYs7R8vZ0nRXqq6rkueboxpkjvLGU9fWifiX/ZZFxQ==",
      "license": "MIT",
      "dependencies": {
        "@types/hast": "^3.0.0",
        "@types/mdast": "^4.0.0",
        "devlop": "^1.0.0",
        "hast-util-to-jsx-runtime": "^2.0.0",
        "html-url-attributes": "^3.0.0",
        "mdast-util-to-hast": "^13.0.0",
        "remark-parse": "^11.0.0",
        "remark-rehype": "^11.0.0",
        "unified": "^11.0.0",
        "unist-util-visit": "^5.0.0",
        "vfile": "^6.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/unified"
      },
      "peerDependencies": {
        "@types/react": ">=18",
        "react": ">=18"
      }
    },
    "node_modules/react-refresh": {
      "version": "0.18.0",
      "resolved": "https://registry.npmjs.org/react-refresh/-/react-refresh-0.18.0.tgz",
      "integrity": "sha512-QgT5//D3jfjJb6Gsjxv0Slpj23ip+HtOpnNgnb2S5zU3CB26G/IDPGoy4RJB42wzFE46DRsstbW6tKHoKbhAxw==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/react-router": {
      "version": "7.11.0",
      "resolved": "https://registry.npmjs.org/react-router/-/react-router-7.11.0.tgz",
      "integrity": "sha512-uI4JkMmjbWCZc01WVP2cH7ZfSzH91JAZUDd7/nIprDgWxBV1TkkmLToFh7EbMTcMak8URFRa2YoBL/W8GWnCTQ==",
      "license": "MIT",
      "dependencies": {
        "cookie": "^1.0.1",
        "set-cookie-parser": "^2.6.0"
      },
      "engines": {
        "node": ">=20.0.0"
      },
      "peerDependencies": {
        "react": ">=18",
        "react-dom": ">=18"
      },
      "peerDependenciesMeta": {
        "react-dom": {
          "optional": true
        }
      }
    },
    "node_modules/react-router-dom": {
      "version": "7.11.0",
      "resolved": "https://registry.npmjs.org/react-router-dom/-/react-router-dom-7.11.0.tgz",
      "integrity": "sha512-e49Ir/kMGRzFOOrYQBdoitq3ULigw4lKbAyKusnvtDu2t4dBX4AGYPrzNvorXmVuOyeakai6FUPW5MmibvVG8g==",
      "license": "MIT",
      "dependencies": {
        "react-router": "7.11.0"
      },
      "engines": {
        "node": ">=20.0.0"
      },
      "peerDependencies": {
        "react": ">=18",
        "react-dom": ">=18"
      }
    },
    "node_modules/react-syntax-highlighter": {
      "version": "15.6.6",
      "resolved": "https://registry.npmjs.org/react-syntax-highlighter/-/react-syntax-highlighter-15.6.6.tgz",
      "integrity": "sha512-DgXrc+AZF47+HvAPEmn7Ua/1p10jNoVZVI/LoPiYdtY+OM+/nG5yefLHKJwdKqY1adMuHFbeyBaG9j64ML7vTw==",
      "license": "MIT",
      "dependencies": {
        "@babel/runtime": "^7.3.1",
        "highlight.js": "^10.4.1",
        "highlightjs-vue": "^1.0.0",
        "lowlight": "^1.17.0",
        "prismjs": "^1.30.0",
        "refractor": "^3.6.0"
      },
      "peerDependencies": {
        "react": ">= 0.14.0"
      }
    },
    "node_modules/refractor": {
      "version": "3.6.0",
      "resolved": "https://registry.npmjs.org/refractor/-/refractor-3.6.0.tgz",
      "integrity": "sha512-MY9W41IOWxxk31o+YvFCNyNzdkc9M20NoZK5vq6jkv4I/uh2zkWcfudj0Q1fovjUQJrNewS9NMzeTtqPf+n5EA==",
      "license": "MIT",
      "dependencies": {
        "hastscript": "^6.0.0",
        "parse-entities": "^2.0.0",
        "prismjs": "~1.27.0"
      },
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/refractor/node_modules/character-entities": {
      "version": "1.2.4",
      "resolved": "https://registry.npmjs.org/character-entities/-/character-entities-1.2.4.tgz",
      "integrity": "sha512-iBMyeEHxfVnIakwOuDXpVkc54HijNgCyQB2w0VfGQThle6NXn50zU6V/u+LDhxHcDUPojn6Kpga3PTAD8W1bQw==",
      "license": "MIT",
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/refractor/node_modules/character-entities-legacy": {
      "version": "1.1.4",
      "resolved": "https://registry.npmjs.org/character-entities-legacy/-/character-entities-legacy-1.1.4.tgz",
      "integrity": "sha512-3Xnr+7ZFS1uxeiUDvV02wQ+QDbc55o97tIV5zHScSPJpcLm/r0DFPcoY3tYRp+VZukxuMeKgXYmsXQHO05zQeA==",
      "license": "MIT",
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/refractor/node_modules/character-reference-invalid": {
      "version": "1.1.4",
      "resolved": "https://registry.npmjs.org/character-reference-invalid/-/character-reference-invalid-1.1.4.tgz",
      "integrity": "sha512-mKKUkUbhPpQlCOfIuZkvSEgktjPFIsZKRRbC6KWVEMvlzblj3i3asQv5ODsrwt0N3pHAEvjP8KTQPHkp0+6jOg==",
      "license": "MIT",
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/refractor/node_modules/is-alphabetical": {
      "version": "1.0.4",
      "resolved": "https://registry.npmjs.org/is-alphabetical/-/is-alphabetical-1.0.4.tgz",
      "integrity": "sha512-DwzsA04LQ10FHTZuL0/grVDk4rFoVH1pjAToYwBrHSxcrBIGQuXrQMtD5U1b0U2XVgKZCTLLP8u2Qxqhy3l2Vg==",
      "license": "MIT",
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/refractor/node_modules/is-alphanumerical": {
      "version": "1.0.4",
      "resolved": "https://registry.npmjs.org/is-alphanumerical/-/is-alphanumerical-1.0.4.tgz",
      "integrity": "sha512-UzoZUr+XfVz3t3v4KyGEniVL9BDRoQtY7tOyrRybkVNjDFWyo1yhXNGrrBTQxp3ib9BLAWs7k2YKBQsFRkZG9A==",
      "license": "MIT",
      "dependencies": {
        "is-alphabetical": "^1.0.0",
        "is-decimal": "^1.0.0"
      },
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/refractor/node_modules/is-decimal": {
      "version": "1.0.4",
      "resolved": "https://registry.npmjs.org/is-decimal/-/is-decimal-1.0.4.tgz",
      "integrity": "sha512-RGdriMmQQvZ2aqaQq3awNA6dCGtKpiDFcOzrTWrDAT2MiWrKQVPmxLGHl7Y2nNu6led0kEyoX0enY0qXYsv9zw==",
      "license": "MIT",
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/refractor/node_modules/is-hexadecimal": {
      "version": "1.0.4",
      "resolved": "https://registry.npmjs.org/is-hexadecimal/-/is-hexadecimal-1.0.4.tgz",
      "integrity": "sha512-gyPJuv83bHMpocVYoqof5VDiZveEoGoFL8m3BXNb2VW8Xs+rz9kqO8LOQ5DH6EsuvilT1ApazU0pyl+ytbPtlw==",
      "license": "MIT",
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/refractor/node_modules/parse-entities": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/parse-entities/-/parse-entities-2.0.0.tgz",
      "integrity": "sha512-kkywGpCcRYhqQIchaWqZ875wzpS/bMKhz5HnN3p7wveJTkTtyAB/AlnS0f8DFSqYW1T82t6yEAkEcB+A1I3MbQ==",
      "license": "MIT",
      "dependencies": {
        "character-entities": "^1.0.0",
        "character-entities-legacy": "^1.0.0",
        "character-reference-invalid": "^1.0.0",
        "is-alphanumerical": "^1.0.0",
        "is-decimal": "^1.0.0",
        "is-hexadecimal": "^1.0.0"
      },
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/refractor/node_modules/prismjs": {
      "version": "1.27.0",
      "resolved": "https://registry.npmjs.org/prismjs/-/prismjs-1.27.0.tgz",
      "integrity": "sha512-t13BGPUlFDR7wRB5kQDG4jjl7XeuH6jbJGt11JHPL96qwsEHNX2+68tFXqc1/k+/jALsbSWJKUOT/hcYAZ5LkA==",
      "license": "MIT",
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/remark-parse": {
      "version": "11.0.0",
      "resolved": "https://registry.npmjs.org/remark-parse/-/remark-parse-11.0.0.tgz",
      "integrity": "sha512-FCxlKLNGknS5ba/1lmpYijMUzX2esxW5xQqjWxw2eHFfS2MSdaHVINFmhjo+qN1WhZhNimq0dZATN9pH0IDrpA==",
      "license": "MIT",
      "dependencies": {
        "@types/mdast": "^4.0.0",
        "mdast-util-from-markdown": "^2.0.0",
        "micromark-util-types": "^2.0.0",
        "unified": "^11.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/unified"
      }
    },
    "node_modules/remark-rehype": {
      "version": "11.1.2",
      "resolved": "https://registry.npmjs.org/remark-rehype/-/remark-rehype-11.1.2.tgz",
      "integrity": "sha512-Dh7l57ianaEoIpzbp0PC9UKAdCSVklD8E5Rpw7ETfbTl3FqcOOgq5q2LVDhgGCkaBv7p24JXikPdvhhmHvKMsw==",
      "license": "MIT",
      "dependencies": {
        "@types/hast": "^3.0.0",
        "@types/mdast": "^4.0.0",
        "mdast-util-to-hast": "^13.0.0",
        "unified": "^11.0.0",
        "vfile": "^6.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/unified"
      }
    },
    "node_modules/resolve-from": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/resolve-from/-/resolve-from-4.0.0.tgz",
      "integrity": "sha512-pb/MYmXstAkysRFx8piNI1tGFNQIFA3vkE3Gq4EuA1dF6gHp/+vgZqsCGJapvy8N3Q+4o7FwvquPJcnZ7RYy4g==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=4"
      }
    },
    "node_modules/rollup": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/rollup/-/rollup-4.53.3.tgz",
      "integrity": "sha512-w8GmOxZfBmKknvdXU1sdM9NHcoQejwF/4mNgj2JuEEdRaHwwF12K7e9eXn1nLZ07ad+du76mkVsyeb2rKGllsA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@types/estree": "1.0.8"
      },
      "bin": {
        "rollup": "dist/bin/rollup"
      },
      "engines": {
        "node": ">=18.0.0",
        "npm": ">=8.0.0"
      },
      "optionalDependencies": {
        "@rollup/rollup-android-arm-eabi": "4.53.3",
        "@rollup/rollup-android-arm64": "4.53.3",
        "@rollup/rollup-darwin-arm64": "4.53.3",
        "@rollup/rollup-darwin-x64": "4.53.3",
        "@rollup/rollup-freebsd-arm64": "4.53.3",
        "@rollup/rollup-freebsd-x64": "4.53.3",
        "@rollup/rollup-linux-arm-gnueabihf": "4.53.3",
        "@rollup/rollup-linux-arm-musleabihf": "4.53.3",
        "@rollup/rollup-linux-arm64-gnu": "4.53.3",
        "@rollup/rollup-linux-arm64-musl": "4.53.3",
        "@rollup/rollup-linux-loong64-gnu": "4.53.3",
        "@rollup/rollup-linux-ppc64-gnu": "4.53.3",
        "@rollup/rollup-linux-riscv64-gnu": "4.53.3",
        "@rollup/rollup-linux-riscv64-musl": "4.53.3",
        "@rollup/rollup-linux-s390x-gnu": "4.53.3",
        "@rollup/rollup-linux-x64-gnu": "4.53.3",
        "@rollup/rollup-linux-x64-musl": "4.53.3",
        "@rollup/rollup-openharmony-arm64": "4.53.3",
        "@rollup/rollup-win32-arm64-msvc": "4.53.3",
        "@rollup/rollup-win32-ia32-msvc": "4.53.3",
        "@rollup/rollup-win32-x64-gnu": "4.53.3",
        "@rollup/rollup-win32-x64-msvc": "4.53.3",
        "fsevents": "~2.3.2"
      }
    },
    "node_modules/scheduler": {
      "version": "0.27.0",
      "resolved": "https://registry.npmjs.org/scheduler/-/scheduler-0.27.0.tgz",
      "integrity": "sha512-eNv+WrVbKu1f3vbYJT/xtiF5syA5HPIMtf9IgY/nKg0sWqzAUEvqY/xm7OcZc/qafLx/iO9FgOmeSAp4v5ti/Q==",
      "license": "MIT"
    },
    "node_modules/semver": {
      "version": "6.3.1",
      "resolved": "https://registry.npmjs.org/semver/-/semver-6.3.1.tgz",
      "integrity": "sha512-BR7VvDCVHO+q2xBEWskxS6DJE1qRnb7DxzUrogb71CWoSficBxYsiAGd+Kl0mmq/MprG9yArRkyrQxTO6XjMzA==",
      "dev": true,
      "license": "ISC",
      "bin": {
        "semver": "bin/semver.js"
      }
    },
    "node_modules/set-cookie-parser": {
      "version": "2.7.2",
      "resolved": "https://registry.npmjs.org/set-cookie-parser/-/set-cookie-parser-2.7.2.tgz",
      "integrity": "sha512-oeM1lpU/UvhTxw+g3cIfxXHyJRc/uidd3yK1P242gzHds0udQBYzs3y8j4gCCW+ZJ7ad0yctld8RYO+bdurlvw==",
      "license": "MIT"
    },
    "node_modules/shebang-command": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/shebang-command/-/shebang-command-2.0.0.tgz",
      "integrity": "sha512-kHxr2zZpYtdmrN1qDjrrX/Z1rR1kG8Dx+gkpK1G4eXmvXswmcE1hTWBWYUzlraYw1/yZp6YuDY77YtvbN0dmDA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "shebang-regex": "^3.0.0"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/shebang-regex": {
      "version": "3.0.0",
      "resolved": "https://registry.npmjs.org/shebang-regex/-/shebang-regex-3.0.0.tgz",
      "integrity": "sha512-7++dFhtcx3353uBaq8DDR4NuxBetBzC7ZQOhmTQInHEd6bSrXdiEyzCvG07Z44UYdLShWUyXt5M/yhz8ekcb1A==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/source-map-js": {
      "version": "1.2.1",
      "resolved": "https://registry.npmjs.org/source-map-js/-/source-map-js-1.2.1.tgz",
      "integrity": "sha512-UXWMKhLOwVKb728IUtQPXxfYU+usdybtUrK/8uGE8CQMvrhOpwvzDBwj0QhSL7MQc7vIsISBG8VQ8+IDQxpfQA==",
      "dev": true,
      "license": "BSD-3-Clause",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/space-separated-tokens": {
      "version": "2.0.2",
      "resolved": "https://registry.npmjs.org/space-separated-tokens/-/space-separated-tokens-2.0.2.tgz",
      "integrity": "sha512-PEGlAwrG8yXGXRjW32fGbg66JAlOAwbObuqVoJpv/mRgoWDQfgH1wDPvtzWyUSNAXBGSk8h755YDbbcEy3SH2Q==",
      "license": "MIT",
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/stringify-entities": {
      "version": "4.0.4",
      "resolved": "https://registry.npmjs.org/stringify-entities/-/stringify-entities-4.0.4.tgz",
      "integrity": "sha512-IwfBptatlO+QCJUo19AqvrPNqlVMpW9YEL2LIVY+Rpv2qsjCGxaDLNRgeGsQWJhfItebuJhsGSLjaBbNSQ+ieg==",
      "license": "MIT",
      "dependencies": {
        "character-entities-html4": "^2.0.0",
        "character-entities-legacy": "^3.0.0"
      },
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/strip-json-comments": {
      "version": "3.1.1",
      "resolved": "https://registry.npmjs.org/strip-json-comments/-/strip-json-comments-3.1.1.tgz",
      "integrity": "sha512-6fPc+R4ihwqP6N/aIv2f1gMH8lOVtWQHoqC4yK6oSDVVocumAsfCqjkXnqiYMhmMwS/mEHLp7Vehlt3ql6lEig==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=8"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/style-to-js": {
      "version": "1.1.21",
      "resolved": "https://registry.npmjs.org/style-to-js/-/style-to-js-1.1.21.tgz",
      "integrity": "sha512-RjQetxJrrUJLQPHbLku6U/ocGtzyjbJMP9lCNK7Ag0CNh690nSH8woqWH9u16nMjYBAok+i7JO1NP2pOy8IsPQ==",
      "license": "MIT",
      "dependencies": {
        "style-to-object": "1.0.14"
      }
    },
    "node_modules/style-to-object": {
      "version": "1.0.14",
      "resolved": "https://registry.npmjs.org/style-to-object/-/style-to-object-1.0.14.tgz",
      "integrity": "sha512-LIN7rULI0jBscWQYaSswptyderlarFkjQ+t79nzty8tcIAceVomEVlLzH5VP4Cmsv6MtKhs7qaAiwlcp+Mgaxw==",
      "license": "MIT",
      "dependencies": {
        "inline-style-parser": "0.2.7"
      }
    },
    "node_modules/supports-color": {
      "version": "7.2.0",
      "resolved": "https://registry.npmjs.org/supports-color/-/supports-color-7.2.0.tgz",
      "integrity": "sha512-qpCAvRl9stuOHveKsn7HncJRvv501qIacKzQlO/+Lwxc9+0q2wLyv4Dfvt80/DPn2pqOBsJdDiogXGR9+OvwRw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "has-flag": "^4.0.0"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/tinyglobby": {
      "version": "0.2.15",
      "resolved": "https://registry.npmjs.org/tinyglobby/-/tinyglobby-0.2.15.tgz",
      "integrity": "sha512-j2Zq4NyQYG5XMST4cbs02Ak8iJUdxRM0XI5QyxXuZOzKOINmWurp3smXu3y5wDcJrptwpSjgXHzIQxR0omXljQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "fdir": "^6.5.0",
        "picomatch": "^4.0.3"
      },
      "engines": {
        "node": ">=12.0.0"
      },
      "funding": {
        "url": "https://github.com/sponsors/SuperchupuDev"
      }
    },
    "node_modules/trim-lines": {
      "version": "3.0.1",
      "resolved": "https://registry.npmjs.org/trim-lines/-/trim-lines-3.0.1.tgz",
      "integrity": "sha512-kRj8B+YHZCc9kQYdWfJB2/oUl9rA99qbowYYBtr4ui4mZyAQ2JpvVBd/6U2YloATfqBhBTSMhTpgBHtU0Mf3Rg==",
      "license": "MIT",
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/trough": {
      "version": "2.2.0",
      "resolved": "https://registry.npmjs.org/trough/-/trough-2.2.0.tgz",
      "integrity": "sha512-tmMpK00BjZiUyVyvrBK7knerNgmgvcV/KLVyuma/SC+TQN167GrMRciANTz09+k3zW8L8t60jWO1GpfkZdjTaw==",
      "license": "MIT",
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    },
    "node_modules/ts-api-utils": {
      "version": "2.1.0",
      "resolved": "https://registry.npmjs.org/ts-api-utils/-/ts-api-utils-2.1.0.tgz",
      "integrity": "sha512-CUgTZL1irw8u29bzrOD/nH85jqyc74D6SshFgujOIA7osm2Rz7dYH77agkx7H4FBNxDq7Cjf+IjaX/8zwFW+ZQ==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=18.12"
      },
      "peerDependencies": {
        "typescript": ">=4.8.4"
      }
    },
    "node_modules/type-check": {
      "version": "0.4.0",
      "resolved": "https://registry.npmjs.org/type-check/-/type-check-0.4.0.tgz",
      "integrity": "sha512-XleUoc9uwGXqjWwXaUTZAmzMcFZ5858QA2vvx1Ur5xIcixXIP+8LnFDgRplU30us6teqdlskFfu+ae4K79Ooew==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "prelude-ls": "^1.2.1"
      },
      "engines": {
        "node": ">= 0.8.0"
      }
    },
    "node_modules/typescript": {
      "version": "5.9.3",
      "resolved": "https://registry.npmjs.org/typescript/-/typescript-5.9.3.tgz",
      "integrity": "sha512-jl1vZzPDinLr9eUt3J/t7V6FgNEw9QjvBPdysz9KfQDD41fQrC2Y4vKQdiaUpFT4bXlb1RHhLpp8wtm6M5TgSw==",
      "dev": true,
      "license": "Apache-2.0",
      "bin": {
        "tsc": "bin/tsc",
        "tsserver": "bin/tsserver"
      },
      "engines": {
        "node": ">=14.17"
      }
    },
    "node_modules/typescript-eslint": {
      "version": "8.49.0",
      "resolved": "https://registry.npmjs.org/typescript-eslint/-/typescript-eslint-8.49.0.tgz",
      "integrity": "sha512-zRSVH1WXD0uXczCXw+nsdjGPUdx4dfrs5VQoHnUWmv1U3oNlAKv4FUNdLDhVUg+gYn+a5hUESqch//Rv5wVhrg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@typescript-eslint/eslint-plugin": "8.49.0",
        "@typescript-eslint/parser": "8.49.0",
        "@typescript-eslint/typescript-estree": "8.49.0",
        "@typescript-eslint/utils": "8.49.0"
      },
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/typescript-eslint"
      },
      "peerDependencies": {
        "eslint": "^8.57.0 || ^9.0.0",
        "typescript": ">=4.8.4 <6.0.0"
      }
    },
    "node_modules/undici-types": {
      "version": "7.16.0",
      "resolved": "https://registry.npmjs.org/undici-types/-/undici-types-7.16.0.tgz",
      "integrity": "sha512-Zz+aZWSj8LE6zoxD+xrjh4VfkIG8Ya6LvYkZqtUQGJPZjYl53ypCaUwWqo7eI0x66KBGeRo+mlBEkMSeSZ38Nw==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/unified": {
      "version": "11.0.5",
      "resolved": "https://registry.npmjs.org/unified/-/unified-11.0.5.tgz",
      "integrity": "sha512-xKvGhPWw3k84Qjh8bI3ZeJjqnyadK+GEFtazSfZv/rKeTkTjOJho6mFqh2SM96iIcZokxiOpg78GazTSg8+KHA==",
      "license": "MIT",
      "dependencies": {
        "@types/unist": "^3.0.0",
        "bail": "^2.0.0",
        "devlop": "^1.0.0",
        "extend": "^3.0.0",
        "is-plain-obj": "^4.0.0",
        "trough": "^2.0.0",
        "vfile": "^6.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/unified"
      }
    },
    "node_modules/unist-util-is": {
      "version": "6.0.1",
      "resolved": "https://registry.npmjs.org/unist-util-is/-/unist-util-is-6.0.1.tgz",
      "integrity": "sha512-LsiILbtBETkDz8I9p1dQ0uyRUWuaQzd/cuEeS1hoRSyW5E5XGmTzlwY1OrNzzakGowI9Dr/I8HVaw4hTtnxy8g==",
      "license": "MIT",
      "dependencies": {
        "@types/unist": "^3.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/unified"
      }
    },
    "node_modules/unist-util-position": {
      "version": "5.0.0",
      "resolved": "https://registry.npmjs.org/unist-util-position/-/unist-util-position-5.0.0.tgz",
      "integrity": "sha512-fucsC7HjXvkB5R3kTCO7kUjRdrS0BJt3M/FPxmHMBOm8JQi2BsHAHFsy27E0EolP8rp0NzXsJ+jNPyDWvOJZPA==",
      "license": "MIT",
      "dependencies": {
        "@types/unist": "^3.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/unified"
      }
    },
    "node_modules/unist-util-stringify-position": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/unist-util-stringify-position/-/unist-util-stringify-position-4.0.0.tgz",
      "integrity": "sha512-0ASV06AAoKCDkS2+xw5RXJywruurpbC4JZSm7nr7MOt1ojAzvyyaO+UxZf18j8FCF6kmzCZKcAgN/yu2gm2XgQ==",
      "license": "MIT",
      "dependencies": {
        "@types/unist": "^3.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/unified"
      }
    },
    "node_modules/unist-util-visit": {
      "version": "5.0.0",
      "resolved": "https://registry.npmjs.org/unist-util-visit/-/unist-util-visit-5.0.0.tgz",
      "integrity": "sha512-MR04uvD+07cwl/yhVuVWAtw+3GOR/knlL55Nd/wAdblk27GCVt3lqpTivy/tkJcZoNPzTwS1Y+KMojlLDhoTzg==",
      "license": "MIT",
      "dependencies": {
        "@types/unist": "^3.0.0",
        "unist-util-is": "^6.0.0",
        "unist-util-visit-parents": "^6.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/unified"
      }
    },
    "node_modules/unist-util-visit-parents": {
      "version": "6.0.2",
      "resolved": "https://registry.npmjs.org/unist-util-visit-parents/-/unist-util-visit-parents-6.0.2.tgz",
      "integrity": "sha512-goh1s1TBrqSqukSc8wrjwWhL0hiJxgA8m4kFxGlQ+8FYQ3C/m11FcTs4YYem7V664AhHVvgoQLk890Ssdsr2IQ==",
      "license": "MIT",
      "dependencies": {
        "@types/unist": "^3.0.0",
        "unist-util-is": "^6.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/unified"
      }
    },
    "node_modules/update-browserslist-db": {
      "version": "1.2.2",
      "resolved": "https://registry.npmjs.org/update-browserslist-db/-/update-browserslist-db-1.2.2.tgz",
      "integrity": "sha512-E85pfNzMQ9jpKkA7+TJAi4TJN+tBCuWh5rUcS/sv6cFi+1q9LYDwDI5dpUL0u/73EElyQ8d3TEaeW4sPedBqYA==",
      "dev": true,
      "funding": [
        {
          "type": "opencollective",
          "url": "https://opencollective.com/browserslist"
        },
        {
          "type": "tidelift",
          "url": "https://tidelift.com/funding/github/npm/browserslist"
        },
        {
          "type": "github",
          "url": "https://github.com/sponsors/ai"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "escalade": "^3.2.0",
        "picocolors": "^1.1.1"
      },
      "bin": {
        "update-browserslist-db": "cli.js"
      },
      "peerDependencies": {
        "browserslist": ">= 4.21.0"
      }
    },
    "node_modules/uri-js": {
      "version": "4.4.1",
      "resolved": "https://registry.npmjs.org/uri-js/-/uri-js-4.4.1.tgz",
      "integrity": "sha512-7rKUyy33Q1yc98pQ1DAmLtwX109F7TIfWlW1Ydo8Wl1ii1SeHieeh0HHfPeL2fMXK6z0s8ecKs9frCuLJvndBg==",
      "dev": true,
      "license": "BSD-2-Clause",
      "dependencies": {
        "punycode": "^2.1.0"
      }
    },
    "node_modules/vfile": {
      "version": "6.0.3",
      "resolved": "https://registry.npmjs.org/vfile/-/vfile-6.0.3.tgz",
      "integrity": "sha512-KzIbH/9tXat2u30jf+smMwFCsno4wHVdNmzFyL+T/L3UGqqk6JKfVqOFOZEpZSHADH1k40ab6NUIXZq422ov3Q==",
      "license": "MIT",
      "dependencies": {
        "@types/unist": "^3.0.0",
        "vfile-message": "^4.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/unified"
      }
    },
    "node_modules/vfile-message": {
      "version": "4.0.3",
      "resolved": "https://registry.npmjs.org/vfile-message/-/vfile-message-4.0.3.tgz",
      "integrity": "sha512-QTHzsGd1EhbZs4AsQ20JX1rC3cOlt/IWJruk893DfLRr57lcnOeMaWG4K0JrRta4mIJZKth2Au3mM3u03/JWKw==",
      "license": "MIT",
      "dependencies": {
        "@types/unist": "^3.0.0",
        "unist-util-stringify-position": "^4.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/unified"
      }
    },
    "node_modules/vite": {
      "version": "7.2.7",
      "resolved": "https://registry.npmjs.org/vite/-/vite-7.2.7.tgz",
      "integrity": "sha512-ITcnkFeR3+fI8P1wMgItjGrR10170d8auB4EpMLPqmx6uxElH3a/hHGQabSHKdqd4FXWO1nFIp9rRn7JQ34ACQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "esbuild": "^0.25.0",
        "fdir": "^6.5.0",
        "picomatch": "^4.0.3",
        "postcss": "^8.5.6",
        "rollup": "^4.43.0",
        "tinyglobby": "^0.2.15"
      },
      "bin": {
        "vite": "bin/vite.js"
      },
      "engines": {
        "node": "^20.19.0 || >=22.12.0"
      },
      "funding": {
        "url": "https://github.com/vitejs/vite?sponsor=1"
      },
      "optionalDependencies": {
        "fsevents": "~2.3.3"
      },
      "peerDependencies": {
        "@types/node": "^20.19.0 || >=22.12.0",
        "jiti": ">=1.21.0",
        "less": "^4.0.0",
        "lightningcss": "^1.21.0",
        "sass": "^1.70.0",
        "sass-embedded": "^1.70.0",
        "stylus": ">=0.54.8",
        "sugarss": "^5.0.0",
        "terser": "^5.16.0",
        "tsx": "^4.8.1",
        "yaml": "^2.4.2"
      },
      "peerDependenciesMeta": {
        "@types/node": {
          "optional": true
        },
        "jiti": {
          "optional": true
        },
        "less": {
          "optional": true
        },
        "lightningcss": {
          "optional": true
        },
        "sass": {
          "optional": true
        },
        "sass-embedded": {
          "optional": true
        },
        "stylus": {
          "optional": true
        },
        "sugarss": {
          "optional": true
        },
        "terser": {
          "optional": true
        },
        "tsx": {
          "optional": true
        },
        "yaml": {
          "optional": true
        }
      }
    },
    "node_modules/which": {
      "version": "2.0.2",
      "resolved": "https://registry.npmjs.org/which/-/which-2.0.2.tgz",
      "integrity": "sha512-BLI3Tl1TW3Pvl70l3yq3Y64i+awpwXqsGBYWkkqMtnbXgrMD+yj7rhW0kuEDxzJaYXGjEW5ogapKNMEKNMjibA==",
      "dev": true,
      "license": "ISC",
      "dependencies": {
        "isexe": "^2.0.0"
      },
      "bin": {
        "node-which": "bin/node-which"
      },
      "engines": {
        "node": ">= 8"
      }
    },
    "node_modules/word-wrap": {
      "version": "1.2.5",
      "resolved": "https://registry.npmjs.org/word-wrap/-/word-wrap-1.2.5.tgz",
      "integrity": "sha512-BN22B5eaMMI9UMtjrGd5g5eCYPpCPDUy0FJXbYsaT5zYxjFOckS53SQDE3pWkVoWpHXVb3BrYcEN4Twa55B5cA==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/xtend": {
      "version": "4.0.2",
      "resolved": "https://registry.npmjs.org/xtend/-/xtend-4.0.2.tgz",
      "integrity": "sha512-LKYU1iAXJXUgAXn9URjiu+MWhyUXHsvfp7mcuYm9dSUKK0/CjtrUwFAxD82/mCWbtLsGjFIad0wIsod4zrTAEQ==",
      "license": "MIT",
      "engines": {
        "node": ">=0.4"
      }
    },
    "node_modules/yallist": {
      "version": "3.1.1",
      "resolved": "https://registry.npmjs.org/yallist/-/yallist-3.1.1.tgz",
      "integrity": "sha512-a4UGQaWPH59mOXUYnAG2ewncQS4i4F43Tv3JoAM+s2VDAmS9NsK8GpDMLrCHPksFT7h3K6TOoUNn2pb7RoXx4g==",
      "dev": true,
      "license": "ISC"
    },
    "node_modules/yocto-queue": {
      "version": "0.1.0",
      "resolved": "https://registry.npmjs.org/yocto-queue/-/yocto-queue-0.1.0.tgz",
      "integrity": "sha512-rVksvsnNCdJ/ohGc6xgPwyN8eheCxsiLM8mxuE/t/mOVqJewPuO1miLpTHQiRgTKCLexL4MeAFVagts7HmNZ2Q==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/zod": {
      "version": "4.1.13",
      "resolved": "https://registry.npmjs.org/zod/-/zod-4.1.13.tgz",
      "integrity": "sha512-AvvthqfqrAhNH9dnfmrfKzX5upOdjUVJYFqNSlkmGf64gRaTzlPwz99IHYnVs28qYAybvAlBV+H7pn0saFY4Ig==",
      "dev": true,
      "license": "MIT",
      "funding": {
        "url": "https://github.com/sponsors/colinhacks"
      }
    },
    "node_modules/zod-validation-error": {
      "version": "4.0.2",
      "resolved": "https://registry.npmjs.org/zod-validation-error/-/zod-validation-error-4.0.2.tgz",
      "integrity": "sha512-Q6/nZLe6jxuU80qb/4uJ4t5v2VEZ44lzQjPDhYJNztRQ4wyWc6VF3D3Kb/fAuPetZQnhS3hnajCf9CsWesghLQ==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=18.0.0"
      },
      "peerDependencies": {
        "zod": "^3.25.0 || ^4.0.0"
      }
    },
    "node_modules/zwitch": {
      "version": "2.0.4",
      "resolved": "https://registry.npmjs.org/zwitch/-/zwitch-2.0.4.tgz",
      "integrity": "sha512-bXE4cR/kVZhKZX/RjPEflHaKVhUVl85noU3v6b8apfQEc1x4A+zBxjZ4lN8LqGd6WZ3dl98pY4o717VFmoPp+A==",
      "license": "MIT",
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/wooorm"
      }
    }
  }
}

```

---

## backend/admin-dashboard/web/package.json

```json
{
  "name": "admin-dashboard-web",
  "private": true,
  "version": "1.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "tsc -b && vite build",
    "lint": "eslint .",
    "preview": "vite preview"
  },
  "dependencies": {
    "@tanstack/react-query": "^5.80.6",
    "clsx": "^2.1.1",
    "date-fns": "^4.1.0",
    "lucide-react": "^0.511.0",
    "react": "^19.2.0",
    "react-dom": "^19.2.0",
    "react-markdown": "^10.1.0",
    "react-router-dom": "^7.6.1",
    "react-syntax-highlighter": "^15.6.1"
  },
  "devDependencies": {
    "@eslint/js": "^9.39.1",
    "@types/node": "^24.10.1",
    "@types/react": "^19.2.7",
    "@types/react-dom": "^19.2.3",
    "@vitejs/plugin-react": "^5.1.1",
    "eslint": "^9.39.1",
    "eslint-plugin-react-hooks": "^7.0.1",
    "eslint-plugin-react-refresh": "^0.4.24",
    "globals": "^16.5.0",
    "typescript": "~5.9.3",
    "typescript-eslint": "^8.46.4",
    "vite": "^7.2.4"
  }
}

```

---

## backend/admin-dashboard/web/src/App.css

```css
#root {
  max-width: 1280px;
  margin: 0 auto;
  padding: 2rem;
  text-align: center;
}

.logo {
  height: 6em;
  padding: 1.5em;
  will-change: filter;
  transition: filter 300ms;
}
.logo:hover {
  filter: drop-shadow(0 0 2em #646cffaa);
}
.logo.react:hover {
  filter: drop-shadow(0 0 2em #61dafbaa);
}

@keyframes logo-spin {
  from {
    transform: rotate(0deg);
  }
  to {
    transform: rotate(360deg);
  }
}

@media (prefers-reduced-motion: no-preference) {
  a:nth-of-type(2) .logo {
    animation: logo-spin infinite 20s linear;
  }
}

.card {
  padding: 2em;
}

.read-the-docs {
  color: #888;
}

```

---

## backend/admin-dashboard/web/src/App.tsx

```tsx
import { BrowserRouter, Routes, Route, Navigate } from 'react-router-dom';
import { QueryClient, QueryClientProvider } from '@tanstack/react-query';
import { AuthProvider, useAuth } from './contexts/AuthContext';
import Layout from './components/Layout';
import Login from './pages/Login';
import Dashboard from './pages/Dashboard';
import Environments from './pages/Environments';
import Scripts from './pages/Scripts';
import Documents from './pages/Documents';
import LlmSettings from './pages/LlmSettings';
import HealthMonitor from './pages/HealthMonitor';
import DataSources from './pages/DataSources';
import Databases from './pages/Databases';
import Kafka from './pages/Kafka';
import './index.css';

const queryClient = new QueryClient({
  defaultOptions: {
    queries: {
      retry: 1,
      refetchOnWindowFocus: false,
    },
  },
});

function ProtectedRoute({ children }: { children: React.ReactNode }) {
  const { isAuthenticated, isLoading } = useAuth();

  if (isLoading) {
    return (
      <div className="min-h-screen bg-gray-900 flex items-center justify-center">
        <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-blue-500" />
      </div>
    );
  }

  if (!isAuthenticated) {
    return <Navigate to="/login" replace />;
  }

  return <Layout>{children}</Layout>;
}

function AppRoutes() {
  const { isAuthenticated, isLoading } = useAuth();

  if (isLoading) {
    return (
      <div className="min-h-screen bg-gray-900 flex items-center justify-center">
        <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-blue-500" />
      </div>
    );
  }

  return (
    <Routes>
      <Route
        path="/login"
        element={isAuthenticated ? <Navigate to="/" replace /> : <Login />}
      />
      <Route
        path="/"
        element={
          <ProtectedRoute>
            <Dashboard />
          </ProtectedRoute>
        }
      />
      <Route
        path="/environments"
        element={
          <ProtectedRoute>
            <Environments />
          </ProtectedRoute>
        }
      />
      <Route
        path="/scripts"
        element={
          <ProtectedRoute>
            <Scripts />
          </ProtectedRoute>
        }
      />
      <Route
        path="/documents"
        element={
          <ProtectedRoute>
            <Documents />
          </ProtectedRoute>
        }
      />
      <Route
        path="/llm-settings"
        element={
          <ProtectedRoute>
            <LlmSettings />
          </ProtectedRoute>
        }
      />
      <Route
        path="/health-monitor"
        element={
          <ProtectedRoute>
            <HealthMonitor />
          </ProtectedRoute>
        }
      />
      <Route
        path="/data-sources"
        element={
          <ProtectedRoute>
            <DataSources />
          </ProtectedRoute>
        }
      />
      <Route
        path="/databases"
        element={
          <ProtectedRoute>
            <Databases />
          </ProtectedRoute>
        }
      />
      <Route
        path="/kafka"
        element={
          <ProtectedRoute>
            <Kafka />
          </ProtectedRoute>
        }
      />
      <Route path="*" element={<Navigate to="/" replace />} />
    </Routes>
  );
}

function App() {
  return (
    <QueryClientProvider client={queryClient}>
      <BrowserRouter>
        <AuthProvider>
          <AppRoutes />
        </AuthProvider>
      </BrowserRouter>
    </QueryClientProvider>
  );
}

export default App;

```

---

## backend/admin-dashboard/web/src/api/client.ts

```ts
// API Client

const API_BASE_URL = import.meta.env.VITE_API_URL || '/api/v1/admin';

class ApiClient {
  private baseUrl: string;
  private token: string | null = null;

  constructor(baseUrl: string) {
    this.baseUrl = baseUrl;
    this.token = localStorage.getItem('admin_token');
  }

  setToken(token: string | null) {
    this.token = token;
    if (token) {
      localStorage.setItem('admin_token', token);
    } else {
      localStorage.removeItem('admin_token');
    }
  }

  getToken(): string | null {
    return this.token;
  }

  private async request<T>(
    endpoint: string,
    options: RequestInit = {}
  ): Promise<T> {
    const url = `${this.baseUrl}${endpoint}`;
    
    const headers: HeadersInit = {
      'Content-Type': 'application/json',
      ...options.headers,
    };

    if (this.token) {
      (headers as Record<string, string>)['Authorization'] = `Bearer ${this.token}`;
    }

    const response = await fetch(url, {
      ...options,
      headers,
    });

    if (response.status === 401) {
      this.setToken(null);
      window.location.href = '/login';
      throw new Error('Unauthorized');
    }

    if (!response.ok) {
      const error = await response.json().catch(() => ({ detail: 'Unknown error' }));
      throw new Error(error.detail || `HTTP ${response.status}`);
    }

    if (response.status === 204) {
      return {} as T;
    }

    return response.json();
  }

  async get<T>(endpoint: string): Promise<T> {
    return this.request<T>(endpoint, { method: 'GET' });
  }

  async post<T>(endpoint: string, data?: unknown): Promise<T> {
    return this.request<T>(endpoint, {
      method: 'POST',
      body: data ? JSON.stringify(data) : undefined,
    });
  }

  async patch<T>(endpoint: string, data: unknown): Promise<T> {
    return this.request<T>(endpoint, {
      method: 'PATCH',
      body: JSON.stringify(data),
    });
  }

  async delete<T>(endpoint: string): Promise<T> {
    return this.request<T>(endpoint, { method: 'DELETE' });
  }

  async stream(
    endpoint: string,
    data: unknown,
    onData: (chunk: string) => void
  ): Promise<void> {
    const url = `${this.baseUrl}${endpoint}`;
    
    const headers: HeadersInit = {
      'Content-Type': 'application/json',
    };

    if (this.token) {
      (headers as Record<string, string>)['Authorization'] = `Bearer ${this.token}`;
    }

    const response = await fetch(url, {
      method: 'POST',
      headers,
      body: JSON.stringify(data),
    });

    if (!response.ok) {
      throw new Error(`HTTP ${response.status}`);
    }

    const reader = response.body?.getReader();
    if (!reader) return;

    const decoder = new TextDecoder();

    while (true) {
      const { done, value } = await reader.read();
      if (done) break;
      onData(decoder.decode(value, { stream: true }));
    }
  }
}

export const apiClient = new ApiClient(API_BASE_URL);
export default apiClient;

```

---

## backend/admin-dashboard/web/src/api/endpoints.ts

```ts
// API Endpoints
import apiClient from './client';
import type {
  Environment,
  EnvironmentStatus,
  Script,
  TaskExecution,
  Document,
  AuditLog,
  User,
  Token,
  HealthCheck,
  ServiceInfo,
  ServiceHealth,
  InfrastructureHealth,
  OverallSystemHealth,
  DataSource,
  DataSourceTestResult,
  DatabaseInfo,
  PostgresDatabaseStats,
  MongoDatabaseStats,
  RedisStats,
  KafkaClusterInfo,
} from '../types';

// Auth
export const authApi = {
  login: async (username: string, password: string): Promise<Token> => {
    const formData = new URLSearchParams();
    formData.append('username', username);
    formData.append('password', password);
    
    const response = await fetch('/api/v1/admin/auth/token', {
      method: 'POST',
      headers: { 'Content-Type': 'application/x-www-form-urlencoded' },
      body: formData,
    });
    
    if (!response.ok) {
      throw new Error('Invalid credentials');
    }
    
    const token = await response.json();
    apiClient.setToken(token.access_token);
    return token;
  },
  
  logout: async (): Promise<void> => {
    await apiClient.post('/auth/logout');
    apiClient.setToken(null);
  },
  
  me: () => apiClient.get<User>('/auth/me'),
  
  changePassword: (oldPassword: string, newPassword: string) =>
    apiClient.post('/auth/change-password', { old_password: oldPassword, new_password: newPassword }),
};

// Environments
export const environmentsApi = {
  list: (activeOnly = false) =>
    apiClient.get<Environment[]>(`/environments?active_only=${activeOnly}`),
  
  get: (id: string) =>
    apiClient.get<Environment>(`/environments/${id}`),
  
  getStatus: (id: string) =>
    apiClient.get<EnvironmentStatus>(`/environments/${id}/status`),
  
  create: (data: Partial<Environment>) =>
    apiClient.post<Environment>('/environments', data),
  
  update: (id: string, data: Partial<Environment>) =>
    apiClient.patch<Environment>(`/environments/${id}`, data),
  
  delete: (id: string) =>
    apiClient.delete(`/environments/${id}`),
  
  up: (id: string, build = true) =>
    apiClient.post(`/environments/${id}/up?build=${build}`),
  
  down: (id: string, volumes = false) =>
    apiClient.post(`/environments/${id}/down?volumes=${volumes}`),
  
  restart: (id: string, service?: string) =>
    apiClient.post(`/environments/${id}/restart${service ? `?service=${service}` : ''}`),
  
  logs: (id: string, service: string, tail = 100) =>
    apiClient.get<{ service: string; logs: string }>(`/environments/${id}/logs/${service}?tail=${tail}`),
};

// Scripts
export const scriptsApi = {
  list: (environment?: string, tag?: string) => {
    const params = new URLSearchParams();
    if (environment) params.append('environment', environment);
    if (tag) params.append('tag', tag);
    return apiClient.get<Script[]>(`/scripts?${params}`);
  },
  
  get: (id: string) =>
    apiClient.get<Script>(`/scripts/${id}`),
  
  create: (data: Partial<Script>) =>
    apiClient.post<Script>('/scripts', data),
  
  update: (id: string, data: Partial<Script>) =>
    apiClient.patch<Script>(`/scripts/${id}`, data),
  
  delete: (id: string) =>
    apiClient.delete(`/scripts/${id}`),
  
  execute: (scriptId: string, environmentId: string, parameters: Record<string, unknown> = {}) =>
    apiClient.post<TaskExecution>('/scripts/execute', {
      script_id: scriptId,
      environment_id: environmentId,
      parameters,
    }),
  
  executeStream: (
    scriptId: string,
    environmentId: string,
    parameters: Record<string, unknown>,
    onData: (chunk: string) => void
  ) =>
    apiClient.stream('/scripts/execute/stream', {
      script_id: scriptId,
      environment_id: environmentId,
      parameters,
    }, onData),
  
  listExecutions: (scriptId?: string, environmentId?: string, limit = 50) => {
    const params = new URLSearchParams();
    if (scriptId) params.append('script_id', scriptId);
    if (environmentId) params.append('environment_id', environmentId);
    params.append('limit', limit.toString());
    return apiClient.get<TaskExecution[]>(`/scripts/executions?${params}`);
  },
  
  getExecution: (id: string) =>
    apiClient.get<TaskExecution>(`/scripts/executions/${id}`),
  
  cancelExecution: (id: string) =>
    apiClient.post(`/scripts/executions/${id}/cancel`),
};

// Documents
export const documentsApi = {
  list: (category?: string, tag?: string, environment?: string, search?: string) => {
    const params = new URLSearchParams();
    if (category) params.append('category', category);
    if (tag) params.append('tag', tag);
    if (environment) params.append('environment', environment);
    if (search) params.append('search', search);
    return apiClient.get<Document[]>(`/documents?${params}`);
  },
  
  get: (id: string) =>
    apiClient.get<Document>(`/documents/${id}`),
  
  getCategories: () =>
    apiClient.get<Record<string, number>>('/documents/categories'),
  
  getTags: () =>
    apiClient.get<Record<string, number>>('/documents/tags'),
  
  getRelated: (environment?: string, scriptId?: string) => {
    const params = new URLSearchParams();
    if (environment) params.append('environment', environment);
    if (scriptId) params.append('script_id', scriptId);
    return apiClient.get<Document[]>(`/documents/related?${params}`);
  },
  
  refresh: () =>
    apiClient.post('/documents/refresh'),
};

// Audit
export const auditApi = {
  list: (filters: {
    userId?: string;
    action?: string;
    resourceType?: string;
    environmentId?: string;
    startDate?: string;
    endDate?: string;
    success?: boolean;
    page?: number;
    pageSize?: number;
  } = {}) => {
    const params = new URLSearchParams();
    if (filters.userId) params.append('user_id', filters.userId);
    if (filters.action) params.append('action', filters.action);
    if (filters.resourceType) params.append('resource_type', filters.resourceType);
    if (filters.environmentId) params.append('environment_id', filters.environmentId);
    if (filters.startDate) params.append('start_date', filters.startDate);
    if (filters.endDate) params.append('end_date', filters.endDate);
    if (filters.success !== undefined) params.append('success', filters.success.toString());
    params.append('page', (filters.page || 1).toString());
    params.append('page_size', (filters.pageSize || 50).toString());
    return apiClient.get<AuditLog[]>(`/audit/logs?${params}`);
  },
  
  get: (id: string) =>
    apiClient.get<AuditLog>(`/audit/logs/${id}`),
  
  statistics: (startDate?: string, endDate?: string) => {
    const params = new URLSearchParams();
    if (startDate) params.append('start_date', startDate);
    if (endDate) params.append('end_date', endDate);
    return apiClient.get<Record<string, unknown>>(`/audit/statistics?${params}`);
  },
};

// Health
export const healthApi = {
  check: () => apiClient.get<HealthCheck>('/health'),
};

// Users (Admin)
export const usersApi = {
  list: (activeOnly = false) =>
    apiClient.get<User[]>(`/auth/users?active_only=${activeOnly}`),
  
  get: (id: string) =>
    apiClient.get<User>(`/auth/users/${id}`),
  
  create: (data: { username: string; password: string; email?: string; role: string }) =>
    apiClient.post<User>('/auth/users', data),
  
  update: (id: string, data: { email?: string; role?: string; is_active?: boolean }) =>
    apiClient.patch<User>(`/auth/users/${id}`, data),
  
  resetPassword: (id: string, newPassword: string) =>
    apiClient.post(`/auth/users/${id}/reset-password`, { new_password: newPassword }),
  
  delete: (id: string) =>
    apiClient.delete(`/auth/users/${id}`),
};

// LLM Provider Settings Types
export type LlmProviderType = 
  | 'OPENAI' 
  | 'ANTHROPIC' 
  | 'GOOGLE' 
  | 'OPENROUTER' 
  | 'OLLAMA' 
  | 'AZURE_OPENAI' 
  | 'CUSTOM';

export interface LlmProviderTypeInfo {
  value: LlmProviderType;
  displayName: string;
  description: string;
  requiresApiKey: boolean;
  defaultBaseUrl?: string;
}

export interface LlmProviderSettings {
  id: number;
  providerType: LlmProviderType;
  userId?: string;
  hasApiKey: boolean;
  maskedApiKey?: string;
  defaultModel: string;
  baseUrl?: string;
  enabled: boolean;
  priority: number;
  maxTokens: number;
  temperature: number;
  timeoutMs: number;
  azureDeploymentName?: string;
  azureApiVersion?: string;
  lastTestedAt?: string;
  lastTestSuccess?: boolean;
  createdAt: string;
  updatedAt: string;
}

export interface LlmProviderSettingsRequest {
  providerType: LlmProviderType;
  apiKey?: string;
  defaultModel: string;
  baseUrl?: string;
  enabled?: boolean;
  priority?: number;
  maxTokens?: number;
  temperature?: number;
  timeoutMs?: number;
  azureDeploymentName?: string;
  azureApiVersion?: string;
}

export interface LlmTestResult {
  providerType: LlmProviderType;
  success: boolean;
  message: string;
  latencyMs?: number;
  testedAt: string;
}

// LLM Providers (Admin)
export const llmProvidersApi = {
  // Get provider type metadata
  getTypes: () =>
    apiClient.get<LlmProviderTypeInfo[]>('/llm-providers/types'),
  
  // Get all global settings
  listGlobal: () =>
    apiClient.get<LlmProviderSettings[]>('/llm-providers/global'),
  
  // Get global setting for specific provider
  getGlobal: (providerType: LlmProviderType) =>
    apiClient.get<LlmProviderSettings>(`/llm-providers/global/${providerType}`),
  
  // Save/update global setting
  saveGlobal: (providerType: LlmProviderType, data: LlmProviderSettingsRequest) =>
    apiClient.put<LlmProviderSettings>(`/llm-providers/global/${providerType}`, data),
  
  // Delete global setting
  deleteGlobal: (providerType: LlmProviderType) =>
    apiClient.delete(`/llm-providers/global/${providerType}`),
  
  // Test connection
  testConnection: (providerType: LlmProviderType, model?: string) => {
    const params = new URLSearchParams({ providerType });
    if (model) params.append('model', model);
    return apiClient.post<LlmTestResult>(`/llm-providers/test?${params}`);
  },
  
  // Get effective settings (global + user overrides)
  getEffective: (userId?: string) => {
    const params = userId ? `?userId=${userId}` : '';
    return apiClient.get<LlmProviderSettings[]>(`/llm-providers/effective${params}`);
  },
  
  // Get enabled providers
  getEnabled: (userId?: string) => {
    const params = userId ? `?userId=${userId}` : '';
    return apiClient.get<LlmProviderSettings[]>(`/llm-providers/enabled${params}`);
  },
};

// Health Monitor API
export const healthMonitorApi = {
  // Get all registered services
  listServices: () =>
    apiClient.get<ServiceInfo[]>('/health-monitor/services'),
  
  // Get infrastructure services
  listInfrastructure: () =>
    apiClient.get<Record<string, unknown>[]>('/health-monitor/infrastructure'),
  
  // Check specific service health
  checkService: (serviceId: string) =>
    apiClient.get<ServiceHealth>(`/health-monitor/check/${serviceId}`),
  
  // Check all services health
  checkAllServices: () =>
    apiClient.get<ServiceHealth[]>('/health-monitor/check-all'),
  
  // Check infrastructure health
  checkInfrastructure: () =>
    apiClient.get<InfrastructureHealth[]>('/health-monitor/check-infrastructure'),
  
  // Get overall system health
  getOverallHealth: () =>
    apiClient.get<OverallSystemHealth>('/health-monitor/overall'),
  
  // Get last check result (cached)
  getLastCheck: (serviceId: string) =>
    apiClient.get<ServiceHealth>(`/health-monitor/last-check/${serviceId}`),
  
  // SSE stream URL builder
  getStreamUrl: (interval = 10) => {
    const token = apiClient.getToken();
    const baseUrl = '/api/v1/admin';
    return `${baseUrl}/health-monitor/stream?interval=${interval}&token=${token}`;
  },
};

// Data Sources API
export const dataSourcesApi = {
  // List all data sources
  list: (filters?: { type?: string; status?: string; category?: string }) => {
    const params = new URLSearchParams();
    if (filters?.type) params.append('type', filters.type);
    if (filters?.status) params.append('status', filters.status);
    if (filters?.category) params.append('category', filters.category);
    return apiClient.get<DataSource[]>(`/data-sources?${params}`);
  },
  
  // Get single data source
  get: (id: string) =>
    apiClient.get<DataSource>(`/data-sources/${id}`),
  
  // Create new data source
  create: (data: Partial<DataSource>) =>
    apiClient.post<DataSource>('/data-sources', data),
  
  // Update data source
  update: (id: string, data: Partial<DataSource>) =>
    apiClient.patch<DataSource>(`/data-sources/${id}`, data),
  
  // Delete data source
  delete: (id: string) =>
    apiClient.delete(`/data-sources/${id}`),
  
  // Test data source connection
  test: (id: string) =>
    apiClient.post<DataSourceTestResult>(`/data-sources/${id}/test`),
  
  // Trigger crawl for data source
  triggerCrawl: (id: string) =>
    apiClient.post(`/data-sources/${id}/crawl`),
  
  // Toggle active status
  toggleActive: (id: string, isActive: boolean) =>
    apiClient.patch<DataSource>(`/data-sources/${id}`, { is_active: isActive }),
  
  // Get categories
  getCategories: () =>
    apiClient.get<string[]>('/data-sources/categories'),
  
  // Get statistics
  getStats: () =>
    apiClient.get<Record<string, unknown>>('/data-sources/stats'),
};

// Database Management API
export const databaseApi = {
  // Get all database info
  listDatabases: () =>
    apiClient.get<DatabaseInfo[]>('/databases'),
  
  // Get PostgreSQL stats
  getPostgresStats: () =>
    apiClient.get<PostgresDatabaseStats>('/databases/postgres/stats'),
  
  // Get MongoDB stats
  getMongoStats: () =>
    apiClient.get<MongoDatabaseStats>('/databases/mongo/stats'),
  
  // Get Redis stats
  getRedisStats: () =>
    apiClient.get<RedisStats>('/databases/redis/stats'),
  
  // Health check specific database
  checkDatabase: (dbType: 'postgres' | 'mongo' | 'redis') =>
    apiClient.get<DatabaseInfo>(`/databases/${dbType}/health`),
};

// Kafka/Redpanda API
export const kafkaApi = {
  // Get cluster info
  getClusterInfo: () =>
    apiClient.get<KafkaClusterInfo>('/kafka/cluster'),
  
  // List topics
  listTopics: () =>
    apiClient.get<KafkaClusterInfo['topics']>('/kafka/topics'),
  
  // Get topic details
  getTopic: (topicName: string) =>
    apiClient.get<KafkaClusterInfo['topics'][0]>(`/kafka/topics/${topicName}`),
  
  // List consumer groups
  listConsumerGroups: () =>
    apiClient.get<KafkaClusterInfo['consumer_groups']>('/kafka/consumer-groups'),
  
  // Get consumer group details
  getConsumerGroup: (groupId: string) =>
    apiClient.get<KafkaClusterInfo['consumer_groups'][0]>(`/kafka/consumer-groups/${groupId}`),
};

```

---

## backend/admin-dashboard/web/src/components/Layout.tsx

```tsx
import { type ReactNode } from 'react';
import { Link, useLocation, useNavigate } from 'react-router-dom';
import {
  LayoutDashboard,
  Server,
  Terminal,
  FileText,
  ClipboardList,
  Users,
  LogOut,
  Menu,
  X,
  Bot,
  Activity,
  Rss,
  Database,
  Radio,
} from 'lucide-react';
import { useState } from 'react';
import { useAuth } from '../contexts/AuthContext';
import clsx from 'clsx';

interface LayoutProps {
  children: ReactNode;
}

const navigation = [
  { name: 'ëŒ€ì‹œë³´ë“œ', href: '/', icon: LayoutDashboard },
  { name: 'í™˜ê²½ ê´€ë¦¬', href: '/environments', icon: Server },
  { name: 'í—¬ìŠ¤ ëª¨ë‹ˆí„°', href: '/health-monitor', icon: Activity },
  { name: 'ë°ì´í„° ì†ŒìŠ¤', href: '/data-sources', icon: Rss },
  { name: 'ë°ì´í„°ë² ì´ìŠ¤', href: '/databases', icon: Database },
  { name: 'Kafka', href: '/kafka', icon: Radio },
  { name: 'ìŠ¤í¬ë¦½íŠ¸', href: '/scripts', icon: Terminal },
  { name: 'ë¬¸ì„œ', href: '/documents', icon: FileText },
  { name: 'LLM ì„¤ì •', href: '/llm-settings', icon: Bot, adminOnly: true },
  { name: 'ê°ì‚¬ ë¡œê·¸', href: '/audit', icon: ClipboardList },
  { name: 'ì‚¬ìš©ì', href: '/users', icon: Users, adminOnly: true },
];

export default function Layout({ children }: LayoutProps) {
  const [sidebarOpen, setSidebarOpen] = useState(false);
  const { user, logout } = useAuth();
  const location = useLocation();
  const navigate = useNavigate();

  const handleLogout = async () => {
    await logout();
    navigate('/login');
  };

  const filteredNav = navigation.filter(
    (item) => !item.adminOnly || user?.role === 'admin'
  );

  return (
    <div className="min-h-screen bg-gray-900">
      {/* Mobile sidebar */}
      <div
        className={clsx(
          'fixed inset-0 z-50 lg:hidden',
          sidebarOpen ? 'block' : 'hidden'
        )}
      >
        <div
          className="fixed inset-0 bg-gray-900/80"
          onClick={() => setSidebarOpen(false)}
        />
        <div className="fixed inset-y-0 left-0 w-64 bg-gray-800 p-4">
          <div className="flex items-center justify-between mb-8">
            <h1 className="text-xl font-bold text-white">Admin Dashboard</h1>
            <button
              onClick={() => setSidebarOpen(false)}
              className="text-gray-400 hover:text-white"
            >
              <X className="w-6 h-6" />
            </button>
          </div>
          <nav className="space-y-1">
            {filteredNav.map((item) => (
              <Link
                key={item.name}
                to={item.href}
                className={clsx(
                  'flex items-center gap-3 px-3 py-2 rounded-lg text-sm font-medium',
                  location.pathname === item.href
                    ? 'bg-gray-700 text-white'
                    : 'text-gray-300 hover:bg-gray-700 hover:text-white'
                )}
                onClick={() => setSidebarOpen(false)}
              >
                <item.icon className="w-5 h-5" />
                {item.name}
              </Link>
            ))}
          </nav>
        </div>
      </div>

      {/* Desktop sidebar */}
      <div className="hidden lg:fixed lg:inset-y-0 lg:flex lg:w-64 lg:flex-col">
        <div className="flex flex-col flex-1 bg-gray-800 border-r border-gray-700">
          <div className="flex items-center h-16 px-4 border-b border-gray-700">
            <h1 className="text-xl font-bold text-white">Admin Dashboard</h1>
          </div>
          <nav className="flex-1 p-4 space-y-1">
            {filteredNav.map((item) => (
              <Link
                key={item.name}
                to={item.href}
                className={clsx(
                  'flex items-center gap-3 px-3 py-2 rounded-lg text-sm font-medium transition-colors',
                  location.pathname === item.href
                    ? 'bg-blue-600 text-white'
                    : 'text-gray-300 hover:bg-gray-700 hover:text-white'
                )}
              >
                <item.icon className="w-5 h-5" />
                {item.name}
              </Link>
            ))}
          </nav>
          <div className="p-4 border-t border-gray-700">
            <div className="flex items-center gap-3 mb-4">
              <div className="w-8 h-8 rounded-full bg-blue-600 flex items-center justify-center text-white font-medium">
                {user?.username?.charAt(0).toUpperCase()}
              </div>
              <div>
                <p className="text-sm font-medium text-white">{user?.username}</p>
                <p className="text-xs text-gray-400 capitalize">{user?.role}</p>
              </div>
            </div>
            <button
              onClick={handleLogout}
              className="flex items-center gap-2 w-full px-3 py-2 text-sm text-gray-300 hover:bg-gray-700 hover:text-white rounded-lg transition-colors"
            >
              <LogOut className="w-4 h-4" />
              ë¡œê·¸ì•„ì›ƒ
            </button>
          </div>
        </div>
      </div>

      {/* Main content */}
      <div className="lg:pl-64">
        {/* Mobile header */}
        <div className="sticky top-0 z-40 flex items-center h-16 px-4 bg-gray-800 border-b border-gray-700 lg:hidden">
          <button
            onClick={() => setSidebarOpen(true)}
            className="text-gray-400 hover:text-white"
          >
            <Menu className="w-6 h-6" />
          </button>
          <h1 className="ml-4 text-lg font-semibold text-white">Admin Dashboard</h1>
        </div>

        {/* Page content */}
        <main className="p-4 lg:p-8">{children}</main>
      </div>
    </div>
  );
}

```

---

## backend/admin-dashboard/web/src/contexts/AuthContext.tsx

```tsx
import { createContext, useContext, useState, useEffect, type ReactNode } from 'react';
import type { User } from '../types';
import { authApi } from '../api/endpoints';
import apiClient from '../api/client';

interface AuthContextType {
  user: User | null;
  isLoading: boolean;
  isAuthenticated: boolean;
  login: (username: string, password: string) => Promise<void>;
  logout: () => Promise<void>;
}

const AuthContext = createContext<AuthContextType | undefined>(undefined);

export function AuthProvider({ children }: { children: ReactNode }) {
  const [user, setUser] = useState<User | null>(null);
  const [isLoading, setIsLoading] = useState(true);

  useEffect(() => {
    const token = apiClient.getToken();
    if (token) {
      authApi.me()
        .then(setUser)
        .catch(() => {
          apiClient.setToken(null);
        })
        .finally(() => setIsLoading(false));
    } else {
      setIsLoading(false);
    }
  }, []);

  const login = async (username: string, password: string) => {
    await authApi.login(username, password);
    const userData = await authApi.me();
    setUser(userData);
  };

  const logout = async () => {
    try {
      await authApi.logout();
    } finally {
      setUser(null);
      apiClient.setToken(null);
    }
  };

  return (
    <AuthContext.Provider value={{
        user,
        isLoading,
        isAuthenticated: !!user,
        login,
        logout,
      }}> 
      {children}
    </AuthContext.Provider>
  );
}

export function useAuth() {
  const context = useContext(AuthContext);
  if (context === undefined) {
    throw new Error('useAuth must be used within an AuthProvider');
  }
  return context;
}

```

---

## backend/admin-dashboard/web/src/index.css

```css
/* Admin Dashboard Styles */
@import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');

*, *::before, *::after {
  box-sizing: border-box;
}

:root {
  font-family: 'Inter', system-ui, -apple-system, sans-serif;
  line-height: 1.5;
  font-weight: 400;
  color-scheme: dark;
  color: rgba(255, 255, 255, 0.87);
  background-color: #111827;
  font-synthesis: none;
  text-rendering: optimizeLegibility;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

body {
  margin: 0;
  min-height: 100vh;
}

#root {
  min-height: 100vh;
}

/* Scrollbar */
::-webkit-scrollbar {
  width: 8px;
  height: 8px;
}

::-webkit-scrollbar-track {
  background: #1f2937;
}

::-webkit-scrollbar-thumb {
  background: #4b5563;
  border-radius: 4px;
}

::-webkit-scrollbar-thumb:hover {
  background: #6b7280;
}

/* Utilities */
.line-clamp-2 {
  display: -webkit-box;
  -webkit-line-clamp: 2;
  -webkit-box-orient: vertical;
  overflow: hidden;
}

/* Animations */
@keyframes spin {
  from {
    transform: rotate(0deg);
  }
  to {
    transform: rotate(360deg);
  }
}

.animate-spin {
  animation: spin 1s linear infinite;
}

/* Prose for markdown */
.prose {
  max-width: 65ch;
}

.prose-invert {
  color: #d1d5db;
}

.prose pre {
  background-color: #1f2937;
  border-radius: 0.5rem;
  padding: 1rem;
  overflow-x: auto;
}

.prose code {
  background-color: #374151;
  padding: 0.125rem 0.25rem;
  border-radius: 0.25rem;
  font-size: 0.875em;
}

/* Form elements */
input[type="checkbox"] {
  accent-color: #3b82f6;
}

select option {
  background-color: #1f2937;
  color: white;
}

```

---

## backend/admin-dashboard/web/src/main.tsx

```tsx
import { StrictMode } from 'react'
import { createRoot } from 'react-dom/client'
import './index.css'
import App from './App.tsx'

createRoot(document.getElementById('root')!).render(
  <StrictMode>
    <App />
  </StrictMode>,
)

```

---

## backend/admin-dashboard/web/src/pages/Dashboard.tsx

```tsx
import { useEffect, useState } from 'react';
import { Link } from 'react-router-dom';
import {
  Server,
  Activity,
  CheckCircle,
  XCircle,
  Clock,
  AlertTriangle,
  ArrowRight,
} from 'lucide-react';
import { environmentsApi, scriptsApi, healthApi } from '../api/endpoints';
import type { Environment, EnvironmentStatus, TaskExecution, HealthCheck } from '../types';
import clsx from 'clsx';

export default function Dashboard() {
  const [environments, setEnvironments] = useState<Environment[]>([]);
  const [statuses, setStatuses] = useState<Record<string, EnvironmentStatus>>({});
  const [recentExecutions, setRecentExecutions] = useState<TaskExecution[]>([]);
  const [health, setHealth] = useState<HealthCheck | null>(null);
  const [isLoading, setIsLoading] = useState(true);

  useEffect(() => {
    loadData();
  }, []);

  const loadData = async () => {
    try {
      const [envs, executions, healthData] = await Promise.all([
        environmentsApi.list(true),
        scriptsApi.listExecutions(undefined, undefined, 10),
        healthApi.check(),
      ]);

      setEnvironments(envs);
      setRecentExecutions(executions);
      setHealth(healthData);

      // Load status for each environment
      const statusPromises = envs.map(async (env) => {
        try {
          const status = await environmentsApi.getStatus(env.id);
          return { id: env.id, status };
        } catch {
          return { id: env.id, status: null };
        }
      });

      const statusResults = await Promise.all(statusPromises);
      const statusMap: Record<string, EnvironmentStatus> = {};
      statusResults.forEach(({ id, status }) => {
        if (status) statusMap[id] = status;
      });
      setStatuses(statusMap);
    } catch (error) {
      console.error('Failed to load dashboard data:', error);
    } finally {
      setIsLoading(false);
    }
  };

  const getStatusColor = (status: string) => {
    switch (status) {
      case 'success':
        return 'text-green-400';
      case 'failed':
        return 'text-red-400';
      case 'running':
        return 'text-blue-400';
      case 'cancelled':
        return 'text-gray-400';
      default:
        return 'text-yellow-400';
    }
  };

  const getStatusIcon = (status: string) => {
    switch (status) {
      case 'success':
        return <CheckCircle className="w-4 h-4" />;
      case 'failed':
        return <XCircle className="w-4 h-4" />;
      case 'running':
        return <Clock className="w-4 h-4 animate-spin" />;
      default:
        return <AlertTriangle className="w-4 h-4" />;
    }
  };

  if (isLoading) {
    return (
      <div className="flex items-center justify-center h-64">
        <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-blue-500" />
      </div>
    );
  }

  return (
    <div className="space-y-6">
      <div className="flex items-center justify-between">
        <h1 className="text-2xl font-bold text-white">ëŒ€ì‹œë³´ë“œ</h1>
        {health && (
          <div className="flex items-center gap-2 text-sm">
            <span className={clsx(
              'flex items-center gap-1',
              health.status === 'healthy' ? 'text-green-400' : 'text-red-400'
            )}>
              <Activity className="w-4 h-4" />
              {health.status}
            </span>
            <span className="text-gray-500">v{health.version}</span>
          </div>
        )}
      </div>

      {/* Environment Cards */}
      <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-4">
        {environments.map((env) => {
          const status = statuses[env.id];
          const runningCount = status?.running_containers || 0;
          const totalCount = status?.total_containers || 0;
          const isHealthy = runningCount === totalCount && totalCount > 0;

          return (
            <Link
              key={env.id}
              to={`/environments?selected=${env.id}`}
              className="bg-gray-800 rounded-xl p-6 border border-gray-700 hover:border-blue-500 transition-colors"
            >
              <div className="flex items-start justify-between mb-4">
                <div className="flex items-center gap-3">
                  <div className={clsx(
                    'p-2 rounded-lg',
                    isHealthy ? 'bg-green-500/10' : 'bg-yellow-500/10'
                  )}>
                    <Server className={clsx(
                      'w-5 h-5',
                      isHealthy ? 'text-green-400' : 'text-yellow-400'
                    )} />
                  </div>
                  <div>
                    <h3 className="font-semibold text-white capitalize">{env.name}</h3>
                    <p className="text-sm text-gray-400">{env.env_type}</p>
                  </div>
                </div>
                <ArrowRight className="w-5 h-5 text-gray-500" />
              </div>

              <div className="flex items-center justify-between text-sm">
                <span className="text-gray-400">ì»¨í…Œì´ë„ˆ</span>
                <span className={clsx(
                  'font-medium',
                  isHealthy ? 'text-green-400' : 'text-yellow-400'
                )}>
                  {runningCount} / {totalCount}
                </span>
              </div>

              {status?.containers && status.containers.length > 0 && (
                <div className="mt-3 flex flex-wrap gap-1">
                  {status.containers.slice(0, 5).map((container) => (
                    <span
                      key={container.name}
                      className={clsx(
                        'px-2 py-0.5 rounded text-xs',
                        container.status === 'up'
                          ? 'bg-green-500/10 text-green-400'
                          : 'bg-red-500/10 text-red-400'
                      )}
                    >
                      {container.name.replace('newsinsight-', '')}
                    </span>
                  ))}
                  {status.containers.length > 5 && (
                    <span className="px-2 py-0.5 rounded text-xs bg-gray-700 text-gray-400">
                      +{status.containers.length - 5}
                    </span>
                  )}
                </div>
              )}
            </Link>
          );
        })}
      </div>

      {/* Recent Executions */}
      <div className="bg-gray-800 rounded-xl border border-gray-700">
        <div className="flex items-center justify-between p-4 border-b border-gray-700">
          <h2 className="font-semibold text-white">ìµœê·¼ ì‹¤í–‰ ì´ë ¥</h2>
          <Link
            to="/scripts"
            className="text-sm text-blue-400 hover:text-blue-300"
          >
            ì „ì²´ ë³´ê¸°
          </Link>
        </div>
        <div className="divide-y divide-gray-700">
          {recentExecutions.length === 0 ? (
            <div className="p-8 text-center text-gray-500">
              ì‹¤í–‰ ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤
            </div>
          ) : (
            recentExecutions.map((execution) => (
              <div
                key={execution.id}
                className="flex items-center justify-between p-4 hover:bg-gray-700/50"
              >
                <div className="flex items-center gap-3">
                  <span className={getStatusColor(execution.status)}>
                    {getStatusIcon(execution.status)}
                  </span>
                  <div>
                    <p className="text-sm font-medium text-white">
                      {execution.script_name}
                    </p>
                    <p className="text-xs text-gray-400">
                      {execution.environment_name} â€¢ {execution.executed_by}
                    </p>
                  </div>
                </div>
                <div className="text-right">
                  <p className={clsx('text-sm capitalize', getStatusColor(execution.status))}>
                    {execution.status}
                  </p>
                  <p className="text-xs text-gray-500">
                    {new Date(execution.started_at).toLocaleString('ko-KR')}
                  </p>
                </div>
              </div>
            ))
          )}
        </div>
      </div>
    </div>
  );
}

```

---

## backend/admin-dashboard/web/src/pages/DataSources.tsx

```tsx
import { useState, useEffect, useCallback } from 'react';
import {
  Rss,
  Globe,
  Database as DatabaseIcon,
  Share2,
  Plus,
  Pencil,
  Trash2,
  TestTube,
  Play,
  CheckCircle,
  XCircle,
  AlertCircle,
  Clock,
  ToggleLeft,
  ToggleRight,
  X,
  Loader2,
} from 'lucide-react';
import { dataSourcesApi } from '../api/endpoints';
import type { DataSource, DataSourceType, DataSourceStatus, DataSourceTestResult } from '../types';
import clsx from 'clsx';

const typeConfig: Record<DataSourceType, { icon: typeof Rss; label: string; color: string }> = {
  rss: { icon: Rss, label: 'RSS', color: 'text-orange-400' },
  web: { icon: Globe, label: 'Web', color: 'text-blue-400' },
  api: { icon: DatabaseIcon, label: 'API', color: 'text-green-400' },
  social: { icon: Share2, label: 'Social', color: 'text-purple-400' },
};

const statusConfig: Record<DataSourceStatus, { color: string; bgColor: string; label: string }> = {
  active: { color: 'text-green-400', bgColor: 'bg-green-400/10', label: 'Active' },
  inactive: { color: 'text-gray-400', bgColor: 'bg-gray-400/10', label: 'Inactive' },
  error: { color: 'text-red-400', bgColor: 'bg-red-400/10', label: 'Error' },
  testing: { color: 'text-yellow-400', bgColor: 'bg-yellow-400/10', label: 'Testing' },
};

interface DataSourceFormData {
  name: string;
  source_type: DataSourceType;
  url: string;
  description: string;
  category: string;
  language: string;
  crawl_interval_minutes: number;
  priority: number;
  is_active: boolean;
}

const defaultFormData: DataSourceFormData = {
  name: '',
  source_type: 'rss',
  url: '',
  description: '',
  category: '',
  language: 'ko',
  crawl_interval_minutes: 60,
  priority: 0,
  is_active: true,
};

function DataSourceModal({
  isOpen,
  onClose,
  onSave,
  source,
  categories,
}: {
  isOpen: boolean;
  onClose: () => void;
  onSave: (data: DataSourceFormData) => void;
  source?: DataSource;
  categories: string[];
}) {
  const [formData, setFormData] = useState<DataSourceFormData>(defaultFormData);
  const [isSubmitting, setIsSubmitting] = useState(false);

  useEffect(() => {
    if (source) {
      setFormData({
        name: source.name,
        source_type: source.source_type,
        url: source.url,
        description: source.description || '',
        category: source.category || '',
        language: source.language,
        crawl_interval_minutes: source.crawl_interval_minutes,
        priority: source.priority,
        is_active: source.is_active,
      });
    } else {
      setFormData(defaultFormData);
    }
  }, [source, isOpen]);

  const handleSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    setIsSubmitting(true);
    try {
      await onSave(formData);
      onClose();
    } finally {
      setIsSubmitting(false);
    }
  };

  if (!isOpen) return null;

  return (
    <div className="fixed inset-0 z-50 flex items-center justify-center bg-black/50">
      <div className="bg-gray-800 rounded-xl w-full max-w-lg mx-4 p-6 max-h-[90vh] overflow-y-auto">
        <div className="flex items-center justify-between mb-6">
          <h2 className="text-xl font-semibold text-white">
            {source ? 'Edit Data Source' : 'New Data Source'}
          </h2>
          <button onClick={onClose} className="text-gray-400 hover:text-white">
            <X className="w-5 h-5" />
          </button>
        </div>

        <form onSubmit={handleSubmit} className="space-y-4">
          <div>
            <label className="block text-sm font-medium text-gray-400 mb-1">Name *</label>
            <input
              type="text"
              value={formData.name}
              onChange={(e) => setFormData({ ...formData, name: e.target.value })}
              required
              className="w-full px-3 py-2 bg-gray-700 border border-gray-600 rounded-lg text-white focus:ring-2 focus:ring-blue-500 focus:border-transparent"
            />
          </div>

          <div>
            <label className="block text-sm font-medium text-gray-400 mb-1">Type *</label>
            <select
              value={formData.source_type}
              onChange={(e) => setFormData({ ...formData, source_type: e.target.value as DataSourceType })}
              className="w-full px-3 py-2 bg-gray-700 border border-gray-600 rounded-lg text-white focus:ring-2 focus:ring-blue-500 focus:border-transparent"
            >
              {Object.entries(typeConfig).map(([value, { label }]) => (
                <option key={value} value={value}>{label}</option>
              ))}
            </select>
          </div>

          <div>
            <label className="block text-sm font-medium text-gray-400 mb-1">URL *</label>
            <input
              type="url"
              value={formData.url}
              onChange={(e) => setFormData({ ...formData, url: e.target.value })}
              required
              placeholder="https://example.com/feed.xml"
              className="w-full px-3 py-2 bg-gray-700 border border-gray-600 rounded-lg text-white focus:ring-2 focus:ring-blue-500 focus:border-transparent"
            />
          </div>

          <div>
            <label className="block text-sm font-medium text-gray-400 mb-1">Description</label>
            <textarea
              value={formData.description}
              onChange={(e) => setFormData({ ...formData, description: e.target.value })}
              rows={2}
              className="w-full px-3 py-2 bg-gray-700 border border-gray-600 rounded-lg text-white focus:ring-2 focus:ring-blue-500 focus:border-transparent"
            />
          </div>

          <div className="grid grid-cols-2 gap-4">
            <div>
              <label className="block text-sm font-medium text-gray-400 mb-1">Category</label>
              <input
                type="text"
                value={formData.category}
                onChange={(e) => setFormData({ ...formData, category: e.target.value })}
                list="categories"
                className="w-full px-3 py-2 bg-gray-700 border border-gray-600 rounded-lg text-white focus:ring-2 focus:ring-blue-500 focus:border-transparent"
              />
              <datalist id="categories">
                {categories.map((cat) => (
                  <option key={cat} value={cat} />
                ))}
              </datalist>
            </div>

            <div>
              <label className="block text-sm font-medium text-gray-400 mb-1">Language</label>
              <select
                value={formData.language}
                onChange={(e) => setFormData({ ...formData, language: e.target.value })}
                className="w-full px-3 py-2 bg-gray-700 border border-gray-600 rounded-lg text-white focus:ring-2 focus:ring-blue-500 focus:border-transparent"
              >
                <option value="ko">Korean</option>
                <option value="en">English</option>
                <option value="ja">Japanese</option>
                <option value="zh">Chinese</option>
              </select>
            </div>
          </div>

          <div className="grid grid-cols-2 gap-4">
            <div>
              <label className="block text-sm font-medium text-gray-400 mb-1">
                Crawl Interval (minutes)
              </label>
              <input
                type="number"
                value={formData.crawl_interval_minutes}
                onChange={(e) => setFormData({ ...formData, crawl_interval_minutes: parseInt(e.target.value) || 60 })}
                min={5}
                max={1440}
                className="w-full px-3 py-2 bg-gray-700 border border-gray-600 rounded-lg text-white focus:ring-2 focus:ring-blue-500 focus:border-transparent"
              />
            </div>

            <div>
              <label className="block text-sm font-medium text-gray-400 mb-1">Priority</label>
              <input
                type="number"
                value={formData.priority}
                onChange={(e) => setFormData({ ...formData, priority: parseInt(e.target.value) || 0 })}
                min={0}
                max={100}
                className="w-full px-3 py-2 bg-gray-700 border border-gray-600 rounded-lg text-white focus:ring-2 focus:ring-blue-500 focus:border-transparent"
              />
            </div>
          </div>

          <div className="flex items-center gap-2">
            <input
              type="checkbox"
              id="is_active"
              checked={formData.is_active}
              onChange={(e) => setFormData({ ...formData, is_active: e.target.checked })}
              className="rounded border-gray-600 bg-gray-700 text-blue-500 focus:ring-blue-500"
            />
            <label htmlFor="is_active" className="text-sm text-gray-300">Active</label>
          </div>

          <div className="flex justify-end gap-3 pt-4">
            <button
              type="button"
              onClick={onClose}
              className="px-4 py-2 text-gray-300 hover:text-white transition-colors"
            >
              Cancel
            </button>
            <button
              type="submit"
              disabled={isSubmitting}
              className="px-4 py-2 bg-blue-600 hover:bg-blue-700 text-white rounded-lg transition-colors disabled:opacity-50 flex items-center gap-2"
            >
              {isSubmitting && <Loader2 className="w-4 h-4 animate-spin" />}
              {source ? 'Update' : 'Create'}
            </button>
          </div>
        </form>
      </div>
    </div>
  );
}

function DataSourceCard({
  source,
  onEdit,
  onDelete,
  onTest,
  onToggle,
  onCrawl,
}: {
  source: DataSource;
  onEdit: () => void;
  onDelete: () => void;
  onTest: () => void;
  onToggle: () => void;
  onCrawl: () => void;
}) {
  const typeInfo = typeConfig[source.source_type];
  const statusInfo = statusConfig[source.status];
  const TypeIcon = typeInfo.icon;

  return (
    <div className="bg-gray-800 rounded-lg border border-gray-700 p-4">
      <div className="flex items-start justify-between mb-3">
        <div className="flex items-center gap-2">
          <TypeIcon className={clsx('w-5 h-5', typeInfo.color)} />
          <h3 className="font-medium text-white truncate max-w-[200px]" title={source.name}>
            {source.name}
          </h3>
        </div>
        <div className="flex items-center gap-1">
          <button
            onClick={onToggle}
            className="p-1.5 text-gray-400 hover:text-white transition-colors"
            title={source.is_active ? 'Deactivate' : 'Activate'}
          >
            {source.is_active ? (
              <ToggleRight className="w-5 h-5 text-green-400" />
            ) : (
              <ToggleLeft className="w-5 h-5" />
            )}
          </button>
        </div>
      </div>

      <div className={clsx('inline-flex items-center px-2 py-0.5 rounded text-xs font-medium', statusInfo.bgColor, statusInfo.color)}>
        {statusInfo.label}
      </div>

      <p className="text-sm text-gray-400 mt-2 truncate" title={source.url}>
        {source.url}
      </p>

      {source.description && (
        <p className="text-sm text-gray-500 mt-1 line-clamp-2">{source.description}</p>
      )}

      <div className="flex items-center gap-4 mt-3 text-xs text-gray-500">
        {source.category && <span className="bg-gray-700 px-2 py-0.5 rounded">{source.category}</span>}
        <span className="flex items-center gap-1">
          <Clock className="w-3 h-3" />
          {source.crawl_interval_minutes}m
        </span>
        <span>{source.total_articles} articles</span>
      </div>

      {source.last_crawled_at && (
        <p className="text-xs text-gray-500 mt-2">
          Last crawled: {new Date(source.last_crawled_at).toLocaleString()}
        </p>
      )}

      <div className="flex items-center justify-end gap-1 mt-4 pt-3 border-t border-gray-700">
        <button
          onClick={onTest}
          className="p-2 text-gray-400 hover:text-blue-400 transition-colors"
          title="Test Connection"
        >
          <TestTube className="w-4 h-4" />
        </button>
        <button
          onClick={onCrawl}
          className="p-2 text-gray-400 hover:text-green-400 transition-colors"
          title="Trigger Crawl"
        >
          <Play className="w-4 h-4" />
        </button>
        <button
          onClick={onEdit}
          className="p-2 text-gray-400 hover:text-yellow-400 transition-colors"
          title="Edit"
        >
          <Pencil className="w-4 h-4" />
        </button>
        <button
          onClick={onDelete}
          className="p-2 text-gray-400 hover:text-red-400 transition-colors"
          title="Delete"
        >
          <Trash2 className="w-4 h-4" />
        </button>
      </div>
    </div>
  );
}

export default function DataSources() {
  const [sources, setSources] = useState<DataSource[]>([]);
  const [categories, setCategories] = useState<string[]>([]);
  const [stats, setStats] = useState<Record<string, unknown> | null>(null);
  const [isLoading, setIsLoading] = useState(true);
  const [isModalOpen, setIsModalOpen] = useState(false);
  const [editingSource, setEditingSource] = useState<DataSource | undefined>();
  const [testResult, setTestResult] = useState<DataSourceTestResult | null>(null);
  
  // Filters
  const [typeFilter, setTypeFilter] = useState<string>('');
  const [statusFilter, setStatusFilter] = useState<string>('');
  const [categoryFilter, setCategoryFilter] = useState<string>('');

  const fetchData = useCallback(async () => {
    setIsLoading(true);
    try {
      const [sourcesData, categoriesData, statsData] = await Promise.all([
        dataSourcesApi.list({
          type: typeFilter || undefined,
          status: statusFilter || undefined,
          category: categoryFilter || undefined,
        }),
        dataSourcesApi.getCategories(),
        dataSourcesApi.getStats(),
      ]);
      setSources(sourcesData);
      setCategories(categoriesData);
      setStats(statsData);
    } catch (error) {
      console.error('Failed to fetch data sources:', error);
    } finally {
      setIsLoading(false);
    }
  }, [typeFilter, statusFilter, categoryFilter]);

  useEffect(() => {
    fetchData();
  }, [fetchData]);

  const handleSave = async (data: DataSourceFormData) => {
    if (editingSource) {
      await dataSourcesApi.update(editingSource.id, data);
    } else {
      await dataSourcesApi.create(data as unknown as Partial<DataSource>);
    }
    setEditingSource(undefined);
    fetchData();
  };

  const handleDelete = async (source: DataSource) => {
    if (!confirm(`Are you sure you want to delete "${source.name}"?`)) return;
    await dataSourcesApi.delete(source.id);
    fetchData();
  };

  const handleTest = async (source: DataSource) => {
    try {
      const result = await dataSourcesApi.test(source.id);
      setTestResult(result);
      fetchData();
    } catch (error) {
      console.error('Test failed:', error);
    }
  };

  const handleToggle = async (source: DataSource) => {
    await dataSourcesApi.toggleActive(source.id, !source.is_active);
    fetchData();
  };

  const handleCrawl = async (source: DataSource) => {
    try {
      await dataSourcesApi.triggerCrawl(source.id);
      alert('Crawl triggered successfully!');
      fetchData();
    } catch (error) {
      alert('Failed to trigger crawl');
    }
  };

  if (isLoading) {
    return (
      <div className="flex items-center justify-center h-64">
        <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-blue-500" />
      </div>
    );
  }

  return (
    <div className="space-y-6">
      {/* Header */}
      <div className="flex items-center justify-between">
        <div>
          <h1 className="text-2xl font-bold text-white">Data Sources</h1>
          <p className="text-gray-400 mt-1">Manage news data collection sources</p>
        </div>
        <button
          onClick={() => {
            setEditingSource(undefined);
            setIsModalOpen(true);
          }}
          className="flex items-center gap-2 px-4 py-2 bg-blue-600 hover:bg-blue-700 text-white rounded-lg transition-colors"
        >
          <Plus className="w-4 h-4" />
          Add Source
        </button>
      </div>

      {/* Stats */}
      {stats && (
        <div className="grid grid-cols-2 md:grid-cols-4 gap-4">
          <div className="bg-gray-800 rounded-lg p-4 border border-gray-700">
            <p className="text-gray-400 text-sm">Total Sources</p>
            <p className="text-2xl font-bold text-white">{stats.total_sources as number}</p>
          </div>
          <div className="bg-gray-800 rounded-lg p-4 border border-gray-700">
            <p className="text-gray-400 text-sm">Active</p>
            <p className="text-2xl font-bold text-green-400">{stats.active_sources as number}</p>
          </div>
          <div className="bg-gray-800 rounded-lg p-4 border border-gray-700">
            <p className="text-gray-400 text-sm">Inactive</p>
            <p className="text-2xl font-bold text-gray-400">{stats.inactive_sources as number}</p>
          </div>
          <div className="bg-gray-800 rounded-lg p-4 border border-gray-700">
            <p className="text-gray-400 text-sm">Total Articles</p>
            <p className="text-2xl font-bold text-blue-400">{stats.total_articles as number}</p>
          </div>
        </div>
      )}

      {/* Filters */}
      <div className="flex items-center gap-4 flex-wrap">
        <select
          value={typeFilter}
          onChange={(e) => setTypeFilter(e.target.value)}
          className="px-3 py-2 bg-gray-800 border border-gray-700 rounded-lg text-white text-sm"
        >
          <option value="">All Types</option>
          {Object.entries(typeConfig).map(([value, { label }]) => (
            <option key={value} value={value}>{label}</option>
          ))}
        </select>

        <select
          value={statusFilter}
          onChange={(e) => setStatusFilter(e.target.value)}
          className="px-3 py-2 bg-gray-800 border border-gray-700 rounded-lg text-white text-sm"
        >
          <option value="">All Status</option>
          {Object.entries(statusConfig).map(([value, { label }]) => (
            <option key={value} value={value}>{label}</option>
          ))}
        </select>

        <select
          value={categoryFilter}
          onChange={(e) => setCategoryFilter(e.target.value)}
          className="px-3 py-2 bg-gray-800 border border-gray-700 rounded-lg text-white text-sm"
        >
          <option value="">All Categories</option>
          {categories.map((cat) => (
            <option key={cat} value={cat}>{cat}</option>
          ))}
        </select>
      </div>

      {/* Sources Grid */}
      <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-4">
        {sources.map((source) => (
          <DataSourceCard
            key={source.id}
            source={source}
            onEdit={() => {
              setEditingSource(source);
              setIsModalOpen(true);
            }}
            onDelete={() => handleDelete(source)}
            onTest={() => handleTest(source)}
            onToggle={() => handleToggle(source)}
            onCrawl={() => handleCrawl(source)}
          />
        ))}
      </div>

      {sources.length === 0 && (
        <div className="text-center py-12 text-gray-400">
          <DatabaseIcon className="w-12 h-12 mx-auto mb-4 opacity-50" />
          <p>No data sources found</p>
          <p className="text-sm mt-1">Add a new data source to get started</p>
        </div>
      )}

      {/* Modal */}
      <DataSourceModal
        isOpen={isModalOpen}
        onClose={() => {
          setIsModalOpen(false);
          setEditingSource(undefined);
        }}
        onSave={handleSave}
        source={editingSource}
        categories={categories}
      />

      {/* Test Result Modal */}
      {testResult && (
        <div className="fixed inset-0 z-50 flex items-center justify-center bg-black/50">
          <div className="bg-gray-800 rounded-xl w-full max-w-md mx-4 p-6">
            <div className="flex items-center justify-between mb-4">
              <h3 className="text-lg font-semibold text-white">Test Result</h3>
              <button onClick={() => setTestResult(null)} className="text-gray-400 hover:text-white">
                <X className="w-5 h-5" />
              </button>
            </div>
            <div className="space-y-3">
              <div className="flex items-center gap-2">
                {testResult.success ? (
                  <CheckCircle className="w-5 h-5 text-green-400" />
                ) : (
                  <XCircle className="w-5 h-5 text-red-400" />
                )}
                <span className={testResult.success ? 'text-green-400' : 'text-red-400'}>
                  {testResult.success ? 'Success' : 'Failed'}
                </span>
              </div>
              <p className="text-gray-300">{testResult.message}</p>
              {testResult.response_time_ms && (
                <p className="text-sm text-gray-400">Response time: {testResult.response_time_ms.toFixed(0)}ms</p>
              )}
            </div>
            <button
              onClick={() => setTestResult(null)}
              className="w-full mt-4 px-4 py-2 bg-gray-700 hover:bg-gray-600 text-white rounded-lg transition-colors"
            >
              Close
            </button>
          </div>
        </div>
      )}
    </div>
  );
}

```

---

## backend/admin-dashboard/web/src/pages/Databases.tsx

```tsx
import { useState, useEffect, useCallback } from 'react';
import {
  Database,
  CheckCircle,
  XCircle,
  AlertCircle,
  RefreshCw,
  HardDrive,
  Table,
  FileStack,
  Users,
  Clock,
  Zap,
  BarChart3,
} from 'lucide-react';
import { databaseApi } from '../api/endpoints';
import type {
  DatabaseInfo,
  PostgresDatabaseStats,
  MongoDatabaseStats,
  RedisStats,
  ServiceHealthStatus,
  DatabaseType,
} from '../types';
import clsx from 'clsx';

const statusConfig: Record<ServiceHealthStatus, { color: string; bgColor: string; icon: typeof CheckCircle; label: string }> = {
  healthy: { color: 'text-green-400', bgColor: 'bg-green-500/20 border-green-500', icon: CheckCircle, label: 'Healthy' },
  unhealthy: { color: 'text-red-400', bgColor: 'bg-red-500/20 border-red-500', icon: XCircle, label: 'Unhealthy' },
  degraded: { color: 'text-yellow-400', bgColor: 'bg-yellow-500/20 border-yellow-500', icon: AlertCircle, label: 'Degraded' },
  unreachable: { color: 'text-gray-400', bgColor: 'bg-gray-500/20 border-gray-500', icon: XCircle, label: 'Unreachable' },
  unknown: { color: 'text-gray-500', bgColor: 'bg-gray-500/20 border-gray-500', icon: AlertCircle, label: 'Unknown' },
};

const dbTypeConfig: Record<DatabaseType, { icon: typeof Database; color: string; label: string }> = {
  postgresql: { icon: Database, color: 'text-blue-400', label: 'PostgreSQL' },
  mongodb: { icon: FileStack, color: 'text-green-400', label: 'MongoDB' },
  redis: { icon: Zap, color: 'text-red-400', label: 'Redis' },
};

function StatusBadge({ status }: { status: ServiceHealthStatus }) {
  const config = statusConfig[status] || statusConfig.unknown;
  const Icon = config.icon;
  
  return (
    <span className={clsx('flex items-center gap-1.5 text-sm font-medium', config.color)}>
      <Icon className="w-4 h-4" />
      {config.label}
    </span>
  );
}

function DatabaseCard({ db, onRefresh }: { db: DatabaseInfo; onRefresh: () => void }) {
  const [isRefreshing, setIsRefreshing] = useState(false);
  const config = dbTypeConfig[db.db_type] || dbTypeConfig.postgresql;
  const statusCfg = statusConfig[db.status] || statusConfig.unknown;
  const Icon = config.icon;
  
  const handleRefresh = async () => {
    setIsRefreshing(true);
    try {
      const dbType = db.db_type === 'postgresql' ? 'postgres' : db.db_type === 'mongodb' ? 'mongo' : 'redis';
      await databaseApi.checkDatabase(dbType);
      onRefresh();
    } finally {
      setIsRefreshing(false);
    }
  };
  
  return (
    <div className={clsx('rounded-xl p-5 border-2', statusCfg.bgColor)}>
      <div className="flex items-start justify-between mb-4">
        <div className="flex items-center gap-3">
          <div className={clsx('p-2 rounded-lg bg-gray-800', config.color)}>
            <Icon className="w-6 h-6" />
          </div>
          <div>
            <h3 className="font-semibold text-white text-lg">{config.label}</h3>
            <p className="text-gray-400 text-sm">{db.name}</p>
          </div>
        </div>
        <button
          onClick={handleRefresh}
          disabled={isRefreshing}
          className="p-2 text-gray-400 hover:text-white hover:bg-gray-700 rounded-lg transition-colors"
        >
          <RefreshCw className={clsx('w-4 h-4', isRefreshing && 'animate-spin')} />
        </button>
      </div>
      
      <div className="mb-4">
        <StatusBadge status={db.status} />
      </div>
      
      <div className="space-y-2 text-sm">
        <div className="flex items-center justify-between text-gray-400">
          <span>Host</span>
          <span className="text-white">{db.host}:{db.port}</span>
        </div>
        {db.version && (
          <div className="flex items-center justify-between text-gray-400">
            <span>Version</span>
            <span className="text-white">{db.version}</span>
          </div>
        )}
        {db.size_human && (
          <div className="flex items-center justify-between text-gray-400">
            <span>Size</span>
            <span className="text-white">{db.size_human}</span>
          </div>
        )}
        {db.connection_count !== undefined && (
          <div className="flex items-center justify-between text-gray-400">
            <span>Connections</span>
            <span className="text-white">{db.connection_count}/{db.max_connections || '?'}</span>
          </div>
        )}
      </div>
      
      <div className="mt-4 pt-3 border-t border-gray-700">
        <p className="text-xs text-gray-500">
          Checked: {new Date(db.checked_at).toLocaleTimeString()}
        </p>
      </div>
    </div>
  );
}

function PostgresStatsCard({ stats }: { stats: PostgresDatabaseStats | null }) {
  if (!stats) return null;
  
  return (
    <div className="bg-gray-800 rounded-xl p-5 border border-gray-700">
      <div className="flex items-center gap-2 mb-4">
        <Database className="w-5 h-5 text-blue-400" />
        <h3 className="font-semibold text-white">PostgreSQL Details</h3>
      </div>
      
      <div className="grid grid-cols-2 md:grid-cols-4 gap-4 mb-6">
        <div className="bg-gray-700/50 rounded-lg p-3">
          <p className="text-gray-400 text-xs">Database</p>
          <p className="text-white font-medium">{stats.database_name}</p>
        </div>
        <div className="bg-gray-700/50 rounded-lg p-3">
          <p className="text-gray-400 text-xs">Size</p>
          <p className="text-white font-medium">{stats.size_human}</p>
        </div>
        <div className="bg-gray-700/50 rounded-lg p-3">
          <p className="text-gray-400 text-xs">Tables</p>
          <p className="text-white font-medium">{stats.total_tables}</p>
        </div>
        <div className="bg-gray-700/50 rounded-lg p-3">
          <p className="text-gray-400 text-xs">Total Rows</p>
          <p className="text-white font-medium">{stats.total_rows.toLocaleString()}</p>
        </div>
      </div>
      
      {stats.tables.length > 0 && (
        <div>
          <h4 className="text-sm font-medium text-gray-300 mb-3 flex items-center gap-2">
            <Table className="w-4 h-4" />
            Tables
          </h4>
          <div className="overflow-x-auto">
            <table className="w-full text-sm">
              <thead>
                <tr className="text-left text-gray-400 border-b border-gray-700">
                  <th className="pb-2">Table</th>
                  <th className="pb-2 text-right">Rows</th>
                  <th className="pb-2 text-right">Size</th>
                </tr>
              </thead>
              <tbody className="text-gray-300">
                {stats.tables.map((table) => (
                  <tr key={`${table.schema_name}.${table.table_name}`} className="border-b border-gray-700/50">
                    <td className="py-2">
                      <span className="text-gray-500">{table.schema_name}.</span>
                      {table.table_name}
                    </td>
                    <td className="py-2 text-right">{table.row_count.toLocaleString()}</td>
                    <td className="py-2 text-right">{table.size_human}</td>
                  </tr>
                ))}
              </tbody>
            </table>
          </div>
        </div>
      )}
    </div>
  );
}

function MongoStatsCard({ stats }: { stats: MongoDatabaseStats | null }) {
  if (!stats) return null;
  
  return (
    <div className="bg-gray-800 rounded-xl p-5 border border-gray-700">
      <div className="flex items-center gap-2 mb-4">
        <FileStack className="w-5 h-5 text-green-400" />
        <h3 className="font-semibold text-white">MongoDB Details</h3>
      </div>
      
      <div className="grid grid-cols-2 md:grid-cols-4 gap-4 mb-6">
        <div className="bg-gray-700/50 rounded-lg p-3">
          <p className="text-gray-400 text-xs">Database</p>
          <p className="text-white font-medium">{stats.database_name}</p>
        </div>
        <div className="bg-gray-700/50 rounded-lg p-3">
          <p className="text-gray-400 text-xs">Size</p>
          <p className="text-white font-medium">{stats.size_human}</p>
        </div>
        <div className="bg-gray-700/50 rounded-lg p-3">
          <p className="text-gray-400 text-xs">Collections</p>
          <p className="text-white font-medium">{stats.total_collections}</p>
        </div>
        <div className="bg-gray-700/50 rounded-lg p-3">
          <p className="text-gray-400 text-xs">Documents</p>
          <p className="text-white font-medium">{stats.total_documents.toLocaleString()}</p>
        </div>
      </div>
      
      {stats.collections.length > 0 && (
        <div>
          <h4 className="text-sm font-medium text-gray-300 mb-3 flex items-center gap-2">
            <FileStack className="w-4 h-4" />
            Collections
          </h4>
          <div className="overflow-x-auto">
            <table className="w-full text-sm">
              <thead>
                <tr className="text-left text-gray-400 border-b border-gray-700">
                  <th className="pb-2">Collection</th>
                  <th className="pb-2 text-right">Documents</th>
                  <th className="pb-2 text-right">Size</th>
                  <th className="pb-2 text-right">Indexes</th>
                </tr>
              </thead>
              <tbody className="text-gray-300">
                {stats.collections.map((col) => (
                  <tr key={col.collection_name} className="border-b border-gray-700/50">
                    <td className="py-2">{col.collection_name}</td>
                    <td className="py-2 text-right">{col.document_count.toLocaleString()}</td>
                    <td className="py-2 text-right">{col.size_human}</td>
                    <td className="py-2 text-right">{col.index_count}</td>
                  </tr>
                ))}
              </tbody>
            </table>
          </div>
        </div>
      )}
    </div>
  );
}

function RedisStatsCard({ stats }: { stats: RedisStats | null }) {
  if (!stats) return null;
  
  const formatUptime = (seconds: number) => {
    const days = Math.floor(seconds / 86400);
    const hours = Math.floor((seconds % 86400) / 3600);
    const minutes = Math.floor((seconds % 3600) / 60);
    
    if (days > 0) return `${days}d ${hours}h`;
    if (hours > 0) return `${hours}h ${minutes}m`;
    return `${minutes}m`;
  };
  
  return (
    <div className="bg-gray-800 rounded-xl p-5 border border-gray-700">
      <div className="flex items-center gap-2 mb-4">
        <Zap className="w-5 h-5 text-red-400" />
        <h3 className="font-semibold text-white">Redis Details</h3>
      </div>
      
      <div className="grid grid-cols-2 md:grid-cols-4 gap-4 mb-6">
        <div className="bg-gray-700/50 rounded-lg p-3">
          <p className="text-gray-400 text-xs">Memory Used</p>
          <p className="text-white font-medium">{stats.used_memory_human}</p>
        </div>
        <div className="bg-gray-700/50 rounded-lg p-3">
          <p className="text-gray-400 text-xs">Total Keys</p>
          <p className="text-white font-medium">{stats.total_keys.toLocaleString()}</p>
        </div>
        <div className="bg-gray-700/50 rounded-lg p-3">
          <p className="text-gray-400 text-xs">Clients</p>
          <p className="text-white font-medium">{stats.connected_clients}</p>
        </div>
        <div className="bg-gray-700/50 rounded-lg p-3">
          <p className="text-gray-400 text-xs">Uptime</p>
          <p className="text-white font-medium">{formatUptime(stats.uptime_seconds)}</p>
        </div>
      </div>
      
      <div className="grid grid-cols-1 md:grid-cols-2 gap-4">
        {/* Hit Rate */}
        <div className="bg-gray-700/30 rounded-lg p-4">
          <div className="flex items-center justify-between mb-2">
            <span className="text-gray-400 text-sm">Cache Hit Rate</span>
            <span className={clsx(
              'font-medium',
              stats.hit_rate >= 90 ? 'text-green-400' :
              stats.hit_rate >= 70 ? 'text-yellow-400' : 'text-red-400'
            )}>
              {stats.hit_rate.toFixed(1)}%
            </span>
          </div>
          <div className="w-full bg-gray-600 rounded-full h-2">
            <div
              className={clsx(
                'h-2 rounded-full',
                stats.hit_rate >= 90 ? 'bg-green-400' :
                stats.hit_rate >= 70 ? 'bg-yellow-400' : 'bg-red-400'
              )}
              style={{ width: `${Math.min(100, stats.hit_rate)}%` }}
            />
          </div>
          <div className="flex justify-between mt-2 text-xs text-gray-500">
            <span>Hits: {stats.keyspace_hits.toLocaleString()}</span>
            <span>Misses: {stats.keyspace_misses.toLocaleString()}</span>
          </div>
        </div>
        
        {/* Key Stats */}
        <div className="bg-gray-700/30 rounded-lg p-4">
          <h4 className="text-gray-400 text-sm mb-3">Key Statistics</h4>
          <div className="space-y-2 text-sm">
            <div className="flex justify-between">
              <span className="text-gray-400">Active Keys</span>
              <span className="text-white">{stats.total_keys.toLocaleString()}</span>
            </div>
            <div className="flex justify-between">
              <span className="text-gray-400">Expired Keys</span>
              <span className="text-white">{stats.expired_keys.toLocaleString()}</span>
            </div>
            {stats.max_memory_bytes && (
              <div className="flex justify-between">
                <span className="text-gray-400">Memory Limit</span>
                <span className="text-white">
                  {(stats.max_memory_bytes / 1024 / 1024 / 1024).toFixed(1)} GB
                </span>
              </div>
            )}
          </div>
        </div>
      </div>
    </div>
  );
}

export default function Databases() {
  const [databases, setDatabases] = useState<DatabaseInfo[]>([]);
  const [postgresStats, setPostgresStats] = useState<PostgresDatabaseStats | null>(null);
  const [mongoStats, setMongoStats] = useState<MongoDatabaseStats | null>(null);
  const [redisStats, setRedisStats] = useState<RedisStats | null>(null);
  const [isLoading, setIsLoading] = useState(true);
  const [isRefreshing, setIsRefreshing] = useState(false);
  const [activeTab, setActiveTab] = useState<'overview' | 'postgres' | 'mongo' | 'redis'>('overview');

  const fetchDatabases = useCallback(async (showLoading = false) => {
    if (showLoading) setIsLoading(true);
    setIsRefreshing(true);
    try {
      const [dbList, pgStats, mgStats, rdStats] = await Promise.all([
        databaseApi.listDatabases(),
        databaseApi.getPostgresStats().catch(() => null),
        databaseApi.getMongoStats().catch(() => null),
        databaseApi.getRedisStats().catch(() => null),
      ]);
      setDatabases(dbList);
      setPostgresStats(pgStats);
      setMongoStats(mgStats);
      setRedisStats(rdStats);
    } catch (error) {
      console.error('Failed to fetch databases:', error);
    } finally {
      setIsLoading(false);
      setIsRefreshing(false);
    }
  }, []);

  useEffect(() => {
    fetchDatabases(true);
  }, [fetchDatabases]);

  if (isLoading) {
    return (
      <div className="flex items-center justify-center h-64">
        <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-blue-500" />
      </div>
    );
  }

  const healthyCount = databases.filter(db => db.status === 'healthy').length;
  const totalCount = databases.length;

  return (
    <div className="space-y-6">
      {/* Header */}
      <div className="flex items-center justify-between">
        <div>
          <h1 className="text-2xl font-bold text-white">Database Management</h1>
          <p className="text-gray-400 mt-1">
            Monitor and manage PostgreSQL, MongoDB, and Redis databases
          </p>
        </div>
        <button
          onClick={() => fetchDatabases(false)}
          disabled={isRefreshing}
          className="flex items-center gap-2 px-4 py-2 bg-blue-600 hover:bg-blue-700 text-white rounded-lg transition-colors disabled:opacity-50"
        >
          <RefreshCw className={clsx('w-4 h-4', isRefreshing && 'animate-spin')} />
          Refresh
        </button>
      </div>

      {/* Summary */}
      <div className={clsx(
        'rounded-xl p-6 border-2',
        healthyCount === totalCount
          ? 'bg-green-500/20 border-green-500'
          : healthyCount > 0
          ? 'bg-yellow-500/20 border-yellow-500'
          : 'bg-red-500/20 border-red-500'
      )}>
        <div className="flex items-center gap-4">
          <div className="p-3 bg-gray-800 rounded-lg">
            <HardDrive className="w-8 h-8 text-white" />
          </div>
          <div>
            <h2 className="text-xl font-bold text-white">Database Status</h2>
            <p className="text-gray-300">
              {healthyCount} of {totalCount} databases healthy
            </p>
          </div>
        </div>
      </div>

      {/* Tabs */}
      <div className="border-b border-gray-700">
        <nav className="flex gap-4">
          {[
            { id: 'overview', label: 'Overview', icon: BarChart3 },
            { id: 'postgres', label: 'PostgreSQL', icon: Database },
            { id: 'mongo', label: 'MongoDB', icon: FileStack },
            { id: 'redis', label: 'Redis', icon: Zap },
          ].map(({ id, label, icon: Icon }) => (
            <button
              key={id}
              onClick={() => setActiveTab(id as typeof activeTab)}
              className={clsx(
                'flex items-center gap-2 px-4 py-3 text-sm font-medium border-b-2 transition-colors',
                activeTab === id
                  ? 'text-blue-400 border-blue-400'
                  : 'text-gray-400 border-transparent hover:text-white hover:border-gray-600'
              )}
            >
              <Icon className="w-4 h-4" />
              {label}
            </button>
          ))}
        </nav>
      </div>

      {/* Tab Content */}
      {activeTab === 'overview' && (
        <div className="grid grid-cols-1 md:grid-cols-3 gap-6">
          {databases.map((db) => (
            <DatabaseCard
              key={db.db_type}
              db={db}
              onRefresh={() => fetchDatabases(false)}
            />
          ))}
        </div>
      )}

      {activeTab === 'postgres' && (
        <PostgresStatsCard stats={postgresStats} />
      )}

      {activeTab === 'mongo' && (
        <MongoStatsCard stats={mongoStats} />
      )}

      {activeTab === 'redis' && (
        <RedisStatsCard stats={redisStats} />
      )}
    </div>
  );
}

```

---

## backend/admin-dashboard/web/src/pages/Documents.tsx

```tsx
import { useEffect, useState } from 'react';
import {
  FileText,
  Search,
  Tag,
  Folder,
  RefreshCw,
  X,
} from 'lucide-react';
import { documentsApi } from '../api/endpoints';
import type { Document } from '../types';
import clsx from 'clsx';

export default function Documents() {
  const [documents, setDocuments] = useState<Document[]>([]);
  const [selectedDoc, setSelectedDoc] = useState<Document | null>(null);
  const [categories, setCategories] = useState<Record<string, number>>({});
  const [tags, setTags] = useState<Record<string, number>>({});
  const [searchQuery, setSearchQuery] = useState('');
  const [selectedCategory, setSelectedCategory] = useState<string>('');
  const [selectedTag, setSelectedTag] = useState<string>('');
  const [isLoading, setIsLoading] = useState(true);

  useEffect(() => {
    loadData();
  }, []);

  useEffect(() => {
    loadDocuments();
  }, [selectedCategory, selectedTag, searchQuery]);

  const loadData = async () => {
    try {
      const [categoriesData, tagsData] = await Promise.all([
        documentsApi.getCategories(),
        documentsApi.getTags(),
      ]);
      setCategories(categoriesData);
      setTags(tagsData);
      await loadDocuments();
    } catch (error) {
      console.error('Failed to load data:', error);
    } finally {
      setIsLoading(false);
    }
  };

  const loadDocuments = async () => {
    try {
      const docs = await documentsApi.list(
        selectedCategory || undefined,
        selectedTag || undefined,
        undefined,
        searchQuery || undefined
      );
      setDocuments(docs);
    } catch (error) {
      console.error('Failed to load documents:', error);
    }
  };

  const loadDocument = async (docId: string) => {
    try {
      const doc = await documentsApi.get(docId);
      setSelectedDoc(doc);
    } catch (error) {
      console.error('Failed to load document:', error);
    }
  };

  const handleRefresh = async () => {
    setIsLoading(true);
    try {
      await documentsApi.refresh();
      await loadData();
    } catch (error) {
      console.error('Failed to refresh:', error);
    } finally {
      setIsLoading(false);
    }
  };

  const getCategoryColor = (category: string) => {
    switch (category) {
      case 'deployment':
        return 'bg-blue-500/10 text-blue-400';
      case 'troubleshooting':
        return 'bg-red-500/10 text-red-400';
      case 'architecture':
        return 'bg-purple-500/10 text-purple-400';
      case 'runbook':
        return 'bg-green-500/10 text-green-400';
      default:
        return 'bg-gray-500/10 text-gray-400';
    }
  };

  if (isLoading) {
    return (
      <div className="flex items-center justify-center h-64">
        <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-blue-500" />
      </div>
    );
  }

  return (
    <div className="space-y-6">
      <div className="flex items-center justify-between">
        <h1 className="text-2xl font-bold text-white">ë¬¸ì„œ</h1>
        <button
          onClick={handleRefresh}
          className="flex items-center gap-2 px-4 py-2 bg-gray-700 hover:bg-gray-600 text-white rounded-lg transition-colors"
        >
          <RefreshCw className="w-4 h-4" />
          ìƒˆë¡œê³ ì¹¨
        </button>
      </div>

      <div className="grid grid-cols-1 lg:grid-cols-4 gap-6">
        {/* Sidebar */}
        <div className="lg:col-span-1 space-y-6">
          {/* Search */}
          <div className="relative">
            <Search className="absolute left-3 top-1/2 -translate-y-1/2 w-4 h-4 text-gray-400" />
            <input
              type="text"
              value={searchQuery}
              onChange={(e) => setSearchQuery(e.target.value)}
              placeholder="ë¬¸ì„œ ê²€ìƒ‰..."
              className="w-full pl-10 pr-4 py-2 bg-gray-800 border border-gray-700 rounded-lg text-white placeholder-gray-400 focus:outline-none focus:ring-2 focus:ring-blue-500"
            />
          </div>

          {/* Categories */}
          <div className="bg-gray-800 rounded-xl border border-gray-700 p-4">
            <h3 className="flex items-center gap-2 font-semibold text-white mb-3">
              <Folder className="w-4 h-4" />
              ì¹´í…Œê³ ë¦¬
            </h3>
            <div className="space-y-1">
              <button
                onClick={() => setSelectedCategory('')}
                className={clsx(
                  'w-full flex items-center justify-between px-3 py-2 rounded-lg text-sm transition-colors',
                  !selectedCategory
                    ? 'bg-blue-600/20 text-blue-400'
                    : 'text-gray-300 hover:bg-gray-700'
                )}
              >
                <span>ì „ì²´</span>
                <span className="text-gray-500">
                  {Object.values(categories).reduce((a, b) => a + b, 0)}
                </span>
              </button>
              {Object.entries(categories).map(([cat, count]) => (
                <button
                  key={cat}
                  onClick={() => setSelectedCategory(cat)}
                  className={clsx(
                    'w-full flex items-center justify-between px-3 py-2 rounded-lg text-sm transition-colors capitalize',
                    selectedCategory === cat
                      ? 'bg-blue-600/20 text-blue-400'
                      : 'text-gray-300 hover:bg-gray-700'
                  )}
                >
                  <span>{cat}</span>
                  <span className="text-gray-500">{count}</span>
                </button>
              ))}
            </div>
          </div>

          {/* Tags */}
          <div className="bg-gray-800 rounded-xl border border-gray-700 p-4">
            <h3 className="flex items-center gap-2 font-semibold text-white mb-3">
              <Tag className="w-4 h-4" />
              íƒœê·¸
            </h3>
            <div className="flex flex-wrap gap-2">
              {Object.entries(tags)
                .sort((a, b) => b[1] - a[1])
                .slice(0, 15)
                .map(([tag, count]) => (
                  <button
                    key={tag}
                    onClick={() =>
                      setSelectedTag(selectedTag === tag ? '' : tag)
                    }
                    className={clsx(
                      'px-2 py-1 rounded text-xs transition-colors',
                      selectedTag === tag
                        ? 'bg-blue-600 text-white'
                        : 'bg-gray-700 text-gray-300 hover:bg-gray-600'
                    )}
                  >
                    {tag} ({count})
                  </button>
                ))}
            </div>
          </div>
        </div>

        {/* Document List & Content */}
        <div className="lg:col-span-3">
          {selectedDoc ? (
            <div className="bg-gray-800 rounded-xl border border-gray-700">
              <div className="flex items-center justify-between p-4 border-b border-gray-700">
                <div>
                  <h2 className="font-semibold text-white">{selectedDoc.title}</h2>
                  <p className="text-sm text-gray-400">{selectedDoc.file_path}</p>
                </div>
                <button
                  onClick={() => setSelectedDoc(null)}
                  className="text-gray-400 hover:text-white"
                >
                  <X className="w-5 h-5" />
                </button>
              </div>
              <div className="p-4 border-b border-gray-700">
                <div className="flex flex-wrap gap-2">
                  <span className={clsx(
                    'px-2 py-1 rounded text-xs capitalize',
                    getCategoryColor(selectedDoc.category)
                  )}>
                    {selectedDoc.category}
                  </span>
                  {selectedDoc.tags.map((tag) => (
                    <span
                      key={tag}
                      className="px-2 py-1 bg-gray-700 rounded text-xs text-gray-300"
                    >
                      {tag}
                    </span>
                  ))}
                </div>
              </div>
              <div className="p-6 prose prose-invert max-w-none">
                <pre className="whitespace-pre-wrap text-sm text-gray-300 font-mono">
                  {selectedDoc.content || 'No content available'}
                </pre>
              </div>
            </div>
          ) : (
            <div className="grid grid-cols-1 md:grid-cols-2 gap-4">
              {documents.length === 0 ? (
                <div className="col-span-2 bg-gray-800 rounded-xl border border-gray-700 p-8 text-center text-gray-500">
                  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤
                </div>
              ) : (
                documents.map((doc) => (
                  <button
                    key={doc.id}
                    onClick={() => loadDocument(doc.id)}
                    className="bg-gray-800 rounded-xl border border-gray-700 p-4 text-left hover:border-blue-500 transition-colors"
                  >
                    <div className="flex items-start gap-3">
                      <FileText className="w-5 h-5 text-gray-400 flex-shrink-0 mt-0.5" />
                      <div className="flex-1 min-w-0">
                        <h3 className="font-medium text-white truncate">
                          {doc.title}
                        </h3>
                        <p className="text-sm text-gray-400 truncate mt-1">
                          {doc.file_path}
                        </p>
                        <div className="flex flex-wrap gap-1 mt-2">
                          <span className={clsx(
                            'px-2 py-0.5 rounded text-xs capitalize',
                            getCategoryColor(doc.category)
                          )}>
                            {doc.category}
                          </span>
                          {doc.tags.slice(0, 2).map((tag) => (
                            <span
                              key={tag}
                              className="px-2 py-0.5 bg-gray-700 rounded text-xs text-gray-300"
                            >
                              {tag}
                            </span>
                          ))}
                        </div>
                      </div>
                    </div>
                  </button>
                ))
              )}
            </div>
          )}
        </div>
      </div>
    </div>
  );
}

```

---

## backend/admin-dashboard/web/src/pages/Environments.tsx

```tsx
import { useEffect, useState } from 'react';
import { useSearchParams } from 'react-router-dom';
import {
  Server,
  Play,
  Square,
  RotateCcw,
  Trash2,
  RefreshCw,
  CheckCircle,
  XCircle,
  AlertTriangle,
  Terminal,
} from 'lucide-react';
import { environmentsApi } from '../api/endpoints';
import type { Environment, EnvironmentStatus, ContainerInfo } from '../types';
import { useAuth } from '../contexts/AuthContext';
import clsx from 'clsx';

export default function Environments() {
  const [searchParams, setSearchParams] = useSearchParams();
  const [environments, setEnvironments] = useState<Environment[]>([]);
  const [selectedEnv, setSelectedEnv] = useState<Environment | null>(null);
  const [status, setStatus] = useState<EnvironmentStatus | null>(null);
  const [logs, setLogs] = useState<string>('');
  const [selectedService, setSelectedService] = useState<string>('');
  const [isLoading, setIsLoading] = useState(true);
  const [isActionLoading, setIsActionLoading] = useState(false);
  const [actionOutput, setActionOutput] = useState<string>('');
  const { user } = useAuth();

  const isOperator = user?.role === 'operator' || user?.role === 'admin';
  const isAdmin = user?.role === 'admin';

  useEffect(() => {
    loadEnvironments();
  }, []);

  useEffect(() => {
    const selectedId = searchParams.get('selected');
    if (selectedId && environments.length > 0) {
      const env = environments.find((e) => e.id === selectedId);
      if (env) {
        setSelectedEnv(env);
        loadStatus(env.id);
      }
    }
  }, [searchParams, environments]);

  const loadEnvironments = async () => {
    try {
      const envs = await environmentsApi.list();
      setEnvironments(envs);
      
      if (envs.length > 0 && !searchParams.get('selected')) {
        setSelectedEnv(envs[0]);
        loadStatus(envs[0].id);
      }
    } catch (error) {
      console.error('Failed to load environments:', error);
    } finally {
      setIsLoading(false);
    }
  };

  const loadStatus = async (envId: string) => {
    try {
      const statusData = await environmentsApi.getStatus(envId);
      setStatus(statusData);
    } catch (error) {
      console.error('Failed to load status:', error);
      setStatus(null);
    }
  };

  const selectEnvironment = (env: Environment) => {
    setSelectedEnv(env);
    setSearchParams({ selected: env.id });
    setLogs('');
    setActionOutput('');
    loadStatus(env.id);
  };

  const handleAction = async (
    action: 'up' | 'down' | 'restart' | 'cleanup',
    service?: string
  ) => {
    if (!selectedEnv) return;

    // Confirm dangerous actions
    if (action === 'cleanup') {
      if (!confirm('âš ï¸ ì „ì²´ ì •ë¦¬ë¥¼ ì§„í–‰í•˜ë©´ ë°ì´í„°ë² ì´ìŠ¤ ë³¼ë¥¨ë„ ì‚­ì œë©ë‹ˆë‹¤. ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ?')) {
        return;
      }
    }

    setIsActionLoading(true);
    setActionOutput('');

    try {
      let result: { output?: string; message?: string; success?: boolean };
      switch (action) {
        case 'up':
          result = await environmentsApi.up(selectedEnv.id, true) as { output?: string; message?: string };
          break;
        case 'down':
          result = await environmentsApi.down(selectedEnv.id, false) as { output?: string; message?: string };
          break;
        case 'restart':
          result = await environmentsApi.restart(selectedEnv.id, service) as { output?: string; message?: string };
          break;
        case 'cleanup':
          result = await environmentsApi.down(selectedEnv.id, true) as { output?: string; message?: string };
          break;
        default:
          result = { message: 'Unknown action' };
      }
      setActionOutput(result.output || result.message || 'Success');
      await loadStatus(selectedEnv.id);
    } catch (error) {
      setActionOutput(`Error: ${error instanceof Error ? error.message : 'Unknown error'}`);
    } finally {
      setIsActionLoading(false);
    }
  };

  const loadServiceLogs = async (service: string) => {
    if (!selectedEnv) return;
    setSelectedService(service);
    
    try {
      const result = await environmentsApi.logs(selectedEnv.id, service, 200);
      setLogs(result.logs);
    } catch (error) {
      setLogs(`Error loading logs: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  };

  const getStatusIcon = (container: ContainerInfo) => {
    if (container.status === 'up') {
      return <CheckCircle className="w-4 h-4 text-green-400" />;
    } else if (container.status === 'down') {
      return <XCircle className="w-4 h-4 text-red-400" />;
    }
    return <AlertTriangle className="w-4 h-4 text-yellow-400" />;
  };

  if (isLoading) {
    return (
      <div className="flex items-center justify-center h-64">
        <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-blue-500" />
      </div>
    );
  }

  return (
    <div className="space-y-6">
      <h1 className="text-2xl font-bold text-white">í™˜ê²½ ê´€ë¦¬</h1>

      <div className="grid grid-cols-1 lg:grid-cols-4 gap-6">
        {/* Environment List */}
        <div className="lg:col-span-1 space-y-2">
          {environments.map((env) => (
            <button
              key={env.id}
              onClick={() => selectEnvironment(env)}
              className={clsx(
                'w-full flex items-center gap-3 p-4 rounded-lg border transition-colors text-left',
                selectedEnv?.id === env.id
                  ? 'bg-blue-600/20 border-blue-500 text-white'
                  : 'bg-gray-800 border-gray-700 text-gray-300 hover:border-gray-600'
              )}
            >
              <Server className="w-5 h-5" />
              <div>
                <p className="font-medium capitalize">{env.name}</p>
                <p className="text-xs text-gray-400">{env.env_type}</p>
              </div>
            </button>
          ))}
        </div>

        {/* Environment Details */}
        <div className="lg:col-span-3 space-y-6">
          {selectedEnv ? (
            <>
              {/* Actions */}
              <div className="bg-gray-800 rounded-xl border border-gray-700 p-6">
                <h2 className="text-lg font-semibold text-white mb-4">
                  {selectedEnv.name} í™˜ê²½
                </h2>
                <p className="text-gray-400 text-sm mb-4">
                  {selectedEnv.description || selectedEnv.compose_file}
                </p>

                <div className="flex flex-wrap gap-3">
                  <button
                    onClick={() => handleAction('up')}
                    disabled={!isOperator || isActionLoading}
                    className="flex items-center gap-2 px-4 py-2 bg-green-600 hover:bg-green-700 disabled:bg-gray-600 text-white rounded-lg transition-colors"
                  >
                    <Play className="w-4 h-4" />
                    ì‹œì‘
                  </button>
                  <button
                    onClick={() => handleAction('down')}
                    disabled={!isOperator || isActionLoading}
                    className="flex items-center gap-2 px-4 py-2 bg-yellow-600 hover:bg-yellow-700 disabled:bg-gray-600 text-white rounded-lg transition-colors"
                  >
                    <Square className="w-4 h-4" />
                    ì¤‘ì§€
                  </button>
                  <button
                    onClick={() => handleAction('restart')}
                    disabled={!isOperator || isActionLoading}
                    className="flex items-center gap-2 px-4 py-2 bg-blue-600 hover:bg-blue-700 disabled:bg-gray-600 text-white rounded-lg transition-colors"
                  >
                    <RotateCcw className="w-4 h-4" />
                    ì¬ì‹œì‘
                  </button>
                  <button
                    onClick={() => handleAction('cleanup')}
                    disabled={!isAdmin || isActionLoading}
                    className="flex items-center gap-2 px-4 py-2 bg-red-600 hover:bg-red-700 disabled:bg-gray-600 text-white rounded-lg transition-colors"
                  >
                    <Trash2 className="w-4 h-4" />
                    ì „ì²´ ì •ë¦¬
                  </button>
                  <button
                    onClick={() => loadStatus(selectedEnv.id)}
                    disabled={isActionLoading}
                    className="flex items-center gap-2 px-4 py-2 bg-gray-600 hover:bg-gray-700 disabled:bg-gray-700 text-white rounded-lg transition-colors"
                  >
                    <RefreshCw className={clsx('w-4 h-4', isActionLoading && 'animate-spin')} />
                    ìƒˆë¡œê³ ì¹¨
                  </button>
                </div>

                {actionOutput && (
                  <pre className="mt-4 p-4 bg-gray-900 rounded-lg text-sm text-gray-300 overflow-x-auto max-h-48">
                    {actionOutput}
                  </pre>
                )}
              </div>

              {/* Container Status */}
              <div className="bg-gray-800 rounded-xl border border-gray-700">
                <div className="p-4 border-b border-gray-700">
                  <h3 className="font-semibold text-white">
                    ì»¨í…Œì´ë„ˆ ìƒíƒœ
                    {status && (
                      <span className="ml-2 text-sm font-normal text-gray-400">
                        ({status.running_containers}/{status.total_containers} ì‹¤í–‰ ì¤‘)
                      </span>
                    )}
                  </h3>
                </div>
                <div className="divide-y divide-gray-700">
                  {status?.containers && status.containers.length > 0 ? (
                    status.containers.map((container) => (
                      <div
                        key={container.name}
                        className="flex items-center justify-between p-4 hover:bg-gray-700/50"
                      >
                        <div className="flex items-center gap-3">
                          {getStatusIcon(container)}
                          <div>
                            <p className="text-sm font-medium text-white">
                              {container.name}
                            </p>
                            <p className="text-xs text-gray-400">{container.image}</p>
                          </div>
                        </div>
                        <div className="flex items-center gap-2">
                          <span className={clsx(
                            'px-2 py-1 rounded text-xs capitalize',
                            container.status === 'up'
                              ? 'bg-green-500/10 text-green-400'
                              : 'bg-red-500/10 text-red-400'
                          )}>
                            {container.status}
                          </span>
                          <button
                            onClick={() => loadServiceLogs(container.name.replace('newsinsight-', ''))}
                            className="p-1 text-gray-400 hover:text-white"
                            title="ë¡œê·¸ ë³´ê¸°"
                          >
                            <Terminal className="w-4 h-4" />
                          </button>
                        </div>
                      </div>
                    ))
                  ) : (
                    <div className="p-8 text-center text-gray-500">
                      ì»¨í…Œì´ë„ˆ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤
                    </div>
                  )}
                </div>
              </div>

              {/* Logs */}
              {logs && (
                <div className="bg-gray-800 rounded-xl border border-gray-700">
                  <div className="flex items-center justify-between p-4 border-b border-gray-700">
                    <h3 className="font-semibold text-white">
                      ë¡œê·¸: {selectedService}
                    </h3>
                    <button
                      onClick={() => setLogs('')}
                      className="text-gray-400 hover:text-white"
                    >
                      ë‹«ê¸°
                    </button>
                  </div>
                  <pre className="p-4 text-sm text-gray-300 overflow-x-auto max-h-96 font-mono">
                    {logs}
                  </pre>
                </div>
              )}
            </>
          ) : (
            <div className="bg-gray-800 rounded-xl border border-gray-700 p-8 text-center text-gray-500">
              í™˜ê²½ì„ ì„ íƒí•´ì£¼ì„¸ìš”
            </div>
          )}
        </div>
      </div>
    </div>
  );
}

```

---

## backend/admin-dashboard/web/src/pages/HealthMonitor.tsx

```tsx
import { useState, useEffect, useCallback } from 'react';
import {
  Activity,
  Server,
  Database,
  CheckCircle,
  XCircle,
  AlertCircle,
  RefreshCw,
  Wifi,
  WifiOff,
  Clock,
  Zap,
} from 'lucide-react';
import { healthMonitorApi } from '../api/endpoints';
import type {
  ServiceHealth,
  InfrastructureHealth,
  OverallSystemHealth,
  ServiceHealthStatus,
} from '../types';
import clsx from 'clsx';

const statusConfig: Record<ServiceHealthStatus, { color: string; icon: typeof CheckCircle; label: string }> = {
  healthy: { color: 'text-green-400', icon: CheckCircle, label: 'Healthy' },
  unhealthy: { color: 'text-red-400', icon: XCircle, label: 'Unhealthy' },
  degraded: { color: 'text-yellow-400', icon: AlertCircle, label: 'Degraded' },
  unreachable: { color: 'text-gray-400', icon: WifiOff, label: 'Unreachable' },
  unknown: { color: 'text-gray-500', icon: AlertCircle, label: 'Unknown' },
};

function StatusBadge({ status }: { status: ServiceHealthStatus }) {
  const config = statusConfig[status] || statusConfig.unknown;
  const Icon = config.icon;
  
  return (
    <span className={clsx('flex items-center gap-1.5 text-sm font-medium', config.color)}>
      <Icon className="w-4 h-4" />
      {config.label}
    </span>
  );
}

function ServiceCard({ service, onRefresh }: { service: ServiceHealth; onRefresh: () => void }) {
  const [isRefreshing, setIsRefreshing] = useState(false);
  
  const handleRefresh = async () => {
    setIsRefreshing(true);
    try {
      await healthMonitorApi.checkService(service.service_id);
      onRefresh();
    } finally {
      setIsRefreshing(false);
    }
  };
  
  return (
    <div className="bg-gray-800 rounded-lg p-4 border border-gray-700">
      <div className="flex items-start justify-between mb-3">
        <div className="flex items-center gap-2">
          <Server className="w-5 h-5 text-blue-400" />
          <h3 className="font-medium text-white">{service.name}</h3>
        </div>
        <button
          onClick={handleRefresh}
          disabled={isRefreshing}
          className="p-1 text-gray-400 hover:text-white transition-colors"
        >
          <RefreshCw className={clsx('w-4 h-4', isRefreshing && 'animate-spin')} />
        </button>
      </div>
      
      <StatusBadge status={service.status} />
      
      <div className="mt-3 space-y-1 text-sm text-gray-400">
        {service.message && <p>{service.message}</p>}
        {service.response_time_ms !== undefined && service.response_time_ms !== null && (
          <p className="flex items-center gap-1">
            <Clock className="w-3 h-3" />
            {service.response_time_ms.toFixed(0)}ms
          </p>
        )}
        {service.url && (
          <p className="truncate text-xs text-gray-500" title={service.url}>
            {service.url}
          </p>
        )}
      </div>
    </div>
  );
}

function InfraCard({ infra }: { infra: InfrastructureHealth }) {
  const iconMap: Record<string, typeof Database> = {
    postgres: Database,
    mongo: Database,
    redis: Zap,
    consul: Server,
    redpanda: Activity,
  };
  
  const Icon = iconMap[infra.service_id] || Database;
  
  return (
    <div className="bg-gray-800 rounded-lg p-4 border border-gray-700">
      <div className="flex items-center gap-2 mb-3">
        <Icon className="w-5 h-5 text-purple-400" />
        <h3 className="font-medium text-white">{infra.name}</h3>
      </div>
      
      <StatusBadge status={infra.status} />
      
      <div className="mt-3 space-y-1 text-sm text-gray-400">
        {infra.message && <p>{infra.message}</p>}
        {infra.port && <p>Port: {infra.port}</p>}
      </div>
    </div>
  );
}

function OverallHealthCard({ health }: { health: OverallSystemHealth }) {
  const statusColor = {
    healthy: 'bg-green-500/20 border-green-500',
    degraded: 'bg-yellow-500/20 border-yellow-500',
    unhealthy: 'bg-red-500/20 border-red-500',
    unreachable: 'bg-gray-500/20 border-gray-500',
    unknown: 'bg-gray-500/20 border-gray-500',
  }[health.status];
  
  return (
    <div className={clsx('rounded-xl p-6 border-2', statusColor)}>
      <div className="flex items-center justify-between mb-4">
        <div className="flex items-center gap-3">
          <Activity className="w-8 h-8 text-white" />
          <div>
            <h2 className="text-xl font-bold text-white">System Health</h2>
            <p className="text-gray-400 text-sm">
              Last checked: {new Date(health.checked_at).toLocaleTimeString()}
            </p>
          </div>
        </div>
        <StatusBadge status={health.status} />
      </div>
      
      <div className="grid grid-cols-2 md:grid-cols-4 gap-4">
        <div className="bg-gray-800/50 rounded-lg p-3">
          <p className="text-gray-400 text-sm">Services</p>
          <p className="text-2xl font-bold text-white">
            {health.healthy_services}/{health.total_services}
          </p>
          <p className="text-green-400 text-xs">healthy</p>
        </div>
        
        <div className="bg-gray-800/50 rounded-lg p-3">
          <p className="text-gray-400 text-sm">Infrastructure</p>
          <p className="text-2xl font-bold text-white">
            {health.healthy_infrastructure}/{health.total_infrastructure}
          </p>
          <p className="text-purple-400 text-xs">healthy</p>
        </div>
        
        <div className="bg-gray-800/50 rounded-lg p-3">
          <p className="text-gray-400 text-sm">Unhealthy</p>
          <p className="text-2xl font-bold text-red-400">
            {health.unhealthy_services}
          </p>
          <p className="text-gray-500 text-xs">services</p>
        </div>
        
        <div className="bg-gray-800/50 rounded-lg p-3">
          <p className="text-gray-400 text-sm">Avg Response</p>
          <p className="text-2xl font-bold text-white">
            {health.average_response_time_ms 
              ? `${health.average_response_time_ms.toFixed(0)}ms` 
              : 'N/A'}
          </p>
          <p className="text-blue-400 text-xs">latency</p>
        </div>
      </div>
    </div>
  );
}

export default function HealthMonitor() {
  const [health, setHealth] = useState<OverallSystemHealth | null>(null);
  const [isLoading, setIsLoading] = useState(true);
  const [isRefreshing, setIsRefreshing] = useState(false);
  const [autoRefresh, setAutoRefresh] = useState(false);
  const [lastUpdated, setLastUpdated] = useState<Date | null>(null);

  const fetchHealth = useCallback(async (showLoading = false) => {
    if (showLoading) setIsLoading(true);
    setIsRefreshing(true);
    try {
      const data = await healthMonitorApi.getOverallHealth();
      setHealth(data);
      setLastUpdated(new Date());
    } catch (error) {
      console.error('Failed to fetch health:', error);
    } finally {
      setIsLoading(false);
      setIsRefreshing(false);
    }
  }, []);

  useEffect(() => {
    fetchHealth(true);
  }, [fetchHealth]);

  useEffect(() => {
    if (!autoRefresh) return;
    
    const interval = setInterval(() => {
      fetchHealth(false);
    }, 10000); // 10 seconds
    
    return () => clearInterval(interval);
  }, [autoRefresh, fetchHealth]);

  if (isLoading) {
    return (
      <div className="flex items-center justify-center h-64">
        <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-blue-500" />
      </div>
    );
  }

  return (
    <div className="space-y-6">
      {/* Header */}
      <div className="flex items-center justify-between">
        <div>
          <h1 className="text-2xl font-bold text-white">Health Monitor</h1>
          <p className="text-gray-400 mt-1">
            Real-time service health and infrastructure monitoring
          </p>
        </div>
        <div className="flex items-center gap-3">
          <label className="flex items-center gap-2 text-sm text-gray-400">
            <input
              type="checkbox"
              checked={autoRefresh}
              onChange={(e) => setAutoRefresh(e.target.checked)}
              className="rounded border-gray-600 bg-gray-700 text-blue-500 focus:ring-blue-500"
            />
            Auto-refresh (10s)
          </label>
          <button
            onClick={() => fetchHealth(false)}
            disabled={isRefreshing}
            className="flex items-center gap-2 px-4 py-2 bg-blue-600 hover:bg-blue-700 text-white rounded-lg transition-colors disabled:opacity-50"
          >
            <RefreshCw className={clsx('w-4 h-4', isRefreshing && 'animate-spin')} />
            Refresh
          </button>
        </div>
      </div>

      {/* Overall Health */}
      {health && <OverallHealthCard health={health} />}

      {/* Services */}
      <div>
        <h2 className="text-lg font-semibold text-white mb-4 flex items-center gap-2">
          <Server className="w-5 h-5 text-blue-400" />
          Services ({health?.services.length || 0})
        </h2>
        <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 xl:grid-cols-4 gap-4">
          {health?.services.map((service) => (
            <ServiceCard
              key={service.service_id}
              service={service}
              onRefresh={() => fetchHealth(false)}
            />
          ))}
        </div>
      </div>

      {/* Infrastructure */}
      <div>
        <h2 className="text-lg font-semibold text-white mb-4 flex items-center gap-2">
          <Database className="w-5 h-5 text-purple-400" />
          Infrastructure ({health?.infrastructure.length || 0})
        </h2>
        <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 xl:grid-cols-5 gap-4">
          {health?.infrastructure.map((infra) => (
            <InfraCard key={infra.service_id} infra={infra} />
          ))}
        </div>
      </div>

      {/* Connection Status */}
      <div className="flex items-center justify-end gap-2 text-sm text-gray-500">
        {autoRefresh ? (
          <Wifi className="w-4 h-4 text-green-400" />
        ) : (
          <WifiOff className="w-4 h-4" />
        )}
        {lastUpdated && (
          <span>Last updated: {lastUpdated.toLocaleTimeString()}</span>
        )}
      </div>
    </div>
  );
}

```

---

## backend/admin-dashboard/web/src/pages/Kafka.tsx

```tsx
import { useState, useEffect, useCallback } from 'react';
import {
  Activity,
  Radio,
  Users,
  MessageSquare,
  Server,
  CheckCircle,
  XCircle,
  AlertTriangle,
  RefreshCw,
  Layers,
  Clock,
  ArrowDownToLine,
} from 'lucide-react';
import { kafkaApi } from '../api/endpoints';
import type {
  KafkaClusterInfo,
  KafkaTopicInfo,
  KafkaConsumerGroupInfo,
} from '../types';
import clsx from 'clsx';

const stateColors: Record<string, string> = {
  Stable: 'text-green-400',
  Empty: 'text-gray-400',
  Dead: 'text-red-400',
  PreparingRebalance: 'text-yellow-400',
  CompletingRebalance: 'text-yellow-400',
  Unknown: 'text-gray-500',
};

function TopicCard({ topic }: { topic: KafkaTopicInfo }) {
  const formatBytes = (bytes?: number) => {
    if (!bytes) return 'N/A';
    const units = ['B', 'KB', 'MB', 'GB', 'TB'];
    let i = 0;
    let size = bytes;
    while (size >= 1024 && i < units.length - 1) {
      size /= 1024;
      i++;
    }
    return `${size.toFixed(1)} ${units[i]}`;
  };

  const formatRetention = (ms?: number) => {
    if (!ms) return 'Default';
    const hours = ms / (1000 * 60 * 60);
    if (hours < 24) return `${hours}h`;
    const days = hours / 24;
    return `${days}d`;
  };

  return (
    <div className={clsx(
      'bg-gray-800 rounded-lg p-4 border border-gray-700',
      topic.is_internal && 'opacity-60'
    )}>
      <div className="flex items-start justify-between mb-3">
        <div className="flex items-center gap-2">
          <MessageSquare className="w-5 h-5 text-blue-400" />
          <div>
            <h3 className="font-medium text-white">{topic.name}</h3>
            {topic.is_internal && (
              <span className="text-xs text-gray-500">Internal</span>
            )}
          </div>
        </div>
      </div>

      <div className="space-y-2 text-sm">
        <div className="flex items-center justify-between text-gray-400">
          <span className="flex items-center gap-1">
            <Layers className="w-3 h-3" />
            Partitions
          </span>
          <span className="text-white">{topic.partition_count}</span>
        </div>
        <div className="flex items-center justify-between text-gray-400">
          <span>Replication</span>
          <span className="text-white">{topic.replication_factor}x</span>
        </div>
        {topic.message_count !== undefined && (
          <div className="flex items-center justify-between text-gray-400">
            <span>Messages</span>
            <span className="text-white">{topic.message_count.toLocaleString()}</span>
          </div>
        )}
        {topic.size_bytes !== undefined && (
          <div className="flex items-center justify-between text-gray-400">
            <span>Size</span>
            <span className="text-white">{formatBytes(topic.size_bytes)}</span>
          </div>
        )}
        <div className="flex items-center justify-between text-gray-400">
          <span>Retention</span>
          <span className="text-white">{formatRetention(topic.retention_ms)}</span>
        </div>
      </div>
    </div>
  );
}

function ConsumerGroupCard({ group }: { group: KafkaConsumerGroupInfo }) {
  const stateColor = stateColors[group.state] || stateColors.Unknown;

  return (
    <div className="bg-gray-800 rounded-lg p-4 border border-gray-700">
      <div className="flex items-start justify-between mb-3">
        <div className="flex items-center gap-2">
          <Users className="w-5 h-5 text-purple-400" />
          <h3 className="font-medium text-white">{group.group_id}</h3>
        </div>
        <span className={clsx('text-sm font-medium', stateColor)}>
          {group.state}
        </span>
      </div>

      <div className="space-y-2 text-sm">
        <div className="flex items-center justify-between text-gray-400">
          <span>Members</span>
          <span className="text-white">{group.members_count}</span>
        </div>
        <div className="flex items-center justify-between text-gray-400">
          <span>Topics</span>
          <span className="text-white">{group.topics.length}</span>
        </div>
        <div className="flex items-center justify-between text-gray-400">
          <span className="flex items-center gap-1">
            <ArrowDownToLine className="w-3 h-3" />
            Total Lag
          </span>
          <span className={clsx(
            'font-medium',
            group.total_lag > 1000 ? 'text-red-400' :
            group.total_lag > 100 ? 'text-yellow-400' : 'text-green-400'
          )}>
            {group.total_lag.toLocaleString()}
          </span>
        </div>
      </div>

      {group.topics.length > 0 && (
        <div className="mt-3 pt-3 border-t border-gray-700">
          <p className="text-xs text-gray-500 mb-2">Subscribed Topics:</p>
          <div className="flex flex-wrap gap-1">
            {group.topics.map((topic) => (
              <span
                key={topic}
                className="px-2 py-0.5 text-xs bg-gray-700 text-gray-300 rounded"
              >
                {topic}
              </span>
            ))}
          </div>
        </div>
      )}
    </div>
  );
}

function ClusterOverview({ cluster }: { cluster: KafkaClusterInfo }) {
  return (
    <div className="bg-gradient-to-r from-blue-900/30 to-purple-900/30 rounded-xl p-6 border border-blue-500/30">
      <div className="flex items-center gap-4 mb-6">
        <div className="p-3 bg-blue-500/20 rounded-lg">
          <Radio className="w-8 h-8 text-blue-400" />
        </div>
        <div>
          <h2 className="text-xl font-bold text-white">Kafka/Redpanda Cluster</h2>
          <p className="text-gray-400 text-sm">
            Last checked: {new Date(cluster.checked_at).toLocaleTimeString()}
          </p>
        </div>
      </div>

      <div className="grid grid-cols-2 md:grid-cols-4 gap-4">
        <div className="bg-gray-800/50 rounded-lg p-4">
          <div className="flex items-center gap-2 text-gray-400 mb-1">
            <Server className="w-4 h-4" />
            <span className="text-sm">Brokers</span>
          </div>
          <p className="text-2xl font-bold text-white">{cluster.broker_count}</p>
          {cluster.controller_id !== undefined && (
            <p className="text-xs text-gray-500">Controller: {cluster.controller_id}</p>
          )}
        </div>

        <div className="bg-gray-800/50 rounded-lg p-4">
          <div className="flex items-center gap-2 text-gray-400 mb-1">
            <MessageSquare className="w-4 h-4" />
            <span className="text-sm">Topics</span>
          </div>
          <p className="text-2xl font-bold text-white">{cluster.total_topics}</p>
          <p className="text-xs text-gray-500">
            {cluster.topics.filter(t => !t.is_internal).length} user topics
          </p>
        </div>

        <div className="bg-gray-800/50 rounded-lg p-4">
          <div className="flex items-center gap-2 text-gray-400 mb-1">
            <Layers className="w-4 h-4" />
            <span className="text-sm">Partitions</span>
          </div>
          <p className="text-2xl font-bold text-white">{cluster.total_partitions}</p>
        </div>

        <div className="bg-gray-800/50 rounded-lg p-4">
          <div className="flex items-center gap-2 text-gray-400 mb-1">
            <Users className="w-4 h-4" />
            <span className="text-sm">Consumer Groups</span>
          </div>
          <p className="text-2xl font-bold text-white">{cluster.consumer_groups.length}</p>
          <p className="text-xs text-gray-500">
            {cluster.consumer_groups.filter(g => g.state === 'Stable').length} stable
          </p>
        </div>
      </div>
    </div>
  );
}

export default function Kafka() {
  const [cluster, setCluster] = useState<KafkaClusterInfo | null>(null);
  const [isLoading, setIsLoading] = useState(true);
  const [isRefreshing, setIsRefreshing] = useState(false);
  const [activeTab, setActiveTab] = useState<'topics' | 'consumers'>('topics');
  const [showInternal, setShowInternal] = useState(false);

  const fetchCluster = useCallback(async (showLoading = false) => {
    if (showLoading) setIsLoading(true);
    setIsRefreshing(true);
    try {
      const data = await kafkaApi.getClusterInfo();
      setCluster(data);
    } catch (error) {
      console.error('Failed to fetch Kafka cluster info:', error);
    } finally {
      setIsLoading(false);
      setIsRefreshing(false);
    }
  }, []);

  useEffect(() => {
    fetchCluster(true);
  }, [fetchCluster]);

  if (isLoading) {
    return (
      <div className="flex items-center justify-center h-64">
        <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-blue-500" />
      </div>
    );
  }

  const filteredTopics = cluster?.topics.filter(t => showInternal || !t.is_internal) || [];

  return (
    <div className="space-y-6">
      {/* Header */}
      <div className="flex items-center justify-between">
        <div>
          <h1 className="text-2xl font-bold text-white">Kafka / Redpanda</h1>
          <p className="text-gray-400 mt-1">
            Monitor message queues, topics, and consumer groups
          </p>
        </div>
        <button
          onClick={() => fetchCluster(false)}
          disabled={isRefreshing}
          className="flex items-center gap-2 px-4 py-2 bg-blue-600 hover:bg-blue-700 text-white rounded-lg transition-colors disabled:opacity-50"
        >
          <RefreshCw className={clsx('w-4 h-4', isRefreshing && 'animate-spin')} />
          Refresh
        </button>
      </div>

      {/* Cluster Overview */}
      {cluster && <ClusterOverview cluster={cluster} />}

      {/* Tabs */}
      <div className="border-b border-gray-700">
        <nav className="flex gap-4">
          <button
            onClick={() => setActiveTab('topics')}
            className={clsx(
              'flex items-center gap-2 px-4 py-3 text-sm font-medium border-b-2 transition-colors',
              activeTab === 'topics'
                ? 'text-blue-400 border-blue-400'
                : 'text-gray-400 border-transparent hover:text-white hover:border-gray-600'
            )}
          >
            <MessageSquare className="w-4 h-4" />
            Topics ({filteredTopics.length})
          </button>
          <button
            onClick={() => setActiveTab('consumers')}
            className={clsx(
              'flex items-center gap-2 px-4 py-3 text-sm font-medium border-b-2 transition-colors',
              activeTab === 'consumers'
                ? 'text-purple-400 border-purple-400'
                : 'text-gray-400 border-transparent hover:text-white hover:border-gray-600'
            )}
          >
            <Users className="w-4 h-4" />
            Consumer Groups ({cluster?.consumer_groups.length || 0})
          </button>
        </nav>
      </div>

      {/* Tab Content */}
      {activeTab === 'topics' && (
        <div>
          <div className="flex items-center justify-between mb-4">
            <h2 className="text-lg font-semibold text-white">Topics</h2>
            <label className="flex items-center gap-2 text-sm text-gray-400">
              <input
                type="checkbox"
                checked={showInternal}
                onChange={(e) => setShowInternal(e.target.checked)}
                className="rounded border-gray-600 bg-gray-700 text-blue-500 focus:ring-blue-500"
              />
              Show internal topics
            </label>
          </div>
          <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 xl:grid-cols-4 gap-4">
            {filteredTopics.map((topic) => (
              <TopicCard key={topic.name} topic={topic} />
            ))}
          </div>
          {filteredTopics.length === 0 && (
            <div className="text-center py-12 text-gray-500">
              <MessageSquare className="w-12 h-12 mx-auto mb-4 opacity-50" />
              <p>No topics found</p>
            </div>
          )}
        </div>
      )}

      {activeTab === 'consumers' && (
        <div>
          <h2 className="text-lg font-semibold text-white mb-4">Consumer Groups</h2>
          <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-4">
            {cluster?.consumer_groups.map((group) => (
              <ConsumerGroupCard key={group.group_id} group={group} />
            ))}
          </div>
          {(!cluster?.consumer_groups || cluster.consumer_groups.length === 0) && (
            <div className="text-center py-12 text-gray-500">
              <Users className="w-12 h-12 mx-auto mb-4 opacity-50" />
              <p>No consumer groups found</p>
            </div>
          )}
        </div>
      )}
    </div>
  );
}

```

---

## backend/admin-dashboard/web/src/pages/LlmSettings.tsx

```tsx
import { useEffect, useState } from 'react';
import {
  Bot,
  CheckCircle,
  XCircle,
  Loader2,
  Zap,
  Save,
  Eye,
  EyeOff,
  Trash2,
  RefreshCw,
  AlertTriangle,
  Settings,
} from 'lucide-react';
import {
  llmProvidersApi,
  type LlmProviderType,
  type LlmProviderTypeInfo,
  type LlmProviderSettings,
  type LlmProviderSettingsRequest,
  type LlmTestResult,
} from '../api/endpoints';
import { useAuth } from '../contexts/AuthContext';
import clsx from 'clsx';

// Default models per provider
const DEFAULT_MODELS: Record<LlmProviderType, string[]> = {
  OPENAI: ['gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo', 'gpt-3.5-turbo', 'o1-preview', 'o1-mini'],
  ANTHROPIC: ['claude-3-5-sonnet-20241022', 'claude-3-5-haiku-20241022', 'claude-3-opus-20240229'],
  GOOGLE: ['gemini-1.5-pro', 'gemini-1.5-flash', 'gemini-2.0-flash-exp'],
  OPENROUTER: ['openai/gpt-4o', 'anthropic/claude-3.5-sonnet', 'google/gemini-pro-1.5', 'meta-llama/llama-3.1-70b-instruct'],
  OLLAMA: ['llama3.1', 'mistral', 'mixtral', 'codellama', 'qwen2.5'],
  AZURE_OPENAI: ['gpt-4o', 'gpt-4-turbo', 'gpt-35-turbo'],
  CUSTOM: ['default'],
};

export default function LlmSettings() {
  const { user } = useAuth();
  const isAdmin = user?.role === 'admin';

  // State
  const [providerTypes, setProviderTypes] = useState<LlmProviderTypeInfo[]>([]);
  const [globalSettings, setGlobalSettings] = useState<LlmProviderSettings[]>([]);
  const [isLoading, setIsLoading] = useState(true);
  const [isSaving, setIsSaving] = useState(false);
  const [showApiKeys, setShowApiKeys] = useState<Record<string, boolean>>({});
  const [testResults, setTestResults] = useState<Record<string, LlmTestResult>>({});
  const [testingProvider, setTestingProvider] = useState<string | null>(null);
  const [error, setError] = useState<string | null>(null);

  // Edit dialog state
  const [editDialogOpen, setEditDialogOpen] = useState(false);
  const [editingProvider, setEditingProvider] = useState<LlmProviderType | null>(null);
  const [editForm, setEditForm] = useState<LlmProviderSettingsRequest>({
    providerType: 'OPENAI',
    apiKey: '',
    defaultModel: '',
    baseUrl: '',
    enabled: true,
    priority: 100,
    maxTokens: 4096,
    temperature: 0.7,
    timeoutMs: 60000,
  });

  // Load data
  const loadData = async () => {
    setIsLoading(true);
    setError(null);
    try {
      const [types, settings] = await Promise.all([
        llmProvidersApi.getTypes(),
        llmProvidersApi.listGlobal(),
      ]);
      setProviderTypes(types);
      setGlobalSettings(settings);
    } catch (err) {
      console.error('Failed to load LLM settings:', err);
      setError('LLM ì„¤ì •ì„ ë¶ˆëŸ¬ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.');
    } finally {
      setIsLoading(false);
    }
  };

  useEffect(() => {
    loadData();
  }, []);

  // Get setting for provider
  const getSetting = (providerType: LlmProviderType): LlmProviderSettings | undefined => {
    return globalSettings.find((s) => s.providerType === providerType);
  };

  // Open edit dialog
  const openEditDialog = (providerType: LlmProviderType) => {
    const existing = getSetting(providerType);

    setEditingProvider(providerType);
    setEditForm({
      providerType,
      apiKey: '', // Always empty for security
      defaultModel: existing?.defaultModel || DEFAULT_MODELS[providerType][0],
      baseUrl: existing?.baseUrl || '',
      enabled: existing?.enabled ?? true,
      priority: existing?.priority ?? 100,
      maxTokens: existing?.maxTokens ?? 4096,
      temperature: existing?.temperature ?? 0.7,
      timeoutMs: existing?.timeoutMs ?? 60000,
      azureDeploymentName: existing?.azureDeploymentName || '',
      azureApiVersion: existing?.azureApiVersion || '2024-02-01',
    });
    setEditDialogOpen(true);
  };

  // Save setting
  const handleSave = async () => {
    if (!editingProvider) return;

    setIsSaving(true);
    try {
      await llmProvidersApi.saveGlobal(editingProvider, editForm);
      setEditDialogOpen(false);
      await loadData();
    } catch (err) {
      console.error('Failed to save LLM setting:', err);
      setError('ì„¤ì • ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.');
    } finally {
      setIsSaving(false);
    }
  };

  // Delete setting
  const handleDelete = async (providerType: LlmProviderType) => {
    if (!confirm(`${providerType} ì „ì—­ ì„¤ì •ì„ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?`)) return;

    try {
      await llmProvidersApi.deleteGlobal(providerType);
      await loadData();
    } catch (err) {
      console.error('Failed to delete LLM setting:', err);
      setError('ì„¤ì • ì‚­ì œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.');
    }
  };

  // Test connection
  const handleTestConnection = async (providerType: LlmProviderType) => {
    setTestingProvider(providerType);
    try {
      const setting = getSetting(providerType);
      const result = await llmProvidersApi.testConnection(
        providerType,
        setting?.defaultModel
      );
      setTestResults((prev) => ({ ...prev, [providerType]: result }));
    } catch (err) {
      console.error('Connection test failed:', err);
      setTestResults((prev) => ({
        ...prev,
        [providerType]: {
          providerType,
          success: false,
          message: 'ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨',
          testedAt: new Date().toISOString(),
        },
      }));
    } finally {
      setTestingProvider(null);
    }
  };

  // Toggle API key visibility
  const toggleKeyVisibility = (provider: string) => {
    setShowApiKeys((prev) => ({ ...prev, [provider]: !prev[provider] }));
  };

  if (!isAdmin) {
    return (
      <div className="flex items-center justify-center h-64">
        <div className="text-center text-gray-400">
          <AlertTriangle className="w-12 h-12 mx-auto mb-4" />
          <p>ê´€ë¦¬ì ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.</p>
        </div>
      </div>
    );
  }

  if (isLoading) {
    return (
      <div className="flex items-center justify-center h-64">
        <Loader2 className="w-8 h-8 animate-spin text-blue-500" />
      </div>
    );
  }

  return (
    <div className="space-y-6">
      {/* Header */}
      <div className="flex items-center justify-between">
        <div>
          <h1 className="text-2xl font-bold text-white flex items-center gap-3">
            <Bot className="w-7 h-7" />
            LLM Provider ì„¤ì •
          </h1>
          <p className="text-gray-400 mt-1">
            ì „ì—­ LLM Provider ì„¤ì •ì„ ê´€ë¦¬í•©ë‹ˆë‹¤. ì‚¬ìš©ìë³„ ì„¤ì •ì´ ì—†ìœ¼ë©´ ì´ ì„¤ì •ì´ ì ìš©ë©ë‹ˆë‹¤.
          </p>
        </div>
        <button
          onClick={loadData}
          className="flex items-center gap-2 px-4 py-2 bg-gray-700 hover:bg-gray-600 text-white rounded-lg transition-colors"
        >
          <RefreshCw className="w-4 h-4" />
          ìƒˆë¡œê³ ì¹¨
        </button>
      </div>

      {/* Error Alert */}
      {error && (
        <div className="bg-red-500/10 border border-red-500/30 rounded-lg p-4 flex items-center gap-3">
          <AlertTriangle className="w-5 h-5 text-red-400" />
          <p className="text-red-400">{error}</p>
          <button
            onClick={() => setError(null)}
            className="ml-auto text-red-400 hover:text-red-300"
          >
            ë‹«ê¸°
          </button>
        </div>
      )}

      {/* Provider List */}
      <div className="grid gap-4">
        {providerTypes.map((type) => {
          const setting = getSetting(type.value);
          const testResult = testResults[type.value];
          const hasSetting = !!setting;

          return (
            <div
              key={type.value}
              className={clsx(
                'bg-gray-800 rounded-xl border p-6 transition-colors',
                hasSetting ? 'border-gray-700' : 'border-gray-800'
              )}
            >
              <div className="flex items-center justify-between">
                <div className="flex items-center gap-4">
                  <div
                    className={clsx(
                      'w-12 h-12 rounded-lg flex items-center justify-center',
                      hasSetting && setting.enabled
                        ? 'bg-blue-500/20'
                        : 'bg-gray-700'
                    )}
                  >
                    <Bot
                      className={clsx(
                        'w-6 h-6',
                        hasSetting && setting.enabled
                          ? 'text-blue-400'
                          : 'text-gray-500'
                      )}
                    />
                  </div>

                  <div>
                    <div className="flex items-center gap-2">
                      <h3 className="text-lg font-semibold text-white">
                        {type.displayName}
                      </h3>
                      {hasSetting && (
                        <span
                          className={clsx(
                            'px-2 py-0.5 rounded text-xs',
                            setting.enabled
                              ? 'bg-green-500/10 text-green-400'
                              : 'bg-gray-700 text-gray-400'
                          )}
                        >
                          {setting.enabled ? 'í™œì„±í™”' : 'ë¹„í™œì„±í™”'}
                        </span>
                      )}
                    </div>
                    <p className="text-sm text-gray-400">{type.description}</p>
                    {hasSetting && (
                      <p className="text-xs text-gray-500 mt-1">
                        ëª¨ë¸: {setting.defaultModel}
                        {setting.hasApiKey && ' | API í‚¤ ì„¤ì •ë¨'}
                      </p>
                    )}
                  </div>
                </div>

                <div className="flex items-center gap-3">
                  {/* Test Result Badge */}
                  {testResult && (
                    <span
                      className={clsx(
                        'flex items-center gap-1 px-2 py-1 rounded text-xs',
                        testResult.success
                          ? 'bg-green-500/10 text-green-400'
                          : 'bg-red-500/10 text-red-400'
                      )}
                    >
                      {testResult.success ? (
                        <CheckCircle className="w-3 h-3" />
                      ) : (
                        <XCircle className="w-3 h-3" />
                      )}
                      {testResult.success ? 'ì—°ê²°ë¨' : 'ì‹¤íŒ¨'}
                      {testResult.latencyMs && (
                        <span className="ml-1">({testResult.latencyMs}ms)</span>
                      )}
                    </span>
                  )}

                  {/* Actions */}
                  <button
                    onClick={() => handleTestConnection(type.value)}
                    disabled={!hasSetting || !setting.hasApiKey || testingProvider === type.value}
                    className="p-2 text-gray-400 hover:text-white disabled:opacity-50 disabled:cursor-not-allowed transition-colors"
                    title="ì—°ê²° í…ŒìŠ¤íŠ¸"
                  >
                    {testingProvider === type.value ? (
                      <Loader2 className="w-5 h-5 animate-spin" />
                    ) : (
                      <Zap className="w-5 h-5" />
                    )}
                  </button>

                  <button
                    onClick={() => openEditDialog(type.value)}
                    className="flex items-center gap-2 px-4 py-2 bg-blue-600 hover:bg-blue-700 text-white rounded-lg transition-colors"
                  >
                    <Settings className="w-4 h-4" />
                    {hasSetting ? 'ìˆ˜ì •' : 'ì„¤ì •'}
                  </button>

                  {hasSetting && (
                    <button
                      onClick={() => handleDelete(type.value)}
                      className="p-2 text-gray-400 hover:text-red-400 transition-colors"
                      title="ì‚­ì œ"
                    >
                      <Trash2 className="w-5 h-5" />
                    </button>
                  )}
                </div>
              </div>
            </div>
          );
        })}
      </div>

      {/* Edit Dialog */}
      {editDialogOpen && editingProvider && (
        <div className="fixed inset-0 bg-black/50 flex items-center justify-center z-50">
          <div className="bg-gray-800 rounded-xl border border-gray-700 w-full max-w-lg p-6 max-h-[90vh] overflow-y-auto">
            <h2 className="text-xl font-bold text-white mb-4">
              {editingProvider} ì „ì—­ ì„¤ì •
            </h2>
            <p className="text-sm text-gray-400 mb-6">
              ëª¨ë“  ì‚¬ìš©ìì—ê²Œ ì ìš©ë˜ëŠ” ê¸°ë³¸ ì„¤ì •ì…ë‹ˆë‹¤.
            </p>

            <div className="space-y-4">
              {/* API Key */}
              <div>
                <label className="block text-sm font-medium text-gray-300 mb-2">
                  API í‚¤
                </label>
                <div className="flex gap-2">
                  <input
                    type={showApiKeys['edit'] ? 'text' : 'password'}
                    value={editForm.apiKey || ''}
                    onChange={(e) =>
                      setEditForm((prev) => ({ ...prev, apiKey: e.target.value }))
                    }
                    placeholder="ìƒˆ API í‚¤ ì…ë ¥ (ë¹„ìš°ë©´ ê¸°ì¡´ ê°’ ìœ ì§€)"
                    className="flex-1 px-3 py-2 bg-gray-900 border border-gray-700 rounded-lg text-white placeholder-gray-500 focus:outline-none focus:border-blue-500"
                  />
                  <button
                    type="button"
                    onClick={() => toggleKeyVisibility('edit')}
                    className="p-2 text-gray-400 hover:text-white"
                  >
                    {showApiKeys['edit'] ? (
                      <EyeOff className="w-5 h-5" />
                    ) : (
                      <Eye className="w-5 h-5" />
                    )}
                  </button>
                </div>
              </div>

              {/* Model */}
              <div>
                <label className="block text-sm font-medium text-gray-300 mb-2">
                  ê¸°ë³¸ ëª¨ë¸
                </label>
                <select
                  value={editForm.defaultModel}
                  onChange={(e) =>
                    setEditForm((prev) => ({ ...prev, defaultModel: e.target.value }))
                  }
                  className="w-full px-3 py-2 bg-gray-900 border border-gray-700 rounded-lg text-white focus:outline-none focus:border-blue-500"
                >
                  {DEFAULT_MODELS[editingProvider]?.map((model) => (
                    <option key={model} value={model}>
                      {model}
                    </option>
                  ))}
                </select>
              </div>

              {/* Base URL (for Ollama/Custom) */}
              {(editingProvider === 'OLLAMA' || editingProvider === 'CUSTOM') && (
                <div>
                  <label className="block text-sm font-medium text-gray-300 mb-2">
                    Base URL
                  </label>
                  <input
                    type="text"
                    value={editForm.baseUrl || ''}
                    onChange={(e) =>
                      setEditForm((prev) => ({ ...prev, baseUrl: e.target.value }))
                    }
                    placeholder={
                      editingProvider === 'OLLAMA'
                        ? 'http://localhost:11434'
                        : 'https://api.example.com'
                    }
                    className="w-full px-3 py-2 bg-gray-900 border border-gray-700 rounded-lg text-white placeholder-gray-500 focus:outline-none focus:border-blue-500"
                  />
                </div>
              )}

              {/* Azure specific fields */}
              {editingProvider === 'AZURE_OPENAI' && (
                <>
                  <div>
                    <label className="block text-sm font-medium text-gray-300 mb-2">
                      Azure Endpoint
                    </label>
                    <input
                      type="text"
                      value={editForm.baseUrl || ''}
                      onChange={(e) =>
                        setEditForm((prev) => ({ ...prev, baseUrl: e.target.value }))
                      }
                      placeholder="https://your-resource.openai.azure.com"
                      className="w-full px-3 py-2 bg-gray-900 border border-gray-700 rounded-lg text-white placeholder-gray-500 focus:outline-none focus:border-blue-500"
                    />
                  </div>
                  <div>
                    <label className="block text-sm font-medium text-gray-300 mb-2">
                      Deployment Name
                    </label>
                    <input
                      type="text"
                      value={editForm.azureDeploymentName || ''}
                      onChange={(e) =>
                        setEditForm((prev) => ({
                          ...prev,
                          azureDeploymentName: e.target.value,
                        }))
                      }
                      className="w-full px-3 py-2 bg-gray-900 border border-gray-700 rounded-lg text-white placeholder-gray-500 focus:outline-none focus:border-blue-500"
                    />
                  </div>
                  <div>
                    <label className="block text-sm font-medium text-gray-300 mb-2">
                      API Version
                    </label>
                    <input
                      type="text"
                      value={editForm.azureApiVersion || ''}
                      onChange={(e) =>
                        setEditForm((prev) => ({
                          ...prev,
                          azureApiVersion: e.target.value,
                        }))
                      }
                      placeholder="2024-02-01"
                      className="w-full px-3 py-2 bg-gray-900 border border-gray-700 rounded-lg text-white placeholder-gray-500 focus:outline-none focus:border-blue-500"
                    />
                  </div>
                </>
              )}

              {/* Advanced Settings */}
              <div className="border-t border-gray-700 pt-4 mt-4">
                <h4 className="text-sm font-medium text-gray-300 mb-3">ê³ ê¸‰ ì„¤ì •</h4>
                <div className="grid grid-cols-2 gap-4">
                  <div>
                    <label className="block text-xs text-gray-400 mb-1">
                      ìš°ì„ ìˆœìœ„
                    </label>
                    <input
                      type="number"
                      min={1}
                      max={999}
                      value={editForm.priority}
                      onChange={(e) =>
                        setEditForm((prev) => ({
                          ...prev,
                          priority: parseInt(e.target.value) || 100,
                        }))
                      }
                      className="w-full px-3 py-2 bg-gray-900 border border-gray-700 rounded-lg text-white focus:outline-none focus:border-blue-500"
                    />
                  </div>
                  <div>
                    <label className="block text-xs text-gray-400 mb-1">
                      ìµœëŒ€ í† í°
                    </label>
                    <input
                      type="number"
                      min={1}
                      max={128000}
                      value={editForm.maxTokens}
                      onChange={(e) =>
                        setEditForm((prev) => ({
                          ...prev,
                          maxTokens: parseInt(e.target.value) || 4096,
                        }))
                      }
                      className="w-full px-3 py-2 bg-gray-900 border border-gray-700 rounded-lg text-white focus:outline-none focus:border-blue-500"
                    />
                  </div>
                  <div>
                    <label className="block text-xs text-gray-400 mb-1">
                      Temperature
                    </label>
                    <input
                      type="number"
                      min={0}
                      max={2}
                      step={0.1}
                      value={editForm.temperature}
                      onChange={(e) =>
                        setEditForm((prev) => ({
                          ...prev,
                          temperature: parseFloat(e.target.value) || 0.7,
                        }))
                      }
                      className="w-full px-3 py-2 bg-gray-900 border border-gray-700 rounded-lg text-white focus:outline-none focus:border-blue-500"
                    />
                  </div>
                  <div>
                    <label className="block text-xs text-gray-400 mb-1">
                      íƒ€ì„ì•„ì›ƒ (ms)
                    </label>
                    <input
                      type="number"
                      min={1000}
                      max={300000}
                      value={editForm.timeoutMs}
                      onChange={(e) =>
                        setEditForm((prev) => ({
                          ...prev,
                          timeoutMs: parseInt(e.target.value) || 60000,
                        }))
                      }
                      className="w-full px-3 py-2 bg-gray-900 border border-gray-700 rounded-lg text-white focus:outline-none focus:border-blue-500"
                    />
                  </div>
                </div>
              </div>

              {/* Enabled Toggle */}
              <div className="flex items-center justify-between pt-2">
                <span className="text-sm text-gray-300">í™œì„±í™”</span>
                <button
                  type="button"
                  onClick={() =>
                    setEditForm((prev) => ({ ...prev, enabled: !prev.enabled }))
                  }
                  className={clsx(
                    'relative w-11 h-6 rounded-full transition-colors',
                    editForm.enabled ? 'bg-blue-600' : 'bg-gray-600'
                  )}
                >
                  <span
                    className={clsx(
                      'absolute top-0.5 left-0.5 w-5 h-5 bg-white rounded-full transition-transform',
                      editForm.enabled && 'translate-x-5'
                    )}
                  />
                </button>
              </div>
            </div>

            {/* Dialog Actions */}
            <div className="flex justify-end gap-3 mt-6 pt-4 border-t border-gray-700">
              <button
                onClick={() => setEditDialogOpen(false)}
                className="px-4 py-2 text-gray-300 hover:text-white transition-colors"
              >
                ì·¨ì†Œ
              </button>
              <button
                onClick={handleSave}
                disabled={isSaving}
                className="flex items-center gap-2 px-4 py-2 bg-blue-600 hover:bg-blue-700 disabled:bg-blue-600/50 text-white rounded-lg transition-colors"
              >
                {isSaving ? (
                  <Loader2 className="w-4 h-4 animate-spin" />
                ) : (
                  <Save className="w-4 h-4" />
                )}
                ì €ì¥
              </button>
            </div>
          </div>
        </div>
      )}
    </div>
  );
}

```

---

## backend/admin-dashboard/web/src/pages/Login.tsx

```tsx
import { useState } from 'react';
import { useNavigate } from 'react-router-dom';
import { useAuth } from '../contexts/AuthContext';
import { Lock, User, AlertCircle } from 'lucide-react';

export default function Login() {
  const [username, setUsername] = useState('');
  const [password, setPassword] = useState('');
  const [error, setError] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const { login } = useAuth();
  const navigate = useNavigate();

  const handleSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    setError('');
    setIsLoading(true);

    try {
      await login(username, password);
      navigate('/');
    } catch (err) {
      setError('ë¡œê·¸ì¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì•„ì´ë””ì™€ ë¹„ë°€ë²ˆí˜¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.');
    } finally {
      setIsLoading(false);
    }
  };

  return (
    <div className="min-h-screen bg-gray-900 flex items-center justify-center p-4">
      <div className="w-full max-w-md">
        <div className="bg-gray-800 rounded-2xl shadow-xl p-8">
          <div className="text-center mb-8">
            <h1 className="text-2xl font-bold text-white mb-2">Admin Dashboard</h1>
            <p className="text-gray-400">NewsInsight ê´€ë¦¬ ì‹œìŠ¤í…œ</p>
          </div>

          <form onSubmit={handleSubmit} className="space-y-6">
            {error && (
              <div className="flex items-center gap-2 p-3 bg-red-500/10 border border-red-500/20 rounded-lg text-red-400 text-sm">
                <AlertCircle className="w-4 h-4 flex-shrink-0" />
                {error}
              </div>
            )}

            <div>
              <label className="block text-sm font-medium text-gray-300 mb-2">
                ì‚¬ìš©ì ì´ë¦„
              </label>
              <div className="relative">
                <User className="absolute left-3 top-1/2 -translate-y-1/2 w-5 h-5 text-gray-400" />
                <input
                  type="text"
                  value={username}
                  onChange={(e) => setUsername(e.target.value)}
                  className="w-full pl-10 pr-4 py-3 bg-gray-700 border border-gray-600 rounded-lg text-white placeholder-gray-400 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:border-transparent"
                  placeholder="admin"
                  required
                />
              </div>
            </div>

            <div>
              <label className="block text-sm font-medium text-gray-300 mb-2">
                ë¹„ë°€ë²ˆí˜¸
              </label>
              <div className="relative">
                <Lock className="absolute left-3 top-1/2 -translate-y-1/2 w-5 h-5 text-gray-400" />
                <input
                  type="password"
                  value={password}
                  onChange={(e) => setPassword(e.target.value)}
                  className="w-full pl-10 pr-4 py-3 bg-gray-700 border border-gray-600 rounded-lg text-white placeholder-gray-400 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:border-transparent"
                  placeholder="â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢"
                  required
                />
              </div>
            </div>

            <button
              type="submit"
              disabled={isLoading}
              className="w-full py-3 px-4 bg-blue-600 hover:bg-blue-700 disabled:bg-blue-600/50 text-white font-medium rounded-lg transition-colors focus:outline-none focus:ring-2 focus:ring-blue-500 focus:ring-offset-2 focus:ring-offset-gray-800"
            >
              {isLoading ? 'ë¡œê·¸ì¸ ì¤‘...' : 'ë¡œê·¸ì¸'}
            </button>
          </form>

          <p className="mt-6 text-center text-sm text-gray-500">
            ê¸°ë³¸ ê³„ì •: admin / admin123
          </p>
        </div>
      </div>
    </div>
  );
}

```

---

## backend/admin-dashboard/web/src/pages/Scripts.tsx

```tsx
import { useEffect, useState, useRef } from 'react';
import {
  Terminal,
  Play,
  AlertTriangle,
  Clock,
  CheckCircle,
  XCircle,
  ChevronDown,
  X,
} from 'lucide-react';
import { scriptsApi, environmentsApi } from '../api/endpoints';
import type { Script, Environment, TaskExecution } from '../types';
import { useAuth } from '../contexts/AuthContext';
import clsx from 'clsx';

export default function Scripts() {
  const [scripts, setScripts] = useState<Script[]>([]);
  const [environments, setEnvironments] = useState<Environment[]>([]);
  const [selectedScript, setSelectedScript] = useState<Script | null>(null);
  const [selectedEnvId, setSelectedEnvId] = useState<string>('');
  const [parameters, setParameters] = useState<Record<string, string>>({});
  const [isExecuting, setIsExecuting] = useState(false);
  const [output, setOutput] = useState<string>('');
  const [executions, setExecutions] = useState<TaskExecution[]>([]);
  const [isLoading, setIsLoading] = useState(true);
  const outputRef = useRef<HTMLPreElement>(null);
  const { user } = useAuth();

  useEffect(() => {
    loadData();
  }, []);

  useEffect(() => {
    if (outputRef.current) {
      outputRef.current.scrollTop = outputRef.current.scrollHeight;
    }
  }, [output]);

  const loadData = async () => {
    try {
      const [scriptsData, envsData, executionsData] = await Promise.all([
        scriptsApi.list(),
        environmentsApi.list(true),
        scriptsApi.listExecutions(),
      ]);
      setScripts(scriptsData);
      setEnvironments(envsData);
      setExecutions(executionsData);
      
      if (envsData.length > 0) {
        setSelectedEnvId(envsData[0].id);
      }
    } catch (error) {
      console.error('Failed to load data:', error);
    } finally {
      setIsLoading(false);
    }
  };

  const getRiskColor = (risk: string) => {
    switch (risk) {
      case 'critical':
        return 'bg-red-500/10 text-red-400 border-red-500/20';
      case 'high':
        return 'bg-orange-500/10 text-orange-400 border-orange-500/20';
      case 'medium':
        return 'bg-yellow-500/10 text-yellow-400 border-yellow-500/20';
      default:
        return 'bg-green-500/10 text-green-400 border-green-500/20';
    }
  };

  const canExecute = (script: Script) => {
    const roleLevel = { viewer: 0, operator: 1, admin: 2 };
    const userLevel = roleLevel[user?.role || 'viewer'];
    const requiredLevel = roleLevel[script.required_role];
    return userLevel >= requiredLevel;
  };

  const handleExecute = async () => {
    if (!selectedScript || !selectedEnvId) return;

    setIsExecuting(true);
    setOutput('');

    try {
      await scriptsApi.executeStream(
        selectedScript.id,
        selectedEnvId,
        parameters,
        (chunk) => {
          setOutput((prev) => prev + chunk);
        }
      );
      
      // Reload executions
      const executionsData = await scriptsApi.listExecutions();
      setExecutions(executionsData);
    } catch (error) {
      setOutput((prev) => prev + `\nError: ${error instanceof Error ? error.message : 'Unknown error'}`);
    } finally {
      setIsExecuting(false);
    }
  };

  const selectScript = (script: Script) => {
    setSelectedScript(script);
    setOutput('');
    
    // Initialize parameters with defaults
    const defaultParams: Record<string, string> = {};
    script.parameters.forEach((param) => {
      if (param.default !== undefined) {
        defaultParams[param.name] = String(param.default);
      }
    });
    setParameters(defaultParams);
  };

  const getStatusIcon = (status: string) => {
    switch (status) {
      case 'success':
        return <CheckCircle className="w-4 h-4 text-green-400" />;
      case 'failed':
        return <XCircle className="w-4 h-4 text-red-400" />;
      case 'running':
        return <Clock className="w-4 h-4 text-blue-400 animate-spin" />;
      default:
        return <AlertTriangle className="w-4 h-4 text-yellow-400" />;
    }
  };

  if (isLoading) {
    return (
      <div className="flex items-center justify-center h-64">
        <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-blue-500" />
      </div>
    );
  }

  return (
    <div className="space-y-6">
      <h1 className="text-2xl font-bold text-white">ìŠ¤í¬ë¦½íŠ¸ ê´€ë¦¬</h1>

      <div className="grid grid-cols-1 lg:grid-cols-3 gap-6">
        {/* Script List */}
        <div className="lg:col-span-1 space-y-2">
          {scripts.map((script) => (
            <button
              key={script.id}
              onClick={() => selectScript(script)}
              className={clsx(
                'w-full p-4 rounded-lg border transition-colors text-left',
                selectedScript?.id === script.id
                  ? 'bg-blue-600/20 border-blue-500'
                  : 'bg-gray-800 border-gray-700 hover:border-gray-600'
              )}
            >
              <div className="flex items-start justify-between mb-2">
                <div className="flex items-center gap-2">
                  <Terminal className="w-4 h-4 text-gray-400" />
                  <span className="font-medium text-white">{script.name}</span>
                </div>
                <span className={clsx(
                  'px-2 py-0.5 rounded text-xs border capitalize',
                  getRiskColor(script.risk_level)
                )}>
                  {script.risk_level}
                </span>
              </div>
              <p className="text-sm text-gray-400 line-clamp-2">
                {script.description}
              </p>
              <div className="mt-2 flex flex-wrap gap-1">
                {script.tags.slice(0, 3).map((tag) => (
                  <span
                    key={tag}
                    className="px-2 py-0.5 bg-gray-700 rounded text-xs text-gray-300"
                  >
                    {tag}
                  </span>
                ))}
              </div>
            </button>
          ))}
        </div>

        {/* Script Details & Execution */}
        <div className="lg:col-span-2 space-y-6">
          {selectedScript ? (
            <>
              {/* Script Info */}
              <div className="bg-gray-800 rounded-xl border border-gray-700 p-6">
                <div className="flex items-start justify-between mb-4">
                  <div>
                    <h2 className="text-lg font-semibold text-white">
                      {selectedScript.name}
                    </h2>
                    <p className="text-gray-400 text-sm mt-1">
                      {selectedScript.description}
                    </p>
                  </div>
                  <button
                    onClick={() => setSelectedScript(null)}
                    className="text-gray-400 hover:text-white"
                  >
                    <X className="w-5 h-5" />
                  </button>
                </div>

                <div className="grid grid-cols-2 gap-4 mb-6 text-sm">
                  <div>
                    <span className="text-gray-500">í•„ìš” ê¶Œí•œ:</span>
                    <span className="ml-2 text-white capitalize">
                      {selectedScript.required_role}
                    </span>
                  </div>
                  <div>
                    <span className="text-gray-500">ì˜ˆìƒ ì‹œê°„:</span>
                    <span className="ml-2 text-white">
                      {selectedScript.estimated_duration
                        ? `${Math.ceil(selectedScript.estimated_duration / 60)}ë¶„`
                        : '-'}
                    </span>
                  </div>
                </div>

                {/* Environment Selection */}
                <div className="mb-4">
                  <label className="block text-sm font-medium text-gray-300 mb-2">
                    ì‹¤í–‰ í™˜ê²½
                  </label>
                  <div className="relative">
                    <select
                      value={selectedEnvId}
                      onChange={(e) => setSelectedEnvId(e.target.value)}
                      className="w-full px-4 py-2 bg-gray-700 border border-gray-600 rounded-lg text-white appearance-none focus:outline-none focus:ring-2 focus:ring-blue-500"
                    >
                      {environments.map((env) => (
                        <option key={env.id} value={env.id}>
                          {env.name} ({env.env_type})
                        </option>
                      ))}
                    </select>
                    <ChevronDown className="absolute right-3 top-1/2 -translate-y-1/2 w-4 h-4 text-gray-400 pointer-events-none" />
                  </div>
                </div>

                {/* Parameters */}
                {selectedScript.parameters.length > 0 && (
                  <div className="mb-4 space-y-3">
                    <label className="block text-sm font-medium text-gray-300">
                      íŒŒë¼ë¯¸í„°
                    </label>
                    {selectedScript.parameters.map((param) => (
                      <div key={param.name}>
                        <label className="block text-xs text-gray-400 mb-1">
                          {param.name}
                          {param.required && <span className="text-red-400 ml-1">*</span>}
                          {param.description && (
                            <span className="ml-2 text-gray-500">- {param.description}</span>
                          )}
                        </label>
                        {param.param_type === 'boolean' ? (
                          <label className="flex items-center gap-2">
                            <input
                              type="checkbox"
                              checked={parameters[param.name] === 'true'}
                              onChange={(e) =>
                                setParameters({
                                  ...parameters,
                                  [param.name]: e.target.checked ? 'true' : 'false',
                                })
                              }
                              className="w-4 h-4 rounded bg-gray-700 border-gray-600"
                            />
                            <span className="text-sm text-gray-300">í™œì„±í™”</span>
                          </label>
                        ) : (
                          <input
                            type={param.param_type === 'number' ? 'number' : 'text'}
                            value={parameters[param.name] || ''}
                            onChange={(e) =>
                              setParameters({
                                ...parameters,
                                [param.name]: e.target.value,
                              })
                            }
                            placeholder={String(param.default || '')}
                            className="w-full px-3 py-2 bg-gray-700 border border-gray-600 rounded-lg text-white text-sm focus:outline-none focus:ring-2 focus:ring-blue-500"
                          />
                        )}
                      </div>
                    ))}
                  </div>
                )}

                {/* Execute Button */}
                <button
                  onClick={handleExecute}
                  disabled={!canExecute(selectedScript) || isExecuting || !selectedEnvId}
                  className={clsx(
                    'flex items-center justify-center gap-2 w-full py-3 rounded-lg font-medium transition-colors',
                    canExecute(selectedScript)
                      ? 'bg-blue-600 hover:bg-blue-700 text-white'
                      : 'bg-gray-600 text-gray-400 cursor-not-allowed'
                  )}
                >
                  {isExecuting ? (
                    <>
                      <Clock className="w-4 h-4 animate-spin" />
                      ì‹¤í–‰ ì¤‘...
                    </>
                  ) : (
                    <>
                      <Play className="w-4 h-4" />
                      ì‹¤í–‰
                    </>
                  )}
                </button>

                {!canExecute(selectedScript) && (
                  <p className="mt-2 text-sm text-yellow-400 text-center">
                    ì´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ {selectedScript.required_role} ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤
                  </p>
                )}
              </div>

              {/* Output */}
              {output && (
                <div className="bg-gray-800 rounded-xl border border-gray-700">
                  <div className="flex items-center justify-between p-4 border-b border-gray-700">
                    <h3 className="font-semibold text-white">ì‹¤í–‰ ì¶œë ¥</h3>
                    <button
                      onClick={() => setOutput('')}
                      className="text-gray-400 hover:text-white"
                    >
                      ì§€ìš°ê¸°
                    </button>
                  </div>
                  <pre
                    ref={outputRef}
                    className="p-4 text-sm text-gray-300 overflow-auto max-h-96 font-mono"
                  >
                    {output}
                  </pre>
                </div>
              )}
            </>
          ) : (
            <div className="bg-gray-800 rounded-xl border border-gray-700 p-8 text-center text-gray-500">
              ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”
            </div>
          )}

          {/* Recent Executions */}
          <div className="bg-gray-800 rounded-xl border border-gray-700">
            <div className="p-4 border-b border-gray-700">
              <h3 className="font-semibold text-white">ìµœê·¼ ì‹¤í–‰ ì´ë ¥</h3>
            </div>
            <div className="divide-y divide-gray-700 max-h-64 overflow-y-auto">
              {executions.length === 0 ? (
                <div className="p-8 text-center text-gray-500">
                  ì‹¤í–‰ ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤
                </div>
              ) : (
                executions.slice(0, 10).map((execution) => (
                  <div
                    key={execution.id}
                    className="flex items-center justify-between p-4"
                  >
                    <div className="flex items-center gap-3">
                      {getStatusIcon(execution.status)}
                      <div>
                        <p className="text-sm font-medium text-white">
                          {execution.script_name}
                        </p>
                        <p className="text-xs text-gray-400">
                          {execution.environment_name} â€¢ {execution.executed_by}
                        </p>
                      </div>
                    </div>
                    <div className="text-right">
                      <p className="text-xs text-gray-500">
                        {new Date(execution.started_at).toLocaleString('ko-KR')}
                      </p>
                    </div>
                  </div>
                ))
              )}
            </div>
          </div>
        </div>
      </div>
    </div>
  );
}

```

---

## backend/admin-dashboard/web/src/types/index.ts

```ts
// Admin Dashboard Types

export type EnvironmentType = 'zerotrust' | 'local' | 'gcp' | 'aws' | 'production' | 'staging';
export type RiskLevel = 'low' | 'medium' | 'high' | 'critical';
export type TaskStatus = 'pending' | 'running' | 'success' | 'failed' | 'cancelled';
export type UserRole = 'viewer' | 'operator' | 'admin';
export type ServiceStatus = 'up' | 'down' | 'starting' | 'stopping' | 'unknown';
export type DocumentCategory = 'deployment' | 'troubleshooting' | 'architecture' | 'runbook' | 'general';
export type AuditAction = 'login' | 'logout' | 'view' | 'create' | 'update' | 'delete' | 'execute' | 'deploy' | 'rollback';

export interface Environment {
  id: string;
  name: string;
  env_type: EnvironmentType;
  description?: string;
  compose_file: string;
  env_file?: string;
  is_active: boolean;
  priority: number;
  created_at: string;
  updated_at: string;
}

export interface ContainerInfo {
  name: string;
  image: string;
  status: ServiceStatus;
  health?: string;
  ports: string[];
  created_at?: string;
  started_at?: string;
}

export interface EnvironmentStatus {
  environment_id: string;
  environment_name: string;
  containers: ContainerInfo[];
  total_containers: number;
  running_containers: number;
  last_deployment?: string;
  deployed_by?: string;
}

export interface ScriptParameter {
  name: string;
  param_type: string;
  required: boolean;
  default?: unknown;
  description?: string;
}

export interface Script {
  id: string;
  name: string;
  description?: string;
  command: string;
  working_dir?: string;
  risk_level: RiskLevel;
  estimated_duration?: number;
  allowed_environments: string[];
  required_role: UserRole;
  parameters: ScriptParameter[];
  pre_hooks: string[];
  post_hooks: string[];
  tags: string[];
  created_at: string;
  updated_at: string;
}

export interface TaskExecution {
  id: string;
  script_id: string;
  script_name: string;
  environment_id: string;
  environment_name: string;
  status: TaskStatus;
  parameters: Record<string, unknown>;
  started_at: string;
  finished_at?: string;
  executed_by: string;
  exit_code?: number;
  error_message?: string;
}

export interface Document {
  id: string;
  title: string;
  file_path: string;
  category: DocumentCategory;
  tags: string[];
  related_environments: string[];
  related_scripts: string[];
  content?: string;
  last_modified: string;
}

export interface AuditLog {
  id: string;
  user_id: string;
  username: string;
  action: AuditAction;
  resource_type: string;
  resource_id?: string;
  resource_name?: string;
  environment_id?: string;
  environment_name?: string;
  details: Record<string, unknown>;
  ip_address?: string;
  user_agent?: string;
  timestamp: string;
  success: boolean;
  error_message?: string;
}

export interface User {
  id: string;
  username: string;
  email?: string;
  role: UserRole;
  is_active: boolean;
  created_at: string;
  last_login?: string;
}

export interface Token {
  access_token: string;
  token_type: string;
  expires_in: number;
}

export interface HealthCheck {
  status: string;
  version: string;
  timestamp: string;
}

// Service Health Monitoring Types
export type ServiceHealthStatus = 'healthy' | 'unhealthy' | 'degraded' | 'unreachable' | 'unknown';

export interface ServiceHealth {
  service_id: string;
  name: string;
  status: ServiceHealthStatus;
  message?: string;
  response_time_ms?: number;
  url?: string;
  checked_at: string;
  details?: Record<string, unknown>;
}

export interface InfrastructureHealth {
  service_id: string;
  name: string;
  status: ServiceHealthStatus;
  message?: string;
  port?: number;
  checked_at: string;
  details?: Record<string, unknown>;
}

export interface OverallSystemHealth {
  status: ServiceHealthStatus;
  total_services: number;
  healthy_services: number;
  unhealthy_services: number;
  degraded_services: number;
  total_infrastructure: number;
  healthy_infrastructure: number;
  average_response_time_ms?: number;
  services: ServiceHealth[];
  infrastructure: InfrastructureHealth[];
  checked_at: string;
}

export interface ServiceInfo {
  id: string;
  name: string;
  description?: string;
  port?: number;
  healthcheck: string;
  hostname: string;
  type: string;
  tags: string[];
}

// Data Source Types
export type DataSourceType = 'rss' | 'web' | 'api' | 'social';
export type DataSourceStatus = 'active' | 'inactive' | 'error' | 'testing';

export interface DataSource {
  id: string;
  name: string;
  source_type: DataSourceType;
  url: string;
  description?: string;
  category?: string;
  language: string;
  is_active: boolean;
  crawl_interval_minutes: number;
  priority: number;
  config: Record<string, unknown>;
  status: DataSourceStatus;
  last_crawled_at?: string;
  total_articles: number;
  success_rate: number;
  created_at: string;
  updated_at: string;
}

export interface DataSourceTestResult {
  source_id: string;
  success: boolean;
  message: string;
  response_time_ms?: number;
  sample_data?: Record<string, unknown>;
  tested_at: string;
}

// Database Types
export type DatabaseType = 'postgresql' | 'mongodb' | 'redis';

export interface DatabaseInfo {
  db_type: DatabaseType;
  name: string;
  host: string;
  port: number;
  status: ServiceHealthStatus;
  version?: string;
  size_bytes?: number;
  size_human?: string;
  connection_count?: number;
  max_connections?: number;
  uptime_seconds?: number;
  checked_at: string;
}

export interface PostgresTableInfo {
  schema_name: string;
  table_name: string;
  row_count: number;
  size_bytes: number;
  size_human: string;
  index_size_bytes?: number;
  last_vacuum?: string;
  last_analyze?: string;
}

export interface PostgresDatabaseStats {
  database_name: string;
  size_bytes: number;
  size_human: string;
  tables: PostgresTableInfo[];
  total_tables: number;
  total_rows: number;
  connection_count: number;
  max_connections: number;
  checked_at: string;
}

export interface MongoCollectionInfo {
  collection_name: string;
  document_count: number;
  size_bytes: number;
  size_human: string;
  avg_document_size_bytes?: number;
  index_count: number;
  total_index_size_bytes?: number;
}

export interface MongoDatabaseStats {
  database_name: string;
  size_bytes: number;
  size_human: string;
  collections: MongoCollectionInfo[];
  total_collections: number;
  total_documents: number;
  checked_at: string;
}

export interface RedisStats {
  used_memory_bytes: number;
  used_memory_human: string;
  max_memory_bytes?: number;
  connected_clients: number;
  total_keys: number;
  expired_keys: number;
  keyspace_hits: number;
  keyspace_misses: number;
  hit_rate: number;
  uptime_seconds: number;
  checked_at: string;
}

// Kafka Types
export interface KafkaTopicInfo {
  name: string;
  partition_count: number;
  replication_factor: number;
  message_count?: number;
  size_bytes?: number;
  retention_ms?: number;
  is_internal: boolean;
}

export interface KafkaConsumerGroupInfo {
  group_id: string;
  state: string;
  members_count: number;
  topics: string[];
  total_lag: number;
  lag_per_partition: Record<string, number>;
}

export interface KafkaClusterInfo {
  broker_count: number;
  controller_id?: number;
  cluster_id?: string;
  topics: KafkaTopicInfo[];
  consumer_groups: KafkaConsumerGroupInfo[];
  total_topics: number;
  total_partitions: number;
  total_messages?: number;
  checked_at: string;
}

```

---

## backend/admin-dashboard/web/tsconfig.app.json

```json
{
  "compilerOptions": {
    "tsBuildInfoFile": "./node_modules/.tmp/tsconfig.app.tsbuildinfo",
    "target": "ES2022",
    "useDefineForClassFields": true,
    "lib": ["ES2022", "DOM", "DOM.Iterable"],
    "module": "ESNext",
    "types": ["vite/client"],
    "skipLibCheck": true,

    /* Bundler mode */
    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "verbatimModuleSyntax": true,
    "moduleDetection": "force",
    "noEmit": true,
    "jsx": "react-jsx",

    /* Linting */
    "strict": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "erasableSyntaxOnly": true,
    "noFallthroughCasesInSwitch": true,
    "noUncheckedSideEffectImports": true
  },
  "include": ["src"]
}

```

---

## backend/admin-dashboard/web/tsconfig.json

```json
{
  "files": [],
  "references": [
    { "path": "./tsconfig.app.json" },
    { "path": "./tsconfig.node.json" }
  ]
}

```

---

## backend/admin-dashboard/web/tsconfig.node.json

```json
{
  "compilerOptions": {
    "tsBuildInfoFile": "./node_modules/.tmp/tsconfig.node.tsbuildinfo",
    "target": "ES2023",
    "lib": ["ES2023"],
    "module": "ESNext",
    "types": ["node"],
    "skipLibCheck": true,

    /* Bundler mode */
    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "verbatimModuleSyntax": true,
    "moduleDetection": "force",
    "noEmit": true,

    /* Linting */
    "strict": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "erasableSyntaxOnly": true,
    "noFallthroughCasesInSwitch": true,
    "noUncheckedSideEffectImports": true
  },
  "include": ["vite.config.ts"]
}

```

---

## backend/admin-dashboard/web/vite.config.ts

```ts
import { defineConfig } from 'vite'
import react from '@vitejs/plugin-react'

// https://vite.dev/config/
export default defineConfig({
  plugins: [react()],
  server: {
    port: 3001,
    proxy: {
      '/api': {
        target: 'http://localhost:8888',
        changeOrigin: true,
      },
    },
  },
  build: {
    outDir: 'dist',
    sourcemap: true,
  },
})

```

---

## backend/api-gateway-service/bin/main/application.yml

```yml
spring:
  application:
    name: api-gateway
  
  config:
    # Consulì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸° (optional - Consul ì—†ì–´ë„ ì‹œì‘ ê°€ëŠ¥)
    import: ${SPRING_CONFIG_IMPORT:optional:consul:}
  
  cloud:
    consul:
      enabled: ${CONSUL_ENABLED:true}
      host: ${CONSUL_HOST:localhost}
      port: ${CONSUL_PORT:8500}
      config:
        enabled: ${CONSUL_CONFIG_ENABLED:true}
        # ê¸°ì¡´ Consul KV êµ¬ì¡°ì™€ ì¼ì¹˜
        prefix: config
        default-context: ${spring.application.name}
        format: PROPERTIES
        # Fail-Fast ë¹„í™œì„±í™”: Consul ì—°ê²° ì‹¤íŒ¨í•´ë„ ì‹œì‘
        fail-fast: ${CONSUL_FAIL_FAST:false}
      discovery:
        enabled: ${CONSUL_DISCOVERY_ENABLED:true}
        health-check-path: /actuator/health
        health-check-interval: 10s
        instance-id: ${spring.application.name}:${random.value}
        prefer-ip-address: true
        # ì„œë¹„ìŠ¤ ë“±ë¡ ì‹¤íŒ¨í•´ë„ ì‹œì‘
        fail-fast: false
    inetutils:
      preferred-networks:
        - 172.20

    gateway:
      routes:
        # ===========================================
        # Collector Service Routes
        # COLLECTOR_SERVICE_URL í™˜ê²½ë³€ìˆ˜ë¡œ ì§ì ‘ URL ì§€ì • ê°€ëŠ¥
        # Consul ì‚¬ìš© ì‹œ: lb://collector-service
        # Consul ë¯¸ì‚¬ìš© ì‹œ: http://collector-service:8081 (ê¸°ë³¸ê°’)
        # ===========================================
        
        # Collector ì„œë¹„ìŠ¤ - ë°ì´í„° API (DataController)
        - id: collector-data
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/data/**
          # NOTE: Redis RateLimiter ë¹„í™œì„±í™” - Redis ì—†ì´ë„ ë™ì‘í•˜ë„ë¡ ì„¤ì •
          # Redis ë„ì… ì‹œ ì•„ë˜ ì£¼ì„ í•´ì œ
          # filters:
          #   - name: RequestRateLimiter
          #     args:
          #       redis-rate-limiter.replenishRate: 100
          #       redis-rate-limiter.burstCapacity: 150

        # Collector ì„œë¹„ìŠ¤ - ì†ŒìŠ¤ ê´€ë¦¬ (SourceController)
        - id: collector-sources
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/sources/**

        # Collector ì„œë¹„ìŠ¤ - ìˆ˜ì§‘ ì‘ì—… (CollectionController)
        - id: collector-collections
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/collections/**

        # Articles & Analysis - í”„ë¡ íŠ¸ì—”ë“œ API
        - id: articles-root
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/articles

        - id: articles-alias
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/articles/**

        - id: analysis-root
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/analysis

        - id: analysis-alias
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/analysis/**
          metadata:
            response-timeout: 300000

        # Search API - í†µí•© ê²€ìƒ‰ (SSE ìŠ¤íŠ¸ë¦¬ë°)
        # NOTE: SSE ìŠ¤íŠ¸ë¦¬ë°ì€ RateLimiterì™€ í˜¸í™˜ì„± ë¬¸ì œê°€ ìˆì–´ ë¹„í™œì„±í™”
        - id: search-api
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/search/**

        # Search History API - ê²€ìƒ‰ ê¸°ë¡ ê´€ë¦¬
        - id: search-history
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/search-history/**
          metadata:
            # SSE ìŠ¤íŠ¸ë¦¼ ì—°ê²°ì„ ìœ„í•œ ê¸´ íƒ€ì„ì•„ì›ƒ (5ë¶„)
            response-timeout: 300000

        # Search Template API - SmartSearch ê²€ìƒ‰ í…œí”Œë¦¿ ê´€ë¦¬
        - id: search-templates
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/search-templates/**

        # Search Jobs API - ê²€ìƒ‰ ì‘ì—… ê´€ë¦¬ (SearchJobController)
        # SSE ìŠ¤íŠ¸ë¦¬ë° ì§€ì›: /api/v1/jobs/stream, /api/v1/jobs/{jobId}/stream
        - id: search-jobs
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/jobs,/api/v1/jobs/**
          metadata:
            # SSE ìŠ¤íŠ¸ë¦¼ ì—°ê²°ì„ ìœ„í•œ ê¸´ íƒ€ì„ì•„ì›ƒ (5ë¶„)
            response-timeout: 300000

        # AutoCrawl API - ìë™ í¬ë¡¤ë§ ê´€ë¦¬ (AutoCrawlController)
        - id: autocrawl
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/autocrawl/**

        # Config API - í”„ë¡ íŠ¸ì—”ë“œ ì„¤ì •
        # NOTE: Gateway ìì²´ FrontendConfigControllerê°€ ìš°ì„  ì²˜ë¦¬ë¨
        # ì´ ë¼ìš°íŠ¸ëŠ” fallbackìœ¼ë¡œ ìœ ì§€ (collector-serviceì—ë„ ConfigControllerê°€ ìˆì„ ê²½ìš°)
        - id: config-api
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/config/**

        # AI Orchestration - AI ë¶„ì„ ìš”ì²­ (collector-service AiOrchestrationController)
        - id: collector-ai
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/ai/**

        - id: collector-ml
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/ml/**

        # Dashboard Events - SSE ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ (collector-service DashboardEventsController)
        - id: collector-events
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/events/**
          metadata:
            # SSE ì—°ê²°ì„ ìœ„í•œ ê¸´ íƒ€ì„ì•„ì›ƒ
            response-timeout: 300000

        # Admin Dashboard API
        # NOTE: Using direct URL fallback if not found in Consul, assuming port 8888 from main.py
        - id: admin-dashboard-api
          uri: ${ADMIN_DASHBOARD_URL:http://localhost:8888}
          predicates:
            - Path=/api/v1/admin/**
          # No StripPrefix needed because the admin API uses /api/v1/admin prefix

        # Autonomous Crawler - í¬ë¡¤ë§ ì‘ì—… ê´€ë¦¬
        - id: autonomous-crawler-api
          uri: ${AUTONOMOUS_CRAWLER_API_URL:${AUTONOMOUS_CRAWLER_URL:http://autonomous-crawler:8030}}
          predicates:
            - Path=/api/v1/crawler/**
          filters:
            # /api/v1/crawler/** -> /** ë¡œ ì „ë‹¬ (3 segments: api, v1, crawler)
            - StripPrefix=3

        # Browser-Use API - AI ë¸Œë¼ìš°ì € ìë™í™”
        - id: browser-use-browse
          uri: ${BROWSER_USE_URL:http://localhost:8500}
          predicates:
            - Path=/api/browser-use/browse/**
          filters:
            # /api/browser-use/browse/** -> /browse/** ë¡œ ì „ë‹¬
            - StripPrefix=2

        - id: browser-use-jobs
          uri: ${BROWSER_USE_URL:http://localhost:8500}
          predicates:
            - Path=/api/browser-use/jobs/**
          filters:
            # /api/browser-use/jobs/** -> /jobs/** ë¡œ ì „ë‹¬
            - StripPrefix=2

        - id: browser-use-health
          uri: ${BROWSER_USE_URL:http://localhost:8500}
          predicates:
            - Path=/api/browser-use/health
          filters:
            # /api/browser-use/health -> /health ë¡œ ì „ë‹¬
            - StripPrefix=2

        # Browser-Use WebSocket
        - id: browser-use-websocket
          uri: ${BROWSER_USE_WS_URL:ws://localhost:8500}
          predicates:
            - Path=/api/browser-use/ws/**
          filters:
            # /api/browser-use/ws/** -> /ws/** ë¡œ ì „ë‹¬
            - StripPrefix=2

        # ML Add-ons - ê°ì • ë¶„ì„ (Sentiment Analysis)
        - id: ml-addon-sentiment
          uri: ${ML_ADDON_SENTIMENT_URL:http://sentiment-addon:8100}
          predicates:
            - Path=/api/ml-addons/sentiment/**
          filters:
            # /api/ml-addons/sentiment/** -> /** ë¡œ ì „ë‹¬
            - StripPrefix=3

        # Fact Check Chat - íŒ©íŠ¸ì²´í¬ ì±—ë´‡
        - id: factcheck-chat
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/factcheck-chat/**
          filters:
            - StripPrefix=0

        # ML Add-ons - íŒ©íŠ¸ì²´í¬ (Fact Check)
        - id: ml-addon-factcheck
          uri: ${ML_ADDON_FACTCHECK_URL:http://factcheck-addon:8101}
          predicates:
            - Path=/api/ml-addons/factcheck/**
          filters:
            # /api/ml-addons/factcheck/** -> /** ë¡œ ì „ë‹¬
            - StripPrefix=3

        # ML Add-ons - í¸í–¥ë„ ë¶„ì„ (Bias Analysis)
        - id: ml-addon-bias
          uri: ${ML_ADDON_BIAS_URL:http://bias-addon:8102}
          predicates:
            - Path=/api/ml-addons/bias/**
          filters:
            # /api/ml-addons/bias/** -> /** ë¡œ ì „ë‹¬
            - StripPrefix=3

        - id: ml-addon-bot-detector
          uri: ${ML_ADDON_BOT_DETECTOR_URL:http://bot-detector:8041}
          predicates:
            - Path=/api/ml-addons/bot-detector/**
          filters:
            - StripPrefix=3

        # ML Trainer Service - ëª¨ë¸ í•™ìŠµ ì„œë¹„ìŠ¤
        - id: ml-trainer
          uri: ${ML_TRAINER_URL:http://ml-trainer:8090}
          predicates:
            - Path=/api/ml-trainer/**
          filters:
            # /api/ml-trainer/** -> /** ë¡œ ì „ë‹¬
            - StripPrefix=2
          metadata:
            # í•™ìŠµ ì‘ì—…ì€ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê¸´ íƒ€ì„ì•„ì›ƒ
            response-timeout: 600000

        # ML Trainer SSE Stream - ì‹¤ì‹œê°„ í•™ìŠµ ì§„í–‰ ìƒí™©
        - id: ml-trainer-stream
          uri: ${ML_TRAINER_URL:http://ml-trainer:8090}
          predicates:
            - Path=/api/ml-trainer/jobs/*/stream
          filters:
            - StripPrefix=2
          metadata:
            # SSE ìŠ¤íŠ¸ë¦¼ì„ ìœ„í•œ ë§¤ìš° ê¸´ íƒ€ì„ì•„ì›ƒ (30ë¶„)
            response-timeout: 1800000

        # Health Check Rewrite - í”„ë¡ íŠ¸ì—”ë“œê°€ /api/actuator/health í˜¸ì¶œ ì‹œ ì²˜ë¦¬
        - id: gateway-health
          uri: http://localhost:${PORT:8000}
          predicates:
            - Path=/api/actuator/health,/api/actuator/health/**
          filters:
            # /api/actuator/health -> /actuator/health ë¡œ ì¬ì‘ì„±
            - RewritePath=/api/actuator/(?<segment>.*), /actuator/${segment}

      # ì „ì—­ CORS ì„¤ì •
      globalcors:
        cors-configurations:
          '[/**]':
            allowedOriginPatterns: "*"
            allowedMethods:
              - GET
              - POST
              - PUT
              - DELETE
              - PATCH
              - OPTIONS
            allowedHeaders: "*"
            allowCredentials: true
            maxAge: 3600
  
  # Redis ì„¤ì • (Rate Limiting ë° ì„¸ì…˜ ê´€ë¦¬ìš©)
  data:
    redis:
      host: ${REDIS_HOST:redis}
      port: ${REDIS_PORT:6379}
      password: ${REDIS_PASSWORD:}

server:
  port: ${PORT:8000}
  # Netty ì„¤ì • - SSE/WebSocket ì—°ê²°ì„ ìœ„í•œ íƒ€ì„ì•„ì›ƒ ì¦ê°€
  netty:
    # ì—°ê²° ìœ ì§€ ì‹œê°„ (5ë¶„)
    idle-timeout: 300000

# Spring Cloud Gateway HTTP Client ì„¤ì •
spring.cloud.gateway.httpclient:
  connect-timeout: 10000
  response-timeout: 300000
  pool:
    max-idle-time: 300000

management:
  endpoints:
    web:
      exposure:
        include: health,info,env,metrics,prometheus
  endpoint:
    health:
      show-details: always

logging:
  level:
    root: INFO
    org.springframework.cloud.gateway: DEBUG
    org.springframework.cloud.gateway.handler.RoutePredicateHandlerMapping: TRACE
    org.springframework.security: DEBUG
    com.newsinsight.gateway: DEBUG

```

---

## backend/api-gateway-service/build.gradle.kts

```kts
// API Gateway ëª¨ë“ˆ ë¹Œë“œ ì„¤ì •

plugins {
    java
    id("org.springframework.boot")
    id("io.spring.dependency-management")
}

dependencies {
    // Spring Cloud Gateway
    implementation("org.springframework.cloud:spring-cloud-starter-gateway")
    
    // Security (JWT ì¸ì¦)
    implementation("org.springframework.boot:spring-boot-starter-security")
    implementation("io.jsonwebtoken:jjwt-api:0.12.3")
    runtimeOnly("io.jsonwebtoken:jjwt-impl:0.12.3")
    runtimeOnly("io.jsonwebtoken:jjwt-jackson:0.12.3")
    
    // Redis for Rate Limiting
    implementation("org.springframework.boot:spring-boot-starter-data-redis-reactive")
    
    // WebFlux (GatewayëŠ” Reactive ìŠ¤íƒ)
    implementation("org.springframework.boot:spring-boot-starter-webflux")
    
    // Test
    testImplementation("org.springframework.security:spring-security-test")
    testImplementation("io.projectreactor:reactor-test")
}

tasks.named<org.springframework.boot.gradle.tasks.bundling.BootJar>("bootJar") {
    archiveBaseName.set("api-gateway")
    archiveVersion.set("1.0.0")
}

```

---

## backend/api-gateway-service/src/main/java/com/newsinsight/gateway/GatewayApplication.java

```java
package com.newsinsight.gateway;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.client.discovery.EnableDiscoveryClient;

/**
 * NewsInsight API Gateway Application
 * 
 * Spring Cloud Gateway ê¸°ë°˜ì˜ API Gateway ì„œë¹„ìŠ¤
 * - JWT ì¸ì¦/ì¸ê°€
 * - RBAC (Role-Based Access Control)
 * - Rate Limiting (Redis ê¸°ë°˜)
 * - Service Discovery (Consul)
 * - Dynamic Configuration (Consul KV)
 */
@SpringBootApplication
@EnableDiscoveryClient
public class GatewayApplication {

    public static void main(String[] args) {
        SpringApplication.run(GatewayApplication.class, args);
    }
}

```

---

## backend/api-gateway-service/src/main/java/com/newsinsight/gateway/config/SecurityConfig.java

```java
package com.newsinsight.gateway.config;

import org.springframework.beans.factory.annotation.Value;
import org.springframework.cloud.gateway.filter.ratelimit.KeyResolver;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.http.HttpMethod;
import org.springframework.security.config.annotation.web.reactive.EnableWebFluxSecurity;
import org.springframework.security.config.web.server.ServerHttpSecurity;
import org.springframework.security.web.server.SecurityWebFilterChain;
import org.springframework.web.cors.CorsConfiguration;
import org.springframework.web.cors.reactive.CorsConfigurationSource;
import org.springframework.web.cors.reactive.UrlBasedCorsConfigurationSource;
import reactor.core.publisher.Mono;

import java.util.Arrays;
import java.util.List;

/**
 * API Gateway Security Configuration
 * 
 * ê²½ë¡œë³„ ì¸ì¦ ì •ì±…:
 * - Public: í—¬ìŠ¤ì²´í¬, ì¸ì¦ ì—”ë“œí¬ì¸íŠ¸ (/api/v1/auth/**)
 * - Protected: ëŒ€ë¶€ë¶„ì˜ API ì—”ë“œí¬ì¸íŠ¸ (JWT ê²€ì¦ì€ downstream ì„œë¹„ìŠ¤ì—ì„œ ì²˜ë¦¬)
 * 
 * NOTE: GatewayëŠ” JWT í† í°ì„ downstream ì„œë¹„ìŠ¤ë¡œ ì „ë‹¬ë§Œ í•˜ê³ ,
 * ì‹¤ì œ ì¸ì¦/ì¸ê°€ëŠ” ê° ì„œë¹„ìŠ¤ì˜ SecurityConfigì—ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
 * Gatewayì—ì„œëŠ” ê¸°ë³¸ì ì¸ ê²½ë¡œ ê¸°ë°˜ ì ‘ê·¼ ì œì–´ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
 */
@Configuration
@EnableWebFluxSecurity
public class SecurityConfig {

    @Value("${security.gateway.enabled:true}")
    private boolean securityEnabled;

    @Bean
    public SecurityWebFilterChain securityWebFilterChain(ServerHttpSecurity http) {
        if (!securityEnabled) {
            // ê°œë°œ í™˜ê²½ì—ì„œë§Œ ì‚¬ìš© - í”„ë¡œë•ì…˜ì—ì„œëŠ” ì ˆëŒ€ ë¹„í™œì„±í™”í•˜ì§€ ë§ˆì„¸ìš”!
            return http
                    .csrf(ServerHttpSecurity.CsrfSpec::disable)
                    .cors(cors -> cors.configurationSource(corsConfigurationSource()))
                    .httpBasic(ServerHttpSecurity.HttpBasicSpec::disable)
                    .formLogin(ServerHttpSecurity.FormLoginSpec::disable)
                    .authorizeExchange(exchange -> exchange.anyExchange().permitAll())
                    .build();
        }

        return http
                .csrf(ServerHttpSecurity.CsrfSpec::disable)
                .cors(cors -> cors.configurationSource(corsConfigurationSource()))
                .httpBasic(ServerHttpSecurity.HttpBasicSpec::disable)
                .formLogin(ServerHttpSecurity.FormLoginSpec::disable)
                .authorizeExchange(exchange -> exchange
                        // ========================================
                        // Public Endpoints (ì¸ì¦ ë¶ˆí•„ìš”)
                        // ========================================
                        // Health checks & Actuator
                        .pathMatchers("/actuator/**").permitAll()
                        .pathMatchers("/api/actuator/**").permitAll()
                        
                        // Authentication endpoints (login, register, token)
                        .pathMatchers("/api/v1/auth/login").permitAll()
                        .pathMatchers("/api/v1/auth/register").permitAll()
                        .pathMatchers("/api/v1/auth/token").permitAll()
                        .pathMatchers("/api/v1/auth/send-verification").permitAll()
                        .pathMatchers("/api/v1/auth/verify-email").permitAll()
                        .pathMatchers("/api/v1/auth/resend-verification").permitAll()
                        .pathMatchers("/api/v1/auth/check-username/**").permitAll()
                        .pathMatchers("/api/v1/auth/check-email/**").permitAll()
                        
                        // CORS preflight
                        .pathMatchers(HttpMethod.OPTIONS, "/**").permitAll()
                        
                        // ========================================
                        // Protected Endpoints (ì¸ì¦ í•„ìš” - downstreamì—ì„œ ê²€ì¦)
                        // GatewayëŠ” í† í°ì„ ì „ë‹¬ë§Œ í•˜ê³ , ì‹¤ì œ ê²€ì¦ì€ ê° ì„œë¹„ìŠ¤ì—ì„œ ìˆ˜í–‰
                        // ========================================
                        // ëª¨ë“  ë‹¤ë¥¸ ìš”ì²­ì€ í†µê³¼ì‹œí‚¤ë˜, downstream ì„œë¹„ìŠ¤ê°€ ì¸ì¦ì„ ì²˜ë¦¬
                        .anyExchange().permitAll()
                )
                .build();
    }

    @Bean
    public CorsConfigurationSource corsConfigurationSource() {
        CorsConfiguration configuration = new CorsConfiguration();
        configuration.setAllowedOriginPatterns(List.of("*"));
        configuration.setAllowedMethods(Arrays.asList("GET", "POST", "PUT", "DELETE", "PATCH", "OPTIONS", "HEAD"));
        configuration.setAllowedHeaders(List.of("*"));
        configuration.setAllowCredentials(true);
        configuration.setMaxAge(3600L);
        
        UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource();
        source.registerCorsConfiguration("/**", configuration);
        return source;
    }

    /**
     * Rate Limiterìš© KeyResolver
     * IP ì£¼ì†Œ ê¸°ë°˜ìœ¼ë¡œ Rate Limit ì ìš©
     */
    @Bean
    public KeyResolver ipKeyResolver() {
        return exchange -> {
            var remoteAddress = exchange.getRequest().getRemoteAddress();
            String ip = remoteAddress != null
                    ? remoteAddress.getAddress().getHostAddress()
                    : "unknown";
            return Mono.just(ip);
        };
    }
}

```

---

## backend/api-gateway-service/src/main/java/com/newsinsight/gateway/controller/FrontendConfigController.java

```java
package com.newsinsight.gateway.controller;

import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Mono;

import java.util.HashMap;
import java.util.Map;
import java.util.Base64;

@RestController
@RequestMapping("/api/v1/config")
public class FrontendConfigController {

    @Value("${FRONTEND_API_BASE_URL:${API_GATEWAY_FRONTEND_API_BASE_URL:http://localhost:8112}}")
    private String frontendApiBaseUrl;

    @Value("${spring.cloud.consul.host:consul}")
    private String consulHost;

    @Value("${spring.cloud.consul.port:8500}")
    private int consulPort;

    private final WebClient webClient;

    public FrontendConfigController(WebClient.Builder webClientBuilder) {
        this.webClient = webClientBuilder.build();
    }

    @CrossOrigin(origins = "*")
    @GetMapping("/frontend")
    public ResponseEntity<Map<String, String>> getFrontendConfig() {
        Map<String, String> body = new HashMap<>();
        body.put("apiBaseUrl", frontendApiBaseUrl);
        return ResponseEntity.ok(body);
    }

    /**
     * Save AI/LLM settings to Consul KV store
     * PUT /api/v1/config/ai-settings
     */
    @CrossOrigin(origins = "*")
    @PutMapping("/ai-settings")
    public Mono<ResponseEntity<Map<String, Object>>> saveAISettings(@RequestBody Map<String, String> settings) {
        String consulUrl = "http://" + consulHost + ":" + consulPort;
        
        // Save each setting to Consul KV under config/autonomous-crawler/ prefix
        return Mono.when(
            settings.entrySet().stream()
                .map(entry -> {
                    String key = "config/autonomous-crawler/" + entry.getKey();
                    Object value = entry.getValue();
                    // Consul expects base64 encoded value for PUT
                    return webClient.put()
                        .uri(consulUrl + "/v1/kv/" + key)
                        .bodyValue(value.toString().isEmpty() ? "" : value)
                        .retrieve()
                        .bodyToMono(Boolean.class)
                        .onErrorReturn(false);
                })
                .toArray(Mono[]::new)
        ).then(Mono.fromCallable(() -> {
            Map<String, Object> response = new HashMap<>();
            response.put("success", true);
            response.put("message", "AI settings saved to Consul");
            response.put("keysCount", settings.size());
            return ResponseEntity.ok(response);
        }));
    }

    /**
     * Get AI/LLM settings from Consul KV store
     * GET /api/v1/config/ai-settings
     */
    @CrossOrigin(origins = "*")
    @GetMapping("/ai-settings")
    public Mono<ResponseEntity<Map<String, String>>> getAISettings() {
        String consulUrl = "http://" + consulHost + ":" + consulPort;
        
        return webClient.get()
            .uri(consulUrl + "/v1/kv/config/autonomous-crawler/?recurse=true")
            .retrieve()
            .bodyToMono(Object[].class)
            .map(entries -> {
                Map<String, String> settings = new HashMap<>();
                if (entries != null) {
                    for (Object entry : entries) {
                        if (entry instanceof Map) {
                            @SuppressWarnings("unchecked")
                            Map<String, Object> kvEntry = (Map<String, Object>) entry;
                            String fullKey = (String) kvEntry.get("Key");
                            String encodedValue = (String) kvEntry.get("Value");
                            
                            if (fullKey != null && encodedValue != null) {
                                // Remove prefix and decode value
                                String key = fullKey.replace("config/autonomous-crawler/", "");
                                String value = new String(Base64.getDecoder().decode(encodedValue));
                                settings.put(key, value);
                            }
                        }
                    }
                }
                return ResponseEntity.ok(settings);
            })
            .onErrorReturn(ResponseEntity.ok(new HashMap<>()));
    }
}

```

---

## backend/api-gateway-service/src/main/java/com/newsinsight/gateway/filter/JwtAuthenticationFilter.java

```java
package com.newsinsight.gateway.filter;

import io.jsonwebtoken.Claims;
import io.jsonwebtoken.Jwts;
import io.jsonwebtoken.security.Keys;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.cloud.gateway.filter.GatewayFilterChain;
import org.springframework.cloud.gateway.filter.GlobalFilter;
import org.springframework.core.Ordered;
import org.springframework.http.HttpCookie;
import org.springframework.http.HttpStatus;
import org.springframework.http.server.reactive.ServerHttpRequest;
import org.springframework.stereotype.Component;
import org.springframework.util.MultiValueMap;
import org.springframework.web.server.ServerWebExchange;
import reactor.core.publisher.Mono;

import javax.crypto.SecretKey;
import java.nio.charset.StandardCharsets;
import java.util.List;

/**
 * JWT ì¸ì¦ í•„í„°
 * 
 * Python FastAPIì˜ auth_middlewareì™€ ë™ì¼í•œ ê¸°ëŠ¥ êµ¬í˜„
 * - Authorization í—¤ë”ì—ì„œ Bearer í† í° ì¶”ì¶œ
 * - Cookieì—ì„œ access_token ì¶”ì¶œ (SSE/EventSource ì§€ì›)
 * - ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ì—ì„œ token ì¶”ì¶œ (fallback)
 * - JWT í† í° ê²€ì¦ (ì„œëª…, ë§Œë£Œ ì‹œê°„ ë“±)
 * - ì‚¬ìš©ì ì •ë³´ë¥¼ í—¤ë”ì— ì¶”ê°€í•˜ì—¬ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì„œë¹„ìŠ¤ë¡œ ì „ë‹¬
 */
@Slf4j
@Component
public class JwtAuthenticationFilter implements GlobalFilter, Ordered {
    
    // ì¸ì¦ ë¶ˆí•„ìš”í•œ ê³µê°œ ê²½ë¡œ
    private static final List<String> PUBLIC_PATHS = List.of(
        "/health",
        "/actuator",
        "/api/v1/auth",          // Public Auth API (register, login, check-username, check-email)
        "/api/v1/articles",
        "/api/v1/analysis",
        "/api/v1/ai",
        "/api/v1/config",
        "/api/v1/sources",
        "/api/v1/collections",
        "/api/v1/data",
        "/api/v1/search",
        "/api/v1/events",        // SSE ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ (EventSourceëŠ” í—¤ë” ì „ì†¡ ë¶ˆê°€)
        "/api/v1/search-history",
        "/api/v1/search-templates",
        "/api/v1/jobs",          // Search Jobs API (SSE ìŠ¤íŠ¸ë¦¼ í¬í•¨)
        "/api/v1/projects",      // Projects API (ìµëª… ì‚¬ìš©ì ì§€ì›)
        "/api/v1/ai",
        "/api/v1/ml",
        "/api/v1/llm-providers", // LLM Provider Settings (ì‚¬ìš©ìë³„ ì„¤ì • ì§€ì›)
        "/api/v1/admin",         // Admin Dashboard (ìì²´ ì¸ì¦ ì²˜ë¦¬)
        "/api/v1/crawler",       // Autonomous Crawler API
        "/api/v1/autocrawl",     // AutoCrawl API (ìë™ í¬ë¡¤ë§ ê´€ë¦¬)
        "/api/browser-use",      // Browser-Use API (gateway path)
        "/api/ml-addons",        // ML Add-ons API (sentiment, factcheck, bias)
        "/browse",               // Browser-Use API (direct path - legacy)
        "/jobs",                 // Browser-Use Jobs (direct path - legacy)
        "/ws"                    // WebSocket (direct path - legacy)
    );
    
    @Value("${JWT_SECRET_KEY:default-secret-key-please-change-in-consul}")
    private String jwtSecretKey;
    
    @Value("${JWT_ALGORITHM:HS256}")
    private String jwtAlgorithm;
    
    @Override
    public Mono<Void> filter(ServerWebExchange exchange, GatewayFilterChain chain) {
        String path = exchange.getRequest().getPath().value();
        
        // ê³µê°œ ì—”ë“œí¬ì¸íŠ¸ëŠ” ì¸ì¦ ìŠ¤í‚µí•˜ì§€ë§Œ ìµëª… ì‚¬ìš©ì í—¤ë”ëŠ” ì¶”ê°€
        if (PUBLIC_PATHS.stream().anyMatch(path::startsWith)) {
            log.debug("Public path: {}, adding anonymous user headers", path);
            return handleAnonymousUser(exchange, chain);
        }
        
        // í† í° ì¶”ì¶œ (ìš°ì„ ìˆœìœ„: Authorization í—¤ë” > Cookie > Query Parameter)
        String token = extractToken(exchange);
        
        if (token == null) {
            log.warn("No valid token found for path: {}", path);
            exchange.getResponse().setStatusCode(HttpStatus.UNAUTHORIZED);
            return exchange.getResponse().setComplete();
        }
        
        try {
            // JWT í† í° íŒŒì‹± ë° ê²€ì¦
            SecretKey key = Keys.hmacShaKeyFor(jwtSecretKey.getBytes(StandardCharsets.UTF_8));
            Claims claims = Jwts.parser()
                    .verifyWith(key)
                    .build()
                    .parseSignedClaims(token)
                    .getPayload();
            
            // ì‚¬ìš©ì ì •ë³´ë¥¼ í—¤ë”ì— ì¶”ê°€ (ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì„œë¹„ìŠ¤ì—ì„œ ì‚¬ìš©)
            ServerHttpRequest mutatedRequest = exchange.getRequest().mutate()
                    .header("X-User-Id", claims.getSubject())
                    .header("X-User-Role", claims.get("role", String.class))
                    .header("X-Username", claims.get("username", String.class))
                    .build();
            
            log.debug("Authenticated user: {} with role: {}", 
                    claims.get("username", String.class), 
                    claims.get("role", String.class));
            
            return chain.filter(exchange.mutate().request(mutatedRequest).build());
            
        } catch (Exception e) {
            log.error("JWT authentication failed: {}", e.getMessage());
            exchange.getResponse().setStatusCode(HttpStatus.UNAUTHORIZED);
            return exchange.getResponse().setComplete();
        }
    }
    
    /**
     * Handle anonymous user by generating unique user ID based on session
     * This prevents data leakage between different anonymous users
     */
    private Mono<Void> handleAnonymousUser(ServerWebExchange exchange, GatewayFilterChain chain) {
        // Extract session ID from headers (sent by frontend)
        String sessionId = exchange.getRequest().getHeaders().getFirst("X-Session-Id");
        String deviceId = exchange.getRequest().getHeaders().getFirst("X-Device-Id");
        
        // Generate unique anonymous user ID based on session
        String anonymousUserId;
        if (sessionId != null && !sessionId.isBlank()) {
            anonymousUserId = "user_anon_" + sessionId;
        } else {
            // Fallback: use device ID or generate random ID
            if (deviceId != null && !deviceId.isBlank()) {
                anonymousUserId = "user_anon_" + deviceId;
            } else {
                // Last resort: generate random ID (not ideal, but prevents null)
                anonymousUserId = "user_anon_" + System.currentTimeMillis();
            }
            log.warn("No session ID provided, using fallback anonymous user ID: {}", anonymousUserId);
        }
        
        // Add anonymous user headers for downstream services
        ServerHttpRequest mutatedRequest = exchange.getRequest().mutate()
                .header("X-User-Id", anonymousUserId)
                .header("X-User-Role", "anonymous")
                .header("X-Session-Id", sessionId != null ? sessionId : "")
                .header("X-Device-Id", deviceId != null ? deviceId : "")
                .build();
        
        log.debug("Anonymous user: userId={}, sessionId={}", anonymousUserId, sessionId);
        
        return chain.filter(exchange.mutate().request(mutatedRequest).build());
    }
    
    /**
     * ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ JWT í† í° ì¶”ì¶œ
     * ìš°ì„ ìˆœìœ„:
     * 1. Authorization í—¤ë” (Bearer token)
     * 2. Cookie (access_token)
     * 3. Query Parameter (token)
     */
    private String extractToken(ServerWebExchange exchange) {
        // 1. Authorization í—¤ë”ì—ì„œ ì¶”ì¶œ
        String authHeader = exchange.getRequest().getHeaders().getFirst("Authorization");
        if (authHeader != null && authHeader.startsWith("Bearer ")) {
            log.debug("Token extracted from Authorization header");
            return authHeader.substring(7);
        }
        
        // 2. Cookieì—ì„œ ì¶”ì¶œ (SSE/EventSource ì§€ì›)
        MultiValueMap<String, HttpCookie> cookies = exchange.getRequest().getCookies();
        HttpCookie accessTokenCookie = cookies.getFirst("access_token");
        if (accessTokenCookie != null && !accessTokenCookie.getValue().isEmpty()) {
            log.debug("Token extracted from access_token cookie");
            return accessTokenCookie.getValue();
        }
        
        // 3. Query Parameterì—ì„œ ì¶”ì¶œ (fallback)
        String queryToken = exchange.getRequest().getQueryParams().getFirst("token");
        if (queryToken != null && !queryToken.isEmpty()) {
            log.debug("Token extracted from query parameter");
            return queryToken;
        }
        
        return null;
    }
    
    @Override
    public int getOrder() {
        return -100; // ë†’ì€ ìš°ì„ ìˆœìœ„ (ë¨¼ì € ì‹¤í–‰)
    }
}

```

---

## backend/api-gateway-service/src/main/java/com/newsinsight/gateway/filter/RbacFilter.java

```java
package com.newsinsight.gateway.filter;

import lombok.extern.slf4j.Slf4j;
import org.springframework.cloud.gateway.filter.GatewayFilterChain;
import org.springframework.cloud.gateway.filter.GlobalFilter;
import org.springframework.core.Ordered;
import org.springframework.http.HttpMethod;
import org.springframework.http.HttpStatus;
import org.springframework.stereotype.Component;
import org.springframework.web.server.ServerWebExchange;
import reactor.core.publisher.Mono;

import java.util.List;
import java.util.Map;

/**
 * RBAC (Role-Based Access Control) í•„í„°
 * 
 * Python FastAPIì˜ rbac_middlewareì™€ ë™ì¼í•œ ê¸°ëŠ¥ êµ¬í˜„
 * - HTTP ë©”ì„œë“œì— ë”°ë¼ í•„ìš”í•œ ê¶Œí•œ í™•ì¸
 * - ì‚¬ìš©ì ì—­í• ì— ë”°ë¼ ì ‘ê·¼ ì œì–´
 */
@Slf4j
@Component
public class RbacFilter implements GlobalFilter, Ordered {
    
    // ì—­í• ë³„ ê¶Œí•œ ë§¤í•‘ (Pythonì˜ ROLE_PERMISSIONSì™€ ë™ì¼)
    private static final Map<String, List<String>> ROLE_PERMISSIONS = Map.of(
        "admin", List.of("READ", "WRITE", "DELETE", "ADMIN"),
        "analyst", List.of("READ", "WRITE"),
        "viewer", List.of("READ"),
        "system", List.of("READ", "WRITE", "DELETE"),
        "anonymous", List.of("READ", "WRITE"),  // ìµëª… ì‚¬ìš©ì - ê²€ìƒ‰ ë° ë¶„ì„ í—ˆìš©
        "user", List.of("READ", "WRITE")        // ì¼ë°˜ íšŒì› - ê²€ìƒ‰ ë° ë¶„ì„ í—ˆìš©
    );
    
    // HTTP ë©”ì„œë“œë³„ í•„ìš” ê¶Œí•œ
    private static final Map<HttpMethod, String> METHOD_PERMISSIONS = Map.of(
        HttpMethod.GET, "READ",
        HttpMethod.POST, "WRITE",
        HttpMethod.PUT, "WRITE",
        HttpMethod.PATCH, "WRITE",
        HttpMethod.DELETE, "DELETE"
    );
    
    @Override
    public Mono<Void> filter(ServerWebExchange exchange, GatewayFilterChain chain) {
        String userRole = exchange.getRequest().getHeaders().getFirst("X-User-Role");
        
        if (userRole == null) {
            // ì¸ì¦ë˜ì§€ ì•Šì€ ìš”ì²­ (public endpoint)
            log.debug("No user role found, allowing request to proceed");
            return chain.filter(exchange);
        }
        
        HttpMethod method = exchange.getRequest().getMethod();
        String requiredPermission = METHOD_PERMISSIONS.get(method);
        List<String> userPermissions = ROLE_PERMISSIONS.getOrDefault(userRole, List.of());
        
        if (requiredPermission != null && !userPermissions.contains(requiredPermission)) {
            log.warn("Access denied for role: {} on method: {}. Required permission: {}", 
                    userRole, method, requiredPermission);
            exchange.getResponse().setStatusCode(HttpStatus.FORBIDDEN);
            return exchange.getResponse().setComplete();
        }
        
        log.debug("Access granted for role: {} on method: {}", userRole, method);
        return chain.filter(exchange);
    }
    
    @Override
    public int getOrder() {
        return -90; // JWT í•„í„° ë‹¤ìŒ ì‹¤í–‰
    }
}

```

---

## backend/api-gateway-service/src/main/resources/application.yml

```yml
spring:
  application:
    name: api-gateway
  
  config:
    # Consulì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸° (optional - Consul ì—†ì–´ë„ ì‹œì‘ ê°€ëŠ¥)
    import: ${SPRING_CONFIG_IMPORT:optional:consul:}
  
  cloud:
    consul:
      enabled: ${CONSUL_ENABLED:true}
      host: ${CONSUL_HOST:localhost}
      port: ${CONSUL_PORT:8500}
      config:
        enabled: ${CONSUL_CONFIG_ENABLED:true}
        # ê¸°ì¡´ Consul KV êµ¬ì¡°ì™€ ì¼ì¹˜
        prefix: config
        default-context: ${spring.application.name}
        format: PROPERTIES
        # Fail-Fast ë¹„í™œì„±í™”: Consul ì—°ê²° ì‹¤íŒ¨í•´ë„ ì‹œì‘
        fail-fast: ${CONSUL_FAIL_FAST:false}
      discovery:
        enabled: ${CONSUL_DISCOVERY_ENABLED:true}
        health-check-path: /actuator/health
        health-check-interval: 10s
        instance-id: ${spring.application.name}:${random.value}
        prefer-ip-address: true
        # ì„œë¹„ìŠ¤ ë“±ë¡ ì‹¤íŒ¨í•´ë„ ì‹œì‘
        fail-fast: false
    inetutils:
      preferred-networks:
        - 172.20

    gateway:
      routes:
        # ===========================================
        # Collector Service Routes
        # COLLECTOR_SERVICE_URL í™˜ê²½ë³€ìˆ˜ë¡œ ì§ì ‘ URL ì§€ì • ê°€ëŠ¥
        # Consul ì‚¬ìš© ì‹œ: lb://collector-service
        # Consul ë¯¸ì‚¬ìš© ì‹œ: http://collector-service:8081 (ê¸°ë³¸ê°’)
        # ===========================================
        
        # Collector ì„œë¹„ìŠ¤ - ë°ì´í„° API (DataController)
        - id: collector-data
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/data/**
          # NOTE: Redis RateLimiter ë¹„í™œì„±í™” - Redis ì—†ì´ë„ ë™ì‘í•˜ë„ë¡ ì„¤ì •
          # Redis ë„ì… ì‹œ ì•„ë˜ ì£¼ì„ í•´ì œ
          # filters:
          #   - name: RequestRateLimiter
          #     args:
          #       redis-rate-limiter.replenishRate: 100
          #       redis-rate-limiter.burstCapacity: 150

        # Collector ì„œë¹„ìŠ¤ - ì†ŒìŠ¤ ê´€ë¦¬ (SourceController)
        - id: collector-sources
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/sources/**

        # Collector ì„œë¹„ìŠ¤ - ìˆ˜ì§‘ ì‘ì—… (CollectionController)
        - id: collector-collections
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/collections/**

        # Articles & Analysis - í”„ë¡ íŠ¸ì—”ë“œ API
        - id: articles-root
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/articles

        - id: articles-alias
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/articles/**

        - id: analysis-root
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/analysis

        - id: analysis-alias
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/analysis/**
          metadata:
            response-timeout: 300000

        # Search API - í†µí•© ê²€ìƒ‰ (SSE ìŠ¤íŠ¸ë¦¬ë°)
        # NOTE: SSE ìŠ¤íŠ¸ë¦¬ë°ì€ RateLimiterì™€ í˜¸í™˜ì„± ë¬¸ì œê°€ ìˆì–´ ë¹„í™œì„±í™”
        - id: search-api
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/search/**

        # Search History API - ê²€ìƒ‰ ê¸°ë¡ ê´€ë¦¬
        - id: search-history
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/search-history/**
          metadata:
            # SSE ìŠ¤íŠ¸ë¦¼ ì—°ê²°ì„ ìœ„í•œ ê¸´ íƒ€ì„ì•„ì›ƒ (5ë¶„)
            response-timeout: 300000

        # Search Template API - SmartSearch ê²€ìƒ‰ í…œí”Œë¦¿ ê´€ë¦¬
        - id: search-templates
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/search-templates/**

        # Reports API - PDF ë³´ê³ ì„œ ìƒì„± ë° ë‹¤ìš´ë¡œë“œ (ReportController)
        - id: reports
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/reports/**
          metadata:
            # PDF ìƒì„±ì— ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê¸´ íƒ€ì„ì•„ì›ƒ (3ë¶„)
            response-timeout: 180000

        # Search Jobs API - ê²€ìƒ‰ ì‘ì—… ê´€ë¦¬ (SearchJobController)
        # SSE ìŠ¤íŠ¸ë¦¬ë° ì§€ì›: /api/v1/jobs/stream, /api/v1/jobs/{jobId}/stream
        - id: search-jobs
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/jobs,/api/v1/jobs/**
          metadata:
            # SSE ìŠ¤íŠ¸ë¦¼ ì—°ê²°ì„ ìœ„í•œ ê¸´ íƒ€ì„ì•„ì›ƒ (5ë¶„)
            response-timeout: 300000

        # AutoCrawl API - ìë™ í¬ë¡¤ë§ ê´€ë¦¬ (AutoCrawlController)
        - id: autocrawl
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/autocrawl/**

        # Projects API - í”„ë¡œì íŠ¸ ê´€ë¦¬ (ProjectController)
        - id: projects
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/projects/**

        # Workspace Files API - íŒŒì¼ ì—…ë¡œë“œ/ë‹¤ìš´ë¡œë“œ (WorkspaceController)
        - id: workspace-files
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/workspace/**
          metadata:
            # íŒŒì¼ ì—…ë¡œë“œ/ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•œ ê¸´ íƒ€ì„ì•„ì›ƒ (5ë¶„)
            response-timeout: 300000

        # Config API - í”„ë¡ íŠ¸ì—”ë“œ ì„¤ì •
        # NOTE: Gateway ìì²´ FrontendConfigControllerê°€ ìš°ì„  ì²˜ë¦¬ë¨
        # ì´ ë¼ìš°íŠ¸ëŠ” fallbackìœ¼ë¡œ ìœ ì§€ (collector-serviceì—ë„ ConfigControllerê°€ ìˆì„ ê²½ìš°)
        - id: config-api
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/config/**

        # LLM Provider Settings - LLM ì œê³µì ì„¤ì • (collector-service LlmProviderSettingsController)
        - id: llm-providers
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/llm-providers/**

        # AI Orchestration - AI ë¶„ì„ ìš”ì²­ (collector-service AiOrchestrationController)
        - id: collector-ai
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/ai/**

        - id: collector-ml
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/ml/**

        # Dashboard Events - SSE ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ (collector-service DashboardEventsController)
        - id: collector-events
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/events/**
          metadata:
            # SSE ì—°ê²°ì„ ìœ„í•œ ê¸´ íƒ€ì„ì•„ì›ƒ
            response-timeout: 300000

        # Admin Dashboard API
        # NOTE: Using direct URL fallback if not found in Consul, assuming port 8888 from main.py
        - id: admin-dashboard-api
          uri: ${ADMIN_DASHBOARD_URL:http://localhost:8888}
          predicates:
            - Path=/api/v1/admin/**
          # No StripPrefix needed because the admin API uses /api/v1/admin prefix

        # Public Auth API - íšŒì›ê°€ì…, ë¡œê·¸ì¸ (Admin Dashboardì—ì„œ ì²˜ë¦¬)
        # /api/v1/auth/register, /api/v1/auth/login, /api/v1/auth/me ë“±
        - id: public-auth
          uri: ${ADMIN_DASHBOARD_URL:http://localhost:8888}
          predicates:
            - Path=/api/v1/auth/**
          # No StripPrefix needed because the admin API uses /api/v1/auth prefix

        # Autonomous Crawler - í¬ë¡¤ë§ ì‘ì—… ê´€ë¦¬
        - id: autonomous-crawler-api
          uri: ${AUTONOMOUS_CRAWLER_API_URL:${AUTONOMOUS_CRAWLER_URL:http://autonomous-crawler:8030}}
          predicates:
            - Path=/api/v1/crawler/**
          filters:
            # /api/v1/crawler/** -> /** ë¡œ ì „ë‹¬ (3 segments: api, v1, crawler)
            - StripPrefix=3

        # Browser-Use API - AI ë¸Œë¼ìš°ì € ìë™í™”
        - id: browser-use-browse
          uri: ${BROWSER_USE_URL:http://localhost:8500}
          predicates:
            - Path=/api/browser-use/browse/**
          filters:
            # /api/browser-use/browse/** -> /browse/** ë¡œ ì „ë‹¬
            - StripPrefix=2

        - id: browser-use-jobs
          uri: ${BROWSER_USE_URL:http://localhost:8500}
          predicates:
            - Path=/api/browser-use/jobs/**
          filters:
            # /api/browser-use/jobs/** -> /jobs/** ë¡œ ì „ë‹¬
            - StripPrefix=2

        - id: browser-use-health
          uri: ${BROWSER_USE_URL:http://localhost:8500}
          predicates:
            - Path=/api/browser-use/health
          filters:
            # /api/browser-use/health -> /health ë¡œ ì „ë‹¬
            - StripPrefix=2

        # Browser-Use WebSocket
        - id: browser-use-websocket
          uri: ${BROWSER_USE_WS_URL:ws://localhost:8500}
          predicates:
            - Path=/api/browser-use/ws/**
          filters:
            # /api/browser-use/ws/** -> /ws/** ë¡œ ì „ë‹¬
            - StripPrefix=2

        # ML Add-ons - ê°ì • ë¶„ì„ (Sentiment Analysis)
        - id: ml-addon-sentiment
          uri: ${ML_ADDON_SENTIMENT_URL:http://sentiment-addon:8100}
          predicates:
            - Path=/api/ml-addons/sentiment/**
          filters:
            # /api/ml-addons/sentiment/** -> /** ë¡œ ì „ë‹¬
            - StripPrefix=3

        # Fact Check Chat - íŒ©íŠ¸ì²´í¬ ì±—ë´‡
        - id: factcheck-chat
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/factcheck-chat/**
          filters:
            - StripPrefix=0

        # ML Add-ons - íŒ©íŠ¸ì²´í¬ (Fact Check)
        - id: ml-addon-factcheck
          uri: ${ML_ADDON_FACTCHECK_URL:http://factcheck-addon:8101}
          predicates:
            - Path=/api/ml-addons/factcheck/**
          filters:
            # /api/ml-addons/factcheck/** -> /** ë¡œ ì „ë‹¬
            - StripPrefix=3

        # ML Add-ons - í¸í–¥ë„ ë¶„ì„ (Bias Analysis)
        - id: ml-addon-bias
          uri: ${ML_ADDON_BIAS_URL:http://bias-addon:8102}
          predicates:
            - Path=/api/ml-addons/bias/**
          filters:
            # /api/ml-addons/bias/** -> /** ë¡œ ì „ë‹¬
            - StripPrefix=3

        - id: ml-addon-bot-detector
          uri: ${ML_ADDON_BOT_DETECTOR_URL:http://bot-detector:8041}
          predicates:
            - Path=/api/ml-addons/bot-detector/**
          filters:
            - StripPrefix=3

        # ML Trainer Service - ëª¨ë¸ í•™ìŠµ ì„œë¹„ìŠ¤
        - id: ml-trainer
          uri: ${ML_TRAINER_URL:http://ml-trainer:8090}
          predicates:
            - Path=/api/ml-trainer/**
          filters:
            # /api/ml-trainer/** -> /** ë¡œ ì „ë‹¬
            - StripPrefix=2
          metadata:
            # í•™ìŠµ ì‘ì—…ì€ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê¸´ íƒ€ì„ì•„ì›ƒ
            response-timeout: 600000

        # ML Trainer SSE Stream - ì‹¤ì‹œê°„ í•™ìŠµ ì§„í–‰ ìƒí™©
        - id: ml-trainer-stream
          uri: ${ML_TRAINER_URL:http://ml-trainer:8090}
          predicates:
            - Path=/api/ml-trainer/jobs/*/stream
          filters:
            - StripPrefix=2
          metadata:
            # SSE ìŠ¤íŠ¸ë¦¼ì„ ìœ„í•œ ë§¤ìš° ê¸´ íƒ€ì„ì•„ì›ƒ (30ë¶„)
            response-timeout: 1800000

        # Health Check Rewrite - í”„ë¡ íŠ¸ì—”ë“œê°€ /api/actuator/health í˜¸ì¶œ ì‹œ ì²˜ë¦¬
        - id: gateway-health
          uri: http://localhost:${PORT:8000}
          predicates:
            - Path=/api/actuator/health,/api/actuator/health/**
          filters:
            # /api/actuator/health -> /actuator/health ë¡œ ì¬ì‘ì„±
            - RewritePath=/api/actuator/(?<segment>.*), /actuator/${segment}

      # NOTE: CORS is handled by SecurityConfig.java - DO NOT enable globalcors
      # Having both causes duplicate Access-Control-Allow-Origin headers
      # globalcors:
      #   cors-configurations:
      #     '[/**]':
      #       allowedOriginPatterns: "*"
      #       allowedMethods:
      #         - GET
      #         - POST
      #         - PUT
      #         - DELETE
      #         - PATCH
      #         - OPTIONS
      #       allowedHeaders: "*"
      #       allowCredentials: true
      #       maxAge: 3600
  
  # Redis ì„¤ì • (Rate Limiting ë° ì„¸ì…˜ ê´€ë¦¬ìš©)
  data:
    redis:
      host: ${REDIS_HOST:redis}
      port: ${REDIS_PORT:6379}
      password: ${REDIS_PASSWORD:}

server:
  port: ${PORT:8000}
  # Netty ì„¤ì • - SSE/WebSocket ì—°ê²°ì„ ìœ„í•œ íƒ€ì„ì•„ì›ƒ ì¦ê°€
  netty:
    # ì—°ê²° ìœ ì§€ ì‹œê°„ (5ë¶„)
    idle-timeout: 300000

# Spring Cloud Gateway HTTP Client ì„¤ì •
spring.cloud.gateway.httpclient:
  connect-timeout: 10000
  response-timeout: 300000
  pool:
    max-idle-time: 300000

management:
  endpoints:
    web:
      exposure:
        include: health,info,env,metrics,prometheus
  endpoint:
    health:
      show-details: always

logging:
  level:
    root: INFO
    org.springframework.cloud.gateway: DEBUG
    org.springframework.cloud.gateway.handler.RoutePredicateHandlerMapping: TRACE
    org.springframework.security: DEBUG
    com.newsinsight.gateway: DEBUG

```

---

## backend/autonomous-crawler-service/crawl-worker/main.py

```py
import asyncio
import os
import sys
import time
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
import uuid
import logging

log = logging.getLogger(__name__)

# Add shared module to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))
try:
    from shared.prometheus_metrics import (
        setup_metrics,
        track_request_time,
        track_operation,
        track_error,
        track_item_processed,
        ServiceMetrics,
    )

    METRICS_AVAILABLE = True
except ImportError:
    METRICS_AVAILABLE = False

try:
    from shared.proxy_client import ProxyRotationClient, ProxyInfo

    PROXY_CLIENT_AVAILABLE = True
except ImportError:
    PROXY_CLIENT_AVAILABLE = False
    ProxyRotationClient = None
    ProxyInfo = None

try:
    from crawl4ai import AsyncWebCrawler  # type: ignore
except Exception:
    AsyncWebCrawler = None  # fallback for environments without crawl4ai


def detect_captcha_type(content: str) -> Optional[str]:
    text = content.lower()

    cloudflare_markers = [
        "cf-turnstile",
        "turnstile",
        "challenges.cloudflare.com",
        "challenge-running",
        "cf-browser-verification",
        "checking your browser",
        "just a moment",
    ]
    if any(m in text for m in cloudflare_markers):
        return "cloudflare"

    if "recaptcha" in text or "g-recaptcha" in text:
        return "recaptcha"

    if "hcaptcha" in text or "h-captcha" in text:
        return "hcaptcha"

    if "captcha" in text:
        return "captcha"

    return None

app = FastAPI(title="Crawl4AI Worker", version="0.3.0")

# Setup Prometheus metrics
SERVICE_NAME = "crawl-worker"
if METRICS_AVAILABLE:
    setup_metrics(app, SERVICE_NAME, version="0.3.0")
    service_metrics = ServiceMetrics(SERVICE_NAME)
    # Create service-specific metrics
    crawl_requests = service_metrics.create_counter(
        "crawl_requests_total",
        "Total crawl requests",
        ["status", "js_render", "proxy_used"],
    )
    crawl_latency = service_metrics.create_histogram(
        "crawl_latency_seconds",
        "Crawl request latency",
        ["status"],
        buckets=(0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0),
    )
    batch_crawls = service_metrics.create_counter(
        "batch_crawls_total", "Total batch crawl operations", ["status"]
    )
    concurrent_crawls_gauge = service_metrics.create_gauge(
        "concurrent_crawls", "Number of concurrent crawl operations"
    )
    proxy_usage = service_metrics.create_counter(
        "proxy_usage_total", "Proxy usage statistics", ["proxy_id", "status"]
    )
else:
    service_metrics = None

# In-memory storage for batch results
batch_results: Dict[str, Dict[str, Any]] = {}

# Semaphore for concurrent crawl limit
MAX_CONCURRENT_CRAWLS = int(os.environ.get("MAX_CONCURRENT_CRAWLS", "5"))
crawl_semaphore = asyncio.Semaphore(MAX_CONCURRENT_CRAWLS)

# Proxy rotation configuration
USE_PROXY_ROTATION = os.environ.get("USE_PROXY_ROTATION", "true").lower() == "true"
PROXY_ROTATION_URL = os.environ.get("PROXY_ROTATION_URL", "http://ip-rotation:8050")

# Initialize proxy client
proxy_client: Optional["ProxyRotationClient"] = None  # type: ignore
if PROXY_CLIENT_AVAILABLE and USE_PROXY_ROTATION:
    proxy_client = ProxyRotationClient(
        base_url=PROXY_ROTATION_URL,
        timeout=5.0,
        enabled=True,
    )
    log.info(f"Proxy rotation enabled, connecting to {PROXY_ROTATION_URL}")


class CrawlRequest(BaseModel):
    url: str
    js_render: bool = False
    wait_for: Optional[str] = None  # CSS selector to wait for (optional)
    use_proxy: bool = True  # Whether to use proxy rotation


class CrawlResponse(BaseModel):
    url: str
    markdown: Optional[str] = None
    html: Optional[str] = None
    status: str
    error: Optional[str] = None
    proxy_used: Optional[str] = None
    latency_ms: Optional[int] = None


class BatchCrawlRequest(BaseModel):
    """ë°°ì¹˜ í¬ë¡¤ë§ ìš”ì²­"""

    urls: List[str] = Field(
        ..., min_length=1, max_length=50, description="í¬ë¡¤ë§í•  URL ëª©ë¡ (ìµœëŒ€ 50ê°œ)"
    )
    js_render: bool = Field(default=False, description="JavaScript ë Œë”ë§ ì—¬ë¶€")
    wait_for: Optional[str] = Field(default=None, description="ëŒ€ê¸°í•  CSS ì…€ë ‰í„°")
    extract_links: bool = Field(default=False, description="ë§í¬ ì¶”ì¶œ ì—¬ë¶€")
    use_proxy: bool = Field(default=True, description="í”„ë¡ì‹œ ë¡œí…Œì´ì…˜ ì‚¬ìš© ì—¬ë¶€")


class BatchCrawlResult(BaseModel):
    """ë°°ì¹˜ í¬ë¡¤ë§ ê²°ê³¼"""

    batch_id: str
    total: int
    completed: int
    failed: int
    status: str  # "processing", "completed", "partial"
    results: List[CrawlResponse]


@app.get("/health")
@app.head("/health")
async def health():
    proxy_healthy = False
    if proxy_client:
        proxy_healthy = await proxy_client.health_check()

    return {
        "status": "ok",
        "service": "crawl-worker",
        "version": "0.3.0",
        "crawl4ai_available": AsyncWebCrawler is not None,
        "max_concurrent": MAX_CONCURRENT_CRAWLS,
        "proxy_rotation_enabled": USE_PROXY_ROTATION,
        "proxy_service_healthy": proxy_healthy,
    }


async def get_proxy_for_crawl(
    use_proxy: bool = True,
) -> tuple[Optional[str], Optional[str]]:
    """
    Get a proxy URL for crawling.

    Returns:
        Tuple of (proxy_url, proxy_id) or (None, None) if no proxy is used
    """
    if not use_proxy or not proxy_client:
        return None, None

    try:
        proxy_info = await proxy_client.get_next_proxy()
        if proxy_info:
            return proxy_info.get_proxy_url(), proxy_info.id
    except Exception as e:
        log.warning(f"Failed to get proxy: {e}")

    return None, None


async def record_proxy_result(
    proxy_id: Optional[str], success: bool, latency_ms: int = 0, error: str = ""
):
    """Record the result of a proxy-enabled request."""
    if not proxy_id or not proxy_client:
        return

    try:
        if success:
            await proxy_client.record_success(proxy_id, latency_ms)
        else:
            await proxy_client.record_failure(proxy_id, error)
    except Exception as e:
        log.debug(f"Failed to record proxy result: {e}")


async def crawl_single_url(
    url: str,
    js_render: bool = False,
    wait_for: Optional[str] = None,
    use_proxy: bool = True,
) -> CrawlResponse:
    """ë‹¨ì¼ URL í¬ë¡¤ë§ (ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì‹¤í–‰ ì œí•œ, í”„ë¡ì‹œ ë¡œí…Œì´ì…˜ ì§€ì›)"""
    async with crawl_semaphore:
        if AsyncWebCrawler is None:
            return CrawlResponse(url=url, status="FAILED", error="crawl4ai not available")

        max_attempts = 3 if use_proxy else 1
        last_error: str | None = None
        last_proxy_id: str | None = None

        for attempt in range(max_attempts):
            start_time = time.time()
            proxy_url, proxy_id = await get_proxy_for_crawl(use_proxy)
            if proxy_id:
                last_proxy_id = proxy_id

            try:
                # Prepare crawler configuration
                crawler_kwargs = {"verbose": False}
                if proxy_url:
                    crawler_kwargs["proxy"] = proxy_url
                    log.debug(f"Using proxy {proxy_id} for {url} (attempt={attempt + 1}/{max_attempts})")

                async with AsyncWebCrawler(**crawler_kwargs) as crawler:
                    result = await crawler.arun(
                        url=url,
                        js_code=wait_for if js_render else None,
                    )

                    latency_ms = int((time.time() - start_time) * 1000)

                    if not getattr(result, "success", False):
                        error_msg = getattr(result, "error_message", "Unknown error")
                        last_error = error_msg
                        await record_proxy_result(proxy_id, False, latency_ms, error_msg)
                        continue

                    html = getattr(result, "html", None) or ""
                    markdown = getattr(result, "markdown", None) or ""
                    captcha_type = detect_captcha_type(html) or detect_captcha_type(markdown)
                    if captcha_type:
                        last_error = f"CAPTCHA detected: {captcha_type}"
                        if proxy_id and proxy_client:
                            try:
                                await proxy_client.record_captcha(proxy_id, captcha_type=captcha_type)
                            except Exception:
                                pass
                        await record_proxy_result(proxy_id, False, latency_ms, last_error)
                        continue

                    await record_proxy_result(proxy_id, True, latency_ms)
                    return CrawlResponse(
                        url=getattr(result, "url", url),
                        markdown=getattr(result, "markdown", None),
                        html=getattr(result, "html", None),
                        status="SUCCESS",
                        proxy_used=proxy_id,
                        latency_ms=latency_ms,
                    )

            except Exception as e:
                latency_ms = int((time.time() - start_time) * 1000)
                last_error = str(e)
                await record_proxy_result(proxy_id, False, latency_ms, last_error)
                continue

        return CrawlResponse(
            url=url,
            status="FAILED",
            error=last_error or "Crawl failed",
            proxy_used=last_proxy_id,
        )


@app.post("/crawl", response_model=CrawlResponse)
async def crawl_url(request: CrawlRequest):
    """ë‹¨ì¼ URL í¬ë¡¤ë§"""
    if AsyncWebCrawler is None:
        raise HTTPException(
            status_code=500, detail="crawl4ai not available in this environment"
        )

    result = await crawl_single_url(
        request.url,
        request.js_render,
        request.wait_for,
        request.use_proxy,
    )

    if result.status == "FAILED":
        raise HTTPException(status_code=400, detail=result.error or "Crawl failed")

    return result


@app.post("/crawl/batch", response_model=BatchCrawlResult)
async def crawl_batch(request: BatchCrawlRequest):
    """
    ë°°ì¹˜ URL í¬ë¡¤ë§ (ë™ê¸°)

    ì—¬ëŸ¬ URLì„ ë³‘ë ¬ë¡œ í¬ë¡¤ë§í•˜ê³  ëª¨ë“  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ í¬ë¡¤ë§ ìˆ˜ë¥¼ ì œí•œí•©ë‹ˆë‹¤.
    ê° ìš”ì²­ì— ëŒ€í•´ í”„ë¡ì‹œ ë¡œí…Œì´ì…˜ì´ ì ìš©ë©ë‹ˆë‹¤.
    """
    if AsyncWebCrawler is None:
        raise HTTPException(
            status_code=500, detail="crawl4ai not available in this environment"
        )

    batch_id = str(uuid.uuid4())

    # ë³‘ë ¬ í¬ë¡¤ë§ ì‹¤í–‰
    tasks = [
        crawl_single_url(url, request.js_render, request.wait_for, request.use_proxy)
        for url in request.urls
    ]

    results = await asyncio.gather(*tasks, return_exceptions=True)

    # ê²°ê³¼ ì •ë¦¬
    crawl_results: List[CrawlResponse] = []
    completed = 0
    failed = 0

    for i, result in enumerate(results):
        if isinstance(result, Exception):
            crawl_results.append(
                CrawlResponse(url=request.urls[i], status="FAILED", error=str(result))
            )
            failed += 1
        elif isinstance(result, CrawlResponse):
            crawl_results.append(result)
            if result.status == "SUCCESS":
                completed += 1
            else:
                failed += 1
        else:
            crawl_results.append(
                CrawlResponse(
                    url=request.urls[i], status="FAILED", error="Unknown error"
                )
            )
            failed += 1

    status = "completed" if failed == 0 else ("partial" if completed > 0 else "failed")

    return BatchCrawlResult(
        batch_id=batch_id,
        total=len(request.urls),
        completed=completed,
        failed=failed,
        status=status,
        results=crawl_results,
    )


@app.post("/crawl/batch/async")
async def crawl_batch_async(
    request: BatchCrawlRequest, background_tasks: BackgroundTasks
):
    """
    ë¹„ë™ê¸° ë°°ì¹˜ í¬ë¡¤ë§

    ì¦‰ì‹œ batch_idë¥¼ ë°˜í™˜í•˜ê³  ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    ê²°ê³¼ëŠ” GET /crawl/batch/{batch_id}ë¡œ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    if AsyncWebCrawler is None:
        raise HTTPException(
            status_code=500, detail="crawl4ai not available in this environment"
        )

    batch_id = str(uuid.uuid4())

    # ì´ˆê¸° ìƒíƒœ ì €ì¥
    batch_results[batch_id] = {
        "batch_id": batch_id,
        "total": len(request.urls),
        "completed": 0,
        "failed": 0,
        "status": "processing",
        "results": [],
    }

    async def run_batch():
        tasks = [
            crawl_single_url(
                url, request.js_render, request.wait_for, request.use_proxy
            )
            for url in request.urls
        ]

        results = await asyncio.gather(*tasks, return_exceptions=True)

        crawl_results = []
        completed = 0
        failed = 0

        for i, result in enumerate(results):
            if isinstance(result, Exception):
                crawl_results.append(
                    {"url": request.urls[i], "status": "FAILED", "error": str(result)}
                )
                failed += 1
            elif isinstance(result, CrawlResponse):
                crawl_results.append(result.model_dump())
                if result.status == "SUCCESS":
                    completed += 1
                else:
                    failed += 1

        status = (
            "completed" if failed == 0 else ("partial" if completed > 0 else "failed")
        )

        batch_results[batch_id] = {
            "batch_id": batch_id,
            "total": len(request.urls),
            "completed": completed,
            "failed": failed,
            "status": status,
            "results": crawl_results,
        }

    background_tasks.add_task(run_batch)

    return {
        "batch_id": batch_id,
        "total": len(request.urls),
        "status": "processing",
        "message": "ë°°ì¹˜ í¬ë¡¤ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. GET /crawl/batch/{batch_id}ë¡œ ê²°ê³¼ë¥¼ ì¡°íšŒí•˜ì„¸ìš”.",
    }


@app.get("/crawl/batch/{batch_id}")
async def get_batch_result(batch_id: str):
    """ë°°ì¹˜ í¬ë¡¤ë§ ê²°ê³¼ ì¡°íšŒ"""
    if batch_id not in batch_results:
        raise HTTPException(status_code=404, detail="Batch not found")

    return batch_results[batch_id]


@app.delete("/crawl/batch/{batch_id}")
async def delete_batch_result(batch_id: str):
    """ë°°ì¹˜ í¬ë¡¤ë§ ê²°ê³¼ ì‚­ì œ"""
    if batch_id in batch_results:
        del batch_results[batch_id]
        return {"status": "deleted", "batch_id": batch_id}
    raise HTTPException(status_code=404, detail="Batch not found")


@app.get("/proxy/stats")
async def get_proxy_stats():
    """í”„ë¡ì‹œ í’€ í†µê³„ ì¡°íšŒ"""
    if not proxy_client:
        return {"error": "Proxy rotation not enabled"}

    stats = await proxy_client.get_pool_stats()
    return stats or {"error": "Failed to get stats"}


@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup on shutdown"""
    if proxy_client:
        await proxy_client.close()

```

---

## backend/autonomous-crawler-service/crawl-worker/state_store.py

```py
"""
State Storage Module for crawl-worker Service

Provides persistent storage for batch crawl results using Redis.
Falls back to in-memory storage if Redis is unavailable.
"""

import os
import json
import asyncio
from datetime import datetime
from typing import Optional, Dict, Any, List

# Redis configuration from environment
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/2")
REDIS_PREFIX = os.getenv("REDIS_PREFIX", "crawl_worker")
RESULT_TTL_HOURS = int(os.getenv("RESULT_TTL_HOURS", "24"))  # Results expire after 24 hours


class StateStore:
    """Persistent state storage with Redis backend."""
    
    def __init__(self):
        self._redis = None
        self._memory_store: Dict[str, Any] = {}
        self._using_redis = False
        self._lock = asyncio.Lock()
    
    async def connect(self) -> bool:
        """Connect to Redis."""
        try:
            import redis.asyncio as redis
            
            self._redis = redis.from_url(
                REDIS_URL,
                encoding="utf-8",
                decode_responses=True,
                socket_connect_timeout=5,
                socket_timeout=5,
            )
            
            await self._redis.ping()
            self._using_redis = True
            return True
            
        except ImportError:
            self._using_redis = False
            return False
        except Exception:
            self._using_redis = False
            return False
    
    async def disconnect(self):
        """Close Redis connection."""
        if self._redis:
            await self._redis.close()
            self._redis = None
            self._using_redis = False
    
    def _key(self, batch_id: str) -> str:
        """Generate Redis key."""
        return f"{REDIS_PREFIX}:batch:{batch_id}"
    
    async def save_batch(self, batch_id: str, data: Dict[str, Any]) -> bool:
        """Save batch result."""
        async with self._lock:
            try:
                data_json = json.dumps(data, default=str)
                self._memory_store[batch_id] = json.loads(data_json)
                
                if self._using_redis and self._redis:
                    ttl_seconds = RESULT_TTL_HOURS * 60 * 60
                    await self._redis.set(self._key(batch_id), data_json, ex=ttl_seconds)
                
                return True
            except Exception:
                return False
    
    async def load_batch(self, batch_id: str) -> Optional[Dict[str, Any]]:
        """Load batch result."""
        if batch_id in self._memory_store:
            return self._memory_store[batch_id]
        
        if self._using_redis and self._redis:
            try:
                data_json = await self._redis.get(self._key(batch_id))
                if data_json:
                    data = json.loads(data_json)
                    self._memory_store[batch_id] = data
                    return data
            except Exception:
                pass
        
        return None
    
    async def delete_batch(self, batch_id: str) -> bool:
        """Delete batch result."""
        async with self._lock:
            self._memory_store.pop(batch_id, None)
            if self._using_redis and self._redis:
                try:
                    await self._redis.delete(self._key(batch_id))
                except Exception:
                    pass
            return True
    
    @property
    def is_redis_connected(self) -> bool:
        return self._using_redis
    
    def get_memory_store(self) -> Dict[str, Any]:
        return self._memory_store


# Singleton instance
_store: Optional[StateStore] = None


async def get_state_store() -> StateStore:
    """Get or create the singleton StateStore instance."""
    global _store
    if _store is None:
        _store = StateStore()
        await _store.connect()
    return _store

```

---

## backend/autonomous-crawler-service/src/__init__.py

```py
"""Autonomous Crawler Service package."""

```

---

## backend/autonomous-crawler-service/src/api/__init__.py

```py
"""REST API module for autonomous-crawler-service."""

# Lazy imports to avoid circular dependencies
__all__ = ["app", "create_app", "SSEManager", "SSEEventType"]


def __getattr__(name):
    """Lazy import for circular dependency prevention."""
    if name == "app":
        from src.api.server import app

        return app
    elif name == "create_app":
        from src.api.server import create_app

        return create_app
    elif name == "SSEManager":
        from src.api.sse import SSEManager

        return SSEManager
    elif name == "SSEEventType":
        from src.api.sse import SSEEventType

        return SSEEventType
    raise AttributeError(f"module {__name__!r} has no attribute {name!r}")

```

---

## backend/autonomous-crawler-service/src/api/server.py

```py
"""
FastAPI REST Server for autonomous-crawler-service.

browser-agentì˜ REST API + SSE ê¸°ëŠ¥ì„ autonomous-crawler-serviceì— í†µí•©.
ê¸°ì¡´ Kafka ê¸°ë°˜ ì•„í‚¤í…ì²˜ì™€ ë³‘í–‰ ìš´ì˜ë©ë‹ˆë‹¤.
"""

import asyncio
import hashlib
import os
import uuid
from contextlib import asynccontextmanager
from datetime import datetime, timezone
from enum import Enum
from typing import Any, Dict, List, Optional

import httpx
import structlog
from fastapi import BackgroundTasks, FastAPI, HTTPException, Query, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field, HttpUrl

from src.api.sse import (
    SSEEvent,
    SSEEventType,
    SSEManager,
    get_sse_manager,
    sse_event_generator,
)
from src.config import Settings, get_settings
from src.crawler import AutonomousCrawlerAgent
from src.state.store import StateStore, get_state_store, close_state_store

# MCP Integration
from src.mcp.router import router as mcp_router

# ML Integration
from src.ml.router import router as ml_router
from src.ml.orchestrator import init_ml_orchestrator, get_ml_orchestrator

# Authentication
from src.auth.middleware import get_current_user, require_auth, require_admin, require_operator
from src.auth.jwt_utils import JWTPayload

logger = structlog.get_logger(__name__)


# ========================================
# Configuration
# ========================================


class LLMProvider(str, Enum):
    """ì§€ì›í•˜ëŠ” LLM Provider"""

    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    GOOGLE = "google"
    OPENROUTER = "openrouter"
    OLLAMA = "ollama"
    AZURE = "azure"
    CUSTOM = "custom"


class APIConfig:
    """API ì„œë²„ ì„¤ì •"""

    # í™˜ê²½ë³€ìˆ˜ì—ì„œ LLM API í‚¤ ë¡œë“œ (fallback)
    OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY", "")
    ANTHROPIC_API_KEY: str = os.getenv("ANTHROPIC_API_KEY", "")
    GOOGLE_API_KEY: str = os.getenv("GOOGLE_API_KEY", "")

    DEFAULT_LLM_PROVIDER: str = os.getenv("DEFAULT_LLM_PROVIDER", "openai")
    DEFAULT_MODEL: str = os.getenv("DEFAULT_MODEL", "gpt-4o")

    MAX_CONCURRENT_SESSIONS: int = int(os.getenv("MAX_CONCURRENT_SESSIONS", "3"))
    DEFAULT_TIMEOUT_SEC: int = int(os.getenv("DEFAULT_TIMEOUT_SEC", "120"))

    # DB ê¸°ë°˜ LLM Provider ì‚¬ìš© ì—¬ë¶€
    USE_DB_PROVIDERS: bool = os.getenv("USE_DB_PROVIDERS", "true").lower() == "true"
    COLLECTOR_SERVICE_URL: str = os.getenv("COLLECTOR_SERVICE_URL", "http://collector:8002")
    WEB_CRAWLER_URL: str = os.getenv("WEB_CRAWLER_URL", "http://web-crawler:11235")
    WEB_CRAWLER_API_TOKEN: str = os.getenv(
        "WEB_CRAWLER_API_TOKEN", os.getenv("CRAWL4AI_API_TOKEN", "")
    )


api_config = APIConfig()


# ========================================
# Request/Response Models (Pydantic)
# ========================================


class CrawlMethod(str, Enum):
    """í¬ë¡¤ë§ ë°©ì‹"""

    BROWSER_AGENT = "browser_agent"
    SIMPLE_FETCH = "simple_fetch"
    JS_RENDER = "js_render"


class AgentTask(BaseModel):
    """ë¸Œë¼ìš°ì € ì—ì´ì „íŠ¸ íƒœìŠ¤í¬ ìš”ì²­"""

    url: HttpUrl = Field(..., description="í¬ë¡¤ë§ ëŒ€ìƒ URL")
    task: str = Field(
        ...,
        description="ìì—°ì–´ íƒœìŠ¤í¬ (ì˜ˆ: 'ì´ í˜ì´ì§€ì—ì„œ ë‰´ìŠ¤ ì œëª©ê³¼ ë³¸ë¬¸ì„ ì¶”ì¶œí•´ì¤˜')",
        min_length=5,
        max_length=2000,
    )

    # LLM ì„¤ì •
    llm_provider: LLMProvider = Field(default=LLMProvider.OPENAI)
    model: Optional[str] = Field(default=None, description="ëª¨ë¸ëª… (ê¸°ë³¸: gpt-4o)")

    # í¬ë¡¤ë§ ì˜µì…˜
    screenshot: bool = Field(default=False, description="ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜ ì—¬ë¶€")
    extract_links: bool = Field(default=True, description="í˜ì´ì§€ ë‚´ ë§í¬ ì¶”ì¶œ")
    max_steps: int = Field(default=10, ge=1, le=50, description="ìµœëŒ€ ì—ì´ì „íŠ¸ ìŠ¤í…")
    timeout_sec: int = Field(default=120, ge=30, le=600)

    # URL ì €ì¥ ì˜µì…˜
    auto_save_url: bool = Field(default=True, description="ì¶”ì¶œëœ URL ìë™ ì €ì¥")
    source_category: str = Field(default="news", description="ì†ŒìŠ¤ ì¹´í…Œê³ ë¦¬")


class ExtractedLink(BaseModel):
    """ì¶”ì¶œëœ ë§í¬"""

    url: str
    text: Optional[str] = None
    context: Optional[str] = None


class AgentAction(BaseModel):
    """ì—ì´ì „íŠ¸ê°€ ìˆ˜í–‰í•œ ì•¡ì…˜"""

    step: int
    action_type: str
    target: Optional[str] = None
    value: Optional[str] = None
    timestamp: datetime


class AgentTaskResult(BaseModel):
    """ì—ì´ì „íŠ¸ íƒœìŠ¤í¬ ê²°ê³¼"""

    task_id: str
    url: str
    status: str  # success, failed, timeout

    # ì¶”ì¶œ ê²°ê³¼
    extracted_data: Optional[Dict[str, Any]] = None
    extracted_text: Optional[str] = None
    extracted_links: List[ExtractedLink] = []

    # ë©”íƒ€ë°ì´í„°
    content_hash: Optional[str] = None
    page_title: Optional[str] = None

    # ì—ì´ì „íŠ¸ ë©”íŠ¸ë¦­
    steps_taken: int = 0
    actions: List[AgentAction] = []
    tokens_used: Optional[int] = None
    duration_ms: int = 0

    # ìŠ¤í¬ë¦°ìƒ· (Base64)
    screenshot_base64: Optional[str] = None

    # ì—ëŸ¬ ì •ë³´
    error_message: Optional[str] = None


class BatchCrawlRequest(BaseModel):
    """ë°°ì¹˜ í¬ë¡¤ë§ ìš”ì²­"""

    urls: List[HttpUrl] = Field(..., min_length=1, max_length=20)
    task: str = Field(..., description="ê³µí†µ íƒœìŠ¤í¬")
    llm_provider: LLMProvider = Field(default=LLMProvider.OPENAI)
    model: Optional[str] = None
    auto_save_url: bool = True


class ChatMessage(BaseModel):
    """ì±„íŒ… ë©”ì‹œì§€"""

    role: str = Field(..., description="ë©”ì‹œì§€ ì—­í• : user, assistant, system")
    content: str = Field(..., description="ë©”ì‹œì§€ ë‚´ìš©")


class ChatRequest(BaseModel):
    """ì±„íŒ… ìš”ì²­"""

    messages: List[ChatMessage] = Field(..., description="ëŒ€í™” ì´ë ¥")
    llm_provider: LLMProvider = Field(default=LLMProvider.OPENAI)
    model: Optional[str] = Field(default=None, description="ì‚¬ìš©í•  ëª¨ë¸")
    stream: bool = Field(default=False, description="ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì—¬ë¶€")
    context_url: Optional[str] = Field(default=None, description="ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•  URL")


class ChatResponse(BaseModel):
    """ì±„íŒ… ì‘ë‹µ"""

    message: str
    provider: str
    model: str
    tokens_used: Optional[int] = None


# ========================================
# In-Memory Storage (deprecated - kept for backward compat, StateStore is primary)
# ========================================

# Note: task_results dict is now managed by StateStore with Redis persistence
# This variable is kept for quick reference but StateStore should be used
session_semaphore: Optional[asyncio.Semaphore] = None


# ========================================
# Provider Config Fetching (DB-based)
# ========================================

_provider_config_cache: Dict[str, dict] = {}
_provider_cache_ttl: Dict[str, datetime] = {}
PROVIDER_CACHE_TTL_SECONDS = 300


async def fetch_provider_config(provider_name: str) -> Optional[Dict[str, Any]]:
    """DBì—ì„œ LLM provider ì„¤ì • ê°€ì ¸ì˜¤ê¸° (ìºì‹œ ì ìš©)"""
    cache_key = provider_name.lower()
    now = datetime.now(timezone.utc)

    if cache_key in _provider_config_cache:
        cache_time = _provider_cache_ttl.get(cache_key)
        if cache_time and (now - cache_time).total_seconds() < PROVIDER_CACHE_TTL_SECONDS:
            return _provider_config_cache[cache_key]

    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(
                f"{api_config.COLLECTOR_SERVICE_URL}/api/v1/llm-providers/config/{provider_name}"
            )
            if response.status_code == 200:
                provider_config = response.json()
                _provider_config_cache[cache_key] = provider_config
                _provider_cache_ttl[cache_key] = now
                return provider_config
    except Exception as e:
        logger.warning("Failed to fetch provider config", provider=provider_name, error=str(e))

    return None


def clear_provider_cache():
    """Provider ìºì‹œ ì‚­ì œ"""
    _provider_config_cache.clear()
    _provider_cache_ttl.clear()


# ========================================
# LLM Factory
# ========================================


def get_llm_from_env(provider: LLMProvider, model: Optional[str] = None):
    """í™˜ê²½ë³€ìˆ˜/Settings ê¸°ë°˜ìœ¼ë¡œ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

    Note: SettingsëŠ” Consulì—ì„œ ë¡œë“œëœ ì„¤ì •ì„ í¬í•¨í•©ë‹ˆë‹¤.
    """
    from langchain_openai import ChatOpenAI
    from langchain_anthropic import ChatAnthropic
    from langchain_google_genai import ChatGoogleGenerativeAI

    # Get settings (includes Consul-loaded config)
    settings = get_settings()

    if provider == LLMProvider.OPENAI:
        return ChatOpenAI(
            model=model or settings.llm.openai_model or "gpt-4o",
            api_key=settings.llm.openai_api_key or api_config.OPENAI_API_KEY,
            temperature=0.1,
        )
    elif provider == LLMProvider.ANTHROPIC:
        return ChatAnthropic(
            model=model or settings.llm.anthropic_model or "claude-3-5-sonnet-20241022",
            api_key=settings.llm.anthropic_api_key or api_config.ANTHROPIC_API_KEY,
            temperature=0.1,
        )
    elif provider == LLMProvider.GOOGLE:
        return ChatGoogleGenerativeAI(
            model=model or "gemini-1.5-pro",
            google_api_key=api_config.GOOGLE_API_KEY,
            temperature=0.1,
        )
    elif provider == LLMProvider.OPENROUTER:
        return ChatOpenAI(
            model=model or settings.llm.openrouter_model or "openai/gpt-4o",
            api_key=settings.llm.openrouter_api_key,
            base_url=settings.llm.openrouter_base_url or "https://openrouter.ai/api/v1",
            temperature=0.1,
        )
    elif provider == LLMProvider.OLLAMA:
        try:
            from langchain_ollama import ChatOllama

            return ChatOllama(
                model=model or settings.llm.ollama_model or "llama3.1",
                base_url=settings.llm.ollama_base_url or "http://localhost:11434",
                temperature=0.1,
            )
        except ImportError:
            raise ValueError("Ollama support requires langchain-ollama package")
    elif provider == LLMProvider.AZURE:
        try:
            from langchain_openai import AzureChatOpenAI

            azure_endpoint = settings.llm.azure_endpoint or os.getenv("AZURE_OPENAI_ENDPOINT", "")
            azure_api_key = settings.llm.azure_api_key or os.getenv("AZURE_OPENAI_API_KEY", "")
            azure_deployment = (
                model
                or settings.llm.azure_deployment_name
                or os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME", "gpt-4o")
            )
            azure_api_version = settings.llm.azure_api_version or os.getenv(
                "AZURE_OPENAI_API_VERSION", "2024-02-15-preview"
            )

            if not azure_endpoint:
                raise ValueError(
                    "Azure OpenAI requires LLM_AZURE_ENDPOINT or AZURE_OPENAI_ENDPOINT"
                )
            if not azure_api_key:
                raise ValueError("Azure OpenAI requires LLM_AZURE_API_KEY or AZURE_OPENAI_API_KEY")

            return AzureChatOpenAI(
                azure_endpoint=azure_endpoint,
                azure_deployment=azure_deployment,
                api_key=azure_api_key,
                api_version=azure_api_version,
                temperature=0.1,
            )
        except ImportError:
            raise ValueError("Azure OpenAI support requires langchain-openai package")
    elif provider == LLMProvider.CUSTOM:
        custom_base_url = settings.llm.custom_base_url or os.getenv("CUSTOM_LLM_BASE_URL", "")
        if not custom_base_url:
            raise ValueError("Custom provider requires CUSTOM_LLM_BASE_URL or LLM_CUSTOM_BASE_URL")

        custom_request_format = settings.llm.custom_request_format
        custom_response_path = settings.llm.custom_response_path

        # If custom request format is provided, use CustomRESTAPIClient for non-OpenAI-compatible APIs
        if custom_request_format:
            from src.crawler.agent import CustomRESTAPIClient

            return CustomRESTAPIClient(
                base_url=custom_base_url,
                api_key=settings.llm.custom_api_key or os.getenv("CUSTOM_LLM_API_KEY", ""),
                model=model or settings.llm.custom_model or "default",
                request_format=custom_request_format,
                response_path=custom_response_path or "reply",
                custom_headers=settings.llm.custom_headers or "{}",
                temperature=0.1,
            )

        # Fallback to OpenAI-compatible format
        return ChatOpenAI(
            model=model or settings.llm.custom_model or "default",
            api_key=settings.llm.custom_api_key or os.getenv("CUSTOM_LLM_API_KEY", "none"),
            base_url=custom_base_url,
            temperature=0.1,
        )
    else:
        raise ValueError(f"Unsupported LLM provider: {provider}")


def get_llm_from_config(provider_config: Dict[str, Any], model_override: Optional[str] = None):
    """DB ì„¤ì • ê¸°ë°˜ìœ¼ë¡œ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    from langchain_openai import ChatOpenAI
    from langchain_anthropic import ChatAnthropic
    from langchain_google_genai import ChatGoogleGenerativeAI

    provider_type = provider_config.get("providerType", "").upper()
    api_key = provider_config.get("apiKey", "")
    base_url = provider_config.get("baseUrl")
    default_model = model_override or provider_config.get("defaultModel", "")
    extra_config = provider_config.get("config", {})
    temperature = extra_config.get("temperature", 0.1)

    if provider_type == "OPENAI":
        kwargs = {
            "model": default_model or "gpt-4o",
            "api_key": api_key,
            "temperature": temperature,
        }
        if base_url and base_url != "https://api.openai.com/v1":
            kwargs["base_url"] = base_url
        return ChatOpenAI(**kwargs)

    elif provider_type == "ANTHROPIC":
        return ChatAnthropic(
            model=default_model or "claude-3-5-sonnet-20241022",
            api_key=api_key,
            temperature=temperature,
        )

    elif provider_type == "GOOGLE":
        return ChatGoogleGenerativeAI(
            model=default_model or "gemini-1.5-pro",
            google_api_key=api_key,
            temperature=temperature,
        )

    elif provider_type == "OPENROUTER":
        return ChatOpenAI(
            model=default_model or "openai/gpt-4o",
            api_key=api_key,
            base_url=base_url or "https://openrouter.ai/api/v1",
            temperature=temperature,
        )

    elif provider_type == "OLLAMA":
        try:
            from langchain_ollama import ChatOllama

            return ChatOllama(
                model=default_model or "llama3.1",
                base_url=base_url or "http://localhost:11434",
                temperature=temperature,
            )
        except ImportError:
            raise ValueError("Ollama support requires langchain-ollama package")

    elif provider_type == "AZURE":
        try:
            from langchain_openai import AzureChatOpenAI

            azure_endpoint = extra_config.get("endpoint") or base_url
            deployment_name = extra_config.get("deployment_name") or default_model
            api_version = extra_config.get("api_version", "2024-02-15-preview")

            if not azure_endpoint:
                raise ValueError("Azure provider requires endpoint")
            if not deployment_name:
                raise ValueError("Azure provider requires deployment_name")

            return AzureChatOpenAI(
                azure_endpoint=azure_endpoint,
                azure_deployment=deployment_name,
                api_key=api_key,
                api_version=api_version,
                temperature=temperature,
            )
        except ImportError:
            raise ValueError("Azure OpenAI support requires langchain-openai package")

    elif provider_type == "CUSTOM":
        if not base_url:
            raise ValueError("Custom provider requires base_url")
        return ChatOpenAI(
            model=default_model or "default",
            api_key=api_key or "none",
            base_url=base_url,
            temperature=temperature,
        )

    else:
        raise ValueError(f"Unsupported provider type: {provider_type}")


async def get_llm(provider: LLMProvider, model: Optional[str] = None):
    """LLM ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸° - DB ìš°ì„ , í™˜ê²½ë³€ìˆ˜ fallback"""
    provider_name = provider.value

    if api_config.USE_DB_PROVIDERS:
        provider_config = await fetch_provider_config(provider_name)
        if provider_config:
            try:
                return get_llm_from_config(provider_config, model)
            except Exception as e:
                logger.warning(
                    "DB provider failed, using fallback",
                    provider=provider_name,
                    error=str(e),
                )

    return get_llm_from_env(provider, model)


# ========================================
# Core Agent Logic
# ========================================


async def run_browser_agent(
    request: AgentTask,
    settings: Settings,
    sse_manager: SSEManager,
) -> AgentTaskResult:
    """
    browser-use ì—ì´ì „íŠ¸ ì‹¤í–‰.

    ê¸°ì¡´ autonomous-crawler-serviceì˜ AutonomousCrawlerAgentë¥¼ í™œìš©í•˜ë©´ì„œ
    browser-agentì˜ API ì¸í„°í˜ì´ìŠ¤ì™€ SSE ì´ë²¤íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    """
    global session_semaphore

    task_id = str(uuid.uuid4())
    start_time = datetime.now(timezone.utc)
    actions_log: List[AgentAction] = []

    logger.info(
        "Agent task started",
        task_id=task_id,
        url=str(request.url),
        task=request.task[:100],
    )

    # SSE: ì—ì´ì „íŠ¸ ì‹œì‘ ì´ë²¤íŠ¸
    await sse_manager.send_agent_event(
        SSEEventType.AGENT_START,
        task_id=task_id,
        url=str(request.url),
        message=f"í¬ë¡¤ë§ ì‹œì‘: {request.task[:50]}...",
        provider=request.llm_provider.value,
    )

    try:
        if session_semaphore is None:
            session_semaphore = asyncio.Semaphore(api_config.MAX_CONCURRENT_SESSIONS)

        async with session_semaphore:
            # AutonomousCrawlerAgent ì‚¬ìš©
            agent = AutonomousCrawlerAgent(settings)

            # ë‹¨ìˆœ URL í¬ë¡¤ë§ + AI ë¶„ì„
            # smart_search ë©”ì„œë“œ í™œìš©
            crawl_result = await agent.crawl_with_camoufox(
                url=str(request.url),
                extract_content=True,
                wait_for_cloudflare=True,
            )

            if crawl_result.get("error"):
                raise Exception(crawl_result["error"])

            extracted_text = crawl_result.get("text", "")
            page_title = crawl_result.get("title", "")
            extracted_links: List[ExtractedLink] = []

            # ë§í¬ ì¶”ì¶œ (ì˜µì…˜)
            if request.extract_links:
                links = crawl_result.get("links", [])
                extracted_links = [
                    ExtractedLink(url=link.get("url", ""), text=link.get("text", ""))
                    for link in links[:50]
                    if link.get("url", "").startswith("http")
                ]

            content_hash = (
                hashlib.sha256((extracted_text or "").encode()).hexdigest()
                if extracted_text
                else None
            )

            duration_ms = int((datetime.now(timezone.utc) - start_time).total_seconds() * 1000)

            task_result = AgentTaskResult(
                task_id=task_id,
                url=str(request.url),
                status="success",
                extracted_text=extracted_text,
                extracted_links=extracted_links,
                content_hash=content_hash,
                page_title=page_title,
                steps_taken=1,
                actions=actions_log,
                duration_ms=duration_ms,
            )

            # ë¸Œë¼ìš°ì € ì •ë¦¬
            await agent.close()

            logger.info(
                "Agent task completed",
                task_id=task_id,
                duration_ms=duration_ms,
                links_count=len(extracted_links),
            )

            # SSE: ì—ì´ì „íŠ¸ ì™„ë£Œ ì´ë²¤íŠ¸
            await sse_manager.send_agent_event(
                SSEEventType.AGENT_COMPLETE,
                task_id=task_id,
                url=str(request.url),
                message=f"í¬ë¡¤ë§ ì™„ë£Œ: {len(extracted_links)}ê°œ ë§í¬ ë°œê²¬",
                duration_ms=duration_ms,
                links_count=len(extracted_links),
                has_content=bool(extracted_text),
            )

            return task_result

    except asyncio.TimeoutError:
        duration_ms = int((datetime.now(timezone.utc) - start_time).total_seconds() * 1000)
        logger.error("Agent task timeout", task_id=task_id, timeout=request.timeout_sec)

        await sse_manager.send_agent_event(
            SSEEventType.AGENT_ERROR,
            task_id=task_id,
            url=str(request.url),
            message=f"íƒ€ì„ì•„ì›ƒ: {request.timeout_sec}ì´ˆ ì´ˆê³¼",
            error_type="timeout",
        )

        return AgentTaskResult(
            task_id=task_id,
            url=str(request.url),
            status="timeout",
            error_message=f"íƒœìŠ¤í¬ê°€ {request.timeout_sec}ì´ˆ ë‚´ì— ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            duration_ms=duration_ms,
        )

    except Exception as e:
        duration_ms = int((datetime.now(timezone.utc) - start_time).total_seconds() * 1000)
        logger.error("Agent task failed", task_id=task_id, error=str(e))

        await sse_manager.send_agent_event(
            SSEEventType.AGENT_ERROR,
            task_id=task_id,
            url=str(request.url),
            message=f"ì—ëŸ¬: {str(e)[:100]}",
            error_type="exception",
            error_detail=str(e),
        )

        return AgentTaskResult(
            task_id=task_id,
            url=str(request.url),
            status="failed",
            error_message=str(e),
            duration_ms=duration_ms,
        )


# ========================================
# FastAPI App
# ========================================


@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ìˆ˜ëª… ì£¼ê¸° ê´€ë¦¬"""
    logger.info("Starting autonomous-crawler REST API server")

    # Initialize StateStore (connects to Redis)
    state_store = await get_state_store()
    app.state.state_store = state_store
    logger.info(
        "StateStore initialized",
        using_redis=state_store.is_redis_connected,
        task_count=state_store.task_count,
    )

    # Initialize ML Orchestrator
    try:
        ml_orchestrator = await init_ml_orchestrator()
        app.state.ml_orchestrator = ml_orchestrator
        logger.info("ML Orchestrator initialized")
    except Exception as e:
        logger.warning("ML Orchestrator initialization failed (non-critical)", error=str(e))
        app.state.ml_orchestrator = None

    yield

    # Cleanup ML Orchestrator
    if hasattr(app.state, "ml_orchestrator") and app.state.ml_orchestrator:
        await app.state.ml_orchestrator.close()
        logger.info("ML Orchestrator closed")

    # Cleanup StateStore
    logger.info("Shutting down autonomous-crawler REST API server")
    await close_state_store()


def create_app(settings: Settings | None = None) -> FastAPI:
    """FastAPI ì•± ìƒì„±"""
    if settings is None:
        settings = get_settings()

    app = FastAPI(
        title="Autonomous Crawler Service API",
        description="AI ê¸°ë°˜ ììœ¨ ë¸Œë¼ìš°ì € í¬ë¡¤ëŸ¬ - browser-agent í†µí•© REST API",
        version="0.2.0",
        lifespan=lifespan,
    )

    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # Store settings in app state
    app.state.settings = settings
    app.state.sse_manager = get_sse_manager()

    # Register routes
    register_routes(app)

    return app


def register_routes(app: FastAPI):
    """API ë¼ìš°íŠ¸ ë“±ë¡"""

    # MCP Add-on Router ë“±ë¡
    app.include_router(mcp_router)

    # ML Analysis Router ë“±ë¡
    app.include_router(ml_router)

    @app.get("/health")
    @app.head("/health")
    async def health(req: Request):
        """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
        state_store: StateStore = req.app.state.state_store
        store_stats = await state_store.get_stats()

        # Check ML orchestrator status
        ml_available = (
            hasattr(req.app.state, "ml_orchestrator") and req.app.state.ml_orchestrator is not None
        )

        return {
            "status": "ok",
            "service": "autonomous-crawler-service",
            "api_version": "0.2.0",
            "features": {
                "rest_api": True,
                "sse_events": True,
                "kafka_consumer": True,
                "camoufox": True,
                "captcha_bypass": True,
                "redis_persistence": state_store.is_redis_connected,
                "ml_analysis": ml_available,
            },
            "storage": store_stats,
            "active_sessions": state_store.task_count,
            "max_sessions": api_config.MAX_CONCURRENT_SESSIONS,
        }

    @app.get("/events")
    async def sse_events(request: Request):
        """
        SSE ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ ì—”ë“œí¬ì¸íŠ¸.

        ë¸Œë¼ìš°ì € ì—ì´ì „íŠ¸ì˜ ì‹¤ì‹œê°„ ìƒíƒœë¥¼ êµ¬ë…í•©ë‹ˆë‹¤.
        """
        sse_manager: SSEManager = request.app.state.sse_manager
        client_id, queue = await sse_manager.connect()

        return StreamingResponse(
            sse_event_generator(client_id, queue, sse_manager),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",
                "Access-Control-Allow-Origin": "*",
            },
        )

    @app.get("/events/clients")
    async def get_sse_clients(request: Request):
        """í˜„ì¬ ì—°ê²°ëœ SSE í´ë¼ì´ì–¸íŠ¸ ìˆ˜ ì¡°íšŒ"""
        sse_manager: SSEManager = request.app.state.sse_manager
        return {
            "connected_clients": sse_manager.client_count,
            "client_ids": sse_manager.client_ids,
        }

    @app.get("/providers")
    async def get_available_providers(user: JWTPayload = Depends(require_auth())):
        """ì‚¬ìš© ê°€ëŠ¥í•œ LLM Provider ëª©ë¡ ì¡°íšŒ (ì¸ì¦ í•„ìš” - API í‚¤ ì •ë³´ ë³´í˜¸)"""
        providers = []
        settings = get_settings()

        if settings.llm.openai_api_key or api_config.OPENAI_API_KEY:
            providers.append(
                {
                    "name": "openai",
                    "providerType": "OPENAI",
                    "defaultModel": settings.llm.openai_model or "gpt-4o",
                    "available": True,
                }
            )

        if settings.llm.anthropic_api_key or api_config.ANTHROPIC_API_KEY:
            providers.append(
                {
                    "name": "anthropic",
                    "providerType": "ANTHROPIC",
                    "defaultModel": settings.llm.anthropic_model or "claude-3-5-sonnet-20241022",
                    "available": True,
                }
            )

        if api_config.GOOGLE_API_KEY:
            providers.append(
                {
                    "name": "google",
                    "providerType": "GOOGLE",
                    "defaultModel": "gemini-1.5-pro",
                    "available": True,
                }
            )

        # OpenRouter (Settings ìš°ì„ )
        if settings.llm.openrouter_api_key or os.getenv("OPENROUTER_API_KEY"):
            providers.append(
                {
                    "name": "openrouter",
                    "providerType": "OPENROUTER",
                    "defaultModel": settings.llm.openrouter_model or "openai/gpt-4o",
                    "available": True,
                }
            )

        # Ollama (í•­ìƒ í‘œì‹œ, ë¡œì»¬ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥)
        providers.append(
            {
                "name": "ollama",
                "providerType": "OLLAMA",
                "defaultModel": settings.llm.ollama_model or "llama3.1",
                "available": True,
                "local": True,
            }
        )

        # Azure OpenAI (Settings ìš°ì„ )
        if (settings.llm.azure_api_key and settings.llm.azure_endpoint) or (
            os.getenv("AZURE_OPENAI_API_KEY") and os.getenv("AZURE_OPENAI_ENDPOINT")
        ):
            providers.append(
                {
                    "name": "azure",
                    "providerType": "AZURE",
                    "defaultModel": settings.llm.azure_deployment_name
                    or os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME", "gpt-4o"),
                    "available": True,
                }
            )

        # Custom (Settings ìš°ì„ )
        if settings.llm.custom_base_url or os.getenv("CUSTOM_LLM_BASE_URL"):
            providers.append(
                {
                    "name": "custom",
                    "providerType": "CUSTOM",
                    "defaultModel": settings.llm.custom_model or "default",
                    "available": True,
                }
            )

        return {
            "providers": providers,
            "defaultProvider": providers[0] if providers else None,
            "source": "environment",
            "supportedProviders": [p.value for p in LLMProvider],
        }

    @app.post("/providers/cache/clear")
    async def clear_providers_cache_endpoint():
        """Provider ì„¤ì • ìºì‹œ ì‚­ì œ"""
        clear_provider_cache()
        return {"status": "ok", "message": "Provider cache cleared"}

    @app.post("/providers/test")
    async def test_provider_connection(
        provider: LLMProvider = Query(..., description="í…ŒìŠ¤íŠ¸í•  Provider"),
        model: Optional[str] = Query(default=None, description="í…ŒìŠ¤íŠ¸í•  ëª¨ë¸"),
    ):
        """
        LLM Provider ì—°ê²° í…ŒìŠ¤íŠ¸.

        ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ë³´ë‚´ Providerê°€ ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        """
        import time

        start_time = time.time()

        try:
            llm = await get_llm(provider, model)

            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
            from langchain_core.messages import HumanMessage

            test_message = HumanMessage(content="Say 'Connection successful!' in one line.")
            response = await llm.ainvoke([test_message])

            elapsed_ms = int((time.time() - start_time) * 1000)

            return {
                "status": "success",
                "provider": provider.value,
                "model": model or "default",
                "response": response.content[:100] if response.content else "No response",
                "latency_ms": elapsed_ms,
                "message": "ì—°ê²° ì„±ê³µ",
            }

        except Exception as e:
            elapsed_ms = int((time.time() - start_time) * 1000)
            logger.error("Provider test failed", provider=provider.value, error=str(e))
            return {
                "status": "failed",
                "provider": provider.value,
                "model": model or "default",
                "error": str(e),
                "latency_ms": elapsed_ms,
                "message": f"ì—°ê²° ì‹¤íŒ¨: {str(e)[:100]}",
            }

    @app.get("/providers/{provider}/models")
    async def get_provider_models(
        provider: LLMProvider,
        api_key: Optional[str] = Query(
            default=None, description="API í‚¤ (ì˜µì…˜, ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)"
        ),
        base_url: Optional[str] = Query(default=None, description="Base URL (Ollama, Customìš©)"),
    ):
        """
        íŠ¹ì • LLM Providerì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ì„ ë™ì ìœ¼ë¡œ ì¡°íšŒí•©ë‹ˆë‹¤.

        - OpenAI: /v1/models API í˜¸ì¶œ
        - OpenRouter: /api/v1/models API í˜¸ì¶œ
        - Ollama: /api/tags API í˜¸ì¶œ
        - Anthropic, Google, Azure: ì •ì  ëª©ë¡ ë°˜í™˜ (ê³µì‹ API ì—†ìŒ)
        - Custom: /v1/models ë˜ëŠ” ì •ì  ëª©ë¡
        """
        try:
            async with httpx.AsyncClient(timeout=15.0) as client:
                if provider == LLMProvider.OPENAI:
                    key = api_key or os.getenv("OPENAI_API_KEY", "")
                    if not key:
                        return {
                            "provider": provider.value,
                            "models": _get_static_models("openai"),
                            "source": "static",
                            "message": "API í‚¤ê°€ ì—†ì–´ ì •ì  ëª©ë¡ ë°˜í™˜",
                        }

                    resp = await client.get(
                        "https://api.openai.com/v1/models",
                        headers={"Authorization": f"Bearer {key}"},
                    )
                    if resp.status_code == 200:
                        data = resp.json()
                        # GPT ëª¨ë¸ë§Œ í•„í„°ë§ (chat ëª¨ë¸)
                        models = [
                            {"id": m["id"], "name": m["id"], "owned_by": m.get("owned_by", "")}
                            for m in data.get("data", [])
                            if any(prefix in m["id"] for prefix in ["gpt-", "o1-", "chatgpt-"])
                        ]
                        # ì •ë ¬: gpt-4o ìš°ì„ 
                        models.sort(key=lambda x: (0 if "gpt-4o" in x["id"] else 1, x["id"]))
                        return {
                            "provider": provider.value,
                            "models": models[:20],  # ìµœëŒ€ 20ê°œ
                            "source": "api",
                            "total": len(models),
                        }
                    else:
                        return {
                            "provider": provider.value,
                            "models": _get_static_models("openai"),
                            "source": "static",
                            "error": f"API í˜¸ì¶œ ì‹¤íŒ¨: {resp.status_code}",
                        }

                elif provider == LLMProvider.OPENROUTER:
                    key = api_key or os.getenv("OPENROUTER_API_KEY", "")
                    headers = {}
                    if key:
                        headers["Authorization"] = f"Bearer {key}"

                    resp = await client.get(
                        "https://openrouter.ai/api/v1/models",
                        headers=headers,
                    )
                    if resp.status_code == 200:
                        data = resp.json()
                        models = [
                            {
                                "id": m["id"],
                                "name": m.get("name", m["id"]),
                                "context_length": m.get("context_length"),
                                "pricing": m.get("pricing"),
                            }
                            for m in data.get("data", [])[:50]  # ìƒìœ„ 50ê°œ
                        ]
                        return {
                            "provider": provider.value,
                            "models": models,
                            "source": "api",
                            "total": len(data.get("data", [])),
                        }
                    else:
                        return {
                            "provider": provider.value,
                            "models": _get_static_models("openrouter"),
                            "source": "static",
                            "error": f"API í˜¸ì¶œ ì‹¤íŒ¨: {resp.status_code}",
                        }

                elif provider == LLMProvider.OLLAMA:
                    ollama_url = base_url or os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
                    try:
                        resp = await client.get(f"{ollama_url}/api/tags")
                        if resp.status_code == 200:
                            data = resp.json()
                            models = [
                                {
                                    "id": m["name"],
                                    "name": m["name"],
                                    "size": m.get("size"),
                                    "modified_at": m.get("modified_at"),
                                }
                                for m in data.get("models", [])
                            ]
                            return {
                                "provider": provider.value,
                                "models": models,
                                "source": "api",
                                "ollama_url": ollama_url,
                            }
                        else:
                            return {
                                "provider": provider.value,
                                "models": _get_static_models("ollama"),
                                "source": "static",
                                "error": f"Ollama ì—°ê²° ì‹¤íŒ¨: {resp.status_code}",
                            }
                    except httpx.ConnectError:
                        return {
                            "provider": provider.value,
                            "models": _get_static_models("ollama"),
                            "source": "static",
                            "error": f"Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŒ: {ollama_url}",
                        }

                elif provider == LLMProvider.ANTHROPIC:
                    # Anthropicì€ ê³µì‹ ëª¨ë¸ ëª©ë¡ APIê°€ ì—†ìŒ
                    return {
                        "provider": provider.value,
                        "models": _get_static_models("anthropic"),
                        "source": "static",
                        "message": "Anthropicì€ ëª¨ë¸ ëª©ë¡ APIë¥¼ ì œê³µí•˜ì§€ ì•ŠìŒ",
                    }

                elif provider == LLMProvider.GOOGLE:
                    # Google AIë„ ì •ì  ëª©ë¡ ì‚¬ìš©
                    return {
                        "provider": provider.value,
                        "models": _get_static_models("google"),
                        "source": "static",
                        "message": "Google AIëŠ” ì •ì  ëª¨ë¸ ëª©ë¡ ì‚¬ìš©",
                    }

                elif provider == LLMProvider.AZURE:
                    # AzureëŠ” ë°°í¬ ê¸°ë°˜ì´ë¼ ë™ì  ì¡°íšŒ ë¶ˆê°€
                    return {
                        "provider": provider.value,
                        "models": _get_static_models("azure"),
                        "source": "static",
                        "message": "Azure OpenAIëŠ” ë°°í¬ ê¸°ë°˜ìœ¼ë¡œ ë™ì  ì¡°íšŒ ë¶ˆê°€",
                    }

                elif provider == LLMProvider.CUSTOM:
                    custom_url = base_url or os.getenv("CUSTOM_LLM_BASE_URL", "")
                    if custom_url:
                        try:
                            resp = await client.get(f"{custom_url}/v1/models")
                            if resp.status_code == 200:
                                data = resp.json()
                                models = [
                                    {"id": m["id"], "name": m.get("id", "")}
                                    for m in data.get("data", [])
                                ]
                                return {
                                    "provider": provider.value,
                                    "models": models,
                                    "source": "api",
                                    "base_url": custom_url,
                                }
                        except Exception:
                            pass

                    return {
                        "provider": provider.value,
                        "models": [{"id": "default", "name": "ê¸°ë³¸ ëª¨ë¸"}],
                        "source": "static",
                    }

                else:
                    return {
                        "provider": provider.value,
                        "models": [],
                        "source": "unknown",
                        "error": "ì•Œ ìˆ˜ ì—†ëŠ” Provider",
                    }

        except Exception as e:
            logger.error("Failed to fetch models", provider=provider.value, error=str(e))
            return {
                "provider": provider.value,
                "models": _get_static_models(provider.value),
                "source": "static",
                "error": str(e),
            }

    def _get_static_models(provider: str) -> List[Dict[str, Any]]:
        """ì •ì  ëª¨ë¸ ëª©ë¡ ë°˜í™˜ (fallback)"""
        static_models = {
            "openai": [
                {"id": "gpt-4o", "name": "GPT-4o (ì¶”ì²œ)"},
                {"id": "gpt-4o-mini", "name": "GPT-4o Mini (ë¹ ë¦„)"},
                {"id": "gpt-4-turbo", "name": "GPT-4 Turbo"},
                {"id": "gpt-3.5-turbo", "name": "GPT-3.5 Turbo (ì €ë ´)"},
                {"id": "o1-preview", "name": "o1-preview (ì¶”ë¡ )"},
                {"id": "o1-mini", "name": "o1-mini (ì¶”ë¡ , ë¹ ë¦„)"},
            ],
            "anthropic": [
                {"id": "claude-3-5-sonnet-20241022", "name": "Claude 3.5 Sonnet (ì¶”ì²œ)"},
                {"id": "claude-3-5-haiku-20241022", "name": "Claude 3.5 Haiku (ë¹ ë¦„)"},
                {"id": "claude-3-opus-20240229", "name": "Claude 3 Opus (ê°•ë ¥)"},
            ],
            "google": [
                {"id": "gemini-1.5-pro", "name": "Gemini 1.5 Pro (ì¶”ì²œ)"},
                {"id": "gemini-1.5-flash", "name": "Gemini 1.5 Flash (ë¹ ë¦„)"},
                {"id": "gemini-2.0-flash-exp", "name": "Gemini 2.0 Flash (ì‹¤í—˜)"},
            ],
            "openrouter": [
                {"id": "openai/gpt-4o", "name": "GPT-4o (OpenAI)"},
                {"id": "anthropic/claude-3.5-sonnet", "name": "Claude 3.5 Sonnet"},
                {"id": "google/gemini-pro-1.5", "name": "Gemini 1.5 Pro"},
                {"id": "meta-llama/llama-3.1-405b-instruct", "name": "Llama 3.1 405B"},
                {"id": "meta-llama/llama-3.1-70b-instruct", "name": "Llama 3.1 70B"},
                {"id": "mistralai/mixtral-8x22b-instruct", "name": "Mixtral 8x22B"},
                {"id": "deepseek/deepseek-chat", "name": "DeepSeek Chat"},
                {"id": "qwen/qwen-2.5-72b-instruct", "name": "Qwen 2.5 72B"},
            ],
            "ollama": [
                {"id": "llama3.1", "name": "Llama 3.1 (ì¶”ì²œ)"},
                {"id": "llama3.1:70b", "name": "Llama 3.1 70B"},
                {"id": "mistral", "name": "Mistral"},
                {"id": "mixtral", "name": "Mixtral"},
                {"id": "codellama", "name": "Code Llama"},
                {"id": "qwen2.5", "name": "Qwen 2.5"},
                {"id": "gemma2", "name": "Gemma 2"},
            ],
            "azure": [
                {"id": "gpt-4o", "name": "GPT-4o"},
                {"id": "gpt-4-turbo", "name": "GPT-4 Turbo"},
                {"id": "gpt-35-turbo", "name": "GPT-3.5 Turbo"},
            ],
            "custom": [
                {"id": "default", "name": "ê¸°ë³¸ ëª¨ë¸"},
            ],
        }
        return static_models.get(provider, [])

    @app.post("/agent/crawl", response_model=AgentTaskResult)
    async def agent_crawl(
        request: AgentTask,
        req: Request,
        user: JWTPayload = Depends(require_auth()),
    ):
        """
        AI ì—ì´ì „íŠ¸ í¬ë¡¤ë§ ì‹¤í–‰ (ë™ê¸°).

        CAPTCHA ìš°íšŒ ë° ìŠ¤í…”ìŠ¤ ëª¨ë“œ ì§€ì›.
        ì¸ì¦ í•„ìš”: Bearer í† í° í•„ìˆ˜
        """
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store

        logger.info("Agent crawl requested", user=user.username, url=str(request.url))
        result = await run_browser_agent(request, settings, sse_manager)
        await state_store.save_task(result.task_id, result)
        return result

    @app.post("/agent/crawl/async")
    async def agent_crawl_async(
        request: AgentTask,
        req: Request,
        background_tasks: BackgroundTasks,
        user: JWTPayload = Depends(require_auth()),
    ):
        """AI ì—ì´ì „íŠ¸ í¬ë¡¤ë§ ë¹„ë™ê¸° ì‹¤í–‰ (ì¸ì¦ í•„ìš”)"""
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store
        task_id = str(uuid.uuid4())

        async def run_task():
            result = await run_browser_agent(request, settings, sse_manager)
            result.task_id = task_id
            await state_store.save_task(task_id, result)

        background_tasks.add_task(asyncio.create_task, run_task())

        return {
            "task_id": task_id,
            "status": "queued",
            "message": "íƒœìŠ¤í¬ê°€ íì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. GET /agent/task/{task_id}ë¡œ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.",
        }

    @app.get("/agent/task/{task_id}", response_model=AgentTaskResult)
    async def get_task_result(task_id: str, req: Request):
        """íƒœìŠ¤í¬ ê²°ê³¼ ì¡°íšŒ"""
        state_store: StateStore = req.app.state.state_store
        result = await state_store.load_task(task_id)
        if result is None:
            raise HTTPException(status_code=404, detail="íƒœìŠ¤í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return result

    @app.delete("/agent/task/{task_id}")
    async def delete_task(task_id: str, req: Request):
        """íƒœìŠ¤í¬ ê²°ê³¼ ì‚­ì œ"""
        state_store: StateStore = req.app.state.state_store
        result = await state_store.load_task(task_id)
        if result is not None:
            await state_store.delete_task(task_id)
            return {"status": "deleted", "task_id": task_id}
        raise HTTPException(status_code=404, detail="íƒœìŠ¤í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    @app.get("/agent/tasks")
    async def list_tasks(
        req: Request,
        limit: int = Query(default=20, le=100),
        status: Optional[str] = Query(default=None),
    ):
        """ìµœê·¼ íƒœìŠ¤í¬ ëª©ë¡ ì¡°íšŒ"""
        state_store: StateStore = req.app.state.state_store
        results = await state_store.list_tasks(status=status, limit=limit)

        # Sort by duration_ms (stored tasks are dicts, not Pydantic models)
        results = sorted(results, key=lambda x: x.get("duration_ms", 0), reverse=True)[:limit]

        return {"total": len(results), "tasks": results}

    @app.post("/agent/batch")
    async def batch_crawl(
        request: BatchCrawlRequest,
        req: Request,
        background_tasks: BackgroundTasks,
        user: JWTPayload = Depends(require_auth()),
    ):
        """ì—¬ëŸ¬ URL ë°°ì¹˜ í¬ë¡¤ë§ (ì¸ì¦ í•„ìš”)"""
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store
        batch_id = str(uuid.uuid4())
        task_ids = []

        tasks_to_run = []
        for url in request.urls:
            agent_task = AgentTask(
                url=url,
                task=request.task,
                llm_provider=request.llm_provider,
                model=request.model,
                auto_save_url=request.auto_save_url,
            )
            task_id = str(uuid.uuid4())
            task_ids.append(task_id)
            tasks_to_run.append((agent_task, task_id))

        async def run_batch():
            for agent_task, tid in tasks_to_run:
                try:
                    result = await run_browser_agent(agent_task, settings, sse_manager)
                    result.task_id = tid
                    await state_store.save_task(tid, result)
                except Exception as e:
                    error_result = AgentTaskResult(
                        task_id=tid,
                        url=str(agent_task.url),
                        status="failed",
                        error_message=str(e),
                        duration_ms=0,
                    )
                    await state_store.save_task(tid, error_result)

        background_tasks.add_task(run_batch)

        return {
            "batch_id": batch_id,
            "task_ids": task_ids,
            "total": len(task_ids),
            "message": "ë°°ì¹˜ í¬ë¡¤ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
        }

    # ========================================
    # ë‰´ìŠ¤ í”„ë¦¬ì…‹ ì—”ë“œí¬ì¸íŠ¸
    # ========================================

    @app.post("/agent/presets/extract-article")
    async def extract_article(url: HttpUrl, req: Request, auto_save: bool = True):
        """
        ë‰´ìŠ¤ ê¸°ì‚¬ ì¶”ì¶œ í”„ë¦¬ì…‹.

        URLì—ì„œ ì œëª©, ì‘ì„±ì, ë‚ ì§œ, ë³¸ë¬¸ì„ ìë™ ì¶”ì¶œ.
        """
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store

        request = AgentTask(
            url=url,
            task="""
            ì´ ë‰´ìŠ¤ ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:
            1. ê¸°ì‚¬ ì œëª©
            2. ì‘ì„±ì/ê¸°ìëª…
            3. ë°œí–‰ì¼
            4. ë³¸ë¬¸ ì „ì²´ í…ìŠ¤íŠ¸
            5. ê´€ë ¨ ê¸°ì‚¬ ë§í¬ë“¤
            
            JSON í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
            """,
            extract_links=True,
            auto_save_url=auto_save,
            source_category="news",
        )
        result = await run_browser_agent(request, settings, sse_manager)
        await state_store.save_task(result.task_id, result)
        return result

    @app.post("/agent/presets/extract-comments")
    async def extract_comments(
        url: HttpUrl,
        req: Request,
        max_scroll: int = 5,
    ):
        """
        ëŒ“ê¸€/ì—¬ë¡  ì¶”ì¶œ í”„ë¦¬ì…‹.

        ê²Œì‹œê¸€ì˜ ëŒ“ê¸€ë“¤ì„ ìˆ˜ì§‘.
        """
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store

        request = AgentTask(
            url=url,
            task=f"""
            ì´ í˜ì´ì§€ì—ì„œ ëŒ“ê¸€ë“¤ì„ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”:
            1. í˜ì´ì§€ë¥¼ {max_scroll}ë²ˆ ìŠ¤í¬ë¡¤í•˜ë©´ì„œ ëŒ“ê¸€ì„ ë¡œë“œí•´ì£¼ì„¸ìš”
            2. ê° ëŒ“ê¸€ì˜ ì‘ì„±ì, ë‚´ìš©, ì‘ì„±ì‹œê°„, ì¢‹ì•„ìš” ìˆ˜ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”
            3. "ë”ë³´ê¸°" ë²„íŠ¼ì´ ìˆìœ¼ë©´ í´ë¦­í•´ì„œ ë” ë§ì€ ëŒ“ê¸€ì„ ë¡œë“œí•´ì£¼ì„¸ìš”
            
            JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
            """,
            max_steps=20,
            auto_save_url=False,
            source_category="forum",
        )
        result = await run_browser_agent(request, settings, sse_manager)
        await state_store.save_task(result.task_id, result)
        return result

    @app.post("/agent/presets/site-structure")
    async def analyze_site_structure(url: HttpUrl, req: Request):
        """
        ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„ í”„ë¦¬ì…‹.

        ì‚¬ì´íŠ¸ì˜ ì£¼ìš” ì„¹ì…˜ê³¼ ë§í¬ êµ¬ì¡° íŒŒì•….
        """
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store

        request = AgentTask(
            url=url,
            task="""
            ì´ ì›¹ì‚¬ì´íŠ¸ì˜ êµ¬ì¡°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:
            1. ë©”ì¸ ë„¤ë¹„ê²Œì´ì…˜ ë©”ë‰´ í•­ëª©ë“¤
            2. ì£¼ìš” ì„¹ì…˜/ì¹´í…Œê³ ë¦¬
            3. RSS í”¼ë“œ ë§í¬ê°€ ìˆëŠ”ì§€
            4. API ì—”ë“œí¬ì¸íŠ¸ íŒíŠ¸ê°€ ìˆëŠ”ì§€
            5. ì‚¬ì´íŠ¸ë§µ ë§í¬
            
            í¬ë¡¤ë§ ì „ëµ ìˆ˜ë¦½ì— ë„ì›€ì´ ë˜ë„ë¡ ì •ë¦¬í•´ì£¼ì„¸ìš”.
            """,
            extract_links=True,
            auto_save_url=True,
            source_category="general",
        )
        result = await run_browser_agent(request, settings, sse_manager)
        await state_store.save_task(result.task_id, result)
        return result

    @app.post("/agent/presets/news-domain-crawl")
    async def news_domain_crawl(
        url: HttpUrl,
        req: Request,
        max_pages: int = Query(default=30, ge=1, le=100, description="ìµœëŒ€ ë°©ë¬¸ í˜ì´ì§€ ìˆ˜"),
        max_depth: int = Query(default=2, ge=1, le=5, description="ìµœëŒ€ íƒìƒ‰ ê¹Šì´"),
        focus_keywords: Optional[str] = Query(default=None, description="ì§‘ì¤‘ í‚¤ì›Œë“œ (ì‰¼í‘œ êµ¬ë¶„)"),
    ):
        """
        ë‰´ìŠ¤ ë„ë©”ì¸ ì „ì²´ í¬ë¡¤ë§ í”„ë¦¬ì…‹.

        ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì˜ ê¸°ì‚¬ë“¤ì„ ìë™ìœ¼ë¡œ íƒìƒ‰í•˜ê³  ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        NEWS_ONLY ì •ì±…ì„ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê¸°ì‚¬ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Args:
            url: ì‹œì‘ URL (ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ë©”ì¸ ë˜ëŠ” ì„¹ì…˜ í˜ì´ì§€)
            max_pages: ìµœëŒ€ ë°©ë¬¸í•  í˜ì´ì§€ ìˆ˜
            max_depth: ë§í¬ íƒìƒ‰ ìµœëŒ€ ê¹Šì´
            focus_keywords: íŠ¹ì • í‚¤ì›Œë“œì— ì§‘ì¤‘í•  ê²½ìš° ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥
        """
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store

        keyword_instruction = ""
        if focus_keywords:
            keyword_instruction = (
                f"\níŠ¹íˆ ë‹¤ìŒ í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ê¸°ì‚¬ì— ì§‘ì¤‘í•´ì£¼ì„¸ìš”: {focus_keywords}"
            )

        request = AgentTask(
            url=url,
            task=f"""
            ì´ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ ê¸°ì‚¬ë“¤ì„ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”:
            
            1. ë©”ì¸ í˜ì´ì§€ì—ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬ë“¤ì„ ì°¾ì•„ì£¼ì„¸ìš”
            2. ê° ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ì œëª©, ê¸°ìëª…, ë°œí–‰ì¼, ë³¸ë¬¸ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”
            3. ìµœëŒ€ {max_pages}ê°œ í˜ì´ì§€ë¥¼ ë°©ë¬¸í•˜ë©´ì„œ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”
            4. ìµœê·¼ ê¸°ì‚¬ë¥¼ ìš°ì„ ìœ¼ë¡œ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”
            5. ê´‘ê³ , ë¡œê·¸ì¸ í˜ì´ì§€, ë¹„ê¸°ì‚¬ ì½˜í…ì¸ ëŠ” ê±´ë„ˆë›°ì„¸ìš”
            {keyword_instruction}
            
            ê° ê¸°ì‚¬ëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:
            ---ARTICLE_START---
            URL: [ê¸°ì‚¬ URL]
            TITLE: [ì œëª©]
            AUTHOR: [ê¸°ìëª…]
            PUBLISHED_AT: [ë°œí–‰ì¼]
            CONTENT: [ë³¸ë¬¸]
            ---ARTICLE_END---
            """,
            max_steps=max_pages + 10,
            timeout_sec=min(max_pages * 15, 600),
            extract_links=True,
            auto_save_url=True,
            source_category="news",
        )
        result = await run_browser_agent(request, settings, sse_manager)
        await state_store.save_task(result.task_id, result)
        return result

    @app.post("/agent/presets/discover-rss")
    async def discover_rss(url: HttpUrl, req: Request):
        """
        RSS í”¼ë“œ ë°œê²¬ í”„ë¦¬ì…‹.

        ì›¹ì‚¬ì´íŠ¸ì—ì„œ RSS/Atom í”¼ë“œ URLì„ ì°¾ì•„ëƒ…ë‹ˆë‹¤.
        ë‰´ìŠ¤ ìˆ˜ì§‘ ìë™í™”ë¥¼ ìœ„í•œ í”¼ë“œ URL ë°œê²¬ì— ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store

        request = AgentTask(
            url=url,
            task="""
            ì´ ì›¹ì‚¬ì´íŠ¸ì—ì„œ RSS ë˜ëŠ” Atom í”¼ë“œë¥¼ ì°¾ì•„ì£¼ì„¸ìš”:
            
            1. í˜ì´ì§€ ì†ŒìŠ¤ì—ì„œ RSS/Atom ë§í¬ íƒœê·¸ í™•ì¸
               - <link rel="alternate" type="application/rss+xml" ...>
               - <link rel="alternate" type="application/atom+xml" ...>
            2. ì¼ë°˜ì ì¸ RSS ê²½ë¡œ í™•ì¸:
               - /feed, /rss, /feeds, /rss.xml, /feed.xml, /atom.xml
               - /news/rss, /blog/feed ë“±
            3. í˜ì´ì§€ í‘¸í„°ë‚˜ ì‚¬ì´ë“œë°”ì˜ RSS ì•„ì´ì½˜/ë§í¬ í™•ì¸
            4. sitemap.xmlì—ì„œ í”¼ë“œ ì •ë³´ í™•ì¸
            
            ë°œê²¬ëœ ëª¨ë“  í”¼ë“œë¥¼ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:
            {
                "feeds": [
                    {
                        "url": "í”¼ë“œ URL",
                        "type": "rss" ë˜ëŠ” "atom",
                        "title": "í”¼ë“œ ì œëª© (ìˆëŠ” ê²½ìš°)",
                        "category": "ì¹´í…Œê³ ë¦¬ (ìˆëŠ” ê²½ìš°)"
                    }
                ],
                "sitemap_url": "ì‚¬ì´íŠ¸ë§µ URL (ë°œê²¬ëœ ê²½ìš°)",
                "has_api_hints": true/false
            }
            """,
            max_steps=15,
            extract_links=True,
            auto_save_url=False,
            source_category="general",
        )
        result = await run_browser_agent(request, settings, sse_manager)
        await state_store.save_task(result.task_id, result)
        return result

    @app.post("/agent/presets/extract-news-list")
    async def extract_news_list(
        url: HttpUrl,
        req: Request,
        max_articles: int = Query(default=20, ge=1, le=50, description="ì¶”ì¶œí•  ìµœëŒ€ ê¸°ì‚¬ ìˆ˜"),
    ):
        """
        ë‰´ìŠ¤ ëª©ë¡ í˜ì´ì§€ ì¶”ì¶œ í”„ë¦¬ì…‹.

        ë‰´ìŠ¤ ì„¹ì…˜/ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ëª©ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        ê° ê¸°ì‚¬ì˜ ì œëª©, URL, ìš”ì•½, ë°œí–‰ì¼ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        """
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store

        request = AgentTask(
            url=url,
            task=f"""
            ì´ í˜ì´ì§€ì—ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”:
            
            1. ìµœëŒ€ {max_articles}ê°œì˜ ê¸°ì‚¬ ì •ë³´ë¥¼ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”
            2. ê° ê¸°ì‚¬ì— ëŒ€í•´ ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œ:
               - ê¸°ì‚¬ ì œëª©
               - ê¸°ì‚¬ URL (ì „ì²´ ë§í¬)
               - ìš”ì•½/ë¦¬ë“œ ë¬¸êµ¬ (ìˆëŠ” ê²½ìš°)
               - ë°œí–‰ì¼/ì‹œê°„ (ìˆëŠ” ê²½ìš°)
               - ì¸ë„¤ì¼ ì´ë¯¸ì§€ URL (ìˆëŠ” ê²½ìš°)
               - ì¹´í…Œê³ ë¦¬/ì„¹ì…˜ (ìˆëŠ” ê²½ìš°)
            3. ê´‘ê³ ë‚˜ í”„ë¡œëª¨ì…˜ ì½˜í…ì¸ ëŠ” ì œì™¸í•´ì£¼ì„¸ìš”
            4. ìµœì‹  ê¸°ì‚¬ ìˆœìœ¼ë¡œ ì •ë ¬í•´ì£¼ì„¸ìš”
            
            JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:
            [
                {{
                    "title": "ê¸°ì‚¬ ì œëª©",
                    "url": "ê¸°ì‚¬ URL",
                    "summary": "ìš”ì•½",
                    "published_at": "ë°œí–‰ì¼",
                    "thumbnail": "ì¸ë„¤ì¼ URL",
                    "category": "ì¹´í…Œê³ ë¦¬"
                }}
            ]
            """,
            max_steps=10,
            extract_links=True,
            auto_save_url=True,
            source_category="news",
        )
        result = await run_browser_agent(request, settings, sse_manager)
        await state_store.save_task(result.task_id, result)
        return result

    @app.post("/agent/presets/monitor-breaking-news")
    async def monitor_breaking_news(
        url: HttpUrl,
        req: Request,
        keywords: Optional[str] = Query(default=None, description="ëª¨ë‹ˆí„°ë§ í‚¤ì›Œë“œ (ì‰¼í‘œ êµ¬ë¶„)"),
    ):
        """
        ì†ë³´/ê¸´ê¸‰ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ í”„ë¦¬ì…‹.

        ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ ì†ë³´ë‚˜ ê¸´ê¸‰ ë‰´ìŠ¤ë¥¼ íƒì§€í•©ë‹ˆë‹¤.
        'ì†ë³´', 'ê¸´ê¸‰', 'Breaking' ë“±ì˜ ë¼ë²¨ì´ ë¶™ì€ ê¸°ì‚¬ë¥¼ ìš°ì„  ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        """
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store

        keyword_filter = ""
        if keywords:
            keyword_filter = f"\níŠ¹íˆ ë‹¤ìŒ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì†ë³´ì— ì£¼ëª©í•´ì£¼ì„¸ìš”: {keywords}"

        request = AgentTask(
            url=url,
            task=f"""
            ì´ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ ì†ë³´/ê¸´ê¸‰ ë‰´ìŠ¤ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”:
            
            1. ë‹¤ìŒ í‘œì‹œê°€ ìˆëŠ” ê¸°ì‚¬ë¥¼ ìš°ì„  íƒì§€:
               - "ì†ë³´", "Breaking", "ê¸´ê¸‰", "ë‹¨ë…", "flash"
               - ë¹¨ê°„ìƒ‰ ë˜ëŠ” ê°•ì¡°ëœ ë¼ë²¨
               - ìƒë‹¨ ê³ ì • ë˜ëŠ” íŠ¹ë³„ ì„¹ì…˜ì˜ ê¸°ì‚¬
            2. ìµœê·¼ 1ì‹œê°„ ì´ë‚´ ë°œí–‰ëœ ê¸°ì‚¬ ìš°ì„ 
            3. ê° ì†ë³´ ê¸°ì‚¬ì˜ ì „ì²´ ë‚´ìš© ì¶”ì¶œ
            {keyword_filter}
            
            ê²°ê³¼ë¥¼ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:
            {{
                "breaking_news": [
                    {{
                        "title": "ê¸°ì‚¬ ì œëª©",
                        "url": "ê¸°ì‚¬ URL",
                        "published_at": "ë°œí–‰ ì‹œê°„",
                        "label": "ì†ë³´/ê¸´ê¸‰/ë‹¨ë… ë“±",
                        "summary": "í•µì‹¬ ë‚´ìš© ìš”ì•½",
                        "full_content": "ì „ì²´ ë³¸ë¬¸"
                    }}
                ],
                "latest_update": "ë§ˆì§€ë§‰ í™•ì¸ ì‹œê°„",
                "total_found": ë°œê²¬ëœ ì†ë³´ ìˆ˜
            }}
            """,
            max_steps=15,
            timeout_sec=180,
            extract_links=True,
            auto_save_url=True,
            source_category="news",
        )
        result = await run_browser_agent(request, settings, sse_manager)
        await state_store.save_task(result.task_id, result)
        return result

    # ========================================
    # ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸
    # ========================================

    async def fetch_page_content_for_context(url: str, max_chars: int = 8000) -> Optional[str]:
        """
        URLì—ì„œ í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì™€ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©.

        crawl4ai ë˜ëŠ” ë‚´ë¶€ í¬ë¡¤ëŸ¬ë¥¼ í†µí•´ í˜ì´ì§€ ë‚´ìš©ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜.
        """
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                # 1ì°¨: crawl4ai ì„œë¹„ìŠ¤ ì‹œë„
                try:

                    def extract_content(payload: Any) -> Optional[str]:
                        if isinstance(payload, dict):
                            results = payload.get("results")
                            if isinstance(results, list) and results:
                                return extract_content(results[0])
                            result = payload.get("result")
                            if result is not None:
                                return extract_content(result)
                            for key in ("markdown", "text", "content"):
                                value = payload.get(key)
                                if isinstance(value, str) and value.strip():
                                    return value
                            return None
                        if isinstance(payload, str) and payload.strip():
                            return payload
                        return None

                    headers: dict[str, str] = {}
                    if api_config.WEB_CRAWLER_API_TOKEN:
                        headers["Authorization"] = f"Bearer {api_config.WEB_CRAWLER_API_TOKEN}"

                    base_url = api_config.WEB_CRAWLER_URL.rstrip("/")
                    endpoint = f"{base_url}/crawl"

                    crawl_response = await client.get(
                        endpoint, params={"url": url}, headers=headers
                    )
                    if crawl_response.status_code == 200:
                        try:
                            crawl_data = crawl_response.json()
                        except Exception:
                            crawl_data = crawl_response.text
                        content = extract_content(crawl_data)
                        if content:
                            return content[:max_chars]

                    crawl_response = await client.post(
                        endpoint,
                        json={"urls": [url], "priority": 10},
                        headers=headers,
                    )
                    if crawl_response.status_code == 200:
                        crawl_data = crawl_response.json()
                        content = extract_content(crawl_data)
                        if content:
                            return content[:max_chars]

                        task_id = crawl_data.get("task_id")
                        if task_id:
                            for status_path in (
                                f"{base_url}/task/{task_id}",
                                f"{base_url}/job/{task_id}",
                            ):
                                try:
                                    status_response = await client.get(status_path, headers=headers)
                                    if status_response.status_code == 200:
                                        status_data = status_response.json()
                                        status_content = extract_content(status_data)
                                        if status_content:
                                            return status_content[:max_chars]
                                except Exception:
                                    continue
                except Exception:
                    pass

                # 2ì°¨: ì§ì ‘ HTTP ìš”ì²­ fallback
                try:
                    response = await client.get(url, follow_redirects=True)
                    if response.status_code == 200:
                        from bs4 import BeautifulSoup

                        soup = BeautifulSoup(response.text, "html.parser")

                        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                        for tag in soup(["script", "style", "nav", "footer", "header", "aside"]):
                            tag.decompose()

                        # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        text = soup.get_text(separator="\n", strip=True)
                        return text[:max_chars] if text else None
                except Exception:
                    pass

        except Exception as e:
            logger.warning("Failed to fetch context URL", url=url, error=str(e))

        return None

    def convert_to_langchain_messages(messages: List[ChatMessage]):
        """ChatMessage ë¦¬ìŠ¤íŠ¸ë¥¼ LangChain ë©”ì‹œì§€ë¡œ ë³€í™˜"""
        from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

        langchain_messages = []
        for msg in messages:
            if msg.role == "user":
                langchain_messages.append(HumanMessage(content=msg.content))
            elif msg.role == "assistant":
                langchain_messages.append(AIMessage(content=msg.content))
            elif msg.role == "system":
                langchain_messages.append(SystemMessage(content=msg.content))
        return langchain_messages

    @app.post("/chat", response_model=ChatResponse)
    async def chat(request: ChatRequest):
        """
        AI ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸.

        LLMê³¼ ì§ì ‘ ëŒ€í™”í•˜ëŠ” ì¸í„°í˜ì´ìŠ¤.
        URL ì»¨í…ìŠ¤íŠ¸ê°€ ì œê³µë˜ë©´ í•´ë‹¹ í˜ì´ì§€ ë‚´ìš©ì„ ì°¸ì¡°í•˜ì—¬ ì‘ë‹µ.

        ì‚¬ìš© ì˜ˆì‹œ:
        ``\`json
        {
            "messages": [
                {"role": "user", "content": "ì´ ê¸°ì‚¬ì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ì¤˜"}
            ],
            "context_url": "https://news.example.com/article/123"
        }
        ``\`
        """
        try:
            llm = await get_llm(request.llm_provider, request.model)
            langchain_messages = convert_to_langchain_messages(request.messages)

            # URL ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ í˜ì´ì§€ ë‚´ìš© ì¶”ê°€
            if request.context_url:
                page_content = await fetch_page_content_for_context(request.context_url)
                if page_content:
                    from langchain_core.messages import SystemMessage

                    context_msg = SystemMessage(
                        content=f"ë‹¤ìŒì€ ì°¸ì¡°í•  ì›¹í˜ì´ì§€ ë‚´ìš©ì…ë‹ˆë‹¤:\n\n{page_content}"
                    )
                    langchain_messages.insert(0, context_msg)
                    logger.debug(
                        "Context URL content added",
                        url=request.context_url,
                        content_length=len(page_content),
                    )

            response = await llm.ainvoke(langchain_messages)

            return ChatResponse(
                message=response.content,
                provider=request.llm_provider.value,
                model=request.model or api_config.DEFAULT_MODEL,
                tokens_used=getattr(response, "usage_metadata", {}).get("total_tokens"),
            )

        except Exception as e:
            logger.error("Chat failed", error=str(e))
            raise HTTPException(status_code=500, detail=f"ì±„íŒ… ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

    @app.post("/chat/stream")
    async def chat_stream(request: ChatRequest):
        """
        AI ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° ì—”ë“œí¬ì¸íŠ¸ (SSE).

        LLM ì‘ë‹µì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.
        URL ì»¨í…ìŠ¤íŠ¸ê°€ ì œê³µë˜ë©´ í•´ë‹¹ í˜ì´ì§€ ë‚´ìš©ì„ ì°¸ì¡°í•˜ì—¬ ì‘ë‹µ.

        ì‚¬ìš© ì˜ˆì‹œ:
        ``\`json
        {
            "messages": [
                {"role": "user", "content": "íŒŒì´ì¬ì˜ ì¥ì ì„ ì„¤ëª…í•´ì¤˜"}
            ],
            "stream": true,
            "context_url": "https://docs.python.org/3/"
        }
        ``\`

        ì´ë²¤íŠ¸ íƒ€ì…:
        - chunk: ìŠ¤íŠ¸ë¦¬ë° í…ìŠ¤íŠ¸ ì¡°ê°
        - done: ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ
        - error: ì—ëŸ¬ ë°œìƒ
        """
        import json

        async def generate_stream():
            try:
                llm = await get_llm(request.llm_provider, request.model)
                langchain_messages = convert_to_langchain_messages(request.messages)

                # URL ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ í˜ì´ì§€ ë‚´ìš© ì¶”ê°€
                if request.context_url:
                    page_content = await fetch_page_content_for_context(request.context_url)
                    if page_content:
                        from langchain_core.messages import SystemMessage

                        context_msg = SystemMessage(
                            content=f"ë‹¤ìŒì€ ì°¸ì¡°í•  ì›¹í˜ì´ì§€ ë‚´ìš©ì…ë‹ˆë‹¤:\n\n{page_content}"
                        )
                        langchain_messages.insert(0, context_msg)
                        logger.debug(
                            "Context URL content added for streaming",
                            url=request.context_url,
                            content_length=len(page_content),
                        )

                # ìŠ¤íŠ¸ë¦¬ë° LLM í˜¸ì¶œ
                full_response = ""
                async for chunk in llm.astream(langchain_messages):
                    if hasattr(chunk, "content") and chunk.content:
                        content = chunk.content
                        full_response += content
                        yield f"data: {json.dumps({'content': content, 'type': 'chunk'})}\n\n"

                # ì™„ë£Œ ì´ë²¤íŠ¸
                yield f"data: {json.dumps({'type': 'done', 'provider': request.llm_provider.value, 'model': request.model or api_config.DEFAULT_MODEL, 'full_response': full_response})}\n\n"

                logger.info(
                    "Chat stream completed",
                    provider=request.llm_provider.value,
                    response_length=len(full_response),
                )

            except Exception as e:
                logger.error("Chat stream failed", error=str(e))
                yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"

        return StreamingResponse(
            generate_stream(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",
                "Access-Control-Allow-Origin": "*",
            },
        )


# Default app instance
app = create_app()


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "src.api.server:app",
        host="0.0.0.0",
        port=8030,
        reload=os.getenv("ENV", "development") == "development",
    )

```

---

## backend/autonomous-crawler-service/src/api/sse.py

```py
"""SSE (Server-Sent Events) Manager for real-time agent status updates."""

import asyncio
import json
import uuid
from datetime import datetime, timezone
from enum import Enum
from typing import Any, Dict

import structlog
from pydantic import BaseModel, Field

logger = structlog.get_logger(__name__)


class SSEEventType(str, Enum):
    """SSE ì´ë²¤íŠ¸ íƒ€ì…"""

    CONNECTED = "connected"
    AGENT_START = "agent_start"
    AGENT_STEP = "agent_step"
    AGENT_COMPLETE = "agent_complete"
    AGENT_ERROR = "agent_error"
    URL_DISCOVERED = "url_discovered"
    HEALTH_UPDATE = "health_update"
    CAPTCHA_DETECTED = "captcha_detected"
    CAPTCHA_SOLVED = "captcha_solved"


class SSEEvent(BaseModel):
    """SSE ì´ë²¤íŠ¸ ë°ì´í„°"""

    type: SSEEventType
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    timestamp: str = Field(default_factory=lambda: datetime.now(timezone.utc).isoformat())
    data: Dict[str, Any] = {}


class SSEManager:
    """
    SSE í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ê´€ë¦¬ì.

    ë¸Œë¼ìš°ì € ì—ì´ì „íŠ¸ì˜ ì‹¤ì‹œê°„ ìƒíƒœë¥¼ êµ¬ë…í•˜ëŠ” í´ë¼ì´ì–¸íŠ¸ë“¤ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
    """

    def __init__(self, max_queue_size: int = 100):
        self._clients: Dict[str, asyncio.Queue] = {}
        self._lock = asyncio.Lock()
        self._max_queue_size = max_queue_size

    @property
    def client_count(self) -> int:
        """í˜„ì¬ ì—°ê²°ëœ í´ë¼ì´ì–¸íŠ¸ ìˆ˜"""
        return len(self._clients)

    @property
    def client_ids(self) -> list[str]:
        """ì—°ê²°ëœ í´ë¼ì´ì–¸íŠ¸ ID ëª©ë¡"""
        return list(self._clients.keys())

    async def connect(self, client_id: str | None = None) -> tuple[str, asyncio.Queue]:
        """
        ìƒˆ í´ë¼ì´ì–¸íŠ¸ ì—°ê²°.

        Args:
            client_id: í´ë¼ì´ì–¸íŠ¸ ID (ì—†ìœ¼ë©´ ìë™ ìƒì„±)

        Returns:
            (client_id, queue) íŠœí”Œ
        """
        if client_id is None:
            client_id = str(uuid.uuid4())

        queue: asyncio.Queue = asyncio.Queue(maxsize=self._max_queue_size)

        async with self._lock:
            self._clients[client_id] = queue

        logger.info(
            "SSE client connected",
            client_id=client_id,
            total_clients=len(self._clients),
        )

        return client_id, queue

    async def disconnect(self, client_id: str) -> None:
        """í´ë¼ì´ì–¸íŠ¸ ì—°ê²° í•´ì œ"""
        async with self._lock:
            if client_id in self._clients:
                del self._clients[client_id]
                logger.info(
                    "SSE client disconnected",
                    client_id=client_id,
                    total_clients=len(self._clients),
                )

    async def broadcast(self, event: SSEEvent) -> None:
        """
        ëª¨ë“  ì—°ê²°ëœ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì´ë²¤íŠ¸ ë¸Œë¡œë“œìºìŠ¤íŠ¸.

        Args:
            event: ì „ì†¡í•  SSE ì´ë²¤íŠ¸
        """
        disconnected = []

        async with self._lock:
            for client_id, queue in self._clients.items():
                try:
                    queue.put_nowait(event)
                except asyncio.QueueFull:
                    logger.warning("SSE queue full", client_id=client_id)
                except Exception as e:
                    logger.warning(
                        "SSE broadcast error",
                        client_id=client_id,
                        error=str(e),
                    )
                    disconnected.append(client_id)

            # ì—°ê²° ëŠê¸´ í´ë¼ì´ì–¸íŠ¸ ì œê±°
            for client_id in disconnected:
                del self._clients[client_id]

    async def send_agent_event(
        self,
        event_type: SSEEventType,
        task_id: str,
        url: str,
        message: str,
        **kwargs,
    ) -> None:
        """
        ì—ì´ì „íŠ¸ ì´ë²¤íŠ¸ ì „ì†¡ í—¬í¼.

        Args:
            event_type: ì´ë²¤íŠ¸ íƒ€ì…
            task_id: íƒœìŠ¤í¬ ID
            url: ê´€ë ¨ URL
            message: ë©”ì‹œì§€
            **kwargs: ì¶”ê°€ ë°ì´í„°
        """
        event = SSEEvent(
            type=event_type,
            data={
                "task_id": task_id,
                "url": url,
                "message": message,
                **kwargs,
            },
        )
        await self.broadcast(event)


async def sse_event_generator(
    client_id: str,
    queue: asyncio.Queue,
    manager: SSEManager,
    heartbeat_interval: float = 30.0,
):
    """
    SSE ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ ìƒì„±ê¸°.

    Args:
        client_id: í´ë¼ì´ì–¸íŠ¸ ID
        queue: ì´ë²¤íŠ¸ í
        manager: SSE ë§¤ë‹ˆì €
        heartbeat_interval: í•˜íŠ¸ë¹„íŠ¸ ê°„ê²© (ì´ˆ)

    Yields:
        SSE í˜•ì‹ì˜ ì´ë²¤íŠ¸ ë¬¸ìì—´
    """
    try:
        # ì—°ê²° í™•ì¸ ì´ë²¤íŠ¸
        connected_event = SSEEvent(
            type=SSEEventType.CONNECTED,
            data={
                "client_id": client_id,
                "message": "Autonomous Crawler SSE connected",
                "active_clients": manager.client_count,
            },
        )
        yield f"event: {connected_event.type.value}\ndata: {json.dumps(connected_event.model_dump())}\n\n"

        while True:
            try:
                # í•˜íŠ¸ë¹„íŠ¸ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì´ë²¤íŠ¸ ëŒ€ê¸°
                event = await asyncio.wait_for(queue.get(), timeout=heartbeat_interval)
                yield f"event: {event.type.value}\ndata: {json.dumps(event.model_dump())}\n\n"
            except asyncio.TimeoutError:
                # Heartbeat ì „ì†¡
                yield ": heartbeat\n\n"

    except asyncio.CancelledError:
        logger.info("SSE client stream cancelled", client_id=client_id)
    finally:
        await manager.disconnect(client_id)


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_sse_manager: SSEManager | None = None


def get_sse_manager() -> SSEManager:
    """ì‹±ê¸€í†¤ SSE ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _sse_manager
    if _sse_manager is None:
        _sse_manager = SSEManager()
    return _sse_manager

```

---

## backend/autonomous-crawler-service/src/auth/__init__.py

```py
"""
Authentication module for autonomous-crawler-service.
"""

from .middleware import AuthMiddleware, get_current_user, require_auth
from .jwt_utils import verify_jwt_token, JWTPayload

__all__ = [
    "AuthMiddleware",
    "get_current_user",
    "require_auth",
    "verify_jwt_token",
    "JWTPayload",
]

```

---

## backend/autonomous-crawler-service/src/auth/jwt_utils.py

```py
"""
JWT Utilities for autonomous-crawler-service.
"""

import os
from dataclasses import dataclass
from typing import Optional

import jwt
import structlog

logger = structlog.get_logger(__name__)

# Secret key for JWT verification (shared with admin-dashboard)
JWT_SECRET = os.getenv("ADMIN_SECRET_KEY", "your-secret-key-change-in-production")
JWT_ALGORITHM = "HS256"


@dataclass
class JWTPayload:
    """JWT Token Payload"""
    user_id: str
    username: str
    role: str
    exp: int
    iat: int


def verify_jwt_token(token: str) -> Optional[JWTPayload]:
    """
    Verify JWT token and return payload.
    
    Args:
        token: JWT token string (without 'Bearer ' prefix)
        
    Returns:
        JWTPayload if valid, None otherwise
    """
    try:
        payload = jwt.decode(
            token,
            JWT_SECRET,
            algorithms=[JWT_ALGORITHM],
        )
        
        return JWTPayload(
            user_id=payload.get("sub", ""),
            username=payload.get("username", ""),
            role=payload.get("role", "user"),
            exp=payload.get("exp", 0),
            iat=payload.get("iat", 0),
        )
        
    except jwt.ExpiredSignatureError:
        logger.warning("JWT token expired")
        return None
    except jwt.InvalidTokenError as e:
        logger.warning("Invalid JWT token", error=str(e))
        return None
    except Exception as e:
        logger.error("JWT verification error", error=str(e))
        return None

```

---

## backend/autonomous-crawler-service/src/auth/middleware.py

```py
"""
Authentication Middleware for autonomous-crawler-service.

Provides FastAPI dependencies for JWT-based authentication.
"""

import os
from functools import wraps
from typing import Callable, Optional

import structlog
from fastapi import Depends, HTTPException, Request, status
from fastapi.security import HTTPAuthorizationCredentials, HTTPBearer

from .jwt_utils import JWTPayload, verify_jwt_token

logger = structlog.get_logger(__name__)

# Security enabled flag - set to False only in development
SECURITY_ENABLED = os.getenv("SECURITY_ENABLED", "true").lower() == "true"

# HTTP Bearer scheme for extracting tokens
bearer_scheme = HTTPBearer(auto_error=False)


class AuthMiddleware:
    """
    Authentication middleware for FastAPI.
    
    Usage:
        app.add_middleware(AuthMiddleware)
    
    Or use the dependency injection approach with get_current_user.
    """
    
    # Endpoints that don't require authentication
    PUBLIC_PATHS = {
        "/health",
        "/",
        "/docs",
        "/openapi.json",
        "/redoc",
    }
    
    # Path prefixes that don't require authentication
    PUBLIC_PREFIXES = (
        "/health",
        "/docs",
        "/openapi",
        "/redoc",
    )

    def __init__(self, app):
        self.app = app

    async def __call__(self, scope, receive, send):
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return
            
        path = scope.get("path", "")
        
        # Skip auth for public paths
        if path in self.PUBLIC_PATHS or path.startswith(self.PUBLIC_PREFIXES):
            await self.app(scope, receive, send)
            return
            
        # Skip auth if disabled
        if not SECURITY_ENABLED:
            await self.app(scope, receive, send)
            return
            
        await self.app(scope, receive, send)


async def get_current_user(
    request: Request,
    credentials: Optional[HTTPAuthorizationCredentials] = Depends(bearer_scheme),
) -> Optional[JWTPayload]:
    """
    FastAPI dependency to get the current authenticated user.
    
    Returns None if:
    - Security is disabled
    - No token is provided
    - Token is invalid
    
    Usage:
        @app.get("/protected")
        async def protected_endpoint(user: JWTPayload = Depends(get_current_user)):
            if user is None:
                raise HTTPException(status_code=401)
            return {"user": user.username}
    """
    if not SECURITY_ENABLED:
        # Return a dummy user when security is disabled
        return JWTPayload(
            user_id="dev-user",
            username="developer",
            role="admin",
            exp=0,
            iat=0,
        )
    
    if credentials is None:
        return None
        
    token = credentials.credentials
    return verify_jwt_token(token)


def require_auth(
    roles: Optional[list[str]] = None,
) -> Callable:
    """
    FastAPI dependency factory that requires authentication.
    
    Args:
        roles: Optional list of required roles (e.g., ["admin", "operator"])
        
    Usage:
        @app.get("/admin-only")
        async def admin_endpoint(user: JWTPayload = Depends(require_auth(roles=["admin"]))):
            return {"message": "Admin access granted"}
            
        @app.get("/authenticated")
        async def auth_endpoint(user: JWTPayload = Depends(require_auth())):
            return {"message": f"Hello {user.username}"}
    """
    async def dependency(
        user: Optional[JWTPayload] = Depends(get_current_user),
    ) -> JWTPayload:
        if user is None:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Authentication required",
                headers={"WWW-Authenticate": "Bearer"},
            )
        
        if roles:
            user_role = user.role.lower()
            allowed_roles = [r.lower() for r in roles]
            
            # Admin has access to everything
            if user_role == "admin":
                return user
                
            if user_role not in allowed_roles:
                raise HTTPException(
                    status_code=status.HTTP_403_FORBIDDEN,
                    detail=f"Requires one of the following roles: {', '.join(roles)}",
                )
        
        return user
    
    return dependency


def require_admin() -> Callable:
    """Shorthand for require_auth(roles=["admin"])"""
    return require_auth(roles=["admin"])


def require_operator() -> Callable:
    """Shorthand for require_auth(roles=["admin", "operator"])"""
    return require_auth(roles=["admin", "operator"])

```

---

## backend/autonomous-crawler-service/src/captcha/__init__.py

```py
"""CAPTCHA solving module using open-source solutions."""

import asyncio
import base64
import os
import tempfile
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum
from typing import Any

import structlog

logger = structlog.get_logger(__name__)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Core Types (defined first to avoid circular imports)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class CaptchaType(str, Enum):
    """Types of CAPTCHAs."""

    RECAPTCHA_V2 = "recaptcha_v2"
    RECAPTCHA_V3 = "recaptcha_v3"
    HCAPTCHA = "hcaptcha"
    IMAGE = "image"
    AUDIO = "audio"
    CLOUDFLARE = "cloudflare"


@dataclass
class CaptchaSolution:
    """Result of CAPTCHA solving attempt."""

    success: bool
    token: str | None = None
    error: str | None = None
    solver_used: str | None = None
    time_ms: float = 0


class CaptchaSolver(ABC):
    """Abstract base class for CAPTCHA solvers."""

    @property
    @abstractmethod
    def name(self) -> str:
        """Solver name."""
        pass

    @abstractmethod
    async def solve(
        self,
        captcha_type: CaptchaType,
        **kwargs,
    ) -> CaptchaSolution:
        """Solve a CAPTCHA."""
        pass

    @abstractmethod
    async def health_check(self) -> bool:
        """Check if solver is available."""
        pass


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Import submodules (after core types are defined)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from src.captcha.stealth import (
    StealthConfig,
    EnhancedStealthConfig,
    apply_stealth_to_playwright_async,
    get_undetected_browser_args,
    get_nopecha_extension_path,
    get_stealth_browser_args_with_extensions,
)
from src.captcha.nopecha import (
    NopeCHAConfig,
    NopeCHAExtensionManager,
    NopeCHAAPI,
    solve_captcha_with_nopecha,
)
from src.captcha.undetected import (
    UndetectedConfig,
    AdvancedStealthPatcher,
    HumanBehaviorSimulator,
    create_undetected_driver,
    get_enhanced_browser_args,
)
from src.captcha.camoufox_driver import (
    CamoufoxConfig,
    CamoufoxHelper,
    create_camoufox_browser,
    create_camoufox_browser_sync,
    get_recommended_camoufox_config,
    is_camoufox_available,
)
from src.captcha.paid_solvers import (
    CapSolverClient,
    CapSolverConfig,
    TwoCaptchaClient,
    TwoCaptchaConfig,
    create_paid_solver,
)

# Re-export for convenience
__all__ = [
    # Core CAPTCHA types and solvers
    "CaptchaType",
    "CaptchaSolution",
    "CaptchaSolver",
    "CaptchaSolverOrchestrator",
    "AudioRecaptchaSolver",
    "HCaptchaChallenger",
    "CloudflareBypasser",
    # Stealth
    "StealthConfig",
    "EnhancedStealthConfig",
    "apply_stealth_to_playwright_async",
    "get_undetected_browser_args",
    "get_nopecha_extension_path",
    "get_stealth_browser_args_with_extensions",
    # NopeCHA
    "NopeCHAConfig",
    "NopeCHAExtensionManager",
    "NopeCHAAPI",
    "solve_captcha_with_nopecha",
    # Undetected ChromeDriver
    "UndetectedConfig",
    "AdvancedStealthPatcher",
    "HumanBehaviorSimulator",
    "create_undetected_driver",
    "get_enhanced_browser_args",
    # Camoufox
    "CamoufoxConfig",
    "CamoufoxHelper",
    "create_camoufox_browser",
    "create_camoufox_browser_sync",
    "get_recommended_camoufox_config",
    "is_camoufox_available",
    # Paid solvers
    "CapSolverClient",
    "CapSolverConfig",
    "TwoCaptchaClient",
    "TwoCaptchaConfig",
    "create_paid_solver",
]


class AudioRecaptchaSolver(CaptchaSolver):
    """
    reCAPTCHA solver using audio challenge (GoogleRecaptchaBypass approach).

    Uses speech recognition to solve audio challenges.
    Requires: speech_recognition, pydub, ffmpeg
    """

    def __init__(self):
        self._sr = None
        self._pydub = None

    @property
    def name(self) -> str:
        return "audio_recaptcha"

    async def _lazy_import(self):
        """Lazy import heavy dependencies."""
        if self._sr is None:
            try:
                import speech_recognition as sr
                from pydub import AudioSegment

                self._sr = sr
                self._pydub = AudioSegment
            except ImportError as e:
                logger.error(
                    "Missing dependencies for audio solver",
                    error=str(e),
                    hint="pip install SpeechRecognition pydub",
                )
                raise

    async def solve(
        self,
        captcha_type: CaptchaType,
        audio_data: bytes | None = None,
        audio_url: str | None = None,
        **kwargs,
    ) -> CaptchaSolution:
        """
        Solve reCAPTCHA using audio challenge.

        Args:
            captcha_type: Must be RECAPTCHA_V2 or AUDIO
            audio_data: Raw audio bytes
            audio_url: URL to download audio from
        """
        import time

        start = time.time()

        if captcha_type not in (CaptchaType.RECAPTCHA_V2, CaptchaType.AUDIO):
            return CaptchaSolution(
                success=False,
                error=f"Unsupported captcha type: {captcha_type}",
                solver_used=self.name,
            )

        try:
            await self._lazy_import()

            # Get audio data
            if audio_url and not audio_data:
                import httpx

                async with httpx.AsyncClient() as client:
                    resp = await client.get(audio_url)
                    audio_data = resp.content

            if not audio_data:
                return CaptchaSolution(
                    success=False,
                    error="No audio data provided",
                    solver_used=self.name,
                )

            # Convert and recognize
            text = await self._recognize_audio(audio_data)

            if text:
                return CaptchaSolution(
                    success=True,
                    token=text,
                    solver_used=self.name,
                    time_ms=(time.time() - start) * 1000,
                )
            else:
                return CaptchaSolution(
                    success=False,
                    error="Could not recognize audio",
                    solver_used=self.name,
                    time_ms=(time.time() - start) * 1000,
                )

        except Exception as e:
            return CaptchaSolution(
                success=False,
                error=str(e),
                solver_used=self.name,
                time_ms=(time.time() - start) * 1000,
            )

    async def _recognize_audio(self, audio_data: bytes) -> str | None:
        """Convert audio to text using speech recognition."""
        with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as f:
            f.write(audio_data)
            mp3_path = f.name

        wav_path = mp3_path.replace(".mp3", ".wav")

        try:
            # Convert MP3 to WAV
            audio = self._pydub.from_mp3(mp3_path)
            audio.export(wav_path, format="wav")

            # Recognize speech
            recognizer = self._sr.Recognizer()
            with self._sr.AudioFile(wav_path) as source:
                audio_data = recognizer.record(source)

            # Try Google Speech Recognition (free)
            try:
                text = recognizer.recognize_google(audio_data)
                return text
            except self._sr.UnknownValueError:
                logger.warning("Google Speech Recognition could not understand audio")
                return None

        finally:
            # Cleanup
            for path in [mp3_path, wav_path]:
                if os.path.exists(path):
                    os.remove(path)

    async def health_check(self) -> bool:
        """Check if dependencies are available."""
        try:
            await self._lazy_import()
            return True
        except Exception:
            return False


class HCaptchaChallenger(CaptchaSolver):
    """
    hCaptcha solver using hcaptcha-challenger library.

    Uses AI models (YOLO, ResNet) to solve image challenges.
    Requires: hcaptcha-challenger
    """

    def __init__(self, model_dir: str | None = None):
        self.model_dir = model_dir
        self._challenger = None

    @property
    def name(self) -> str:
        return "hcaptcha_challenger"

    async def _lazy_import(self):
        """Lazy import hcaptcha-challenger."""
        if self._challenger is None:
            try:
                from hcaptcha_challenger import AgentChallenger

                self._challenger = AgentChallenger
            except ImportError as e:
                logger.error(
                    "Missing hcaptcha-challenger",
                    error=str(e),
                    hint="pip install hcaptcha-challenger",
                )
                raise

    async def solve(
        self,
        captcha_type: CaptchaType,
        page: Any = None,  # Playwright page
        **kwargs,
    ) -> CaptchaSolution:
        """
        Solve hCaptcha on a Playwright page.

        Args:
            captcha_type: Must be HCAPTCHA
            page: Playwright page object
        """
        import time

        start = time.time()

        if captcha_type != CaptchaType.HCAPTCHA:
            return CaptchaSolution(
                success=False,
                error=f"Unsupported captcha type: {captcha_type}",
                solver_used=self.name,
            )

        if not page:
            return CaptchaSolution(
                success=False,
                error="Playwright page required",
                solver_used=self.name,
            )

        try:
            await self._lazy_import()

            challenger = self._challenger(page)
            result = await challenger.solve()

            return CaptchaSolution(
                success=result,
                solver_used=self.name,
                time_ms=(time.time() - start) * 1000,
            )

        except Exception as e:
            return CaptchaSolution(
                success=False,
                error=str(e),
                solver_used=self.name,
                time_ms=(time.time() - start) * 1000,
            )

    async def health_check(self) -> bool:
        """Check if library is available."""
        try:
            await self._lazy_import()
            return True
        except Exception:
            return False


class CloudflareBypasser(CaptchaSolver):
    """
    Cloudflare bypass using cloudscraper library.

    Handles Cloudflare's JavaScript challenges and Turnstile.
    Requires: cloudscraper
    """

    def __init__(self):
        self._scraper = None

    @property
    def name(self) -> str:
        return "cloudscraper"

    def _get_scraper(self):
        """Get or create cloudscraper instance."""
        if self._scraper is None:
            try:
                import cloudscraper

                self._scraper = cloudscraper.create_scraper(
                    browser={
                        "browser": "chrome",
                        "platform": "windows",
                        "mobile": False,
                    },
                    delay=10,
                )
            except ImportError as e:
                logger.error(
                    "Missing cloudscraper",
                    error=str(e),
                    hint="pip install cloudscraper",
                )
                raise
        return self._scraper

    async def solve(
        self,
        captcha_type: CaptchaType,
        url: str | None = None,
        **kwargs,
    ) -> CaptchaSolution:
        """
        Bypass Cloudflare protection.

        Args:
            captcha_type: Must be CLOUDFLARE
            url: URL to access through Cloudflare
        """
        import time

        start = time.time()

        if captcha_type != CaptchaType.CLOUDFLARE:
            return CaptchaSolution(
                success=False,
                error=f"Unsupported captcha type: {captcha_type}",
                solver_used=self.name,
            )

        if not url:
            return CaptchaSolution(
                success=False,
                error="URL required",
                solver_used=self.name,
            )

        try:
            scraper = self._get_scraper()

            # Execute in thread pool since cloudscraper is synchronous
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: scraper.get(url),
            )

            if response.status_code == 200:
                return CaptchaSolution(
                    success=True,
                    token=response.text[:100],  # First 100 chars as verification
                    solver_used=self.name,
                    time_ms=(time.time() - start) * 1000,
                )
            else:
                return CaptchaSolution(
                    success=False,
                    error=f"HTTP {response.status_code}",
                    solver_used=self.name,
                    time_ms=(time.time() - start) * 1000,
                )

        except Exception as e:
            return CaptchaSolution(
                success=False,
                error=str(e),
                solver_used=self.name,
                time_ms=(time.time() - start) * 1000,
            )

    async def get_session_cookies(self, url: str) -> dict[str, str]:
        """Get Cloudflare bypass cookies for a URL."""
        try:
            scraper = self._get_scraper()
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, lambda: scraper.get(url))
            return dict(scraper.cookies)
        except Exception as e:
            logger.error("Failed to get Cloudflare cookies", url=url, error=str(e))
            return {}

    async def health_check(self) -> bool:
        """Check if cloudscraper is available."""
        try:
            self._get_scraper()
            return True
        except Exception:
            return False


class CaptchaSolverOrchestrator:
    """
    Orchestrates multiple CAPTCHA solvers.

    Tries different solvers based on CAPTCHA type and availability.
    Supports both free and paid solvers, with paid solvers prioritized when configured.
    """

    def __init__(
        self,
        capsolver_api_key: str = "",
        twocaptcha_api_key: str = "",
        prefer_paid: bool = True,
        paid_timeout: float = 120.0,
    ):
        """
        Initialize CAPTCHA solver orchestrator.

        Args:
            capsolver_api_key: CapSolver API key (recommended for Turnstile)
            twocaptcha_api_key: 2Captcha API key
            prefer_paid: If True, try paid solvers first when available
            paid_timeout: Timeout for paid solver requests
        """
        self.prefer_paid = prefer_paid

        # Initialize free solvers
        free_solvers: dict[CaptchaType, list[CaptchaSolver]] = {
            CaptchaType.RECAPTCHA_V2: [AudioRecaptchaSolver()],
            CaptchaType.AUDIO: [AudioRecaptchaSolver()],
            CaptchaType.HCAPTCHA: [HCaptchaChallenger()],
            CaptchaType.CLOUDFLARE: [CloudflareBypasser()],
        }

        # Initialize paid solvers if API keys provided
        paid_solvers: dict[CaptchaType, list[CaptchaSolver]] = {}

        if capsolver_api_key:
            capsolver = CapSolverClient(
                CapSolverConfig(api_key=capsolver_api_key, timeout=paid_timeout)
            )
            # CapSolver supports all major CAPTCHA types
            for ctype in [
                CaptchaType.RECAPTCHA_V2,
                CaptchaType.RECAPTCHA_V3,
                CaptchaType.HCAPTCHA,
                CaptchaType.CLOUDFLARE,
            ]:
                if ctype not in paid_solvers:
                    paid_solvers[ctype] = []
                paid_solvers[ctype].append(capsolver)
            logger.info("CapSolver enabled for CAPTCHA solving")

        if twocaptcha_api_key:
            twocaptcha = TwoCaptchaClient(
                TwoCaptchaConfig(api_key=twocaptcha_api_key, timeout=paid_timeout)
            )
            # 2Captcha also supports all major types
            for ctype in [
                CaptchaType.RECAPTCHA_V2,
                CaptchaType.RECAPTCHA_V3,
                CaptchaType.HCAPTCHA,
                CaptchaType.CLOUDFLARE,
            ]:
                if ctype not in paid_solvers:
                    paid_solvers[ctype] = []
                paid_solvers[ctype].append(twocaptcha)
            logger.info("2Captcha enabled for CAPTCHA solving")

        # Combine solvers: paid first if prefer_paid, else free first
        self.solvers: dict[CaptchaType, list[CaptchaSolver]] = {}
        all_types = set(free_solvers.keys()) | set(paid_solvers.keys())

        for ctype in all_types:
            paid = paid_solvers.get(ctype, [])
            free = free_solvers.get(ctype, [])

            if prefer_paid:
                self.solvers[ctype] = paid + free
            else:
                self.solvers[ctype] = free + paid

        logger.info(
            "CAPTCHA solver orchestrator initialized",
            paid_solvers_enabled=bool(capsolver_api_key or twocaptcha_api_key),
            prefer_paid=prefer_paid,
            supported_types=list(self.solvers.keys()),
        )

    def add_solver(self, captcha_type: CaptchaType, solver: CaptchaSolver):
        """Add a solver for a captcha type."""
        if captcha_type not in self.solvers:
            self.solvers[captcha_type] = []
        self.solvers[captcha_type].append(solver)

    async def solve(
        self,
        captcha_type: CaptchaType,
        **kwargs,
    ) -> CaptchaSolution:
        """
        Try to solve CAPTCHA using available solvers.

        Args:
            captcha_type: Type of CAPTCHA
            **kwargs: Solver-specific arguments

        Returns:
            CaptchaSolution with result
        """
        solvers = self.solvers.get(captcha_type, [])

        if not solvers:
            return CaptchaSolution(
                success=False,
                error=f"No solver available for {captcha_type}",
            )

        errors = []
        for solver in solvers:
            try:
                # Check health first
                if not await solver.health_check():
                    errors.append(f"{solver.name}: not available")
                    continue

                result = await solver.solve(captcha_type, **kwargs)

                if result.success:
                    logger.info(
                        "CAPTCHA solved",
                        captcha_type=captcha_type.value,
                        solver=solver.name,
                        time_ms=result.time_ms,
                    )
                    return result
                else:
                    errors.append(f"{solver.name}: {result.error}")

            except Exception as e:
                errors.append(f"{solver.name}: {str(e)}")

        return CaptchaSolution(
            success=False,
            error=f"All solvers failed: {'; '.join(errors)}",
        )

    async def get_available_solvers(self) -> dict[str, list[str]]:
        """Get list of available solvers by captcha type."""
        available = {}
        for captcha_type, solvers in self.solvers.items():
            available[captcha_type.value] = []
            for solver in solvers:
                if await solver.health_check():
                    available[captcha_type.value].append(solver.name)
        return available

```

---

## backend/autonomous-crawler-service/src/captcha/camoufox_driver.py

```py
"""
Camoufox Firefox-based Anti-Detect Browser Integration.

Camoufox is a Firefox-based anti-detect browser that provides:
- Advanced fingerprint spoofing
- Human-like behavior simulation
- Cloudflare Turnstile bypass
- Compatible with Playwright API

Installation:
    pip install camoufox[geoip]
    python -m camoufox fetch
"""

import asyncio
from dataclasses import dataclass, field
from typing import Any

import structlog

logger = structlog.get_logger(__name__)


@dataclass
class CamoufoxConfig:
    """Configuration for Camoufox browser."""
    
    # Display mode
    headless: bool = True
    
    # Human-like behavior simulation
    humanize: bool = True
    humanize_level: int = 2  # 1-3, higher = more human-like
    
    # Fingerprint options
    os: str | None = None  # "windows", "macos", "linux" or None for random
    screen_width: int | None = None
    screen_height: int | None = None
    
    # Locale/timezone
    locale: str = "ko-KR"
    timezone: str = "Asia/Seoul"
    
    # Geolocation (requires geoip addon)
    geoip: bool = True
    
    # Proxy
    proxy: str | None = None
    
    # Browser settings
    block_images: bool = False
    block_webrtc: bool = True
    
    # Extra Firefox preferences
    firefox_prefs: dict[str, Any] = field(default_factory=dict)


def is_camoufox_available() -> bool:
    """Check if Camoufox is installed and available."""
    try:
        import camoufox
        return True
    except ImportError:
        return False


async def create_camoufox_browser(
    config: CamoufoxConfig | None = None,
) -> Any:
    """
    Create a Camoufox browser instance using async API.
    
    Args:
        config: Camoufox configuration
        
    Returns:
        Browser context or None if not available
    """
    if config is None:
        config = CamoufoxConfig()
    
    if not is_camoufox_available():
        logger.warning("Camoufox not installed. Install with: pip install camoufox[geoip]")
        return None
    
    try:
        from camoufox.async_api import AsyncCamoufox
        
        # Build kwargs
        kwargs = {
            "headless": config.headless,
            "humanize": config.humanize,
        }
        
        if config.os:
            kwargs["os"] = config.os
        
        if config.screen_width and config.screen_height:
            kwargs["screen"] = {"width": config.screen_width, "height": config.screen_height}
        
        if config.locale:
            kwargs["locale"] = config.locale
            
        if config.timezone:
            kwargs["timezone"] = config.timezone
        
        if config.geoip:
            kwargs["geoip"] = True
        
        if config.proxy:
            kwargs["proxy"] = {"server": config.proxy}
        
        if config.block_webrtc:
            kwargs["block_webrtc"] = True
        
        if config.block_images:
            kwargs["block_images"] = True
        
        # Apply extra Firefox preferences
        if config.firefox_prefs:
            kwargs["firefox_prefs"] = config.firefox_prefs
        
        # Create browser
        camoufox = AsyncCamoufox(**kwargs)
        browser = await camoufox.__aenter__()
        
        logger.info("Created Camoufox browser", headless=config.headless, humanize=config.humanize)
        return browser
        
    except Exception as e:
        logger.error("Failed to create Camoufox browser", error=str(e))
        return None


def create_camoufox_browser_sync(
    config: CamoufoxConfig | None = None,
) -> Any:
    """
    Create a Camoufox browser instance using sync API.
    
    Args:
        config: Camoufox configuration
        
    Returns:
        Browser context or None if not available
    """
    if config is None:
        config = CamoufoxConfig()
    
    if not is_camoufox_available():
        logger.warning("Camoufox not installed")
        return None
    
    try:
        from camoufox.sync_api import Camoufox
        
        kwargs = {
            "headless": config.headless,
            "humanize": config.humanize,
        }
        
        if config.os:
            kwargs["os"] = config.os
        
        if config.locale:
            kwargs["locale"] = config.locale
            
        if config.timezone:
            kwargs["timezone"] = config.timezone
        
        if config.geoip:
            kwargs["geoip"] = True
        
        if config.proxy:
            kwargs["proxy"] = {"server": config.proxy}
        
        if config.block_webrtc:
            kwargs["block_webrtc"] = True
        
        camoufox = Camoufox(**kwargs)
        browser = camoufox.__enter__()
        
        logger.info("Created Camoufox browser (sync)", headless=config.headless)
        return browser
        
    except Exception as e:
        logger.error("Failed to create Camoufox browser", error=str(e))
        return None


class CamoufoxHelper:
    """Helper utilities for Camoufox browser automation."""
    
    @staticmethod
    async def wait_for_cloudflare(page: Any, timeout: int = 30) -> bool:
        """
        Wait for Cloudflare challenge to complete.
        
        Camoufox handles Cloudflare automatically in most cases,
        but this provides explicit waiting if needed.
        
        Args:
            page: Camoufox page object
            timeout: Maximum wait time in seconds
            
        Returns:
            True if challenge passed, False if timeout
        """
        try:
            # Common Cloudflare challenge indicators
            challenge_selectors = [
                "#challenge-running",
                "#challenge-stage",
                ".cf-browser-verification",
                "#trk_jschal_js",
            ]
            
            start_time = asyncio.get_event_loop().time()
            
            while asyncio.get_event_loop().time() - start_time < timeout:
                # Check if any challenge elements are visible
                is_challenging = False
                
                for selector in challenge_selectors:
                    try:
                        element = await page.query_selector(selector)
                        if element:
                            is_visible = await element.is_visible()
                            if is_visible:
                                is_challenging = True
                                break
                    except Exception:
                        continue
                
                if not is_challenging:
                    # No challenge visible, likely passed
                    logger.debug("Cloudflare challenge completed")
                    return True
                
                await asyncio.sleep(0.5)
            
            logger.warning("Cloudflare challenge timeout")
            return False
            
        except Exception as e:
            logger.error("Error waiting for Cloudflare", error=str(e))
            return False
    
    @staticmethod
    async def solve_turnstile(page: Any, timeout: int = 30) -> bool:
        """
        Wait for Cloudflare Turnstile CAPTCHA to complete.
        
        Camoufox with humanize=True should handle most Turnstile
        challenges automatically.
        
        Args:
            page: Camoufox page object
            timeout: Maximum wait time in seconds
            
        Returns:
            True if solved, False if timeout
        """
        try:
            turnstile_selectors = [
                "iframe[src*='turnstile']",
                "#cf-turnstile",
                ".cf-turnstile",
            ]
            
            start_time = asyncio.get_event_loop().time()
            
            # First, wait for turnstile to appear
            turnstile_frame = None
            while asyncio.get_event_loop().time() - start_time < timeout / 2:
                for selector in turnstile_selectors:
                    try:
                        element = await page.query_selector(selector)
                        if element:
                            turnstile_frame = element
                            break
                    except Exception:
                        continue
                
                if turnstile_frame:
                    break
                    
                await asyncio.sleep(0.3)
            
            if not turnstile_frame:
                # No turnstile found, may not be needed
                logger.debug("No Turnstile CAPTCHA found")
                return True
            
            logger.debug("Turnstile CAPTCHA detected, waiting for auto-solve...")
            
            # Wait for turnstile to complete (it should auto-solve with humanize)
            # Check for success indicator or turnstile disappearing
            while asyncio.get_event_loop().time() - start_time < timeout:
                # Check if turnstile is still visible
                try:
                    is_visible = await turnstile_frame.is_visible()
                    if not is_visible:
                        logger.info("Turnstile CAPTCHA solved")
                        return True
                except Exception:
                    # Element may have been removed
                    return True
                
                # Check for success response in page
                try:
                    response = await page.evaluate("""
                        () => {
                            const input = document.querySelector('[name="cf-turnstile-response"]');
                            return input ? input.value : null;
                        }
                    """)
                    if response:
                        logger.info("Turnstile response received")
                        return True
                except Exception:
                    pass
                
                await asyncio.sleep(0.5)
            
            logger.warning("Turnstile solve timeout")
            return False
            
        except Exception as e:
            logger.error("Error solving Turnstile", error=str(e))
            return False
    
    @staticmethod
    async def extract_page_content(page: Any) -> dict[str, Any]:
        """
        Extract main content from a page.
        
        Args:
            page: Camoufox page object
            
        Returns:
            Dictionary with extracted content
        """
        try:
            content = await page.evaluate("""
                () => {
                    const result = {
                        title: document.title,
                        url: window.location.href,
                        text: '',
                        links: [],
                        images: [],
                    };
                    
                    // Get main text content
                    const article = document.querySelector('article') || 
                                   document.querySelector('main') || 
                                   document.body;
                    
                    if (article) {
                        result.text = article.innerText;
                    }
                    
                    // Get links
                    const links = document.querySelectorAll('a[href]');
                    links.forEach(link => {
                        if (link.href && link.href.startsWith('http')) {
                            result.links.push({
                                href: link.href,
                                text: link.innerText.trim().substring(0, 200)
                            });
                        }
                    });
                    
                    // Get images
                    const images = document.querySelectorAll('img[src]');
                    images.forEach(img => {
                        if (img.src && img.src.startsWith('http')) {
                            result.images.push({
                                src: img.src,
                                alt: img.alt || ''
                            });
                        }
                    });
                    
                    return result;
                }
            """)
            
            return content
            
        except Exception as e:
            logger.error("Failed to extract page content", error=str(e))
            return {"error": str(e)}


# Firefox-specific preferences for anti-detection
FIREFOX_ANTI_DETECT_PREFS = {
    # Disable WebRTC IP leak
    "media.peerconnection.enabled": False,
    "media.peerconnection.ice.no_host": True,
    "media.peerconnection.ice.default_address_only": True,
    
    # Disable tracking
    "privacy.trackingprotection.enabled": True,
    "privacy.trackingprotection.socialtracking.enabled": True,
    
    # Fingerprint resistance
    "privacy.resistFingerprinting": False,  # Camoufox handles this
    
    # Disable telemetry
    "toolkit.telemetry.enabled": False,
    "toolkit.telemetry.unified": False,
    "toolkit.telemetry.archive.enabled": False,
    
    # Disable crash reporter
    "breakpad.reportURL": "",
    "browser.crashReports.unsubmittedCheck.autoSubmit2": False,
    
    # Disable prefetch
    "network.prefetch-next": False,
    "network.dns.disablePrefetch": True,
    
    # Disable speculative connections
    "network.http.speculative-parallel-limit": 0,
    
    # Improve privacy
    "dom.battery.enabled": False,
    "geo.enabled": False,
    "media.navigator.enabled": False,
    
    # Performance
    "browser.cache.disk.enable": False,
    "browser.cache.memory.enable": True,
}


def get_recommended_camoufox_config(
    purpose: str = "general",
    headless: bool = True,
) -> CamoufoxConfig:
    """
    Get recommended Camoufox configuration for different purposes.
    
    Args:
        purpose: "general", "scraping", "turnstile", "cloudflare"
        headless: Whether to run headless
        
    Returns:
        Optimized CamoufoxConfig
    """
    base_config = CamoufoxConfig(
        headless=headless,
        humanize=True,
        humanize_level=2,
        locale="ko-KR",
        timezone="Asia/Seoul",
        geoip=True,
        block_webrtc=True,
    )
    
    if purpose == "scraping":
        base_config.block_images = True
        base_config.humanize_level = 1
    
    elif purpose == "turnstile":
        base_config.humanize_level = 3
        base_config.block_images = False
    
    elif purpose == "cloudflare":
        base_config.humanize_level = 3
        base_config.block_images = False
        base_config.firefox_prefs = FIREFOX_ANTI_DETECT_PREFS
    
    return base_config

```

---

## backend/autonomous-crawler-service/src/captcha/nopecha.py

```py
"""
NopeCHA CAPTCHA Solver Integration.

NopeCHA is a free/open-source CAPTCHA solving extension that supports:
- reCAPTCHA v2/v3
- hCaptcha
- FunCAPTCHA
- AWS WAF
- Turnstile
- Text CAPTCHA

Usage:
1. Browser extension (Chrome/Firefox)
2. API integration for headless automation
"""

import asyncio
import base64
import json
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Literal

import aiohttp
import structlog

logger = structlog.get_logger(__name__)

# NopeCHA Chrome Extension ID
NOPECHA_EXTENSION_ID = "dknlfmjaanfblgfdfebhijalfmhmjjjo"
NOPECHA_CRX_URL = f"https://clients2.google.com/service/update2/crx?response=redirect&prodversion=133&acceptformat=crx3&x=id%3D{NOPECHA_EXTENSION_ID}%26uc"


@dataclass
class NopeCHAConfig:
    """NopeCHA configuration."""
    
    # API key (optional, for faster solving via API)
    api_key: str = ""
    
    # Extension settings
    enabled: bool = True
    auto_solve: bool = True
    
    # Solve settings
    solve_delay_ms: int = 500
    max_retries: int = 3
    
    # Supported CAPTCHA types
    solve_recaptcha: bool = True
    solve_hcaptcha: bool = True
    solve_funcaptcha: bool = True
    solve_turnstile: bool = True
    solve_text: bool = True
    
    # Audio solving for accessibility (free reCAPTCHA bypass)
    use_audio_challenge: bool = True
    
    # Extension cache directory
    cache_dir: Path = field(default_factory=lambda: Path("/tmp/nopecha-extension"))


class NopeCHAExtensionManager:
    """Manage NopeCHA extension installation and configuration."""
    
    def __init__(self, config: NopeCHAConfig | None = None):
        self.config = config or NopeCHAConfig()
    
    async def download_extension(self) -> Path:
        """Download NopeCHA extension CRX file."""
        cache_dir = self.config.cache_dir
        cache_dir.mkdir(parents=True, exist_ok=True)
        
        crx_path = cache_dir / f"{NOPECHA_EXTENSION_ID}.crx"
        ext_dir = cache_dir / NOPECHA_EXTENSION_ID
        
        # Check if already downloaded and extracted
        if ext_dir.exists() and (ext_dir / "manifest.json").exists():
            logger.debug("NopeCHA extension already cached", path=str(ext_dir))
            return ext_dir
        
        # Download CRX
        logger.info("Downloading NopeCHA extension...")
        async with aiohttp.ClientSession() as session:
            async with session.get(NOPECHA_CRX_URL, allow_redirects=True) as resp:
                if resp.status == 200:
                    content = await resp.read()
                    with open(crx_path, 'wb') as f:
                        f.write(content)
                    logger.info("NopeCHA extension downloaded", path=str(crx_path))
                else:
                    raise Exception(f"Failed to download NopeCHA: HTTP {resp.status}")
        
        # Extract CRX
        await self._extract_crx(crx_path, ext_dir)
        
        # Apply configuration
        await self._configure_extension(ext_dir)
        
        return ext_dir
    
    async def _extract_crx(self, crx_path: Path, extract_dir: Path) -> None:
        """Extract CRX file to directory."""
        import zipfile
        import shutil
        
        if extract_dir.exists():
            shutil.rmtree(extract_dir)
        extract_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            with zipfile.ZipFile(crx_path, 'r') as zip_ref:
                zip_ref.extractall(extract_dir)
        except zipfile.BadZipFile:
            # CRX has a header, skip it
            with open(crx_path, 'rb') as f:
                magic = f.read(4)
                if magic != b'Cr24':
                    raise Exception("Invalid CRX format")
                
                version = int.from_bytes(f.read(4), 'little')
                if version == 2:
                    pubkey_len = int.from_bytes(f.read(4), 'little')
                    sig_len = int.from_bytes(f.read(4), 'little')
                    f.seek(16 + pubkey_len + sig_len)
                elif version == 3:
                    header_len = int.from_bytes(f.read(4), 'little')
                    f.seek(12 + header_len)
                
                zip_data = f.read()
            
            import tempfile
            with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as tmp:
                tmp.write(zip_data)
                tmp.flush()
                with zipfile.ZipFile(tmp.name, 'r') as zip_ref:
                    zip_ref.extractall(extract_dir)
                Path(tmp.name).unlink()
        
        logger.info("NopeCHA extension extracted", path=str(extract_dir))
    
    async def _configure_extension(self, ext_dir: Path) -> None:
        """Configure NopeCHA extension settings."""
        # Create settings file for extension
        settings = {
            "key": self.config.api_key,
            "enabled": self.config.enabled,
            "auto_solve": self.config.auto_solve,
            "delay": self.config.solve_delay_ms,
            "recaptcha": self.config.solve_recaptcha,
            "hcaptcha": self.config.solve_hcaptcha,
            "funcaptcha": self.config.solve_funcaptcha,
            "turnstile": self.config.solve_turnstile,
            "textcaptcha": self.config.solve_text,
            "audio": self.config.use_audio_challenge,
        }
        
        settings_path = ext_dir / "settings.json"
        with open(settings_path, 'w') as f:
            json.dump(settings, f)
        
        logger.debug("NopeCHA settings configured", settings=settings)
    
    def get_extension_path(self) -> str:
        """Get the path to the extracted extension directory."""
        ext_dir = self.config.cache_dir / NOPECHA_EXTENSION_ID
        if ext_dir.exists():
            return str(ext_dir)
        return ""


class NopeCHAAPI:
    """NopeCHA API client for programmatic CAPTCHA solving."""
    
    API_BASE = "https://api.nopecha.com"
    
    def __init__(self, api_key: str = ""):
        self.api_key = api_key
    
    async def solve_recaptcha(
        self,
        site_key: str,
        site_url: str,
        version: Literal["v2", "v3"] = "v2",
        action: str = "",
        invisible: bool = False,
    ) -> str | None:
        """
        Solve reCAPTCHA using NopeCHA API.
        
        Args:
            site_key: reCAPTCHA site key
            site_url: URL of the page with CAPTCHA
            version: reCAPTCHA version (v2 or v3)
            action: Action for v3 scoring
            invisible: Whether CAPTCHA is invisible
            
        Returns:
            Solution token or None if failed
        """
        if not self.api_key:
            logger.warning("NopeCHA API key not configured, using extension mode")
            return None
        
        payload = {
            "key": self.api_key,
            "type": "recaptcha2" if version == "v2" else "recaptcha3",
            "sitekey": site_key,
            "url": site_url,
        }
        
        if version == "v3" and action:
            payload["action"] = action
        if invisible:
            payload["invisible"] = True
        
        return await self._solve(payload)
    
    async def solve_hcaptcha(
        self,
        site_key: str,
        site_url: str,
    ) -> str | None:
        """Solve hCaptcha using NopeCHA API."""
        if not self.api_key:
            return None
        
        payload = {
            "key": self.api_key,
            "type": "hcaptcha",
            "sitekey": site_key,
            "url": site_url,
        }
        
        return await self._solve(payload)
    
    async def solve_turnstile(
        self,
        site_key: str,
        site_url: str,
    ) -> str | None:
        """Solve Cloudflare Turnstile using NopeCHA API."""
        if not self.api_key:
            return None
        
        payload = {
            "key": self.api_key,
            "type": "turnstile",
            "sitekey": site_key,
            "url": site_url,
        }
        
        return await self._solve(payload)
    
    async def solve_image_captcha(
        self,
        image_base64: str,
        captcha_type: str = "text",
    ) -> str | None:
        """Solve image-based CAPTCHA."""
        if not self.api_key:
            return None
        
        payload = {
            "key": self.api_key,
            "type": captcha_type,
            "image": image_base64,
        }
        
        return await self._solve(payload)
    
    async def _solve(self, payload: dict[str, Any]) -> str | None:
        """Send solve request to NopeCHA API."""
        try:
            async with aiohttp.ClientSession() as session:
                # Create task
                async with session.post(
                    f"{self.API_BASE}/",
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=30),
                ) as resp:
                    result = await resp.json()
                    
                    if "error" in result:
                        logger.error("NopeCHA API error", error=result.get("error"))
                        return None
                    
                    task_id = result.get("data")
                    if not task_id:
                        return None
                
                # Poll for result
                for _ in range(60):  # Max 60 seconds
                    await asyncio.sleep(1)
                    
                    async with session.get(
                        f"{self.API_BASE}/?key={self.api_key}&id={task_id}",
                        timeout=aiohttp.ClientTimeout(total=10),
                    ) as poll_resp:
                        poll_result = await poll_resp.json()
                        
                        if "error" in poll_result:
                            error = poll_result.get("error")
                            if error == "Incomplete job":
                                continue
                            logger.error("NopeCHA polling error", error=error)
                            return None
                        
                        if "data" in poll_result:
                            return poll_result["data"]
                
                logger.warning("NopeCHA solve timeout")
                return None
                
        except Exception as e:
            logger.error("NopeCHA API request failed", error=str(e))
            return None


# Helper function for quick CAPTCHA solving
async def solve_captcha_with_nopecha(
    captcha_type: Literal["recaptcha2", "recaptcha3", "hcaptcha", "turnstile"],
    site_key: str,
    site_url: str,
    api_key: str = "",
) -> str | None:
    """
    Quick helper to solve CAPTCHA using NopeCHA.
    
    Args:
        captcha_type: Type of CAPTCHA
        site_key: CAPTCHA site key
        site_url: URL of the page
        api_key: NopeCHA API key (optional)
        
    Returns:
        Solution token or None
    """
    client = NopeCHAAPI(api_key)
    
    if captcha_type == "recaptcha2":
        return await client.solve_recaptcha(site_key, site_url, "v2")
    elif captcha_type == "recaptcha3":
        return await client.solve_recaptcha(site_key, site_url, "v3")
    elif captcha_type == "hcaptcha":
        return await client.solve_hcaptcha(site_key, site_url)
    elif captcha_type == "turnstile":
        return await client.solve_turnstile(site_key, site_url)
    
    return None

```

---

## backend/autonomous-crawler-service/src/captcha/paid_solvers.py

```py
"""
Paid CAPTCHA Solver Integrations.

Integrates with reliable paid CAPTCHA solving services:
- CapSolver (https://capsolver.com) - Recommended, supports Turnstile
- 2Captcha (https://2captcha.com) - Widely used, reliable

These services are more reliable than free solutions for:
- reCAPTCHA v2/v3
- hCaptcha
- Cloudflare Turnstile
- FunCAPTCHA
- Image CAPTCHAs
"""

import asyncio
import time
from dataclasses import dataclass
from typing import Any, Literal

import httpx
import structlog

from src.captcha import CaptchaType, CaptchaSolution, CaptchaSolver

logger = structlog.get_logger(__name__)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CapSolver Integration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@dataclass
class CapSolverConfig:
    """CapSolver configuration."""

    api_key: str = ""
    base_url: str = "https://api.capsolver.com"
    timeout: float = 120.0
    poll_interval: float = 3.0


class CapSolverClient(CaptchaSolver):
    """
    CapSolver CAPTCHA solving service client.

    Supports:
    - reCAPTCHA v2/v3
    - hCaptcha
    - Cloudflare Turnstile
    - FunCAPTCHA
    - Image CAPTCHA

    Docs: https://docs.capsolver.com/
    """

    def __init__(self, config: CapSolverConfig | None = None):
        self.config = config or CapSolverConfig()
        self._client: httpx.AsyncClient | None = None

    @property
    def name(self) -> str:
        return "capsolver"

    async def _get_client(self) -> httpx.AsyncClient:
        if self._client is None:
            self._client = httpx.AsyncClient(
                base_url=self.config.base_url,
                timeout=self.config.timeout,
            )
        return self._client

    async def health_check(self) -> bool:
        """Check if CapSolver API is available and key is valid."""
        if not self.config.api_key:
            return False

        try:
            client = await self._get_client()
            resp = await client.post(
                "/getBalance",
                json={"clientKey": self.config.api_key},
            )
            data = resp.json()
            if data.get("errorId") == 0:
                balance = data.get("balance", 0)
                logger.debug("CapSolver balance check", balance=balance)
                return balance > 0
            return False
        except Exception as e:
            logger.debug("CapSolver health check failed", error=str(e))
            return False

    async def solve(
        self,
        captcha_type: CaptchaType,
        site_key: str | None = None,
        site_url: str | None = None,
        page: Any = None,
        **kwargs,
    ) -> CaptchaSolution:
        """
        Solve CAPTCHA using CapSolver API.

        Args:
            captcha_type: Type of CAPTCHA
            site_key: Site key for reCAPTCHA/hCaptcha
            site_url: URL of the page with CAPTCHA
            page: Playwright page (for extracting site_key if not provided)
        """
        start_time = time.time()

        if not self.config.api_key:
            return CaptchaSolution(
                success=False,
                error="CapSolver API key not configured",
                solver_used=self.name,
            )

        try:
            # Extract site_key from page if not provided
            if not site_key and page:
                site_key = await self._extract_site_key(page, captcha_type)
                site_url = page.url

            if not site_key:
                return CaptchaSolution(
                    success=False,
                    error="Site key not provided or could not be extracted",
                    solver_used=self.name,
                )

            # Map CAPTCHA type to CapSolver task type
            task_type = self._get_task_type(captcha_type)
            if not task_type:
                return CaptchaSolution(
                    success=False,
                    error=f"Unsupported CAPTCHA type: {captcha_type}",
                    solver_used=self.name,
                )

            # Create task
            task_data = {
                "type": task_type,
                "websiteURL": site_url,
                "websiteKey": site_key,
            }

            # Add type-specific parameters
            if captcha_type == CaptchaType.RECAPTCHA_V3:
                task_data["pageAction"] = kwargs.get("action", "verify")
                task_data["minScore"] = kwargs.get("min_score", 0.7)

            client = await self._get_client()

            # Create task
            create_resp = await client.post(
                "/createTask",
                json={
                    "clientKey": self.config.api_key,
                    "task": task_data,
                },
            )
            create_data = create_resp.json()

            if create_data.get("errorId") != 0:
                return CaptchaSolution(
                    success=False,
                    error=create_data.get("errorDescription", "Unknown error"),
                    solver_used=self.name,
                    time_ms=(time.time() - start_time) * 1000,
                )

            task_id = create_data.get("taskId")
            if not task_id:
                return CaptchaSolution(
                    success=False,
                    error="No task ID returned",
                    solver_used=self.name,
                    time_ms=(time.time() - start_time) * 1000,
                )

            # Poll for result
            token = await self._poll_result(task_id)

            if token:
                return CaptchaSolution(
                    success=True,
                    token=token,
                    solver_used=self.name,
                    time_ms=(time.time() - start_time) * 1000,
                )
            else:
                return CaptchaSolution(
                    success=False,
                    error="Failed to get solution within timeout",
                    solver_used=self.name,
                    time_ms=(time.time() - start_time) * 1000,
                )

        except Exception as e:
            logger.error("CapSolver error", error=str(e))
            return CaptchaSolution(
                success=False,
                error=str(e),
                solver_used=self.name,
                time_ms=(time.time() - start_time) * 1000,
            )

    async def _poll_result(self, task_id: str) -> str | None:
        """Poll for task result."""
        client = await self._get_client()
        max_attempts = int(self.config.timeout / self.config.poll_interval)

        for _ in range(max_attempts):
            await asyncio.sleep(self.config.poll_interval)

            resp = await client.post(
                "/getTaskResult",
                json={
                    "clientKey": self.config.api_key,
                    "taskId": task_id,
                },
            )
            data = resp.json()

            if data.get("errorId") != 0:
                logger.warning("CapSolver poll error", error=data.get("errorDescription"))
                return None

            status = data.get("status")
            if status == "ready":
                solution = data.get("solution", {})
                # Different CAPTCHA types return token in different fields
                return (
                    solution.get("gRecaptchaResponse")
                    or solution.get("token")
                    or solution.get("text")
                )
            elif status == "failed":
                logger.warning("CapSolver task failed", data=data)
                return None

        return None

    def _get_task_type(self, captcha_type: CaptchaType) -> str | None:
        """Map CaptchaType to CapSolver task type."""
        mapping = {
            CaptchaType.RECAPTCHA_V2: "ReCaptchaV2TaskProxyLess",
            CaptchaType.RECAPTCHA_V3: "ReCaptchaV3TaskProxyLess",
            CaptchaType.HCAPTCHA: "HCaptchaTaskProxyLess",
            CaptchaType.CLOUDFLARE: "AntiTurnstileTaskProxyLess",
        }
        return mapping.get(captcha_type)

    async def _extract_site_key(self, page: Any, captcha_type: CaptchaType) -> str | None:
        """Extract site key from page."""
        try:
            if captcha_type in (CaptchaType.RECAPTCHA_V2, CaptchaType.RECAPTCHA_V3):
                # reCAPTCHA site key extraction
                selectors = [
                    '[data-sitekey]',
                    '.g-recaptcha[data-sitekey]',
                    '#recaptcha[data-sitekey]',
                ]
                for selector in selectors:
                    element = await page.query_selector(selector)
                    if element:
                        return await element.get_attribute("data-sitekey")

                # Try script-based extraction
                site_key = await page.evaluate("""
                    () => {
                        const scripts = document.querySelectorAll('script');
                        for (const script of scripts) {
                            const match = script.src?.match(/render=([^&]+)/);
                            if (match) return match[1];
                        }
                        return window.___grecaptcha_cfg?.clients?.[0]?.N?.sitekey || null;
                    }
                """)
                if site_key:
                    return site_key

            elif captcha_type == CaptchaType.HCAPTCHA:
                element = await page.query_selector('[data-sitekey], .h-captcha[data-sitekey]')
                if element:
                    return await element.get_attribute("data-sitekey")

            elif captcha_type == CaptchaType.CLOUDFLARE:
                # Turnstile site key
                element = await page.query_selector('[data-sitekey], .cf-turnstile[data-sitekey]')
                if element:
                    return await element.get_attribute("data-sitekey")

        except Exception as e:
            logger.debug("Failed to extract site key", error=str(e))

        return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2Captcha Integration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@dataclass
class TwoCaptchaConfig:
    """2Captcha configuration."""

    api_key: str = ""
    base_url: str = "https://2captcha.com"
    timeout: float = 120.0
    poll_interval: float = 5.0


class TwoCaptchaClient(CaptchaSolver):
    """
    2Captcha CAPTCHA solving service client.

    Supports:
    - reCAPTCHA v2/v3
    - hCaptcha
    - Cloudflare Turnstile
    - FunCAPTCHA
    - Image CAPTCHA

    Docs: https://2captcha.com/api-docs
    """

    def __init__(self, config: TwoCaptchaConfig | None = None):
        self.config = config or TwoCaptchaConfig()

    @property
    def name(self) -> str:
        return "2captcha"

    async def health_check(self) -> bool:
        """Check if 2Captcha API is available and key is valid."""
        if not self.config.api_key:
            return False

        try:
            async with httpx.AsyncClient() as client:
                resp = await client.get(
                    f"{self.config.base_url}/res.php",
                    params={
                        "key": self.config.api_key,
                        "action": "getbalance",
                        "json": 1,
                    },
                )
                data = resp.json()
                if data.get("status") == 1:
                    balance = float(data.get("request", 0))
                    logger.debug("2Captcha balance check", balance=balance)
                    return balance > 0
                return False
        except Exception as e:
            logger.debug("2Captcha health check failed", error=str(e))
            return False

    async def solve(
        self,
        captcha_type: CaptchaType,
        site_key: str | None = None,
        site_url: str | None = None,
        page: Any = None,
        **kwargs,
    ) -> CaptchaSolution:
        """
        Solve CAPTCHA using 2Captcha API.

        Args:
            captcha_type: Type of CAPTCHA
            site_key: Site key for reCAPTCHA/hCaptcha
            site_url: URL of the page with CAPTCHA
            page: Playwright page (for extracting site_key if not provided)
        """
        start_time = time.time()

        if not self.config.api_key:
            return CaptchaSolution(
                success=False,
                error="2Captcha API key not configured",
                solver_used=self.name,
            )

        try:
            # Extract site_key from page if not provided
            if not site_key and page:
                site_key = await self._extract_site_key(page, captcha_type)
                site_url = page.url

            if not site_key:
                return CaptchaSolution(
                    success=False,
                    error="Site key not provided or could not be extracted",
                    solver_used=self.name,
                )

            # Build request parameters
            params = {
                "key": self.config.api_key,
                "json": 1,
                "pageurl": site_url,
            }

            # Add type-specific parameters
            if captcha_type in (CaptchaType.RECAPTCHA_V2, CaptchaType.RECAPTCHA_V3):
                params["method"] = "userrecaptcha"
                params["googlekey"] = site_key
                if captcha_type == CaptchaType.RECAPTCHA_V3:
                    params["version"] = "v3"
                    params["action"] = kwargs.get("action", "verify")
                    params["min_score"] = kwargs.get("min_score", 0.7)

            elif captcha_type == CaptchaType.HCAPTCHA:
                params["method"] = "hcaptcha"
                params["sitekey"] = site_key

            elif captcha_type == CaptchaType.CLOUDFLARE:
                params["method"] = "turnstile"
                params["sitekey"] = site_key

            else:
                return CaptchaSolution(
                    success=False,
                    error=f"Unsupported CAPTCHA type: {captcha_type}",
                    solver_used=self.name,
                )

            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                # Submit task
                submit_resp = await client.get(
                    f"{self.config.base_url}/in.php",
                    params=params,
                )
                submit_data = submit_resp.json()

                if submit_data.get("status") != 1:
                    return CaptchaSolution(
                        success=False,
                        error=submit_data.get("request", "Unknown error"),
                        solver_used=self.name,
                        time_ms=(time.time() - start_time) * 1000,
                    )

                task_id = submit_data.get("request")

                # Poll for result
                token = await self._poll_result(client, task_id)

                if token:
                    return CaptchaSolution(
                        success=True,
                        token=token,
                        solver_used=self.name,
                        time_ms=(time.time() - start_time) * 1000,
                    )
                else:
                    return CaptchaSolution(
                        success=False,
                        error="Failed to get solution within timeout",
                        solver_used=self.name,
                        time_ms=(time.time() - start_time) * 1000,
                    )

        except Exception as e:
            logger.error("2Captcha error", error=str(e))
            return CaptchaSolution(
                success=False,
                error=str(e),
                solver_used=self.name,
                time_ms=(time.time() - start_time) * 1000,
            )

    async def _poll_result(self, client: httpx.AsyncClient, task_id: str) -> str | None:
        """Poll for task result."""
        max_attempts = int(self.config.timeout / self.config.poll_interval)

        for _ in range(max_attempts):
            await asyncio.sleep(self.config.poll_interval)

            resp = await client.get(
                f"{self.config.base_url}/res.php",
                params={
                    "key": self.config.api_key,
                    "action": "get",
                    "id": task_id,
                    "json": 1,
                },
            )
            data = resp.json()

            if data.get("status") == 1:
                return data.get("request")
            elif data.get("request") == "CAPCHA_NOT_READY":
                continue
            else:
                logger.warning("2Captcha poll error", error=data.get("request"))
                return None

        return None

    async def _extract_site_key(self, page: Any, captcha_type: CaptchaType) -> str | None:
        """Extract site key from page (same logic as CapSolver)."""
        try:
            if captcha_type in (CaptchaType.RECAPTCHA_V2, CaptchaType.RECAPTCHA_V3):
                selectors = [
                    '[data-sitekey]',
                    '.g-recaptcha[data-sitekey]',
                    '#recaptcha[data-sitekey]',
                ]
                for selector in selectors:
                    element = await page.query_selector(selector)
                    if element:
                        return await element.get_attribute("data-sitekey")

            elif captcha_type == CaptchaType.HCAPTCHA:
                element = await page.query_selector('[data-sitekey], .h-captcha[data-sitekey]')
                if element:
                    return await element.get_attribute("data-sitekey")

            elif captcha_type == CaptchaType.CLOUDFLARE:
                element = await page.query_selector('[data-sitekey], .cf-turnstile[data-sitekey]')
                if element:
                    return await element.get_attribute("data-sitekey")

        except Exception as e:
            logger.debug("Failed to extract site key", error=str(e))

        return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Factory function
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def create_paid_solver(
    provider: Literal["capsolver", "2captcha"] = "capsolver",
    api_key: str = "",
) -> CaptchaSolver:
    """
    Create a paid CAPTCHA solver instance.

    Args:
        provider: Which service to use
        api_key: API key for the service

    Returns:
        CaptchaSolver instance
    """
    if provider == "capsolver":
        return CapSolverClient(CapSolverConfig(api_key=api_key))
    elif provider == "2captcha":
        return TwoCaptchaClient(TwoCaptchaConfig(api_key=api_key))
    else:
        raise ValueError(f"Unknown CAPTCHA solver provider: {provider}")

```

---

## backend/autonomous-crawler-service/src/captcha/stealth.py

```py
"""Stealth browser configuration for bot detection bypass."""

from dataclasses import dataclass, field
from pathlib import Path
from typing import Any

import structlog

logger = structlog.get_logger(__name__)


# Extension paths for CAPTCHA bypass
EXTENSION_CACHE_DIR = Path("/tmp/browser-extensions")


@dataclass
class StealthConfig:
    """Configuration for stealth browser mode."""
    
    # User agent rotation
    user_agents: list[str] = field(default_factory=lambda: [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:134.0) Gecko/20100101 Firefox/134.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.2 Safari/605.1.15",
    ])
    
    # Viewport sizes (common resolutions)
    viewports: list[dict[str, int]] = field(default_factory=lambda: [
        {"width": 1920, "height": 1080},
        {"width": 1366, "height": 768},
        {"width": 1536, "height": 864},
        {"width": 1440, "height": 900},
    ])
    
    # Timezone/locale
    timezone: str = "Asia/Seoul"
    locale: str = "ko-KR"
    
    # Extra args for Chromium
    extra_args: list[str] = field(default_factory=lambda: [
        "--disable-blink-features=AutomationControlled",
        "--disable-features=IsolateOrigins,site-per-process",
        "--disable-dev-shm-usage",
        "--disable-accelerated-2d-canvas",
        "--no-first-run",
        "--no-zygote",
        "--disable-gpu",
        "--hide-scrollbars",
        "--mute-audio",
    ])
    
    # Webdriver navigator override
    hide_webdriver: bool = True
    
    # Random delays (ms)
    min_delay: int = 100
    max_delay: int = 500


def apply_stealth_to_playwright(page: Any, config: StealthConfig | None = None) -> None:
    """
    Apply stealth settings to a Playwright page.
    
    Uses playwright_stealth if available, otherwise applies manual patches.
    """
    if config is None:
        config = StealthConfig()
    
    try:
        from playwright_stealth import stealth_sync
        stealth_sync(page)
        logger.debug("Applied playwright_stealth")
    except ImportError:
        logger.warning("playwright_stealth not available, using manual patches")
        _apply_manual_stealth(page, config)


async def apply_stealth_to_playwright_async(page: Any, config: StealthConfig | None = None) -> None:
    """Async version of stealth application."""
    if config is None:
        config = StealthConfig()
    
    try:
        from playwright_stealth import stealth_async
        await stealth_async(page)
        logger.debug("Applied playwright_stealth (async)")
    except ImportError:
        logger.warning("playwright_stealth not available, using manual patches")
        await _apply_manual_stealth_async(page, config)


def _apply_manual_stealth(page: Any, config: StealthConfig) -> None:
    """Apply manual stealth patches."""
    # Hide webdriver
    if config.hide_webdriver:
        page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
        """)
    
    # Override navigator properties
    page.add_init_script("""
        // Override plugins
        Object.defineProperty(navigator, 'plugins', {
            get: () => [1, 2, 3, 4, 5]
        });
        
        // Override languages
        Object.defineProperty(navigator, 'languages', {
            get: () => ['ko-KR', 'ko', 'en-US', 'en']
        });
        
        // Override platform
        Object.defineProperty(navigator, 'platform', {
            get: () => 'Win32'
        });
        
        // Chrome runtime
        window.chrome = {
            runtime: {}
        };
        
        // Permissions
        const originalQuery = window.navigator.permissions.query;
        window.navigator.permissions.query = (parameters) => (
            parameters.name === 'notifications' ?
                Promise.resolve({ state: Notification.permission }) :
                originalQuery(parameters)
        );
    """)


async def _apply_manual_stealth_async(page: Any, config: StealthConfig) -> None:
    """Apply manual stealth patches (async)."""
    # Same as sync but using await
    if config.hide_webdriver:
        await page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
        """)
    
    await page.add_init_script("""
        Object.defineProperty(navigator, 'plugins', {
            get: () => [1, 2, 3, 4, 5]
        });
        
        Object.defineProperty(navigator, 'languages', {
            get: () => ['ko-KR', 'ko', 'en-US', 'en']
        });
        
        Object.defineProperty(navigator, 'platform', {
            get: () => 'Win32'
        });
        
        window.chrome = { runtime: {} };
        
        const originalQuery = window.navigator.permissions.query;
        window.navigator.permissions.query = (parameters) => (
            parameters.name === 'notifications' ?
                Promise.resolve({ state: Notification.permission }) :
                originalQuery(parameters)
        );
    """)


def get_undetected_browser_args() -> list[str]:
    """Get Chrome args for undetected browsing."""
    return [
        "--disable-blink-features=AutomationControlled",
        "--disable-features=IsolateOrigins,site-per-process",
        "--disable-infobars",
        "--disable-background-networking",
        "--disable-background-timer-throttling",
        "--disable-backgrounding-occluded-windows",
        "--disable-breakpad",
        "--disable-component-extensions-with-background-pages",
        "--disable-component-update",
        "--disable-default-apps",
        "--disable-extensions",
        "--disable-hang-monitor",
        "--disable-ipc-flooding-protection",
        "--disable-popup-blocking",
        "--disable-prompt-on-repost",
        "--disable-renderer-backgrounding",
        "--disable-sync",
        "--enable-features=NetworkService,NetworkServiceInProcess",
        "--force-color-profile=srgb",
        "--metrics-recording-only",
        "--no-first-run",
        "--password-store=basic",
        "--use-mock-keychain",
        "--ignore-certificate-errors",
    ]


async def get_nopecha_extension_path(api_key: str = "") -> str | None:
    """
    Download and configure NopeCHA extension for browser use.
    
    Args:
        api_key: Optional NopeCHA API key for faster solving
        
    Returns:
        Path to the extracted extension directory, or None if failed
    """
    try:
        from src.captcha.nopecha import NopeCHAConfig, NopeCHAExtensionManager
        
        config = NopeCHAConfig(
            api_key=api_key,
            enabled=True,
            auto_solve=True,
            use_audio_challenge=True,
            cache_dir=EXTENSION_CACHE_DIR / "nopecha",
        )
        
        manager = NopeCHAExtensionManager(config)
        ext_path = await manager.download_extension()
        
        logger.info("NopeCHA extension ready", path=str(ext_path))
        return str(ext_path)
        
    except Exception as e:
        logger.error("Failed to setup NopeCHA extension", error=str(e))
        return None


def get_stealth_browser_args_with_extensions(
    extension_paths: list[str] | None = None,
    include_docker_args: bool = False,
) -> list[str]:
    """
    Get Chrome args for stealth browsing with extension support.
    
    Args:
        extension_paths: List of paths to unpacked extensions
        include_docker_args: Include Docker-specific args
        
    Returns:
        List of Chrome arguments
    """
    args = [
        "--disable-blink-features=AutomationControlled",
        "--disable-features=IsolateOrigins,site-per-process,AutomationControlled",
        "--disable-infobars",
        "--disable-popup-blocking",
        "--disable-notifications",
        "--no-first-run",
        "--no-default-browser-check",
        "--no-service-autorun",
        "--disable-background-networking",
        "--disable-background-timer-throttling",
        "--disable-backgrounding-occluded-windows",
        "--disable-breakpad",
        "--disable-component-update",
        "--disable-default-apps",
        "--disable-hang-monitor",
        "--disable-ipc-flooding-protection",
        "--disable-renderer-backgrounding",
        "--disable-sync",
        "--disable-client-side-phishing-detection",
        "--disable-domain-reliability",
        "--metrics-recording-only",
        "--safebrowsing-disable-auto-update",
        "--enable-webgl",
        "--enable-accelerated-2d-canvas",
        "--enable-features=NetworkService,NetworkServiceInProcess",
        "--ignore-certificate-errors",
        "--ignore-ssl-errors",
        "--allow-running-insecure-content",
        "--password-store=basic",
        "--use-mock-keychain",
        "--log-level=3",
    ]
    
    # Add extension paths (requires extensions to NOT be disabled)
    if extension_paths:
        # Remove --disable-extensions if present
        args = [a for a in args if a != "--disable-extensions"]
        
        # Add load-extension args
        for ext_path in extension_paths:
            if ext_path:
                args.append(f"--load-extension={ext_path}")
                
        # Also add to disable-extensions-except
        valid_paths = [p for p in extension_paths if p]
        if valid_paths:
            args.append(f"--disable-extensions-except={','.join(valid_paths)}")
    else:
        args.append("--disable-extensions")
    
    if include_docker_args:
        args.extend([
            "--no-sandbox",
            "--disable-dev-shm-usage",
            "--disable-gpu-sandbox",
            "--disable-setuid-sandbox",
            "--no-zygote",
            "--single-process",
        ])
    
    return args


@dataclass
class EnhancedStealthConfig(StealthConfig):
    """Enhanced stealth configuration with extension support."""
    
    # NopeCHA settings - ENABLED by default
    use_nopecha: bool = True
    nopecha_api_key: str = ""
    
    # Camoufox as alternative - can be enabled via settings
    use_camoufox: bool = False
    
    # Human-like behavior - ENABLED by default
    enable_human_simulation: bool = True
    
    # Extension paths
    extension_paths: list[str] = field(default_factory=list)
    
    async def setup_extensions(self) -> None:
        """Download and configure required extensions."""
        if self.use_nopecha:
            nopecha_path = await get_nopecha_extension_path(self.nopecha_api_key)
            if nopecha_path and nopecha_path not in self.extension_paths:
                self.extension_paths.append(nopecha_path)
    
    def get_browser_args(self, include_docker: bool = False) -> list[str]:
        """Get Chrome args with configured extensions."""
        return get_stealth_browser_args_with_extensions(
            extension_paths=self.extension_paths if self.extension_paths else None,
            include_docker_args=include_docker,
        )
    
    def get_random_user_agent(self) -> str:
        """Get a random user agent from the configured list."""
        import random
        return random.choice(self.user_agents)
    
    def get_random_viewport(self) -> dict[str, int]:
        """Get a random viewport size from the configured list."""
        import random
        return random.choice(self.viewports)

```

---

## backend/autonomous-crawler-service/src/captcha/undetected.py

```py
"""
Undetected ChromeDriver Integration for Bot Detection Bypass.

This module provides:
1. Undetected ChromeDriver - Patched ChromeDriver that bypasses detection
2. Advanced Stealth - JavaScript patches to hide automation fingerprints
3. Human-like behavior simulation

Supports:
- Cloudflare
- PerimeterX
- DataDome
- Incapsula
- reCAPTCHA detection
"""

import asyncio
import random
from dataclasses import dataclass
from typing import Any

import structlog

logger = structlog.get_logger(__name__)


@dataclass
class UndetectedConfig:
    """Configuration for undetected browser mode."""
    
    # Driver settings
    driver_executable_path: str | None = None
    browser_executable_path: str | None = None
    
    # Version matching
    version_main: int | None = None  # Chrome major version
    
    # User data
    user_data_dir: str | None = None
    use_subprocess: bool = True
    
    # Stealth options
    enable_cdp_events: bool = True
    suppress_welcome: bool = True
    log_level: int = 0
    
    # Headless mode (use new headless mode)
    headless: bool = False
    use_new_headless: bool = True  # Chrome 109+ new headless mode
    
    # Window size
    window_size: tuple[int, int] = (1920, 1080)
    
    # Proxy
    proxy: str | None = None


def get_undetected_chromedriver():
    """
    Get an undetected ChromeDriver instance.
    
    Uses undetected-chromedriver library if available,
    otherwise falls back to manual patching.
    """
    try:
        import undetected_chromedriver as uc
        return uc
    except ImportError:
        logger.warning("undetected-chromedriver not installed, using manual patches")
        return None


async def create_undetected_driver(
    config: UndetectedConfig | None = None,
) -> Any:
    """
    Create an undetected ChromeDriver instance.
    
    Args:
        config: Configuration options
        
    Returns:
        Undetected ChromeDriver instance or None
    """
    if config is None:
        config = UndetectedConfig()
    
    uc = get_undetected_chromedriver()
    if uc is None:
        return None
    
    options = uc.ChromeOptions()
    
    # Basic options
    options.add_argument(f"--window-size={config.window_size[0]},{config.window_size[1]}")
    
    if config.headless:
        if config.use_new_headless:
            options.add_argument("--headless=new")
        else:
            options.add_argument("--headless")
    
    if config.proxy:
        options.add_argument(f"--proxy-server={config.proxy}")
    
    # Additional stealth arguments
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-gpu")
    options.add_argument("--disable-infobars")
    
    # Create driver
    try:
        driver = uc.Chrome(
            options=options,
            driver_executable_path=config.driver_executable_path,
            browser_executable_path=config.browser_executable_path,
            version_main=config.version_main,
            user_data_dir=config.user_data_dir,
            use_subprocess=config.use_subprocess,
            enable_cdp_events=config.enable_cdp_events,
            suppress_welcome=config.suppress_welcome,
            log_level=config.log_level,
        )
        
        logger.info("Created undetected ChromeDriver")
        return driver
        
    except Exception as e:
        logger.error("Failed to create undetected ChromeDriver", error=str(e))
        return None


class AdvancedStealthPatcher:
    """
    Advanced JavaScript stealth patches for bot detection bypass.
    
    These patches are applied via CDP or page.evaluate to hide
    automation fingerprints that bot detection systems look for.
    """
    
    # Chrome properties patch
    CHROME_RUNTIME_PATCH = """
    (() => {
        // Add chrome.runtime if missing
        if (!window.chrome) {
            window.chrome = {};
        }
        if (!window.chrome.runtime) {
            window.chrome.runtime = {
                PlatformOs: { MAC: 'mac', WIN: 'win', ANDROID: 'android', CROS: 'cros', LINUX: 'linux', OPENBSD: 'openbsd' },
                PlatformArch: { ARM: 'arm', X86_32: 'x86-32', X86_64: 'x86-64' },
                PlatformNaclArch: { ARM: 'arm', X86_32: 'x86-32', X86_64: 'x86-64' },
                RequestUpdateCheckStatus: { THROTTLED: 'throttled', NO_UPDATE: 'no_update', UPDATE_AVAILABLE: 'update_available' },
                OnInstalledReason: { INSTALL: 'install', UPDATE: 'update', CHROME_UPDATE: 'chrome_update', SHARED_MODULE_UPDATE: 'shared_module_update' },
                OnRestartRequiredReason: { APP_UPDATE: 'app_update', OS_UPDATE: 'os_update', PERIODIC: 'periodic' }
            };
        }
        
        // Add chrome.csi if missing
        if (!window.chrome.csi) {
            window.chrome.csi = function() { return {}; };
        }
        
        // Add chrome.loadTimes if missing
        if (!window.chrome.loadTimes) {
            window.chrome.loadTimes = function() {
                return {
                    commitLoadTime: Date.now() / 1000,
                    connectionInfo: 'http/1.1',
                    finishDocumentLoadTime: Date.now() / 1000,
                    finishLoadTime: Date.now() / 1000,
                    firstPaintAfterLoadTime: 0,
                    firstPaintTime: Date.now() / 1000,
                    navigationType: 'Other',
                    npnNegotiatedProtocol: 'unknown',
                    requestTime: Date.now() / 1000,
                    startLoadTime: Date.now() / 1000,
                    wasAlternateProtocolAvailable: false,
                    wasFetchedViaSpdy: false,
                    wasNpnNegotiated: false
                };
            };
        }
    })();
    """
    
    # WebDriver property patch
    WEBDRIVER_PATCH = """
    (() => {
        // Remove webdriver property
        Object.defineProperty(navigator, 'webdriver', {
            get: () => undefined,
            configurable: true
        });
        
        // Also patch the prototype
        const originalQuery = window.Navigator.prototype.hasOwnProperty;
        Object.defineProperty(Navigator.prototype, 'webdriver', {
            get: () => undefined,
            configurable: true
        });
        
        // Delete if exists
        delete navigator.webdriver;
    })();
    """
    
    # Plugins patch
    PLUGINS_PATCH = """
    (() => {
        // Mock plugins array
        const mockPlugins = [
            {
                name: 'Chrome PDF Plugin',
                filename: 'internal-pdf-viewer',
                description: 'Portable Document Format',
                length: 1,
                0: { type: 'application/pdf', suffixes: 'pdf', description: 'Portable Document Format' }
            },
            {
                name: 'Chrome PDF Viewer',
                filename: 'mhjfbmdgcfjbbpaeojofohoefgiehjai',
                description: '',
                length: 1,
                0: { type: 'application/pdf', suffixes: 'pdf', description: '' }
            },
            {
                name: 'Native Client',
                filename: 'internal-nacl-plugin',
                description: '',
                length: 2,
                0: { type: 'application/x-nacl', suffixes: '', description: 'Native Client Executable' },
                1: { type: 'application/x-pnacl', suffixes: '', description: 'Portable Native Client Executable' }
            }
        ];
        
        // Create a proper PluginArray
        const pluginArray = Object.create(PluginArray.prototype);
        mockPlugins.forEach((plugin, i) => {
            pluginArray[i] = plugin;
        });
        pluginArray.length = mockPlugins.length;
        
        // Patch namedItem and item methods
        pluginArray.item = function(index) { return this[index]; };
        pluginArray.namedItem = function(name) {
            return mockPlugins.find(p => p.name === name) || null;
        };
        pluginArray.refresh = function() {};
        
        Object.defineProperty(navigator, 'plugins', {
            get: () => pluginArray,
            configurable: true
        });
    })();
    """
    
    # Languages patch
    LANGUAGES_PATCH = """
    (() => {
        Object.defineProperty(navigator, 'languages', {
            get: () => ['ko-KR', 'ko', 'en-US', 'en'],
            configurable: true
        });
        
        Object.defineProperty(navigator, 'language', {
            get: () => 'ko-KR',
            configurable: true
        });
    })();
    """
    
    # Hardware concurrency patch
    HARDWARE_PATCH = """
    (() => {
        Object.defineProperty(navigator, 'hardwareConcurrency', {
            get: () => 8,
            configurable: true
        });
        
        Object.defineProperty(navigator, 'deviceMemory', {
            get: () => 8,
            configurable: true
        });
    })();
    """
    
    # Permissions patch
    PERMISSIONS_PATCH = """
    (() => {
        const originalQuery = window.navigator.permissions.query;
        window.navigator.permissions.query = (parameters) => {
            if (parameters.name === 'notifications') {
                return Promise.resolve({ state: Notification.permission });
            }
            return originalQuery.call(window.navigator.permissions, parameters);
        };
    })();
    """
    
    # WebGL vendor/renderer patch
    WEBGL_PATCH = """
    (() => {
        const getParameter = WebGLRenderingContext.prototype.getParameter;
        WebGLRenderingContext.prototype.getParameter = function(parameter) {
            // UNMASKED_VENDOR_WEBGL
            if (parameter === 37445) {
                return 'Intel Inc.';
            }
            // UNMASKED_RENDERER_WEBGL
            if (parameter === 37446) {
                return 'Intel Iris OpenGL Engine';
            }
            return getParameter.call(this, parameter);
        };
        
        // Also patch WebGL2
        if (typeof WebGL2RenderingContext !== 'undefined') {
            const getParameter2 = WebGL2RenderingContext.prototype.getParameter;
            WebGL2RenderingContext.prototype.getParameter = function(parameter) {
                if (parameter === 37445) {
                    return 'Intel Inc.';
                }
                if (parameter === 37446) {
                    return 'Intel Iris OpenGL Engine';
                }
                return getParameter2.call(this, parameter);
            };
        }
    })();
    """
    
    # Iframe contentWindow patch
    IFRAME_PATCH = """
    (() => {
        // Prevent iframe detection
        try {
            if (window.top === window.self) {
                Object.defineProperty(window, 'frameElement', {
                    get: () => null,
                    configurable: true
                });
            }
        } catch (e) {}
    })();
    """
    
    # Console debug patch (hide console.debug modifications)
    CONSOLE_PATCH = """
    (() => {
        // Preserve original console methods
        const originalDebug = console.debug;
        console.debug = function(...args) {
            // Filter out automation-related debug messages
            const filtered = args.filter(arg => {
                if (typeof arg === 'string') {
                    const lower = arg.toLowerCase();
                    return !lower.includes('webdriver') && 
                           !lower.includes('automation') &&
                           !lower.includes('puppeteer') &&
                           !lower.includes('playwright');
                }
                return true;
            });
            if (filtered.length > 0) {
                originalDebug.apply(console, filtered);
            }
        };
    })();
    """
    
    @classmethod
    def get_all_patches(cls) -> str:
        """Get all stealth patches combined."""
        return "\n".join([
            cls.CHROME_RUNTIME_PATCH,
            cls.WEBDRIVER_PATCH,
            cls.PLUGINS_PATCH,
            cls.LANGUAGES_PATCH,
            cls.HARDWARE_PATCH,
            cls.PERMISSIONS_PATCH,
            cls.WEBGL_PATCH,
            cls.IFRAME_PATCH,
            cls.CONSOLE_PATCH,
        ])
    
    @classmethod
    async def apply_to_page(cls, page: Any) -> None:
        """Apply all stealth patches to a Playwright page."""
        try:
            await page.add_init_script(cls.get_all_patches())
            logger.debug("Applied advanced stealth patches to page")
        except Exception as e:
            logger.error("Failed to apply stealth patches", error=str(e))


class HumanBehaviorSimulator:
    """
    Simulate human-like behavior to avoid bot detection.
    
    Includes:
    - Random delays
    - Mouse movements
    - Scroll patterns
    - Typing patterns
    """
    
    @staticmethod
    def random_delay(min_ms: int = 100, max_ms: int = 500) -> float:
        """Get random delay in seconds."""
        return random.randint(min_ms, max_ms) / 1000
    
    @staticmethod
    async def human_type(page: Any, selector: str, text: str) -> None:
        """Type text with human-like delays."""
        element = await page.query_selector(selector)
        if not element:
            return
        
        await element.click()
        await asyncio.sleep(HumanBehaviorSimulator.random_delay(50, 150))
        
        for char in text:
            await page.keyboard.type(char)
            # Variable delay between keystrokes
            delay = random.uniform(0.05, 0.15)
            if char in " .,!?":
                delay += random.uniform(0.1, 0.2)
            await asyncio.sleep(delay)
    
    @staticmethod
    async def human_click(page: Any, selector: str) -> None:
        """Click with human-like behavior."""
        element = await page.query_selector(selector)
        if not element:
            return
        
        box = await element.bounding_box()
        if not box:
            await element.click()
            return
        
        # Click at random position within element
        x = box["x"] + random.uniform(5, box["width"] - 5)
        y = box["y"] + random.uniform(5, box["height"] - 5)
        
        # Move mouse first
        await page.mouse.move(x, y)
        await asyncio.sleep(HumanBehaviorSimulator.random_delay(50, 150))
        await page.mouse.click(x, y)
    
    @staticmethod
    async def human_scroll(page: Any, direction: str = "down", amount: int = 300) -> None:
        """Scroll with human-like patterns."""
        if direction == "down":
            delta = random.randint(amount - 50, amount + 50)
        else:
            delta = -random.randint(amount - 50, amount + 50)
        
        await page.mouse.wheel(0, delta)
        await asyncio.sleep(HumanBehaviorSimulator.random_delay(200, 400))
    
    @staticmethod
    async def random_mouse_movements(page: Any, count: int = 3) -> None:
        """Make random mouse movements."""
        viewport = page.viewport_size
        if not viewport:
            return
        
        for _ in range(count):
            x = random.randint(100, viewport["width"] - 100)
            y = random.randint(100, viewport["height"] - 100)
            await page.mouse.move(x, y)
            await asyncio.sleep(HumanBehaviorSimulator.random_delay(100, 300))


def get_enhanced_browser_args(
    include_docker: bool = False,
    include_stealth: bool = True,
) -> list[str]:
    """
    Get enhanced Chrome arguments for maximum undetectability.
    
    Args:
        include_docker: Include Docker-specific args
        include_stealth: Include stealth-related args
        
    Returns:
        List of Chrome arguments
    """
    args = [
        # Core anti-detection
        "--disable-blink-features=AutomationControlled",
        "--disable-features=IsolateOrigins,site-per-process,AutomationControlled",
        
        # Disable infobars and popups
        "--disable-infobars",
        "--disable-popup-blocking",
        "--disable-notifications",
        
        # Disable extensions welcome
        "--no-first-run",
        "--no-default-browser-check",
        "--no-service-autorun",
        
        # Performance
        "--disable-background-networking",
        "--disable-background-timer-throttling",
        "--disable-backgrounding-occluded-windows",
        "--disable-breakpad",
        "--disable-component-update",
        "--disable-default-apps",
        "--disable-hang-monitor",
        "--disable-ipc-flooding-protection",
        "--disable-renderer-backgrounding",
        "--disable-sync",
        
        # Privacy/security that helps with detection
        "--disable-client-side-phishing-detection",
        "--disable-domain-reliability",
        "--metrics-recording-only",
        "--safebrowsing-disable-auto-update",
        
        # WebRTC IP leak prevention
        "--disable-webrtc-apm-in-audio-service",
        "--disable-webrtc-encryption",
        "--disable-webrtc-hw-decoding",
        "--disable-webrtc-hw-encoding",
        "--force-webrtc-ip-handling-policy=disable_non_proxied_udp",
        
        # GPU (often checked by detection systems)
        "--enable-webgl",
        "--enable-accelerated-2d-canvas",
        "--enable-features=NetworkService,NetworkServiceInProcess",
        
        # Misc
        "--ignore-certificate-errors",
        "--ignore-ssl-errors",
        "--allow-running-insecure-content",
        "--password-store=basic",
        "--use-mock-keychain",
        "--log-level=3",
    ]
    
    if include_docker:
        args.extend([
            "--no-sandbox",
            "--disable-dev-shm-usage",
            "--disable-gpu-sandbox",
            "--disable-setuid-sandbox",
            "--no-zygote",
            "--single-process",
        ])
    
    return args

```

---

## backend/autonomous-crawler-service/src/config/__init__.py

```py
"""Configuration module for autonomous-crawler-service."""

from .settings import Settings, get_settings
from .consul import (
    load_config_from_consul,
    wait_for_consul,
    check_consul_health,
    CONSUL_ENABLED,
)

__all__ = [
    "Settings",
    "get_settings",
    "load_config_from_consul",
    "wait_for_consul",
    "check_consul_health",
    "CONSUL_ENABLED",
]

```

---

## backend/autonomous-crawler-service/src/config/consul.py

```py
"""Consul KV configuration loader for autonomous-crawler service.

Loads configuration with the following precedence:
1. Consul KV (highest priority) - key path: config/autonomous-crawler/{KEY}
2. Environment Variables - {KEY}
3. Error if required key not found

Usage:
    from src.config.consul import load_config_from_consul

    # Load all config from Consul and apply to environment
    consul_keys, env_keys = load_config_from_consul()
    
    # Then use Settings as normal
    from src.config import get_settings
    settings = get_settings()
"""

import base64
import os
from typing import Any

import httpx
import structlog

logger = structlog.get_logger(__name__)

# Consul configuration
CONSUL_HOST = os.getenv("CONSUL_HOST", "localhost")
CONSUL_PORT = os.getenv("CONSUL_PORT", "8500")
CONSUL_HTTP_TOKEN = os.getenv("CONSUL_HTTP_TOKEN", "")
CONSUL_ENABLED = os.getenv("CONSUL_ENABLED", "true").lower() == "true"
CONSUL_SERVICE_NAME = os.getenv("CONSUL_SERVICE_NAME", "autonomous-crawler")

# Consul KV prefix for this service
CONSUL_KV_PREFIX = f"config/{CONSUL_SERVICE_NAME}/"


def get_consul_url() -> str:
    """Get the Consul HTTP API URL."""
    return f"http://{CONSUL_HOST}:{CONSUL_PORT}"


def get_consul_headers() -> dict[str, str]:
    """Get headers for Consul API requests."""
    headers = {"Accept": "application/json"}
    if CONSUL_HTTP_TOKEN:
        headers["X-Consul-Token"] = CONSUL_HTTP_TOKEN
    return headers


async def fetch_consul_kv_async(key: str) -> str | None:
    """
    Fetch a single key from Consul KV (async version).
    
    Args:
        key: The key name (without prefix)
        
    Returns:
        The value as a string, or None if not found
    """
    url = f"{get_consul_url()}/v1/kv/{CONSUL_KV_PREFIX}{key}"
    
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(url, headers=get_consul_headers(), timeout=5.0)
            
            if response.status_code == 404:
                return None
            
            response.raise_for_status()
            data = response.json()
            
            if data and len(data) > 0:
                # Consul returns base64-encoded values
                value_b64 = data[0].get("Value")
                if value_b64:
                    return base64.b64decode(value_b64).decode("utf-8")
            
            return None
            
    except httpx.HTTPError as e:
        logger.warning("Failed to fetch Consul key", key=key, error=str(e))
        return None
    except Exception as e:
        logger.warning("Error fetching Consul key", key=key, error=str(e))
        return None


def fetch_consul_kv_sync(key: str) -> str | None:
    """
    Fetch a single key from Consul KV (sync version).
    
    Args:
        key: The key name (without prefix)
        
    Returns:
        The value as a string, or None if not found
    """
    url = f"{get_consul_url()}/v1/kv/{CONSUL_KV_PREFIX}{key}"
    
    try:
        with httpx.Client() as client:
            response = client.get(url, headers=get_consul_headers(), timeout=5.0)
            
            if response.status_code == 404:
                return None
            
            response.raise_for_status()
            data = response.json()
            
            if data and len(data) > 0:
                value_b64 = data[0].get("Value")
                if value_b64:
                    return base64.b64decode(value_b64).decode("utf-8")
            
            return None
            
    except httpx.HTTPError as e:
        logger.warning("Failed to fetch Consul key", key=key, error=str(e))
        return None
    except Exception as e:
        logger.warning("Error fetching Consul key", key=key, error=str(e))
        return None


def fetch_all_consul_keys_sync() -> dict[str, str]:
    """
    Fetch all keys under the service prefix from Consul KV.
    
    Returns:
        Dictionary of key-value pairs
    """
    url = f"{get_consul_url()}/v1/kv/{CONSUL_KV_PREFIX}?recurse=true"
    
    try:
        with httpx.Client() as client:
            response = client.get(url, headers=get_consul_headers(), timeout=10.0)
            
            if response.status_code == 404:
                logger.info("No keys found in Consul", prefix=CONSUL_KV_PREFIX)
                return {}
            
            response.raise_for_status()
            data = response.json()
            
            result = {}
            for item in data or []:
                full_key = item.get("Key", "")
                value_b64 = item.get("Value")
                
                # Extract key name (remove prefix)
                if full_key.startswith(CONSUL_KV_PREFIX):
                    key_name = full_key[len(CONSUL_KV_PREFIX):]
                    if value_b64 and key_name:
                        result[key_name] = base64.b64decode(value_b64).decode("utf-8")
            
            return result
            
    except httpx.HTTPError as e:
        logger.warning("Failed to fetch Consul keys", prefix=CONSUL_KV_PREFIX, error=str(e))
        return {}
    except Exception as e:
        logger.warning("Error fetching Consul keys", prefix=CONSUL_KV_PREFIX, error=str(e))
        return {}


def load_config_from_consul() -> tuple[list[str], list[str]]:
    """
    Load configuration from Consul KV and inject into environment variables.
    
    This should be called at application startup, before Settings are loaded.
    
    Returns:
        Tuple of (consul_loaded_keys, env_loaded_keys)
    """
    if not CONSUL_ENABLED:
        logger.info("Consul configuration disabled, using environment variables only")
        return [], []
    
    logger.info(
        "Loading configuration from Consul",
        consul_url=get_consul_url(),
        service_name=CONSUL_SERVICE_NAME,
        prefix=CONSUL_KV_PREFIX,
    )
    
    # Fetch all keys from Consul
    consul_config = fetch_all_consul_keys_sync()
    
    consul_loaded_keys = []
    env_loaded_keys = []
    
    # Inject Consul values into environment (they take precedence)
    for key, value in consul_config.items():
        os.environ[key] = value
        consul_loaded_keys.append(key)
        logger.debug("Loaded from Consul", key=key)
    
    # Track which keys came from existing environment variables
    # (These are keys that weren't in Consul but exist in env)
    # Note: We only track this for logging purposes
    env_only_keys = set(os.environ.keys()) - set(consul_loaded_keys)
    
    logger.info(
        "Configuration loaded",
        consul_keys_count=len(consul_loaded_keys),
        consul_keys=consul_loaded_keys,
    )
    
    return consul_loaded_keys, list(env_only_keys)


def check_consul_health() -> bool:
    """Check if Consul is reachable and healthy."""
    url = f"{get_consul_url()}/v1/status/leader"
    
    try:
        with httpx.Client() as client:
            response = client.get(url, headers=get_consul_headers(), timeout=5.0)
            return response.status_code == 200
    except Exception:
        return False


def wait_for_consul(max_attempts: int = 30, delay: float = 2.0) -> bool:
    """
    Wait for Consul to become available.
    
    Args:
        max_attempts: Maximum number of connection attempts
        delay: Delay between attempts in seconds
        
    Returns:
        True if Consul became available, False otherwise
    """
    import time
    
    logger.info("Waiting for Consul to be ready", consul_url=get_consul_url())
    
    for attempt in range(1, max_attempts + 1):
        if check_consul_health():
            logger.info("Consul is ready", attempts=attempt)
            return True
        
        logger.info(
            "Consul not ready, retrying",
            attempt=attempt,
            max_attempts=max_attempts,
        )
        time.sleep(delay)
    
    logger.error(
        "Consul did not become ready",
        max_attempts=max_attempts,
    )
    return False


# Type coercion utilities (matching ConsulConfigLoader pattern)
def coerce_bool(value: str | bool | None) -> bool:
    """Convert string value to boolean."""
    if isinstance(value, bool):
        return value
    if value is None:
        return False
    return value.lower() in ("true", "1", "yes", "on")


def coerce_int(value: str | int | None, default: int = 0) -> int:
    """Convert string value to integer."""
    if isinstance(value, int):
        return value
    if value is None:
        return default
    try:
        return int(value)
    except ValueError:
        return default


def coerce_float(value: str | float | None, default: float = 0.0) -> float:
    """Convert string value to float."""
    if isinstance(value, float):
        return value
    if value is None:
        return default
    try:
        return float(value)
    except ValueError:
        return default


def coerce_list(value: str | list | None, separator: str = ",") -> list[str]:
    """Convert comma-separated string to list."""
    if isinstance(value, list):
        return value
    if value is None or value == "":
        return []
    return [item.strip() for item in value.split(separator) if item.strip()]

```

---

## backend/autonomous-crawler-service/src/config/settings.py

```py
"""Application settings using Pydantic Settings."""

from functools import lru_cache
from typing import Literal

from pydantic import Field
from pydantic_settings import BaseSettings, SettingsConfigDict


class KafkaSettings(BaseSettings):
    """Kafka connection settings."""

    model_config = SettingsConfigDict(env_prefix="KAFKA_")

    bootstrap_servers: str = Field(
        default="localhost:9092",
        description="Kafka bootstrap servers",
    )
    consumer_group_id: str = Field(
        default="autonomous-crawler-group",
        description="Consumer group ID",
    )
    browser_task_topic: str = Field(
        default="newsinsight.crawl.browser.tasks",
        description="Topic for browser task messages",
    )
    crawl_result_topic: str = Field(
        default="newsinsight.crawl.results",
        description="Topic for crawl result messages",
    )
    auto_offset_reset: Literal["earliest", "latest"] = Field(
        default="earliest",
        description="Auto offset reset policy",
    )
    enable_auto_commit: bool = Field(
        default=False,
        description="Enable auto commit (disabled for manual acknowledgment)",
    )
    max_poll_records: int = Field(
        default=1,
        description="Maximum records per poll (1 for sequential processing)",
    )
    session_timeout_ms: int = Field(
        default=30000,
        description="Session timeout in milliseconds",
    )
    heartbeat_interval_ms: int = Field(
        default=10000,
        description="Heartbeat interval in milliseconds",
    )


class BrowserSettings(BaseSettings):
    """Browser and AI agent settings."""

    model_config = SettingsConfigDict(env_prefix="BROWSER_")

    headless: bool = Field(
        default=True,
        description="Run browser in headless mode",
    )
    max_concurrent_sessions: int = Field(
        default=2,
        description="Maximum concurrent browser sessions",
    )
    default_timeout_seconds: int = Field(
        default=300,
        description="Default timeout for browser tasks in seconds",
    )
    max_timeout_seconds: int = Field(
        default=600,
        description="Maximum allowed timeout in seconds",
    )
    screenshot_dir: str = Field(
        default="/tmp/crawler-screenshots",
        description="Directory for storing screenshots",
    )
    user_agent: str = Field(
        default="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
        description="User agent string for browser",
    )
    is_docker_env: bool = Field(
        default=False,
        description="Whether running in Docker environment (enables no-sandbox, etc.)",
    )
    # Browser backend selection
    backend: Literal["playwright", "camoufox"] = Field(
        default="playwright",
        description="Browser backend: 'playwright' (Chrome/Chromium) or 'camoufox' (Firefox anti-detect)",
    )


class LLMSettings(BaseSettings):
    """LLM provider settings.

    Supported providers:
    - openai: OpenAI API (default)
    - anthropic: Anthropic Claude API
    - openrouter: OpenRouter (access to multiple models via single API)
    - ollama: Local Ollama server
    - custom: Custom OpenAI-compatible REST API endpoint
    """

    model_config = SettingsConfigDict(env_prefix="LLM_")

    provider: Literal["openai", "anthropic", "openrouter", "ollama", "custom"] = Field(
        default="openai",
        description="LLM provider: openai, anthropic, openrouter, ollama, or custom",
    )

    # OpenAI settings
    openai_api_key: str = Field(
        default="",
        description="OpenAI API key",
    )
    openai_model: str = Field(
        default="gpt-4o",
        description="OpenAI model to use",
    )
    openai_base_url: str = Field(
        default="",
        description="Custom OpenAI API base URL (leave empty for default)",
    )

    # Anthropic settings
    anthropic_api_key: str = Field(
        default="",
        description="Anthropic API key",
    )
    anthropic_model: str = Field(
        default="claude-3-5-sonnet-20241022",
        description="Anthropic model to use",
    )

    # OpenRouter settings (https://openrouter.ai)
    openrouter_api_key: str = Field(
        default="",
        description="OpenRouter API key",
    )
    openrouter_model: str = Field(
        default="anthropic/claude-3.5-sonnet",
        description="OpenRouter model (e.g., anthropic/claude-3.5-sonnet, openai/gpt-4o, google/gemini-pro)",
    )
    openrouter_base_url: str = Field(
        default="https://openrouter.ai/api/v1",
        description="OpenRouter API base URL",
    )

    # Ollama settings (local LLM)
    ollama_base_url: str = Field(
        default="http://localhost:11434",
        description="Ollama server URL",
    )
    ollama_model: str = Field(
        default="llama3.2",
        description="Ollama model name",
    )

    # Azure OpenAI settings
    azure_api_key: str = Field(
        default="",
        description="Azure OpenAI API key",
    )
    azure_endpoint: str = Field(
        default="",
        description="Azure OpenAI endpoint URL (e.g., https://your-resource.openai.azure.com)",
    )
    azure_deployment_name: str = Field(
        default="gpt-4o",
        description="Azure OpenAI deployment name",
    )
    azure_api_version: str = Field(
        default="2024-02-15-preview",
        description="Azure OpenAI API version",
    )

    # Custom REST API settings (supports non-OpenAI-compatible APIs)
    custom_api_key: str = Field(
        default="",
        description="API key for custom endpoint (optional if API doesn't require auth)",
    )
    custom_base_url: str = Field(
        default="",
        description="Custom REST API endpoint URL (e.g., https://workflow.nodove.com/webhook/aidove)",
    )
    custom_model: str = Field(
        default="",
        description="Model name for custom endpoint (used in request if format includes it)",
    )
    custom_request_format: str = Field(
        default="",
        description="""JSON template for custom API request body.
Use placeholders: {prompt} for user message, {session_id} for session ID, {model} for model name.
Example for AI Dove: {"chatInput": "{prompt}", "sessionId": "{session_id}"}
Example for standard: {"message": "{prompt}", "model": "{model}"}
Leave empty for OpenAI-compatible format.""",
    )
    custom_response_path: str = Field(
        default="reply",
        description="""JSON path to extract response text from API response.
Use dot notation for nested fields (e.g., 'choices.0.message.content' for OpenAI format).
For AI Dove API, use 'reply'.""",
    )
    custom_headers: str = Field(
        default="",
        description="""Custom HTTP headers as JSON object.
Example: {"X-Custom-Header": "value", "Authorization": "Bearer {api_key}"}
Use {api_key} placeholder for API key substitution.""",
    )

    # Common settings
    temperature: float = Field(
        default=0.0,
        description="LLM temperature",
    )
    max_tokens: int = Field(
        default=4096,
        description="Maximum tokens for LLM response",
    )


class SearchSettings(BaseSettings):
    """Search provider settings."""

    model_config = SettingsConfigDict(env_prefix="SEARCH_")

    # API Keys
    brave_api_key: str = Field(
        default="",
        description="Brave Search API key",
    )
    tavily_api_key: str = Field(
        default="",
        description="Tavily Search API key",
    )
    perplexity_api_key: str = Field(
        default="",
        description="Perplexity API key",
    )

    # Configuration
    timeout: float = Field(
        default=30.0,
        description="Timeout for search requests in seconds",
    )
    max_results_per_provider: int = Field(
        default=10,
        description="Maximum results per search provider",
    )
    max_total_results: int = Field(
        default=30,
        description="Maximum total aggregated results",
    )
    enable_parallel: bool = Field(
        default=True,
        description="Enable parallel search across providers",
    )

    # RRF (Reciprocal Rank Fusion) Settings
    enable_rrf: bool = Field(
        default=True,
        description="Enable RRF-based multi-strategy search for improved accuracy",
    )
    rrf_k: int = Field(
        default=60,
        ge=1,
        le=1000,
        description="RRF constant k (higher = more weight to lower ranks, default: 60)",
    )
    enable_semantic_rrf: bool = Field(
        default=True,
        description="Enable semantic similarity scoring in RRF",
    )
    enable_query_expansion: bool = Field(
        default=True,
        description="Enable LLM-based query expansion for better search accuracy",
    )
    max_expanded_queries: int = Field(
        default=5,
        ge=1,
        le=10,
        description="Maximum number of expanded queries for multi-strategy search",
    )
    cache_query_analysis: bool = Field(
        default=True,
        description="Cache query analysis results to reduce LLM calls",
    )


class StealthSettings(BaseSettings):
    """Stealth/anti-detection settings."""

    model_config = SettingsConfigDict(env_prefix="STEALTH_")

    enabled: bool = Field(
        default=True,
        description="Enable stealth mode for browser",
    )
    hide_webdriver: bool = Field(
        default=True,
        description="Hide webdriver detection flags",
    )
    hide_automation: bool = Field(
        default=True,
        description="Hide automation flags",
    )
    mask_webgl: bool = Field(
        default=True,
        description="Mask WebGL vendor/renderer",
    )
    random_user_agent: bool = Field(
        default=False,
        description="Use random user agent on each session",
    )
    # NopeCHA CAPTCHA solver extension
    use_nopecha: bool = Field(
        default=True,
        description="Enable NopeCHA extension for automatic CAPTCHA solving",
    )
    nopecha_api_key: str = Field(
        default="",
        description="NopeCHA API key (optional, for faster solving)",
    )
    # Advanced stealth patches
    use_advanced_patches: bool = Field(
        default=True,
        description="Apply advanced JavaScript patches for bot detection bypass",
    )
    # Human behavior simulation
    simulate_human_behavior: bool = Field(
        default=True,
        description="Simulate human-like mouse movements and scrolling",
    )


class CamoufoxSettings(BaseSettings):
    """Camoufox Firefox-based anti-detect browser settings."""

    model_config = SettingsConfigDict(env_prefix="CAMOUFOX_")

    enabled: bool = Field(
        default=True,
        description="Enable Camoufox as alternative browser (when BROWSER_BACKEND=camoufox)",
    )
    humanize: bool = Field(
        default=True,
        description="Enable human-like behavior simulation",
    )
    humanize_level: int = Field(
        default=2,
        ge=1,
        le=3,
        description="Humanization level (1=low, 2=medium, 3=high)",
    )
    geoip: bool = Field(
        default=True,
        description="Enable GeoIP-based fingerprint matching",
    )
    block_webrtc: bool = Field(
        default=True,
        description="Block WebRTC to prevent IP leaks",
    )
    block_images: bool = Field(
        default=False,
        description="Block images for faster loading",
    )
    locale: str = Field(
        default="ko-KR",
        description="Browser locale",
    )
    timezone: str = Field(
        default="Asia/Seoul",
        description="Browser timezone",
    )
    os_type: Literal["windows", "macos", "linux", "random"] = Field(
        default="random",
        description="OS fingerprint type",
    )


class CaptchaSettings(BaseSettings):
    """CAPTCHA solving settings."""

    model_config = SettingsConfigDict(env_prefix="CAPTCHA_")

    enabled: bool = Field(
        default=True,
        description="Enable CAPTCHA solving",
    )
    prefer_audio: bool = Field(
        default=True,
        description="Prefer audio challenges for reCAPTCHA",
    )
    cloudflare_delay: int = Field(
        default=10,
        description="Delay for Cloudflare challenge (seconds)",
    )
    max_attempts: int = Field(
        default=3,
        description="Maximum CAPTCHA solve attempts",
    )
    # Paid CAPTCHA solver settings (more reliable for search portals)
    capsolver_api_key: str = Field(
        default="",
        description="CapSolver API key (https://capsolver.com) - Recommended for Turnstile",
    )
    twocaptcha_api_key: str = Field(
        default="",
        description="2Captcha API key (https://2captcha.com)",
    )
    prefer_paid_solver: bool = Field(
        default=True,
        description="Prefer paid solvers over free ones when API key is available",
    )
    paid_solver_timeout: float = Field(
        default=120.0,
        description="Timeout for paid CAPTCHA solving (seconds)",
    )


class RedisSettings(BaseSettings):
    """Redis connection settings for task state persistence."""

    model_config = SettingsConfigDict(env_prefix="REDIS_")

    enabled: bool = Field(
        default=True,
        description="Enable Redis for task state persistence",
    )
    url: str = Field(
        default="redis://localhost:6379/4",
        description="Redis connection URL",
    )
    prefix: str = Field(
        default="autonomous_crawler",
        description="Key prefix for Redis entries",
    )
    result_ttl_hours: int = Field(
        default=48,
        description="Task result TTL in hours",
    )
    socket_timeout: float = Field(
        default=5.0,
        description="Socket timeout in seconds",
    )
    connection_timeout: float = Field(
        default=5.0,
        description="Connection timeout in seconds",
    )
    max_connections: int = Field(
        default=10,
        description="Maximum Redis connections in pool",
    )
    retry_on_timeout: bool = Field(
        default=True,
        description="Retry operations on timeout",
    )


class MetricsSettings(BaseSettings):
    """Prometheus metrics settings."""

    model_config = SettingsConfigDict(env_prefix="METRICS_")

    enabled: bool = Field(
        default=True,
        description="Enable Prometheus metrics",
    )
    port: int = Field(
        default=9090,
        description="Metrics server port",
    )
    path: str = Field(
        default="/metrics",
        description="Metrics endpoint path",
    )


class Settings(BaseSettings):
    """Main application settings."""

    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        extra="ignore",
    )

    # Service settings
    service_name: str = Field(
        default="autonomous-crawler-service",
        description="Service name",
    )
    log_level: Literal["DEBUG", "INFO", "WARNING", "ERROR"] = Field(
        default="INFO",
        description="Logging level",
    )
    log_format: Literal["json", "console"] = Field(
        default="json",
        description="Log output format",
    )

    # Nested settings
    kafka: KafkaSettings = Field(default_factory=KafkaSettings)
    browser: BrowserSettings = Field(default_factory=BrowserSettings)
    llm: LLMSettings = Field(default_factory=LLMSettings)
    search: SearchSettings = Field(default_factory=SearchSettings)
    stealth: StealthSettings = Field(default_factory=StealthSettings)
    captcha: CaptchaSettings = Field(default_factory=CaptchaSettings)
    camoufox: CamoufoxSettings = Field(default_factory=CamoufoxSettings)
    redis: RedisSettings = Field(default_factory=RedisSettings)
    metrics: MetricsSettings = Field(default_factory=MetricsSettings)


@lru_cache
def get_settings() -> Settings:
    """Get cached application settings."""
    return Settings()

```

---

## backend/autonomous-crawler-service/src/crawler/__init__.py

```py
"""Crawler module for autonomous-crawler-service."""

from .agent import AutonomousCrawlerAgent
from .policies import CrawlPolicy, get_policy_prompt

__all__ = [
    "AutonomousCrawlerAgent",
    "CrawlPolicy",
    "get_policy_prompt",
]

```

---

## backend/autonomous-crawler-service/src/crawler/agent.py

```py
"""Autonomous crawler agent using browser-use with CAPTCHA bypass and proxy rotation."""

import asyncio
import json
import os
import re
import time
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Optional, TYPE_CHECKING

import httpx
import structlog
from browser_use.agent.service import Agent
from browser_use.browser.session import BrowserSession
from browser_use.browser.profile import BrowserProfile, ProxySettings
from browser_use.llm.openai.chat import ChatOpenAI
from browser_use.llm.anthropic.chat import ChatAnthropic
from pydantic import BaseModel

from src.config import Settings

# Proxy rotation client
try:
    import sys

    sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "..", ".."))
    from shared.proxy_client import ProxyRotationClient, ProxyInfo

    PROXY_CLIENT_AVAILABLE = True
except ImportError:
    PROXY_CLIENT_AVAILABLE = False
    ProxyRotationClient = None  # type: ignore
    ProxyInfo = None  # type: ignore
from src.crawler.policies import CrawlPolicy, get_policy_prompt
from src.kafka.messages import BrowserTaskMessage, CrawlResultMessage
from src.captcha.stealth import (
    StealthConfig,
    EnhancedStealthConfig,
    apply_stealth_to_playwright_async,
    get_undetected_browser_args,
    get_stealth_browser_args_with_extensions,
)
from src.captcha import (
    CaptchaSolverOrchestrator,
    CaptchaType,
    AdvancedStealthPatcher,
    HumanBehaviorSimulator,
    # Camoufox
    CamoufoxConfig,
    CamoufoxHelper,
    create_camoufox_browser,
    get_recommended_camoufox_config,
    is_camoufox_available,
)
from src.search.orchestrator import (
    ParallelSearchOrchestrator,
    RRFSearchOrchestrator,
    create_rrf_orchestrator,
)
from src.search.brave import BraveSearchProvider
from src.search.tavily import TavilySearchProvider
from src.search.perplexity import PerplexitySearchProvider
from src.search.query_analyzer import QueryAnalyzer

if TYPE_CHECKING:
    from browser_use.agent.service import Agent as BrowserUseAgent

logger = structlog.get_logger(__name__)


# =============================================================================
# Custom REST API Adapter for non-OpenAI-compatible APIs
# =============================================================================


class CustomRESTAPIClient:
    """
    Adapter for custom REST APIs with configurable request/response formats.

    Supports APIs like AI Dove that use non-standard request formats.
    Implements a minimal interface compatible with browser-use's LLM requirements.
    """

    def __init__(
        self,
        base_url: str,
        api_key: str = "",
        model: str = "",
        request_format: str = "",
        response_path: str = "reply",
        custom_headers: str = "",
        temperature: float = 0.0,
        timeout: float = 120.0,
    ):
        """
        Initialize the custom REST API client.

        Args:
            base_url: The API endpoint URL
            api_key: Optional API key for authentication
            model: Model name (used in request if format includes it)
            request_format: JSON template for request body with placeholders
            response_path: Dot-notation path to extract response from JSON
            custom_headers: JSON string of custom headers
            temperature: Temperature parameter (passed to API if supported)
            timeout: Request timeout in seconds
        """
        self.base_url = base_url.rstrip("/")
        self.api_key = api_key
        self.model = model
        self.request_format = request_format
        self.response_path = response_path
        self.custom_headers = custom_headers
        self.temperature = temperature
        self.timeout = timeout
        self._session_id = f"crawler_{int(time.time())}"

        # For browser-use compatibility
        self.provider = "custom"

    def _build_headers(self) -> dict[str, str]:
        """Build HTTP headers for the request."""
        headers = {"Content-Type": "application/json"}

        # Parse custom headers if provided
        if self.custom_headers:
            try:
                custom = json.loads(self.custom_headers)
                for key, value in custom.items():
                    # Replace {api_key} placeholder
                    if isinstance(value, str):
                        value = value.replace("{api_key}", self.api_key)
                    headers[key] = value
            except json.JSONDecodeError:
                logger.warning("Failed to parse custom_headers JSON", headers=self.custom_headers)

        # Add default Authorization header if API key is provided and not in custom headers
        if self.api_key and "Authorization" not in headers:
            headers["Authorization"] = f"Bearer {self.api_key}"

        return headers

    def _build_request_body(self, prompt: str) -> dict[str, Any]:
        """Build request body from template or default format."""
        if self.request_format:
            try:
                # Parse the template and substitute placeholders
                template = self.request_format
                template = template.replace("{prompt}", prompt)
                template = template.replace("{session_id}", self._session_id)
                template = template.replace("{model}", self.model)
                template = template.replace("{temperature}", str(self.temperature))
                return json.loads(template)
            except json.JSONDecodeError as e:
                logger.error(
                    "Failed to parse request_format JSON",
                    error=str(e),
                    template=self.request_format,
                )
                raise ValueError(f"Invalid custom_request_format JSON: {e}")

        # Default OpenAI-compatible format
        return {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": self.temperature,
        }

    def _extract_response(self, response_json: dict[str, Any]) -> str:
        """Extract response text using the configured path."""
        if not self.response_path:
            # Return full response as string
            return json.dumps(response_json)

        # Navigate the JSON path (supports dot notation and array indices)
        current = response_json
        for part in self.response_path.split("."):
            if part.isdigit():
                # Array index
                idx = int(part)
                if isinstance(current, list) and len(current) > idx:
                    current = current[idx]
                else:
                    raise KeyError(f"Array index {idx} not found in response")
            elif isinstance(current, dict):
                if part in current:
                    current = current[part]
                else:
                    raise KeyError(f"Key '{part}' not found in response: {list(current.keys())}")
            else:
                raise KeyError(f"Cannot navigate '{part}' in non-dict/list value")

        return str(current) if current is not None else ""

    async def ainvoke(self, messages: list[dict[str, Any]], **kwargs) -> Any:
        """
        Async invoke the custom API (compatible with LangChain interface).

        Args:
            messages: List of message dicts with 'role' and 'content' keys
            **kwargs: Additional arguments (ignored for custom API)

        Returns:
            Response object with 'content' attribute
        """
        # Extract the last user message as the prompt
        prompt = ""
        for msg in reversed(messages):
            if isinstance(msg, dict) and msg.get("role") == "user":
                prompt = msg.get("content", "")
                break
            elif hasattr(msg, "content"):
                prompt = msg.content
                break

        if not prompt:
            # Fallback: concatenate all messages
            prompt = "\n".join(
                msg.get("content", str(msg)) if isinstance(msg, dict) else str(msg)
                for msg in messages
            )

        headers = self._build_headers()
        body = self._build_request_body(prompt)

        logger.debug(
            "Custom API request",
            url=self.base_url,
            body_keys=list(body.keys()) if isinstance(body, dict) else "raw",
        )

        async with httpx.AsyncClient(timeout=self.timeout) as client:
            response = await client.post(
                self.base_url,
                headers=headers,
                json=body,
            )

            if response.status_code != 200:
                error_text = response.text[:500]
                logger.error(
                    "Custom API error",
                    status=response.status_code,
                    error=error_text,
                )
                raise RuntimeError(f"Custom API error {response.status_code}: {error_text}")

            response_json = response.json()
            content = self._extract_response(response_json)

            logger.debug(
                "Custom API response",
                response_keys=list(response_json.keys())
                if isinstance(response_json, dict)
                else "raw",
                content_length=len(content),
            )

        # Return an object with 'content' attribute for browser-use compatibility
        class Response:
            def __init__(self, text: str):
                self.content = text

        return Response(content)

    def invoke(self, messages: list[dict[str, Any]], **kwargs) -> Any:
        """Sync invoke - runs async version in event loop."""
        return asyncio.run(self.ainvoke(messages, **kwargs))


# =============================================================================
# CAPTCHA Detection Hook for browser-use Agent
# =============================================================================


async def create_captcha_detection_hook(
    crawler_agent: "AutonomousCrawlerAgent",
    on_captcha_detected: callable = None,
) -> callable:
    """
    Create a hook function for browser-use Agent that detects CAPTCHAs.

    This hook is called at the start of each step to check for CAPTCHAs
    and attempt to solve them before the agent takes action.

    Args:
        crawler_agent: The AutonomousCrawlerAgent instance
        on_captcha_detected: Optional callback when CAPTCHA is detected

    Returns:
        Async hook function compatible with browser-use Agent
    """

    async def on_step_start_hook(agent: "BrowserUseAgent") -> None:
        """Hook called at the start of each browser-use step."""
        try:
            # Get the current page from browser session
            browser_session = agent.browser_session
            if not browser_session:
                return

            # Access the current page
            page = None
            try:
                # browser-use stores pages internally
                if hasattr(browser_session, "_context") and browser_session._context:
                    pages = browser_session._context.pages
                    if pages:
                        page = pages[-1]  # Get the most recent page
            except Exception:
                pass

            if not page:
                return

            # Check for CAPTCHA indicators
            captcha_detected = await _quick_captcha_check(page)

            if captcha_detected:
                logger.info("CAPTCHA detected in browser-use step, attempting to solve...")

                if on_captcha_detected:
                    await on_captcha_detected(captcha_detected)

                # Try to solve the CAPTCHA
                solved = await crawler_agent._detect_and_handle_captcha(page)

                if solved:
                    logger.info("CAPTCHA solved successfully, continuing agent step")
                else:
                    logger.warning("CAPTCHA could not be solved, agent may fail on this step")

                    # Simulate human behavior to appear more legitimate
                    if crawler_agent._stealth_config.enable_human_simulation:
                        await crawler_agent._simulate_human_behavior(page)

        except Exception as e:
            logger.debug("Error in CAPTCHA detection hook", error=str(e))

    return on_step_start_hook


async def create_stealth_hook(crawler_agent: "AutonomousCrawlerAgent") -> callable:
    """
    Create a hook function that applies stealth patches after navigation.

    This hook is called at the end of each step to re-apply stealth patches
    if the page has navigated to a new URL.
    """
    _last_url = {"value": None}

    async def on_step_end_hook(agent: "BrowserUseAgent") -> None:
        """Hook called at the end of each browser-use step."""
        try:
            browser_session = agent.browser_session
            if not browser_session:
                return

            page = None
            try:
                if hasattr(browser_session, "_context") and browser_session._context:
                    pages = browser_session._context.pages
                    if pages:
                        page = pages[-1]
            except Exception:
                pass

            if not page:
                return

            current_url = page.url

            # Only apply patches if we navigated to a new URL
            if current_url != _last_url["value"]:
                _last_url["value"] = current_url

                # Re-apply stealth patches to the new page
                await AdvancedStealthPatcher.apply_to_page(page)

                # Brief human-like delay after navigation
                if crawler_agent._stealth_config.enable_human_simulation:
                    await asyncio.sleep(0.5)

        except Exception as e:
            logger.debug("Error in stealth hook", error=str(e))

    return on_step_end_hook


async def _quick_captcha_check(page) -> str | None:
    """
    Quick check for common CAPTCHA indicators on a page.

    Returns the CAPTCHA type if detected, None otherwise.
    """
    captcha_indicators = [
        # Cloudflare
        (
            "cloudflare",
            [
                "#challenge-running",
                ".cf-browser-verification",
                "iframe[src*='turnstile']",
                "#cf-turnstile",
                "div[class*='challenge']",
            ],
        ),
        # reCAPTCHA
        (
            "recaptcha",
            [
                "iframe[src*='recaptcha']",
                ".g-recaptcha",
                "#recaptcha",
                "div[class*='recaptcha']",
            ],
        ),
        # hCaptcha
        (
            "hcaptcha",
            [
                "iframe[src*='hcaptcha']",
                ".h-captcha",
                "div[class*='hcaptcha']",
            ],
        ),
        # Generic bot detection
        (
            "bot_detection",
            [
                "text=checking your browser",
                "text=please verify you are human",
                "text=access denied",
                "text=blocked",
            ],
        ),
    ]

    for captcha_type, selectors in captcha_indicators:
        for selector in selectors:
            try:
                if selector.startswith("text="):
                    # Text-based detection
                    text = selector[5:].lower()
                    page_text = await page.inner_text("body")
                    if text in page_text.lower():
                        return captcha_type
                else:
                    # Selector-based detection
                    element = await page.query_selector(selector)
                    if element:
                        is_visible = await element.is_visible()
                        if is_visible:
                            return captcha_type
            except Exception:
                continue

    return None


class ExtractedArticle(BaseModel):
    """Extracted article content from a page."""

    url: str
    title: str
    content: str
    published_at: str | None = None
    author: str | None = None
    summary: str | None = None
    extraction_time: datetime = field(default_factory=datetime.now)


@dataclass
class CrawlSession:
    """Tracks the state of a crawling session."""

    job_id: int
    source_id: int
    seed_url: str
    max_depth: int
    max_pages: int
    budget_seconds: int
    policy: CrawlPolicy
    focus_keywords: list[str]
    excluded_domains: list[str]

    # Runtime state
    visited_urls: set[str] = field(default_factory=set)
    extracted_articles: list[CrawlResultMessage] = field(default_factory=list)
    start_time: datetime | None = None
    end_time: datetime | None = None
    error: str | None = None

    # AutoCrawl metadata (passed from BrowserTaskMessage.metadata)
    metadata: dict[str, Any] | None = None

    def is_budget_exceeded(self) -> bool:
        """Check if time budget has been exceeded."""
        if not self.start_time:
            return False
        elapsed = (datetime.now() - self.start_time).total_seconds()
        return elapsed >= self.budget_seconds

    def is_page_limit_reached(self) -> bool:
        """Check if page limit has been reached."""
        return len(self.visited_urls) >= self.max_pages

    def can_continue(self) -> bool:
        """Check if crawling can continue."""
        return not self.is_budget_exceeded() and not self.is_page_limit_reached()


class AutonomousCrawlerAgent:
    """
    AI-driven autonomous web crawler using browser-use.

    Consumes BrowserTaskMessage from Kafka and produces CrawlResultMessage
    for each extracted article.

    Supports two browser backends:
    - Playwright (Chrome/Chromium) with stealth patches and NopeCHA extension
    - Camoufox (Firefox-based) anti-detect browser with built-in fingerprint spoofing

    Integrates with IP Rotation service to:
    - Automatically rotate proxies for each crawl session
    - Report CAPTCHA encounters to help weighted proxy selection
    - Retry with different proxies when CAPTCHA solving fails
    """

    def __init__(self, settings: Settings) -> None:
        self.settings = settings
        self._browser_session: BrowserSession | None = None
        self._camoufox_browser: Any = None  # Camoufox browser instance
        self._llm = self._create_llm()
        self._search_orchestrator: ParallelSearchOrchestrator | None = None
        self._rrf_search_orchestrator: RRFSearchOrchestrator | None = None
        self._query_analyzer: QueryAnalyzer | None = None
        self._captcha_solver: CaptchaSolverOrchestrator | None = None
        self._stealth_config = EnhancedStealthConfig(
            use_nopecha=getattr(settings.stealth, "use_nopecha", True),
            nopecha_api_key=getattr(settings.stealth, "nopecha_api_key", ""),
            use_camoufox=getattr(settings.browser, "backend", "playwright") == "camoufox",
            enable_human_simulation=getattr(settings.stealth, "simulate_human_behavior", True),
        )

        # Determine browser backend
        self._use_camoufox = getattr(settings.browser, "backend", "playwright") == "camoufox"
        if self._use_camoufox and not is_camoufox_available():
            logger.warning("Camoufox requested but not available, falling back to Playwright")
            self._use_camoufox = False

        # Proxy rotation integration
        self._proxy_client: Optional[Any] = None
        self._current_proxy: Optional[Any] = None  # Current ProxyInfo
        self._use_proxy_rotation = getattr(settings, "use_proxy_rotation", True)
        self._proxy_rotation_url = getattr(
            settings,
            "proxy_rotation_url",
            os.environ.get("PROXY_ROTATION_URL", "http://ip-rotation:8050"),
        )

        # Initialize proxy client if available
        if PROXY_CLIENT_AVAILABLE and self._use_proxy_rotation:
            self._proxy_client = ProxyRotationClient(
                base_url=self._proxy_rotation_url,
                timeout=5.0,
                enabled=True,
            )
            logger.info("Proxy rotation enabled", url=self._proxy_rotation_url)

    def _create_llm(self) -> ChatOpenAI | ChatAnthropic:
        """Create the LLM instance based on settings.

        Uses browser-use's LLM classes which implement the BaseChatModel protocol
        with the required .provider property.

        Supported providers:
        - openai: OpenAI API
        - anthropic: Anthropic Claude API
        - openrouter: OpenRouter (multiple models via single API)
        - ollama: Local Ollama server
        - custom: Custom OpenAI-compatible REST API
        """
        llm_settings = self.settings.llm
        provider = llm_settings.provider.lower()

        if provider == "anthropic":
            logger.info("Using Anthropic provider", model=llm_settings.anthropic_model)
            return ChatAnthropic(
                model=llm_settings.anthropic_model,
                api_key=llm_settings.anthropic_api_key,
                temperature=llm_settings.temperature,
                max_tokens=llm_settings.max_tokens,
            )

        elif provider == "openrouter":
            # OpenRouter uses OpenAI-compatible API
            logger.info("Using OpenRouter provider", model=llm_settings.openrouter_model)
            return ChatOpenAI(
                model=llm_settings.openrouter_model,
                api_key=llm_settings.openrouter_api_key,
                base_url=llm_settings.openrouter_base_url,
                temperature=llm_settings.temperature,
                max_completion_tokens=llm_settings.max_tokens,
                default_headers={
                    "HTTP-Referer": "https://newsinsight.app",
                    "X-Title": "NewsInsight Crawler",
                },
            )

        elif provider == "ollama":
            # Ollama uses OpenAI-compatible API
            logger.info(
                "Using Ollama provider",
                model=llm_settings.ollama_model,
                base_url=llm_settings.ollama_base_url,
            )
            return ChatOpenAI(
                model=llm_settings.ollama_model,
                api_key="ollama",  # Ollama doesn't require API key but field is required
                base_url=f"{llm_settings.ollama_base_url}/v1",
                temperature=llm_settings.temperature,
                max_completion_tokens=llm_settings.max_tokens,
            )

        elif provider == "custom":
            # Custom REST API endpoint (supports non-OpenAI-compatible APIs)
            if not llm_settings.custom_base_url:
                raise ValueError("LLM_CUSTOM_BASE_URL is required when using custom provider")

            # Check if custom request format is provided (non-OpenAI-compatible API)
            if llm_settings.custom_request_format:
                logger.info(
                    "Using custom REST API provider with custom format",
                    base_url=llm_settings.custom_base_url,
                    response_path=llm_settings.custom_response_path,
                )
                return CustomRESTAPIClient(
                    base_url=llm_settings.custom_base_url,
                    api_key=llm_settings.custom_api_key,
                    model=llm_settings.custom_model,
                    request_format=llm_settings.custom_request_format,
                    response_path=llm_settings.custom_response_path,
                    custom_headers=llm_settings.custom_headers,
                    temperature=llm_settings.temperature,
                )
            else:
                # Fallback to OpenAI-compatible format
                logger.info(
                    "Using custom provider (OpenAI-compatible)",
                    model=llm_settings.custom_model,
                    base_url=llm_settings.custom_base_url,
                )
                return ChatOpenAI(
                    model=llm_settings.custom_model,
                    api_key=llm_settings.custom_api_key or "not-required",
                    base_url=llm_settings.custom_base_url,
                    temperature=llm_settings.temperature,
                    max_completion_tokens=llm_settings.max_tokens,
                )

        else:
            # Default: OpenAI
            base_url = llm_settings.openai_base_url or None
            logger.info("Using OpenAI provider", model=llm_settings.openai_model, base_url=base_url)
            kwargs = {
                "model": llm_settings.openai_model,
                "api_key": llm_settings.openai_api_key,
                "temperature": llm_settings.temperature,
                "max_completion_tokens": llm_settings.max_tokens,
            }
            if base_url:
                kwargs["base_url"] = base_url
            return ChatOpenAI(**kwargs)

    def _get_search_orchestrator(self) -> ParallelSearchOrchestrator:
        """Get or create the search orchestrator with configured providers."""
        if self._search_orchestrator is None:
            providers = []
            search_settings = self.settings.search

            # Add Brave Search if API key is configured
            if search_settings.brave_api_key:
                providers.append(BraveSearchProvider(search_settings.brave_api_key))
                logger.info("Brave Search provider enabled")

            # Add Tavily if API key is configured
            if search_settings.tavily_api_key:
                providers.append(TavilySearchProvider(search_settings.tavily_api_key))
                logger.info("Tavily Search provider enabled")

            # Add Perplexity if API key is configured
            if search_settings.perplexity_api_key:
                providers.append(PerplexitySearchProvider(search_settings.perplexity_api_key))
                logger.info("Perplexity Search provider enabled")

            if not providers:
                logger.warning("No search providers configured - API search disabled")

            self._search_orchestrator = ParallelSearchOrchestrator(
                providers=providers,
                timeout=search_settings.timeout,
                deduplicate=True,
            )

        return self._search_orchestrator

    def _get_rrf_search_orchestrator(self) -> RRFSearchOrchestrator:
        """Get or create the RRF search orchestrator with query analysis."""
        if self._rrf_search_orchestrator is None:
            providers = []
            search_settings = self.settings.search

            # Add Brave Search if API key is configured
            if search_settings.brave_api_key:
                providers.append(BraveSearchProvider(search_settings.brave_api_key))

            # Add Tavily if API key is configured
            if search_settings.tavily_api_key:
                providers.append(TavilySearchProvider(search_settings.tavily_api_key))

            # Add Perplexity if API key is configured
            if search_settings.perplexity_api_key:
                providers.append(PerplexitySearchProvider(search_settings.perplexity_api_key))

            if not providers:
                logger.warning("No search providers configured - RRF search disabled")

            # Create query analyzer with the LLM
            self._query_analyzer = QueryAnalyzer(
                llm=self._llm,
                enable_expansion=True,
                max_expanded_queries=5,
                cache_results=True,
            )

            # Get RRF settings
            rrf_k = getattr(search_settings, "rrf_k", 60)
            enable_semantic = getattr(search_settings, "enable_semantic_rrf", True)

            self._rrf_search_orchestrator = create_rrf_orchestrator(
                providers=providers,
                llm=self._llm,
                timeout=search_settings.timeout,
                rrf_k=rrf_k,
                enable_semantic=enable_semantic,
            )

            logger.info(
                "RRF Search orchestrator initialized",
                providers=len(providers),
                rrf_k=rrf_k,
                semantic_enabled=enable_semantic,
            )

        return self._rrf_search_orchestrator

    def _get_captcha_solver(self) -> CaptchaSolverOrchestrator:
        """Get or create the CAPTCHA solver orchestrator with paid solver support."""
        if self._captcha_solver is None:
            # Get paid solver API keys from settings
            captcha_settings = getattr(self.settings, "captcha", None)
            capsolver_key = getattr(captcha_settings, "capsolver_api_key", "") if captcha_settings else ""
            twocaptcha_key = getattr(captcha_settings, "twocaptcha_api_key", "") if captcha_settings else ""
            prefer_paid = getattr(captcha_settings, "prefer_paid_solver", True) if captcha_settings else True
            paid_timeout = getattr(captcha_settings, "paid_solver_timeout", 120.0) if captcha_settings else 120.0
            
            self._captcha_solver = CaptchaSolverOrchestrator(
                capsolver_api_key=capsolver_key,
                twocaptcha_api_key=twocaptcha_key,
                prefer_paid=prefer_paid,
                paid_timeout=paid_timeout,
            )
        return self._captcha_solver

    async def _get_browser_session(self, force_new: bool = False) -> BrowserSession:
        """Get or create the browser session with enhanced stealth configuration.

        Args:
            force_new: If True, always create a new browser session (recommended for task isolation)
        """
        # Use Camoufox if configured
        if self._use_camoufox:
            return await self._get_camoufox_session()

        # Close existing session if force_new or if session is not healthy
        if force_new and self._browser_session is not None:
            try:
                await self._browser_session.stop()
            except Exception as e:
                logger.debug("Error closing existing browser session", error=str(e))
            self._browser_session = None

        # Use Playwright with stealth
        if self._browser_session is None:
            stealth_settings = self.settings.stealth

            # Setup extensions if NopeCHA is enabled
            if self._stealth_config.use_nopecha:
                await self._stealth_config.setup_extensions()
                logger.info("NopeCHA extension configured for CAPTCHA bypass")

            # Build browser args for stealth mode
            extra_args = []
            if stealth_settings.enabled:
                if self._stealth_config.extension_paths:
                    # Use enhanced args with extension support
                    extra_args = self._stealth_config.get_browser_args(
                        include_docker=getattr(self.settings.browser, "is_docker_env", False)
                    )
                else:
                    extra_args = get_undetected_browser_args()
                logger.info(
                    "Stealth mode enabled for browser session",
                    extensions_loaded=len(self._stealth_config.extension_paths),
                )

            # Get proxy from rotation service if available
            proxy_settings = None
            if self._proxy_client:
                try:
                    self._current_proxy = await self._proxy_client.get_next_proxy()
                    if self._current_proxy:
                        proxy_settings = ProxySettings(
                            server=self._current_proxy.address,
                            username=self._current_proxy.username,
                            password=self._current_proxy.password,
                        )
                        logger.info(
                            "Proxy assigned for browser session",
                            proxy_id=self._current_proxy.id,
                            proxy_address=self._current_proxy.address,
                        )
                except Exception as e:
                    logger.warning("Failed to get proxy from rotation service", error=str(e))

            # Detect Playwright browser path in Docker environment
            executable_path = None
            if getattr(self.settings.browser, "is_docker_env", False):
                # Look for installed Chromium in Playwright's cache
                playwright_path = os.environ.get(
                    "PLAYWRIGHT_BROWSERS_PATH", os.path.expanduser("~/.cache/ms-playwright")
                )
                # Find chromium executable
                import glob

                chromium_patterns = [
                    f"{playwright_path}/chromium-*/chrome-linux64/chrome",
                    f"{playwright_path}/chromium-*/chrome-linux/chrome",
                    f"{playwright_path}/chromium_headless_shell-*/chrome-linux64/headless_shell",
                    f"{playwright_path}/chromium_headless_shell-*/chrome-linux/headless_shell",
                ]
                for pattern in chromium_patterns:
                    matches = glob.glob(pattern)
                    if matches:
                        executable_path = sorted(matches)[-1]  # Use latest version
                        logger.debug("Found Playwright browser", executable_path=executable_path)
                        break
                if not executable_path:
                    logger.warning("No Playwright browser found, browser-use will try to install")

            profile = BrowserProfile(
                headless=self.settings.browser.headless,
                disable_security=True,  # Required for some sites
                extra_chromium_args=extra_args,
                proxy=proxy_settings,  # Apply proxy from rotation service
                executable_path=executable_path,  # Use pre-installed Playwright browser
            )
            self._browser_session = BrowserSession(browser_profile=profile)
        return self._browser_session

    async def _get_camoufox_session(self) -> Any:
        """Get or create Camoufox browser session."""
        if self._camoufox_browser is None:
            camoufox_settings = getattr(self.settings, "camoufox", None)

            # Build Camoufox config
            if camoufox_settings:
                config = CamoufoxConfig(
                    headless=self.settings.browser.headless,
                    humanize=camoufox_settings.humanize,
                    humanize_level=camoufox_settings.humanize_level,
                    locale=camoufox_settings.locale,
                    timezone=camoufox_settings.timezone,
                    geoip=camoufox_settings.geoip,
                    block_webrtc=camoufox_settings.block_webrtc,
                    block_images=camoufox_settings.block_images,
                    os=camoufox_settings.os_type if camoufox_settings.os_type != "random" else None,
                )
            else:
                # Use recommended config for Cloudflare bypass
                config = get_recommended_camoufox_config(
                    purpose="cloudflare",
                    headless=self.settings.browser.headless,
                )

            self._camoufox_browser = await create_camoufox_browser(config)

            if self._camoufox_browser:
                logger.info(
                    "Camoufox browser created",
                    headless=config.headless,
                    humanize=config.humanize,
                    humanize_level=config.humanize_level,
                )
            else:
                logger.error("Failed to create Camoufox browser, falling back to Playwright")
                self._use_camoufox = False
                return await self._get_browser_session()

        return self._camoufox_browser

    async def _get_camoufox_page(self) -> Any:
        """Get a new page from Camoufox browser."""
        browser = await self._get_camoufox_session()
        if browser:
            try:
                page = await browser.new_page()
                logger.debug("Created new Camoufox page")
                return page
            except Exception as e:
                logger.error("Failed to create Camoufox page", error=str(e))
        return None

    async def _apply_page_stealth(self, page) -> None:
        """Apply advanced stealth patches to a page."""
        # Apply playwright_stealth or manual patches
        await apply_stealth_to_playwright_async(page, self._stealth_config)

        # Apply advanced stealth patches from undetected module
        await AdvancedStealthPatcher.apply_to_page(page)

        logger.debug("Applied advanced stealth patches to page")

    async def search_before_crawl(
        self,
        query: str,
        max_results: int = 20,
        use_rrf: bool = True,
    ) -> list[str]:
        """
        Perform API-based search before browser crawling.

        Returns list of URLs to visit based on search results.
        Useful for bypassing search engine CAPTCHAs.

        Args:
            query: Search query
            max_results: Maximum number of URLs to return
            use_rrf: Use RRF-based multi-strategy search for better accuracy
        """
        if use_rrf:
            return await self._search_with_rrf(query, max_results)

        # Fallback to simple parallel search
        orchestrator = self._get_search_orchestrator()

        if not orchestrator.providers:
            logger.warning("No search providers available")
            return []

        try:
            result = await orchestrator.search_news(
                query=query,
                max_results_per_provider=self.settings.search.max_results_per_provider,
            )

            urls = [r.url for r in result.results[:max_results]]

            logger.info(
                "Search completed",
                query=query,
                results_count=len(urls),
                providers_used=result.providers_used,
            )

            return urls

        except Exception as e:
            logger.error("Search failed", query=query, error=str(e))
            return []

    async def _search_with_rrf(
        self,
        query: str,
        max_results: int = 20,
    ) -> list[str]:
        """
        Perform RRF-based multi-strategy search.

        This method:
        1. Analyzes the query to understand intent and extract keywords
        2. Expands the query into multiple semantically related queries
        3. Executes parallel searches across all providers for each query variant
        4. Merges results using Reciprocal Rank Fusion algorithm
        5. Returns URLs ranked by combined relevance
        """
        orchestrator = self._get_rrf_search_orchestrator()

        if not orchestrator.providers:
            logger.warning("No search providers available for RRF search")
            return []

        try:
            result = await orchestrator.search_news_with_rrf(
                query=query,
                max_results_per_strategy=self.settings.search.max_results_per_provider,
            )

            urls = [r.url for r in result.results[:max_results]]

            logger.info(
                "RRF search completed",
                query=query,
                results_count=len(urls),
                strategies_used=result.strategies_used,
                providers_used=result.providers_used,
                query_analysis=result.query_analysis,
            )

            return urls

        except Exception as e:
            logger.error(
                "RRF search failed, falling back to simple search", query=query, error=str(e)
            )
            # Fallback to simple search
            return await self.search_before_crawl(query, max_results, use_rrf=False)

    async def close(self) -> None:
        """Close the browser and cleanup resources."""
        if self._browser_session:
            await self._browser_session.stop()
            self._browser_session = None

        if self._camoufox_browser:
            try:
                await self._camoufox_browser.close()
            except Exception as e:
                logger.debug("Error closing Camoufox browser", error=str(e))
            self._camoufox_browser = None

        if self._search_orchestrator:
            await self._search_orchestrator.close_all()
            self._search_orchestrator = None

        # Close proxy client
        if self._proxy_client:
            try:
                await self._proxy_client.close()
            except Exception as e:
                logger.debug("Error closing proxy client", error=str(e))
            self._proxy_client = None
            self._current_proxy = None

    async def crawl_with_camoufox(
        self,
        url: str,
        extract_content: bool = True,
        wait_for_cloudflare: bool = True,
    ) -> dict[str, Any]:
        """
        Crawl a URL using Camoufox browser for maximum anti-detection.

        Args:
            url: URL to crawl
            extract_content: Whether to extract page content
            wait_for_cloudflare: Whether to wait for Cloudflare challenge

        Returns:
            Dictionary with page content and metadata
        """
        page = await self._get_camoufox_page()
        if not page:
            return {"error": "Failed to create Camoufox page"}

        try:
            # Navigate to URL
            await page.goto(url, wait_until="domcontentloaded")

            # Wait for Cloudflare challenge if needed
            if wait_for_cloudflare:
                passed = await CamoufoxHelper.wait_for_cloudflare(page, timeout=30)
                if not passed:
                    logger.warning("Cloudflare challenge may not have completed", url=url)

            # Simulate human behavior
            if self._stealth_config.enable_human_simulation:
                await asyncio.sleep(1)  # Brief pause

            # Extract content
            if extract_content:
                content = await CamoufoxHelper.extract_page_content(page)
                content["success"] = True
                return content

            return {
                "success": True,
                "url": url,
                "title": await page.title(),
            }

        except Exception as e:
            logger.error("Camoufox crawl failed", url=url, error=str(e))
            return {"error": str(e), "success": False}
        finally:
            try:
                await page.close()
            except Exception:
                pass

    async def _detect_and_handle_captcha(self, page) -> bool:
        """
        Detect and attempt to handle CAPTCHAs on a page.

        Args:
            page: Playwright page object

        Returns:
            True if CAPTCHA was detected and handled (or not detected),
            False if CAPTCHA was detected but could not be handled
        """
        try:
            # Check for common CAPTCHA indicators
            captcha_selectors = {
                CaptchaType.RECAPTCHA_V2: [
                    "iframe[src*='recaptcha']",
                    ".g-recaptcha",
                    "#recaptcha",
                ],
                CaptchaType.HCAPTCHA: [
                    "iframe[src*='hcaptcha']",
                    ".h-captcha",
                ],
                CaptchaType.CLOUDFLARE: [
                    "#challenge-running",
                    ".cf-browser-verification",
                    "iframe[src*='turnstile']",
                    "#cf-turnstile",
                ],
            }

            detected_type = None
            for captcha_type, selectors in captcha_selectors.items():
                for selector in selectors:
                    try:
                        element = await page.query_selector(selector)
                        if element:
                            is_visible = await element.is_visible()
                            if is_visible:
                                detected_type = captcha_type
                                logger.info(
                                    "CAPTCHA detected", type=captcha_type.value, selector=selector
                                )
                                break
                    except Exception:
                        continue
                if detected_type:
                    break

            if not detected_type:
                return True  # No CAPTCHA detected

            # Report CAPTCHA to IP rotation service for weighted proxy selection
            if self._proxy_client and self._current_proxy:
                try:
                    await self._proxy_client.record_captcha(
                        proxy_id=self._current_proxy.id,
                        captcha_type=detected_type.value,
                    )
                    logger.info(
                        "CAPTCHA reported to IP rotation service",
                        proxy_id=self._current_proxy.id,
                        captcha_type=detected_type.value,
                    )
                except Exception as e:
                    logger.debug("Failed to report CAPTCHA to IP rotation service", error=str(e))

            # Try to solve the CAPTCHA
            solver = self._get_captcha_solver()
            result = await solver.solve(detected_type, page=page)

            if result.success:
                logger.info(
                    "CAPTCHA solved successfully",
                    type=detected_type.value,
                    solver=result.solver_used,
                    time_ms=result.time_ms,
                )
                # Wait for page to update after CAPTCHA solve
                await asyncio.sleep(2)
                return True
            else:
                logger.warning("CAPTCHA solve failed", type=detected_type.value, error=result.error)
                return False

        except Exception as e:
            logger.error("Error in CAPTCHA detection/handling", error=str(e))
            return False

    async def _simulate_human_behavior(self, page) -> None:
        """Simulate human-like behavior on a page to avoid detection."""
        try:
            # Random mouse movements
            await HumanBehaviorSimulator.random_mouse_movements(page, count=2)

            # Random scroll
            await HumanBehaviorSimulator.human_scroll(page, "down", 200)
            await asyncio.sleep(HumanBehaviorSimulator.random_delay(500, 1000))

        except Exception as e:
            logger.debug("Human behavior simulation failed", error=str(e))

    async def smart_search(
        self,
        query: str,
        max_results: int = 20,
        use_browser_fallback: bool = True,
        use_rrf: bool = True,
    ) -> list[dict[str, Any]]:
        """
        Smart search with API-first strategy and browser fallback.

        Tries API-based search first to avoid CAPTCHA, then falls back
        to browser-based search with Camoufox if APIs fail.

        Now uses RRF (Reciprocal Rank Fusion) for improved accuracy by:
        1. Analyzing query intent and extracting semantic meaning
        2. Expanding query into multiple search strategies
        3. Merging results from multiple providers and strategies

        Args:
            query: Search query
            max_results: Maximum number of results
            use_browser_fallback: Whether to try browser search if API fails
            use_rrf: Use RRF-based multi-strategy search

        Returns:
            List of search results with url, title, snippet
        """
        results = []

        # Step 1: Try API-based search (no CAPTCHA)
        if use_rrf:
            logger.info("Attempting RRF-based API search", query=query)
            try:
                orchestrator = self._get_rrf_search_orchestrator()
                if orchestrator.providers:
                    rrf_result = await orchestrator.search_news_with_rrf(
                        query=query,
                        max_results_per_strategy=self.settings.search.max_results_per_provider,
                    )

                    if rrf_result.results:
                        logger.info(
                            "RRF API search successful",
                            query=query,
                            results_count=len(rrf_result.results),
                            strategies=rrf_result.strategies_used,
                            query_analysis=rrf_result.query_analysis,
                        )
                        return [
                            {
                                "url": r.url,
                                "title": r.title,
                                "snippet": r.snippet,
                                "source": f"rrf_{r.source_provider}",
                            }
                            for r in rrf_result.results[:max_results]
                        ]
            except Exception as e:
                logger.warning("RRF search failed", error=str(e))

        # Fallback to simple API search
        logger.info("Attempting simple API-based search", query=query)
        api_urls = await self.search_before_crawl(query, max_results, use_rrf=False)

        if api_urls:
            logger.info("API search successful", query=query, results_count=len(api_urls))
            results = [{"url": url, "source": "api"} for url in api_urls]
            return results

        if not use_browser_fallback:
            logger.warning("API search failed and browser fallback disabled")
            return results

        # Step 2: Try browser search with Camoufox (best anti-detection)
        if is_camoufox_available():
            logger.info("Trying Camoufox browser search", query=query)
            camoufox_results = await self._browser_search_with_camoufox(query, max_results)
            if camoufox_results:
                return camoufox_results

        # Step 3: Try browser search with enhanced Playwright stealth
        logger.info("Trying Playwright stealth browser search", query=query)
        playwright_results = await self._browser_search_with_stealth(query, max_results)

        return playwright_results

    async def _browser_search_with_camoufox(
        self,
        query: str,
        max_results: int = 20,
    ) -> list[dict[str, Any]]:
        """
        Perform browser search using Camoufox anti-detect browser.

        Uses DuckDuckGo HTML version which is less likely to show CAPTCHA.
        """
        results = []

        try:
            page = await self._get_camoufox_page()
            if not page:
                return results

            # Use DuckDuckGo HTML version (lighter, less detection)
            search_url = f"https://html.duckduckgo.com/html/?q={query.replace(' ', '+')}"

            await page.goto(search_url, wait_until="domcontentloaded")

            # Wait for Cloudflare if present
            await CamoufoxHelper.wait_for_cloudflare(page, timeout=15)

            # Simulate human behavior
            await asyncio.sleep(1)

            # Extract search results
            result_elements = await page.query_selector_all(".result")

            for element in result_elements[:max_results]:
                try:
                    link = await element.query_selector(".result__a")
                    snippet_el = await element.query_selector(".result__snippet")

                    if link:
                        url = await link.get_attribute("href")
                        title = await link.inner_text()
                        snippet = ""
                        if snippet_el:
                            snippet = await snippet_el.inner_text()

                        if url and title:
                            results.append(
                                {
                                    "url": url,
                                    "title": title.strip(),
                                    "snippet": snippet.strip(),
                                    "source": "camoufox_duckduckgo",
                                }
                            )
                except Exception:
                    continue

            await page.close()

            if results:
                logger.info("Camoufox search successful", query=query, results_count=len(results))

        except Exception as e:
            logger.error("Camoufox search failed", query=query, error=str(e))

        return results

    async def _browser_search_with_stealth(
        self,
        query: str,
        max_results: int = 20,
    ) -> list[dict[str, Any]]:
        """
        Perform browser search using Playwright with stealth patches.

        Tries multiple search engines with different strategies.
        Prioritizes engines that are less likely to show CAPTCHAs.
        """
        results = []

        # Search engines to try (in order of CAPTCHA likelihood - least likely first)
        search_engines = [
            {
                "name": "duckduckgo_html",
                "url": f"https://html.duckduckgo.com/html/?q={query.replace(' ', '+')}",
                "result_selector": ".result",
                "link_selector": ".result__a",
                "snippet_selector": ".result__snippet",
                "wait_selector": ".result",
            },
            {
                "name": "startpage",
                "url": f"https://www.startpage.com/do/search?q={query.replace(' ', '+')}",
                "result_selector": ".w-gl__result",
                "link_selector": "a.w-gl__result-title",
                "snippet_selector": ".w-gl__description",
                "wait_selector": ".w-gl__result",
            },
            {
                "name": "ecosia",
                "url": f"https://www.ecosia.org/search?q={query.replace(' ', '+')}",
                "result_selector": "[data-test-id='mainline-result-web']",
                "link_selector": "a[data-test-id='result-link']",
                "snippet_selector": "[data-test-id='result-snippet']",
                "wait_selector": "[data-test-id='mainline-result-web']",
            },
            {
                "name": "mojeek",  # Privacy-focused, rarely uses CAPTCHA
                "url": f"https://www.mojeek.com/search?q={query.replace(' ', '+')}",
                "result_selector": ".results-standard li",
                "link_selector": "a.title",
                "snippet_selector": ".s",
                "wait_selector": ".results-standard",
            },
        ]

        browser_session = None
        context = None
        page = None

        try:
            browser_session = await self._get_browser_session()

            # Get the underlying playwright browser to create isolated context
            if hasattr(browser_session, "_browser") and browser_session._browser:
                browser = browser_session._browser

                # Create isolated context with stealth args
                context = await browser.new_context(
                    user_agent=self._stealth_config.get_random_user_agent()
                    if hasattr(self._stealth_config, "get_random_user_agent")
                    else None,
                    locale="en-US",
                    timezone_id="America/New_York",
                )
                page = await context.new_page()

                # Apply stealth patches
                await self._apply_page_stealth(page)
            else:
                logger.warning("Could not access underlying browser for stealth search")
                return results

            for engine in search_engines:
                try:
                    logger.info("Trying search engine", engine=engine["name"], query=query)

                    # Navigate to search engine
                    await page.goto(engine["url"], wait_until="domcontentloaded", timeout=15000)

                    # Wait for results to load
                    try:
                        await page.wait_for_selector(
                            engine["wait_selector"], timeout=10000, state="visible"
                        )
                    except Exception:
                        # Check if we hit a CAPTCHA
                        captcha_type = await _quick_captcha_check(page)
                        if captcha_type:
                            logger.warning(
                                "CAPTCHA detected on search engine",
                                engine=engine["name"],
                                captcha_type=captcha_type,
                            )
                            # Try to solve it
                            solved = await self._detect_and_handle_captcha(page)
                            if not solved:
                                continue  # Try next engine
                            # Wait again for results after solving
                            try:
                                await page.wait_for_selector(engine["wait_selector"], timeout=5000)
                            except Exception:
                                continue
                        else:
                            logger.debug(
                                "Results not found, trying next engine", engine=engine["name"]
                            )
                            continue

                    # Simulate human behavior
                    if self._stealth_config.enable_human_simulation:
                        await HumanBehaviorSimulator.random_mouse_movements(page, count=1)
                        await asyncio.sleep(0.3)

                    # Extract search results
                    result_elements = await page.query_selector_all(engine["result_selector"])

                    for element in result_elements[:max_results]:
                        try:
                            link = await element.query_selector(engine["link_selector"])
                            snippet_el = await element.query_selector(engine["snippet_selector"])

                            if link:
                                url = await link.get_attribute("href")
                                title = await link.inner_text()
                                snippet = ""
                                if snippet_el:
                                    snippet = await snippet_el.inner_text()

                                # Clean up URL (some engines use redirect URLs)
                                if url and title:
                                    # Skip ad/sponsored results
                                    if "ad" in url.lower() or "sponsor" in title.lower():
                                        continue

                                    results.append(
                                        {
                                            "url": url,
                                            "title": title.strip(),
                                            "snippet": snippet.strip() if snippet else "",
                                            "source": f"stealth_{engine['name']}",
                                        }
                                    )
                        except Exception as e:
                            logger.debug("Failed to extract result", error=str(e))
                            continue

                    if results:
                        logger.info(
                            "Stealth search successful",
                            engine=engine["name"],
                            query=query,
                            results_count=len(results),
                        )
                        break  # Got results, stop trying other engines

                except Exception as e:
                    logger.debug("Search engine failed", engine=engine["name"], error=str(e))
                    continue

        except Exception as e:
            logger.error("Stealth browser search failed", query=query, error=str(e))
        finally:
            # Clean up
            if page:
                try:
                    await page.close()
                except Exception:
                    pass
            if context:
                try:
                    await context.close()
                except Exception:
                    pass

        return results

    async def handle_captcha_and_retry(
        self,
        page,
        action_func,
        max_retries: int = 3,
        switch_backend_on_failure: bool = True,
        rotate_proxy_on_failure: bool = True,
    ) -> Any:
        """
        Execute an action with CAPTCHA detection and retry logic.

        If CAPTCHA is detected and cannot be solved, optionally switches
        to a different browser backend or rotates to a new proxy and retries.

        Args:
            page: Current page object
            action_func: Async function to execute
            max_retries: Maximum retry attempts
            switch_backend_on_failure: Try different browser if CAPTCHA persists
            rotate_proxy_on_failure: Get a new proxy from rotation service on failure

        Returns:
            Result of action_func or None if all retries fail
        """
        for attempt in range(max_retries):
            try:
                # Check for CAPTCHA before action
                captcha_handled = await self._detect_and_handle_captcha(page)

                if not captcha_handled:
                    logger.warning(
                        "CAPTCHA detected but not solved",
                        attempt=attempt + 1,
                        max_retries=max_retries,
                    )

                    # Try rotating to a new proxy first
                    if rotate_proxy_on_failure and self._proxy_client:
                        try:
                            new_proxy = await self._proxy_client.get_next_proxy()
                            if new_proxy and (
                                not self._current_proxy or new_proxy.id != self._current_proxy.id
                            ):
                                self._current_proxy = new_proxy
                                logger.info(
                                    "Rotating to new proxy after CAPTCHA failure",
                                    proxy_id=new_proxy.id,
                                    proxy_address=new_proxy.address,
                                    attempt=attempt + 1,
                                )
                                # Recreate browser session with new proxy
                                if self._browser_session:
                                    await self._browser_session.stop()
                                    self._browser_session = None
                                # Get new session with new proxy
                                browser_session = await self._get_browser_session()
                                if (
                                    hasattr(browser_session, "_context")
                                    and browser_session._context
                                ):
                                    pages = browser_session._context.pages
                                    if pages:
                                        page = pages[-1]
                                        continue
                        except Exception as e:
                            logger.debug("Failed to rotate proxy", error=str(e))

                    # If Camoufox available and we're not already using it, switch
                    if (
                        switch_backend_on_failure
                        and not self._use_camoufox
                        and is_camoufox_available()
                    ):
                        logger.info("Switching to Camoufox browser for better CAPTCHA bypass")
                        self._use_camoufox = True

                        # Get new page from Camoufox
                        new_page = await self._get_camoufox_page()
                        if new_page:
                            page = new_page
                            continue

                    await asyncio.sleep(2**attempt)  # Exponential backoff
                    continue

                # Execute the action
                result = await action_func(page)

                # Check for CAPTCHA after action (might have triggered one)
                await self._detect_and_handle_captcha(page)

                return result

            except Exception as e:
                logger.error("Action failed", attempt=attempt + 1, error=str(e))
                await asyncio.sleep(2**attempt)

        logger.error("All retry attempts failed")
        return None

    async def execute_task(self, task: BrowserTaskMessage) -> list[CrawlResultMessage]:
        """
        Execute a browser crawling task.

        Args:
            task: The browser task message from Kafka

        Returns:
            List of extracted crawl results
        """
        # Parse policy
        try:
            policy = CrawlPolicy(task.policy.lower()) if task.policy else CrawlPolicy.NEWS_ONLY
        except ValueError:
            policy = CrawlPolicy.NEWS_ONLY

        # Create session with metadata from task (for AutoCrawl callback)
        session = CrawlSession(
            job_id=task.job_id,
            source_id=task.source_id,
            seed_url=task.seed_url,
            max_depth=task.max_depth or 2,
            max_pages=task.max_pages or 10,
            budget_seconds=min(
                task.budget_seconds or self.settings.browser.default_timeout_seconds,
                self.settings.browser.max_timeout_seconds,
            ),
            policy=policy,
            focus_keywords=task.get_focus_keywords_list(),
            excluded_domains=task.get_excluded_domains_list(),
            metadata=task.metadata,  # Pass metadata for AutoCrawl callback
        )

        # Track proxy used for this task
        task_proxy_id = None

        logger.info(
            "Starting crawl session",
            job_id=session.job_id,
            source_id=session.source_id,
            seed_url=session.seed_url,
            policy=policy.value,
            max_pages=session.max_pages,
            budget_seconds=session.budget_seconds,
        )

        session.start_time = datetime.now()

        try:
            # Generate the system prompt based on policy
            system_prompt = get_policy_prompt(
                policy=policy,
                focus_keywords=session.focus_keywords,
                custom_prompt=task.custom_prompt,
                excluded_domains=session.excluded_domains,
            )

            # Create the task prompt
            task_prompt = self._build_task_prompt(session)

            # Get browser session and create agent (this will also get proxy)
            # Use force_new=True to ensure a fresh browser for each task (avoids CDP issues)
            browser_session = await self._get_browser_session(force_new=True)

            # Track the proxy being used for this task
            if self._current_proxy:
                task_proxy_id = self._current_proxy.id
                logger.info(
                    "Task using proxy",
                    job_id=session.job_id,
                    proxy_id=task_proxy_id,
                )

            # Create CAPTCHA detection and stealth hooks for the agent
            captcha_hook = await create_captcha_detection_hook(
                crawler_agent=self,
                on_captcha_detected=lambda ct: logger.info(
                    "CAPTCHA detected during crawl",
                    captcha_type=ct,
                    job_id=session.job_id,
                ),
            )
            stealth_hook = await create_stealth_hook(self)

            agent = Agent(
                task=task_prompt,
                llm=self._llm,
                browser_session=browser_session,
                max_actions_per_step=5,
                extend_system_message=system_prompt,  # Add crawl policy to system prompt
            )

            # Run the agent with timeout and CAPTCHA/stealth hooks
            try:
                result = await asyncio.wait_for(
                    agent.run(
                        max_steps=session.max_pages * 3,  # Allow multiple steps per page
                        on_step_start=captcha_hook,  # CAPTCHA detection before each step
                        on_step_end=stealth_hook,  # Re-apply stealth after navigation
                    ),
                    timeout=session.budget_seconds,
                )

                # Parse the agent's output to extract articles
                session.extracted_articles = self._parse_agent_output(
                    result, session.job_id, session.source_id
                )

            except asyncio.TimeoutError:
                logger.warning(
                    "Crawl session timed out",
                    job_id=session.job_id,
                    elapsed_seconds=session.budget_seconds,
                )

        except Exception as e:
            session.error = str(e)
            logger.error(
                "Crawl session failed",
                job_id=session.job_id,
                error=str(e),
                exc_info=True,
            )

        finally:
            session.end_time = datetime.now()
            elapsed = (session.end_time - session.start_time).total_seconds()

            # Clean up browser session to prevent CDP issues on next task
            # Use kill() instead of stop() for more aggressive cleanup
            if self._browser_session:
                try:
                    await self._browser_session.kill()
                    logger.debug("Browser session killed successfully")
                except Exception as e:
                    logger.debug("Error killing browser session", error=str(e))
                self._browser_session = None

            # Small delay to ensure browser process fully terminates before next task
            await asyncio.sleep(0.5)

            # Record proxy usage result to IP rotation service
            if self._proxy_client and task_proxy_id:
                try:
                    if session.error:
                        await self._proxy_client.record_failure(
                            proxy_id=task_proxy_id,
                            reason=session.error[:200],  # Truncate error message
                        )
                    else:
                        await self._proxy_client.record_success(
                            proxy_id=task_proxy_id,
                            latency_ms=int(elapsed * 1000),
                        )
                    logger.debug(
                        "Proxy usage recorded",
                        proxy_id=task_proxy_id,
                        success=session.error is None,
                    )
                except Exception as e:
                    logger.debug("Failed to record proxy usage", error=str(e))

            # Send callback if configured
            if task.callback_url:
                await self._send_callback(task, session)

        logger.info(
            "Crawl session completed",
            job_id=session.job_id,
            articles_extracted=len(session.extracted_articles),
            elapsed_seconds=elapsed,
            error=session.error,
            proxy_id=task_proxy_id,
        )

        return session.extracted_articles

    def _build_task_prompt(self, session: CrawlSession) -> str:
        """Build the task prompt for the browser-use agent."""
        prompt_parts = [
            f"Navigate to {session.seed_url} and extract article content.",
            f"",
            f"## Constraints:",
            f"- Maximum pages to visit: {session.max_pages}",
            f"- Maximum link depth: {session.max_depth}",
            f"- Time budget: {session.budget_seconds} seconds",
            f"",
            f"## Output Format:",
            f"For each article you extract, output in this exact format:",
            f"---ARTICLE_START---",
            f"URL: [the page URL]",
            f"TITLE: [the article title]",
            f"PUBLISHED_AT: [publication date in ISO format, or 'unknown']",
            f"CONTENT: [the full article text]",
            f"---ARTICLE_END---",
            f"",
            f"Extract as many relevant articles as possible within the constraints.",
        ]

        if session.focus_keywords:
            prompt_parts.append(f"Focus on articles about: {', '.join(session.focus_keywords)}")

        return "\n".join(prompt_parts)

    def _is_invalid_agent_output(self, text: str) -> bool:
        """Check if text contains raw Python object representations that shouldn't be used as article content."""
        invalid_patterns = [
            "ActionResult(",
            "AgentHistoryList(",
            "AgentHistory(",
            "include_extracted_content_only_once=",
            "include_in_memory=",
            "metadata=None",
            "is_done=",
            "extracted_content=",
        ]
        return any(pattern in text for pattern in invalid_patterns)

    def _parse_agent_output(
        self, result: Any, job_id: int, source_id: int
    ) -> list[CrawlResultMessage]:
        """Parse the agent's output to extract article data."""
        articles = []

        # Get the final output from the agent
        output_text = ""
        if hasattr(result, "final_result"):
            try:
                final_result = result.final_result
                value = final_result() if callable(final_result) else final_result
                if isinstance(value, str) and value.strip():
                    # Validate that this is actual content, not raw Python objects
                    if not self._is_invalid_agent_output(value):
                        output_text = value
                    else:
                        logger.warning(
                            "final_result contains raw Python object representation, skipping",
                            job_id=job_id,
                        )
            except Exception:
                output_text = ""

        if not output_text and hasattr(result, "history") and result.history:
            # Get the last message content
            for item in reversed(result.history):
                if hasattr(item, "result") and item.result:
                    for r in reversed(item.result):
                        extracted_content = getattr(r, "extracted_content", None)
                        if isinstance(extracted_content, str) and extracted_content.strip():
                            # Validate extracted content is not raw debug output
                            if not self._is_invalid_agent_output(extracted_content):
                                output_text = extracted_content
                                break
                            else:
                                logger.debug(
                                    "Skipping invalid extracted_content",
                                    job_id=job_id,
                                    content_preview=extracted_content[:100],
                                )
                    if output_text:
                        break

        if not output_text:
            logger.warning("No valid output from agent", job_id=job_id)
            return articles

        # Parse articles from the output
        article_pattern = r"---ARTICLE_START---(.+?)---ARTICLE_END---"
        matches = re.findall(article_pattern, output_text, re.DOTALL)

        for match in matches:
            try:
                article = self._parse_article_block(match, job_id, source_id)
                if article:
                    articles.append(article)
            except Exception as e:
                logger.warning("Failed to parse article block", error=str(e))

        # If no structured output, try to extract from unstructured text
        if not articles:
            articles = self._extract_from_unstructured(output_text, job_id, source_id)

        return articles

    def _parse_article_block(
        self, block: str, job_id: int, source_id: int
    ) -> CrawlResultMessage | None:
        """Parse a single article block from agent output."""
        lines = block.strip().split("\n")
        data: dict[str, str] = {}

        current_key = None
        current_value = []

        for line in lines:
            if line.startswith("URL:"):
                if current_key:
                    data[current_key] = "\n".join(current_value).strip()
                current_key = "url"
                current_value = [line[4:].strip()]
            elif line.startswith("TITLE:"):
                if current_key:
                    data[current_key] = "\n".join(current_value).strip()
                current_key = "title"
                current_value = [line[6:].strip()]
            elif line.startswith("PUBLISHED_AT:"):
                if current_key:
                    data[current_key] = "\n".join(current_value).strip()
                current_key = "published_at"
                current_value = [line[13:].strip()]
            elif line.startswith("CONTENT:"):
                if current_key:
                    data[current_key] = "\n".join(current_value).strip()
                current_key = "content"
                current_value = [line[8:].strip()]
            elif current_key:
                current_value.append(line)

        # Don't forget the last field
        if current_key:
            data[current_key] = "\n".join(current_value).strip()

        # Validate required fields
        if not data.get("url") or not data.get("title") or not data.get("content"):
            return None

        # Handle "unknown" published_at
        published_at = data.get("published_at")
        if published_at and published_at.lower() == "unknown":
            published_at = None

        return CrawlResultMessage(
            job_id=job_id,
            source_id=source_id,
            url=data["url"],
            title=data["title"],
            content=data["content"],
            published_at=published_at,
            metadata_json=json.dumps({"source": "browser-agent"}),
        )

    def _extract_title_from_content(self, content: str) -> str | None:
        """Extract a meaningful title from content, or return None if not possible."""
        # Skip content that contains raw Python object representations
        if self._is_invalid_agent_output(content):
            return None

        # Try to find a headline-like first line
        lines = content.strip().split("\n")
        for line in lines[:3]:  # Check first 3 lines
            line = line.strip()
            # Skip empty lines and very short lines
            if len(line) < 10:
                continue
            # Skip lines that look like metadata or code
            if any(char in line for char in ["=", "(", ")", "{", "}", "[", "]"]):
                continue
            # Found a good candidate for title
            if len(line) <= 100:
                return line
            return line[:97] + "..."

        # Fallback: use first 100 chars if content looks valid
        clean_content = content.strip()
        if len(clean_content) >= 20:
            return clean_content[:97] + "..."
        return None

    def _extract_from_unstructured(
        self, text: str, job_id: int, source_id: int
    ) -> list[CrawlResultMessage]:
        """Try to extract articles from unstructured agent output."""
        # This is a fallback for when the agent doesn't follow the exact format
        # Look for URL patterns and try to associate content
        articles = []

        # First, validate the entire text is not invalid output
        if self._is_invalid_agent_output(text):
            logger.warning(
                "Skipping unstructured extraction - text contains invalid agent output",
                job_id=job_id,
                text_preview=text[:200] if len(text) > 200 else text,
            )
            return articles

        # Simple heuristic: split by URL patterns
        url_pattern = r"(https?://[^\s]+)"
        parts = re.split(url_pattern, text)

        current_url = None
        current_content = []

        for part in parts:
            if re.match(url_pattern, part):
                # Save previous article if exists
                if current_url and current_content:
                    content = " ".join(current_content).strip()
                    if len(content) > 100:  # Minimum content length
                        # Extract a valid title from content
                        title = self._extract_title_from_content(content)
                        if title:
                            articles.append(
                                CrawlResultMessage(
                                    job_id=job_id,
                                    source_id=source_id,
                                    url=current_url,
                                    title=title,
                                    content=content,
                                    published_at=None,
                                    metadata_json=json.dumps(
                                        {"source": "browser-agent", "extraction": "unstructured"}
                                    ),
                                )
                            )
                        else:
                            logger.debug(
                                "Skipping article - could not extract valid title",
                                job_id=job_id,
                                url=current_url,
                            )
                current_url = part
                current_content = []
            else:
                current_content.append(part)

        return articles

    async def _send_callback(self, task: BrowserTaskMessage, session: CrawlSession) -> None:
        """Send completion callback to the configured URL.

        The callback payload matches the schema expected by
        AutoCrawlController.handleCrawlerCallback() in data-collection-service:
        - targetId: CrawlTarget ID from metadata
        - urlHash: URL hash from metadata
        - success: boolean indicating success/failure
        - collectedDataId: (optional) ID of saved CollectedData
        - error: error message if failed
        """
        if not task.callback_url:
            return

        # Extract AutoCrawl metadata if available
        target_id = None
        url_hash = None
        if session.metadata:
            target_id = session.metadata.get("targetId")
            url_hash = session.metadata.get("urlHash")

        # Build callback payload matching Java's CrawlerCallbackRequest
        callback_data = {
            "targetId": int(target_id) if target_id else session.job_id,
            "urlHash": url_hash,
            "success": session.error is None,
            "collectedDataId": None,  # Will be set by CrawlResultConsumer
            "error": session.error,
            # Include additional stats for debugging/monitoring
            "articlesExtracted": len(session.extracted_articles),
            "pagesVisited": len(session.visited_urls),
            "elapsedSeconds": (
                (session.end_time - session.start_time).total_seconds()
                if session.end_time and session.start_time
                else 0
            ),
        }

        headers = {"Content-Type": "application/json"}
        if task.callback_token:
            headers["Authorization"] = f"Bearer {task.callback_token}"

        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    task.callback_url,
                    json=callback_data,
                    headers=headers,
                    timeout=30.0,
                )
                logger.info(
                    "Callback sent",
                    job_id=session.job_id,
                    target_id=target_id,
                    url_hash=url_hash[:16] if url_hash else None,
                    success=callback_data["success"],
                    callback_url=task.callback_url,
                    status_code=response.status_code,
                )
        except Exception as e:
            logger.error(
                "Failed to send callback",
                job_id=session.job_id,
                callback_url=task.callback_url,
                error=str(e),
            )

```

---

## backend/autonomous-crawler-service/src/crawler/policies.py

```py
"""Crawling policies and prompt generation."""

from enum import Enum


class CrawlPolicy(Enum):
    """Exploration policies for autonomous crawling."""

    # ê¸°ë³¸ ì •ì±…
    FOCUSED_TOPIC = "focused_topic"
    DOMAIN_WIDE = "domain_wide"
    NEWS_ONLY = "news_only"
    CROSS_DOMAIN = "cross_domain"
    SINGLE_PAGE = "single_page"

    # ë‰´ìŠ¤ íŠ¹í™” ì •ì±… (ì‹ ê·œ)
    NEWS_BREAKING = "news_breaking"  # ì†ë³´/ê¸´ê¸‰ ë‰´ìŠ¤ ìš°ì„ 
    NEWS_ARCHIVE = "news_archive"  # ê³¼ê±° ê¸°ì‚¬ ì•„ì¹´ì´ë¸Œ ìˆ˜ì§‘
    NEWS_OPINION = "news_opinion"  # ì˜¤í”¼ë‹ˆì–¸/ì¹¼ëŸ¼ ìˆ˜ì§‘
    NEWS_LOCAL = "news_local"  # ì§€ì—­ ë‰´ìŠ¤ íŠ¹í™”


# Base system prompt for all policies
BASE_SYSTEM_PROMPT = """You are an autonomous web crawler agent specialized in extracting news and article content.
Your goal is to navigate websites, identify valuable content, and extract structured information.

## Core Behaviors:
1. Navigate to the seed URL first
2. Identify and extract article content (title, body text, publication date, author)
3. Find relevant links to other articles/pages based on the policy
4. Avoid non-content pages (login, signup, ads, social media shares)
5. Respect the page budget and depth limits

## Content Extraction Guidelines:
- Extract the main article title (usually h1 or article header)
- Extract the full article body text, preserving paragraphs
- Look for publication date in meta tags, article headers, or bylines
- Skip navigation menus, footers, sidebars, and advertisements
- If a page is not an article, briefly note what type of page it is and move on

## Navigation Rules:
- Prioritize links that appear to be article links (news headlines, blog posts)
- Avoid external links unless specifically allowed by the policy
- Skip links to media files (images, PDFs, videos)
- Skip pagination if you've already seen the content pattern

## Output Format:
For each article extracted, use this format:
---ARTICLE_START---
URL: [article URL]
TITLE: [headline]
AUTHOR: [author name if found]
PUBLISHED_AT: [publication date in ISO format if found]
CATEGORY: [category/section if identified]
CONTENT: [full article text]
---ARTICLE_END---
"""

POLICY_PROMPTS = {
    CrawlPolicy.FOCUSED_TOPIC: """
## Policy: FOCUSED_TOPIC
Focus exclusively on content related to the specified keywords/topics.

### Specific Instructions:
- Only follow links that appear related to the focus keywords: {focus_keywords}
- Prioritize articles with titles containing the keywords
- Skip unrelated content even if it looks interesting
- Extract articles that discuss or mention the focus topics
- Look for related terms and synonyms of the focus keywords
""",
    CrawlPolicy.DOMAIN_WIDE: """
## Policy: DOMAIN_WIDE
Explore the entire domain broadly to discover all available content.

### Specific Instructions:
- Follow links to all content sections of the website
- Prioritize category/section pages that lead to more articles
- Create a broad coverage of the site's content
- Balance between depth and breadth of exploration
- Identify and visit major site sections (news, blog, articles, etc.)
""",
    CrawlPolicy.NEWS_ONLY: """
## Policy: NEWS_ONLY
Focus strictly on news articles and current events content.

### Specific Instructions:
- Only extract content that appears to be news articles
- Look for date indicators showing recent publication
- Prioritize breaking news, current events, and timely content
- Skip evergreen content, guides, and static pages
- Follow links from news sections, headlines, and latest articles
- Identify news patterns: bylines, datelines, news categories
""",
    CrawlPolicy.CROSS_DOMAIN: """
## Policy: CROSS_DOMAIN
Follow links across different domains to discover related content.

### Specific Instructions:
- You may follow external links to other websites
- Prioritize links that appear to lead to related news sources
- Respect the excluded domains list if provided
- Track which domains you've visited to ensure diversity
- Extract content from each domain you visit
- Be cautious of redirect chains and avoid loops
""",
    CrawlPolicy.SINGLE_PAGE: """
## Policy: SINGLE_PAGE
Extract content only from the seed URL without following any links.

### Specific Instructions:
- Do NOT navigate to any other pages
- Focus entirely on extracting content from the current page
- Extract all article content, metadata, and structured data
- Identify any embedded content or data on the page
- This is a single-page extraction task only
""",
    CrawlPolicy.NEWS_BREAKING: """
## Policy: NEWS_BREAKING
Priority collection of breaking news and urgent updates.

### Specific Instructions:
- Look for visual indicators of breaking news:
  - Labels: "ì†ë³´", "Breaking", "ê¸´ê¸‰", "ë‹¨ë…", "Flash", "Urgent"
  - Red or highlighted text, special formatting
  - Pinned or featured articles at the top
- Prioritize articles published in the last few hours
- Extract the FULL content of breaking news articles
- Note the exact publication time if available
- Skip older news and evergreen content
- Look for live update sections or real-time feeds
- Mark each article as breaking: true in metadata
""",
    CrawlPolicy.NEWS_ARCHIVE: """
## Policy: NEWS_ARCHIVE
Historical article collection from archives.

### Specific Instructions:
- Navigate through pagination and archive pages
- Look for "ì´ì „ ê¸°ì‚¬", "ë”ë³´ê¸°", "Load More" buttons
- Accept older publication dates (weeks, months, or years old)
- Follow links to category archives and date-based listings
- Collect articles systematically by date or category
- Note the original publication date accurately
- Skip duplicate or redirected articles
- Be patient with slower-loading archive pages
""",
    CrawlPolicy.NEWS_OPINION: """
## Policy: NEWS_OPINION
Focus on opinion pieces, editorials, and columns.

### Specific Instructions:
- Look for opinion/editorial sections:
  - "ì˜¤í”¼ë‹ˆì–¸", "ì¹¼ëŸ¼", "ì‚¬ì„¤", "Opinion", "Editorial", "Column"
  - Author-focused pages with byline photos
- Identify opinion content markers:
  - Personal pronouns and subjective language
  - Author bio sections
  - Regular column series
- Extract author information prominently
- Mark content as opinion: true in metadata
- Note if the author is a regular columnist
- Skip straight news reporting
""",
    CrawlPolicy.NEWS_LOCAL: """
## Policy: NEWS_LOCAL
Local and regional news collection.

### Specific Instructions:
- Focus on local news sections:
  - "ì§€ì—­", "Local", geographic region names
  - City or province-specific categories
- Look for location markers in articles:
  - City names, district names
  - Local government references
  - Regional business news
- Prioritize community-focused stories
- Note the geographic focus of each article
- Skip national or international news
- Include local events and announcements
""",
}


def get_policy_prompt(
    policy: CrawlPolicy | str,
    focus_keywords: list[str] | None = None,
    custom_prompt: str | None = None,
    excluded_domains: list[str] | None = None,
) -> str:
    """
    Generate the full system prompt for the crawler agent.

    Args:
        policy: The crawling policy to use
        focus_keywords: Keywords for FOCUSED_TOPIC policy
        custom_prompt: Optional custom instructions to append
        excluded_domains: Domains to exclude from crawling

    Returns:
        Complete system prompt for the browser-use agent
    """
    # Convert string to enum if needed
    if isinstance(policy, str):
        try:
            policy = CrawlPolicy(policy.lower())
        except ValueError:
            policy = CrawlPolicy.NEWS_ONLY  # Default fallback

    # Build the prompt
    prompt_parts = [BASE_SYSTEM_PROMPT]

    # Add policy-specific instructions
    policy_prompt = POLICY_PROMPTS.get(policy, POLICY_PROMPTS[CrawlPolicy.NEWS_ONLY])

    # Format with focus keywords if applicable
    if policy == CrawlPolicy.FOCUSED_TOPIC and focus_keywords:
        policy_prompt = policy_prompt.format(focus_keywords=", ".join(focus_keywords))
    else:
        policy_prompt = policy_prompt.replace("{focus_keywords}", "")

    prompt_parts.append(policy_prompt)

    # Add excluded domains if any
    if excluded_domains:
        prompt_parts.append(f"""
## Excluded Domains:
Do NOT visit or follow links to these domains:
{chr(10).join(f"- {d}" for d in excluded_domains)}
""")

    # Add custom instructions if provided
    if custom_prompt:
        prompt_parts.append(f"""
## Custom Instructions:
{custom_prompt}
""")

    return "\n".join(prompt_parts)


def get_extraction_prompt(url: str) -> str:
    """
    Generate the task prompt for extracting content from a specific page.

    Args:
        url: The URL to extract content from

    Returns:
        Task prompt for content extraction
    """
    return f"""
Extract the article content from this page: {url}

Return the extracted content in the following format:
1. TITLE: The main article headline
2. CONTENT: The full article text (preserve paragraph breaks)
3. PUBLISHED_AT: The publication date if found (ISO format preferred)
4. AUTHOR: The author name if found
5. SUMMARY: A brief 2-3 sentence summary of the article

If this is not an article page, indicate what type of page it is.
"""


def get_news_list_extraction_prompt(url: str, max_articles: int = 20) -> str:
    """
    Generate prompt for extracting article list from a news section page.

    Args:
        url: The news section/category URL
        max_articles: Maximum number of articles to extract

    Returns:
        Task prompt for list extraction
    """
    return f"""
Extract the list of news articles from this page: {url}

Find up to {max_articles} news articles and return each in this format:
---ARTICLE_LINK---
TITLE: [article headline]
URL: [full article URL]
SUMMARY: [brief description or lead text if visible]
PUBLISHED_AT: [date if shown]
THUMBNAIL: [image URL if present]
---END_LINK---

Focus on:
- Main content area article links
- Skip navigation, ads, and sidebar widgets
- Include only actual news article links
- Preserve the order as shown on the page
"""


def get_rss_discovery_prompt(url: str) -> str:
    """
    Generate prompt for discovering RSS/Atom feeds.

    Args:
        url: The website URL to search for feeds

    Returns:
        Task prompt for RSS discovery
    """
    return f"""
Find all RSS and Atom feeds available on this website: {url}

Search in these locations:
1. HTML head section:
   - <link rel="alternate" type="application/rss+xml" ...>
   - <link rel="alternate" type="application/atom+xml" ...>
2. Common feed paths:
   - /feed, /rss, /atom, /feeds
   - /rss.xml, /feed.xml, /atom.xml
   - /news/rss, /blog/feed
3. Page footer or sidebar RSS icons/links
4. sitemap.xml references

Return found feeds in JSON format:
{{
    "feeds": [
        {{
            "url": "feed URL",
            "type": "rss|atom",
            "title": "feed title if known",
            "category": "category if specified"
        }}
    ],
    "sitemap_url": "sitemap URL if found",
    "robots_txt_checked": true|false
}}
"""

```

---

## backend/autonomous-crawler-service/src/kafka/__init__.py

```py
"""Kafka module for autonomous-crawler-service."""

from .consumer import BrowserTaskConsumer
from .producer import CrawlResultProducer
from .messages import BrowserTaskMessage, CrawlResultMessage

__all__ = [
    "BrowserTaskConsumer",
    "CrawlResultProducer",
    "BrowserTaskMessage",
    "CrawlResultMessage",
]

```

---

## backend/autonomous-crawler-service/src/kafka/consumer.py

```py
"""Kafka consumer for browser task messages."""

import asyncio
import json
from typing import AsyncGenerator, Callable, Awaitable

import structlog
from aiokafka import AIOKafkaConsumer
from aiokafka.errors import KafkaError

from src.config import Settings
from src.kafka.messages import BrowserTaskMessage

logger = structlog.get_logger(__name__)


class BrowserTaskConsumer:
    """Async Kafka consumer for browser task messages."""

    def __init__(self, settings: Settings) -> None:
        self.settings = settings
        self._consumer: AIOKafkaConsumer | None = None
        self._running = False

    async def start(self) -> None:
        """Start the Kafka consumer with retry logic."""
        kafka_settings = self.settings.kafka

        self._consumer = AIOKafkaConsumer(
            kafka_settings.browser_task_topic,
            bootstrap_servers=kafka_settings.bootstrap_servers,
            group_id=kafka_settings.consumer_group_id,
            auto_offset_reset=kafka_settings.auto_offset_reset,
            enable_auto_commit=kafka_settings.enable_auto_commit,
            max_poll_records=kafka_settings.max_poll_records,
            session_timeout_ms=kafka_settings.session_timeout_ms,
            heartbeat_interval_ms=kafka_settings.heartbeat_interval_ms,
            value_deserializer=lambda m: json.loads(m.decode("utf-8")),
        )

        # Retry connection with exponential backoff
        max_retries = 10
        retry_delay = 2
        for attempt in range(max_retries):
            try:
                await self._consumer.start()
                self._running = True
                logger.info(
                    "Kafka consumer started",
                    topic=kafka_settings.browser_task_topic,
                    group_id=kafka_settings.consumer_group_id,
                )
                return
            except Exception as e:
                if attempt < max_retries - 1:
                    logger.warning(
                        "Failed to connect to Kafka, retrying...",
                        attempt=attempt + 1,
                        max_retries=max_retries,
                        retry_delay=retry_delay,
                        error=str(e),
                    )
                    await asyncio.sleep(retry_delay)
                    retry_delay = min(retry_delay * 2, 30)  # Max 30 seconds
                else:
                    logger.error(
                        "Failed to connect to Kafka after all retries",
                        error=str(e),
                    )
                    raise

    async def stop(self) -> None:
        """Stop the Kafka consumer."""
        self._running = False
        if self._consumer:
            await self._consumer.stop()
            logger.info("Kafka consumer stopped")

    async def consume(self) -> AsyncGenerator[BrowserTaskMessage, None]:
        """
        Consume messages from Kafka topic.

        Yields BrowserTaskMessage objects. Caller is responsible for
        committing offsets after successful processing.
        """
        if not self._consumer:
            raise RuntimeError("Consumer not started. Call start() first.")

        while self._running:
            try:
                # Get batch of messages (max_poll_records=1 means one at a time)
                result = await self._consumer.getmany(timeout_ms=1000)

                for topic_partition, messages in result.items():
                    for msg in messages:
                        try:
                            task = BrowserTaskMessage.model_validate(msg.value)
                            logger.info(
                                "Received browser task",
                                job_id=task.job_id,
                                source_id=task.source_id,
                                seed_url=task.seed_url,
                                offset=msg.offset,
                            )
                            yield task

                            # Commit after successful processing
                            await self._consumer.commit()
                            logger.debug(
                                "Committed offset",
                                offset=msg.offset,
                                partition=topic_partition.partition,
                            )

                        except Exception as e:
                            logger.error(
                                "Failed to parse browser task message",
                                error=str(e),
                                raw_value=msg.value,
                            )
                            # Still commit to avoid infinite retry on malformed messages
                            await self._consumer.commit()

            except KafkaError as e:
                logger.error("Kafka consumer error", error=str(e))
                await asyncio.sleep(1)  # Back off on errors

    async def run_with_handler(
        self,
        handler: Callable[[BrowserTaskMessage], Awaitable[None]],
    ) -> None:
        """
        Run consumer with a message handler callback.

        Args:
            handler: Async function to process each message
        """
        async for task in self.consume():
            try:
                await handler(task)
            except Exception as e:
                logger.error(
                    "Handler failed for task",
                    job_id=task.job_id,
                    error=str(e),
                    exc_info=True,
                )
                # Continue processing next messages

```

---

## backend/autonomous-crawler-service/src/kafka/messages.py

```py
"""Kafka message schemas matching Java DTOs."""

import json
from datetime import datetime
from typing import Any, Optional, Union

from pydantic import BaseModel, Field, field_validator


def parse_java_datetime(value: Any) -> datetime | None:
    """
    Parse datetime from various formats:
    - ISO-8601 string: "2025-12-17T11:29:39"
    - Java LocalDateTime array: [2025, 12, 17, 11, 29, 39, 532902301]
    - Python datetime object
    - None
    """
    if value is None:
        return None
    if isinstance(value, datetime):
        return value
    if isinstance(value, str):
        # ISO-8601 string format
        try:
            # Try with microseconds
            return datetime.fromisoformat(value.replace("Z", "+00:00"))
        except ValueError:
            # Try without timezone
            return datetime.fromisoformat(value)
    if isinstance(value, (list, tuple)) and len(value) >= 6:
        # Java LocalDateTime array format: [year, month, day, hour, minute, second, nano?]
        year, month, day, hour, minute, second = value[:6]
        microsecond = value[6] // 1000 if len(value) > 6 else 0  # nano to micro
        return datetime(year, month, day, hour, minute, second, microsecond)
    raise ValueError(f"Cannot parse datetime from: {value} (type: {type(value).__name__})")


class BrowserTaskMessage(BaseModel):
    """
    Kafka message for browser-based autonomous crawling tasks.
    Matches: com.newsinsight.collector.dto.BrowserTaskMessage
    """

    job_id: int = Field(..., alias="jobId", description="Unique job ID for tracking")
    source_id: int = Field(..., alias="sourceId", description="Data source ID")
    source_name: str | None = Field(
        default=None, alias="sourceName", description="Source name for logging/display"
    )
    seed_url: str = Field(..., alias="seedUrl", description="Seed URL to start exploration from")
    max_depth: int | None = Field(
        default=2, alias="maxDepth", description="Maximum link traversal depth"
    )
    max_pages: int | None = Field(
        default=10, alias="maxPages", description="Maximum pages to visit"
    )
    budget_seconds: int | None = Field(
        default=300, alias="budgetSeconds", description="Time budget in seconds"
    )
    policy: str | None = Field(
        default="NEWS_ONLY",
        description="Exploration policy (focused_topic, domain_wide, news_only, etc.)",
    )
    focus_keywords: str | None = Field(
        default=None, alias="focusKeywords", description="Focus keywords for FOCUSED_TOPIC policy"
    )
    custom_prompt: str | None = Field(
        default=None, alias="customPrompt", description="Custom prompt/instructions for AI agent"
    )
    capture_screenshots: bool | None = Field(
        default=False, alias="captureScreenshots", description="Whether to capture screenshots"
    )
    extract_structured: bool | None = Field(
        default=True, alias="extractStructured", description="Whether to extract structured data"
    )
    excluded_domains: str | None = Field(
        default=None, alias="excludedDomains", description="Domains to exclude"
    )
    callback_url: str | None = Field(
        default=None, alias="callbackUrl", description="Callback URL for session completion"
    )
    callback_token: str | None = Field(
        default=None, alias="callbackToken", description="Callback authentication token"
    )
    metadata: dict[str, Any] | None = Field(default=None, description="Additional metadata")
    created_at: datetime | None = Field(
        default=None, alias="createdAt", description="Task creation timestamp"
    )

    # Validator to handle Java LocalDateTime array format
    @field_validator("created_at", mode="before")
    @classmethod
    def parse_created_at(cls, value: Any) -> datetime | None:
        """Parse createdAt from Java LocalDateTime array or ISO string."""
        return parse_java_datetime(value)

    class Config:
        populate_by_name = True

    def get_excluded_domains_list(self) -> list[str]:
        """Parse excluded domains string into list."""
        if not self.excluded_domains:
            return []
        return [d.strip() for d in self.excluded_domains.split(",") if d.strip()]

    def get_focus_keywords_list(self) -> list[str]:
        """Parse focus keywords string into list."""
        if not self.focus_keywords:
            return []
        return [k.strip() for k in self.focus_keywords.split(",") if k.strip()]


class CrawlResultMessage(BaseModel):
    """
    Kafka message for crawl results.
    Matches: com.newsinsight.collector.dto.CrawlResultMessage

    ê¸°ë³¸ í•„ë“œëŠ” Java DTOì™€ í˜¸í™˜ë˜ë©°, ì¶”ê°€ ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„°ëŠ” metadata_jsonì— JSONìœ¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤.
    """

    job_id: int = Field(..., alias="jobId", description="Job ID this result belongs to")
    source_id: int = Field(..., alias="sourceId", description="Data source ID")
    title: str = Field(..., description="Article/page title")
    content: str = Field(..., description="Extracted content")
    url: str = Field(..., description="Page URL")
    published_at: str | None = Field(
        default=None, alias="publishedAt", description="Publication date as ISO string"
    )
    metadata_json: str | None = Field(
        default=None, alias="metadataJson", description="Additional metadata as JSON string"
    )

    class Config:
        populate_by_name = True

    def to_kafka_dict(self) -> dict[str, Any]:
        """Convert to Kafka-compatible dict with Java-style camelCase keys."""
        return {
            "jobId": self.job_id,
            "sourceId": self.source_id,
            "title": self.title,
            "content": self.content,
            "url": self.url,
            "publishedAt": self.published_at,
            "metadataJson": self.metadata_json,
        }


class NewsArticleMetadata(BaseModel):
    """
    ë‰´ìŠ¤ ê¸°ì‚¬ ì „ìš© ë©”íƒ€ë°ì´í„°.

    CrawlResultMessage.metadata_jsonì— JSONìœ¼ë¡œ ì§ë ¬í™”ë˜ì–´ ì €ì¥ë©ë‹ˆë‹¤.
    Java ì¸¡ì—ì„œëŠ” ì´ JSONì„ íŒŒì‹±í•˜ì—¬ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """

    # ê¸°ì/ì‘ì„±ì ì •ë³´
    authors: list[str] | None = Field(default=None, description="ê¸°ì‚¬ ì‘ì„±ì/ê¸°ì ëª©ë¡")
    author_email: str | None = Field(default=None, description="ê¸°ì ì´ë©”ì¼")

    # ë¶„ë¥˜ ì •ë³´
    category: str | None = Field(default=None, description="ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ (ì •ì¹˜, ê²½ì œ, ì‚¬íšŒ ë“±)")
    subcategory: str | None = Field(default=None, description="ì„¸ë¶€ ì¹´í…Œê³ ë¦¬")
    tags: list[str] | None = Field(default=None, description="ê¸°ì‚¬ íƒœê·¸/í‚¤ì›Œë“œ")

    # ì½˜í…ì¸  íŠ¹ì„±
    word_count: int | None = Field(default=None, description="ë³¸ë¬¸ ë‹¨ì–´ ìˆ˜")
    reading_time_minutes: float | None = Field(default=None, description="ì˜ˆìƒ ì½ê¸° ì‹œê°„ (ë¶„)")
    language: str | None = Field(default="ko", description="ê¸°ì‚¬ ì–¸ì–´ ì½”ë“œ")

    # ë‰´ìŠ¤ íŠ¹ì„±
    is_breaking: bool = Field(default=False, description="ì†ë³´ ì—¬ë¶€")
    is_exclusive: bool = Field(default=False, description="ë‹¨ë… ê¸°ì‚¬ ì—¬ë¶€")
    is_opinion: bool = Field(default=False, description="ì˜¤í”¼ë‹ˆì–¸/ì¹¼ëŸ¼ ì—¬ë¶€")
    has_paywall: bool = Field(default=False, description="ìœ ë£Œ êµ¬ë… í•„ìš” ì—¬ë¶€")

    # ë¯¸ë””ì–´ ì •ë³´
    thumbnail_url: str | None = Field(default=None, description="ëŒ€í‘œ ì´ë¯¸ì§€ URL")
    image_urls: list[str] | None = Field(default=None, description="ë³¸ë¬¸ ì´ë¯¸ì§€ URL ëª©ë¡")
    video_urls: list[str] | None = Field(default=None, description="ê´€ë ¨ ë™ì˜ìƒ URL ëª©ë¡")

    # ê´€ë ¨ ì½˜í…ì¸ 
    related_article_urls: list[str] | None = Field(default=None, description="ê´€ë ¨ ê¸°ì‚¬ URL ëª©ë¡")

    # ì†ŒìŠ¤ ì •ë³´
    source_name: str | None = Field(default=None, description="ì–¸ë¡ ì‚¬/ë§¤ì²´ëª…")
    source_bias: str | None = Field(default=None, description="ì •ì¹˜ ì„±í–¥ (ì•Œë ¤ì§„ ê²½ìš°)")

    # ì¶”ì¶œ í’ˆì§ˆ
    extraction_confidence: float | None = Field(default=None, description="ì¶”ì¶œ ì‹ ë¢°ë„ (0.0-1.0)")
    missing_fields: list[str] | None = Field(default=None, description="ì¶”ì¶œ ì‹¤íŒ¨í•œ í•„ë“œ ëª©ë¡")

    # ìˆ˜ì§‘ ì •ë³´
    crawled_at: str | None = Field(default=None, description="ìˆ˜ì§‘ ì‹œê°„ (ISO í˜•ì‹)")
    crawl_method: str | None = Field(default=None, description="ìˆ˜ì§‘ ë°©ë²• (ai_agent, rss, api)")

    def to_json(self) -> str:
        """JSON ë¬¸ìì—´ë¡œ ì§ë ¬í™”"""
        return self.model_dump_json(exclude_none=True)

    @classmethod
    def from_json(cls, json_str: str) -> "NewsArticleMetadata":
        """JSON ë¬¸ìì—´ì—ì„œ ë³µì›"""
        return cls.model_validate_json(json_str)


class EnhancedCrawlResultMessage(CrawlResultMessage):
    """
    í™•ì¥ëœ í¬ë¡¤ë§ ê²°ê³¼ ë©”ì‹œì§€.

    ê¸°ë³¸ CrawlResultMessageì™€ í˜¸í™˜ë˜ë©´ì„œ ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„°ë¥¼ í¸ë¦¬í•˜ê²Œ ë‹¤ë£° ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """

    def set_news_metadata(self, metadata: NewsArticleMetadata) -> None:
        """ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„° ì„¤ì •"""
        self.metadata_json = metadata.to_json()

    def get_news_metadata(self) -> Optional[NewsArticleMetadata]:
        """ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„° ì¡°íšŒ"""
        if not self.metadata_json:
            return None
        try:
            return NewsArticleMetadata.from_json(self.metadata_json)
        except Exception:
            return None

    @classmethod
    def create_news_result(
        cls,
        job_id: int,
        source_id: int,
        url: str,
        title: str,
        content: str,
        published_at: str | None = None,
        authors: list[str] | None = None,
        category: str | None = None,
        is_breaking: bool = False,
        thumbnail_url: str | None = None,
        source_name: str | None = None,
        word_count: int | None = None,
        **extra_metadata,
    ) -> "EnhancedCrawlResultMessage":
        """
        ë‰´ìŠ¤ ê¸°ì‚¬ ê²°ê³¼ ë©”ì‹œì§€ë¥¼ í¸ë¦¬í•˜ê²Œ ìƒì„±.

        Args:
            job_id: ì‘ì—… ID
            source_id: ì†ŒìŠ¤ ID
            url: ê¸°ì‚¬ URL
            title: ê¸°ì‚¬ ì œëª©
            content: ê¸°ì‚¬ ë³¸ë¬¸
            published_at: ë°œí–‰ì¼ (ISO í˜•ì‹)
            authors: ê¸°ì ëª©ë¡
            category: ì¹´í…Œê³ ë¦¬
            is_breaking: ì†ë³´ ì—¬ë¶€
            thumbnail_url: ì¸ë„¤ì¼ URL
            source_name: ì–¸ë¡ ì‚¬ëª…
            word_count: ë‹¨ì–´ ìˆ˜
            **extra_metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°

        Returns:
            EnhancedCrawlResultMessage ì¸ìŠ¤í„´ìŠ¤
        """
        # ë‹¨ì–´ ìˆ˜ ìë™ ê³„ì‚°
        if word_count is None and content:
            word_count = len(content.split())

        # ë©”íƒ€ë°ì´í„° ìƒì„±
        metadata = NewsArticleMetadata(
            authors=authors,
            category=category,
            is_breaking=is_breaking,
            thumbnail_url=thumbnail_url,
            source_name=source_name,
            word_count=word_count,
            crawled_at=datetime.utcnow().isoformat() + "Z",
            crawl_method="ai_agent",
            **extra_metadata,
        )

        # ê²°ê³¼ ë©”ì‹œì§€ ìƒì„±
        result = cls(
            jobId=job_id,
            sourceId=source_id,
            url=url,
            title=title,
            content=content,
            publishedAt=published_at,
        )
        result.set_news_metadata(metadata)

        return result


class CrawlSessionCallback(BaseModel):
    """
    í¬ë¡¤ë§ ì„¸ì…˜ ì™„ë£Œ ì½œë°± ë©”ì‹œì§€.

    autonomous-crawler-serviceê°€ í¬ë¡¤ë§ ì™„ë£Œ í›„ data-collection-serviceì— ì•Œë¦¼.
    """

    job_id: int = Field(..., alias="jobId", description="ì‘ì—… ID")
    source_id: int = Field(..., alias="sourceId", description="ì†ŒìŠ¤ ID")
    status: str = Field(..., description="ì™„ë£Œ ìƒíƒœ (COMPLETED, FAILED, TIMEOUT)")
    articles_extracted: int = Field(
        default=0, alias="articlesExtracted", description="ì¶”ì¶œëœ ê¸°ì‚¬ ìˆ˜"
    )
    pages_visited: int = Field(default=0, alias="pagesVisited", description="ë°©ë¬¸í•œ í˜ì´ì§€ ìˆ˜")
    elapsed_seconds: float = Field(default=0, alias="elapsedSeconds", description="ì†Œìš” ì‹œê°„ (ì´ˆ)")
    error: str | None = Field(default=None, description="ì—ëŸ¬ ë©”ì‹œì§€ (ì‹¤íŒ¨ ì‹œ)")
    captcha_encountered: bool = Field(
        default=False, alias="captchaEncountered", description="CAPTCHA ë°œê²¬ ì—¬ë¶€"
    )
    captcha_solved: bool = Field(
        default=False, alias="captchaSolved", description="CAPTCHA í•´ê²° ì—¬ë¶€"
    )

    class Config:
        populate_by_name = True

    def to_callback_dict(self) -> dict[str, Any]:
        """ì½œë°± API í˜¸ì¶œìš© ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "jobId": self.job_id,
            "sourceId": self.source_id,
            "status": self.status,
            "articlesExtracted": self.articles_extracted,
            "pagesVisited": self.pages_visited,
            "elapsedSeconds": self.elapsed_seconds,
            "error": self.error,
            "captchaEncountered": self.captcha_encountered,
            "captchaSolved": self.captcha_solved,
        }

```

---

## backend/autonomous-crawler-service/src/kafka/producer.py

```py
"""Kafka producer for crawl result messages."""

import json
from typing import Any

import structlog
from aiokafka import AIOKafkaProducer
from aiokafka.errors import KafkaError
from tenacity import retry, stop_after_attempt, wait_exponential

from src.config import Settings
from src.kafka.messages import CrawlResultMessage

logger = structlog.get_logger(__name__)


class CrawlResultProducer:
    """Async Kafka producer for crawl result messages."""

    def __init__(self, settings: Settings) -> None:
        self.settings = settings
        self._producer: AIOKafkaProducer | None = None

    async def start(self) -> None:
        """Start the Kafka producer."""
        self._producer = AIOKafkaProducer(
            bootstrap_servers=self.settings.kafka.bootstrap_servers,
            value_serializer=lambda v: json.dumps(v, default=str).encode("utf-8"),
            # Reliability settings
            acks="all",
        )

        await self._producer.start()
        logger.info(
            "Kafka producer started",
            bootstrap_servers=self.settings.kafka.bootstrap_servers,
        )

    async def stop(self) -> None:
        """Stop the Kafka producer."""
        if self._producer:
            await self._producer.stop()
            logger.info("Kafka producer stopped")

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=10),
    )
    async def send_result(self, result: CrawlResultMessage) -> None:
        """
        Send a crawl result to Kafka.

        Args:
            result: CrawlResultMessage to send
        """
        if not self._producer:
            raise RuntimeError("Producer not started. Call start() first.")

        topic = self.settings.kafka.crawl_result_topic
        value = result.to_kafka_dict()

        try:
            # Use job_id as key for partitioning (all results for same job go to same partition)
            key = str(result.job_id).encode("utf-8")

            await self._producer.send_and_wait(
                topic=topic,
                value=value,
                key=key,
            )

            logger.info(
                "Sent crawl result",
                job_id=result.job_id,
                source_id=result.source_id,
                url=result.url,
                title=result.title[:50] if result.title else None,
            )

        except KafkaError as e:
            logger.error(
                "Failed to send crawl result",
                job_id=result.job_id,
                error=str(e),
            )
            raise

    async def send_batch(self, results: list[CrawlResultMessage]) -> tuple[int, int]:
        """
        Send multiple crawl results to Kafka.

        Args:
            results: List of CrawlResultMessage objects

        Returns:
            Tuple of (successful_count, failed_count)
        """
        success_count = 0
        fail_count = 0

        for result in results:
            try:
                await self.send_result(result)
                success_count += 1
            except Exception as e:
                logger.error(
                    "Failed to send result in batch",
                    job_id=result.job_id,
                    url=result.url,
                    error=str(e),
                )
                fail_count += 1

        logger.info(
            "Batch send completed",
            success=success_count,
            failed=fail_count,
            total=len(results),
        )

        return success_count, fail_count

```

---

## backend/autonomous-crawler-service/src/main.py

```py
"""
Main entry point for autonomous-crawler-service.

Supports two modes:
1. Kafka mode (default): Consumes tasks from Kafka, produces results to Kafka
2. API mode: REST API + SSE for direct web UI access (browser-agent integration)
3. Hybrid mode: Both Kafka consumer and REST API running together

Usage:
    # Kafka mode (default)
    python -m src.main

    # API mode only
    python -m src.main --mode api

    # Hybrid mode (both Kafka + API)
    python -m src.main --mode hybrid

    # Or via environment variable
    SERVICE_MODE=hybrid python -m src.main
"""

import argparse
import asyncio
import logging
import os
import signal
import sys
from contextlib import asynccontextmanager
from enum import Enum
from typing import AsyncGenerator, Optional

import structlog
from prometheus_client import start_http_server

from src.config import Settings, get_settings
from src.config.consul import load_config_from_consul, wait_for_consul, CONSUL_ENABLED
from src.crawler import AutonomousCrawlerAgent
from src.kafka import BrowserTaskConsumer, CrawlResultProducer
from src.kafka.messages import BrowserTaskMessage
from src.metrics import (
    ARTICLES_EXTRACTED,
    BROWSER_SESSIONS_ACTIVE,
    KAFKA_MESSAGES_CONSUMED,
    KAFKA_MESSAGES_PRODUCED,
    TASK_DURATION,
    TASKS_COMPLETED,
    TASKS_IN_PROGRESS,
    TASKS_RECEIVED,
    init_service_info,
)


class ServiceMode(str, Enum):
    """Service operation mode"""

    KAFKA = "kafka"  # Kafka consumer only (original)
    API = "api"  # REST API only (browser-agent style)
    HYBRID = "hybrid"  # Both Kafka + REST API


def configure_logging(settings: Settings) -> None:
    """Configure structured logging."""
    processors = [
        structlog.contextvars.merge_contextvars,
        structlog.processors.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
    ]

    if settings.log_format == "json":
        processors.append(structlog.processors.JSONRenderer())
    else:
        processors.append(structlog.dev.ConsoleRenderer(colors=True))

    # Map log level string to logging module level
    log_level = getattr(logging, settings.log_level.upper(), logging.INFO)

    structlog.configure(
        processors=processors,
        wrapper_class=structlog.make_filtering_bound_logger(log_level),
        context_class=dict,
        logger_factory=structlog.PrintLoggerFactory(),
        cache_logger_on_first_use=True,
    )


class CrawlerService:
    """Main service class orchestrating the crawler components."""

    def __init__(self, settings: Settings) -> None:
        self.settings = settings
        self.logger = structlog.get_logger(__name__)
        self.consumer = BrowserTaskConsumer(settings)
        self.producer = CrawlResultProducer(settings)
        self.agent = AutonomousCrawlerAgent(settings)
        self._shutdown_event = asyncio.Event()

    async def start(self) -> None:
        """Start all service components."""
        self.logger.info("Starting autonomous-crawler-service")

        # Start metrics server
        if self.settings.metrics.enabled:
            start_http_server(self.settings.metrics.port)
            self.logger.info(
                "Metrics server started",
                port=self.settings.metrics.port,
            )

        # Initialize service info metrics
        init_service_info(
            version="0.1.0",
            llm_provider=self.settings.llm.provider,
        )

        # Start Kafka components
        await self.consumer.start()
        await self.producer.start()

        BROWSER_SESSIONS_ACTIVE.set(0)

        self.logger.info("Service started successfully")

    async def stop(self) -> None:
        """Stop all service components."""
        self.logger.info("Stopping autonomous-crawler-service")

        self._shutdown_event.set()

        await self.agent.close()
        await self.consumer.stop()
        await self.producer.stop()

        self.logger.info("Service stopped")

    async def handle_task(self, task: BrowserTaskMessage) -> None:
        """
        Handle a single browser task.

        Args:
            task: The browser task to process
        """
        import time

        start_time = time.time()
        policy = task.policy or "news_only"

        TASKS_RECEIVED.labels(policy=policy).inc()
        TASKS_IN_PROGRESS.inc()
        BROWSER_SESSIONS_ACTIVE.inc()
        KAFKA_MESSAGES_CONSUMED.labels(topic=self.settings.kafka.browser_task_topic).inc()

        self.logger.info(
            "Processing browser task",
            job_id=task.job_id,
            source_id=task.source_id,
            seed_url=task.seed_url,
            policy=policy,
        )

        status = "success"
        try:
            # Execute the crawl task
            results = await self.agent.execute_task(task)

            # Send results to Kafka
            for result in results:
                await self.producer.send_result(result)
                KAFKA_MESSAGES_PRODUCED.labels(topic=self.settings.kafka.crawl_result_topic).inc()
                ARTICLES_EXTRACTED.labels(source_id=str(task.source_id)).inc()

            self.logger.info(
                "Task completed",
                job_id=task.job_id,
                articles_extracted=len(results),
            )

        except Exception as e:
            status = "error"
            self.logger.error(
                "Task failed",
                job_id=task.job_id,
                error=str(e),
                exc_info=True,
            )

        finally:
            duration = time.time() - start_time
            TASK_DURATION.labels(policy=policy).observe(duration)
            TASKS_COMPLETED.labels(policy=policy, status=status).inc()
            TASKS_IN_PROGRESS.dec()
            BROWSER_SESSIONS_ACTIVE.dec()

    async def run(self) -> None:
        """Main run loop - consume and process tasks."""
        await self.start()

        try:
            await self.consumer.run_with_handler(self.handle_task)
        except asyncio.CancelledError:
            self.logger.info("Run loop cancelled")
        finally:
            await self.stop()


async def run_api_server(settings: Settings, port: int = 8030) -> None:
    """Run the FastAPI REST API server."""
    import uvicorn
    from src.api.server import create_app

    logger = structlog.get_logger(__name__)
    logger.info("Starting REST API server", port=port)

    app = create_app(settings)

    config = uvicorn.Config(
        app,
        host="0.0.0.0",
        port=port,
        log_level="info",
        access_log=True,
    )
    server = uvicorn.Server(config)
    await server.serve()


async def run_hybrid_mode(settings: Settings, api_port: int = 8030) -> None:
    """Run both Kafka consumer and REST API server concurrently."""
    logger = structlog.get_logger(__name__)
    logger.info(
        "Starting hybrid mode",
        api_port=api_port,
        kafka_enabled=True,
    )

    # Import here to avoid circular imports
    import uvicorn
    from src.api.server import create_app

    # Create services
    service = CrawlerService(settings)
    app = create_app(settings)

    # Create uvicorn config
    config = uvicorn.Config(
        app,
        host="0.0.0.0",
        port=api_port,
        log_level="info",
        access_log=True,
    )
    api_server = uvicorn.Server(config)

    # Setup signal handling
    shutdown_event = asyncio.Event()

    async def shutdown():
        logger.info("Shutting down hybrid mode...")
        shutdown_event.set()
        await service.stop()
        api_server.should_exit = True

    loop = asyncio.get_running_loop()
    for sig in (signal.SIGINT, signal.SIGTERM):
        loop.add_signal_handler(sig, lambda: asyncio.create_task(shutdown()))

    # Start Kafka service components (without the run loop)
    await service.start()

    # Run both concurrently
    async def kafka_consumer_loop():
        try:
            await service.consumer.run_with_handler(service.handle_task)
        except asyncio.CancelledError:
            logger.info("Kafka consumer loop cancelled")

    await asyncio.gather(
        kafka_consumer_loop(),
        api_server.serve(),
        return_exceptions=True,
    )


async def main_async(mode: ServiceMode = ServiceMode.KAFKA) -> None:
    """Async main function with mode selection."""
    # Load configuration from Consul (if enabled)
    consul_keys, env_keys = [], []
    if CONSUL_ENABLED:
        # Wait for Consul to be available
        if wait_for_consul(max_attempts=30, delay=2.0):
            consul_keys, env_keys = load_config_from_consul()
        else:
            print(
                "WARNING: Consul not available, using environment variables only", file=sys.stderr
            )

    settings = get_settings()
    configure_logging(settings)

    logger = structlog.get_logger(__name__)

    api_port = int(os.getenv("API_PORT", "8030"))

    logger.info(
        "Initializing autonomous-crawler-service",
        mode=mode.value,
        kafka_servers=settings.kafka.bootstrap_servers,
        llm_provider=settings.llm.provider,
        consul_enabled=CONSUL_ENABLED,
        consul_keys_loaded=len(consul_keys),
        api_port=api_port if mode in (ServiceMode.API, ServiceMode.HYBRID) else None,
    )

    if mode == ServiceMode.KAFKA:
        # Original Kafka-only mode
        service = CrawlerService(settings)

        loop = asyncio.get_running_loop()

        def signal_handler() -> None:
            logger.info("Received shutdown signal")
            asyncio.create_task(service.stop())

        for sig in (signal.SIGINT, signal.SIGTERM):
            loop.add_signal_handler(sig, signal_handler)

        await service.run()

    elif mode == ServiceMode.API:
        # REST API only mode (browser-agent style)
        await run_api_server(settings, api_port)

    elif mode == ServiceMode.HYBRID:
        # Both Kafka and REST API
        await run_hybrid_mode(settings, api_port)


def parse_args() -> argparse.Namespace:
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Autonomous Crawler Service - AI-driven browser crawler",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Modes:
  kafka   - Kafka consumer only (default, for internal job processing)
  api     - REST API only (browser-agent style, for direct web UI access)
  hybrid  - Both Kafka consumer and REST API running together

Examples:
  python -m src.main                    # Kafka mode (default)
  python -m src.main --mode api         # REST API only
  python -m src.main --mode hybrid      # Both Kafka + REST API
  SERVICE_MODE=hybrid python -m src.main  # Via environment variable
        """,
    )
    parser.add_argument(
        "--mode",
        "-m",
        type=str,
        choices=["kafka", "api", "hybrid"],
        default=os.getenv("SERVICE_MODE", "kafka"),
        help="Service operation mode (default: kafka, or SERVICE_MODE env var)",
    )
    parser.add_argument(
        "--port",
        "-p",
        type=int,
        default=int(os.getenv("API_PORT", "8030")),
        help="API server port (default: 8030, or API_PORT env var)",
    )
    return parser.parse_args()


def main() -> None:
    """Main entry point."""
    args = parse_args()

    # Convert string mode to enum
    mode_map = {
        "kafka": ServiceMode.KAFKA,
        "api": ServiceMode.API,
        "hybrid": ServiceMode.HYBRID,
    }
    mode = mode_map.get(args.mode, ServiceMode.KAFKA)

    # Set API_PORT env var so it's accessible in main_async
    os.environ["API_PORT"] = str(args.port)

    try:
        asyncio.run(main_async(mode))
    except KeyboardInterrupt:
        print("Interrupted")
        sys.exit(0)


if __name__ == "__main__":
    main()

```

---

## backend/autonomous-crawler-service/src/mcp/__init__.py

```py
"""
MCP (Model Context Protocol) Adapter Module

MCP ì„œë²„ë“¤ì„ ML Add-onìœ¼ë¡œ ë˜í•‘í•˜ì—¬ REST APIë¡œ ë…¸ì¶œí•©ë‹ˆë‹¤.
"""

from src.mcp.client import MCPClient
from src.mcp.adapter import MCPAdapter, get_mcp_adapter
from src.mcp.router import router as mcp_router

__all__ = ["MCPClient", "MCPAdapter", "get_mcp_adapter", "mcp_router"]

```

---

## backend/autonomous-crawler-service/src/mcp/adapter.py

```py
"""
MCP Adapter - MCP ì„œë²„ë“¤ì„ ML Add-onìœ¼ë¡œ ë˜í•‘í•˜ëŠ” ì–´ëŒ‘í„°

MCP ì„œë²„ì˜ toolë“¤ì„ REST API í˜•íƒœë¡œ ë…¸ì¶œí•˜ê³ ,
ML Add-on ì¸í„°í˜ì´ìŠ¤ì™€ í˜¸í™˜ë˜ëŠ” ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
"""

import os
from typing import Any, Dict, List, Optional
from datetime import datetime
from enum import Enum

import structlog
from pydantic import BaseModel, Field

from .client import (
    BiasMCPClient,
    FactcheckMCPClient,
    TopicMCPClient,
    HuggingFaceMCPClient,
    NewsInsightMCPClient,
    MCPClient,
)

logger = structlog.get_logger(__name__)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MCP ì„œë²„ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class MCPServerConfig:
    """MCP ì„œë²„ ì—°ê²° ì„¤ì •"""

    BIAS_MCP_URL = os.environ.get("BIAS_MCP_URL", "http://bias-mcp:5001")
    FACTCHECK_MCP_URL = os.environ.get("FACTCHECK_MCP_URL", "http://factcheck-mcp:5002")
    TOPIC_MCP_URL = os.environ.get("TOPIC_MCP_URL", "http://topic-mcp:5003")
    NEWSINSIGHT_MCP_URL = os.environ.get("NEWSINSIGHT_MCP_URL", "http://newsinsight-mcp:5000")
    HUGGINGFACE_MCP_URL = os.environ.get("HUGGINGFACE_MCP_URL", "http://huggingface-mcp:5011")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‘ë‹µ ëª¨ë¸ (ML Add-on í˜¸í™˜)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class MCPAddonCategory(str, Enum):
    """MCP Add-on ì¹´í…Œê³ ë¦¬"""

    BIAS = "BIAS_ANALYSIS"
    FACTCHECK = "FACTCHECK"
    TOPIC = "TOPIC_CLASSIFICATION"
    SENTIMENT = "SENTIMENT"
    SUMMARIZATION = "SUMMARIZATION"
    ENTITY = "ENTITY_EXTRACTION"


class MCPAddonResponse(BaseModel):
    """MCP Add-on í‘œì¤€ ì‘ë‹µ"""

    addon_key: str
    category: MCPAddonCategory
    success: bool
    data: Optional[Dict[str, Any]] = None
    report: Optional[str] = None
    error: Optional[str] = None
    latency_ms: int = 0
    generated_at: str = Field(default_factory=lambda: datetime.utcnow().isoformat())


class MCPAddonInfo(BaseModel):
    """MCP Add-on ì •ë³´"""

    addon_key: str
    name: str
    description: str
    category: MCPAddonCategory
    endpoint_url: str
    tools: List[str] = []
    enabled: bool = True
    health_status: str = "unknown"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MCP ì–´ëŒ‘í„° í´ë˜ìŠ¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class MCPAdapter:
    """MCP ì„œë²„ë“¤ì„ ML Add-onìœ¼ë¡œ ë˜í•‘í•˜ëŠ” ì–´ëŒ‘í„°"""

    def __init__(self):
        self.bias_client = BiasMCPClient(MCPServerConfig.BIAS_MCP_URL)
        self.factcheck_client = FactcheckMCPClient(MCPServerConfig.FACTCHECK_MCP_URL)
        self.topic_client = TopicMCPClient(MCPServerConfig.TOPIC_MCP_URL)
        self.newsinsight_client = NewsInsightMCPClient(MCPServerConfig.NEWSINSIGHT_MCP_URL)
        self.huggingface_client = HuggingFaceMCPClient(MCPServerConfig.HUGGINGFACE_MCP_URL)

        self._clients: Dict[str, MCPClient] = {
            "bias": self.bias_client,
            "factcheck": self.factcheck_client,
            "topic": self.topic_client,
            "newsinsight": self.newsinsight_client,
            "huggingface": self.huggingface_client,
        }

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Add-on ëª©ë¡ ë° ìƒíƒœ ì¡°íšŒ
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def list_addons(self) -> List[MCPAddonInfo]:
        """ë“±ë¡ëœ MCP Add-on ëª©ë¡ ë°˜í™˜"""
        addons = [
            MCPAddonInfo(
                addon_key="mcp-bias",
                name="í¸í–¥ë„ ë¶„ì„ (MCP)",
                description="ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì •ì¹˜ì /ì´ë…ì  í¸í–¥ë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤",
                category=MCPAddonCategory.BIAS,
                endpoint_url=MCPServerConfig.BIAS_MCP_URL,
                tools=["get_bias_raw", "get_bias_report", "get_source_bias_list"],
            ),
            MCPAddonInfo(
                addon_key="mcp-factcheck",
                name="íŒ©íŠ¸ì²´í¬/ì‹ ë¢°ë„ (MCP)",
                description="ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì‹ ë¢°ë„ì™€ íŒ©íŠ¸ì²´í¬ ìƒíƒœë¥¼ ë¶„ì„í•©ë‹ˆë‹¤",
                category=MCPAddonCategory.FACTCHECK,
                endpoint_url=MCPServerConfig.FACTCHECK_MCP_URL,
                tools=[
                    "get_factcheck_raw",
                    "get_factcheck_report",
                    "get_source_reliability_list",
                ],
            ),
            MCPAddonInfo(
                addon_key="mcp-topic",
                name="í† í”½ ë¶„ì„ (MCP)",
                description="ë‰´ìŠ¤ í† í”½, í‚¤ì›Œë“œ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤",
                category=MCPAddonCategory.TOPIC,
                endpoint_url=MCPServerConfig.TOPIC_MCP_URL,
                tools=[
                    "get_topic_raw",
                    "get_topic_report",
                    "get_trending_topics",
                    "get_category_list",
                ],
            ),
            MCPAddonInfo(
                addon_key="mcp-sentiment",
                name="ê°ì„± ë¶„ì„ (MCP)",
                description="ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ê°ì„±(ê¸ì •/ë¶€ì •/ì¤‘ë¦½)ì„ ë¶„ì„í•©ë‹ˆë‹¤",
                category=MCPAddonCategory.SENTIMENT,
                endpoint_url=MCPServerConfig.NEWSINSIGHT_MCP_URL,
                tools=[
                    "get_sentiment_raw",
                    "get_sentiment_report",
                    "get_article_list",
                ],
            ),
            MCPAddonInfo(
                addon_key="mcp-huggingface",
                name="HuggingFace NLP (MCP)",
                description="HuggingFace ëª¨ë¸ì„ í™œìš©í•œ NLP ë¶„ì„",
                category=MCPAddonCategory.SUMMARIZATION,
                endpoint_url=MCPServerConfig.HUGGINGFACE_MCP_URL,
                tools=[
                    "analyze_sentiment",
                    "summarize_article",
                    "extract_entities",
                    "extract_keywords",
                    "classify_news",
                ],
            ),
        ]
        return addons

    async def check_all_health(self) -> Dict[str, Dict[str, Any]]:
        """ëª¨ë“  MCP ì„œë²„ í—¬ìŠ¤ì²´í¬"""
        results = {}
        for name, client in self._clients.items():
            results[name] = await client.health_check()
        return results

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Bias Analysis
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def analyze_bias(
        self,
        keyword: str,
        days: int = 7,
        include_report: bool = False,
    ) -> MCPAddonResponse:
        """í¸í–¥ë„ ë¶„ì„ ì‹¤í–‰"""
        start_time = datetime.utcnow()

        try:
            result = await self.bias_client.get_bias_raw(keyword, days)

            if not result.get("success"):
                return MCPAddonResponse(
                    addon_key="mcp-bias",
                    category=MCPAddonCategory.BIAS,
                    success=False,
                    error=result.get("error", "Unknown error"),
                    latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
                )

            response = MCPAddonResponse(
                addon_key="mcp-bias",
                category=MCPAddonCategory.BIAS,
                success=True,
                data=result.get("data"),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

            # ë¦¬í¬íŠ¸ë„ í•¨ê»˜ ìš”ì²­ëœ ê²½ìš°
            if include_report:
                report_result = await self.bias_client.get_bias_report(keyword, days)
                if report_result.get("success"):
                    response.report = report_result.get("data")

            return response

        except Exception as e:
            logger.error("Bias analysis failed", error=str(e))
            return MCPAddonResponse(
                addon_key="mcp-bias",
                category=MCPAddonCategory.BIAS,
                success=False,
                error=str(e),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Factcheck Analysis
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def analyze_factcheck(
        self,
        keyword: str,
        days: int = 7,
        include_report: bool = False,
    ) -> MCPAddonResponse:
        """íŒ©íŠ¸ì²´í¬/ì‹ ë¢°ë„ ë¶„ì„ ì‹¤í–‰"""
        start_time = datetime.utcnow()

        try:
            result = await self.factcheck_client.get_factcheck_raw(keyword, days)

            if not result.get("success"):
                return MCPAddonResponse(
                    addon_key="mcp-factcheck",
                    category=MCPAddonCategory.FACTCHECK,
                    success=False,
                    error=result.get("error", "Unknown error"),
                    latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
                )

            response = MCPAddonResponse(
                addon_key="mcp-factcheck",
                category=MCPAddonCategory.FACTCHECK,
                success=True,
                data=result.get("data"),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

            if include_report:
                report_result = await self.factcheck_client.get_factcheck_report(keyword, days)
                if report_result.get("success"):
                    response.report = report_result.get("data")

            return response

        except Exception as e:
            logger.error("Factcheck analysis failed", error=str(e))
            return MCPAddonResponse(
                addon_key="mcp-factcheck",
                category=MCPAddonCategory.FACTCHECK,
                success=False,
                error=str(e),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Topic Analysis
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def analyze_topic(
        self,
        keyword: Optional[str] = None,
        days: int = 7,
        include_report: bool = False,
    ) -> MCPAddonResponse:
        """í† í”½ ë¶„ì„ ì‹¤í–‰"""
        start_time = datetime.utcnow()

        try:
            result = await self.topic_client.get_topic_raw(keyword, days)

            if not result.get("success"):
                return MCPAddonResponse(
                    addon_key="mcp-topic",
                    category=MCPAddonCategory.TOPIC,
                    success=False,
                    error=result.get("error", "Unknown error"),
                    latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
                )

            response = MCPAddonResponse(
                addon_key="mcp-topic",
                category=MCPAddonCategory.TOPIC,
                success=True,
                data=result.get("data"),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

            if include_report:
                report_result = await self.topic_client.get_topic_report(keyword, days)
                if report_result.get("success"):
                    response.report = report_result.get("data")

            return response

        except Exception as e:
            logger.error("Topic analysis failed", error=str(e))
            return MCPAddonResponse(
                addon_key="mcp-topic",
                category=MCPAddonCategory.TOPIC,
                success=False,
                error=str(e),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

    async def get_trending_topics(self, days: int = 1, limit: int = 10) -> MCPAddonResponse:
        """íŠ¸ë Œë”© í† í”½ ì¡°íšŒ"""
        start_time = datetime.utcnow()

        try:
            result = await self.topic_client.get_trending_topics(days, limit)

            return MCPAddonResponse(
                addon_key="mcp-topic",
                category=MCPAddonCategory.TOPIC,
                success=result.get("success", False),
                data=result.get("data"),
                error=result.get("error"),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

        except Exception as e:
            logger.error("Get trending topics failed", error=str(e))
            return MCPAddonResponse(
                addon_key="mcp-topic",
                category=MCPAddonCategory.TOPIC,
                success=False,
                error=str(e),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Sentiment Analysis
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def analyze_sentiment(
        self,
        keyword: str,
        days: int = 7,
        include_report: bool = False,
    ) -> MCPAddonResponse:
        """ê°ì„± ë¶„ì„ ì‹¤í–‰"""
        start_time = datetime.utcnow()

        try:
            result = await self.newsinsight_client.get_sentiment_raw(keyword, days)

            if not result.get("success"):
                return MCPAddonResponse(
                    addon_key="mcp-sentiment",
                    category=MCPAddonCategory.SENTIMENT,
                    success=False,
                    error=result.get("error", "Unknown error"),
                    latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
                )

            response = MCPAddonResponse(
                addon_key="mcp-sentiment",
                category=MCPAddonCategory.SENTIMENT,
                success=True,
                data=result.get("data"),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

            if include_report:
                report_result = await self.newsinsight_client.get_sentiment_report(keyword, days)
                if report_result.get("success"):
                    response.report = report_result.get("data")

            return response

        except Exception as e:
            logger.error("Sentiment analysis failed", error=str(e))
            return MCPAddonResponse(
                addon_key="mcp-sentiment",
                category=MCPAddonCategory.SENTIMENT,
                success=False,
                error=str(e),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # HuggingFace NLP
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def summarize_article(
        self,
        text: str,
        max_length: int = 150,
        min_length: int = 50,
    ) -> MCPAddonResponse:
        """ê¸°ì‚¬ ìš”ì•½"""
        start_time = datetime.utcnow()

        try:
            result = await self.huggingface_client.summarize_article(text, max_length, min_length)

            return MCPAddonResponse(
                addon_key="mcp-huggingface",
                category=MCPAddonCategory.SUMMARIZATION,
                success=result.get("success", False),
                data=result.get("data"),
                error=result.get("error"),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

        except Exception as e:
            logger.error("Summarization failed", error=str(e))
            return MCPAddonResponse(
                addon_key="mcp-huggingface",
                category=MCPAddonCategory.SUMMARIZATION,
                success=False,
                error=str(e),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

    async def extract_entities(self, text: str) -> MCPAddonResponse:
        """ê°œì²´ëª… ì¸ì‹"""
        start_time = datetime.utcnow()

        try:
            result = await self.huggingface_client.extract_entities(text)

            return MCPAddonResponse(
                addon_key="mcp-huggingface",
                category=MCPAddonCategory.ENTITY,
                success=result.get("success", False),
                data=result.get("data"),
                error=result.get("error"),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

        except Exception as e:
            logger.error("Entity extraction failed", error=str(e))
            return MCPAddonResponse(
                addon_key="mcp-huggingface",
                category=MCPAddonCategory.ENTITY,
                success=False,
                error=str(e),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_mcp_adapter: Optional[MCPAdapter] = None


def get_mcp_adapter() -> MCPAdapter:
    """MCP ì–´ëŒ‘í„° ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _mcp_adapter
    if _mcp_adapter is None:
        _mcp_adapter = MCPAdapter()
    return _mcp_adapter

```

---

## backend/autonomous-crawler-service/src/mcp/client.py

```py
"""
MCP Client - MCP ì„œë²„ JSON-RPC í˜¸ì¶œ í´ë¼ì´ì–¸íŠ¸

MCP ì„œë²„ë“¤ (bias, factcheck, topic, huggingface ë“±)ì—
JSON-RPC í˜•ì‹ìœ¼ë¡œ toolì„ í˜¸ì¶œí•˜ëŠ” í´ë¼ì´ì–¸íŠ¸ì…ë‹ˆë‹¤.
"""

import asyncio
import json
from typing import Any, Dict, List, Optional
from datetime import datetime

import httpx
import structlog

logger = structlog.get_logger(__name__)


class MCPClient:
    """MCP ì„œë²„ JSON-RPC í´ë¼ì´ì–¸íŠ¸"""

    def __init__(
        self,
        base_url: str,
        timeout: float = 60.0,
        health_check_path: str = "/health",
    ):
        """
        Args:
            base_url: MCP ì„œë²„ ë² ì´ìŠ¤ URL (ì˜ˆ: http://bias-mcp:5001)
            timeout: HTTP ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
            health_check_path: í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ ê²½ë¡œ
        """
        self.base_url = base_url.rstrip("/")
        self.timeout = timeout
        self.health_check_path = health_check_path
        self._mcp_path = "/mcp"

    async def health_check(self) -> Dict[str, Any]:
        """ì„œë²„ í—¬ìŠ¤ì²´í¬"""
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                resp = await client.get(f"{self.base_url}{self.health_check_path}")
                if resp.status_code == 200:
                    return {"status": "healthy", "data": resp.json()}
                return {"status": "unhealthy", "status_code": resp.status_code}
            except Exception as e:
                return {"status": "error", "error": str(e)}

    async def call_tool(
        self,
        tool_name: str,
        arguments: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        MCP ì„œë²„ì˜ toolì„ í˜¸ì¶œí•©ë‹ˆë‹¤.

        Args:
            tool_name: í˜¸ì¶œí•  tool ì´ë¦„ (ì˜ˆ: get_bias_raw)
            arguments: tool ì¸ì

        Returns:
            tool ì‹¤í–‰ ê²°ê³¼
        """
        # JSON-RPC 2.0 í˜•ì‹ì˜ ìš”ì²­ ìƒì„±
        request_payload = {
            "jsonrpc": "2.0",
            "method": "tools/call",
            "params": {
                "name": tool_name,
                "arguments": arguments or {},
            },
            "id": int(datetime.utcnow().timestamp() * 1000),
        }

        async with httpx.AsyncClient(timeout=self.timeout) as client:
            try:
                logger.debug(
                    "Calling MCP tool",
                    url=f"{self.base_url}{self._mcp_path}",
                    tool=tool_name,
                    arguments=arguments,
                )

                resp = await client.post(
                    f"{self.base_url}{self._mcp_path}",
                    json=request_payload,
                    headers={"Content-Type": "application/json"},
                )

                if resp.status_code != 200:
                    error_text = resp.text
                    logger.error(
                        "MCP call failed",
                        tool=tool_name,
                        status_code=resp.status_code,
                        error=error_text,
                    )
                    return {
                        "success": False,
                        "error": f"HTTP {resp.status_code}: {error_text}",
                    }

                result = resp.json()

                # JSON-RPC ì—ëŸ¬ ì²´í¬
                if "error" in result:
                    error = result["error"]
                    return {
                        "success": False,
                        "error": error.get("message", str(error)),
                        "code": error.get("code"),
                    }

                # ì„±ê³µ ì‘ë‹µ
                return {
                    "success": True,
                    "data": result.get("result"),
                }

            except httpx.TimeoutException:
                logger.error("MCP call timeout", tool=tool_name)
                return {"success": False, "error": "Request timeout"}
            except Exception as e:
                logger.error("MCP call exception", tool=tool_name, error=str(e))
                return {"success": False, "error": str(e)}

    async def list_tools(self) -> Dict[str, Any]:
        """MCP ì„œë²„ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ tool ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
        request_payload = {
            "jsonrpc": "2.0",
            "method": "tools/list",
            "params": {},
            "id": int(datetime.utcnow().timestamp() * 1000),
        }

        async with httpx.AsyncClient(timeout=self.timeout) as client:
            try:
                resp = await client.post(
                    f"{self.base_url}{self._mcp_path}",
                    json=request_payload,
                    headers={"Content-Type": "application/json"},
                )

                if resp.status_code != 200:
                    return {"success": False, "error": f"HTTP {resp.status_code}"}

                result = resp.json()
                return {
                    "success": True,
                    "tools": result.get("result", {}).get("tools", []),
                }
            except Exception as e:
                return {"success": False, "error": str(e)}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MCP ì„œë²„ë³„ íŠ¹í™” í´ë¼ì´ì–¸íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class BiasMCPClient(MCPClient):
    """Bias Analysis MCP í´ë¼ì´ì–¸íŠ¸"""

    async def get_bias_raw(self, keyword: str, days: int = 7) -> Dict[str, Any]:
        """í‚¤ì›Œë“œ ê´€ë ¨ í¸í–¥ë„ ë¶„ì„ ë°ì´í„° ì¡°íšŒ"""
        return await self.call_tool("get_bias_raw", {"keyword": keyword, "days": days})

    async def get_bias_report(
        self, keyword: str, days: int = 7, session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """í¸í–¥ë„ ë¶„ì„ ìì—°ì–´ ë¦¬í¬íŠ¸ ìƒì„±"""
        args = {"keyword": keyword, "days": days}
        if session_id:
            args["session_id"] = session_id
        return await self.call_tool("get_bias_report", args)

    async def get_source_bias_list(self) -> Dict[str, Any]:
        """ì–¸ë¡ ì‚¬ë³„ í¸í–¥ ì°¸ì¡° ë°ì´í„° ì¡°íšŒ"""
        return await self.call_tool("get_source_bias_list")


class FactcheckMCPClient(MCPClient):
    """Fact Check MCP í´ë¼ì´ì–¸íŠ¸"""

    async def get_factcheck_raw(self, keyword: str, days: int = 7) -> Dict[str, Any]:
        """í‚¤ì›Œë“œ ê´€ë ¨ ì‹ ë¢°ë„/íŒ©íŠ¸ì²´í¬ ë°ì´í„° ì¡°íšŒ"""
        return await self.call_tool("get_factcheck_raw", {"keyword": keyword, "days": days})

    async def get_factcheck_report(
        self, keyword: str, days: int = 7, session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """ì‹ ë¢°ë„ ë¶„ì„ ìì—°ì–´ ë¦¬í¬íŠ¸ ìƒì„±"""
        args = {"keyword": keyword, "days": days}
        if session_id:
            args["session_id"] = session_id
        return await self.call_tool("get_factcheck_report", args)

    async def get_source_reliability_list(self) -> Dict[str, Any]:
        """ì–¸ë¡ ì‚¬ë³„ ì‹ ë¢°ë„ ì°¸ì¡° ë°ì´í„° ì¡°íšŒ"""
        return await self.call_tool("get_source_reliability_list")


class TopicMCPClient(MCPClient):
    """Topic Analysis MCP í´ë¼ì´ì–¸íŠ¸"""

    async def get_topic_raw(self, keyword: Optional[str] = None, days: int = 7) -> Dict[str, Any]:
        """í† í”½ ë¶„ì„ ë°ì´í„° ì¡°íšŒ"""
        return await self.call_tool("get_topic_raw", {"keyword": keyword, "days": days})

    async def get_topic_report(
        self,
        keyword: Optional[str] = None,
        days: int = 7,
        session_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """í† í”½ ë¶„ì„ ìì—°ì–´ ë¦¬í¬íŠ¸ ìƒì„±"""
        args = {"keyword": keyword, "days": days}
        if session_id:
            args["session_id"] = session_id
        return await self.call_tool("get_topic_report", args)

    async def get_trending_topics(self, days: int = 1, limit: int = 10) -> Dict[str, Any]:
        """íŠ¸ë Œë”© í† í”½ ëª©ë¡ ì¡°íšŒ"""
        return await self.call_tool("get_trending_topics", {"days": days, "limit": limit})

    async def get_category_list(self) -> Dict[str, Any]:
        """ì¹´í…Œê³ ë¦¬ ëª©ë¡ ì¡°íšŒ"""
        return await self.call_tool("get_category_list")


class HuggingFaceMCPClient(MCPClient):
    """Hugging Face MCP í´ë¼ì´ì–¸íŠ¸"""

    async def analyze_sentiment(self, text: str, model_id: Optional[str] = None) -> Dict[str, Any]:
        """ê°ì„± ë¶„ì„"""
        args = {"text": text}
        if model_id:
            args["model_id"] = model_id
        return await self.call_tool("analyze_sentiment", args)

    async def summarize_article(
        self, text: str, max_length: int = 150, min_length: int = 50
    ) -> Dict[str, Any]:
        """ê¸°ì‚¬ ìš”ì•½"""
        return await self.call_tool(
            "summarize_article",
            {"text": text, "max_length": max_length, "min_length": min_length},
        )

    async def extract_entities(self, text: str) -> Dict[str, Any]:
        """ê°œì²´ëª… ì¸ì‹"""
        return await self.call_tool("extract_entities", {"text": text})

    async def extract_keywords(self, text: str, top_k: int = 10) -> Dict[str, Any]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        return await self.call_tool("extract_keywords", {"text": text, "top_k": top_k})

    async def classify_news(
        self, text: str, categories: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """ë‰´ìŠ¤ ë¶„ë¥˜"""
        args: Dict[str, Any] = {"text": text}
        if categories:
            args["categories"] = categories
        return await self.call_tool("classify_news", args)


class NewsInsightMCPClient(MCPClient):
    """NewsInsight MCP í´ë¼ì´ì–¸íŠ¸"""

    async def get_sentiment_raw(self, keyword: str, days: int = 7) -> Dict[str, Any]:
        """ê°ì„± ë¶„ì„ ë°ì´í„° ì¡°íšŒ"""
        return await self.call_tool("get_sentiment_raw", {"keyword": keyword, "days": days})

    async def get_sentiment_report(
        self, keyword: str, days: int = 7, session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """ê°ì„± ë¶„ì„ ìì—°ì–´ ë¦¬í¬íŠ¸ ìƒì„±"""
        args = {"keyword": keyword, "days": days}
        if session_id:
            args["session_id"] = session_id
        return await self.call_tool("get_sentiment_report", args)

    async def get_article_list(
        self, keyword: str, days: int = 7, limit: int = 50
    ) -> Dict[str, Any]:
        """ê¸°ì‚¬ ëª©ë¡ ì¡°íšŒ"""
        return await self.call_tool(
            "get_article_list", {"keyword": keyword, "days": days, "limit": limit}
        )

    async def get_discussion_summary(self, keyword: str, days: int = 7) -> Dict[str, Any]:
        """í† ë¡  ìš”ì•½ ì¡°íšŒ"""
        return await self.call_tool("get_discussion_summary", {"keyword": keyword, "days": days})

```

---

## backend/autonomous-crawler-service/src/mcp/router.py

```py
"""
MCP API Router - MCP Add-on REST API ë¼ìš°í„°

MCP ì–´ëŒ‘í„°ë¥¼ í†µí•´ MCP ì„œë²„ë“¤ì˜ ê¸°ëŠ¥ì„ REST APIë¡œ ë…¸ì¶œí•©ë‹ˆë‹¤.
"""

from typing import Any, Dict, List, Optional

import structlog
from fastapi import APIRouter, HTTPException, Query
from pydantic import BaseModel, Field

from .adapter import MCPAdapter, MCPAddonResponse, MCPAddonInfo, get_mcp_adapter

logger = structlog.get_logger(__name__)

router = APIRouter(prefix="/mcp", tags=["MCP Add-ons"])


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Request Models
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class KeywordAnalysisRequest(BaseModel):
    """í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„ ìš”ì²­"""

    keyword: str = Field(..., description="ë¶„ì„í•  í‚¤ì›Œë“œ", min_length=1, max_length=100)
    days: int = Field(default=7, ge=1, le=90, description="ë¶„ì„ ê¸°ê°„ (ì¼)")
    include_report: bool = Field(default=False, description="ìì—°ì–´ ë¦¬í¬íŠ¸ í¬í•¨ ì—¬ë¶€")


class TextAnalysisRequest(BaseModel):
    """í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶„ì„ ìš”ì²­"""

    text: str = Field(..., description="ë¶„ì„í•  í…ìŠ¤íŠ¸", min_length=10, max_length=50000)
    max_length: int = Field(default=150, ge=50, le=500, description="ìš”ì•½ ìµœëŒ€ ê¸¸ì´")
    min_length: int = Field(default=50, ge=20, le=200, description="ìš”ì•½ ìµœì†Œ ê¸¸ì´")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Add-on ê´€ë¦¬ ì—”ë“œí¬ì¸íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@router.get("/addons", response_model=List[MCPAddonInfo])
async def list_mcp_addons():
    """
    ë“±ë¡ëœ MCP Add-on ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    adapter = get_mcp_adapter()
    return await adapter.list_addons()


@router.get("/health")
async def check_mcp_health():
    """
    ëª¨ë“  MCP ì„œë²„ì˜ í—¬ìŠ¤ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    """
    adapter = get_mcp_adapter()
    results = await adapter.check_all_health()

    # ì „ì²´ ìƒíƒœ ìš”ì•½
    healthy_count = sum(1 for r in results.values() if r.get("status") == "healthy")
    total_count = len(results)

    return {
        "status": "healthy" if healthy_count == total_count else "degraded",
        "healthy": healthy_count,
        "total": total_count,
        "servers": results,
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Bias Analysis ì—”ë“œí¬ì¸íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@router.post("/bias/analyze", response_model=MCPAddonResponse)
async def analyze_bias(request: KeywordAnalysisRequest):
    """
    í‚¤ì›Œë“œ ê´€ë ¨ ë‰´ìŠ¤ì˜ í¸í–¥ë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

    - ì •ì¹˜ì /ì´ë…ì  í¸í–¥ ìŠ¤í™íŠ¸ëŸ¼ ë¶„ì„
    - ì–¸ë¡ ì‚¬ë³„ í¸í–¥ ë¶„í¬
    - ê°ê´€ì„± ì ìˆ˜
    """
    adapter = get_mcp_adapter()
    return await adapter.analyze_bias(
        keyword=request.keyword,
        days=request.days,
        include_report=request.include_report,
    )


@router.get("/bias/sources")
async def get_source_bias_list():
    """
    ì–¸ë¡ ì‚¬ë³„ ì¼ë°˜ì ì¸ í¸í–¥ ì„±í–¥ ì°¸ì¡° ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    adapter = get_mcp_adapter()
    result = await adapter.bias_client.get_source_bias_list()
    if not result.get("success"):
        raise HTTPException(status_code=500, detail=result.get("error"))
    return result.get("data")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Factcheck Analysis ì—”ë“œí¬ì¸íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@router.post("/factcheck/analyze", response_model=MCPAddonResponse)
async def analyze_factcheck(request: KeywordAnalysisRequest):
    """
    í‚¤ì›Œë“œ ê´€ë ¨ ë‰´ìŠ¤ì˜ ì‹ ë¢°ë„ ë° íŒ©íŠ¸ì²´í¬ ìƒíƒœë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

    - ì „ì²´ ì‹ ë¢°ë„ ì ìˆ˜
    - ì–¸ë¡ ì‚¬ë³„ ì‹ ë¢°ë„
    - ì£¼ì¥/ê²€ì¦ ë¹„ìœ¨
    - ì¸ìš© í’ˆì§ˆ ì ìˆ˜
    """
    adapter = get_mcp_adapter()
    return await adapter.analyze_factcheck(
        keyword=request.keyword,
        days=request.days,
        include_report=request.include_report,
    )


@router.get("/factcheck/sources")
async def get_source_reliability_list():
    """
    ì–¸ë¡ ì‚¬ë³„ ê¸°ë³¸ ì‹ ë¢°ë„ ì°¸ì¡° ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    adapter = get_mcp_adapter()
    result = await adapter.factcheck_client.get_source_reliability_list()
    if not result.get("success"):
        raise HTTPException(status_code=500, detail=result.get("error"))
    return result.get("data")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Topic Analysis ì—”ë“œí¬ì¸íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@router.post("/topics/analyze", response_model=MCPAddonResponse)
async def analyze_topic(request: KeywordAnalysisRequest):
    """
    í‚¤ì›Œë“œ ê´€ë ¨(ë˜ëŠ” ì „ì²´) ë‰´ìŠ¤ì˜ í† í”½ì„ ë¶„ì„í•©ë‹ˆë‹¤.

    - ì£¼ìš” í‚¤ì›Œë“œ/í† í”½ íŠ¸ë Œë“œ
    - ì¹´í…Œê³ ë¦¬ ë¶„í¬
    - íƒ€ì„ë¼ì¸ ë¶„ì„
    - ê´€ë ¨ ì—”í‹°í‹°
    """
    adapter = get_mcp_adapter()
    return await adapter.analyze_topic(
        keyword=request.keyword,
        days=request.days,
        include_report=request.include_report,
    )


@router.get("/topics/trending", response_model=MCPAddonResponse)
async def get_trending_topics(
    days: int = Query(default=1, ge=1, le=7, description="ë¶„ì„ ê¸°ê°„ (ì¼)"),
    limit: int = Query(default=10, ge=1, le=50, description="ë°˜í™˜í•  í† í”½ ìˆ˜"),
):
    """
    ìµœê·¼ Nì¼ê°„ íŠ¸ë Œë”© í† í”½ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.

    ëŒ€ì‹œë³´ë“œ ìœ„ì ¯ìš© APIì…ë‹ˆë‹¤.
    """
    adapter = get_mcp_adapter()
    return await adapter.get_trending_topics(days=days, limit=limit)


@router.get("/topics/categories")
async def get_category_list():
    """
    ì§€ì›í•˜ëŠ” ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    adapter = get_mcp_adapter()
    result = await adapter.topic_client.get_category_list()
    if not result.get("success"):
        raise HTTPException(status_code=500, detail=result.get("error"))
    return result.get("data")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Sentiment Analysis ì—”ë“œí¬ì¸íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@router.post("/sentiment/analyze", response_model=MCPAddonResponse)
async def analyze_sentiment(request: KeywordAnalysisRequest):
    """
    í‚¤ì›Œë“œ ê´€ë ¨ ë‰´ìŠ¤ì˜ ê°ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤.

    - ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ë¶„í¬
    - ê°ì„± íŠ¸ë Œë“œ
    - ì–¸ë¡ ì‚¬ë³„ ê°ì„± ì°¨ì´
    """
    adapter = get_mcp_adapter()
    return await adapter.analyze_sentiment(
        keyword=request.keyword,
        days=request.days,
        include_report=request.include_report,
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# HuggingFace NLP ì—”ë“œí¬ì¸íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@router.post("/nlp/summarize", response_model=MCPAddonResponse)
async def summarize_article(request: TextAnalysisRequest):
    """
    í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.

    HuggingFace ëª¨ë¸ì„ ì‚¬ìš©í•œ abstractive summarizationì…ë‹ˆë‹¤.
    """
    adapter = get_mcp_adapter()
    return await adapter.summarize_article(
        text=request.text,
        max_length=request.max_length,
        min_length=request.min_length,
    )


@router.post("/nlp/entities", response_model=MCPAddonResponse)
async def extract_entities(text: str = Query(..., description="ë¶„ì„í•  í…ìŠ¤íŠ¸", min_length=10)):
    """
    í…ìŠ¤íŠ¸ì—ì„œ ê°œì²´ëª…(ì¸ë¬¼, ê¸°ê´€, ì¥ì†Œ ë“±)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    adapter = get_mcp_adapter()
    return await adapter.extract_entities(text)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í†µí•© ë¶„ì„ ì—”ë“œí¬ì¸íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@router.post("/analyze/comprehensive")
async def comprehensive_analysis(request: KeywordAnalysisRequest):
    """
    í‚¤ì›Œë“œì— ëŒ€í•œ ì¢…í•© ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    í¸í–¥ë„, ì‹ ë¢°ë„, í† í”½, ê°ì„± ë¶„ì„ì„ ëª¨ë‘ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ í†µí•©í•©ë‹ˆë‹¤.
    """
    adapter = get_mcp_adapter()

    # ë³‘ë ¬ë¡œ ëª¨ë“  ë¶„ì„ ì‹¤í–‰
    import asyncio

    results = await asyncio.gather(
        adapter.analyze_bias(request.keyword, request.days),
        adapter.analyze_factcheck(request.keyword, request.days),
        adapter.analyze_topic(request.keyword, request.days),
        adapter.analyze_sentiment(request.keyword, request.days),
        return_exceptions=True,
    )

    # ê²°ê³¼ í†µí•©
    analysis_results = {
        "keyword": request.keyword,
        "days": request.days,
        "bias": (
            results[0].model_dump()
            if not isinstance(results[0], Exception)
            else {"error": str(results[0])}
        ),
        "factcheck": (
            results[1].model_dump()
            if not isinstance(results[1], Exception)
            else {"error": str(results[1])}
        ),
        "topic": (
            results[2].model_dump()
            if not isinstance(results[2], Exception)
            else {"error": str(results[2])}
        ),
        "sentiment": (
            results[3].model_dump()
            if not isinstance(results[3], Exception)
            else {"error": str(results[3])}
        ),
    }

    # ì„±ê³µë¥  ê³„ì‚°
    success_count = sum(1 for r in results if not isinstance(r, Exception) and r.success)

    return {
        "success": success_count > 0,
        "success_rate": success_count / 4,
        "results": analysis_results,
    }

```

---

## backend/autonomous-crawler-service/src/metrics/__init__.py

```py
"""Prometheus metrics for autonomous-crawler-service."""

from prometheus_client import Counter, Gauge, Histogram, Info

# Service info
SERVICE_INFO = Info("crawler_service", "Autonomous crawler service information")

# ========================================
# Task metrics
# ========================================

TASKS_RECEIVED = Counter(
    "crawler_tasks_received_total",
    "Total number of crawl tasks received from Kafka",
    ["policy"],
)

TASKS_COMPLETED = Counter(
    "crawler_tasks_completed_total",
    "Total number of crawl tasks completed",
    ["policy", "status"],
)

TASKS_IN_PROGRESS = Gauge(
    "crawler_tasks_in_progress",
    "Number of crawl tasks currently in progress",
)

# API-based task metrics (browser-agent compatibility)
API_CRAWL_TASKS = Counter(
    "crawler_api_tasks_total",
    "Total API-based crawl tasks",
    ["status", "llm_provider"],
)

# ========================================
# Article extraction metrics
# ========================================

ARTICLES_EXTRACTED = Counter(
    "crawler_articles_extracted_total",
    "Total number of articles extracted",
    ["source_id"],
)

PAGES_VISITED = Counter(
    "crawler_pages_visited_total",
    "Total number of pages visited",
    ["domain"],
)

URLS_DISCOVERED = Counter(
    "crawler_urls_discovered_total",
    "Total URLs discovered by agent",
    ["category"],
)

# ========================================
# Performance metrics
# ========================================

TASK_DURATION = Histogram(
    "crawler_task_duration_seconds",
    "Time spent processing a crawl task",
    ["policy"],
    buckets=[10, 30, 60, 120, 300, 600],
)

API_CRAWL_DURATION = Histogram(
    "crawler_api_crawl_duration_seconds",
    "API crawl task duration",
    ["status"],
    buckets=[5.0, 10.0, 30.0, 60.0, 120.0, 300.0, 600.0],
)

EXTRACTION_DURATION = Histogram(
    "crawler_extraction_duration_seconds",
    "Time spent extracting content from a single page",
    buckets=[1, 2, 5, 10, 30],
)

AGENT_STEPS = Histogram(
    "crawler_agent_steps",
    "Number of steps per agent task",
    ["status"],
    buckets=[1, 2, 5, 10, 20, 30, 50],
)

# ========================================
# Browser metrics
# ========================================

BROWSER_SESSIONS_ACTIVE = Gauge(
    "crawler_browser_sessions_active",
    "Number of active browser sessions",
)

BROWSER_ERRORS = Counter(
    "crawler_browser_errors_total",
    "Total number of browser errors",
    ["error_type"],
)

# ========================================
# CAPTCHA metrics
# ========================================

CAPTCHA_DETECTED = Counter(
    "crawler_captcha_detected_total",
    "Total number of CAPTCHAs detected",
    ["type"],
)

CAPTCHA_SOLVED = Counter(
    "crawler_captcha_solved_total",
    "Total number of CAPTCHAs successfully solved",
    ["type", "method"],
)

CAPTCHA_FAILED = Counter(
    "crawler_captcha_failed_total",
    "Total number of CAPTCHA solve failures",
    ["type", "reason"],
)

# ========================================
# Kafka metrics
# ========================================

KAFKA_MESSAGES_CONSUMED = Counter(
    "crawler_kafka_messages_consumed_total",
    "Total number of Kafka messages consumed",
    ["topic"],
)

KAFKA_MESSAGES_PRODUCED = Counter(
    "crawler_kafka_messages_produced_total",
    "Total number of Kafka messages produced",
    ["topic"],
)

KAFKA_CONSUMER_LAG = Gauge(
    "crawler_kafka_consumer_lag",
    "Kafka consumer lag (messages behind)",
    ["topic", "partition"],
)

# ========================================
# SSE metrics
# ========================================

SSE_CLIENTS_CONNECTED = Gauge(
    "crawler_sse_clients_connected",
    "Number of connected SSE clients",
)

SSE_EVENTS_SENT = Counter(
    "crawler_sse_events_sent_total",
    "Total number of SSE events sent",
    ["event_type"],
)

# ========================================
# Chat metrics
# ========================================

CHAT_REQUESTS = Counter(
    "crawler_chat_requests_total",
    "Total chat requests",
    ["provider", "streaming"],
)

CHAT_TOKENS_USED = Counter(
    "crawler_chat_tokens_used_total",
    "Total tokens used in chat requests",
    ["provider"],
)


def init_service_info(version: str = "0.1.0", llm_provider: str = "openai") -> None:
    """Initialize service info metrics."""
    SERVICE_INFO.info(
        {
            "version": version,
            "llm_provider": llm_provider,
        }
    )


def track_crawl_task(status: str, llm_provider: str, duration_seconds: float, steps: int = 0):
    """Track an API crawl task completion."""
    API_CRAWL_TASKS.labels(status=status, llm_provider=llm_provider).inc()
    API_CRAWL_DURATION.labels(status=status).observe(duration_seconds)
    if steps > 0:
        AGENT_STEPS.labels(status=status).observe(steps)


def track_url_discovery(category: str, count: int = 1):
    """Track URL discovery."""
    URLS_DISCOVERED.labels(category=category).inc(count)


def track_captcha(captcha_type: str, solved: bool, method: str = "unknown", reason: str = ""):
    """Track CAPTCHA detection and resolution."""
    CAPTCHA_DETECTED.labels(type=captcha_type).inc()
    if solved:
        CAPTCHA_SOLVED.labels(type=captcha_type, method=method).inc()
    else:
        CAPTCHA_FAILED.labels(type=captcha_type, reason=reason).inc()


def track_sse_event(event_type: str):
    """Track SSE event sent."""
    SSE_EVENTS_SENT.labels(event_type=event_type).inc()


def track_chat_request(provider: str, streaming: bool, tokens: int = 0):
    """Track chat request."""
    CHAT_REQUESTS.labels(provider=provider, streaming=str(streaming).lower()).inc()
    if tokens > 0:
        CHAT_TOKENS_USED.labels(provider=provider).inc(tokens)

```

---

## backend/autonomous-crawler-service/src/ml/__init__.py

```py
"""
ML Addon Integration for autonomous-crawler-service.

í¬ë¡¤ë§ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ ML ì• ë“œì˜¨ ë¶„ì„ì„ íŠ¸ë¦¬ê±°í•˜ëŠ” ëª¨ë“ˆ.
"""

from .orchestrator import (
    MLOrchestrator,
    MLAddonType,
    MLAddonConfig,
    MLAnalysisResult,
    BatchAnalysisResult,
    get_ml_orchestrator,
    init_ml_orchestrator,
)

__all__ = [
    "MLOrchestrator",
    "MLAddonType",
    "MLAddonConfig",
    "MLAnalysisResult",
    "BatchAnalysisResult",
    "get_ml_orchestrator",
    "init_ml_orchestrator",
]

```

---

## backend/autonomous-crawler-service/src/ml/orchestrator.py

```py
"""
ML Addon Orchestrator for autonomous-crawler-service.

í¬ë¡¤ë§ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ ML ì• ë“œì˜¨ ë¶„ì„ì„ íŠ¸ë¦¬ê±°í•˜ëŠ” ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°.
Sentiment, Factcheck, Bias ë¶„ì„ì„ ë³‘ë ¬ë¡œ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ DBì— ì €ì¥í•©ë‹ˆë‹¤.

Features:
- ë¹„ë™ê¸° HTTP í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•œ ML ì• ë“œì˜¨ í˜¸ì¶œ
- ë³‘ë ¬ ë¶„ì„ ì‹¤í–‰ (asyncio.gather)
- ê²°ê³¼ DB ì €ì¥ (article_analysis í…Œì´ë¸”)
- í—¬ìŠ¤ì²´í¬ ë° ì—°ê²° ìƒíƒœ ëª¨ë‹ˆí„°ë§
- ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ ë° í´ë°± ì²˜ë¦¬
"""

import os
import asyncio
import uuid
from typing import Any, Dict, List, Optional, Tuple
from datetime import datetime
from enum import Enum

import structlog
import httpx
from pydantic import BaseModel, Field

logger = structlog.get_logger(__name__)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Configuration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class MLAddonConfig:
    """ML Addon ì„œë²„ ì—°ê²° ì„¤ì •"""

    SENTIMENT_ADDON_URL = os.environ.get("SENTIMENT_ADDON_URL", "http://sentiment-addon:8100")
    FACTCHECK_ADDON_URL = os.environ.get("FACTCHECK_ADDON_URL", "http://factcheck-addon:8101")
    BIAS_ADDON_URL = os.environ.get("BIAS_ADDON_URL", "http://bias-addon:8102")

    # HTTP client settings
    TIMEOUT_SECONDS = int(os.environ.get("ML_ADDON_TIMEOUT", "60"))
    MAX_RETRIES = int(os.environ.get("ML_ADDON_MAX_RETRIES", "2"))
    RETRY_DELAY_SECONDS = float(os.environ.get("ML_ADDON_RETRY_DELAY", "1.0"))

    # Feature flags
    AUTO_ANALYSIS_ENABLED = os.environ.get("ML_AUTO_ANALYSIS_ENABLED", "true").lower() == "true"
    PARALLEL_ANALYSIS = os.environ.get("ML_PARALLEL_ANALYSIS", "true").lower() == "true"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Enums and Models
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class MLAddonType(str, Enum):
    """ML Addon íƒ€ì…"""

    SENTIMENT = "sentiment"
    FACTCHECK = "factcheck"
    BIAS = "bias"


class AddonHealthStatus(str, Enum):
    """Addon ìƒíƒœ"""

    HEALTHY = "healthy"
    UNHEALTHY = "unhealthy"
    UNKNOWN = "unknown"
    WARMING_UP = "warming_up"


class ArticleInput(BaseModel):
    """ë¶„ì„í•  ê¸°ì‚¬ ì…ë ¥"""

    id: Optional[int] = None
    title: Optional[str] = None
    content: Optional[str] = None
    url: Optional[str] = None
    source: Optional[str] = None
    published_at: Optional[str] = None


class MLAddonRequest(BaseModel):
    """ML Addon ë¶„ì„ ìš”ì²­"""

    request_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    addon_id: str
    task: str = "article_analysis"
    input_schema_version: str = "1.0"
    article: ArticleInput
    context: Optional[Dict[str, Any]] = None
    options: Optional[Dict[str, Any]] = None


class MLAnalysisResult(BaseModel):
    """ML ë¶„ì„ ê²°ê³¼"""

    addon_type: MLAddonType
    success: bool
    request_id: str
    results: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    latency_ms: int = 0
    analyzed_at: str = Field(default_factory=lambda: datetime.utcnow().isoformat())


class BatchAnalysisResult(BaseModel):
    """ë°°ì¹˜ ë¶„ì„ ê²°ê³¼"""

    article_id: int
    sentiment: Optional[MLAnalysisResult] = None
    factcheck: Optional[MLAnalysisResult] = None
    bias: Optional[MLAnalysisResult] = None
    total_latency_ms: int = 0
    success_count: int = 0
    failure_count: int = 0


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ML Addon Client
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class MLAddonClient:
    """ML Addon HTTP í´ë¼ì´ì–¸íŠ¸"""

    def __init__(
        self,
        addon_type: MLAddonType,
        base_url: str,
        timeout: float = MLAddonConfig.TIMEOUT_SECONDS,
    ):
        self.addon_type = addon_type
        self.base_url = base_url
        self.timeout = timeout
        self._client: Optional[httpx.AsyncClient] = None
        self._health_status = AddonHealthStatus.UNKNOWN

    async def _get_client(self) -> httpx.AsyncClient:
        """HTTP í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (lazy initialization)"""
        if self._client is None or self._client.is_closed:
            self._client = httpx.AsyncClient(
                base_url=self.base_url,
                timeout=httpx.Timeout(self.timeout),
                limits=httpx.Limits(max_connections=10, max_keepalive_connections=5),
            )
        return self._client

    async def close(self):
        """í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì¢…ë£Œ"""
        if self._client and not self._client.is_closed:
            await self._client.aclose()
            self._client = None

    async def health_check(self) -> Dict[str, Any]:
        """í—¬ìŠ¤ì²´í¬ ìˆ˜í–‰"""
        try:
            client = await self._get_client()
            response = await client.get("/health")

            if response.status_code == 200:
                data = response.json()
                status = data.get("status", "unknown")

                if status == "healthy":
                    self._health_status = AddonHealthStatus.HEALTHY
                elif data.get("warmup_complete") is False:
                    self._health_status = AddonHealthStatus.WARMING_UP
                else:
                    self._health_status = AddonHealthStatus.HEALTHY

                return {
                    "addon_type": self.addon_type.value,
                    "status": self._health_status.value,
                    "details": data,
                }
            else:
                self._health_status = AddonHealthStatus.UNHEALTHY
                return {
                    "addon_type": self.addon_type.value,
                    "status": AddonHealthStatus.UNHEALTHY.value,
                    "error": f"HTTP {response.status_code}",
                }

        except Exception as e:
            self._health_status = AddonHealthStatus.UNHEALTHY
            logger.warning(
                f"Health check failed for {self.addon_type.value}",
                error=str(e),
                url=self.base_url,
            )
            return {
                "addon_type": self.addon_type.value,
                "status": AddonHealthStatus.UNHEALTHY.value,
                "error": str(e),
            }

    async def analyze(
        self,
        article: ArticleInput,
        options: Optional[Dict[str, Any]] = None,
        retries: int = MLAddonConfig.MAX_RETRIES,
    ) -> MLAnalysisResult:
        """ê¸°ì‚¬ ë¶„ì„ ìˆ˜í–‰"""
        start_time = datetime.utcnow()
        request_id = str(uuid.uuid4())

        request_data = MLAddonRequest(
            request_id=request_id,
            addon_id=f"{self.addon_type.value}-addon",
            article=article,
            options=options or {},
        )

        for attempt in range(retries + 1):
            try:
                client = await self._get_client()
                response = await client.post(
                    "/analyze",
                    json=request_data.model_dump(),
                )

                latency_ms = int((datetime.utcnow() - start_time).total_seconds() * 1000)

                if response.status_code == 200:
                    data = response.json()
                    return MLAnalysisResult(
                        addon_type=self.addon_type,
                        success=data.get("status") == "success",
                        request_id=request_id,
                        results=data.get("results"),
                        error=data.get("error", {}).get("message") if data.get("error") else None,
                        latency_ms=latency_ms,
                    )
                else:
                    error_msg = f"HTTP {response.status_code}: {response.text[:200]}"
                    if attempt < retries:
                        logger.warning(
                            f"Retry {attempt + 1}/{retries} for {self.addon_type.value}",
                            error=error_msg,
                        )
                        await asyncio.sleep(MLAddonConfig.RETRY_DELAY_SECONDS)
                        continue

                    return MLAnalysisResult(
                        addon_type=self.addon_type,
                        success=False,
                        request_id=request_id,
                        error=error_msg,
                        latency_ms=latency_ms,
                    )

            except httpx.TimeoutException as e:
                latency_ms = int((datetime.utcnow() - start_time).total_seconds() * 1000)
                if attempt < retries:
                    logger.warning(
                        f"Timeout retry {attempt + 1}/{retries} for {self.addon_type.value}",
                        error=str(e),
                    )
                    await asyncio.sleep(MLAddonConfig.RETRY_DELAY_SECONDS)
                    continue

                return MLAnalysisResult(
                    addon_type=self.addon_type,
                    success=False,
                    request_id=request_id,
                    error=f"Timeout after {self.timeout}s",
                    latency_ms=latency_ms,
                )

            except Exception as e:
                latency_ms = int((datetime.utcnow() - start_time).total_seconds() * 1000)
                logger.error(
                    f"Analysis failed for {self.addon_type.value}",
                    error=str(e),
                    attempt=attempt + 1,
                )

                if attempt < retries:
                    await asyncio.sleep(MLAddonConfig.RETRY_DELAY_SECONDS)
                    continue

                return MLAnalysisResult(
                    addon_type=self.addon_type,
                    success=False,
                    request_id=request_id,
                    error=str(e),
                    latency_ms=latency_ms,
                )

        # Should not reach here
        return MLAnalysisResult(
            addon_type=self.addon_type,
            success=False,
            request_id=request_id,
            error="Max retries exceeded",
            latency_ms=0,
        )

    @property
    def is_healthy(self) -> bool:
        """Addonì´ ì •ìƒ ìƒíƒœì¸ì§€ í™•ì¸"""
        return self._health_status in [
            AddonHealthStatus.HEALTHY,
            AddonHealthStatus.WARMING_UP,
        ]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ML Orchestrator
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class MLOrchestrator:
    """
    ML Addon ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°.

    í¬ë¡¤ë§ëœ ê¸°ì‚¬ì— ëŒ€í•´ ìë™ìœ¼ë¡œ ML ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³ 
    ê²°ê³¼ë¥¼ DBì— ì €ì¥í•©ë‹ˆë‹¤.
    """

    def __init__(self, db_pool=None):
        """
        Args:
            db_pool: PostgreSQL ì—°ê²° í’€ (asyncpg)
        """
        self.db_pool = db_pool

        # Initialize addon clients
        self.sentiment_client = MLAddonClient(
            MLAddonType.SENTIMENT, MLAddonConfig.SENTIMENT_ADDON_URL
        )
        self.factcheck_client = MLAddonClient(
            MLAddonType.FACTCHECK, MLAddonConfig.FACTCHECK_ADDON_URL
        )
        self.bias_client = MLAddonClient(MLAddonType.BIAS, MLAddonConfig.BIAS_ADDON_URL)

        self._clients = {
            MLAddonType.SENTIMENT: self.sentiment_client,
            MLAddonType.FACTCHECK: self.factcheck_client,
            MLAddonType.BIAS: self.bias_client,
        }

        self._initialized = False

    async def initialize(self):
        """ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì´ˆê¸°í™” ë° í—¬ìŠ¤ì²´í¬"""
        if self._initialized:
            return

        logger.info("Initializing ML Orchestrator...")

        # Perform health checks
        health_results = await self.check_all_health()

        healthy_count = sum(
            1 for r in health_results.values() if r.get("status") in ["healthy", "warming_up"]
        )

        logger.info(
            f"ML Orchestrator initialized",
            healthy_addons=healthy_count,
            total_addons=len(self._clients),
            auto_analysis_enabled=MLAddonConfig.AUTO_ANALYSIS_ENABLED,
        )

        self._initialized = True

    async def close(self):
        """ëª¨ë“  í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì¢…ë£Œ"""
        for client in self._clients.values():
            await client.close()

    async def check_all_health(self) -> Dict[str, Dict[str, Any]]:
        """ëª¨ë“  ML Addon í—¬ìŠ¤ì²´í¬"""
        results = {}

        if MLAddonConfig.PARALLEL_ANALYSIS:
            health_tasks = [
                (addon_type, client.health_check()) for addon_type, client in self._clients.items()
            ]
            health_results = await asyncio.gather(
                *[task[1] for task in health_tasks], return_exceptions=True
            )

            for i, (addon_type, _) in enumerate(health_tasks):
                result = health_results[i]
                if isinstance(result, Exception):
                    results[addon_type.value] = {
                        "status": "error",
                        "error": str(result),
                    }
                else:
                    results[addon_type.value] = result
        else:
            for addon_type, client in self._clients.items():
                try:
                    results[addon_type.value] = await client.health_check()
                except Exception as e:
                    results[addon_type.value] = {
                        "status": "error",
                        "error": str(e),
                    }

        return results

    async def analyze_article(
        self,
        article_id: int,
        title: str,
        content: str,
        source: Optional[str] = None,
        url: Optional[str] = None,
        published_at: Optional[str] = None,
        addon_types: Optional[List[MLAddonType]] = None,
        save_to_db: bool = True,
    ) -> BatchAnalysisResult:
        """
        ê¸°ì‚¬ì— ëŒ€í•´ ML ë¶„ì„ ìˆ˜í–‰.

        Args:
            article_id: ê¸°ì‚¬ ID
            title: ê¸°ì‚¬ ì œëª©
            content: ê¸°ì‚¬ ë³¸ë¬¸
            source: ì–¸ë¡ ì‚¬
            url: ê¸°ì‚¬ URL
            published_at: ë°œí–‰ì¼
            addon_types: ì‹¤í–‰í•  ì• ë“œì˜¨ íƒ€ì… ëª©ë¡ (Noneì´ë©´ ëª¨ë‘ ì‹¤í–‰)
            save_to_db: ê²°ê³¼ë¥¼ DBì— ì €ì¥í• ì§€ ì—¬ë¶€

        Returns:
            BatchAnalysisResult: ë¶„ì„ ê²°ê³¼
        """
        if not MLAddonConfig.AUTO_ANALYSIS_ENABLED:
            logger.debug("ML auto-analysis is disabled")
            return BatchAnalysisResult(
                article_id=article_id,
                success_count=0,
                failure_count=0,
            )

        start_time = datetime.utcnow()

        # Prepare article input
        article = ArticleInput(
            id=article_id,
            title=title,
            content=content,
            source=source,
            url=url,
            published_at=published_at,
        )

        # Determine which addons to run
        if addon_types is None:
            addon_types = list(MLAddonType)

        # Filter to only healthy addons
        active_clients = {
            addon_type: self._clients[addon_type]
            for addon_type in addon_types
            if addon_type in self._clients
        }

        if not active_clients:
            logger.warning("No ML addons available for analysis")
            return BatchAnalysisResult(
                article_id=article_id,
                success_count=0,
                failure_count=len(addon_types),
            )

        # Execute analysis
        results: Dict[MLAddonType, MLAnalysisResult] = {}

        if MLAddonConfig.PARALLEL_ANALYSIS:
            # Parallel execution
            analysis_tasks = [
                (addon_type, client.analyze(article))
                for addon_type, client in active_clients.items()
            ]
            analysis_results = await asyncio.gather(
                *[task[1] for task in analysis_tasks], return_exceptions=True
            )

            for i, (addon_type, _) in enumerate(analysis_tasks):
                result = analysis_results[i]
                if isinstance(result, Exception):
                    results[addon_type] = MLAnalysisResult(
                        addon_type=addon_type,
                        success=False,
                        request_id="",
                        error=str(result),
                    )
                elif isinstance(result, MLAnalysisResult):
                    results[addon_type] = result
        else:
            # Sequential execution
            for addon_type, client in active_clients.items():
                try:
                    results[addon_type] = await client.analyze(article)
                except Exception as e:
                    results[addon_type] = MLAnalysisResult(
                        addon_type=addon_type,
                        success=False,
                        request_id="",
                        error=str(e),
                    )

        # Calculate totals
        total_latency = int((datetime.utcnow() - start_time).total_seconds() * 1000)
        success_count = sum(1 for r in results.values() if r.success)
        failure_count = len(results) - success_count

        batch_result = BatchAnalysisResult(
            article_id=article_id,
            sentiment=results.get(MLAddonType.SENTIMENT),
            factcheck=results.get(MLAddonType.FACTCHECK),
            bias=results.get(MLAddonType.BIAS),
            total_latency_ms=total_latency,
            success_count=success_count,
            failure_count=failure_count,
        )

        # Save to database
        if save_to_db and self.db_pool:
            await self._save_results_to_db(article_id, results)

        logger.info(
            f"ML analysis completed for article {article_id}",
            success=success_count,
            failure=failure_count,
            latency_ms=total_latency,
        )

        return batch_result

    async def analyze_batch(
        self,
        articles: List[Dict[str, Any]],
        addon_types: Optional[List[MLAddonType]] = None,
        save_to_db: bool = True,
        max_concurrent: int = 5,
    ) -> List[BatchAnalysisResult]:
        """
        ì—¬ëŸ¬ ê¸°ì‚¬ì— ëŒ€í•´ ë°°ì¹˜ ë¶„ì„ ìˆ˜í–‰.

        Args:
            articles: ê¸°ì‚¬ ëª©ë¡ (dict with id, title, content, source, url)
            addon_types: ì‹¤í–‰í•  ì• ë“œì˜¨ íƒ€ì…
            save_to_db: DB ì €ì¥ ì—¬ë¶€
            max_concurrent: ë™ì‹œ ì²˜ë¦¬ ê¸°ì‚¬ ìˆ˜

        Returns:
            List[BatchAnalysisResult]: ë¶„ì„ ê²°ê³¼ ëª©ë¡
        """
        if not articles:
            return []

        semaphore = asyncio.Semaphore(max_concurrent)

        async def analyze_with_semaphore(article: Dict[str, Any]) -> BatchAnalysisResult:
            async with semaphore:
                return await self.analyze_article(
                    article_id=article.get("id", 0),
                    title=article.get("title", ""),
                    content=article.get("content", ""),
                    source=article.get("source"),
                    url=article.get("url"),
                    published_at=article.get("published_at"),
                    addon_types=addon_types,
                    save_to_db=save_to_db,
                )

        results = await asyncio.gather(
            *[analyze_with_semaphore(a) for a in articles], return_exceptions=True
        )

        # Filter out exceptions
        valid_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(
                    f"Batch analysis failed for article",
                    article_id=articles[i].get("id"),
                    error=str(result),
                )
                valid_results.append(
                    BatchAnalysisResult(
                        article_id=articles[i].get("id", 0),
                        success_count=0,
                        failure_count=3,
                    )
                )
            else:
                valid_results.append(result)

        return valid_results

    async def _save_results_to_db(
        self,
        article_id: int,
        results: Dict[MLAddonType, MLAnalysisResult],
    ):
        """ë¶„ì„ ê²°ê³¼ë¥¼ DBì— ì €ì¥"""
        if not self.db_pool:
            logger.warning("No database pool configured, skipping DB save")
            return

        try:
            async with self.db_pool.acquire() as conn:
                for addon_type, result in results.items():
                    if not result.success:
                        continue

                    # Extract result data based on addon type
                    result_data = result.results or {}

                    if addon_type == MLAddonType.SENTIMENT:
                        sentiment_data = result_data.get("sentiment", {})
                        await conn.execute(
                            """
                            INSERT INTO article_analysis 
                                (article_id, addon_key, analysis_type, result_json, 
                                 score, confidence, created_at)
                            VALUES ($1, $2, $3, $4, $5, $6, NOW())
                            ON CONFLICT (article_id, addon_key) 
                            DO UPDATE SET 
                                result_json = EXCLUDED.result_json,
                                score = EXCLUDED.score,
                                confidence = EXCLUDED.confidence,
                                updated_at = NOW()
                            """,
                            article_id,
                            "sentiment-addon",
                            "sentiment",
                            result_data,
                            sentiment_data.get("score", 0),
                            sentiment_data.get("confidence", 0),
                        )

                    elif addon_type == MLAddonType.FACTCHECK:
                        factcheck_data = result_data.get("factcheck", {})
                        await conn.execute(
                            """
                            INSERT INTO article_analysis 
                                (article_id, addon_key, analysis_type, result_json, 
                                 score, confidence, created_at)
                            VALUES ($1, $2, $3, $4, $5, $6, NOW())
                            ON CONFLICT (article_id, addon_key) 
                            DO UPDATE SET 
                                result_json = EXCLUDED.result_json,
                                score = EXCLUDED.score,
                                confidence = EXCLUDED.confidence,
                                updated_at = NOW()
                            """,
                            article_id,
                            "factcheck-addon",
                            "factcheck",
                            result_data,
                            factcheck_data.get("overall_credibility", 0) / 100,
                            0.7,  # Default confidence for factcheck
                        )

                    elif addon_type == MLAddonType.BIAS:
                        bias_data = result_data.get("bias", {})
                        await conn.execute(
                            """
                            INSERT INTO article_analysis 
                                (article_id, addon_key, analysis_type, result_json, 
                                 score, confidence, created_at)
                            VALUES ($1, $2, $3, $4, $5, $6, NOW())
                            ON CONFLICT (article_id, addon_key) 
                            DO UPDATE SET 
                                result_json = EXCLUDED.result_json,
                                score = EXCLUDED.score,
                                confidence = EXCLUDED.confidence,
                                updated_at = NOW()
                            """,
                            article_id,
                            "bias-addon",
                            "bias",
                            result_data,
                            bias_data.get("overall_bias_score", 0),
                            bias_data.get("confidence", 0),
                        )

                logger.debug(f"Saved ML analysis results to DB for article {article_id}")

        except Exception as e:
            logger.error(
                f"Failed to save ML results to DB",
                article_id=article_id,
                error=str(e),
            )

    def get_addon_status(self) -> Dict[str, Any]:
        """í˜„ì¬ Addon ìƒíƒœ ë°˜í™˜"""
        return {
            "auto_analysis_enabled": MLAddonConfig.AUTO_ANALYSIS_ENABLED,
            "parallel_analysis": MLAddonConfig.PARALLEL_ANALYSIS,
            "addons": {
                addon_type.value: {
                    "url": client.base_url,
                    "healthy": client.is_healthy,
                    "status": client._health_status.value,
                }
                for addon_type, client in self._clients.items()
            },
        }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Singleton Instance
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_ml_orchestrator: Optional[MLOrchestrator] = None


def get_ml_orchestrator(db_pool=None) -> MLOrchestrator:
    """ML ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _ml_orchestrator
    if _ml_orchestrator is None:
        _ml_orchestrator = MLOrchestrator(db_pool=db_pool)
    elif db_pool is not None and _ml_orchestrator.db_pool is None:
        _ml_orchestrator.db_pool = db_pool
    return _ml_orchestrator


async def init_ml_orchestrator(db_pool=None) -> MLOrchestrator:
    """ML ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì´ˆê¸°í™” ë° ë°˜í™˜"""
    orchestrator = get_ml_orchestrator(db_pool)
    await orchestrator.initialize()
    return orchestrator

```

---

## backend/autonomous-crawler-service/src/ml/router.py

```py
"""
ML Analysis API Router for autonomous-crawler-service.

ML Addon ë¶„ì„ ê¸°ëŠ¥ì„ REST APIë¡œ ë…¸ì¶œí•©ë‹ˆë‹¤.
í¬ë¡¤ë§ëœ ê¸°ì‚¬ì— ëŒ€í•´ sentiment, factcheck, bias ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import os
from typing import Any, Dict, List, Optional

import structlog
from fastapi import APIRouter, HTTPException, Query, Request
from pydantic import BaseModel, Field, HttpUrl

from src.ml.orchestrator import (
    MLOrchestrator,
    MLAddonType,
    MLAddonConfig,
    MLAnalysisResult,
    BatchAnalysisResult,
    ArticleInput,
    get_ml_orchestrator,
    init_ml_orchestrator,
)

logger = structlog.get_logger(__name__)

router = APIRouter(prefix="/ml", tags=["ML Analysis"])


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Request/Response Models
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class MLAnalyzeRequest(BaseModel):
    """ML ë¶„ì„ ìš”ì²­"""

    article_id: int = Field(..., description="ê¸°ì‚¬ ID")
    title: str = Field(..., description="ê¸°ì‚¬ ì œëª©", min_length=1)
    content: str = Field(..., description="ê¸°ì‚¬ ë³¸ë¬¸", min_length=10)
    source: Optional[str] = Field(default=None, description="ì–¸ë¡ ì‚¬ëª…")
    url: Optional[str] = Field(default=None, description="ê¸°ì‚¬ URL")
    published_at: Optional[str] = Field(default=None, description="ë°œí–‰ì¼")

    # ë¶„ì„ ì˜µì…˜
    addons: Optional[List[str]] = Field(
        default=None,
        description="ì‹¤í–‰í•  ì• ë“œì˜¨ ëª©ë¡ (sentiment, factcheck, bias). Noneì´ë©´ ëª¨ë‘ ì‹¤í–‰",
    )
    save_to_db: bool = Field(default=True, description="ê²°ê³¼ë¥¼ DBì— ì €ì¥í• ì§€ ì—¬ë¶€")


class MLBatchAnalyzeRequest(BaseModel):
    """ML ë°°ì¹˜ ë¶„ì„ ìš”ì²­"""

    articles: List[MLAnalyzeRequest] = Field(..., min_length=1, max_length=50)
    addons: Optional[List[str]] = None
    save_to_db: bool = True
    max_concurrent: int = Field(default=5, ge=1, le=20)


class MLSimpleAnalyzeRequest(BaseModel):
    """ê°„ë‹¨í•œ ML ë¶„ì„ ìš”ì²­ (í…ìŠ¤íŠ¸ë§Œ)"""

    text: str = Field(..., description="ë¶„ì„í•  í…ìŠ¤íŠ¸", min_length=10)
    source: Optional[str] = Field(default=None, description="ì¶œì²˜/ì–¸ë¡ ì‚¬ëª…")
    addons: Optional[List[str]] = Field(default=None, description="ì‹¤í–‰í•  ì• ë“œì˜¨ ëª©ë¡")


class MLAddonInfo(BaseModel):
    """ML Addon ì •ë³´"""

    type: str
    url: str
    healthy: bool
    status: str


class MLStatusResponse(BaseModel):
    """ML ì‹œìŠ¤í…œ ìƒíƒœ"""

    auto_analysis_enabled: bool
    parallel_analysis: bool
    addons: Dict[str, MLAddonInfo]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Helper Functions
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def parse_addon_types(addons: Optional[List[str]]) -> Optional[List[MLAddonType]]:
    """ë¬¸ìì—´ ì• ë“œì˜¨ ëª©ë¡ì„ MLAddonType ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    if addons is None:
        return None

    addon_map = {
        "sentiment": MLAddonType.SENTIMENT,
        "factcheck": MLAddonType.FACTCHECK,
        "bias": MLAddonType.BIAS,
    }

    result = []
    for addon in addons:
        addon_type = addon_map.get(addon.lower())
        if addon_type:
            result.append(addon_type)

    return result if result else None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# API Endpoints
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@router.get("/health")
async def ml_health_check(request: Request):
    """
    ML ì‹œìŠ¤í…œ í—¬ìŠ¤ì²´í¬.

    ëª¨ë“  ML Addonì˜ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    """
    orchestrator = get_ml_orchestrator()
    health_results = await orchestrator.check_all_health()

    all_healthy = all(r.get("status") in ["healthy", "warming_up"] for r in health_results.values())

    return {
        "status": "healthy" if all_healthy else "degraded",
        "auto_analysis_enabled": MLAddonConfig.AUTO_ANALYSIS_ENABLED,
        "addons": health_results,
    }


@router.get("/status", response_model=MLStatusResponse)
async def ml_status(request: Request):
    """
    ML ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ.

    í˜„ì¬ ì„¤ì • ë° Addon ì—°ê²° ìƒíƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    orchestrator = get_ml_orchestrator()
    status = orchestrator.get_addon_status()

    return MLStatusResponse(
        auto_analysis_enabled=status["auto_analysis_enabled"],
        parallel_analysis=status["parallel_analysis"],
        addons={
            k: MLAddonInfo(
                type=k,
                url=v["url"],
                healthy=v["healthy"],
                status=v["status"],
            )
            for k, v in status["addons"].items()
        },
    )


@router.post("/analyze", response_model=BatchAnalysisResult)
async def analyze_article(
    request: MLAnalyzeRequest,
    req: Request,
):
    """
    ë‹¨ì¼ ê¸°ì‚¬ ML ë¶„ì„.

    ê¸°ì‚¬ì— ëŒ€í•´ sentiment, factcheck, bias ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    **ì‚¬ìš© ì˜ˆì‹œ:**
    ``\`json
    {
        "article_id": 12345,
        "title": "ë‰´ìŠ¤ ì œëª©",
        "content": "ë‰´ìŠ¤ ë³¸ë¬¸ ë‚´ìš©...",
        "source": "ì¡°ì„ ì¼ë³´",
        "addons": ["sentiment", "bias"]
    }
    ``\`
    """
    orchestrator = get_ml_orchestrator()
    addon_types = parse_addon_types(request.addons)

    result = await orchestrator.analyze_article(
        article_id=request.article_id,
        title=request.title,
        content=request.content,
        source=request.source,
        url=request.url,
        published_at=request.published_at,
        addon_types=addon_types,
        save_to_db=request.save_to_db,
    )

    return result


@router.post("/analyze/simple")
async def analyze_text_simple(
    request: MLSimpleAnalyzeRequest,
    req: Request,
):
    """
    ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ML ë¶„ì„.

    ê¸°ì‚¬ ID ì—†ì´ í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    ê²°ê³¼ëŠ” DBì— ì €ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

    **ì‚¬ìš© ì˜ˆì‹œ:**
    ``\`json
    {
        "text": "ë¶„ì„í•  ë‰´ìŠ¤ í…ìŠ¤íŠ¸...",
        "source": "í•œê²¨ë ˆ",
        "addons": ["sentiment"]
    }
    ``\`
    """
    orchestrator = get_ml_orchestrator()
    addon_types = parse_addon_types(request.addons)

    # ì„ì‹œ article_id ì‚¬ìš© (DB ì €ì¥ ì•ˆí•¨)
    result = await orchestrator.analyze_article(
        article_id=0,
        title="",
        content=request.text,
        source=request.source,
        addon_types=addon_types,
        save_to_db=False,
    )

    # ì‘ë‹µì—ì„œ ë¶ˆí•„ìš”í•œ í•„ë“œ ì œê±°
    return {
        "sentiment": result.sentiment.model_dump() if result.sentiment else None,
        "factcheck": result.factcheck.model_dump() if result.factcheck else None,
        "bias": result.bias.model_dump() if result.bias else None,
        "total_latency_ms": result.total_latency_ms,
        "success_count": result.success_count,
        "failure_count": result.failure_count,
    }


@router.post("/analyze/batch")
async def analyze_batch(
    request: MLBatchAnalyzeRequest,
    req: Request,
):
    """
    ë°°ì¹˜ ê¸°ì‚¬ ML ë¶„ì„.

    ì—¬ëŸ¬ ê¸°ì‚¬ë¥¼ í•œ ë²ˆì— ë¶„ì„í•©ë‹ˆë‹¤.

    **ì‚¬ìš© ì˜ˆì‹œ:**
    ``\`json
    {
        "articles": [
            {"article_id": 1, "title": "ì œëª©1", "content": "ë‚´ìš©1"},
            {"article_id": 2, "title": "ì œëª©2", "content": "ë‚´ìš©2"}
        ],
        "addons": ["sentiment", "factcheck", "bias"],
        "max_concurrent": 5
    }
    ``\`
    """
    orchestrator = get_ml_orchestrator()
    addon_types = parse_addon_types(request.addons)

    # ìš”ì²­ì„ dict ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    articles = [
        {
            "id": a.article_id,
            "title": a.title,
            "content": a.content,
            "source": a.source,
            "url": a.url,
            "published_at": a.published_at,
        }
        for a in request.articles
    ]

    results = await orchestrator.analyze_batch(
        articles=articles,
        addon_types=addon_types,
        save_to_db=request.save_to_db,
        max_concurrent=request.max_concurrent,
    )

    # í†µê³„ ê³„ì‚°
    total_success = sum(r.success_count for r in results)
    total_failure = sum(r.failure_count for r in results)

    return {
        "total_articles": len(results),
        "total_success": total_success,
        "total_failure": total_failure,
        "results": [
            {
                "article_id": r.article_id,
                "success_count": r.success_count,
                "failure_count": r.failure_count,
                "total_latency_ms": r.total_latency_ms,
            }
            for r in results
        ],
    }


@router.post("/analyze/url")
async def analyze_url(
    url: HttpUrl = Query(..., description="ë¶„ì„í•  URL"),
    req: Request = None,
):
    """
    URLì—ì„œ ê¸°ì‚¬ë¥¼ í¬ë¡¤ë§í•˜ê³  ML ë¶„ì„ ìˆ˜í–‰.

    URLì˜ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•œ í›„ sentiment, factcheck, bias ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    import httpx
    from bs4 import BeautifulSoup

    try:
        # ê°„ë‹¨í•œ URL í˜ì¹˜
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(str(url), follow_redirects=True)
            response.raise_for_status()

        # HTML íŒŒì‹±
        soup = BeautifulSoup(response.text, "html.parser")

        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for tag in soup(["script", "style", "nav", "footer", "header", "aside", "ad"]):
            tag.decompose()

        # ì œëª©ê³¼ ë³¸ë¬¸ ì¶”ì¶œ
        title = soup.title.string if soup.title else ""
        content = soup.get_text(separator="\n", strip=True)

        # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ì—ëŸ¬
        if len(content) < 100:
            raise HTTPException(
                status_code=400,
                detail="í˜ì´ì§€ì—ì„œ ì¶©ë¶„í•œ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            )

        # ML ë¶„ì„ ìˆ˜í–‰
        orchestrator = get_ml_orchestrator()
        result = await orchestrator.analyze_article(
            article_id=0,
            title=title[:500] if title else "",
            content=content[:10000],  # ìµœëŒ€ 10000ì
            url=str(url),
            save_to_db=False,
        )

        return {
            "url": str(url),
            "title": title[:200] if title else None,
            "content_length": len(content),
            "sentiment": result.sentiment.model_dump() if result.sentiment else None,
            "factcheck": result.factcheck.model_dump() if result.factcheck else None,
            "bias": result.bias.model_dump() if result.bias else None,
            "total_latency_ms": result.total_latency_ms,
        }

    except httpx.HTTPError as e:
        raise HTTPException(status_code=400, detail=f"URL ì ‘ê·¼ ì‹¤íŒ¨: {str(e)}")
    except Exception as e:
        logger.error("URL analysis failed", url=str(url), error=str(e))
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ì‹¤íŒ¨: {str(e)}")


@router.get("/addons")
async def list_addons():
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ ML Addon ëª©ë¡.

    ê° ì• ë“œì˜¨ì˜ ê¸°ëŠ¥ê³¼ í˜„ì¬ ìƒíƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    orchestrator = get_ml_orchestrator()
    health_results = await orchestrator.check_all_health()

    addons = [
        {
            "key": "sentiment",
            "name": "ê°ì„± ë¶„ì„ (Sentiment Analysis)",
            "description": "ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ê°ì •(ê¸ì •/ë¶€ì •/ì¤‘ë¦½)ì„ ë¶„ì„í•©ë‹ˆë‹¤. KoELECTRA ê¸°ë°˜ ML ëª¨ë¸ ì‚¬ìš©.",
            "endpoint": MLAddonConfig.SENTIMENT_ADDON_URL,
            "status": health_results.get("sentiment", {}).get("status", "unknown"),
            "features": ["sentiment_score", "emotion_detection", "tone_analysis"],
        },
        {
            "key": "factcheck",
            "name": "íŒ©íŠ¸ì²´í¬ (Fact-Check Analysis)",
            "description": "ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì‚¬ì‹¤ì„±ê³¼ ì‹ ë¢°ë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. ì£¼ì¥ ì¶”ì¶œ, í´ë¦­ë² ì´íŠ¸ íƒì§€, í—ˆìœ„ì •ë³´ ìœ„í—˜ë„ í‰ê°€.",
            "endpoint": MLAddonConfig.FACTCHECK_ADDON_URL,
            "status": health_results.get("factcheck", {}).get("status", "unknown"),
            "features": [
                "claim_extraction",
                "credibility_score",
                "clickbait_detection",
                "misinformation_risk",
            ],
        },
        {
            "key": "bias",
            "name": "í¸í–¥ ë¶„ì„ (Bias Analysis)",
            "description": "ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì •ì¹˜ì /ì´ë…ì  í¸í–¥ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤. ì–¸ë¡ ì‚¬ ì„±í–¥, í‚¤ì›Œë“œ ê¸°ë°˜, í”„ë ˆì´ë° ë¶„ì„.",
            "endpoint": MLAddonConfig.BIAS_ADDON_URL,
            "status": health_results.get("bias", {}).get("status", "unknown"),
            "features": [
                "political_lean",
                "source_bias",
                "framing_analysis",
                "objectivity_score",
            ],
        },
    ]

    return {
        "addons": addons,
        "total": len(addons),
        "auto_analysis_enabled": MLAddonConfig.AUTO_ANALYSIS_ENABLED,
    }


@router.post("/config/toggle")
async def toggle_auto_analysis(
    enabled: bool = Query(..., description="ìë™ ë¶„ì„ í™œì„±í™” ì—¬ë¶€"),
):
    """
    ìë™ ML ë¶„ì„ í† ê¸€.

    í¬ë¡¤ë§ í›„ ìë™ ML ë¶„ì„ ê¸°ëŠ¥ì„ í™œì„±í™”/ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.
    (ëŸ°íƒ€ì„ì—ë§Œ ì ìš©, í™˜ê²½ë³€ìˆ˜ ì„¤ì •ì€ ë³€ê²½ë˜ì§€ ì•ŠìŒ)
    """
    # Note: ì´ ì„¤ì •ì€ ëŸ°íƒ€ì„ì—ë§Œ ì ìš©ë¨
    # ì˜êµ¬ì ì¸ ë³€ê²½ì„ ìœ„í•´ì„œëŠ” í™˜ê²½ë³€ìˆ˜ ML_AUTO_ANALYSIS_ENABLED ìˆ˜ì • í•„ìš”
    MLAddonConfig.AUTO_ANALYSIS_ENABLED = enabled

    logger.info(f"ML auto-analysis toggled", enabled=enabled)

    return {
        "status": "ok",
        "auto_analysis_enabled": MLAddonConfig.AUTO_ANALYSIS_ENABLED,
        "message": f"ìë™ ML ë¶„ì„ì´ {'í™œì„±í™”' if enabled else 'ë¹„í™œì„±í™”'}ë˜ì—ˆìŠµë‹ˆë‹¤.",
    }

```

---

## backend/autonomous-crawler-service/src/search/__init__.py

```py
"""Search providers package with RRF-based multi-strategy search."""

from src.search.base import SearchResult, SearchProvider
from src.search.brave import BraveSearchProvider
from src.search.tavily import TavilySearchProvider
from src.search.perplexity import PerplexitySearchProvider
from src.search.orchestrator import (
    ParallelSearchOrchestrator,
    RRFSearchOrchestrator,
    RRFSearchResult,
    AggregatedSearchResult,
    create_rrf_orchestrator,
)
from src.search.rrf import (
    ReciprocalRankFusion,
    SemanticRRF,
    RRFConfig,
    RRFResult,
    RRFMergeResult,
    create_rrf_merger,
    create_semantic_rrf_merger,
)
from src.search.query_analyzer import (
    QueryAnalyzer,
    QueryAnalysis,
    MultiStrategyQueryExpander,
)

__all__ = [
    # Base classes
    "SearchResult",
    "SearchProvider",
    # Providers
    "BraveSearchProvider",
    "TavilySearchProvider",
    "PerplexitySearchProvider",
    # Orchestrators
    "ParallelSearchOrchestrator",
    "RRFSearchOrchestrator",
    "RRFSearchResult",
    "AggregatedSearchResult",
    "create_rrf_orchestrator",
    # RRF algorithm
    "ReciprocalRankFusion",
    "SemanticRRF",
    "RRFConfig",
    "RRFResult",
    "RRFMergeResult",
    "create_rrf_merger",
    "create_semantic_rrf_merger",
    # Query analysis
    "QueryAnalyzer",
    "QueryAnalysis",
    "MultiStrategyQueryExpander",
]

```

---

## backend/autonomous-crawler-service/src/search/base.py

```py
"""Base classes for search providers."""

from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any


@dataclass
class SearchResult:
    """Unified search result structure."""
    
    title: str
    url: str
    snippet: str
    source_provider: str  # brave, tavily, perplexity, browser
    
    # Optional fields
    published_date: str | None = None
    score: float | None = None
    raw_data: dict[str, Any] = field(default_factory=dict)
    
    # Metadata
    fetched_at: datetime = field(default_factory=datetime.now)
    
    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary."""
        return {
            "title": self.title,
            "url": self.url,
            "snippet": self.snippet,
            "source_provider": self.source_provider,
            "published_date": self.published_date,
            "score": self.score,
            "fetched_at": self.fetched_at.isoformat(),
        }


class SearchProvider(ABC):
    """Abstract base class for search providers."""
    
    @property
    @abstractmethod
    def name(self) -> str:
        """Provider name."""
        pass
    
    @abstractmethod
    async def search(
        self,
        query: str,
        max_results: int = 10,
        **kwargs,
    ) -> list[SearchResult]:
        """
        Execute search query.
        
        Args:
            query: Search query string
            max_results: Maximum number of results to return
            **kwargs: Provider-specific options
            
        Returns:
            List of SearchResult objects
        """
        pass
    
    @abstractmethod
    async def health_check(self) -> bool:
        """Check if the provider is healthy and accessible."""
        pass

```

---

## backend/autonomous-crawler-service/src/search/brave.py

```py
"""Brave Search API provider."""

import httpx
import structlog
from typing import Any

from src.search.base import SearchProvider, SearchResult

logger = structlog.get_logger(__name__)


class BraveSearchProvider(SearchProvider):
    """
    Brave Search API client.
    
    Docs: https://api.search.brave.com/app/documentation/web-search/get-started
    """
    
    BASE_URL = "https://api.search.brave.com/res/v1"
    
    def __init__(self, api_key: str, timeout: float = 30.0):
        self.api_key = api_key
        self.timeout = timeout
        self._client: httpx.AsyncClient | None = None
    
    @property
    def name(self) -> str:
        return "brave"
    
    async def _get_client(self) -> httpx.AsyncClient:
        """Get or create HTTP client."""
        if self._client is None or self._client.is_closed:
            self._client = httpx.AsyncClient(
                timeout=self.timeout,
                headers={
                    "Accept": "application/json",
                    "Accept-Encoding": "gzip",
                    "X-Subscription-Token": self.api_key,
                },
            )
        return self._client
    
    async def close(self) -> None:
        """Close the HTTP client."""
        if self._client and not self._client.is_closed:
            await self._client.aclose()
            self._client = None
    
    async def search(
        self,
        query: str,
        max_results: int = 10,
        country: str = "kr",
        search_lang: str = "ko",
        ui_lang: str = "ko-KR",
        freshness: str | None = None,  # pd (past day), pw (past week), pm (past month)
        **kwargs,
    ) -> list[SearchResult]:
        """
        Execute Brave web search.
        
        Args:
            query: Search query
            max_results: Max results (1-20 for free tier)
            country: Country code
            search_lang: Search language
            ui_lang: UI language
            freshness: Time filter (pd, pw, pm, py)
        """
        client = await self._get_client()
        
        params: dict[str, Any] = {
            "q": query,
            "count": min(max_results, 20),  # Brave max is 20
            "country": country,
            "search_lang": search_lang,
            "ui_lang": ui_lang,
        }
        
        if freshness:
            params["freshness"] = freshness
        
        try:
            response = await client.get(
                f"{self.BASE_URL}/web/search",
                params=params,
            )
            response.raise_for_status()
            data = response.json()
            
            results: list[SearchResult] = []
            
            # Parse web results
            web_results = data.get("web", {}).get("results", [])
            for item in web_results[:max_results]:
                results.append(SearchResult(
                    title=item.get("title", ""),
                    url=item.get("url", ""),
                    snippet=item.get("description", ""),
                    source_provider=self.name,
                    published_date=item.get("age"),  # e.g., "2 hours ago"
                    score=item.get("relevancy_score"),
                    raw_data=item,
                ))
            
            logger.info(
                "Brave search completed",
                query=query,
                results_count=len(results),
            )
            
            return results
            
        except httpx.HTTPStatusError as e:
            logger.error(
                "Brave search HTTP error",
                query=query,
                status_code=e.response.status_code,
                error=str(e),
            )
            return []
        except Exception as e:
            logger.error(
                "Brave search failed",
                query=query,
                error=str(e),
            )
            return []
    
    async def search_news(
        self,
        query: str,
        max_results: int = 10,
        country: str = "kr",
        **kwargs,
    ) -> list[SearchResult]:
        """Search news specifically."""
        client = await self._get_client()
        
        params = {
            "q": query,
            "count": min(max_results, 20),
            "country": country,
            "freshness": "pw",  # Past week for news
        }
        
        try:
            response = await client.get(
                f"{self.BASE_URL}/news/search",
                params=params,
            )
            response.raise_for_status()
            data = response.json()
            
            results: list[SearchResult] = []
            news_results = data.get("results", [])
            
            for item in news_results[:max_results]:
                results.append(SearchResult(
                    title=item.get("title", ""),
                    url=item.get("url", ""),
                    snippet=item.get("description", ""),
                    source_provider=f"{self.name}_news",
                    published_date=item.get("age"),
                    raw_data=item,
                ))
            
            return results
            
        except Exception as e:
            logger.error("Brave news search failed", query=query, error=str(e))
            return []
    
    async def health_check(self) -> bool:
        """Check API health."""
        try:
            results = await self.search("test", max_results=1)
            return len(results) > 0
        except Exception:
            return False

```

---

## backend/autonomous-crawler-service/src/search/orchestrator.py

```py
"""Parallel search orchestrator for multiple providers."""

import asyncio
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Optional
from urllib.parse import urlparse

import structlog

from src.search.base import SearchProvider, SearchResult
from src.search.rrf import (
    ReciprocalRankFusion,
    SemanticRRF,
    RRFConfig,
    RRFMergeResult,
)

logger = structlog.get_logger(__name__)


@dataclass
class AggregatedSearchResult:
    """Aggregated results from multiple search providers."""

    query: str
    results: list[SearchResult]
    providers_used: list[str]
    providers_failed: list[str]
    total_results: int
    unique_urls: int
    search_time_ms: float
    timestamp: datetime = field(default_factory=datetime.now)

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary."""
        return {
            "query": self.query,
            "results": [r.to_dict() for r in self.results],
            "providers_used": self.providers_used,
            "providers_failed": self.providers_failed,
            "total_results": self.total_results,
            "unique_urls": self.unique_urls,
            "search_time_ms": self.search_time_ms,
            "timestamp": self.timestamp.isoformat(),
        }


class ParallelSearchOrchestrator:
    """
    Orchestrates parallel searches across multiple providers.

    Features:
    - Parallel execution for speed
    - Deduplication by URL
    - Result ranking and merging
    - Provider health tracking
    - Fallback strategies
    """

    def __init__(
        self,
        providers: list[SearchProvider],
        timeout: float = 30.0,
        deduplicate: bool = True,
    ):
        """
        Initialize orchestrator.

        Args:
            providers: List of search providers to use
            timeout: Timeout for each provider search
            deduplicate: Whether to deduplicate results by URL
        """
        self.providers = providers
        self.timeout = timeout
        self.deduplicate = deduplicate
        self._provider_health: dict[str, bool] = {}

    async def search(
        self,
        query: str,
        max_results_per_provider: int = 10,
        max_total_results: int = 30,
        **kwargs,
    ) -> AggregatedSearchResult:
        """
        Execute parallel search across all providers.

        Args:
            query: Search query
            max_results_per_provider: Max results from each provider
            max_total_results: Max total results after aggregation
            **kwargs: Provider-specific options

        Returns:
            AggregatedSearchResult with merged, deduplicated results
        """
        start_time = datetime.now()

        # Create tasks for all providers
        tasks = []
        provider_names = []

        for provider in self.providers:
            task = asyncio.create_task(
                self._search_with_timeout(
                    provider,
                    query,
                    max_results_per_provider,
                    **kwargs,
                )
            )
            tasks.append(task)
            provider_names.append(provider.name)

        # Wait for all tasks to complete
        results_by_provider = await asyncio.gather(*tasks, return_exceptions=True)

        # Process results
        all_results: list[SearchResult] = []
        providers_used: list[str] = []
        providers_failed: list[str] = []

        for provider_name, result in zip(provider_names, results_by_provider):
            if isinstance(result, Exception):
                logger.error(
                    "Provider search failed",
                    provider=provider_name,
                    error=str(result),
                )
                providers_failed.append(provider_name)
                self._provider_health[provider_name] = False
            elif isinstance(result, list):
                all_results.extend(result)
                if result:
                    providers_used.append(provider_name)
                    self._provider_health[provider_name] = True
                else:
                    # Empty results but no error
                    providers_used.append(provider_name)

        # Deduplicate by URL
        if self.deduplicate:
            all_results = self._deduplicate_results(all_results)

        # Rank and limit results
        all_results = self._rank_results(all_results)[:max_total_results]

        # Calculate search time
        search_time_ms = (datetime.now() - start_time).total_seconds() * 1000

        logger.info(
            "Parallel search completed",
            query=query,
            total_results=len(all_results),
            providers_used=providers_used,
            providers_failed=providers_failed,
            search_time_ms=search_time_ms,
        )

        return AggregatedSearchResult(
            query=query,
            results=all_results,
            providers_used=providers_used,
            providers_failed=providers_failed,
            total_results=len(all_results),
            unique_urls=len(set(r.url for r in all_results)),
            search_time_ms=search_time_ms,
        )

    async def _search_with_timeout(
        self,
        provider: SearchProvider,
        query: str,
        max_results: int,
        **kwargs,
    ) -> list[SearchResult]:
        """Execute provider search with timeout."""
        try:
            return await asyncio.wait_for(
                provider.search(query, max_results, **kwargs),
                timeout=self.timeout,
            )
        except asyncio.TimeoutError:
            logger.warning(
                "Provider search timed out",
                provider=provider.name,
                timeout=self.timeout,
            )
            raise

    def _deduplicate_results(
        self,
        results: list[SearchResult],
    ) -> list[SearchResult]:
        """
        Deduplicate results by URL, keeping the first occurrence.

        Also merges information from duplicate entries.
        """
        seen_urls: dict[str, SearchResult] = {}

        for result in results:
            # Normalize URL for comparison
            normalized_url = self._normalize_url(result.url)

            if normalized_url not in seen_urls:
                seen_urls[normalized_url] = result
            else:
                # Merge: keep the one with more information
                existing = seen_urls[normalized_url]
                if len(result.snippet) > len(existing.snippet):
                    # Keep longer snippet
                    result.raw_data["merged_from"] = existing.source_provider
                    seen_urls[normalized_url] = result

        return list(seen_urls.values())

    def _normalize_url(self, url: str) -> str:
        """Normalize URL for deduplication."""
        try:
            parsed = urlparse(url)
            # Remove www. prefix and trailing slash
            netloc = parsed.netloc.lower().lstrip("www.")
            path = parsed.path.rstrip("/")
            return f"{netloc}{path}"
        except Exception:
            return url.lower()

    def _rank_results(self, results: list[SearchResult]) -> list[SearchResult]:
        """
        Rank results by relevance.

        Scoring factors:
        - Provider reliability
        - Relevance score (if available)
        - Content length
        - Recency (if date available)
        """

        def score_result(result: SearchResult) -> float:
            score = 0.0

            # Provider weight
            provider_weights = {
                "tavily": 1.0,
                "brave": 0.9,
                "perplexity": 0.85,
                "brave_news": 0.95,
            }
            score += provider_weights.get(result.source_provider, 0.5) * 10

            # Relevance score if available
            if result.score:
                score += result.score * 5

            # Content length (prefer informative snippets)
            score += min(len(result.snippet) / 100, 5)

            # Title quality
            if result.title and len(result.title) > 10:
                score += 2

            return score

        return sorted(results, key=score_result, reverse=True)

    async def search_news(
        self,
        query: str,
        max_results_per_provider: int = 10,
        days: int = 7,
        **kwargs,
    ) -> AggregatedSearchResult:
        """Search for news specifically."""
        # Add news-specific parameters
        kwargs["topic"] = "news"
        kwargs["days"] = days
        kwargs["freshness"] = "pw"  # Past week for Brave
        kwargs["search_recency_filter"] = "week"  # For Perplexity

        return await self.search(
            query=query,
            max_results_per_provider=max_results_per_provider,
            **kwargs,
        )

    async def health_check_all(self) -> dict[str, bool]:
        """Check health of all providers."""
        tasks = [(provider.name, provider.health_check()) for provider in self.providers]

        results = {}
        for name, task in tasks:
            try:
                results[name] = await asyncio.wait_for(task, timeout=10.0)
            except Exception:
                results[name] = False

        self._provider_health = results
        return results

    def get_healthy_providers(self) -> list[str]:
        """Get list of healthy provider names."""
        return [name for name, healthy in self._provider_health.items() if healthy]

    async def close_all(self) -> None:
        """Close all provider connections."""
        for provider in self.providers:
            if hasattr(provider, "close"):
                await provider.close()


@dataclass
class RRFSearchResult:
    """Result from RRF-based multi-strategy search."""

    original_query: str
    results: list[SearchResult]
    strategies_used: list[str]
    providers_used: list[str]
    total_results: int
    unique_urls: int
    search_time_ms: float
    rrf_merge_result: Optional[RRFMergeResult] = None
    query_analysis: Optional[dict[str, Any]] = None
    timestamp: datetime = field(default_factory=datetime.now)

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary."""
        return {
            "original_query": self.original_query,
            "results": [r.to_dict() for r in self.results],
            "strategies_used": self.strategies_used,
            "providers_used": self.providers_used,
            "total_results": self.total_results,
            "unique_urls": self.unique_urls,
            "search_time_ms": self.search_time_ms,
            "query_analysis": self.query_analysis,
            "timestamp": self.timestamp.isoformat(),
        }


class RRFSearchOrchestrator:
    """
    Advanced search orchestrator using RRF (Reciprocal Rank Fusion)
    to combine results from multiple search strategies.

    Features:
    - Query analysis and expansion using LLM
    - Multi-strategy parallel search
    - RRF-based result fusion
    - Semantic relevance scoring
    - Provider failover and health tracking

    Strategies:
    1. Original query search
    2. Keyword-extracted search
    3. Semantically expanded queries
    4. Cross-lingual search (for non-English queries)
    """

    def __init__(
        self,
        providers: list[SearchProvider],
        query_analyzer: Optional[Any] = None,  # QueryAnalyzer
        timeout: float = 30.0,
        rrf_config: Optional[RRFConfig] = None,
        enable_semantic_rrf: bool = True,
    ):
        """
        Initialize RRF Search Orchestrator.

        Args:
            providers: List of search providers
            query_analyzer: Optional QueryAnalyzer for query expansion
            timeout: Timeout per provider search
            rrf_config: RRF algorithm configuration
            enable_semantic_rrf: Use semantic similarity in RRF scoring
        """
        self.providers = providers
        self.query_analyzer = query_analyzer
        self.timeout = timeout
        self.rrf_config = rrf_config or RRFConfig()
        self._provider_health: dict[str, bool] = {}

        # Initialize RRF merger
        if enable_semantic_rrf:
            self._rrf_merger = SemanticRRF(self.rrf_config)
        else:
            self._rrf_merger = ReciprocalRankFusion(self.rrf_config)

    async def search_with_rrf(
        self,
        query: str,
        max_results_per_strategy: int = 10,
        max_total_results: int = 30,
        enable_query_expansion: bool = True,
        context: Optional[str] = None,
        **kwargs,
    ) -> RRFSearchResult:
        """
        Execute multi-strategy search with RRF fusion.

        Args:
            query: Original search query
            max_results_per_strategy: Max results per search strategy
            max_total_results: Max final results after RRF fusion
            enable_query_expansion: Whether to expand query using analyzer
            context: Optional context for query analysis
            **kwargs: Additional provider-specific options

        Returns:
            RRFSearchResult with fused results
        """
        start_time = datetime.now()

        # Step 1: Analyze and expand query
        search_queries = [query]  # Always include original
        query_analysis_dict = None

        if enable_query_expansion and self.query_analyzer:
            try:
                analysis = await self.query_analyzer.analyze(query, context)
                search_queries = analysis.get_all_search_queries()[:5]  # Limit strategies
                query_analysis_dict = {
                    "intent": analysis.intent,
                    "language": analysis.language,
                    "keywords": analysis.keywords,
                    "expanded_queries": analysis.expanded_queries,
                    "confidence": analysis.confidence,
                }
                logger.info(
                    "Query expanded for RRF search",
                    original=query,
                    expanded_count=len(search_queries),
                    intent=analysis.intent,
                )
            except Exception as e:
                logger.warning("Query expansion failed, using original", error=str(e))

        # Step 2: Execute parallel searches for each strategy
        ranked_lists: list[tuple[str, list[SearchResult]]] = []
        all_providers_used: set[str] = set()

        # Create search tasks for each query variant
        async def search_single_query(q: str, strategy_name: str) -> tuple[str, list[SearchResult]]:
            """Execute search for a single query across all providers."""
            results: list[SearchResult] = []

            tasks = [
                self._search_provider_with_timeout(provider, q, max_results_per_strategy, **kwargs)
                for provider in self.providers
            ]

            provider_results = await asyncio.gather(*tasks, return_exceptions=True)

            for provider, result in zip(self.providers, provider_results):
                if isinstance(result, Exception):
                    logger.debug(
                        "Provider search failed for strategy",
                        provider=provider.name,
                        strategy=strategy_name,
                        error=str(result),
                    )
                elif isinstance(result, list) and result:
                    results.extend(result)
                    all_providers_used.add(provider.name)

            return (strategy_name, results)

        # Execute all strategy searches in parallel
        strategy_tasks = [
            search_single_query(q, f"strategy_{i}" if i > 0 else "original")
            for i, q in enumerate(search_queries)
        ]

        ranked_lists = await asyncio.gather(*strategy_tasks)

        # Step 3: Apply RRF fusion
        strategy_weights = self._calculate_strategy_weights(search_queries)

        if isinstance(self._rrf_merger, SemanticRRF):
            # Use semantic RRF with query relevance
            keywords = query_analysis_dict.get("keywords", []) if query_analysis_dict else None
            rrf_result = self._rrf_merger.merge_with_query_relevance(
                query=query,
                ranked_lists=list(ranked_lists),
                query_keywords=keywords,
                weights=strategy_weights,
                max_results=max_total_results,
            )
        else:
            rrf_result = self._rrf_merger.merge(
                ranked_lists=list(ranked_lists),
                weights=strategy_weights,
                max_results=max_total_results,
            )

        # Extract final results
        final_results = rrf_result.get_search_results()

        search_time_ms = (datetime.now() - start_time).total_seconds() * 1000

        logger.info(
            "RRF search completed",
            query=query,
            strategies_used=len(search_queries),
            providers_used=list(all_providers_used),
            input_results=rrf_result.total_input_results,
            final_results=len(final_results),
            search_time_ms=round(search_time_ms, 2),
        )

        return RRFSearchResult(
            original_query=query,
            results=final_results,
            strategies_used=rrf_result.strategies_used,
            providers_used=list(all_providers_used),
            total_results=len(final_results),
            unique_urls=rrf_result.unique_results,
            search_time_ms=search_time_ms,
            rrf_merge_result=rrf_result,
            query_analysis=query_analysis_dict,
        )

    async def search_news_with_rrf(
        self,
        query: str,
        max_results_per_strategy: int = 10,
        days: int = 7,
        **kwargs,
    ) -> RRFSearchResult:
        """Search for news with RRF fusion."""
        kwargs["topic"] = "news"
        kwargs["days"] = days
        kwargs["freshness"] = "pw"
        kwargs["search_recency_filter"] = "week"

        return await self.search_with_rrf(
            query=query,
            max_results_per_strategy=max_results_per_strategy,
            **kwargs,
        )

    async def _search_provider_with_timeout(
        self,
        provider: SearchProvider,
        query: str,
        max_results: int,
        **kwargs,
    ) -> list[SearchResult]:
        """Execute provider search with timeout."""
        try:
            return await asyncio.wait_for(
                provider.search(query, max_results, **kwargs),
                timeout=self.timeout,
            )
        except asyncio.TimeoutError:
            logger.debug(
                "Provider search timed out",
                provider=provider.name,
                timeout=self.timeout,
            )
            raise
        except Exception as e:
            logger.debug(
                "Provider search failed",
                provider=provider.name,
                error=str(e),
            )
            raise

    def _calculate_strategy_weights(
        self,
        queries: list[str],
    ) -> dict[str, float]:
        """
        Calculate weights for each search strategy.

        Original query gets highest weight, expanded queries get decreasing weights.
        """
        weights = {}
        for i, _ in enumerate(queries):
            strategy_name = f"strategy_{i}" if i > 0 else "original"
            # Original: 1.0, then decreasing: 0.9, 0.8, 0.7...
            weight = max(1.0 - (i * 0.1), 0.5)
            weights[strategy_name] = weight
        return weights

    async def health_check_all(self) -> dict[str, bool]:
        """Check health of all providers."""
        results = {}
        for provider in self.providers:
            try:
                results[provider.name] = await asyncio.wait_for(
                    provider.health_check(),
                    timeout=10.0,
                )
            except Exception:
                results[provider.name] = False

        self._provider_health = results
        return results

    async def close_all(self) -> None:
        """Close all provider connections."""
        for provider in self.providers:
            if hasattr(provider, "close"):
                await provider.close()


def create_rrf_orchestrator(
    providers: list[SearchProvider],
    llm: Optional[Any] = None,
    timeout: float = 30.0,
    rrf_k: int = 60,
    enable_semantic: bool = True,
) -> RRFSearchOrchestrator:
    """
    Factory function to create an RRF Search Orchestrator.

    Args:
        providers: List of search providers
        llm: Optional LLM for query analysis
        timeout: Timeout per provider
        rrf_k: RRF constant
        enable_semantic: Enable semantic similarity in scoring

    Returns:
        Configured RRFSearchOrchestrator
    """
    from src.search.query_analyzer import QueryAnalyzer

    query_analyzer = None
    if llm:
        query_analyzer = QueryAnalyzer(llm)

    rrf_config = RRFConfig(
        k=rrf_k,
        boost_exact_matches=True,
        normalize_scores=True,
    )

    return RRFSearchOrchestrator(
        providers=providers,
        query_analyzer=query_analyzer,
        timeout=timeout,
        rrf_config=rrf_config,
        enable_semantic_rrf=enable_semantic,
    )

```

---

## backend/autonomous-crawler-service/src/search/perplexity.py

```py
"""Perplexity API provider."""

import httpx
import structlog
from typing import Any, Literal

from src.search.base import SearchProvider, SearchResult

logger = structlog.get_logger(__name__)


class PerplexitySearchProvider(SearchProvider):
    """
    Perplexity API client (Sonar models for search).
    
    Docs: https://docs.perplexity.ai/api-reference/chat-completions
    """
    
    BASE_URL = "https://api.perplexity.ai"
    
    # Available models
    MODELS = {
        "sonar": "sonar",  # Lightweight, fast
        "sonar-pro": "sonar-pro",  # More comprehensive
        "sonar-reasoning": "sonar-reasoning",  # With reasoning
        "sonar-reasoning-pro": "sonar-reasoning-pro",  # Best quality
    }
    
    def __init__(
        self,
        api_key: str,
        model: str = "sonar",
        timeout: float = 60.0,
    ):
        self.api_key = api_key
        self.model = self.MODELS.get(model, model)
        self.timeout = timeout
        self._client: httpx.AsyncClient | None = None
    
    @property
    def name(self) -> str:
        return "perplexity"
    
    async def _get_client(self) -> httpx.AsyncClient:
        """Get or create HTTP client."""
        if self._client is None or self._client.is_closed:
            self._client = httpx.AsyncClient(
                timeout=self.timeout,
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json",
                },
            )
        return self._client
    
    async def close(self) -> None:
        """Close the HTTP client."""
        if self._client and not self._client.is_closed:
            await self._client.aclose()
            self._client = None
    
    async def search(
        self,
        query: str,
        max_results: int = 10,
        search_recency_filter: Literal["month", "week", "day", "hour"] | None = None,
        search_domain_filter: list[str] | None = None,
        return_citations: bool = True,
        return_related_questions: bool = False,
        **kwargs,
    ) -> list[SearchResult]:
        """
        Execute Perplexity search using Sonar models.
        
        Args:
            query: Search query
            max_results: Not directly used (Perplexity returns variable citations)
            search_recency_filter: Filter by recency (month, week, day, hour)
            search_domain_filter: List of domains to restrict search
            return_citations: Return source citations
            return_related_questions: Return related questions
        """
        client = await self._get_client()
        
        # Build system prompt for search
        system_prompt = (
            "You are a search assistant. Return factual information with sources. "
            "Focus on finding relevant web pages and their content."
        )
        
        payload: dict[str, Any] = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": query},
            ],
            "return_citations": return_citations,
            "return_related_questions": return_related_questions,
        }
        
        # Add search filters if specified
        if search_recency_filter:
            payload["search_recency_filter"] = search_recency_filter
        if search_domain_filter:
            payload["search_domain_filter"] = search_domain_filter
        
        try:
            response = await client.post(
                f"{self.BASE_URL}/chat/completions",
                json=payload,
            )
            response.raise_for_status()
            data = response.json()
            
            results: list[SearchResult] = []
            
            # Extract citations as search results
            citations = data.get("citations", [])
            content = ""
            
            if data.get("choices"):
                content = data["choices"][0].get("message", {}).get("content", "")
            
            # Parse citations into results
            for i, citation_url in enumerate(citations[:max_results]):
                # Try to extract title from the content that references this citation
                # Citations are referenced as [1], [2], etc. in the content
                results.append(SearchResult(
                    title=f"Source {i + 1}",  # Perplexity doesn't provide titles directly
                    url=citation_url,
                    snippet=content[:500] if i == 0 else "",  # Full answer as snippet for first result
                    source_provider=self.name,
                    score=1.0 - (i * 0.1),  # Assume earlier citations are more relevant
                    raw_data={
                        "full_response": content,
                        "citation_index": i,
                        "model": self.model,
                    },
                ))
            
            logger.info(
                "Perplexity search completed",
                query=query,
                citations_count=len(citations),
                model=self.model,
            )
            
            return results
            
        except httpx.HTTPStatusError as e:
            logger.error(
                "Perplexity search HTTP error",
                query=query,
                status_code=e.response.status_code,
                error=str(e),
            )
            return []
        except Exception as e:
            logger.error(
                "Perplexity search failed",
                query=query,
                error=str(e),
            )
            return []
    
    async def search_with_context(
        self,
        query: str,
        context: str | None = None,
        focus: Literal["internet", "academic", "news"] = "internet",
        **kwargs,
    ) -> tuple[str, list[SearchResult]]:
        """
        Search with context and return both answer and citations.
        
        Args:
            query: Search query
            context: Additional context to consider
            focus: Search focus area
            
        Returns:
            Tuple of (AI answer, list of SearchResult)
        """
        client = await self._get_client()
        
        messages = []
        if context:
            messages.append({
                "role": "system",
                "content": f"Context: {context}\n\nProvide a comprehensive answer with citations.",
            })
        
        messages.append({"role": "user", "content": query})
        
        payload = {
            "model": self.model,
            "messages": messages,
            "return_citations": True,
        }
        
        try:
            response = await client.post(
                f"{self.BASE_URL}/chat/completions",
                json=payload,
            )
            response.raise_for_status()
            data = response.json()
            
            answer = ""
            if data.get("choices"):
                answer = data["choices"][0].get("message", {}).get("content", "")
            
            citations = data.get("citations", [])
            results = [
                SearchResult(
                    title=f"Citation {i + 1}",
                    url=url,
                    snippet="",
                    source_provider=self.name,
                    raw_data={"citation_index": i},
                )
                for i, url in enumerate(citations)
            ]
            
            return answer, results
            
        except Exception as e:
            logger.error("Perplexity context search failed", query=query, error=str(e))
            return "", []
    
    async def health_check(self) -> bool:
        """Check API health."""
        try:
            results = await self.search("test", max_results=1)
            return True
        except Exception:
            return False

```

---

## backend/autonomous-crawler-service/src/search/query_analyzer.py

```py
"""Query analyzer for semantic search enhancement using LLM."""

import asyncio
import re
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Callable, Optional

import structlog
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import HumanMessage, SystemMessage

logger = structlog.get_logger(__name__)


class SearchStrategy(Enum):
    """Search strategy types for fallback mechanisms."""

    FULL_QUERY = "full_query"
    KEYWORDS_AND = "keywords_and"
    KEYWORDS_OR = "keywords_or"
    PRIMARY_KEYWORD = "primary_keyword"
    SEMANTIC_VARIANT = "semantic_variant"
    RELATED_TOPIC = "related_topic"
    PARTIAL_MATCH = "partial_match"


@dataclass
class FallbackStrategy:
    """Represents a fallback search strategy."""

    strategy_type: SearchStrategy
    query: str
    priority: int
    description: str
    weight: float = 1.0


@dataclass
class QueryAnalysis:
    """Result of query analysis."""

    original_query: str
    intent: str  # search_intent: news, research, factcheck, general
    language: str  # detected language
    keywords: list[str]  # extracted keywords
    primary_keyword: str  # most important keyword
    entities: list[str]  # named entities (people, places, organizations)
    expanded_queries: list[str]  # semantically expanded queries
    synonyms: dict[str, list[str]]  # word -> synonyms mapping
    search_terms: list[str]  # optimized search terms for different strategies
    fallback_strategies: list[FallbackStrategy]  # ordered fallback strategies
    confidence: float  # analysis confidence score
    metadata: dict[str, Any] = field(default_factory=dict)

    def get_all_search_queries(self) -> list[str]:
        """Get all search queries for multi-strategy search."""
        queries = [self.original_query]
        queries.extend(self.expanded_queries)
        queries.extend(self.search_terms)
        # Deduplicate while preserving order
        seen = set()
        unique = []
        for q in queries:
            if q.lower() not in seen:
                seen.add(q.lower())
                unique.append(q)
        return unique

    def get_fallback_query(self, attempt_index: int) -> Optional[str]:
        """Get fallback query by attempt index."""
        if attempt_index < len(self.fallback_strategies):
            return self.fallback_strategies[attempt_index].query
        return None


class QueryAnalyzer:
    """
    Analyzes and expands search queries using LLM for better search accuracy.

    Features:
    - Intent detection (news, research, factcheck, general)
    - Keyword extraction
    - Named entity recognition
    - Query expansion with synonyms and related terms
    - Multi-language support
    - Search term optimization
    """

    ANALYSIS_PROMPT = """You are a search query analyzer. Analyze the given query and provide structured information to improve search accuracy.

For the query: "{query}"

Respond in the following JSON format ONLY (no additional text):
{{
    "intent": "<news|research|factcheck|opinion|general>",
    "language": "<detected language code, e.g., ko, en, ja>",
    "keywords": ["<key term 1>", "<key term 2>", ...],
    "entities": ["<named entity 1>", "<named entity 2>", ...],
    "expanded_queries": [
        "<semantically related query 1>",
        "<semantically related query 2>",
        "<semantically related query 3>"
    ],
    "synonyms": {{
        "<original term>": ["<synonym 1>", "<synonym 2>"]
    }},
    "search_terms": [
        "<optimized search term for web search>",
        "<optimized search term for news search>",
        "<english translation if non-english>"
    ],
    "confidence": <0.0 to 1.0>
}}

Guidelines:
1. For ambiguous or metaphorical queries (like "ë‘ë”ì§€ì˜ ê³µê²©ë ¥" which could be about moles' attack power, a game, or slang), generate multiple interpretations
2. Extract the core semantic meaning, not just literal keywords
3. For non-English queries, include English translations in search_terms
4. Identify if the query is about specific domains (politics, sports, tech, entertainment, etc.)
5. Generate expanded_queries that capture different aspects or interpretations of the query
6. Keep search_terms concise and optimized for search engines"""

    QUICK_KEYWORDS_PROMPT = """Extract key search terms from this query. Return ONLY a JSON array of strings, no explanation.
Query: "{query}"
Example output: ["term1", "term2", "term3"]"""

    def __init__(
        self,
        llm: BaseChatModel,
        enable_expansion: bool = True,
        max_expanded_queries: int = 5,
        cache_results: bool = True,
    ):
        """
        Initialize the query analyzer.

        Args:
            llm: Language model for query analysis
            enable_expansion: Whether to enable query expansion
            max_expanded_queries: Maximum number of expanded queries
            cache_results: Whether to cache analysis results
        """
        self.llm = llm
        self.enable_expansion = enable_expansion
        self.max_expanded_queries = max_expanded_queries
        self._cache: Optional[dict[str, QueryAnalysis]] = {} if cache_results else None

    async def analyze(
        self,
        query: str,
        context: Optional[str] = None,
        force_refresh: bool = False,
    ) -> QueryAnalysis:
        """
        Analyze a search query and generate expanded search terms.

        Args:
            query: The original search query
            context: Optional context about the search domain
            force_refresh: Force re-analysis even if cached

        Returns:
            QueryAnalysis with expanded queries and keywords
        """
        # Check cache
        cache_key = f"{query}:{context or ''}"
        if self._cache is not None and not force_refresh:
            if cache_key in self._cache:
                logger.debug("Using cached query analysis", query=query)
                return self._cache[cache_key]

        try:
            analysis = await self._analyze_with_llm(query, context)

            # Cache result
            if self._cache is not None:
                self._cache[cache_key] = analysis

            logger.info(
                "Query analyzed",
                query=query,
                intent=analysis.intent,
                keywords=analysis.keywords,
                expanded_count=len(analysis.expanded_queries),
            )

            return analysis

        except Exception as e:
            logger.error("Query analysis failed, using fallback", query=query, error=str(e))
            return self._fallback_analysis(query)

    async def _analyze_with_llm(
        self,
        query: str,
        context: Optional[str] = None,
    ) -> QueryAnalysis:
        """Perform LLM-based query analysis."""
        prompt = self.ANALYSIS_PROMPT.format(query=query)

        if context:
            prompt += f"\n\nAdditional context: {context}"

        messages = [
            SystemMessage(content="You are a search query analyzer. Respond only with valid JSON."),
            HumanMessage(content=prompt),
        ]

        response = await self.llm.ainvoke(messages)
        content = response.content.strip()

        # Parse JSON response
        import json

        # Try to extract JSON from the response
        json_match = re.search(r"\{[\s\S]*\}", content)
        if json_match:
            content = json_match.group()

        try:
            data = json.loads(content)
        except json.JSONDecodeError:
            logger.warning("Failed to parse LLM response as JSON", response=content[:200])
            return self._fallback_analysis(query)

        # Validate and extract fields with defaults
        keywords = data.get("keywords", [])[:10]
        primary_keyword = self._identify_primary_keyword(keywords, query)

        analysis = QueryAnalysis(
            original_query=query,
            intent=data.get("intent", "general"),
            language=data.get("language", "unknown"),
            keywords=keywords,
            primary_keyword=primary_keyword,
            entities=data.get("entities", [])[:5],
            expanded_queries=data.get("expanded_queries", [])[: self.max_expanded_queries],
            synonyms=data.get("synonyms", {}),
            search_terms=data.get("search_terms", [])[:5],
            fallback_strategies=[],  # Will be generated below
            confidence=min(max(data.get("confidence", 0.5), 0.0), 1.0),
            metadata={"context": context} if context else {},
        )

        # Generate fallback strategies
        analysis.fallback_strategies = self._generate_fallback_strategies(analysis)

        return analysis

    async def extract_keywords_quick(self, query: str) -> list[str]:
        """
        Quick keyword extraction without full analysis.

        Useful for simple queries or when speed is critical.
        """
        try:
            prompt = self.QUICK_KEYWORDS_PROMPT.format(query=query)
            messages = [HumanMessage(content=prompt)]

            response = await self.llm.ainvoke(messages)
            content = response.content.strip()

            # Parse JSON array
            import json

            # Try to extract JSON array
            array_match = re.search(r"\[[\s\S]*\]", content)
            if array_match:
                keywords = json.loads(array_match.group())
                if isinstance(keywords, list):
                    return [str(k) for k in keywords[:10]]

            return self._extract_keywords_rule_based(query)

        except Exception as e:
            logger.debug("Quick keyword extraction failed", error=str(e))
            return self._extract_keywords_rule_based(query)

    def _fallback_analysis(self, query: str) -> QueryAnalysis:
        """Fallback analysis when LLM fails."""
        keywords = self._extract_keywords_rule_based(query)
        primary_keyword = self._identify_primary_keyword(keywords, query)
        language = self._detect_language(query)

        analysis = QueryAnalysis(
            original_query=query,
            intent="general",
            language=language,
            keywords=keywords,
            primary_keyword=primary_keyword,
            entities=[],
            expanded_queries=[],
            synonyms={},
            search_terms=keywords[:3],
            fallback_strategies=[],  # Will be generated below
            confidence=0.3,
            metadata={"fallback": True},
        )

        # Generate fallback strategies
        analysis.fallback_strategies = self._generate_fallback_strategies(analysis)

        return analysis

    def _extract_keywords_rule_based(self, query: str) -> list[str]:
        """Rule-based keyword extraction as fallback."""
        # Remove common stop words (basic implementation)
        stop_words = {
            # English
            "the",
            "a",
            "an",
            "is",
            "are",
            "was",
            "were",
            "be",
            "been",
            "being",
            "have",
            "has",
            "had",
            "do",
            "does",
            "did",
            "will",
            "would",
            "could",
            "should",
            "may",
            "might",
            "must",
            "shall",
            "can",
            "need",
            "dare",
            "ought",
            "used",
            "to",
            "of",
            "in",
            "for",
            "on",
            "with",
            "at",
            "by",
            "from",
            "as",
            "into",
            "through",
            "during",
            "before",
            "after",
            "above",
            "below",
            "between",
            "under",
            "again",
            "further",
            "then",
            "once",
            "here",
            "there",
            "when",
            "where",
            "why",
            "how",
            "all",
            "each",
            "few",
            "more",
            "most",
            "other",
            "some",
            "such",
            "no",
            "nor",
            "not",
            "only",
            "own",
            "same",
            "so",
            "than",
            "too",
            "very",
            "just",
            "and",
            "but",
            "if",
            "or",
            "because",
            "until",
            "while",
            "although",
            # Korean particles and common words
            "ì˜",
            "ê°€",
            "ì´",
            "ì€",
            "ëŠ”",
            "ì„",
            "ë¥¼",
            "ì—",
            "ì—ì„œ",
            "ë¡œ",
            "ìœ¼ë¡œ",
            "ì™€",
            "ê³¼",
            "ë„",
            "ë§Œ",
            "ê¹Œì§€",
            "ë¶€í„°",
            "ì—ê²Œ",
            "í•œí…Œ",
            "ê»˜",
            "ì´ë‹¤",
            "ìˆë‹¤",
            "í•˜ë‹¤",
            "ë˜ë‹¤",
            "ì—†ë‹¤",
            "ì•„ë‹ˆë‹¤",
            "ê·¸",
            "ì´",
            "ì €",
            "ê²ƒ",
            "ìˆ˜",
            "ë“±",
            "ë“¤",
            "ë°",
            "ë”",
            "ë˜",
        }

        # Tokenize
        words = re.findall(r"[\wê°€-í£]+", query.lower())

        # Filter stop words and short words
        keywords = [w for w in words if w not in stop_words and len(w) > 1]

        return keywords[:10]

    def _detect_language(self, text: str) -> str:
        """Simple language detection based on character ranges."""
        # Count character types
        korean_count = len(re.findall(r"[ê°€-í£]", text))
        japanese_count = len(re.findall(r"[ã-ã‚“ã‚¡-ãƒ³]", text))
        chinese_count = len(re.findall(r"[\u4e00-\u9fff]", text))
        latin_count = len(re.findall(r"[a-zA-Z]", text))

        total = korean_count + japanese_count + chinese_count + latin_count
        if total == 0:
            return "unknown"

        if korean_count / total > 0.3:
            return "ko"
        elif japanese_count / total > 0.3:
            return "ja"
        elif chinese_count / total > 0.3:
            return "zh"
        else:
            return "en"

    def _identify_primary_keyword(self, keywords: list[str], original_query: str) -> str:
        """Identify the most important keyword from the list."""
        if not keywords:
            words = original_query.split()
            return words[0] if words else original_query

        # Score-based primary keyword selection
        scores = {}
        for keyword in keywords:
            score = 0.0

            # Length weight (longer keywords are more specific)
            score += min(len(keyword) / 10.0, 1.0) * 0.3

            # Position weight (earlier keywords are often more important)
            pos = original_query.lower().find(keyword.lower())
            if pos >= 0:
                score += (1.0 - pos / len(original_query)) * 0.3

            # Uppercase start (potential proper noun)
            if keyword[0].isupper():
                score += 0.2

            # Contains numbers (specific identifier)
            if any(c.isdigit() for c in keyword):
                score += 0.1

            scores[keyword] = score

        return max(scores.items(), key=lambda x: x[1])[0] if scores else keywords[0]

    def _generate_fallback_strategies(self, analysis: QueryAnalysis) -> list[FallbackStrategy]:
        """Generate ordered fallback search strategies."""
        strategies = []
        priority = 1

        # Strategy 1: Full query
        strategies.append(
            FallbackStrategy(
                strategy_type=SearchStrategy.FULL_QUERY,
                query=analysis.original_query,
                priority=priority,
                description="ì›ë³¸ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰"
                if analysis.language == "ko"
                else "Search with original query",
                weight=1.0,
            )
        )
        priority += 1

        # Strategy 2: Keywords AND
        if len(analysis.keywords) > 1:
            strategies.append(
                FallbackStrategy(
                    strategy_type=SearchStrategy.KEYWORDS_AND,
                    query=" ".join(analysis.keywords),
                    priority=priority,
                    description="ëª¨ë“  í‚¤ì›Œë“œë¡œ ê²€ìƒ‰"
                    if analysis.language == "ko"
                    else "Search with all keywords",
                    weight=0.9,
                )
            )
            priority += 1

        # Strategy 3: Primary keyword
        if analysis.primary_keyword:
            strategies.append(
                FallbackStrategy(
                    strategy_type=SearchStrategy.PRIMARY_KEYWORD,
                    query=analysis.primary_keyword,
                    priority=priority,
                    description="ì£¼ìš” í‚¤ì›Œë“œë§Œìœ¼ë¡œ ê²€ìƒ‰"
                    if analysis.language == "ko"
                    else "Search with primary keyword only",
                    weight=0.85,
                )
            )
            priority += 1

        # Strategy 4: Semantic variants (expanded queries)
        for i, expanded in enumerate(analysis.expanded_queries[:3]):
            if expanded.lower() != analysis.original_query.lower():
                strategies.append(
                    FallbackStrategy(
                        strategy_type=SearchStrategy.SEMANTIC_VARIANT,
                        query=expanded,
                        priority=priority,
                        description=f"ë³€í˜• ì¿¼ë¦¬ {i + 1}: {expanded}"
                        if analysis.language == "ko"
                        else f"Variant query {i + 1}: {expanded}",
                        weight=0.8 - (i * 0.1),
                    )
                )
                priority += 1

        # Strategy 5: Keywords OR (broader search)
        if len(analysis.keywords) > 1:
            or_query = " OR ".join(analysis.keywords[:5])
            strategies.append(
                FallbackStrategy(
                    strategy_type=SearchStrategy.KEYWORDS_OR,
                    query=or_query,
                    priority=priority,
                    description="í‚¤ì›Œë“œ OR ê²€ìƒ‰ (ë„“ì€ ê²€ìƒ‰)"
                    if analysis.language == "ko"
                    else "Keywords OR search (broader)",
                    weight=0.7,
                )
            )
            priority += 1

        # Strategy 6: Partial match (top 2 keywords)
        if len(analysis.keywords) >= 2:
            partial_query = f"{analysis.keywords[0]} {analysis.keywords[1]}"
            strategies.append(
                FallbackStrategy(
                    strategy_type=SearchStrategy.PARTIAL_MATCH,
                    query=partial_query,
                    priority=priority,
                    description="ìƒìœ„ í‚¤ì›Œë“œ ë¶€ë¶„ ë§¤ì¹­"
                    if analysis.language == "ko"
                    else "Top keywords partial match",
                    weight=0.65,
                )
            )
            priority += 1

        # Sort by priority
        strategies.sort(key=lambda s: s.priority)
        return strategies

    def build_no_result_message(self, analysis: QueryAnalysis) -> str:
        """Build a helpful message when no results are found."""
        if analysis.language == "ko":
            message = "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ê¸° ì–´ë ¤ì› ìŠµë‹ˆë‹¤. ë‹¤ìŒì„ ì‹œë„í•´ ë³´ì„¸ìš”:\n\n"
            message += "ì‹œë„í•œ ê²€ìƒ‰ì–´:\n"
            message += f"- {analysis.original_query}\n"
            message += f"- {analysis.primary_keyword}\n"

            message += "\nì¶”ì²œ ê²€ìƒ‰ ë°©ë²•:\n"
            message += "1. ê²€ìƒ‰ì–´ë¥¼ ë” êµ¬ì²´ì ìœ¼ë¡œ ë³€ê²½í•´ ë³´ì„¸ìš”\n"
            message += f"2. ë‹¤ë¥¸ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•´ ë³´ì„¸ìš”: {', '.join(analysis.keywords[:3])}\n"
            message += "3. ì‹œê°„ ë²”ìœ„ë¥¼ ì¡°ì •í•´ ë³´ì„¸ìš”\n"

            intent_desc = {
                "news": "ìµœì‹  ë‰´ìŠ¤",
                "research": "ì‹¬ì¸µ ë¶„ì„",
                "factcheck": "íŒ©íŠ¸ì²´í¬",
                "opinion": "ì—¬ë¡  ê²€ìƒ‰",
                "general": "ì¼ë°˜ ê²€ìƒ‰",
            }
            message += f"\në¶„ì„ëœ ì˜ë„: {intent_desc.get(analysis.intent, 'ì¼ë°˜ ê²€ìƒ‰')}"
        else:
            message = "Search results were difficult to find. Try the following:\n\n"
            message += "Queries attempted:\n"
            message += f"- {analysis.original_query}\n"
            message += f"- {analysis.primary_keyword}\n"

            message += "\nRecommended approaches:\n"
            message += "1. Try more specific keywords\n"
            message += f"2. Use alternative keywords: {', '.join(analysis.keywords[:3])}\n"
            message += "3. Adjust the time range\n"

            message += f"\nDetected intent: {analysis.intent}"

        return message

    def build_enhanced_task(self, analysis: QueryAnalysis, original_task: str) -> str:
        """Build an enhanced task with fallback instructions."""
        task = original_task + "\n\n"

        if analysis.language == "ko":
            task += "ê²€ìƒ‰ ì „ëµ (ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ìˆœì„œëŒ€ë¡œ ì‹œë„í•˜ì„¸ìš”):\n"
            for i, strategy in enumerate(analysis.fallback_strategies[:5], 1):
                task += f'{i}. {strategy.description}: "{strategy.query}"\n'
            task += "\nì¤‘ìš”: ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ë‹¤ê³  ë§í•˜ì§€ ë§ˆì„¸ìš”. ìœ„ì˜ ì „ëµì„ ëª¨ë‘ ì‹œë„í•˜ê³ ,\n"
            task += "ê´€ë ¨ëœ ì •ë³´ë¼ë„ ì°¾ì•„ì„œ ì œê³µí•˜ì„¸ìš”. ì™„ì „íˆ ì¼ì¹˜í•˜ì§€ ì•Šë”ë¼ë„\n"
            task += "ê°€ì¥ ê´€ë ¨ì„± ìˆëŠ” ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤."
        else:
            task += "Search strategies (try in order if no results):\n"
            for i, strategy in enumerate(analysis.fallback_strategies[:5], 1):
                task += f'{i}. {strategy.description}: "{strategy.query}"\n'
            task += (
                '\nIMPORTANT: Never say "not found" or "no results". Try ALL strategies above,\n'
            )
            task += "and provide whatever relevant information you can find. Even if not an exact match,\n"
            task += "providing the most relevant information is important."

        return task

    def clear_cache(self) -> None:
        """Clear the analysis cache."""
        if self._cache is not None:
            self._cache.clear()


class MultiStrategyQueryExpander:
    """
    Expands a single query into multiple search strategies.

    Strategies:
    1. Original query (exact match)
    2. Keyword-based query (extracted keywords)
    3. Semantic expansion (related concepts)
    4. Entity-focused query (named entities)
    5. Cross-lingual query (translations)
    """

    def __init__(self, analyzer: QueryAnalyzer):
        self.analyzer = analyzer

    async def expand(
        self,
        query: str,
        max_strategies: int = 5,
        context: Optional[str] = None,
    ) -> list[dict[str, Any]]:
        """
        Expand query into multiple search strategies.

        Returns:
            List of dicts with 'query', 'strategy', and 'weight' keys
        """
        analysis = await self.analyzer.analyze(query, context)

        strategies = []

        # Strategy 1: Original query (highest weight)
        strategies.append(
            {
                "query": analysis.original_query,
                "strategy": "original",
                "weight": 1.0,
            }
        )

        # Strategy 2: Keywords joined
        if analysis.keywords:
            keyword_query = " ".join(analysis.keywords[:5])
            if keyword_query.lower() != query.lower():
                strategies.append(
                    {
                        "query": keyword_query,
                        "strategy": "keywords",
                        "weight": 0.9,
                    }
                )

        # Strategy 3: Semantic expansions
        for i, expanded in enumerate(analysis.expanded_queries[:3]):
            strategies.append(
                {
                    "query": expanded,
                    "strategy": f"semantic_{i + 1}",
                    "weight": 0.8 - (i * 0.1),
                }
            )

        # Strategy 4: Entity-focused
        if analysis.entities:
            entity_query = " ".join(analysis.entities[:3])
            if entity_query.lower() != query.lower():
                strategies.append(
                    {
                        "query": entity_query,
                        "strategy": "entities",
                        "weight": 0.7,
                    }
                )

        # Strategy 5: Search terms (often includes translations)
        for i, term in enumerate(analysis.search_terms[:2]):
            if term.lower() != query.lower():
                strategies.append(
                    {
                        "query": term,
                        "strategy": f"search_term_{i + 1}",
                        "weight": 0.75 - (i * 0.1),
                    }
                )

        # Deduplicate and limit
        seen_queries = set()
        unique_strategies = []
        for s in strategies:
            q_lower = s["query"].lower()
            if q_lower not in seen_queries:
                seen_queries.add(q_lower)
                unique_strategies.append(s)

        return unique_strategies[:max_strategies]

```

---

## backend/autonomous-crawler-service/src/search/rrf.py

```py
"""Reciprocal Rank Fusion (RRF) for multi-strategy search result merging."""

from dataclasses import dataclass, field
from typing import Any, TypeVar, Callable, Optional
from collections import defaultdict

import structlog

from src.search.base import SearchResult

logger = structlog.get_logger(__name__)

T = TypeVar("T")


@dataclass
class RRFConfig:
    """Configuration for RRF algorithm."""

    k: int = 60  # RRF constant (default: 60, higher = more weight to lower ranks)
    min_score_threshold: float = 0.0  # Minimum RRF score to include result
    boost_exact_matches: bool = True  # Boost results that appear in multiple rankings
    multi_appearance_bonus: float = 0.1  # Bonus per additional appearance
    normalize_scores: bool = True  # Normalize final scores to 0-1 range


@dataclass
class RRFResult:
    """Result with RRF scoring information."""

    item: SearchResult
    rrf_score: float
    appearances: int  # Number of rankings this item appeared in
    rank_positions: list[int]  # Original positions in each ranking
    source_strategies: list[str]  # Which strategies found this result

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary."""
        return {
            "item": self.item.to_dict(),
            "rrf_score": self.rrf_score,
            "appearances": self.appearances,
            "rank_positions": self.rank_positions,
            "source_strategies": self.source_strategies,
        }


@dataclass
class RRFMergeResult:
    """Result of RRF merge operation."""

    results: list[RRFResult]
    total_input_results: int
    unique_results: int
    strategies_used: list[str]
    processing_time_ms: float
    config: RRFConfig = field(default_factory=RRFConfig)

    def get_search_results(self) -> list[SearchResult]:
        """Get just the SearchResult items, sorted by RRF score."""
        return [r.item for r in sorted(self.results, key=lambda x: x.rrf_score, reverse=True)]

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary."""
        return {
            "results": [r.to_dict() for r in self.results],
            "total_input_results": self.total_input_results,
            "unique_results": self.unique_results,
            "strategies_used": self.strategies_used,
            "processing_time_ms": self.processing_time_ms,
        }


class ReciprocalRankFusion:
    """
    Implements Reciprocal Rank Fusion (RRF) algorithm for combining
    multiple ranked lists into a single ranking.

    RRF Score = Î£ 1 / (k + rank_i) for each ranking list

    Features:
    - Combines results from multiple search strategies
    - Handles duplicates by URL normalization
    - Boosts results that appear in multiple rankings
    - Configurable k parameter and scoring thresholds

    Reference:
    Cormack, G. V., Clarke, C. L., & Buettcher, S. (2009).
    Reciprocal rank fusion outperforms condorcet and individual rank learning methods.
    """

    def __init__(self, config: Optional[RRFConfig] = None):
        """
        Initialize RRF merger.

        Args:
            config: RRF configuration options
        """
        self.config = config or RRFConfig()

    def merge(
        self,
        ranked_lists: list[tuple[str, list[SearchResult]]],
        weights: Optional[dict[str, float]] = None,
        max_results: int = 50,
    ) -> RRFMergeResult:
        """
        Merge multiple ranked lists using RRF.

        Args:
            ranked_lists: List of (strategy_name, results) tuples
            weights: Optional weights per strategy (default: equal weights)
            max_results: Maximum number of results to return

        Returns:
            RRFMergeResult with merged and scored results
        """
        import time

        start_time = time.time()

        if not ranked_lists:
            return RRFMergeResult(
                results=[],
                total_input_results=0,
                unique_results=0,
                strategies_used=[],
                processing_time_ms=0,
                config=self.config,
            )

        # Default equal weights
        if weights is None:
            weights = {name: 1.0 for name, _ in ranked_lists}

        # Track scores and metadata per unique URL
        url_scores: dict[str, float] = defaultdict(float)
        url_items: dict[str, SearchResult] = {}
        url_appearances: dict[str, int] = defaultdict(int)
        url_ranks: dict[str, list[int]] = defaultdict(list)
        url_strategies: dict[str, list[str]] = defaultdict(list)

        total_input = 0
        strategies_used = []

        for strategy_name, results in ranked_lists:
            if not results:
                continue

            strategies_used.append(strategy_name)
            strategy_weight = weights.get(strategy_name, 1.0)

            for rank, result in enumerate(results, start=1):
                total_input += 1

                # Normalize URL for deduplication
                normalized_url = self._normalize_url(result.url)

                # Calculate RRF score for this position
                rrf_score = strategy_weight / (self.config.k + rank)

                url_scores[normalized_url] += rrf_score
                url_appearances[normalized_url] += 1
                url_ranks[normalized_url].append(rank)
                url_strategies[normalized_url].append(strategy_name)

                # Keep the result with most information
                if normalized_url not in url_items:
                    url_items[normalized_url] = result
                else:
                    existing = url_items[normalized_url]
                    # Prefer result with longer snippet or more metadata
                    if len(result.snippet) > len(existing.snippet):
                        url_items[normalized_url] = result

        # Apply multi-appearance bonus
        if self.config.boost_exact_matches:
            for url, appearances in url_appearances.items():
                if appearances > 1:
                    bonus = self.config.multi_appearance_bonus * (appearances - 1)
                    url_scores[url] += bonus

        # Normalize scores if configured
        if self.config.normalize_scores and url_scores:
            max_score = max(url_scores.values())
            if max_score > 0:
                url_scores = {url: score / max_score for url, score in url_scores.items()}

        # Build final results
        rrf_results = []
        for url, score in url_scores.items():
            if score >= self.config.min_score_threshold:
                rrf_results.append(
                    RRFResult(
                        item=url_items[url],
                        rrf_score=score,
                        appearances=url_appearances[url],
                        rank_positions=url_ranks[url],
                        source_strategies=url_strategies[url],
                    )
                )

        # Sort by RRF score (descending)
        rrf_results.sort(key=lambda x: x.rrf_score, reverse=True)

        # Limit results
        rrf_results = rrf_results[:max_results]

        processing_time = (time.time() - start_time) * 1000

        logger.info(
            "RRF merge completed",
            total_input=total_input,
            unique_results=len(rrf_results),
            strategies=strategies_used,
            processing_time_ms=round(processing_time, 2),
        )

        return RRFMergeResult(
            results=rrf_results,
            total_input_results=total_input,
            unique_results=len(rrf_results),
            strategies_used=strategies_used,
            processing_time_ms=processing_time,
            config=self.config,
        )

    def merge_with_reranking(
        self,
        ranked_lists: list[tuple[str, list[SearchResult]]],
        rerank_fn: Callable[[SearchResult], float],
        rerank_weight: float = 0.3,
        **kwargs,
    ) -> RRFMergeResult:
        """
        Merge with additional reranking based on a custom scoring function.

        Args:
            ranked_lists: List of (strategy_name, results) tuples
            rerank_fn: Function that takes a SearchResult and returns a score (0-1)
            rerank_weight: Weight for the reranking score (RRF weight = 1 - rerank_weight)
            **kwargs: Additional arguments passed to merge()

        Returns:
            RRFMergeResult with combined RRF and reranking scores
        """
        # First do standard RRF merge
        rrf_result = self.merge(ranked_lists, **kwargs)

        # Apply reranking
        for result in rrf_result.results:
            try:
                rerank_score = rerank_fn(result.item)
                # Combine RRF score with rerank score
                combined = result.rrf_score * (1 - rerank_weight) + rerank_score * rerank_weight
                result.rrf_score = combined
            except Exception as e:
                logger.debug("Reranking failed for result", url=result.item.url, error=str(e))

        # Re-sort by new scores
        rrf_result.results.sort(key=lambda x: x.rrf_score, reverse=True)

        return rrf_result

    def _normalize_url(self, url: str) -> str:
        """Normalize URL for deduplication."""
        try:
            from urllib.parse import urlparse

            parsed = urlparse(url)
            # Remove www. prefix and trailing slash
            netloc = parsed.netloc.lower().lstrip("www.")
            path = parsed.path.rstrip("/")
            # Remove common tracking parameters
            return f"{netloc}{path}"
        except Exception:
            return url.lower()


class SemanticRRF(ReciprocalRankFusion):
    """
    Extended RRF with semantic similarity scoring.

    Combines RRF with semantic similarity between query and results
    for better relevance ranking.
    """

    def __init__(
        self,
        config: Optional[RRFConfig] = None,
        similarity_weight: float = 0.2,
    ):
        """
        Initialize Semantic RRF.

        Args:
            config: RRF configuration
            similarity_weight: Weight for semantic similarity score (0-1)
        """
        super().__init__(config)
        self.similarity_weight = similarity_weight

    def merge_with_query_relevance(
        self,
        query: str,
        ranked_lists: list[tuple[str, list[SearchResult]]],
        query_keywords: Optional[list[str]] = None,
        **kwargs,
    ) -> RRFMergeResult:
        """
        Merge with query relevance scoring.

        Args:
            query: Original search query
            ranked_lists: List of (strategy_name, results) tuples
            query_keywords: Pre-extracted keywords for matching
            **kwargs: Additional arguments passed to merge()
        """
        keywords = query_keywords or self._extract_keywords(query)

        def relevance_scorer(result: SearchResult) -> float:
            """Score result based on keyword presence."""
            text = f"{result.title} {result.snippet}".lower()

            if not keywords:
                return 0.5

            matches = sum(1 for kw in keywords if kw.lower() in text)
            return min(matches / len(keywords), 1.0)

        return self.merge_with_reranking(
            ranked_lists=ranked_lists,
            rerank_fn=relevance_scorer,
            rerank_weight=self.similarity_weight,
            **kwargs,
        )

    def _extract_keywords(self, query: str) -> list[str]:
        """Simple keyword extraction."""
        import re

        # Basic tokenization
        words = re.findall(r"[\wê°€-í£]+", query.lower())
        # Filter short words
        return [w for w in words if len(w) > 1]


def create_rrf_merger(
    k: int = 60,
    boost_duplicates: bool = True,
    normalize: bool = True,
) -> ReciprocalRankFusion:
    """
    Factory function to create an RRF merger with common configurations.

    Args:
        k: RRF constant (higher = more weight to lower ranks)
        boost_duplicates: Boost results appearing in multiple rankings
        normalize: Normalize final scores to 0-1

    Returns:
        Configured ReciprocalRankFusion instance
    """
    config = RRFConfig(
        k=k,
        boost_exact_matches=boost_duplicates,
        normalize_scores=normalize,
    )
    return ReciprocalRankFusion(config)


def create_semantic_rrf_merger(
    similarity_weight: float = 0.2,
    **kwargs,
) -> SemanticRRF:
    """
    Factory function to create a Semantic RRF merger.

    Args:
        similarity_weight: Weight for semantic similarity (0-1)
        **kwargs: Additional RRFConfig arguments

    Returns:
        Configured SemanticRRF instance
    """
    config = RRFConfig(**kwargs) if kwargs else None
    return SemanticRRF(config, similarity_weight=similarity_weight)

```

---

## backend/autonomous-crawler-service/src/search/tavily.py

```py
"""Tavily Search API provider."""

import httpx
import structlog
from typing import Any, Literal

from src.search.base import SearchProvider, SearchResult

logger = structlog.get_logger(__name__)


class TavilySearchProvider(SearchProvider):
    """
    Tavily Search API client.
    
    Docs: https://docs.tavily.com/docs/tavily-api/rest_api
    """
    
    BASE_URL = "https://api.tavily.com"
    
    def __init__(self, api_key: str, timeout: float = 60.0):
        self.api_key = api_key
        self.timeout = timeout
        self._client: httpx.AsyncClient | None = None
    
    @property
    def name(self) -> str:
        return "tavily"
    
    async def _get_client(self) -> httpx.AsyncClient:
        """Get or create HTTP client."""
        if self._client is None or self._client.is_closed:
            self._client = httpx.AsyncClient(
                timeout=self.timeout,
                headers={"Content-Type": "application/json"},
            )
        return self._client
    
    async def close(self) -> None:
        """Close the HTTP client."""
        if self._client and not self._client.is_closed:
            await self._client.aclose()
            self._client = None
    
    async def search(
        self,
        query: str,
        max_results: int = 10,
        search_depth: Literal["basic", "advanced"] = "basic",
        include_domains: list[str] | None = None,
        exclude_domains: list[str] | None = None,
        include_answer: bool = False,
        include_raw_content: bool = False,
        topic: Literal["general", "news"] = "general",
        days: int | None = None,  # For news topic, limit to N days
        **kwargs,
    ) -> list[SearchResult]:
        """
        Execute Tavily search.
        
        Args:
            query: Search query
            max_results: Maximum results (up to 10 for basic, 20 for advanced)
            search_depth: basic (faster) or advanced (more comprehensive)
            include_domains: List of domains to include
            exclude_domains: List of domains to exclude
            include_answer: Include AI-generated answer
            include_raw_content: Include raw HTML content
            topic: general or news
            days: For news, limit to past N days
        """
        client = await self._get_client()
        
        payload: dict[str, Any] = {
            "api_key": self.api_key,
            "query": query,
            "max_results": max_results,
            "search_depth": search_depth,
            "include_answer": include_answer,
            "include_raw_content": include_raw_content,
            "topic": topic,
        }
        
        if include_domains:
            payload["include_domains"] = include_domains
        if exclude_domains:
            payload["exclude_domains"] = exclude_domains
        if days and topic == "news":
            payload["days"] = days
        
        try:
            response = await client.post(
                f"{self.BASE_URL}/search",
                json=payload,
            )
            response.raise_for_status()
            data = response.json()
            
            results: list[SearchResult] = []
            
            for item in data.get("results", []):
                results.append(SearchResult(
                    title=item.get("title", ""),
                    url=item.get("url", ""),
                    snippet=item.get("content", ""),
                    source_provider=self.name,
                    published_date=item.get("published_date"),
                    score=item.get("score"),
                    raw_data={
                        "raw_content": item.get("raw_content"),
                        **item,
                    },
                ))
            
            # Add AI answer if available
            if include_answer and data.get("answer"):
                logger.info(
                    "Tavily AI answer generated",
                    query=query,
                    answer_preview=data["answer"][:100],
                )
            
            logger.info(
                "Tavily search completed",
                query=query,
                results_count=len(results),
                search_depth=search_depth,
            )
            
            return results
            
        except httpx.HTTPStatusError as e:
            logger.error(
                "Tavily search HTTP error",
                query=query,
                status_code=e.response.status_code,
                error=str(e),
            )
            return []
        except Exception as e:
            logger.error(
                "Tavily search failed",
                query=query,
                error=str(e),
            )
            return []
    
    async def search_news(
        self,
        query: str,
        max_results: int = 10,
        days: int = 7,
        **kwargs,
    ) -> list[SearchResult]:
        """Search news specifically."""
        return await self.search(
            query=query,
            max_results=max_results,
            topic="news",
            days=days,
            **kwargs,
        )
    
    async def extract_content(
        self,
        urls: list[str],
    ) -> list[dict[str, Any]]:
        """
        Extract content from URLs using Tavily Extract API.
        
        Args:
            urls: List of URLs to extract content from
            
        Returns:
            List of extracted content dictionaries
        """
        client = await self._get_client()
        
        try:
            response = await client.post(
                f"{self.BASE_URL}/extract",
                json={
                    "api_key": self.api_key,
                    "urls": urls,
                },
            )
            response.raise_for_status()
            data = response.json()
            
            return data.get("results", [])
            
        except Exception as e:
            logger.error("Tavily extract failed", urls=urls, error=str(e))
            return []
    
    async def health_check(self) -> bool:
        """Check API health."""
        try:
            results = await self.search("test", max_results=1)
            return True  # Tavily returns empty for simple queries but API works
        except Exception:
            return False

```

---

## backend/autonomous-crawler-service/src/state/__init__.py

```py
"""
State management module for autonomous-crawler-service.

Provides persistent storage for task results using Redis with in-memory fallback.
"""

from src.state.store import StateStore, get_state_store

__all__ = ["StateStore", "get_state_store"]

```

---

## backend/autonomous-crawler-service/src/state/store.py

```py
"""
State Storage Module for autonomous-crawler-service.

Provides persistent storage for task results using Redis.
Falls back to in-memory storage if Redis is unavailable.

Features:
- Redis backend with configurable TTL
- In-memory fallback for resilience
- Automatic state restoration on startup
- Async-safe with lock protection
"""

import asyncio
import json
from dataclasses import asdict
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional

import structlog

from src.config import get_settings

logger = structlog.get_logger(__name__)


class EnhancedJSONEncoder(json.JSONEncoder):
    """JSON encoder that handles dataclasses, enums, and datetime objects."""

    def default(self, o: Any) -> Any:
        if hasattr(o, "__dataclass_fields__"):
            return asdict(o)
        if hasattr(o, "model_dump"):
            # Pydantic v2 models
            return o.model_dump()
        if hasattr(o, "dict"):
            # Pydantic v1 models
            return o.dict()
        if isinstance(o, Enum):
            return o.value
        if isinstance(o, datetime):
            return o.isoformat()
        return super().default(o)


class StateStore:
    """
    Persistent state storage with Redis backend and in-memory fallback.

    Usage:
        store = StateStore()
        await store.connect()

        # Save task result
        await store.save_task("task-123", task_result)

        # Load task result
        result = await store.load_task("task-123")

        # List recent tasks
        tasks = await store.list_tasks(status="success", limit=10)

        # Cleanup
        await store.disconnect()
    """

    def __init__(self):
        self._redis = None
        self._memory_store: Dict[str, Any] = {}
        self._using_redis = False
        self._lock = asyncio.Lock()
        self._settings = get_settings().redis

    async def connect(self) -> bool:
        """
        Connect to Redis.

        Returns:
            True if Redis connection successful, False if falling back to memory.
        """
        if not self._settings.enabled:
            logger.info("Redis disabled in settings, using in-memory storage")
            return False

        try:
            import redis.asyncio as redis

            self._redis = redis.from_url(
                self._settings.url,
                encoding="utf-8",
                decode_responses=True,
                socket_connect_timeout=self._settings.connection_timeout,
                socket_timeout=self._settings.socket_timeout,
                max_connections=self._settings.max_connections,
                retry_on_timeout=self._settings.retry_on_timeout,
            )

            # Test connection
            await self._redis.ping()
            self._using_redis = True

            logger.info(
                "Connected to Redis",
                url=self._settings.url.split("@")[-1],  # Hide password if present
                prefix=self._settings.prefix,
            )

            # Load existing results from Redis into memory cache
            await self._load_existing_results()

            return True

        except ImportError:
            logger.warning(
                "redis package not installed, falling back to in-memory storage",
                hint="Install with: pip install redis>=5.0.0",
            )
            self._using_redis = False
            return False

        except Exception as e:
            logger.warning(
                "Failed to connect to Redis, falling back to in-memory storage",
                error=str(e),
                url=self._settings.url.split("@")[-1],
            )
            self._using_redis = False
            return False

    async def _load_existing_results(self):
        """Load existing results from Redis into memory on startup."""
        if not self._using_redis or not self._redis:
            return

        try:
            pattern = f"{self._settings.prefix}:task:*"
            cursor = 0
            loaded_count = 0

            while True:
                cursor, keys = await self._redis.scan(cursor, match=pattern, count=100)
                for key in keys:
                    task_id = key.split(":")[-1]
                    task_data = await self._redis.get(key)
                    if task_data:
                        self._memory_store[task_id] = json.loads(task_data)
                        loaded_count += 1

                if cursor == 0:
                    break

            if loaded_count > 0:
                logger.info(
                    "Restored tasks from Redis",
                    count=loaded_count,
                )

        except Exception as e:
            logger.warning(
                "Failed to load existing results from Redis",
                error=str(e),
            )

    async def disconnect(self):
        """Close Redis connection."""
        if self._redis:
            try:
                await self._redis.close()
                logger.info("Disconnected from Redis")
            except Exception as e:
                logger.warning("Error closing Redis connection", error=str(e))
            finally:
                self._redis = None
                self._using_redis = False

    def _key(self, task_id: str) -> str:
        """Generate Redis key for a task."""
        return f"{self._settings.prefix}:task:{task_id}"

    async def save_task(self, task_id: str, task_result: Any) -> bool:
        """
        Save task result to storage.

        Args:
            task_id: Unique task identifier
            task_result: Task result object (dict, dataclass, or Pydantic model)

        Returns:
            True if save successful
        """
        async with self._lock:
            try:
                # Convert to dict if needed
                if hasattr(task_result, "__dataclass_fields__"):
                    data = asdict(task_result)
                elif hasattr(task_result, "model_dump"):
                    data = task_result.model_dump()
                elif hasattr(task_result, "dict"):
                    data = task_result.dict()
                elif isinstance(task_result, dict):
                    data = task_result
                else:
                    data = task_result

                # Serialize to JSON
                data_json = json.dumps(data, cls=EnhancedJSONEncoder)

                # Store in memory cache (for fast reads)
                self._memory_store[task_id] = json.loads(data_json)

                # Store in Redis (for persistence)
                if self._using_redis and self._redis:
                    ttl_seconds = self._settings.result_ttl_hours * 60 * 60
                    await self._redis.set(self._key(task_id), data_json, ex=ttl_seconds)

                logger.debug(
                    "Task saved",
                    task_id=task_id,
                    redis=self._using_redis,
                )
                return True

            except Exception as e:
                logger.error(
                    "Failed to save task",
                    task_id=task_id,
                    error=str(e),
                )
                return False

    async def load_task(self, task_id: str) -> Optional[Dict[str, Any]]:
        """
        Load task result from storage.

        Args:
            task_id: Unique task identifier

        Returns:
            Task result dict or None if not found
        """
        # Check memory cache first (fast path)
        if task_id in self._memory_store:
            return self._memory_store[task_id]

        # Try Redis if available
        if self._using_redis and self._redis:
            try:
                data_json = await self._redis.get(self._key(task_id))
                if data_json:
                    data = json.loads(data_json)
                    # Cache in memory for faster subsequent access
                    self._memory_store[task_id] = data
                    return data
            except Exception as e:
                logger.warning(
                    "Failed to load task from Redis",
                    task_id=task_id,
                    error=str(e),
                )

        return None

    async def delete_task(self, task_id: str) -> bool:
        """
        Delete task result from storage.

        Args:
            task_id: Unique task identifier

        Returns:
            True if deletion successful
        """
        async with self._lock:
            # Remove from memory
            self._memory_store.pop(task_id, None)

            # Remove from Redis
            if self._using_redis and self._redis:
                try:
                    await self._redis.delete(self._key(task_id))
                except Exception as e:
                    logger.warning(
                        "Failed to delete task from Redis",
                        task_id=task_id,
                        error=str(e),
                    )

            logger.debug("Task deleted", task_id=task_id)
            return True

    async def list_tasks(
        self,
        status: Optional[str] = None,
        limit: int = 50,
    ) -> List[Dict[str, Any]]:
        """
        List tasks with optional filtering.

        Args:
            status: Filter by status (e.g., "success", "failed", "timeout")
            limit: Maximum number of tasks to return

        Returns:
            List of task result dicts
        """
        tasks = []
        for task_id, task_data in list(self._memory_store.items())[-limit * 2 :]:
            if status and task_data.get("status", "").lower() != status.lower():
                continue
            tasks.append(task_data)
            if len(tasks) >= limit:
                break

        return tasks

    async def get_stats(self) -> Dict[str, Any]:
        """
        Get storage statistics.

        Returns:
            Dict with storage stats
        """
        stats = {
            "using_redis": self._using_redis,
            "memory_count": len(self._memory_store),
            "redis_url": self._settings.url.split("@")[-1] if self._using_redis else None,
            "ttl_hours": self._settings.result_ttl_hours,
        }

        if self._using_redis and self._redis:
            try:
                pattern = f"{self._settings.prefix}:task:*"
                cursor = 0
                redis_count = 0
                while True:
                    cursor, keys = await self._redis.scan(cursor, match=pattern, count=100)
                    redis_count += len(keys)
                    if cursor == 0:
                        break
                stats["redis_count"] = redis_count
            except Exception:
                stats["redis_count"] = "unknown"

        return stats

    @property
    def is_redis_connected(self) -> bool:
        """Check if Redis is connected."""
        return self._using_redis

    @property
    def task_count(self) -> int:
        """Get number of tasks in memory cache."""
        return len(self._memory_store)

    def get_memory_store(self) -> Dict[str, Any]:
        """Get direct access to memory store (for backward compatibility)."""
        return self._memory_store


# Singleton instance
_store: Optional[StateStore] = None


async def get_state_store() -> StateStore:
    """
    Get or create the singleton StateStore instance.

    This ensures only one StateStore exists throughout the application lifecycle.

    Returns:
        Initialized StateStore instance
    """
    global _store
    if _store is None:
        _store = StateStore()
        await _store.connect()
    return _store


async def close_state_store():
    """Close the singleton StateStore instance."""
    global _store
    if _store is not None:
        await _store.disconnect()
        _store = None

```

---

## backend/browser-use/.pre-commit-config.yaml

```yaml
repos:
  - repo: https://github.com/asottile/yesqa
    rev: v1.5.0
    hooks:
      - id: yesqa

  - repo: https://github.com/codespell-project/codespell
    rev: v2.4.1
    hooks:
      - id: codespell # See pyproject.toml for args
        additional_dependencies:
          - tomli

  - repo: https://github.com/asottile/pyupgrade
    rev: v3.20.0
    hooks:
      - id: pyupgrade
        args: [--py311-plus]

  # - repo: https://github.com/asottile/add-trailing-comma
  #   rev: v3.1.0
  #   hooks:
  #     - id: add-trailing-comma

  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.12.10
    hooks:
      - id: ruff-check
        args: [ --fix ]
      - id: ruff-format
      # see pyproject.toml for more details on ruff config

  - repo: https://github.com/RobertCraigie/pyright-python
    rev: v1.1.404
    hooks:
    - id: pyright

  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v6.0.0
    hooks:
      # check for basic syntax errors in python and data files
      - id: check-ast
      - id: check-toml
      - id: check-yaml
      - id: check-json
      - id: check-merge-conflict
      # check for bad files and folders
      - id: check-symlinks
      - id: destroyed-symlinks
      - id: check-case-conflict
      - id: check-illegal-windows-names
      - id: check-shebang-scripts-are-executable
      - id: mixed-line-ending
      - id: fix-byte-order-marker
      - id: end-of-file-fixer
      # best practices enforcement
      - id: detect-private-key
      # - id: check-docstring-first
      - id: debug-statements
      - id: forbid-submodules
      - id: check-added-large-files
        args: ["--maxkb=600"]
      # - id: name-tests-test
      #   args: ["--pytest-test-first"]

```

---

## backend/browser-use/bot-detector/main.py

```py
"""
Bot Detection Service - AI/ë´‡ í…ìŠ¤íŠ¸ íƒì§€ ë° ì‚¬ìš©ì í¬ë Œì‹ ì„œë¹„ìŠ¤

ê¸°ëŠ¥:
1. AI ìƒì„± í…ìŠ¤íŠ¸ íƒì§€ (GPT, Claude ë“±)
2. ë´‡ í–‰ë™ íŒ¨í„´ ë¶„ì„ (ì‹œê°„, ë°˜ë³µ, í™œë™ëŸ‰)
3. ì‚¬ìš©ì í”„ë¡œí•„ ì—…ë°ì´íŠ¸
"""

import os
import sys
import re
import math
import hashlib
import time
from typing import Optional, List, Dict, Any
from datetime import datetime, timezone
from collections import Counter

import structlog
from pydantic import BaseModel, Field
from pydantic_settings import BaseSettings
from fastapi import FastAPI, HTTPException
from cachetools import TTLCache

# Add shared module to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))
try:
    from shared.prometheus_metrics import (
        setup_metrics,
        track_request_time,
        track_operation,
        track_error,
        track_item_processed,
        ServiceMetrics,
    )

    METRICS_AVAILABLE = True
except ImportError:
    METRICS_AVAILABLE = False

# Lazy imports for ML models
log = structlog.get_logger()

# ============================================
# Configuration
# ============================================


class Settings(BaseSettings):
    """Application settings"""

    database_url: str = Field(default="postgresql://osint:osint@postgres:5432/osint")
    model_name: str = Field(default="roberta-base-openai-detector")
    cache_ttl: int = Field(default=3600)
    min_text_length: int = Field(default=50)
    bot_threshold: float = Field(default=0.7)

    # Pattern detection thresholds
    max_posts_per_minute: int = Field(default=3)
    max_posts_per_hour: int = Field(default=30)
    repetition_threshold: float = Field(default=0.8)

    class Config:
        env_prefix = "BOT_DETECTOR_"


settings = Settings()

# ============================================
# Request/Response Models
# ============================================


class BotDetectionRequest(BaseModel):
    """ë´‡ íƒì§€ ìš”ì²­"""

    content_id: Optional[str] = None
    text: str
    author: Optional[str] = None
    source_id: Optional[str] = None
    timestamp: Optional[datetime] = None
    user_hash: Optional[str] = None


class BotDetectionResponse(BaseModel):
    """ë´‡ íƒì§€ ê²°ê³¼"""

    is_bot: bool
    confidence: float
    detection_model: str
    detection_reasons: List[str]
    pattern_flags: Dict[str, Any]
    perplexity: Optional[float] = None
    burstiness: Optional[float] = None
    repetition_rate: Optional[float] = None


class BatchDetectionRequest(BaseModel):
    """ë°°ì¹˜ ë´‡ íƒì§€ ìš”ì²­"""

    items: List[BotDetectionRequest]


class BatchDetectionResponse(BaseModel):
    """ë°°ì¹˜ ë´‡ íƒì§€ ê²°ê³¼"""

    results: List[BotDetectionResponse]
    total: int
    bot_count: int


class UserProfileUpdateRequest(BaseModel):
    """ì‚¬ìš©ì í”„ë¡œí•„ ì—…ë°ì´íŠ¸ ìš”ì²­"""

    user_hash: str
    source_id: Optional[str] = None
    display_name: Optional[str] = None
    activity_timestamps: List[datetime] = []
    contents: List[str] = []


class UserProfileResponse(BaseModel):
    """ì‚¬ìš©ì í”„ë¡œí•„ ì‘ë‹µ"""

    user_hash: str
    bot_probability: float
    troll_score: float
    credibility_score: float
    activity_pattern: Dict[str, Any]
    writing_style: Dict[str, Any]


class AddonArticleInput(BaseModel):
    id: Optional[int] = None
    title: Optional[str] = None
    content: Optional[str] = None
    url: Optional[str] = None
    source: Optional[str] = None
    published_at: Optional[str] = None


class AddonCommentItem(BaseModel):
    id: Optional[str] = None
    content: Optional[str] = None
    created_at: Optional[str] = None
    likes: Optional[int] = None
    replies: Optional[int] = None
    author_id: Optional[str] = None


class AddonCommentsInput(BaseModel):
    article_id: Optional[int] = None
    items: Optional[List[AddonCommentItem]] = None
    platform: Optional[str] = None


class AddonRequest(BaseModel):
    request_id: str
    addon_id: str
    task: str = "article_analysis"
    input_schema_version: str = "1.0"
    article: Optional[AddonArticleInput] = None
    comments: Optional[AddonCommentsInput] = None
    context: Optional[Dict[str, Any]] = None
    options: Optional[Dict[str, Any]] = None


class AddonDiscussionResult(BaseModel):
    overall_sentiment: Optional[str] = None
    sentiment_distribution: Optional[Dict[str, float]] = None
    stance_distribution: Optional[Dict[str, float]] = None
    toxicity_score: Optional[float] = None
    top_keywords: Optional[List[Dict[str, Any]]] = None
    time_series: Optional[List[Dict[str, Any]]] = None
    bot_likelihood: Optional[float] = None


class AddonAnalysisResults(BaseModel):
    discussion: Optional[AddonDiscussionResult] = None
    raw: Optional[Dict[str, Any]] = None


class AddonResponse(BaseModel):
    request_id: str
    addon_id: str
    status: str
    output_schema_version: str = "1.0"
    results: Optional[AddonAnalysisResults] = None
    error: Optional[Dict[str, Any]] = None
    meta: Optional[Dict[str, Any]] = None


# ============================================
# Bot Detection Service
# ============================================


class BotDetectionService:
    """AI ë´‡ íƒì§€ ì„œë¹„ìŠ¤"""

    def __init__(self):
        self._model = None
        self._tokenizer = None
        self._model_loaded = False
        self._cache = TTLCache(maxsize=10000, ttl=settings.cache_ttl)

    def _load_model(self):
        """ëª¨ë¸ lazy loading"""
        if self._model_loaded:
            return

        try:
            from transformers import AutoModelForSequenceClassification, AutoTokenizer
            import torch

            log.info("Loading bot detection model", model=settings.model_name)

            self._tokenizer = AutoTokenizer.from_pretrained(settings.model_name)
            self._model = AutoModelForSequenceClassification.from_pretrained(
                settings.model_name
            )

            # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ì‚¬ìš©
            if torch.cuda.is_available():
                self._model = self._model.cuda()

            self._model.eval()
            self._model_loaded = True
            log.info("Bot detection model loaded successfully")

        except Exception as e:
            log.error("Failed to load bot detection model", error=str(e))
            self._model_loaded = False

    def _compute_text_hash(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ í•´ì‹œ ìƒì„±"""
        return hashlib.sha256(text.encode()).hexdigest()[:16]

    def detect_bot(self, request: BotDetectionRequest) -> BotDetectionResponse:
        """ë´‡ íƒì§€ ë©”ì¸ í•¨ìˆ˜"""
        text = request.text.strip()
        reasons = []
        pattern_flags = {}

        # ìºì‹œ í™•ì¸
        text_hash = self._compute_text_hash(text)
        if text_hash in self._cache:
            return self._cache[text_hash]

        # 1. í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸
        if len(text) < settings.min_text_length:
            return BotDetectionResponse(
                is_bot=False,
                confidence=0.0,
                detection_model="rule_based",
                detection_reasons=["text_too_short"],
                pattern_flags={"text_length": len(text)},
            )

        # 2. íŒ¨í„´ ê¸°ë°˜ íƒì§€
        pattern_score, pattern_reasons, pattern_details = self._pattern_based_detection(
            text
        )
        reasons.extend(pattern_reasons)
        pattern_flags.update(pattern_details)

        # 3. ML ëª¨ë¸ ê¸°ë°˜ íƒì§€
        ml_score = 0.0
        perplexity = None
        burstiness = None

        try:
            self._load_model()
            if self._model_loaded:
                ml_score = self._ml_based_detection(text)
                perplexity = self._calculate_perplexity(text)
                burstiness = self._calculate_burstiness(text)

                if ml_score > 0.7:
                    reasons.append("ml_model_high_confidence")
                if perplexity and perplexity < 20:
                    reasons.append("low_perplexity_suspicious")
                if burstiness and burstiness < 0.3:
                    reasons.append("low_burstiness_suspicious")
        except Exception as e:
            log.warning("ML detection failed", error=str(e))

        # 4. ìµœì¢… ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
        if self._model_loaded:
            final_score = (pattern_score * 0.4) + (ml_score * 0.6)
        else:
            final_score = pattern_score

        # 5. ë°˜ë³µë¥  ê³„ì‚°
        repetition_rate = self._calculate_repetition_rate(text)
        if repetition_rate > settings.repetition_threshold:
            reasons.append("high_repetition_rate")
            pattern_flags["repetition_rate"] = repetition_rate
            final_score = max(final_score, 0.6)

        result = BotDetectionResponse(
            is_bot=final_score >= settings.bot_threshold,
            confidence=round(final_score, 4),
            detection_model="hybrid" if self._model_loaded else "rule_based",
            detection_reasons=reasons,
            pattern_flags=pattern_flags,
            perplexity=perplexity,
            burstiness=burstiness,
            repetition_rate=repetition_rate,
        )

        # ìºì‹œ ì €ì¥
        self._cache[text_hash] = result
        return result

    def _pattern_based_detection(self, text: str) -> tuple:
        """íŒ¨í„´ ê¸°ë°˜ ë´‡ íƒì§€"""
        score = 0.0
        reasons = []
        details = {}

        # 1. ì´ëª¨ì§€ ê³¼ë‹¤ ì‚¬ìš©
        emoji_pattern = re.compile(
            r"[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F700-\U0001F77F\U0001F780-\U0001F7FF\U0001F800-\U0001F8FF\U0001F900-\U0001F9FF\U0001FA00-\U0001FA6F\U0001FA70-\U0001FAFF\U00002702-\U000027B0\U000024C2-\U0001F251]"
        )
        emoji_count = len(emoji_pattern.findall(text))
        emoji_ratio = emoji_count / max(len(text.split()), 1)
        if emoji_ratio > 0.3:
            score += 0.2
            reasons.append("excessive_emoji_usage")
            details["emoji_ratio"] = emoji_ratio

        # 2. URL ìŠ¤íŒ¸
        url_pattern = re.compile(r"https?://\S+")
        url_count = len(url_pattern.findall(text))
        if url_count > 3:
            score += 0.3
            reasons.append("multiple_urls")
            details["url_count"] = url_count

        # 3. ë°˜ë³µì ì¸ êµ¬ë¬¸
        words = text.lower().split()
        if len(words) > 5:
            word_freq = Counter(words)
            max_repeat = max(word_freq.values())
            if max_repeat / len(words) > 0.3:
                score += 0.2
                reasons.append("repetitive_words")
                details["max_word_repeat_ratio"] = max_repeat / len(words)

        # 4. ë„ˆë¬´ ì™„ë²½í•œ ë¬¸ì¥ (AI íŠ¹ì„±)
        # ë§ˆì¹¨í‘œ, ì‰¼í‘œ ë“±ì´ ê·œì¹™ì ìœ¼ë¡œ ë°°ì¹˜ëœ ê²½ìš°
        sentences = re.split(r"[.!?]", text)
        if len(sentences) > 3:
            sentence_lengths = [len(s.split()) for s in sentences if s.strip()]
            if sentence_lengths:
                avg_len = sum(sentence_lengths) / len(sentence_lengths)
                variance = sum((l - avg_len) ** 2 for l in sentence_lengths) / len(
                    sentence_lengths
                )
                std_dev = math.sqrt(variance) if variance > 0 else 0
                # ë„ˆë¬´ ì¼ê´€ëœ ë¬¸ì¥ ê¸¸ì´ëŠ” AI ì˜ì‹¬
                if std_dev < 2 and len(sentence_lengths) > 3:
                    score += 0.15
                    reasons.append("uniform_sentence_length")
                    details["sentence_length_std"] = std_dev

        # 5. íŠ¹ì • ë´‡ í‚¤ì›Œë“œ íŒ¨í„´
        bot_keywords = [
            r"ai\s+assistant",
            r"as\s+an\s+ai",
            r"i\'m\s+here\s+to\s+help",
            r"ëŒ€í™”í˜•\s+ì¸ê³µì§€ëŠ¥",
            r"AI\s+ì–¸ì–´\s+ëª¨ë¸",
        ]
        for pattern in bot_keywords:
            if re.search(pattern, text, re.IGNORECASE):
                score += 0.4
                reasons.append("bot_keyword_detected")
                details["bot_keyword_pattern"] = pattern
                break

        return min(score, 1.0), reasons, details

    def _ml_based_detection(self, text: str) -> float:
        """ML ëª¨ë¸ ê¸°ë°˜ AI í…ìŠ¤íŠ¸ íƒì§€"""
        if not self._model_loaded:
            return 0.0

        try:
            import torch

            # í† í°í™”
            inputs = self._tokenizer(
                text, return_tensors="pt", truncation=True, max_length=512
            )

            # GPUë¡œ ì´ë™
            if torch.cuda.is_available():
                inputs = {k: v.cuda() for k, v in inputs.items()}

            # ì¶”ë¡ 
            with torch.no_grad():
                outputs = self._model(**inputs)
                probs = torch.softmax(outputs.logits, dim=-1)

            # AI ìƒì„± í™•ë¥  (ëª¨ë¸ì— ë”°ë¼ ì¸ë±ìŠ¤ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
            # roberta-base-openai-detector: label 1 = AI generated
            ai_prob = probs[0][1].item()
            return ai_prob

        except Exception as e:
            log.error("ML detection error", error=str(e))
            return 0.0

    def _calculate_perplexity(self, text: str) -> Optional[float]:
        """í…ìŠ¤íŠ¸ Perplexity ê³„ì‚° (ê°„ë‹¨í•œ ê·¼ì‚¬)"""
        try:
            # ë‹¨ì–´ ë¹ˆë„ ê¸°ë°˜ ê°„ë‹¨í•œ perplexity ê·¼ì‚¬
            words = text.lower().split()
            if len(words) < 5:
                return None

            word_freq = Counter(words)
            total = len(words)

            # ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
            entropy = 0
            for count in word_freq.values():
                prob = count / total
                if prob > 0:
                    entropy -= prob * math.log2(prob)

            # Perplexity = 2^entropy
            perplexity = 2**entropy
            return round(perplexity, 2)

        except Exception:
            return None

    def _calculate_burstiness(self, text: str) -> Optional[float]:
        """í…ìŠ¤íŠ¸ Burstiness ê³„ì‚°"""
        try:
            # ë¬¸ì¥ ê¸¸ì´ì˜ ë³€ë™ì„±ìœ¼ë¡œ burstiness ê·¼ì‚¬
            sentences = re.split(r"[.!?]", text)
            sentence_lengths = [len(s.split()) for s in sentences if s.strip()]

            if len(sentence_lengths) < 3:
                return None

            mean_len = sum(sentence_lengths) / len(sentence_lengths)
            if mean_len == 0:
                return None

            variance = sum((l - mean_len) ** 2 for l in sentence_lengths) / len(
                sentence_lengths
            )
            std_dev = math.sqrt(variance)

            # ì •ê·œí™”ëœ burstiness (0~1, ë†’ì„ìˆ˜ë¡ ë³€ë™ì„± ë†’ìŒ)
            burstiness = (
                std_dev / (std_dev + mean_len) if (std_dev + mean_len) > 0 else 0
            )
            return round(burstiness, 4)

        except Exception:
            return None

    def _calculate_repetition_rate(self, text: str) -> float:
        """í…ìŠ¤íŠ¸ ë‚´ ë°˜ë³µë¥  ê³„ì‚°"""
        try:
            words = text.lower().split()
            if len(words) < 5:
                return 0.0

            unique_words = set(words)
            repetition_rate = 1 - (len(unique_words) / len(words))
            return round(repetition_rate, 4)

        except Exception:
            return 0.0


# ============================================
# User Forensics Service
# ============================================


class UserForensicsService:
    """ì‚¬ìš©ì í¬ë Œì‹ ì„œë¹„ìŠ¤"""

    def __init__(self, bot_detector: BotDetectionService):
        self.bot_detector = bot_detector

    def analyze_user_activity(
        self, request: UserProfileUpdateRequest
    ) -> UserProfileResponse:
        """ì‚¬ìš©ì í™œë™ ë¶„ì„ ë° í”„ë¡œí•„ ì—…ë°ì´íŠ¸"""

        # 1. í™œë™ íŒ¨í„´ ë¶„ì„
        activity_pattern = self._analyze_activity_pattern(request.activity_timestamps)

        # 2. ì‘ë¬¸ ìŠ¤íƒ€ì¼ ë¶„ì„
        writing_style = self._analyze_writing_style(request.contents)

        # 3. ë´‡ í™•ë¥  ê³„ì‚°
        bot_probability = self._calculate_bot_probability(
            activity_pattern, writing_style, request.contents
        )

        # 4. íŠ¸ë¡¤ ì ìˆ˜ ê³„ì‚°
        troll_score = self._calculate_troll_score(writing_style)

        # 5. ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        credibility_score = 1.0 - (bot_probability * 0.6 + troll_score * 0.4)

        return UserProfileResponse(
            user_hash=request.user_hash,
            bot_probability=round(bot_probability, 4),
            troll_score=round(troll_score, 4),
            credibility_score=round(max(0, credibility_score), 4),
            activity_pattern=activity_pattern,
            writing_style=writing_style,
        )

    def _analyze_activity_pattern(self, timestamps: List[datetime]) -> Dict[str, Any]:
        """í™œë™ íŒ¨í„´ ë¶„ì„"""
        if not timestamps:
            return {}

        pattern = {
            "total_activities": len(timestamps),
            "hour_distribution": {},
            "is_24h_active": False,
            "avg_interval_seconds": 0,
            "suspicious_burst": False,
        }

        # ì‹œê°„ëŒ€ë³„ ë¶„í¬
        hours = [ts.hour for ts in timestamps]
        hour_dist = Counter(hours)
        pattern["hour_distribution"] = dict(hour_dist)

        # 24ì‹œê°„ í™œë™ ì—¬ë¶€ (ë´‡ ì˜ì‹¬)
        active_hours = len(hour_dist)
        pattern["is_24h_active"] = active_hours >= 20

        # í™œë™ ê°„ê²© ë¶„ì„
        if len(timestamps) > 1:
            sorted_ts = sorted(timestamps)
            intervals = [
                (sorted_ts[i + 1] - sorted_ts[i]).total_seconds()
                for i in range(len(sorted_ts) - 1)
            ]
            avg_interval = sum(intervals) / len(intervals)
            pattern["avg_interval_seconds"] = round(avg_interval, 2)

            # 1ë¶„ ì´ë‚´ ì—°ì† í™œë™ì´ ë§ìœ¼ë©´ ë´‡ ì˜ì‹¬
            burst_count = sum(1 for i in intervals if i < 60)
            pattern["suspicious_burst"] = burst_count > len(intervals) * 0.3

        return pattern

    def _analyze_writing_style(self, contents: List[str]) -> Dict[str, Any]:
        """ì‘ë¬¸ ìŠ¤íƒ€ì¼ ë¶„ì„"""
        if not contents:
            return {}

        style = {
            "avg_length": 0,
            "vocabulary_diversity": 0,
            "sentiment_variance": 0,
            "formality_score": 0,
            "aggression_score": 0,
        }

        # í‰ê·  ê¸¸ì´
        lengths = [len(c) for c in contents]
        style["avg_length"] = round(sum(lengths) / len(lengths), 2)

        # ì–´íœ˜ ë‹¤ì–‘ì„±
        all_words = []
        for content in contents:
            all_words.extend(content.lower().split())
        if all_words:
            unique_ratio = len(set(all_words)) / len(all_words)
            style["vocabulary_diversity"] = round(unique_ratio, 4)

        # ê³µê²©ì„± ì ìˆ˜ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜)
        aggressive_keywords = [
            "ë°”ë³´",
            "ë©ì²­",
            "êº¼ì ¸",
            "ì£½ì–´",
            "ì“°ë ˆê¸°",
            "ë†ˆ",
            "ë…„",
            "ë³‘ì‹ ",
            "ìƒˆë¼",
        ]
        all_text = " ".join(contents).lower()
        aggression_count = sum(1 for kw in aggressive_keywords if kw in all_text)
        style["aggression_score"] = min(aggression_count / max(len(contents), 1), 1.0)

        return style

    def _calculate_bot_probability(
        self,
        activity_pattern: Dict[str, Any],
        writing_style: Dict[str, Any],
        contents: List[str],
    ) -> float:
        """ë´‡ í™•ë¥  ì¢…í•© ê³„ì‚°"""
        score = 0.0

        # í™œë™ íŒ¨í„´ ê¸°ë°˜
        if activity_pattern.get("is_24h_active"):
            score += 0.3
        if activity_pattern.get("suspicious_burst"):
            score += 0.25
        avg_interval = activity_pattern.get("avg_interval_seconds", 1000)
        if avg_interval < 30:  # 30ì´ˆ ë¯¸ë§Œ í‰ê·  ê°„ê²©
            score += 0.2

        # ì‘ë¬¸ ìŠ¤íƒ€ì¼ ê¸°ë°˜
        vocab_div = writing_style.get("vocabulary_diversity", 0.5)
        if vocab_div < 0.3:  # ë‚®ì€ ì–´íœ˜ ë‹¤ì–‘ì„±
            score += 0.15

        # ì½˜í…ì¸  ML ë¶„ì„
        if contents:
            sample_contents = contents[:10]  # ìµœëŒ€ 10ê°œë§Œ ë¶„ì„
            ml_scores = []
            for content in sample_contents:
                if len(content) >= settings.min_text_length:
                    result = self.bot_detector.detect_bot(
                        BotDetectionRequest(text=content)
                    )
                    ml_scores.append(result.confidence)
            if ml_scores:
                avg_ml_score = sum(ml_scores) / len(ml_scores)
                score += avg_ml_score * 0.3

        return min(score, 1.0)

    def _calculate_troll_score(self, writing_style: Dict[str, Any]) -> float:
        """íŠ¸ë¡¤ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0

        # ê³µê²©ì„± ê¸°ë°˜
        aggression = writing_style.get("aggression_score", 0)
        score += aggression * 0.7

        # ì§§ì€ ê¸€ ìœ„ì£¼ (íŠ¸ë¡¤ íŠ¹ì„±)
        avg_length = writing_style.get("avg_length", 100)
        if avg_length < 50:
            score += 0.2

        return min(score, 1.0)


# ============================================
# FastAPI Application
# ============================================

app = FastAPI(
    title="Bot Detection Service",
    description="AI ë´‡ íƒì§€ ë° ì‚¬ìš©ì í¬ë Œì‹ ì„œë¹„ìŠ¤",
    version="1.0.0",
)

# Setup Prometheus metrics
SERVICE_NAME = "bot-detector"
if METRICS_AVAILABLE:
    setup_metrics(app, SERVICE_NAME, version="1.0.0")
    service_metrics = ServiceMetrics(SERVICE_NAME)
    # Create service-specific metrics
    detections_total = service_metrics.create_counter(
        "detections_total",
        "Total bot detection requests",
        ["result", "detection_model"],
    )
    detection_confidence = service_metrics.create_histogram(
        "detection_confidence",
        "Bot detection confidence distribution",
        ["result"],
        buckets=(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0),
    )
    user_analyses = service_metrics.create_counter(
        "user_analyses_total", "Total user profile analyses", ["status"]
    )
    log.info("Prometheus metrics enabled for bot-detector service")
else:
    service_metrics = None
    log.warning("Prometheus metrics not available - shared module not found")

# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
bot_detector = BotDetectionService()
user_forensics = UserForensicsService(bot_detector)


@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "model_loaded": bot_detector._model_loaded,
        "timestamp": datetime.now(timezone.utc).isoformat(),
    }


@app.post("/detect", response_model=BotDetectionResponse)
async def detect_bot(request: BotDetectionRequest):
    """ë‹¨ì¼ í…ìŠ¤íŠ¸ ë´‡ íƒì§€"""
    try:
        return bot_detector.detect_bot(request)
    except Exception as e:
        log.error("Bot detection failed", error=str(e))
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/analyze", response_model=AddonResponse)
async def analyze_addon(request: AddonRequest):
    start_time = time.time()
    try:
        texts: List[str] = []
        if request.comments and request.comments.items:
            for item in request.comments.items:
                if item.content:
                    texts.append(item.content)

        if not texts and request.article:
            parts: List[str] = []
            if request.article.title:
                parts.append(request.article.title)
            if request.article.content:
                parts.append(request.article.content)
            merged = "\n".join(parts).strip()
            if merged:
                texts = [merged]

        results: List[BotDetectionResponse] = []
        confidences: List[float] = []
        reasons_set: set[str] = set()
        merged_flags: Dict[str, Any] = {}

        for text in texts:
            r = bot_detector.detect_bot(BotDetectionRequest(text=text))
            results.append(r)
            confidences.append(float(r.confidence))
            for reason in r.detection_reasons:
                reasons_set.add(reason)
            for k, v in r.pattern_flags.items():
                if k not in merged_flags:
                    merged_flags[k] = v

        avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0
        latency_ms = int((time.time() - start_time) * 1000)

        return AddonResponse(
            request_id=request.request_id,
            addon_id=request.addon_id,
            status="success",
            results=AddonAnalysisResults(
                discussion=AddonDiscussionResult(
                    bot_likelihood=round(avg_confidence, 4),
                ),
                raw={
                    "total": len(results),
                    "avg_confidence": round(avg_confidence, 4),
                    "detection_reasons": sorted(reasons_set),
                    "pattern_flags": merged_flags,
                },
            ),
            meta={
                "model_version": settings.model_name,
                "latency_ms": latency_ms,
                "processed_at": datetime.now(timezone.utc).isoformat(),
            },
        )
    except Exception as e:
        log.error("Addon analyze failed", error=str(e))
        return AddonResponse(
            request_id=request.request_id,
            addon_id=request.addon_id,
            status="error",
            error={"code": "BOT_DETECTOR_ERROR", "message": str(e)},
            meta={
                "model_version": settings.model_name,
                "latency_ms": int((time.time() - start_time) * 1000),
                "processed_at": datetime.now(timezone.utc).isoformat(),
            },
        )


@app.post("/detect/batch", response_model=BatchDetectionResponse)
async def detect_bot_batch(request: BatchDetectionRequest):
    """ë°°ì¹˜ ë´‡ íƒì§€"""
    try:
        results = []
        bot_count = 0

        for item in request.items:
            result = bot_detector.detect_bot(item)
            results.append(result)
            if result.is_bot:
                bot_count += 1

        return BatchDetectionResponse(
            results=results, total=len(results), bot_count=bot_count
        )
    except Exception as e:
        log.error("Batch detection failed", error=str(e))
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/user/analyze", response_model=UserProfileResponse)
async def analyze_user(request: UserProfileUpdateRequest):
    """ì‚¬ìš©ì í™œë™ ë¶„ì„"""
    try:
        return user_forensics.analyze_user_activity(request)
    except Exception as e:
        log.error("User analysis failed", error=str(e))
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/preload-model")
async def preload_model():
    """ML ëª¨ë¸ ì‚¬ì „ ë¡œë”©"""
    try:
        bot_detector._load_model()
        return {"status": "success", "model_loaded": bot_detector._model_loaded}
    except Exception as e:
        log.error("Model preload failed", error=str(e))
        raise HTTPException(status_code=500, detail=str(e))


# ============================================
# Main
# ============================================

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8040)

```

---

## backend/browser-use/browser_use/__init__.py

```py
import os
from typing import TYPE_CHECKING

from browser_use.logging_config import setup_logging

# Only set up logging if not in MCP mode or if explicitly requested
if os.environ.get('BROWSER_USE_SETUP_LOGGING', 'true').lower() != 'false':
	from browser_use.config import CONFIG

	# Get log file paths from config/environment
	debug_log_file = getattr(CONFIG, 'BROWSER_USE_DEBUG_LOG_FILE', None)
	info_log_file = getattr(CONFIG, 'BROWSER_USE_INFO_LOG_FILE', None)

	# Set up logging with file handlers if specified
	logger = setup_logging(debug_log_file=debug_log_file, info_log_file=info_log_file)
else:
	import logging

	logger = logging.getLogger('browser_use')

# Monkeypatch BaseSubprocessTransport.__del__ to handle closed event loops gracefully
from asyncio import base_subprocess

_original_del = base_subprocess.BaseSubprocessTransport.__del__


def _patched_del(self):
	"""Patched __del__ that handles closed event loops without throwing noisy red-herring errors like RuntimeError: Event loop is closed"""
	try:
		# Check if the event loop is closed before calling the original
		if hasattr(self, '_loop') and self._loop and self._loop.is_closed():
			# Event loop is closed, skip cleanup that requires the loop
			return
		_original_del(self)
	except RuntimeError as e:
		if 'Event loop is closed' in str(e):
			# Silently ignore this specific error
			pass
		else:
			raise


base_subprocess.BaseSubprocessTransport.__del__ = _patched_del


# Type stubs for lazy imports - fixes linter warnings
if TYPE_CHECKING:
	from browser_use.agent.prompts import SystemPrompt
	from browser_use.agent.service import Agent

	# from browser_use.agent.service import Agent
	from browser_use.agent.views import ActionModel, ActionResult, AgentHistoryList
	from browser_use.browser import BrowserProfile, BrowserSession
	from browser_use.browser import BrowserSession as Browser
	from browser_use.code_use.service import CodeAgent
	from browser_use.dom.service import DomService
	from browser_use.llm import models
	from browser_use.llm.anthropic.chat import ChatAnthropic
	from browser_use.llm.azure.chat import ChatAzureOpenAI
	from browser_use.llm.browser_use.chat import ChatBrowserUse
	from browser_use.llm.google.chat import ChatGoogle
	from browser_use.llm.groq.chat import ChatGroq
	from browser_use.llm.oci_raw.chat import ChatOCIRaw
	from browser_use.llm.ollama.chat import ChatOllama
	from browser_use.llm.openai.chat import ChatOpenAI
	from browser_use.llm.vercel.chat import ChatVercel
	from browser_use.sandbox import sandbox
	from browser_use.tools.service import Controller, Tools

	# Lazy imports mapping - only import when actually accessed
_LAZY_IMPORTS = {
	# Agent service (heavy due to dependencies)
	# 'Agent': ('browser_use.agent.service', 'Agent'),
	# Code-use agent (Jupyter notebook-like execution)
	'CodeAgent': ('browser_use.code_use.service', 'CodeAgent'),
	'Agent': ('browser_use.agent.service', 'Agent'),
	# System prompt (moderate weight due to agent.views imports)
	'SystemPrompt': ('browser_use.agent.prompts', 'SystemPrompt'),
	# Agent views (very heavy - over 1 second!)
	'ActionModel': ('browser_use.agent.views', 'ActionModel'),
	'ActionResult': ('browser_use.agent.views', 'ActionResult'),
	'AgentHistoryList': ('browser_use.agent.views', 'AgentHistoryList'),
	'BrowserSession': ('browser_use.browser', 'BrowserSession'),
	'Browser': ('browser_use.browser', 'BrowserSession'),  # Alias for BrowserSession
	'BrowserProfile': ('browser_use.browser', 'BrowserProfile'),
	# Tools (moderate weight)
	'Tools': ('browser_use.tools.service', 'Tools'),
	'Controller': ('browser_use.tools.service', 'Controller'),  # alias
	# DOM service (moderate weight)
	'DomService': ('browser_use.dom.service', 'DomService'),
	# Chat models (very heavy imports)
	'ChatOpenAI': ('browser_use.llm.openai.chat', 'ChatOpenAI'),
	'ChatGoogle': ('browser_use.llm.google.chat', 'ChatGoogle'),
	'ChatAnthropic': ('browser_use.llm.anthropic.chat', 'ChatAnthropic'),
	'ChatBrowserUse': ('browser_use.llm.browser_use.chat', 'ChatBrowserUse'),
	'ChatGroq': ('browser_use.llm.groq.chat', 'ChatGroq'),
	'ChatAzureOpenAI': ('browser_use.llm.azure.chat', 'ChatAzureOpenAI'),
	'ChatOCIRaw': ('browser_use.llm.oci_raw.chat', 'ChatOCIRaw'),
	'ChatOllama': ('browser_use.llm.ollama.chat', 'ChatOllama'),
	'ChatVercel': ('browser_use.llm.vercel.chat', 'ChatVercel'),
	# LLM models module
	'models': ('browser_use.llm.models', None),
	# Sandbox execution
	'sandbox': ('browser_use.sandbox', 'sandbox'),
}


def __getattr__(name: str):
	"""Lazy import mechanism - only import modules when they're actually accessed."""
	if name in _LAZY_IMPORTS:
		module_path, attr_name = _LAZY_IMPORTS[name]
		try:
			from importlib import import_module

			module = import_module(module_path)
			if attr_name is None:
				# For modules like 'models', return the module itself
				attr = module
			else:
				attr = getattr(module, attr_name)
			# Cache the imported attribute in the module's globals
			globals()[name] = attr
			return attr
		except ImportError as e:
			raise ImportError(f'Failed to import {name} from {module_path}: {e}') from e

	raise AttributeError(f"module '{__name__}' has no attribute '{name}'")


__all__ = [
	'Agent',
	'CodeAgent',
	# 'CodeAgent',
	'BrowserSession',
	'Browser',  # Alias for BrowserSession
	'BrowserProfile',
	'Controller',
	'DomService',
	'SystemPrompt',
	'ActionResult',
	'ActionModel',
	'AgentHistoryList',
	# Chat models
	'ChatOpenAI',
	'ChatGoogle',
	'ChatAnthropic',
	'ChatBrowserUse',
	'ChatGroq',
	'ChatAzureOpenAI',
	'ChatOCIRaw',
	'ChatOllama',
	'ChatVercel',
	'Tools',
	'Controller',
	# LLM models module
	'models',
	# Sandbox execution
	'sandbox',
]

```

---

## backend/browser-use/browser_use/actor/__init__.py

```py
"""CDP-Use High-Level Library

A Playwright-like library built on top of CDP (Chrome DevTools Protocol).
"""

from .element import Element
from .mouse import Mouse
from .page import Page
from .utils import Utils

__all__ = ['Page', 'Element', 'Mouse', 'Utils']

```

---

## backend/browser-use/browser_use/actor/element.py

```py
"""Element class for element operations."""

import asyncio
from typing import TYPE_CHECKING, Literal, Union

from cdp_use.client import logger
from typing_extensions import TypedDict

if TYPE_CHECKING:
	from cdp_use.cdp.dom.commands import (
		DescribeNodeParameters,
		FocusParameters,
		GetAttributesParameters,
		GetBoxModelParameters,
		PushNodesByBackendIdsToFrontendParameters,
		RequestChildNodesParameters,
		ResolveNodeParameters,
	)
	from cdp_use.cdp.input.commands import (
		DispatchMouseEventParameters,
	)
	from cdp_use.cdp.input.types import MouseButton
	from cdp_use.cdp.page.commands import CaptureScreenshotParameters
	from cdp_use.cdp.page.types import Viewport
	from cdp_use.cdp.runtime.commands import CallFunctionOnParameters

	from browser_use.browser.session import BrowserSession

# Type definitions for element operations
ModifierType = Literal['Alt', 'Control', 'Meta', 'Shift']


class Position(TypedDict):
	"""2D position coordinates."""

	x: float
	y: float


class BoundingBox(TypedDict):
	"""Element bounding box with position and dimensions."""

	x: float
	y: float
	width: float
	height: float


class ElementInfo(TypedDict):
	"""Basic information about a DOM element."""

	backendNodeId: int
	nodeId: int | None
	nodeName: str
	nodeType: int
	nodeValue: str | None
	attributes: dict[str, str]
	boundingBox: BoundingBox | None
	error: str | None


class Element:
	"""Element operations using BackendNodeId."""

	def __init__(
		self,
		browser_session: 'BrowserSession',
		backend_node_id: int,
		session_id: str | None = None,
	):
		self._browser_session = browser_session
		self._client = browser_session.cdp_client
		self._backend_node_id = backend_node_id
		self._session_id = session_id

	async def _get_node_id(self) -> int:
		"""Get DOM node ID from backend node ID."""
		params: 'PushNodesByBackendIdsToFrontendParameters' = {'backendNodeIds': [self._backend_node_id]}
		result = await self._client.send.DOM.pushNodesByBackendIdsToFrontend(params, session_id=self._session_id)
		return result['nodeIds'][0]

	async def _get_remote_object_id(self) -> str | None:
		"""Get remote object ID for this element."""
		node_id = await self._get_node_id()
		params: 'ResolveNodeParameters' = {'nodeId': node_id}
		result = await self._client.send.DOM.resolveNode(params, session_id=self._session_id)
		object_id = result['object'].get('objectId', None)

		if not object_id:
			return None
		return object_id

	async def click(
		self,
		button: 'MouseButton' = 'left',
		click_count: int = 1,
		modifiers: list[ModifierType] | None = None,
	) -> None:
		"""Click the element using the advanced watchdog implementation."""

		try:
			# Get viewport dimensions for visibility checks
			layout_metrics = await self._client.send.Page.getLayoutMetrics(session_id=self._session_id)
			viewport_width = layout_metrics['layoutViewport']['clientWidth']
			viewport_height = layout_metrics['layoutViewport']['clientHeight']

			# Try multiple methods to get element geometry
			quads = []

			# Method 1: Try DOM.getContentQuads first (best for inline elements and complex layouts)
			try:
				content_quads_result = await self._client.send.DOM.getContentQuads(
					params={'backendNodeId': self._backend_node_id}, session_id=self._session_id
				)
				if 'quads' in content_quads_result and content_quads_result['quads']:
					quads = content_quads_result['quads']
			except Exception:
				pass

			# Method 2: Fall back to DOM.getBoxModel
			if not quads:
				try:
					box_model = await self._client.send.DOM.getBoxModel(
						params={'backendNodeId': self._backend_node_id}, session_id=self._session_id
					)
					if 'model' in box_model and 'content' in box_model['model']:
						content_quad = box_model['model']['content']
						if len(content_quad) >= 8:
							# Convert box model format to quad format
							quads = [
								[
									content_quad[0],
									content_quad[1],  # x1, y1
									content_quad[2],
									content_quad[3],  # x2, y2
									content_quad[4],
									content_quad[5],  # x3, y3
									content_quad[6],
									content_quad[7],  # x4, y4
								]
							]
				except Exception:
					pass

			# Method 3: Fall back to JavaScript getBoundingClientRect
			if not quads:
				try:
					result = await self._client.send.DOM.resolveNode(
						params={'backendNodeId': self._backend_node_id}, session_id=self._session_id
					)
					if 'object' in result and 'objectId' in result['object']:
						object_id = result['object']['objectId']

						# Get bounding rect via JavaScript
						bounds_result = await self._client.send.Runtime.callFunctionOn(
							params={
								'functionDeclaration': """
									function() {
										const rect = this.getBoundingClientRect();
										return {
											x: rect.left,
											y: rect.top,
											width: rect.width,
											height: rect.height
										};
									}
								""",
								'objectId': object_id,
								'returnByValue': True,
							},
							session_id=self._session_id,
						)

						if 'result' in bounds_result and 'value' in bounds_result['result']:
							rect = bounds_result['result']['value']
							# Convert rect to quad format
							x, y, w, h = rect['x'], rect['y'], rect['width'], rect['height']
							quads = [
								[
									x,
									y,  # top-left
									x + w,
									y,  # top-right
									x + w,
									y + h,  # bottom-right
									x,
									y + h,  # bottom-left
								]
							]
				except Exception:
					pass

			# If we still don't have quads, fall back to JS click
			if not quads:
				try:
					result = await self._client.send.DOM.resolveNode(
						params={'backendNodeId': self._backend_node_id}, session_id=self._session_id
					)
					if 'object' not in result or 'objectId' not in result['object']:
						raise Exception('Failed to find DOM element based on backendNodeId, maybe page content changed?')
					object_id = result['object']['objectId']

					await self._client.send.Runtime.callFunctionOn(
						params={
							'functionDeclaration': 'function() { this.click(); }',
							'objectId': object_id,
						},
						session_id=self._session_id,
					)
					await asyncio.sleep(0.05)
					return
				except Exception as js_e:
					raise Exception(f'Failed to click element: {js_e}')

			# Find the largest visible quad within the viewport
			best_quad = None
			best_area = 0

			for quad in quads:
				if len(quad) < 8:
					continue

				# Calculate quad bounds
				xs = [quad[i] for i in range(0, 8, 2)]
				ys = [quad[i] for i in range(1, 8, 2)]
				min_x, max_x = min(xs), max(xs)
				min_y, max_y = min(ys), max(ys)

				# Check if quad intersects with viewport
				if max_x < 0 or max_y < 0 or min_x > viewport_width or min_y > viewport_height:
					continue  # Quad is completely outside viewport

				# Calculate visible area (intersection with viewport)
				visible_min_x = max(0, min_x)
				visible_max_x = min(viewport_width, max_x)
				visible_min_y = max(0, min_y)
				visible_max_y = min(viewport_height, max_y)

				visible_width = visible_max_x - visible_min_x
				visible_height = visible_max_y - visible_min_y
				visible_area = visible_width * visible_height

				if visible_area > best_area:
					best_area = visible_area
					best_quad = quad

			if not best_quad:
				# No visible quad found, use the first quad anyway
				best_quad = quads[0]

			# Calculate center point of the best quad
			center_x = sum(best_quad[i] for i in range(0, 8, 2)) / 4
			center_y = sum(best_quad[i] for i in range(1, 8, 2)) / 4

			# Ensure click point is within viewport bounds
			center_x = max(0, min(viewport_width - 1, center_x))
			center_y = max(0, min(viewport_height - 1, center_y))

			# Scroll element into view
			try:
				await self._client.send.DOM.scrollIntoViewIfNeeded(
					params={'backendNodeId': self._backend_node_id}, session_id=self._session_id
				)
				await asyncio.sleep(0.05)  # Wait for scroll to complete
			except Exception:
				pass

			# Calculate modifier bitmask for CDP
			modifier_value = 0
			if modifiers:
				modifier_map = {'Alt': 1, 'Control': 2, 'Meta': 4, 'Shift': 8}
				for mod in modifiers:
					modifier_value |= modifier_map.get(mod, 0)

			# Perform the click using CDP
			try:
				# Move mouse to element
				await self._client.send.Input.dispatchMouseEvent(
					params={
						'type': 'mouseMoved',
						'x': center_x,
						'y': center_y,
					},
					session_id=self._session_id,
				)
				await asyncio.sleep(0.05)

				# Mouse down
				try:
					await asyncio.wait_for(
						self._client.send.Input.dispatchMouseEvent(
							params={
								'type': 'mousePressed',
								'x': center_x,
								'y': center_y,
								'button': button,
								'clickCount': click_count,
								'modifiers': modifier_value,
							},
							session_id=self._session_id,
						),
						timeout=1.0,  # 1 second timeout for mousePressed
					)
					await asyncio.sleep(0.08)
				except TimeoutError:
					pass  # Don't sleep if we timed out

				# Mouse up
				try:
					await asyncio.wait_for(
						self._client.send.Input.dispatchMouseEvent(
							params={
								'type': 'mouseReleased',
								'x': center_x,
								'y': center_y,
								'button': button,
								'clickCount': click_count,
								'modifiers': modifier_value,
							},
							session_id=self._session_id,
						),
						timeout=3.0,  # 3 second timeout for mouseReleased
					)
				except TimeoutError:
					pass

			except Exception as e:
				# Fall back to JavaScript click via CDP
				try:
					result = await self._client.send.DOM.resolveNode(
						params={'backendNodeId': self._backend_node_id}, session_id=self._session_id
					)
					if 'object' not in result or 'objectId' not in result['object']:
						raise Exception('Failed to find DOM element based on backendNodeId, maybe page content changed?')
					object_id = result['object']['objectId']

					await self._client.send.Runtime.callFunctionOn(
						params={
							'functionDeclaration': 'function() { this.click(); }',
							'objectId': object_id,
						},
						session_id=self._session_id,
					)
					await asyncio.sleep(0.1)
					return
				except Exception as js_e:
					raise Exception(f'Failed to click element: {e}')

		except Exception as e:
			# Extract key element info for error message
			raise RuntimeError(f'Failed to click element: {e}')

	async def fill(self, value: str, clear: bool = True) -> None:
		"""Fill the input element using proper CDP methods with improved focus handling."""
		try:
			# Use the existing CDP client and session
			cdp_client = self._client
			session_id = self._session_id
			backend_node_id = self._backend_node_id

			# Track coordinates for metadata
			input_coordinates = None

			# Scroll element into view
			try:
				await cdp_client.send.DOM.scrollIntoViewIfNeeded(params={'backendNodeId': backend_node_id}, session_id=session_id)
				await asyncio.sleep(0.01)
			except Exception as e:
				logger.warning(f'Failed to scroll element into view: {e}')

			# Get object ID for the element
			result = await cdp_client.send.DOM.resolveNode(
				params={'backendNodeId': backend_node_id},
				session_id=session_id,
			)
			if 'object' not in result or 'objectId' not in result['object']:
				raise RuntimeError('Failed to get object ID for element')
			object_id = result['object']['objectId']

			# Get element coordinates for focus
			try:
				bounds_result = await cdp_client.send.Runtime.callFunctionOn(
					params={
						'functionDeclaration': 'function() { return this.getBoundingClientRect(); }',
						'objectId': object_id,
						'returnByValue': True,
					},
					session_id=session_id,
				)
				if bounds_result.get('result', {}).get('value'):
					bounds = bounds_result['result']['value']  # type: ignore
					center_x = bounds['x'] + bounds['width'] / 2
					center_y = bounds['y'] + bounds['height'] / 2
					input_coordinates = {'input_x': center_x, 'input_y': center_y}
					logger.debug(f'Using element coordinates: x={center_x:.1f}, y={center_y:.1f}')
			except Exception as e:
				logger.debug(f'Could not get element coordinates: {e}')

			# Ensure session_id is not None
			if session_id is None:
				raise RuntimeError('Session ID is required for fill operation')

			# Step 1: Focus the element
			focused_successfully = await self._focus_element_simple(
				backend_node_id=backend_node_id,
				object_id=object_id,
				cdp_client=cdp_client,
				session_id=session_id,
				input_coordinates=input_coordinates,
			)

			# Step 2: Clear existing text if requested
			if clear:
				cleared_successfully = await self._clear_text_field(
					object_id=object_id, cdp_client=cdp_client, session_id=session_id
				)
				if not cleared_successfully:
					logger.warning('Text field clearing failed, typing may append to existing text')

			# Step 3: Type the text character by character using proper human-like key events
			logger.debug(f'Typing text character by character: "{value}"')

			for i, char in enumerate(value):
				# Handle newline characters as Enter key
				if char == '\n':
					# Send proper Enter key sequence
					await cdp_client.send.Input.dispatchKeyEvent(
						params={
							'type': 'keyDown',
							'key': 'Enter',
							'code': 'Enter',
							'windowsVirtualKeyCode': 13,
						},
						session_id=session_id,
					)

					# Small delay to emulate human typing speed
					await asyncio.sleep(0.001)

					# Send char event with carriage return
					await cdp_client.send.Input.dispatchKeyEvent(
						params={
							'type': 'char',
							'text': '\r',
							'key': 'Enter',
						},
						session_id=session_id,
					)

					# Send keyUp event
					await cdp_client.send.Input.dispatchKeyEvent(
						params={
							'type': 'keyUp',
							'key': 'Enter',
							'code': 'Enter',
							'windowsVirtualKeyCode': 13,
						},
						session_id=session_id,
					)
				else:
					# Handle regular characters
					# Get proper modifiers, VK code, and base key for the character
					modifiers, vk_code, base_key = self._get_char_modifiers_and_vk(char)
					key_code = self._get_key_code_for_char(base_key)

					# Step 1: Send keyDown event (NO text parameter)
					await cdp_client.send.Input.dispatchKeyEvent(
						params={
							'type': 'keyDown',
							'key': base_key,
							'code': key_code,
							'modifiers': modifiers,
							'windowsVirtualKeyCode': vk_code,
						},
						session_id=session_id,
					)

					# Small delay to emulate human typing speed
					await asyncio.sleep(0.001)

					# Step 2: Send char event (WITH text parameter) - this is crucial for text input
					await cdp_client.send.Input.dispatchKeyEvent(
						params={
							'type': 'char',
							'text': char,
							'key': char,
						},
						session_id=session_id,
					)

					# Step 3: Send keyUp event (NO text parameter)
					await cdp_client.send.Input.dispatchKeyEvent(
						params={
							'type': 'keyUp',
							'key': base_key,
							'code': key_code,
							'modifiers': modifiers,
							'windowsVirtualKeyCode': vk_code,
						},
						session_id=session_id,
					)

				# Add 18ms delay between keystrokes
				await asyncio.sleep(0.018)

		except Exception as e:
			raise Exception(f'Failed to fill element: {str(e)}')

	async def hover(self) -> None:
		"""Hover over the element."""
		box = await self.get_bounding_box()
		if not box:
			raise RuntimeError('Element is not visible or has no bounding box')

		x = box['x'] + box['width'] / 2
		y = box['y'] + box['height'] / 2

		params: 'DispatchMouseEventParameters' = {'type': 'mouseMoved', 'x': x, 'y': y}
		await self._client.send.Input.dispatchMouseEvent(params, session_id=self._session_id)

	async def focus(self) -> None:
		"""Focus the element."""
		node_id = await self._get_node_id()
		params: 'FocusParameters' = {'nodeId': node_id}
		await self._client.send.DOM.focus(params, session_id=self._session_id)

	async def check(self) -> None:
		"""Check or uncheck a checkbox/radio button."""
		await self.click()

	async def select_option(self, values: str | list[str]) -> None:
		"""Select option(s) in a select element."""
		if isinstance(values, str):
			values = [values]

		# Focus the element first
		try:
			await self.focus()
		except Exception:
			logger.warning('Failed to focus element')

		# For select elements, we need to find option elements and click them
		# This is a simplified approach - in practice, you might need to handle
		# different select types (single vs multi-select) differently
		node_id = await self._get_node_id()

		# Request child nodes to get the options
		params: 'RequestChildNodesParameters' = {'nodeId': node_id, 'depth': 1}
		await self._client.send.DOM.requestChildNodes(params, session_id=self._session_id)

		# Get the updated node description with children
		describe_params: 'DescribeNodeParameters' = {'nodeId': node_id, 'depth': 1}
		describe_result = await self._client.send.DOM.describeNode(describe_params, session_id=self._session_id)

		select_node = describe_result['node']

		# Find and select matching options
		for child in select_node.get('children', []):
			if child.get('nodeName', '').lower() == 'option':
				# Get option attributes
				attrs = child.get('attributes', [])
				option_attrs = {}
				for i in range(0, len(attrs), 2):
					if i + 1 < len(attrs):
						option_attrs[attrs[i]] = attrs[i + 1]

				option_value = option_attrs.get('value', '')
				option_text = child.get('nodeValue', '')

				# Check if this option should be selected
				should_select = option_value in values or option_text in values

				if should_select:
					# Click the option to select it
					option_node_id = child.get('nodeId')
					if option_node_id:
						# Get backend node ID for the option
						option_describe_params: 'DescribeNodeParameters' = {'nodeId': option_node_id}
						option_backend_result = await self._client.send.DOM.describeNode(
							option_describe_params, session_id=self._session_id
						)
						option_backend_id = option_backend_result['node']['backendNodeId']

						# Create an Element for the option and click it
						option_element = Element(self._browser_session, option_backend_id, self._session_id)
						await option_element.click()

	async def drag_to(
		self,
		target: Union['Element', Position],
		source_position: Position | None = None,
		target_position: Position | None = None,
	) -> None:
		"""Drag this element to another element or position."""
		# Get source coordinates
		if source_position:
			source_x = source_position['x']
			source_y = source_position['y']
		else:
			source_box = await self.get_bounding_box()
			if not source_box:
				raise RuntimeError('Source element is not visible')
			source_x = source_box['x'] + source_box['width'] / 2
			source_y = source_box['y'] + source_box['height'] / 2

		# Get target coordinates
		if isinstance(target, dict) and 'x' in target and 'y' in target:
			target_x = target['x']
			target_y = target['y']
		else:
			if target_position:
				target_box = await target.get_bounding_box()
				if not target_box:
					raise RuntimeError('Target element is not visible')
				target_x = target_box['x'] + target_position['x']
				target_y = target_box['y'] + target_position['y']
			else:
				target_box = await target.get_bounding_box()
				if not target_box:
					raise RuntimeError('Target element is not visible')
				target_x = target_box['x'] + target_box['width'] / 2
				target_y = target_box['y'] + target_box['height'] / 2

		# Perform drag operation
		await self._client.send.Input.dispatchMouseEvent(
			{'type': 'mousePressed', 'x': source_x, 'y': source_y, 'button': 'left'},
			session_id=self._session_id,
		)

		await self._client.send.Input.dispatchMouseEvent(
			{'type': 'mouseMoved', 'x': target_x, 'y': target_y},
			session_id=self._session_id,
		)

		await self._client.send.Input.dispatchMouseEvent(
			{'type': 'mouseReleased', 'x': target_x, 'y': target_y, 'button': 'left'},
			session_id=self._session_id,
		)

	# Element properties and queries
	async def get_attribute(self, name: str) -> str | None:
		"""Get an attribute value."""
		node_id = await self._get_node_id()
		params: 'GetAttributesParameters' = {'nodeId': node_id}
		result = await self._client.send.DOM.getAttributes(params, session_id=self._session_id)

		attributes = result['attributes']
		for i in range(0, len(attributes), 2):
			if attributes[i] == name:
				return attributes[i + 1]
		return None

	async def get_bounding_box(self) -> BoundingBox | None:
		"""Get the bounding box of the element."""
		try:
			node_id = await self._get_node_id()
			params: 'GetBoxModelParameters' = {'nodeId': node_id}
			result = await self._client.send.DOM.getBoxModel(params, session_id=self._session_id)

			if 'model' not in result:
				return None

			# Get content box (first 8 values are content quad: x1,y1,x2,y2,x3,y3,x4,y4)
			content = result['model']['content']
			if len(content) < 8:
				return None

			# Calculate bounding box from quad
			x_coords = [content[i] for i in range(0, 8, 2)]
			y_coords = [content[i] for i in range(1, 8, 2)]

			x = min(x_coords)
			y = min(y_coords)
			width = max(x_coords) - x
			height = max(y_coords) - y

			return BoundingBox(x=x, y=y, width=width, height=height)

		except Exception:
			return None

	async def screenshot(self, format: str = 'png', quality: int | None = None) -> str:
		"""Take a screenshot of this element and return base64 encoded image.

		Args:
			format: Image format ('jpeg', 'png', 'webp')
			quality: Quality 0-100 for JPEG format

		Returns:
			Base64-encoded image data
		"""
		# Get element's bounding box
		box = await self.get_bounding_box()
		if not box:
			raise RuntimeError('Element is not visible or has no bounding box')

		# Create viewport clip for the element
		viewport: 'Viewport' = {'x': box['x'], 'y': box['y'], 'width': box['width'], 'height': box['height'], 'scale': 1.0}

		# Prepare screenshot parameters
		params: 'CaptureScreenshotParameters' = {'format': format, 'clip': viewport}

		if quality is not None and format.lower() == 'jpeg':
			params['quality'] = quality

		# Take screenshot
		result = await self._client.send.Page.captureScreenshot(params, session_id=self._session_id)

		return result['data']

	async def evaluate(self, page_function: str, *args) -> str:
		"""Execute JavaScript code in the context of this element.

		The JavaScript code executes with 'this' bound to the element, allowing direct
		access to element properties and methods.

		Args:
			page_function: JavaScript code that MUST start with (...args) => format
			*args: Arguments to pass to the function

		Returns:
			String representation of the JavaScript execution result.
			Objects and arrays are JSON-stringified.

		Example:
			# Get element's text content
			text = await element.evaluate("() => this.textContent")

			# Set style with argument
			await element.evaluate("(color) => this.style.color = color", "red")

			# Get computed style
			color = await element.evaluate("() => getComputedStyle(this).color")

			# Async operations
			result = await element.evaluate("async () => { await new Promise(r => setTimeout(r, 100)); return this.id; }")
		"""
		# Get remote object ID for this element
		object_id = await self._get_remote_object_id()
		if not object_id:
			raise RuntimeError('Element has no remote object ID (element may be detached from DOM)')

		# Validate arrow function format (allow async prefix)
		page_function = page_function.strip()
		# Check for arrow function with optional async prefix
		if not ('=>' in page_function and (page_function.startswith('(') or page_function.startswith('async'))):
			raise ValueError(
				f'JavaScript code must start with (...args) => or async (...args) => format. Got: {page_function[:50]}...'
			)

		# Convert arrow function to function declaration for CallFunctionOn
		# CallFunctionOn expects 'function(...args) { ... }' format, not arrow functions
		# We need to convert: '() => expression' to 'function() { return expression; }'
		# or: '(x, y) => { statements }' to 'function(x, y) { statements }'

		# Extract parameters and body from arrow function
		import re

		# Check if it's an async arrow function
		is_async = page_function.strip().startswith('async')
		async_prefix = 'async ' if is_async else ''

		# Match: (params) => body  or  async (params) => body
		# Strip 'async' prefix if present for parsing
		func_to_parse = page_function.strip()
		if is_async:
			func_to_parse = func_to_parse[5:].strip()  # Remove 'async' prefix

		arrow_match = re.match(r'\s*\(([^)]*)\)\s*=>\s*(.+)', func_to_parse, re.DOTALL)
		if not arrow_match:
			raise ValueError(f'Could not parse arrow function: {page_function[:50]}...')

		params_str = arrow_match.group(1).strip()  # e.g., '', 'x', 'x, y'
		body = arrow_match.group(2).strip()

		# If body doesn't start with {, it's an expression that needs implicit return
		if not body.startswith('{'):
			function_declaration = f'{async_prefix}function({params_str}) {{ return {body}; }}'
		else:
			# Body already has braces, use as-is
			function_declaration = f'{async_prefix}function({params_str}) {body}'

		# Build CallArgument list for args if provided
		call_arguments = []
		if args:
			from cdp_use.cdp.runtime.types import CallArgument

			for arg in args:
				# Convert Python values to CallArgument format
				call_arguments.append(CallArgument(value=arg))

		# Prepare CallFunctionOn parameters

		params: 'CallFunctionOnParameters' = {
			'functionDeclaration': function_declaration,
			'objectId': object_id,
			'returnByValue': True,
			'awaitPromise': True,
		}

		if call_arguments:
			params['arguments'] = call_arguments

		# Execute the function on the element
		result = await self._client.send.Runtime.callFunctionOn(
			params,
			session_id=self._session_id,
		)

		# Handle exceptions
		if 'exceptionDetails' in result:
			raise RuntimeError(f'JavaScript evaluation failed: {result["exceptionDetails"]}')

		# Extract and return value
		value = result.get('result', {}).get('value')

		# Return string representation (matching Page.evaluate behavior)
		if value is None:
			return ''
		elif isinstance(value, str):
			return value
		else:
			# Convert objects, numbers, booleans to string
			import json

			try:
				return json.dumps(value) if isinstance(value, (dict, list)) else str(value)
			except (TypeError, ValueError):
				return str(value)

	# Helpers for modifiers etc
	def _get_char_modifiers_and_vk(self, char: str) -> tuple[int, int, str]:
		"""Get modifiers, virtual key code, and base key for a character.

		Returns:
			(modifiers, windowsVirtualKeyCode, base_key)
		"""
		# Characters that require Shift modifier
		shift_chars = {
			'!': ('1', 49),
			'@': ('2', 50),
			'#': ('3', 51),
			'$': ('4', 52),
			'%': ('5', 53),
			'^': ('6', 54),
			'&': ('7', 55),
			'*': ('8', 56),
			'(': ('9', 57),
			')': ('0', 48),
			'_': ('-', 189),
			'+': ('=', 187),
			'{': ('[', 219),
			'}': (']', 221),
			'|': ('\\', 220),
			':': (';', 186),
			'"': ("'", 222),
			'<': (',', 188),
			'>': ('.', 190),
			'?': ('/', 191),
			'~': ('`', 192),
		}

		# Check if character requires Shift
		if char in shift_chars:
			base_key, vk_code = shift_chars[char]
			return (8, vk_code, base_key)  # Shift=8

		# Uppercase letters require Shift
		if char.isupper():
			return (8, ord(char), char.lower())  # Shift=8

		# Lowercase letters
		if char.islower():
			return (0, ord(char.upper()), char)

		# Numbers
		if char.isdigit():
			return (0, ord(char), char)

		# Special characters without Shift
		no_shift_chars = {
			' ': 32,
			'-': 189,
			'=': 187,
			'[': 219,
			']': 221,
			'\\': 220,
			';': 186,
			"'": 222,
			',': 188,
			'.': 190,
			'/': 191,
			'`': 192,
		}

		if char in no_shift_chars:
			return (0, no_shift_chars[char], char)

		# Fallback
		return (0, ord(char.upper()) if char.isalpha() else ord(char), char)

	def _get_key_code_for_char(self, char: str) -> str:
		"""Get the proper key code for a character (like Playwright does)."""
		# Key code mapping for common characters (using proper base keys + modifiers)
		key_codes = {
			' ': 'Space',
			'.': 'Period',
			',': 'Comma',
			'-': 'Minus',
			'_': 'Minus',  # Underscore uses Minus with Shift
			'@': 'Digit2',  # @ uses Digit2 with Shift
			'!': 'Digit1',  # ! uses Digit1 with Shift (not 'Exclamation')
			'?': 'Slash',  # ? uses Slash with Shift
			':': 'Semicolon',  # : uses Semicolon with Shift
			';': 'Semicolon',
			'(': 'Digit9',  # ( uses Digit9 with Shift
			')': 'Digit0',  # ) uses Digit0 with Shift
			'[': 'BracketLeft',
			']': 'BracketRight',
			'{': 'BracketLeft',  # { uses BracketLeft with Shift
			'}': 'BracketRight',  # } uses BracketRight with Shift
			'/': 'Slash',
			'\\': 'Backslash',
			'=': 'Equal',
			'+': 'Equal',  # + uses Equal with Shift
			'*': 'Digit8',  # * uses Digit8 with Shift
			'&': 'Digit7',  # & uses Digit7 with Shift
			'%': 'Digit5',  # % uses Digit5 with Shift
			'$': 'Digit4',  # $ uses Digit4 with Shift
			'#': 'Digit3',  # # uses Digit3 with Shift
			'^': 'Digit6',  # ^ uses Digit6 with Shift
			'~': 'Backquote',  # ~ uses Backquote with Shift
			'`': 'Backquote',
			'"': 'Quote',  # " uses Quote with Shift
			"'": 'Quote',
			'<': 'Comma',  # < uses Comma with Shift
			'>': 'Period',  # > uses Period with Shift
			'|': 'Backslash',  # | uses Backslash with Shift
		}

		if char in key_codes:
			return key_codes[char]
		elif char.isalpha():
			return f'Key{char.upper()}'
		elif char.isdigit():
			return f'Digit{char}'
		else:
			# Fallback for unknown characters
			return f'Key{char.upper()}' if char.isascii() and char.isalpha() else 'Unidentified'

	async def _clear_text_field(self, object_id: str, cdp_client, session_id: str) -> bool:
		"""Clear text field using multiple strategies, starting with the most reliable."""
		try:
			# Strategy 1: Direct JavaScript value setting (most reliable for modern web apps)
			logger.debug('Clearing text field using JavaScript value setting')

			await cdp_client.send.Runtime.callFunctionOn(
				params={
					'functionDeclaration': """
						function() {
							// Try to select all text first (only works on text-like inputs)
							// This handles cases where cursor is in the middle of text
							try {
								this.select();
							} catch (e) {
								// Some input types (date, color, number, etc.) don't support select()
								// That's fine, we'll just clear the value directly
							}
							// Set value to empty
							this.value = "";
							// Dispatch events to notify frameworks like React
							this.dispatchEvent(new Event("input", { bubbles: true }));
							this.dispatchEvent(new Event("change", { bubbles: true }));
							return this.value;
						}
					""",
					'objectId': object_id,
					'returnByValue': True,
				},
				session_id=session_id,
			)

			# Verify clearing worked by checking the value
			verify_result = await cdp_client.send.Runtime.callFunctionOn(
				params={
					'functionDeclaration': 'function() { return this.value; }',
					'objectId': object_id,
					'returnByValue': True,
				},
				session_id=session_id,
			)

			current_value = verify_result.get('result', {}).get('value', '')
			if not current_value:
				logger.debug('Text field cleared successfully using JavaScript')
				return True
			else:
				logger.debug(f'JavaScript clear partially failed, field still contains: "{current_value}"')

		except Exception as e:
			logger.debug(f'JavaScript clear failed: {e}')

		# Strategy 2: Triple-click + Delete (fallback for stubborn fields)
		try:
			logger.debug('Fallback: Clearing using triple-click + Delete')

			# Get element center coordinates for triple-click
			bounds_result = await cdp_client.send.Runtime.callFunctionOn(
				params={
					'functionDeclaration': 'function() { return this.getBoundingClientRect(); }',
					'objectId': object_id,
					'returnByValue': True,
				},
				session_id=session_id,
			)

			if bounds_result.get('result', {}).get('value'):
				bounds = bounds_result['result']['value']  # type: ignore  # type: ignore
				center_x = bounds['x'] + bounds['width'] / 2
				center_y = bounds['y'] + bounds['height'] / 2

				# Triple-click to select all text
				await cdp_client.send.Input.dispatchMouseEvent(
					params={
						'type': 'mousePressed',
						'x': center_x,
						'y': center_y,
						'button': 'left',
						'clickCount': 3,
					},
					session_id=session_id,
				)
				await cdp_client.send.Input.dispatchMouseEvent(
					params={
						'type': 'mouseReleased',
						'x': center_x,
						'y': center_y,
						'button': 'left',
						'clickCount': 3,
					},
					session_id=session_id,
				)

				# Delete selected text
				await cdp_client.send.Input.dispatchKeyEvent(
					params={
						'type': 'keyDown',
						'key': 'Delete',
						'code': 'Delete',
					},
					session_id=session_id,
				)
				await cdp_client.send.Input.dispatchKeyEvent(
					params={
						'type': 'keyUp',
						'key': 'Delete',
						'code': 'Delete',
					},
					session_id=session_id,
				)

				logger.debug('Text field cleared using triple-click + Delete')
				return True

		except Exception as e:
			logger.debug(f'Triple-click clear failed: {e}')

		# If all strategies failed
		logger.warning('All text clearing strategies failed')
		return False

	async def _focus_element_simple(
		self, backend_node_id: int, object_id: str, cdp_client, session_id: str, input_coordinates=None
	) -> bool:
		"""Focus element using multiple strategies with robust fallbacks."""
		try:
			# Strategy 1: CDP focus (most reliable)
			logger.debug('Focusing element using CDP focus')
			await cdp_client.send.DOM.focus(params={'backendNodeId': backend_node_id}, session_id=session_id)
			logger.debug('Element focused successfully using CDP focus')
			return True
		except Exception as e:
			logger.debug(f'CDP focus failed: {e}, trying JavaScript focus')

		try:
			# Strategy 2: JavaScript focus (fallback)
			logger.debug('Focusing element using JavaScript focus')
			await cdp_client.send.Runtime.callFunctionOn(
				params={
					'functionDeclaration': 'function() { this.focus(); }',
					'objectId': object_id,
				},
				session_id=session_id,
			)
			logger.debug('Element focused successfully using JavaScript')
			return True
		except Exception as e:
			logger.debug(f'JavaScript focus failed: {e}, trying click focus')

		try:
			# Strategy 3: Click to focus (last resort)
			if input_coordinates:
				logger.debug(f'Focusing element by clicking at coordinates: {input_coordinates}')
				center_x = input_coordinates['input_x']
				center_y = input_coordinates['input_y']

				# Click on the element to focus it
				await cdp_client.send.Input.dispatchMouseEvent(
					params={
						'type': 'mousePressed',
						'x': center_x,
						'y': center_y,
						'button': 'left',
						'clickCount': 1,
					},
					session_id=session_id,
				)
				await cdp_client.send.Input.dispatchMouseEvent(
					params={
						'type': 'mouseReleased',
						'x': center_x,
						'y': center_y,
						'button': 'left',
						'clickCount': 1,
					},
					session_id=session_id,
				)
				logger.debug('Element focused using click')
				return True
			else:
				logger.debug('No coordinates available for click focus')
		except Exception as e:
			logger.warning(f'All focus strategies failed: {e}')
		return False

	async def get_basic_info(self) -> ElementInfo:
		"""Get basic information about the element including coordinates and properties."""
		try:
			# Get basic node information
			node_id = await self._get_node_id()
			describe_result = await self._client.send.DOM.describeNode({'nodeId': node_id}, session_id=self._session_id)

			node_info = describe_result['node']

			# Get bounding box
			bounding_box = await self.get_bounding_box()

			# Get attributes as a proper dict
			attributes_list = node_info.get('attributes', [])
			attributes_dict: dict[str, str] = {}
			for i in range(0, len(attributes_list), 2):
				if i + 1 < len(attributes_list):
					attributes_dict[attributes_list[i]] = attributes_list[i + 1]

			return ElementInfo(
				backendNodeId=self._backend_node_id,
				nodeId=node_id,
				nodeName=node_info.get('nodeName', ''),
				nodeType=node_info.get('nodeType', 0),
				nodeValue=node_info.get('nodeValue'),
				attributes=attributes_dict,
				boundingBox=bounding_box,
				error=None,
			)
		except Exception as e:
			return ElementInfo(
				backendNodeId=self._backend_node_id,
				nodeId=None,
				nodeName='',
				nodeType=0,
				nodeValue=None,
				attributes={},
				boundingBox=None,
				error=str(e),
			)

```

---

## backend/browser-use/browser_use/actor/mouse.py

```py
"""Mouse class for mouse operations."""

from typing import TYPE_CHECKING

if TYPE_CHECKING:
	from cdp_use.cdp.input.commands import DispatchMouseEventParameters, SynthesizeScrollGestureParameters
	from cdp_use.cdp.input.types import MouseButton

	from browser_use.browser.session import BrowserSession


class Mouse:
	"""Mouse operations for a target."""

	def __init__(self, browser_session: 'BrowserSession', session_id: str | None = None, target_id: str | None = None):
		self._browser_session = browser_session
		self._client = browser_session.cdp_client
		self._session_id = session_id
		self._target_id = target_id

	async def click(self, x: int, y: int, button: 'MouseButton' = 'left', click_count: int = 1) -> None:
		"""Click at the specified coordinates."""
		# Mouse press
		press_params: 'DispatchMouseEventParameters' = {
			'type': 'mousePressed',
			'x': x,
			'y': y,
			'button': button,
			'clickCount': click_count,
		}
		await self._client.send.Input.dispatchMouseEvent(
			press_params,
			session_id=self._session_id,
		)

		# Mouse release
		release_params: 'DispatchMouseEventParameters' = {
			'type': 'mouseReleased',
			'x': x,
			'y': y,
			'button': button,
			'clickCount': click_count,
		}
		await self._client.send.Input.dispatchMouseEvent(
			release_params,
			session_id=self._session_id,
		)

	async def down(self, button: 'MouseButton' = 'left', click_count: int = 1) -> None:
		"""Press mouse button down."""
		params: 'DispatchMouseEventParameters' = {
			'type': 'mousePressed',
			'x': 0,  # Will use last mouse position
			'y': 0,
			'button': button,
			'clickCount': click_count,
		}
		await self._client.send.Input.dispatchMouseEvent(
			params,
			session_id=self._session_id,
		)

	async def up(self, button: 'MouseButton' = 'left', click_count: int = 1) -> None:
		"""Release mouse button."""
		params: 'DispatchMouseEventParameters' = {
			'type': 'mouseReleased',
			'x': 0,  # Will use last mouse position
			'y': 0,
			'button': button,
			'clickCount': click_count,
		}
		await self._client.send.Input.dispatchMouseEvent(
			params,
			session_id=self._session_id,
		)

	async def move(self, x: int, y: int, steps: int = 1) -> None:
		"""Move mouse to the specified coordinates."""
		# TODO: Implement smooth movement with multiple steps if needed
		_ = steps  # Acknowledge parameter for future use

		params: 'DispatchMouseEventParameters' = {'type': 'mouseMoved', 'x': x, 'y': y}
		await self._client.send.Input.dispatchMouseEvent(params, session_id=self._session_id)

	async def scroll(self, x: int = 0, y: int = 0, delta_x: int | None = None, delta_y: int | None = None) -> None:
		"""Scroll the page using robust CDP methods."""
		if not self._session_id:
			raise RuntimeError('Session ID is required for scroll operations')

		# Method 1: Try mouse wheel event (most reliable)
		try:
			# Get viewport dimensions
			layout_metrics = await self._client.send.Page.getLayoutMetrics(session_id=self._session_id)
			viewport_width = layout_metrics['layoutViewport']['clientWidth']
			viewport_height = layout_metrics['layoutViewport']['clientHeight']

			# Use provided coordinates or center of viewport
			scroll_x = x if x > 0 else viewport_width / 2
			scroll_y = y if y > 0 else viewport_height / 2

			# Calculate scroll deltas (positive = down/right)
			scroll_delta_x = delta_x or 0
			scroll_delta_y = delta_y or 0

			# Dispatch mouse wheel event
			await self._client.send.Input.dispatchMouseEvent(
				params={
					'type': 'mouseWheel',
					'x': scroll_x,
					'y': scroll_y,
					'deltaX': scroll_delta_x,
					'deltaY': scroll_delta_y,
				},
				session_id=self._session_id,
			)
			return

		except Exception:
			pass

		# Method 2: Fallback to synthesizeScrollGesture
		try:
			params: 'SynthesizeScrollGestureParameters' = {'x': x, 'y': y, 'xDistance': delta_x or 0, 'yDistance': delta_y or 0}
			await self._client.send.Input.synthesizeScrollGesture(
				params,
				session_id=self._session_id,
			)
		except Exception:
			# Method 3: JavaScript fallback
			scroll_js = f'window.scrollBy({delta_x or 0}, {delta_y or 0})'
			await self._client.send.Runtime.evaluate(
				params={'expression': scroll_js, 'returnByValue': True},
				session_id=self._session_id,
			)

```

---

## backend/browser-use/browser_use/actor/page.py

```py
"""Page class for page-level operations."""

from typing import TYPE_CHECKING, TypeVar

from pydantic import BaseModel

from browser_use.actor.utils import get_key_info
from browser_use.dom.serializer.serializer import DOMTreeSerializer
from browser_use.dom.service import DomService
from browser_use.llm.messages import SystemMessage, UserMessage

T = TypeVar('T', bound=BaseModel)

if TYPE_CHECKING:
	from cdp_use.cdp.dom.commands import (
		DescribeNodeParameters,
		QuerySelectorAllParameters,
	)
	from cdp_use.cdp.emulation.commands import SetDeviceMetricsOverrideParameters
	from cdp_use.cdp.input.commands import (
		DispatchKeyEventParameters,
	)
	from cdp_use.cdp.page.commands import CaptureScreenshotParameters, NavigateParameters, NavigateToHistoryEntryParameters
	from cdp_use.cdp.runtime.commands import EvaluateParameters
	from cdp_use.cdp.target.commands import (
		AttachToTargetParameters,
		GetTargetInfoParameters,
	)
	from cdp_use.cdp.target.types import TargetInfo

	from browser_use.browser.session import BrowserSession
	from browser_use.llm.base import BaseChatModel

	from .element import Element
	from .mouse import Mouse


class Page:
	"""Page operations (tab or iframe)."""

	def __init__(
		self, browser_session: 'BrowserSession', target_id: str, session_id: str | None = None, llm: 'BaseChatModel | None' = None
	):
		self._browser_session = browser_session
		self._client = browser_session.cdp_client
		self._target_id = target_id
		self._session_id: str | None = session_id
		self._mouse: 'Mouse | None' = None

		self._llm = llm

	async def _ensure_session(self) -> str:
		"""Ensure we have a session ID for this target."""
		if not self._session_id:
			params: 'AttachToTargetParameters' = {'targetId': self._target_id, 'flatten': True}
			result = await self._client.send.Target.attachToTarget(params)
			self._session_id = result['sessionId']

			# Enable necessary domains
			import asyncio

			await asyncio.gather(
				self._client.send.Page.enable(session_id=self._session_id),
				self._client.send.DOM.enable(session_id=self._session_id),
				self._client.send.Runtime.enable(session_id=self._session_id),
				self._client.send.Network.enable(session_id=self._session_id),
			)

		return self._session_id

	@property
	async def session_id(self) -> str:
		"""Get the session ID for this target.

		@dev Pass this to an arbitrary CDP call
		"""
		return await self._ensure_session()

	@property
	async def mouse(self) -> 'Mouse':
		"""Get the mouse interface for this target."""
		if not self._mouse:
			session_id = await self._ensure_session()
			from .mouse import Mouse

			self._mouse = Mouse(self._browser_session, session_id, self._target_id)
		return self._mouse

	async def reload(self) -> None:
		"""Reload the target."""
		session_id = await self._ensure_session()
		await self._client.send.Page.reload(session_id=session_id)

	async def get_element(self, backend_node_id: int) -> 'Element':
		"""Get an element by its backend node ID."""
		session_id = await self._ensure_session()

		from .element import Element as Element_

		return Element_(self._browser_session, backend_node_id, session_id)

	async def evaluate(self, page_function: str, *args) -> str:
		"""Execute JavaScript in the target.

		Args:
			page_function: JavaScript code that MUST start with (...args) => format
			*args: Arguments to pass to the function

		Returns:
			String representation of the JavaScript execution result.
			Objects and arrays are JSON-stringified.
		"""
		session_id = await self._ensure_session()

		# Clean and fix common JavaScript string parsing issues
		page_function = self._fix_javascript_string(page_function)

		# Enforce arrow function format
		if not (page_function.startswith('(') and '=>' in page_function):
			raise ValueError(f'JavaScript code must start with (...args) => format. Got: {page_function[:50]}...')

		# Build the expression - call the arrow function with provided args
		if args:
			# Convert args to JSON representation for safe passing
			import json

			arg_strs = [json.dumps(arg) for arg in args]
			expression = f'({page_function})({", ".join(arg_strs)})'
		else:
			expression = f'({page_function})()'

		# Debug: print the actual expression being evaluated
		print(f'DEBUG: Evaluating JavaScript: {repr(expression)}')

		params: 'EvaluateParameters' = {'expression': expression, 'returnByValue': True, 'awaitPromise': True}
		result = await self._client.send.Runtime.evaluate(
			params,
			session_id=session_id,
		)

		if 'exceptionDetails' in result:
			raise RuntimeError(f'JavaScript evaluation failed: {result["exceptionDetails"]}')

		value = result.get('result', {}).get('value')

		# Always return string representation
		if value is None:
			return ''
		elif isinstance(value, str):
			return value
		else:
			# Convert objects, numbers, booleans to string
			import json

			try:
				return json.dumps(value) if isinstance(value, (dict, list)) else str(value)
			except (TypeError, ValueError):
				return str(value)

	def _fix_javascript_string(self, js_code: str) -> str:
		"""Fix common JavaScript string parsing issues when written as Python string."""

		# Just do minimal, safe cleaning
		js_code = js_code.strip()

		# Only fix the most common and safe issues:

		# 1. Remove obvious Python string wrapper quotes if they exist
		if (js_code.startswith('"') and js_code.endswith('"')) or (js_code.startswith("'") and js_code.endswith("'")):
			# Check if it's a wrapped string (not part of JS syntax)
			inner = js_code[1:-1]
			if inner.count('"') + inner.count("'") == 0 or '() =>' in inner:
				js_code = inner

		# 2. Only fix clearly escaped quotes that shouldn't be
		# But be very conservative - only if we're sure it's a Python string artifact
		if '\\"' in js_code and js_code.count('\\"') > js_code.count('"'):
			js_code = js_code.replace('\\"', '"')
		if "\\'" in js_code and js_code.count("\\'") > js_code.count("'"):
			js_code = js_code.replace("\\'", "'")

		# 3. Basic whitespace normalization only
		js_code = js_code.strip()

		# Final validation - ensure it's not empty
		if not js_code:
			raise ValueError('JavaScript code is empty after cleaning')

		return js_code

	async def screenshot(self, format: str = 'png', quality: int | None = None) -> str:
		"""Take a screenshot and return base64 encoded image.

		Args:
		    format: Image format ('jpeg', 'png', 'webp')
		    quality: Quality 0-100 for JPEG format

		Returns:
		    Base64-encoded image data
		"""
		session_id = await self._ensure_session()

		params: 'CaptureScreenshotParameters' = {'format': format}

		if quality is not None and format.lower() == 'jpeg':
			params['quality'] = quality

		result = await self._client.send.Page.captureScreenshot(params, session_id=session_id)

		return result['data']

	async def press(self, key: str) -> None:
		"""Press a key on the page (sends keyboard input to the focused element or page)."""
		session_id = await self._ensure_session()

		# Handle key combinations like "Control+A"
		if '+' in key:
			parts = key.split('+')
			modifiers = parts[:-1]
			main_key = parts[-1]

			# Calculate modifier bitmask
			modifier_value = 0
			modifier_map = {'Alt': 1, 'Control': 2, 'Meta': 4, 'Shift': 8}
			for mod in modifiers:
				modifier_value |= modifier_map.get(mod, 0)

			# Press modifier keys
			for mod in modifiers:
				code, vk_code = get_key_info(mod)
				params: 'DispatchKeyEventParameters' = {'type': 'keyDown', 'key': mod, 'code': code}
				if vk_code is not None:
					params['windowsVirtualKeyCode'] = vk_code
				await self._client.send.Input.dispatchKeyEvent(params, session_id=session_id)

			# Press main key with modifiers bitmask
			main_code, main_vk_code = get_key_info(main_key)
			main_down_params: 'DispatchKeyEventParameters' = {
				'type': 'keyDown',
				'key': main_key,
				'code': main_code,
				'modifiers': modifier_value,
			}
			if main_vk_code is not None:
				main_down_params['windowsVirtualKeyCode'] = main_vk_code
			await self._client.send.Input.dispatchKeyEvent(main_down_params, session_id=session_id)

			main_up_params: 'DispatchKeyEventParameters' = {
				'type': 'keyUp',
				'key': main_key,
				'code': main_code,
				'modifiers': modifier_value,
			}
			if main_vk_code is not None:
				main_up_params['windowsVirtualKeyCode'] = main_vk_code
			await self._client.send.Input.dispatchKeyEvent(main_up_params, session_id=session_id)

			# Release modifier keys
			for mod in reversed(modifiers):
				code, vk_code = get_key_info(mod)
				release_params: 'DispatchKeyEventParameters' = {'type': 'keyUp', 'key': mod, 'code': code}
				if vk_code is not None:
					release_params['windowsVirtualKeyCode'] = vk_code
				await self._client.send.Input.dispatchKeyEvent(release_params, session_id=session_id)
		else:
			# Simple key press
			code, vk_code = get_key_info(key)
			key_down_params: 'DispatchKeyEventParameters' = {'type': 'keyDown', 'key': key, 'code': code}
			if vk_code is not None:
				key_down_params['windowsVirtualKeyCode'] = vk_code
			await self._client.send.Input.dispatchKeyEvent(key_down_params, session_id=session_id)

			key_up_params: 'DispatchKeyEventParameters' = {'type': 'keyUp', 'key': key, 'code': code}
			if vk_code is not None:
				key_up_params['windowsVirtualKeyCode'] = vk_code
			await self._client.send.Input.dispatchKeyEvent(key_up_params, session_id=session_id)

	async def set_viewport_size(self, width: int, height: int) -> None:
		"""Set the viewport size."""
		session_id = await self._ensure_session()

		params: 'SetDeviceMetricsOverrideParameters' = {
			'width': width,
			'height': height,
			'deviceScaleFactor': 1.0,
			'mobile': False,
		}
		await self._client.send.Emulation.setDeviceMetricsOverride(
			params,
			session_id=session_id,
		)

	# Target properties (from CDP getTargetInfo)
	async def get_target_info(self) -> 'TargetInfo':
		"""Get target information."""
		params: 'GetTargetInfoParameters' = {'targetId': self._target_id}
		result = await self._client.send.Target.getTargetInfo(params)
		return result['targetInfo']

	async def get_url(self) -> str:
		"""Get the current URL."""
		info = await self.get_target_info()
		return info.get('url', '')

	async def get_title(self) -> str:
		"""Get the current title."""
		info = await self.get_target_info()
		return info.get('title', '')

	async def goto(self, url: str) -> None:
		"""Navigate this target to a URL."""
		session_id = await self._ensure_session()

		params: 'NavigateParameters' = {'url': url}
		await self._client.send.Page.navigate(params, session_id=session_id)

	async def navigate(self, url: str) -> None:
		"""Alias for goto."""
		await self.goto(url)

	async def go_back(self) -> None:
		"""Navigate back in history."""
		session_id = await self._ensure_session()

		try:
			# Get navigation history
			history = await self._client.send.Page.getNavigationHistory(session_id=session_id)
			current_index = history['currentIndex']
			entries = history['entries']

			# Check if we can go back
			if current_index <= 0:
				raise RuntimeError('Cannot go back - no previous entry in history')

			# Navigate to the previous entry
			previous_entry_id = entries[current_index - 1]['id']
			params: 'NavigateToHistoryEntryParameters' = {'entryId': previous_entry_id}
			await self._client.send.Page.navigateToHistoryEntry(params, session_id=session_id)

		except Exception as e:
			raise RuntimeError(f'Failed to navigate back: {e}')

	async def go_forward(self) -> None:
		"""Navigate forward in history."""
		session_id = await self._ensure_session()

		try:
			# Get navigation history
			history = await self._client.send.Page.getNavigationHistory(session_id=session_id)
			current_index = history['currentIndex']
			entries = history['entries']

			# Check if we can go forward
			if current_index >= len(entries) - 1:
				raise RuntimeError('Cannot go forward - no next entry in history')

			# Navigate to the next entry
			next_entry_id = entries[current_index + 1]['id']
			params: 'NavigateToHistoryEntryParameters' = {'entryId': next_entry_id}
			await self._client.send.Page.navigateToHistoryEntry(params, session_id=session_id)

		except Exception as e:
			raise RuntimeError(f'Failed to navigate forward: {e}')

	# Element finding methods (these would need to be implemented based on DOM queries)
	async def get_elements_by_css_selector(self, selector: str) -> list['Element']:
		"""Get elements by CSS selector."""
		session_id = await self._ensure_session()

		# Get document first
		doc_result = await self._client.send.DOM.getDocument(session_id=session_id)
		document_node_id = doc_result['root']['nodeId']

		# Query selector all
		query_params: 'QuerySelectorAllParameters' = {'nodeId': document_node_id, 'selector': selector}
		result = await self._client.send.DOM.querySelectorAll(query_params, session_id=session_id)

		elements = []
		from .element import Element as Element_

		# Convert node IDs to backend node IDs
		for node_id in result['nodeIds']:
			# Get backend node ID
			describe_params: 'DescribeNodeParameters' = {'nodeId': node_id}
			node_result = await self._client.send.DOM.describeNode(describe_params, session_id=session_id)
			backend_node_id = node_result['node']['backendNodeId']
			elements.append(Element_(self._browser_session, backend_node_id, session_id))

		return elements

	# AI METHODS

	@property
	def dom_service(self) -> 'DomService':
		"""Get the DOM service for this target."""
		return DomService(self._browser_session)

	async def get_element_by_prompt(self, prompt: str, llm: 'BaseChatModel | None' = None) -> 'Element | None':
		"""Get an element by a prompt."""
		await self._ensure_session()
		llm = llm or self._llm

		if not llm:
			raise ValueError('LLM not provided')

		dom_service = self.dom_service

		# Lazy fetch all_frames inside get_dom_tree if needed (for cross-origin iframes)
		enhanced_dom_tree, _ = await dom_service.get_dom_tree(target_id=self._target_id, all_frames=None)

		session_id = self._browser_session.id
		serialized_dom_state, _ = DOMTreeSerializer(
			enhanced_dom_tree, None, paint_order_filtering=True, session_id=session_id
		).serialize_accessible_elements()

		llm_representation = serialized_dom_state.llm_representation()

		system_message = SystemMessage(
			content="""You are an AI created to find an element on a page by a prompt.

<browser_state>
Interactive Elements: All interactive elements will be provided in format as [index]<type>text</type> where
- index: Numeric identifier for interaction
- type: HTML element type (button, input, etc.)
- text: Element description

Examples:
[33]<div>User form</div>
[35]<button aria-label='Submit form'>Submit</button>

Note that:
- Only elements with numeric indexes in [] are interactive
- (stacked) indentation (with \t) is important and means that the element is a (html) child of the element above (with a lower index)
- Pure text elements without [] are not interactive.
</browser_state>

Your task is to find an element index (if any) that matches the prompt (written in <prompt> tag).

If non of the elements matches the, return None.

Before you return the element index, reason about the state and elements for a sentence or two."""
		)

		state_message = UserMessage(
			content=f"""
			<browser_state>
			{llm_representation}
			</browser_state>

			<prompt>
			{prompt}
			</prompt>
			"""
		)

		class ElementResponse(BaseModel):
			# thinking: str
			element_highlight_index: int | None

		llm_response = await llm.ainvoke(
			[
				system_message,
				state_message,
			],
			output_format=ElementResponse,
		)

		element_highlight_index = llm_response.completion.element_highlight_index

		if element_highlight_index is None or element_highlight_index not in serialized_dom_state.selector_map:
			return None

		element = serialized_dom_state.selector_map[element_highlight_index]

		from .element import Element as Element_

		return Element_(self._browser_session, element.backend_node_id, self._session_id)

	async def must_get_element_by_prompt(self, prompt: str, llm: 'BaseChatModel | None' = None) -> 'Element':
		"""Get an element by a prompt.

		@dev LLM can still return None, this just raises an error if the element is not found.
		"""
		element = await self.get_element_by_prompt(prompt, llm)
		if element is None:
			raise ValueError(f'No element found for prompt: {prompt}')

		return element

	async def extract_content(self, prompt: str, structured_output: type[T], llm: 'BaseChatModel | None' = None) -> T:
		"""Extract structured content from the current page using LLM.

		Extracts clean markdown from the page and sends it to LLM for structured data extraction.

		Args:
			prompt: Description of what content to extract
			structured_output: Pydantic BaseModel class defining the expected output structure
			llm: Language model to use for extraction

		Returns:
			The structured BaseModel instance with extracted content
		"""
		llm = llm or self._llm

		if not llm:
			raise ValueError('LLM not provided')

		# Extract clean markdown using the same method as in tools/service.py
		try:
			content, content_stats = await self._extract_clean_markdown()
		except Exception as e:
			raise RuntimeError(f'Could not extract clean markdown: {type(e).__name__}')

		# System prompt for structured extraction
		system_prompt = """
You are an expert at extracting structured data from the markdown of a webpage.

<input>
You will be given a query and the markdown of a webpage that has been filtered to remove noise and advertising content.
</input>

<instructions>
- You are tasked to extract information from the webpage that is relevant to the query.
- You should ONLY use the information available in the webpage to answer the query. Do not make up information or provide guess from your own knowledge.
- If the information relevant to the query is not available in the page, your response should mention that.
- If the query asks for all items, products, etc., make sure to directly list all of them.
- Return the extracted content in the exact structured format specified.
</instructions>

<output>
- Your output should present ALL the information relevant to the query in the specified structured format.
- Do not answer in conversational format - directly output the relevant information in the structured format.
</output>
""".strip()

		# Build prompt with just query and content
		prompt_content = f'<query>\n{prompt}\n</query>\n\n<webpage_content>\n{content}\n</webpage_content>'

		# Send to LLM with structured output
		import asyncio

		try:
			response = await asyncio.wait_for(
				llm.ainvoke(
					[SystemMessage(content=system_prompt), UserMessage(content=prompt_content)], output_format=structured_output
				),
				timeout=120.0,
			)

			# Return the structured output BaseModel instance
			return response.completion
		except Exception as e:
			raise RuntimeError(str(e))

	async def _extract_clean_markdown(self, extract_links: bool = False) -> tuple[str, dict]:
		"""Extract clean markdown from the current page using enhanced DOM tree.

		Uses the shared markdown extractor for consistency with tools/service.py.
		"""
		from browser_use.dom.markdown_extractor import extract_clean_markdown

		dom_service = self.dom_service
		return await extract_clean_markdown(dom_service=dom_service, target_id=self._target_id, extract_links=extract_links)

```

---

## backend/browser-use/browser_use/actor/playground/flights.py

```py
import asyncio

from browser_use import Agent, Browser, ChatOpenAI

llm = ChatOpenAI('gpt-4.1-mini')


async def main():
	"""
	Main function demonstrating mixed automation with Browser-Use and Playwright.
	"""
	print('ğŸš€ Mixed Automation with Browser-Use and Actor API')

	browser = Browser(keep_alive=True)
	await browser.start()

	page = await browser.get_current_page() or await browser.new_page()

	# Go to apple wikipedia page
	await page.goto('https://www.google.com/travel/flights')

	await asyncio.sleep(1)

	round_trip_button = await page.must_get_element_by_prompt('round trip button', llm)
	await round_trip_button.click()

	one_way_button = await page.must_get_element_by_prompt('one way button', llm)
	await one_way_button.click()

	await asyncio.sleep(1)

	agent = Agent(task='Find the cheapest flight from London to Paris on 2025-10-15', llm=llm, browser_session=browser)
	await agent.run()

	input('Press Enter to continue...')

	await browser.stop()


if __name__ == '__main__':
	asyncio.run(main())

```

---

## backend/browser-use/browser_use/actor/playground/mixed_automation.py

```py
import asyncio

from pydantic import BaseModel

from browser_use import Browser, ChatOpenAI

TASK = """
On the current wikipedia page, find the latest huge edit and tell me what is was about.
"""


class LatestEditFinder(BaseModel):
	"""Find the latest huge edit on the current wikipedia page."""

	latest_edit: str
	edit_time: str
	edit_author: str
	edit_summary: str
	edit_url: str


llm = ChatOpenAI('gpt-4.1-mini')


async def main():
	"""
	Main function demonstrating mixed automation with Browser-Use and Playwright.
	"""
	print('ğŸš€ Mixed Automation with Browser-Use and Actor API')

	browser = Browser(keep_alive=True)
	await browser.start()

	page = await browser.get_current_page() or await browser.new_page()

	# Go to apple wikipedia page
	await page.goto('https://browser-use.github.io/stress-tests/challenges/angularjs-form.html')

	await asyncio.sleep(1)

	element = await page.get_element_by_prompt('zip code input', llm)

	print('Element found', element)

	if element:
		await element.click()
	else:
		print('No element found')

	await browser.stop()


if __name__ == '__main__':
	asyncio.run(main())

```

---

## backend/browser-use/browser_use/actor/playground/playground.py

```py
#!/usr/bin/env python3
"""
Playground script to test the browser-use actor API.

This script demonstrates:
- Starting a browser session
- Using the actor API to navigate and interact
- Finding elements, clicking, scrolling, JavaScript evaluation
- Testing most of the available methods
"""

import asyncio
import json
import logging

from browser_use import Browser

# Configure logging to see what's happening
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


async def main():
	"""Main playground function."""
	logger.info('ğŸš€ Starting browser actor playground')

	# Create browser session
	browser = Browser()

	try:
		# Start the browser
		await browser.start()
		logger.info('âœ… Browser session started')

		# Navigate to Wikipedia using integrated methods
		logger.info('ğŸ“– Navigating to Wikipedia...')
		page = await browser.new_page('https://en.wikipedia.org')

		# Get basic page info
		url = await page.get_url()
		title = await page.get_title()
		logger.info(f'ğŸ“„ Page loaded: {title} ({url})')

		# Take a screenshot
		logger.info('ğŸ“¸ Taking initial screenshot...')
		screenshot_b64 = await page.screenshot()
		logger.info(f'ğŸ“¸ Screenshot captured: {len(screenshot_b64)} bytes')

		# Set viewport size
		logger.info('ğŸ–¥ï¸ Setting viewport to 1920x1080...')
		await page.set_viewport_size(1920, 1080)

		# Execute some JavaScript to count links
		logger.info('ğŸ” Counting article links using JavaScript...')
		js_code = """() => {
			// Find all article links on the page
			const links = Array.from(document.querySelectorAll('a[href*="/wiki/"]:not([href*=":"])'))
				.filter(link => !link.href.includes('Main_Page') && !link.href.includes('Special:'));
			
			return {
				total: links.length,
				sample: links.slice(0, 3).map(link => ({
					href: link.href,
					text: link.textContent.trim() 
				}))
			};
		}"""

		link_info = json.loads(await page.evaluate(js_code))
		logger.info(f'ğŸ”— Found {link_info["total"]} article links')
		# Try to find and interact with links using CSS selector
		try:
			# Find article links on the page
			links = await page.get_elements_by_css_selector('a[href*="/wiki/"]:not([href*=":"])')

			if links:
				logger.info(f'ğŸ“‹ Found {len(links)} wiki links via CSS selector')

				# Pick the first link
				link_element = links[0]

				# Get link info using available methods
				basic_info = await link_element.get_basic_info()
				link_href = await link_element.get_attribute('href')

				logger.info(f'ğŸ¯ Selected element: <{basic_info["nodeName"]}>')
				logger.info(f'ğŸ”— Link href: {link_href}')

				if basic_info['boundingBox']:
					bbox = basic_info['boundingBox']
					logger.info(f'ğŸ“ Position: ({bbox["x"]}, {bbox["y"]}) Size: {bbox["width"]}x{bbox["height"]}')

				# Test element interactions with robust implementations
				logger.info('ğŸ‘† Hovering over the element...')
				await link_element.hover()
				await asyncio.sleep(1)

				logger.info('ğŸ” Focusing the element...')
				await link_element.focus()
				await asyncio.sleep(0.5)

				# Click the link using robust click method
				logger.info('ğŸ–±ï¸ Clicking the link with robust fallbacks...')
				await link_element.click()

				# Wait for navigation
				await asyncio.sleep(3)

				# Get new page info
				new_url = await page.get_url()
				new_title = await page.get_title()
				logger.info(f'ğŸ“„ Navigated to: {new_title}')
				logger.info(f'ğŸŒ New URL: {new_url}')
			else:
				logger.warning('âŒ No links found to interact with')

		except Exception as e:
			logger.warning(f'âš ï¸ Link interaction failed: {e}')

		# Scroll down the page
		logger.info('ğŸ“œ Scrolling down the page...')
		mouse = await page.mouse
		await mouse.scroll(x=0, y=100, delta_y=500)
		await asyncio.sleep(1)

		# Test mouse operations
		logger.info('ğŸ–±ï¸ Testing mouse operations...')
		await mouse.move(x=100, y=200)
		await mouse.click(x=150, y=250)

		# Execute more JavaScript examples
		logger.info('ğŸ§ª Testing JavaScript evaluation...')

		# Simple expressions
		page_height = await page.evaluate('() => document.body.scrollHeight')
		current_scroll = await page.evaluate('() => window.pageYOffset')
		logger.info(f'ğŸ“ Page height: {page_height}px, current scroll: {current_scroll}px')

		# JavaScript with arguments
		result = await page.evaluate('(x) => x * 2', 21)
		logger.info(f'ğŸ§® JavaScript with args: 21 * 2 = {result}')

		# More complex JavaScript
		page_stats = json.loads(
			await page.evaluate("""() => {
			return {
				url: window.location.href,
				title: document.title,
				links: document.querySelectorAll('a').length,
				images: document.querySelectorAll('img').length,
				scrollTop: window.pageYOffset,
				viewportHeight: window.innerHeight
			};
		}""")
		)
		logger.info(f'ğŸ“Š Page stats: {page_stats}')

		# Get page title using different methods
		title_via_js = await page.evaluate('() => document.title')
		title_via_api = await page.get_title()
		logger.info(f'ğŸ“ Title via JS: "{title_via_js}"')
		logger.info(f'ğŸ“ Title via API: "{title_via_api}"')

		# Take a final screenshot
		logger.info('ğŸ“¸ Taking final screenshot...')
		final_screenshot = await page.screenshot()
		logger.info(f'ğŸ“¸ Final screenshot: {len(final_screenshot)} bytes')

		# Test browser navigation with error handling
		logger.info('â¬…ï¸ Testing browser back navigation...')
		try:
			await page.go_back()
			await asyncio.sleep(2)

			back_url = await page.get_url()
			back_title = await page.get_title()
			logger.info(f'ğŸ“„ After going back: {back_title}')
			logger.info(f'ğŸŒ Back URL: {back_url}')
		except RuntimeError as e:
			logger.info(f'â„¹ï¸ Navigation back failed as expected: {e}')

		# Test creating new page
		logger.info('ğŸ†• Creating new blank page...')
		new_page = await browser.new_page()
		new_page_url = await new_page.get_url()
		logger.info(f'ğŸ†• New page created with URL: {new_page_url}')

		# Get all pages
		all_pages = await browser.get_pages()
		logger.info(f'ğŸ“‘ Total pages: {len(all_pages)}')

		# Test form interaction if we can find a form
		try:
			# Look for search input on the page
			search_inputs = await page.get_elements_by_css_selector('input[type="search"], input[name*="search"]')

			if search_inputs:
				search_input = search_inputs[0]
				logger.info('ğŸ” Found search input, testing form interaction...')

				await search_input.focus()
				await search_input.fill('test search query')
				await page.press('Enter')

				logger.info('âœ… Form interaction test completed')
			else:
				logger.info('â„¹ï¸ No search inputs found for form testing')

		except Exception as e:
			logger.info(f'â„¹ï¸ Form interaction test skipped: {e}')

			# wait 2 seconds before closing the new page
		logger.info('ğŸ•’ Waiting 2 seconds before closing the new page...')
		await asyncio.sleep(2)
		logger.info('ğŸ—‘ï¸ Closing new page...')
		await browser.close_page(new_page)

		logger.info('âœ… Playground completed successfully!')

		input('Press Enter to continue...')

	except Exception as e:
		logger.error(f'âŒ Error in playground: {e}', exc_info=True)

	finally:
		# Clean up
		logger.info('ğŸ§¹ Cleaning up...')
		try:
			await browser.stop()
			logger.info('âœ… Browser session stopped')
		except Exception as e:
			logger.error(f'âŒ Error stopping browser: {e}')


if __name__ == '__main__':
	asyncio.run(main())

```

---

## backend/browser-use/browser_use/actor/utils.py

```py
"""Utility functions for actor operations."""


class Utils:
	"""Utility functions for actor operations."""

	@staticmethod
	def get_key_info(key: str) -> tuple[str, int | None]:
		"""Get the code and windowsVirtualKeyCode for a key.

		Args:
			key: Key name (e.g., 'Enter', 'ArrowUp', 'a', 'A')

		Returns:
			Tuple of (code, windowsVirtualKeyCode)

		Reference: Windows Virtual Key Codes
		https://docs.microsoft.com/en-us/windows/win32/inputdev/virtual-key-codes
		"""
		# Complete mapping of key names to (code, virtualKeyCode)
		# Based on standard Windows Virtual Key Codes
		key_map = {
			# Navigation keys
			'Backspace': ('Backspace', 8),
			'Tab': ('Tab', 9),
			'Enter': ('Enter', 13),
			'Escape': ('Escape', 27),
			'Space': ('Space', 32),
			' ': ('Space', 32),
			'PageUp': ('PageUp', 33),
			'PageDown': ('PageDown', 34),
			'End': ('End', 35),
			'Home': ('Home', 36),
			'ArrowLeft': ('ArrowLeft', 37),
			'ArrowUp': ('ArrowUp', 38),
			'ArrowRight': ('ArrowRight', 39),
			'ArrowDown': ('ArrowDown', 40),
			'Insert': ('Insert', 45),
			'Delete': ('Delete', 46),
			# Modifier keys
			'Shift': ('ShiftLeft', 16),
			'ShiftLeft': ('ShiftLeft', 16),
			'ShiftRight': ('ShiftRight', 16),
			'Control': ('ControlLeft', 17),
			'ControlLeft': ('ControlLeft', 17),
			'ControlRight': ('ControlRight', 17),
			'Alt': ('AltLeft', 18),
			'AltLeft': ('AltLeft', 18),
			'AltRight': ('AltRight', 18),
			'Meta': ('MetaLeft', 91),
			'MetaLeft': ('MetaLeft', 91),
			'MetaRight': ('MetaRight', 92),
			# Function keys F1-F24
			'F1': ('F1', 112),
			'F2': ('F2', 113),
			'F3': ('F3', 114),
			'F4': ('F4', 115),
			'F5': ('F5', 116),
			'F6': ('F6', 117),
			'F7': ('F7', 118),
			'F8': ('F8', 119),
			'F9': ('F9', 120),
			'F10': ('F10', 121),
			'F11': ('F11', 122),
			'F12': ('F12', 123),
			'F13': ('F13', 124),
			'F14': ('F14', 125),
			'F15': ('F15', 126),
			'F16': ('F16', 127),
			'F17': ('F17', 128),
			'F18': ('F18', 129),
			'F19': ('F19', 130),
			'F20': ('F20', 131),
			'F21': ('F21', 132),
			'F22': ('F22', 133),
			'F23': ('F23', 134),
			'F24': ('F24', 135),
			# Numpad keys
			'NumLock': ('NumLock', 144),
			'Numpad0': ('Numpad0', 96),
			'Numpad1': ('Numpad1', 97),
			'Numpad2': ('Numpad2', 98),
			'Numpad3': ('Numpad3', 99),
			'Numpad4': ('Numpad4', 100),
			'Numpad5': ('Numpad5', 101),
			'Numpad6': ('Numpad6', 102),
			'Numpad7': ('Numpad7', 103),
			'Numpad8': ('Numpad8', 104),
			'Numpad9': ('Numpad9', 105),
			'NumpadMultiply': ('NumpadMultiply', 106),
			'NumpadAdd': ('NumpadAdd', 107),
			'NumpadSubtract': ('NumpadSubtract', 109),
			'NumpadDecimal': ('NumpadDecimal', 110),
			'NumpadDivide': ('NumpadDivide', 111),
			# Lock keys
			'CapsLock': ('CapsLock', 20),
			'ScrollLock': ('ScrollLock', 145),
			# OEM/Punctuation keys (US keyboard layout)
			'Semicolon': ('Semicolon', 186),
			';': ('Semicolon', 186),
			'Equal': ('Equal', 187),
			'=': ('Equal', 187),
			'Comma': ('Comma', 188),
			',': ('Comma', 188),
			'Minus': ('Minus', 189),
			'-': ('Minus', 189),
			'Period': ('Period', 190),
			'.': ('Period', 190),
			'Slash': ('Slash', 191),
			'/': ('Slash', 191),
			'Backquote': ('Backquote', 192),
			'`': ('Backquote', 192),
			'BracketLeft': ('BracketLeft', 219),
			'[': ('BracketLeft', 219),
			'Backslash': ('Backslash', 220),
			'\\': ('Backslash', 220),
			'BracketRight': ('BracketRight', 221),
			']': ('BracketRight', 221),
			'Quote': ('Quote', 222),
			"'": ('Quote', 222),
			# Media/Browser keys
			'AudioVolumeMute': ('AudioVolumeMute', 173),
			'AudioVolumeDown': ('AudioVolumeDown', 174),
			'AudioVolumeUp': ('AudioVolumeUp', 175),
			'MediaTrackNext': ('MediaTrackNext', 176),
			'MediaTrackPrevious': ('MediaTrackPrevious', 177),
			'MediaStop': ('MediaStop', 178),
			'MediaPlayPause': ('MediaPlayPause', 179),
			'BrowserBack': ('BrowserBack', 166),
			'BrowserForward': ('BrowserForward', 167),
			'BrowserRefresh': ('BrowserRefresh', 168),
			'BrowserStop': ('BrowserStop', 169),
			'BrowserSearch': ('BrowserSearch', 170),
			'BrowserFavorites': ('BrowserFavorites', 171),
			'BrowserHome': ('BrowserHome', 172),
			# Additional common keys
			'Clear': ('Clear', 12),
			'Pause': ('Pause', 19),
			'Select': ('Select', 41),
			'Print': ('Print', 42),
			'Execute': ('Execute', 43),
			'PrintScreen': ('PrintScreen', 44),
			'Help': ('Help', 47),
			'ContextMenu': ('ContextMenu', 93),
		}

		if key in key_map:
			return key_map[key]

		# Handle alphanumeric keys dynamically
		if len(key) == 1:
			if key.isalpha():
				# Letter keys: A-Z have VK codes 65-90
				return (f'Key{key.upper()}', ord(key.upper()))
			elif key.isdigit():
				# Digit keys: 0-9 have VK codes 48-57 (same as ASCII)
				return (f'Digit{key}', ord(key))

		# Fallback: use the key name as code, no virtual key code
		return (key, None)


# Backward compatibility: provide standalone function
def get_key_info(key: str) -> tuple[str, int | None]:
	"""Get the code and windowsVirtualKeyCode for a key.

	Args:
		key: Key name (e.g., 'Enter', 'ArrowUp', 'a', 'A')

	Returns:
		Tuple of (code, windowsVirtualKeyCode)

	Reference: Windows Virtual Key Codes
	https://docs.microsoft.com/en-us/windows/win32/inputdev/virtual-key-codes
	"""
	return Utils.get_key_info(key)

```

---

## backend/browser-use/browser_use/agent/cloud_events.py

```py
import base64
import os
from datetime import datetime, timezone
from pathlib import Path

import anyio
from bubus import BaseEvent
from pydantic import Field, field_validator
from uuid_extensions import uuid7str

MAX_STRING_LENGTH = 100000  # 100K chars ~ 25k tokens should be enough
MAX_URL_LENGTH = 100000
MAX_TASK_LENGTH = 100000
MAX_COMMENT_LENGTH = 2000
MAX_FILE_CONTENT_SIZE = 50 * 1024 * 1024  # 50MB


class UpdateAgentTaskEvent(BaseEvent):
	# Required fields for identification
	id: str  # The task ID to update
	user_id: str = Field(max_length=255)  # For authorization
	device_id: str | None = Field(None, max_length=255)  # Device ID for auth lookup

	# Optional fields that can be updated
	stopped: bool | None = None
	paused: bool | None = None
	done_output: str | None = Field(None, max_length=MAX_STRING_LENGTH)
	finished_at: datetime | None = None
	agent_state: dict | None = None
	user_feedback_type: str | None = Field(None, max_length=10)  # UserFeedbackType enum value as string
	user_comment: str | None = Field(None, max_length=MAX_COMMENT_LENGTH)
	gif_url: str | None = Field(None, max_length=MAX_URL_LENGTH)

	@classmethod
	def from_agent(cls, agent) -> 'UpdateAgentTaskEvent':
		"""Create an UpdateAgentTaskEvent from an Agent instance"""
		if not hasattr(agent, '_task_start_time'):
			raise ValueError('Agent must have _task_start_time attribute')

		done_output = agent.history.final_result() if agent.history else None
		return cls(
			id=str(agent.task_id),
			user_id='',  # To be filled by cloud handler
			device_id=agent.cloud_sync.auth_client.device_id
			if hasattr(agent, 'cloud_sync') and agent.cloud_sync and agent.cloud_sync.auth_client
			else None,
			stopped=agent.state.stopped if hasattr(agent.state, 'stopped') else False,
			paused=agent.state.paused if hasattr(agent.state, 'paused') else False,
			done_output=done_output,
			finished_at=datetime.now(timezone.utc) if agent.history and agent.history.is_done() else None,
			agent_state=agent.state.model_dump() if hasattr(agent.state, 'model_dump') else {},
			user_feedback_type=None,
			user_comment=None,
			gif_url=None,
			# user_feedback_type and user_comment would be set by the API/frontend
			# gif_url would be set after GIF generation if needed
		)


class CreateAgentOutputFileEvent(BaseEvent):
	# Model fields
	id: str = Field(default_factory=uuid7str)
	user_id: str = Field(max_length=255)
	device_id: str | None = Field(None, max_length=255)  # Device ID for auth lookup
	task_id: str
	file_name: str = Field(max_length=255)
	file_content: str | None = None  # Base64 encoded file content
	content_type: str | None = Field(None, max_length=100)  # MIME type for file uploads
	created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))

	@field_validator('file_content')
	@classmethod
	def validate_file_size(cls, v: str | None) -> str | None:
		"""Validate base64 file content size."""
		if v is None:
			return v
		# Remove data URL prefix if present
		if ',' in v:
			v = v.split(',')[1]
		# Estimate decoded size (base64 is ~33% larger)
		estimated_size = len(v) * 3 / 4
		if estimated_size > MAX_FILE_CONTENT_SIZE:
			raise ValueError(f'File content exceeds maximum size of {MAX_FILE_CONTENT_SIZE / 1024 / 1024}MB')
		return v

	@classmethod
	async def from_agent_and_file(cls, agent, output_path: str) -> 'CreateAgentOutputFileEvent':
		"""Create a CreateAgentOutputFileEvent from a file path"""

		gif_path = Path(output_path)
		if not gif_path.exists():
			raise FileNotFoundError(f'File not found: {output_path}')

		gif_size = os.path.getsize(gif_path)

		# Read GIF content for base64 encoding if needed
		gif_content = None
		if gif_size < 50 * 1024 * 1024:  # Only read if < 50MB
			async with await anyio.open_file(gif_path, 'rb') as f:
				gif_bytes = await f.read()
				gif_content = base64.b64encode(gif_bytes).decode('utf-8')

		return cls(
			user_id='',  # To be filled by cloud handler
			device_id=agent.cloud_sync.auth_client.device_id
			if hasattr(agent, 'cloud_sync') and agent.cloud_sync and agent.cloud_sync.auth_client
			else None,
			task_id=str(agent.task_id),
			file_name=gif_path.name,
			file_content=gif_content,  # Base64 encoded
			content_type='image/gif',
		)


class CreateAgentStepEvent(BaseEvent):
	# Model fields
	id: str = Field(default_factory=uuid7str)
	user_id: str = Field(max_length=255)  # Added for authorization checks
	device_id: str | None = Field(None, max_length=255)  # Device ID for auth lookup
	created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
	agent_task_id: str
	step: int
	evaluation_previous_goal: str = Field(max_length=MAX_STRING_LENGTH)
	memory: str = Field(max_length=MAX_STRING_LENGTH)
	next_goal: str = Field(max_length=MAX_STRING_LENGTH)
	actions: list[dict]
	screenshot_url: str | None = Field(None, max_length=MAX_FILE_CONTENT_SIZE)  # ~50MB for base64 images
	url: str = Field(default='', max_length=MAX_URL_LENGTH)

	@field_validator('screenshot_url')
	@classmethod
	def validate_screenshot_size(cls, v: str | None) -> str | None:
		"""Validate screenshot URL or base64 content size."""
		if v is None or not v.startswith('data:'):
			return v
		# It's base64 data, check size
		if ',' in v:
			base64_part = v.split(',')[1]
			estimated_size = len(base64_part) * 3 / 4
			if estimated_size > MAX_FILE_CONTENT_SIZE:
				raise ValueError(f'Screenshot content exceeds maximum size of {MAX_FILE_CONTENT_SIZE / 1024 / 1024}MB')
		return v

	@classmethod
	def from_agent_step(
		cls, agent, model_output, result: list, actions_data: list[dict], browser_state_summary
	) -> 'CreateAgentStepEvent':
		"""Create a CreateAgentStepEvent from agent step data"""
		# Get first action details if available
		first_action = model_output.action[0] if model_output.action else None

		# Extract current state from model output
		current_state = model_output.current_state if hasattr(model_output, 'current_state') else None

		# Capture screenshot as base64 data URL if available
		screenshot_url = None
		if browser_state_summary.screenshot:
			screenshot_url = f'data:image/png;base64,{browser_state_summary.screenshot}'
			import logging

			logger = logging.getLogger(__name__)
			logger.debug(f'ğŸ“¸ Including screenshot in CreateAgentStepEvent, length: {len(browser_state_summary.screenshot)}')
		else:
			import logging

			logger = logging.getLogger(__name__)
			logger.debug('ğŸ“¸ No screenshot in browser_state_summary for CreateAgentStepEvent')

		return cls(
			user_id='',  # To be filled by cloud handler
			device_id=agent.cloud_sync.auth_client.device_id
			if hasattr(agent, 'cloud_sync') and agent.cloud_sync and agent.cloud_sync.auth_client
			else None,
			agent_task_id=str(agent.task_id),
			step=agent.state.n_steps,
			evaluation_previous_goal=current_state.evaluation_previous_goal if current_state else '',
			memory=current_state.memory if current_state else '',
			next_goal=current_state.next_goal if current_state else '',
			actions=actions_data,  # List of action dicts
			url=browser_state_summary.url,
			screenshot_url=screenshot_url,
		)


class CreateAgentTaskEvent(BaseEvent):
	# Model fields
	id: str = Field(default_factory=uuid7str)
	user_id: str = Field(max_length=255)  # Added for authorization checks
	device_id: str | None = Field(None, max_length=255)  # Device ID for auth lookup
	agent_session_id: str
	llm_model: str = Field(max_length=200)  # LLMModel enum value as string
	stopped: bool = False
	paused: bool = False
	task: str = Field(max_length=MAX_TASK_LENGTH)
	done_output: str | None = Field(None, max_length=MAX_STRING_LENGTH)
	scheduled_task_id: str | None = None
	started_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
	finished_at: datetime | None = None
	agent_state: dict = Field(default_factory=dict)
	user_feedback_type: str | None = Field(None, max_length=10)  # UserFeedbackType enum value as string
	user_comment: str | None = Field(None, max_length=MAX_COMMENT_LENGTH)
	gif_url: str | None = Field(None, max_length=MAX_URL_LENGTH)

	@classmethod
	def from_agent(cls, agent) -> 'CreateAgentTaskEvent':
		"""Create a CreateAgentTaskEvent from an Agent instance"""
		return cls(
			id=str(agent.task_id),
			user_id='',  # To be filled by cloud handler
			device_id=agent.cloud_sync.auth_client.device_id
			if hasattr(agent, 'cloud_sync') and agent.cloud_sync and agent.cloud_sync.auth_client
			else None,
			agent_session_id=str(agent.session_id),
			task=agent.task,
			llm_model=agent.llm.model_name,
			agent_state=agent.state.model_dump() if hasattr(agent.state, 'model_dump') else {},
			stopped=False,
			paused=False,
			done_output=None,
			started_at=datetime.fromtimestamp(agent._task_start_time, tz=timezone.utc),
			finished_at=None,
			user_feedback_type=None,
			user_comment=None,
			gif_url=None,
		)


class CreateAgentSessionEvent(BaseEvent):
	# Model fields
	id: str = Field(default_factory=uuid7str)
	user_id: str = Field(max_length=255)
	device_id: str | None = Field(None, max_length=255)  # Device ID for auth lookup
	browser_session_id: str = Field(max_length=255)
	browser_session_live_url: str = Field(max_length=MAX_URL_LENGTH)
	browser_session_cdp_url: str = Field(max_length=MAX_URL_LENGTH)
	browser_session_stopped: bool = False
	browser_session_stopped_at: datetime | None = None
	is_source_api: bool | None = None
	browser_state: dict = Field(default_factory=dict)
	browser_session_data: dict | None = None

	@classmethod
	def from_agent(cls, agent) -> 'CreateAgentSessionEvent':
		"""Create a CreateAgentSessionEvent from an Agent instance"""
		return cls(
			id=str(agent.session_id),
			user_id='',  # To be filled by cloud handler
			device_id=agent.cloud_sync.auth_client.device_id
			if hasattr(agent, 'cloud_sync') and agent.cloud_sync and agent.cloud_sync.auth_client
			else None,
			browser_session_id=agent.browser_session.id,
			browser_session_live_url='',  # To be filled by cloud handler
			browser_session_cdp_url='',  # To be filled by cloud handler
			browser_state={
				'viewport': agent.browser_profile.viewport if agent.browser_profile else {'width': 1280, 'height': 720},
				'user_agent': agent.browser_profile.user_agent if agent.browser_profile else None,
				'headless': agent.browser_profile.headless if agent.browser_profile else True,
				'initial_url': None,  # Will be updated during execution
				'final_url': None,  # Will be updated during execution
				'total_pages_visited': 0,  # Will be updated during execution
				'session_duration_seconds': 0,  # Will be updated during execution
			},
			browser_session_data={
				'cookies': [],
				'secrets': {},
				# TODO: send secrets safely so tasks can be replayed on cloud seamlessly
				# 'secrets': dict(agent.sensitive_data) if agent.sensitive_data else {},
				'allowed_domains': agent.browser_profile.allowed_domains if agent.browser_profile else [],
			},
		)


class UpdateAgentSessionEvent(BaseEvent):
	"""Event to update an existing agent session"""

	# Model fields
	id: str  # Session ID to update
	user_id: str = Field(max_length=255)
	device_id: str | None = Field(None, max_length=255)
	browser_session_stopped: bool | None = None
	browser_session_stopped_at: datetime | None = None
	end_reason: str | None = Field(None, max_length=100)  # Why the session ended

```

---

## backend/browser-use/browser_use/agent/gif.py

```py
from __future__ import annotations

import base64
import io
import logging
import os
import platform
from typing import TYPE_CHECKING

from browser_use.agent.views import AgentHistoryList
from browser_use.browser.views import PLACEHOLDER_4PX_SCREENSHOT
from browser_use.config import CONFIG

if TYPE_CHECKING:
	from PIL import Image, ImageFont

logger = logging.getLogger(__name__)


def decode_unicode_escapes_to_utf8(text: str) -> str:
	"""Handle decoding any unicode escape sequences embedded in a string (needed to render non-ASCII languages like chinese or arabic in the GIF overlay text)"""

	if r'\u' not in text:
		# doesn't have any escape sequences that need to be decoded
		return text

	try:
		# Try to decode Unicode escape sequences
		return text.encode('latin1').decode('unicode_escape')
	except (UnicodeEncodeError, UnicodeDecodeError):
		# logger.debug(f"Failed to decode unicode escape sequences while generating gif text: {text}")
		return text


def create_history_gif(
	task: str,
	history: AgentHistoryList,
	#
	output_path: str = 'agent_history.gif',
	duration: int = 3000,
	show_goals: bool = True,
	show_task: bool = True,
	show_logo: bool = False,
	font_size: int = 40,
	title_font_size: int = 56,
	goal_font_size: int = 44,
	margin: int = 40,
	line_spacing: float = 1.5,
) -> None:
	"""Create a GIF from the agent's history with overlaid task and goal text."""
	if not history.history:
		logger.warning('No history to create GIF from')
		return

	from PIL import Image, ImageFont

	images = []

	# if history is empty, we can't create a gif
	if not history.history:
		logger.warning('No history to create GIF from')
		return

	# Get all screenshots from history (including None placeholders)
	screenshots = history.screenshots(return_none_if_not_screenshot=True)

	if not screenshots:
		logger.warning('No screenshots found in history')
		return

	# Find the first non-placeholder screenshot
	# A screenshot is considered a placeholder if:
	# 1. It's the exact 4px placeholder for about:blank pages, OR
	# 2. It comes from a new tab page (chrome://newtab/, about:blank, etc.)
	first_real_screenshot = None
	for screenshot in screenshots:
		if screenshot and screenshot != PLACEHOLDER_4PX_SCREENSHOT:
			first_real_screenshot = screenshot
			break

	if not first_real_screenshot:
		logger.warning('No valid screenshots found (all are placeholders or from new tab pages)')
		return

	# Try to load nicer fonts
	try:
		# Try different font options in order of preference
		# ArialUni is a font that comes with Office and can render most non-alphabet characters
		font_options = [
			'PingFang',
			'STHeiti Medium',
			'Microsoft YaHei',  # å¾®è½¯é›…é»‘
			'SimHei',  # é»‘ä½“
			'SimSun',  # å®‹ä½“
			'Noto Sans CJK SC',  # æ€æºé»‘ä½“
			'WenQuanYi Micro Hei',  # æ–‡æ³‰é©¿å¾®ç±³é»‘
			'Helvetica',
			'Arial',
			'DejaVuSans',
			'Verdana',
		]
		font_loaded = False

		for font_name in font_options:
			try:
				if platform.system() == 'Windows':
					# Need to specify the abs font path on Windows
					font_name = os.path.join(CONFIG.WIN_FONT_DIR, font_name + '.ttf')
				regular_font = ImageFont.truetype(font_name, font_size)
				title_font = ImageFont.truetype(font_name, title_font_size)
				goal_font = ImageFont.truetype(font_name, goal_font_size)
				font_loaded = True
				break
			except OSError:
				continue

		if not font_loaded:
			raise OSError('No preferred fonts found')

	except OSError:
		regular_font = ImageFont.load_default()
		title_font = ImageFont.load_default()

		goal_font = regular_font

	# Load logo if requested
	logo = None
	if show_logo:
		try:
			logo = Image.open('./static/browser-use.png')
			# Resize logo to be small (e.g., 40px height)
			logo_height = 150
			aspect_ratio = logo.width / logo.height
			logo_width = int(logo_height * aspect_ratio)
			logo = logo.resize((logo_width, logo_height), Image.Resampling.LANCZOS)
		except Exception as e:
			logger.warning(f'Could not load logo: {e}')

	# Create task frame if requested
	if show_task and task:
		# Find the first non-placeholder screenshot for the task frame
		first_real_screenshot = None
		for item in history.history:
			screenshot_b64 = item.state.get_screenshot()
			if screenshot_b64 and screenshot_b64 != PLACEHOLDER_4PX_SCREENSHOT:
				first_real_screenshot = screenshot_b64
				break

		if first_real_screenshot:
			task_frame = _create_task_frame(
				task,
				first_real_screenshot,
				title_font,  # type: ignore
				regular_font,  # type: ignore
				logo,
				line_spacing,
			)
			images.append(task_frame)
		else:
			logger.warning('No real screenshots found for task frame, skipping task frame')

	# Process each history item with its corresponding screenshot
	for i, (item, screenshot) in enumerate(zip(history.history, screenshots), 1):
		if not screenshot:
			continue

		# Skip placeholder screenshots from about:blank pages
		# These are 4x4 white PNGs encoded as a specific base64 string
		if screenshot == PLACEHOLDER_4PX_SCREENSHOT:
			logger.debug(f'Skipping placeholder screenshot from about:blank page at step {i}')
			continue

		# Skip screenshots from new tab pages
		from browser_use.utils import is_new_tab_page

		if is_new_tab_page(item.state.url):
			logger.debug(f'Skipping screenshot from new tab page ({item.state.url}) at step {i}')
			continue

		# Convert base64 screenshot to PIL Image
		img_data = base64.b64decode(screenshot)
		image = Image.open(io.BytesIO(img_data))

		if show_goals and item.model_output:
			image = _add_overlay_to_image(
				image=image,
				step_number=i,
				goal_text=item.model_output.current_state.next_goal,
				regular_font=regular_font,  # type: ignore
				title_font=title_font,  # type: ignore
				margin=margin,
				logo=logo,
			)

		images.append(image)

	if images:
		# Save the GIF
		images[0].save(
			output_path,
			save_all=True,
			append_images=images[1:],
			duration=duration,
			loop=0,
			optimize=False,
		)
		logger.info(f'Created GIF at {output_path}')
	else:
		logger.warning('No images found in history to create GIF')


def _create_task_frame(
	task: str,
	first_screenshot: str,
	title_font: ImageFont.FreeTypeFont,
	regular_font: ImageFont.FreeTypeFont,
	logo: Image.Image | None = None,
	line_spacing: float = 1.5,
) -> Image.Image:
	"""Create initial frame showing the task."""
	from PIL import Image, ImageDraw, ImageFont

	img_data = base64.b64decode(first_screenshot)
	template = Image.open(io.BytesIO(img_data))
	image = Image.new('RGB', template.size, (0, 0, 0))
	draw = ImageDraw.Draw(image)

	# Calculate vertical center of image
	center_y = image.height // 2

	# Draw task text with dynamic font size based on task length
	margin = 140  # Increased margin
	max_width = image.width - (2 * margin)

	# Dynamic font size calculation based on task length
	# Start with base font size (regular + 16)
	base_font_size = regular_font.size + 16
	min_font_size = max(regular_font.size - 10, 16)  # Don't go below 16pt
	max_font_size = base_font_size  # Cap at the base font size

	# Calculate dynamic font size based on text length and complexity
	# Longer texts get progressively smaller fonts
	text_length = len(task)
	if text_length > 200:
		# For very long text, reduce font size logarithmically
		font_size = max(base_font_size - int(10 * (text_length / 200)), min_font_size)
	else:
		font_size = base_font_size

	# Try to create a larger font, but fall back to regular font if it fails
	try:
		larger_font = ImageFont.truetype(regular_font.path, font_size)  # type: ignore
	except (OSError, AttributeError):
		# Fall back to regular font if .path is not available or font loading fails
		larger_font = regular_font

	# Generate wrapped text with the calculated font size
	wrapped_text = _wrap_text(task, larger_font, max_width)

	# Calculate line height with spacing
	line_height = larger_font.size * line_spacing

	# Split text into lines and draw with custom spacing
	lines = wrapped_text.split('\n')
	total_height = line_height * len(lines)

	# Start position for first line
	text_y = center_y - (total_height / 2) + 50  # Shifted down slightly

	for line in lines:
		# Get line width for centering
		line_bbox = draw.textbbox((0, 0), line, font=larger_font)
		text_x = (image.width - (line_bbox[2] - line_bbox[0])) // 2

		draw.text(
			(text_x, text_y),
			line,
			font=larger_font,
			fill=(255, 255, 255),
		)
		text_y += line_height

	# Add logo if provided (top right corner)
	if logo:
		logo_margin = 20
		logo_x = image.width - logo.width - logo_margin
		image.paste(logo, (logo_x, logo_margin), logo if logo.mode == 'RGBA' else None)

	return image


def _add_overlay_to_image(
	image: Image.Image,
	step_number: int,
	goal_text: str,
	regular_font: ImageFont.FreeTypeFont,
	title_font: ImageFont.FreeTypeFont,
	margin: int,
	logo: Image.Image | None = None,
	display_step: bool = True,
	text_color: tuple[int, int, int, int] = (255, 255, 255, 255),
	text_box_color: tuple[int, int, int, int] = (0, 0, 0, 255),
) -> Image.Image:
	"""Add step number and goal overlay to an image."""

	from PIL import Image, ImageDraw

	goal_text = decode_unicode_escapes_to_utf8(goal_text)
	image = image.convert('RGBA')
	txt_layer = Image.new('RGBA', image.size, (0, 0, 0, 0))
	draw = ImageDraw.Draw(txt_layer)
	if display_step:
		# Add step number (bottom left)
		step_text = str(step_number)
		step_bbox = draw.textbbox((0, 0), step_text, font=title_font)
		step_width = step_bbox[2] - step_bbox[0]
		step_height = step_bbox[3] - step_bbox[1]

		# Position step number in bottom left
		x_step = margin + 10  # Slight additional offset from edge
		y_step = image.height - margin - step_height - 10  # Slight offset from bottom

		# Draw rounded rectangle background for step number
		padding = 20  # Increased padding
		step_bg_bbox = (
			x_step - padding,
			y_step - padding,
			x_step + step_width + padding,
			y_step + step_height + padding,
		)
		draw.rounded_rectangle(
			step_bg_bbox,
			radius=15,  # Add rounded corners
			fill=text_box_color,
		)

		# Draw step number
		draw.text(
			(x_step, y_step),
			step_text,
			font=title_font,
			fill=text_color,
		)

	# Draw goal text (centered, bottom)
	max_width = image.width - (4 * margin)
	wrapped_goal = _wrap_text(goal_text, title_font, max_width)
	goal_bbox = draw.multiline_textbbox((0, 0), wrapped_goal, font=title_font)
	goal_width = goal_bbox[2] - goal_bbox[0]
	goal_height = goal_bbox[3] - goal_bbox[1]

	# Center goal text horizontally, place above step number
	x_goal = (image.width - goal_width) // 2
	y_goal = y_step - goal_height - padding * 4  # More space between step and goal

	# Draw rounded rectangle background for goal
	padding_goal = 25  # Increased padding for goal
	goal_bg_bbox = (
		x_goal - padding_goal,  # Remove extra space for logo
		y_goal - padding_goal,
		x_goal + goal_width + padding_goal,
		y_goal + goal_height + padding_goal,
	)
	draw.rounded_rectangle(
		goal_bg_bbox,
		radius=15,  # Add rounded corners
		fill=text_box_color,
	)

	# Draw goal text
	draw.multiline_text(
		(x_goal, y_goal),
		wrapped_goal,
		font=title_font,
		fill=text_color,
		align='center',
	)

	# Add logo if provided (top right corner)
	if logo:
		logo_layer = Image.new('RGBA', image.size, (0, 0, 0, 0))
		logo_margin = 20
		logo_x = image.width - logo.width - logo_margin
		logo_layer.paste(logo, (logo_x, logo_margin), logo if logo.mode == 'RGBA' else None)
		txt_layer = Image.alpha_composite(logo_layer, txt_layer)

	# Composite and convert
	result = Image.alpha_composite(image, txt_layer)
	return result.convert('RGB')


def _wrap_text(text: str, font: ImageFont.FreeTypeFont, max_width: int) -> str:
	"""
	Wrap text to fit within a given width.

	Args:
	    text: Text to wrap
	    font: Font to use for text
	    max_width: Maximum width in pixels

	Returns:
	    Wrapped text with newlines
	"""
	text = decode_unicode_escapes_to_utf8(text)
	words = text.split()
	lines = []
	current_line = []

	for word in words:
		current_line.append(word)
		line = ' '.join(current_line)
		bbox = font.getbbox(line)
		if bbox[2] > max_width:
			if len(current_line) == 1:
				lines.append(current_line.pop())
			else:
				current_line.pop()
				lines.append(' '.join(current_line))
				current_line = [word]

	if current_line:
		lines.append(' '.join(current_line))

	return '\n'.join(lines)

```

---

## backend/browser-use/browser_use/agent/judge.py

```py
"""Judge system for evaluating browser-use agent execution traces."""

import base64
import logging
from pathlib import Path

from browser_use.llm.messages import (
	BaseMessage,
	ContentPartImageParam,
	ContentPartTextParam,
	ImageURL,
	SystemMessage,
	UserMessage,
)

logger = logging.getLogger(__name__)


def _encode_image(image_path: str) -> str | None:
	"""Encode image to base64 string."""
	try:
		path = Path(image_path)
		if not path.exists():
			return None
		with open(path, 'rb') as f:
			return base64.b64encode(f.read()).decode('utf-8')
	except Exception as e:
		logger.warning(f'Failed to encode image {image_path}: {e}')
		return None


def _truncate_text(text: str, max_length: int, from_beginning: bool = False) -> str:
	"""Truncate text to maximum length with eval system indicator."""
	if len(text) <= max_length:
		return text
	if from_beginning:
		return '...[text truncated]' + text[-max_length + 23 :]
	else:
		return text[: max_length - 23] + '...[text truncated]...'


def construct_judge_messages(
	task: str,
	final_result: str,
	agent_steps: list[str],
	screenshot_paths: list[str],
	max_images: int = 10,
	ground_truth: str | None = None,
) -> list[BaseMessage]:
	"""
	Construct messages for judge evaluation of agent trace.

	Args:
		task: The original task description
		final_result: The final result returned to the user
		agent_steps: List of formatted agent step descriptions
		screenshot_paths: List of screenshot file paths
		max_images: Maximum number of screenshots to include
		ground_truth: Optional ground truth answer or criteria that must be satisfied for success

	Returns:
		List of messages for LLM judge evaluation
	"""
	task_truncated = _truncate_text(task, 40000)
	final_result_truncated = _truncate_text(final_result, 40000)
	steps_text = '\n'.join(agent_steps)
	steps_text_truncated = _truncate_text(steps_text, 40000)

	# Select last N screenshots
	selected_screenshots = screenshot_paths[-max_images:] if len(screenshot_paths) > max_images else screenshot_paths

	# Encode screenshots
	encoded_images: list[ContentPartImageParam] = []
	for img_path in selected_screenshots:
		encoded = _encode_image(img_path)
		if encoded:
			encoded_images.append(
				ContentPartImageParam(
					image_url=ImageURL(
						url=f'data:image/png;base64,{encoded}',
						media_type='image/png',
					)
				)
			)

	# System prompt for judge - conditionally add ground truth section
	ground_truth_section = ''
	if ground_truth:
		ground_truth_section = """
**GROUND TRUTH VALIDATION (HIGHEST PRIORITY):**
The <ground_truth> section contains verified correct information for this task. This can be:
- **Evaluation criteria**: Specific conditions that must be met (e.g., "The success popup should show up", "Must extract exactly 5 items")
- **Factual answers**: The correct answer to a question or information retrieval task (e.g. "10/11/24", "Paris")
- **Expected outcomes**: What should happen after task completion (e.g., "Google Doc must be created", "File should be downloaded")

The ground truth takes ABSOLUTE precedence over all other evaluation criteria. If the ground truth is not satisfied by the agent's execution and final response, the verdict MUST be false.
"""

	system_prompt = f"""You are an expert judge evaluating browser automation agent performance.

<evaluation_framework>
{ground_truth_section}
**PRIMARY EVALUATION CRITERIA (in order of importance):**
1. **Task Satisfaction (Most Important)**: Did the agent accomplish what the user asked for? Break down the task into the key criteria and evaluate if the agent all of them. Focus on user intent and final outcome.
2. **Output Quality**: Is the final result in the correct format and complete? Does it match exactly what was requested?
3. **Tool Effectiveness**: Did the browser interactions work as expected? Were tools used appropriately? How many % of the tools failed? 
4. **Agent Reasoning**: Quality of decision-making, planning, and problem-solving throughout the trajectory. 
5. **Browser Handling**: Navigation stability, error recovery, and technical execution. If the browser crashes, does not load or a captcha blocks the task, the score must be very low.

**VERDICT GUIDELINES:**
- true: Task completed as requested, human-like execution, all of the users criteria were met and the agent did not make up any information.
- false: Task not completed, or only partially completed.

**Examples of task completion verdict:**
- If task asks for 10 items and agent finds 4 items correctly: false
- If task completed to full user requirements but with some errors to improve in the trajectory: true
- If task impossible due to captcha/login requirements: false
- If the trajectory is ideal and the output is perfect: true
- If the task asks to search all headphones in amazon under $100 but the agent searches all headphones and the lowest price is $150: false
- If the task asks to research a property and create a google doc with the result but the agents only returns the results in text: false
- If the task asks to complete an action on the page, and the agent reports that the action is completed but the screenshot or page shows the action is not actually complete: false
- If the task asks to use a certain tool or site to complete the task but the agent completes the task without using it: false
- If the task asks to look for a section of a page that does not exist: false
- If the agent concludes the task is impossible but it is not: false
- If the agent concludes the task is impossible and it truly is impossible: false
- If the agent is unable to complete the task because no login information was provided and it is truly needed to complete the task: false

**FAILURE CONDITIONS (automatically set verdict to false):**
- Blocked by captcha or missing authentication 
- Output format completely wrong or missing
- Infinite loops or severe technical failures
- Critical user requirements ignored
- Page not loaded
- Browser crashed
- Agent could not interact with required UI elements
- The agent moved on from a important step in the task without completing it
- The agent made up content that is not in the screenshot or the page state
- The agent calls done action before completing all key points of the task

**IMPOSSIBLE TASK DETECTION:**
Set `impossible_task` to true when the task fundamentally could not be completed due to:
- Vague or ambiguous task instructions that cannot be reasonably interpreted
- Website genuinely broken or non-functional (be conservative - temporary issues don't count)
- Required links/pages truly inaccessible (404, 403, etc.)
- Task requires authentication/login but no credentials were provided
- Task asks for functionality that doesn't exist on the target site
- Other insurmountable external obstacles beyond the agent's control

Do NOT mark as impossible if:
- Agent made poor decisions but task was achievable
- Temporary page loading issues that could be retried
- Agent didn't try the right approach
- Website works but agent struggled with it

**CAPTCHA DETECTION:**
Set `reached_captcha` to true if:
- Screenshots show captcha challenges (reCAPTCHA, hCaptcha, etc.)
- Agent reports being blocked by bot detection
- Error messages indicate captcha/verification requirements
- Any evidence the agent encountered anti-bot measures during execution

**IMPORTANT EVALUATION NOTES:**
- **evaluate for action** - For each key step of the trace, double check whether the action that the agent tried to performed actually happened. If the required action did not actually occur, the verdict should be false.
- **screenshot is not entire content** - The agent has the entire DOM content, but the screenshot is only part of the content. If the agent extracts information from the page, but you do not see it in the screenshot, you can assume this information is there.
- **Penalize poor tool usage** - Wrong tools, inefficient approaches, ignoring available information.
- **ignore unexpected dates and times** - These agent traces are from varying dates, you can assume the dates the agent uses for search or filtering are correct.
- **IMPORTANT**: be very picky about the user's request - Have very high standard for the agent completing the task exactly to the user's request. 
- **IMPORTANT**: be initially doubtful of the agent's self reported success, be sure to verify that its methods are valid and fulfill the user's desires to a tee.

</evaluation_framework>

<response_format>
Respond with EXACTLY this JSON structure (no additional text before or after):

{{
	"reasoning": "Breakdown of user task into key points. Detailed analysis covering: what went well, what didn't work, trajectory quality assessment, tool usage evaluation, output quality review, and overall user satisfaction prediction.",
	"verdict": true or false,
	"failure_reason": "Max 5 sentences explanation of why the task was not completed successfully in case of failure. If verdict is true, use an empty string.",
	"impossible_task": true or false,
	"reached_captcha": true or false
}}
</response_format>
"""

	# Build user prompt with conditional ground truth section
	ground_truth_prompt = ''
	if ground_truth:
		ground_truth_prompt = f"""
<ground_truth>
{ground_truth}
</ground_truth>
"""

	user_prompt = f"""
<task>
{task_truncated or 'No task provided'}
</task>
{ground_truth_prompt}
<agent_trajectory>
{steps_text_truncated or 'No agent trajectory provided'}
</agent_trajectory>

<final_result>
{final_result_truncated or 'No final result provided'}
</final_result>

{len(encoded_images)} screenshots from execution are attached.

Evaluate this agent execution given the criteria and respond with the exact JSON structure requested."""

	# Build messages with screenshots
	content_parts: list[ContentPartTextParam | ContentPartImageParam] = [ContentPartTextParam(text=user_prompt)]
	content_parts.extend(encoded_images)

	return [
		SystemMessage(content=system_prompt),
		UserMessage(content=content_parts),
	]

```

---

## backend/browser-use/browser_use/agent/message_manager/service.py

```py
from __future__ import annotations

import logging
from typing import Literal

from browser_use.agent.message_manager.views import (
	HistoryItem,
)
from browser_use.agent.prompts import AgentMessagePrompt
from browser_use.agent.views import (
	ActionResult,
	AgentOutput,
	AgentStepInfo,
	MessageManagerState,
)
from browser_use.browser.views import BrowserStateSummary
from browser_use.filesystem.file_system import FileSystem
from browser_use.llm.messages import (
	BaseMessage,
	ContentPartImageParam,
	ContentPartTextParam,
	SystemMessage,
)
from browser_use.observability import observe_debug
from browser_use.utils import match_url_with_domain_pattern, time_execution_sync

logger = logging.getLogger(__name__)


# ========== Logging Helper Functions ==========
# These functions are used ONLY for formatting debug log output.
# They do NOT affect the actual message content sent to the LLM.
# All logging functions start with _log_ for easy identification.


def _log_get_message_emoji(message: BaseMessage) -> str:
	"""Get emoji for a message type - used only for logging display"""
	emoji_map = {
		'UserMessage': 'ğŸ’¬',
		'SystemMessage': 'ğŸ§ ',
		'AssistantMessage': 'ğŸ”¨',
	}
	return emoji_map.get(message.__class__.__name__, 'ğŸ®')


def _log_format_message_line(message: BaseMessage, content: str, is_last_message: bool, terminal_width: int) -> list[str]:
	"""Format a single message for logging display"""
	try:
		lines = []

		# Get emoji and token info
		emoji = _log_get_message_emoji(message)
		# token_str = str(message.metadata.tokens).rjust(4)
		# TODO: fix the token count
		token_str = '??? (TODO)'
		prefix = f'{emoji}[{token_str}]: '

		# Calculate available width (emoji=2 visual cols + [token]: =8 chars)
		content_width = terminal_width - 10

		# Handle last message wrapping
		if is_last_message and len(content) > content_width:
			# Find a good break point
			break_point = content.rfind(' ', 0, content_width)
			if break_point > content_width * 0.7:  # Keep at least 70% of line
				first_line = content[:break_point]
				rest = content[break_point + 1 :]
			else:
				# No good break point, just truncate
				first_line = content[:content_width]
				rest = content[content_width:]

			lines.append(prefix + first_line)

			# Second line with 10-space indent
			if rest:
				if len(rest) > terminal_width - 10:
					rest = rest[: terminal_width - 10]
				lines.append(' ' * 10 + rest)
		else:
			# Single line - truncate if needed
			if len(content) > content_width:
				content = content[:content_width]
			lines.append(prefix + content)

		return lines
	except Exception as e:
		logger.warning(f'Failed to format message line for logging: {e}')
		# Return a simple fallback line
		return ['â“[   ?]: [Error formatting message]']


# ========== End of Logging Helper Functions ==========


class MessageManager:
	vision_detail_level: Literal['auto', 'low', 'high']

	def __init__(
		self,
		task: str,
		system_message: SystemMessage,
		file_system: FileSystem,
		state: MessageManagerState = MessageManagerState(),
		use_thinking: bool = True,
		include_attributes: list[str] | None = None,
		sensitive_data: dict[str, str | dict[str, str]] | None = None,
		max_history_items: int | None = None,
		vision_detail_level: Literal['auto', 'low', 'high'] = 'auto',
		include_tool_call_examples: bool = False,
		include_recent_events: bool = False,
		sample_images: list[ContentPartTextParam | ContentPartImageParam] | None = None,
		llm_screenshot_size: tuple[int, int] | None = None,
	):
		self.task = task
		self.state = state
		self.system_prompt = system_message
		self.file_system = file_system
		self.sensitive_data_description = ''
		self.use_thinking = use_thinking
		self.max_history_items = max_history_items
		self.vision_detail_level = vision_detail_level
		self.include_tool_call_examples = include_tool_call_examples
		self.include_recent_events = include_recent_events
		self.sample_images = sample_images
		self.llm_screenshot_size = llm_screenshot_size

		assert max_history_items is None or max_history_items > 5, 'max_history_items must be None or greater than 5'

		# Store settings as direct attributes instead of in a settings object
		self.include_attributes = include_attributes or []
		self.sensitive_data = sensitive_data
		self.last_input_messages = []
		self.last_state_message_text: str | None = None
		# Only initialize messages if state is empty
		if len(self.state.history.get_messages()) == 0:
			self._set_message_with_type(self.system_prompt, 'system')

	@property
	def agent_history_description(self) -> str:
		"""Build agent history description from list of items, respecting max_history_items limit"""
		if self.max_history_items is None:
			# Include all items
			return '\n'.join(item.to_string() for item in self.state.agent_history_items)

		total_items = len(self.state.agent_history_items)

		# If we have fewer items than the limit, just return all items
		if total_items <= self.max_history_items:
			return '\n'.join(item.to_string() for item in self.state.agent_history_items)

		# We have more items than the limit, so we need to omit some
		omitted_count = total_items - self.max_history_items

		# Show first item + omitted message + most recent (max_history_items - 1) items
		# The omitted message doesn't count against the limit, only real history items do
		recent_items_count = self.max_history_items - 1  # -1 for first item

		items_to_include = [
			self.state.agent_history_items[0].to_string(),  # Keep first item (initialization)
			f'<sys>[... {omitted_count} previous steps omitted...]</sys>',
		]
		# Add most recent items
		items_to_include.extend([item.to_string() for item in self.state.agent_history_items[-recent_items_count:]])

		return '\n'.join(items_to_include)

	def add_new_task(self, new_task: str) -> None:
		new_task = '<follow_up_user_request> ' + new_task.strip() + ' </follow_up_user_request>'
		if '<initial_user_request>' not in self.task:
			self.task = '<initial_user_request>' + self.task + '</initial_user_request>'
		self.task += '\n' + new_task
		task_update_item = HistoryItem(system_message=new_task)
		self.state.agent_history_items.append(task_update_item)

	def _update_agent_history_description(
		self,
		model_output: AgentOutput | None = None,
		result: list[ActionResult] | None = None,
		step_info: AgentStepInfo | None = None,
	) -> None:
		"""Update the agent history description"""

		if result is None:
			result = []
		step_number = step_info.step_number if step_info else None

		self.state.read_state_description = ''
		self.state.read_state_images = []  # Clear images from previous step

		action_results = ''
		result_len = len(result)
		read_state_idx = 0

		for idx, action_result in enumerate(result):
			if action_result.include_extracted_content_only_once and action_result.extracted_content:
				self.state.read_state_description += (
					f'<read_state_{read_state_idx}>\n{action_result.extracted_content}\n</read_state_{read_state_idx}>\n'
				)
				read_state_idx += 1
				logger.debug(f'Added extracted_content to read_state_description: {action_result.extracted_content}')

			# Store images for one-time inclusion in the next message
			if action_result.images:
				self.state.read_state_images.extend(action_result.images)
				logger.debug(f'Added {len(action_result.images)} image(s) to read_state_images')

			if action_result.long_term_memory:
				action_results += f'{action_result.long_term_memory}\n'
				logger.debug(f'Added long_term_memory to action_results: {action_result.long_term_memory}')
			elif action_result.extracted_content and not action_result.include_extracted_content_only_once:
				action_results += f'{action_result.extracted_content}\n'
				logger.debug(f'Added extracted_content to action_results: {action_result.extracted_content}')

			if action_result.error:
				if len(action_result.error) > 200:
					error_text = action_result.error[:100] + '......' + action_result.error[-100:]
				else:
					error_text = action_result.error
				action_results += f'{error_text}\n'
				logger.debug(f'Added error to action_results: {error_text}')

		# Simple 60k character limit for read_state_description
		MAX_CONTENT_SIZE = 60000
		if len(self.state.read_state_description) > MAX_CONTENT_SIZE:
			self.state.read_state_description = (
				self.state.read_state_description[:MAX_CONTENT_SIZE] + '\n... [Content truncated at 60k characters]'
			)
			logger.debug(f'Truncated read_state_description to {MAX_CONTENT_SIZE} characters')

		self.state.read_state_description = self.state.read_state_description.strip('\n')

		if action_results:
			action_results = f'Result\n{action_results}'
		action_results = action_results.strip('\n') if action_results else None

		# Simple 60k character limit for action_results
		if action_results and len(action_results) > MAX_CONTENT_SIZE:
			action_results = action_results[:MAX_CONTENT_SIZE] + '\n... [Content truncated at 60k characters]'
			logger.debug(f'Truncated action_results to {MAX_CONTENT_SIZE} characters')

		# Build the history item
		if model_output is None:
			# Add history item for initial actions (step 0) or errors (step > 0)
			if step_number is not None:
				if step_number == 0 and action_results:
					# Step 0 with initial action results
					history_item = HistoryItem(step_number=step_number, action_results=action_results)
					self.state.agent_history_items.append(history_item)
				elif step_number > 0:
					# Error case for steps > 0
					history_item = HistoryItem(step_number=step_number, error='Agent failed to output in the right format.')
					self.state.agent_history_items.append(history_item)
		else:
			history_item = HistoryItem(
				step_number=step_number,
				evaluation_previous_goal=model_output.current_state.evaluation_previous_goal,
				memory=model_output.current_state.memory,
				next_goal=model_output.current_state.next_goal,
				action_results=action_results,
			)
			self.state.agent_history_items.append(history_item)

	def _get_sensitive_data_description(self, current_page_url) -> str:
		sensitive_data = self.sensitive_data
		if not sensitive_data:
			return ''

		# Collect placeholders for sensitive data
		placeholders: set[str] = set()

		for key, value in sensitive_data.items():
			if isinstance(value, dict):
				# New format: {domain: {key: value}}
				if current_page_url and match_url_with_domain_pattern(current_page_url, key, True):
					placeholders.update(value.keys())
			else:
				# Old format: {key: value}
				placeholders.add(key)

		if placeholders:
			placeholder_list = sorted(list(placeholders))
			info = f'Here are placeholders for sensitive data:\n{placeholder_list}\n'
			info += 'To use them, write <secret>the placeholder name</secret>'
			return info

		return ''

	@observe_debug(ignore_input=True, ignore_output=True, name='create_state_messages')
	@time_execution_sync('--create_state_messages')
	def create_state_messages(
		self,
		browser_state_summary: BrowserStateSummary,
		model_output: AgentOutput | None = None,
		result: list[ActionResult] | None = None,
		step_info: AgentStepInfo | None = None,
		use_vision: bool | Literal['auto'] = True,
		page_filtered_actions: str | None = None,
		sensitive_data=None,
		available_file_paths: list[str] | None = None,  # Always pass current available_file_paths
	) -> None:
		"""Create single state message with all content"""

		# Clear contextual messages from previous steps to prevent accumulation
		self.state.history.context_messages.clear()

		# First, update the agent history items with the latest step results
		self._update_agent_history_description(model_output, result, step_info)

		# Use the passed sensitive_data parameter, falling back to instance variable
		effective_sensitive_data = sensitive_data if sensitive_data is not None else self.sensitive_data
		if effective_sensitive_data is not None:
			# Update instance variable to keep it in sync
			self.sensitive_data = effective_sensitive_data
			self.sensitive_data_description = self._get_sensitive_data_description(browser_state_summary.url)

		# Use only the current screenshot, but check if action results request screenshot inclusion
		screenshots = []
		include_screenshot_requested = False

		# Check if any action results request screenshot inclusion
		if result:
			for action_result in result:
				if action_result.metadata and action_result.metadata.get('include_screenshot'):
					include_screenshot_requested = True
					logger.debug('Screenshot inclusion requested by action result')
					break

		# Handle different use_vision modes:
		# - "auto": Only include screenshot if explicitly requested by action (e.g., screenshot)
		# - True: Always include screenshot
		# - False: Never include screenshot
		include_screenshot = False
		if use_vision is True:
			# Always include screenshot when use_vision=True
			include_screenshot = True
		elif use_vision == 'auto':
			# Only include screenshot if explicitly requested by action when use_vision="auto"
			include_screenshot = include_screenshot_requested
		# else: use_vision is False, never include screenshot (include_screenshot stays False)

		if include_screenshot and browser_state_summary.screenshot:
			screenshots.append(browser_state_summary.screenshot)

		# Use vision in the user message if screenshots are included
		effective_use_vision = len(screenshots) > 0

		# Create single state message with all content
		assert browser_state_summary
		state_message = AgentMessagePrompt(
			browser_state_summary=browser_state_summary,
			file_system=self.file_system,
			agent_history_description=self.agent_history_description,
			read_state_description=self.state.read_state_description,
			task=self.task,
			include_attributes=self.include_attributes,
			step_info=step_info,
			page_filtered_actions=page_filtered_actions,
			sensitive_data=self.sensitive_data_description,
			available_file_paths=available_file_paths,
			screenshots=screenshots,
			vision_detail_level=self.vision_detail_level,
			include_recent_events=self.include_recent_events,
			sample_images=self.sample_images,
			read_state_images=self.state.read_state_images,
			llm_screenshot_size=self.llm_screenshot_size,
		).get_user_message(effective_use_vision)

		# Store state message text for history
		self.last_state_message_text = state_message.text

		# Set the state message with caching enabled
		self._set_message_with_type(state_message, 'state')

	def _log_history_lines(self) -> str:
		"""Generate a formatted log string of message history for debugging / printing to terminal"""
		# TODO: fix logging

		# try:
		# 	total_input_tokens = 0
		# 	message_lines = []
		# 	terminal_width = shutil.get_terminal_size((80, 20)).columns

		# 	for i, m in enumerate(self.state.history.messages):
		# 		try:
		# 			total_input_tokens += m.metadata.tokens
		# 			is_last_message = i == len(self.state.history.messages) - 1

		# 			# Extract content for logging
		# 			content = _log_extract_message_content(m.message, is_last_message, m.metadata)

		# 			# Format the message line(s)
		# 			lines = _log_format_message_line(m, content, is_last_message, terminal_width)
		# 			message_lines.extend(lines)
		# 		except Exception as e:
		# 			logger.warning(f'Failed to format message {i} for logging: {e}')
		# 			# Add a fallback line for this message
		# 			message_lines.append('â“[   ?]: [Error formatting this message]')

		# 	# Build final log message
		# 	return (
		# 		f'ğŸ“œ LLM Message history ({len(self.state.history.messages)} messages, {total_input_tokens} tokens):\n'
		# 		+ '\n'.join(message_lines)
		# 	)
		# except Exception as e:
		# 	logger.warning(f'Failed to generate history log: {e}')
		# 	# Return a minimal fallback message
		# 	return f'ğŸ“œ LLM Message history (error generating log: {e})'

		return ''

	@time_execution_sync('--get_messages')
	def get_messages(self) -> list[BaseMessage]:
		"""Get current message list, potentially trimmed to max tokens"""

		# Log message history for debugging
		logger.debug(self._log_history_lines())
		self.last_input_messages = self.state.history.get_messages()
		return self.last_input_messages

	def _set_message_with_type(self, message: BaseMessage, message_type: Literal['system', 'state']) -> None:
		"""Replace a specific state message slot with a new message"""
		# Don't filter system and state messages - they should contain placeholder tags or normal conversation
		if message_type == 'system':
			self.state.history.system_message = message
		elif message_type == 'state':
			self.state.history.state_message = message
		else:
			raise ValueError(f'Invalid state message type: {message_type}')

	def _add_context_message(self, message: BaseMessage) -> None:
		"""Add a contextual message specific to this step (e.g., validation errors, retry instructions, timeout warnings)"""
		# Don't filter context messages - they should contain normal conversation or error messages
		self.state.history.context_messages.append(message)

	@time_execution_sync('--filter_sensitive_data')
	def _filter_sensitive_data(self, message: BaseMessage) -> BaseMessage:
		"""Filter out sensitive data from the message"""

		def replace_sensitive(value: str) -> str:
			if not self.sensitive_data:
				return value

			# Collect all sensitive values, immediately converting old format to new format
			sensitive_values: dict[str, str] = {}

			# Process all sensitive data entries
			for key_or_domain, content in self.sensitive_data.items():
				if isinstance(content, dict):
					# Already in new format: {domain: {key: value}}
					for key, val in content.items():
						if val:  # Skip empty values
							sensitive_values[key] = val
				elif content:  # Old format: {key: value} - convert to new format internally
					# We treat this as if it was {'http*://*': {key_or_domain: content}}
					sensitive_values[key_or_domain] = content

			# If there are no valid sensitive data entries, just return the original value
			if not sensitive_values:
				logger.warning('No valid entries found in sensitive_data dictionary')
				return value

			# Replace all valid sensitive data values with their placeholder tags
			for key, val in sensitive_values.items():
				value = value.replace(val, f'<secret>{key}</secret>')

			return value

		if isinstance(message.content, str):
			message.content = replace_sensitive(message.content)
		elif isinstance(message.content, list):
			for i, item in enumerate(message.content):
				if isinstance(item, ContentPartTextParam):
					item.text = replace_sensitive(item.text)
					message.content[i] = item
		return message

```

---

## backend/browser-use/browser_use/agent/message_manager/utils.py

```py
from __future__ import annotations

import json
import logging
from pathlib import Path
from typing import Any

import anyio

from browser_use.llm.messages import BaseMessage

logger = logging.getLogger(__name__)


async def save_conversation(
	input_messages: list[BaseMessage],
	response: Any,
	target: str | Path,
	encoding: str | None = None,
) -> None:
	"""Save conversation history to file asynchronously."""
	target_path = Path(target)
	# create folders if not exists
	if target_path.parent:
		await anyio.Path(target_path.parent).mkdir(parents=True, exist_ok=True)

	await anyio.Path(target_path).write_text(
		await _format_conversation(input_messages, response),
		encoding=encoding or 'utf-8',
	)


async def _format_conversation(messages: list[BaseMessage], response: Any) -> str:
	"""Format the conversation including messages and response."""
	lines = []

	# Format messages
	for message in messages:
		lines.append(f' {message.role} ')

		lines.append(message.text)
		lines.append('')  # Empty line after each message

	# Format response
	lines.append(json.dumps(json.loads(response.model_dump_json(exclude_unset=True)), indent=2, ensure_ascii=False))

	return '\n'.join(lines)


# Note: _write_messages_to_file and _write_response_to_file have been merged into _format_conversation
# This is more efficient for async operations and reduces file I/O

```

---

## backend/browser-use/browser_use/agent/message_manager/views.py

```py
from __future__ import annotations

from typing import TYPE_CHECKING, Any

from pydantic import BaseModel, ConfigDict, Field

from browser_use.llm.messages import (
	BaseMessage,
)

if TYPE_CHECKING:
	pass


class HistoryItem(BaseModel):
	"""Represents a single agent history item with its data and string representation"""

	step_number: int | None = None
	evaluation_previous_goal: str | None = None
	memory: str | None = None
	next_goal: str | None = None
	action_results: str | None = None
	error: str | None = None
	system_message: str | None = None

	model_config = ConfigDict(arbitrary_types_allowed=True)

	def model_post_init(self, __context) -> None:
		"""Validate that error and system_message are not both provided"""
		if self.error is not None and self.system_message is not None:
			raise ValueError('Cannot have both error and system_message at the same time')

	def to_string(self) -> str:
		"""Get string representation of the history item"""
		step_str = 'step' if self.step_number is not None else 'step_unknown'

		if self.error:
			return f"""<{step_str}>
{self.error}"""
		elif self.system_message:
			return self.system_message
		else:
			content_parts = []

			# Only include evaluation_previous_goal if it's not None/empty
			if self.evaluation_previous_goal:
				content_parts.append(f'{self.evaluation_previous_goal}')

			# Always include memory
			if self.memory:
				content_parts.append(f'{self.memory}')

			# Only include next_goal if it's not None/empty
			if self.next_goal:
				content_parts.append(f'{self.next_goal}')

			if self.action_results:
				content_parts.append(self.action_results)

			content = '\n'.join(content_parts)

			return f"""<{step_str}>
{content}"""


class MessageHistory(BaseModel):
	"""History of messages"""

	system_message: BaseMessage | None = None
	state_message: BaseMessage | None = None
	context_messages: list[BaseMessage] = Field(default_factory=list)
	model_config = ConfigDict(arbitrary_types_allowed=True)

	def get_messages(self) -> list[BaseMessage]:
		"""Get all messages in the correct order: system -> state -> contextual"""
		messages = []
		if self.system_message:
			messages.append(self.system_message)
		if self.state_message:
			messages.append(self.state_message)
		messages.extend(self.context_messages)

		return messages


class MessageManagerState(BaseModel):
	"""Holds the state for MessageManager"""

	history: MessageHistory = Field(default_factory=MessageHistory)
	tool_id: int = 1
	agent_history_items: list[HistoryItem] = Field(
		default_factory=lambda: [HistoryItem(step_number=0, system_message='Agent initialized')]
	)
	read_state_description: str = ''
	# Images to include in the next state message (cleared after each step)
	read_state_images: list[dict[str, Any]] = Field(default_factory=list)

	model_config = ConfigDict(arbitrary_types_allowed=True)

```

---

## backend/browser-use/browser_use/agent/prompts.py

```py
import importlib.resources
from datetime import datetime
from typing import TYPE_CHECKING, Literal, Optional

from browser_use.dom.views import NodeType, SimplifiedNode
from browser_use.llm.messages import ContentPartImageParam, ContentPartTextParam, ImageURL, SystemMessage, UserMessage
from browser_use.observability import observe_debug
from browser_use.utils import is_new_tab_page, sanitize_surrogates

if TYPE_CHECKING:
	from browser_use.agent.views import AgentStepInfo
	from browser_use.browser.views import BrowserStateSummary
	from browser_use.filesystem.file_system import FileSystem


class SystemPrompt:
	def __init__(
		self,
		max_actions_per_step: int = 3,
		override_system_message: str | None = None,
		extend_system_message: str | None = None,
		use_thinking: bool = True,
		flash_mode: bool = False,
		is_anthropic: bool = False,
	):
		self.max_actions_per_step = max_actions_per_step
		self.use_thinking = use_thinking
		self.flash_mode = flash_mode
		self.is_anthropic = is_anthropic
		prompt = ''
		if override_system_message is not None:
			prompt = override_system_message
		else:
			self._load_prompt_template()
			prompt = self.prompt_template.format(max_actions=self.max_actions_per_step)

		if extend_system_message:
			prompt += f'\n{extend_system_message}'

		self.system_message = SystemMessage(content=prompt, cache=True)

	def _load_prompt_template(self) -> None:
		"""Load the prompt template from the markdown file."""
		try:
			# Choose the appropriate template based on flash_mode, use_thinking, and is_anthropic
			if self.flash_mode and self.is_anthropic:
				template_filename = 'system_prompt_flash_anthropic.md'
			elif self.flash_mode:
				template_filename = 'system_prompt_flash.md'
			elif self.use_thinking:
				template_filename = 'system_prompt.md'
			else:
				template_filename = 'system_prompt_no_thinking.md'

			# This works both in development and when installed as a package
			with importlib.resources.files('browser_use.agent').joinpath(template_filename).open('r', encoding='utf-8') as f:
				self.prompt_template = f.read()
		except Exception as e:
			raise RuntimeError(f'Failed to load system prompt template: {e}')

	def get_system_message(self) -> SystemMessage:
		"""
		Get the system prompt for the agent.

		Returns:
		    SystemMessage: Formatted system prompt
		"""
		return self.system_message


class AgentMessagePrompt:
	vision_detail_level: Literal['auto', 'low', 'high']

	def __init__(
		self,
		browser_state_summary: 'BrowserStateSummary',
		file_system: 'FileSystem',
		agent_history_description: str | None = None,
		read_state_description: str | None = None,
		task: str | None = None,
		include_attributes: list[str] | None = None,
		step_info: Optional['AgentStepInfo'] = None,
		page_filtered_actions: str | None = None,
		max_clickable_elements_length: int = 40000,
		sensitive_data: str | None = None,
		available_file_paths: list[str] | None = None,
		screenshots: list[str] | None = None,
		vision_detail_level: Literal['auto', 'low', 'high'] = 'auto',
		include_recent_events: bool = False,
		sample_images: list[ContentPartTextParam | ContentPartImageParam] | None = None,
		read_state_images: list[dict] | None = None,
		llm_screenshot_size: tuple[int, int] | None = None,
	):
		self.browser_state: 'BrowserStateSummary' = browser_state_summary
		self.file_system: 'FileSystem | None' = file_system
		self.agent_history_description: str | None = agent_history_description
		self.read_state_description: str | None = read_state_description
		self.task: str | None = task
		self.include_attributes = include_attributes
		self.step_info = step_info
		self.page_filtered_actions: str | None = page_filtered_actions
		self.max_clickable_elements_length: int = max_clickable_elements_length
		self.sensitive_data: str | None = sensitive_data
		self.available_file_paths: list[str] | None = available_file_paths
		self.screenshots = screenshots or []
		self.vision_detail_level = vision_detail_level
		self.include_recent_events = include_recent_events
		self.sample_images = sample_images or []
		self.read_state_images = read_state_images or []
		self.llm_screenshot_size = llm_screenshot_size
		assert self.browser_state

	def _extract_page_statistics(self) -> dict[str, int]:
		"""Extract high-level page statistics from DOM tree for LLM context"""
		stats = {
			'links': 0,
			'iframes': 0,
			'shadow_open': 0,
			'shadow_closed': 0,
			'scroll_containers': 0,
			'images': 0,
			'interactive_elements': 0,
			'total_elements': 0,
		}

		if not self.browser_state.dom_state or not self.browser_state.dom_state._root:
			return stats

		def traverse_node(node: SimplifiedNode) -> None:
			"""Recursively traverse simplified DOM tree to count elements"""
			if not node or not node.original_node:
				return

			original = node.original_node
			stats['total_elements'] += 1

			# Count by node type and tag
			if original.node_type == NodeType.ELEMENT_NODE:
				tag = original.tag_name.lower() if original.tag_name else ''

				if tag == 'a':
					stats['links'] += 1
				elif tag in ('iframe', 'frame'):
					stats['iframes'] += 1
				elif tag == 'img':
					stats['images'] += 1

				# Check if scrollable
				if original.is_actually_scrollable:
					stats['scroll_containers'] += 1

				# Check if interactive
				if node.is_interactive:
					stats['interactive_elements'] += 1

				# Check if this element hosts shadow DOM
				if node.is_shadow_host:
					# Check if any shadow children are closed
					has_closed_shadow = any(
						child.original_node.node_type == NodeType.DOCUMENT_FRAGMENT_NODE
						and child.original_node.shadow_root_type
						and child.original_node.shadow_root_type.lower() == 'closed'
						for child in node.children
					)
					if has_closed_shadow:
						stats['shadow_closed'] += 1
					else:
						stats['shadow_open'] += 1

			elif original.node_type == NodeType.DOCUMENT_FRAGMENT_NODE:
				# Shadow DOM fragment - these are the actual shadow roots
				# But don't double-count since we count them at the host level above
				pass

			# Traverse children
			for child in node.children:
				traverse_node(child)

		traverse_node(self.browser_state.dom_state._root)
		return stats

	@observe_debug(ignore_input=True, ignore_output=True, name='_get_browser_state_description')
	def _get_browser_state_description(self) -> str:
		# Extract page statistics first
		page_stats = self._extract_page_statistics()

		# Format statistics for LLM
		stats_text = '<page_stats>'
		if page_stats['total_elements'] < 10:
			stats_text += 'Page appears empty (SPA not loaded?) - '
		stats_text += f'{page_stats["links"]} links, {page_stats["interactive_elements"]} interactive, '
		stats_text += f'{page_stats["iframes"]} iframes, {page_stats["scroll_containers"]} scroll containers'
		if page_stats['shadow_open'] > 0 or page_stats['shadow_closed'] > 0:
			stats_text += f', {page_stats["shadow_open"]} shadow(open), {page_stats["shadow_closed"]} shadow(closed)'
		if page_stats['images'] > 0:
			stats_text += f', {page_stats["images"]} images'
		stats_text += f', {page_stats["total_elements"]} total elements'
		stats_text += '</page_stats>\n'

		elements_text = self.browser_state.dom_state.llm_representation(include_attributes=self.include_attributes)

		if len(elements_text) > self.max_clickable_elements_length:
			elements_text = elements_text[: self.max_clickable_elements_length]
			truncated_text = f' (truncated to {self.max_clickable_elements_length} characters)'
		else:
			truncated_text = ''

		has_content_above = False
		has_content_below = False
		# Enhanced page information for the model
		page_info_text = ''
		if self.browser_state.page_info:
			pi = self.browser_state.page_info
			# Compute page statistics dynamically
			pages_above = pi.pixels_above / pi.viewport_height if pi.viewport_height > 0 else 0
			pages_below = pi.pixels_below / pi.viewport_height if pi.viewport_height > 0 else 0
			has_content_above = pages_above > 0
			has_content_below = pages_below > 0
			total_pages = pi.page_height / pi.viewport_height if pi.viewport_height > 0 else 0
			current_page_position = pi.scroll_y / max(pi.page_height - pi.viewport_height, 1)
			page_info_text = '<page_info>'
			page_info_text += f'{pages_above:.1f} pages above, '
			page_info_text += f'{pages_below:.1f} pages below, '
			page_info_text += f'{total_pages:.1f} total pages'
			page_info_text += '</page_info>\n'
			# , at {current_page_position:.0%} of page
		if elements_text != '':
			if has_content_above:
				if self.browser_state.page_info:
					pi = self.browser_state.page_info
					pages_above = pi.pixels_above / pi.viewport_height if pi.viewport_height > 0 else 0
					elements_text = f'... {pages_above:.1f} pages above ...\n{elements_text}'
			else:
				elements_text = f'[Start of page]\n{elements_text}'
			if not has_content_below:
				elements_text = f'{elements_text}\n[End of page]'
		else:
			elements_text = 'empty page'

		tabs_text = ''
		current_tab_candidates = []

		# Find tabs that match both URL and title to identify current tab more reliably
		for tab in self.browser_state.tabs:
			if tab.url == self.browser_state.url and tab.title == self.browser_state.title:
				current_tab_candidates.append(tab.target_id)

		# If we have exactly one match, mark it as current
		# Otherwise, don't mark any tab as current to avoid confusion
		current_target_id = current_tab_candidates[0] if len(current_tab_candidates) == 1 else None

		for tab in self.browser_state.tabs:
			tabs_text += f'Tab {tab.target_id[-4:]}: {tab.url} - {tab.title[:30]}\n'

		current_tab_text = f'Current tab: {current_target_id[-4:]}' if current_target_id is not None else ''

		# Check if current page is a PDF viewer and add appropriate message
		pdf_message = ''
		if self.browser_state.is_pdf_viewer:
			pdf_message = (
				'PDF viewer cannot be rendered. In this page, DO NOT use the extract action as PDF content cannot be rendered. '
			)
			pdf_message += (
				'Use the read_file action on the downloaded PDF in available_file_paths to read the full text content.\n\n'
			)

		# Add recent events if available and requested
		recent_events_text = ''
		if self.include_recent_events and self.browser_state.recent_events:
			recent_events_text = f'Recent browser events: {self.browser_state.recent_events}\n'

		# Add closed popup messages if any
		closed_popups_text = ''
		if self.browser_state.closed_popup_messages:
			closed_popups_text = 'Auto-closed JavaScript dialogs:\n'
			for popup_msg in self.browser_state.closed_popup_messages:
				closed_popups_text += f'  - {popup_msg}\n'
			closed_popups_text += '\n'

		browser_state = f"""{stats_text}{current_tab_text}
Available tabs:
{tabs_text}
{page_info_text}
{recent_events_text}{closed_popups_text}{pdf_message}Interactive elements{truncated_text}:
{elements_text}
"""
		return browser_state

	def _get_agent_state_description(self) -> str:
		if self.step_info:
			step_info_description = f'Step{self.step_info.step_number + 1} maximum:{self.step_info.max_steps}\n'
		else:
			step_info_description = ''

		time_str = datetime.now().strftime('%Y-%m-%d')
		step_info_description += f'Today:{time_str}'

		_todo_contents = self.file_system.get_todo_contents() if self.file_system else ''
		if not len(_todo_contents):
			_todo_contents = '[empty todo.md, fill it when applicable]'

		agent_state = f"""
<user_request>
{self.task}
</user_request>
<file_system>
{self.file_system.describe() if self.file_system else 'No file system available'}
</file_system>
<todo_contents>
{_todo_contents}
</todo_contents>
"""
		if self.sensitive_data:
			agent_state += f'<sensitive_data>{self.sensitive_data}</sensitive_data>\n'

		agent_state += f'<step_info>{step_info_description}</step_info>\n'
		if self.available_file_paths:
			available_file_paths_text = '\n'.join(self.available_file_paths)
			agent_state += f'<available_file_paths>{available_file_paths_text}\nUse with absolute paths</available_file_paths>\n'
		return agent_state

	def _resize_screenshot(self, screenshot_b64: str) -> str:
		"""Resize screenshot to llm_screenshot_size if configured."""
		if not self.llm_screenshot_size:
			return screenshot_b64

		try:
			import base64
			import logging
			from io import BytesIO

			from PIL import Image

			img = Image.open(BytesIO(base64.b64decode(screenshot_b64)))
			if img.size == self.llm_screenshot_size:
				return screenshot_b64

			logging.getLogger(__name__).info(
				f'ğŸ”„ Resizing screenshot from {img.size[0]}x{img.size[1]} to {self.llm_screenshot_size[0]}x{self.llm_screenshot_size[1]} for LLM'
			)

			img_resized = img.resize(self.llm_screenshot_size, Image.Resampling.LANCZOS)
			buffer = BytesIO()
			img_resized.save(buffer, format='PNG')
			return base64.b64encode(buffer.getvalue()).decode('utf-8')
		except Exception as e:
			logging.getLogger(__name__).warning(f'Failed to resize screenshot: {e}, using original')
			return screenshot_b64

	@observe_debug(ignore_input=True, ignore_output=True, name='get_user_message')
	def get_user_message(self, use_vision: bool = True) -> UserMessage:
		"""Get complete state as a single cached message"""
		# Don't pass screenshot to model if page is a new tab page, step is 0, and there's only one tab
		if (
			is_new_tab_page(self.browser_state.url)
			and self.step_info is not None
			and self.step_info.step_number == 0
			and len(self.browser_state.tabs) == 1
		):
			use_vision = False

		# Build complete state description
		state_description = (
			'<agent_history>\n'
			+ (self.agent_history_description.strip('\n') if self.agent_history_description else '')
			+ '\n</agent_history>\n\n'
		)
		state_description += '<agent_state>\n' + self._get_agent_state_description().strip('\n') + '\n</agent_state>\n'
		state_description += '<browser_state>\n' + self._get_browser_state_description().strip('\n') + '\n</browser_state>\n'
		# Only add read_state if it has content
		read_state_description = self.read_state_description.strip('\n').strip() if self.read_state_description else ''
		if read_state_description:
			state_description += '<read_state>\n' + read_state_description + '\n</read_state>\n'

		if self.page_filtered_actions:
			state_description += '<page_specific_actions>\n'
			state_description += self.page_filtered_actions + '\n'
			state_description += '</page_specific_actions>\n'

		# Sanitize surrogates from all text content
		state_description = sanitize_surrogates(state_description)

		# Check if we have images to include (from read_file action)
		has_images = bool(self.read_state_images)

		if (use_vision is True and self.screenshots) or has_images:
			# Start with text description
			content_parts: list[ContentPartTextParam | ContentPartImageParam] = [ContentPartTextParam(text=state_description)]

			# Add sample images
			content_parts.extend(self.sample_images)

			# Add screenshots with labels
			for i, screenshot in enumerate(self.screenshots):
				if i == len(self.screenshots) - 1:
					label = 'Current screenshot:'
				else:
					# Use simple, accurate labeling since we don't have actual step timing info
					label = 'Previous screenshot:'

				# Add label as text content
				content_parts.append(ContentPartTextParam(text=label))

				# Resize screenshot if llm_screenshot_size is configured
				processed_screenshot = self._resize_screenshot(screenshot)

				# Add the screenshot
				content_parts.append(
					ContentPartImageParam(
						image_url=ImageURL(
							url=f'data:image/png;base64,{processed_screenshot}',
							media_type='image/png',
							detail=self.vision_detail_level,
						),
					)
				)

			# Add read_state images (from read_file action) before screenshots
			for img_data in self.read_state_images:
				img_name = img_data.get('name', 'unknown')
				img_base64 = img_data.get('data', '')

				if not img_base64:
					continue

				# Detect image format from name
				if img_name.lower().endswith('.png'):
					media_type = 'image/png'
				else:
					media_type = 'image/jpeg'

				# Add label
				content_parts.append(ContentPartTextParam(text=f'Image from file: {img_name}'))

				# Add the image
				content_parts.append(
					ContentPartImageParam(
						image_url=ImageURL(
							url=f'data:{media_type};base64,{img_base64}',
							media_type=media_type,
							detail=self.vision_detail_level,
						),
					)
				)

			return UserMessage(content=content_parts, cache=True)

		return UserMessage(content=state_description, cache=True)


def get_rerun_summary_prompt(original_task: str, total_steps: int, success_count: int, error_count: int) -> str:
	return f'''You are analyzing the completion of a rerun task. Based on the screenshot and execution info, provide a summary.

Original task: {original_task}

Execution statistics:
- Total steps: {total_steps}
- Successful steps: {success_count}
- Failed steps: {error_count}

Analyze the screenshot to determine:
1. Whether the task completed successfully
2. What the final state shows
3. Overall completion status (complete/partial/failed)

Respond with:
- summary: A clear, concise summary of what happened during the rerun
- success: Whether the task completed successfully (true/false)
- completion_status: One of "complete", "partial", or "failed"'''


def get_rerun_summary_message(prompt: str, screenshot_b64: str | None = None) -> UserMessage:
	"""
	Build a UserMessage for rerun summary generation.

	Args:
		prompt: The prompt text
		screenshot_b64: Optional base64-encoded screenshot

	Returns:
		UserMessage with prompt and optional screenshot
	"""
	if screenshot_b64:
		# With screenshot: use multi-part content
		content_parts: list[ContentPartTextParam | ContentPartImageParam] = [
			ContentPartTextParam(type='text', text=prompt),
			ContentPartImageParam(
				type='image_url',
				image_url=ImageURL(url=f'data:image/png;base64,{screenshot_b64}'),
			),
		]
		return UserMessage(content=content_parts)
	else:
		# Without screenshot: use simple string content
		return UserMessage(content=prompt)

```

---

## backend/browser-use/browser_use/agent/service.py

```py
import asyncio
import gc
import inspect
import json
import logging
import re
import tempfile
import time
from collections.abc import Awaitable, Callable
from pathlib import Path
from typing import Any, Generic, Literal, TypeVar
from urllib.parse import urlparse

from dotenv import load_dotenv

from browser_use.agent.cloud_events import (
	CreateAgentOutputFileEvent,
	CreateAgentSessionEvent,
	CreateAgentStepEvent,
	CreateAgentTaskEvent,
	UpdateAgentTaskEvent,
)
from browser_use.agent.message_manager.utils import save_conversation
from browser_use.llm.base import BaseChatModel
from browser_use.llm.messages import BaseMessage, ContentPartImageParam, ContentPartTextParam, UserMessage
from browser_use.tokens.service import TokenCost

load_dotenv()

from bubus import EventBus
from pydantic import BaseModel, ValidationError
from uuid_extensions import uuid7str

from browser_use import Browser, BrowserProfile, BrowserSession
from browser_use.agent.judge import construct_judge_messages

# Lazy import for gif to avoid heavy agent.views import at startup
# from browser_use.agent.gif import create_history_gif
from browser_use.agent.message_manager.service import (
	MessageManager,
)
from browser_use.agent.prompts import SystemPrompt
from browser_use.agent.views import (
	ActionResult,
	AgentError,
	AgentHistory,
	AgentHistoryList,
	AgentOutput,
	AgentSettings,
	AgentState,
	AgentStepInfo,
	AgentStructuredOutput,
	BrowserStateHistory,
	DetectedVariable,
	JudgementResult,
	StepMetadata,
)
from browser_use.browser.session import DEFAULT_BROWSER_PROFILE
from browser_use.browser.views import BrowserStateSummary
from browser_use.config import CONFIG
from browser_use.dom.views import DOMInteractedElement
from browser_use.filesystem.file_system import FileSystem
from browser_use.observability import observe, observe_debug
from browser_use.telemetry.service import ProductTelemetry
from browser_use.telemetry.views import AgentTelemetryEvent
from browser_use.tools.registry.views import ActionModel
from browser_use.tools.service import Tools
from browser_use.utils import (
	URL_PATTERN,
	_log_pretty_path,
	check_latest_browser_use_version,
	get_browser_use_version,
	time_execution_async,
	time_execution_sync,
)

logger = logging.getLogger(__name__)


def log_response(response: AgentOutput, registry=None, logger=None) -> None:
	"""Utility function to log the model's response."""

	# Use module logger if no logger provided
	if logger is None:
		logger = logging.getLogger(__name__)

	# Only log thinking if it's present
	if response.current_state.thinking:
		logger.debug(f'ğŸ’¡ Thinking:\n{response.current_state.thinking}')

	# Only log evaluation if it's not empty
	eval_goal = response.current_state.evaluation_previous_goal
	if eval_goal:
		if 'success' in eval_goal.lower():
			emoji = 'ğŸ‘'
			# Green color for success
			logger.info(f'  \033[32m{emoji} Eval: {eval_goal}\033[0m')
		elif 'failure' in eval_goal.lower():
			emoji = 'âš ï¸'
			# Red color for failure
			logger.info(f'  \033[31m{emoji} Eval: {eval_goal}\033[0m')
		else:
			emoji = 'â”'
			# No color for unknown/neutral
			logger.info(f'  {emoji} Eval: {eval_goal}')

	# Always log memory if present
	if response.current_state.memory:
		logger.info(f'  ğŸ§  Memory: {response.current_state.memory}')

	# Only log next goal if it's not empty
	next_goal = response.current_state.next_goal
	if next_goal:
		# Blue color for next goal
		logger.info(f'  \033[34mğŸ¯ Next goal: {next_goal}\033[0m')


Context = TypeVar('Context')


AgentHookFunc = Callable[['Agent'], Awaitable[None]]


class Agent(Generic[Context, AgentStructuredOutput]):
	@time_execution_sync('--init')
	def __init__(
		self,
		task: str,
		llm: BaseChatModel | None = None,
		# Optional parameters
		browser_profile: BrowserProfile | None = None,
		browser_session: BrowserSession | None = None,
		browser: Browser | None = None,  # Alias for browser_session
		tools: Tools[Context] | None = None,
		controller: Tools[Context] | None = None,  # Alias for tools
		# Initial agent run parameters
		sensitive_data: dict[str, str | dict[str, str]] | None = None,
		initial_actions: list[dict[str, dict[str, Any]]] | None = None,
		# Cloud Callbacks
		register_new_step_callback: (
			Callable[['BrowserStateSummary', 'AgentOutput', int], None]  # Sync callback
			| Callable[['BrowserStateSummary', 'AgentOutput', int], Awaitable[None]]  # Async callback
			| None
		) = None,
		register_done_callback: (
			Callable[['AgentHistoryList'], Awaitable[None]]  # Async Callback
			| Callable[['AgentHistoryList'], None]  # Sync Callback
			| None
		) = None,
		register_external_agent_status_raise_error_callback: Callable[[], Awaitable[bool]] | None = None,
		register_should_stop_callback: Callable[[], Awaitable[bool]] | None = None,
		# Agent settings
		output_model_schema: type[AgentStructuredOutput] | None = None,
		use_vision: bool | Literal['auto'] = True,
		save_conversation_path: str | Path | None = None,
		save_conversation_path_encoding: str | None = 'utf-8',
		max_failures: int = 3,
		override_system_message: str | None = None,
		extend_system_message: str | None = None,
		generate_gif: bool | str = False,
		available_file_paths: list[str] | None = None,
		include_attributes: list[str] | None = None,
		max_actions_per_step: int = 3,
		use_thinking: bool = True,
		flash_mode: bool = False,
		demo_mode: bool | None = None,
		max_history_items: int | None = None,
		page_extraction_llm: BaseChatModel | None = None,
		use_judge: bool = True,
		ground_truth: str | None = None,
		judge_llm: BaseChatModel | None = None,
		injected_agent_state: AgentState | None = None,
		source: str | None = None,
		file_system_path: str | None = None,
		task_id: str | None = None,
		calculate_cost: bool = False,
		display_files_in_done_text: bool = True,
		include_tool_call_examples: bool = False,
		vision_detail_level: Literal['auto', 'low', 'high'] = 'auto',
		llm_timeout: int | None = None,
		step_timeout: int = 120,
		directly_open_url: bool = True,
		include_recent_events: bool = False,
		sample_images: list[ContentPartTextParam | ContentPartImageParam] | None = None,
		final_response_after_failure: bool = True,
		llm_screenshot_size: tuple[int, int] | None = None,
		_url_shortening_limit: int = 25,
		**kwargs,
	):
		# Validate llm_screenshot_size
		if llm_screenshot_size is not None:
			if not isinstance(llm_screenshot_size, tuple) or len(llm_screenshot_size) != 2:
				raise ValueError('llm_screenshot_size must be a tuple of (width, height)')
			width, height = llm_screenshot_size
			if not isinstance(width, int) or not isinstance(height, int):
				raise ValueError('llm_screenshot_size dimensions must be integers')
			if width < 100 or height < 100:
				raise ValueError('llm_screenshot_size dimensions must be at least 100 pixels')
			self.logger.info(f'ğŸ–¼ï¸  LLM screenshot resizing enabled: {width}x{height}')
		if llm is None:
			default_llm_name = CONFIG.DEFAULT_LLM
			if default_llm_name:
				from browser_use.llm.models import get_llm_by_name

				llm = get_llm_by_name(default_llm_name)
			else:
				# No default LLM specified, use the original default
				from browser_use import ChatBrowserUse

				llm = ChatBrowserUse()

		# set flashmode = True if llm is ChatBrowserUse
		if llm.provider == 'browser-use':
			flash_mode = True

		# Auto-configure llm_screenshot_size for Claude Sonnet models
		if llm_screenshot_size is None:
			model_name = getattr(llm, 'model', '')
			if isinstance(model_name, str) and model_name.startswith('claude-sonnet'):
				llm_screenshot_size = (1400, 850)
				logger.info('ğŸ–¼ï¸  Auto-configured LLM screenshot size for Claude Sonnet: 1400x850')

		if page_extraction_llm is None:
			page_extraction_llm = llm
		if judge_llm is None:
			judge_llm = llm
		if available_file_paths is None:
			available_file_paths = []

		# Set timeout based on model name if not explicitly provided
		if llm_timeout is None:

			def _get_model_timeout(llm_model: BaseChatModel) -> int:
				"""Determine timeout based on model name"""
				model_name = getattr(llm_model, 'model', '').lower()
				if 'gemini' in model_name:
					if '3-pro' in model_name:
						return 90
					return 45
				elif 'groq' in model_name:
					return 30
				elif 'o3' in model_name or 'claude' in model_name or 'sonnet' in model_name or 'deepseek' in model_name:
					return 90
				else:
					return 60  # Default timeout

			llm_timeout = _get_model_timeout(llm)

		self.id = task_id or uuid7str()
		self.task_id: str = self.id
		self.session_id: str = uuid7str()

		base_profile = browser_profile or DEFAULT_BROWSER_PROFILE
		if base_profile is DEFAULT_BROWSER_PROFILE:
			base_profile = base_profile.model_copy()
		if demo_mode is not None and base_profile.demo_mode != demo_mode:
			base_profile = base_profile.model_copy(update={'demo_mode': demo_mode})
		browser_profile = base_profile

		# Handle browser vs browser_session parameter (browser takes precedence)
		if browser and browser_session:
			raise ValueError('Cannot specify both "browser" and "browser_session" parameters. Use "browser" for the cleaner API.')
		browser_session = browser or browser_session

		if browser_session is not None and demo_mode is not None and browser_session.browser_profile.demo_mode != demo_mode:
			browser_session.browser_profile = browser_session.browser_profile.model_copy(update={'demo_mode': demo_mode})

		self.browser_session = browser_session or BrowserSession(
			browser_profile=browser_profile,
			id=uuid7str()[:-4] + self.id[-4:],  # re-use the same 4-char suffix so they show up together in logs
		)

		self._demo_mode_enabled: bool = bool(self.browser_profile.demo_mode) if self.browser_session else False
		if self._demo_mode_enabled and getattr(self.browser_profile, 'headless', False):
			self.logger.warning(
				'Demo mode is enabled but the browser is headless=True; set headless=False to view the in-browser panel.'
			)

		# Initialize available file paths as direct attribute
		self.available_file_paths = available_file_paths

		# Core components
		self.task = self._enhance_task_with_schema(task, output_model_schema)
		self.llm = llm
		self.judge_llm = judge_llm
		self.directly_open_url = directly_open_url
		self.include_recent_events = include_recent_events
		self._url_shortening_limit = _url_shortening_limit
		if tools is not None:
			self.tools = tools
		elif controller is not None:
			self.tools = controller
		else:
			# Exclude screenshot tool when use_vision is not auto
			exclude_actions = ['screenshot'] if use_vision != 'auto' else []
			self.tools = Tools(exclude_actions=exclude_actions, display_files_in_done_text=display_files_in_done_text)

		# Enforce screenshot exclusion when use_vision != 'auto', even if user passed custom tools
		if use_vision != 'auto':
			self.tools.exclude_action('screenshot')

		# Structured output
		self.output_model_schema = output_model_schema
		if self.output_model_schema is not None:
			self.tools.use_structured_output_action(self.output_model_schema)

		self.sensitive_data = sensitive_data

		self.sample_images = sample_images

		self.settings = AgentSettings(
			use_vision=use_vision,
			vision_detail_level=vision_detail_level,
			save_conversation_path=save_conversation_path,
			save_conversation_path_encoding=save_conversation_path_encoding,
			max_failures=max_failures,
			override_system_message=override_system_message,
			extend_system_message=extend_system_message,
			generate_gif=generate_gif,
			include_attributes=include_attributes,
			max_actions_per_step=max_actions_per_step,
			use_thinking=use_thinking,
			flash_mode=flash_mode,
			max_history_items=max_history_items,
			page_extraction_llm=page_extraction_llm,
			calculate_cost=calculate_cost,
			include_tool_call_examples=include_tool_call_examples,
			llm_timeout=llm_timeout,
			step_timeout=step_timeout,
			final_response_after_failure=final_response_after_failure,
			use_judge=use_judge,
			ground_truth=ground_truth,
		)

		# Token cost service
		self.token_cost_service = TokenCost(include_cost=calculate_cost)
		self.token_cost_service.register_llm(llm)
		self.token_cost_service.register_llm(page_extraction_llm)
		self.token_cost_service.register_llm(judge_llm)

		# Initialize state
		self.state = injected_agent_state or AgentState()

		# Initialize history
		self.history = AgentHistoryList(history=[], usage=None)

		# Initialize agent directory
		import time

		timestamp = int(time.time())
		base_tmp = Path(tempfile.gettempdir())
		self.agent_directory = base_tmp / f'browser_use_agent_{self.id}_{timestamp}'

		# Initialize file system and screenshot service
		self._set_file_system(file_system_path)
		self._set_screenshot_service()

		# Action setup
		self._setup_action_models()
		self._set_browser_use_version_and_source(source)

		initial_url = None

		# only load url if no initial actions are provided
		if self.directly_open_url and not self.state.follow_up_task and not initial_actions:
			initial_url = self._extract_start_url(self.task)
			if initial_url:
				self.logger.info(f'ğŸ”— Found URL in task: {initial_url}, adding as initial action...')
				initial_actions = [{'navigate': {'url': initial_url, 'new_tab': False}}]

		self.initial_url = initial_url

		self.initial_actions = self._convert_initial_actions(initial_actions) if initial_actions else None
		# Verify we can connect to the model
		self._verify_and_setup_llm()

		# TODO: move this logic to the LLMs
		# Handle users trying to use use_vision=True with DeepSeek models
		if 'deepseek' in self.llm.model.lower():
			self.logger.warning('âš ï¸ DeepSeek models do not support use_vision=True yet. Setting use_vision=False for now...')
			self.settings.use_vision = False

		# Handle users trying to use use_vision=True with XAI models
		if 'grok' in self.llm.model.lower():
			self.logger.warning('âš ï¸ XAI models do not support use_vision=True yet. Setting use_vision=False for now...')
			self.settings.use_vision = False

		logger.debug(
			f'{" +vision" if self.settings.use_vision else ""}'
			f' extraction_model={self.settings.page_extraction_llm.model if self.settings.page_extraction_llm else "Unknown"}'
			f'{" +file_system" if self.file_system else ""}'
		)

		# Store llm_screenshot_size in browser_session so tools can access it
		self.browser_session.llm_screenshot_size = llm_screenshot_size

		# Check if LLM is ChatAnthropic instance
		from browser_use.llm.anthropic.chat import ChatAnthropic

		is_anthropic = isinstance(self.llm, ChatAnthropic)

		# Initialize message manager with state
		# Initial system prompt with all actions - will be updated during each step
		self._message_manager = MessageManager(
			task=self.task,
			system_message=SystemPrompt(
				max_actions_per_step=self.settings.max_actions_per_step,
				override_system_message=override_system_message,
				extend_system_message=extend_system_message,
				use_thinking=self.settings.use_thinking,
				flash_mode=self.settings.flash_mode,
				is_anthropic=is_anthropic,
			).get_system_message(),
			file_system=self.file_system,
			state=self.state.message_manager_state,
			use_thinking=self.settings.use_thinking,
			# Settings that were previously in MessageManagerSettings
			include_attributes=self.settings.include_attributes,
			sensitive_data=sensitive_data,
			max_history_items=self.settings.max_history_items,
			vision_detail_level=self.settings.vision_detail_level,
			include_tool_call_examples=self.settings.include_tool_call_examples,
			include_recent_events=self.include_recent_events,
			sample_images=self.sample_images,
			llm_screenshot_size=llm_screenshot_size,
		)

		if self.sensitive_data:
			# Check if sensitive_data has domain-specific credentials
			has_domain_specific_credentials = any(isinstance(v, dict) for v in self.sensitive_data.values())

			# If no allowed_domains are configured, show a security warning
			if not self.browser_profile.allowed_domains:
				self.logger.warning(
					'âš ï¸ Agent(sensitive_data=â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢) was provided but Browser(allowed_domains=[...]) is not locked down! âš ï¸\n'
					'          â˜ ï¸ If the agent visits a malicious website and encounters a prompt-injection attack, your sensitive_data may be exposed!\n\n'
					'   \n'
				)

			# If we're using domain-specific credentials, validate domain patterns
			elif has_domain_specific_credentials:
				# For domain-specific format, ensure all domain patterns are included in allowed_domains
				domain_patterns = [k for k, v in self.sensitive_data.items() if isinstance(v, dict)]

				# Validate each domain pattern against allowed_domains
				for domain_pattern in domain_patterns:
					is_allowed = False
					for allowed_domain in self.browser_profile.allowed_domains:
						# Special cases that don't require URL matching
						if domain_pattern == allowed_domain or allowed_domain == '*':
							is_allowed = True
							break

						# Need to create example URLs to compare the patterns
						# Extract the domain parts, ignoring scheme
						pattern_domain = domain_pattern.split('://')[-1] if '://' in domain_pattern else domain_pattern
						allowed_domain_part = allowed_domain.split('://')[-1] if '://' in allowed_domain else allowed_domain

						# Check if pattern is covered by an allowed domain
						# Example: "google.com" is covered by "*.google.com"
						if pattern_domain == allowed_domain_part or (
							allowed_domain_part.startswith('*.')
							and (
								pattern_domain == allowed_domain_part[2:]
								or pattern_domain.endswith('.' + allowed_domain_part[2:])
							)
						):
							is_allowed = True
							break

					if not is_allowed:
						self.logger.warning(
							f'âš ï¸ Domain pattern "{domain_pattern}" in sensitive_data is not covered by any pattern in allowed_domains={self.browser_profile.allowed_domains}\n'
							f'   This may be a security risk as credentials could be used on unintended domains.'
						)

		# Callbacks
		self.register_new_step_callback = register_new_step_callback
		self.register_done_callback = register_done_callback
		self.register_should_stop_callback = register_should_stop_callback
		self.register_external_agent_status_raise_error_callback = register_external_agent_status_raise_error_callback

		# Telemetry
		self.telemetry = ProductTelemetry()

		# Event bus with WAL persistence
		# Default to ~/.config/browseruse/events/{agent_session_id}.jsonl
		# wal_path = CONFIG.BROWSER_USE_CONFIG_DIR / 'events' / f'{self.session_id}.jsonl'
		self.eventbus = EventBus(name=f'Agent_{str(self.id)[-4:]}')

		if self.settings.save_conversation_path:
			self.settings.save_conversation_path = Path(self.settings.save_conversation_path).expanduser().resolve()
			self.logger.info(f'ğŸ’¬ Saving conversation to {_log_pretty_path(self.settings.save_conversation_path)}')

		# Initialize download tracking
		assert self.browser_session is not None, 'BrowserSession is not set up'
		self.has_downloads_path = self.browser_session.browser_profile.downloads_path is not None
		if self.has_downloads_path:
			self._last_known_downloads: list[str] = []
			self.logger.debug('ğŸ“ Initialized download tracking for agent')

		# Event-based pause control (kept out of AgentState for serialization)
		self._external_pause_event = asyncio.Event()
		self._external_pause_event.set()

	def _enhance_task_with_schema(self, task: str, output_model_schema: type[AgentStructuredOutput] | None) -> str:
		"""Enhance task description with output schema information if provided."""
		if output_model_schema is None:
			return task

		try:
			schema = output_model_schema.model_json_schema()
			import json

			schema_json = json.dumps(schema, indent=2)

			enhancement = f'\nExpected output format: {output_model_schema.__name__}\n{schema_json}'
			return task + enhancement
		except Exception as e:
			self.logger.debug(f'Could not parse output schema: {e}')

		return task

	@property
	def logger(self) -> logging.Logger:
		"""Get instance-specific logger with task ID in the name"""

		_browser_session_id = self.browser_session.id if self.browser_session else '----'
		_current_target_id = (
			self.browser_session.agent_focus_target_id[-2:]
			if self.browser_session and self.browser_session.agent_focus_target_id
			else '--'
		)
		return logging.getLogger(f'browser_use.AgentğŸ…° {self.task_id[-4:]} â‡¢ ğŸ…‘ {_browser_session_id[-4:]} ğŸ…£ {_current_target_id}')

	@property
	def browser_profile(self) -> BrowserProfile:
		assert self.browser_session is not None, 'BrowserSession is not set up'
		return self.browser_session.browser_profile

	async def _check_and_update_downloads(self, context: str = '') -> None:
		"""Check for new downloads and update available file paths."""
		if not self.has_downloads_path:
			return

		assert self.browser_session is not None, 'BrowserSession is not set up'

		try:
			current_downloads = self.browser_session.downloaded_files
			if current_downloads != self._last_known_downloads:
				self._update_available_file_paths(current_downloads)
				self._last_known_downloads = current_downloads
				if context:
					self.logger.debug(f'ğŸ“ {context}: Updated available files')
		except Exception as e:
			error_context = f' {context}' if context else ''
			self.logger.debug(f'ğŸ“ Failed to check for downloads{error_context}: {type(e).__name__}: {e}')

	def _update_available_file_paths(self, downloads: list[str]) -> None:
		"""Update available_file_paths with downloaded files."""
		if not self.has_downloads_path:
			return

		current_files = set(self.available_file_paths or [])
		new_files = set(downloads) - current_files

		if new_files:
			self.available_file_paths = list(current_files | new_files)

			self.logger.info(
				f'ğŸ“ Added {len(new_files)} downloaded files to available_file_paths (total: {len(self.available_file_paths)} files)'
			)
			for file_path in new_files:
				self.logger.info(f'ğŸ“„ New file available: {file_path}')
		else:
			self.logger.debug(f'ğŸ“ No new downloads detected (tracking {len(current_files)} files)')

	def _set_file_system(self, file_system_path: str | None = None) -> None:
		# Check for conflicting parameters
		if self.state.file_system_state and file_system_path:
			raise ValueError(
				'Cannot provide both file_system_state (from agent state) and file_system_path. '
				'Either restore from existing state or create new file system at specified path, not both.'
			)

		# Check if we should restore from existing state first
		if self.state.file_system_state:
			try:
				# Restore file system from state at the exact same location
				self.file_system = FileSystem.from_state(self.state.file_system_state)
				# The parent directory of base_dir is the original file_system_path
				self.file_system_path = str(self.file_system.base_dir)
				self.logger.debug(f'ğŸ’¾ File system restored from state to: {self.file_system_path}')
				return
			except Exception as e:
				self.logger.error(f'ğŸ’¾ Failed to restore file system from state: {e}')
				raise e

		# Initialize new file system
		try:
			if file_system_path:
				self.file_system = FileSystem(file_system_path)
				self.file_system_path = file_system_path
			else:
				# Use the agent directory for file system
				self.file_system = FileSystem(self.agent_directory)
				self.file_system_path = str(self.agent_directory)
		except Exception as e:
			self.logger.error(f'ğŸ’¾ Failed to initialize file system: {e}.')
			raise e

		# Save file system state to agent state
		self.state.file_system_state = self.file_system.get_state()

		self.logger.debug(f'ğŸ’¾ File system path: {self.file_system_path}')

	def _set_screenshot_service(self) -> None:
		"""Initialize screenshot service using agent directory"""
		try:
			from browser_use.screenshots.service import ScreenshotService

			self.screenshot_service = ScreenshotService(self.agent_directory)
			self.logger.debug(f'ğŸ“¸ Screenshot service initialized in: {self.agent_directory}/screenshots')
		except Exception as e:
			self.logger.error(f'ğŸ“¸ Failed to initialize screenshot service: {e}.')
			raise e

	def save_file_system_state(self) -> None:
		"""Save current file system state to agent state"""
		if self.file_system:
			self.state.file_system_state = self.file_system.get_state()
		else:
			self.logger.error('ğŸ’¾ File system is not set up. Cannot save state.')
			raise ValueError('File system is not set up. Cannot save state.')

	def _set_browser_use_version_and_source(self, source_override: str | None = None) -> None:
		"""Get the version from pyproject.toml and determine the source of the browser-use package"""
		# Use the helper function for version detection
		version = get_browser_use_version()

		# Determine source
		try:
			package_root = Path(__file__).parent.parent.parent
			repo_files = ['.git', 'README.md', 'docs', 'examples']
			if all(Path(package_root / file).exists() for file in repo_files):
				source = 'git'
			else:
				source = 'pip'
		except Exception as e:
			self.logger.debug(f'Error determining source: {e}')
			source = 'unknown'

		if source_override is not None:
			source = source_override
		# self.logger.debug(f'Version: {version}, Source: {source}')  # moved later to _log_agent_run so that people are more likely to include it in copy-pasted support ticket logs
		self.version = version
		self.source = source

	def _setup_action_models(self) -> None:
		"""Setup dynamic action models from tools registry"""
		# Initially only include actions with no filters
		self.ActionModel = self.tools.registry.create_action_model()
		# Create output model with the dynamic actions
		if self.settings.flash_mode:
			self.AgentOutput = AgentOutput.type_with_custom_actions_flash_mode(self.ActionModel)
		elif self.settings.use_thinking:
			self.AgentOutput = AgentOutput.type_with_custom_actions(self.ActionModel)
		else:
			self.AgentOutput = AgentOutput.type_with_custom_actions_no_thinking(self.ActionModel)

		# used to force the done action when max_steps is reached
		self.DoneActionModel = self.tools.registry.create_action_model(include_actions=['done'])
		if self.settings.flash_mode:
			self.DoneAgentOutput = AgentOutput.type_with_custom_actions_flash_mode(self.DoneActionModel)
		elif self.settings.use_thinking:
			self.DoneAgentOutput = AgentOutput.type_with_custom_actions(self.DoneActionModel)
		else:
			self.DoneAgentOutput = AgentOutput.type_with_custom_actions_no_thinking(self.DoneActionModel)

	def add_new_task(self, new_task: str) -> None:
		"""Add a new task to the agent, keeping the same task_id as tasks are continuous"""
		# Simply delegate to message manager - no need for new task_id or events
		# The task continues with new instructions, it doesn't end and start a new one
		self.task = new_task
		self._message_manager.add_new_task(new_task)
		# Mark as follow-up task and recreate eventbus (gets shut down after each run)
		self.state.follow_up_task = True
		# Reset control flags so agent can continue
		self.state.stopped = False
		self.state.paused = False
		agent_id_suffix = str(self.id)[-4:].replace('-', '_')
		if agent_id_suffix and agent_id_suffix[0].isdigit():
			agent_id_suffix = 'a' + agent_id_suffix
		self.eventbus = EventBus(name=f'Agent_{agent_id_suffix}')

	async def _check_stop_or_pause(self) -> None:
		"""Check if the agent should stop or pause, and handle accordingly."""

		# Check new should_stop_callback - sets stopped state cleanly without raising
		if self.register_should_stop_callback:
			if await self.register_should_stop_callback():
				self.logger.info('External callback requested stop')
				self.state.stopped = True
				raise InterruptedError

		if self.register_external_agent_status_raise_error_callback:
			if await self.register_external_agent_status_raise_error_callback():
				raise InterruptedError

		if self.state.stopped:
			raise InterruptedError

		if self.state.paused:
			raise InterruptedError

	@observe(name='agent.step', ignore_output=True, ignore_input=True)
	@time_execution_async('--step')
	async def step(self, step_info: AgentStepInfo | None = None) -> None:
		"""Execute one step of the task"""
		# Initialize timing first, before any exceptions can occur

		self.step_start_time = time.time()

		browser_state_summary = None

		try:
			# Phase 1: Prepare context and timing
			browser_state_summary = await self._prepare_context(step_info)

			# Phase 2: Get model output and execute actions
			await self._get_next_action(browser_state_summary)
			await self._execute_actions()

			# Phase 3: Post-processing
			await self._post_process()

		except Exception as e:
			# Handle ALL exceptions in one place
			await self._handle_step_error(e)

		finally:
			await self._finalize(browser_state_summary)

	async def _prepare_context(self, step_info: AgentStepInfo | None = None) -> BrowserStateSummary:
		"""Prepare the context for the step: browser state, action models, page actions"""
		# step_start_time is now set in step() method

		assert self.browser_session is not None, 'BrowserSession is not set up'

		self.logger.debug(f'ğŸŒ Step {self.state.n_steps}: Getting browser state...')
		# Always take screenshots for all steps
		self.logger.debug('ğŸ“¸ Requesting browser state with include_screenshot=True')
		browser_state_summary = await self.browser_session.get_browser_state_summary(
			include_screenshot=True,  # always capture even if use_vision=False so that cloud sync is useful (it's fast now anyway)
			include_recent_events=self.include_recent_events,
		)
		if browser_state_summary.screenshot:
			self.logger.debug(f'ğŸ“¸ Got browser state WITH screenshot, length: {len(browser_state_summary.screenshot)}')
		else:
			self.logger.debug('ğŸ“¸ Got browser state WITHOUT screenshot')

		# Check for new downloads after getting browser state (catches PDF auto-downloads and previous step downloads)
		await self._check_and_update_downloads(f'Step {self.state.n_steps}: after getting browser state')

		self._log_step_context(browser_state_summary)
		await self._check_stop_or_pause()

		# Update action models with page-specific actions
		self.logger.debug(f'ğŸ“ Step {self.state.n_steps}: Updating action models...')
		await self._update_action_models_for_page(browser_state_summary.url)

		# Get page-specific filtered actions
		page_filtered_actions = self.tools.registry.get_prompt_description(browser_state_summary.url)

		# Page-specific actions will be included directly in the browser_state message
		self.logger.debug(f'ğŸ’¬ Step {self.state.n_steps}: Creating state messages for context...')

		self._message_manager.create_state_messages(
			browser_state_summary=browser_state_summary,
			model_output=self.state.last_model_output,
			result=self.state.last_result,
			step_info=step_info,
			use_vision=self.settings.use_vision,
			page_filtered_actions=page_filtered_actions if page_filtered_actions else None,
			sensitive_data=self.sensitive_data,
			available_file_paths=self.available_file_paths,  # Always pass current available_file_paths
		)

		await self._force_done_after_last_step(step_info)
		await self._force_done_after_failure()
		return browser_state_summary

	@observe_debug(ignore_input=True, name='get_next_action')
	async def _get_next_action(self, browser_state_summary: BrowserStateSummary) -> None:
		"""Execute LLM interaction with retry logic and handle callbacks"""
		input_messages = self._message_manager.get_messages()
		self.logger.debug(
			f'ğŸ¤– Step {self.state.n_steps}: Calling LLM with {len(input_messages)} messages (model: {self.llm.model})...'
		)

		try:
			model_output = await asyncio.wait_for(
				self._get_model_output_with_retry(input_messages), timeout=self.settings.llm_timeout
			)
		except TimeoutError:

			@observe(name='_llm_call_timed_out_with_input')
			async def _log_model_input_to_lmnr(input_messages: list[BaseMessage]) -> None:
				"""Log the model input"""
				pass

			await _log_model_input_to_lmnr(input_messages)

			raise TimeoutError(
				f'LLM call timed out after {self.settings.llm_timeout} seconds. Keep your thinking and output short.'
			)

		self.state.last_model_output = model_output

		# Check again for paused/stopped state after getting model output
		await self._check_stop_or_pause()

		# Handle callbacks and conversation saving
		await self._handle_post_llm_processing(browser_state_summary, input_messages)

		# check again if Ctrl+C was pressed before we commit the output to history
		await self._check_stop_or_pause()

	async def _execute_actions(self) -> None:
		"""Execute the actions from model output"""
		if self.state.last_model_output is None:
			raise ValueError('No model output to execute actions from')

		result = await self.multi_act(self.state.last_model_output.action)
		self.state.last_result = result

	async def _post_process(self) -> None:
		"""Handle post-action processing like download tracking and result logging"""
		assert self.browser_session is not None, 'BrowserSession is not set up'

		# Check for new downloads after executing actions
		await self._check_and_update_downloads('after executing actions')

		# check for action errors  and len more than 1
		if self.state.last_result and len(self.state.last_result) == 1 and self.state.last_result[-1].error:
			self.state.consecutive_failures += 1
			self.logger.debug(f'ğŸ”„ Step {self.state.n_steps}: Consecutive failures: {self.state.consecutive_failures}')
			return

		if self.state.consecutive_failures > 0:
			self.state.consecutive_failures = 0
			self.logger.debug(f'ğŸ”„ Step {self.state.n_steps}: Consecutive failures reset to: {self.state.consecutive_failures}')

		# Log completion results
		if self.state.last_result and len(self.state.last_result) > 0 and self.state.last_result[-1].is_done:
			success = self.state.last_result[-1].success
			if success:
				# Green color for success
				self.logger.info(f'\nğŸ“„ \033[32m Final Result:\033[0m \n{self.state.last_result[-1].extracted_content}\n\n')
			else:
				# Red color for failure
				self.logger.info(f'\nğŸ“„ \033[31m Final Result:\033[0m \n{self.state.last_result[-1].extracted_content}\n\n')
			if self.state.last_result[-1].attachments:
				total_attachments = len(self.state.last_result[-1].attachments)
				for i, file_path in enumerate(self.state.last_result[-1].attachments):
					self.logger.info(f'ğŸ‘‰ Attachment {i + 1 if total_attachments > 1 else ""}: {file_path}')

	async def _handle_step_error(self, error: Exception) -> None:
		"""Handle all types of errors that can occur during a step"""

		# Handle InterruptedError specially
		if isinstance(error, InterruptedError):
			error_msg = 'The agent was interrupted mid-step' + (f' - {str(error)}' if str(error) else '')
			# NOTE: This is not an error, it's a normal part of the execution when the user interrupts the agent
			self.logger.warning(f'{error_msg}')
			return

		# Handle all other exceptions
		include_trace = self.logger.isEnabledFor(logging.DEBUG)
		error_msg = AgentError.format_error(error, include_trace=include_trace)
		max_total_failures = self.settings.max_failures + int(self.settings.final_response_after_failure)
		prefix = f'âŒ Result failed {self.state.consecutive_failures + 1}/{max_total_failures} times: '
		self.state.consecutive_failures += 1

		# Use WARNING for partial failures, ERROR only when max failures reached
		is_final_failure = self.state.consecutive_failures >= max_total_failures
		log_level = logging.ERROR if is_final_failure else logging.WARNING

		if 'Could not parse response' in error_msg or 'tool_use_failed' in error_msg:
			# give model a hint how output should look like
			self.logger.log(log_level, f'Model: {self.llm.model} failed')
			self.logger.log(log_level, f'{prefix}{error_msg}')
		else:
			self.logger.log(log_level, f'{prefix}{error_msg}')

		await self._demo_mode_log(f'Step error: {error_msg}', 'error', {'step': self.state.n_steps})
		self.state.last_result = [ActionResult(error=error_msg)]
		return None

	async def _finalize(self, browser_state_summary: BrowserStateSummary | None) -> None:
		"""Finalize the step with history, logging, and events"""
		step_end_time = time.time()
		if not self.state.last_result:
			return

		if browser_state_summary:
			step_interval = None
			if len(self.history.history) > 0:
				last_history_item = self.history.history[-1]

				if last_history_item.metadata:
					previous_end_time = last_history_item.metadata.step_end_time
					previous_start_time = last_history_item.metadata.step_start_time
					step_interval = max(0, previous_end_time - previous_start_time)
			metadata = StepMetadata(
				step_number=self.state.n_steps,
				step_start_time=self.step_start_time,
				step_end_time=step_end_time,
				step_interval=step_interval,
			)

			# Use _make_history_item like main branch
			await self._make_history_item(
				self.state.last_model_output,
				browser_state_summary,
				self.state.last_result,
				metadata,
				state_message=self._message_manager.last_state_message_text,
			)

		# Log step completion summary
		summary_message = self._log_step_completion_summary(self.step_start_time, self.state.last_result)

		# Save file system state after step completion
		self.save_file_system_state()

		# Emit both step created and executed events
		if browser_state_summary and self.state.last_model_output:
			# Extract key step data for the event
			actions_data = []
			if self.state.last_model_output.action:
				for action in self.state.last_model_output.action:
					action_dict = action.model_dump() if hasattr(action, 'model_dump') else {}
					actions_data.append(action_dict)

			# Emit CreateAgentStepEvent
			step_event = CreateAgentStepEvent.from_agent_step(
				self,
				self.state.last_model_output,
				self.state.last_result,
				actions_data,
				browser_state_summary,
			)
			self.eventbus.dispatch(step_event)

		# Increment step counter after step is fully completed
		self.state.n_steps += 1

	async def _force_done_after_last_step(self, step_info: AgentStepInfo | None = None) -> None:
		"""Handle special processing for the last step"""
		if step_info and step_info.is_last_step():
			# Add last step warning if needed
			msg = 'You reached max_steps - this is your last step. Your only tool available is the "done" tool. No other tool is available. All other tools which you see in history or examples are not available.'
			msg += '\nIf the task is not yet fully finished as requested by the user, set success in "done" to false! E.g. if not all steps are fully completed. Else success to true.'
			msg += '\nInclude everything you found out for the ultimate task in the done text.'
			self.logger.debug('Last step finishing up')
			self._message_manager._add_context_message(UserMessage(content=msg))
			self.AgentOutput = self.DoneAgentOutput

	async def _force_done_after_failure(self) -> None:
		"""Force done after failure"""
		# Create recovery message
		if self.state.consecutive_failures >= self.settings.max_failures and self.settings.final_response_after_failure:
			msg = f'You failed {self.settings.max_failures} times. Therefore we terminate the agent.'
			msg += '\nYour only tool available is the "done" tool. No other tool is available. All other tools which you see in history or examples are not available.'
			msg += '\nIf the task is not yet fully finished as requested by the user, set success in "done" to false! E.g. if not all steps are fully completed. Else success to true.'
			msg += '\nInclude everything you found out for the ultimate task in the done text.'

			self.logger.debug('Force done action, because we reached max_failures.')
			self._message_manager._add_context_message(UserMessage(content=msg))
			self.AgentOutput = self.DoneAgentOutput

	@observe(ignore_input=True, ignore_output=False)
	async def _judge_trace(self) -> JudgementResult | None:
		"""Judge the trace of the agent"""
		task = self.task
		final_result = self.history.final_result() or ''
		agent_steps = self.history.agent_steps()
		screenshot_paths = [p for p in self.history.screenshot_paths() if p is not None]

		# Construct input messages for judge evaluation
		input_messages = construct_judge_messages(
			task=task,
			final_result=final_result,
			agent_steps=agent_steps,
			screenshot_paths=screenshot_paths,
			max_images=10,
			ground_truth=self.settings.ground_truth,
		)

		# Call LLM with JudgementResult as output format
		kwargs: dict = {'output_format': JudgementResult}

		# Only pass request_type for ChatBrowserUse (other providers don't support it)
		if self.judge_llm.provider == 'browser-use':
			kwargs['request_type'] = 'judge'

		try:
			response = await self.judge_llm.ainvoke(input_messages, **kwargs)
			judgement: JudgementResult = response.completion  # type: ignore[assignment]
			return judgement
		except Exception as e:
			self.logger.error(f'Judge trace failed: {e}')
			# Return a default judgement on failure
			return None

	async def _judge_and_log(self) -> None:
		"""Run judge evaluation and log the verdict"""
		judgement = await self._judge_trace()

		# Attach judgement to last action result
		if self.history.history[-1].result[-1].is_done:
			last_result = self.history.history[-1].result[-1]
			last_result.judgement = judgement

			# Get self-reported success
			self_reported_success = last_result.success

			# Log the verdict based on self-reported success and judge verdict
			if judgement:
				# If both self-reported and judge agree on success, don't log
				if self_reported_success is True and judgement.verdict is True:
					return

				judge_log = '\n'
				# If agent reported success but judge thinks it failed, show warning
				if self_reported_success is True and judgement.verdict is False:
					judge_log += 'âš ï¸  \033[33mAgent reported success but judge thinks task failed\033[0m\n'

				# Otherwise, show full judge result
				verdict_color = '\033[32m' if judgement.verdict else '\033[31m'
				verdict_text = 'âœ… PASS' if judgement.verdict else 'âŒ FAIL'
				judge_log += f'âš–ï¸  {verdict_color}Judge Verdict: {verdict_text}\033[0m\n'
				if judgement.failure_reason:
					judge_log += f'   Failure Reason: {judgement.failure_reason}\n'
				if judgement.reached_captcha:
					judge_log += '   ğŸ¤– Captcha Detected: Agent encountered captcha challenges\n'
					judge_log += '   ğŸ‘‰ ğŸ¥· Use Browser Use Cloud for the most stealth browser infra: https://docs.browser-use.com/customize/browser/remote\n'
				judge_log += f'   {judgement.reasoning}\n'
				self.logger.info(judge_log)

	async def _get_model_output_with_retry(self, input_messages: list[BaseMessage]) -> AgentOutput:
		"""Get model output with retry logic for empty actions"""
		model_output = await self.get_model_output(input_messages)
		self.logger.debug(
			f'âœ… Step {self.state.n_steps}: Got LLM response with {len(model_output.action) if model_output.action else 0} actions'
		)

		if (
			not model_output.action
			or not isinstance(model_output.action, list)
			or all(action.model_dump() == {} for action in model_output.action)
		):
			self.logger.warning('Model returned empty action. Retrying...')

			clarification_message = UserMessage(
				content='You forgot to return an action. Please respond with a valid JSON action according to the expected schema with your assessment and next actions.'
			)

			retry_messages = input_messages + [clarification_message]
			model_output = await self.get_model_output(retry_messages)

			if not model_output.action or all(action.model_dump() == {} for action in model_output.action):
				self.logger.warning('Model still returned empty after retry. Inserting safe noop action.')
				action_instance = self.ActionModel()
				setattr(
					action_instance,
					'done',
					{
						'success': False,
						'text': 'No next action returned by LLM!',
					},
				)
				model_output.action = [action_instance]

		return model_output

	async def _handle_post_llm_processing(
		self,
		browser_state_summary: BrowserStateSummary,
		input_messages: list[BaseMessage],
	) -> None:
		"""Handle callbacks and conversation saving after LLM interaction"""
		if self.register_new_step_callback and self.state.last_model_output:
			if inspect.iscoroutinefunction(self.register_new_step_callback):
				await self.register_new_step_callback(
					browser_state_summary,
					self.state.last_model_output,
					self.state.n_steps,
				)
			else:
				self.register_new_step_callback(
					browser_state_summary,
					self.state.last_model_output,
					self.state.n_steps,
				)

		if self.settings.save_conversation_path and self.state.last_model_output:
			# Treat save_conversation_path as a directory (consistent with other recording paths)
			conversation_dir = Path(self.settings.save_conversation_path)
			conversation_filename = f'conversation_{self.id}_{self.state.n_steps}.txt'
			target = conversation_dir / conversation_filename
			await save_conversation(
				input_messages,
				self.state.last_model_output,
				target,
				self.settings.save_conversation_path_encoding,
			)

	async def _make_history_item(
		self,
		model_output: AgentOutput | None,
		browser_state_summary: BrowserStateSummary,
		result: list[ActionResult],
		metadata: StepMetadata | None = None,
		state_message: str | None = None,
	) -> None:
		"""Create and store history item"""

		if model_output:
			interacted_elements = AgentHistory.get_interacted_element(model_output, browser_state_summary.dom_state.selector_map)
		else:
			interacted_elements = [None]

		# Store screenshot and get path
		screenshot_path = None
		if browser_state_summary.screenshot:
			self.logger.debug(
				f'ğŸ“¸ Storing screenshot for step {self.state.n_steps}, screenshot length: {len(browser_state_summary.screenshot)}'
			)
			screenshot_path = await self.screenshot_service.store_screenshot(browser_state_summary.screenshot, self.state.n_steps)
			self.logger.debug(f'ğŸ“¸ Screenshot stored at: {screenshot_path}')
		else:
			self.logger.debug(f'ğŸ“¸ No screenshot in browser_state_summary for step {self.state.n_steps}')

		state_history = BrowserStateHistory(
			url=browser_state_summary.url,
			title=browser_state_summary.title,
			tabs=browser_state_summary.tabs,
			interacted_element=interacted_elements,
			screenshot_path=screenshot_path,
		)

		history_item = AgentHistory(
			model_output=model_output,
			result=result,
			state=state_history,
			metadata=metadata,
			state_message=state_message,
		)

		self.history.add_item(history_item)

	def _remove_think_tags(self, text: str) -> str:
		THINK_TAGS = re.compile(r'<think>.*?</think>', re.DOTALL)
		STRAY_CLOSE_TAG = re.compile(r'.*?</think>', re.DOTALL)
		# Step 1: Remove well-formed <think>...</think>
		text = re.sub(THINK_TAGS, '', text)
		# Step 2: If there's an unmatched closing tag </think>,
		#         remove everything up to and including that.
		text = re.sub(STRAY_CLOSE_TAG, '', text)
		return text.strip()

	# region - URL replacement
	def _replace_urls_in_text(self, text: str) -> tuple[str, dict[str, str]]:
		"""Replace URLs in a text string"""

		replaced_urls: dict[str, str] = {}

		def replace_url(match: re.Match) -> str:
			"""Url can only have 1 query and 1 fragment"""
			import hashlib

			original_url = match.group(0)

			# Find where the query/fragment starts
			query_start = original_url.find('?')
			fragment_start = original_url.find('#')

			# Find the earliest position of query or fragment
			after_path_start = len(original_url)  # Default: no query/fragment
			if query_start != -1:
				after_path_start = min(after_path_start, query_start)
			if fragment_start != -1:
				after_path_start = min(after_path_start, fragment_start)

			# Split URL into base (up to path) and after_path (query + fragment)
			base_url = original_url[:after_path_start]
			after_path = original_url[after_path_start:]

			# If after_path is within the limit, don't shorten
			if len(after_path) <= self._url_shortening_limit:
				return original_url

			# If after_path is too long, truncate and add hash
			if after_path:
				truncated_after_path = after_path[: self._url_shortening_limit]
				# Create a short hash of the full after_path content
				hash_obj = hashlib.md5(after_path.encode('utf-8'))
				short_hash = hash_obj.hexdigest()[:7]
				# Create shortened URL
				shortened = f'{base_url}{truncated_after_path}...{short_hash}'
				# Only use shortened URL if it's actually shorter than the original
				if len(shortened) < len(original_url):
					replaced_urls[shortened] = original_url
					return shortened

			return original_url

		return URL_PATTERN.sub(replace_url, text), replaced_urls

	def _process_messsages_and_replace_long_urls_shorter_ones(self, input_messages: list[BaseMessage]) -> dict[str, str]:
		"""Replace long URLs with shorter ones
		? @dev edits input_messages in place

		returns:
			tuple[filtered_input_messages, urls we replaced {shorter_url: original_url}]
		"""
		from browser_use.llm.messages import AssistantMessage, UserMessage

		urls_replaced: dict[str, str] = {}

		# Process each message, in place
		for message in input_messages:
			# no need to process SystemMessage, we have control over that anyway
			if isinstance(message, (UserMessage, AssistantMessage)):
				if isinstance(message.content, str):
					# Simple string content
					message.content, replaced_urls = self._replace_urls_in_text(message.content)
					urls_replaced.update(replaced_urls)

				elif isinstance(message.content, list):
					# List of content parts
					for part in message.content:
						if isinstance(part, ContentPartTextParam):
							part.text, replaced_urls = self._replace_urls_in_text(part.text)
							urls_replaced.update(replaced_urls)

		return urls_replaced

	@staticmethod
	def _recursive_process_all_strings_inside_pydantic_model(model: BaseModel, url_replacements: dict[str, str]) -> None:
		"""Recursively process all strings inside a Pydantic model, replacing shortened URLs with originals in place."""
		for field_name, field_value in model.__dict__.items():
			if isinstance(field_value, str):
				# Replace shortened URLs with original URLs in string
				processed_string = Agent._replace_shortened_urls_in_string(field_value, url_replacements)
				setattr(model, field_name, processed_string)
			elif isinstance(field_value, BaseModel):
				# Recursively process nested Pydantic models
				Agent._recursive_process_all_strings_inside_pydantic_model(field_value, url_replacements)
			elif isinstance(field_value, dict):
				# Process dictionary values in place
				Agent._recursive_process_dict(field_value, url_replacements)
			elif isinstance(field_value, (list, tuple)):
				processed_value = Agent._recursive_process_list_or_tuple(field_value, url_replacements)
				setattr(model, field_name, processed_value)

	@staticmethod
	def _recursive_process_dict(dictionary: dict, url_replacements: dict[str, str]) -> None:
		"""Helper method to process dictionaries."""
		for k, v in dictionary.items():
			if isinstance(v, str):
				dictionary[k] = Agent._replace_shortened_urls_in_string(v, url_replacements)
			elif isinstance(v, BaseModel):
				Agent._recursive_process_all_strings_inside_pydantic_model(v, url_replacements)
			elif isinstance(v, dict):
				Agent._recursive_process_dict(v, url_replacements)
			elif isinstance(v, (list, tuple)):
				dictionary[k] = Agent._recursive_process_list_or_tuple(v, url_replacements)

	@staticmethod
	def _recursive_process_list_or_tuple(container: list | tuple, url_replacements: dict[str, str]) -> list | tuple:
		"""Helper method to process lists and tuples."""
		if isinstance(container, tuple):
			# For tuples, create a new tuple with processed items
			processed_items = []
			for item in container:
				if isinstance(item, str):
					processed_items.append(Agent._replace_shortened_urls_in_string(item, url_replacements))
				elif isinstance(item, BaseModel):
					Agent._recursive_process_all_strings_inside_pydantic_model(item, url_replacements)
					processed_items.append(item)
				elif isinstance(item, dict):
					Agent._recursive_process_dict(item, url_replacements)
					processed_items.append(item)
				elif isinstance(item, (list, tuple)):
					processed_items.append(Agent._recursive_process_list_or_tuple(item, url_replacements))
				else:
					processed_items.append(item)
			return tuple(processed_items)
		else:
			# For lists, modify in place
			for i, item in enumerate(container):
				if isinstance(item, str):
					container[i] = Agent._replace_shortened_urls_in_string(item, url_replacements)
				elif isinstance(item, BaseModel):
					Agent._recursive_process_all_strings_inside_pydantic_model(item, url_replacements)
				elif isinstance(item, dict):
					Agent._recursive_process_dict(item, url_replacements)
				elif isinstance(item, (list, tuple)):
					container[i] = Agent._recursive_process_list_or_tuple(item, url_replacements)
			return container

	@staticmethod
	def _replace_shortened_urls_in_string(text: str, url_replacements: dict[str, str]) -> str:
		"""Replace all shortened URLs in a string with their original URLs."""
		result = text
		for shortened_url, original_url in url_replacements.items():
			result = result.replace(shortened_url, original_url)
		return result

	# endregion - URL replacement

	@time_execution_async('--get_next_action')
	@observe_debug(ignore_input=True, ignore_output=True, name='get_model_output')
	async def get_model_output(self, input_messages: list[BaseMessage]) -> AgentOutput:
		"""Get next action from LLM based on current state"""

		urls_replaced = self._process_messsages_and_replace_long_urls_shorter_ones(input_messages)

		# Build kwargs for ainvoke
		# Note: ChatBrowserUse will automatically generate action descriptions from output_format schema
		kwargs: dict = {'output_format': self.AgentOutput}

		try:
			response = await self.llm.ainvoke(input_messages, **kwargs)
			parsed: AgentOutput = response.completion  # type: ignore[assignment]

			# Replace any shortened URLs in the LLM response back to original URLs
			if urls_replaced:
				self._recursive_process_all_strings_inside_pydantic_model(parsed, urls_replaced)

			# cut the number of actions to max_actions_per_step if needed
			if len(parsed.action) > self.settings.max_actions_per_step:
				parsed.action = parsed.action[: self.settings.max_actions_per_step]

			if not (hasattr(self.state, 'paused') and (self.state.paused or self.state.stopped)):
				log_response(parsed, self.tools.registry.registry, self.logger)
				await self._broadcast_model_state(parsed)

			self._log_next_action_summary(parsed)
			return parsed
		except ValidationError:
			# Just re-raise - Pydantic's validation errors are already descriptive
			raise

	async def _log_agent_run(self) -> None:
		"""Log the agent run"""
		# Blue color for task
		self.logger.info(f'\033[34mğŸ¯ Task: {self.task}\033[0m')

		self.logger.debug(f'ğŸ¤– Browser-Use Library Version {self.version} ({self.source})')

		# Check for latest version and log upgrade message if needed
		if CONFIG.BROWSER_USE_VERSION_CHECK:
			latest_version = await check_latest_browser_use_version()
			if latest_version and latest_version != self.version:
				self.logger.info(
					f'ğŸ“¦ Newer version available: {latest_version} (current: {self.version}). Upgrade with: uv add browser-use=={latest_version}'
				)

	def _log_first_step_startup(self) -> None:
		"""Log startup message only on the first step"""
		if len(self.history.history) == 0:
			self.logger.info(
				f'Starting a browser-use agent with version {self.version}, with provider={self.llm.provider} and model={self.llm.model}'
			)

	def _log_step_context(self, browser_state_summary: BrowserStateSummary) -> None:
		"""Log step context information"""
		url = browser_state_summary.url if browser_state_summary else ''
		url_short = url[:50] + '...' if len(url) > 50 else url
		interactive_count = len(browser_state_summary.dom_state.selector_map) if browser_state_summary else 0
		self.logger.info('\n')
		self.logger.info(f'ğŸ“ Step {self.state.n_steps}:')
		self.logger.debug(f'Evaluating page with {interactive_count} interactive elements on: {url_short}')

	def _log_next_action_summary(self, parsed: 'AgentOutput') -> None:
		"""Log a comprehensive summary of the next action(s)"""
		if not (self.logger.isEnabledFor(logging.DEBUG) and parsed.action):
			return

		action_count = len(parsed.action)

		# Collect action details
		action_details = []
		for i, action in enumerate(parsed.action):
			action_data = action.model_dump(exclude_unset=True)
			action_name = next(iter(action_data.keys())) if action_data else 'unknown'
			action_params = action_data.get(action_name, {}) if action_data else {}

			# Format key parameters concisely
			param_summary = []
			if isinstance(action_params, dict):
				for key, value in action_params.items():
					if key == 'index':
						param_summary.append(f'#{value}')
					elif key == 'text' and isinstance(value, str):
						text_preview = value[:30] + '...' if len(value) > 30 else value
						param_summary.append(f'text="{text_preview}"')
					elif key == 'url':
						param_summary.append(f'url="{value}"')
					elif key == 'success':
						param_summary.append(f'success={value}')
					elif isinstance(value, (str, int, bool)):
						val_str = str(value)[:30] + '...' if len(str(value)) > 30 else str(value)
						param_summary.append(f'{key}={val_str}')

			param_str = f'({", ".join(param_summary)})' if param_summary else ''
			action_details.append(f'{action_name}{param_str}')

	def _prepare_demo_message(self, message: str, limit: int = 600) -> str:
		# Previously truncated long entries; keep full text for better context in demo panel
		return message.strip()

	async def _demo_mode_log(self, message: str, level: str = 'info', metadata: dict[str, Any] | None = None) -> None:
		if not self._demo_mode_enabled or not message or self.browser_session is None:
			return
		try:
			await self.browser_session.send_demo_mode_log(
				message=self._prepare_demo_message(message),
				level=level,
				metadata=metadata or {},
			)
		except Exception as exc:
			self.logger.debug(f'[DemoMode] Failed to send overlay log: {exc}')

	async def _broadcast_model_state(self, parsed: 'AgentOutput') -> None:
		if not self._demo_mode_enabled:
			return

		state = parsed.current_state
		step_meta = {'step': self.state.n_steps}

		if state.thinking:
			await self._demo_mode_log(state.thinking, 'thought', step_meta)

		if state.evaluation_previous_goal:
			eval_text = state.evaluation_previous_goal
			level = 'success' if 'success' in eval_text.lower() else 'warning' if 'failure' in eval_text.lower() else 'info'
			await self._demo_mode_log(eval_text, level, step_meta)

		if state.memory:
			await self._demo_mode_log(f'Memory: {state.memory}', 'info', step_meta)

		if state.next_goal:
			await self._demo_mode_log(f'Next goal: {state.next_goal}', 'info', step_meta)

	def _log_step_completion_summary(self, step_start_time: float, result: list[ActionResult]) -> str | None:
		"""Log step completion summary with action count, timing, and success/failure stats"""
		if not result:
			return None

		step_duration = time.time() - step_start_time
		action_count = len(result)

		# Count success and failures
		success_count = sum(1 for r in result if not r.error)
		failure_count = action_count - success_count

		# Format success/failure indicators
		success_indicator = f'âœ… {success_count}' if success_count > 0 else ''
		failure_indicator = f'âŒ {failure_count}' if failure_count > 0 else ''
		status_parts = [part for part in [success_indicator, failure_indicator] if part]
		status_str = ' | '.join(status_parts) if status_parts else 'âœ… 0'

		message = (
			f'ğŸ“ Step {self.state.n_steps}: Ran {action_count} action{"" if action_count == 1 else "s"} '
			f'in {step_duration:.2f}s: {status_str}'
		)
		self.logger.debug(message)
		return message

	def _log_final_outcome_messages(self) -> None:
		"""Log helpful messages to user based on agent run outcome"""
		# Check if agent failed
		is_successful = self.history.is_successful()

		if is_successful is False or is_successful is None:
			# Get final result to check for specific failure reasons
			final_result = self.history.final_result()
			final_result_str = str(final_result).lower() if final_result else ''

			# Check for captcha/cloudflare related failures
			captcha_keywords = ['captcha', 'cloudflare', 'recaptcha', 'challenge', 'bot detection', 'access denied']
			has_captcha_issue = any(keyword in final_result_str for keyword in captcha_keywords)

			if has_captcha_issue:
				# Suggest use_cloud=True for captcha/cloudflare issues
				task_preview = self.task[:10] if len(self.task) > 10 else self.task
				self.logger.info('')
				self.logger.info('Failed because of CAPTCHA? For better browser stealth, try:')
				self.logger.info(f'   agent = Agent(task="{task_preview}...", browser=Browser(use_cloud=True))')

			# General failure message
			self.logger.info('')
			self.logger.info('Did the Agent not work as expected? Let us fix this!')
			self.logger.info('   Open a short issue on GitHub: https://github.com/browser-use/browser-use/issues')

	def _log_agent_event(self, max_steps: int, agent_run_error: str | None = None) -> None:
		"""Sent the agent event for this run to telemetry"""

		token_summary = self.token_cost_service.get_usage_tokens_for_model(self.llm.model)

		# Prepare action_history data correctly
		action_history_data = []
		for item in self.history.history:
			if item.model_output and item.model_output.action:
				# Convert each ActionModel in the step to its dictionary representation
				step_actions = [
					action.model_dump(exclude_unset=True)
					for action in item.model_output.action
					if action  # Ensure action is not None if list allows it
				]
				action_history_data.append(step_actions)
			else:
				# Append None or [] if a step had no actions or no model output
				action_history_data.append(None)

		final_res = self.history.final_result()
		final_result_str = json.dumps(final_res) if final_res is not None else None

		# Extract judgement data if available
		judgement_data = self.history.judgement()
		judge_verdict = judgement_data.get('verdict') if judgement_data else None
		judge_reasoning = judgement_data.get('reasoning') if judgement_data else None
		judge_failure_reason = judgement_data.get('failure_reason') if judgement_data else None
		judge_reached_captcha = judgement_data.get('reached_captcha') if judgement_data else None
		judge_impossible_task = judgement_data.get('impossible_task') if judgement_data else None

		self.telemetry.capture(
			AgentTelemetryEvent(
				task=self.task,
				model=self.llm.model,
				model_provider=self.llm.provider,
				max_steps=max_steps,
				max_actions_per_step=self.settings.max_actions_per_step,
				use_vision=self.settings.use_vision,
				version=self.version,
				source=self.source,
				cdp_url=urlparse(self.browser_session.cdp_url).hostname
				if self.browser_session and self.browser_session.cdp_url
				else None,
				agent_type=None,  # Regular Agent (not code-use)
				action_errors=self.history.errors(),
				action_history=action_history_data,
				urls_visited=self.history.urls(),
				steps=self.state.n_steps,
				total_input_tokens=token_summary.prompt_tokens,
				total_output_tokens=token_summary.completion_tokens,
				prompt_cached_tokens=token_summary.prompt_cached_tokens,
				total_tokens=token_summary.total_tokens,
				total_duration_seconds=self.history.total_duration_seconds(),
				success=self.history.is_successful(),
				final_result_response=final_result_str,
				error_message=agent_run_error,
				judge_verdict=judge_verdict,
				judge_reasoning=judge_reasoning,
				judge_failure_reason=judge_failure_reason,
				judge_reached_captcha=judge_reached_captcha,
				judge_impossible_task=judge_impossible_task,
			)
		)

	async def take_step(self, step_info: AgentStepInfo | None = None) -> tuple[bool, bool]:
		"""Take a step

		Returns:
		        Tuple[bool, bool]: (is_done, is_valid)
		"""
		if step_info is not None and step_info.step_number == 0:
			# First step
			self._log_first_step_startup()
			# Normally there was no try catch here but the callback can raise an InterruptedError which we skip
			try:
				await self._execute_initial_actions()
			except InterruptedError:
				pass
			except Exception as e:
				raise e

		await self.step(step_info)

		if self.history.is_done():
			await self.log_completion()

			# Run judge before done callback if enabled
			if self.settings.use_judge:
				await self._judge_and_log()

			if self.register_done_callback:
				if inspect.iscoroutinefunction(self.register_done_callback):
					await self.register_done_callback(self.history)
				else:
					self.register_done_callback(self.history)
			return True, True

		return False, False

	def _extract_start_url(self, task: str) -> str | None:
		"""Extract URL from task string using naive pattern matching."""

		import re

		# Remove email addresses from task before looking for URLs
		task_without_emails = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '', task)

		# Look for common URL patterns
		patterns = [
			r'https?://[^\s<>"\']+',  # Full URLs with http/https
			r'(?:www\.)?[a-zA-Z0-9-]+(?:\.[a-zA-Z0-9-]+)*\.[a-zA-Z]{2,}(?:/[^\s<>"\']*)?',  # Domain names with subdomains and optional paths
		]

		# File extensions that should be excluded from URL detection
		# These are likely files rather than web pages to navigate to
		excluded_extensions = {
			# Documents
			'pdf',
			'doc',
			'docx',
			'xls',
			'xlsx',
			'ppt',
			'pptx',
			'odt',
			'ods',
			'odp',
			# Text files
			'txt',
			'md',
			'csv',
			'json',
			'xml',
			'yaml',
			'yml',
			# Archives
			'zip',
			'rar',
			'7z',
			'tar',
			'gz',
			'bz2',
			'xz',
			# Images
			'jpg',
			'jpeg',
			'png',
			'gif',
			'bmp',
			'svg',
			'webp',
			'ico',
			# Audio/Video
			'mp3',
			'mp4',
			'avi',
			'mkv',
			'mov',
			'wav',
			'flac',
			'ogg',
			# Code/Data
			'py',
			'js',
			'css',
			'java',
			'cpp',
			# Academic/Research
			'bib',
			'bibtex',
			'tex',
			'latex',
			'cls',
			'sty',
			# Other common file types
			'exe',
			'msi',
			'dmg',
			'pkg',
			'deb',
			'rpm',
			'iso',
			# GitHub/Project paths
			'polynomial',
		}

		excluded_words = {
			'never',
			'dont',
			'not',
			"don't",
		}

		found_urls = []
		for pattern in patterns:
			matches = re.finditer(pattern, task_without_emails)
			for match in matches:
				url = match.group(0)
				original_position = match.start()  # Store original position before URL modification

				# Remove trailing punctuation that's not part of URLs
				url = re.sub(r'[.,;:!?()\[\]]+$', '', url)

				# Check if URL ends with a file extension that should be excluded
				url_lower = url.lower()
				should_exclude = False
				for ext in excluded_extensions:
					if f'.{ext}' in url_lower:
						should_exclude = True
						break

				if should_exclude:
					self.logger.debug(f'Excluding URL with file extension from auto-navigation: {url}')
					continue

				# If in the 20 characters before the url position is a word in excluded_words skip to avoid "Never go to this url"
				context_start = max(0, original_position - 20)
				context_text = task_without_emails[context_start:original_position]
				if any(word.lower() in context_text.lower() for word in excluded_words):
					self.logger.debug(
						f'Excluding URL with word in excluded words from auto-navigation: {url} (context: "{context_text.strip()}")'
					)
					continue

				# Add https:// if missing (after excluded words check to avoid position calculation issues)
				if not url.startswith(('http://', 'https://')):
					url = 'https://' + url

				found_urls.append(url)

		unique_urls = list(set(found_urls))
		# If multiple URLs found, skip directly_open_urling
		if len(unique_urls) > 1:
			self.logger.debug(f'Multiple URLs found ({len(found_urls)}), skipping directly_open_url to avoid ambiguity')
			return None

		# If exactly one URL found, return it
		if len(unique_urls) == 1:
			return unique_urls[0]

		return None

	async def _execute_step(
		self,
		step: int,
		max_steps: int,
		step_info: AgentStepInfo,
		on_step_start: AgentHookFunc | None = None,
		on_step_end: AgentHookFunc | None = None,
	) -> bool:
		"""
		Execute a single step with timeout.

		Returns:
			bool: True if task is done, False otherwise
		"""
		if on_step_start is not None:
			await on_step_start(self)

		self.logger.debug(f'ğŸš¶ Starting step {step + 1}/{max_steps}...')

		try:
			await asyncio.wait_for(
				self.step(step_info),
				timeout=self.settings.step_timeout,
			)
			self.logger.debug(f'âœ… Completed step {step + 1}/{max_steps}')
		except TimeoutError:
			# Handle step timeout gracefully
			error_msg = f'Step {step + 1} timed out after {self.settings.step_timeout} seconds'
			self.logger.error(f'â° {error_msg}')
			await self._demo_mode_log(error_msg, 'error', {'step': step + 1})
			self.state.consecutive_failures += 1
			self.state.last_result = [ActionResult(error=error_msg)]

		if on_step_end is not None:
			await on_step_end(self)

		if self.history.is_done():
			await self.log_completion()

			# Run judge before done callback if enabled
			if self.settings.use_judge:
				await self._judge_and_log()

			if self.register_done_callback:
				if inspect.iscoroutinefunction(self.register_done_callback):
					await self.register_done_callback(self.history)
				else:
					self.register_done_callback(self.history)

			return True

		return False

	@observe(name='agent.run', ignore_input=True, ignore_output=True)
	@time_execution_async('--run')
	async def run(
		self,
		max_steps: int = 100,
		on_step_start: AgentHookFunc | None = None,
		on_step_end: AgentHookFunc | None = None,
	) -> AgentHistoryList[AgentStructuredOutput]:
		"""Execute the task with maximum number of steps"""

		loop = asyncio.get_event_loop()
		agent_run_error: str | None = None  # Initialize error tracking variable
		self._force_exit_telemetry_logged = False  # ADDED: Flag for custom telemetry on force exit
		should_delay_close = False

		# Set up the  signal handler with callbacks specific to this agent
		from browser_use.utils import SignalHandler

		# Define the custom exit callback function for second CTRL+C
		def on_force_exit_log_telemetry():
			self._log_agent_event(max_steps=max_steps, agent_run_error='SIGINT: Cancelled by user')
			# NEW: Call the flush method on the telemetry instance
			if hasattr(self, 'telemetry') and self.telemetry:
				self.telemetry.flush()
			self._force_exit_telemetry_logged = True  # Set the flag

		signal_handler = SignalHandler(
			loop=loop,
			pause_callback=self.pause,
			resume_callback=self.resume,
			custom_exit_callback=on_force_exit_log_telemetry,  # Pass the new telemetrycallback
			exit_on_second_int=True,
		)
		signal_handler.register()

		try:
			await self._log_agent_run()

			self.logger.debug(
				f'ğŸ”§ Agent setup: Agent Session ID {self.session_id[-4:]}, Task ID {self.task_id[-4:]}, Browser Session ID {self.browser_session.id[-4:] if self.browser_session else "None"} {"(connecting via CDP)" if (self.browser_session and self.browser_session.cdp_url) else "(launching local browser)"}'
			)

			# Initialize timing for session and task
			self._session_start_time = time.time()
			self._task_start_time = self._session_start_time  # Initialize task start time

			# Only dispatch session events if this is the first run
			if not self.state.session_initialized:
				self.logger.debug('ğŸ“¡ Dispatching CreateAgentSessionEvent...')
				# Emit CreateAgentSessionEvent at the START of run()
				self.eventbus.dispatch(CreateAgentSessionEvent.from_agent(self))

				self.state.session_initialized = True

			self.logger.debug('ğŸ“¡ Dispatching CreateAgentTaskEvent...')
			# Emit CreateAgentTaskEvent at the START of run()
			self.eventbus.dispatch(CreateAgentTaskEvent.from_agent(self))

			# Log startup message on first step (only if we haven't already done steps)
			self._log_first_step_startup()
			# Start browser session and attach watchdogs
			await self.browser_session.start()
			if self._demo_mode_enabled:
				await self._demo_mode_log(f'Started task: {self.task}', 'info', {'tag': 'task'})
				await self._demo_mode_log(
					'Demo mode active - follow the side panel for live thoughts and actions.',
					'info',
					{'tag': 'status'},
				)

			# Normally there was no try catch here but the callback can raise an InterruptedError
			try:
				await self._execute_initial_actions()
			except InterruptedError:
				pass
			except Exception as e:
				raise e

			self.logger.debug(
				f'ğŸ”„ Starting main execution loop with max {max_steps} steps (currently at step {self.state.n_steps})...'
			)
			while self.state.n_steps <= max_steps:
				current_step = self.state.n_steps - 1  # Convert to 0-indexed for step_info

				# Use the consolidated pause state management
				if self.state.paused:
					self.logger.debug(f'â¸ï¸ Step {self.state.n_steps}: Agent paused, waiting to resume...')
					await self._external_pause_event.wait()
					signal_handler.reset()

				# Check if we should stop due to too many failures, if final_response_after_failure is True, we try one last time
				if (self.state.consecutive_failures) >= self.settings.max_failures + int(
					self.settings.final_response_after_failure
				):
					self.logger.error(f'âŒ Stopping due to {self.settings.max_failures} consecutive failures')
					agent_run_error = f'Stopped due to {self.settings.max_failures} consecutive failures'
					break

				# Check control flags before each step
				if self.state.stopped:
					self.logger.info('ğŸ›‘ Agent stopped')
					agent_run_error = 'Agent stopped programmatically'
					break

				step_info = AgentStepInfo(step_number=current_step, max_steps=max_steps)
				is_done = await self._execute_step(current_step, max_steps, step_info, on_step_start, on_step_end)

				if is_done:
					# Agent has marked the task as done
					if self._demo_mode_enabled and self.history.history:
						final_result_text = self.history.final_result() or 'Task completed'
						await self._demo_mode_log(f'Final Result: {final_result_text}', 'success', {'tag': 'task'})

					should_delay_close = True
					break
			else:
				agent_run_error = 'Failed to complete task in maximum steps'

				self.history.add_item(
					AgentHistory(
						model_output=None,
						result=[ActionResult(error=agent_run_error, include_in_memory=True)],
						state=BrowserStateHistory(
							url='',
							title='',
							tabs=[],
							interacted_element=[],
							screenshot_path=None,
						),
						metadata=None,
					)
				)

				self.logger.info(f'âŒ {agent_run_error}')

			self.history.usage = await self.token_cost_service.get_usage_summary()

			# set the model output schema and call it on the fly
			if self.history._output_model_schema is None and self.output_model_schema is not None:
				self.history._output_model_schema = self.output_model_schema

			return self.history

		except KeyboardInterrupt:
			# Already handled by our signal handler, but catch any direct KeyboardInterrupt as well
			self.logger.debug('Got KeyboardInterrupt during execution, returning current history')
			agent_run_error = 'KeyboardInterrupt'

			self.history.usage = await self.token_cost_service.get_usage_summary()

			return self.history

		except Exception as e:
			self.logger.error(f'Agent run failed with exception: {e}', exc_info=True)
			agent_run_error = str(e)
			raise e

		finally:
			if should_delay_close and self._demo_mode_enabled and agent_run_error is None:
				await asyncio.sleep(30)
			if agent_run_error:
				await self._demo_mode_log(f'Agent stopped: {agent_run_error}', 'error', {'tag': 'run'})
			# Log token usage summary
			await self.token_cost_service.log_usage_summary()

			# Unregister signal handlers before cleanup
			signal_handler.unregister()

			if not self._force_exit_telemetry_logged:  # MODIFIED: Check the flag
				try:
					self._log_agent_event(max_steps=max_steps, agent_run_error=agent_run_error)
				except Exception as log_e:  # Catch potential errors during logging itself
					self.logger.error(f'Failed to log telemetry event: {log_e}', exc_info=True)
			else:
				# ADDED: Info message when custom telemetry for SIGINT was already logged
				self.logger.debug('Telemetry for force exit (SIGINT) was logged by custom exit callback.')

			# NOTE: CreateAgentSessionEvent and CreateAgentTaskEvent are now emitted at the START of run()
			# to match backend requirements for CREATE events to be fired when entities are created,
			# not when they are completed

			# Emit UpdateAgentTaskEvent at the END of run() with final task state
			self.eventbus.dispatch(UpdateAgentTaskEvent.from_agent(self))

			# Generate GIF if needed before stopping event bus
			if self.settings.generate_gif:
				output_path: str = 'agent_history.gif'
				if isinstance(self.settings.generate_gif, str):
					output_path = self.settings.generate_gif

				# Lazy import gif module to avoid heavy startup cost
				from browser_use.agent.gif import create_history_gif

				create_history_gif(task=self.task, history=self.history, output_path=output_path)

				# Only emit output file event if GIF was actually created
				if Path(output_path).exists():
					output_event = await CreateAgentOutputFileEvent.from_agent_and_file(self, output_path)
					self.eventbus.dispatch(output_event)

			# Log final messages to user based on outcome
			self._log_final_outcome_messages()

			# Stop the event bus gracefully, waiting for all events to be processed
			# Use longer timeout to avoid deadlocks in tests with multiple agents
			await self.eventbus.stop(timeout=3.0)

			await self.close()

	@observe_debug(ignore_input=True, ignore_output=True)
	@time_execution_async('--multi_act')
	async def multi_act(self, actions: list[ActionModel]) -> list[ActionResult]:
		"""Execute multiple actions"""
		results: list[ActionResult] = []
		time_elapsed = 0
		total_actions = len(actions)

		assert self.browser_session is not None, 'BrowserSession is not set up'
		try:
			if (
				self.browser_session._cached_browser_state_summary is not None
				and self.browser_session._cached_browser_state_summary.dom_state is not None
			):
				cached_selector_map = dict(self.browser_session._cached_browser_state_summary.dom_state.selector_map)
				cached_element_hashes = {e.parent_branch_hash() for e in cached_selector_map.values()}
			else:
				cached_selector_map = {}
				cached_element_hashes = set()
		except Exception as e:
			self.logger.error(f'Error getting cached selector map: {e}')
			cached_selector_map = {}
			cached_element_hashes = set()

		for i, action in enumerate(actions):
			if i > 0:
				# ONLY ALLOW TO CALL `done` IF IT IS A SINGLE ACTION
				if action.model_dump(exclude_unset=True).get('done') is not None:
					msg = f'Done action is allowed only as a single action - stopped after action {i} / {total_actions}.'
					self.logger.debug(msg)
					break

			# wait between actions (only after first action)
			if i > 0:
				self.logger.debug(f'Waiting {self.browser_profile.wait_between_actions} seconds between actions')
				await asyncio.sleep(self.browser_profile.wait_between_actions)

			try:
				await self._check_stop_or_pause()
				# Get action name from the action model
				action_data = action.model_dump(exclude_unset=True)
				action_name = next(iter(action_data.keys())) if action_data else 'unknown'

				# Log action before execution
				await self._log_action(action, action_name, i + 1, total_actions)

				time_start = time.time()

				result = await self.tools.act(
					action=action,
					browser_session=self.browser_session,
					file_system=self.file_system,
					page_extraction_llm=self.settings.page_extraction_llm,
					sensitive_data=self.sensitive_data,
					available_file_paths=self.available_file_paths,
				)

				time_end = time.time()
				time_elapsed = time_end - time_start

				if result.error:
					await self._demo_mode_log(
						f'Action "{action_name}" failed: {result.error}',
						'error',
						{'action': action_name, 'step': self.state.n_steps},
					)
				elif result.is_done:
					completion_text = result.long_term_memory or result.extracted_content or 'Task marked as done.'
					level = 'success' if result.success is not False else 'warning'
					await self._demo_mode_log(
						completion_text,
						level,
						{'action': action_name, 'step': self.state.n_steps},
					)

				results.append(result)

				if results[-1].is_done or results[-1].error or i == total_actions - 1:
					break

			except Exception as e:
				# Handle any exceptions during action execution
				self.logger.error(f'âŒ Executing action {i + 1} failed -> {type(e).__name__}: {e}')
				await self._demo_mode_log(
					f'Action "{action_name}" raised {type(e).__name__}: {e}',
					'error',
					{'action': action_name, 'step': self.state.n_steps},
				)
				raise e

		return results

	async def _log_action(self, action, action_name: str, action_num: int, total_actions: int) -> None:
		"""Log the action before execution with colored formatting"""
		# Color definitions
		blue = '\033[34m'  # Action name
		magenta = '\033[35m'  # Parameter names
		reset = '\033[0m'

		# Format action number and name
		if total_actions > 1:
			action_header = f'â–¶ï¸  [{action_num}/{total_actions}] {blue}{action_name}{reset}:'
			plain_header = f'â–¶ï¸  [{action_num}/{total_actions}] {action_name}:'
		else:
			action_header = f'â–¶ï¸   {blue}{action_name}{reset}:'
			plain_header = f'â–¶ï¸  {action_name}:'

		# Get action parameters
		action_data = action.model_dump(exclude_unset=True)
		params = action_data.get(action_name, {})

		# Build parameter parts with colored formatting
		param_parts = []
		plain_param_parts = []

		if params and isinstance(params, dict):
			for param_name, value in params.items():
				# Truncate long values for readability
				if isinstance(value, str) and len(value) > 150:
					display_value = value[:150] + '...'
				elif isinstance(value, list) and len(str(value)) > 200:
					display_value = str(value)[:200] + '...'
				else:
					display_value = value

				param_parts.append(f'{magenta}{param_name}{reset}: {display_value}')
				plain_param_parts.append(f'{param_name}: {display_value}')

		# Join all parts
		if param_parts:
			params_string = ', '.join(param_parts)
			self.logger.info(f'  {action_header} {params_string}')
		else:
			self.logger.info(f'  {action_header}')

		if self._demo_mode_enabled:
			panel_message = plain_header
			if plain_param_parts:
				panel_message = f'{panel_message} {", ".join(plain_param_parts)}'
			await self._demo_mode_log(panel_message.strip(), 'action', {'action': action_name, 'step': self.state.n_steps})

	async def log_completion(self) -> None:
		"""Log the completion of the task"""
		# self._task_end_time = time.time()
		# self._task_duration = self._task_end_time - self._task_start_time TODO: this is not working when using take_step
		if self.history.is_successful():
			self.logger.info('âœ… Task completed successfully')
			await self._demo_mode_log('Task completed successfully', 'success', {'tag': 'task'})

	async def _generate_rerun_summary(
		self, original_task: str, results: list[ActionResult], summary_llm: BaseChatModel | None = None
	) -> ActionResult:
		"""Generate AI summary of rerun completion using screenshot and last step info"""
		from browser_use.agent.views import RerunSummaryAction

		# Get current screenshot
		screenshot_b64 = None
		try:
			screenshot = await self.browser_session.take_screenshot(full_page=False)
			if screenshot:
				import base64

				screenshot_b64 = base64.b64encode(screenshot).decode('utf-8')
		except Exception as e:
			self.logger.warning(f'Failed to capture screenshot for rerun summary: {e}')

		# Build summary prompt and message
		error_count = sum(1 for r in results if r.error)
		success_count = len(results) - error_count

		from browser_use.agent.prompts import get_rerun_summary_message, get_rerun_summary_prompt

		prompt = get_rerun_summary_prompt(
			original_task=original_task,
			total_steps=len(results),
			success_count=success_count,
			error_count=error_count,
		)

		# Use provided LLM, agent's LLM, or fall back to OpenAI with structured output
		try:
			# Determine which LLM to use
			if summary_llm is None:
				# Try to use the agent's LLM first
				summary_llm = self.llm
				self.logger.debug('Using agent LLM for rerun summary')
			else:
				self.logger.debug(f'Using provided LLM for rerun summary: {summary_llm.model}')

			# Build message with prompt and optional screenshot
			from browser_use.llm.messages import BaseMessage

			message = get_rerun_summary_message(prompt, screenshot_b64)
			messages: list[BaseMessage] = [message]  # type: ignore[list-item]

			# Try calling with structured output first
			self.logger.debug(f'Calling LLM for rerun summary with {len(messages)} message(s)')
			try:
				kwargs: dict = {'output_format': RerunSummaryAction}
				response = await summary_llm.ainvoke(messages, **kwargs)
				summary: RerunSummaryAction = response.completion  # type: ignore[assignment]
				self.logger.debug(f'LLM response type: {type(summary)}')
				self.logger.debug(f'LLM response: {summary}')
			except Exception as structured_error:
				# If structured output fails (e.g., Browser-Use LLM doesn't support it for this type),
				# fall back to text response without parsing
				self.logger.debug(f'Structured output failed: {structured_error}, falling back to text response')

				response = await summary_llm.ainvoke(messages, None)
				response_text = response.completion
				self.logger.debug(f'LLM text response: {response_text}')

				# Use the text response directly as the summary
				summary = RerunSummaryAction(
					summary=response_text if isinstance(response_text, str) else str(response_text),
					success=error_count == 0,
					completion_status='complete' if error_count == 0 else ('partial' if success_count > 0 else 'failed'),
				)

			self.logger.info(f'ğŸ“Š Rerun Summary: {summary.summary}')
			self.logger.info(f'ğŸ“Š Status: {summary.completion_status} (success={summary.success})')

			return ActionResult(
				is_done=True,
				success=summary.success,
				extracted_content=summary.summary,
				long_term_memory=f'Rerun completed with status: {summary.completion_status}. {summary.summary[:100]}',
			)

		except Exception as e:
			self.logger.warning(f'Failed to generate AI summary: {e.__class__.__name__}: {e}')
			self.logger.debug('Full error traceback:', exc_info=True)
			# Fallback to simple summary
			return ActionResult(
				is_done=True,
				success=error_count == 0,
				extracted_content=f'Rerun completed: {success_count}/{len(results)} steps succeeded',
				long_term_memory=f'Rerun completed: {success_count} steps succeeded, {error_count} errors',
			)

	async def rerun_history(
		self,
		history: AgentHistoryList,
		max_retries: int = 3,
		skip_failures: bool = True,
		delay_between_actions: float = 2.0,
		summary_llm: BaseChatModel | None = None,
	) -> list[ActionResult]:
		"""
		Rerun a saved history of actions with error handling and retry logic.

		Args:
		                history: The history to replay
		                max_retries: Maximum number of retries per action
		                skip_failures: Whether to skip failed actions or stop execution
		                delay_between_actions: Delay between actions in seconds
		                summary_llm: Optional LLM to use for generating the final summary. If not provided, uses the agent's LLM

		Returns:
		                List of action results (including AI summary as the final result)
		"""
		# Skip cloud sync session events for rerunning (we're replaying, not starting new)
		self.state.session_initialized = True

		# Initialize browser session
		await self.browser_session.start()

		results = []

		for i, history_item in enumerate(history.history):
			goal = history_item.model_output.current_state.next_goal if history_item.model_output else ''
			step_num = history_item.metadata.step_number if history_item.metadata else i
			step_name = 'Initial actions' if step_num == 0 else f'Step {step_num}'

			# Determine step delay
			if history_item.metadata and history_item.metadata.step_interval is not None:
				step_delay = history_item.metadata.step_interval
				# Format delay nicely - show ms for values < 1s, otherwise show seconds
				if step_delay < 1.0:
					delay_str = f'{step_delay * 1000:.0f}ms'
				else:
					delay_str = f'{step_delay:.1f}s'
				delay_source = f'using saved step_interval={delay_str}'
			else:
				step_delay = delay_between_actions
				if step_delay < 1.0:
					delay_str = f'{step_delay * 1000:.0f}ms'
				else:
					delay_str = f'{step_delay:.1f}s'
				delay_source = f'using default delay={delay_str}'

			self.logger.info(f'Replaying {step_name} ({i + 1}/{len(history.history)}) [{delay_source}]: {goal}')

			if (
				not history_item.model_output
				or not history_item.model_output.action
				or history_item.model_output.action == [None]
			):
				self.logger.warning(f'{step_name}: No action to replay, skipping')
				results.append(ActionResult(error='No action to replay'))
				continue

			retry_count = 0
			while retry_count < max_retries:
				try:
					result = await self._execute_history_step(history_item, step_delay)
					results.extend(result)
					break

				except Exception as e:
					retry_count += 1
					if retry_count == max_retries:
						error_msg = f'{step_name} failed after {max_retries} attempts: {str(e)}'
						self.logger.error(error_msg)
						if not skip_failures:
							results.append(ActionResult(error=error_msg))
							raise RuntimeError(error_msg)
					else:
						self.logger.warning(f'{step_name} failed (attempt {retry_count}/{max_retries}), retrying...')
						await asyncio.sleep(delay_between_actions)

		# Generate AI summary of rerun completion
		self.logger.info('ğŸ¤– Generating AI summary of rerun completion...')
		summary_result = await self._generate_rerun_summary(self.task, results, summary_llm)
		results.append(summary_result)

		await self.close()
		return results

	async def _execute_initial_actions(self) -> None:
		# Execute initial actions if provided
		if self.initial_actions and not self.state.follow_up_task:
			self.logger.debug(f'âš¡ Executing {len(self.initial_actions)} initial actions...')
			result = await self.multi_act(self.initial_actions)
			# update result 1 to mention that its was automatically loaded
			if result and self.initial_url and result[0].long_term_memory:
				result[0].long_term_memory = f'Found initial url and automatically loaded it. {result[0].long_term_memory}'
			self.state.last_result = result

			# Save initial actions to history as step 0 for rerun capability
			# Skip browser state capture for initial actions (usually just URL navigation)
			if self.settings.flash_mode:
				model_output = self.AgentOutput(
					evaluation_previous_goal=None,
					memory='Initial navigation',
					next_goal=None,
					action=self.initial_actions,
				)
			else:
				model_output = self.AgentOutput(
					evaluation_previous_goal='Start',
					memory=None,
					next_goal='Initial navigation',
					action=self.initial_actions,
				)

			metadata = StepMetadata(step_number=0, step_start_time=time.time(), step_end_time=time.time(), step_interval=None)

			# Create minimal browser state history for initial actions
			state_history = BrowserStateHistory(
				url=self.initial_url or '',
				title='Initial Actions',
				tabs=[],
				interacted_element=[None] * len(self.initial_actions),  # No DOM elements needed
				screenshot_path=None,
			)

			history_item = AgentHistory(
				model_output=model_output,
				result=result,
				state=state_history,
				metadata=metadata,
			)

			self.history.add_item(history_item)
			self.logger.debug('ğŸ“ Saved initial actions to history as step 0')
			self.logger.debug('Initial actions completed')

	async def _execute_history_step(self, history_item: AgentHistory, delay: float) -> list[ActionResult]:
		"""Execute a single step from history with element validation"""
		assert self.browser_session is not None, 'BrowserSession is not set up'

		await asyncio.sleep(delay)
		state = await self.browser_session.get_browser_state_summary(include_screenshot=False)
		if not state or not history_item.model_output:
			raise ValueError('Invalid state or model output')
		updated_actions = []
		for i, action in enumerate(history_item.model_output.action):
			updated_action = await self._update_action_indices(
				history_item.state.interacted_element[i],
				action,
				state,
			)
			updated_actions.append(updated_action)

			if updated_action is None:
				raise ValueError(f'Could not find matching element {i} in current page')

		result = await self.multi_act(updated_actions)
		return result

	async def _update_action_indices(
		self,
		historical_element: DOMInteractedElement | None,
		action: ActionModel,  # Type this properly based on your action model
		browser_state_summary: BrowserStateSummary,
	) -> ActionModel | None:
		"""
		Update action indices based on current page state.
		Returns updated action or None if element cannot be found.
		"""
		if not historical_element or not browser_state_summary.dom_state.selector_map:
			return action

		# selector_hash_map = {hash(e): e for e in browser_state_summary.dom_state.selector_map.values()}

		highlight_index, current_element = next(
			(
				(highlight_index, element)
				for highlight_index, element in browser_state_summary.dom_state.selector_map.items()
				if element.element_hash == historical_element.element_hash
			),
			(None, None),
		)

		if not current_element or highlight_index is None:
			return None

		old_index = action.get_index()
		if old_index != highlight_index:
			action.set_index(highlight_index)
			self.logger.info(f'Element moved in DOM, updated index from {old_index} to {highlight_index}')

		return action

	async def load_and_rerun(
		self,
		history_file: str | Path | None = None,
		variables: dict[str, str] | None = None,
		**kwargs,
	) -> list[ActionResult]:
		"""
		Load history from file and rerun it, optionally substituting variables.

		Args:
			history_file: Path to the history file
			variables: Optional dict mapping variable names to new values (e.g. {'email': 'new@example.com'})
			**kwargs: Additional arguments passed to rerun_history
		"""
		if not history_file:
			history_file = 'AgentHistory.json'
		history = AgentHistoryList.load_from_file(history_file, self.AgentOutput)

		# Substitute variables if provided
		if variables:
			history = self._substitute_variables_in_history(history, variables)

		return await self.rerun_history(history, **kwargs)

	def save_history(self, file_path: str | Path | None = None) -> None:
		"""Save the history to a file with sensitive data filtering"""
		if not file_path:
			file_path = 'AgentHistory.json'
		self.history.save_to_file(file_path, sensitive_data=self.sensitive_data)

	def pause(self) -> None:
		"""Pause the agent before the next step"""
		print('\n\nâ¸ï¸ Paused the agent and left the browser open.\n\tPress [Enter] to resume or [Ctrl+C] again to quit.')
		self.state.paused = True
		self._external_pause_event.clear()

	def resume(self) -> None:
		"""Resume the agent"""
		# TODO: Locally the browser got closed
		print('----------------------------------------------------------------------')
		print('â–¶ï¸  Resuming agent execution where it left off...\n')
		self.state.paused = False
		self._external_pause_event.set()

	def stop(self) -> None:
		"""Stop the agent"""
		self.logger.info('â¹ï¸ Agent stopping')
		self.state.stopped = True

		# Signal pause event to unblock any waiting code so it can check the stopped state
		self._external_pause_event.set()

		# Task stopped

	def _convert_initial_actions(self, actions: list[dict[str, dict[str, Any]]]) -> list[ActionModel]:
		"""Convert dictionary-based actions to ActionModel instances"""
		converted_actions = []
		action_model = self.ActionModel
		for action_dict in actions:
			# Each action_dict should have a single key-value pair
			action_name = next(iter(action_dict))
			params = action_dict[action_name]

			# Get the parameter model for this action from registry
			action_info = self.tools.registry.registry.actions[action_name]
			param_model = action_info.param_model

			# Create validated parameters using the appropriate param model
			validated_params = param_model(**params)

			# Create ActionModel instance with the validated parameters
			action_model = self.ActionModel(**{action_name: validated_params})
			converted_actions.append(action_model)

		return converted_actions

	def _verify_and_setup_llm(self):
		"""
		Verify that the LLM API keys are setup and the LLM API is responding properly.
		Also handles tool calling method detection if in auto mode.
		"""

		# Skip verification if already done
		if getattr(self.llm, '_verified_api_keys', None) is True or CONFIG.SKIP_LLM_API_KEY_VERIFICATION:
			setattr(self.llm, '_verified_api_keys', True)
			return True

	@property
	def message_manager(self) -> MessageManager:
		return self._message_manager

	async def close(self):
		"""Close all resources"""
		try:
			# Only close browser if keep_alive is False (or not set)
			if self.browser_session is not None:
				if not self.browser_session.browser_profile.keep_alive:
					# Kill the browser session - this dispatches BrowserStopEvent,
					# stops the EventBus with clear=True, and recreates a fresh EventBus
					await self.browser_session.kill()

			# Force garbage collection
			gc.collect()

			# Debug: Log remaining threads and asyncio tasks
			import threading

			threads = threading.enumerate()
			self.logger.debug(f'ğŸ§µ Remaining threads ({len(threads)}): {[t.name for t in threads]}')

			# Get all asyncio tasks
			tasks = asyncio.all_tasks(asyncio.get_event_loop())
			# Filter out the current task (this close() coroutine)
			other_tasks = [t for t in tasks if t != asyncio.current_task()]
			if other_tasks:
				self.logger.debug(f'âš¡ Remaining asyncio tasks ({len(other_tasks)}):')
				for task in other_tasks[:10]:  # Limit to first 10 to avoid spam
					self.logger.debug(f'  - {task.get_name()}: {task}')

		except Exception as e:
			self.logger.error(f'Error during cleanup: {e}')

	async def _update_action_models_for_page(self, page_url: str) -> None:
		"""Update action models with page-specific actions"""
		# Create new action model with current page's filtered actions
		self.ActionModel = self.tools.registry.create_action_model(page_url=page_url)
		# Update output model with the new actions
		if self.settings.flash_mode:
			self.AgentOutput = AgentOutput.type_with_custom_actions_flash_mode(self.ActionModel)
		elif self.settings.use_thinking:
			self.AgentOutput = AgentOutput.type_with_custom_actions(self.ActionModel)
		else:
			self.AgentOutput = AgentOutput.type_with_custom_actions_no_thinking(self.ActionModel)

		# Update done action model too
		self.DoneActionModel = self.tools.registry.create_action_model(include_actions=['done'], page_url=page_url)
		if self.settings.flash_mode:
			self.DoneAgentOutput = AgentOutput.type_with_custom_actions_flash_mode(self.DoneActionModel)
		elif self.settings.use_thinking:
			self.DoneAgentOutput = AgentOutput.type_with_custom_actions(self.DoneActionModel)
		else:
			self.DoneAgentOutput = AgentOutput.type_with_custom_actions_no_thinking(self.DoneActionModel)

	async def authenticate_cloud_sync(self, show_instructions: bool = True) -> bool:
		"""
		Authenticate with cloud service for future runs.

		This is useful when users want to authenticate after a task has completed
		so that future runs will sync to the cloud.

		Args:
			show_instructions: Whether to show authentication instructions to user

		Returns:
			bool: True if authentication was successful
		"""
		self.logger.warning('Cloud sync has been removed and is no longer available')
		return False

	def run_sync(
		self,
		max_steps: int = 100,
		on_step_start: AgentHookFunc | None = None,
		on_step_end: AgentHookFunc | None = None,
	) -> AgentHistoryList[AgentStructuredOutput]:
		"""Synchronous wrapper around the async run method for easier usage without asyncio."""
		import asyncio

		return asyncio.run(self.run(max_steps=max_steps, on_step_start=on_step_start, on_step_end=on_step_end))

	def detect_variables(self) -> dict[str, DetectedVariable]:
		"""Detect reusable variables in agent history"""
		from browser_use.agent.variable_detector import detect_variables_in_history

		return detect_variables_in_history(self.history)

	def _substitute_variables_in_history(self, history: AgentHistoryList, variables: dict[str, str]) -> AgentHistoryList:
		"""Substitute variables in history with new values for rerunning with different data"""
		from browser_use.agent.variable_detector import detect_variables_in_history

		# Detect variables in the history
		detected_vars = detect_variables_in_history(history)

		# Build a mapping of original values to new values
		value_replacements: dict[str, str] = {}
		for var_name, new_value in variables.items():
			if var_name in detected_vars:
				old_value = detected_vars[var_name].original_value
				value_replacements[old_value] = new_value
			else:
				self.logger.warning(f'Variable "{var_name}" not found in history, skipping substitution')

		if not value_replacements:
			self.logger.info('No variables to substitute')
			return history

		# Create a deep copy of history to avoid modifying the original
		import copy

		modified_history = copy.deepcopy(history)

		# Substitute values in all actions
		substitution_count = 0
		for history_item in modified_history.history:
			if not history_item.model_output or not history_item.model_output.action:
				continue

			for action in history_item.model_output.action:
				# Handle both Pydantic models and dicts
				if hasattr(action, 'model_dump'):
					action_dict = action.model_dump()
				elif isinstance(action, dict):
					action_dict = action
				else:
					action_dict = vars(action) if hasattr(action, '__dict__') else {}

				# Substitute in all string fields
				substitution_count += self._substitute_in_dict(action_dict, value_replacements)

				# Update the action with modified values
				if hasattr(action, 'model_dump'):
					# For Pydantic RootModel, we need to recreate from the modified dict
					if hasattr(action, 'root'):
						# This is a RootModel - recreate it from the modified dict
						new_action = type(action).model_validate(action_dict)
						# Replace the root field in-place using object.__setattr__ to bypass Pydantic's immutability
						object.__setattr__(action, 'root', getattr(new_action, 'root'))
					else:
						# Regular Pydantic model - update fields in-place
						for key, val in action_dict.items():
							if hasattr(action, key):
								setattr(action, key, val)
				elif isinstance(action, dict):
					action.update(action_dict)

		self.logger.info(f'Substituted {substitution_count} value(s) in {len(value_replacements)} variable type(s) in history')
		return modified_history

	def _substitute_in_dict(self, data: dict, replacements: dict[str, str]) -> int:
		"""Recursively substitute values in a dictionary, returns count of substitutions made"""
		count = 0
		for key, value in data.items():
			if isinstance(value, str):
				# Replace if exact match
				if value in replacements:
					data[key] = replacements[value]
					count += 1
			elif isinstance(value, dict):
				# Recurse into nested dicts
				count += self._substitute_in_dict(value, replacements)
			elif isinstance(value, list):
				# Handle lists
				for i, item in enumerate(value):
					if isinstance(item, str) and item in replacements:
						value[i] = replacements[item]
						count += 1
					elif isinstance(item, dict):
						count += self._substitute_in_dict(item, replacements)
		return count

```

---

## backend/browser-use/browser_use/agent/variable_detector.py

```py
"""Detect variables in agent history for reuse"""

import re

from browser_use.agent.views import AgentHistoryList, DetectedVariable
from browser_use.dom.views import DOMInteractedElement


def detect_variables_in_history(history: AgentHistoryList) -> dict[str, DetectedVariable]:
	"""
	Analyze agent history and detect reusable variables.

	Uses two strategies:
	1. Element attributes (id, name, type, placeholder, aria-label) - most reliable
	2. Value pattern matching (email, phone, date formats) - fallback

	Returns:
		Dictionary mapping variable names to DetectedVariable objects
	"""
	detected: dict[str, DetectedVariable] = {}
	detected_values: set[str] = set()  # Track which values we've already detected

	for step_idx, history_item in enumerate(history.history):
		if not history_item.model_output:
			continue

		for action_idx, action in enumerate(history_item.model_output.action):
			# Convert action to dict - handle both Pydantic models and dict-like objects
			if hasattr(action, 'model_dump'):
				action_dict = action.model_dump()
			elif isinstance(action, dict):
				action_dict = action
			else:
				# For SimpleNamespace or similar objects
				action_dict = vars(action)

			# Get the interacted element for this action (if available)
			element = None
			if history_item.state and history_item.state.interacted_element:
				if len(history_item.state.interacted_element) > action_idx:
					element = history_item.state.interacted_element[action_idx]

			# Detect variables in this action
			_detect_in_action(action_dict, element, detected, detected_values)

	return detected


def _detect_in_action(
	action_dict: dict,
	element: DOMInteractedElement | None,
	detected: dict[str, DetectedVariable],
	detected_values: set[str],
) -> None:
	"""Detect variables in a single action using element context"""

	# Extract action type and parameters
	for action_type, params in action_dict.items():
		if not isinstance(params, dict):
			continue

		# Check fields that commonly contain variables
		fields_to_check = ['text', 'query']

		for field in fields_to_check:
			if field not in params:
				continue

			value = params[field]
			if not isinstance(value, str) or not value.strip():
				continue

			# Skip if we already detected this exact value
			if value in detected_values:
				continue

			# Try to detect variable type (with element context)
			var_info = _detect_variable_type(value, element)
			if not var_info:
				continue

			var_name, var_format = var_info

			# Ensure unique variable name
			var_name = _ensure_unique_name(var_name, detected)

			# Add detected variable
			detected[var_name] = DetectedVariable(
				name=var_name,
				original_value=value,
				type='string',
				format=var_format,
			)

			detected_values.add(value)


def _detect_variable_type(
	value: str,
	element: DOMInteractedElement | None = None,
) -> tuple[str, str | None] | None:
	"""
	Detect if a value looks like a variable, using element context when available.

	Priority:
	1. Element attributes (id, name, type, placeholder, aria-label) - most reliable
	2. Value pattern matching (email, phone, date formats) - fallback

	Returns:
		(variable_name, format) or None if not detected
	"""

	# STRATEGY 1: Use element attributes (most reliable)
	if element and element.attributes:
		attr_detection = _detect_from_attributes(element.attributes)
		if attr_detection:
			return attr_detection

	# STRATEGY 2: Pattern matching on value (fallback)
	return _detect_from_value_pattern(value)


def _detect_from_attributes(attributes: dict[str, str]) -> tuple[str, str | None] | None:
	"""
	Detect variable from element attributes.

	Check attributes in priority order:
	1. type attribute (HTML5 input types - most specific)
	2. id, name, placeholder, aria-label (semantic hints)
	"""

	# Check 'type' attribute first (HTML5 input types)
	input_type = attributes.get('type', '').lower()
	if input_type == 'email':
		return ('email', 'email')
	elif input_type == 'tel':
		return ('phone', 'phone')
	elif input_type == 'date':
		return ('date', 'date')
	elif input_type == 'number':
		return ('number', 'number')
	elif input_type == 'url':
		return ('url', 'url')

	# Combine semantic attributes for keyword matching
	semantic_attrs = [
		attributes.get('id', ''),
		attributes.get('name', ''),
		attributes.get('placeholder', ''),
		attributes.get('aria-label', ''),
	]

	combined_text = ' '.join(semantic_attrs).lower()

	# Address detection
	if any(keyword in combined_text for keyword in ['address', 'street', 'addr']):
		if 'billing' in combined_text:
			return ('billing_address', None)
		elif 'shipping' in combined_text:
			return ('shipping_address', None)
		else:
			return ('address', None)

	# Comment/Note detection
	if any(keyword in combined_text for keyword in ['comment', 'note', 'message', 'description']):
		return ('comment', None)

	# Email detection
	if 'email' in combined_text or 'e-mail' in combined_text:
		return ('email', 'email')

	# Phone detection
	if any(keyword in combined_text for keyword in ['phone', 'tel', 'mobile', 'cell']):
		return ('phone', 'phone')

	# Name detection (order matters - check specific before general)
	if 'first' in combined_text and 'name' in combined_text:
		return ('first_name', None)
	elif 'last' in combined_text and 'name' in combined_text:
		return ('last_name', None)
	elif 'full' in combined_text and 'name' in combined_text:
		return ('full_name', None)
	elif 'name' in combined_text:
		return ('name', None)

	# Date detection
	if any(keyword in combined_text for keyword in ['date', 'dob', 'birth']):
		return ('date', 'date')

	# City detection
	if 'city' in combined_text:
		return ('city', None)

	# State/Province detection
	if 'state' in combined_text or 'province' in combined_text:
		return ('state', None)

	# Country detection
	if 'country' in combined_text:
		return ('country', None)

	# Zip code detection
	if any(keyword in combined_text for keyword in ['zip', 'postal', 'postcode']):
		return ('zip_code', 'postal_code')

	# Company detection
	if 'company' in combined_text or 'organization' in combined_text:
		return ('company', None)

	return None


def _detect_from_value_pattern(value: str) -> tuple[str, str | None] | None:
	"""
	Detect variable type from value pattern (fallback when no element context).

	Patterns:
	- Email: contains @ and . with valid format
	- Phone: digits with separators, 10+ chars
	- Date: YYYY-MM-DD format
	- Name: Capitalized word(s), 2-30 chars, letters only
	- Number: Pure digits, 1-9 chars
	"""

	# Email detection - most specific first
	if '@' in value and '.' in value:
		# Basic email validation
		if re.match(r'^[\w\.-]+@[\w\.-]+\.\w+$', value):
			return ('email', 'email')

	# Phone detection (digits with separators, 10+ chars)
	if re.match(r'^[\d\s\-\(\)\+]+$', value):
		# Remove separators and check length
		digits_only = re.sub(r'[\s\-\(\)\+]', '', value)
		if len(digits_only) >= 10:
			return ('phone', 'phone')

	# Date detection (YYYY-MM-DD or similar)
	if re.match(r'^\d{4}-\d{2}-\d{2}$', value):
		return ('date', 'date')

	# Name detection (capitalized, only letters/spaces, 2-30 chars)
	if value and value[0].isupper() and value.replace(' ', '').replace('-', '').isalpha() and 2 <= len(value) <= 30:
		words = value.split()
		if len(words) == 1:
			return ('first_name', None)
		elif len(words) == 2:
			return ('full_name', None)
		else:
			return ('name', None)

	# Number detection (pure digits, not phone length)
	if value.isdigit() and 1 <= len(value) <= 9:
		return ('number', 'number')

	return None


def _ensure_unique_name(base_name: str, existing: dict[str, DetectedVariable]) -> str:
	"""
	Ensure variable name is unique by adding suffix if needed.

	Examples:
		first_name â†’ first_name
		first_name (exists) â†’ first_name_2
		first_name_2 (exists) â†’ first_name_3
	"""
	if base_name not in existing:
		return base_name

	# Add numeric suffix
	counter = 2
	while f'{base_name}_{counter}' in existing:
		counter += 1

	return f'{base_name}_{counter}'

```

---

## backend/browser-use/browser_use/agent/views.py

```py
from __future__ import annotations

import json
import logging
import traceback
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Generic, Literal

from openai import RateLimitError
from pydantic import BaseModel, ConfigDict, Field, ValidationError, create_model, model_validator
from typing_extensions import TypeVar
from uuid_extensions import uuid7str

from browser_use.agent.message_manager.views import MessageManagerState
from browser_use.browser.views import BrowserStateHistory
from browser_use.dom.views import DEFAULT_INCLUDE_ATTRIBUTES, DOMInteractedElement, DOMSelectorMap

# from browser_use.dom.history_tree_processor.service import (
# 	DOMElementNode,
# 	DOMHistoryElement,
# 	HistoryTreeProcessor,
# )
# from browser_use.dom.views import SelectorMap
from browser_use.filesystem.file_system import FileSystemState
from browser_use.llm.base import BaseChatModel
from browser_use.tokens.views import UsageSummary
from browser_use.tools.registry.views import ActionModel

logger = logging.getLogger(__name__)


class AgentSettings(BaseModel):
	"""Configuration options for the Agent"""

	use_vision: bool | Literal['auto'] = True
	vision_detail_level: Literal['auto', 'low', 'high'] = 'auto'
	save_conversation_path: str | Path | None = None
	save_conversation_path_encoding: str | None = 'utf-8'
	max_failures: int = 3
	generate_gif: bool | str = False
	override_system_message: str | None = None
	extend_system_message: str | None = None
	include_attributes: list[str] | None = DEFAULT_INCLUDE_ATTRIBUTES
	max_actions_per_step: int = 3
	use_thinking: bool = True
	flash_mode: bool = False  # If enabled, disables evaluation_previous_goal and next_goal, and sets use_thinking = False
	use_judge: bool = True
	ground_truth: str | None = None  # Ground truth answer or criteria for judge validation
	max_history_items: int | None = None

	page_extraction_llm: BaseChatModel | None = None
	calculate_cost: bool = False
	include_tool_call_examples: bool = False
	llm_timeout: int = 60  # Timeout in seconds for LLM calls (auto-detected: 30s for gemini, 90s for o3, 60s default)
	step_timeout: int = 180  # Timeout in seconds for each step
	final_response_after_failure: bool = True  # If True, attempt one final recovery call after max_failures


class AgentState(BaseModel):
	"""Holds all state information for an Agent"""

	model_config = ConfigDict(arbitrary_types_allowed=True)

	agent_id: str = Field(default_factory=uuid7str)
	n_steps: int = 1
	consecutive_failures: int = 0
	last_result: list[ActionResult] | None = None
	last_plan: str | None = None
	last_model_output: AgentOutput | None = None

	# Pause/resume state (kept serialisable for checkpointing)
	paused: bool = False
	stopped: bool = False
	session_initialized: bool = False  # Track if session events have been dispatched
	follow_up_task: bool = False  # Track if the agent is a follow-up task

	message_manager_state: MessageManagerState = Field(default_factory=MessageManagerState)
	file_system_state: FileSystemState | None = None


@dataclass
class AgentStepInfo:
	step_number: int
	max_steps: int

	def is_last_step(self) -> bool:
		"""Check if this is the last step"""
		return self.step_number >= self.max_steps - 1


class JudgementResult(BaseModel):
	"""LLM judgement of agent trace"""

	reasoning: str | None = Field(default=None, description='Explanation of the judgement')
	verdict: bool = Field(description='Whether the trace was successful or not')
	failure_reason: str | None = Field(
		default=None,
		description='Max 5 sentences explanation of why the task was not completed successfully in case of failure. If verdict is true, use an empty string.',
	)
	impossible_task: bool = Field(
		default=False,
		description='True if the task was impossible to complete due to vague instructions, broken website, inaccessible links, missing login credentials, or other insurmountable obstacles',
	)
	reached_captcha: bool = Field(
		default=False,
		description='True if the agent encountered captcha challenges during task execution',
	)


class ActionResult(BaseModel):
	"""Result of executing an action"""

	# For done action
	is_done: bool | None = False
	success: bool | None = None

	# For trace judgement
	judgement: JudgementResult | None = None

	# Error handling - always include in long term memory
	error: str | None = None

	# Files
	attachments: list[str] | None = None  # Files to display in the done message

	# Images (base64 encoded) - separate from text content for efficient handling
	images: list[dict[str, Any]] | None = None  # [{"name": "file.jpg", "data": "base64_string"}]

	# Always include in long term memory
	long_term_memory: str | None = None  # Memory of this action

	# if update_only_read_state is True we add the extracted_content to the agent context only once for the next step
	# if update_only_read_state is False we add the extracted_content to the agent long term memory if no long_term_memory is provided
	extracted_content: str | None = None
	include_extracted_content_only_once: bool = False  # Whether the extracted content should be used to update the read_state

	# Metadata for observability (e.g., click coordinates)
	metadata: dict | None = None

	# Deprecated
	include_in_memory: bool = False  # whether to include in extracted_content inside long_term_memory

	@model_validator(mode='after')
	def validate_success_requires_done(self):
		"""Ensure success=True can only be set when is_done=True"""
		if self.success is True and self.is_done is not True:
			raise ValueError(
				'success=True can only be set when is_done=True. '
				'For regular actions that succeed, leave success as None. '
				'Use success=False only for actions that fail.'
			)
		return self


class RerunSummaryAction(BaseModel):
	"""AI-generated summary for rerun completion"""

	summary: str = Field(description='Summary of what happened during the rerun')
	success: bool = Field(description='Whether the rerun completed successfully based on visual inspection')
	completion_status: Literal['complete', 'partial', 'failed'] = Field(
		description='Status of rerun completion: complete (all steps succeeded), partial (some steps succeeded), failed (task did not complete)'
	)


class StepMetadata(BaseModel):
	"""Metadata for a single step including timing and token information"""

	step_start_time: float
	step_end_time: float
	step_number: int
	step_interval: float | None = None

	@property
	def duration_seconds(self) -> float:
		"""Calculate step duration in seconds"""
		return self.step_end_time - self.step_start_time


class AgentBrain(BaseModel):
	thinking: str | None = None
	evaluation_previous_goal: str
	memory: str
	next_goal: str


class AgentOutput(BaseModel):
	model_config = ConfigDict(arbitrary_types_allowed=True, extra='forbid')

	thinking: str | None = None
	evaluation_previous_goal: str | None = None
	memory: str | None = None
	next_goal: str | None = None
	action: list[ActionModel] = Field(
		...,
		json_schema_extra={'min_items': 1},  # Ensure at least one action is provided
	)

	@classmethod
	def model_json_schema(cls, **kwargs):
		schema = super().model_json_schema(**kwargs)
		schema['required'] = ['evaluation_previous_goal', 'memory', 'next_goal', 'action']
		return schema

	@property
	def current_state(self) -> AgentBrain:
		"""For backward compatibility - returns an AgentBrain with the flattened properties"""
		return AgentBrain(
			thinking=self.thinking,
			evaluation_previous_goal=self.evaluation_previous_goal if self.evaluation_previous_goal else '',
			memory=self.memory if self.memory else '',
			next_goal=self.next_goal if self.next_goal else '',
		)

	@staticmethod
	def type_with_custom_actions(custom_actions: type[ActionModel]) -> type[AgentOutput]:
		"""Extend actions with custom actions"""

		model_ = create_model(
			'AgentOutput',
			__base__=AgentOutput,
			action=(
				list[custom_actions],  # type: ignore
				Field(..., description='List of actions to execute', json_schema_extra={'min_items': 1}),
			),
			__module__=AgentOutput.__module__,
		)
		return model_

	@staticmethod
	def type_with_custom_actions_no_thinking(custom_actions: type[ActionModel]) -> type[AgentOutput]:
		"""Extend actions with custom actions and exclude thinking field"""

		class AgentOutputNoThinking(AgentOutput):
			@classmethod
			def model_json_schema(cls, **kwargs):
				schema = super().model_json_schema(**kwargs)
				del schema['properties']['thinking']
				schema['required'] = ['evaluation_previous_goal', 'memory', 'next_goal', 'action']
				return schema

		model = create_model(
			'AgentOutput',
			__base__=AgentOutputNoThinking,
			action=(
				list[custom_actions],  # type: ignore
				Field(..., json_schema_extra={'min_items': 1}),
			),
			__module__=AgentOutputNoThinking.__module__,
		)

		return model

	@staticmethod
	def type_with_custom_actions_flash_mode(custom_actions: type[ActionModel]) -> type[AgentOutput]:
		"""Extend actions with custom actions for flash mode - memory and action fields only"""

		class AgentOutputFlashMode(AgentOutput):
			@classmethod
			def model_json_schema(cls, **kwargs):
				schema = super().model_json_schema(**kwargs)
				# Remove thinking, evaluation_previous_goal, and next_goal fields
				del schema['properties']['thinking']
				del schema['properties']['evaluation_previous_goal']
				del schema['properties']['next_goal']
				# Update required fields to only include remaining properties
				schema['required'] = ['memory', 'action']
				return schema

		model = create_model(
			'AgentOutput',
			__base__=AgentOutputFlashMode,
			action=(
				list[custom_actions],  # type: ignore
				Field(..., json_schema_extra={'min_items': 1}),
			),
			__module__=AgentOutputFlashMode.__module__,
		)

		return model


class AgentHistory(BaseModel):
	"""History item for agent actions"""

	model_output: AgentOutput | None
	result: list[ActionResult]
	state: BrowserStateHistory
	metadata: StepMetadata | None = None
	state_message: str | None = None

	model_config = ConfigDict(arbitrary_types_allowed=True, protected_namespaces=())

	@staticmethod
	def get_interacted_element(model_output: AgentOutput, selector_map: DOMSelectorMap) -> list[DOMInteractedElement | None]:
		elements = []
		for action in model_output.action:
			index = action.get_index()
			if index is not None and index in selector_map:
				el = selector_map[index]
				elements.append(DOMInteractedElement.load_from_enhanced_dom_tree(el))
			else:
				elements.append(None)
		return elements

	def _filter_sensitive_data_from_string(self, value: str, sensitive_data: dict[str, str | dict[str, str]] | None) -> str:
		"""Filter out sensitive data from a string value"""
		if not sensitive_data:
			return value

		# Collect all sensitive values, immediately converting old format to new format
		sensitive_values: dict[str, str] = {}

		# Process all sensitive data entries
		for key_or_domain, content in sensitive_data.items():
			if isinstance(content, dict):
				# Already in new format: {domain: {key: value}}
				for key, val in content.items():
					if val:  # Skip empty values
						sensitive_values[key] = val
			elif content:  # Old format: {key: value} - convert to new format internally
				# We treat this as if it was {'http*://*': {key_or_domain: content}}
				sensitive_values[key_or_domain] = content

		# If there are no valid sensitive data entries, just return the original value
		if not sensitive_values:
			return value

		# Replace all valid sensitive data values with their placeholder tags
		for key, val in sensitive_values.items():
			value = value.replace(val, f'<secret>{key}</secret>')

		return value

	def _filter_sensitive_data_from_dict(
		self, data: dict[str, Any], sensitive_data: dict[str, str | dict[str, str]] | None
	) -> dict[str, Any]:
		"""Recursively filter sensitive data from a dictionary"""
		if not sensitive_data:
			return data

		filtered_data = {}
		for key, value in data.items():
			if isinstance(value, str):
				filtered_data[key] = self._filter_sensitive_data_from_string(value, sensitive_data)
			elif isinstance(value, dict):
				filtered_data[key] = self._filter_sensitive_data_from_dict(value, sensitive_data)
			elif isinstance(value, list):
				filtered_data[key] = [
					self._filter_sensitive_data_from_string(item, sensitive_data)
					if isinstance(item, str)
					else self._filter_sensitive_data_from_dict(item, sensitive_data)
					if isinstance(item, dict)
					else item
					for item in value
				]
			else:
				filtered_data[key] = value
		return filtered_data

	def model_dump(self, sensitive_data: dict[str, str | dict[str, str]] | None = None, **kwargs) -> dict[str, Any]:
		"""Custom serialization handling circular references and filtering sensitive data"""

		# Handle action serialization
		model_output_dump = None
		if self.model_output:
			action_dump = [action.model_dump(exclude_none=True, mode='json') for action in self.model_output.action]

			# Filter sensitive data only from input action parameters if sensitive_data is provided
			if sensitive_data:
				action_dump = [
					self._filter_sensitive_data_from_dict(action, sensitive_data) if 'input' in action else action
					for action in action_dump
				]

			model_output_dump = {
				'evaluation_previous_goal': self.model_output.evaluation_previous_goal,
				'memory': self.model_output.memory,
				'next_goal': self.model_output.next_goal,
				'action': action_dump,  # This preserves the actual action data
			}
			# Only include thinking if it's present
			if self.model_output.thinking is not None:
				model_output_dump['thinking'] = self.model_output.thinking

		# Handle result serialization - don't filter ActionResult data
		# as it should contain meaningful information for the agent
		result_dump = [r.model_dump(exclude_none=True, mode='json') for r in self.result]

		return {
			'model_output': model_output_dump,
			'result': result_dump,
			'state': self.state.to_dict(),
			'metadata': self.metadata.model_dump() if self.metadata else None,
			'state_message': self.state_message,
		}


AgentStructuredOutput = TypeVar('AgentStructuredOutput', bound=BaseModel)


class AgentHistoryList(BaseModel, Generic[AgentStructuredOutput]):
	"""List of AgentHistory messages, i.e. the history of the agent's actions and thoughts."""

	history: list[AgentHistory]
	usage: UsageSummary | None = None

	_output_model_schema: type[AgentStructuredOutput] | None = None

	def total_duration_seconds(self) -> float:
		"""Get total duration of all steps in seconds"""
		total = 0.0
		for h in self.history:
			if h.metadata:
				total += h.metadata.duration_seconds
		return total

	def __len__(self) -> int:
		"""Return the number of history items"""
		return len(self.history)

	def __str__(self) -> str:
		"""Representation of the AgentHistoryList object"""
		return f'AgentHistoryList(all_results={self.action_results()}, all_model_outputs={self.model_actions()})'

	def add_item(self, history_item: AgentHistory) -> None:
		"""Add a history item to the list"""
		self.history.append(history_item)

	def __repr__(self) -> str:
		"""Representation of the AgentHistoryList object"""
		return self.__str__()

	def save_to_file(self, filepath: str | Path, sensitive_data: dict[str, str | dict[str, str]] | None = None) -> None:
		"""Save history to JSON file with proper serialization and optional sensitive data filtering"""
		try:
			Path(filepath).parent.mkdir(parents=True, exist_ok=True)
			data = self.model_dump(sensitive_data=sensitive_data)
			with open(filepath, 'w', encoding='utf-8') as f:
				json.dump(data, f, indent=2)
		except Exception as e:
			raise e

	# def save_as_playwright_script(
	# 	self,
	# 	output_path: str | Path,
	# 	sensitive_data_keys: list[str] | None = None,
	# 	browser_config: BrowserConfig | None = None,
	# 	context_config: BrowserContextConfig | None = None,
	# ) -> None:
	# 	"""
	# 	Generates a Playwright script based on the agent's history and saves it to a file.
	# 	Args:
	# 		output_path: The path where the generated Python script will be saved.
	# 		sensitive_data_keys: A list of keys used as placeholders for sensitive data
	# 							 (e.g., ['username_placeholder', 'password_placeholder']).
	# 							 These will be loaded from environment variables in the
	# 							 generated script.
	# 		browser_config: Configuration of the original Browser instance.
	# 		context_config: Configuration of the original BrowserContext instance.
	# 	"""
	# 	from browser_use.agent.playwright_script_generator import PlaywrightScriptGenerator

	# 	try:
	# 		serialized_history = self.model_dump()['history']
	# 		generator = PlaywrightScriptGenerator(serialized_history, sensitive_data_keys, browser_config, context_config)

	# 		script_content = generator.generate_script_content()
	# 		path_obj = Path(output_path)
	# 		path_obj.parent.mkdir(parents=True, exist_ok=True)
	# 		with open(path_obj, 'w', encoding='utf-8') as f:
	# 			f.write(script_content)
	# 	except Exception as e:
	# 		raise e

	def model_dump(self, **kwargs) -> dict[str, Any]:
		"""Custom serialization that properly uses AgentHistory's model_dump"""
		return {
			'history': [h.model_dump(**kwargs) for h in self.history],
		}

	@classmethod
	def load_from_dict(cls, data: dict[str, Any], output_model: type[AgentOutput]) -> AgentHistoryList:
		# loop through history and validate output_model actions to enrich with custom actions
		for h in data['history']:
			if h['model_output']:
				if isinstance(h['model_output'], dict):
					h['model_output'] = output_model.model_validate(h['model_output'])
				else:
					h['model_output'] = None
			if 'interacted_element' not in h['state']:
				h['state']['interacted_element'] = None

		history = cls.model_validate(data)
		return history

	@classmethod
	def load_from_file(cls, filepath: str | Path, output_model: type[AgentOutput]) -> AgentHistoryList:
		"""Load history from JSON file"""
		with open(filepath, encoding='utf-8') as f:
			data = json.load(f)
		return cls.load_from_dict(data, output_model)

	def last_action(self) -> None | dict:
		"""Last action in history"""
		if self.history and self.history[-1].model_output:
			return self.history[-1].model_output.action[-1].model_dump(exclude_none=True, mode='json')
		return None

	def errors(self) -> list[str | None]:
		"""Get all errors from history, with None for steps without errors"""
		errors = []
		for h in self.history:
			step_errors = [r.error for r in h.result if r.error]

			# each step can have only one error
			errors.append(step_errors[0] if step_errors else None)
		return errors

	def final_result(self) -> None | str:
		"""Final result from history"""
		if self.history and self.history[-1].result[-1].extracted_content:
			return self.history[-1].result[-1].extracted_content
		return None

	def is_done(self) -> bool:
		"""Check if the agent is done"""
		if self.history and len(self.history[-1].result) > 0:
			last_result = self.history[-1].result[-1]
			return last_result.is_done is True
		return False

	def is_successful(self) -> bool | None:
		"""Check if the agent completed successfully - the agent decides in the last step if it was successful or not. None if not done yet."""
		if self.history and len(self.history[-1].result) > 0:
			last_result = self.history[-1].result[-1]
			if last_result.is_done is True:
				return last_result.success
		return None

	def has_errors(self) -> bool:
		"""Check if the agent has any non-None errors"""
		return any(error is not None for error in self.errors())

	def judgement(self) -> dict | None:
		"""Get the judgement result as a dictionary if it exists"""
		if self.history and len(self.history[-1].result) > 0:
			last_result = self.history[-1].result[-1]
			if last_result.judgement:
				return last_result.judgement.model_dump()
		return None

	def is_judged(self) -> bool:
		"""Check if the agent trace has been judged"""
		if self.history and len(self.history[-1].result) > 0:
			last_result = self.history[-1].result[-1]
			return last_result.judgement is not None
		return False

	def is_validated(self) -> bool | None:
		"""Check if the judge validated the agent execution (verdict is True). Returns None if not judged yet."""
		if self.history and len(self.history[-1].result) > 0:
			last_result = self.history[-1].result[-1]
			if last_result.judgement:
				return last_result.judgement.verdict
		return None

	def urls(self) -> list[str | None]:
		"""Get all unique URLs from history"""
		return [h.state.url if h.state.url is not None else None for h in self.history]

	def screenshot_paths(self, n_last: int | None = None, return_none_if_not_screenshot: bool = True) -> list[str | None]:
		"""Get all screenshot paths from history"""
		if n_last == 0:
			return []
		if n_last is None:
			if return_none_if_not_screenshot:
				return [h.state.screenshot_path if h.state.screenshot_path is not None else None for h in self.history]
			else:
				return [h.state.screenshot_path for h in self.history if h.state.screenshot_path is not None]
		else:
			if return_none_if_not_screenshot:
				return [h.state.screenshot_path if h.state.screenshot_path is not None else None for h in self.history[-n_last:]]
			else:
				return [h.state.screenshot_path for h in self.history[-n_last:] if h.state.screenshot_path is not None]

	def screenshots(self, n_last: int | None = None, return_none_if_not_screenshot: bool = True) -> list[str | None]:
		"""Get all screenshots from history as base64 strings"""
		if n_last == 0:
			return []

		history_items = self.history if n_last is None else self.history[-n_last:]
		screenshots = []

		for item in history_items:
			screenshot_b64 = item.state.get_screenshot()
			if screenshot_b64:
				screenshots.append(screenshot_b64)
			else:
				if return_none_if_not_screenshot:
					screenshots.append(None)
				# If return_none_if_not_screenshot is False, we skip None values

		return screenshots

	def action_names(self) -> list[str]:
		"""Get all action names from history"""
		action_names = []
		for action in self.model_actions():
			actions = list(action.keys())
			if actions:
				action_names.append(actions[0])
		return action_names

	def model_thoughts(self) -> list[AgentBrain]:
		"""Get all thoughts from history"""
		return [h.model_output.current_state for h in self.history if h.model_output]

	def model_outputs(self) -> list[AgentOutput]:
		"""Get all model outputs from history"""
		return [h.model_output for h in self.history if h.model_output]

	# get all actions with params
	def model_actions(self) -> list[dict]:
		"""Get all actions from history"""
		outputs = []

		for h in self.history:
			if h.model_output:
				# Guard against None interacted_element before zipping
				interacted_elements = h.state.interacted_element or [None] * len(h.model_output.action)
				for action, interacted_element in zip(h.model_output.action, interacted_elements):
					output = action.model_dump(exclude_none=True, mode='json')
					output['interacted_element'] = interacted_element
					outputs.append(output)
		return outputs

	def action_history(self) -> list[list[dict]]:
		"""Get truncated action history with only essential fields"""
		step_outputs = []

		for h in self.history:
			step_actions = []
			if h.model_output:
				# Guard against None interacted_element before zipping
				interacted_elements = h.state.interacted_element or [None] * len(h.model_output.action)
				# Zip actions with interacted elements and results
				for action, interacted_element, result in zip(h.model_output.action, interacted_elements, h.result):
					action_output = action.model_dump(exclude_none=True, mode='json')
					action_output['interacted_element'] = interacted_element
					# Only keep long_term_memory from result
					action_output['result'] = result.long_term_memory if result and result.long_term_memory else None
					step_actions.append(action_output)
			step_outputs.append(step_actions)

		return step_outputs

	def action_results(self) -> list[ActionResult]:
		"""Get all results from history"""
		results = []
		for h in self.history:
			results.extend([r for r in h.result if r])
		return results

	def extracted_content(self) -> list[str]:
		"""Get all extracted content from history"""
		content = []
		for h in self.history:
			content.extend([r.extracted_content for r in h.result if r.extracted_content])
		return content

	def model_actions_filtered(self, include: list[str] | None = None) -> list[dict]:
		"""Get all model actions from history as JSON"""
		if include is None:
			include = []
		outputs = self.model_actions()
		result = []
		for o in outputs:
			for i in include:
				if i == list(o.keys())[0]:
					result.append(o)
		return result

	def number_of_steps(self) -> int:
		"""Get the number of steps in the history"""
		return len(self.history)

	def agent_steps(self) -> list[str]:
		"""Format agent history as readable step descriptions for judge evaluation."""
		steps = []

		# Iterate through history items (each is an AgentHistory)
		for i, h in enumerate(self.history):
			step_text = f'Step {i + 1}:\n'

			# Get actions from model_output
			if h.model_output and h.model_output.action:
				# Use model_dump with mode='json' to serialize enums properly
				actions_list = [action.model_dump(exclude_none=True, mode='json') for action in h.model_output.action]
				action_json = json.dumps(actions_list, indent=1)
				step_text += f'Actions: {action_json}\n'

			# Get results (already a list[ActionResult] in h.result)
			if h.result:
				for j, result in enumerate(h.result):
					if result.extracted_content:
						content = str(result.extracted_content)
						step_text += f'Result {j + 1}: {content}\n'

					if result.error:
						error = str(result.error)
						step_text += f'Error {j + 1}: {error}\n'

			steps.append(step_text)

		return steps

	@property
	def structured_output(self) -> AgentStructuredOutput | None:
		"""Get the structured output from the history

		Returns:
			The structured output if both final_result and _output_model_schema are available,
			otherwise None
		"""
		final_result = self.final_result()
		if final_result is not None and self._output_model_schema is not None:
			return self._output_model_schema.model_validate_json(final_result)

		return None


class AgentError:
	"""Container for agent error handling"""

	VALIDATION_ERROR = 'Invalid model output format. Please follow the correct schema.'
	RATE_LIMIT_ERROR = 'Rate limit reached. Waiting before retry.'
	NO_VALID_ACTION = 'No valid action found'

	@staticmethod
	def format_error(error: Exception, include_trace: bool = False) -> str:
		"""Format error message based on error type and optionally include trace"""
		message = ''
		if isinstance(error, ValidationError):
			return f'{AgentError.VALIDATION_ERROR}\nDetails: {str(error)}'
		if isinstance(error, RateLimitError):
			return AgentError.RATE_LIMIT_ERROR

		# Handle LLM response validation errors from llm_use
		error_str = str(error)
		if 'LLM response missing required fields' in error_str or 'Expected format: AgentOutput' in error_str:
			# Extract the main error message without the huge stacktrace
			lines = error_str.split('\n')
			main_error = lines[0] if lines else error_str

			# Provide a clearer error message
			helpful_msg = f'{main_error}\n\nThe previous response had an invalid output structure. Please stick to the required output format. \n\n'

			if include_trace:
				helpful_msg += f'\n\nFull stacktrace:\n{traceback.format_exc()}'

			return helpful_msg

		if include_trace:
			return f'{str(error)}\nStacktrace:\n{traceback.format_exc()}'
		return f'{str(error)}'


class DetectedVariable(BaseModel):
	"""A detected variable in agent history"""

	name: str
	original_value: str
	type: str = 'string'
	format: str | None = None


class VariableMetadata(BaseModel):
	"""Metadata about detected variables in history"""

	detected_variables: dict[str, DetectedVariable] = Field(default_factory=dict)

```

---

## backend/browser-use/browser_use/browser/__init__.py

```py
from typing import TYPE_CHECKING

# Type stubs for lazy imports
if TYPE_CHECKING:
	from .profile import BrowserProfile, ProxySettings
	from .session import BrowserSession


# Lazy imports mapping for heavy browser components
_LAZY_IMPORTS = {
	'ProxySettings': ('.profile', 'ProxySettings'),
	'BrowserProfile': ('.profile', 'BrowserProfile'),
	'BrowserSession': ('.session', 'BrowserSession'),
}


def __getattr__(name: str):
	"""Lazy import mechanism for heavy browser components."""
	if name in _LAZY_IMPORTS:
		module_path, attr_name = _LAZY_IMPORTS[name]
		try:
			from importlib import import_module

			# Use relative import for current package
			full_module_path = f'browser_use.browser{module_path}'
			module = import_module(full_module_path)
			attr = getattr(module, attr_name)
			# Cache the imported attribute in the module's globals
			globals()[name] = attr
			return attr
		except ImportError as e:
			raise ImportError(f'Failed to import {name} from {full_module_path}: {e}') from e

	raise AttributeError(f"module '{__name__}' has no attribute '{name}'")


__all__ = [
	'BrowserSession',
	'BrowserProfile',
	'ProxySettings',
]

```

---

## backend/browser-use/browser_use/browser/cloud/cloud.py

```py
"""Cloud browser service integration for browser-use.

This module provides integration with the browser-use cloud browser service.
When cloud_browser=True, it automatically creates a cloud browser instance
and returns the CDP URL for connection.
"""

import logging
import os

import httpx

from browser_use.browser.cloud.views import CloudBrowserAuthError, CloudBrowserError, CloudBrowserResponse, CreateBrowserRequest
from browser_use.sync.auth import CloudAuthConfig

logger = logging.getLogger(__name__)


class CloudBrowserClient:
	"""Client for browser-use cloud browser service."""

	def __init__(self, api_base_url: str = 'https://api.browser-use.com'):
		self.api_base_url = api_base_url
		self.client = httpx.AsyncClient(timeout=30.0)
		self.current_session_id: str | None = None

	async def create_browser(
		self, request: CreateBrowserRequest, extra_headers: dict[str, str] | None = None
	) -> CloudBrowserResponse:
		"""Create a new cloud browser instance. For full docs refer to https://docs.cloud.browser-use.com/api-reference/v-2-api-current/browsers/create-browser-session-browsers-post

		Args:
			request: CreateBrowserRequest object containing browser creation parameters

		Returns:
			CloudBrowserResponse: Contains CDP URL and other browser info
		"""
		url = f'{self.api_base_url}/api/v2/browsers'

		# Try to get API key from environment variable first, then auth config
		api_token = os.getenv('BROWSER_USE_API_KEY')

		if not api_token:
			# Fallback to auth config file
			try:
				auth_config = CloudAuthConfig.load_from_file()
				api_token = auth_config.api_token
			except Exception:
				pass

		if not api_token:
			raise CloudBrowserAuthError(
				'No authentication token found. Please set BROWSER_USE_API_KEY environment variable to authenticate with the cloud service. You can also create an API key at https://cloud.browser-use.com/new-api-key'
			)

		headers = {'X-Browser-Use-API-Key': api_token, 'Content-Type': 'application/json', **(extra_headers or {})}

		# Convert request to dictionary and exclude unset fields
		request_body = request.model_dump(exclude_unset=True)

		try:
			logger.info('ğŸŒ¤ï¸ Creating cloud browser instance...')

			response = await self.client.post(url, headers=headers, json=request_body)

			if response.status_code == 401:
				raise CloudBrowserAuthError(
					'Authentication failed. Please make sure you have set BROWSER_USE_API_KEY environment variable to authenticate with the cloud service. You can also create an API key at https://cloud.browser-use.com/new-api-key'
				)
			elif response.status_code == 403:
				raise CloudBrowserAuthError('Access forbidden. Please check your browser-use cloud subscription status.')
			elif not response.is_success:
				error_msg = f'Failed to create cloud browser: HTTP {response.status_code}'
				try:
					error_data = response.json()
					if 'detail' in error_data:
						error_msg += f' - {error_data["detail"]}'
				except Exception:
					pass
				raise CloudBrowserError(error_msg)

			browser_data = response.json()
			browser_response = CloudBrowserResponse(**browser_data)

			# Store session ID for cleanup
			self.current_session_id = browser_response.id

			logger.info(f'ğŸŒ¤ï¸ Cloud browser created successfully: {browser_response.id}')
			logger.debug(f'ğŸŒ¤ï¸ CDP URL: {browser_response.cdpUrl}')
			# Cyan color for live URL
			logger.info(f'\033[36mğŸ”— Live URL: {browser_response.liveUrl}\033[0m')

			return browser_response

		except httpx.TimeoutException:
			raise CloudBrowserError('Timeout while creating cloud browser. Please try again.')
		except httpx.ConnectError:
			raise CloudBrowserError('Failed to connect to cloud browser service. Please check your internet connection.')
		except Exception as e:
			if isinstance(e, (CloudBrowserError, CloudBrowserAuthError)):
				raise
			raise CloudBrowserError(f'Unexpected error creating cloud browser: {e}')

	async def stop_browser(
		self, session_id: str | None = None, extra_headers: dict[str, str] | None = None
	) -> CloudBrowserResponse:
		"""Stop a cloud browser session.

		Args:
			session_id: Session ID to stop. If None, uses current session.

		Returns:
			CloudBrowserResponse: Updated browser info with stopped status

		Raises:
			CloudBrowserAuthError: If authentication fails
			CloudBrowserError: If stopping fails
		"""
		if session_id is None:
			session_id = self.current_session_id

		if not session_id:
			raise CloudBrowserError('No session ID provided and no current session available')

		url = f'{self.api_base_url}/api/v2/browsers/{session_id}'

		# Try to get API key from environment variable first, then auth config
		api_token = os.getenv('BROWSER_USE_API_KEY')

		if not api_token:
			# Fallback to auth config file
			try:
				auth_config = CloudAuthConfig.load_from_file()
				api_token = auth_config.api_token
			except Exception:
				pass

		if not api_token:
			raise CloudBrowserAuthError(
				'No authentication token found. Please set BROWSER_USE_API_KEY environment variable to authenticate with the cloud service. You can also create an API key at https://cloud.browser-use.com/new-api-key'
			)

		headers = {'X-Browser-Use-API-Key': api_token, 'Content-Type': 'application/json', **(extra_headers or {})}

		request_body = {'action': 'stop'}

		try:
			logger.info(f'ğŸŒ¤ï¸ Stopping cloud browser session: {session_id}')

			response = await self.client.patch(url, headers=headers, json=request_body)

			if response.status_code == 401:
				raise CloudBrowserAuthError(
					'Authentication failed. Please make sure you have set the BROWSER_USE_API_KEY environment variable to authenticate with the cloud service.'
				)
			elif response.status_code == 404:
				# Session already stopped or doesn't exist - treating as error and clearing session
				logger.debug(f'ğŸŒ¤ï¸ Cloud browser session {session_id} not found (already stopped)')
				# Clear current session if it was this one
				if session_id == self.current_session_id:
					self.current_session_id = None
				raise CloudBrowserError(f'Cloud browser session {session_id} not found')
			elif not response.is_success:
				error_msg = f'Failed to stop cloud browser: HTTP {response.status_code}'
				try:
					error_data = response.json()
					if 'detail' in error_data:
						error_msg += f' - {error_data["detail"]}'
				except Exception:
					pass
				raise CloudBrowserError(error_msg)

			browser_data = response.json()
			browser_response = CloudBrowserResponse(**browser_data)

			# Clear current session if it was this one
			if session_id == self.current_session_id:
				self.current_session_id = None

			logger.info(f'ğŸŒ¤ï¸ Cloud browser session stopped: {browser_response.id}')
			logger.debug(f'ğŸŒ¤ï¸ Status: {browser_response.status}')

			return browser_response

		except httpx.TimeoutException:
			raise CloudBrowserError('Timeout while stopping cloud browser. Please try again.')
		except httpx.ConnectError:
			raise CloudBrowserError('Failed to connect to cloud browser service. Please check your internet connection.')
		except Exception as e:
			if isinstance(e, (CloudBrowserError, CloudBrowserAuthError)):
				raise
			raise CloudBrowserError(f'Unexpected error stopping cloud browser: {e}')

	async def close(self):
		"""Close the HTTP client and cleanup any active sessions."""
		# Try to stop current session if active
		if self.current_session_id:
			try:
				await self.stop_browser()
			except Exception as e:
				logger.debug(f'Failed to stop cloud browser session during cleanup: {e}')

		await self.client.aclose()

```

---

## backend/browser-use/browser_use/browser/cloud/views.py

```py
from typing import Literal
from uuid import UUID

from pydantic import BaseModel, ConfigDict, Field

ProxyCountryCode = (
	Literal[
		'us',  # United States
		'uk',  # United Kingdom
		'fr',  # France
		'it',  # Italy
		'jp',  # Japan
		'au',  # Australia
		'de',  # Germany
		'fi',  # Finland
		'ca',  # Canada
		'in',  # India
	]
	| str
)

# Browser session timeout limits (in minutes)
MAX_FREE_USER_SESSION_TIMEOUT = 15  # Free users limited to 15 minutes
MAX_PAID_USER_SESSION_TIMEOUT = 240  # Paid users can go up to 4 hours


# Requests
class CreateBrowserRequest(BaseModel):
	"""Request to create a cloud browser instance.

	Args:
	    cloud_profile_id: The ID of the profile to use for the session
	    cloud_proxy_country_code: Country code for proxy location
	    cloud_timeout: The timeout for the session in minutes
	"""

	model_config = ConfigDict(extra='forbid', populate_by_name=True)

	profile_id: UUID | str | None = Field(
		default=None,
		alias='cloud_profile_id',
		description='The ID of the profile to use for the session. Can be a UUID or a string of UUID.',
		title='Cloud Profile ID',
	)

	proxy_country_code: ProxyCountryCode | None = Field(
		default=None,
		alias='cloud_proxy_country_code',
		description='Country code for proxy location.',
		title='Cloud Proxy Country Code',
	)

	timeout: int | None = Field(
		ge=1,
		le=MAX_PAID_USER_SESSION_TIMEOUT,
		default=None,
		alias='cloud_timeout',
		description=f'The timeout for the session in minutes. Free users are limited to {MAX_FREE_USER_SESSION_TIMEOUT} minutes, paid users can use up to {MAX_PAID_USER_SESSION_TIMEOUT} minutes ({MAX_PAID_USER_SESSION_TIMEOUT // 60} hours).',
		title='Cloud Timeout',
	)


CloudBrowserParams = CreateBrowserRequest  # alias for easier readability


# Responses
class CloudBrowserResponse(BaseModel):
	"""Response from cloud browser API."""

	id: str
	status: str
	liveUrl: str = Field(alias='liveUrl')
	cdpUrl: str = Field(alias='cdpUrl')
	timeoutAt: str = Field(alias='timeoutAt')
	startedAt: str = Field(alias='startedAt')
	finishedAt: str | None = Field(alias='finishedAt', default=None)


# Errors
class CloudBrowserError(Exception):
	"""Exception raised when cloud browser operations fail."""

	pass


class CloudBrowserAuthError(CloudBrowserError):
	"""Exception raised when cloud browser authentication fails."""

	pass

```

---

## backend/browser-use/browser_use/browser/demo_mode.py

```py
"""Demo mode helper for injecting and updating the in-browser log panel."""

from __future__ import annotations

import asyncio
import json
import logging
from collections import deque
from datetime import datetime, timezone
from typing import Any

from browser_use.browser.demo_panel_scripts import get_full_panel_script, get_last_panel_script
from browser_use.browser.session import BrowserSession


class DemoMode:
	VALID_LEVELS = {'info', 'action', 'thought', 'error', 'success', 'warning'}
	MAX_BUFFERED_MESSAGES = 100

	def __init__(self, session: BrowserSession):
		self.session = session
		self.logger = logging.getLogger(f'{__name__}.DemoMode')
		self._script_source: str | None = None
		self._panel_ready = False
		self._lock = asyncio.Lock()
		self._script_identifiers: dict[str, str] = {}
		self._message_buffer: deque[dict[str, Any]] = deque(maxlen=self.MAX_BUFFERED_MESSAGES)

	def reset(self) -> None:
		self._script_source = None
		self._script_identifiers.clear()
		self._message_buffer.clear()
		self._panel_ready = False

	def _load_script(self) -> str:
		if self._script_source is None:
			session_id = self.session.id
			accent_color = '#fe750e'  # Default accent color
			display_mode = self.session.browser_profile.demo_mode_display

			if display_mode == 'last':
				self._script_source = get_last_panel_script(session_id, accent_color)
			else:
				self._script_source = get_full_panel_script(session_id, accent_color)

			self.logger.debug(f'Loaded {display_mode} mode script for session {session_id}')

		return self._script_source

	async def ensure_ready(self) -> None:
		"""Add init script and inject overlay into currently open pages."""
		if not self.session.browser_profile.demo_mode:
			return
		if self.session._cdp_client_root is None:
			raise RuntimeError('Root CDP client not initialized')

		async with self._lock:
			script = self._load_script()
			target_ids = await self._get_relevant_target_ids()
			for target_id in target_ids:
				await self._ensure_script_for_target(target_id, script)

			self._panel_ready = True
			self.logger.debug('Demo overlay injected successfully')

	async def register_new_target(self, target_id: str) -> None:
		"""Ensure demo overlay is attached to a newly created target."""
		await self.refresh_target(target_id)

	async def refresh_target(self, target_id: str) -> None:
		"""Reinstate the overlay and replay logs for the given target."""
		if not self.session.browser_profile.demo_mode:
			return
		if self.session._cdp_client_root is None:
			return

		async with self._lock:
			script = self._load_script()
			if target_id not in self._script_identifiers:
				await self._ensure_script_for_target(target_id, script)
			else:
				await self._reinstate_target(target_id, script)
			self._panel_ready = True

	def unregister_target(self, target_id: str) -> None:
		"""Stop tracking a target after it closes."""
		self._script_identifiers.pop(target_id, None)

	async def send_log(self, message: str, level: str = 'info', metadata: dict[str, Any] | None = None) -> None:
		"""Send a log entry to the in-browser panel."""
		if not message or not self.session.browser_profile.demo_mode:
			return

		try:
			await self.ensure_ready()
		except Exception as exc:
			self.logger.warning(f'Failed to ensure demo mode is ready: {exc}')
			return

		if self.session.agent_focus_target_id is None:
			self.logger.debug('Cannot send demo log: no active target')
			return

		level_value = level.lower()
		if level_value not in self.VALID_LEVELS:
			level_value = 'info'

		metadata = dict(metadata) if metadata else {}

		payload = {
			'message': message,
			'level': level_value,
			'metadata': metadata,
			'timestamp': datetime.now(timezone.utc).isoformat(),
		}

		self._message_buffer.append(payload)

		script = self._build_event_expression(json.dumps(payload, ensure_ascii=False))

		try:
			session = await self.session.get_or_create_cdp_session(target_id=None, focus=False)
		except Exception as exc:
			self.logger.debug(f'Cannot acquire CDP session for demo log: {exc}')
			return

		try:
			await session.cdp_client.send.Runtime.evaluate(
				params={'expression': script, 'awaitPromise': False}, session_id=session.session_id
			)
		except Exception as exc:
			self.logger.debug(f'Failed to send demo log: {exc}')

	def _build_event_expression(self, payload: str) -> str:
		return f"""
(() => {{
	const detail = {payload};
	const event = new CustomEvent('browser-use-log', {{ detail }});
	window.dispatchEvent(event);
}})();
""".strip()

	async def _get_relevant_target_ids(self) -> list[str]:
		targets = await self.session._cdp_get_all_pages(  # - intentional private access
			include_http=True,
			include_about=True,
			include_pages=True,
			include_iframes=False,
			include_workers=False,
			include_chrome=False,
			include_chrome_extensions=False,
			include_chrome_error=False,
		)

		target_ids = [t['targetId'] for t in targets]
		if not target_ids and self.session.agent_focus_target_id:
			target_ids = [self.session.agent_focus_target_id]
		return target_ids

	async def _ensure_script_for_target(self, target_id: str, script: str) -> None:
		if target_id in self._script_identifiers:
			return

		try:
			identifier = await self.session._cdp_add_init_script(script, target_id=target_id)
			self._script_identifiers[target_id] = identifier
		except Exception as exc:
			self.logger.debug(f'Failed to register demo overlay script for {target_id}: {exc}')
			return

		await self._reinstate_target(target_id, script)

	async def _reinstate_target(self, target_id: str, script: str) -> None:
		try:
			await self._inject_into_target(target_id, script)
		except Exception as exc:
			self.logger.debug(f'Failed to inject demo overlay into {target_id}: {exc}')
			return

		try:
			await self._replay_buffer_to_target(target_id)
		except Exception as exc:
			self.logger.debug(f'Failed to replay demo logs into {target_id}: {exc}')

	async def _inject_into_target(self, target_id: str, script: str) -> None:
		session = await self.session.get_or_create_cdp_session(target_id=target_id, focus=False)
		await session.cdp_client.send.Runtime.evaluate(
			params={'expression': script, 'awaitPromise': False},
			session_id=session.session_id,
		)

	async def _replay_buffer_to_target(self, target_id: str) -> None:
		if not self._message_buffer:
			return

		try:
			session = await self.session.get_or_create_cdp_session(target_id=target_id, focus=False)
		except Exception as exc:
			self.logger.debug(f'Cannot replay demo logs to {target_id}: {exc}')
			return

		for payload in self._message_buffer:
			script = self._build_event_expression(json.dumps(payload, ensure_ascii=False))
			try:
				await session.cdp_client.send.Runtime.evaluate(
					params={'expression': script, 'awaitPromise': False},
					session_id=session.session_id,
				)
			except Exception as exc:
				self.logger.debug(f'Failed to replay demo log to {target_id}: {exc}')

```

---

## backend/browser-use/browser_use/browser/demo_panel_scripts.py

```py
"""Demo panel script generators for different display modes."""


def get_full_panel_script(session_id: str, accent_color: str = '#fe750e') -> str:
	"""Generate JavaScript for the full side panel display mode."""
	script = r"""(function () {
  // SESSION_ID_PLACEHOLDER will be replaced by DemoMode with actual session ID
  const SESSION_ID = '__BROWSER_USE_SESSION_ID_PLACEHOLDER__';
  const EXCLUDE_ATTR = 'data-browser-use-exclude-' + SESSION_ID;
  const PANEL_ID = 'browser-use-demo-panel';
  const STYLE_ID = 'browser-use-demo-panel-style';
  const STORAGE_KEY = '__browserUseDemoLogs__';
  const STORAGE_HTML_KEY = '__browserUseDemoLogsHTML__';
  const PANEL_STATE_KEY = '__browserUseDemoPanelState__';
  const TOGGLE_BUTTON_ID = 'browser-use-demo-toggle';
  const ACCENT_COLOR = '#fe750e';
  const FINAL_RESULT_KEY = '__browserUseDemoFinalResult__';
  const DEFAULT_LOGO_SVG = '<svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="none" viewBox="0 0 1000 1000"><path fill="#fff" d="M585.941 28.655C672.468-.3 755.454-7.585 825.373 10.74l1.022.272.605.144.641.19c19.554 5.302 37.541 12.489 53.914 21.402l.672.343.537.32c23.35 12.898 43.372 29.338 59.918 48.857l.691.755.872 1.108a213 213 0 0 1 5.969 7.545l.091.09.05.06.409.582a219 219 0 0 1 13.063 19.595l.355.469h-.077a217 217 0 0 1 3.518 6.192l.113.213.055.101.345.592c.282.517.537 1.036.814 1.555l.223.408a234 234 0 0 1 3.591 7.094l.427.859.077.19a227 227 0 0 1 3.232 7.029l.168.332.173.442a251 251 0 0 1 9.154 24.407l.037.035.032.117.122.453a277 277 0 0 1 6.105 22.763l.095.286.018.056.61 2.895.077.448q.83 3.96 1.559 7.986l.095.419h.096v.619a318 318 0 0 1 3.459 25.541l.009.063a352 352 0 0 1 1.491 26.558l.018.179.005.056.041.44-.041.536c.059 3.969.063 7.973 0 12.009v.773l-.005.427c-.173 9.126-.686 18.422-1.504 27.874v.213c-1.187 13.544-3.055 27.412-5.591 41.568l-.309 1.849-.414 2.106c-5.773 30.846-14.759 63.039-27.064 96.242l-.477 1.39-.327.936-.432-.75a.46.46 0 0 1-.145-.245l-8.828-15.26-9.054-15.393-9.387-15.049-.245-.385-9.627-14.811-.064-.096-10.118-15.101-6.037-8.599-4.372-6.338-.132-.179-10.741-14.616-6.341-8.455-4.686-6.345-.114-.155.032-.177c0-.004 0 .005 0 0l1.354-7.983v-.29l1.06-7.584.836-6.28.077-.541.041-.214.105-.528.75-7.52.454-7.382.45-7.083.15-7.095.005-.063v-.381l.15-6.329-.141-4.696-.009-.408v-1.533l-.305-6.492-.418-5.006-.036-.414v-.764l-.6-6.156-.75-5.733-.905-5.725-.782-4.821-.122-.749-.596-2.091-.614-3.212-1.2-4.943-.018-.061-1.241-4.14h-.113v-.382l-.005-.07v-.235l-1.486-4.606-1.509-4.53-1.196-2.839-.459-1.367-1.8-4.053-1.782-3.856-.027-.058-.041-.081-1.913-3.683-1.955-3.603-2.1-3.454-2.259-3.314-2.255-3.157-4.672-6.028-5.119-5.417-3.754-3.451-3.896-3.45-4.218-3.313-3.909-2.696-.445-.309-3.459-2.231-1.21-.78-2.85-1.499-2.127-1.214-5.259-2.704-5.554-2.402-5.878-2.261-4.659-1.352-1.65-.601-6.645-1.812-6.923-1.656-7.377-1.505-.523-.082-7.168-1.125-8.141-.905-8.577-.603-8.873-.3h-9.318l-9.564.149-9.95.754-4.9.433-5.368.475-10.395 1.203-10.864 1.659-11.018 2.115-11.314 2.414-.05.011-11.491 2.685-.077.02-11.768 3.167-11.914 3.466-.441.141-11.777 3.782-12.232 4.38-9.354 3.62-2.9 1.104-.269.104-12.522 5.127-12.66 5.426-12.854 5.897-12.673 6.261-.141.074-.095.05-12.891 6.587-12.986 6.949-12.982 7.245-12.977 7.695-9.055 5.585-4.086 2.42-3.428 2.191-9.545 6.106-2.55 1.8-10.273 6.796-.248.175-8.358 5.867-4.385 3.022-12.829 9.206-12.197 9.147-.481.361-6.037 4.677-6.643 5.135-9.422 7.646-.941.763-.05.041-2.121 1.667-12.366 10.402-12.226 10.716-12.02 10.97-.057.053-11.932 11.023-11.62 11.469-.15.148-11.476 11.478-11.164 11.763-5.587 6.044-5.479 5.773-.111.119-10.712 12.216-10.556 12.214-.119.179-10.31 12.368-1.808 2.259-8.306 10.269-3.769 4.973-5.337 6.978-.553.726-9.509 12.672-9.207 12.832-8.909 12.836-8.599 12.971-8.018 12.536-.286.445-.078.127-7.733 12.79-.038.064-4.53 7.701-3.172 5.289-.029.05-5.551 10.204-1.658 2.708-.126.246-6.516 12.59-.034.063-.052.05-.094.096-6.459 12.912-1.055 2.267-4.983 10.564-5.706 12.762-3.2 7.465-2.262 5.275-4.797 12.44-.028.078-4.656 12.312.001.219.001.458h-.135l-4.123 11.777-.901 2.999-2.869 9.069-1.658 5.874-1.814 6.052-2.111 8.138-.904 3.471-2.286 9.855-.129.559-.303 1.208-2.262 11.159-1.96 11-.01.077-1.498 10.641-.027.241-.728 6.692-.443 3.394-.009.068-.748 10.032-.002.046.002.454-.608 9.391-.002 1.481.002 1.509v.454l-.152 6.052.152 6.32v2.731l.451 8.705.105.959.046.4.603 7.101.754 5.27.295 2.663.008.059 1.351 7.674 1.508 7.383 1.61 6.734.053.209 1.948 6.451 1.556 4.38.089.255.017.045.412 1.377.041.136 2.261 5.725L126.55 824l2.555 5.261 2.712 4.521.064.109v.286l1.029 1.545 1.906 2.858 3.168 4.375 3.292 4.04 3.467 3.771 3.606 3.757.17.155 3.555 3.271.035.032 3.916 3.312 4.205 3.158 3.801 2.485.141.091.375.281.202.15 4.8 2.854 1.288.681 3.834 2.031 5.276 2.563 5.686 2.39 4.544 1.708 1.508.563 5.803 1.799.508.155 6.792 1.813 7.07 1.504 7.532 1.354 3.13.45.05.004 4.681.754 6.777.618 1.486.132 4.16.295 4.435.309 9.054.3 9.348-.15 9.797-.45 9.956-.604 10.395-1.054 10.561-1.508 10.824-1.804.236-.046.162.123.258.195.345.259 1.693 1.268.092.068.037.027.16.137 3.643 2.749.281.19.097.064.039.027.479.359h.305v.232l.899.677h-.049l6.421 4.848.096.072.174.128 14.619 10.59 5.65 3.994 7.333 5.062 1.96 1.508 10.567 6.943 4.529 3.171.587.377 14.665 9.437.286.177 4.087 2.54 10.867 6.788.084.05 15.315 9.009.35.2 1.126.646 13.674 7.919a.45.45 0 0 1 .356.082h2.283l-1.541.549-.917.332-4.23 1.508-.072.028-2.341.822a639 639 0 0 1-9.93 3.494l-3.059 1.095-.074.023-11.116 3.607-4.073 1.354-8.207 2.44h.435l-3.425.891-2.642.686a599 599 0 0 1-11.746 3.194l-.719.195-.911.227a514 514 0 0 1-8.024 2.008l-2.606.655h.449l-3.981.881-.05.014-2.65.586-1.426.318-10.729 2.267-3.5.627a500 500 0 0 1-8.442 1.559l-2.869.536-1.455.204a482 482 0 0 1-10.892 1.69l-2.313.373-1.811.2c-2.585.345-5.159.672-7.723.972l-4.825.641-.277.018-2.974.213q-4.341.429-8.64.768l-2.327.214-4.538.3-2.75.131q-3.054.172-6.085.296l-.541.027h-.116l-.067.005c-4.394.167-8.75.267-13.066.287l-.204.01h-.71c-3.748.01-7.465-.03-11.153-.13l-2.047-.02-1.422-.09a335 335 0 0 1-6.562-.289l-5.317-.227-2.857-.3-1.66-.15a299 299 0 0 1-4.522-.404l-3.938-.355-3.058-.304-9.784-1.354-.055-.005-2.982-.45-.052-.009-9.362-1.658-3.03-.604-9.217-1.968-2.888-.758-8.91-2.268-1.047-.313-1.992-.595-.05-.018-2.909-.868h.153l-5.737-1.813-.101-.032-2.589-1.009a249 249 0 0 1-4.897-1.753l-3.602-1.286-.087-.027-2.718-1.059-8.162-3.326-2.587-1.368-7.852-3.621-2.092-1.226a229 229 0 0 1-3.892-2.063l-4.154-2.158-2.588-1.522-7.253-4.38-3.769-2.631a236 236 0 0 1-3.54-2.413l-2.064-1.308-8.944-6.815-.6-.455-3.58-3.035q-.964-.811-1.919-1.631l-2.526-2.14-1.101-1.045a214 214 0 0 1-4.673-4.33l-2.394-2.19-2.215-2.358a209 209 0 0 1-8.774-9.482l-2.938-3.294-1.224-1.527-6.8-8.764-.046-.077-.146-.286-.543-.777a216 216 0 0 1-4.077-5.821l-1.559-2.23-.128-.182-1.077-1.686-.1-.159-.342-.536-3.034-4.898-.045-.086-.127-.255-.117-.236-.29-.481-.447-.559-.043-.05-.025-.064-.248-.622q-.09-.151-.18-.309l-.131-.196-.436-.781-.575-1-.663-1.104-.344-.686q-.577-1.029-1.143-2.072l-.625-1.013-1.095-2.053-.09-.169-.036-.068-.182-.363-.128-.25-.902-1.813-.023-.045-.305-.614-.322-.718a258 258 0 0 1-1.675-3.421l-.12-.241-.098-.195-.026-.055-.328-.654-1.024-2.194-.281-.423-.031-.05-.436-.654h.167L23.785 865l-.984-2.294q-.21-.484-.418-.968l-1.472-3.239-.603-1.659-.637-1.535-.126-.296-.251-.609-1.108-3.166-.164-.459a255 255 0 0 1-1.275-3.512l-.666-1.595-.768-2.299-1.362-4.385-1.506-4.525-.024-.068v-.318l-.47-1.758a272 272 0 0 1-1.603-5.789l-.628-2.203-.305-1.223-.314-1.467a284 284 0 0 1-1.955-8.51l-.445-1.481-.012-.055-.516-3.194a299 299 0 0 1-1.475-8.083l-.117-.468-.14-.558h.057l-.216-1.513q-.518-3.237-.971-6.507l-.55-3.389-.155-1.84-.108-1.095a333 333 0 0 1-1.368-14.203l-.035-.423-.021-.331a349 349 0 0 1-.4-6.252l-.032-.514-.001-.136a361 361 0 0 1-.442-12.526l-.011-.177v-.35c-.272-16.243.516-32.913 2.328-49.924l.241-2.644.221-1.5q.67-5.778 1.495-11.604l.007-.059a482 482 0 0 1 2.073-13.276l.133-1.063.886-4.712q.515-2.822 1.065-5.652l.015-.086a543 543 0 0 1 7.405-32.636c3.818-14.739 8.309-29.66 13.442-44.649l.126-.399.19-.609.095-.296.454-1.204a681 681 0 0 1 10.567-28.369l.366-.945.078-.173q1.072-2.686 2.169-5.366C85.014 443.52 154.799 339.427 246.82 247.415l4.322-4.297q2.311-2.283 4.633-4.548h-.096l.671-.56c50.277-49.001 103.949-91.419 158.66-126.292h-1.118l2.317-.765a901 901 0 0 1 14.8-9.198l.2-.122a882 882 0 0 1 15.303-9.093l.045-.027c32.543-18.892 65.288-35.085 97.752-48.369a678 678 0 0 1 41.632-15.49m86.295 146.339a.45.45 0 0 1 .528-.087l.077.05 4.386 3.566c185.218 151.444 311.755 364.181 322.091 542.24l.223 4.194c1.149 24.021.127 46.775-2.941 68.134l-.036.241c-1.746 12.094-4.15 23.739-7.187 34.912l.146.031-.296.596-.027.063-.032.059c-5.318 19.337-12.541 37.261-21.559 53.654l-.145.327-.023.046-.027.041-.128.163a218 218 0 0 1-2.941 5.157l-.363.654-.105.255-.168.2-.059.095h.25l-.7.713a212 212 0 0 1-3.905 6.248l-.622.986-3.328 4.993-1.054 1.404a215 215 0 0 1-4.314 5.87l-1.286 1.799-2.5 3.044a205 205 0 0 1-3.677 4.426l-1.091 1.313-.605.654a216 216 0 0 1-5.75 6.316l-1.209 1.349-3.8 3.798-4.227 4.08-2.027 1.795a213 213 0 0 1-10.037 8.569l-.65.532-4.536 3.63-4.323 3.026a214 214 0 0 1-7.923 5.388l-1.677 1.118-4.991 3.176-4.854 2.735-2.146 1.159q-1.44.798-2.9 1.576l-.25.137-.15.077a247 247 0 0 1-5.591 2.871l-4.691 2.345-5.154 2.271-5.591 2.422-5.455 2.117-.545.2a236 236 0 0 1-5.096 1.877l-.104.041-.1.031a255 255 0 0 1-10.014 3.331l-1.222.418-2.346.659a266 266 0 0 1-6.141 1.731l-3.318.94-2.545.568c-2.16.532-4.337 1.045-6.532 1.527l-3.173.777-2.018.345-.782.132a300 300 0 0 1-8.123 1.504l-1.482.29-.05.01-2.018.286q-4.656.737-9.409 1.345l-1.232.172-.05.005-2.168.241c-1.073.127-2.15.245-3.232.363l-1.359.15-6.2.604-2.068.128q-3.803.298-7.654.508l-3.578.273h-.104l-3.937.072q-4.001.135-8.05.18l-1.518.05-1.791-.02q-5.345.015-10.786-.12l-1.186-.01-5.096-.152h-.054l-8.86-.45h-.045l-4.023-.313a514 514 0 0 1-3.632-.273l-6.559-.477-4.572-.554a489 489 0 0 1-7.15-.814l-2.641-.295-3.723-.518c-.577-.077-1.155-.163-1.732-.241l-1.05-.145-.295-.045a516 516 0 0 1-6.078-.9l-1.74-.259-.055-.005-6.645-1.058-8.164-1.513-3.555-.727a597 597 0 0 1-5.8-1.177l-5.604-1.118-4.605-1.126a549 549 0 0 1-8.868-2.113l-1.645-.391-2.514-.654a587 587 0 0 1-5.314-1.386l-7.286-1.885-7.109-2.118-8.305-2.417-7.041-2.244-.072-.023-.237-.077-2.527-.809h.059l-5.6-1.836-3.732-1.349a681 681 0 0 1-6.891-2.413l-4.94-1.676-7.11-2.726-8.459-3.167-6.345-2.644-2.796-1.132-6.427-2.571-7.254-3.176-8.459-3.626-4.596-2.199a753 753 0 0 1-5.736-2.649l-5.382-2.403-7.268-3.63-8.45-4.076-7.264-3.78-8.454-4.38-7.255-3.93-8.209-4.475-.109-.06-.059-.036-1.491-.809h.068l-5.812-3.325-.109-.059-.1-.059-1.374-.787-6.831-3.989-7.144-4.166-.112-.064-.078-.045-1.362-.795-6.872-4.148-7.137-4.462-.115-.068-.387-.246-.947-.59-2.742-1.731-.042-.028-.037-.036-.109-.109-.182-.109-.528-.318-1.1-.686-.119-.073-1.661-1.058-.234-.16-.228-.149-1.813-1.209-2.56-1.658-.122-.078-1.163-.754-.775-.541c-.606-.395-1.211-.795-1.817-1.19l-3.068-1.963-4.085-2.726-.193-.182-2.002-1.354a617 617 0 0 1-3.456-2.339l-4.205-2.845-.118-.077-.166-.113-1.054-.714-4.066-2.799-.333-.222-.128-.086-1.23-.818h.099l-3.513-2.549a969 969 0 0 1-8.648-6.197l-1.495-.996-4.7-3.489-7.252-5.434-3.162-2.263-4.398-3.484-7.847-6.039-1.343-1.104a951 951 0 0 1-6.351-5.025l-.466-.368-8.461-6.797-.755-.604-1.007-.804-6.852-5.693-6.798-5.743-1.964-1.659-4.449-3.921a901 901 0 0 1-5.375-4.689l-2.558-2.108-4.952-4.552a1066 1066 0 0 1-3.364-3.022l-5.894-5.275-.773-.772-2.731-2.553a1045 1045 0 0 1-14.523-13.713l-1.313-1.24-1.704-1.677a1023 1023 0 0 1-16.366-16.247l-3.844-3.844-6.645-6.942-6.359-6.657-.137-.14-3.019-3.172-9.51-10.413-.451-.455-4.35-4.939a972 972 0 0 1-6.839-7.728l-1.514-1.695-5.592-6.497-.511-.604a1009 1009 0 0 1-5.003-5.889l-1.286-1.517-.634-.745-1.932-2.276-9.08-11.014-.164-.2-.304-.454-.114-.173.054-.195 1.812-6.797 1.058-4.23.755-2.722 2.118-7.101 2.225-6.974.043-.137.052-.163 1.87-5.739.193-.595.265-.795.037-.109 2.416-7.251 2.722-7.561 3.016-7.537.145-.573.019-.063 2.869-6.947 2.113-5.28 1.061-2.426 3.323-7.706.906-2.108 2.725-5.757.025-.054 2.704-5.943.295-.659.404-.804.2-.4.127-.25 5.494-10.99 2.233-4.467 4.234-8.014.344-.65.424.6.037.05 8.831 12.426.04.055 9.055 12.371 5.134 6.648 4.247 5.561.134.177 9.658 12.222 9.966 12.231 10.108 12.067 6.048 6.838 3.615 4.035.765.913 10.559 11.768 10.866 11.617 4.075 4.226 6.645 6.797.053.064.277.418 11.288 11.136 3.17 3.021 8.304 8.001.093.086 11.444 10.55.086.078 11.776 10.568 1.051.795h-.142l11.01 9.614 11.956 9.864.123.1 12.233 9.814 12.252 9.414.133.1.086.068 12.271 9.278 7.401 5.134 5.133 3.775 12.534 8.606 12.532 8.455 12.668 7.992 12.682 7.851 3.009 1.654 1.541.85h-.114l8.241 4.889 11.632 6.492 1.059.609 12.664 6.783 8.609 4.38 3.873 2.086.059.032 12.518 6.029 12.373 5.738 12.372 5.43.137.054 12.209 4.912 12.114 4.693 2.409.877 9.65 3.494 11.918 3.925 11.623 3.472 11.468 3.166.072.019.532.136 10.705 2.712 11.177 2.417 10.814 2.104 10.6 1.663 8.759 1.209 1.673.304 9.927 1.054 9.818.754.382.019 9.118.436 9.114.15h.068l8.913-.155h.06l8.363-.45 8.287-.754 7.69-1.054 4.187-.681 3.191-.523 7.086-1.504 6.641-1.663 6.464-1.953 4.709-1.573 1.172-.39 5.705-2.249 5.436-2.417 5.1-2.549 4.819-2.713 4.504-2.849 2.268-1.622 1.959-1.394.705-.568.532-.427h-.068l1.086-.814 1.786-1.336 3.764-3.316 3.618-3.462 3.464-3.612 3.309-3.762 3.154-4.058 3.009-4.216 2.846-4.494 2.718-4.679 2.546-5.089 2.259-5.271 2.259-5.724 2.113-6.039 1.805-6.315 1.804-6.615.246-1.268 1.114-5.825.754-4.516.6-3.021.905-7.829.004-.045.75-8.092.6-8.437.155-8.737-.15-9.205-.305-9.492-.718-8.946-.036-.418v-.304l-.314-3.017-.741-7.074-.009-.06-.441-2.953-.9-7.188-.009-.059-.023-.145-1.786-10.559-2.109-10.864-2.409-10.99-.455-1.672-2.418-9.669-.055-.204-3.09-11.322-.028-.096-3.463-11.599-3.323-9.973-.6-1.949-4.268-11.632-.118-.313-4.523-12.209-1.65-3.762-3.309-8.41-.023-.059-5.427-12.362-5.732-12.527-5.768-11.822h-.05l-.1-.313-.141-.423-6.309-12.472-1.359-2.267-5.441-10.427-2.868-4.98-4.228-7.697-.309-.518-7.241-12.013-.145-.236-6.064-9.814-.132-.218-.427-.69-.927-1.718-8.127-12.644-.119-.118-.054-.069-8.305-12.376-8.6-12.526-4.077-5.584-4.932-6.88-.054-.071-6.187-8.299-3.018-4.07-9.659-12.38-9.682-12.061-.05-.059-.082-.105-9.959-12.075-.032-.033-3.454-3.885-.146-.161-.045-.057-.614-.689-5.927-7.04-.055-.064-.049-.058-10.51-11.71-9.063-9.815-1.673-1.977-11.005-11.457-1.049-1.052-10.119-10.264-11.259-10.961-.063-.06-7.55-7.249-3.919-3.765-11.195-9.999-.145-.134-.451-.447-11.913-10.556-.064-.056-11.854-10.053-.159-.131-12.041-9.807-.037-.029-12.218-9.655-12.386-9.362-.123-.094-12.259-9.117-10.705-7.389-1.777-1.333-.041-.028-.6-.42.646-.351 7.854-4.228 8.014-4.082 4.327-2.164h-.345l2.072-.864 1.814-.756 7.85-3.771 7.709-3.478 7.705-3.474 7.677-3.162 7.586-3.186 7.409-2.871 5.014-1.843h-.382l2.746-.871 7.441-2.578 7.259-2.269 7.1-2.265 7.113-1.968 6.946-1.964 6.504-1.663.055-.014zm105.978 377.474 4.231 8.001 4.078 8.01 3.782 7.861 3.59 7.787.096.091.782.772h-.396l3.182 6.911 3.327 7.715 3.318 7.701 2.914 7.429h.046l.077.309 2.863 7.378 2.723 7.561.041.131 2.527 7.274 2.419 7.252.454 1.358 1.759 5.575.05.164 1.968 7.115.605 1.958 1.364 4.998 1.59 6.665a.44.44 0 0 1 .119.486l-.018.046a1 1 0 0 1-.05.082c-.091.109-.178.222-.269.331l-1.909 2.449-4.586 5.489a1023 1023 0 0 1-27.155 31.536l-1.195 1.354-.391.404a1027 1027 0 0 1-22.182 23.844l-2.059 2.19-1.591 1.591a1035 1035 0 0 1-50.45 48.615l-.023.054-.068.059-2.177 1.927a992 992 0 0 1-8.359 7.397l-1.241 1.09-.791.722h.136l-1.004.786-.127.119-.769.586a951 951 0 0 1-8.9 7.628l-2.445 2.099-.805.645c-4.804 4.031-9.64 8.02-14.509 11.954l-.081.078-.182.163-.241-.063-.2-.055-6.546-1.74-.045-.014-6.95-1.813-7.105-1.967-7.113-2.267-4.837-1.613-2.413-.804-7.4-2.417-7.418-2.726-7.555-2.872-7.482-3.139-.082-.032-3.995-1.631-.086-.032-3.45-1.504-7.641-3.294-.091-.036-7.859-3.631-.059-.027-7.255-3.403-.118-.055-.259-.172-.2-.137-7.823-3.757-7.868-4.085-7.855-4.08-.241-.122v-.427l-.004-.232.191-.136 14.709-10.505.868-.622.282-.2 15.4-11.441.45-.336.041-.027 15.65-12.19 15.55-12.686 15.25-13.13 15.086-13.426 3.473-3.322 11.177-10.572 2.391-2.39.677-.682.096-.091.132-.131 10.745-10.455.45-.441 8.618-8.891.741-.764.127-.14 4.705-4.844 1.804-1.958 6.491-7.092 5.446-5.748 13.423-15.08.754-.904 6.5-7.556 5.764-6.651.563-.736.128-.168 6.068-7.533 6.041-7.252.277-.418 6.064-7.728 5.463-7.006.423-.546 11.768-15.843 11.327-16.006.428-.6zM183.482 8.61c56.203-12.83 122.13-10.008 193.358 8.84l1.423.34h-.14c17.617 4.705 35.557 10.386 53.752 17.054l3.58 1.323.081.04a.44.44 0 0 1 .161.18l.519.188-.617.349a.45.45 0 0 1-.174.098l-.063.038-12.8 7.276-2.675 1.524-15.323 9.162-8.699 5.288-11.53 7.24-5.888 3.778-6.201 3.984-6.351 4.256-6.167 4.136-11.443 7.98-.116.119-.059.048-8.539 6.034-1.584 1.148-.325.234-7.82 5.665-1.508 1.11-2.718 2.053h.063l-1.102.799-.124.107-.474.329-2.563 1.936v.127l-.247.125-.256.126-.685.518-.576.463h.239l-1.219.813-.112.094-.252.15-.784.523-2.482 1.873-.572.509-.157.139-.07-.01-.173.087-.123.063-.136-.018-.701-.087-.508-.063-6.796-1.207-3.857-.531-3.842-.527-7.702-.904-7.515-.753-7.236-.602-3.54-.223-3.734-.233-6.93-.151h-3.928l-2.841-.15-1.326.147-5.315.003-6.482.3-6.344.457-6.022.599-5.87.752-5.586.907-5.41.901-4.681 1.068-.603.139-4.977 1.208-4.731 1.328-.085.024-2.378.691-2.301.669-4.342 1.495-4.211 1.654-4.077 1.814-3.92 1.809-3.752 1.949-3.613 1.957-3.454 2.104-3.312 2.259-3.117 2.224-2.606 2.083-3.453 2.764-5.394 5.093-5.123 5.422-4.826 6.033-2.251 3.151-2.243 3.283-1.048 1.954-.861 1.427-.052.091-2.106 3.61-1.953 3.756-.632 1.472-.258.604-.024.052-.897 1.792-1.654 4.059-1.652 4.208-.352 1.054-.099.302-1.212 3.178-1.352 4.653-.07.258-.985 3.52-.303 1.054-1.356 4.97-.945 4.726-.089.454-.018.083-1.028 5.423-.029.162-.903 5.728-.077.768-.07.715-.041.408-.567 3.842-.6 6.146.001 1.945-.304 4.248-.3 6.493-.151 6.635v6.776l.151 7.094.426 6.658.027.416.521 6.421.081.97.605 6.04.141 1.423.009.087.904 7.528 1.206 7.845 1.359 8.005.032.189-.114.154-1.661 2.265-9.381 12.356-.131.175-10.723 14.799-10.403 14.922-.107.216-8.06 11.881-2.071 2.958-.042.065-.119.181-6.068 9.474-3.619 5.427-4.531 7.4-4.98 7.846-.045.081-2.82 4.898-6.141 10.328-.052.09-1.357 2.415-5.135 9.062-2.188 3.789a.45.45 0 0 1-.14.245l-.414.718-.34-.886-1.51-3.928-3.173-9.063-.907-2.417-3.478-10.435-.958-3.164a613 613 0 0 1-1.268-4.001l-1.854-5.674-2.12-7.415-1.509-5.438-2.72-9.666-.757-3.033-2.868-12.08-.153-.604-3.023-14.06-.01-.047v-.31l-.094-.493a510 510 0 0 1-1.846-9.777l-.615-3.251-.302-1.208-1.97-12.574-.14-2.049a465 465 0 0 1-.942-7.254l-.128-.977-.4-3.079-.056-.406-.005-.053-.06-.447-.387-3.88-.714-7.145-.045-.408-.106-1.1-.044-.431-.044-.476-.2-4.37a418 418 0 0 1-.462-7.661l-.04-.607-.801-.802h.632l-.141-6.632-.152-5.739v-2.331q-.044-4.108 0-8.173v-4.319l.174-3.721q.098-3.05.245-6.074l.034-.915-.001-1.963.455-6.225.268-3.745.03-.437.005-.047.151-1.522.604-5.742.603-4.967v-.611l.712-4.155q.172-1.223.355-2.441l.387-3.182.061-.517.15-1.056.76-3.801.905-4.678.604-3.171.303-1.367.907-4.229.785-3.53.122-.564.456-1.811.608-2.129.272-.971q.553-2.12 1.138-4.22l.41-1.631.143-.585.082-.321.374-1.188.423-1.379.161-.526.019-.06.098-.32.961-3.008.321-.966q.406-1.261.825-2.515l.511-1.789 1.367-3.951 2.114-5.435.302-.903.892-2.086q1.158-2.896 2.383-5.746l.503-1.238.301-.898.034-.076.657-1.188.203-.449.527-1.314h-.066l.328-.655.1-.251.952-1.851.905-1.962.61-1.322h-.084l.4-.681.97-1.663q.324-.64.652-1.278l.328-.657.125-.251.228-.398q.429-.82.864-1.634l.444-.888.126-.25.63-1.11.605-1.208.033-.065.05-.05.112-.115.062-.097q.564-1.008 1.136-2.011l.115-.229.029-.049.11-.168c.181-.315.367-.626.55-.94l.385-.687.112-.219.243-.368.05-.083h-.105l.443-.668.283-.426.034-.049.134-.271.101-.202v-.197l.19-.002.85-1.509.383-.687.036-.062.089-.157.33-.535 5.032-7.62h-.1l.498-.68.114-.226h.055l5.752-7.82.044-.052.144-.145.6-.756q1.88-2.446 3.83-4.834l1.886-2.38 1.948-2.188a212 212 0 0 1 4.135-4.643l1.333-1.494.873-.875a210 210 0 0 1 5.674-5.777l1.167-1.214 1.85-1.679a212 212 0 0 1 5.452-4.956h-.072l.906-.752.718-.602a212 212 0 0 1 7.003-5.765l.947-.778 1.104-.82a219 219 0 0 1 25.577-16.938l1.466-.859.924-.476q4.356-2.4 8.843-4.623l.367-.196.509-.235a237 237 0 0 1 9.335-4.327l.599-.279.455-.183a248 248 0 0 1 14.16-5.598h-.146l2.35-.785a263 263 0 0 1 13.598-4.349h-.37l3.027-.758a278 278 0 0 1 11.728-3.002l.582-.153z"></path></svg>';
  const MAX_MESSAGES = 100;
  const EXPANDED_IDS_KEY = '__browserUseExpandedEntries__';
  const TOGGLE_POSITION_KEY = '__browserUseTogglePosition__';
  const LEVEL_LABELS = {
    info: 'info',
    action: 'action',
    thought: 'thought',
    success: 'success',
    warning: 'warning',
    error: 'error',
  };

  if (window.__browserUseDemoPanelLoaded) {
    const existingPanel = document.getElementById(PANEL_ID);
    if (!existingPanel) {
      initializePanel();
    }
    return;
  }
  window.__browserUseDemoPanelLoaded = true;

  const state = {
    panel: null,
    list: null,
    messages: [],
    isOpen: true,
    toggleButton: null,
    finalResult: loadFinalResult(),
    finalResultSection: null,
    finalResultContent: null,
  };
  state.messages = restoreMessages();
  if (!state.finalResult) {
    const storedFinal = findFinalResultInMessages(state.messages);
    if (storedFinal) {
      state.finalResult = storedFinal;
    }
  }

  function initializePanel() {
    console.log('Browser-use demo panel initialized with session ID:', SESSION_ID);
    addStyles();
    state.isOpen = loadPanelState();
    state.panel = buildPanel();
    state.list = state.panel.querySelector('[data-role="log-list"]');
    state.finalResultSection = state.panel.querySelector('[data-role="final-result"]');
    state.finalResultContent = state.panel.querySelector('[data-role="final-result-content"]');
    appendToHost(state.panel);
    state.toggleButton = buildToggleButton();
    appendToHost(state.toggleButton);
    updateFinalResultDisplay(state.finalResult || '', false);
    const savedWidth = loadPanelWidth();
    if (savedWidth) {
      document.documentElement.style.setProperty('--browser-use-demo-panel-width', `${savedWidth}px`);
    }

    if (!hydrateFromStoredMarkup()) {
      state.messages.forEach((entry) => appendEntry(entry, false));
    }
    highlightLatestEntry(state.list?.lastElementChild || null);
    attachCloseHandler();
    if (state.isOpen) {
      openPanel(false);
    } else {
      closePanel(false);
    }
    adjustLayout();
    window.addEventListener('resize', debounce(adjustLayout, 150));
  }

  function appendToHost(node) {
    if (!node) {
      return;
    }

    const host = document.body || document.documentElement;
    if (!host.contains(node)) {
      host.appendChild(node);
    }

    if (!document.body) {
      document.addEventListener(
        'DOMContentLoaded',
        () => {
          if (document.body && node.parentNode !== document.body) {
            document.body.appendChild(node);
          }
        },
        { once: true }
      );
    }
  }

  function addStyles() {
    if (document.getElementById(STYLE_ID)) {
      return;
    }
    const style = document.createElement('style');
    style.id = STYLE_ID;
    style.setAttribute(EXCLUDE_ATTR, 'true');
    style.textContent = `
      #${PANEL_ID} {
        --browser-use-demo-accent: ${ACCENT_COLOR};
        position: fixed;
        top: 0;
        right: 0;
        width: var(--browser-use-demo-panel-width, 340px);
        max-width: calc(100vw - 64px);
        height: 100vh;
        display: flex;
        flex-direction: column;
        background: #000000;
        color: #f8f9ff;
        font-family: 'JetBrains Mono', 'Fira Code', 'Monaco', 'Menlo', monospace;
        font-size: 13px;
        line-height: 1.4;
        box-shadow: -6px 0 25px rgba(0, 0, 0, 0.35);
        z-index: 2147480000;
        border-left: 1px solid rgba(255, 255, 255, 0.14);
        backdrop-filter: blur(10px);
        pointer-events: auto;
        transform: translateX(0);
        opacity: 1;
        transition: transform 0.25s ease, opacity 0.25s ease;
      }

      #${PANEL_ID}[data-open="false"] {
        transform: translateX(110%);
        opacity: 0;
        pointer-events: none;
      }

      #${PANEL_ID} .browser-use-demo-header {
        padding: 16px 18px 12px;
        border-bottom: 1px solid rgba(255, 255, 255, 0.14);
        display: flex;
        align-items: baseline;
        justify-content: space-between;
        gap: 8px;
        flex-wrap: wrap;
      }

      #${PANEL_ID} .browser-use-demo-header h1 {
        font-size: 15px;
        text-transform: uppercase;
        letter-spacing: 0.12em;
        margin: 0;
        color: #f8f9ff;
      }

      #${PANEL_ID} .browser-use-badge {
        font-size: 11px;
        padding: 2px 10px;
        border-radius: 999px;
        border: 1px solid rgba(255, 255, 255, 0.4);
        text-transform: uppercase;
        letter-spacing: 0.08em;
        color: #f8f9ff;
      }

      #${PANEL_ID} .browser-use-logo svg {
        height: 36px;
        width: auto;
        display: block;
      }

      #${PANEL_ID} .browser-use-header-actions {
        margin-left: auto;
        display: flex;
        align-items: center;
        gap: 8px;
      }

      #${PANEL_ID} .browser-use-final-result {
        margin: 12px 18px 0;
        padding: 14px 16px;
        border-radius: 14px;
        border: 1px solid rgba(255, 255, 255, 0.12);
        background: radial-gradient(circle at top, rgba(255, 255, 255, 0.05), transparent 70%);
        box-shadow: inset 0 0 0 1px rgba(255, 255, 255, 0.04);
        position: relative;
        overflow: hidden;
        display: none;
        opacity: 0;
        transform: translateY(-8px);
      }

      #${PANEL_ID} .browser-use-final-result[data-visible="true"] {
        display: block;
        opacity: 1;
        transform: translateY(0);
      }

      #${PANEL_ID} .browser-use-final-result.is-revealed {
        animation: browser-use-final-pop 0.65s cubic-bezier(0.23, 1, 0.32, 1);
        opacity: 1;
        transform: translateY(0);
      }

      #${PANEL_ID} .browser-use-final-result::after {
        content: '';
        position: absolute;
        inset: 0;
        border-radius: 14px;
        background: linear-gradient(120deg, rgba(255, 117, 14, 0.18), transparent 60%);
        opacity: 0.45;
        pointer-events: none;
      }

      #${PANEL_ID} .browser-use-final-result-label {
        font-size: 11px;
        letter-spacing: 0.08em;
        text-transform: uppercase;
        color: rgba(255, 255, 255, 0.82);
        margin-bottom: 6px;
        position: relative;
        z-index: 2;
      }

      #${PANEL_ID} .browser-use-final-result-body {
        position: relative;
        z-index: 2;
        font-size: 12px;
        color: #fefefe;
        max-height: 180px;
        overflow-y: auto;
        padding-right: 6px;
        white-space: pre-wrap;
        word-break: break-word;
      }

      #${PANEL_ID} .browser-use-final-result-body::-webkit-scrollbar {
        width: 6px;
      }

      #${PANEL_ID} .browser-use-final-result-body::-webkit-scrollbar-thumb {
        background: rgba(255, 255, 255, 0.25);
        border-radius: 999px;
      }

      #${PANEL_ID} .browser-use-close-btn {
        width: 28px;
        height: 28px;
        border-radius: 50%;
        border: 1px solid rgba(255, 255, 255, 0.2);
        background: #000000;
        color: #f8f9ff;
        cursor: pointer;
        font-size: 16px;
        line-height: 1;
        display: flex;
        align-items: center;
        justify-content: center;
        transition: border 0.2s ease, box-shadow 0.2s ease;
      }

      #${PANEL_ID} .browser-use-close-btn:hover {
        border-color: rgba(255, 255, 255, 0.35);
        box-shadow: 0 0 8px rgba(255, 255, 255, 0.2);
      }

      #${PANEL_ID} .browser-use-demo-body {
        flex: 1;
        overflow-y: auto;
        scrollbar-width: thin;
        scrollbar-color: rgba(255, 255, 255, 0.3) transparent;
        padding: 8px 0 12px;
      }

      #${PANEL_ID} .browser-use-demo-body::-webkit-scrollbar {
        width: 8px;
      }

      #${PANEL_ID} .browser-use-demo-body::-webkit-scrollbar-thumb {
        background: rgba(255, 255, 255, 0.25);
        border-radius: 999px;
      }

      .browser-use-demo-entry {
        display: flex;
        gap: 14px;
        padding: 16px 22px;
        border-bottom: 1px solid rgba(255, 255, 255, 0.04);
        animation: browser-use-fade-in 0.25s ease;
        background: #000000;
        position: relative;
        overflow: hidden;
      }

      .browser-use-demo-entry:last-child {
        border-bottom-color: transparent;
      }

      .browser-use-entry-content {
        flex: 1;
        min-width: 0;
        position: relative;
        z-index: 2;
      }

      .browser-use-entry-meta {
        font-size: 11px;
        letter-spacing: 0.08em;
        text-transform: uppercase;
        color: white;
        margin-bottom: 4px;
        display: flex;
        justify-content: space-between;
        gap: 12px;
      }

      .browser-use-entry-message {
        margin: 0;
        word-break: break-word;
        font-size: 12px;
        color: #f8f9ff;
        display: flex;
        flex-direction: column;
        gap: 6px;
      }

      .browser-use-corner {
        position: absolute;
        width: min(24px, 12%);
        height: min(24px, 30%);
        border: 2px solid var(--browser-use-demo-accent);
        pointer-events: none;
        z-index: 1;
        display: none;
        opacity: 0.3;
      }

      .browser-use-corner.corner-top-left {
        top: 6px;
        left: 6px;
        border-right: none;
        border-bottom: none;
      }

      .browser-use-corner.corner-top-right {
        top: 6px;
        right: 6px;
        border-left: none;
        border-bottom: none;
      }

      .browser-use-corner.corner-bottom-left {
        bottom: 6px;
        left: 6px;
        border-right: none;
        border-top: none;
      }

      .browser-use-corner.corner-bottom-right {
        bottom: 6px;
        right: 6px;
        border-left: none;
        border-top: none;
      }

      .browser-use-demo-entry.highlighted .browser-use-corner {
        display: block;
        animation: browser-use-corner-glow 1.8s ease-in-out infinite;
      }

      .browser-use-demo-entry.highlighted {
        animation: browser-use-highlight-in 0.45s ease;
      }

      @keyframes browser-use-corner-glow {
        0% { opacity: 0.2; }
        50% { opacity: 0.85; }
        100% { opacity: 0.2; }
      }

      @keyframes browser-use-highlight-in {
        from { opacity: 0.65; }
        to { opacity: 1; }
      }

      .browser-use-markdown-content {
        margin: 0;
        line-height: 1.5;
      }

      .browser-use-markdown-content p {
        margin: 0 0 8px 0;
      }

      .browser-use-markdown-content p:last-child {
        margin-bottom: 0;
      }

      .browser-use-markdown-content h1,
      .browser-use-markdown-content h2,
      .browser-use-markdown-content h3 {
        margin: 8px 0 4px 0;
        font-weight: 600;
        color: #f8f9ff;
      }

      .browser-use-markdown-content h1 {
        font-size: 16px;
      }

      .browser-use-markdown-content h2 {
        font-size: 14px;
      }

      .browser-use-markdown-content h3 {
        font-size: 13px;
      }

      .browser-use-markdown-content code {
        background: rgba(255, 255, 255, 0.1);
        padding: 2px 6px;
        border-radius: 3px;
        font-family: 'JetBrains Mono', 'Fira Code', 'Monaco', 'Menlo', monospace;
        font-size: 11px;
        color: #60a5fa;
      }

      .browser-use-markdown-content pre {
        background: rgba(0, 0, 0, 0.3);
        padding: 8px 12px;
        border-radius: 4px;
        overflow-x: auto;
        margin: 8px 0;
        border: 1px solid rgba(255, 255, 255, 0.1);
      }

      .browser-use-markdown-content pre code {
        background: transparent;
        padding: 0;
        color: #f8f9ff;
        font-size: 11px;
        white-space: pre;
      }

      .browser-use-markdown-content ul,
      .browser-use-markdown-content ol {
        margin: 4px 0 4px 16px;
        padding: 0;
      }

      .browser-use-markdown-content li {
        margin: 2px 0;
      }

      .browser-use-markdown-content a {
        color: #60a5fa;
        text-decoration: underline;
      }

      .browser-use-markdown-content a:hover {
        color: #93c5fd;
      }

      .browser-use-markdown-content strong {
        font-weight: 600;
        color: #f8f9ff;
      }

      .browser-use-markdown-content em {
        font-style: italic;
      }

      .browser-use-demo-entry:not(.expanded) .browser-use-markdown-content {
        max-height: 150px;
        overflow: hidden;
        mask-image: linear-gradient(to bottom, rgba(0,0,0,1), rgba(0,0,0,0));
        -webkit-mask-image: linear-gradient(to bottom, rgba(0,0,0,1), rgba(0,0,0,0));
      }

      .browser-use-demo-entry:not(.expanded) .browser-use-markdown-content.is-scrollable {
        max-height: 220px;
        overflow-y: auto;
        mask-image: none;
        -webkit-mask-image: none;
        padding-right: 6px;
      }

      .browser-use-markdown-content.is-scrollable::-webkit-scrollbar {
        width: 6px;
      }

      .browser-use-markdown-content.is-scrollable::-webkit-scrollbar-thumb {
        background: rgba(255, 255, 255, 0.25);
        border-radius: 999px;
      }

      .browser-use-entry-toggle {
        align-self: flex-start;
        background: rgba(255, 255, 255, 0.1);
        border: 1px solid rgba(255, 255, 255, 0.2);
        color: #f8f9ff;
        padding: 2px 10px;
        font-size: 11px;
        border-radius: 999px;
        cursor: pointer;
      }

      @keyframes browser-use-fade-in {
        from { opacity: 0; transform: translateY(6px); }
        to { opacity: 1; transform: translateY(0); }
      }

      @keyframes browser-use-final-pop {
        0% { opacity: 0; transform: translateY(12px) scale(0.96); box-shadow: none; }
        60% { opacity: 1; transform: translateY(0) scale(1.02); box-shadow: 0 18px 32px rgba(0, 0, 0, 0.35); }
        100% { opacity: 1; transform: translateY(0) scale(1); box-shadow: 0 12px 24px rgba(0, 0, 0, 0.35); }
      }

      @media (max-width: 1024px) {
        #${PANEL_ID} {
          font-size: 12px;
        }
        #${PANEL_ID} .browser-use-demo-header {
          padding: 12px 16px 10px;
        }
      }

      #${TOGGLE_BUTTON_ID} {
        position: fixed;
        bottom: 20px;
        right: 20px;
        width: 44px;
        height: 44px;
        border-radius: 50%;
        border: 1px solid rgba(255, 255, 255, 0.2);
        background: #000000;
        color: #f8f9ff;
        font-size: 18px;
        display: none;
        align-items: center;
        justify-content: center;
        cursor: grab;
        z-index: 2147480001;
        box-shadow: 0 8px 20px rgba(0, 0, 0, 0.4);
        transition: transform 0.2s ease, box-shadow 0.2s ease;
        user-select: none;
      }

      #${TOGGLE_BUTTON_ID}:hover {
        transform: scale(1.05);
        box-shadow: 0 10px 24px rgba(0, 0, 0, 0.55);
      }

      #${TOGGLE_BUTTON_ID}:active {
        cursor: grabbing;
      }

      #${TOGGLE_BUTTON_ID} img {
        display: block;
        width: 24px;
        height: auto;
        max-width: 100%;
        max-height: 100%;
        object-fit: contain;
        pointer-events: none;
        user-select: none;
      }
    `;
    document.head.appendChild(style);
  }

  function buildPanel() {
    const panel = document.createElement('section');
    panel.id = PANEL_ID;
    panel.setAttribute('role', 'complementary');
    panel.setAttribute('aria-label', 'Browser-use demo panel');
    panel.setAttribute(EXCLUDE_ATTR, 'true');

    const header = document.createElement('header');
    header.className = 'browser-use-demo-header';
    const title = document.createElement('div');
    title.className = 'browser-use-logo';
    title.innerHTML = DEFAULT_LOGO_SVG;
    const inlineSvg = title.querySelector('svg');
    if (inlineSvg) {
      inlineSvg.setAttribute('role', 'img');
      inlineSvg.setAttribute('aria-label', 'Browser-use');
    }
    const actions = document.createElement('div');
    actions.className = 'browser-use-header-actions';
    const closeBtn = document.createElement('button');
    closeBtn.type = 'button';
    closeBtn.className = 'browser-use-close-btn';
    closeBtn.setAttribute(EXCLUDE_ATTR, 'true');
    closeBtn.setAttribute('aria-label', 'Hide demo panel');
    closeBtn.dataset.role = 'close-toggle';
    closeBtn.innerHTML = '&times;';
    actions.appendChild(closeBtn);
    header.appendChild(title);
    header.appendChild(actions);

    const body = document.createElement('div');
    body.className = 'browser-use-demo-body';
    body.setAttribute('data-role', 'log-list');

    panel.appendChild(header);
    panel.appendChild(buildFinalResultSection());
    panel.appendChild(body);
    panel.setAttribute('data-open', 'true');
    return panel;
  }

  function buildFinalResultSection() {
    const section = document.createElement('section');
    section.className = 'browser-use-final-result';
    section.setAttribute('data-role', 'final-result');
    section.setAttribute('aria-live', 'polite');
    section.dataset.visible = 'false';

    const label = document.createElement('div');
    label.className = 'browser-use-final-result-label';
    label.textContent = 'Final Result';

    const body = document.createElement('div');
    body.className = 'browser-use-final-result-body';
    body.setAttribute('data-role', 'final-result-content');

    section.appendChild(label);
    section.appendChild(body);
    return section;
  }

  function updateFinalResultDisplay(value, animate = true) {
    if (!state.finalResultSection || !state.finalResultContent) {
      return;
    }
    const finalValue = typeof value === 'string' ? value.trim() : '';
    const hasValue = Boolean(finalValue);
    state.finalResultSection.dataset.visible = hasValue ? 'true' : 'false';
    if (!hasValue) {
      state.finalResultContent.textContent = '';
      state.finalResultSection.classList.remove('is-revealed');
      return;
    }
    state.finalResultContent.textContent = finalValue;
    if (animate) {
      state.finalResultSection.classList.remove('is-revealed');
      void state.finalResultSection.offsetWidth;
      state.finalResultSection.classList.add('is-revealed');
    } else {
      state.finalResultSection.classList.remove('is-revealed');
    }
  }

  function buildToggleButton() {
    const button = document.createElement('button');
    button.id = TOGGLE_BUTTON_ID;
    button.type = 'button';
    button.setAttribute(EXCLUDE_ATTR, 'true');
    button.setAttribute('aria-label', 'Open demo panel');
    const img = document.createElement('img');
    img.alt = 'Browser-use';
    img.loading = 'eager';
    img.src = 'data:image/svg+xml;charset=utf-8,' + encodeURIComponent(DEFAULT_LOGO_SVG);
    button.appendChild(img);
    
    // Restore saved position
    const savedPos = loadTogglePosition();
    if (savedPos) {
      button.style.left = savedPos.x + 'px';
      button.style.top = savedPos.y + 'px';
      button.style.right = 'auto';
      button.style.bottom = 'auto';
    }
    
    // Drag functionality
    let isDragging = false;
    let dragStartX = 0;
    let dragStartY = 0;
    let initialX = 0;
    let initialY = 0;
    
    button.addEventListener('mousedown', (e) => {
      if (e.button !== 0) return; // Only left mouse button
      isDragging = false;
      const rect = button.getBoundingClientRect();
      dragStartX = e.clientX;
      dragStartY = e.clientY;
      initialX = rect.left;
      initialY = rect.top;
      
      const handleMouseMove = (moveEvent) => {
        const deltaX = moveEvent.clientX - dragStartX;
        const deltaY = moveEvent.clientY - dragStartY;
        const distance = Math.sqrt(deltaX * deltaX + deltaY * deltaY);
        
        if (distance > 5) {
          isDragging = true;
        }
        
        if (isDragging) {
          const newX = initialX + deltaX;
          const newY = initialY + deltaY;
          
          // Constrain to viewport
          const maxX = window.innerWidth - rect.width;
          const maxY = window.innerHeight - rect.height;
          const constrainedX = Math.max(0, Math.min(newX, maxX));
          const constrainedY = Math.max(0, Math.min(newY, maxY));
          
          button.style.left = constrainedX + 'px';
          button.style.top = constrainedY + 'px';
          button.style.right = 'auto';
          button.style.bottom = 'auto';
        }
      };
      
      const handleMouseUp = () => {
        if (isDragging) {
          const rect = button.getBoundingClientRect();
          saveTogglePosition(rect.left, rect.top);
        }
        document.removeEventListener('mousemove', handleMouseMove);
        document.removeEventListener('mouseup', handleMouseUp);
      };
      
      document.addEventListener('mousemove', handleMouseMove);
      document.addEventListener('mouseup', handleMouseUp);
    });
    
    // Click handler (only if not dragging)
    button.addEventListener('click', (e) => {
      if (!isDragging) {
        openPanel(true);
      }
      isDragging = false;
    });
    
    return button;
  }

  function attachCloseHandler() {
    const closeBtn = state.panel?.querySelector('[data-role="close-toggle"]');
    if (closeBtn) {
      closeBtn.addEventListener('click', () => closePanel(true));
    }
  }

  function openPanel(saveState = true) {
    state.isOpen = true;
    if (state.panel) {
      state.panel.setAttribute('data-open', 'true');
    }
    if (state.toggleButton) {
      state.toggleButton.style.display = 'none';
    }
    adjustLayout();
    if (saveState) {
      persistPanelState();
    }
  }

  function closePanel(saveState = true) {
    state.isOpen = false;
    if (state.panel) {
      state.panel.setAttribute('data-open', 'false');
    }
    document.body.style.marginRight = '';
    if (state.toggleButton) {
      state.toggleButton.style.display = 'flex';
    }
    if (saveState) {
      persistPanelState();
    }
  }

  function persistPanelState() {
    try {
      sessionStorage.setItem(PANEL_STATE_KEY, state.isOpen ? 'open' : 'closed');
    } catch (err) {
      // Ignore storage errors
    }
  }

  function loadPanelState() {
    try {
      const stored = sessionStorage.getItem(PANEL_STATE_KEY);
      if (!stored) return true;
      return stored === 'open';
    } catch (err) {
      return false;
    }
  }

  function adjustLayout() {
    const width = computePanelWidth();
    document.documentElement.style.setProperty('--browser-use-demo-panel-width', `${width}px`);
    if (state.isOpen) {
      document.body.style.marginRight = `${width + 16}px`;
      if (state.toggleButton) {
        state.toggleButton.style.display = 'none';
      }
    } else {
      document.body.style.marginRight = '';
      if (state.toggleButton) {
        state.toggleButton.style.display = 'flex';
      }
    }
  }

  function computePanelWidth() {
    const viewport = Math.max(window.innerWidth, 320);
    const maxAvailable = Math.max(220, viewport - 240);
    const target = Math.min(380, Math.max(260, viewport * 0.3));
    const width = Math.max(220, Math.min(target, maxAvailable));
    try {
      sessionStorage.setItem('__browserUsePanelWidth__', String(width));
    } catch {
      // fallthrough
    }
    return width;
  }

  function loadPanelWidth() {
    try {
      const saved = sessionStorage.getItem('__browserUsePanelWidth__');
      return saved ? Number(saved) : null;
    } catch {
      return null;
    }
  }

  function saveTogglePosition(x, y) {
    try {
      localStorage.setItem(TOGGLE_POSITION_KEY, JSON.stringify({ x, y }));
    } catch {
      // Ignore storage errors
    }
  }

  function loadTogglePosition() {
    try {
      const saved = localStorage.getItem(TOGGLE_POSITION_KEY);
      if (!saved) return null;
      const parsed = JSON.parse(saved);
      if (typeof parsed.x === 'number' && typeof parsed.y === 'number') {
        return { x: parsed.x, y: parsed.y };
      }
    } catch {
      // Ignore parse errors
    }
    return null;
  }

  function persistFinalResult(value) {
    try {
      if (value) {
        sessionStorage.setItem(FINAL_RESULT_KEY, value);
      } else {
        sessionStorage.removeItem(FINAL_RESULT_KEY);
      }
    } catch {
      // ignore storage errors
    }
  }

  function loadFinalResult() {
    try {
      return sessionStorage.getItem(FINAL_RESULT_KEY) || '';
    } catch {
      return '';
    }
  }

  function restoreMessages() {
    try {
      const raw = sessionStorage.getItem(STORAGE_KEY);
      if (!raw) return [];
      const parsed = JSON.parse(raw);
      return Array.isArray(parsed) ? parsed : [];
    } catch (err) {
      return [];
    }
  }

  function persistMessages() {
    try {
      sessionStorage.setItem(STORAGE_KEY, JSON.stringify(state.messages.slice(-MAX_MESSAGES)));
      if (state.list) {
        sessionStorage.setItem(STORAGE_HTML_KEY, state.list.innerHTML);
      }
    } catch (err) {
      // Ignore sessionStorage errors (private mode, etc.)
    }
  }

  function hydrateFromStoredMarkup() {
    if (!state.list) return false;
    try {
      const html = sessionStorage.getItem(STORAGE_HTML_KEY);
      if (html) {
        state.list.innerHTML = html;
        for (const entryNode of state.list.querySelectorAll('.browser-use-demo-entry')) {
          const toggle = entryNode.querySelector('.browser-use-entry-toggle');
          if (toggle) {
            toggle.addEventListener('click', () =>
              toggleEntryExpansion(entryNode, toggle, entryNode.getAttribute('data-id'))
            );
          }
          applyPersistedExpansion(entryNode);
        }
        state.list.scrollTop = state.list.scrollHeight;
        return true;
      }
    } catch (err) {
      // ignore hydration failures
    }
    return false;
  }

  function findFinalResultInMessages(messages) {
    if (!Array.isArray(messages)) {
      return '';
    }
    for (let index = messages.length - 1; index >= 0; index--) {
      const entry = normalizeEntry(messages[index]);
      if (!entry) {
        continue;
      }
      const result = extractFinalResult(entry);
      if (result) {
        return result;
      }
    }
    return '';
  }

  function normalizeEntry(detail) {
    if (!detail) return null;
    const entry = typeof detail === 'string' ? { message: detail } : { ...detail };
    entry.message = typeof entry.message === 'string' ? entry.message : JSON.stringify(entry.message ?? '');
    entry.level = (entry.level || 'info').toLowerCase();
    if (!LEVEL_LABELS[entry.level]) {
      entry.level = 'info';
    }

    if (!entry.metadata || typeof entry.metadata !== 'object') {
      entry.metadata = {};
    }

    entry.timestamp = entry.timestamp || new Date().toISOString();
    entry.id = entry.id || `${Date.now()}-${Math.random().toString(16).slice(2)}`;
    return entry;
  }

  function extractFinalResult(entry) {
    if (!entry || typeof entry.message !== 'string') {
      return '';
    }
    const metadata = entry.metadata || {};
    const messageText = entry.message.trim();
    const hasMetadataFlag = metadata.final === true || metadata.tag === 'final-result';
    if (!hasMetadataFlag && !/^final result\b/i.test(messageText)) {
      return '';
    }
    const cleaned = messageText.replace(/^final result\s*:?\s*/i, '').trim();
    return cleaned || messageText;
  }

  function appendEntry(entry, shouldPersist = true) {
    if (shouldPersist) {
      state.messages.push(entry);
      if (state.messages.length > MAX_MESSAGES) {
        state.messages = state.messages.slice(-MAX_MESSAGES);
      }
      persistMessages();
    }

    if (!state.list) {
      return;
    }

    const node = createEntryNode(entry);
    applyPersistedExpansion(node);
    state.list.appendChild(node);
    highlightLatestEntry(node);
    state.list.scrollTop = state.list.scrollHeight;
    handlePotentialFinalResult(entry);
  }

  function createEntryNode(entry) {
    const row = document.createElement('article');
    row.className = `browser-use-demo-entry level-${entry.level}`;
    row.setAttribute('data-id', entry.id);

    const content = document.createElement('div');
    content.className = 'browser-use-entry-content';

    const meta = document.createElement('div');
    meta.className = 'browser-use-entry-meta';
    const time = formatTime(entry.timestamp);
    const label = LEVEL_LABELS[entry.level] || entry.level;
    meta.innerHTML = `<span>${time}</span><span>${label}</span>`;

    const messageWrapper = document.createElement('div');
    messageWrapper.className = 'browser-use-entry-message';
    const messageText = entry.message.trim();
    const messageHtml = messageText;
    const message = document.createElement('div');
    message.className = 'browser-use-markdown-content';
    message.textContent = messageHtml;
    if (messageText.length > 280) {
      message.classList.add('is-scrollable');
    }
    messageWrapper.appendChild(message);

    if (messageText.length > 160) {
      const toggle = document.createElement('button');
      toggle.type = 'button';
      toggle.className = 'browser-use-entry-toggle';
      toggle.setAttribute(EXCLUDE_ATTR, 'true');
      toggle.textContent = 'Expand';
      toggle.addEventListener('click', () => toggleEntryExpansion(row, toggle, entry.id));
      messageWrapper.appendChild(toggle);
    } else {
      row.classList.add('expanded');
    }

    content.appendChild(meta);
    content.appendChild(messageWrapper);
    row.appendChild(content);
    return row;
  }

  function addCornerHighlights(row) {
    if (!row) return;
    removeCornerHighlights(row);
    ['top-left', 'top-right', 'bottom-left', 'bottom-right'].forEach((position) => {
      const corner = document.createElement('span');
      corner.className = `browser-use-corner corner-${position}`;
      corner.setAttribute(EXCLUDE_ATTR, 'true');
      row.appendChild(corner);
    });
  }

  function removeCornerHighlights(row) {
    if (!row) return;
    row.querySelectorAll('.browser-use-corner').forEach((corner) => corner.remove());
  }

  function highlightLatestEntry(latestNode) {
    if (!state.list) return;
    state.list.querySelectorAll('.browser-use-demo-entry.highlighted').forEach((entry) => {
      entry.classList.remove('highlighted');
      removeCornerHighlights(entry);
    });
    const target = latestNode || state.list.lastElementChild;
    if (!target) return;
    target.classList.add('highlighted');
    addCornerHighlights(target);
  }

  function applyPersistedExpansion(node) {
    if (!node) return;
    try {
      const expanded = new Set(JSON.parse(sessionStorage.getItem(EXPANDED_IDS_KEY) || '[]'));
      const id = node.getAttribute('data-id');
      if (id && expanded.has(id)) {
        node.classList.add('expanded');
        const toggle = node.querySelector('.browser-use-entry-toggle');
        if (toggle) {
          toggle.textContent = 'Collapse';
        }
      }
    } catch {
      // ignore
    }
  }

  function toggleEntryExpansion(row, toggle, entryId) {
    if (!row) return;
    const isExpanded = row.classList.toggle('expanded');
    if (toggle) {
      toggle.textContent = isExpanded ? 'Collapse' : 'Expand';
    }
    try {
      const expanded = new Set(JSON.parse(sessionStorage.getItem(EXPANDED_IDS_KEY) || '[]'));
      if (isExpanded) {
        expanded.add(entryId);
      } else {
        expanded.delete(entryId);
      }
      sessionStorage.setItem(EXPANDED_IDS_KEY, JSON.stringify(Array.from(expanded)));
    } catch {
      // ignore persistence issues
    }
  }

  function handlePotentialFinalResult(entry) {
    const resultText = extractFinalResult(entry);
    if (!resultText) {
      return;
    }
    state.finalResult = resultText;
    persistFinalResult(resultText);
    updateFinalResultDisplay(resultText);
  }

  function formatTime(value) {
    const date = new Date(value);
    if (Number.isNaN(date.getTime())) {
      return new Date().toLocaleTimeString();
    }
    return date.toLocaleTimeString([], { hour: '2-digit', minute: '2-digit', second: '2-digit' });
  }

  function debounce(fn, delay) {
    let frame;
    return (...args) => {
      cancelAnimationFrame(frame);
      frame = requestAnimationFrame(() => fn.apply(null, args));
    };
  }

  function handleLogEvent(event) {
    const entry = normalizeEntry(event?.detail);
    if (!entry) return;
    appendEntry(entry, true);
  }

  const boot = () => {
    if (window.__browserUseDemoPanelBootstrapped) {
      return;
    }

    const start = () => {
      if (window.__browserUseDemoPanelBootstrapped) {
        return;
      }
      if (!document.body) {
        requestAnimationFrame(start);
        return;
      }
      window.__browserUseDemoPanelBootstrapped = true;
      initializePanel();
    };

    start();
  };

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', boot, { once: true });
  } else {
    boot();
  }
  window.addEventListener('browser-use-log', handleLogEvent);
})();
"""
	script = script.replace('__BROWSER_USE_SESSION_ID_PLACEHOLDER__', session_id)
	script = script.replace("const ACCENT_COLOR = '#fe750e';", f"const ACCENT_COLOR = '{accent_color}';")
	return script


def get_last_panel_script(session_id: str, accent_color: str = '#fe750e') -> str:
	"""Generate JavaScript for the compact draggable display mode."""
	script = r"""(function () {
	  const SESSION_ID = '__BROWSER_USE_SESSION_ID__';
	  const EXCLUDE_ATTR = 'data-browser-use-exclude-' + SESSION_ID;
	  const BOX_ID = 'browser-use-demo-last-box';
	  const STYLE_ID = 'browser-use-demo-last-style';
	const ACCENT_COLOR = '__BROWSER_USE_ACCENT__';
	const STORAGE_KEY = '__browserUseDemoLastState__' + SESSION_ID;
	const POSITION_KEY = '__browserUseDemoLastPosition__' + SESSION_ID;
	const THOUGHT_ID = 'browser-use-last-thought';
	const MEMORY_ID = 'browser-use-last-memory';
	const FINAL_ID = 'browser-use-last-final';
	const PLACEHOLDER_ID = 'browser-use-last-placeholder';
	const LOGO_HTML = `
	  <div class="browser-use-last-logo" ${EXCLUDE_ATTR}="true">
	      <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="none" viewBox="0 0 1000 1000"><path fill="#fff" d="M585.941 28.655C672.468-.3 755.454-7.585 825.373 10.74l1.022.272.605.144.641.19c19.554 5.302 37.541 12.489 53.914 21.402l.672.343.537.32c23.35 12.898 43.372 29.338 59.918 48.857l.691.755.872 1.108a213 213 0 0 1 5.969 7.545l.091.09.05.06.409.582a219 219 0 0 1 13.063 19.595l.355.469h-.077a217 217 0 0 1 3.518 6.192l.113.213.055.101.345.592c.282.517.537 1.036.814 1.555l.223.408a234 234 0 0 1 3.591 7.094l.427.859.077.19a227 227 0 0 1 3.232 7.029l.168.332.173.442a251 251 0 0 1 9.154 24.407l.037.035.032.117.122.453a277 277 0 0 1 6.105 22.763l.095.286.018.056.61 2.895.077.448q.83 3.96 1.559 7.986l.095.419h.096v.619a318 318 0 0 1 3.459 25.541l.009.063a352 352 0 0 1 1.491 26.558l.018.179.005.056.041.44-.041.536c.059 3.969.063 7.973 0 12.009v.773l-.005.427c-.173 9.126-.686 18.422-1.504 27.874v.213c-1.187 13.544-3.055 27.412-5.591 41.568l-.309 1.849-.414 2.106c-5.773 30.846-14.759 63.039-27.064 96.242l-.477 1.39-.327.936-.432-.75a.46.46 0 0 1-.145-.245l-8.828-15.26-9.054-15.393-9.387-15.049-.245-.385-9.627-14.811-.064-.096-10.118-15.101-6.037-8.599-4.372-6.338-.132-.179-10.741-14.616-6.341-8.455-4.686-6.345-.114-.155.032-.177c0-.004 0 .005 0 0l1.354-7.983v-.29l1.06-7.584.836-6.28.077-.541.041-.214.105-.528.75-7.52.454-7.382.45-7.083.15-7.095.005-.063v-.381l.15-6.329-.141-4.696-.009-.408v-1.533l-.305-6.492-.418-5.006-.036-.414v-.764l-.6-6.156-.75-5.733-.905-5.725-.782-4.821-.122-.749-.596-2.091-.614-3.212-1.2-4.943-.018-.061-1.241-4.14h-.113v-.382l-.005-.07v-.235l-1.486-4.606-1.509-4.53-1.196-2.839-.459-1.367-1.8-4.053-1.782-3.856-.027-.058-.041-.081-1.913-3.683-1.955-3.603-2.1-3.454-2.259-3.314-2.255-3.157-4.672-6.028-5.119-5.417-3.754-3.451-3.896-3.45-4.218-3.313-3.909-2.696-.445-.309-3.459-2.231-1.21-.78-2.85-1.499-2.127-1.214-5.259-2.704-5.554-2.402-5.878-2.261-4.659-1.352-1.65-.601-6.645-1.812-6.923-1.656-7.377-1.505-.523-.082-7.168-1.125-8.141-.905-8.577-.603-8.873-.3h-9.318l-9.564.149-9.95.754-4.9.433-5.368.475-10.395 1.203-10.864 1.659-11.018 2.115-11.314 2.414-.05.011-11.491 2.685-.077.02-11.768 3.167-11.914 3.466-.441.141-11.777 3.782-12.232 4.38-9.354 3.62-2.9 1.104-.269.104-12.522 5.127-12.66 5.426-12.854 5.897-12.673 6.261-.141.074-.095.05-12.891 6.587-12.986 6.949-12.982 7.245-12.977 7.695-9.055 5.585-4.086 2.42-3.428 2.191-9.545 6.106-2.55 1.8-10.273 6.796-.248.175-8.358 5.867-4.385 3.022-12.829 9.206-12.197 9.147-.481.361-6.037 4.677-6.643 5.135-9.422 7.646-.941.763-.05.041-2.121 1.667-12.366 10.402-12.226 10.716-12.02 10.97-.057.053-11.932 11.023-11.62 11.469-.15.148-11.476 11.478-11.164 11.763-5.587 6.044-5.479 5.773-.111.119-10.712 12.216-10.556 12.214-.119.179-10.31 12.368-1.808 2.259-8.306 10.269-3.769 4.973-5.337 6.978-.553.726-9.509 12.672-9.207 12.832-8.909 12.836-8.599 12.971-8.018 12.536-.286.445-.078.127-7.733 12.79-.038.064-4.53 7.701-3.172 5.289-.029.05-5.551 10.204-1.658 2.708-.126.246-6.516 12.59-.034.063-.052.05-.094.096-6.459 12.912-1.055 2.267-4.983 10.564-5.706 12.762-3.2 7.465-2.262 5.275-4.797 12.44-.028.078-4.656 12.312.001.219.001.458h-.135l-4.123 11.777-.901 2.999-2.869 9.069-1.658 5.874-1.814 6.052-2.111 8.138-.904 3.471-2.286 9.855-.129.559-.303 1.208-2.262 11.159-1.96 11-.01.077-1.498 10.641-.027.241-.728 6.692-.443 3.394-.009.068-.748 10.032-.002.046.002.454-.608 9.391-.002 1.481.002 1.509v.454l-.152 6.052.152 6.32v2.731l.451 8.705.105.959.046.4.603 7.101.754 5.27.295 2.663.008.059 1.351 7.674 1.508 7.383 1.61 6.734.053.209 1.948 6.451 1.556 4.38.089.255.017.045.412 1.377.041.136 2.261 5.725L126.55 824l2.555 5.261 2.712 4.521.064.109v.286l1.029 1.545 1.906 2.858 3.168 4.375 3.292 4.04 3.467 3.771 3.606 3.757.17.155 3.555 3.271.035.032 3.916 3.312 4.205 3.158 3.801 2.485.141.091.375.281.202.15 4.8 2.854 1.288.681 3.834 2.031 5.276 2.563 5.686 2.39 4.544 1.708 1.508.563 5.803 1.799.508.155 6.792 1.813 7.07 1.504 7.532 1.354 3.13.45.05.004 4.681.754 6.777.618 1.486.132 4.16.295 4.435.309 9.054.3 9.348-.15 9.797-.45 9.956-.604 10.395-1.054 10.561-1.508 10.824-1.804.236-.046.162.123.258.195.345.259 1.693 1.268.092.068.037.027.16.137 3.643 2.749.281.19.097.064.039.027.479.359h.305v.232l.899.677h-.049l6.421 4.848.096.072.174.128 14.619 10.59 5.65 3.994 7.333 5.062 1.96 1.508 10.567 6.943 4.529 3.171.587.377 14.665 9.437.286.177 4.087 2.54 10.867 6.788.084.05 15.315 9.009.35.2 1.126.646 13.674 7.919a.45.45 0 0 1 .356.082h2.283l-1.541.549-.917.332-4.23 1.508-.072.028-2.341.822a639 639 0 0 1-9.93 3.494l-3.059 1.095-.074.023-11.116 3.607-4.073 1.354-8.207 2.44h.435l-3.425.891-2.642.686a599 599 0 0 1-11.746 3.194l-.719.195-.911.227a514 514 0 0 1-8.024 2.008l-2.606.655h.449l-3.981.881-.05.014-2.65.586-1.426.318-10.729 2.267-3.5.627a500 500 0 0 1-8.442 1.559l-2.869.536-1.455.204a482 482 0 0 1-10.892 1.69l-2.313.373-1.811.2c-2.585.345-5.159.672-7.723.972l-4.825.641-.277.018-2.974.213q-4.341.429-8.64.768l-2.327.214-4.538.3-2.75.131q-3.054.172-6.085.296l-.541.027h-.116l-.067.005c-4.394.167-8.75.267-13.066.287l-.204.01h-.71c-3.748.01-7.465-.03-11.153-.13l-2.047-.02-1.422-.09a335 335 0 0 1-6.562-.289l-5.317-.227-2.857-.3-1.66-.15a299 299 0 0 1-4.522-.404l-3.938-.355-3.058-.304-9.784-1.354-.055-.005-2.982-.45-.052-.009-9.362-1.658-3.03-.604-9.217-1.968-2.888-.758-8.91-2.268-1.047-.313-1.992-.595-.05-.018-2.909-.868h.153l-5.737-1.813-.101-.032-2.589-1.009a249 249 0 0 1-4.897-1.753l-3.602-1.286-.087-.027-2.718-1.059-8.162-3.326-2.587-1.368-7.852-3.621-2.092-1.226a229 229 0 0 1-3.892-2.063l-4.154-2.158-2.588-1.522-7.253-4.38-3.769-2.631a236 236 0 0 1-3.54-2.413l-2.064-1.308-8.944-6.815-.6-.455-3.58-3.035q-.964-.811-1.919-1.631l-2.526-2.14-1.101-1.045a214 214 0 0 1-4.673-4.33l-2.394-2.19-2.215-2.358a209 209 0 0 1-8.774-9.482l-2.938-3.294-1.224-1.527-6.8-8.764-.046-.077-.146-.286-.543-.777a216 216 0 0 1-4.077-5.821l-1.559-2.23-.128-.182-1.077-1.686-.1-.159-.342-.536-3.034-4.898-.045-.086-.127-.255-.117-.236-.29-.481-.447-.559-.043-.05-.025-.064-.248-.622q-.09-.151-.18-.309l-.131-.196-.436-.781-.575-1-.663-1.104-.344-.686q-.577-1.029-1.143-2.072l-.625-1.013-1.095-2.053-.09-.169-.036-.068-.182-.363-.128-.25-.902-1.813-.023-.045-.305-.614-.322-.718a258 258 0 0 1-1.675-3.421l-.12-.241-.098-.195-.026-.055-.328-.654-1.024-2.194-.281-.423-.031-.05-.436-.654h.167L23.785 865l-.984-2.294q-.21-.484-.418-.968l-1.472-3.239-.603-1.659-.637-1.535-.126-.296-.251-.609-1.108-3.166-.164-.459a255 255 0 0 1-1.275-3.512l-.666-1.595-.768-2.299-1.362-4.385-1.506-4.525-.024-.068v-.318l-.47-1.758a272 272 0 0 1-1.603-5.789l-.628-2.203-.305-1.223-.314-1.467a284 284 0 0 1-1.955-8.51l-.445-1.481-.012-.055-.516-3.194a299 299 0 0 1-1.475-8.083l-.117-.468-.14-.558h.057l-.216-1.513q-.518-3.237-.971-6.507l-.55-3.389-.155-1.84-.108-1.095a333 333 0 0 1-1.368-14.203l-.035-.423-.021-.331a349 349 0 0 1-.4-6.252l-.032-.514-.001-.136a361 361 0 0 1-.442-12.526l-.011-.177v-.35c-.272-16.243.516-32.913 2.328-49.924l.241-2.644.221-1.5q.67-5.778 1.495-11.604l.007-.059a482 482 0 0 1 2.073-13.276l.133-1.063.886-4.712q.515-2.822 1.065-5.652l.015-.086a543 543 0 0 1 7.405-32.636c3.818-14.739 8.309-29.66 13.442-44.649l.126-.399.19-.609.095-.296.454-1.204a681 681 0 0 1 10.567-28.369l.366-.945.078-.173q1.072-2.686 2.169-5.366C85.014 443.52 154.799 339.427 246.82 247.415l4.322-4.297q2.311-2.283 4.633-4.548h-.096l.671-.56c50.277-49.001 103.949-91.419 158.66-126.292h-1.118l2.317-.765a901 901 0 0 1 14.8-9.198l.2-.122a882 882 0 0 1 15.303-9.093l.045-.027c32.543-18.892 65.288-35.085 97.752-48.369a678 678 0 0 1 41.632-15.49m86.295 146.339a.45.45 0 0 1 .528-.087l.077.05 4.386 3.566c185.218 151.444 311.755 364.181 322.091 542.24l.223 4.194c1.149 24.021.127 46.775-2.941 68.134l-.036.241c-1.746 12.094-4.15 23.739-7.187 34.912l.146.031-.296.596-.027.063-.032.059c-5.318 19.337-12.541 37.261-21.559 53.654l-.145.327-.023.046-.027.041-.128.163a218 218 0 0 1-2.941 5.157l-.363.654-.105.255-.168.2-.059.095h.25l-.7.713a212 212 0 0 1-3.905 6.248l-.622.986-3.328 4.993-1.054 1.404a215 215 0 0 1-4.314 5.87l-1.286 1.799-2.5 3.044a205 205 0 0 1-3.677 4.426l-1.091 1.313-.605.654a216 216 0 0 1-5.75 6.316l-1.209 1.349-3.8 3.798-4.227 4.08-2.027 1.795a213 213 0 0 1-10.037 8.569l-.65.532-4.536 3.63-4.323 3.026a214 214 0 0 1-7.923 5.388l-1.677 1.118-4.991 3.176-4.854 2.735-2.146 1.159q-1.44.798-2.9 1.576l-.25.137-.15.077a247 247 0 0 1-5.591 2.871l-4.691 2.345-5.154 2.271-5.591 2.422-5.455 2.117-.545.2a236 236 0 0 1-5.096 1.877l-.104.041-.1.031a255 255 0 0 1-10.014 3.331l-1.222.418-2.346.659a266 266 0 0 1-6.141 1.731l-3.318.94-2.545.568c-2.16.532-4.337 1.045-6.532 1.527l-3.173.777-2.018.345-.782.132a300 300 0 0 1-8.123 1.504l-1.482.29-.05.01-2.018.286q-4.656.737-9.409 1.345l-1.232.172-.05.005-2.168.241c-1.073.127-2.15.245-3.232.363l-1.359.15-6.2.604-2.068.128q-3.803.298-7.654.508l-3.578.273h-.104l-3.937.072q-4.001.135-8.05.18l-1.518.05-1.791-.02q-5.345.015-10.786-.12l-1.186-.01-5.096-.152h-.054l-8.86-.45h-.045l-4.023-.313a514 514 0 0 1-3.632-.273l-6.559-.477-4.572-.554a489 489 0 0 1-7.15-.814l-2.641-.295-3.723-.518c-.577-.077-1.155-.163-1.732-.241l-1.05-.145-.295-.045a516 516 0 0 1-6.078-.9l-1.74-.259-.055-.005-6.645-1.058-8.164-1.513-3.555-.727a597 597 0 0 1-5.8-1.177l-5.604-1.118-4.605-1.126a549 549 0 0 1-8.868-2.113l-1.645-.391-2.514-.654a587 587 0 0 1-5.314-1.386l-7.286-1.885-7.109-2.118-8.305-2.417-7.041-2.244-.072-.023-.237-.077-2.527-.809h.059l-5.6-1.836-3.732-1.349a681 681 0 0 1-6.891-2.413l-4.94-1.676-7.11-2.726-8.459-3.167-6.345-2.644-2.796-1.132-6.427-2.571-7.254-3.176-8.459-3.626-4.596-2.199a753 753 0 0 1-5.736-2.649l-5.382-2.403-7.268-3.63-8.45-4.076-7.264-3.78-8.454-4.38-7.255-3.93-8.209-4.475-.109-.06-.059-.036-1.491-.809h.068l-5.812-3.325-.109-.059-.1-.059-1.374-.787-6.831-3.989-7.144-4.166-.112-.064-.078-.045-1.362-.795-6.872-4.148-7.137-4.462-.115-.068-.387-.246-.947-.59-2.742-1.731-.042-.028-.037-.036-.109-.109-.182-.109-.528-.318-1.1-.686-.119-.073-1.661-1.058-.234-.16-.228-.149-1.813-1.209-2.56-1.658-.122-.078-1.163-.754-.775-.541c-.606-.395-1.211-.795-1.817-1.19l-3.068-1.963-4.085-2.726-.193-.182-2.002-1.354a617 617 0 0 1-3.456-2.339l-4.205-2.845-.118-.077-.166-.113-1.054-.714-4.066-2.799-.333-.222-.128-.086-1.23-.818h.099l-3.513-2.549a969 969 0 0 1-8.648-6.197l-1.495-.996-4.7-3.489-7.252-5.434-3.162-2.263-4.398-3.484-7.847-6.039-1.343-1.104a951 951 0 0 1-6.351-5.025l-.466-.368-8.461-6.797-.755-.604-1.007-.804-6.852-5.693-6.798-5.743-1.964-1.659-4.449-3.921a901 901 0 0 1-5.375-4.689l-2.558-2.108-4.952-4.552a1066 1066 0 0 1-3.364-3.022l-5.894-5.275-.773-.772-2.731-2.553a1045 1045 0 0 1-14.523-13.713l-1.313-1.24-1.704-1.677a1023 1023 0 0 1-16.366-16.247l-3.844-3.844-6.645-6.942-6.359-6.657-.137-.14-3.019-3.172-9.51-10.413-.451-.455-4.35-4.939a972 972 0 0 1-6.839-7.728l-1.514-1.695-5.592-6.497-.511-.604a1009 1009 0 0 1-5.003-5.889l-1.286-1.517-.634-.745-1.932-2.276-9.08-11.014-.164-.2-.304-.454-.114-.173.054-.195 1.812-6.797 1.058-4.23.755-2.722 2.118-7.101 2.225-6.974.043-.137.052-.163 1.87-5.739.193-.595.265-.795.037-.109 2.416-7.251 2.722-7.561 3.016-7.537.145-.573.019-.063 2.869-6.947 2.113-5.28 1.061-2.426 3.323-7.706.906-2.108 2.725-5.757.025-.054 2.704-5.943.295-.659.404-.804.2-.4.127-.25 5.494-10.99 2.233-4.467 4.234-8.014.344-.65.424.6.037.05 8.831 12.426.04.055 9.055 12.371 5.134 6.648 4.247 5.561.134.177 9.658 12.222 9.966 12.231 10.108 12.067 6.048 6.838 3.615 4.035.765.913 10.559 11.768 10.866 11.617 4.075 4.226 6.645 6.797.053.064.277.418 11.288 11.136 3.17 3.021 8.304 8.001.093.086 11.444 10.55.086.078 11.776 10.568 1.051.795h-.142l11.01 9.614 11.956 9.864.123.1 12.233 9.814 12.252 9.414.133.1.086.068 12.271 9.278 7.401 5.134 5.133 3.775 12.534 8.606 12.532 8.455 12.668 7.992 12.682 7.851 3.009 1.654 1.541.85h-.114l8.241 4.889 11.632 6.492 1.059.609 12.664 6.783 8.609 4.38 3.873 2.086.059.032 12.518 6.029 12.373 5.738 12.372 5.43.137.054 12.209 4.912 12.114 4.693 2.409.877 9.65 3.494 11.918 3.925 11.623 3.472 11.468 3.166.072.019.532.136 10.705 2.712 11.177 2.417 10.814 2.104 10.6 1.663 8.759 1.209 1.673.304 9.927 1.054 9.818.754.382.019 9.118.436 9.114.15h.068l8.913-.155h.06l8.363-.45 8.287-.754 7.69-1.054 4.187-.681 3.191-.523 7.086-1.504 6.641-1.663 6.464-1.953 4.709-1.573 1.172-.39 5.705-2.249 5.436-2.417 5.1-2.549 4.819-2.713 4.504-2.849 2.268-1.622 1.959-1.394.705-.568.532-.427h-.068l1.086-.814 1.786-1.336 3.764-3.316 3.618-3.462 3.464-3.612 3.309-3.762 3.154-4.058 3.009-4.216 2.846-4.494 2.718-4.679 2.546-5.089 2.259-5.271 2.259-5.724 2.113-6.039 1.805-6.315 1.804-6.615.246-1.268 1.114-5.825.754-4.516.6-3.021.905-7.829.004-.045.75-8.092.6-8.437.155-8.737-.15-9.205-.305-9.492-.718-8.946-.036-.418v-.304l-.314-3.017-.741-7.074-.009-.06-.441-2.953-.9-7.188-.009-.059-.023-.145-1.786-10.559-2.109-10.864-2.409-10.99-.455-1.672-2.418-9.669-.055-.204-3.09-11.322-.028-.096-3.463-11.599-3.323-9.973-.6-1.949-4.268-11.632-.118-.313-4.523-12.209-1.65-3.762-3.309-8.41-.023-.059-5.427-12.362-5.732-12.527-5.768-11.822h-.05l-.1-.313-.141-.423-6.309-12.472-1.359-2.267-5.441-10.427-2.868-4.98-4.228-7.697-.309-.518-7.241-12.013-.145-.236-6.064-9.814-.132-.218-.427-.69-.927-1.718-8.127-12.644-.119-.118-.054-.069-8.305-12.376-8.6-12.526-4.077-5.584-4.932-6.88-.054-.071-6.187-8.299-3.018-4.07-9.659-12.38-9.682-12.061-.05-.059-.082-.105-9.959-12.075-.032-.033-3.454-3.885-.146-.161-.045-.057-.614-.689-5.927-7.04-.055-.064-.049-.058-10.51-11.71-9.063-9.815-1.673-1.977-11.005-11.457-1.049-1.052-10.119-10.264-11.259-10.961-.063-.06-7.55-7.249-3.919-3.765-11.195-9.999-.145-.134-.451-.447-11.913-10.556-.064-.056-11.854-10.053-.159-.131-12.041-9.807-.037-.029-12.218-9.655-12.386-9.362-.123-.094-12.259-9.117-10.705-7.389-1.777-1.333-.041-.028-.6-.42.646-.351 7.854-4.228 8.014-4.082 4.327-2.164h-.345l2.072-.864 1.814-.756 7.85-3.771 7.709-3.478 7.705-3.474 7.677-3.162 7.586-3.186 7.409-2.871 5.014-1.843h-.382l2.746-.871 7.441-2.578 7.259-2.269 7.1-2.265 7.113-1.968 6.946-1.964 6.504-1.663.055-.014zm105.978 377.474 4.231 8.001 4.078 8.01 3.782 7.861 3.59 7.787.096.091.782.772h-.396l3.182 6.911 3.327 7.715 3.318 7.701 2.914 7.429h.046l.077.309 2.863 7.378 2.723 7.561.041.131 2.527 7.274 2.419 7.252.454 1.358 1.759 5.575.05.164 1.968 7.115.605 1.958 1.364 4.998 1.59 6.665a.44.44 0 0 1 .119.486l-.018.046a1 1 0 0 1-.05.082c-.091.109-.178.222-.269.331l-1.909 2.449-4.586 5.489a1023 1023 0 0 1-27.155 31.536l-1.195 1.354-.391.404a1027 1027 0 0 1-22.182 23.844l-2.059 2.19-1.591 1.591a1035 1035 0 0 1-50.45 48.615l-.023.054-.068.059-2.177 1.927a992 992 0 0 1-8.359 7.397l-1.241 1.09-.791.722h.136l-1.004.786-.127.119-.769.586a951 951 0 0 1-8.9 7.628l-2.445 2.099-.805.645c-4.804 4.031-9.64 8.02-14.509 11.954l-.081.078-.182.163-.241-.063-.2-.055-6.546-1.74-.045-.014-6.95-1.813-7.105-1.967-7.113-2.267-4.837-1.613-2.413-.804-7.4-2.417-7.418-2.726-7.555-2.872-7.482-3.139-.082-.032-3.995-1.631-.086-.032-3.45-1.504-7.641-3.294-.091-.036-7.859-3.631-.059-.027-7.255-3.403-.118-.055-.259-.172-.2-.137-7.823-3.757-7.868-4.085-7.855-4.08-.241-.122v-.427l-.004-.232.191-.136 14.709-10.505.868-.622.282-.2 15.4-11.441.45-.336.041-.027 15.65-12.19 15.55-12.686 15.25-13.13 15.086-13.426 3.473-3.322 11.177-10.572 2.391-2.39.677-.682.096-.091.132-.131 10.745-10.455.45-.441 8.618-8.891.741-.764.127-.14 4.705-4.844 1.804-1.958 6.491-7.092 5.446-5.748 13.423-15.08.754-.904 6.5-7.556 5.764-6.651.563-.736.128-.168 6.068-7.533 6.041-7.252.277-.418 6.064-7.728 5.463-7.006.423-.546 11.768-15.843 11.327-16.006.428-.6zM183.482 8.61c56.203-12.83 122.13-10.008 193.358 8.84l1.423.34h-.14c17.617 4.705 35.557 10.386 53.752 17.054l3.58 1.323.081.04a.44.44 0 0 1 .161.18l.519.188-.617.349a.45.45 0 0 1-.174.098l-.063.038-12.8 7.276-2.675 1.524-15.323 9.162-8.699 5.288-11.53 7.24-5.888 3.778-6.201 3.984-6.351 4.256-6.167 4.136-11.443 7.98-.116.119-.059.048-8.539 6.034-1.584 1.148-.325.234-7.82 5.665-1.508 1.11-2.718 2.053h.063l-1.102.799-.124.107-.474.329-2.563 1.936v.127l-.247.125-.256.126-.685.518-.576.463h.239l-1.219.813-.112.094-.252.15-.784.523-2.482 1.873-.572.509-.157.139-.07-.01-.173.087-.123.063-.136-.018-.701-.087-.508-.063-6.796-1.207-3.857-.531-3.842-.527-7.702-.904-7.515-.753-7.236-.602-3.54-.223-3.734-.233-6.93-.151h-3.928l-2.841-.15-1.326.147-5.315.003-6.482.3-6.344.457-6.022.599-5.87.752-5.586.907-5.41.901-4.681 1.068-.603.139-4.977 1.208-4.731 1.328-.085.024-2.378.691-2.301.669-4.342 1.495-4.211 1.654-4.077 1.814-3.92 1.809-3.752 1.949-3.613 1.957-3.454 2.104-3.312 2.259-3.117 2.224-2.606 2.083-3.453 2.764-5.394 5.093-5.123 5.422-4.826 6.033-2.251 3.151-2.243 3.283-1.048 1.954-.861 1.427-.052.091-2.106 3.61-1.953 3.756-.632 1.472-.258.604-.024.052-.897 1.792-1.654 4.059-1.652 4.208-.352 1.054-.099.302-1.212 3.178-1.352 4.653-.07.258-.985 3.52-.303 1.054-1.356 4.97-.945 4.726-.089.454-.018.083-1.028 5.423-.029.162-.903 5.728-.077.768-.07.715-.041.408-.567 3.842-.6 6.146.001 1.945-.304 4.248-.3 6.493-.151 6.635v6.776l.151 7.094.426 6.658.027.416.521 6.421.081.97.605 6.04.141 1.423.009.087.904 7.528 1.206 7.845 1.359 8.005.032.189-.114.154-1.661 2.265-9.381 12.356-.131.175-10.723 14.799-10.403 14.922-.107.216-8.06 11.881-2.071 2.958-.042.065-.119.181-6.068 9.474-3.619 5.427-4.531 7.4-4.98 7.846-.045.081-2.82 4.898-6.141 10.328-.052.09-1.357 2.415-5.135 9.062-2.188 3.789a.45.45 0 0 1-.14.245l-.414.718-.34-.886-1.51-3.928-3.173-9.063-.907-2.417-3.478-10.435-.958-3.164a613 613 0 0 1-1.268-4.001l-1.854-5.674-2.12-7.415-1.509-5.438-2.72-9.666-.757-3.033-2.868-12.08-.153-.604-3.023-14.06-.01-.047v-.31l-.094-.493a510 510 0 0 1-1.846-9.777l-.615-3.251-.302-1.208-1.97-12.574-.14-2.049a465 465 0 0 1-.942-7.254l-.128-.977-.4-3.079-.056-.406-.005-.053-.06-.447-.387-3.88-.714-7.145-.045-.408-.106-1.1-.044-.431-.044-.476-.2-4.37a418 418 0 0 1-.462-7.661l-.04-.607-.801-.802h.632l-.141-6.632-.152-5.739v-2.331q-.044-4.108 0-8.173v-4.319l.174-3.721q.098-3.05.245-6.074l.034-.915-.001-1.963.455-6.225.268-3.745.03-.437.005-.047.151-1.522.604-5.742.603-4.967v-.611l.712-4.155q.172-1.223.355-2.441l.387-3.182.061-.517.15-1.056.76-3.801.905-4.678.604-3.171.303-1.367.907-4.229.785-3.53.122-.564.456-1.811.608-2.129.272-.971q.553-2.12 1.138-4.22l.41-1.631.143-.585.082-.321.374-1.188.423-1.379.161-.526.019-.06.098-.32.961-3.008.321-.966q.406-1.261.825-2.515l.511-1.789 1.367-3.951 2.114-5.435.302-.903.892-2.086q1.158-2.896 2.383-5.746l.503-1.238.301-.898.034-.076.657-1.188.203-.449.527-1.314h-.066l.328-.655.1-.251.952-1.851.905-1.962.61-1.322h-.084l.4-.681.97-1.663q.324-.64.652-1.278l.328-.657.125-.251.228-.398q.429-.82.864-1.634l.444-.888.126-.25.63-1.11.605-1.208.033-.065.05-.05.112-.115.062-.097q.564-1.008 1.136-2.011l.115-.229.029-.049.11-.168c.181-.315.367-.626.55-.94l.385-.687.112-.219.243-.368.05-.083h-.105l.443-.668.283-.426.034-.049.134-.271.101-.202v-.197l.19-.002.85-1.509.383-.687.036-.062.089-.157.33-.535 5.032-7.62h-.1l.498-.68.114-.226h.055l5.752-7.82.044-.052.144-.145.6-.756q1.88-2.446 3.83-4.834l1.886-2.38 1.948-2.188a212 212 0 0 1 4.135-4.643l1.333-1.494.873-.875a210 210 0 0 1 5.674-5.777l1.167-1.214 1.85-1.679a212 212 0 0 1 5.452-4.956h-.072l.906-.752.718-.602a212 212 0 0 1 7.003-5.765l.947-.778 1.104-.82a219 219 0 0 1 25.577-16.938l1.466-.859.924-.476q4.356-2.4 8.843-4.623l.367-.196.509-.235a237 237 0 0 1 9.335-4.327l.599-.279.455-.183a248 248 0 0 1 14.16-5.598h-.146l2.35-.785a263 263 0 0 1 13.598-4.349h-.37l3.027-.758a278 278 0 0 1 11.728-3.002l.582-.153z"></path></svg>
	  </div>`;

	  if (window.__browserUseDemoLastBoxLoaded) {
	    return;
	  }
	  window.__browserUseDemoLastBoxLoaded = true;

	  const state = {
	    box: null,
	    lastThought: null,
	    lastMemory: null,
	    lastFinal: null,
	  };

	  function restoreState() {
	    try {
	      const raw = sessionStorage.getItem(STORAGE_KEY);
	      if (!raw) return;
	      const parsed = JSON.parse(raw);
	      if (parsed.lastThought) {
	        state.lastThought = parsed.lastThought;
	      }
	      if (parsed.lastMemory) {
	        state.lastMemory = parsed.lastMemory;
	      }
	      if (parsed.lastFinal) {
	        state.lastFinal = parsed.lastFinal;
	      }
	    } catch (err) {
	      // ignore storage errors
	    }
	  }

	  function persistState() {
	    try {
	      sessionStorage.setItem(STORAGE_KEY, JSON.stringify({
	        lastThought: state.lastThought,
	        lastMemory: state.lastMemory,
	        lastFinal: state.lastFinal,
	      }));
	    } catch (err) {
	      // ignore storage errors
	    }
	  }

	  function savePosition(pos) {
	    try {
	      sessionStorage.setItem(POSITION_KEY, JSON.stringify(pos));
	    } catch (err) {
	      // ignore storage errors
	    }
	  }

	  function loadPosition() {
	    try {
	      const raw = sessionStorage.getItem(POSITION_KEY);
	      if (!raw) return null;
	      const parsed = JSON.parse(raw);
	      if (typeof parsed.x === 'number' && typeof parsed.y === 'number') {
	        return parsed;
	      }
	    } catch (err) {
	      // ignore storage errors
	    }
	    return null;
	  }

	  function addStyles() {
	    if (document.getElementById(STYLE_ID)) return;
	    const style = document.createElement('style');
	    style.id = STYLE_ID;
	    style.setAttribute(EXCLUDE_ATTR, 'true');
	    style.textContent = `
      #${BOX_ID} {
        --browser-use-demo-accent: ${ACCENT_COLOR};
        position: fixed;
        top: auto;
        left: auto;
        bottom: calc(24px + env(safe-area-inset-bottom, 0px));
        right: calc(24px + env(safe-area-inset-right, 0px));
        min-width: 240px;
        min-height: 150px;
        max-width: min(380px, calc(100vw - 48px));
        max-height: 360px;
        overflow: hidden;
        background: rgba(5, 5, 5, 0.94);
        color: #f8f9ff;
        font-family: 'JetBrains Mono', 'Fira Code', 'Monaco', 'Menlo', monospace;
        font-size: 12px;
        line-height: 1.4;
        box-shadow: 0 22px 40px rgba(0, 0, 0, 0.55);
        padding: 18px 20px;
        display: flex;
        flex-direction: column;
        gap: 12px;
        box-sizing: border-box;
        cursor: grab;
        pointer-events: auto;
        z-index: 2147480000;
        transition: box-shadow 0.25s ease;
      }

	      #${BOX_ID}.dragging {
	        cursor: grabbing;
	        box-shadow: 0 12px 25px rgba(0, 0, 0, 0.65);
	      }

	      #${BOX_ID}.highlighted {
	        animation: browser-use-last-pulse 0.85s ease;
	      }

      @keyframes browser-use-last-pulse {
        0% { box-shadow: 0 22px 40px rgba(0, 0, 0, 0.55); }
        50% { box-shadow: 0 22px 48px rgba(255, 117, 14, 0.28); }
        100% { box-shadow: 0 22px 40px rgba(0, 0, 0, 0.55); }
      }

      @keyframes browser-use-last-final-pop {
        0% { opacity: 0; transform: translateY(12px) scale(0.95); }
        60% { opacity: 1; transform: translateY(0) scale(1.02); }
        100% { opacity: 1; transform: translateY(0) scale(1); }
      }

      #${BOX_ID} .browser-use-last-content {
        position: relative;
        z-index: 2;
        font-size: 12px;
        word-break: break-word;
        color: #fefefe;
        padding: 0;
        min-height: 48px;
        max-height: 200px;
        overflow-y: auto;
        background: transparent;
        border: none;
        border-radius: 0;
        box-shadow: none;
      }

      #${BOX_ID} .browser-use-last-content::-webkit-scrollbar {
        width: 6px;
      }

      #${BOX_ID} .browser-use-last-content::-webkit-scrollbar-thumb {
        background: rgba(255, 255, 255, 0.25);
        border-radius: 999px;
      }

      #${BOX_ID} .browser-use-last-empty {
        color: rgba(255, 255, 255, 0.35);
        font-style: italic;
      }

      #${BOX_ID} .browser-use-last-content.browser-use-last-final {
        max-height: none;
        padding: 14px 16px;
        border: 1px solid rgba(255, 117, 14, 0.35);
        border-radius: 14px;
        background: rgba(255, 255, 255, 0.03);
        box-shadow: inset 0 0 0 1px rgba(255, 255, 255, 0.02);
      }

      #${BOX_ID} .browser-use-last-content.browser-use-last-final::-webkit-scrollbar {
        width: 6px;
      }

      #${BOX_ID} .browser-use-last-content.browser-use-last-final::-webkit-scrollbar-thumb {
        background: rgba(255, 255, 255, 0.35);
        border-radius: 999px;
      }

      #${BOX_ID} .browser-use-last-content.browser-use-last-final[data-visible="false"] {
        display: none;
      }

      #${BOX_ID} .browser-use-last-content.browser-use-last-final.is-revealed {
        animation: browser-use-last-final-pop 0.6s ease;
      }

      #${BOX_ID} .browser-use-last-placeholder {
        position: absolute;
        inset: 18px 20px;
        border-radius: 18px;
        display: flex;
        align-items: center;
        justify-content: center;
        opacity: 0.55;
        pointer-events: none;
        transition: opacity 0.35s ease, transform 0.35s ease;
        z-index: 1;
        background: rgba(0, 0, 0, 0.35);
        backdrop-filter: blur(2px);
      }

      #${BOX_ID} .browser-use-last-placeholder[data-visible="false"] {
        opacity: 0;
        transform: scale(0.9);
        visibility: hidden;
      }

      #${BOX_ID} .browser-use-last-logo {
        display: flex;
        align-items: center;
        justify-content: center;
        width: 100%;
        height: 100%;
        opacity: 0.85;
      }

      #${BOX_ID} .browser-use-last-logo svg {
        width: 70px;
        height: 70px;
      }

	      .browser-use-corner {
	        position: absolute;
	        width: 26px;
	        height: 26px;
	        border: 2px solid var(--browser-use-demo-accent);
	        opacity: 0.45;
	        pointer-events: none;
	        z-index: 4;
	      }

	      .browser-use-corner.corner-top-left {
	        top: 6px;
	        left: 6px;
	        border-right: none;
	        border-bottom: none;
	      }

	      .browser-use-corner.corner-top-right {
	        top: 6px;
	        right: 6px;
	        border-left: none;
	        border-bottom: none;
	      }

	      .browser-use-corner.corner-bottom-left {
	        bottom: 6px;
	        left: 6px;
	        border-right: none;
	        border-top: none;
	      }

	      .browser-use-corner.corner-bottom-right {
	        bottom: 6px;
	        right: 6px;
	        border-left: none;
	        border-top: none;
	      }
	    `;
	    document.head.appendChild(style);
	  }

  function createContent(id) {
    const node = document.createElement('div');
    node.id = id;
    node.className = 'browser-use-last-content browser-use-last-empty';
    node.setAttribute(EXCLUDE_ATTR, 'true');
    return node;
  }

	  function buildBox() {
	    const box = document.createElement('div');
	    box.id = BOX_ID;
	    box.setAttribute(EXCLUDE_ATTR, 'true');
	    box.classList.add('highlighted');

    const thought = createContent(THOUGHT_ID);
    const memory = createContent(MEMORY_ID);
    const finalResult = createContent(FINAL_ID);
	    finalResult.classList.add('browser-use-last-final');
	    finalResult.dataset.visible = 'false';

	    box.appendChild(thought);
	    box.appendChild(memory);
	    box.appendChild(finalResult);

	    const placeholder = document.createElement('div');
	    placeholder.id = PLACEHOLDER_ID;
	    placeholder.className = 'browser-use-last-placeholder';
	    placeholder.innerHTML = LOGO_HTML;
	    placeholder.dataset.visible = 'true';
	    placeholder.setAttribute(EXCLUDE_ATTR, 'true');
	    box.appendChild(placeholder);

	    ['top-left', 'top-right', 'bottom-left', 'bottom-right'].forEach((position) => {
	      const corner = document.createElement('span');
	      corner.className = 'browser-use-corner corner-' + position;
	      corner.setAttribute(EXCLUDE_ATTR, 'true');
	      box.appendChild(corner);
	    });

	    return box;
	  }

	  function applyStoredPosition(node) {
	    const stored = loadPosition();
	    if (!stored) return;
	    node.style.right = 'auto';
	    node.style.bottom = 'auto';
	    node.style.left = stored.x + 'px';
	    node.style.top = stored.y + 'px';
	  }

	  function initDrag(node) {
	    let dragData = null;

	    function onPointerDown(event) {
	      if (event.button !== undefined && event.button !== 0) return;
	      const rect = node.getBoundingClientRect();
	      dragData = {
	        startX: event.clientX,
	        startY: event.clientY,
	        origX: rect.left + window.scrollX,
	        origY: rect.top + window.scrollY,
	      };
	      node.classList.add('dragging');
	      node.setPointerCapture && node.setPointerCapture(event.pointerId);
	      window.addEventListener('pointermove', onPointerMove);
	      window.addEventListener('pointerup', onPointerUp);
	    }

	    function onPointerMove(event) {
	      if (!dragData) return;
	      const dx = event.clientX - dragData.startX;
	      const dy = event.clientY - dragData.startY;
	      let nextX = dragData.origX + dx;
	      let nextY = dragData.origY + dy;
	      const maxX = window.scrollX + window.innerWidth - node.offsetWidth - 12;
	      const maxY = window.scrollY + window.innerHeight - node.offsetHeight - 12;
	      nextX = Math.min(Math.max(window.scrollX + 8, nextX), Math.max(window.scrollX + 8, maxX));
	      nextY = Math.min(Math.max(window.scrollY + 8, nextY), Math.max(window.scrollY + 8, maxY));
	      node.style.right = 'auto';
	      node.style.bottom = 'auto';
	      node.style.left = nextX + 'px';
	      node.style.top = nextY + 'px';
	      savePosition({ x: nextX, y: nextY });
	    }

	    function onPointerUp(event) {
	      if (dragData && node.releasePointerCapture) {
	        node.releasePointerCapture(event.pointerId);
	      }
	      dragData = null;
	      node.classList.remove('dragging');
	      window.removeEventListener('pointermove', onPointerMove);
	      window.removeEventListener('pointerup', onPointerUp);
	    }

	    node.addEventListener('pointerdown', onPointerDown);
	  }

	  function ensureBox() {
	    if (!document.body) return;
	    addStyles();
	    let node = document.getElementById(BOX_ID);
	    if (!node) {
	      node = buildBox();
	      document.body.appendChild(node);
	      initDrag(node);
	    }
	    state.box = node;
	    applyStoredPosition(node);
	    updateDisplay();
	  }

	  function updateDisplay() {
	    const thoughtEl = document.getElementById(THOUGHT_ID);
	    const memoryEl = document.getElementById(MEMORY_ID);
	    const finalEl = document.getElementById(FINAL_ID);

    const hasFinal = Boolean(state.lastFinal);

	    if (thoughtEl) {
	      const showThought = Boolean(state.lastThought) && !hasFinal;
	      if (showThought) {
	        thoughtEl.textContent = state.lastThought;
	        thoughtEl.classList.remove('browser-use-last-empty');
	      } else {
	        thoughtEl.textContent = '';
	        thoughtEl.classList.add('browser-use-last-empty');
	      }
	      thoughtEl.style.display = showThought ? '' : 'none';
	    }

	    if (memoryEl) {
	      const showMemory = Boolean(state.lastMemory) && !hasFinal;
	      if (showMemory) {
	        memoryEl.textContent = state.lastMemory;
	        memoryEl.classList.remove('browser-use-last-empty');
	      } else {
	        memoryEl.textContent = '';
	        memoryEl.classList.add('browser-use-last-empty');
	      }
	      memoryEl.style.display = showMemory ? '' : 'none';
	    }

    if (finalEl) {
      const wasVisible = finalEl.dataset.visible === 'true';
      finalEl.dataset.visible = hasFinal ? 'true' : 'false';
      finalEl.style.display = hasFinal ? '' : 'none';
      if (hasFinal) {
        finalEl.textContent = state.lastFinal;
        finalEl.classList.remove('browser-use-last-empty');
        if (!wasVisible) {
          finalEl.classList.remove('is-revealed');
          void finalEl.offsetWidth;
          finalEl.classList.add('is-revealed');
        }
      } else {
        finalEl.textContent = '';
        finalEl.classList.add('browser-use-last-empty');
        finalEl.classList.remove('is-revealed');
      }
    }

	    updatePlaceholderVisibility();

	    if (state.box) {
	      state.box.classList.remove('highlighted');
	      void state.box.offsetWidth;
	      state.box.classList.add('highlighted');
	    }
	  }

	  function updatePlaceholderVisibility() {
	    const placeholder = document.getElementById(PLACEHOLDER_ID);
	    if (!placeholder) return;
	    const hasContent = Boolean(state.lastThought || state.lastMemory || state.lastFinal);
	    placeholder.dataset.visible = hasContent ? 'false' : 'true';
	  }

	  function normalizeEntry(detail) {
	    if (!detail) return null;
	    const entry = typeof detail === 'string' ? { message: detail } : { ...detail };
	    entry.message = typeof entry.message === 'string' ? entry.message : JSON.stringify(entry.message ?? '');
	    entry.level = (entry.level || 'info').toLowerCase();
	    if (!entry.metadata || typeof entry.metadata !== 'object') {
	      entry.metadata = {};
	    }
	    return entry;
	  }

	  function extractFinalResult(entry) {
	    if (!entry || typeof entry.message !== 'string') {
	      return '';
	    }
	    const metadata = entry.metadata || {};
	    const text = entry.message.trim();
	    const hasFlag = metadata.final === true || metadata.tag === 'final-result';
	    if (!hasFlag && !/^final result\b/i.test(text)) {
	      return '';
	    }
	    const cleaned = text.replace(/^final result\s*:?\s*/i, '').trim();
	    return cleaned || text;
	  }

	  function handleLogEvent(event) {
	    const entry = normalizeEntry(event && event.detail);
	    if (!entry || !entry.message) return;

	    let updated = false;
	    if (entry.level === 'thought') {
	      state.lastThought = entry.message;
	      updated = true;
	    }
	    const messageText = entry.message.trim();
	    if (entry.level === 'info' && messageText.toLowerCase().startsWith('memory:')) {
	      state.lastMemory = messageText.replace(/^memory:\s*/i, '');
	      updated = true;
	    }
	    const finalResult = extractFinalResult(entry);
	    if (finalResult) {
	      state.lastFinal = finalResult;
	      updated = true;
	    }

	    if (updated) {
	      persistState();
	      updateDisplay();
	    }
	  }

	  restoreState();
	  if (document.readyState === 'loading') {
	    document.addEventListener('DOMContentLoaded', ensureBox, { once: true });
	  } else {
	    ensureBox();
	  }

	  window.addEventListener('browser-use-log', handleLogEvent);
	})();
"""
	script = script.replace('__BROWSER_USE_SESSION_ID__', session_id)
	script = script.replace('__BROWSER_USE_ACCENT__', accent_color)
	return script

```

---

## backend/browser-use/browser_use/browser/events.py

```py
"""Event definitions for browser communication."""

import inspect
import os
from typing import Any, Literal

from bubus import BaseEvent
from bubus.models import T_EventResultType
from cdp_use.cdp.target import TargetID
from pydantic import BaseModel, Field, field_validator

from browser_use.browser.views import BrowserStateSummary
from browser_use.dom.views import EnhancedDOMTreeNode


def _get_timeout(env_var: str, default: float) -> float | None:
	"""
	Safely parse environment variable timeout values with robust error handling.

	Args:
		env_var: Environment variable name (e.g. 'TIMEOUT_NavigateToUrlEvent')
		default: Default timeout value as float (e.g. 15.0)

	Returns:
		Parsed float value or the default if parsing fails

	Raises:
		ValueError: Only if both env_var and default are invalid (should not happen with valid defaults)
	"""
	# Try environment variable first
	env_value = os.getenv(env_var)
	if env_value:
		try:
			parsed = float(env_value)
			if parsed < 0:
				print(f'Warning: {env_var}={env_value} is negative, using default {default}')
				return default
			return parsed
		except (ValueError, TypeError):
			print(f'Warning: {env_var}={env_value} is not a valid number, using default {default}')

	# Fall back to default
	return default


# ============================================================================
# Agent/Tools -> BrowserSession Events (High-level browser actions)
# ============================================================================


class ElementSelectedEvent(BaseEvent[T_EventResultType]):
	"""An element was selected."""

	node: EnhancedDOMTreeNode

	@field_validator('node', mode='before')
	@classmethod
	def serialize_node(cls, data: EnhancedDOMTreeNode | None) -> EnhancedDOMTreeNode | None:
		if data is None:
			return None
		return EnhancedDOMTreeNode(
			node_id=data.node_id,
			backend_node_id=data.backend_node_id,
			session_id=data.session_id,
			frame_id=data.frame_id,
			target_id=data.target_id,
			node_type=data.node_type,
			node_name=data.node_name,
			node_value=data.node_value,
			attributes=data.attributes,
			is_scrollable=data.is_scrollable,
			is_visible=data.is_visible,
			absolute_position=data.absolute_position,
			# override the circular reference fields in EnhancedDOMTreeNode as they cant be serialized and aren't needed by event handlers
			# only used internally by the DOM service during DOM tree building process, not intended public API use
			content_document=None,
			shadow_root_type=None,
			shadow_roots=[],
			parent_node=None,
			children_nodes=[],
			ax_node=None,
			snapshot_node=None,
		)


# TODO: add page handle to events
# class PageHandle(share a base with browser.session.CDPSession?):
# 	url: str
# 	target_id: TargetID
#   @classmethod
#   def from_target_id(cls, target_id: TargetID) -> Self:
#     return cls(target_id=target_id)
#   @classmethod
#   def from_target_id(cls, target_id: TargetID) -> Self:
#     return cls(target_id=target_id)
#   @classmethod
#   def from_url(cls, url: str) -> Self:
#   @property
#   def root_frame_id(self) -> str:
#     return self.target_id
#   @property
#   def session_id(self) -> str:
#     return browser_session.get_or_create_cdp_session(self.target_id).session_id

# class PageSelectedEvent(BaseEvent[T_EventResultType]):
# 	"""An event like SwitchToTabEvent(page=PageHandle) or CloseTabEvent(page=PageHandle)"""
# 	page: PageHandle


class NavigateToUrlEvent(BaseEvent[None]):
	"""Navigate to a specific URL."""

	url: str
	wait_until: Literal['load', 'domcontentloaded', 'networkidle', 'commit'] = 'load'
	timeout_ms: int | None = None
	new_tab: bool = Field(
		default=False, description='Set True to leave the current tab alone and open a new tab in the foreground for the new URL'
	)
	# existing_tab: PageHandle | None = None  # TODO

	# time limits enforced by bubus, not exposed to LLM:
	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_NavigateToUrlEvent', 15.0))  # seconds


class ClickElementEvent(ElementSelectedEvent[dict[str, Any] | None]):
	"""Click an element."""

	node: 'EnhancedDOMTreeNode'
	button: Literal['left', 'right', 'middle'] = 'left'
	# click_count: int = 1           # TODO
	# expect_download: bool = False  # moved to downloads_watchdog.py

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_ClickElementEvent', 15.0))  # seconds


class ClickCoordinateEvent(BaseEvent[dict]):
	"""Click at specific coordinates."""

	coordinate_x: int
	coordinate_y: int
	button: Literal['left', 'right', 'middle'] = 'left'
	force: bool = False  # If True, skip safety checks (file input, print, select)

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_ClickCoordinateEvent', 15.0))  # seconds


class TypeTextEvent(ElementSelectedEvent[dict | None]):
	"""Type text into an element."""

	node: 'EnhancedDOMTreeNode'
	text: str
	clear: bool = True
	is_sensitive: bool = False  # Flag to indicate if text contains sensitive data
	sensitive_key_name: str | None = None  # Name of the sensitive key being typed (e.g., 'username', 'password')

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_TypeTextEvent', 60.0))  # seconds


class ScrollEvent(ElementSelectedEvent[None]):
	"""Scroll the page or element."""

	direction: Literal['up', 'down', 'left', 'right']
	amount: int  # pixels
	node: 'EnhancedDOMTreeNode | None' = None  # None means scroll page

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_ScrollEvent', 8.0))  # seconds


class SwitchTabEvent(BaseEvent[TargetID]):
	"""Switch to a different tab."""

	target_id: TargetID | None = Field(default=None, description='None means switch to the most recently opened tab')

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_SwitchTabEvent', 10.0))  # seconds


class CloseTabEvent(BaseEvent[None]):
	"""Close a tab."""

	target_id: TargetID

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_CloseTabEvent', 10.0))  # seconds


class ScreenshotEvent(BaseEvent[str]):
	"""Request to take a screenshot."""

	full_page: bool = False
	clip: dict[str, float] | None = None  # {x, y, width, height}

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_ScreenshotEvent', 15.0))  # seconds


class BrowserStateRequestEvent(BaseEvent[BrowserStateSummary]):
	"""Request current browser state."""

	include_dom: bool = True
	include_screenshot: bool = True
	include_recent_events: bool = False

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_BrowserStateRequestEvent', 30.0))  # seconds


# class WaitForConditionEvent(BaseEvent):
# 	"""Wait for a condition."""

# 	condition: Literal['navigation', 'selector', 'timeout', 'load_state']
# 	timeout: float = 30000
# 	selector: str | None = None
# 	state: Literal['attached', 'detached', 'visible', 'hidden'] | None = None


class GoBackEvent(BaseEvent[None]):
	"""Navigate back in browser history."""

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_GoBackEvent', 15.0))  # seconds


class GoForwardEvent(BaseEvent[None]):
	"""Navigate forward in browser history."""

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_GoForwardEvent', 15.0))  # seconds


class RefreshEvent(BaseEvent[None]):
	"""Refresh/reload the current page."""

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_RefreshEvent', 15.0))  # seconds


class WaitEvent(BaseEvent[None]):
	"""Wait for a specified number of seconds."""

	seconds: float = 3.0
	max_seconds: float = 10.0  # Safety cap

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_WaitEvent', 60.0))  # seconds


class SendKeysEvent(BaseEvent[None]):
	"""Send keyboard keys/shortcuts."""

	keys: str  # e.g., "ctrl+a", "cmd+c", "Enter"

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_SendKeysEvent', 60.0))  # seconds


class UploadFileEvent(ElementSelectedEvent[None]):
	"""Upload a file to an element."""

	node: 'EnhancedDOMTreeNode'
	file_path: str

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_UploadFileEvent', 30.0))  # seconds


class GetDropdownOptionsEvent(ElementSelectedEvent[dict[str, str]]):
	"""Get all options from any dropdown (native <select>, ARIA menus, or custom dropdowns).

	Returns a dict containing dropdown type, options list, and element metadata."""

	node: 'EnhancedDOMTreeNode'

	event_timeout: float | None = Field(
		default_factory=lambda: _get_timeout('TIMEOUT_GetDropdownOptionsEvent', 15.0)
	)  # some dropdowns lazy-load the list of options on first interaction, so we need to wait for them to load (e.g. table filter lists can have thousands of options)


class SelectDropdownOptionEvent(ElementSelectedEvent[dict[str, str]]):
	"""Select a dropdown option by exact text from any dropdown type.

	Returns a dict containing success status and selection details."""

	node: 'EnhancedDOMTreeNode'
	text: str  # The option text to select

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_SelectDropdownOptionEvent', 8.0))  # seconds


class ScrollToTextEvent(BaseEvent[None]):
	"""Scroll to specific text on the page. Raises exception if text not found."""

	text: str
	direction: Literal['up', 'down'] = 'down'

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_ScrollToTextEvent', 15.0))  # seconds


# ============================================================================


class BrowserStartEvent(BaseEvent):
	"""Start/connect to browser."""

	cdp_url: str | None = None
	launch_options: dict[str, Any] = Field(default_factory=dict)

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_BrowserStartEvent', 30.0))  # seconds


class BrowserStopEvent(BaseEvent):
	"""Stop/disconnect from browser."""

	force: bool = False

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_BrowserStopEvent', 45.0))  # seconds


class BrowserLaunchResult(BaseModel):
	"""Result of launching a browser."""

	# TODO: add browser executable_path, pid, version, latency, user_data_dir, X11 $DISPLAY, host IP address, etc.
	cdp_url: str


class BrowserLaunchEvent(BaseEvent[BrowserLaunchResult]):
	"""Launch a local browser process."""

	# TODO: add executable_path, proxy settings, preferences, extra launch args, etc.

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_BrowserLaunchEvent', 30.0))  # seconds


class BrowserKillEvent(BaseEvent):
	"""Kill local browser subprocess."""

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_BrowserKillEvent', 30.0))  # seconds


# TODO: replace all Runtime.evaluate() calls with this event
# class ExecuteJavaScriptEvent(BaseEvent):
# 	"""Execute JavaScript in page context."""

# 	target_id: TargetID
# 	expression: str
# 	await_promise: bool = True

# 	event_timeout: float | None = 60.0  # seconds

# TODO: add this and use the old BrowserProfile.viewport options to set it
# class SetViewportEvent(BaseEvent):
# 	"""Set the viewport size."""

# 	width: int
# 	height: int
# 	device_scale_factor: float = 1.0

# 	event_timeout: float | None = 15.0  # seconds


# Moved to storage state
# class SetCookiesEvent(BaseEvent):
# 	"""Set browser cookies."""

# 	cookies: list[dict[str, Any]]

# 	event_timeout: float | None = (
# 		30.0  # only long to support the edge case of restoring a big localStorage / on many origins (has to O(n) visit each origin to restore)
# 	)


# class GetCookiesEvent(BaseEvent):
# 	"""Get browser cookies."""

# 	urls: list[str] | None = None

# 	event_timeout: float | None = 30.0  # seconds


# ============================================================================
# DOM-related Events
# ============================================================================


class BrowserConnectedEvent(BaseEvent):
	"""Browser has started/connected."""

	cdp_url: str

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_BrowserConnectedEvent', 30.0))  # seconds


class BrowserStoppedEvent(BaseEvent):
	"""Browser has stopped/disconnected."""

	reason: str | None = None

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_BrowserStoppedEvent', 30.0))  # seconds


class TabCreatedEvent(BaseEvent):
	"""A new tab was created."""

	target_id: TargetID
	url: str

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_TabCreatedEvent', 30.0))  # seconds


class TabClosedEvent(BaseEvent):
	"""A tab was closed."""

	target_id: TargetID

	# TODO:
	# new_focus_target_id: int | None = None
	# new_focus_url: str | None = None

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_TabClosedEvent', 10.0))  # seconds


# TODO: emit this when DOM changes significantly, inner frame navigates, form submits, history.pushState(), etc.
# class TabUpdatedEvent(BaseEvent):
# 	"""Tab information updated (URL changed, etc.)."""

# 	target_id: TargetID
# 	url: str


class AgentFocusChangedEvent(BaseEvent):
	"""Agent focus changed to a different tab."""

	target_id: TargetID
	url: str

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_AgentFocusChangedEvent', 10.0))  # seconds


class TargetCrashedEvent(BaseEvent):
	"""A target has crashed."""

	target_id: TargetID
	error: str

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_TargetCrashedEvent', 10.0))  # seconds


class NavigationStartedEvent(BaseEvent):
	"""Navigation started."""

	target_id: TargetID
	url: str

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_NavigationStartedEvent', 30.0))  # seconds


class NavigationCompleteEvent(BaseEvent):
	"""Navigation completed."""

	target_id: TargetID
	url: str
	status: int | None = None
	error_message: str | None = None  # Error/timeout message if navigation had issues
	loading_status: str | None = None  # Detailed loading status (e.g., network timeout info)

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_NavigationCompleteEvent', 30.0))  # seconds


# ============================================================================
# Error Events
# ============================================================================


class BrowserErrorEvent(BaseEvent):
	"""An error occurred in the browser layer."""

	error_type: str
	message: str
	details: dict[str, Any] = Field(default_factory=dict)

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_BrowserErrorEvent', 30.0))  # seconds


# ============================================================================
# Storage State Events
# ============================================================================


class SaveStorageStateEvent(BaseEvent):
	"""Request to save browser storage state."""

	path: str | None = None  # Optional path, uses profile default if not provided

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_SaveStorageStateEvent', 45.0))  # seconds


class StorageStateSavedEvent(BaseEvent):
	"""Notification that storage state was saved."""

	path: str
	cookies_count: int
	origins_count: int

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_StorageStateSavedEvent', 30.0))  # seconds


class LoadStorageStateEvent(BaseEvent):
	"""Request to load browser storage state."""

	path: str | None = None  # Optional path, uses profile default if not provided

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_LoadStorageStateEvent', 45.0))  # seconds


# TODO: refactor this to:
# - on_BrowserConnectedEvent() -> dispatch(LoadStorageStateEvent()) -> _copy_storage_state_from_json_to_browser(json_file, new_cdp_session) + return storage_state from handler
# - on_BrowserStopEvent() -> dispatch(SaveStorageStateEvent()) -> _copy_storage_state_from_browser_to_json(new_cdp_session, json_file)
# and get rid of StorageStateSavedEvent and StorageStateLoadedEvent, have the original events + provide handler return values for any results
class StorageStateLoadedEvent(BaseEvent):
	"""Notification that storage state was loaded."""

	path: str
	cookies_count: int
	origins_count: int

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_StorageStateLoadedEvent', 30.0))  # seconds


# ============================================================================
# File Download Events
# ============================================================================


class FileDownloadedEvent(BaseEvent):
	"""A file has been downloaded."""

	url: str
	path: str
	file_name: str
	file_size: int
	file_type: str | None = None  # e.g., 'pdf', 'zip', 'docx', etc.
	mime_type: str | None = None  # e.g., 'application/pdf'
	from_cache: bool = False
	auto_download: bool = False  # Whether this was an automatic download (e.g., PDF auto-download)

	event_timeout: float | None = Field(default_factory=lambda: _get_timeout('TIMEOUT_FileDownloadedEvent', 30.0))  # seconds


class AboutBlankDVDScreensaverShownEvent(BaseEvent):
	"""AboutBlankWatchdog has shown DVD screensaver animation on an about:blank tab."""

	target_id: TargetID
	error: str | None = None


class DialogOpenedEvent(BaseEvent):
	"""Event dispatched when a JavaScript dialog is opened and handled."""

	dialog_type: str  # 'alert', 'confirm', 'prompt', or 'beforeunload'
	message: str
	url: str
	frame_id: str | None = None  # Can be None when frameId is not provided by CDP
	# target_id: TargetID   # TODO: add this to avoid needing target_id_from_frame() later


# Note: Model rebuilding for forward references is handled in the importing modules
# Events with 'EnhancedDOMTreeNode' forward references (ClickElementEvent, TypeTextEvent,
# ScrollEvent, UploadFileEvent) need model_rebuild() called after imports are complete


def _check_event_names_dont_overlap():
	"""
	check that event names defined in this file are valid and non-overlapping
	(naiively n^2 so it's pretty slow but ok for now, optimize when >20 events)
	"""
	event_names = {
		name.split('[')[0]
		for name in globals().keys()
		if not name.startswith('_')
		and inspect.isclass(globals()[name])
		and issubclass(globals()[name], BaseEvent)
		and name != 'BaseEvent'
	}
	for name_a in event_names:
		assert name_a.endswith('Event'), f'Event with name {name_a} does not end with "Event"'
		for name_b in event_names:
			if name_a != name_b:  # Skip self-comparison
				assert name_a not in name_b, (
					f'Event with name {name_a} is a substring of {name_b}, all events must be completely unique to avoid find-and-replace accidents'
				)


# overlapping event names are a nightmare to trace and rename later, dont do it!
# e.g. prevent ClickEvent and FailedClickEvent are terrible names because one is a substring of the other,
# must be ClickEvent and ClickFailedEvent to preserve the usefulnes of codebase grep/sed/awk as refactoring tools.
# at import time, we do a quick check that all event names defined above are valid and non-overlapping.
# this is hand written in blood by a human! not LLM slop. feel free to optimize but do not remove it without a good reason.
_check_event_names_dont_overlap()

```

---

## backend/browser-use/browser_use/browser/profile.py

```py
import sys
import tempfile
from collections.abc import Iterable
from enum import Enum
from functools import cache
from pathlib import Path
from typing import Annotated, Any, Literal, Self
from urllib.parse import urlparse

from pydantic import AfterValidator, AliasChoices, BaseModel, ConfigDict, Field, field_validator, model_validator

from browser_use.browser.cloud.views import CloudBrowserParams
from browser_use.config import CONFIG
from browser_use.utils import _log_pretty_path, logger

CHROME_DEBUG_PORT = 9242  # use a non-default port to avoid conflicts with other tools / devs using 9222
DOMAIN_OPTIMIZATION_THRESHOLD = 100  # Convert domain lists to sets for O(1) lookup when >= this size
CHROME_DISABLED_COMPONENTS = [
	# Playwright defaults: https://github.com/microsoft/playwright/blob/41008eeddd020e2dee1c540f7c0cdfa337e99637/packages/playwright-core/src/server/chromium/chromiumSwitches.ts#L76
	# AcceptCHFrame,AutoExpandDetailsElement,AvoidUnnecessaryBeforeUnloadCheckSync,CertificateTransparencyComponentUpdater,DeferRendererTasksAfterInput,DestroyProfileOnBrowserClose,DialMediaRouteProvider,ExtensionManifestV2Disabled,GlobalMediaControls,HttpsUpgrades,ImprovedCookieControls,LazyFrameLoading,LensOverlay,MediaRouter,PaintHolding,ThirdPartyStoragePartitioning,Translate
	# See https:#github.com/microsoft/playwright/pull/10380
	'AcceptCHFrame',
	# See https:#github.com/microsoft/playwright/pull/10679
	'AutoExpandDetailsElement',
	# See https:#github.com/microsoft/playwright/issues/14047
	'AvoidUnnecessaryBeforeUnloadCheckSync',
	# See https:#github.com/microsoft/playwright/pull/12992
	'CertificateTransparencyComponentUpdater',
	'DestroyProfileOnBrowserClose',
	# See https:#github.com/microsoft/playwright/pull/13854
	'DialMediaRouteProvider',
	# Chromium is disabling manifest version 2. Allow testing it as long as Chromium can actually run it.
	# Disabled in https:#chromium-review.googlesource.com/c/chromium/src/+/6265903.
	'ExtensionManifestV2Disabled',
	'GlobalMediaControls',
	# See https:#github.com/microsoft/playwright/pull/27605
	'HttpsUpgrades',
	'ImprovedCookieControls',
	'LazyFrameLoading',
	# Hides the Lens feature in the URL address bar. Its not working in unofficial builds.
	'LensOverlay',
	# See https:#github.com/microsoft/playwright/pull/8162
	'MediaRouter',
	# See https:#github.com/microsoft/playwright/issues/28023
	'PaintHolding',
	# See https:#github.com/microsoft/playwright/issues/32230
	'ThirdPartyStoragePartitioning',
	# See https://github.com/microsoft/playwright/issues/16126
	'Translate',
	# 3
	# Added by us:
	'AutomationControlled',
	'BackForwardCache',
	'OptimizationHints',
	'ProcessPerSiteUpToMainFrameThreshold',
	'InterestFeedContentSuggestions',
	'CalculateNativeWinOcclusion',  # chrome normally stops rendering tabs if they are not visible (occluded by a foreground window or other app)
	# 'BackForwardCache',  # agent does actually use back/forward navigation, but we can disable if we ever remove that
	'HeavyAdPrivacyMitigations',
	'PrivacySandboxSettings4',
	'AutofillServerCommunication',
	'CrashReporting',
	'OverscrollHistoryNavigation',
	'InfiniteSessionRestore',
	'ExtensionDisableUnsupportedDeveloper',
	'ExtensionManifestV2Unsupported',
]

CHROME_HEADLESS_ARGS = [
	'--headless=new',
]

CHROME_DOCKER_ARGS = [
	# '--disable-gpu',    # GPU is actually supported in headless docker mode now, but sometimes useful to test without it
	'--no-sandbox',
	'--disable-gpu-sandbox',
	'--disable-setuid-sandbox',
	'--disable-dev-shm-usage',
	'--no-xshm',
	'--no-zygote',
	# '--single-process',  # might be the cause of "Target page, context or browser has been closed" errors during CDP page.captureScreenshot https://stackoverflow.com/questions/51629151/puppeteer-protocol-error-page-navigate-target-closed
	'--disable-site-isolation-trials',  # lowers RAM use by 10-16% in docker, but could lead to easier bot blocking if pages can detect it?
]


CHROME_DISABLE_SECURITY_ARGS = [
	'--disable-site-isolation-trials',
	'--disable-web-security',
	'--disable-features=IsolateOrigins,site-per-process',
	'--allow-running-insecure-content',
	'--ignore-certificate-errors',
	'--ignore-ssl-errors',
	'--ignore-certificate-errors-spki-list',
]

CHROME_DETERMINISTIC_RENDERING_ARGS = [
	'--deterministic-mode',
	'--js-flags=--random-seed=1157259159',
	'--force-device-scale-factor=2',
	'--enable-webgl',
	# '--disable-skia-runtime-opts',
	# '--disable-2d-canvas-clip-aa',
	'--font-render-hinting=none',
	'--force-color-profile=srgb',
]

CHROME_DEFAULT_ARGS = [
	# # provided by playwright by default: https://github.com/microsoft/playwright/blob/41008eeddd020e2dee1c540f7c0cdfa337e99637/packages/playwright-core/src/server/chromium/chromiumSwitches.ts#L76
	'--disable-field-trial-config',  # https://source.chromium.org/chromium/chromium/src/+/main:testing/variations/README.md
	'--disable-background-networking',
	'--disable-background-timer-throttling',  # agents might be working on background pages if the human switches to another tab
	'--disable-backgrounding-occluded-windows',  # same deal, agents are often working on backgrounded browser windows
	'--disable-back-forward-cache',  # Avoids surprises like main request not being intercepted during page.goBack().
	'--disable-breakpad',
	'--disable-client-side-phishing-detection',
	'--disable-component-extensions-with-background-pages',
	'--disable-component-update',  # Avoids unneeded network activity after startup.
	'--no-default-browser-check',
	# '--disable-default-apps',
	'--disable-dev-shm-usage',  # crucial for docker support, harmless in non-docker environments
	# '--disable-extensions',
	# '--disable-features=' + disabledFeatures(assistantMode).join(','),
	# '--allow-pre-commit-input',  # duplicate removed
	'--disable-hang-monitor',
	'--disable-ipc-flooding-protection',  # important to be able to make lots of CDP calls in a tight loop
	'--disable-popup-blocking',
	'--disable-prompt-on-repost',
	'--disable-renderer-backgrounding',
	# '--force-color-profile=srgb',  # moved to CHROME_DETERMINISTIC_RENDERING_ARGS
	'--metrics-recording-only',
	'--no-first-run',
	# // See https://chromium-review.googlesource.com/c/chromium/src/+/2436773
	'--no-service-autorun',
	'--export-tagged-pdf',
	# // https://chromium-review.googlesource.com/c/chromium/src/+/4853540
	'--disable-search-engine-choice-screen',
	# // https://issues.chromium.org/41491762
	'--unsafely-disable-devtools-self-xss-warnings',
	# added by us:
	'--enable-features=NetworkService,NetworkServiceInProcess',
	'--enable-network-information-downlink-max',
	'--test-type=gpu',
	'--disable-sync',
	'--allow-legacy-extension-manifests',
	'--allow-pre-commit-input',
	'--disable-blink-features=AutomationControlled',
	'--install-autogenerated-theme=0,0,0',
	# '--hide-scrollbars',                     # leave them visible! the agent uses them to know when it needs to scroll to see more options
	'--log-level=2',
	# '--enable-logging=stderr',
	'--disable-focus-on-load',
	'--disable-window-activation',
	'--generate-pdf-document-outline',
	'--no-pings',
	'--ash-no-nudges',
	'--disable-infobars',
	'--simulate-outdated-no-au="Tue, 31 Dec 2099 23:59:59 GMT"',
	'--hide-crash-restore-bubble',
	'--suppress-message-center-popups',
	'--disable-domain-reliability',
	'--disable-datasaver-prompt',
	'--disable-speech-synthesis-api',
	'--disable-speech-api',
	'--disable-print-preview',
	'--safebrowsing-disable-auto-update',
	'--disable-external-intent-requests',
	'--disable-desktop-notifications',
	'--noerrdialogs',
	'--silent-debugger-extension-api',
	# Extension welcome tab suppression for automation
	'--disable-extensions-http-throttling',
	'--extensions-on-chrome-urls',
	'--disable-default-apps',
	f'--disable-features={",".join(CHROME_DISABLED_COMPONENTS)}',
]


class ViewportSize(BaseModel):
	width: int = Field(ge=0)
	height: int = Field(ge=0)

	def __getitem__(self, key: str) -> int:
		return dict(self)[key]

	def __setitem__(self, key: str, value: int) -> None:
		setattr(self, key, value)


@cache
def get_display_size() -> ViewportSize | None:
	# macOS
	try:
		from AppKit import NSScreen  # type: ignore[import]

		screen = NSScreen.mainScreen().frame()
		size = ViewportSize(width=int(screen.size.width), height=int(screen.size.height))
		logger.debug(f'Display size: {size}')
		return size
	except Exception:
		pass

	# Windows & Linux
	try:
		from screeninfo import get_monitors

		monitors = get_monitors()
		monitor = monitors[0]
		size = ViewportSize(width=int(monitor.width), height=int(monitor.height))
		logger.debug(f'Display size: {size}')
		return size
	except Exception:
		pass

	logger.debug('No display size found')
	return None


def get_window_adjustments() -> tuple[int, int]:
	"""Returns recommended x, y offsets for window positioning"""

	if sys.platform == 'darwin':  # macOS
		return -4, 24  # macOS has a small title bar, no border
	elif sys.platform == 'win32':  # Windows
		return -8, 0  # Windows has a border on the left
	else:  # Linux
		return 0, 0


def validate_url(url: str, schemes: Iterable[str] = ()) -> str:
	"""Validate URL format and optionally check for specific schemes."""
	parsed_url = urlparse(url)
	if not parsed_url.netloc:
		raise ValueError(f'Invalid URL format: {url}')
	if schemes and parsed_url.scheme and parsed_url.scheme.lower() not in schemes:
		raise ValueError(f'URL has invalid scheme: {url} (expected one of {schemes})')
	return url


def validate_float_range(value: float, min_val: float, max_val: float) -> float:
	"""Validate that float is within specified range."""
	if not min_val <= value <= max_val:
		raise ValueError(f'Value {value} outside of range {min_val}-{max_val}')
	return value


def validate_cli_arg(arg: str) -> str:
	"""Validate that arg is a valid CLI argument."""
	if not arg.startswith('--'):
		raise ValueError(f'Invalid CLI argument: {arg} (should start with --, e.g. --some-key="some value here")')
	return arg


# ===== Enum definitions =====


class RecordHarContent(str, Enum):
	OMIT = 'omit'
	EMBED = 'embed'
	ATTACH = 'attach'


class RecordHarMode(str, Enum):
	FULL = 'full'
	MINIMAL = 'minimal'


class BrowserChannel(str, Enum):
	CHROMIUM = 'chromium'
	CHROME = 'chrome'
	CHROME_BETA = 'chrome-beta'
	CHROME_DEV = 'chrome-dev'
	CHROME_CANARY = 'chrome-canary'
	MSEDGE = 'msedge'
	MSEDGE_BETA = 'msedge-beta'
	MSEDGE_DEV = 'msedge-dev'
	MSEDGE_CANARY = 'msedge-canary'


# Using constants from central location in browser_use.config
BROWSERUSE_DEFAULT_CHANNEL = BrowserChannel.CHROMIUM


# ===== Type definitions with validators =====

UrlStr = Annotated[str, AfterValidator(validate_url)]
NonNegativeFloat = Annotated[float, AfterValidator(lambda x: validate_float_range(x, 0, float('inf')))]
CliArgStr = Annotated[str, AfterValidator(validate_cli_arg)]


# ===== Base Models =====


class BrowserContextArgs(BaseModel):
	"""
	Base model for common browser context parameters used by
	both BrowserType.new_context() and BrowserType.launch_persistent_context().

	https://playwright.dev/python/docs/api/class-browser#browser-new-context
	"""

	model_config = ConfigDict(extra='ignore', validate_assignment=False, revalidate_instances='always', populate_by_name=True)

	# Browser context parameters
	accept_downloads: bool = True

	# Security options
	# proxy: ProxySettings | None = None
	permissions: list[str] = Field(
		default_factory=lambda: ['clipboardReadWrite', 'notifications'],
		description='Browser permissions to grant (CDP Browser.grantPermissions).',
		# clipboardReadWrite is for google sheets and pyperclip automations
		# notifications are to avoid browser fingerprinting
	)
	# client_certificates: list[ClientCertificate] = Field(default_factory=list)
	# http_credentials: HttpCredentials | None = None

	# Viewport options
	user_agent: str | None = None
	screen: ViewportSize | None = None
	viewport: ViewportSize | None = Field(default=None)
	no_viewport: bool | None = None
	device_scale_factor: NonNegativeFloat | None = None
	# geolocation: Geolocation | None = None

	# Recording Options
	record_har_content: RecordHarContent = RecordHarContent.EMBED
	record_har_mode: RecordHarMode = RecordHarMode.FULL
	record_har_path: str | Path | None = Field(default=None, validation_alias=AliasChoices('save_har_path', 'record_har_path'))
	record_video_dir: str | Path | None = Field(
		default=None, validation_alias=AliasChoices('save_recording_path', 'record_video_dir')
	)


class BrowserConnectArgs(BaseModel):
	"""
	Base model for common browser connect parameters used by
	both connect_over_cdp() and connect_over_ws().

	https://playwright.dev/python/docs/api/class-browsertype#browser-type-connect
	https://playwright.dev/python/docs/api/class-browsertype#browser-type-connect-over-cdp
	"""

	model_config = ConfigDict(extra='ignore', validate_assignment=True, revalidate_instances='always', populate_by_name=True)

	headers: dict[str, str] | None = Field(default=None, description='Additional HTTP headers to be sent with connect request')


class BrowserLaunchArgs(BaseModel):
	"""
	Base model for common browser launch parameters used by
	both launch() and launch_persistent_context().

	https://playwright.dev/python/docs/api/class-browsertype#browser-type-launch
	"""

	model_config = ConfigDict(
		extra='ignore',
		validate_assignment=True,
		revalidate_instances='always',
		from_attributes=True,
		validate_by_name=True,
		validate_by_alias=True,
		populate_by_name=True,
	)

	env: dict[str, str | float | bool] | None = Field(
		default=None,
		description='Extra environment variables to set when launching the browser. If None, inherits from the current process.',
	)
	executable_path: str | Path | None = Field(
		default=None,
		validation_alias=AliasChoices('browser_binary_path', 'chrome_binary_path'),
		description='Path to the chromium-based browser executable to use.',
	)
	headless: bool | None = Field(default=None, description='Whether to run the browser in headless or windowed mode.')
	args: list[CliArgStr] = Field(
		default_factory=list, description='List of *extra* CLI args to pass to the browser when launching.'
	)
	ignore_default_args: list[CliArgStr] | Literal[True] = Field(
		default_factory=lambda: [
			'--enable-automation',  # we mask the automation fingerprint via JS and other flags
			'--disable-extensions',  # allow browser extensions
			'--hide-scrollbars',  # always show scrollbars in screenshots so agent knows there is more content below it can scroll down to
			'--disable-features=AcceptCHFrame,AutoExpandDetailsElement,AvoidUnnecessaryBeforeUnloadCheckSync,CertificateTransparencyComponentUpdater,DeferRendererTasksAfterInput,DestroyProfileOnBrowserClose,DialMediaRouteProvider,ExtensionManifestV2Disabled,GlobalMediaControls,HttpsUpgrades,ImprovedCookieControls,LazyFrameLoading,LensOverlay,MediaRouter,PaintHolding,ThirdPartyStoragePartitioning,Translate',
		],
		description='List of default CLI args to stop playwright from applying (see https://github.com/microsoft/playwright/blob/41008eeddd020e2dee1c540f7c0cdfa337e99637/packages/playwright-core/src/server/chromium/chromiumSwitches.ts)',
	)
	channel: BrowserChannel | None = None  # https://playwright.dev/docs/browsers#chromium-headless-shell
	chromium_sandbox: bool = Field(
		default=not CONFIG.IN_DOCKER, description='Whether to enable Chromium sandboxing (recommended unless inside Docker).'
	)
	devtools: bool = Field(
		default=False, description='Whether to open DevTools panel automatically for every page, only works when headless=False.'
	)

	# proxy: ProxySettings | None = Field(default=None, description='Proxy settings to use to connect to the browser.')
	downloads_path: str | Path | None = Field(
		default=None,
		description='Directory to save downloads to.',
		validation_alias=AliasChoices('downloads_dir', 'save_downloads_path'),
	)
	traces_dir: str | Path | None = Field(
		default=None,
		description='Directory for saving playwright trace.zip files (playwright actions, screenshots, DOM snapshots, HAR traces).',
		validation_alias=AliasChoices('trace_path', 'traces_dir'),
	)

	# firefox_user_prefs: dict[str, str | float | bool] = Field(default_factory=dict)

	@model_validator(mode='after')
	def validate_devtools_headless(self) -> Self:
		"""Cannot open devtools when headless is True"""
		assert not (self.headless and self.devtools), 'headless=True and devtools=True cannot both be set at the same time'
		return self

	@model_validator(mode='after')
	def set_default_downloads_path(self) -> Self:
		"""Set a unique default downloads path if none is provided."""
		if self.downloads_path is None:
			import uuid

			# Create unique directory in /tmp for downloads
			unique_id = str(uuid.uuid4())[:8]  # 8 characters
			downloads_path = Path(f'/tmp/browser-use-downloads-{unique_id}')

			# Ensure path doesn't already exist (extremely unlikely but possible)
			while downloads_path.exists():
				unique_id = str(uuid.uuid4())[:8]
				downloads_path = Path(f'/tmp/browser-use-downloads-{unique_id}')

			self.downloads_path = downloads_path
			self.downloads_path.mkdir(parents=True, exist_ok=True)
		return self

	@staticmethod
	def args_as_dict(args: list[str]) -> dict[str, str]:
		"""Return the extra launch CLI args as a dictionary."""
		args_dict = {}
		for arg in args:
			key, value, *_ = [*arg.split('=', 1), '', '', '']
			args_dict[key.strip().lstrip('-')] = value.strip()
		return args_dict

	@staticmethod
	def args_as_list(args: dict[str, str]) -> list[str]:
		"""Return the extra launch CLI args as a list of strings."""
		return [f'--{key.lstrip("-")}={value}' if value else f'--{key.lstrip("-")}' for key, value in args.items()]


# ===== API-specific Models =====


class BrowserNewContextArgs(BrowserContextArgs):
	"""
	Pydantic model for new_context() arguments.
	Extends BaseContextParams with storage_state parameter.

	https://playwright.dev/python/docs/api/class-browser#browser-new-context
	"""

	model_config = ConfigDict(extra='ignore', validate_assignment=False, revalidate_instances='always', populate_by_name=True)

	# storage_state is not supported in launch_persistent_context()
	storage_state: str | Path | dict[str, Any] | None = None
	# TODO: use StorageState type instead of dict[str, Any]

	# to apply this to existing contexts (incl cookies, localStorage, IndexedDB), see:
	# - https://github.com/microsoft/playwright/pull/34591/files
	# - playwright-core/src/server/storageScript.ts restore() function
	# - https://github.com/Skn0tt/playwright/blob/c446bc44bac4fbfdf52439ba434f92192459be4e/packages/playwright-core/src/server/storageScript.ts#L84C1-L123C2

	# @field_validator('storage_state', mode='after')
	# def load_storage_state_from_file(self) -> Self:
	# 	"""Load storage_state from file if it's a path."""
	# 	if isinstance(self.storage_state, (str, Path)):
	# 		storage_state_file = Path(self.storage_state)
	# 		try:
	# 			parsed_storage_state = json.loads(storage_state_file.read_text())
	# 			validated_storage_state = StorageState(**parsed_storage_state)
	# 			self.storage_state = validated_storage_state
	# 		except Exception as e:
	# 			raise ValueError(f'Failed to load storage state file {self.storage_state}: {e}') from e
	# 	return self
	pass


class BrowserLaunchPersistentContextArgs(BrowserLaunchArgs, BrowserContextArgs):
	"""
	Pydantic model for launch_persistent_context() arguments.
	Combines browser launch parameters and context parameters,
	plus adds the user_data_dir parameter.

	https://playwright.dev/python/docs/api/class-browsertype#browser-type-launch-persistent-context
	"""

	model_config = ConfigDict(extra='ignore', validate_assignment=False, revalidate_instances='always')

	# Required parameter specific to launch_persistent_context, but can be None to use incognito temp dir
	user_data_dir: str | Path | None = None

	@field_validator('user_data_dir', mode='after')
	@classmethod
	def validate_user_data_dir(cls, v: str | Path | None) -> str | Path:
		"""Validate user data dir is set to a non-default path."""
		if v is None:
			return tempfile.mkdtemp(prefix='browser-use-user-data-dir-')
		return Path(v).expanduser().resolve()


class ProxySettings(BaseModel):
	"""Typed proxy settings for Chromium traffic.

	- server: Full proxy URL, e.g. "http://host:8080" or "socks5://host:1080"
	- bypass: Comma-separated hosts to bypass (e.g. "localhost,127.0.0.1,*.internal")
	- username/password: Optional credentials for authenticated proxies
	"""

	server: str | None = Field(default=None, description='Proxy URL, e.g. http://host:8080 or socks5://host:1080')
	bypass: str | None = Field(default=None, description='Comma-separated hosts to bypass, e.g. localhost,127.0.0.1,*.internal')
	username: str | None = Field(default=None, description='Proxy auth username')
	password: str | None = Field(default=None, description='Proxy auth password')

	def __getitem__(self, key: str) -> str | None:
		return getattr(self, key)


class BrowserProfile(BrowserConnectArgs, BrowserLaunchPersistentContextArgs, BrowserLaunchArgs, BrowserNewContextArgs):
	"""
	A BrowserProfile is a static template collection of kwargs that can be passed to:
		- BrowserType.launch(**BrowserLaunchArgs)
		- BrowserType.connect(**BrowserConnectArgs)
		- BrowserType.connect_over_cdp(**BrowserConnectArgs)
		- BrowserType.launch_persistent_context(**BrowserLaunchPersistentContextArgs)
		- BrowserContext.new_context(**BrowserNewContextArgs)
		- BrowserSession(**BrowserProfile)
	"""

	model_config = ConfigDict(
		extra='ignore',
		validate_assignment=True,
		revalidate_instances='always',
		from_attributes=True,
		validate_by_name=True,
		validate_by_alias=True,
	)

	# ... extends options defined in:
	# BrowserLaunchPersistentContextArgs, BrowserLaunchArgs, BrowserNewContextArgs, BrowserConnectArgs

	# Session/connection configuration
	cdp_url: str | None = Field(default=None, description='CDP URL for connecting to existing browser instance')
	is_local: bool = Field(default=False, description='Whether this is a local browser instance')
	use_cloud: bool = Field(
		default=False,
		description='Use browser-use cloud browser service instead of local browser',
	)

	@property
	def cloud_browser(self) -> bool:
		"""Alias for use_cloud field for compatibility."""
		return self.use_cloud

	cloud_browser_params: CloudBrowserParams | None = Field(
		default=None, description='Parameters for creating a cloud browser instance'
	)

	# custom options we provide that aren't native playwright kwargs
	disable_security: bool = Field(default=False, description='Disable browser security features.')
	deterministic_rendering: bool = Field(default=False, description='Enable deterministic rendering flags.')
	allowed_domains: list[str] | set[str] | None = Field(
		default=None,
		description='List of allowed domains for navigation e.g. ["*.google.com", "https://example.com", "chrome-extension://*"]. Lists with 100+ items are auto-optimized to sets (no pattern matching).',
	)
	prohibited_domains: list[str] | set[str] | None = Field(
		default=None,
		description='List of prohibited domains for navigation e.g. ["*.google.com", "https://example.com", "chrome-extension://*"]. Allowed domains take precedence over prohibited domains. Lists with 100+ items are auto-optimized to sets (no pattern matching).',
	)
	block_ip_addresses: bool = Field(
		default=False,
		description='Block navigation to URLs containing IP addresses (both IPv4 and IPv6). When True, blocks all IP-based URLs including localhost and private networks.',
	)
	keep_alive: bool | None = Field(default=None, description='Keep browser alive after agent run.')

	# --- Proxy settings ---
	# New consolidated proxy config (typed)
	proxy: ProxySettings | None = Field(
		default=None,
		description='Proxy settings. Use browser_use.browser.profile.ProxySettings(server, bypass, username, password)',
	)
	enable_default_extensions: bool = Field(
		default=True,
		description="Enable automation-optimized extensions: ad blocking (uBlock Origin), cookie handling (I still don't care about cookies), and URL cleaning (ClearURLs). All extensions work automatically without manual intervention. Extensions are automatically downloaded and loaded when enabled.",
	)
	demo_mode: bool = Field(
		default=False,
		description='Enable demo mode side panel that streams agent logs directly inside the browser window (requires headless=False).',
	)
	demo_mode_display: Literal['full', 'last'] = Field(
		default='last',
		description="Display mode for demo panel: 'full' shows complete log panel, 'last' shows only latest action and memory in bottom-right corner",
	)
	cookie_whitelist_domains: list[str] = Field(
		default_factory=lambda: ['nature.com', 'qatarairways.com'],
		description='List of domains to whitelist in the "I still don\'t care about cookies" extension, preventing automatic cookie banner handling on these sites.',
	)

	window_size: ViewportSize | None = Field(
		default=None,
		description='Browser window size to use when headless=False.',
	)
	window_height: int | None = Field(default=None, description='DEPRECATED, use window_size["height"] instead', exclude=True)
	window_width: int | None = Field(default=None, description='DEPRECATED, use window_size["width"] instead', exclude=True)
	window_position: ViewportSize | None = Field(
		default=ViewportSize(width=0, height=0),
		description='Window position to use for the browser x,y from the top left when headless=False.',
	)
	cross_origin_iframes: bool = Field(
		default=True,
		description='Enable cross-origin iframe support (OOPIF/Out-of-Process iframes). When False, only same-origin frames are processed to avoid complexity and hanging.',
	)
	max_iframes: int = Field(
		default=100,
		description='Maximum number of iframe documents to process to prevent crashes.',
	)
	max_iframe_depth: int = Field(
		ge=0,
		default=5,
		description='Maximum depth for cross-origin iframe recursion (default: 5 levels deep).',
	)

	# --- Page load/wait timings ---

	minimum_wait_page_load_time: float = Field(default=0.25, description='Minimum time to wait before capturing page state.')
	wait_for_network_idle_page_load_time: float = Field(default=0.5, description='Time to wait for network idle.')

	wait_between_actions: float = Field(default=0.1, description='Time to wait between actions.')

	# --- UI/viewport/DOM ---
	highlight_elements: bool = Field(default=True, description='Highlight interactive elements on the page.')
	dom_highlight_elements: bool = Field(
		default=False, description='Highlight interactive elements in the DOM (only for debugging purposes).'
	)
	filter_highlight_ids: bool = Field(
		default=True, description='Only show element IDs in highlights if llm_representation is less than 10 characters.'
	)
	paint_order_filtering: bool = Field(default=True, description='Enable paint order filtering. Slightly experimental.')
	interaction_highlight_color: str = Field(
		default='rgb(255, 127, 39)',
		description='Color to use for highlighting elements during interactions (CSS color string).',
	)
	interaction_highlight_duration: float = Field(default=1.0, description='Duration in seconds to show interaction highlights.')

	# --- Downloads ---
	auto_download_pdfs: bool = Field(default=True, description='Automatically download PDFs when navigating to PDF viewer pages.')

	profile_directory: str = 'Default'  # e.g. 'Profile 1', 'Profile 2', 'Custom Profile', etc.

	# these can be found in BrowserLaunchArgs, BrowserLaunchPersistentContextArgs, BrowserNewContextArgs, BrowserConnectArgs:
	# save_recording_path: alias of record_video_dir
	# save_har_path: alias of record_har_path
	# trace_path: alias of traces_dir

	# these shadow the old playwright args on BrowserContextArgs, but it's ok
	# because we handle them ourselves in a watchdog and we no longer use playwright, so they should live in the scope for our own config in BrowserProfile long-term
	record_video_dir: Path | None = Field(
		default=None,
		description='Directory to save video recordings. If set, a video of the session will be recorded.',
		validation_alias=AliasChoices('save_recording_path', 'record_video_dir'),
	)
	record_video_size: ViewportSize | None = Field(
		default=None, description='Video frame size. If not set, it will use the viewport size.'
	)
	record_video_framerate: int = Field(default=30, description='The framerate to use for the video recording.')

	# TODO: finish implementing extension support in extensions.py
	# extension_ids_to_preinstall: list[str] = Field(
	# 	default_factory=list, description='List of Chrome extension IDs to preinstall.'
	# )
	# extensions_dir: Path = Field(
	# 	default_factory=lambda: Path('~/.config/browseruse/cache/extensions').expanduser(),
	# 	description='Directory containing .crx extension files.',
	# )

	def __repr__(self) -> str:
		short_dir = _log_pretty_path(self.user_data_dir) if self.user_data_dir else '<incognito>'
		return f'BrowserProfile(user_data_dir= {short_dir}, headless={self.headless})'

	def __str__(self) -> str:
		return 'BrowserProfile'

	@field_validator('allowed_domains', 'prohibited_domains', mode='after')
	@classmethod
	def optimize_large_domain_lists(cls, v: list[str] | set[str] | None) -> list[str] | set[str] | None:
		"""Convert large domain lists (>=100 items) to sets for O(1) lookup performance."""
		if v is None or isinstance(v, set):
			return v

		if len(v) >= DOMAIN_OPTIMIZATION_THRESHOLD:
			logger.warning(
				f'ğŸ”§ Optimizing domain list with {len(v)} items to set for O(1) lookup. '
				f'Note: Pattern matching (*.domain.com, etc.) is not supported for lists >= {DOMAIN_OPTIMIZATION_THRESHOLD} items. '
				f'Use exact domains only or keep list size < {DOMAIN_OPTIMIZATION_THRESHOLD} for pattern support.'
			)
			return set(v)

		return v

	@model_validator(mode='after')
	def copy_old_config_names_to_new(self) -> Self:
		"""Copy old config window_width & window_height to window_size."""
		if self.window_width or self.window_height:
			logger.warning(
				f'âš ï¸ BrowserProfile(window_width=..., window_height=...) are deprecated, use BrowserProfile(window_size={"width": 1920, "height": 1080}) instead.'
			)
			window_size = self.window_size or ViewportSize(width=0, height=0)
			window_size['width'] = window_size['width'] or self.window_width or 1920
			window_size['height'] = window_size['height'] or self.window_height or 1080
			self.window_size = window_size

		return self

	@model_validator(mode='after')
	def warn_storage_state_user_data_dir_conflict(self) -> Self:
		"""Warn when both storage_state and user_data_dir are set, as this can cause conflicts."""
		has_storage_state = self.storage_state is not None
		has_user_data_dir = (self.user_data_dir is not None) and ('tmp' not in str(self.user_data_dir).lower())

		if has_storage_state and has_user_data_dir:
			logger.warning(
				f'âš ï¸ BrowserSession(...) was passed both storage_state AND user_data_dir. storage_state={self.storage_state} will forcibly overwrite '
				f'cookies/localStorage/sessionStorage in user_data_dir={self.user_data_dir}. '
				f'For multiple browsers in parallel, use only storage_state with user_data_dir=None, '
				f'or use a separate user_data_dir for each browser and set storage_state=None.'
			)
		return self

	@model_validator(mode='after')
	def warn_user_data_dir_non_default_version(self) -> Self:
		"""
		If user is using default profile dir with a non-default channel, force-change it
		to avoid corrupting the default data dir created with a different channel.
		"""

		is_not_using_default_chromium = self.executable_path or self.channel not in (BROWSERUSE_DEFAULT_CHANNEL, None)
		if self.user_data_dir == CONFIG.BROWSER_USE_DEFAULT_USER_DATA_DIR and is_not_using_default_chromium:
			alternate_name = (
				Path(self.executable_path).name.lower().replace(' ', '-')
				if self.executable_path
				else self.channel.name.lower()
				if self.channel
				else 'None'
			)
			logger.warning(
				f'âš ï¸ {self} Changing user_data_dir= {_log_pretty_path(self.user_data_dir)} â¡ï¸ .../default-{alternate_name} to avoid {alternate_name.upper()} corruping default profile created by {BROWSERUSE_DEFAULT_CHANNEL.name}'
			)
			self.user_data_dir = CONFIG.BROWSER_USE_DEFAULT_USER_DATA_DIR.parent / f'default-{alternate_name}'
		return self

	@model_validator(mode='after')
	def warn_deterministic_rendering_weirdness(self) -> Self:
		if self.deterministic_rendering:
			logger.warning(
				'âš ï¸ BrowserSession(deterministic_rendering=True) is NOT RECOMMENDED. It breaks many sites and increases chances of getting blocked by anti-bot systems. '
				'It hardcodes the JS random seed and forces browsers across Linux/Mac/Windows to use the same font rendering engine so that identical screenshots can be generated.'
			)
		return self

	@model_validator(mode='after')
	def validate_proxy_settings(self) -> Self:
		"""Ensure proxy configuration is consistent."""
		if self.proxy and (self.proxy.bypass and not self.proxy.server):
			logger.warning('BrowserProfile.proxy.bypass provided but proxy has no server; bypass will be ignored.')
		return self

	@model_validator(mode='after')
	def validate_highlight_elements_conflict(self) -> Self:
		"""Ensure highlight_elements and dom_highlight_elements are not both enabled, with dom_highlight_elements taking priority."""
		if self.highlight_elements and self.dom_highlight_elements:
			logger.warning(
				'âš ï¸ Both highlight_elements and dom_highlight_elements are enabled. '
				'dom_highlight_elements takes priority. Setting highlight_elements=False.'
			)
			self.highlight_elements = False
		return self

	def model_post_init(self, __context: Any) -> None:
		"""Called after model initialization to set up display configuration."""
		self.detect_display_configuration()
		self._copy_profile()

	def _copy_profile(self) -> None:
		"""Copy profile to temp directory if user_data_dir is not None and not already a temp dir."""
		if self.user_data_dir is None:
			return

		user_data_str = str(self.user_data_dir)
		if 'browser-use-user-data-dir-' in user_data_str.lower():
			# Already using a temp directory, no need to copy
			return

		is_chrome = (
			'chrome' in user_data_str.lower()
			or ('chrome' in str(self.executable_path).lower())
			or self.channel
			in (BrowserChannel.CHROME, BrowserChannel.CHROME_BETA, BrowserChannel.CHROME_DEV, BrowserChannel.CHROME_CANARY)
		)

		if not is_chrome:
			return

		temp_dir = tempfile.mkdtemp(prefix='browser-use-user-data-dir-')
		path_original_user_data = Path(self.user_data_dir)
		path_original_profile = path_original_user_data / self.profile_directory
		path_temp_profile = Path(temp_dir) / self.profile_directory

		if path_original_profile.exists():
			import shutil

			shutil.copytree(path_original_profile, path_temp_profile)
			local_state_src = path_original_user_data / 'Local State'
			local_state_dst = Path(temp_dir) / 'Local State'
			if local_state_src.exists():
				shutil.copy(local_state_src, local_state_dst)
			logger.info(f'Copied profile ({self.profile_directory}) and Local State to temp directory: {temp_dir}')

		else:
			Path(temp_dir).mkdir(parents=True, exist_ok=True)
			path_temp_profile.mkdir(parents=True, exist_ok=True)
			logger.info(f'Created new profile ({self.profile_directory}) in temp directory: {temp_dir}')

		self.user_data_dir = temp_dir

	def get_args(self) -> list[str]:
		"""Get the list of all Chrome CLI launch args for this profile (compiled from defaults, user-provided, and system-specific)."""

		if isinstance(self.ignore_default_args, list):
			default_args = set(CHROME_DEFAULT_ARGS) - set(self.ignore_default_args)
		elif self.ignore_default_args is True:
			default_args = []
		elif not self.ignore_default_args:
			default_args = CHROME_DEFAULT_ARGS

		assert self.user_data_dir is not None, 'user_data_dir must be set to a non-default path'

		# Capture args before conversion for logging
		pre_conversion_args = [
			*default_args,
			*self.args,
			f'--user-data-dir={self.user_data_dir}',
			f'--profile-directory={self.profile_directory}',
			*(CHROME_DOCKER_ARGS if (CONFIG.IN_DOCKER or not self.chromium_sandbox) else []),
			*(CHROME_HEADLESS_ARGS if self.headless else []),
			*(CHROME_DISABLE_SECURITY_ARGS if self.disable_security else []),
			*(CHROME_DETERMINISTIC_RENDERING_ARGS if self.deterministic_rendering else []),
			*(
				[f'--window-size={self.window_size["width"]},{self.window_size["height"]}']
				if self.window_size
				else (['--start-maximized'] if not self.headless else [])
			),
			*(
				[f'--window-position={self.window_position["width"]},{self.window_position["height"]}']
				if self.window_position
				else []
			),
			*(self._get_extension_args() if self.enable_default_extensions else []),
		]

		# Proxy flags
		proxy_server = self.proxy.server if self.proxy else None
		proxy_bypass = self.proxy.bypass if self.proxy else None

		if proxy_server:
			pre_conversion_args.append(f'--proxy-server={proxy_server}')
			if proxy_bypass:
				pre_conversion_args.append(f'--proxy-bypass-list={proxy_bypass}')

		# User agent flag
		if self.user_agent:
			pre_conversion_args.append(f'--user-agent={self.user_agent}')

		# Special handling for --disable-features to merge values instead of overwriting
		# This prevents disable_security=True from breaking extensions by ensuring
		# both default features (including extension-related) and security features are preserved
		disable_features_values = []
		non_disable_features_args = []

		# Extract and merge all --disable-features values
		for arg in pre_conversion_args:
			if arg.startswith('--disable-features='):
				features = arg.split('=', 1)[1]
				disable_features_values.extend(features.split(','))
			else:
				non_disable_features_args.append(arg)

		# Remove duplicates while preserving order
		if disable_features_values:
			unique_features = []
			seen = set()
			for feature in disable_features_values:
				feature = feature.strip()
				if feature and feature not in seen:
					unique_features.append(feature)
					seen.add(feature)

			# Add merged disable-features back
			non_disable_features_args.append(f'--disable-features={",".join(unique_features)}')

		# convert to dict and back to dedupe and merge other duplicate args
		final_args_list = BrowserLaunchArgs.args_as_list(BrowserLaunchArgs.args_as_dict(non_disable_features_args))

		return final_args_list

	def _get_extension_args(self) -> list[str]:
		"""Get Chrome args for enabling default extensions (ad blocker and cookie handler)."""
		extension_paths = self._ensure_default_extensions_downloaded()

		args = [
			'--enable-extensions',
			'--disable-extensions-file-access-check',
			'--disable-extensions-http-throttling',
			'--enable-extension-activity-logging',
		]

		if extension_paths:
			args.append(f'--load-extension={",".join(extension_paths)}')

		return args

	def _ensure_default_extensions_downloaded(self) -> list[str]:
		"""
		Ensure default extensions are downloaded and cached locally.
		Returns list of paths to extension directories.
		"""

		# Extension definitions - optimized for automation and content extraction
		# Combines uBlock Origin (ad blocking) + "I still don't care about cookies" (cookie banner handling)
		extensions = [
			{
				'name': 'uBlock Origin',
				'id': 'cjpalhdlnbpafiamejdnhcphjbkeiagm',
				'url': 'https://clients2.google.com/service/update2/crx?response=redirect&prodversion=133&acceptformat=crx3&x=id%3Dcjpalhdlnbpafiamejdnhcphjbkeiagm%26uc',
			},
			{
				'name': "I still don't care about cookies",
				'id': 'edibdbjcniadpccecjdfdjjppcpchdlm',
				'url': 'https://clients2.google.com/service/update2/crx?response=redirect&prodversion=133&acceptformat=crx3&x=id%3Dedibdbjcniadpccecjdfdjjppcpchdlm%26uc',
			},
			{
				'name': 'ClearURLs',
				'id': 'lckanjgmijmafbedllaakclkaicjfmnk',
				'url': 'https://clients2.google.com/service/update2/crx?response=redirect&prodversion=133&acceptformat=crx3&x=id%3Dlckanjgmijmafbedllaakclkaicjfmnk%26uc',
			},
			{
				'name': 'Force Background Tab',
				'id': 'gidlfommnbibbmegmgajdbikelkdcmcl',
				'url': 'https://clients2.google.com/service/update2/crx?response=redirect&prodversion=133&acceptformat=crx3&x=id%3Dgidlfommnbibbmegmgajdbikelkdcmcl%26uc',
			},
			# {
			# 	'name': 'Captcha Solver: Auto captcha solving service',
			# 	'id': 'pgojnojmmhpofjgdmaebadhbocahppod',
			# 	'url': 'https://clients2.google.com/service/update2/crx?response=redirect&prodversion=130&acceptformat=crx3&x=id%3Dpgojnojmmhpofjgdmaebadhbocahppod%26uc',
			# },
			# Consent-O-Matic disabled - using uBlock Origin's cookie lists instead for simplicity
			# {
			# 	'name': 'Consent-O-Matic',
			# 	'id': 'mdjildafknihdffpkfmmpnpoiajfjnjd',
			# 	'url': 'https://clients2.google.com/service/update2/crx?response=redirect&prodversion=130&acceptformat=crx3&x=id%3Dmdjildafknihdffpkfmmpnpoiajfjnjd%26uc',
			# },
			# {
			# 	'name': 'Privacy | Protect Your Payments',
			# 	'id': 'hmgpakheknboplhmlicfkkgjipfabmhp',
			# 	'url': 'https://clients2.google.com/service/update2/crx?response=redirect&prodversion=130&acceptformat=crx3&x=id%3Dhmgpakheknboplhmlicfkkgjipfabmhp%26uc',
			# },
		]

		# Create extensions cache directory
		cache_dir = CONFIG.BROWSER_USE_EXTENSIONS_DIR
		cache_dir.mkdir(parents=True, exist_ok=True)
		# logger.debug(f'ğŸ“ Extensions cache directory: {_log_pretty_path(cache_dir)}')

		extension_paths = []
		loaded_extension_names = []

		for ext in extensions:
			ext_dir = cache_dir / ext['id']
			crx_file = cache_dir / f'{ext["id"]}.crx'

			# Check if extension is already extracted
			if ext_dir.exists() and (ext_dir / 'manifest.json').exists():
				# logger.debug(f'âœ… Using cached {ext["name"]} extension from {_log_pretty_path(ext_dir)}')
				extension_paths.append(str(ext_dir))
				loaded_extension_names.append(ext['name'])
				continue

			try:
				# Download extension if not cached
				if not crx_file.exists():
					logger.info(f'ğŸ“¦ Downloading {ext["name"]} extension...')
					self._download_extension(ext['url'], crx_file)
				else:
					logger.debug(f'ğŸ“¦ Found cached {ext["name"]} .crx file')

				# Extract extension
				logger.info(f'ğŸ“‚ Extracting {ext["name"]} extension...')
				self._extract_extension(crx_file, ext_dir)

				extension_paths.append(str(ext_dir))
				loaded_extension_names.append(ext['name'])

			except Exception as e:
				logger.warning(f'âš ï¸ Failed to setup {ext["name"]} extension: {e}')
				continue

		# Apply minimal patch to cookie extension with configurable whitelist
		for i, path in enumerate(extension_paths):
			if loaded_extension_names[i] == "I still don't care about cookies":
				self._apply_minimal_extension_patch(Path(path), self.cookie_whitelist_domains)

		if extension_paths:
			logger.debug(f'[BrowserProfile] ğŸ§© Extensions loaded ({len(extension_paths)}): [{", ".join(loaded_extension_names)}]')
		else:
			logger.warning('[BrowserProfile] âš ï¸ No default extensions could be loaded')

		return extension_paths

	def _apply_minimal_extension_patch(self, ext_dir: Path, whitelist_domains: list[str]) -> None:
		"""Minimal patch: pre-populate chrome.storage.local with configurable domain whitelist."""
		try:
			bg_path = ext_dir / 'data' / 'background.js'
			if not bg_path.exists():
				return

			with open(bg_path, encoding='utf-8') as f:
				content = f.read()

			# Create the whitelisted domains object for JavaScript with proper indentation
			whitelist_entries = [f'        "{domain}": true' for domain in whitelist_domains]
			whitelist_js = '{\n' + ',\n'.join(whitelist_entries) + '\n      }'

			# Find the initialize() function and inject storage setup before updateSettings()
			# The actual function uses 2-space indentation, not tabs
			old_init = """async function initialize(checkInitialized, magic) {
  if (checkInitialized && initialized) {
    return;
  }
  loadCachedRules();
  await updateSettings();
  await recreateTabList(magic);
  initialized = true;
}"""

			# New function with configurable whitelist initialization
			new_init = f"""// Pre-populate storage with configurable domain whitelist if empty
async function ensureWhitelistStorage() {{
  const result = await chrome.storage.local.get({{ settings: null }});
  if (!result.settings) {{
    const defaultSettings = {{
      statusIndicators: true,
      whitelistedDomains: {whitelist_js}
    }};
    await chrome.storage.local.set({{ settings: defaultSettings }});
  }}
}}

async function initialize(checkInitialized, magic) {{
  if (checkInitialized && initialized) {{
    return;
  }}
  loadCachedRules();
  await ensureWhitelistStorage(); // Add storage initialization
  await updateSettings();
  await recreateTabList(magic);
  initialized = true;
}}"""

			if old_init in content:
				content = content.replace(old_init, new_init)

				with open(bg_path, 'w', encoding='utf-8') as f:
					f.write(content)

				domain_list = ', '.join(whitelist_domains)
				logger.info(f'[BrowserProfile] âœ… Cookie extension: {domain_list} pre-populated in storage')
			else:
				logger.debug('[BrowserProfile] Initialize function not found for patching')

		except Exception as e:
			logger.debug(f'[BrowserProfile] Could not patch extension storage: {e}')

	def _download_extension(self, url: str, output_path: Path) -> None:
		"""Download extension .crx file."""
		import urllib.request

		try:
			with urllib.request.urlopen(url) as response:
				with open(output_path, 'wb') as f:
					f.write(response.read())
		except Exception as e:
			raise Exception(f'Failed to download extension: {e}')

	def _extract_extension(self, crx_path: Path, extract_dir: Path) -> None:
		"""Extract .crx file to directory."""
		import os
		import zipfile

		# Remove existing directory
		if extract_dir.exists():
			import shutil

			shutil.rmtree(extract_dir)

		extract_dir.mkdir(parents=True, exist_ok=True)

		try:
			# CRX files are ZIP files with a header, try to extract as ZIP
			with zipfile.ZipFile(crx_path, 'r') as zip_ref:
				zip_ref.extractall(extract_dir)

			# Verify manifest exists
			if not (extract_dir / 'manifest.json').exists():
				raise Exception('No manifest.json found in extension')

		except zipfile.BadZipFile:
			# CRX files have a header before the ZIP data
			# Skip the CRX header and extract the ZIP part
			with open(crx_path, 'rb') as f:
				# Read CRX header to find ZIP start
				magic = f.read(4)
				if magic != b'Cr24':
					raise Exception('Invalid CRX file format')

				version = int.from_bytes(f.read(4), 'little')
				if version == 2:
					pubkey_len = int.from_bytes(f.read(4), 'little')
					sig_len = int.from_bytes(f.read(4), 'little')
					f.seek(16 + pubkey_len + sig_len)  # Skip to ZIP data
				elif version == 3:
					header_len = int.from_bytes(f.read(4), 'little')
					f.seek(12 + header_len)  # Skip to ZIP data

				# Extract ZIP data
				zip_data = f.read()

			# Write ZIP data to temp file and extract
			import tempfile

			with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as temp_zip:
				temp_zip.write(zip_data)
				temp_zip.flush()

				with zipfile.ZipFile(temp_zip.name, 'r') as zip_ref:
					zip_ref.extractall(extract_dir)

				os.unlink(temp_zip.name)

	def detect_display_configuration(self) -> None:
		"""
		Detect the system display size and initialize the display-related config defaults:
		        screen, window_size, window_position, viewport, no_viewport, device_scale_factor
		"""

		display_size = get_display_size()
		has_screen_available = bool(display_size)
		self.screen = self.screen or display_size or ViewportSize(width=1920, height=1080)

		# if no headless preference specified, prefer headful if there is a display available
		if self.headless is None:
			self.headless = not has_screen_available

		# Determine viewport behavior based on mode and user preferences
		user_provided_viewport = self.viewport is not None

		if self.headless:
			# Headless mode: always use viewport for content size control
			self.viewport = self.viewport or self.window_size or self.screen
			self.window_position = None
			self.window_size = None
			self.no_viewport = False
		else:
			# Headful mode: respect user's viewport preference
			self.window_size = self.window_size or self.screen

			if user_provided_viewport:
				# User explicitly set viewport - enable viewport mode
				self.no_viewport = False
			else:
				# Default headful: content fits to window (no viewport)
				self.no_viewport = True if self.no_viewport is None else self.no_viewport

		# Handle special requirements (device_scale_factor forces viewport mode)
		if self.device_scale_factor and self.no_viewport is None:
			self.no_viewport = False

		# Finalize configuration
		if self.no_viewport:
			# No viewport mode: content adapts to window
			self.viewport = None
			self.device_scale_factor = None
			self.screen = None
			assert self.viewport is None
			assert self.no_viewport is True
		else:
			# Viewport mode: ensure viewport is set
			self.viewport = self.viewport or self.screen
			self.device_scale_factor = self.device_scale_factor or 1.0
			assert self.viewport is not None
			assert self.no_viewport is False

		assert not (self.headless and self.no_viewport), 'headless=True and no_viewport=True cannot both be set at the same time'

```

---

## backend/browser-use/browser_use/browser/python_highlights.py

```py
"""Python-based highlighting system for drawing bounding boxes on screenshots.

This module replaces JavaScript-based highlighting with fast Python image processing
to draw bounding boxes around interactive elements directly on screenshots.
"""

import asyncio
import base64
import io
import logging
import os

from PIL import Image, ImageDraw, ImageFont

from browser_use.dom.views import DOMSelectorMap, EnhancedDOMTreeNode
from browser_use.observability import observe_debug
from browser_use.utils import time_execution_async

logger = logging.getLogger(__name__)

# Font cache to prevent repeated font loading and reduce memory usage
_FONT_CACHE: dict[tuple[str, int], ImageFont.FreeTypeFont | None] = {}

# Cross-platform font paths
_FONT_PATHS = [
	'/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf',  # Linux (Debian/Ubuntu)
	'/usr/share/fonts/TTF/DejaVuSans-Bold.ttf',  # Linux (Arch/Fedora)
	'/System/Library/Fonts/Arial.ttf',  # macOS
	'C:\\Windows\\Fonts\\arial.ttf',  # Windows
	'arial.ttf',  # Windows (system path)
	'Arial Bold.ttf',  # macOS alternative
	'/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf',  # Linux alternative
]


def get_cross_platform_font(font_size: int) -> ImageFont.FreeTypeFont | None:
	"""Get a cross-platform compatible font with caching to prevent memory leaks.

	Args:
	    font_size: Size of the font to load

	Returns:
	    ImageFont object or None if no system fonts are available
	"""
	# Use cache key based on font size
	cache_key = ('system_font', font_size)

	# Return cached font if available
	if cache_key in _FONT_CACHE:
		return _FONT_CACHE[cache_key]

	# Try to load a system font
	font = None
	for font_path in _FONT_PATHS:
		try:
			font = ImageFont.truetype(font_path, font_size)
			break
		except OSError:
			continue

	# Cache the result (even if None) to avoid repeated attempts
	_FONT_CACHE[cache_key] = font
	return font


def cleanup_font_cache() -> None:
	"""Clean up the font cache to prevent memory leaks in long-running applications."""
	global _FONT_CACHE
	_FONT_CACHE.clear()


# Color scheme for different element types
ELEMENT_COLORS = {
	'button': '#FF6B6B',  # Red for buttons
	'input': '#4ECDC4',  # Teal for inputs
	'select': '#45B7D1',  # Blue for dropdowns
	'a': '#96CEB4',  # Green for links
	'textarea': '#FF8C42',  # Orange for text areas (was yellow, now more visible)
	'default': '#DDA0DD',  # Light purple for other interactive elements
}

# Element type mappings
ELEMENT_TYPE_MAP = {
	'button': 'button',
	'input': 'input',
	'select': 'select',
	'a': 'a',
	'textarea': 'textarea',
}


def get_element_color(tag_name: str, element_type: str | None = None) -> str:
	"""Get color for element based on tag name and type."""
	# Check input type first
	if tag_name == 'input' and element_type:
		if element_type in ['button', 'submit']:
			return ELEMENT_COLORS['button']

	# Use tag-based color
	return ELEMENT_COLORS.get(tag_name.lower(), ELEMENT_COLORS['default'])


def should_show_index_overlay(backend_node_id: int | None) -> bool:
	"""Determine if index overlay should be shown."""
	return backend_node_id is not None


def draw_enhanced_bounding_box_with_text(
	draw,  # ImageDraw.Draw - avoiding type annotation due to PIL typing issues
	bbox: tuple[int, int, int, int],
	color: str,
	text: str | None = None,
	font: ImageFont.FreeTypeFont | None = None,
	element_type: str = 'div',
	image_size: tuple[int, int] = (2000, 1500),
	device_pixel_ratio: float = 1.0,
) -> None:
	"""Draw an enhanced bounding box with much bigger index containers and dashed borders."""
	x1, y1, x2, y2 = bbox

	# Draw dashed bounding box with pattern: 1 line, 2 spaces, 1 line, 2 spaces...
	dash_length = 4
	gap_length = 8
	line_width = 2

	# Helper function to draw dashed line
	def draw_dashed_line(start_x, start_y, end_x, end_y):
		if start_x == end_x:  # Vertical line
			y = start_y
			while y < end_y:
				dash_end = min(y + dash_length, end_y)
				draw.line([(start_x, y), (start_x, dash_end)], fill=color, width=line_width)
				y += dash_length + gap_length
		else:  # Horizontal line
			x = start_x
			while x < end_x:
				dash_end = min(x + dash_length, end_x)
				draw.line([(x, start_y), (dash_end, start_y)], fill=color, width=line_width)
				x += dash_length + gap_length

	# Draw dashed rectangle
	draw_dashed_line(x1, y1, x2, y1)  # Top
	draw_dashed_line(x2, y1, x2, y2)  # Right
	draw_dashed_line(x2, y2, x1, y2)  # Bottom
	draw_dashed_line(x1, y2, x1, y1)  # Left

	# Draw much bigger index overlay if we have index text
	if text:
		try:
			# Scale font size for appropriate sizing across different resolutions
			img_width, img_height = image_size

			css_width = img_width  # / device_pixel_ratio
			# Much smaller scaling - 1% of CSS viewport width, max 16px to prevent huge highlights
			base_font_size = max(10, min(20, int(css_width * 0.01)))
			# Use shared font loading function with caching
			big_font = get_cross_platform_font(base_font_size)
			if big_font is None:
				big_font = font  # Fallback to original font if no system fonts found

			# Get text size with bigger font
			if big_font:
				bbox_text = draw.textbbox((0, 0), text, font=big_font)
				text_width = bbox_text[2] - bbox_text[0]
				text_height = bbox_text[3] - bbox_text[1]
			else:
				# Fallback for default font
				bbox_text = draw.textbbox((0, 0), text)
				text_width = bbox_text[2] - bbox_text[0]
				text_height = bbox_text[3] - bbox_text[1]

			# Scale padding appropriately for different resolutions
			padding = max(4, min(10, int(css_width * 0.005)))  # 0.3% of CSS width, max 4px
			element_width = x2 - x1
			element_height = y2 - y1

			# Container dimensions
			container_width = text_width + padding * 2
			container_height = text_height + padding * 2

			# Position in top center - for small elements, place further up to avoid blocking content
			# Center horizontally within the element
			bg_x1 = x1 + (element_width - container_width) // 2

			# Simple rule: if element is small, place index further up to avoid blocking icons
			if element_width < 60 or element_height < 30:
				# Small element: place well above to avoid blocking content
				bg_y1 = max(0, y1 - container_height - 5)
			else:
				# Regular element: place inside with small offset
				bg_y1 = y1 + 2

			bg_x2 = bg_x1 + container_width
			bg_y2 = bg_y1 + container_height

			# Center the number within the index box with proper baseline handling
			text_x = bg_x1 + (container_width - text_width) // 2
			# Add extra vertical space to prevent clipping
			text_y = bg_y1 + (container_height - text_height) // 2 - bbox_text[1]  # Subtract top offset

			# Ensure container stays within image bounds
			img_width, img_height = image_size
			if bg_x1 < 0:
				offset = -bg_x1
				bg_x1 += offset
				bg_x2 += offset
				text_x += offset
			if bg_y1 < 0:
				offset = -bg_y1
				bg_y1 += offset
				bg_y2 += offset
				text_y += offset
			if bg_x2 > img_width:
				offset = bg_x2 - img_width
				bg_x1 -= offset
				bg_x2 -= offset
				text_x -= offset
			if bg_y2 > img_height:
				offset = bg_y2 - img_height
				bg_y1 -= offset
				bg_y2 -= offset
				text_y -= offset

			# Draw bigger background rectangle with thicker border
			draw.rectangle([bg_x1, bg_y1, bg_x2, bg_y2], fill=color, outline='white', width=2)

			# Draw white text centered in the index box
			draw.text((text_x, text_y), text, fill='white', font=big_font or font)

		except Exception as e:
			logger.debug(f'Failed to draw enhanced text overlay: {e}')


def draw_bounding_box_with_text(
	draw,  # ImageDraw.Draw - avoiding type annotation due to PIL typing issues
	bbox: tuple[int, int, int, int],
	color: str,
	text: str | None = None,
	font: ImageFont.FreeTypeFont | None = None,
) -> None:
	"""Draw a bounding box with optional text overlay."""
	x1, y1, x2, y2 = bbox

	# Draw dashed bounding box
	dash_length = 2
	gap_length = 6

	# Top edge
	x = x1
	while x < x2:
		end_x = min(x + dash_length, x2)
		draw.line([(x, y1), (end_x, y1)], fill=color, width=2)
		draw.line([(x, y1 + 1), (end_x, y1 + 1)], fill=color, width=2)
		x += dash_length + gap_length

	# Bottom edge
	x = x1
	while x < x2:
		end_x = min(x + dash_length, x2)
		draw.line([(x, y2), (end_x, y2)], fill=color, width=2)
		draw.line([(x, y2 - 1), (end_x, y2 - 1)], fill=color, width=2)
		x += dash_length + gap_length

	# Left edge
	y = y1
	while y < y2:
		end_y = min(y + dash_length, y2)
		draw.line([(x1, y), (x1, end_y)], fill=color, width=2)
		draw.line([(x1 + 1, y), (x1 + 1, end_y)], fill=color, width=2)
		y += dash_length + gap_length

	# Right edge
	y = y1
	while y < y2:
		end_y = min(y + dash_length, y2)
		draw.line([(x2, y), (x2, end_y)], fill=color, width=2)
		draw.line([(x2 - 1, y), (x2 - 1, end_y)], fill=color, width=2)
		y += dash_length + gap_length

	# Draw index overlay if we have index text
	if text:
		try:
			# Get text size
			if font:
				bbox_text = draw.textbbox((0, 0), text, font=font)
				text_width = bbox_text[2] - bbox_text[0]
				text_height = bbox_text[3] - bbox_text[1]
			else:
				# Fallback for default font
				bbox_text = draw.textbbox((0, 0), text)
				text_width = bbox_text[2] - bbox_text[0]
				text_height = bbox_text[3] - bbox_text[1]

			# Smart positioning based on element size
			padding = 5
			element_width = x2 - x1
			element_height = y2 - y1
			element_area = element_width * element_height
			index_box_area = (text_width + padding * 2) * (text_height + padding * 2)

			# Calculate size ratio to determine positioning strategy
			size_ratio = element_area / max(index_box_area, 1)

			if size_ratio < 4:
				# Very small elements: place outside in bottom-right corner
				text_x = x2 + padding
				text_y = y2 - text_height
				# Ensure it doesn't go off screen
				text_x = min(text_x, 1200 - text_width - padding)
				text_y = max(text_y, 0)
			elif size_ratio < 16:
				# Medium elements: place in bottom-right corner inside
				text_x = x2 - text_width - padding
				text_y = y2 - text_height - padding
			else:
				# Large elements: place in center
				text_x = x1 + (element_width - text_width) // 2
				text_y = y1 + (element_height - text_height) // 2

			# Ensure text stays within bounds
			text_x = max(0, min(text_x, 1200 - text_width))
			text_y = max(0, min(text_y, 800 - text_height))

			# Draw background rectangle for maximum contrast
			bg_x1 = text_x - padding
			bg_y1 = text_y - padding
			bg_x2 = text_x + text_width + padding
			bg_y2 = text_y + text_height + padding

			# Use white background with thick black border for maximum visibility
			draw.rectangle([bg_x1, bg_y1, bg_x2, bg_y2], fill='white', outline='black', width=2)

			# Draw bold dark text on light background for best contrast
			draw.text((text_x, text_y), text, fill='black', font=font)

		except Exception as e:
			logger.debug(f'Failed to draw text overlay: {e}')


def process_element_highlight(
	element_id: int,
	element: EnhancedDOMTreeNode,
	draw,
	device_pixel_ratio: float,
	font,
	filter_highlight_ids: bool,
	image_size: tuple[int, int],
) -> None:
	"""Process a single element for highlighting."""
	try:
		# Use absolute_position coordinates directly
		if not element.absolute_position:
			return

		bounds = element.absolute_position

		# Scale coordinates from CSS pixels to device pixels for screenshot
		# The screenshot is captured at device pixel resolution, but coordinates are in CSS pixels
		x1 = int(bounds.x * device_pixel_ratio)
		y1 = int(bounds.y * device_pixel_ratio)
		x2 = int((bounds.x + bounds.width) * device_pixel_ratio)
		y2 = int((bounds.y + bounds.height) * device_pixel_ratio)

		# Ensure coordinates are within image bounds
		img_width, img_height = image_size
		x1 = max(0, min(x1, img_width))
		y1 = max(0, min(y1, img_height))
		x2 = max(x1, min(x2, img_width))
		y2 = max(y1, min(y2, img_height))

		# Skip if bounding box is too small or invalid
		if x2 - x1 < 2 or y2 - y1 < 2:
			return

		# Get element color based on type
		tag_name = element.tag_name if hasattr(element, 'tag_name') else 'div'
		element_type = None
		if hasattr(element, 'attributes') and element.attributes:
			element_type = element.attributes.get('type')

		color = get_element_color(tag_name, element_type)

		# Get element index for overlay and apply filtering
		backend_node_id = getattr(element, 'backend_node_id', None)
		index_text = None

		if backend_node_id is not None:
			if filter_highlight_ids:
				# Use the meaningful text that matches what the LLM sees
				meaningful_text = element.get_meaningful_text_for_llm()
				# Show ID only if meaningful text is less than 5 characters
				if len(meaningful_text) < 3:
					index_text = str(backend_node_id)
			else:
				# Always show ID when filter is disabled
				index_text = str(backend_node_id)

		# Draw enhanced bounding box with bigger index
		draw_enhanced_bounding_box_with_text(
			draw, (x1, y1, x2, y2), color, index_text, font, tag_name, image_size, device_pixel_ratio
		)

	except Exception as e:
		logger.debug(f'Failed to draw highlight for element {element_id}: {e}')


@observe_debug(ignore_input=True, ignore_output=True, name='create_highlighted_screenshot')
@time_execution_async('create_highlighted_screenshot')
async def create_highlighted_screenshot(
	screenshot_b64: str,
	selector_map: DOMSelectorMap,
	device_pixel_ratio: float = 1.0,
	viewport_offset_x: int = 0,
	viewport_offset_y: int = 0,
	filter_highlight_ids: bool = True,
) -> str:
	"""Create a highlighted screenshot with bounding boxes around interactive elements.

	Args:
	    screenshot_b64: Base64 encoded screenshot
	    selector_map: Map of interactive elements with their positions
	    device_pixel_ratio: Device pixel ratio for scaling coordinates
	    viewport_offset_x: X offset for viewport positioning
	    viewport_offset_y: Y offset for viewport positioning

	Returns:
	    Base64 encoded highlighted screenshot
	"""
	try:
		# Decode screenshot
		screenshot_data = base64.b64decode(screenshot_b64)
		image = Image.open(io.BytesIO(screenshot_data)).convert('RGBA')

		# Create drawing context
		draw = ImageDraw.Draw(image)

		# Load font using shared function with caching
		font = get_cross_platform_font(12)
		# If no system fonts found, font remains None and will use default font

		# Process elements sequentially to avoid ImageDraw thread safety issues
		# PIL ImageDraw is not thread-safe, so we process elements one by one
		for element_id, element in selector_map.items():
			process_element_highlight(element_id, element, draw, device_pixel_ratio, font, filter_highlight_ids, image.size)

		# Convert back to base64
		output_buffer = io.BytesIO()
		try:
			image.save(output_buffer, format='PNG')
			output_buffer.seek(0)
			highlighted_b64 = base64.b64encode(output_buffer.getvalue()).decode('utf-8')

			logger.debug(f'Successfully created highlighted screenshot with {len(selector_map)} elements')
			return highlighted_b64
		finally:
			# Explicit cleanup to prevent memory leaks
			output_buffer.close()
			if 'image' in locals():
				image.close()

	except Exception as e:
		logger.error(f'Failed to create highlighted screenshot: {e}')
		# Clean up on error as well
		if 'image' in locals():
			image.close()
		# Return original screenshot on error
		return screenshot_b64


async def get_viewport_info_from_cdp(cdp_session) -> tuple[float, int, int]:
	"""Get viewport information from CDP session.

	Returns:
	    Tuple of (device_pixel_ratio, scroll_x, scroll_y)
	"""
	try:
		# Get layout metrics which includes viewport info and device pixel ratio
		metrics = await cdp_session.cdp_client.send.Page.getLayoutMetrics(session_id=cdp_session.session_id)

		# Extract viewport information
		visual_viewport = metrics.get('visualViewport', {})
		css_visual_viewport = metrics.get('cssVisualViewport', {})
		css_layout_viewport = metrics.get('cssLayoutViewport', {})

		# Calculate device pixel ratio
		css_width = css_visual_viewport.get('clientWidth', css_layout_viewport.get('clientWidth', 1280.0))
		device_width = visual_viewport.get('clientWidth', css_width)
		device_pixel_ratio = device_width / css_width if css_width > 0 else 1.0

		# Get scroll position in CSS pixels
		scroll_x = int(css_visual_viewport.get('pageX', 0))
		scroll_y = int(css_visual_viewport.get('pageY', 0))

		return float(device_pixel_ratio), scroll_x, scroll_y

	except Exception as e:
		logger.debug(f'Failed to get viewport info from CDP: {e}')
		return 1.0, 0, 0


@time_execution_async('create_highlighted_screenshot_async')
async def create_highlighted_screenshot_async(
	screenshot_b64: str, selector_map: DOMSelectorMap, cdp_session=None, filter_highlight_ids: bool = True
) -> str:
	"""Async wrapper for creating highlighted screenshots.

	Args:
	    screenshot_b64: Base64 encoded screenshot
	    selector_map: Map of interactive elements
	    cdp_session: CDP session for getting viewport info
	    filter_highlight_ids: Whether to filter element IDs based on meaningful text

	Returns:
	    Base64 encoded highlighted screenshot
	"""
	# Get viewport information if CDP session is available
	device_pixel_ratio = 1.0
	viewport_offset_x = 0
	viewport_offset_y = 0

	if cdp_session:
		try:
			device_pixel_ratio, viewport_offset_x, viewport_offset_y = await get_viewport_info_from_cdp(cdp_session)
		except Exception as e:
			logger.debug(f'Failed to get viewport info from CDP: {e}')

	# Create highlighted screenshot with async processing
	final_screenshot = await create_highlighted_screenshot(
		screenshot_b64, selector_map, device_pixel_ratio, viewport_offset_x, viewport_offset_y, filter_highlight_ids
	)

	filename = os.getenv('BROWSER_USE_SCREENSHOT_FILE')
	if filename:

		def _write_screenshot():
			try:
				with open(filename, 'wb') as f:
					f.write(base64.b64decode(final_screenshot))
				logger.debug('Saved screenshot to ' + str(filename))
			except Exception as e:
				logger.warning(f'Failed to save screenshot to {filename}: {e}')

		await asyncio.to_thread(_write_screenshot)
	return final_screenshot


# Export the cleanup function for external use in long-running applications
__all__ = ['create_highlighted_screenshot', 'create_highlighted_screenshot_async', 'cleanup_font_cache']

```

---

## backend/browser-use/browser_use/browser/session.py

```py
"""Event-driven browser session with backwards compatibility."""

import asyncio
import logging
from functools import cached_property
from pathlib import Path
from typing import TYPE_CHECKING, Any, Literal, Self, Union, cast, overload
from urllib.parse import urlparse, urlunparse
from uuid import UUID

import httpx
from bubus import EventBus
from cdp_use import CDPClient
from cdp_use.cdp.fetch import AuthRequiredEvent, RequestPausedEvent
from cdp_use.cdp.network import Cookie
from cdp_use.cdp.target import AttachedToTargetEvent, SessionID, TargetID
from pydantic import BaseModel, ConfigDict, Field, PrivateAttr
from uuid_extensions import uuid7str

from browser_use.browser.cloud.cloud import CloudBrowserAuthError, CloudBrowserClient, CloudBrowserError

# CDP logging is now handled by setup_logging() in logging_config.py
# It automatically sets CDP logs to the same level as browser_use logs
from browser_use.browser.cloud.views import CloudBrowserParams, CreateBrowserRequest, ProxyCountryCode
from browser_use.browser.events import (
	AgentFocusChangedEvent,
	BrowserConnectedEvent,
	BrowserErrorEvent,
	BrowserLaunchEvent,
	BrowserLaunchResult,
	BrowserStartEvent,
	BrowserStateRequestEvent,
	BrowserStopEvent,
	BrowserStoppedEvent,
	CloseTabEvent,
	FileDownloadedEvent,
	NavigateToUrlEvent,
	NavigationCompleteEvent,
	NavigationStartedEvent,
	SwitchTabEvent,
	TabClosedEvent,
	TabCreatedEvent,
)
from browser_use.browser.profile import BrowserProfile, ProxySettings
from browser_use.browser.views import BrowserStateSummary, TabInfo
from browser_use.dom.views import DOMRect, EnhancedDOMTreeNode, TargetInfo
from browser_use.observability import observe_debug
from browser_use.utils import _log_pretty_url, create_task_with_error_handling, is_new_tab_page

if TYPE_CHECKING:
	from browser_use.actor.page import Page
	from browser_use.browser.demo_mode import DemoMode

DEFAULT_BROWSER_PROFILE = BrowserProfile()

_LOGGED_UNIQUE_SESSION_IDS = set()  # track unique session IDs that have been logged to make sure we always assign a unique enough id to new sessions and avoid ambiguity in logs
red = '\033[91m'
reset = '\033[0m'


class Target(BaseModel):
	"""Browser target (page, iframe, worker) - the actual entity being controlled.

	A target represents a browsing context with its own URL, title, and type.
	Multiple CDP sessions can attach to the same target for communication.
	"""

	model_config = ConfigDict(arbitrary_types_allowed=True, revalidate_instances='never')

	target_id: TargetID
	target_type: str  # 'page', 'iframe', 'worker', etc.
	url: str = 'about:blank'
	title: str = 'Unknown title'


class CDPSession(BaseModel):
	"""CDP communication channel to a target.

	A session is a connection that allows sending CDP commands to a specific target.
	Multiple sessions can attach to the same target.
	"""

	model_config = ConfigDict(arbitrary_types_allowed=True, revalidate_instances='never')

	cdp_client: CDPClient
	target_id: TargetID
	session_id: SessionID

	# Lifecycle monitoring (populated by SessionManager)
	_lifecycle_events: Any = PrivateAttr(default=None)
	_lifecycle_lock: Any = PrivateAttr(default=None)


class BrowserSession(BaseModel):
	"""Event-driven browser session with backwards compatibility.

	This class provides a 2-layer architecture:
	- High-level event handling for agents/tools
	- Direct CDP/Playwright calls for browser operations

	Supports both event-driven and imperative calling styles.

	Browser configuration is stored in the browser_profile, session identity in direct fields:
	``\`python
	# Direct settings (recommended for most users)
	session = BrowserSession(headless=True, user_data_dir='./profile')

	# Or use a profile (for advanced use cases)
	session = BrowserSession(browser_profile=BrowserProfile(...))

	# Access session fields directly, browser settings via profile or property
	print(session.id)  # Session field
	``\`
	"""

	model_config = ConfigDict(
		arbitrary_types_allowed=True,
		validate_assignment=True,
		extra='forbid',
		revalidate_instances='never',  # resets private attrs on every model rebuild
	)

	# Overload 1: Cloud browser mode (use cloud-specific params)
	@overload
	def __init__(
		self,
		*,
		# Cloud browser params - use these for cloud mode
		cloud_profile_id: UUID | str | None = None,
		cloud_proxy_country_code: ProxyCountryCode | None = None,
		cloud_timeout: int | None = None,
		# Backward compatibility aliases
		profile_id: UUID | str | None = None,
		proxy_country_code: ProxyCountryCode | None = None,
		timeout: int | None = None,
		use_cloud: bool | None = None,
		cloud_browser: bool | None = None,  # Backward compatibility alias
		cloud_browser_params: CloudBrowserParams | None = None,
		# Common params that work with cloud
		id: str | None = None,
		headers: dict[str, str] | None = None,
		allowed_domains: list[str] | None = None,
		keep_alive: bool | None = None,
		minimum_wait_page_load_time: float | None = None,
		wait_for_network_idle_page_load_time: float | None = None,
		wait_between_actions: float | None = None,
		auto_download_pdfs: bool | None = None,
		cookie_whitelist_domains: list[str] | None = None,
		cross_origin_iframes: bool | None = None,
		highlight_elements: bool | None = None,
		dom_highlight_elements: bool | None = None,
		paint_order_filtering: bool | None = None,
		max_iframes: int | None = None,
		max_iframe_depth: int | None = None,
	) -> None: ...

	# Overload 2: Local browser mode (use local browser params)
	@overload
	def __init__(
		self,
		*,
		# Core configuration for local
		id: str | None = None,
		cdp_url: str | None = None,
		browser_profile: BrowserProfile | None = None,
		# Local browser launch params
		executable_path: str | Path | None = None,
		headless: bool | None = None,
		user_data_dir: str | Path | None = None,
		args: list[str] | None = None,
		downloads_path: str | Path | None = None,
		# Common params
		headers: dict[str, str] | None = None,
		allowed_domains: list[str] | None = None,
		keep_alive: bool | None = None,
		minimum_wait_page_load_time: float | None = None,
		wait_for_network_idle_page_load_time: float | None = None,
		wait_between_actions: float | None = None,
		auto_download_pdfs: bool | None = None,
		cookie_whitelist_domains: list[str] | None = None,
		cross_origin_iframes: bool | None = None,
		highlight_elements: bool | None = None,
		dom_highlight_elements: bool | None = None,
		paint_order_filtering: bool | None = None,
		max_iframes: int | None = None,
		max_iframe_depth: int | None = None,
		# All other local params
		env: dict[str, str | float | bool] | None = None,
		ignore_default_args: list[str] | Literal[True] | None = None,
		channel: str | None = None,
		chromium_sandbox: bool | None = None,
		devtools: bool | None = None,
		traces_dir: str | Path | None = None,
		accept_downloads: bool | None = None,
		permissions: list[str] | None = None,
		user_agent: str | None = None,
		screen: dict | None = None,
		viewport: dict | None = None,
		no_viewport: bool | None = None,
		device_scale_factor: float | None = None,
		record_har_content: str | None = None,
		record_har_mode: str | None = None,
		record_har_path: str | Path | None = None,
		record_video_dir: str | Path | None = None,
		record_video_framerate: int | None = None,
		record_video_size: dict | None = None,
		storage_state: str | Path | dict[str, Any] | None = None,
		disable_security: bool | None = None,
		deterministic_rendering: bool | None = None,
		proxy: ProxySettings | None = None,
		enable_default_extensions: bool | None = None,
		window_size: dict | None = None,
		window_position: dict | None = None,
		filter_highlight_ids: bool | None = None,
		profile_directory: str | None = None,
	) -> None: ...

	def __init__(
		self,
		# Core configuration
		id: str | None = None,
		cdp_url: str | None = None,
		is_local: bool = False,
		browser_profile: BrowserProfile | None = None,
		# Cloud browser params (don't mix with local browser params)
		cloud_profile_id: UUID | str | None = None,
		cloud_proxy_country_code: ProxyCountryCode | None = None,
		cloud_timeout: int | None = None,
		# Backward compatibility aliases for cloud params
		profile_id: UUID | str | None = None,
		proxy_country_code: ProxyCountryCode | None = None,
		timeout: int | None = None,
		# BrowserProfile fields that can be passed directly
		# From BrowserConnectArgs
		headers: dict[str, str] | None = None,
		# From BrowserLaunchArgs
		env: dict[str, str | float | bool] | None = None,
		executable_path: str | Path | None = None,
		headless: bool | None = None,
		args: list[str] | None = None,
		ignore_default_args: list[str] | Literal[True] | None = None,
		channel: str | None = None,
		chromium_sandbox: bool | None = None,
		devtools: bool | None = None,
		downloads_path: str | Path | None = None,
		traces_dir: str | Path | None = None,
		# From BrowserContextArgs
		accept_downloads: bool | None = None,
		permissions: list[str] | None = None,
		user_agent: str | None = None,
		screen: dict | None = None,
		viewport: dict | None = None,
		no_viewport: bool | None = None,
		device_scale_factor: float | None = None,
		record_har_content: str | None = None,
		record_har_mode: str | None = None,
		record_har_path: str | Path | None = None,
		record_video_dir: str | Path | None = None,
		record_video_framerate: int | None = None,
		record_video_size: dict | None = None,
		# From BrowserLaunchPersistentContextArgs
		user_data_dir: str | Path | None = None,
		# From BrowserNewContextArgs
		storage_state: str | Path | dict[str, Any] | None = None,
		# BrowserProfile specific fields
		## Cloud Browser Fields
		use_cloud: bool | None = None,
		cloud_browser: bool | None = None,  # Backward compatibility alias
		cloud_browser_params: CloudBrowserParams | None = None,
		## Other params
		disable_security: bool | None = None,
		deterministic_rendering: bool | None = None,
		allowed_domains: list[str] | None = None,
		keep_alive: bool | None = None,
		proxy: ProxySettings | None = None,
		enable_default_extensions: bool | None = None,
		window_size: dict | None = None,
		window_position: dict | None = None,
		minimum_wait_page_load_time: float | None = None,
		wait_for_network_idle_page_load_time: float | None = None,
		wait_between_actions: float | None = None,
		filter_highlight_ids: bool | None = None,
		auto_download_pdfs: bool | None = None,
		profile_directory: str | None = None,
		cookie_whitelist_domains: list[str] | None = None,
		# DOM extraction layer configuration
		cross_origin_iframes: bool | None = None,
		highlight_elements: bool | None = None,
		dom_highlight_elements: bool | None = None,
		paint_order_filtering: bool | None = None,
		# Iframe processing limits
		max_iframes: int | None = None,
		max_iframe_depth: int | None = None,
	):
		# Following the same pattern as AgentSettings in service.py
		# Only pass non-None values to avoid validation errors
		profile_kwargs = {
			k: v
			for k, v in locals().items()
			if k
			not in [
				'self',
				'browser_profile',
				'id',
				'cloud_profile_id',
				'cloud_proxy_country_code',
				'cloud_timeout',
				'profile_id',
				'proxy_country_code',
				'timeout',
			]
			and v is not None
		}

		# Handle backward compatibility: prefer cloud_* params over old names
		final_profile_id = cloud_profile_id if cloud_profile_id is not None else profile_id
		final_proxy_country_code = cloud_proxy_country_code if cloud_proxy_country_code is not None else proxy_country_code
		final_timeout = cloud_timeout if cloud_timeout is not None else timeout

		# If any cloud params are provided, create cloud_browser_params
		if final_profile_id is not None or final_proxy_country_code is not None or final_timeout is not None:
			cloud_params = CreateBrowserRequest(
				cloud_profile_id=final_profile_id,
				cloud_proxy_country_code=final_proxy_country_code,
				cloud_timeout=final_timeout,
			)
			profile_kwargs['cloud_browser_params'] = cloud_params
			profile_kwargs['use_cloud'] = True

		# Handle backward compatibility: map cloud_browser to use_cloud
		if 'cloud_browser' in profile_kwargs:
			profile_kwargs['use_cloud'] = profile_kwargs.pop('cloud_browser')

		# If cloud_browser_params is set, force use_cloud=True
		if cloud_browser_params is not None:
			profile_kwargs['use_cloud'] = True

		# if is_local is False but executable_path is provided, set is_local to True
		if is_local is False and executable_path is not None:
			profile_kwargs['is_local'] = True
		# Only set is_local=True when cdp_url is missing if we're not using cloud browser
		# (cloud browser will provide cdp_url later)
		use_cloud = profile_kwargs.get('use_cloud') or profile_kwargs.get('cloud_browser')
		if not cdp_url and not use_cloud:
			profile_kwargs['is_local'] = True

		# Create browser profile from direct parameters or use provided one
		if browser_profile is not None:
			# Merge any direct kwargs into the provided browser_profile (direct kwargs take precedence)
			merged_kwargs = {**browser_profile.model_dump(exclude_unset=True), **profile_kwargs}
			resolved_browser_profile = BrowserProfile(**merged_kwargs)
		else:
			resolved_browser_profile = BrowserProfile(**profile_kwargs)

		# Initialize the Pydantic model
		super().__init__(
			id=id or str(uuid7str()),
			browser_profile=resolved_browser_profile,
		)

	# Session configuration (session identity only)
	id: str = Field(default_factory=lambda: str(uuid7str()), description='Unique identifier for this browser session')

	# Browser configuration (reusable profile)
	browser_profile: BrowserProfile = Field(
		default_factory=lambda: DEFAULT_BROWSER_PROFILE,
		description='BrowserProfile() options to use for the session, otherwise a default profile will be used',
	)

	# LLM screenshot resizing configuration
	llm_screenshot_size: tuple[int, int] | None = Field(
		default=None,
		description='Target size (width, height) to resize screenshots before sending to LLM. Coordinates from LLM will be scaled back to original viewport size.',
	)

	# Cache of original viewport size for coordinate conversion (set when browser state is captured)
	_original_viewport_size: tuple[int, int] | None = PrivateAttr(default=None)

	# Convenience properties for common browser settings
	@property
	def cdp_url(self) -> str | None:
		"""CDP URL from browser profile."""
		return self.browser_profile.cdp_url

	@property
	def is_local(self) -> bool:
		"""Whether this is a local browser instance from browser profile."""
		return self.browser_profile.is_local

	@property
	def cloud_browser(self) -> bool:
		"""Whether to use cloud browser service from browser profile."""
		return self.browser_profile.use_cloud

	@property
	def demo_mode(self) -> 'DemoMode | None':
		"""Lazy init demo mode helper when enabled."""
		if not self.browser_profile.demo_mode:
			return None
		if self._demo_mode is None:
			from browser_use.browser.demo_mode import DemoMode

			self._demo_mode = DemoMode(self)
		return self._demo_mode

	# Main shared event bus for all browser session + all watchdogs
	event_bus: EventBus = Field(default_factory=EventBus)

	# Mutable public state - which target has agent focus
	agent_focus_target_id: TargetID | None = None

	# Mutable private state shared between watchdogs
	_cdp_client_root: CDPClient | None = PrivateAttr(default=None)
	_connection_lock: Any = PrivateAttr(default=None)  # asyncio.Lock for preventing concurrent connections

	# PUBLIC: SessionManager instance (OWNS all targets and sessions)
	session_manager: Any = Field(default=None, exclude=True)  # SessionManager

	_cached_browser_state_summary: Any = PrivateAttr(default=None)
	_cached_selector_map: dict[int, EnhancedDOMTreeNode] = PrivateAttr(default_factory=dict)
	_downloaded_files: list[str] = PrivateAttr(default_factory=list)  # Track files downloaded during this session
	_closed_popup_messages: list[str] = PrivateAttr(default_factory=list)  # Store messages from auto-closed JavaScript dialogs

	# Watchdogs
	_crash_watchdog: Any | None = PrivateAttr(default=None)
	_downloads_watchdog: Any | None = PrivateAttr(default=None)
	_aboutblank_watchdog: Any | None = PrivateAttr(default=None)
	_security_watchdog: Any | None = PrivateAttr(default=None)
	_storage_state_watchdog: Any | None = PrivateAttr(default=None)
	_local_browser_watchdog: Any | None = PrivateAttr(default=None)
	_default_action_watchdog: Any | None = PrivateAttr(default=None)
	_dom_watchdog: Any | None = PrivateAttr(default=None)
	_screenshot_watchdog: Any | None = PrivateAttr(default=None)
	_permissions_watchdog: Any | None = PrivateAttr(default=None)
	_recording_watchdog: Any | None = PrivateAttr(default=None)

	_cloud_browser_client: CloudBrowserClient = PrivateAttr(default_factory=lambda: CloudBrowserClient())
	_demo_mode: 'DemoMode | None' = PrivateAttr(default=None)
	_demo_nav_handler_event_bus: EventBus | None = PrivateAttr(default=None)

	_logger: Any = PrivateAttr(default=None)

	@property
	def logger(self) -> Any:
		"""Get instance-specific logger with session ID in the name"""
		# **regenerate it every time** because our id and str(self) can change as browser connection state changes
		# if self._logger is None or not self._cdp_client_root:
		# 	self._logger = logging.getLogger(f'browser_use.{self}')
		return logging.getLogger(f'browser_use.{self}')

	@cached_property
	def _id_for_logs(self) -> str:
		"""Get human-friendly semi-unique identifier for differentiating different BrowserSession instances in logs"""
		str_id = self.id[-4:]  # default to last 4 chars of truly random uuid, less helpful than cdp port but always unique enough
		port_number = (self.cdp_url or 'no-cdp').rsplit(':', 1)[-1].split('/', 1)[0].strip()
		port_is_random = not port_number.startswith('922')
		port_is_unique_enough = port_number not in _LOGGED_UNIQUE_SESSION_IDS
		if port_number and port_number.isdigit() and port_is_random and port_is_unique_enough:
			# if cdp port is random/unique enough to identify this session, use it as our id in logs
			_LOGGED_UNIQUE_SESSION_IDS.add(port_number)
			str_id = port_number
		return str_id

	@property
	def _tab_id_for_logs(self) -> str:
		return self.agent_focus_target_id[-2:] if self.agent_focus_target_id else f'{red}--{reset}'

	def __repr__(self) -> str:
		return f'BrowserSessionğŸ…‘ {self._id_for_logs} ğŸ…£ {self._tab_id_for_logs} (cdp_url={self.cdp_url}, profile={self.browser_profile})'

	def __str__(self) -> str:
		return f'BrowserSessionğŸ…‘ {self._id_for_logs} ğŸ…£ {self._tab_id_for_logs}'

	async def reset(self) -> None:
		"""Clear all cached CDP sessions with proper cleanup."""

		cdp_status = 'connected' if self._cdp_client_root else 'not connected'
		session_mgr_status = 'exists' if self.session_manager else 'None'
		self.logger.debug(
			f'ğŸ”„ Resetting browser session (CDP: {cdp_status}, SessionManager: {session_mgr_status}, '
			f'focus: {self.agent_focus_target_id[-4:] if self.agent_focus_target_id else "None"})'
		)

		# Clear session manager (which owns _targets, _sessions, _target_sessions)
		if self.session_manager:
			await self.session_manager.clear()
			self.session_manager = None

		# Close CDP WebSocket before clearing to prevent stale event handlers
		if self._cdp_client_root:
			try:
				await self._cdp_client_root.stop()
				self.logger.debug('Closed CDP client WebSocket during reset')
			except Exception as e:
				self.logger.debug(f'Error closing CDP client during reset: {e}')

		self._cdp_client_root = None  # type: ignore
		self._cached_browser_state_summary = None
		self._cached_selector_map.clear()
		self._downloaded_files.clear()

		self.agent_focus_target_id = None
		if self.is_local:
			self.browser_profile.cdp_url = None

		self._crash_watchdog = None
		self._downloads_watchdog = None
		self._aboutblank_watchdog = None
		self._security_watchdog = None
		self._storage_state_watchdog = None
		self._local_browser_watchdog = None
		self._default_action_watchdog = None
		self._dom_watchdog = None
		self._screenshot_watchdog = None
		self._permissions_watchdog = None
		self._recording_watchdog = None
		if self._demo_mode:
			self._demo_mode.reset()
			self._demo_mode = None

		self.logger.debug('âœ… Browser session reset complete')

	def model_post_init(self, __context) -> None:
		"""Register event handlers after model initialization."""
		self._connection_lock = asyncio.Lock()

		# Check if handlers are already registered to prevent duplicates
		start_handlers = self.event_bus.handlers.get('BrowserStartEvent', [])
		start_handler_names = [getattr(h, '__name__', str(h)) for h in start_handlers]

		if any('on_BrowserStartEvent' in name for name in start_handler_names):
			raise RuntimeError(
				'[BrowserSession] Duplicate handler registration attempted! '
				'on_BrowserStartEvent is already registered. '
				'This likely means BrowserSession was initialized multiple times with the same EventBus.'
			)

		self._register_essential_handlers()

	def _register_essential_handlers(self) -> None:
		"""Register all essential event handlers on the current event bus."""
		from browser_use.browser.watchdog_base import BaseWatchdog

		BaseWatchdog.attach_handler_to_session(self, BrowserStartEvent, self.on_BrowserStartEvent)
		BaseWatchdog.attach_handler_to_session(self, BrowserStopEvent, self.on_BrowserStopEvent)
		BaseWatchdog.attach_handler_to_session(self, NavigateToUrlEvent, self.on_NavigateToUrlEvent)
		self._ensure_demo_mode_handlers()
		BaseWatchdog.attach_handler_to_session(self, SwitchTabEvent, self.on_SwitchTabEvent)
		BaseWatchdog.attach_handler_to_session(self, TabCreatedEvent, self.on_TabCreatedEvent)
		BaseWatchdog.attach_handler_to_session(self, TabClosedEvent, self.on_TabClosedEvent)
		BaseWatchdog.attach_handler_to_session(self, AgentFocusChangedEvent, self.on_AgentFocusChangedEvent)
		BaseWatchdog.attach_handler_to_session(self, FileDownloadedEvent, self.on_FileDownloadedEvent)
		BaseWatchdog.attach_handler_to_session(self, CloseTabEvent, self.on_CloseTabEvent)

	def _ensure_demo_mode_handlers(self) -> None:
		"""Ensure demo mode handlers are attached to the active event bus."""
		if self._demo_nav_handler_event_bus is self.event_bus:
			return

		self.event_bus.on(NavigationCompleteEvent, self._on_demo_mode_navigation_complete)
		self._demo_nav_handler_event_bus = self.event_bus

	@observe_debug(ignore_input=True, ignore_output=True, name='browser_session_start')
	async def start(self) -> None:
		"""Start the browser session."""
		start_event = self.event_bus.dispatch(BrowserStartEvent())
		await start_event
		# Ensure any exceptions from the event handler are propagated
		await start_event.event_result(raise_if_any=True, raise_if_none=False)

	async def kill(self) -> None:
		"""Kill the browser session and reset all state."""
		self.logger.debug('ğŸ›‘ kill() called - stopping browser with force=True and resetting state')

		# First save storage state while CDP is still connected
		from browser_use.browser.events import SaveStorageStateEvent

		save_event = self.event_bus.dispatch(SaveStorageStateEvent())
		await save_event

		# Dispatch stop event to kill the browser
		await self.event_bus.dispatch(BrowserStopEvent(force=True))
		# Stop the event bus
		await self.event_bus.stop(clear=True, timeout=5)
		# Reset all state
		await self.reset()
		# Create fresh event bus
		self.event_bus = EventBus()
		# Re-register all essential handlers on the new event bus
		self._register_essential_handlers()

	async def stop(self) -> None:
		"""Stop the browser session without killing the browser process.

		This clears event buses and cached state but keeps the browser alive.
		Useful when you want to clean up resources but plan to reconnect later.
		"""
		self.logger.debug('â¸ï¸  stop() called - stopping browser gracefully (force=False) and resetting state')

		# First save storage state while CDP is still connected
		from browser_use.browser.events import SaveStorageStateEvent

		save_event = self.event_bus.dispatch(SaveStorageStateEvent())
		await save_event

		# Now dispatch BrowserStopEvent to notify watchdogs
		await self.event_bus.dispatch(BrowserStopEvent(force=False))

		# Stop the event bus
		await self.event_bus.stop(clear=True, timeout=5)
		# Reset all state
		await self.reset()
		# Create fresh event bus
		self.event_bus = EventBus()
		# Re-register all essential handlers on the new event bus
		self._register_essential_handlers()

	@observe_debug(ignore_input=True, ignore_output=True, name='browser_start_event_handler')
	async def on_BrowserStartEvent(self, event: BrowserStartEvent) -> dict[str, str]:
		"""Handle browser start request.

		Returns:
			Dict with 'cdp_url' key containing the CDP URL

		Note: This method is idempotent - calling start() multiple times is safe.
		- If already connected, it skips reconnection
		- If you need to reset state, call stop() or kill() first
		"""

		# Initialize and attach all watchdogs FIRST so LocalBrowserWatchdog can handle BrowserLaunchEvent
		await self.attach_all_watchdogs()

		try:
			# If no CDP URL, launch local browser or cloud browser
			if not self.cdp_url:
				if self.browser_profile.use_cloud or self.browser_profile.cloud_browser_params is not None:
					# Use cloud browser service
					try:
						# Use cloud_browser_params if provided, otherwise create empty request
						cloud_params = self.browser_profile.cloud_browser_params or CreateBrowserRequest()
						cloud_browser_response = await self._cloud_browser_client.create_browser(cloud_params)
						self.browser_profile.cdp_url = cloud_browser_response.cdpUrl
						self.browser_profile.is_local = False
						self.logger.info('ğŸŒ¤ï¸ Successfully connected to cloud browser service')
					except CloudBrowserAuthError:
						raise CloudBrowserAuthError(
							'Authentication failed for cloud browser service. Set BROWSER_USE_API_KEY environment variable. You can also create an API key at https://cloud.browser-use.com/new-api-key'
						)
					except CloudBrowserError as e:
						raise CloudBrowserError(f'Failed to create cloud browser: {e}')
				elif self.is_local:
					# Launch local browser using event-driven approach
					launch_event = self.event_bus.dispatch(BrowserLaunchEvent())
					await launch_event

					# Get the CDP URL from LocalBrowserWatchdog handler result
					launch_result: BrowserLaunchResult = cast(
						BrowserLaunchResult, await launch_event.event_result(raise_if_none=True, raise_if_any=True)
					)
					self.browser_profile.cdp_url = launch_result.cdp_url
				else:
					raise ValueError('Got BrowserSession(is_local=False) but no cdp_url was provided to connect to!')

			assert self.cdp_url and '://' in self.cdp_url

			# Use lock to prevent concurrent connection attempts (race condition protection)
			async with self._connection_lock:
				# Only connect if not already connected
				if self._cdp_client_root is None:
					# Setup browser via CDP (for both local and remote cases)
					await self.connect(cdp_url=self.cdp_url)
					assert self.cdp_client is not None

					# Notify that browser is connected (single place)
					self.event_bus.dispatch(BrowserConnectedEvent(cdp_url=self.cdp_url))

					if self.browser_profile.demo_mode:
						try:
							demo = self.demo_mode
							if demo:
								await demo.ensure_ready()
						except Exception as exc:
							self.logger.warning(f'[DemoMode] Failed to inject demo overlay: {exc}')
				else:
					self.logger.debug('Already connected to CDP, skipping reconnection')
					if self.browser_profile.demo_mode:
						try:
							demo = self.demo_mode
							if demo:
								await demo.ensure_ready()
						except Exception as exc:
							self.logger.warning(f'[DemoMode] Failed to inject demo overlay: {exc}')

			# Return the CDP URL for other components
			return {'cdp_url': self.cdp_url}

		except Exception as e:
			self.event_bus.dispatch(
				BrowserErrorEvent(
					error_type='BrowserStartEventError',
					message=f'Failed to start browser: {type(e).__name__} {e}',
					details={'cdp_url': self.cdp_url, 'is_local': self.is_local},
				)
			)
			raise

	async def on_NavigateToUrlEvent(self, event: NavigateToUrlEvent) -> None:
		"""Handle navigation requests - core browser functionality."""
		self.logger.debug(f'[on_NavigateToUrlEvent] Received NavigateToUrlEvent: url={event.url}, new_tab={event.new_tab}')
		if not self.agent_focus_target_id:
			self.logger.warning('Cannot navigate - browser not connected')
			return

		target_id = None
		current_target_id = self.agent_focus_target_id

		# If new_tab=True but we're already in a new tab, set new_tab=False
		current_target = self.session_manager.get_target(current_target_id)
		if event.new_tab and is_new_tab_page(current_target.url):
			self.logger.debug(f'[on_NavigateToUrlEvent] Already on blank tab ({current_target.url}), reusing')
			event.new_tab = False

		try:
			# Find or create target for navigation
			self.logger.debug(f'[on_NavigateToUrlEvent] Processing new_tab={event.new_tab}')

			if event.new_tab:
				page_targets = self.session_manager.get_all_page_targets()
				self.logger.debug(f'[on_NavigateToUrlEvent] Found {len(page_targets)} existing tabs')

				# Look for existing about:blank tab that's not the current one
				for idx, target in enumerate(page_targets):
					self.logger.debug(f'[on_NavigateToUrlEvent] Tab {idx}: url={target.url}, targetId={target.target_id}')
					if target.url == 'about:blank' and target.target_id != current_target_id:
						target_id = target.target_id
						self.logger.debug(f'Reusing existing about:blank tab #{target_id[-4:]}')
						break

				# Create new tab if no reusable one found
				if not target_id:
					self.logger.debug('[on_NavigateToUrlEvent] No reusable about:blank tab found, creating new tab...')
					try:
						target_id = await self._cdp_create_new_page('about:blank')
						self.logger.debug(f'Created new tab #{target_id[-4:]}')
						# Dispatch TabCreatedEvent for new tab
						await self.event_bus.dispatch(TabCreatedEvent(target_id=target_id, url='about:blank'))
					except Exception as e:
						self.logger.error(f'[on_NavigateToUrlEvent] Failed to create new tab: {type(e).__name__}: {e}')
						# Fall back to using current tab
						target_id = current_target_id
						self.logger.warning(f'[on_NavigateToUrlEvent] Falling back to current tab #{target_id[-4:]}')
			else:
				# Use current tab
				target_id = target_id or current_target_id

			# Switch to target tab if needed (for both new_tab=True and new_tab=False)
			if self.agent_focus_target_id is None or self.agent_focus_target_id != target_id:
				self.logger.debug(
					f'[on_NavigateToUrlEvent] Switching to target tab {target_id[-4:]} (current: {self.agent_focus_target_id[-4:] if self.agent_focus_target_id else "none"})'
				)
				# Activate target (bring to foreground)
				await self.event_bus.dispatch(SwitchTabEvent(target_id=target_id))
			else:
				self.logger.debug(f'[on_NavigateToUrlEvent] Already on target tab {target_id[-4:]}, skipping SwitchTabEvent')

			assert self.agent_focus_target_id is not None and self.agent_focus_target_id == target_id, (
				'Agent focus not updated to new target_id after SwitchTabEvent should have switched to it'
			)

			# Dispatch navigation started
			await self.event_bus.dispatch(NavigationStartedEvent(target_id=target_id, url=event.url))

			# Navigate to URL with proper lifecycle waiting
			await self._navigate_and_wait(event.url, target_id)

			# Close any extension options pages that might have opened
			await self._close_extension_options_pages()

			# Dispatch navigation complete
			self.logger.debug(f'Dispatching NavigationCompleteEvent for {event.url} (tab #{target_id[-4:]})')
			await self.event_bus.dispatch(
				NavigationCompleteEvent(
					target_id=target_id,
					url=event.url,
					status=None,  # CDP doesn't provide status directly
				)
			)
			await self.event_bus.dispatch(AgentFocusChangedEvent(target_id=target_id, url=event.url))

			# Note: These should be handled by dedicated watchdogs:
			# - Security checks (security_watchdog)
			# - Page health checks (crash_watchdog)
			# - Dialog handling (dialog_watchdog)
			# - Download handling (downloads_watchdog)
			# - DOM rebuilding (dom_watchdog)

		except Exception as e:
			self.logger.error(f'Navigation failed: {type(e).__name__}: {e}')
			# target_id might be unbound if exception happens early
			if 'target_id' in locals() and target_id:
				await self.event_bus.dispatch(
					NavigationCompleteEvent(
						target_id=target_id,
						url=event.url,
						error_message=f'{type(e).__name__}: {e}',
					)
				)
				await self.event_bus.dispatch(AgentFocusChangedEvent(target_id=target_id, url=event.url))
			raise

	async def _navigate_and_wait(self, url: str, target_id: str, timeout: float | None = None) -> None:
		"""Navigate to URL and wait for page readiness using CDP lifecycle events.

		Two-strategy approach optimized for speed with robust fallback:
		1. networkIdle - Returns ASAP when no network activity (~50-200ms for cached pages)
		2. load - Fallback when page has ongoing network activity (all resources loaded)

		This gives us instant returns for cached content while being robust for dynamic pages.

		NO handler registration here - handlers are registered ONCE per session in SessionManager.
		We poll stored events instead to avoid handler accumulation.
		"""
		cdp_session = await self.get_or_create_cdp_session(target_id, focus=False)

		if timeout is None:
			target = self.session_manager.get_target(target_id)
			current_url = target.url
			same_domain = (
				url.split('/')[2] == current_url.split('/')[2]
				if url.startswith('http') and current_url.startswith('http')
				else False
			)
			timeout = 2.0 if same_domain else 4.0

		# Start performance tracking
		nav_start_time = asyncio.get_event_loop().time()

		nav_result = await cdp_session.cdp_client.send.Page.navigate(
			params={'url': url, 'transitionType': 'address_bar'},
			session_id=cdp_session.session_id,
		)

		# Check for immediate navigation errors
		if nav_result.get('errorText'):
			raise RuntimeError(f'Navigation failed: {nav_result["errorText"]}')

		# Track this specific navigation
		navigation_id = nav_result.get('loaderId')
		start_time = asyncio.get_event_loop().time()

		# Poll stored lifecycle events
		seen_events = []  # Track events for timeout diagnostics

		# Check if session has lifecycle monitoring enabled
		if not hasattr(cdp_session, '_lifecycle_events'):
			raise RuntimeError(
				f'âŒ Lifecycle monitoring not enabled for {cdp_session.target_id[:8]}! '
				f'This is a bug - SessionManager should have initialized it. '
				f'Session: {cdp_session}'
			)

		# Poll for lifecycle events until timeout
		poll_interval = 0.05  # Poll every 50ms
		while (asyncio.get_event_loop().time() - start_time) < timeout:
			# Check stored events
			try:
				# Get recent events matching our navigation
				for event_data in list(cdp_session._lifecycle_events):
					event_name = event_data.get('name')
					event_loader_id = event_data.get('loaderId')

					# Track events
					event_str = f'{event_name}(loader={event_loader_id[:8] if event_loader_id else "none"})'
					if event_str not in seen_events:
						seen_events.append(event_str)

					# Only respond to events from our navigation (or accept all if no loaderId)
					if event_loader_id and navigation_id and event_loader_id != navigation_id:
						continue

					if event_name == 'networkIdle':
						duration_ms = (asyncio.get_event_loop().time() - nav_start_time) * 1000
						self.logger.debug(f'âœ… Page ready for {url} (networkIdle, {duration_ms:.0f}ms)')
						return

					elif event_name == 'load':
						duration_ms = (asyncio.get_event_loop().time() - nav_start_time) * 1000
						self.logger.debug(f'âœ… Page ready for {url} (load, {duration_ms:.0f}ms)')
						return

			except Exception as e:
				self.logger.debug(f'Error polling lifecycle events: {e}')

			# Wait before next poll
			await asyncio.sleep(poll_interval)

		# Timeout - continue anyway with detailed diagnostics
		duration_ms = (asyncio.get_event_loop().time() - nav_start_time) * 1000
		if not seen_events:
			self.logger.error(
				f'âŒ No lifecycle events received for {url} after {duration_ms:.0f}ms! '
				f'Monitoring may have failed. Target: {cdp_session.target_id[:8]}'
			)
		else:
			self.logger.warning(f'âš ï¸ Page readiness timeout ({timeout}s, {duration_ms:.0f}ms) for {url}')

	async def _on_demo_mode_navigation_complete(self, event: NavigationCompleteEvent) -> None:
		"""Rehydrate the demo overlay and logs after navigation."""
		if not self.browser_profile.demo_mode:
			return

		demo = self.demo_mode
		if not demo:
			return

		try:
			await demo.refresh_target(event.target_id)
		except Exception as exc:
			self.logger.debug(f'[DemoMode] Failed to refresh overlay for target {event.target_id[:8]}...: {exc}')

	async def on_SwitchTabEvent(self, event: SwitchTabEvent) -> TargetID:
		"""Handle tab switching - core browser functionality."""
		if not self.agent_focus_target_id:
			raise RuntimeError('Cannot switch tabs - browser not connected')

		# Get all page targets
		page_targets = self.session_manager.get_all_page_targets()
		if event.target_id is None:
			# Most recently opened page
			if page_targets:
				# Update the target id to be the id of the most recently opened page, then proceed to switch to it
				event.target_id = page_targets[-1].target_id
			else:
				# No pages open at all, create a new one (handles switching to it automatically)
				assert self._cdp_client_root is not None, 'CDP client root not initialized - browser may not be connected yet'
				new_target = await self._cdp_client_root.send.Target.createTarget(params={'url': 'about:blank'})
				target_id = new_target['targetId']
				# Don't await, these may circularly trigger SwitchTabEvent and could deadlock, dispatch to enqueue and return
				self.event_bus.dispatch(TabCreatedEvent(url='about:blank', target_id=target_id))
				self.event_bus.dispatch(AgentFocusChangedEvent(target_id=target_id, url='about:blank'))
				return target_id

		# Switch to the target
		assert event.target_id is not None, 'target_id must be set at this point'
		# Ensure session exists and update agent focus (only for page/tab targets)
		cdp_session = await self.get_or_create_cdp_session(target_id=event.target_id, focus=True)

		# Visually switch to the tab in the browser
		# The Force Background Tab extension prevents Chrome from auto-switching when links create new tabs,
		# but we still want the agent to be able to explicitly switch tabs when needed
		await cdp_session.cdp_client.send.Target.activateTarget(params={'targetId': event.target_id})

		# Get target to access url
		target = self.session_manager.get_target(event.target_id)

		# dispatch focus changed event
		await self.event_bus.dispatch(
			AgentFocusChangedEvent(
				target_id=target.target_id,
				url=target.url,
			)
		)
		return target.target_id

	async def on_CloseTabEvent(self, event: CloseTabEvent) -> None:
		"""Handle tab closure - update focus if needed."""
		try:
			# Dispatch tab closed event
			await self.event_bus.dispatch(TabClosedEvent(target_id=event.target_id))

			# Try to close the target, but don't fail if it's already closed
			try:
				cdp_session = await self.get_or_create_cdp_session(target_id=None, focus=False)
				await cdp_session.cdp_client.send.Target.closeTarget(params={'targetId': event.target_id})
			except Exception as e:
				self.logger.debug(f'Target may already be closed: {e}')
		except Exception as e:
			self.logger.warning(f'Error during tab close cleanup: {e}')

	async def on_TabCreatedEvent(self, event: TabCreatedEvent) -> None:
		"""Handle tab creation - apply viewport settings to new tab."""
		# Note: Tab switching prevention is handled by the Force Background Tab extension
		# The extension automatically keeps focus on the current tab when new tabs are created

		# Apply viewport settings if configured
		if self.browser_profile.viewport and not self.browser_profile.no_viewport:
			try:
				viewport_width = self.browser_profile.viewport.width
				viewport_height = self.browser_profile.viewport.height
				device_scale_factor = self.browser_profile.device_scale_factor or 1.0

				self.logger.info(
					f'Setting viewport to {viewport_width}x{viewport_height} with device scale factor {device_scale_factor} whereas original device scale factor was {self.browser_profile.device_scale_factor}'
				)
				# Use the helper method with the new tab's target_id
				await self._cdp_set_viewport(viewport_width, viewport_height, device_scale_factor, target_id=event.target_id)

				self.logger.debug(f'Applied viewport {viewport_width}x{viewport_height} to tab {event.target_id[-8:]}')
			except Exception as e:
				self.logger.warning(f'Failed to set viewport for new tab {event.target_id[-8:]}: {e}')

	async def on_TabClosedEvent(self, event: TabClosedEvent) -> None:
		"""Handle tab closure - update focus if needed."""
		if not self.agent_focus_target_id:
			return

		# Get current tab index
		current_target_id = self.agent_focus_target_id

		# If the closed tab was the current one, find a new target
		if current_target_id == event.target_id:
			await self.event_bus.dispatch(SwitchTabEvent(target_id=None))

	async def on_AgentFocusChangedEvent(self, event: AgentFocusChangedEvent) -> None:
		"""Handle agent focus change - update focus and clear cache."""
		self.logger.debug(f'ğŸ”„ AgentFocusChangedEvent received: target_id=...{event.target_id[-4:]} url={event.url}')

		# Clear cached DOM state since focus changed
		if self._dom_watchdog:
			self._dom_watchdog.clear_cache()

		# Clear cached browser state
		self._cached_browser_state_summary = None
		self._cached_selector_map.clear()
		self.logger.debug('ğŸ”„ Cached browser state cleared')

		# Update agent focus if a specific target_id is provided (only for page/tab targets)
		if event.target_id:
			# Ensure session exists and update agent focus (validates target_type internally)
			await self.get_or_create_cdp_session(target_id=event.target_id, focus=True)

			# Apply viewport settings to the newly focused tab
			if self.browser_profile.viewport and not self.browser_profile.no_viewport:
				try:
					viewport_width = self.browser_profile.viewport.width
					viewport_height = self.browser_profile.viewport.height
					device_scale_factor = self.browser_profile.device_scale_factor or 1.0

					# Use the helper method with the current tab's target_id
					await self._cdp_set_viewport(viewport_width, viewport_height, device_scale_factor, target_id=event.target_id)

					self.logger.debug(f'Applied viewport {viewport_width}x{viewport_height} to tab {event.target_id[-8:]}')
				except Exception as e:
					self.logger.warning(f'Failed to set viewport for tab {event.target_id[-8:]}: {e}')
		else:
			raise RuntimeError('AgentFocusChangedEvent received with no target_id for newly focused tab')

	async def on_FileDownloadedEvent(self, event: FileDownloadedEvent) -> None:
		"""Track downloaded files during this session."""
		self.logger.debug(f'FileDownloadedEvent received: {event.file_name} at {event.path}')
		if event.path and event.path not in self._downloaded_files:
			self._downloaded_files.append(event.path)
			self.logger.info(f'ğŸ“ Tracked download: {event.file_name} ({len(self._downloaded_files)} total downloads in session)')
		else:
			if not event.path:
				self.logger.warning(f'FileDownloadedEvent has no path: {event}')
			else:
				self.logger.debug(f'File already tracked: {event.path}')

	async def on_BrowserStopEvent(self, event: BrowserStopEvent) -> None:
		"""Handle browser stop request."""

		try:
			# Check if we should keep the browser alive
			if self.browser_profile.keep_alive and not event.force:
				self.event_bus.dispatch(BrowserStoppedEvent(reason='Kept alive due to keep_alive=True'))
				return

			# Clean up cloud browser session if using cloud browser
			if self.browser_profile.use_cloud:
				try:
					await self._cloud_browser_client.stop_browser()
					self.logger.info('ğŸŒ¤ï¸ Cloud browser session cleaned up')
				except Exception as e:
					self.logger.debug(f'Failed to cleanup cloud browser session: {e}')

			# Clear CDP session cache before stopping
			self.logger.debug(
				f'ğŸ“¢ on_BrowserStopEvent - Calling reset() (force={event.force}, keep_alive={self.browser_profile.keep_alive})'
			)
			await self.reset()

			# Reset state
			if self.is_local:
				self.browser_profile.cdp_url = None

			# Notify stop and wait for all handlers to complete
			# LocalBrowserWatchdog listens for BrowserStopEvent and dispatches BrowserKillEvent
			stop_event = self.event_bus.dispatch(BrowserStoppedEvent(reason='Stopped by request'))
			await stop_event

		except Exception as e:
			self.event_bus.dispatch(
				BrowserErrorEvent(
					error_type='BrowserStopEventError',
					message=f'Failed to stop browser: {type(e).__name__} {e}',
					details={'cdp_url': self.cdp_url, 'is_local': self.is_local},
				)
			)

	# region - ========== CDP-based replacements for browser_context operations ==========
	@property
	def cdp_client(self) -> CDPClient:
		"""Get the cached root CDP cdp_session.cdp_client. The client is created and started in self.connect()."""
		assert self._cdp_client_root is not None, 'CDP client not initialized - browser may not be connected yet'
		return self._cdp_client_root

	async def new_page(self, url: str | None = None) -> 'Page':
		"""Create a new page (tab)."""
		from cdp_use.cdp.target.commands import CreateTargetParameters

		params: CreateTargetParameters = {'url': url or 'about:blank'}
		result = await self.cdp_client.send.Target.createTarget(params)

		target_id = result['targetId']

		# Import here to avoid circular import
		from browser_use.actor.page import Page as Target

		return Target(self, target_id)

	async def get_current_page(self) -> 'Page | None':
		"""Get the current page as an actor Page."""
		target_info = await self.get_current_target_info()

		if not target_info:
			return None

		from browser_use.actor.page import Page as Target

		return Target(self, target_info['targetId'])

	async def must_get_current_page(self) -> 'Page':
		"""Get the current page as an actor Page."""
		page = await self.get_current_page()
		if not page:
			raise RuntimeError('No current target found')

		return page

	async def get_pages(self) -> list['Page']:
		"""Get all available pages using SessionManager (source of truth)."""
		# Import here to avoid circular import
		from browser_use.actor.page import Page as PageActor

		page_targets = self.session_manager.get_all_page_targets() if self.session_manager else []

		targets = []
		for target in page_targets:
			targets.append(PageActor(self, target.target_id))

		return targets

	def get_focused_target(self) -> 'Target | None':
		"""Get the target that currently has agent focus.

		Returns:
			Target object if agent has focus, None otherwise.
		"""
		if not self.session_manager:
			return None
		return self.session_manager.get_focused_target()

	def get_page_targets(self) -> list['Target']:
		"""Get all page/tab targets (excludes iframes, workers, etc.).

		Returns:
			List of Target objects for all page/tab targets.
		"""
		if not self.session_manager:
			return []
		return self.session_manager.get_all_page_targets()

	async def close_page(self, page: 'Union[Page, str]') -> None:
		"""Close a page by Page object or target ID."""
		from cdp_use.cdp.target.commands import CloseTargetParameters

		# Import here to avoid circular import
		from browser_use.actor.page import Page as Target

		if isinstance(page, Target):
			target_id = page._target_id
		else:
			target_id = str(page)

		params: CloseTargetParameters = {'targetId': target_id}
		await self.cdp_client.send.Target.closeTarget(params)

	async def cookies(self, urls: list[str] | None = None) -> list['Cookie']:
		"""Get cookies, optionally filtered by URLs."""
		from cdp_use.cdp.network.library import GetCookiesParameters

		params: GetCookiesParameters = {}
		if urls:
			params['urls'] = urls

		result = await self.cdp_client.send.Network.getCookies(params)
		return result['cookies']

	async def clear_cookies(self) -> None:
		"""Clear all cookies."""
		await self.cdp_client.send.Network.clearBrowserCookies()

	async def export_storage_state(self, output_path: str | Path | None = None) -> dict[str, Any]:
		"""Export all browser cookies and storage to storage_state format.

		Extracts decrypted cookies via CDP, bypassing keychain encryption.

		Args:
			output_path: Optional path to save storage_state.json. If None, returns dict only.

		Returns:
			Storage state dict with cookies in Playwright format.

		"""
		from pathlib import Path

		# Get all cookies using Storage.getCookies (returns decrypted cookies from all domains)
		cookies = await self._cdp_get_cookies()

		# Convert CDP cookie format to Playwright storage_state format
		storage_state = {
			'cookies': [
				{
					'name': c['name'],
					'value': c['value'],
					'domain': c['domain'],
					'path': c['path'],
					'expires': c.get('expires', -1),
					'httpOnly': c.get('httpOnly', False),
					'secure': c.get('secure', False),
					'sameSite': c.get('sameSite', 'Lax'),
				}
				for c in cookies
			],
			'origins': [],  # Could add localStorage/sessionStorage extraction if needed
		}

		if output_path:
			import json

			output_file = Path(output_path).expanduser().resolve()
			output_file.parent.mkdir(parents=True, exist_ok=True)
			output_file.write_text(json.dumps(storage_state, indent=2))
			self.logger.info(f'ğŸ’¾ Exported {len(cookies)} cookies to {output_file}')

		return storage_state

	async def get_or_create_cdp_session(self, target_id: TargetID | None = None, focus: bool = True) -> CDPSession:
		"""Get CDP session for a target from the event-driven pool.

		With autoAttach=True, sessions are created automatically by Chrome and added
		to the pool via Target.attachedToTarget events. This method retrieves them.

		Args:
			target_id: Target ID to get session for. If None, uses current agent focus.
			focus: If True, switches agent focus to this target (page targets only).

		Returns:
			CDPSession for the specified target.

		Raises:
			ValueError: If target doesn't exist or session is not available.
		"""
		assert self._cdp_client_root is not None, 'Root CDP client not initialized'
		assert self.session_manager is not None, 'SessionManager not initialized'

		# If no target_id specified, ensure current agent focus is valid and wait for recovery if needed
		if target_id is None:
			# Validate and wait for focus recovery if stale (centralized protection)
			focus_valid = await self.session_manager.ensure_valid_focus(timeout=5.0)
			if not focus_valid:
				raise ValueError(
					'No valid agent focus available - target may have detached and recovery failed. '
					'This indicates browser is in an unstable state.'
				)

			assert self.agent_focus_target_id is not None, 'Focus validation passed but agent_focus_target_id is None'
			target_id = self.agent_focus_target_id

		session = self.session_manager._get_session_for_target(target_id)

		if not session:
			# Session not in pool yet - wait for attach event
			self.logger.debug(f'[SessionManager] Waiting for target {target_id[:8]}... to attach...')

			# Wait up to 2 seconds for the attach event
			for attempt in range(20):
				await asyncio.sleep(0.1)
				session = self.session_manager._get_session_for_target(target_id)
				if session:
					self.logger.debug(f'[SessionManager] Target appeared after {attempt * 100}ms')
					break

			if not session:
				# Timeout - target doesn't exist
				raise ValueError(f'Target {target_id} not found - may have detached or never existed')

		# Validate session is still active
		is_valid = await self.session_manager.validate_session(target_id)
		if not is_valid:
			raise ValueError(f'Target {target_id} has detached - no active sessions')

		# Update focus if requested
		# CRITICAL: Only allow focus change to 'page' type targets, not iframes/workers
		if focus and self.agent_focus_target_id != target_id:
			# Get target type from SessionManager
			target = self.session_manager.get_target(target_id)
			target_type = target.target_type if target else 'unknown'

			if target_type == 'page':
				# Format current focus safely (could be None after detach)
				current_focus = self.agent_focus_target_id[:8] if self.agent_focus_target_id else 'None'
				self.logger.debug(f'[SessionManager] Switching focus: {current_focus}... â†’ {target_id[:8]}...')
				self.agent_focus_target_id = target_id
			else:
				# Ignore focus request for non-page targets (iframes, workers, etc.)
				# These can detach at any time, causing agent_focus to point to dead target
				current_focus = self.agent_focus_target_id[:8] if self.agent_focus_target_id else 'None'
				self.logger.debug(
					f'[SessionManager] Ignoring focus request for {target_type} target {target_id[:8]}... '
					f'(agent_focus stays on {current_focus}...)'
				)

		# Resume if waiting for debugger
		if focus:
			try:
				await session.cdp_client.send.Runtime.runIfWaitingForDebugger(session_id=session.session_id)
			except Exception:
				pass  # May fail if not waiting

		return session

	# endregion - ========== CDP-based ... ==========

	# region - ========== Helper Methods ==========
	@observe_debug(ignore_input=True, ignore_output=True, name='get_browser_state_summary')
	async def get_browser_state_summary(
		self,
		include_screenshot: bool = True,
		cached: bool = False,
		include_recent_events: bool = False,
	) -> BrowserStateSummary:
		if cached and self._cached_browser_state_summary is not None and self._cached_browser_state_summary.dom_state:
			# Don't use cached state if it has 0 interactive elements
			selector_map = self._cached_browser_state_summary.dom_state.selector_map

			# Don't use cached state if we need a screenshot but the cached state doesn't have one
			if include_screenshot and not self._cached_browser_state_summary.screenshot:
				self.logger.debug('âš ï¸ Cached browser state has no screenshot, fetching fresh state with screenshot')
				# Fall through to fetch fresh state with screenshot
			elif selector_map and len(selector_map) > 0:
				self.logger.debug('ğŸ”„ Using pre-cached browser state summary for open tab')
				return self._cached_browser_state_summary
			else:
				self.logger.debug('âš ï¸ Cached browser state has 0 interactive elements, fetching fresh state')
				# Fall through to fetch fresh state

		# Dispatch the event and wait for result
		event: BrowserStateRequestEvent = cast(
			BrowserStateRequestEvent,
			self.event_bus.dispatch(
				BrowserStateRequestEvent(
					include_dom=True,
					include_screenshot=include_screenshot,
					include_recent_events=include_recent_events,
				)
			),
		)

		# The handler returns the BrowserStateSummary directly
		result = await event.event_result(raise_if_none=True, raise_if_any=True)
		assert result is not None and result.dom_state is not None
		return result

	async def get_state_as_text(self) -> str:
		"""Get the browser state as text."""
		state = await self.get_browser_state_summary()
		assert state.dom_state is not None
		dom_state = state.dom_state
		return dom_state.llm_representation()

	async def attach_all_watchdogs(self) -> None:
		"""Initialize and attach all watchdogs with explicit handler registration."""
		# Prevent duplicate watchdog attachment
		if hasattr(self, '_watchdogs_attached') and self._watchdogs_attached:
			self.logger.debug('Watchdogs already attached, skipping duplicate attachment')
			return

		from browser_use.browser.watchdogs.aboutblank_watchdog import AboutBlankWatchdog

		# from browser_use.browser.crash_watchdog import CrashWatchdog
		from browser_use.browser.watchdogs.default_action_watchdog import DefaultActionWatchdog
		from browser_use.browser.watchdogs.dom_watchdog import DOMWatchdog
		from browser_use.browser.watchdogs.downloads_watchdog import DownloadsWatchdog
		from browser_use.browser.watchdogs.local_browser_watchdog import LocalBrowserWatchdog
		from browser_use.browser.watchdogs.permissions_watchdog import PermissionsWatchdog
		from browser_use.browser.watchdogs.popups_watchdog import PopupsWatchdog
		from browser_use.browser.watchdogs.recording_watchdog import RecordingWatchdog
		from browser_use.browser.watchdogs.screenshot_watchdog import ScreenshotWatchdog
		from browser_use.browser.watchdogs.security_watchdog import SecurityWatchdog
		from browser_use.browser.watchdogs.storage_state_watchdog import StorageStateWatchdog

		# Initialize CrashWatchdog
		# CrashWatchdog.model_rebuild()
		# self._crash_watchdog = CrashWatchdog(event_bus=self.event_bus, browser_session=self)
		# self.event_bus.on(BrowserConnectedEvent, self._crash_watchdog.on_BrowserConnectedEvent)
		# self.event_bus.on(BrowserStoppedEvent, self._crash_watchdog.on_BrowserStoppedEvent)
		# self._crash_watchdog.attach_to_session()

		# Initialize DownloadsWatchdog
		DownloadsWatchdog.model_rebuild()
		self._downloads_watchdog = DownloadsWatchdog(event_bus=self.event_bus, browser_session=self)
		# self.event_bus.on(BrowserLaunchEvent, self._downloads_watchdog.on_BrowserLaunchEvent)
		# self.event_bus.on(TabCreatedEvent, self._downloads_watchdog.on_TabCreatedEvent)
		# self.event_bus.on(TabClosedEvent, self._downloads_watchdog.on_TabClosedEvent)
		# self.event_bus.on(BrowserStoppedEvent, self._downloads_watchdog.on_BrowserStoppedEvent)
		# self.event_bus.on(NavigationCompleteEvent, self._downloads_watchdog.on_NavigationCompleteEvent)
		self._downloads_watchdog.attach_to_session()
		if self.browser_profile.auto_download_pdfs:
			self.logger.debug('ğŸ“„ PDF auto-download enabled for this session')

		# Initialize StorageStateWatchdog conditionally
		# Enable when user provides either storage_state or user_data_dir (indicating they want persistence)
		should_enable_storage_state = (
			self.browser_profile.storage_state is not None or self.browser_profile.user_data_dir is not None
		)

		if should_enable_storage_state:
			StorageStateWatchdog.model_rebuild()
			self._storage_state_watchdog = StorageStateWatchdog(
				event_bus=self.event_bus,
				browser_session=self,
				# More conservative defaults when auto-enabled
				auto_save_interval=60.0,  # 1 minute instead of 30 seconds
				save_on_change=False,  # Only save on shutdown by default
			)
			self._storage_state_watchdog.attach_to_session()
			self.logger.debug(
				f'ğŸª StorageStateWatchdog enabled (storage_state: {bool(self.browser_profile.storage_state)}, user_data_dir: {bool(self.browser_profile.user_data_dir)})'
			)
		else:
			self.logger.debug('ğŸª StorageStateWatchdog disabled (no storage_state or user_data_dir configured)')

		# Initialize LocalBrowserWatchdog
		LocalBrowserWatchdog.model_rebuild()
		self._local_browser_watchdog = LocalBrowserWatchdog(event_bus=self.event_bus, browser_session=self)
		# self.event_bus.on(BrowserLaunchEvent, self._local_browser_watchdog.on_BrowserLaunchEvent)
		# self.event_bus.on(BrowserKillEvent, self._local_browser_watchdog.on_BrowserKillEvent)
		# self.event_bus.on(BrowserStopEvent, self._local_browser_watchdog.on_BrowserStopEvent)
		self._local_browser_watchdog.attach_to_session()

		# Initialize SecurityWatchdog (hooks NavigationWatchdog and implements allowed_domains restriction)
		SecurityWatchdog.model_rebuild()
		self._security_watchdog = SecurityWatchdog(event_bus=self.event_bus, browser_session=self)
		# Core navigation is now handled in BrowserSession directly
		# SecurityWatchdog only handles security policy enforcement
		self._security_watchdog.attach_to_session()

		# Initialize AboutBlankWatchdog (handles about:blank pages and DVD loading animation on first load)
		AboutBlankWatchdog.model_rebuild()
		self._aboutblank_watchdog = AboutBlankWatchdog(event_bus=self.event_bus, browser_session=self)
		# self.event_bus.on(BrowserStopEvent, self._aboutblank_watchdog.on_BrowserStopEvent)
		# self.event_bus.on(BrowserStoppedEvent, self._aboutblank_watchdog.on_BrowserStoppedEvent)
		# self.event_bus.on(TabCreatedEvent, self._aboutblank_watchdog.on_TabCreatedEvent)
		# self.event_bus.on(TabClosedEvent, self._aboutblank_watchdog.on_TabClosedEvent)
		self._aboutblank_watchdog.attach_to_session()

		# Initialize PopupsWatchdog (handles accepting and dismissing JS dialogs, alerts, confirm, onbeforeunload, etc.)
		PopupsWatchdog.model_rebuild()
		self._popups_watchdog = PopupsWatchdog(event_bus=self.event_bus, browser_session=self)
		# self.event_bus.on(TabCreatedEvent, self._popups_watchdog.on_TabCreatedEvent)
		# self.event_bus.on(DialogCloseEvent, self._popups_watchdog.on_DialogCloseEvent)
		self._popups_watchdog.attach_to_session()

		# Initialize PermissionsWatchdog (handles granting and revoking browser permissions like clipboard, microphone, camera, etc.)
		PermissionsWatchdog.model_rebuild()
		self._permissions_watchdog = PermissionsWatchdog(event_bus=self.event_bus, browser_session=self)
		# self.event_bus.on(BrowserConnectedEvent, self._permissions_watchdog.on_BrowserConnectedEvent)
		self._permissions_watchdog.attach_to_session()

		# Initialize DefaultActionWatchdog (handles all default actions like click, type, scroll, go back, go forward, refresh, wait, send keys, upload file, scroll to text, etc.)
		DefaultActionWatchdog.model_rebuild()
		self._default_action_watchdog = DefaultActionWatchdog(event_bus=self.event_bus, browser_session=self)
		# self.event_bus.on(ClickElementEvent, self._default_action_watchdog.on_ClickElementEvent)
		# self.event_bus.on(TypeTextEvent, self._default_action_watchdog.on_TypeTextEvent)
		# self.event_bus.on(ScrollEvent, self._default_action_watchdog.on_ScrollEvent)
		# self.event_bus.on(GoBackEvent, self._default_action_watchdog.on_GoBackEvent)
		# self.event_bus.on(GoForwardEvent, self._default_action_watchdog.on_GoForwardEvent)
		# self.event_bus.on(RefreshEvent, self._default_action_watchdog.on_RefreshEvent)
		# self.event_bus.on(WaitEvent, self._default_action_watchdog.on_WaitEvent)
		# self.event_bus.on(SendKeysEvent, self._default_action_watchdog.on_SendKeysEvent)
		# self.event_bus.on(UploadFileEvent, self._default_action_watchdog.on_UploadFileEvent)
		# self.event_bus.on(ScrollToTextEvent, self._default_action_watchdog.on_ScrollToTextEvent)
		self._default_action_watchdog.attach_to_session()

		# Initialize ScreenshotWatchdog (handles taking screenshots of the browser)
		ScreenshotWatchdog.model_rebuild()
		self._screenshot_watchdog = ScreenshotWatchdog(event_bus=self.event_bus, browser_session=self)
		# self.event_bus.on(BrowserStartEvent, self._screenshot_watchdog.on_BrowserStartEvent)
		# self.event_bus.on(BrowserStoppedEvent, self._screenshot_watchdog.on_BrowserStoppedEvent)
		# self.event_bus.on(ScreenshotEvent, self._screenshot_watchdog.on_ScreenshotEvent)
		self._screenshot_watchdog.attach_to_session()

		# Initialize DOMWatchdog (handles building the DOM tree and detecting interactive elements, depends on ScreenshotWatchdog)
		DOMWatchdog.model_rebuild()
		self._dom_watchdog = DOMWatchdog(event_bus=self.event_bus, browser_session=self)
		# self.event_bus.on(TabCreatedEvent, self._dom_watchdog.on_TabCreatedEvent)
		# self.event_bus.on(BrowserStateRequestEvent, self._dom_watchdog.on_BrowserStateRequestEvent)
		self._dom_watchdog.attach_to_session()

		# Initialize RecordingWatchdog (handles video recording)
		RecordingWatchdog.model_rebuild()
		self._recording_watchdog = RecordingWatchdog(event_bus=self.event_bus, browser_session=self)
		self._recording_watchdog.attach_to_session()

		# Mark watchdogs as attached to prevent duplicate attachment
		self._watchdogs_attached = True

	async def connect(self, cdp_url: str | None = None) -> Self:
		"""Connect to a remote chromium-based browser via CDP using cdp-use.

		This MUST succeed or the browser is unusable. Fails hard on any error.
		"""

		self.browser_profile.cdp_url = cdp_url or self.cdp_url
		if not self.cdp_url:
			raise RuntimeError('Cannot setup CDP connection without CDP URL')

		# Prevent duplicate connections - clean up existing connection first
		if self._cdp_client_root is not None:
			self.logger.warning(
				'âš ï¸ connect() called but CDP client already exists! Cleaning up old connection before creating new one.'
			)
			try:
				await self._cdp_client_root.stop()
			except Exception as e:
				self.logger.debug(f'Error stopping old CDP client: {e}')
			self._cdp_client_root = None

		if not self.cdp_url.startswith('ws'):
			# If it's an HTTP URL, fetch the WebSocket URL from /json/version endpoint
			parsed_url = urlparse(self.cdp_url)
			path = parsed_url.path.rstrip('/')

			if not path.endswith('/json/version'):
				path = path + '/json/version'

			url = urlunparse(
				(parsed_url.scheme, parsed_url.netloc, path, parsed_url.params, parsed_url.query, parsed_url.fragment)
			)

			# Run a tiny HTTP client to query for the WebSocket URL from the /json/version endpoint
			async with httpx.AsyncClient() as client:
				headers = self.browser_profile.headers or {}
				version_info = await client.get(url, headers=headers)
				self.logger.debug(f'Raw version info: {str(version_info)}')
				self.browser_profile.cdp_url = version_info.json()['webSocketDebuggerUrl']

		assert self.cdp_url is not None, 'CDP URL is None.'

		browser_location = 'local browser' if self.is_local else 'remote browser'
		self.logger.debug(f'ğŸŒ Connecting to existing chromium-based browser via CDP: {self.cdp_url} -> ({browser_location})')

		try:
			# Create and store the CDP client for direct CDP communication
			headers = getattr(self.browser_profile, 'headers', None)
			self._cdp_client_root = CDPClient(
				self.cdp_url,
				additional_headers=headers,
				max_ws_frame_size=200 * 1024 * 1024,  # Use 200MB limit to handle pages with very large DOMs
			)
			assert self._cdp_client_root is not None
			await self._cdp_client_root.start()

			# Initialize event-driven session manager FIRST (before enabling autoAttach)
			# SessionManager will:
			# 1. Register attach/detach event handlers
			# 2. Discover and attach to all existing targets
			# 3. Initialize sessions and enable lifecycle monitoring
			# 4. Enable autoAttach for future targets
			from browser_use.browser.session_manager import SessionManager

			self.session_manager = SessionManager(self)
			await self.session_manager.start_monitoring()
			self.logger.debug('Event-driven session manager started')

			# Enable auto-attach so Chrome automatically notifies us when NEW targets attach/detach
			# This is the foundation of event-driven session management
			await self._cdp_client_root.send.Target.setAutoAttach(
				params={'autoAttach': True, 'waitForDebuggerOnStart': False, 'flatten': True}
			)
			self.logger.debug('CDP client connected with auto-attach enabled')

			# Get browser targets from SessionManager (source of truth)
			# SessionManager has already discovered all targets via start_monitoring()
			page_targets_from_manager = self.session_manager.get_all_page_targets()

			# Check for chrome://newtab pages and redirect them to about:blank
			from browser_use.utils import is_new_tab_page

			for target in page_targets_from_manager:
				target_url = target.url
				if is_new_tab_page(target_url) and target_url != 'about:blank':
					target_id = target.target_id
					self.logger.debug(f'ğŸ”„ Redirecting {target_url} to about:blank for target {target_id}')
					try:
						# Use public API with focus=False to avoid changing focus during init
						session = await self.get_or_create_cdp_session(target_id, focus=False)
						await session.cdp_client.send.Page.navigate(params={'url': 'about:blank'}, session_id=session.session_id)
						# Update target url
						target.url = 'about:blank'
					except Exception as e:
						self.logger.warning(f'Failed to redirect {target_url}: {e}')

			# Ensure we have at least one page
			if not page_targets_from_manager:
				new_target = await self._cdp_client_root.send.Target.createTarget(params={'url': 'about:blank'})
				target_id = new_target['targetId']
				self.logger.debug(f'ğŸ“„ Created new blank page: {target_id}')
			else:
				target_id = page_targets_from_manager[0].target_id
				self.logger.debug(f'ğŸ“„ Using existing page: {target_id}')

			# Set up initial focus using the public API
			# Note: get_or_create_cdp_session() will wait for attach event and set focus
			try:
				await self.get_or_create_cdp_session(target_id, focus=True)
				# agent_focus_target_id is now set by get_or_create_cdp_session
				self.logger.debug(f'ğŸ“„ Agent focus set to {target_id[:8]}...')
			except ValueError as e:
				raise RuntimeError(f'Failed to get session for initial target {target_id}: {e}') from e

			# Note: Lifecycle monitoring is enabled automatically in SessionManager._handle_target_attached()
			# when targets attach, so no manual enablement needed!

			# Enable proxy authentication handling if configured
			await self._setup_proxy_auth()

			# Verify the target is working
			if self.agent_focus_target_id:
				target = self.session_manager.get_target(self.agent_focus_target_id)
				if target.title == 'Unknown title':
					self.logger.warning('Target created but title is unknown (may be normal for about:blank)')

			# Dispatch TabCreatedEvent for all initial tabs (so watchdogs can initialize)
			for idx, target in enumerate(page_targets_from_manager):
				target_url = target.url
				self.logger.debug(f'Dispatching TabCreatedEvent for initial tab {idx}: {target_url}')
				self.event_bus.dispatch(TabCreatedEvent(url=target_url, target_id=target.target_id))

			# Dispatch initial focus event
			if page_targets_from_manager:
				initial_url = page_targets_from_manager[0].url
				self.event_bus.dispatch(AgentFocusChangedEvent(target_id=page_targets_from_manager[0].target_id, url=initial_url))
				self.logger.debug(f'Initial agent focus set to tab 0: {initial_url}')

		except Exception as e:
			# Fatal error - browser is not usable without CDP connection
			self.logger.error(f'âŒ FATAL: Failed to setup CDP connection: {e}')
			self.logger.error('âŒ Browser cannot continue without CDP connection')

			# Clear SessionManager state
			if self.session_manager:
				try:
					await self.session_manager.clear()
					self.logger.debug('Cleared SessionManager state after initialization failure')
				except Exception as cleanup_error:
					self.logger.debug(f'Error clearing SessionManager: {cleanup_error}')

			# Close CDP client WebSocket and unregister handlers
			if self._cdp_client_root:
				try:
					await self._cdp_client_root.stop()  # Close WebSocket and unregister handlers
					self.logger.debug('Closed CDP client WebSocket after initialization failure')
				except Exception as cleanup_error:
					self.logger.debug(f'Error closing CDP client: {cleanup_error}')

			self.session_manager = None
			self._cdp_client_root = None
			self.agent_focus_target_id = None
			# Re-raise as a fatal error
			raise RuntimeError(f'Failed to establish CDP connection to browser: {e}') from e

		return self

	async def _setup_proxy_auth(self) -> None:
		"""Enable CDP Fetch auth handling for authenticated proxy, if credentials provided.

		Handles HTTP proxy authentication challenges (Basic/Proxy) by providing
		configured credentials from BrowserProfile.
		"""

		assert self._cdp_client_root

		try:
			proxy_cfg = self.browser_profile.proxy
			username = proxy_cfg.username if proxy_cfg else None
			password = proxy_cfg.password if proxy_cfg else None
			if not username or not password:
				self.logger.debug('Proxy credentials not provided; skipping proxy auth setup')
				return

			# Enable Fetch domain with auth handling (do not pause all requests)
			try:
				await self._cdp_client_root.send.Fetch.enable(params={'handleAuthRequests': True})
				self.logger.debug('Fetch.enable(handleAuthRequests=True) enabled on root client')
			except Exception as e:
				self.logger.debug(f'Fetch.enable on root failed: {type(e).__name__}: {e}')

			# Also enable on the focused target's session if available to ensure events are delivered
			try:
				if self.agent_focus_target_id:
					cdp_session = await self.get_or_create_cdp_session(self.agent_focus_target_id, focus=False)
					await cdp_session.cdp_client.send.Fetch.enable(
						params={'handleAuthRequests': True},
						session_id=cdp_session.session_id,
					)
					self.logger.debug('Fetch.enable(handleAuthRequests=True) enabled on focused session')
			except Exception as e:
				self.logger.debug(f'Fetch.enable on focused session failed: {type(e).__name__}: {e}')

			def _on_auth_required(event: AuthRequiredEvent, session_id: SessionID | None = None):
				# event keys may be snake_case or camelCase depending on generator; handle both
				request_id = event.get('requestId') or event.get('request_id')
				if not request_id:
					return

				challenge = event.get('authChallenge') or event.get('auth_challenge') or {}
				source = (challenge.get('source') or '').lower()
				# Only respond to proxy challenges
				if source == 'proxy' and request_id:

					async def _respond():
						assert self._cdp_client_root
						try:
							await self._cdp_client_root.send.Fetch.continueWithAuth(
								params={
									'requestId': request_id,
									'authChallengeResponse': {
										'response': 'ProvideCredentials',
										'username': username,
										'password': password,
									},
								},
								session_id=session_id,
							)
						except Exception as e:
							self.logger.debug(f'Proxy auth respond failed: {type(e).__name__}: {e}')

					# schedule
					create_task_with_error_handling(
						_respond(), name='auth_respond', logger_instance=self.logger, suppress_exceptions=True
					)
				else:
					# Default behaviour for non-proxy challenges: let browser handle
					async def _default():
						assert self._cdp_client_root
						try:
							await self._cdp_client_root.send.Fetch.continueWithAuth(
								params={'requestId': request_id, 'authChallengeResponse': {'response': 'Default'}},
								session_id=session_id,
							)
						except Exception as e:
							self.logger.debug(f'Default auth respond failed: {type(e).__name__}: {e}')

					if request_id:
						create_task_with_error_handling(
							_default(), name='auth_default', logger_instance=self.logger, suppress_exceptions=True
						)

			def _on_request_paused(event: RequestPausedEvent, session_id: SessionID | None = None):
				# Continue all paused requests to avoid stalling the network
				request_id = event.get('requestId') or event.get('request_id')
				if not request_id:
					return

				async def _continue():
					assert self._cdp_client_root
					try:
						await self._cdp_client_root.send.Fetch.continueRequest(
							params={'requestId': request_id},
							session_id=session_id,
						)
					except Exception:
						pass

				create_task_with_error_handling(
					_continue(), name='request_continue', logger_instance=self.logger, suppress_exceptions=True
				)

			# Register event handler on root client
			try:
				self._cdp_client_root.register.Fetch.authRequired(_on_auth_required)
				self._cdp_client_root.register.Fetch.requestPaused(_on_request_paused)
				if self.agent_focus_target_id:
					cdp_session = await self.get_or_create_cdp_session(self.agent_focus_target_id, focus=False)
					cdp_session.cdp_client.register.Fetch.authRequired(_on_auth_required)
					cdp_session.cdp_client.register.Fetch.requestPaused(_on_request_paused)
				self.logger.debug('Registered Fetch.authRequired handlers')
			except Exception as e:
				self.logger.debug(f'Failed to register authRequired handlers: {type(e).__name__}: {e}')

			# Auto-enable Fetch on every newly attached target to ensure auth callbacks fire
			def _on_attached(event: AttachedToTargetEvent, session_id: SessionID | None = None):
				sid = event.get('sessionId') or event.get('session_id') or session_id
				if not sid:
					return

				async def _enable():
					assert self._cdp_client_root
					try:
						await self._cdp_client_root.send.Fetch.enable(
							params={'handleAuthRequests': True},
							session_id=sid,
						)
						self.logger.debug(f'Fetch.enable(handleAuthRequests=True) enabled on attached session {sid}')
					except Exception as e:
						self.logger.debug(f'Fetch.enable on attached session failed: {type(e).__name__}: {e}')

				create_task_with_error_handling(
					_enable(), name='fetch_enable_attached', logger_instance=self.logger, suppress_exceptions=True
				)

			try:
				self._cdp_client_root.register.Target.attachedToTarget(_on_attached)
				self.logger.debug('Registered Target.attachedToTarget handler for Fetch.enable')
			except Exception as e:
				self.logger.debug(f'Failed to register attachedToTarget handler: {type(e).__name__}: {e}')

			# Ensure Fetch is enabled for the current focused target's session, too
			try:
				if self.agent_focus_target_id:
					# Use safe API with focus=False to avoid changing focus
					cdp_session = await self.get_or_create_cdp_session(self.agent_focus_target_id, focus=False)
					await cdp_session.cdp_client.send.Fetch.enable(
						params={'handleAuthRequests': True, 'patterns': [{'urlPattern': '*'}]},
						session_id=cdp_session.session_id,
					)
			except Exception as e:
				self.logger.debug(f'Fetch.enable on focused session failed: {type(e).__name__}: {e}')
		except Exception as e:
			self.logger.debug(f'Skipping proxy auth setup: {type(e).__name__}: {e}')

	async def get_tabs(self) -> list[TabInfo]:
		"""Get information about all open tabs using cached target data."""
		tabs = []

		# Safety check - return empty list if browser not connected yet
		if not self.session_manager:
			return tabs

		# Get all page targets from SessionManager
		page_targets = self.session_manager.get_all_page_targets()

		for i, target in enumerate(page_targets):
			target_id = target.target_id
			url = target.url
			title = target.title

			try:
				# Skip JS execution for chrome:// pages and new tab pages
				if is_new_tab_page(url) or url.startswith('chrome://'):
					# Use URL as title for chrome pages, or mark new tabs as unusable
					if is_new_tab_page(url):
						title = ''
					elif not title:
						# For chrome:// pages without a title, use the URL itself
						title = url

				# Special handling for PDF pages without titles
				if (not title or title == '') and (url.endswith('.pdf') or 'pdf' in url):
					# PDF pages might not have a title, use URL filename
					try:
						from urllib.parse import urlparse

						filename = urlparse(url).path.split('/')[-1]
						if filename:
							title = filename
					except Exception:
						pass

			except Exception as e:
				# Fallback to basic title handling
				self.logger.debug(f'âš ï¸ Failed to get target info for tab #{i}: {_log_pretty_url(url)} - {type(e).__name__}')

				if is_new_tab_page(url):
					title = ''
				elif url.startswith('chrome://'):
					title = url
				else:
					title = ''

			tab_info = TabInfo(
				target_id=target_id,
				url=url,
				title=title,
				parent_target_id=None,
			)
			tabs.append(tab_info)

		return tabs

	# endregion - ========== Helper Methods ==========

	# region - ========== ID Lookup Methods ==========
	async def get_current_target_info(self) -> TargetInfo | None:
		"""Get info about the current active target using cached session data."""
		if not self.agent_focus_target_id:
			return None

		target = self.session_manager.get_target(self.agent_focus_target_id)

		return {
			'targetId': target.target_id,
			'url': target.url,
			'title': target.title,
			'type': target.target_type,
			'attached': True,
			'canAccessOpener': False,
		}

	async def get_current_page_url(self) -> str:
		"""Get the URL of the current page."""
		if self.agent_focus_target_id:
			target = self.session_manager.get_target(self.agent_focus_target_id)
			return target.url
		return 'about:blank'

	async def get_current_page_title(self) -> str:
		"""Get the title of the current page."""
		if self.agent_focus_target_id:
			target = self.session_manager.get_target(self.agent_focus_target_id)
			return target.title
		return 'Unknown page title'

	async def navigate_to(self, url: str, new_tab: bool = False) -> None:
		"""Navigate to a URL using the standard event system.

		Args:
			url: URL to navigate to
			new_tab: Whether to open in a new tab
		"""
		from browser_use.browser.events import NavigateToUrlEvent

		event = self.event_bus.dispatch(NavigateToUrlEvent(url=url, new_tab=new_tab))
		await event
		await event.event_result(raise_if_any=True, raise_if_none=False)

	# endregion - ========== ID Lookup Methods ==========

	# region - ========== DOM Helper Methods ==========

	async def get_dom_element_by_index(self, index: int) -> EnhancedDOMTreeNode | None:
		"""Get DOM element by index.

		Get element from cached selector map.

		Args:
			index: The element index from the serialized DOM

		Returns:
			EnhancedDOMTreeNode or None if index not found
		"""
		#  Check cached selector map
		if self._cached_selector_map and index in self._cached_selector_map:
			return self._cached_selector_map[index]

		return None

	def update_cached_selector_map(self, selector_map: dict[int, EnhancedDOMTreeNode]) -> None:
		"""Update the cached selector map with new DOM state.

		This should be called by the DOM watchdog after rebuilding the DOM.

		Args:
			selector_map: The new selector map from DOM serialization
		"""
		self._cached_selector_map = selector_map

	# Alias for backwards compatibility
	async def get_element_by_index(self, index: int) -> EnhancedDOMTreeNode | None:
		"""Alias for get_dom_element_by_index for backwards compatibility."""
		return await self.get_dom_element_by_index(index)

	async def get_dom_element_at_coordinates(self, x: int, y: int) -> EnhancedDOMTreeNode | None:
		"""Get DOM element at coordinates as EnhancedDOMTreeNode.

		First checks the cached selector_map for a matching element, then falls back
		to CDP DOM.describeNode if not found. This ensures safety checks (e.g., for
		<select> elements and file inputs) work correctly.

		Args:
			x: X coordinate relative to viewport
			y: Y coordinate relative to viewport

		Returns:
			EnhancedDOMTreeNode at the coordinates, or None if no element found
		"""
		from browser_use.dom.views import NodeType

		# Get current page to access CDP session
		page = await self.get_current_page()
		if page is None:
			raise RuntimeError('No active page found')

		# Get session ID for CDP call
		session_id = await page._ensure_session()

		try:
			# Call CDP DOM.getNodeForLocation to get backend_node_id
			result = await self.cdp_client.send.DOM.getNodeForLocation(
				params={
					'x': x,
					'y': y,
					'includeUserAgentShadowDOM': False,
					'ignorePointerEventsNone': False,
				},
				session_id=session_id,
			)

			backend_node_id = result.get('backendNodeId')
			if backend_node_id is None:
				self.logger.debug(f'No element found at coordinates ({x}, {y})')
				return None

			# Try to find element in cached selector_map (avoids extra CDP call)
			if self._cached_selector_map:
				for node in self._cached_selector_map.values():
					if node.backend_node_id == backend_node_id:
						self.logger.debug(f'Found element at ({x}, {y}) in cached selector_map')
						return node

			# Not in cache - fall back to CDP DOM.describeNode to get actual node info
			try:
				describe_result = await self.cdp_client.send.DOM.describeNode(
					params={'backendNodeId': backend_node_id},
					session_id=session_id,
				)
				node_info = describe_result.get('node', {})
				node_name = node_info.get('nodeName', '')

				# Parse attributes from flat list [key1, val1, key2, val2, ...] to dict
				attrs_list = node_info.get('attributes', [])
				attributes = {attrs_list[i]: attrs_list[i + 1] for i in range(0, len(attrs_list), 2)}

				return EnhancedDOMTreeNode(
					node_id=result.get('nodeId', 0),
					backend_node_id=backend_node_id,
					node_type=NodeType(node_info.get('nodeType', NodeType.ELEMENT_NODE.value)),
					node_name=node_name,
					node_value=node_info.get('nodeValue', '') or '',
					attributes=attributes,
					is_scrollable=None,
					frame_id=result.get('frameId'),
					session_id=session_id,
					target_id=self.agent_focus_target_id or '',
					content_document=None,
					shadow_root_type=None,
					shadow_roots=None,
					parent_node=None,
					children_nodes=None,
					ax_node=None,
					snapshot_node=None,
					is_visible=None,
					absolute_position=None,
				)
			except Exception as e:
				self.logger.debug(f'DOM.describeNode failed for backend_node_id={backend_node_id}: {e}')
				# Fall back to minimal node if describeNode fails
				return EnhancedDOMTreeNode(
					node_id=result.get('nodeId', 0),
					backend_node_id=backend_node_id,
					node_type=NodeType.ELEMENT_NODE,
					node_name='',
					node_value='',
					attributes={},
					is_scrollable=None,
					frame_id=result.get('frameId'),
					session_id=session_id,
					target_id=self.agent_focus_target_id or '',
					content_document=None,
					shadow_root_type=None,
					shadow_roots=None,
					parent_node=None,
					children_nodes=None,
					ax_node=None,
					snapshot_node=None,
					is_visible=None,
					absolute_position=None,
				)

		except Exception as e:
			self.logger.warning(f'Failed to get DOM element at coordinates ({x}, {y}): {e}')
			return None

	async def get_target_id_from_tab_id(self, tab_id: str) -> TargetID:
		"""Get the full-length TargetID from the truncated 4-char tab_id using SessionManager."""
		if not self.session_manager:
			raise RuntimeError('SessionManager not initialized')

		for full_target_id in self.session_manager.get_all_target_ids():
			if full_target_id.endswith(tab_id):
				if await self.session_manager.is_target_valid(full_target_id):
					return full_target_id
				# Stale target - Chrome should have sent detach event
				# If we're here, event listener will clean it up
				self.logger.debug(f'Found stale target {full_target_id}, skipping')

		raise ValueError(f'No TargetID found ending in tab_id=...{tab_id}')

	async def get_target_id_from_url(self, url: str) -> TargetID:
		"""Get the TargetID from a URL using SessionManager (source of truth)."""
		if not self.session_manager:
			raise RuntimeError('SessionManager not initialized')

		# Search in SessionManager targets (exact match first)
		for target_id, target in self.session_manager.get_all_targets().items():
			if target.target_type in ('page', 'tab') and target.url == url:
				return target_id

		# Still not found, try substring match as fallback
		for target_id, target in self.session_manager.get_all_targets().items():
			if target.target_type in ('page', 'tab') and url in target.url:
				return target_id

		raise ValueError(f'No TargetID found for url={url}')

	async def get_most_recently_opened_target_id(self) -> TargetID:
		"""Get the most recently opened target ID using SessionManager."""
		# Get all page targets from SessionManager
		page_targets = self.session_manager.get_all_page_targets()
		if not page_targets:
			raise RuntimeError('No page targets available')
		return page_targets[-1].target_id

	def is_file_input(self, element: Any) -> bool:
		"""Check if element is a file input.

		Args:
			element: The DOM element to check

		Returns:
			True if element is a file input, False otherwise
		"""
		if self._dom_watchdog:
			return self._dom_watchdog.is_file_input(element)
		# Fallback if watchdog not available
		return (
			hasattr(element, 'node_name')
			and element.node_name.upper() == 'INPUT'
			and hasattr(element, 'attributes')
			and element.attributes.get('type', '').lower() == 'file'
		)

	async def get_selector_map(self) -> dict[int, EnhancedDOMTreeNode]:
		"""Get the current selector map from cached state or DOM watchdog.

		Returns:
			Dictionary mapping element indices to EnhancedDOMTreeNode objects
		"""
		# First try cached selector map
		if self._cached_selector_map:
			return self._cached_selector_map

		# Try to get from DOM watchdog
		if self._dom_watchdog and hasattr(self._dom_watchdog, 'selector_map'):
			return self._dom_watchdog.selector_map or {}

		# Return empty dict if nothing available
		return {}

	async def get_index_by_id(self, element_id: str) -> int | None:
		"""Find element index by its id attribute.

		Args:
			element_id: The id attribute value to search for

		Returns:
			Index of the element, or None if not found
		"""
		selector_map = await self.get_selector_map()
		for idx, element in selector_map.items():
			if element.attributes and element.attributes.get('id') == element_id:
				return idx
		return None

	async def get_index_by_class(self, class_name: str) -> int | None:
		"""Find element index by its class attribute (matches if class contains the given name).

		Args:
			class_name: The class name to search for

		Returns:
			Index of the first matching element, or None if not found
		"""
		selector_map = await self.get_selector_map()
		for idx, element in selector_map.items():
			if element.attributes:
				element_class = element.attributes.get('class', '')
				if class_name in element_class.split():
					return idx
		return None

	async def remove_highlights(self) -> None:
		"""Remove highlights from the page using CDP."""
		if not self.browser_profile.highlight_elements:
			return

		try:
			# Get cached session
			cdp_session = await self.get_or_create_cdp_session()

			# Remove highlights via JavaScript - be thorough
			script = """
			(function() {
				// Remove all browser-use highlight elements
				const highlights = document.querySelectorAll('[data-browser-use-highlight]');
				console.log('Removing', highlights.length, 'browser-use highlight elements');
				highlights.forEach(el => el.remove());

				// Also remove by ID in case selector missed anything
				const highlightContainer = document.getElementById('browser-use-debug-highlights');
				if (highlightContainer) {
					console.log('Removing highlight container by ID');
					highlightContainer.remove();
				}

				// Final cleanup - remove any orphaned tooltips
				const orphanedTooltips = document.querySelectorAll('[data-browser-use-highlight="tooltip"]');
				orphanedTooltips.forEach(el => el.remove());

				return { removed: highlights.length };
			})();
			"""
			result = await cdp_session.cdp_client.send.Runtime.evaluate(
				params={'expression': script, 'returnByValue': True}, session_id=cdp_session.session_id
			)

			# Log the result for debugging
			if result and 'result' in result and 'value' in result['result']:
				removed_count = result['result']['value'].get('removed', 0)
				self.logger.debug(f'Successfully removed {removed_count} highlight elements')
			else:
				self.logger.debug('Highlight removal completed')

		except Exception as e:
			self.logger.warning(f'Failed to remove highlights: {e}')

	@observe_debug(ignore_input=True, ignore_output=True, name='get_element_coordinates')
	async def get_element_coordinates(self, backend_node_id: int, cdp_session: CDPSession) -> DOMRect | None:
		"""Get element coordinates for a backend node ID using multiple methods.

		This method tries DOM.getContentQuads first, then falls back to DOM.getBoxModel,
		and finally uses JavaScript getBoundingClientRect as a last resort.

		Args:
			backend_node_id: The backend node ID to get coordinates for
			cdp_session: The CDP session to use

		Returns:
			DOMRect with coordinates or None if element not found/no bounds
		"""
		session_id = cdp_session.session_id
		quads = []

		# Method 1: Try DOM.getContentQuads first (best for inline elements and complex layouts)
		try:
			content_quads_result = await cdp_session.cdp_client.send.DOM.getContentQuads(
				params={'backendNodeId': backend_node_id}, session_id=session_id
			)
			if 'quads' in content_quads_result and content_quads_result['quads']:
				quads = content_quads_result['quads']
				self.logger.debug(f'Got {len(quads)} quads from DOM.getContentQuads')
			else:
				self.logger.debug(f'No quads found from DOM.getContentQuads {content_quads_result}')
		except Exception as e:
			self.logger.debug(f'DOM.getContentQuads failed: {e}')

		# Method 2: Fall back to DOM.getBoxModel
		if not quads:
			try:
				box_model = await cdp_session.cdp_client.send.DOM.getBoxModel(
					params={'backendNodeId': backend_node_id}, session_id=session_id
				)
				if 'model' in box_model and 'content' in box_model['model']:
					content_quad = box_model['model']['content']
					if len(content_quad) >= 8:
						# Convert box model format to quad format
						quads = [
							[
								content_quad[0],
								content_quad[1],  # x1, y1
								content_quad[2],
								content_quad[3],  # x2, y2
								content_quad[4],
								content_quad[5],  # x3, y3
								content_quad[6],
								content_quad[7],  # x4, y4
							]
						]
						self.logger.debug('Got quad from DOM.getBoxModel')
			except Exception as e:
				self.logger.debug(f'DOM.getBoxModel failed: {e}')

		# Method 3: Fall back to JavaScript getBoundingClientRect
		if not quads:
			try:
				result = await cdp_session.cdp_client.send.DOM.resolveNode(
					params={'backendNodeId': backend_node_id},
					session_id=session_id,
				)
				if 'object' in result and 'objectId' in result['object']:
					object_id = result['object']['objectId']
					js_result = await cdp_session.cdp_client.send.Runtime.callFunctionOn(
						params={
							'objectId': object_id,
							'functionDeclaration': """
							function() {
								const rect = this.getBoundingClientRect();
								return {
									x: rect.x,
									y: rect.y,
									width: rect.width,
									height: rect.height
								};
							}
							""",
							'returnByValue': True,
						},
						session_id=session_id,
					)
					if 'result' in js_result and 'value' in js_result['result']:
						rect_data = js_result['result']['value']
						if rect_data['width'] > 0 and rect_data['height'] > 0:
							return DOMRect(
								x=rect_data['x'], y=rect_data['y'], width=rect_data['width'], height=rect_data['height']
							)
			except Exception as e:
				self.logger.debug(f'JavaScript getBoundingClientRect failed: {e}')

		# Convert quads to bounding rectangle if we have them
		if quads:
			# Use the first quad (most relevant for the element)
			quad = quads[0]
			if len(quad) >= 8:
				# Calculate bounding rect from quad points
				x_coords = [quad[i] for i in range(0, 8, 2)]
				y_coords = [quad[i] for i in range(1, 8, 2)]

				min_x = min(x_coords)
				min_y = min(y_coords)
				max_x = max(x_coords)
				max_y = max(y_coords)

				width = max_x - min_x
				height = max_y - min_y

				if width > 0 and height > 0:
					return DOMRect(x=min_x, y=min_y, width=width, height=height)

		return None

	async def highlight_interaction_element(self, node: 'EnhancedDOMTreeNode') -> None:
		"""Temporarily highlight an element during interaction for user visibility.

		This creates a visual highlight on the browser that shows the user which element
		is being interacted with. The highlight automatically fades after the configured duration.

		Args:
			node: The DOM node to highlight with backend_node_id for coordinate lookup
		"""
		if not self.browser_profile.highlight_elements:
			return

		try:
			import json

			cdp_session = await self.get_or_create_cdp_session()

			# Get current coordinates
			rect = await self.get_element_coordinates(node.backend_node_id, cdp_session)

			color = self.browser_profile.interaction_highlight_color
			duration_ms = int(self.browser_profile.interaction_highlight_duration * 1000)

			if not rect:
				self.logger.debug(f'No coordinates found for backend node {node.backend_node_id}')
				return

			# Create animated corner brackets that start offset and animate inward
			script = f"""
			(function() {{
				const rect = {json.dumps({'x': rect.x, 'y': rect.y, 'width': rect.width, 'height': rect.height})};
				const color = {json.dumps(color)};
				const duration = {duration_ms};

				// Scale corner size based on element dimensions to ensure gaps between corners
				const maxCornerSize = 20;
				const minCornerSize = 8;
				const cornerSize = Math.max(
					minCornerSize,
					Math.min(maxCornerSize, Math.min(rect.width, rect.height) * 0.35)
				);
				const borderWidth = 3;
				const startOffset = 10; // Starting offset in pixels
				const finalOffset = -3; // Final position slightly outside the element

				// Get current scroll position
				const scrollX = window.pageXOffset || document.documentElement.scrollLeft || 0;
				const scrollY = window.pageYOffset || document.documentElement.scrollTop || 0;

				// Create container for all corners
				const container = document.createElement('div');
				container.setAttribute('data-browser-use-interaction-highlight', 'true');
				container.style.cssText = `
					position: absolute;
					left: ${{rect.x + scrollX}}px;
					top: ${{rect.y + scrollY}}px;
					width: ${{rect.width}}px;
					height: ${{rect.height}}px;
					pointer-events: none;
					z-index: 2147483647;
				`;

				// Create 4 corner brackets
				const corners = [
					{{ pos: 'top-left', startX: -startOffset, startY: -startOffset, finalX: finalOffset, finalY: finalOffset }},
					{{ pos: 'top-right', startX: startOffset, startY: -startOffset, finalX: -finalOffset, finalY: finalOffset }},
					{{ pos: 'bottom-left', startX: -startOffset, startY: startOffset, finalX: finalOffset, finalY: -finalOffset }},
					{{ pos: 'bottom-right', startX: startOffset, startY: startOffset, finalX: -finalOffset, finalY: -finalOffset }}
				];

				corners.forEach(corner => {{
					const bracket = document.createElement('div');
					bracket.style.cssText = `
						position: absolute;
						width: ${{cornerSize}}px;
						height: ${{cornerSize}}px;
						pointer-events: none;
						transition: all 0.15s ease-out;
					`;

					// Position corners
					if (corner.pos === 'top-left') {{
						bracket.style.top = '0';
						bracket.style.left = '0';
						bracket.style.borderTop = `${{borderWidth}}px solid ${{color}}`;
						bracket.style.borderLeft = `${{borderWidth}}px solid ${{color}}`;
						bracket.style.transform = `translate(${{corner.startX}}px, ${{corner.startY}}px)`;
					}} else if (corner.pos === 'top-right') {{
						bracket.style.top = '0';
						bracket.style.right = '0';
						bracket.style.borderTop = `${{borderWidth}}px solid ${{color}}`;
						bracket.style.borderRight = `${{borderWidth}}px solid ${{color}}`;
						bracket.style.transform = `translate(${{corner.startX}}px, ${{corner.startY}}px)`;
					}} else if (corner.pos === 'bottom-left') {{
						bracket.style.bottom = '0';
						bracket.style.left = '0';
						bracket.style.borderBottom = `${{borderWidth}}px solid ${{color}}`;
						bracket.style.borderLeft = `${{borderWidth}}px solid ${{color}}`;
						bracket.style.transform = `translate(${{corner.startX}}px, ${{corner.startY}}px)`;
					}} else if (corner.pos === 'bottom-right') {{
						bracket.style.bottom = '0';
						bracket.style.right = '0';
						bracket.style.borderBottom = `${{borderWidth}}px solid ${{color}}`;
						bracket.style.borderRight = `${{borderWidth}}px solid ${{color}}`;
						bracket.style.transform = `translate(${{corner.startX}}px, ${{corner.startY}}px)`;
					}}

					container.appendChild(bracket);

					// Animate to final position slightly outside the element
					setTimeout(() => {{
						bracket.style.transform = `translate(${{corner.finalX}}px, ${{corner.finalY}}px)`;
					}}, 10);
				}});

				document.body.appendChild(container);

				// Auto-remove after duration
				setTimeout(() => {{
					container.style.opacity = '0';
					container.style.transition = 'opacity 0.3s ease-out';
					setTimeout(() => container.remove(), 300);
				}}, duration);

				return {{ created: true }};
			}})();
			"""

			# Fire and forget - don't wait for completion

			await cdp_session.cdp_client.send.Runtime.evaluate(
				params={'expression': script, 'returnByValue': True}, session_id=cdp_session.session_id
			)

		except Exception as e:
			# Don't fail the action if highlighting fails
			self.logger.debug(f'Failed to highlight interaction element: {e}')

	async def highlight_coordinate_click(self, x: int, y: int) -> None:
		"""Temporarily highlight a coordinate click position for user visibility.

		This creates a visual highlight at the specified coordinates showing where
		the click action occurred. The highlight automatically fades after the configured duration.

		Args:
			x: Horizontal coordinate relative to viewport left edge
			y: Vertical coordinate relative to viewport top edge
		"""
		if not self.browser_profile.highlight_elements:
			return

		try:
			import json

			cdp_session = await self.get_or_create_cdp_session()

			color = self.browser_profile.interaction_highlight_color
			duration_ms = int(self.browser_profile.interaction_highlight_duration * 1000)

			# Create animated crosshair and circle at the click coordinates
			script = f"""
			(function() {{
				const x = {x};
				const y = {y};
				const color = {json.dumps(color)};
				const duration = {duration_ms};

				// Get current scroll position
				const scrollX = window.pageXOffset || document.documentElement.scrollLeft || 0;
				const scrollY = window.pageYOffset || document.documentElement.scrollTop || 0;

				// Create container
				const container = document.createElement('div');
				container.setAttribute('data-browser-use-coordinate-highlight', 'true');
				container.style.cssText = `
					position: absolute;
					left: ${{x + scrollX}}px;
					top: ${{y + scrollY}}px;
					width: 0;
					height: 0;
					pointer-events: none;
					z-index: 2147483647;
				`;

				// Create outer circle
				const outerCircle = document.createElement('div');
				outerCircle.style.cssText = `
					position: absolute;
					left: -15px;
					top: -15px;
					width: 30px;
					height: 30px;
					border: 3px solid ${{color}};
					border-radius: 50%;
					opacity: 0;
					transform: scale(0.3);
					transition: all 0.2s ease-out;
				`;
				container.appendChild(outerCircle);

				// Create center dot
				const centerDot = document.createElement('div');
				centerDot.style.cssText = `
					position: absolute;
					left: -4px;
					top: -4px;
					width: 8px;
					height: 8px;
					background: ${{color}};
					border-radius: 50%;
					opacity: 0;
					transform: scale(0);
					transition: all 0.15s ease-out;
				`;
				container.appendChild(centerDot);

				document.body.appendChild(container);

				// Animate in
				setTimeout(() => {{
					outerCircle.style.opacity = '0.8';
					outerCircle.style.transform = 'scale(1)';
					centerDot.style.opacity = '1';
					centerDot.style.transform = 'scale(1)';
				}}, 10);

				// Animate out and remove
				setTimeout(() => {{
					outerCircle.style.opacity = '0';
					outerCircle.style.transform = 'scale(1.5)';
					centerDot.style.opacity = '0';
					setTimeout(() => container.remove(), 300);
				}}, duration);

				return {{ created: true }};
			}})();
			"""

			# Fire and forget - don't wait for completion
			await cdp_session.cdp_client.send.Runtime.evaluate(
				params={'expression': script, 'returnByValue': True}, session_id=cdp_session.session_id
			)

		except Exception as e:
			# Don't fail the action if highlighting fails
			self.logger.debug(f'Failed to highlight coordinate click: {e}')

	async def add_highlights(self, selector_map: dict[int, 'EnhancedDOMTreeNode']) -> None:
		"""Add visual highlights to the browser DOM for user visibility."""
		if not self.browser_profile.dom_highlight_elements or not selector_map:
			return

		try:
			import json

			# Convert selector_map to the format expected by the highlighting script
			elements_data = []
			for _, node in selector_map.items():
				# Get bounding box using absolute position (includes iframe translations) if available
				if node.absolute_position:
					# Use absolute position which includes iframe coordinate translations
					rect = node.absolute_position
					bbox = {'x': rect.x, 'y': rect.y, 'width': rect.width, 'height': rect.height}

					# Only include elements with valid bounding boxes
					if bbox and bbox.get('width', 0) > 0 and bbox.get('height', 0) > 0:
						element = {
							'x': bbox['x'],
							'y': bbox['y'],
							'width': bbox['width'],
							'height': bbox['height'],
							'element_name': node.node_name,
							'is_clickable': node.snapshot_node.is_clickable if node.snapshot_node else True,
							'is_scrollable': getattr(node, 'is_scrollable', False),
							'attributes': node.attributes or {},
							'frame_id': getattr(node, 'frame_id', None),
							'node_id': node.node_id,
							'backend_node_id': node.backend_node_id,
							'xpath': node.xpath,
							'text_content': node.get_all_children_text()[:50]
							if hasattr(node, 'get_all_children_text')
							else node.node_value[:50],
						}
						elements_data.append(element)

			if not elements_data:
				self.logger.debug('âš ï¸ No valid elements to highlight')
				return

			self.logger.debug(f'ğŸ“ Creating highlights for {len(elements_data)} elements')

			# Always remove existing highlights first
			await self.remove_highlights()

			# Add a small delay to ensure removal completes
			import asyncio

			await asyncio.sleep(0.05)

			# Get CDP session
			cdp_session = await self.get_or_create_cdp_session()

			# Create the proven highlighting script from v0.6.0 with fixed positioning
			script = f"""
			(function() {{
				// Interactive elements data
				const interactiveElements = {json.dumps(elements_data)};

				console.log('=== BROWSER-USE HIGHLIGHTING ===');
				console.log('Highlighting', interactiveElements.length, 'interactive elements');

				// Double-check: Remove any existing highlight container first
				const existingContainer = document.getElementById('browser-use-debug-highlights');
				if (existingContainer) {{
					console.log('âš ï¸ Found existing highlight container, removing it first');
					existingContainer.remove();
				}}

				// Also remove any stray highlight elements
				const strayHighlights = document.querySelectorAll('[data-browser-use-highlight]');
				if (strayHighlights.length > 0) {{
					console.log('âš ï¸ Found', strayHighlights.length, 'stray highlight elements, removing them');
					strayHighlights.forEach(el => el.remove());
				}}

				// Use maximum z-index for visibility
				const HIGHLIGHT_Z_INDEX = 2147483647;

				// Create container for all highlights - use FIXED positioning (key insight from v0.6.0)
				const container = document.createElement('div');
				container.id = 'browser-use-debug-highlights';
				container.setAttribute('data-browser-use-highlight', 'container');

				container.style.cssText = `
					position: absolute;
					top: 0;
					left: 0;
					width: 100vw;
					height: 100vh;
					pointer-events: none;
					z-index: ${{HIGHLIGHT_Z_INDEX}};
					overflow: visible;
					margin: 0;
					padding: 0;
					border: none;
					outline: none;
					box-shadow: none;
					background: none;
					font-family: inherit;
				`;

				// Helper function to create text elements safely
				function createTextElement(tag, text, styles) {{
					const element = document.createElement(tag);
					element.textContent = text;
					if (styles) element.style.cssText = styles;
					return element;
				}}

				// Add highlights for each element
				interactiveElements.forEach((element, index) => {{
					const highlight = document.createElement('div');
					highlight.setAttribute('data-browser-use-highlight', 'element');
					highlight.setAttribute('data-element-id', element.backend_node_id);
					highlight.style.cssText = `
						position: absolute;
						left: ${{element.x}}px;
						top: ${{element.y}}px;
						width: ${{element.width}}px;
						height: ${{element.height}}px;
						outline: 2px dashed #4a90e2;
						outline-offset: -2px;
						background: transparent;
						pointer-events: none;
						box-sizing: content-box;
						transition: outline 0.2s ease;
						margin: 0;
						padding: 0;
						border: none;
					`;

					// Enhanced label with backend node ID
					const label = createTextElement('div', element.backend_node_id, `
						position: absolute;
						top: -20px;
						left: 0;
						background-color: #4a90e2;
						color: white;
						padding: 2px 6px;
						font-size: 11px;
						font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
						font-weight: bold;
						border-radius: 3px;
						white-space: nowrap;
						z-index: ${{HIGHLIGHT_Z_INDEX + 1}};
						box-shadow: 0 2px 4px rgba(0,0,0,0.3);
						border: none;
						outline: none;
						margin: 0;
						line-height: 1.2;
					`);

					highlight.appendChild(label);
					container.appendChild(highlight);
				}});

				// Add container to document
				document.body.appendChild(container);

				console.log('Highlighting complete - added', interactiveElements.length, 'highlights');
				return {{ added: interactiveElements.length }};
			}})();
			"""

			# Execute the script
			result = await cdp_session.cdp_client.send.Runtime.evaluate(
				params={'expression': script, 'returnByValue': True}, session_id=cdp_session.session_id
			)

			# Log the result
			if result and 'result' in result and 'value' in result['result']:
				added_count = result['result']['value'].get('added', 0)
				self.logger.debug(f'Successfully added {added_count} highlight elements to browser DOM')
			else:
				self.logger.debug('Browser highlight injection completed')

		except Exception as e:
			self.logger.warning(f'Failed to add browser highlights: {e}')
			import traceback

			self.logger.debug(f'Browser highlight traceback: {traceback.format_exc()}')

	async def _close_extension_options_pages(self) -> None:
		"""Close any extension options/welcome pages that have opened."""
		try:
			# Get all page targets from SessionManager
			page_targets = self.session_manager.get_all_page_targets()

			for target in page_targets:
				target_url = target.url
				target_id = target.target_id

				# Check if this is an extension options/welcome page
				if 'chrome-extension://' in target_url and (
					'options.html' in target_url or 'welcome.html' in target_url or 'onboarding.html' in target_url
				):
					self.logger.info(f'[BrowserSession] ğŸš« Closing extension options page: {target_url}')
					try:
						await self._cdp_close_page(target_id)
					except Exception as e:
						self.logger.debug(f'[BrowserSession] Could not close extension page {target_id}: {e}')

		except Exception as e:
			self.logger.debug(f'[BrowserSession] Error closing extension options pages: {e}')

	async def send_demo_mode_log(self, message: str, level: str = 'info', metadata: dict[str, Any] | None = None) -> None:
		"""Send a message to the in-browser demo panel if enabled."""
		if not self.browser_profile.demo_mode:
			return
		demo = self.demo_mode
		if not demo:
			return
		try:
			await demo.send_log(message=message, level=level, metadata=metadata or {})
		except Exception as exc:
			self.logger.debug(f'[DemoMode] Failed to send log: {exc}')

	@property
	def downloaded_files(self) -> list[str]:
		"""Get list of files downloaded during this browser session.

		Returns:
			list[str]: List of absolute file paths to downloaded files in this session
		"""
		return self._downloaded_files.copy()

	# endregion - ========== Helper Methods ==========

	# region - ========== CDP-based replacements for browser_context operations ==========

	async def _cdp_get_all_pages(
		self,
		include_http: bool = True,
		include_about: bool = True,
		include_pages: bool = True,
		include_iframes: bool = False,
		include_workers: bool = False,
		include_chrome: bool = False,
		include_chrome_extensions: bool = False,
		include_chrome_error: bool = False,
	) -> list[TargetInfo]:
		"""Get all browser pages/tabs using SessionManager (source of truth)."""
		# Safety check - return empty list if browser not connected yet
		if not self.session_manager:
			return []

		# Build TargetInfo dicts from SessionManager owned data (crystal clear ownership)
		result = []
		for target_id, target in self.session_manager.get_all_targets().items():
			# Create TargetInfo dict
			target_info: TargetInfo = {
				'targetId': target.target_id,
				'type': target.target_type,
				'title': target.title,
				'url': target.url,
				'attached': True,
				'canAccessOpener': False,
			}

			# Apply filters
			if self._is_valid_target(
				target_info,
				include_http=include_http,
				include_about=include_about,
				include_pages=include_pages,
				include_iframes=include_iframes,
				include_workers=include_workers,
				include_chrome=include_chrome,
				include_chrome_extensions=include_chrome_extensions,
				include_chrome_error=include_chrome_error,
			):
				result.append(target_info)

		return result

	async def _cdp_create_new_page(self, url: str = 'about:blank', background: bool = False, new_window: bool = False) -> str:
		"""Create a new page/tab using CDP Target.createTarget. Returns target ID."""
		# Use the root CDP client to create tabs at the browser level
		if self._cdp_client_root:
			result = await self._cdp_client_root.send.Target.createTarget(
				params={'url': url, 'newWindow': new_window, 'background': background}
			)
		else:
			# Fallback to using cdp_client if root is not available
			result = await self.cdp_client.send.Target.createTarget(
				params={'url': url, 'newWindow': new_window, 'background': background}
			)
		return result['targetId']

	async def _cdp_close_page(self, target_id: TargetID) -> None:
		"""Close a page/tab using CDP Target.closeTarget."""
		await self.cdp_client.send.Target.closeTarget(params={'targetId': target_id})

	async def _cdp_get_cookies(self) -> list[Cookie]:
		"""Get cookies using CDP Network.getCookies."""
		cdp_session = await self.get_or_create_cdp_session(target_id=None)
		result = await asyncio.wait_for(
			cdp_session.cdp_client.send.Storage.getCookies(session_id=cdp_session.session_id), timeout=8.0
		)
		return result.get('cookies', [])

	async def _cdp_set_cookies(self, cookies: list[Cookie]) -> None:
		"""Set cookies using CDP Storage.setCookies."""
		if not self.agent_focus_target_id or not cookies:
			return

		cdp_session = await self.get_or_create_cdp_session(target_id=None)
		# Storage.setCookies expects params dict with 'cookies' key
		await cdp_session.cdp_client.send.Storage.setCookies(
			params={'cookies': cookies},  # type: ignore[arg-type]
			session_id=cdp_session.session_id,
		)

	async def _cdp_clear_cookies(self) -> None:
		"""Clear all cookies using CDP Network.clearBrowserCookies."""
		cdp_session = await self.get_or_create_cdp_session()
		await cdp_session.cdp_client.send.Storage.clearCookies(session_id=cdp_session.session_id)

	async def _cdp_set_extra_headers(self, headers: dict[str, str]) -> None:
		"""Set extra HTTP headers using CDP Network.setExtraHTTPHeaders."""
		if not self.agent_focus_target_id:
			return

		cdp_session = await self.get_or_create_cdp_session()
		# await cdp_session.cdp_client.send.Network.setExtraHTTPHeaders(params={'headers': headers}, session_id=cdp_session.session_id)
		raise NotImplementedError('Not implemented yet')

	async def _cdp_grant_permissions(self, permissions: list[str], origin: str | None = None) -> None:
		"""Grant permissions using CDP Browser.grantPermissions."""
		params = {'permissions': permissions}
		# if origin:
		# 	params['origin'] = origin
		cdp_session = await self.get_or_create_cdp_session()
		# await cdp_session.cdp_client.send.Browser.grantPermissions(params=params, session_id=cdp_session.session_id)
		raise NotImplementedError('Not implemented yet')

	async def _cdp_set_geolocation(self, latitude: float, longitude: float, accuracy: float = 100) -> None:
		"""Set geolocation using CDP Emulation.setGeolocationOverride."""
		await self.cdp_client.send.Emulation.setGeolocationOverride(
			params={'latitude': latitude, 'longitude': longitude, 'accuracy': accuracy}
		)

	async def _cdp_clear_geolocation(self) -> None:
		"""Clear geolocation override using CDP."""
		await self.cdp_client.send.Emulation.clearGeolocationOverride()

	async def _cdp_add_init_script(self, script: str, target_id: TargetID | None = None) -> str:
		"""Add script to evaluate on new document for a specific target."""
		assert self._cdp_client_root is not None
		cdp_session = await self.get_or_create_cdp_session(target_id=target_id, focus=target_id is None)

		result = await cdp_session.cdp_client.send.Page.addScriptToEvaluateOnNewDocument(
			params={'source': script, 'runImmediately': True}, session_id=cdp_session.session_id
		)
		return result['identifier']

	async def _cdp_remove_init_script(self, identifier: str, target_id: TargetID | None = None) -> None:
		"""Remove script added with addScriptToEvaluateOnNewDocument for a target."""
		cdp_session = await self.get_or_create_cdp_session(target_id=target_id, focus=target_id is None)
		await cdp_session.cdp_client.send.Page.removeScriptToEvaluateOnNewDocument(
			params={'identifier': identifier}, session_id=cdp_session.session_id
		)

	async def _cdp_set_viewport(
		self, width: int, height: int, device_scale_factor: float = 1.0, mobile: bool = False, target_id: str | None = None
	) -> None:
		"""Set viewport using CDP Emulation.setDeviceMetricsOverride.

		Args:
			width: Viewport width
			height: Viewport height
			device_scale_factor: Device scale factor (default 1.0)
			mobile: Whether to emulate mobile device (default False)
			target_id: Optional target ID to set viewport for. If not provided, uses agent_focus.
		"""
		if target_id:
			# Set viewport for specific target
			cdp_session = await self.get_or_create_cdp_session(target_id, focus=False)
		elif self.agent_focus_target_id:
			# Use current focus - use safe API with focus=False to avoid changing focus
			try:
				cdp_session = await self.get_or_create_cdp_session(self.agent_focus_target_id, focus=False)
			except ValueError:
				self.logger.warning('Cannot set viewport: focused target has no sessions')
				return
		else:
			self.logger.warning('Cannot set viewport: no target_id provided and agent_focus not initialized')
			return

		await cdp_session.cdp_client.send.Emulation.setDeviceMetricsOverride(
			params={'width': width, 'height': height, 'deviceScaleFactor': device_scale_factor, 'mobile': mobile},
			session_id=cdp_session.session_id,
		)

	async def _cdp_get_origins(self) -> list[dict[str, Any]]:
		"""Get origins with localStorage and sessionStorage using CDP."""
		origins = []
		cdp_session = await self.get_or_create_cdp_session(target_id=None)

		try:
			# Enable DOMStorage domain to track storage
			await cdp_session.cdp_client.send.DOMStorage.enable(session_id=cdp_session.session_id)

			try:
				# Get all frames to find unique origins
				frames_result = await cdp_session.cdp_client.send.Page.getFrameTree(session_id=cdp_session.session_id)

				# Extract unique origins from frames
				unique_origins = set()

				def _extract_origins(frame_tree):
					"""Recursively extract origins from frame tree."""
					frame = frame_tree.get('frame', {})
					origin = frame.get('securityOrigin')
					if origin and origin != 'null':
						unique_origins.add(origin)

					# Process child frames
					for child in frame_tree.get('childFrames', []):
						_extract_origins(child)

				async def _get_storage_items(origin: str, is_local_storage: bool) -> list[dict[str, str]] | None:
					"""Helper to get storage items for an origin."""
					storage_type = 'localStorage' if is_local_storage else 'sessionStorage'
					try:
						result = await cdp_session.cdp_client.send.DOMStorage.getDOMStorageItems(
							params={'storageId': {'securityOrigin': origin, 'isLocalStorage': is_local_storage}},
							session_id=cdp_session.session_id,
						)

						items = []
						for item in result.get('entries', []):
							if len(item) == 2:  # Each item is [key, value]
								items.append({'name': item[0], 'value': item[1]})

						return items if items else None
					except Exception as e:
						self.logger.debug(f'Failed to get {storage_type} for {origin}: {e}')
						return None

				_extract_origins(frames_result.get('frameTree', {}))

				# For each unique origin, get localStorage and sessionStorage
				for origin in unique_origins:
					origin_data = {'origin': origin}

					# Get localStorage
					local_storage = await _get_storage_items(origin, is_local_storage=True)
					if local_storage:
						origin_data['localStorage'] = local_storage

					# Get sessionStorage
					session_storage = await _get_storage_items(origin, is_local_storage=False)
					if session_storage:
						origin_data['sessionStorage'] = session_storage

					# Only add origin if it has storage data
					if 'localStorage' in origin_data or 'sessionStorage' in origin_data:
						origins.append(origin_data)

			finally:
				# Always disable DOMStorage tracking when done
				await cdp_session.cdp_client.send.DOMStorage.disable(session_id=cdp_session.session_id)

		except Exception as e:
			self.logger.warning(f'Failed to get origins: {e}')

		return origins

	async def _cdp_get_storage_state(self) -> dict:
		"""Get storage state (cookies, localStorage, sessionStorage) using CDP."""
		# Use the _cdp_get_cookies helper which handles session attachment
		cookies = await self._cdp_get_cookies()

		# Get origins with localStorage/sessionStorage
		origins = await self._cdp_get_origins()

		return {
			'cookies': cookies,
			'origins': origins,
		}

	async def _cdp_navigate(self, url: str, target_id: TargetID | None = None) -> None:
		"""Navigate to URL using CDP Page.navigate."""
		# Use provided target_id or fall back to agent_focus_target_id

		assert self._cdp_client_root is not None, 'CDP client not initialized - browser may not be connected yet'
		assert self.agent_focus_target_id is not None, 'Agent focus not initialized - browser may not be connected yet'

		target_id_to_use = target_id or self.agent_focus_target_id
		cdp_session = await self.get_or_create_cdp_session(target_id_to_use, focus=True)

		# Use helper to navigate on the target
		await cdp_session.cdp_client.send.Page.navigate(params={'url': url}, session_id=cdp_session.session_id)

	@staticmethod
	def _is_valid_target(
		target_info: TargetInfo,
		include_http: bool = True,
		include_chrome: bool = False,
		include_chrome_extensions: bool = False,
		include_chrome_error: bool = False,
		include_about: bool = True,
		include_iframes: bool = True,
		include_pages: bool = True,
		include_workers: bool = False,
	) -> bool:
		"""Check if a target should be processed.

		Args:
			target_info: Target info dict from CDP

		Returns:
			True if target should be processed, False if it should be skipped
		"""
		target_type = target_info.get('type', '')
		url = target_info.get('url', '')

		url_allowed, type_allowed = False, False

		# Always allow new tab pages (chrome://new-tab-page/, chrome://newtab/, about:blank)
		# so they can be redirected to about:blank in connect()
		from browser_use.utils import is_new_tab_page

		if is_new_tab_page(url):
			url_allowed = True

		if url.startswith('chrome-error://') and include_chrome_error:
			url_allowed = True

		if url.startswith('chrome://') and include_chrome:
			url_allowed = True

		if url.startswith('chrome-extension://') and include_chrome_extensions:
			url_allowed = True

		# dont allow about:srcdoc! there are also other rare about: pages that we want to avoid
		if url == 'about:blank' and include_about:
			url_allowed = True

		if (url.startswith('http://') or url.startswith('https://')) and include_http:
			url_allowed = True

		if target_type in ('service_worker', 'shared_worker', 'worker') and include_workers:
			type_allowed = True

		if target_type in ('page', 'tab') and include_pages:
			type_allowed = True

		if target_type in ('iframe', 'webview') and include_iframes:
			type_allowed = True

		return url_allowed and type_allowed

	async def get_all_frames(self) -> tuple[dict[str, dict], dict[str, str]]:
		"""Get a complete frame hierarchy from all browser targets.

		Returns:
			Tuple of (all_frames, target_sessions) where:
			- all_frames: dict mapping frame_id -> frame info dict with all metadata
			- target_sessions: dict mapping target_id -> session_id for active sessions
		"""
		all_frames = {}  # frame_id -> FrameInfo dict
		target_sessions = {}  # target_id -> session_id (keep sessions alive during collection)

		# Check if cross-origin iframe support is enabled
		include_cross_origin = self.browser_profile.cross_origin_iframes

		# Get all targets - only include iframes if cross-origin support is enabled
		targets = await self._cdp_get_all_pages(
			include_http=True,
			include_about=True,
			include_pages=True,
			include_iframes=include_cross_origin,  # Only include iframe targets if flag is set
			include_workers=False,
			include_chrome=False,
			include_chrome_extensions=False,
			include_chrome_error=include_cross_origin,  # Only include error pages if cross-origin is enabled
		)
		all_targets = targets

		# First pass: collect frame trees from ALL targets
		for target in all_targets:
			target_id = target['targetId']

			# Skip iframe targets if cross-origin support is disabled
			if not include_cross_origin and target.get('type') == 'iframe':
				continue

			# When cross-origin support is disabled, only process the current target
			if not include_cross_origin:
				# Only process the current focus target
				if self.agent_focus_target_id and target_id != self.agent_focus_target_id:
					continue
				# Use the existing agent_focus target's session - use safe API with focus=False
				try:
					cdp_session = await self.get_or_create_cdp_session(self.agent_focus_target_id, focus=False)
				except ValueError:
					continue  # Skip if no session available
			else:
				# Get cached session for this target (don't change focus - iterating frames)
				cdp_session = await self.get_or_create_cdp_session(target_id, focus=False)

			if cdp_session:
				target_sessions[target_id] = cdp_session.session_id

				try:
					# Try to get frame tree (not all target types support this)
					frame_tree_result = await cdp_session.cdp_client.send.Page.getFrameTree(session_id=cdp_session.session_id)

					# Process the frame tree recursively
					def process_frame_tree(node, parent_frame_id=None):
						"""Recursively process frame tree and add to all_frames."""
						frame = node.get('frame', {})
						current_frame_id = frame.get('id')

						if current_frame_id:
							# For iframe targets, check if the frame has a parentId field
							# This indicates it's an OOPIF with a parent in another target
							actual_parent_id = frame.get('parentId') or parent_frame_id

							# Create frame info with all CDP response data plus our additions
							frame_info = {
								**frame,  # Include all original frame data: id, url, parentId, etc.
								'frameTargetId': target_id,  # Target that can access this frame
								'parentFrameId': actual_parent_id,  # Use parentId from frame if available
								'childFrameIds': [],  # Will be populated below
								'isCrossOrigin': False,  # Will be determined based on context
								'isValidTarget': self._is_valid_target(
									target,
									include_http=True,
									include_about=True,
									include_pages=True,
									include_iframes=True,
									include_workers=False,
									include_chrome=False,  # chrome://newtab, chrome://settings, etc. are not valid frames we can control (for sanity reasons)
									include_chrome_extensions=False,  # chrome-extension://
									include_chrome_error=False,  # chrome-error://  (e.g. when iframes fail to load or are blocked by uBlock Origin)
								),
							}

							# Check if frame is cross-origin based on crossOriginIsolatedContextType
							cross_origin_type = frame.get('crossOriginIsolatedContextType')
							if cross_origin_type and cross_origin_type != 'NotIsolated':
								frame_info['isCrossOrigin'] = True

							# For iframe targets, the frame itself is likely cross-origin
							if target.get('type') == 'iframe':
								frame_info['isCrossOrigin'] = True

							# Skip cross-origin frames if support is disabled
							if not include_cross_origin and frame_info.get('isCrossOrigin'):
								return  # Skip this frame and its children

							# Add child frame IDs (note: OOPIFs won't appear here)
							child_frames = node.get('childFrames', [])
							for child in child_frames:
								child_frame = child.get('frame', {})
								child_frame_id = child_frame.get('id')
								if child_frame_id:
									frame_info['childFrameIds'].append(child_frame_id)

							# Store or merge frame info
							if current_frame_id in all_frames:
								# Frame already seen from another target, merge info
								existing = all_frames[current_frame_id]
								# If this is an iframe target, it has direct access to the frame
								if target.get('type') == 'iframe':
									existing['frameTargetId'] = target_id
									existing['isCrossOrigin'] = True
							else:
								all_frames[current_frame_id] = frame_info

							# Process child frames recursively (only if we're not skipping this frame)
							if include_cross_origin or not frame_info.get('isCrossOrigin'):
								for child in child_frames:
									process_frame_tree(child, current_frame_id)

					# Process the entire frame tree
					process_frame_tree(frame_tree_result.get('frameTree', {}))

				except Exception as e:
					# Target doesn't support Page domain or has no frames
					self.logger.debug(f'Failed to get frame tree for target {target_id}: {e}')

		# Second pass: populate backend node IDs and parent target IDs
		# Only do this if cross-origin support is enabled
		if include_cross_origin:
			await self._populate_frame_metadata(all_frames, target_sessions)

		return all_frames, target_sessions

	async def _populate_frame_metadata(self, all_frames: dict[str, dict], target_sessions: dict[str, str]) -> None:
		"""Populate additional frame metadata like backend node IDs and parent target IDs.

		Args:
			all_frames: Frame hierarchy dict to populate
			target_sessions: Active target sessions
		"""
		for frame_id_iter, frame_info in all_frames.items():
			parent_frame_id = frame_info.get('parentFrameId')

			if parent_frame_id and parent_frame_id in all_frames:
				parent_frame_info = all_frames[parent_frame_id]
				parent_target_id = parent_frame_info.get('frameTargetId')

				# Store parent target ID
				frame_info['parentTargetId'] = parent_target_id

				# Try to get backend node ID from parent context
				if parent_target_id in target_sessions:
					assert parent_target_id is not None
					parent_session_id = target_sessions[parent_target_id]
					try:
						# Enable DOM domain
						await self.cdp_client.send.DOM.enable(session_id=parent_session_id)

						# Get frame owner info to find backend node ID
						frame_owner = await self.cdp_client.send.DOM.getFrameOwner(
							params={'frameId': frame_id_iter}, session_id=parent_session_id
						)

						if frame_owner:
							frame_info['backendNodeId'] = frame_owner.get('backendNodeId')
							frame_info['nodeId'] = frame_owner.get('nodeId')

					except Exception:
						# Frame owner not available (likely cross-origin)
						pass

	async def find_frame_target(self, frame_id: str, all_frames: dict[str, dict] | None = None) -> dict | None:
		"""Find the frame info for a specific frame ID.

		Args:
			frame_id: The frame ID to search for
			all_frames: Optional pre-built frame hierarchy. If None, will call get_all_frames()

		Returns:
			Frame info dict if found, None otherwise
		"""
		if all_frames is None:
			all_frames, _ = await self.get_all_frames()

		return all_frames.get(frame_id)

	async def cdp_client_for_target(self, target_id: TargetID) -> CDPSession:
		return await self.get_or_create_cdp_session(target_id, focus=False)

	async def cdp_client_for_frame(self, frame_id: str) -> CDPSession:
		"""Get a CDP client attached to the target containing the specified frame.

		Builds a unified frame hierarchy from all targets to find the correct target
		for any frame, including OOPIFs (Out-of-Process iframes).

		Args:
			frame_id: The frame ID to search for

		Returns:
			Tuple of (cdp_cdp_session, target_id) for the target containing the frame

		Raises:
			ValueError: If the frame is not found in any target
		"""
		# If cross-origin iframes are disabled, just use the main session
		if not self.browser_profile.cross_origin_iframes:
			return await self.get_or_create_cdp_session()

		# Get complete frame hierarchy
		all_frames, target_sessions = await self.get_all_frames()

		# Find the requested frame
		frame_info = await self.find_frame_target(frame_id, all_frames)

		if frame_info:
			target_id = frame_info.get('frameTargetId')

			if target_id in target_sessions:
				assert target_id is not None
				# Use existing session
				session_id = target_sessions[target_id]
				# Return the client with session attached (don't change focus)
				return await self.get_or_create_cdp_session(target_id, focus=False)

		# Frame not found
		raise ValueError(f"Frame with ID '{frame_id}' not found in any target")

	async def cdp_client_for_node(self, node: EnhancedDOMTreeNode) -> CDPSession:
		"""Get CDP client for a specific DOM node based on its frame.

		IMPORTANT: backend_node_id is only valid in the session where the DOM was captured.
		We trust the node's session_id/frame_id/target_id instead of searching all sessions.
		"""

		# Strategy 1: If node has session_id, try to use that exact session (most specific)
		if node.session_id and self.session_manager:
			try:
				# Find the CDP session by session_id from SessionManager
				cdp_session = self.session_manager.get_session(node.session_id)
				if cdp_session:
					# Get target to log URL
					target = self.session_manager.get_target(cdp_session.target_id)
					self.logger.debug(f'âœ… Using session from node.session_id for node {node.backend_node_id}: {target.url}')
					return cdp_session
			except Exception as e:
				self.logger.debug(f'Failed to get session by session_id {node.session_id}: {e}')

		# Strategy 2: If node has frame_id, use that frame's session
		if node.frame_id:
			try:
				cdp_session = await self.cdp_client_for_frame(node.frame_id)
				target = self.session_manager.get_target(cdp_session.target_id)
				self.logger.debug(f'âœ… Using session from node.frame_id for node {node.backend_node_id}: {target.url}')
				return cdp_session
			except Exception as e:
				self.logger.debug(f'Failed to get session for frame {node.frame_id}: {e}')

		# Strategy 3: If node has target_id, use that target's session
		if node.target_id:
			try:
				cdp_session = await self.get_or_create_cdp_session(target_id=node.target_id, focus=False)
				target = self.session_manager.get_target(cdp_session.target_id)
				self.logger.debug(f'âœ… Using session from node.target_id for node {node.backend_node_id}: {target.url}')
				return cdp_session
			except Exception as e:
				self.logger.debug(f'Failed to get session for target {node.target_id}: {e}')

		# Strategy 4: Fallback to agent_focus_target_id (the page where agent is currently working)
		if self.agent_focus_target_id:
			target = self.session_manager.get_target(self.agent_focus_target_id)
			try:
				# Use safe API with focus=False to avoid changing focus
				cdp_session = await self.get_or_create_cdp_session(self.agent_focus_target_id, focus=False)
				if target:
					self.logger.warning(
						f'âš ï¸ Node {node.backend_node_id} has no session/frame/target info. Using agent_focus session: {target.url}'
					)
				return cdp_session
			except ValueError:
				pass  # Fall through to last resort

		# Last resort: use main session
		self.logger.error(f'âŒ No session info for node {node.backend_node_id} and no agent_focus available. Using main session.')
		return await self.get_or_create_cdp_session()

	@observe_debug(ignore_input=True, ignore_output=True, name='take_screenshot')
	async def take_screenshot(
		self,
		path: str | None = None,
		full_page: bool = False,
		format: str = 'png',
		quality: int | None = None,
		clip: dict | None = None,
	) -> bytes:
		"""Take a screenshot using CDP.

		Args:
			path: Optional file path to save screenshot
			full_page: Capture entire scrollable page beyond viewport
			format: Image format ('png', 'jpeg', 'webp')
			quality: Quality 0-100 for JPEG format
			clip: Region to capture {'x': int, 'y': int, 'width': int, 'height': int}

		Returns:
			Screenshot data as bytes
		"""
		import base64

		from cdp_use.cdp.page import CaptureScreenshotParameters

		cdp_session = await self.get_or_create_cdp_session()

		# Build parameters dict explicitly to satisfy TypedDict expectations
		params: CaptureScreenshotParameters = {
			'format': format,
			'captureBeyondViewport': full_page,
		}

		if quality is not None and format == 'jpeg':
			params['quality'] = quality

		if clip:
			params['clip'] = {
				'x': clip['x'],
				'y': clip['y'],
				'width': clip['width'],
				'height': clip['height'],
				'scale': 1,
			}

		params = CaptureScreenshotParameters(**params)

		result = await cdp_session.cdp_client.send.Page.captureScreenshot(params=params, session_id=cdp_session.session_id)

		if not result or 'data' not in result:
			raise Exception('Screenshot failed - no data returned')

		screenshot_data = base64.b64decode(result['data'])

		if path:
			Path(path).write_bytes(screenshot_data)

		return screenshot_data

	async def screenshot_element(
		self,
		selector: str,
		path: str | None = None,
		format: str = 'png',
		quality: int | None = None,
	) -> bytes:
		"""Take a screenshot of a specific element.

		Args:
			selector: CSS selector for the element
			path: Optional file path to save screenshot
			format: Image format ('png', 'jpeg', 'webp')
			quality: Quality 0-100 for JPEG format

		Returns:
			Screenshot data as bytes
		"""

		bounds = await self._get_element_bounds(selector)
		if not bounds:
			raise ValueError(f"Element '{selector}' not found or has no bounds")

		return await self.take_screenshot(
			path=path,
			format=format,
			quality=quality,
			clip=bounds,
		)

	async def _get_element_bounds(self, selector: str) -> dict | None:
		"""Get element bounding box using CDP."""

		cdp_session = await self.get_or_create_cdp_session()

		# Get document
		doc = await cdp_session.cdp_client.send.DOM.getDocument(params={'depth': 1}, session_id=cdp_session.session_id)

		# Query selector
		node_result = await cdp_session.cdp_client.send.DOM.querySelector(
			params={'nodeId': doc['root']['nodeId'], 'selector': selector}, session_id=cdp_session.session_id
		)

		node_id = node_result.get('nodeId')
		if not node_id:
			return None

		# Get bounding box
		box_result = await cdp_session.cdp_client.send.DOM.getBoxModel(
			params={'nodeId': node_id}, session_id=cdp_session.session_id
		)

		box_model = box_result.get('model')
		if not box_model:
			return None

		content = box_model['content']
		return {
			'x': min(content[0], content[2], content[4], content[6]),
			'y': min(content[1], content[3], content[5], content[7]),
			'width': max(content[0], content[2], content[4], content[6]) - min(content[0], content[2], content[4], content[6]),
			'height': max(content[1], content[3], content[5], content[7]) - min(content[1], content[3], content[5], content[7]),
		}

```

---

## backend/browser-use/browser_use/browser/session_manager.py

```py
"""Event-driven CDP session management.

Manages CDP sessions by listening to Target.attachedToTarget and Target.detachedFromTarget
events, ensuring the session pool always reflects the current browser state.
"""

import asyncio
from typing import TYPE_CHECKING

from cdp_use.cdp.target import AttachedToTargetEvent, DetachedFromTargetEvent, SessionID, TargetID

from browser_use.utils import create_task_with_error_handling

if TYPE_CHECKING:
	from browser_use.browser.session import BrowserSession, CDPSession, Target


class SessionManager:
	"""Event-driven CDP session manager.

	Automatically synchronizes the CDP session pool with browser state via CDP events.

	Key features:
	- Sessions added/removed automatically via Target attach/detach events
	- Multiple sessions can attach to the same target
	- Targets only removed when ALL sessions detach
	- No stale sessions - pool always reflects browser reality

	SessionManager is the SINGLE SOURCE OF TRUTH for all targets and sessions.
	"""

	def __init__(self, browser_session: 'BrowserSession'):
		self.browser_session = browser_session
		self.logger = browser_session.logger

		# All targets (entities: pages, iframes, workers)
		self._targets: dict[TargetID, 'Target'] = {}

		# All sessions (communication channels)
		self._sessions: dict[SessionID, 'CDPSession'] = {}

		# Mapping: target -> sessions attached to it
		self._target_sessions: dict[TargetID, set[SessionID]] = {}

		# Reverse mapping: session -> target it belongs to
		self._session_to_target: dict[SessionID, TargetID] = {}

		self._lock = asyncio.Lock()
		self._recovery_lock = asyncio.Lock()

		# Focus recovery coordination - event-driven instead of polling
		self._recovery_in_progress: bool = False
		self._recovery_complete_event: asyncio.Event | None = None
		self._recovery_task: asyncio.Task | None = None

	async def start_monitoring(self) -> None:
		"""Start monitoring Target attach/detach events.

		Registers CDP event handlers to keep the session pool synchronized with browser state.
		Also discovers and initializes all existing targets on startup.
		"""
		if not self.browser_session._cdp_client_root:
			raise RuntimeError('CDP client not initialized')

		# Capture cdp_client_root in closure to avoid type errors
		cdp_client = self.browser_session._cdp_client_root

		# Enable target discovery to receive targetInfoChanged events automatically
		# This eliminates the need for getTargetInfo() polling calls
		await cdp_client.send.Target.setDiscoverTargets(
			params={'discover': True, 'filter': [{'type': 'page'}, {'type': 'iframe'}]}
		)

		# Register synchronous event handlers (CDP requirement)
		def on_attached(event: AttachedToTargetEvent, session_id: SessionID | None = None):
			# _handle_target_attached() handles:
			# - setAutoAttach for children
			# - Create CDPSession
			# - Enable monitoring (for pages/tabs)
			# - Add to pool
			create_task_with_error_handling(
				self._handle_target_attached(event),
				name='handle_target_attached',
				logger_instance=self.logger,
				suppress_exceptions=True,
			)

		def on_detached(event: DetachedFromTargetEvent, session_id: SessionID | None = None):
			create_task_with_error_handling(
				self._handle_target_detached(event),
				name='handle_target_detached',
				logger_instance=self.logger,
				suppress_exceptions=True,
			)

		def on_target_info_changed(event, session_id: SessionID | None = None):
			# Update session info from targetInfoChanged events (no polling needed!)
			create_task_with_error_handling(
				self._handle_target_info_changed(event),
				name='handle_target_info_changed',
				logger_instance=self.logger,
				suppress_exceptions=True,
			)

		cdp_client.register.Target.attachedToTarget(on_attached)
		cdp_client.register.Target.detachedFromTarget(on_detached)
		cdp_client.register.Target.targetInfoChanged(on_target_info_changed)

		self.logger.debug('[SessionManager] Event monitoring started')

		# Discover and initialize ALL existing targets
		await self._initialize_existing_targets()

	def _get_session_for_target(self, target_id: TargetID) -> 'CDPSession | None':
		"""Internal: Get ANY valid session for a target (picks first available).

		âš ï¸ INTERNAL API - Use browser_session.get_or_create_cdp_session() instead!
		This method has no validation, no focus management, no recovery.

		Args:
			target_id: Target ID to get session for

		Returns:
			CDPSession if exists, None if target has detached
		"""
		session_ids = self._target_sessions.get(target_id, set())
		if not session_ids:
			# Check if this is the focused target - indicates stale focus that needs cleanup
			if self.browser_session.agent_focus_target_id == target_id:
				self.logger.warning(
					f'[SessionManager] âš ï¸ Attempted to get session for stale focused target {target_id[:8]}... '
					f'Clearing stale focus and triggering recovery.'
				)

				# Clear stale focus immediately (defense in depth)
				self.browser_session.agent_focus_target_id = None

				# Trigger recovery if not already in progress
				if not self._recovery_in_progress:
					self.logger.warning('[SessionManager] Recovery was not in progress! Triggering now.')
					self._recovery_task = create_task_with_error_handling(
						self._recover_agent_focus(target_id),
						name='recover_agent_focus_from_stale_get',
						logger_instance=self.logger,
						suppress_exceptions=False,
					)
			return None
		return self._sessions.get(next(iter(session_ids)))

	def get_all_page_targets(self) -> list:
		"""Get all page/tab targets using owned data.

		Returns:
			List of Target objects for all page/tab targets
		"""
		page_targets = []
		for target in self._targets.values():
			if target.target_type in ('page', 'tab'):
				page_targets.append(target)
		return page_targets

	async def validate_session(self, target_id: TargetID) -> bool:
		"""Check if a target still has active sessions.

		Args:
			target_id: Target ID to validate

		Returns:
			True if target has active sessions, False if it should be removed
		"""
		if target_id not in self._target_sessions:
			return False
		return len(self._target_sessions[target_id]) > 0

	async def clear(self) -> None:
		"""Clear all owned data structures for cleanup."""
		async with self._lock:
			# Clear owned data (single source of truth)
			self._targets.clear()
			self._sessions.clear()
			self._target_sessions.clear()
			self._session_to_target.clear()

		self.logger.debug('[SessionManager] Cleared all owned data (targets, sessions, mappings)')

	async def is_target_valid(self, target_id: TargetID) -> bool:
		"""Check if a target is still valid and has active sessions.

		Args:
			target_id: Target ID to validate

		Returns:
			True if target is valid and has active sessions, False otherwise
		"""
		if target_id not in self._target_sessions:
			return False
		return len(self._target_sessions[target_id]) > 0

	def get_target_id_from_session_id(self, session_id: SessionID) -> TargetID | None:
		"""Look up which target a session belongs to.

		Args:
			session_id: The session ID to look up

		Returns:
			Target ID if found, None otherwise
		"""
		return self._session_to_target.get(session_id)

	def get_target(self, target_id: TargetID) -> 'Target | None':
		"""Get target from owned data.

		Args:
			target_id: Target ID to get

		Returns:
			Target object if found, None otherwise
		"""
		return self._targets.get(target_id)

	def get_all_targets(self) -> dict[TargetID, 'Target']:
		"""Get all targets (read-only access to owned data).

		Returns:
			Dict mapping target_id to Target objects
		"""
		return self._targets

	def get_all_target_ids(self) -> list[TargetID]:
		"""Get all target IDs from owned data.

		Returns:
			List of all target IDs
		"""
		return list(self._targets.keys())

	def get_all_sessions(self) -> dict[SessionID, 'CDPSession']:
		"""Get all sessions (read-only access to owned data).

		Returns:
			Dict mapping session_id to CDPSession objects
		"""
		return self._sessions

	def get_session(self, session_id: SessionID) -> 'CDPSession | None':
		"""Get session from owned data.

		Args:
			session_id: Session ID to get

		Returns:
			CDPSession object if found, None otherwise
		"""
		return self._sessions.get(session_id)

	def get_all_sessions_for_target(self, target_id: TargetID) -> list['CDPSession']:
		"""Get ALL sessions attached to a target from owned data.

		Args:
			target_id: Target ID to get sessions for

		Returns:
			List of all CDPSession objects for this target
		"""
		session_ids = self._target_sessions.get(target_id, set())
		return [self._sessions[sid] for sid in session_ids if sid in self._sessions]

	def get_target_sessions_mapping(self) -> dict[TargetID, set[SessionID]]:
		"""Get target->sessions mapping (read-only access).

		Returns:
			Dict mapping target_id to set of session_ids
		"""
		return self._target_sessions

	def get_focused_target(self) -> 'Target | None':
		"""Get the target that currently has agent focus.

		Convenience method that uses browser_session.agent_focus_target_id.

		Returns:
			Target object if agent has focus, None otherwise
		"""
		if not self.browser_session.agent_focus_target_id:
			return None
		return self.get_target(self.browser_session.agent_focus_target_id)

	async def ensure_valid_focus(self, timeout: float = 3.0) -> bool:
		"""Ensure agent_focus_target_id points to a valid, attached CDP session.

		If the focus target is stale (detached), this method waits for automatic recovery.
		Uses event-driven coordination instead of polling for efficiency.

		Args:
			timeout: Maximum time to wait for recovery in seconds (default: 3.0)

		Returns:
			True if focus is valid or successfully recovered, False if no focus or recovery failed
		"""
		if not self.browser_session.agent_focus_target_id:
			# No focus at all - might be initial state or complete failure
			if self._recovery_in_progress and self._recovery_complete_event:
				# Recovery is happening, wait for it
				try:
					await asyncio.wait_for(self._recovery_complete_event.wait(), timeout=timeout)
					# Check again after recovery - simple existence check
					focus_id = self.browser_session.agent_focus_target_id
					return bool(focus_id and self._get_session_for_target(focus_id))
				except TimeoutError:
					self.logger.error(f'[SessionManager] âŒ Timed out waiting for recovery after {timeout}s')
					return False
			return False

		# Simple existence check - does the focused target have a session?
		cdp_session = self._get_session_for_target(self.browser_session.agent_focus_target_id)
		if cdp_session:
			# Session exists - validate it's still active
			is_valid = await self.validate_session(self.browser_session.agent_focus_target_id)
			if is_valid:
				return True

		# Focus is stale - wait for recovery using event instead of polling
		stale_target_id = self.browser_session.agent_focus_target_id
		self.logger.warning(
			f'[SessionManager] âš ï¸ Stale agent_focus detected (target {stale_target_id[:8] if stale_target_id else "None"}... detached), '
			f'waiting for recovery...'
		)

		# Check if recovery is already in progress
		if not self._recovery_in_progress:
			self.logger.warning(
				'[SessionManager] âš ï¸ Recovery not in progress for stale focus! '
				'This indicates a bug - recovery should have been triggered.'
			)
			return False

		# Wait for recovery complete event (event-driven, not polling!)
		if self._recovery_complete_event:
			try:
				start_time = asyncio.get_event_loop().time()
				await asyncio.wait_for(self._recovery_complete_event.wait(), timeout=timeout)
				elapsed = asyncio.get_event_loop().time() - start_time

				# Verify recovery succeeded - simple existence check
				focus_id = self.browser_session.agent_focus_target_id
				if focus_id and self._get_session_for_target(focus_id):
					self.logger.info(
						f'[SessionManager] âœ… Agent focus recovered to {self.browser_session.agent_focus_target_id[:8]}... '
						f'after {elapsed * 1000:.0f}ms'
					)
					return True
				else:
					self.logger.error(
						f'[SessionManager] âŒ Recovery completed but focus still invalid after {elapsed * 1000:.0f}ms'
					)
					return False

			except TimeoutError:
				self.logger.error(
					f'[SessionManager] âŒ Recovery timed out after {timeout}s '
					f'(was: {stale_target_id[:8] if stale_target_id else "None"}..., '
					f'now: {self.browser_session.agent_focus_target_id[:8] if self.browser_session.agent_focus_target_id else "None"})'
				)
				return False
		else:
			self.logger.error('[SessionManager] âŒ Recovery event not initialized')
			return False

	async def _handle_target_attached(self, event: AttachedToTargetEvent) -> None:
		"""Handle Target.attachedToTarget event.

		Called automatically by Chrome when a new target/session is created.
		This is the ONLY place where sessions are added to the pool.
		"""
		target_id = event['targetInfo']['targetId']
		session_id = event['sessionId']
		target_type = event['targetInfo']['type']
		target_info = event['targetInfo']
		waiting_for_debugger = event.get('waitingForDebugger', False)

		self.logger.debug(
			f'[SessionManager] Target attached: {target_id[:8]}... (session={session_id[:8]}..., '
			f'type={target_type}, waitingForDebugger={waiting_for_debugger})'
		)

		# Defensive check: browser may be shutting down and _cdp_client_root could be None
		if self.browser_session._cdp_client_root is None:
			self.logger.debug(
				f'[SessionManager] Skipping target attach for {target_id[:8]}... - browser shutting down (no CDP client)'
			)
			return

		# Enable auto-attach for this session's children (do this FIRST, outside lock)
		try:
			await self.browser_session._cdp_client_root.send.Target.setAutoAttach(
				params={'autoAttach': True, 'waitForDebuggerOnStart': False, 'flatten': True}, session_id=session_id
			)
		except Exception as e:
			error_str = str(e)
			# Expected for short-lived targets (workers, temp iframes) that detach before this executes
			if '-32001' not in error_str and 'Session with given id not found' not in error_str:
				self.logger.debug(f'[SessionManager] Auto-attach failed for {target_type}: {e}')

		async with self._lock:
			# Track this session for the target
			if target_id not in self._target_sessions:
				self._target_sessions[target_id] = set()

			self._target_sessions[target_id].add(session_id)
			self._session_to_target[session_id] = target_id

		# Create or update Target (source of truth for url/title)
		if target_id not in self._targets:
			from browser_use.browser.session import Target

			target = Target(
				target_id=target_id,
				target_type=target_type,
				url=target_info.get('url', 'about:blank'),
				title=target_info.get('title', 'Unknown title'),
			)
			self._targets[target_id] = target
			self.logger.debug(f'[SessionManager] Created target {target_id[:8]}... (type={target_type})')
		else:
			# Update existing target info
			existing_target = self._targets[target_id]
			existing_target.url = target_info.get('url', existing_target.url)
			existing_target.title = target_info.get('title', existing_target.title)

		# Create CDPSession (communication channel)
		from browser_use.browser.session import CDPSession

		assert self.browser_session._cdp_client_root is not None, 'Root CDP client required'

		cdp_session = CDPSession(
			cdp_client=self.browser_session._cdp_client_root,
			target_id=target_id,
			session_id=session_id,
		)

		# Add to sessions dict
		self._sessions[session_id] = cdp_session

		self.logger.debug(
			f'[SessionManager] Created session {session_id[:8]}... for target {target_id[:8]}... '
			f'(total sessions: {len(self._sessions)})'
		)

		# Enable lifecycle events and network monitoring for page targets
		if target_type in ('page', 'tab'):
			await self._enable_page_monitoring(cdp_session)

		# Resume execution if waiting for debugger
		if waiting_for_debugger:
			try:
				assert self.browser_session._cdp_client_root is not None
				await self.browser_session._cdp_client_root.send.Runtime.runIfWaitingForDebugger(session_id=session_id)
			except Exception as e:
				self.logger.warning(f'[SessionManager] Failed to resume execution: {e}')

		if target_type in ('page', 'tab') and self.browser_session.browser_profile.demo_mode:
			demo = self.browser_session.demo_mode
			if demo:
				try:
					await demo.register_new_target(target_id)
				except Exception as exc:
					self.logger.debug(f'[SessionManager] Failed to register demo overlay for {target_id[:8]}...: {exc}')

	async def _handle_target_info_changed(self, event: dict) -> None:
		"""Handle Target.targetInfoChanged event.

		Updates target title/URL without polling getTargetInfo().
		Chrome fires this automatically when title or URL changes.
		"""
		target_info = event.get('targetInfo', {})
		target_id = target_info.get('targetId')

		if not target_id:
			return

		url_changed = False
		target_type = None

		async with self._lock:
			# Update target if it exists (source of truth for url/title)
			if target_id in self._targets:
				target = self._targets[target_id]
				target_type = target.target_type
				previous_url = target.url
				new_url = target_info.get('url', previous_url)

				target.title = target_info.get('title', target.title)
				target.url = new_url
				url_changed = previous_url != new_url

		if url_changed and target_type in ('page', 'tab') and self.browser_session.browser_profile.demo_mode:
			demo = self.browser_session.demo_mode
			if demo:
				try:
					await demo.refresh_target(target_id)
				except Exception as exc:
					self.logger.debug(
						f'[SessionManager] Failed to refresh demo overlay after URL change for {target_id[:8]}...: {exc}'
					)

	async def _handle_target_detached(self, event: DetachedFromTargetEvent) -> None:
		"""Handle Target.detachedFromTarget event.

		Called automatically by Chrome when a target/session is destroyed.
		This is the ONLY place where sessions are removed from the pool.
		"""
		session_id = event['sessionId']
		target_id = event.get('targetId')  # May be empty

		# If targetId not in event, look it up via session mapping
		if not target_id:
			async with self._lock:
				target_id = self._session_to_target.get(session_id)

		if not target_id:
			self.logger.warning(f'[SessionManager] Session detached but target unknown (session={session_id[:8]}...)')
			return

		agent_focus_lost = False
		target_fully_removed = False
		target_type = None

		async with self._lock:
			# Remove this session from target's session set
			if target_id in self._target_sessions:
				self._target_sessions[target_id].discard(session_id)

				remaining_sessions = len(self._target_sessions[target_id])

				self.logger.debug(
					f'[SessionManager] Session detached: target={target_id[:8]}... '
					f'session={session_id[:8]}... (remaining={remaining_sessions})'
				)

				# Only remove target when NO sessions remain
				if remaining_sessions == 0:
					self.logger.debug(f'[SessionManager] No sessions remain for target {target_id[:8]}..., removing target')

					target_fully_removed = True

					# Check if agent_focus points to this target
					agent_focus_lost = self.browser_session.agent_focus_target_id == target_id

					# Immediately clear stale focus to prevent operations on detached target
					if agent_focus_lost:
						self.logger.debug(
							f'[SessionManager] Clearing stale agent_focus_target_id {target_id[:8]}... '
							f'to prevent operations on detached target'
						)
						self.browser_session.agent_focus_target_id = None

					# Get target type before removing (needed for TabClosedEvent dispatch)
					target = self._targets.get(target_id)
					target_type = target.target_type if target else None

					# Remove target (entity) from owned data
					if target_id in self._targets:
						self._targets.pop(target_id)
						self.logger.debug(
							f'[SessionManager] Removed target {target_id[:8]}... (remaining targets: {len(self._targets)})'
						)

					# Clean up tracking
					del self._target_sessions[target_id]
			else:
				# Target not tracked - already removed or never attached
				self.logger.debug(
					f'[SessionManager] Session detached from untracked target: target={target_id[:8]}... '
					f'session={session_id[:8]}... (target was already removed or attach event was missed)'
				)

			# Remove session from owned sessions dict
			if session_id in self._sessions:
				self._sessions.pop(session_id)
				self.logger.debug(
					f'[SessionManager] Removed session {session_id[:8]}... (remaining sessions: {len(self._sessions)})'
				)

			# Remove from reverse mapping
			if session_id in self._session_to_target:
				del self._session_to_target[session_id]

		# Dispatch TabClosedEvent only for page/tab targets that are fully removed (not iframes/workers or partial detaches)
		if target_fully_removed:
			if target_type in ('page', 'tab'):
				from browser_use.browser.events import TabClosedEvent

				self.browser_session.event_bus.dispatch(TabClosedEvent(target_id=target_id))
				self.logger.debug(f'[SessionManager] Dispatched TabClosedEvent for page target {target_id[:8]}...')
				demo = self.browser_session.demo_mode
				if demo:
					demo.unregister_target(target_id)
			elif target_type:
				self.logger.debug(
					f'[SessionManager] Target {target_id[:8]}... fully removed (type={target_type}) - not dispatching TabClosedEvent'
				)

		# Auto-recover agent_focus outside the lock to avoid blocking other operations
		if agent_focus_lost:
			# Create recovery task instead of awaiting directly - allows concurrent operations to wait on same recovery
			if not self._recovery_in_progress:
				self._recovery_task = create_task_with_error_handling(
					self._recover_agent_focus(target_id),
					name='recover_agent_focus',
					logger_instance=self.logger,
					suppress_exceptions=False,
				)

	async def _recover_agent_focus(self, crashed_target_id: TargetID) -> None:
		"""Auto-recover agent_focus when the focused target crashes/detaches.

		Uses recovery lock to prevent concurrent recovery attempts from creating multiple emergency tabs.
		Coordinates with ensure_valid_focus() via events for efficient waiting.

		Args:
			crashed_target_id: The target ID that was lost
		"""
		try:
			# Prevent concurrent recovery attempts
			async with self._recovery_lock:
				# Set recovery state INSIDE lock to prevent race conditions
				if self._recovery_in_progress:
					self.logger.debug('[SessionManager] Recovery already in progress, waiting for it to complete')
					# Wait for ongoing recovery instead of starting a new one
					if self._recovery_complete_event:
						try:
							await asyncio.wait_for(self._recovery_complete_event.wait(), timeout=5.0)
						except TimeoutError:
							self.logger.error('[SessionManager] Timed out waiting for ongoing recovery')
					return

				# Set recovery state
				self._recovery_in_progress = True
				self._recovery_complete_event = asyncio.Event()

				if self.browser_session._cdp_client_root is None:
					self.logger.debug('[SessionManager] Skipping focus recovery - browser shutting down (no CDP client)')
					return

				# Check if another recovery already fixed agent_focus
				if self.browser_session.agent_focus_target_id and self.browser_session.agent_focus_target_id != crashed_target_id:
					self.logger.debug(
						f'[SessionManager] Agent focus already recovered by concurrent operation '
						f'(now: {self.browser_session.agent_focus_target_id[:8]}...), skipping recovery'
					)
					return

				# Note: agent_focus_target_id may already be None (cleared in _handle_target_detached)
				current_focus_desc = (
					f'{self.browser_session.agent_focus_target_id[:8]}...'
					if self.browser_session.agent_focus_target_id
					else 'None (already cleared)'
				)

				self.logger.warning(
					f'[SessionManager] Agent focus target {crashed_target_id[:8]}... detached! '
					f'Current focus: {current_focus_desc}. Auto-recovering by switching to another target...'
				)

			# Perform recovery (outside lock to allow concurrent operations)
			# Try to find another valid page target
			page_targets = self.get_all_page_targets()

			new_target_id = None
			is_existing_tab = False

			if page_targets:
				# Switch to most recent page that's not the crashed one
				new_target_id = page_targets[-1].target_id
				is_existing_tab = True
				self.logger.info(f'[SessionManager] Switching agent_focus to existing tab {new_target_id[:8]}...')
			else:
				# No pages exist - create a new one
				self.logger.warning('[SessionManager] No tabs remain! Creating new tab for agent...')
				new_target_id = await self.browser_session._cdp_create_new_page('about:blank')
				self.logger.info(f'[SessionManager] Created new tab {new_target_id[:8]}... for agent')

				# Dispatch TabCreatedEvent so watchdogs can initialize
				from browser_use.browser.events import TabCreatedEvent

				self.browser_session.event_bus.dispatch(TabCreatedEvent(url='about:blank', target_id=new_target_id))

			# Wait for CDP attach event to create session
			# Note: This polling is necessary - waiting for external Chrome CDP event
			# _handle_target_attached will add session to pool when Chrome fires attachedToTarget
			new_session = None
			for attempt in range(20):  # Wait up to 2 seconds
				await asyncio.sleep(0.1)
				new_session = self._get_session_for_target(new_target_id)
				if new_session:
					break

			if new_session:
				self.browser_session.agent_focus_target_id = new_target_id
				self.logger.info(f'[SessionManager] âœ… Agent focus recovered: {new_target_id[:8]}...')

				# Visually activate the tab in browser (only for existing tabs)
				if is_existing_tab:
					try:
						assert self.browser_session._cdp_client_root is not None
						await self.browser_session._cdp_client_root.send.Target.activateTarget(params={'targetId': new_target_id})
						self.logger.debug(f'[SessionManager] Activated tab {new_target_id[:8]}... in browser UI')
					except Exception as e:
						self.logger.debug(f'[SessionManager] Failed to activate tab visually: {e}')

				# Get target to access url (from owned data)
				target = self.get_target(new_target_id)
				target_url = target.url if target else 'about:blank'

				# Dispatch focus changed event
				from browser_use.browser.events import AgentFocusChangedEvent

				self.browser_session.event_bus.dispatch(AgentFocusChangedEvent(target_id=new_target_id, url=target_url))
				return

			# Recovery failed - create emergency fallback tab
			self.logger.error(
				f'[SessionManager] âŒ Failed to get session for {new_target_id[:8]}... after 2s, creating emergency fallback tab'
			)

			fallback_target_id = await self.browser_session._cdp_create_new_page('about:blank')
			self.logger.warning(f'[SessionManager] Created emergency fallback tab {fallback_target_id[:8]}...')

			# Try one more time with fallback
			# Note: This polling is necessary - waiting for external Chrome CDP event
			for _ in range(20):
				await asyncio.sleep(0.1)
				fallback_session = self._get_session_for_target(fallback_target_id)
				if fallback_session:
					self.browser_session.agent_focus_target_id = fallback_target_id
					self.logger.warning(f'[SessionManager] âš ï¸ Agent focus set to emergency fallback: {fallback_target_id[:8]}...')

					from browser_use.browser.events import AgentFocusChangedEvent, TabCreatedEvent

					self.browser_session.event_bus.dispatch(TabCreatedEvent(url='about:blank', target_id=fallback_target_id))
					self.browser_session.event_bus.dispatch(
						AgentFocusChangedEvent(target_id=fallback_target_id, url='about:blank')
					)
					return

			# Complete failure - this should never happen
			self.logger.critical(
				'[SessionManager] ğŸš¨ CRITICAL: Failed to recover agent_focus even with fallback! Agent may be in broken state.'
			)

		except Exception as e:
			self.logger.error(f'[SessionManager] âŒ Error during agent_focus recovery: {type(e).__name__}: {e}')
		finally:
			# Always signal completion and reset recovery state
			# This allows all waiting operations to proceed (success or failure)
			if self._recovery_complete_event:
				self._recovery_complete_event.set()
			self._recovery_in_progress = False
			self._recovery_task = None
			self.logger.debug('[SessionManager] Recovery state reset')

	async def _initialize_existing_targets(self) -> None:
		"""Discover and initialize all existing targets at startup.

		Attaches to each target and initializes it SYNCHRONOUSLY.
		Chrome will also fire attachedToTarget events, but _handle_target_attached() is
		idempotent (checks if target already in pool), so duplicate handling is safe.

		This eliminates race conditions - monitoring is guaranteed ready before navigation.
		"""
		cdp_client = self.browser_session._cdp_client_root
		assert cdp_client is not None

		# Get all existing targets
		targets_result = await cdp_client.send.Target.getTargets()
		existing_targets = targets_result.get('targetInfos', [])

		self.logger.debug(f'[SessionManager] Discovered {len(existing_targets)} existing targets')

		# Track target IDs for verification
		target_ids_to_wait_for = []

		# Just attach to ALL existing targets - Chrome fires attachedToTarget events
		# The on_attached handler (via create_task) does ALL the work
		for target in existing_targets:
			target_id = target['targetId']
			target_type = target.get('type', 'unknown')

			try:
				# Just attach - event handler does everything
				await cdp_client.send.Target.attachToTarget(params={'targetId': target_id, 'flatten': True})
				target_ids_to_wait_for.append(target_id)
			except Exception as e:
				self.logger.debug(
					f'[SessionManager] Failed to attach to existing target {target_id[:8]}... (type={target_type}): {e}'
				)

		# Wait for event handlers to complete their work (they run via create_task)
		# Use event-driven approach instead of polling for better performance
		ready_event = asyncio.Event()

		async def check_all_ready():
			"""Check if all sessions are ready and signal completion."""
			while True:
				ready_count = 0
				for tid in target_ids_to_wait_for:
					session = self._get_session_for_target(tid)
					if session:
						target = self._targets.get(tid)
						target_type = target.target_type if target else 'unknown'
						# For pages, verify monitoring is enabled
						if target_type in ('page', 'tab'):
							if hasattr(session, '_lifecycle_events') and session._lifecycle_events is not None:
								ready_count += 1
						else:
							# Non-page targets don't need monitoring
							ready_count += 1

				if ready_count == len(target_ids_to_wait_for):
					ready_event.set()
					return

				await asyncio.sleep(0.05)

		# Start checking in background
		check_task = create_task_with_error_handling(
			check_all_ready(), name='check_all_targets_ready', logger_instance=self.logger
		)

		try:
			# Wait for completion with timeout
			await asyncio.wait_for(ready_event.wait(), timeout=2.0)
		except TimeoutError:
			# Timeout - count what's ready
			ready_count = 0
			for tid in target_ids_to_wait_for:
				session = self._get_session_for_target(tid)
				if session:
					target = self._targets.get(tid)
					target_type = target.target_type if target else 'unknown'
					# For pages, verify monitoring is enabled
					if target_type in ('page', 'tab'):
						if hasattr(session, '_lifecycle_events') and session._lifecycle_events is not None:
							ready_count += 1
					else:
						# Non-page targets don't need monitoring
						ready_count += 1
			self.logger.warning(
				f'[SessionManager] Initialization timeout after 2.0s: {ready_count}/{len(target_ids_to_wait_for)} sessions ready'
			)
		finally:
			check_task.cancel()
			try:
				await check_task
			except asyncio.CancelledError:
				pass

	async def _enable_page_monitoring(self, cdp_session: 'CDPSession') -> None:
		"""Enable lifecycle events and network monitoring for a page target.

		This is called once per page when it's created, avoiding handler accumulation.
		Registers a SINGLE lifecycle handler per session that stores events for navigations to consume.

		Args:
			cdp_session: The CDP session to enable monitoring on
		"""
		try:
			# Enable Page domain first (required for lifecycle events)
			await cdp_session.cdp_client.send.Page.enable(session_id=cdp_session.session_id)

			# Enable lifecycle events (load, DOMContentLoaded, networkIdle, etc.)
			await cdp_session.cdp_client.send.Page.setLifecycleEventsEnabled(
				params={'enabled': True}, session_id=cdp_session.session_id
			)

			# Enable network monitoring for networkIdle detection
			await cdp_session.cdp_client.send.Network.enable(session_id=cdp_session.session_id)

			# Initialize lifecycle event storage for this session (thread-safe)
			from collections import deque

			cdp_session._lifecycle_events = deque(maxlen=50)  # Keep last 50 events
			cdp_session._lifecycle_lock = asyncio.Lock()

			# Register ONE handler per session that stores events
			def on_lifecycle_event(event, session_id=None):
				event_name = event.get('name', 'unknown')
				event_loader_id = event.get('loaderId', 'none')

				# Find which target this session belongs to
				target_id_from_event = None
				if session_id:
					target_id_from_event = self.get_target_id_from_session_id(session_id)

				# Check if this event is for our target
				if target_id_from_event == cdp_session.target_id:
					# Store event for navigations to consume
					event_data = {
						'name': event_name,
						'loaderId': event_loader_id,
						'timestamp': asyncio.get_event_loop().time(),
					}
					# Append is atomic in CPython
					try:
						cdp_session._lifecycle_events.append(event_data)
					except Exception as e:
						# Only log errors, not every event
						self.logger.error(f'[SessionManager] Failed to store lifecycle event: {e}')

			# Register the handler ONCE (this is the only place we register)
			cdp_session.cdp_client.register.Page.lifecycleEvent(on_lifecycle_event)

		except Exception as e:
			# Don't fail - target might be short-lived or already detached
			error_str = str(e)
			if '-32001' in error_str or 'Session with given id not found' in error_str:
				self.logger.debug(
					f'[SessionManager] Target {cdp_session.target_id[:8]}... detached before monitoring could be enabled (normal for short-lived targets)'
				)
			else:
				self.logger.warning(
					f'[SessionManager] Failed to enable monitoring for target {cdp_session.target_id[:8]}...: {e}'
				)

```

---

## backend/browser-use/browser_use/browser/video_recorder.py

```py
"""Video Recording Service for Browser Use Sessions."""

import base64
import logging
import math
import subprocess
from pathlib import Path
from typing import Optional

from browser_use.browser.profile import ViewportSize

try:
	import imageio.v2 as iio  # type: ignore[import-not-found]
	import imageio_ffmpeg  # type: ignore[import-not-found]
	import numpy as np  # type: ignore[import-not-found]
	from imageio.core.format import Format  # type: ignore[import-not-found]

	IMAGEIO_AVAILABLE = True
except ImportError:
	IMAGEIO_AVAILABLE = False

logger = logging.getLogger(__name__)


def _get_padded_size(size: ViewportSize, macro_block_size: int = 16) -> ViewportSize:
	"""Calculates the dimensions padded to the nearest multiple of macro_block_size."""
	width = int(math.ceil(size['width'] / macro_block_size)) * macro_block_size
	height = int(math.ceil(size['height'] / macro_block_size)) * macro_block_size
	return ViewportSize(width=width, height=height)


class VideoRecorderService:
	"""
	Handles the video encoding process for a browser session using imageio.

	This service captures individual frames from the CDP screencast, decodes them,
	and appends them to a video file using a pip-installable ffmpeg backend.
	It automatically resizes frames to match the target video dimensions.
	"""

	def __init__(self, output_path: Path, size: ViewportSize, framerate: int):
		"""
		Initializes the video recorder.

		Args:
		    output_path: The full path where the video will be saved.
		    size: A ViewportSize object specifying the width and height of the video.
		    framerate: The desired framerate for the output video.
		"""
		self.output_path = output_path
		self.size = size
		self.framerate = framerate
		self._writer: Optional['Format.Writer'] = None
		self._is_active = False
		self.padded_size = _get_padded_size(self.size)

	def start(self) -> None:
		"""
		Prepares and starts the video writer.

		If the required optional dependencies are not installed, this method will
		log an error and do nothing.
		"""
		if not IMAGEIO_AVAILABLE:
			logger.error(
				'MP4 recording requires optional dependencies. Please install them with: pip install "browser-use[video]"'
			)
			return

		try:
			self.output_path.parent.mkdir(parents=True, exist_ok=True)
			# The macro_block_size is set to None because we handle padding ourselves
			self._writer = iio.get_writer(
				str(self.output_path),
				fps=self.framerate,
				codec='libx264',
				quality=8,  # A good balance of quality and file size (1-10 scale)
				pixelformat='yuv420p',  # Ensures compatibility with most players
				macro_block_size=None,
			)
			self._is_active = True
			logger.debug(f'Video recorder started. Output will be saved to {self.output_path}')
		except Exception as e:
			logger.error(f'Failed to initialize video writer: {e}')
			self._is_active = False

	def add_frame(self, frame_data_b64: str) -> None:
		"""
		Decodes a base64-encoded PNG frame, resizes it, pads it to be codec-compatible,
		and appends it to the video.

		Args:
		    frame_data_b64: A base64-encoded string of the PNG frame data.
		"""
		if not self._is_active or not self._writer:
			return

		try:
			frame_bytes = base64.b64decode(frame_data_b64)

			# Build a filter chain for ffmpeg:
			# 1. scale: Resizes the frame to the user-specified dimensions.
			# 2. pad: Adds black bars to meet codec's macro-block requirements,
			#    centering the original content.
			vf_chain = (
				f'scale={self.size["width"]}:{self.size["height"]},'
				f'pad={self.padded_size["width"]}:{self.padded_size["height"]}:(ow-iw)/2:(oh-ih)/2:color=black'
			)

			output_pix_fmt = 'rgb24'
			command = [
				imageio_ffmpeg.get_ffmpeg_exe(),
				'-f',
				'image2pipe',  # Input format from a pipe
				'-c:v',
				'png',  # Specify input codec is PNG
				'-i',
				'-',  # Input from stdin
				'-vf',
				vf_chain,  # Video filter for resizing and padding
				'-f',
				'rawvideo',  # Output format is raw video
				'-pix_fmt',
				output_pix_fmt,  # Output pixel format
				'-',  # Output to stdout
			]

			# Execute ffmpeg as a subprocess
			proc = subprocess.Popen(command, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
			out, err = proc.communicate(input=frame_bytes)

			if proc.returncode != 0:
				err_msg = err.decode(errors='ignore').strip()
				if 'deprecated pixel format used' not in err_msg.lower():
					raise OSError(f'ffmpeg error during resizing/padding: {err_msg}')
				else:
					logger.debug(f'ffmpeg warning during resizing/padding: {err_msg}')

			# Convert the raw output bytes to a numpy array with the padded dimensions
			img_array = np.frombuffer(out, dtype=np.uint8).reshape((self.padded_size['height'], self.padded_size['width'], 3))

			self._writer.append_data(img_array)
		except Exception as e:
			logger.warning(f'Could not process and add video frame: {e}')

	def stop_and_save(self) -> None:
		"""
		Finalizes the video file by closing the writer.

		This method should be called when the recording session is complete.
		"""
		if not self._is_active or not self._writer:
			return

		try:
			self._writer.close()
			logger.info(f'ğŸ“¹ Video recording saved successfully to: {self.output_path}')
		except Exception as e:
			logger.error(f'Failed to finalize and save video: {e}')
		finally:
			self._is_active = False
			self._writer = None

```

---

## backend/browser-use/browser_use/browser/views.py

```py
from dataclasses import dataclass, field
from typing import Any

from bubus import BaseEvent
from cdp_use.cdp.target import TargetID
from pydantic import AliasChoices, BaseModel, ConfigDict, Field, field_serializer

from browser_use.dom.views import DOMInteractedElement, SerializedDOMState

# Known placeholder image data for about:blank pages - a 4x4 white PNG
PLACEHOLDER_4PX_SCREENSHOT = (
	'iVBORw0KGgoAAAANSUhEUgAAAAQAAAAECAIAAAAmkwkpAAAAFElEQVR4nGP8//8/AwwwMSAB3BwAlm4DBfIlvvkAAAAASUVORK5CYII='
)


# Pydantic
class TabInfo(BaseModel):
	"""Represents information about a browser tab"""

	model_config = ConfigDict(
		extra='forbid',
		validate_by_name=True,
		validate_by_alias=True,
		populate_by_name=True,
	)

	# Original fields
	url: str
	title: str
	target_id: TargetID = Field(serialization_alias='tab_id', validation_alias=AliasChoices('tab_id', 'target_id'))
	parent_target_id: TargetID | None = Field(
		default=None, serialization_alias='parent_tab_id', validation_alias=AliasChoices('parent_tab_id', 'parent_target_id')
	)  # parent page that contains this popup or cross-origin iframe

	@field_serializer('target_id')
	def serialize_target_id(self, target_id: TargetID, _info: Any) -> str:
		return target_id[-4:]

	@field_serializer('parent_target_id')
	def serialize_parent_target_id(self, parent_target_id: TargetID | None, _info: Any) -> str | None:
		return parent_target_id[-4:] if parent_target_id else None


class PageInfo(BaseModel):
	"""Comprehensive page size and scroll information"""

	# Current viewport dimensions
	viewport_width: int
	viewport_height: int

	# Total page dimensions
	page_width: int
	page_height: int

	# Current scroll position
	scroll_x: int
	scroll_y: int

	# Calculated scroll information
	pixels_above: int
	pixels_below: int
	pixels_left: int
	pixels_right: int

	# Page statistics are now computed dynamically instead of stored


@dataclass
class NetworkRequest:
	"""Information about a pending network request"""

	url: str
	method: str = 'GET'
	loading_duration_ms: float = 0.0  # How long this request has been loading (ms since request started, max 10s)
	resource_type: str | None = None  # e.g., 'Document', 'Stylesheet', 'Image', 'Script', 'XHR', 'Fetch'


@dataclass
class PaginationButton:
	"""Information about a pagination button detected on the page"""

	button_type: str  # 'next', 'prev', 'first', 'last', 'page_number'
	backend_node_id: int  # Backend node ID for clicking
	text: str  # Button text/label
	selector: str  # XPath or other selector to locate the element
	is_disabled: bool = False  # Whether the button appears disabled


@dataclass
class BrowserStateSummary:
	"""The summary of the browser's current state designed for an LLM to process"""

	# provided by SerializedDOMState:
	dom_state: SerializedDOMState

	url: str
	title: str
	tabs: list[TabInfo]
	screenshot: str | None = field(default=None, repr=False)
	page_info: PageInfo | None = None  # Enhanced page information

	# Keep legacy fields for backward compatibility
	pixels_above: int = 0
	pixels_below: int = 0
	browser_errors: list[str] = field(default_factory=list)
	is_pdf_viewer: bool = False  # Whether the current page is a PDF viewer
	recent_events: str | None = None  # Text summary of recent browser events
	pending_network_requests: list[NetworkRequest] = field(default_factory=list)  # Currently loading network requests
	pagination_buttons: list[PaginationButton] = field(default_factory=list)  # Detected pagination buttons
	closed_popup_messages: list[str] = field(default_factory=list)  # Messages from auto-closed JavaScript dialogs


@dataclass
class BrowserStateHistory:
	"""The summary of the browser's state at a past point in time to usse in LLM message history"""

	url: str
	title: str
	tabs: list[TabInfo]
	interacted_element: list[DOMInteractedElement | None] | list[None]
	screenshot_path: str | None = None

	def get_screenshot(self) -> str | None:
		"""Load screenshot from disk and return as base64 string"""
		if not self.screenshot_path:
			return None

		import base64
		from pathlib import Path

		path_obj = Path(self.screenshot_path)
		if not path_obj.exists():
			return None

		try:
			with open(path_obj, 'rb') as f:
				screenshot_data = f.read()
			return base64.b64encode(screenshot_data).decode('utf-8')
		except Exception:
			return None

	def to_dict(self) -> dict[str, Any]:
		data = {}
		data['tabs'] = [tab.model_dump() for tab in self.tabs]
		data['screenshot_path'] = self.screenshot_path
		data['interacted_element'] = [el.to_dict() if el else None for el in self.interacted_element]
		data['url'] = self.url
		data['title'] = self.title
		return data


class BrowserError(Exception):
	"""Browser error with structured memory for LLM context management.

	This exception class provides separate memory contexts for browser actions:
	- short_term_memory: Immediate context shown once to the LLM for the next action
	- long_term_memory: Persistent error information stored across steps
	"""

	message: str
	short_term_memory: str | None = None
	long_term_memory: str | None = None
	details: dict[str, Any] | None = None
	while_handling_event: BaseEvent[Any] | None = None

	def __init__(
		self,
		message: str,
		short_term_memory: str | None = None,
		long_term_memory: str | None = None,
		details: dict[str, Any] | None = None,
		event: BaseEvent[Any] | None = None,
	):
		"""Initialize a BrowserError with structured memory contexts.

		Args:
			message: Technical error message for logging and debugging
			short_term_memory: Context shown once to LLM (e.g., available actions, options)
			long_term_memory: Persistent error info stored in agent memory
			details: Additional metadata for debugging
			event: The browser event that triggered this error
		"""
		self.message = message
		self.short_term_memory = short_term_memory
		self.long_term_memory = long_term_memory
		self.details = details
		self.while_handling_event = event
		super().__init__(message)

	def __str__(self) -> str:
		if self.details:
			return f'{self.message} ({self.details}) during: {self.while_handling_event}'
		elif self.while_handling_event:
			return f'{self.message} (while handling: {self.while_handling_event})'
		else:
			return self.message


class URLNotAllowedError(BrowserError):
	"""Error raised when a URL is not allowed"""

```

---

## backend/browser-use/browser_use/browser/watchdog_base.py

```py
"""Base watchdog class for browser monitoring components."""

import inspect
import time
from collections.abc import Iterable
from typing import Any, ClassVar

from bubus import BaseEvent, EventBus
from pydantic import BaseModel, ConfigDict, Field

from browser_use.browser.session import BrowserSession


class BaseWatchdog(BaseModel):
	"""Base class for all browser watchdogs.

	Watchdogs monitor browser state and emit events based on changes.
	They automatically register event handlers based on method names.

	Handler methods should be named: on_EventTypeName(self, event: EventTypeName)
	"""

	model_config = ConfigDict(
		arbitrary_types_allowed=True,  # allow non-serializable objects like EventBus/BrowserSession in fields
		extra='forbid',  # dont allow implicit class/instance state, everything must be a properly typed Field or PrivateAttr
		validate_assignment=False,  # avoid re-triggering  __init__ / validators on values on every assignment
		revalidate_instances='never',  # avoid re-triggering __init__ / validators and erasing private attrs
	)

	# Class variables to statically define the list of events relevant to each watchdog
	# (not enforced, just to make it easier to understand the code and debug watchdogs at runtime)
	LISTENS_TO: ClassVar[list[type[BaseEvent[Any]]]] = []  # Events this watchdog listens to
	EMITS: ClassVar[list[type[BaseEvent[Any]]]] = []  # Events this watchdog emits

	# Core dependencies
	event_bus: EventBus = Field()
	browser_session: BrowserSession = Field()

	# Shared state that other watchdogs might need to access should not be defined on BrowserSession, not here!
	# Shared helper methods needed by other watchdogs should be defined on BrowserSession, not here!
	# Alternatively, expose some events on the watchdog to allow access to state/helpers via event_bus system.

	# Private state internal to the watchdog can be defined like this on BaseWatchdog subclasses:
	# _screenshot_cache: dict[str, bytes] = PrivateAttr(default_factory=dict)
	# _browser_crash_watcher_task: asyncio.Task | None = PrivateAttr(default=None)
	# _cdp_download_tasks: WeakSet[asyncio.Task] = PrivateAttr(default_factory=WeakSet)
	# ...

	@property
	def logger(self):
		"""Get the logger from the browser session."""
		return self.browser_session.logger

	@staticmethod
	def attach_handler_to_session(browser_session: 'BrowserSession', event_class: type[BaseEvent[Any]], handler) -> None:
		"""Attach a single event handler to a browser session.

		Args:
			browser_session: The browser session to attach to
			event_class: The event class to listen for
			handler: The handler method (must start with 'on_' and end with event type)
		"""
		event_bus = browser_session.event_bus

		# Validate handler naming convention
		assert hasattr(handler, '__name__'), 'Handler must have a __name__ attribute'
		assert handler.__name__.startswith('on_'), f'Handler {handler.__name__} must start with "on_"'
		assert handler.__name__.endswith(event_class.__name__), (
			f'Handler {handler.__name__} must end with event type {event_class.__name__}'
		)

		# Get the watchdog instance if this is a bound method
		watchdog_instance = getattr(handler, '__self__', None)
		watchdog_class_name = watchdog_instance.__class__.__name__ if watchdog_instance else 'Unknown'

		# Create a wrapper function with unique name to avoid duplicate handler warnings
		# Capture handler by value to avoid closure issues
		def make_unique_handler(actual_handler):
			async def unique_handler(event):
				# just for debug logging, not used for anything else
				parent_event = event_bus.event_history.get(event.event_parent_id) if event.event_parent_id else None
				grandparent_event = (
					event_bus.event_history.get(parent_event.event_parent_id)
					if parent_event and parent_event.event_parent_id
					else None
				)
				parent = (
					f'â†²  triggered by on_{parent_event.event_type}#{parent_event.event_id[-4:]}'
					if parent_event
					else 'ğŸ‘ˆ by Agent'
				)
				grandparent = (
					(
						f'â†²  under {grandparent_event.event_type}#{grandparent_event.event_id[-4:]}'
						if grandparent_event
						else 'ğŸ‘ˆ by Agent'
					)
					if parent_event
					else ''
				)
				event_str = f'#{event.event_id[-4:]}'
				time_start = time.time()
				watchdog_and_handler_str = f'[{watchdog_class_name}.{actual_handler.__name__}({event_str})]'.ljust(54)
				browser_session.logger.debug(f'ğŸšŒ {watchdog_and_handler_str} â³ Starting...       {parent} {grandparent}')

				try:
					# **EXECUTE THE EVENT HANDLER FUNCTION**
					result = await actual_handler(event)

					if isinstance(result, Exception):
						raise result

					# just for debug logging, not used for anything else
					time_end = time.time()
					time_elapsed = time_end - time_start
					result_summary = '' if result is None else f' â¡ï¸ <{type(result).__name__}>'
					parents_summary = f' {parent}'.replace('â†²  triggered by ', 'â¤´  returned to  ').replace(
						'ğŸ‘ˆ by Agent', 'ğŸ‘‰ returned to  Agent'
					)
					browser_session.logger.debug(
						f'ğŸšŒ {watchdog_and_handler_str} Succeeded ({time_elapsed:.2f}s){result_summary}{parents_summary}'
					)
					return result
				except Exception as e:
					time_end = time.time()
					time_elapsed = time_end - time_start
					original_error = e
					browser_session.logger.error(
						f'ğŸšŒ {watchdog_and_handler_str} âŒ Failed ({time_elapsed:.2f}s): {type(e).__name__}: {e}'
					)

					# attempt to repair potentially crashed CDP session
					try:
						if browser_session.agent_focus_target_id:
							# With event-driven sessions, Chrome will send detach/attach events
							# SessionManager handles pool cleanup automatically
							target_id_to_restore = browser_session.agent_focus_target_id
							browser_session.logger.debug(
								f'ğŸšŒ {watchdog_and_handler_str} âš ï¸ Session error detected, waiting for CDP events to sync (target: {target_id_to_restore})'
							)

							# Wait for new attach event to restore the session
							# This will raise ValueError if target doesn't re-attach
							await browser_session.get_or_create_cdp_session(target_id=target_id_to_restore, focus=True)
						else:
							# Try to get any available session
							await browser_session.get_or_create_cdp_session(target_id=None, focus=True)
					except Exception as sub_error:
						if 'ConnectionClosedError' in str(type(sub_error)) or 'ConnectionError' in str(type(sub_error)):
							browser_session.logger.error(
								f'ğŸšŒ {watchdog_and_handler_str} âŒ Browser closed or CDP Connection disconnected by remote. {type(sub_error).__name__}: {sub_error}\n'
							)
							raise
						else:
							browser_session.logger.error(
								f'ğŸšŒ {watchdog_and_handler_str} âŒ CDP connected but failed to re-create CDP session after error "{type(original_error).__name__}: {original_error}" in {actual_handler.__name__}({event.event_type}#{event.event_id[-4:]}): due to {type(sub_error).__name__}: {sub_error}\n'
							)

					# Always re-raise the original error with its traceback preserved
					raise

			return unique_handler

		unique_handler = make_unique_handler(handler)
		unique_handler.__name__ = f'{watchdog_class_name}.{handler.__name__}'

		# Check if this handler is already registered - throw error if duplicate
		existing_handlers = event_bus.handlers.get(event_class.__name__, [])
		handler_names = [getattr(h, '__name__', str(h)) for h in existing_handlers]

		if unique_handler.__name__ in handler_names:
			raise RuntimeError(
				f'[{watchdog_class_name}] Duplicate handler registration attempted! '
				f'Handler {unique_handler.__name__} is already registered for {event_class.__name__}. '
				f'This likely means attach_to_session() was called multiple times.'
			)

		event_bus.on(event_class, unique_handler)

	def attach_to_session(self) -> None:
		"""Attach watchdog to its browser session and start monitoring.

		This method handles event listener registration. The watchdog is already
		bound to a browser session via self.browser_session from initialization.
		"""
		# Register event handlers automatically based on method names
		assert self.browser_session is not None, 'Root CDP client not initialized - browser may not be connected yet'

		from browser_use.browser import events

		event_classes = {}
		for name in dir(events):
			obj = getattr(events, name)
			if inspect.isclass(obj) and issubclass(obj, BaseEvent) and obj is not BaseEvent:
				event_classes[name] = obj

		# Find all handler methods (on_EventName)
		registered_events = set()
		for method_name in dir(self):
			if method_name.startswith('on_') and callable(getattr(self, method_name)):
				# Extract event name from method name (on_EventName -> EventName)
				event_name = method_name[3:]  # Remove 'on_' prefix

				if event_name in event_classes:
					event_class = event_classes[event_name]

					# ASSERTION: If LISTENS_TO is defined, enforce it
					if self.LISTENS_TO:
						assert event_class in self.LISTENS_TO, (
							f'[{self.__class__.__name__}] Handler {method_name} listens to {event_name} '
							f'but {event_name} is not declared in LISTENS_TO: {[e.__name__ for e in self.LISTENS_TO]}'
						)

					handler = getattr(self, method_name)

					# Use the static helper to attach the handler
					self.attach_handler_to_session(self.browser_session, event_class, handler)
					registered_events.add(event_class)

		# ASSERTION: If LISTENS_TO is defined, ensure all declared events have handlers
		if self.LISTENS_TO:
			missing_handlers = set(self.LISTENS_TO) - registered_events
			if missing_handlers:
				missing_names = [e.__name__ for e in missing_handlers]
				self.logger.warning(
					f'[{self.__class__.__name__}] LISTENS_TO declares {missing_names} '
					f'but no handlers found (missing on_{"_, on_".join(missing_names)} methods)'
				)

	def __del__(self) -> None:
		"""Clean up any running tasks during garbage collection."""

		# A BIT OF MAGIC: Cancel any private attributes that look like asyncio tasks
		try:
			for attr_name in dir(self):
				# e.g. _browser_crash_watcher_task = asyncio.Task
				if attr_name.startswith('_') and attr_name.endswith('_task'):
					try:
						task = getattr(self, attr_name)
						if hasattr(task, 'cancel') and callable(task.cancel) and not task.done():
							task.cancel()
							# self.logger.debug(f'[{self.__class__.__name__}] Cancelled {attr_name} during cleanup')
					except Exception:
						pass  # Ignore errors during cleanup

				# e.g. _cdp_download_tasks = WeakSet[asyncio.Task] or list[asyncio.Task]
				if attr_name.startswith('_') and attr_name.endswith('_tasks') and isinstance(getattr(self, attr_name), Iterable):
					for task in getattr(self, attr_name):
						try:
							if hasattr(task, 'cancel') and callable(task.cancel) and not task.done():
								task.cancel()
								# self.logger.debug(f'[{self.__class__.__name__}] Cancelled {attr_name} during cleanup')
						except Exception:
							pass  # Ignore errors during cleanup
		except Exception as e:
			from browser_use.utils import logger

			logger.error(f'âš ï¸ Error during BrowserSession {self.__class__.__name__} garbage collection __del__(): {type(e)}: {e}')

```

---

## backend/browser-use/browser_use/browser/watchdogs/__init__.py

```py

```

---

## backend/browser-use/browser_use/browser/watchdogs/aboutblank_watchdog.py

```py
"""About:blank watchdog for managing about:blank tabs with DVD screensaver."""

from typing import TYPE_CHECKING, ClassVar

from bubus import BaseEvent
from cdp_use.cdp.target import TargetID
from pydantic import PrivateAttr

from browser_use.browser.events import (
	AboutBlankDVDScreensaverShownEvent,
	BrowserStopEvent,
	BrowserStoppedEvent,
	CloseTabEvent,
	NavigateToUrlEvent,
	TabClosedEvent,
	TabCreatedEvent,
)
from browser_use.browser.watchdog_base import BaseWatchdog

if TYPE_CHECKING:
	pass


class AboutBlankWatchdog(BaseWatchdog):
	"""Ensures there's always exactly one about:blank tab with DVD screensaver."""

	# Event contracts
	LISTENS_TO: ClassVar[list[type[BaseEvent]]] = [
		BrowserStopEvent,
		BrowserStoppedEvent,
		TabCreatedEvent,
		TabClosedEvent,
	]
	EMITS: ClassVar[list[type[BaseEvent]]] = [
		NavigateToUrlEvent,
		CloseTabEvent,
		AboutBlankDVDScreensaverShownEvent,
	]

	_stopping: bool = PrivateAttr(default=False)

	async def on_BrowserStopEvent(self, event: BrowserStopEvent) -> None:
		"""Handle browser stop request - stop creating new tabs."""
		# logger.info('[AboutBlankWatchdog] Browser stop requested, stopping tab creation')
		self._stopping = True

	async def on_BrowserStoppedEvent(self, event: BrowserStoppedEvent) -> None:
		"""Handle browser stopped event."""
		# logger.info('[AboutBlankWatchdog] Browser stopped')
		self._stopping = True

	async def on_TabCreatedEvent(self, event: TabCreatedEvent) -> None:
		"""Check tabs when a new tab is created."""
		# logger.debug(f'[AboutBlankWatchdog] â• New tab created: {event.url}')

		# If an about:blank tab was created, show DVD screensaver on all about:blank tabs
		if event.url == 'about:blank':
			await self._show_dvd_screensaver_on_about_blank_tabs()

	async def on_TabClosedEvent(self, event: TabClosedEvent) -> None:
		"""Check tabs when a tab is closed and proactively create about:blank if needed."""
		# logger.debug('[AboutBlankWatchdog] Tab closing, checking if we need to create about:blank tab')

		# Don't create new tabs if browser is shutting down
		if self._stopping:
			# logger.debug('[AboutBlankWatchdog] Browser is stopping, not creating new tabs')
			return

		# Check if we're about to close the last tab (event happens BEFORE tab closes)
		# Use _cdp_get_all_pages for quick check without fetching titles
		page_targets = await self.browser_session._cdp_get_all_pages()
		if len(page_targets) <= 1:
			self.logger.debug(
				'[AboutBlankWatchdog] Last tab closing, creating new about:blank tab to avoid closing entire browser'
			)
			# Create the animation tab since no tabs should remain
			navigate_event = self.event_bus.dispatch(NavigateToUrlEvent(url='about:blank', new_tab=True))
			await navigate_event
			# Show DVD screensaver on the new tab
			await self._show_dvd_screensaver_on_about_blank_tabs()
		else:
			# Multiple tabs exist, check after close
			await self._check_and_ensure_about_blank_tab()

	async def attach_to_target(self, target_id: TargetID) -> None:
		"""AboutBlankWatchdog doesn't monitor individual targets."""
		pass

	async def _check_and_ensure_about_blank_tab(self) -> None:
		"""Check current tabs and ensure exactly one about:blank tab with animation exists."""
		try:
			# For quick checks, just get page targets without titles to reduce noise
			page_targets = await self.browser_session._cdp_get_all_pages()

			# If no tabs exist at all, create one to keep browser alive
			if len(page_targets) == 0:
				# Only create a new tab if there are no tabs at all
				self.logger.debug('[AboutBlankWatchdog] No tabs exist, creating new about:blank DVD screensaver tab')
				navigate_event = self.event_bus.dispatch(NavigateToUrlEvent(url='about:blank', new_tab=True))
				await navigate_event
				# Show DVD screensaver on the new tab
				await self._show_dvd_screensaver_on_about_blank_tabs()
			# Otherwise there are tabs, don't create new ones to avoid interfering

		except Exception as e:
			self.logger.error(f'[AboutBlankWatchdog] Error ensuring about:blank tab: {e}')

	async def _show_dvd_screensaver_on_about_blank_tabs(self) -> None:
		"""Show DVD screensaver on all about:blank pages only."""
		try:
			# Get just the page targets without expensive title fetching
			page_targets = await self.browser_session._cdp_get_all_pages()
			browser_session_label = str(self.browser_session.id)[-4:]

			for page_target in page_targets:
				target_id = page_target['targetId']
				url = page_target['url']

				# Only target about:blank pages specifically
				if url == 'about:blank':
					await self._show_dvd_screensaver_loading_animation_cdp(target_id, browser_session_label)

		except Exception as e:
			self.logger.error(f'[AboutBlankWatchdog] Error showing DVD screensaver: {e}')

	async def _show_dvd_screensaver_loading_animation_cdp(self, target_id: TargetID, browser_session_label: str) -> None:
		"""
		Injects a DVD screensaver-style bouncing logo loading animation overlay into the target using CDP.
		This is used to visually indicate that the browser is setting up or waiting.
		"""
		try:
			# Create temporary session for this target without switching focus
			temp_session = await self.browser_session.get_or_create_cdp_session(target_id, focus=False)

			# Inject the DVD screensaver script (from main branch with idempotency added)
			script = f"""
				(function(browser_session_label) {{
					// Idempotency check
					if (window.__dvdAnimationRunning) {{
						return; // Already running, don't add another
					}}
					window.__dvdAnimationRunning = true;
					
					// Ensure document.body exists before proceeding
					if (!document.body) {{
						// Try again after DOM is ready
						window.__dvdAnimationRunning = false; // Reset flag to retry
						if (document.readyState === 'loading') {{
							document.addEventListener('DOMContentLoaded', () => arguments.callee(browser_session_label));
						}}
						return;
					}}
					
					const animated_title = `Starting agent ${{browser_session_label}}...`;
					if (document.title === animated_title) {{
						return;      // already run on this tab, dont run again
					}}
					document.title = animated_title;

					// Create the main overlay
					const loadingOverlay = document.createElement('div');
					loadingOverlay.id = 'pretty-loading-animation';
					loadingOverlay.style.position = 'fixed';
					loadingOverlay.style.top = '0';
					loadingOverlay.style.left = '0';
					loadingOverlay.style.width = '100vw';
					loadingOverlay.style.height = '100vh';
					loadingOverlay.style.background = '#000';
					loadingOverlay.style.zIndex = '99999';
					loadingOverlay.style.overflow = 'hidden';

					// Create the image element
					const img = document.createElement('img');
					img.src = 'https://cf.browser-use.com/logo.svg';
					img.alt = 'Browser-Use';
					img.style.width = '200px';
					img.style.height = 'auto';
					img.style.position = 'absolute';
					img.style.left = '0px';
					img.style.top = '0px';
					img.style.zIndex = '2';
					img.style.opacity = '0.8';

					loadingOverlay.appendChild(img);
					document.body.appendChild(loadingOverlay);

					// DVD screensaver bounce logic
					let x = Math.random() * (window.innerWidth - 300);
					let y = Math.random() * (window.innerHeight - 300);
					let dx = 1.2 + Math.random() * 0.4; // px per frame
					let dy = 1.2 + Math.random() * 0.4;
					// Randomize direction
					if (Math.random() > 0.5) dx = -dx;
					if (Math.random() > 0.5) dy = -dy;

					function animate() {{
						const imgWidth = img.offsetWidth || 300;
						const imgHeight = img.offsetHeight || 300;
						x += dx;
						y += dy;

						if (x <= 0) {{
							x = 0;
							dx = Math.abs(dx);
						}} else if (x + imgWidth >= window.innerWidth) {{
							x = window.innerWidth - imgWidth;
							dx = -Math.abs(dx);
						}}
						if (y <= 0) {{
							y = 0;
							dy = Math.abs(dy);
						}} else if (y + imgHeight >= window.innerHeight) {{
							y = window.innerHeight - imgHeight;
							dy = -Math.abs(dy);
						}}

						img.style.left = `${{x}}px`;
						img.style.top = `${{y}}px`;

						requestAnimationFrame(animate);
					}}
					animate();

					// Responsive: update bounds on resize
					window.addEventListener('resize', () => {{
						x = Math.min(x, window.innerWidth - img.offsetWidth);
						y = Math.min(y, window.innerHeight - img.offsetHeight);
					}});

					// Add a little CSS for smoothness
					const style = document.createElement('style');
					style.textContent = `
						#pretty-loading-animation {{
							/*backdrop-filter: blur(2px) brightness(0.9);*/
						}}
						#pretty-loading-animation img {{
							user-select: none;
							pointer-events: none;
						}}
					`;
					document.head.appendChild(style);
				}})('{browser_session_label}');
			"""

			await temp_session.cdp_client.send.Runtime.evaluate(params={'expression': script}, session_id=temp_session.session_id)

			# No need to detach - session is cached

			# Dispatch event
			self.event_bus.dispatch(AboutBlankDVDScreensaverShownEvent(target_id=target_id))

		except Exception as e:
			self.logger.error(f'[AboutBlankWatchdog] Error injecting DVD screensaver: {e}')

```

---

## backend/browser-use/browser_use/browser/watchdogs/crash_watchdog.py

```py
"""Browser watchdog for monitoring crashes and network timeouts using CDP."""

import asyncio
import time
from typing import TYPE_CHECKING, ClassVar

import psutil
from bubus import BaseEvent
from cdp_use.cdp.target import SessionID, TargetID
from cdp_use.cdp.target.events import TargetCrashedEvent
from pydantic import Field, PrivateAttr

from browser_use.browser.events import (
	BrowserConnectedEvent,
	BrowserErrorEvent,
	BrowserStoppedEvent,
	TabClosedEvent,
	TabCreatedEvent,
)
from browser_use.browser.watchdog_base import BaseWatchdog
from browser_use.utils import create_task_with_error_handling

if TYPE_CHECKING:
	pass


class NetworkRequestTracker:
	"""Tracks ongoing network requests."""

	def __init__(self, request_id: str, start_time: float, url: str, method: str, resource_type: str | None = None):
		self.request_id = request_id
		self.start_time = start_time
		self.url = url
		self.method = method
		self.resource_type = resource_type


class CrashWatchdog(BaseWatchdog):
	"""Monitors browser health for crashes and network timeouts using CDP."""

	# Event contracts
	LISTENS_TO: ClassVar[list[type[BaseEvent]]] = [
		BrowserConnectedEvent,
		BrowserStoppedEvent,
		TabCreatedEvent,
		TabClosedEvent,
	]
	EMITS: ClassVar[list[type[BaseEvent]]] = [BrowserErrorEvent]

	# Configuration
	network_timeout_seconds: float = Field(default=10.0)
	check_interval_seconds: float = Field(default=5.0)  # Reduced frequency to reduce noise

	# Private state
	_active_requests: dict[str, NetworkRequestTracker] = PrivateAttr(default_factory=dict)
	_monitoring_task: asyncio.Task | None = PrivateAttr(default=None)
	_last_responsive_checks: dict[str, float] = PrivateAttr(default_factory=dict)  # target_url -> timestamp
	_cdp_event_tasks: set[asyncio.Task] = PrivateAttr(default_factory=set)  # Track CDP event handler tasks
	_targets_with_listeners: set[str] = PrivateAttr(default_factory=set)  # Track targets that already have event listeners

	async def on_BrowserConnectedEvent(self, event: BrowserConnectedEvent) -> None:
		"""Start monitoring when browser is connected."""
		# logger.debug('[CrashWatchdog] Browser connected event received, beginning monitoring')

		create_task_with_error_handling(
			self._start_monitoring(), name='start_crash_monitoring', logger_instance=self.logger, suppress_exceptions=True
		)
		# logger.debug(f'[CrashWatchdog] Monitoring task started: {self._monitoring_task and not self._monitoring_task.done()}')

	async def on_BrowserStoppedEvent(self, event: BrowserStoppedEvent) -> None:
		"""Stop monitoring when browser stops."""
		# logger.debug('[CrashWatchdog] Browser stopped, ending monitoring')
		await self._stop_monitoring()

	async def on_TabCreatedEvent(self, event: TabCreatedEvent) -> None:
		"""Attach to new tab."""
		assert self.browser_session.agent_focus_target_id is not None, 'No current target ID'
		await self.attach_to_target(self.browser_session.agent_focus_target_id)

	async def on_TabClosedEvent(self, event: TabClosedEvent) -> None:
		"""Clean up tracking when tab closes."""
		# Remove target from listener tracking to prevent memory leak
		if event.target_id in self._targets_with_listeners:
			self._targets_with_listeners.discard(event.target_id)
			self.logger.debug(f'[CrashWatchdog] Removed target {event.target_id[:8]}... from monitoring')

	async def attach_to_target(self, target_id: TargetID) -> None:
		"""Set up crash monitoring for a specific target using CDP."""
		try:
			# Check if we already have listeners for this target
			if target_id in self._targets_with_listeners:
				self.logger.debug(f'[CrashWatchdog] Event listeners already exist for target: {target_id[:8]}...')
				return

			# Create temporary session for monitoring without switching focus
			cdp_session = await self.browser_session.get_or_create_cdp_session(target_id, focus=False)

			# Register crash event handler
			def on_target_crashed(event: TargetCrashedEvent, session_id: SessionID | None = None):
				# Create and track the task
				task = create_task_with_error_handling(
					self._on_target_crash_cdp(target_id),
					name='handle_target_crash',
					logger_instance=self.logger,
					suppress_exceptions=True,
				)
				self._cdp_event_tasks.add(task)
				# Remove from set when done
				task.add_done_callback(lambda t: self._cdp_event_tasks.discard(t))

			cdp_session.cdp_client.register.Target.targetCrashed(on_target_crashed)

			# Track that we've added listeners to this target
			self._targets_with_listeners.add(target_id)

			target = self.browser_session.session_manager.get_target(target_id)
			if target:
				self.logger.debug(f'[CrashWatchdog] Added target to monitoring: {target.url}')

		except Exception as e:
			self.logger.warning(f'[CrashWatchdog] Failed to attach to target {target_id}: {e}')

	async def _on_request_cdp(self, event: dict) -> None:
		"""Track new network request from CDP event."""
		request_id = event.get('requestId', '')
		request = event.get('request', {})

		self._active_requests[request_id] = NetworkRequestTracker(
			request_id=request_id,
			start_time=time.time(),
			url=request.get('url', ''),
			method=request.get('method', ''),
			resource_type=event.get('type'),
		)
		# logger.debug(f'[CrashWatchdog] Tracking request: {request.get("method", "")} {request.get("url", "")[:50]}...')

	def _on_response_cdp(self, event: dict) -> None:
		"""Remove request from tracking on response."""
		request_id = event.get('requestId', '')
		if request_id in self._active_requests:
			elapsed = time.time() - self._active_requests[request_id].start_time
			response = event.get('response', {})
			self.logger.debug(f'[CrashWatchdog] Request completed in {elapsed:.2f}s: {response.get("url", "")[:50]}...')
			# Don't remove yet - wait for loadingFinished

	def _on_request_failed_cdp(self, event: dict) -> None:
		"""Remove request from tracking on failure."""
		request_id = event.get('requestId', '')
		if request_id in self._active_requests:
			elapsed = time.time() - self._active_requests[request_id].start_time
			self.logger.debug(
				f'[CrashWatchdog] Request failed after {elapsed:.2f}s: {self._active_requests[request_id].url[:50]}...'
			)
			del self._active_requests[request_id]

	def _on_request_finished_cdp(self, event: dict) -> None:
		"""Remove request from tracking when loading is finished."""
		request_id = event.get('requestId', '')
		self._active_requests.pop(request_id, None)

	async def _on_target_crash_cdp(self, target_id: TargetID) -> None:
		"""Handle target crash detected via CDP."""
		self.logger.debug(f'[CrashWatchdog] Target crashed: {target_id[:8]}..., waiting for detach event')

		target = self.browser_session.session_manager.get_target(target_id)

		is_agent_focus = (
			target
			and self.browser_session.agent_focus_target_id
			and target.target_id == self.browser_session.agent_focus_target_id
		)

		if is_agent_focus:
			self.logger.error(f'[CrashWatchdog] ğŸ’¥ Agent focus tab crashed: {target.url} (SessionManager will auto-recover)')

		# Emit browser error event
		self.event_bus.dispatch(
			BrowserErrorEvent(
				error_type='TargetCrash',
				message=f'Target crashed: {target_id}',
				details={
					'url': target.url if target else None,
					'target_id': target_id,
					'was_agent_focus': is_agent_focus,
				},
			)
		)

	async def _start_monitoring(self) -> None:
		"""Start the monitoring loop."""
		assert self.browser_session.cdp_client is not None, 'Root CDP client not initialized - browser may not be connected yet'

		if self._monitoring_task and not self._monitoring_task.done():
			# logger.info('[CrashWatchdog] Monitoring already running')
			return

		self._monitoring_task = create_task_with_error_handling(
			self._monitoring_loop(), name='crash_monitoring_loop', logger_instance=self.logger, suppress_exceptions=True
		)
		# logger.debug('[CrashWatchdog] Monitoring loop created and started')

	async def _stop_monitoring(self) -> None:
		"""Stop the monitoring loop and clean up all tracking."""
		if self._monitoring_task and not self._monitoring_task.done():
			self._monitoring_task.cancel()
			try:
				await self._monitoring_task
			except asyncio.CancelledError:
				pass
			self.logger.debug('[CrashWatchdog] Monitoring loop stopped')

		# Cancel all CDP event handler tasks
		for task in list(self._cdp_event_tasks):
			if not task.done():
				task.cancel()
		# Wait for all tasks to complete cancellation
		if self._cdp_event_tasks:
			await asyncio.gather(*self._cdp_event_tasks, return_exceptions=True)
		self._cdp_event_tasks.clear()

		# Clear all tracking
		self._active_requests.clear()
		self._targets_with_listeners.clear()
		self._last_responsive_checks.clear()

	async def _monitoring_loop(self) -> None:
		"""Main monitoring loop."""
		await asyncio.sleep(10)  # give browser time to start up and load the first page after first LLM call
		while True:
			try:
				await self._check_network_timeouts()
				await self._check_browser_health()
				await asyncio.sleep(self.check_interval_seconds)
			except asyncio.CancelledError:
				break
			except Exception as e:
				self.logger.error(f'[CrashWatchdog] Error in monitoring loop: {e}')

	async def _check_network_timeouts(self) -> None:
		"""Check for network requests exceeding timeout."""
		current_time = time.time()
		timed_out_requests = []

		# Debug logging
		if self._active_requests:
			self.logger.debug(
				f'[CrashWatchdog] Checking {len(self._active_requests)} active requests for timeouts (threshold: {self.network_timeout_seconds}s)'
			)

		for request_id, tracker in self._active_requests.items():
			elapsed = current_time - tracker.start_time
			self.logger.debug(
				f'[CrashWatchdog] Request {tracker.url[:30]}... elapsed: {elapsed:.1f}s, timeout: {self.network_timeout_seconds}s'
			)
			if elapsed >= self.network_timeout_seconds:
				timed_out_requests.append((request_id, tracker))

		# Emit events for timed out requests
		for request_id, tracker in timed_out_requests:
			self.logger.warning(
				f'[CrashWatchdog] Network request timeout after {self.network_timeout_seconds}s: '
				f'{tracker.method} {tracker.url[:100]}...'
			)

			self.event_bus.dispatch(
				BrowserErrorEvent(
					error_type='NetworkTimeout',
					message=f'Network request timed out after {self.network_timeout_seconds}s',
					details={
						'url': tracker.url,
						'method': tracker.method,
						'resource_type': tracker.resource_type,
						'elapsed_seconds': current_time - tracker.start_time,
					},
				)
			)

			# Remove from tracking
			del self._active_requests[request_id]

	async def _check_browser_health(self) -> None:
		"""Check if browser and targets are still responsive."""

		try:
			self.logger.debug(f'[CrashWatchdog] Checking browser health for target {self.browser_session.agent_focus_target_id}')
			cdp_session = await self.browser_session.get_or_create_cdp_session()

			for target in self.browser_session.session_manager.get_all_page_targets():
				if self._is_new_tab_page(target.url) and target.url != 'about:blank':
					self.logger.debug(f'[CrashWatchdog] Redirecting chrome://new-tab-page/ to about:blank {target.url}')
					cdp_session = await self.browser_session.get_or_create_cdp_session(target_id=target.target_id)
					await cdp_session.cdp_client.send.Page.navigate(
						params={'url': 'about:blank'}, session_id=cdp_session.session_id
					)

			# Quick ping to check if session is alive
			self.logger.debug(f'[CrashWatchdog] Attempting to run simple JS test expression in session {cdp_session} 1+1')
			await asyncio.wait_for(
				cdp_session.cdp_client.send.Runtime.evaluate(params={'expression': '1+1'}, session_id=cdp_session.session_id),
				timeout=1.0,
			)
			self.logger.debug(
				f'[CrashWatchdog] Browser health check passed for target {self.browser_session.agent_focus_target_id}'
			)
		except Exception as e:
			self.logger.error(
				f'[CrashWatchdog] âŒ Crashed/unresponsive session detected for target {self.browser_session.agent_focus_target_id} '
				f'error: {type(e).__name__}: {e} (Chrome will send detach event, SessionManager will auto-recover)'
			)

		# Check browser process if we have PID
		if self.browser_session._local_browser_watchdog and (proc := self.browser_session._local_browser_watchdog._subprocess):
			try:
				if proc.status() in (psutil.STATUS_ZOMBIE, psutil.STATUS_DEAD):
					self.logger.error(f'[CrashWatchdog] Browser process {proc.pid} has crashed')

					# Browser process crashed - SessionManager will clean up via detach events
					# Just dispatch error event and stop monitoring
					self.event_bus.dispatch(
						BrowserErrorEvent(
							error_type='BrowserProcessCrashed',
							message=f'Browser process {proc.pid} has crashed',
							details={'pid': proc.pid, 'status': proc.status()},
						)
					)

					self.logger.warning('[CrashWatchdog] Browser process dead - stopping health monitoring')
					await self._stop_monitoring()
					return
			except Exception:
				pass  # psutil not available or process doesn't exist

	@staticmethod
	def _is_new_tab_page(url: str) -> bool:
		"""Check if URL is a new tab page."""
		return url in ['about:blank', 'chrome://new-tab-page/', 'chrome://newtab/']

```

---

## backend/browser-use/browser_use/browser/watchdogs/default_action_watchdog.py

```py
"""Default browser action handlers using CDP."""

import asyncio
import json

from cdp_use.cdp.input.commands import DispatchKeyEventParameters

from browser_use.actor.utils import get_key_info
from browser_use.browser.events import (
	ClickCoordinateEvent,
	ClickElementEvent,
	GetDropdownOptionsEvent,
	GoBackEvent,
	GoForwardEvent,
	RefreshEvent,
	ScrollEvent,
	ScrollToTextEvent,
	SelectDropdownOptionEvent,
	SendKeysEvent,
	TypeTextEvent,
	UploadFileEvent,
	WaitEvent,
)
from browser_use.browser.views import BrowserError, URLNotAllowedError
from browser_use.browser.watchdog_base import BaseWatchdog
from browser_use.dom.service import EnhancedDOMTreeNode
from browser_use.observability import observe_debug

# Import EnhancedDOMTreeNode and rebuild event models that have forward references to it
# This must be done after all imports are complete
ClickCoordinateEvent.model_rebuild()
ClickElementEvent.model_rebuild()
GetDropdownOptionsEvent.model_rebuild()
SelectDropdownOptionEvent.model_rebuild()
TypeTextEvent.model_rebuild()
ScrollEvent.model_rebuild()
UploadFileEvent.model_rebuild()


class DefaultActionWatchdog(BaseWatchdog):
	"""Handles default browser actions like click, type, and scroll using CDP."""

	def _is_print_related_element(self, element_node: EnhancedDOMTreeNode) -> bool:
		"""Check if an element is related to printing (print buttons, print dialogs, etc.).

		Primary check: onclick attribute (most reliable for print detection)
		Fallback: button text/value (for cases without onclick)
		"""
		# Primary: Check onclick attribute for print-related functions (most reliable)
		onclick = element_node.attributes.get('onclick', '').lower() if element_node.attributes else ''
		if onclick and 'print' in onclick:
			# Matches: window.print(), PrintElem(), print(), etc.
			return True

		return False

	async def _handle_print_button_click(self, element_node: EnhancedDOMTreeNode) -> dict | None:
		"""Handle print button by directly generating PDF via CDP instead of opening dialog.

		Returns:
			Metadata dict with download path if successful, None otherwise
		"""
		try:
			import base64
			import os
			from pathlib import Path

			# Get CDP session
			cdp_session = await self.browser_session.get_or_create_cdp_session(focus=True)

			# Generate PDF using CDP Page.printToPDF
			result = await asyncio.wait_for(
				cdp_session.cdp_client.send.Page.printToPDF(
					params={
						'printBackground': True,
						'preferCSSPageSize': True,
					},
					session_id=cdp_session.session_id,
				),
				timeout=15.0,  # 15 second timeout for PDF generation
			)

			pdf_data = result.get('data')
			if not pdf_data:
				self.logger.warning('âš ï¸ PDF generation returned no data')
				return None

			# Decode base64 PDF data
			pdf_bytes = base64.b64decode(pdf_data)

			# Get downloads path
			downloads_path = self.browser_session.browser_profile.downloads_path
			if not downloads_path:
				self.logger.warning('âš ï¸ No downloads path configured, cannot save PDF')
				return None

			# Generate filename from page title or URL
			try:
				page_title = await asyncio.wait_for(self.browser_session.get_current_page_title(), timeout=2.0)
				# Sanitize title for filename
				import re

				safe_title = re.sub(r'[^\w\s-]', '', page_title)[:50]  # Max 50 chars
				filename = f'{safe_title}.pdf' if safe_title else 'print.pdf'
			except Exception:
				filename = 'print.pdf'

			# Ensure downloads directory exists
			downloads_dir = Path(downloads_path).expanduser().resolve()
			downloads_dir.mkdir(parents=True, exist_ok=True)

			# Generate unique filename if file exists
			final_path = downloads_dir / filename
			if final_path.exists():
				base, ext = os.path.splitext(filename)
				counter = 1
				while (downloads_dir / f'{base} ({counter}){ext}').exists():
					counter += 1
				final_path = downloads_dir / f'{base} ({counter}){ext}'

			# Write PDF to file
			import anyio

			async with await anyio.open_file(final_path, 'wb') as f:
				await f.write(pdf_bytes)

			file_size = final_path.stat().st_size
			self.logger.info(f'âœ… Generated PDF via CDP: {final_path} ({file_size:,} bytes)')

			# Dispatch FileDownloadedEvent
			from browser_use.browser.events import FileDownloadedEvent

			page_url = await self.browser_session.get_current_page_url()
			self.browser_session.event_bus.dispatch(
				FileDownloadedEvent(
					url=page_url,
					path=str(final_path),
					file_name=final_path.name,
					file_size=file_size,
					file_type='pdf',
					mime_type='application/pdf',
					auto_download=False,  # This was intentional (user clicked print)
				)
			)

			return {'pdf_generated': True, 'path': str(final_path)}

		except TimeoutError:
			self.logger.warning('â±ï¸ PDF generation timed out')
			return None
		except Exception as e:
			self.logger.warning(f'âš ï¸ Failed to generate PDF via CDP: {type(e).__name__}: {e}')
			return None

	@observe_debug(ignore_input=True, ignore_output=True, name='click_element_event')
	async def on_ClickElementEvent(self, event: ClickElementEvent) -> dict | None:
		"""Handle click request with CDP."""
		try:
			# Check if session is alive before attempting any operations
			if not self.browser_session.agent_focus_target_id:
				error_msg = 'Cannot execute click: browser session is corrupted (target_id=None). Session may have crashed.'
				self.logger.error(f'{error_msg}')
				raise BrowserError(error_msg)

			# Use the provided node
			element_node = event.node
			index_for_logging = element_node.backend_node_id or 'unknown'
			starting_target_id = self.browser_session.agent_focus_target_id

			# Check if element is a file input (should not be clicked)
			if self.browser_session.is_file_input(element_node):
				msg = f'Index {index_for_logging} - has an element which opens file upload dialog. To upload files please use a specific function to upload files'
				self.logger.info(f'{msg}')
				# Return validation error instead of raising to avoid ERROR logs
				return {'validation_error': msg}

			# Detect print-related elements and handle them specially
			is_print_element = self._is_print_related_element(element_node)
			if is_print_element:
				self.logger.info(
					f'ğŸ–¨ï¸ Detected print button (index {index_for_logging}), generating PDF directly instead of opening dialog...'
				)

				# Instead of clicking, directly generate PDF via CDP
				click_metadata = await self._handle_print_button_click(element_node)

				if click_metadata and click_metadata.get('pdf_generated'):
					msg = f'Generated PDF: {click_metadata.get("path")}'
					self.logger.info(f'ğŸ’¾ {msg}')
					return click_metadata
				else:
					# Fallback to regular click if PDF generation failed
					self.logger.warning('âš ï¸ PDF generation failed, falling back to regular click')

			# Perform the actual click using internal implementation
			click_metadata = await self._click_element_node_impl(element_node)
			download_path = None  # moved to downloads_watchdog.py

			# Check for validation errors - return them without raising to avoid ERROR logs
			if isinstance(click_metadata, dict) and 'validation_error' in click_metadata:
				self.logger.info(f'{click_metadata["validation_error"]}')
				return click_metadata

			# Build success message
			if download_path:
				msg = f'Downloaded file to {download_path}'
				self.logger.info(f'ğŸ’¾ {msg}')
			else:
				msg = f'Clicked button {element_node.node_name}: {element_node.get_all_children_text(max_depth=2)}'
				self.logger.debug(f'ğŸ–±ï¸ {msg}')
			self.logger.debug(f'Element xpath: {element_node.xpath}')

			return click_metadata if isinstance(click_metadata, dict) else None
		except Exception as e:
			raise

	async def on_ClickCoordinateEvent(self, event: ClickCoordinateEvent) -> dict | None:
		"""Handle click at coordinates with CDP."""
		try:
			# Check if session is alive before attempting any operations
			if not self.browser_session.agent_focus_target_id:
				error_msg = 'Cannot execute click: browser session is corrupted (target_id=None). Session may have crashed.'
				self.logger.error(f'{error_msg}')
				raise BrowserError(error_msg)

			# If force=True, skip safety checks and click directly
			if event.force:
				self.logger.debug(f'Force clicking at coordinates ({event.coordinate_x}, {event.coordinate_y})')
				return await self._click_on_coordinate(event.coordinate_x, event.coordinate_y, force=True)

			# Get element at coordinates for safety checks
			element_node = await self.browser_session.get_dom_element_at_coordinates(event.coordinate_x, event.coordinate_y)
			if element_node is None:
				# No element found, click directly
				self.logger.debug(
					f'No element found at coordinates ({event.coordinate_x}, {event.coordinate_y}), proceeding with click anyway'
				)
				return await self._click_on_coordinate(event.coordinate_x, event.coordinate_y, force=False)

			# Safety check: file input
			if self.browser_session.is_file_input(element_node):
				msg = f'Cannot click at ({event.coordinate_x}, {event.coordinate_y}) - element is a file input. To upload files please use upload_file action'
				self.logger.info(f'{msg}')
				return {'validation_error': msg}

			# Safety check: select element
			tag_name = element_node.tag_name.lower() if element_node.tag_name else ''
			if tag_name == 'select':
				msg = f'Cannot click at ({event.coordinate_x}, {event.coordinate_y}) - element is a <select>. Use dropdown_options action instead.'
				self.logger.info(f'{msg}')
				return {'validation_error': msg}

			# Safety check: print-related elements
			is_print_element = self._is_print_related_element(element_node)
			if is_print_element:
				self.logger.info(
					f'ğŸ–¨ï¸ Detected print button at ({event.coordinate_x}, {event.coordinate_y}), generating PDF directly instead of opening dialog...'
				)
				click_metadata = await self._handle_print_button_click(element_node)
				if click_metadata and click_metadata.get('pdf_generated'):
					msg = f'Generated PDF: {click_metadata.get("path")}'
					self.logger.info(f'ğŸ’¾ {msg}')
					return click_metadata
				else:
					self.logger.warning('âš ï¸ PDF generation failed, falling back to regular click')

			# All safety checks passed, click at coordinates
			return await self._click_on_coordinate(event.coordinate_x, event.coordinate_y, force=False)

		except Exception:
			raise

	async def on_TypeTextEvent(self, event: TypeTextEvent) -> dict | None:
		"""Handle text input request with CDP."""
		try:
			# Use the provided node
			element_node = event.node
			index_for_logging = element_node.backend_node_id or 'unknown'

			# Check if this is index 0 or a falsy index - type to the page (whatever has focus)
			if not element_node.backend_node_id or element_node.backend_node_id == 0:
				# Type to the page without focusing any specific element
				await self._type_to_page(event.text)
				# Log with sensitive data protection
				if event.is_sensitive:
					if event.sensitive_key_name:
						self.logger.info(f'âŒ¨ï¸ Typed <{event.sensitive_key_name}> to the page (current focus)')
					else:
						self.logger.info('âŒ¨ï¸ Typed <sensitive> to the page (current focus)')
				else:
					self.logger.info(f'âŒ¨ï¸ Typed "{event.text}" to the page (current focus)')
				return None  # No coordinates available for page typing
			else:
				try:
					# Try to type to the specific element
					input_metadata = await self._input_text_element_node_impl(
						element_node,
						event.text,
						clear=event.clear or (not event.text),
						is_sensitive=event.is_sensitive,
					)
					# Log with sensitive data protection
					if event.is_sensitive:
						if event.sensitive_key_name:
							self.logger.info(f'âŒ¨ï¸ Typed <{event.sensitive_key_name}> into element with index {index_for_logging}')
						else:
							self.logger.info(f'âŒ¨ï¸ Typed <sensitive> into element with index {index_for_logging}')
					else:
						self.logger.info(f'âŒ¨ï¸ Typed "{event.text}" into element with index {index_for_logging}')
					self.logger.debug(f'Element xpath: {element_node.xpath}')
					return input_metadata  # Return coordinates if available
				except Exception as e:
					# Element not found or error - fall back to typing to the page
					self.logger.warning(f'Failed to type to element {index_for_logging}: {e}. Falling back to page typing.')
					try:
						await asyncio.wait_for(self._click_element_node_impl(element_node), timeout=10.0)
					except Exception as e:
						pass
					await self._type_to_page(event.text)
					# Log with sensitive data protection
					if event.is_sensitive:
						if event.sensitive_key_name:
							self.logger.info(f'âŒ¨ï¸ Typed <{event.sensitive_key_name}> to the page as fallback')
						else:
							self.logger.info('âŒ¨ï¸ Typed <sensitive> to the page as fallback')
					else:
						self.logger.info(f'âŒ¨ï¸ Typed "{event.text}" to the page as fallback')
					return None  # No coordinates available for fallback typing

			# Note: We don't clear cached state here - let multi_act handle DOM change detection
			# by explicitly rebuilding and comparing when needed
		except Exception as e:
			raise

	async def on_ScrollEvent(self, event: ScrollEvent) -> None:
		"""Handle scroll request with CDP."""
		# Check if we have a current target for scrolling
		if not self.browser_session.agent_focus_target_id:
			error_msg = 'No active target for scrolling'
			raise BrowserError(error_msg)

		try:
			# Convert direction and amount to pixels
			# Positive pixels = scroll down, negative = scroll up
			pixels = event.amount if event.direction == 'down' else -event.amount

			# Element-specific scrolling if node is provided
			if event.node is not None:
				element_node = event.node
				index_for_logging = element_node.backend_node_id or 'unknown'

				# Check if the element is an iframe
				is_iframe = element_node.tag_name and element_node.tag_name.upper() == 'IFRAME'

				# Try to scroll the element's container
				success = await self._scroll_element_container(element_node, pixels)
				if success:
					self.logger.debug(
						f'ğŸ“œ Scrolled element {index_for_logging} container {event.direction} by {event.amount} pixels'
					)

					# For iframe scrolling, we need to force a full DOM refresh
					# because the iframe's content has changed position
					if is_iframe:
						self.logger.debug('ğŸ”„ Forcing DOM refresh after iframe scroll')
						# Note: We don't clear cached state here - let multi_act handle DOM change detection
						# by explicitly rebuilding and comparing when needed

						# Wait a bit for the scroll to settle and DOM to update
						await asyncio.sleep(0.2)

					return None

			# Perform target-level scroll
			await self._scroll_with_cdp_gesture(pixels)

			# Note: We don't clear cached state here - let multi_act handle DOM change detection
			# by explicitly rebuilding and comparing when needed

			# Log success
			self.logger.debug(f'ğŸ“œ Scrolled {event.direction} by {event.amount} pixels')
			return None
		except Exception as e:
			raise

	# ========== Implementation Methods ==========

	async def _check_element_occlusion(self, backend_node_id: int, x: float, y: float, cdp_session) -> bool:
		"""Check if an element is occluded by other elements at the given coordinates.

		Args:
			backend_node_id: The backend node ID of the target element
			x: X coordinate to check
			y: Y coordinate to check
			cdp_session: CDP session to use

		Returns:
			True if element is occluded, False if clickable
		"""
		try:
			session_id = cdp_session.session_id

			# Get target element info for comparison
			target_result = await cdp_session.cdp_client.send.DOM.resolveNode(
				params={'backendNodeId': backend_node_id}, session_id=session_id
			)

			if 'object' not in target_result:
				self.logger.debug('Could not resolve target element, assuming occluded')
				return True

			object_id = target_result['object']['objectId']

			# Get target element info
			target_info_result = await cdp_session.cdp_client.send.Runtime.callFunctionOn(
				params={
					'objectId': object_id,
					'functionDeclaration': """
					function() {
						const getElementInfo = (el) => {
							return {
								tagName: el.tagName,
								id: el.id || '',
								className: el.className || '',
								textContent: (el.textContent || '').substring(0, 100)
							};
						};


						const elementAtPoint = document.elementFromPoint(arguments[0], arguments[1]);
						if (!elementAtPoint) {
							return { targetInfo: getElementInfo(this), isClickable: false };
						}


						// Simple containment-based clickability logic
						const isClickable = this === elementAtPoint ||
							this.contains(elementAtPoint) ||
							elementAtPoint.contains(this);

						return {
							targetInfo: getElementInfo(this),
							elementAtPointInfo: getElementInfo(elementAtPoint),
							isClickable: isClickable
						};
					}
					""",
					'arguments': [{'value': x}, {'value': y}],
					'returnByValue': True,
				},
				session_id=session_id,
			)

			if 'result' not in target_info_result or 'value' not in target_info_result['result']:
				self.logger.debug('Could not get target element info, assuming occluded')
				return True

			target_data = target_info_result['result']['value']
			is_clickable = target_data.get('isClickable', False)

			if is_clickable:
				self.logger.debug('Element is clickable (target, contained, or semantically related)')
				return False
			else:
				target_info = target_data.get('targetInfo', {})
				element_at_point_info = target_data.get('elementAtPointInfo', {})
				self.logger.debug(
					f'Element is occluded. Target: {target_info.get("tagName", "unknown")} '
					f'(id={target_info.get("id", "none")}), '
					f'ElementAtPoint: {element_at_point_info.get("tagName", "unknown")} '
					f'(id={element_at_point_info.get("id", "none")})'
				)
				return True

		except Exception as e:
			self.logger.debug(f'Occlusion check failed: {e}, assuming not occluded')
			return False

	async def _click_element_node_impl(self, element_node) -> dict | None:
		"""
		Click an element using pure CDP with multiple fallback methods for getting element geometry.

		Args:
			element_node: The DOM element to click
		"""

		try:
			# Check if element is a file input or select dropdown - these should not be clicked
			tag_name = element_node.tag_name.lower() if element_node.tag_name else ''
			element_type = element_node.attributes.get('type', '').lower() if element_node.attributes else ''

			if tag_name == 'select':
				msg = f'Cannot click on <select> elements. Use dropdown_options(index={element_node.backend_node_id}) action instead.'
				# Return error dict instead of raising to avoid ERROR logs
				return {'validation_error': msg}

			if tag_name == 'input' and element_type == 'file':
				msg = f'Cannot click on file input element (index={element_node.backend_node_id}). File uploads must be handled using upload_file_to_element action.'
				# Return error dict instead of raising to avoid ERROR logs
				return {'validation_error': msg}

			# Get CDP client
			cdp_session = await self.browser_session.cdp_client_for_node(element_node)

			# Get the correct session ID for the element's frame
			session_id = cdp_session.session_id

			# Get element bounds
			backend_node_id = element_node.backend_node_id

			# Get viewport dimensions for visibility checks
			layout_metrics = await cdp_session.cdp_client.send.Page.getLayoutMetrics(session_id=session_id)
			viewport_width = layout_metrics['layoutViewport']['clientWidth']
			viewport_height = layout_metrics['layoutViewport']['clientHeight']

			# Scroll element into view FIRST before getting coordinates
			try:
				await cdp_session.cdp_client.send.DOM.scrollIntoViewIfNeeded(
					params={'backendNodeId': backend_node_id}, session_id=session_id
				)
				await asyncio.sleep(0.05)  # Wait for scroll to complete
				self.logger.debug('Scrolled element into view before getting coordinates')
			except Exception as e:
				self.logger.debug(f'Failed to scroll element into view: {e}')

			# Get element coordinates using the unified method AFTER scrolling
			element_rect = await self.browser_session.get_element_coordinates(backend_node_id, cdp_session)

			# Convert rect to quads format if we got coordinates
			quads = []
			if element_rect:
				# Convert DOMRect to quad format
				x, y, w, h = element_rect.x, element_rect.y, element_rect.width, element_rect.height
				quads = [
					[
						x,
						y,  # top-left
						x + w,
						y,  # top-right
						x + w,
						y + h,  # bottom-right
						x,
						y + h,  # bottom-left
					]
				]
				self.logger.debug(
					f'Got coordinates from unified method: {element_rect.x}, {element_rect.y}, {element_rect.width}x{element_rect.height}'
				)

			# If we still don't have quads, fall back to JS click
			if not quads:
				self.logger.warning('Could not get element geometry from any method, falling back to JavaScript click')
				try:
					result = await cdp_session.cdp_client.send.DOM.resolveNode(
						params={'backendNodeId': backend_node_id},
						session_id=session_id,
					)
					assert 'object' in result and 'objectId' in result['object'], (
						'Failed to find DOM element based on backendNodeId, maybe page content changed?'
					)
					object_id = result['object']['objectId']

					await cdp_session.cdp_client.send.Runtime.callFunctionOn(
						params={
							'functionDeclaration': 'function() { this.click(); }',
							'objectId': object_id,
						},
						session_id=session_id,
					)
					await asyncio.sleep(0.05)
					# Navigation is handled by BrowserSession via events
					return None
				except Exception as js_e:
					self.logger.warning(f'CDP JavaScript click also failed: {js_e}')
					if 'No node with given id found' in str(js_e):
						raise Exception('Element with given id not found')
					else:
						raise Exception(f'Failed to click element: {js_e}')

			# Find the largest visible quad within the viewport
			best_quad = None
			best_area = 0

			for quad in quads:
				if len(quad) < 8:
					continue

				# Calculate quad bounds
				xs = [quad[i] for i in range(0, 8, 2)]
				ys = [quad[i] for i in range(1, 8, 2)]
				min_x, max_x = min(xs), max(xs)
				min_y, max_y = min(ys), max(ys)

				# Check if quad intersects with viewport
				if max_x < 0 or max_y < 0 or min_x > viewport_width or min_y > viewport_height:
					continue  # Quad is completely outside viewport

				# Calculate visible area (intersection with viewport)
				visible_min_x = max(0, min_x)
				visible_max_x = min(viewport_width, max_x)
				visible_min_y = max(0, min_y)
				visible_max_y = min(viewport_height, max_y)

				visible_width = visible_max_x - visible_min_x
				visible_height = visible_max_y - visible_min_y
				visible_area = visible_width * visible_height

				if visible_area > best_area:
					best_area = visible_area
					best_quad = quad

			if not best_quad:
				# No visible quad found, use the first quad anyway
				best_quad = quads[0]
				self.logger.warning('No visible quad found, using first quad')

			# Calculate center point of the best quad
			center_x = sum(best_quad[i] for i in range(0, 8, 2)) / 4
			center_y = sum(best_quad[i] for i in range(1, 8, 2)) / 4

			# Ensure click point is within viewport bounds
			center_x = max(0, min(viewport_width - 1, center_x))
			center_y = max(0, min(viewport_height - 1, center_y))

			# Check for occlusion before attempting CDP click
			is_occluded = await self._check_element_occlusion(backend_node_id, center_x, center_y, cdp_session)

			if is_occluded:
				self.logger.debug('ğŸš« Element is occluded, falling back to JavaScript click')
				try:
					result = await cdp_session.cdp_client.send.DOM.resolveNode(
						params={'backendNodeId': backend_node_id},
						session_id=session_id,
					)
					assert 'object' in result and 'objectId' in result['object'], (
						'Failed to find DOM element based on backendNodeId'
					)
					object_id = result['object']['objectId']

					await cdp_session.cdp_client.send.Runtime.callFunctionOn(
						params={
							'functionDeclaration': 'function() { this.click(); }',
							'objectId': object_id,
						},
						session_id=session_id,
					)
					await asyncio.sleep(0.05)
					return None
				except Exception as js_e:
					self.logger.error(f'JavaScript click fallback failed: {js_e}')
					raise Exception(f'Failed to click occluded element: {js_e}')

			# Perform the click using CDP (element is not occluded)
			try:
				self.logger.debug(f'ğŸ‘† Dragging mouse over element before clicking x: {center_x}px y: {center_y}px ...')
				# Move mouse to element
				await cdp_session.cdp_client.send.Input.dispatchMouseEvent(
					params={
						'type': 'mouseMoved',
						'x': center_x,
						'y': center_y,
					},
					session_id=session_id,
				)
				await asyncio.sleep(0.05)

				# Mouse down
				self.logger.debug(f'ğŸ‘†ğŸ¾ Clicking x: {center_x}px y: {center_y}px ...')
				try:
					await asyncio.wait_for(
						cdp_session.cdp_client.send.Input.dispatchMouseEvent(
							params={
								'type': 'mousePressed',
								'x': center_x,
								'y': center_y,
								'button': 'left',
								'clickCount': 1,
							},
							session_id=session_id,
						),
						timeout=3.0,  # 3 second timeout for mousePressed
					)
					await asyncio.sleep(0.08)
				except TimeoutError:
					self.logger.debug('â±ï¸ Mouse down timed out (likely due to dialog), continuing...')
					# Don't sleep if we timed out

				# Mouse up
				try:
					await asyncio.wait_for(
						cdp_session.cdp_client.send.Input.dispatchMouseEvent(
							params={
								'type': 'mouseReleased',
								'x': center_x,
								'y': center_y,
								'button': 'left',
								'clickCount': 1,
							},
							session_id=session_id,
						),
						timeout=5.0,  # 5 second timeout for mouseReleased
					)
				except TimeoutError:
					self.logger.debug('â±ï¸ Mouse up timed out (possibly due to lag or dialog popup), continuing...')

				self.logger.debug('ğŸ–±ï¸ Clicked successfully using x,y coordinates')

				# Return coordinates as dict for metadata
				return {'click_x': center_x, 'click_y': center_y}

			except Exception as e:
				self.logger.warning(f'CDP click failed: {type(e).__name__}: {e}')
				# Fall back to JavaScript click via CDP
				try:
					result = await cdp_session.cdp_client.send.DOM.resolveNode(
						params={'backendNodeId': backend_node_id},
						session_id=session_id,
					)
					assert 'object' in result and 'objectId' in result['object'], (
						'Failed to find DOM element based on backendNodeId, maybe page content changed?'
					)
					object_id = result['object']['objectId']

					await cdp_session.cdp_client.send.Runtime.callFunctionOn(
						params={
							'functionDeclaration': 'function() { this.click(); }',
							'objectId': object_id,
						},
						session_id=session_id,
					)

					# Small delay for dialog dismissal
					await asyncio.sleep(0.1)

					return None
				except Exception as js_e:
					self.logger.warning(f'CDP JavaScript click also failed: {js_e}')
					raise Exception(f'Failed to click element: {e}')
			finally:
				# Always re-focus back to original top-level page session context in case click opened a new tab/popup/window/dialog/etc.
				# Use timeout to prevent hanging if dialog is blocking
				try:
					cdp_session = await asyncio.wait_for(self.browser_session.get_or_create_cdp_session(focus=True), timeout=3.0)
					await asyncio.wait_for(
						cdp_session.cdp_client.send.Runtime.runIfWaitingForDebugger(session_id=cdp_session.session_id),
						timeout=2.0,
					)
				except TimeoutError:
					self.logger.debug('â±ï¸ Refocus after click timed out (page may be blocked by dialog). Continuing...')
				except Exception as e:
					self.logger.debug(f'âš ï¸ Refocus error (non-critical): {type(e).__name__}: {e}')

		except URLNotAllowedError as e:
			raise e
		except BrowserError as e:
			raise e
		except Exception as e:
			# Extract key element info for error message
			element_info = f'<{element_node.tag_name or "unknown"}'
			if element_node.backend_node_id:
				element_info += f' index={element_node.backend_node_id}'
			element_info += '>'

			# Create helpful error message based on context
			error_detail = f'Failed to click element {element_info}. The element may not be interactable or visible.'

			# Add hint if element has index (common in code-use mode)
			if element_node.backend_node_id:
				error_detail += f' If the page changed after navigation/interaction, the index [{element_node.backend_node_id}] may be stale. Get fresh browser state before retrying.'

			raise BrowserError(
				message=f'Failed to click element: {str(e)}',
				long_term_memory=error_detail,
			)

	async def _click_on_coordinate(self, coordinate_x: int, coordinate_y: int, force: bool = False) -> dict | None:
		"""
		Click directly at coordinates using CDP Input.dispatchMouseEvent.

		Args:
			coordinate_x: X coordinate in viewport
			coordinate_y: Y coordinate in viewport
			force: If True, skip all safety checks (used when force=True in event)

		Returns:
			Dict with click coordinates or None
		"""
		try:
			# Get CDP session
			cdp_session = await self.browser_session.get_or_create_cdp_session()
			session_id = cdp_session.session_id

			self.logger.debug(f'ğŸ‘† Moving mouse to ({coordinate_x}, {coordinate_y})...')

			# Move mouse to coordinates
			await cdp_session.cdp_client.send.Input.dispatchMouseEvent(
				params={
					'type': 'mouseMoved',
					'x': coordinate_x,
					'y': coordinate_y,
				},
				session_id=session_id,
			)
			await asyncio.sleep(0.05)

			# Mouse down
			self.logger.debug(f'ğŸ‘†ğŸ¾ Clicking at ({coordinate_x}, {coordinate_y})...')
			try:
				await asyncio.wait_for(
					cdp_session.cdp_client.send.Input.dispatchMouseEvent(
						params={
							'type': 'mousePressed',
							'x': coordinate_x,
							'y': coordinate_y,
							'button': 'left',
							'clickCount': 1,
						},
						session_id=session_id,
					),
					timeout=3.0,
				)
				await asyncio.sleep(0.05)
			except TimeoutError:
				self.logger.debug('â±ï¸ Mouse down timed out (likely due to dialog), continuing...')

			# Mouse up
			try:
				await asyncio.wait_for(
					cdp_session.cdp_client.send.Input.dispatchMouseEvent(
						params={
							'type': 'mouseReleased',
							'x': coordinate_x,
							'y': coordinate_y,
							'button': 'left',
							'clickCount': 1,
						},
						session_id=session_id,
					),
					timeout=5.0,
				)
			except TimeoutError:
				self.logger.debug('â±ï¸ Mouse up timed out (possibly due to lag or dialog popup), continuing...')

			self.logger.debug(f'ğŸ–±ï¸ Clicked successfully at ({coordinate_x}, {coordinate_y})')

			# Return coordinates as metadata
			return {'click_x': coordinate_x, 'click_y': coordinate_y}

		except Exception as e:
			self.logger.error(f'Failed to click at coordinates ({coordinate_x}, {coordinate_y}): {type(e).__name__}: {e}')
			raise BrowserError(
				message=f'Failed to click at coordinates: {e}',
				long_term_memory=f'Failed to click at coordinates ({coordinate_x}, {coordinate_y}). The coordinates may be outside viewport or the page may have changed.',
			)

	async def _type_to_page(self, text: str):
		"""
		Type text to the page (whatever element currently has focus).
		This is used when index is 0 or when an element can't be found.
		"""
		try:
			# Get CDP client and session
			cdp_session = await self.browser_session.get_or_create_cdp_session(target_id=None, focus=True)

			# Type the text character by character to the focused element
			for char in text:
				# Handle newline characters as Enter key
				if char == '\n':
					# Send proper Enter key sequence
					await cdp_session.cdp_client.send.Input.dispatchKeyEvent(
						params={
							'type': 'keyDown',
							'key': 'Enter',
							'code': 'Enter',
							'windowsVirtualKeyCode': 13,
						},
						session_id=cdp_session.session_id,
					)
					# Send char event with carriage return
					await cdp_session.cdp_client.send.Input.dispatchKeyEvent(
						params={
							'type': 'char',
							'text': '\r',
						},
						session_id=cdp_session.session_id,
					)
					# Send keyup
					await cdp_session.cdp_client.send.Input.dispatchKeyEvent(
						params={
							'type': 'keyUp',
							'key': 'Enter',
							'code': 'Enter',
							'windowsVirtualKeyCode': 13,
						},
						session_id=cdp_session.session_id,
					)
				else:
					# Handle regular characters
					# Send keydown
					await cdp_session.cdp_client.send.Input.dispatchKeyEvent(
						params={
							'type': 'keyDown',
							'key': char,
						},
						session_id=cdp_session.session_id,
					)
					# Send char for actual text input
					await cdp_session.cdp_client.send.Input.dispatchKeyEvent(
						params={
							'type': 'char',
							'text': char,
						},
						session_id=cdp_session.session_id,
					)
					# Send keyup
					await cdp_session.cdp_client.send.Input.dispatchKeyEvent(
						params={
							'type': 'keyUp',
							'key': char,
						},
						session_id=cdp_session.session_id,
					)
		except Exception as e:
			raise Exception(f'Failed to type to page: {str(e)}')

	def _get_char_modifiers_and_vk(self, char: str) -> tuple[int, int, str]:
		"""Get modifiers, virtual key code, and base key for a character.

		Returns:
			(modifiers, windowsVirtualKeyCode, base_key)
		"""
		# Characters that require Shift modifier
		shift_chars = {
			'!': ('1', 49),
			'@': ('2', 50),
			'#': ('3', 51),
			'$': ('4', 52),
			'%': ('5', 53),
			'^': ('6', 54),
			'&': ('7', 55),
			'*': ('8', 56),
			'(': ('9', 57),
			')': ('0', 48),
			'_': ('-', 189),
			'+': ('=', 187),
			'{': ('[', 219),
			'}': (']', 221),
			'|': ('\\', 220),
			':': (';', 186),
			'"': ("'", 222),
			'<': (',', 188),
			'>': ('.', 190),
			'?': ('/', 191),
			'~': ('`', 192),
		}

		# Check if character requires Shift
		if char in shift_chars:
			base_key, vk_code = shift_chars[char]
			return (8, vk_code, base_key)  # Shift=8

		# Uppercase letters require Shift
		if char.isupper():
			return (8, ord(char), char.lower())  # Shift=8

		# Lowercase letters
		if char.islower():
			return (0, ord(char.upper()), char)

		# Numbers
		if char.isdigit():
			return (0, ord(char), char)

		# Special characters without Shift
		no_shift_chars = {
			' ': 32,
			'-': 189,
			'=': 187,
			'[': 219,
			']': 221,
			'\\': 220,
			';': 186,
			"'": 222,
			',': 188,
			'.': 190,
			'/': 191,
			'`': 192,
		}

		if char in no_shift_chars:
			return (0, no_shift_chars[char], char)

		# Fallback
		return (0, ord(char.upper()) if char.isalpha() else ord(char), char)

	def _get_key_code_for_char(self, char: str) -> str:
		"""Get the proper key code for a character (like Playwright does)."""
		# Key code mapping for common characters (using proper base keys + modifiers)
		key_codes = {
			' ': 'Space',
			'.': 'Period',
			',': 'Comma',
			'-': 'Minus',
			'_': 'Minus',  # Underscore uses Minus with Shift
			'@': 'Digit2',  # @ uses Digit2 with Shift
			'!': 'Digit1',  # ! uses Digit1 with Shift (not 'Exclamation')
			'?': 'Slash',  # ? uses Slash with Shift
			':': 'Semicolon',  # : uses Semicolon with Shift
			';': 'Semicolon',
			'(': 'Digit9',  # ( uses Digit9 with Shift
			')': 'Digit0',  # ) uses Digit0 with Shift
			'[': 'BracketLeft',
			']': 'BracketRight',
			'{': 'BracketLeft',  # { uses BracketLeft with Shift
			'}': 'BracketRight',  # } uses BracketRight with Shift
			'/': 'Slash',
			'\\': 'Backslash',
			'=': 'Equal',
			'+': 'Equal',  # + uses Equal with Shift
			'*': 'Digit8',  # * uses Digit8 with Shift
			'&': 'Digit7',  # & uses Digit7 with Shift
			'%': 'Digit5',  # % uses Digit5 with Shift
			'$': 'Digit4',  # $ uses Digit4 with Shift
			'#': 'Digit3',  # # uses Digit3 with Shift
			'^': 'Digit6',  # ^ uses Digit6 with Shift
			'~': 'Backquote',  # ~ uses Backquote with Shift
			'`': 'Backquote',
			"'": 'Quote',
			'"': 'Quote',  # " uses Quote with Shift
		}

		# Numbers
		if char.isdigit():
			return f'Digit{char}'

		# Letters
		if char.isalpha():
			return f'Key{char.upper()}'

		# Special characters
		if char in key_codes:
			return key_codes[char]

		# Fallback for unknown characters
		return f'Key{char.upper()}'

	async def _clear_text_field(self, object_id: str, cdp_session) -> bool:
		"""Clear text field using multiple strategies, starting with the most reliable."""
		try:
			# Strategy 1: Direct JavaScript value/content setting (handles both inputs and contenteditable)
			self.logger.debug('ğŸ§¹ Clearing text field using JavaScript value setting')

			clear_result = await cdp_session.cdp_client.send.Runtime.callFunctionOn(
				params={
					'functionDeclaration': """
						function() {
							// Check if it's a contenteditable element
							const hasContentEditable = this.getAttribute('contenteditable') === 'true' ||
													this.getAttribute('contenteditable') === '' ||
													this.isContentEditable === true;

							if (hasContentEditable) {
								// For contenteditable elements, clear all content
								while (this.firstChild) {
									this.removeChild(this.firstChild);
								}
								this.textContent = "";
								this.innerHTML = "";

								// Focus and position cursor at the beginning
								this.focus();
								const selection = window.getSelection();
								const range = document.createRange();
								range.setStart(this, 0);
								range.setEnd(this, 0);
								selection.removeAllRanges();
								selection.addRange(range);

								// Dispatch events
								this.dispatchEvent(new Event("input", { bubbles: true }));
								this.dispatchEvent(new Event("change", { bubbles: true }));

								return {cleared: true, method: 'contenteditable', finalText: this.textContent};
							} else if (this.value !== undefined) {
								// For regular inputs with value property
								try {
									this.select();
								} catch (e) {
									// ignore
								}
								this.value = "";
								this.dispatchEvent(new Event("input", { bubbles: true }));
								this.dispatchEvent(new Event("change", { bubbles: true }));
								return {cleared: true, method: 'value', finalText: this.value};
							} else {
								return {cleared: false, method: 'none', error: 'Not a supported input type'};
							}
						}
					""",
					'objectId': object_id,
					'returnByValue': True,
				},
				session_id=cdp_session.session_id,
			)

			# Check the clear result
			clear_info = clear_result.get('result', {}).get('value', {})
			self.logger.debug(f'Clear result: {clear_info}')

			if clear_info.get('cleared'):
				final_text = clear_info.get('finalText', '')
				if not final_text or not final_text.strip():
					self.logger.debug(f'âœ… Text field cleared successfully using {clear_info.get("method")}')
					return True
				else:
					self.logger.debug(f'âš ï¸ JavaScript clear partially failed, field still contains: "{final_text}"')
					return False
			else:
				self.logger.debug(f'âŒ JavaScript clear failed: {clear_info.get("error", "Unknown error")}')
				return False

		except Exception as e:
			self.logger.debug(f'JavaScript clear failed with exception: {e}')
			return False

		# Strategy 2: Triple-click + Delete (fallback for stubborn fields)
		try:
			self.logger.debug('ğŸ§¹ Fallback: Clearing using triple-click + Delete')

			# Get element center coordinates for triple-click
			bounds_result = await cdp_session.cdp_client.send.Runtime.callFunctionOn(
				params={
					'functionDeclaration': 'function() { return this.getBoundingClientRect(); }',
					'objectId': object_id,
					'returnByValue': True,
				},
				session_id=cdp_session.session_id,
			)

			if bounds_result.get('result', {}).get('value'):
				bounds = bounds_result['result']['value']
				center_x = bounds['x'] + bounds['width'] / 2
				center_y = bounds['y'] + bounds['height'] / 2

				# Triple-click to select all text
				await cdp_session.cdp_client.send.Input.dispatchMouseEvent(
					params={
						'type': 'mousePressed',
						'x': center_x,
						'y': center_y,
						'button': 'left',
						'clickCount': 3,
					},
					session_id=cdp_session.session_id,
				)
				await cdp_session.cdp_client.send.Input.dispatchMouseEvent(
					params={
						'type': 'mouseReleased',
						'x': center_x,
						'y': center_y,
						'button': 'left',
						'clickCount': 3,
					},
					session_id=cdp_session.session_id,
				)

				# Delete selected text
				await cdp_session.cdp_client.send.Input.dispatchKeyEvent(
					params={
						'type': 'keyDown',
						'key': 'Delete',
						'code': 'Delete',
					},
					session_id=cdp_session.session_id,
				)
				await cdp_session.cdp_client.send.Input.dispatchKeyEvent(
					params={
						'type': 'keyUp',
						'key': 'Delete',
						'code': 'Delete',
					},
					session_id=cdp_session.session_id,
				)

				self.logger.debug('âœ… Text field cleared using triple-click + Delete')
				return True

		except Exception as e:
			self.logger.debug(f'Triple-click clear failed: {e}')

		# Strategy 3: Keyboard shortcuts (last resort)
		try:
			import platform

			is_macos = platform.system() == 'Darwin'
			select_all_modifier = 4 if is_macos else 2  # Meta=4 (Cmd), Ctrl=2
			modifier_name = 'Cmd' if is_macos else 'Ctrl'

			self.logger.debug(f'ğŸ§¹ Last resort: Clearing using {modifier_name}+A + Backspace')

			# Select all text (Ctrl/Cmd+A)
			await cdp_session.cdp_client.send.Input.dispatchKeyEvent(
				params={
					'type': 'keyDown',
					'key': 'a',
					'code': 'KeyA',
					'modifiers': select_all_modifier,
				},
				session_id=cdp_session.session_id,
			)
			await cdp_session.cdp_client.send.Input.dispatchKeyEvent(
				params={
					'type': 'keyUp',
					'key': 'a',
					'code': 'KeyA',
					'modifiers': select_all_modifier,
				},
				session_id=cdp_session.session_id,
			)

			# Delete selected text (Backspace)
			await cdp_session.cdp_client.send.Input.dispatchKeyEvent(
				params={
					'type': 'keyDown',
					'key': 'Backspace',
					'code': 'Backspace',
				},
				session_id=cdp_session.session_id,
			)
			await cdp_session.cdp_client.send.Input.dispatchKeyEvent(
				params={
					'type': 'keyUp',
					'key': 'Backspace',
					'code': 'Backspace',
				},
				session_id=cdp_session.session_id,
			)

			self.logger.debug('âœ… Text field cleared using keyboard shortcuts')
			return True

		except Exception as e:
			self.logger.debug(f'All clearing strategies failed: {e}')
			return False

	async def _focus_element_simple(
		self, backend_node_id: int, object_id: str, cdp_session, input_coordinates: dict | None = None
	) -> bool:
		"""Simple focus strategy: CDP first, then click if failed."""

		# Strategy 1: Try CDP DOM.focus first
		try:
			result = await cdp_session.cdp_client.send.DOM.focus(
				params={'backendNodeId': backend_node_id},
				session_id=cdp_session.session_id,
			)
			self.logger.debug(f'Element focused using CDP DOM.focus (result: {result})')
			return True

		except Exception as e:
			self.logger.debug(f'âŒ CDP DOM.focus threw exception: {type(e).__name__}: {e}')

		# Strategy 2: Try click to focus if CDP failed
		if input_coordinates and 'input_x' in input_coordinates and 'input_y' in input_coordinates:
			try:
				click_x = input_coordinates['input_x']
				click_y = input_coordinates['input_y']

				self.logger.debug(f'ğŸ¯ Attempting click-to-focus at ({click_x:.1f}, {click_y:.1f})')

				# Click to focus
				await cdp_session.cdp_client.send.Input.dispatchMouseEvent(
					params={
						'type': 'mousePressed',
						'x': click_x,
						'y': click_y,
						'button': 'left',
						'clickCount': 1,
					},
					session_id=cdp_session.session_id,
				)
				await cdp_session.cdp_client.send.Input.dispatchMouseEvent(
					params={
						'type': 'mouseReleased',
						'x': click_x,
						'y': click_y,
						'button': 'left',
						'clickCount': 1,
					},
					session_id=cdp_session.session_id,
				)

				self.logger.debug('âœ… Element focused using click method')
				return True

			except Exception as e:
				self.logger.debug(f'Click focus failed: {e}')

		# Both strategies failed
		self.logger.debug('Focus strategies failed, will attempt typing anyway')
		return False

	def _requires_direct_value_assignment(self, element_node: EnhancedDOMTreeNode) -> bool:
		"""
		Check if an element requires direct value assignment instead of character-by-character typing.

		Certain input types have compound components, custom plugins, or special requirements
		that make character-by-character typing unreliable. These need direct .value assignment:

		Native HTML5:
		- date, time, datetime-local: Have spinbutton components (ISO format required)
		- month, week: Similar compound structure
		- color: Expects hex format #RRGGBB
		- range: Needs numeric value within min/max

		jQuery/Bootstrap Datepickers:
		- Detected by class names or data attributes
		- Often expect specific date formats (MM/DD/YYYY, DD/MM/YYYY, etc.)

		Note: We use direct assignment because:
		1. Typing triggers intermediate validation that might reject partial values
		2. Compound components (like date spinbuttons) don't work with sequential typing
		3. It's much faster and more reliable
		4. We dispatch proper input/change events afterward to trigger listeners
		"""
		if not element_node.tag_name or not element_node.attributes:
			return False

		tag_name = element_node.tag_name.lower()

		# Check for native HTML5 inputs that need direct assignment
		if tag_name == 'input':
			input_type = element_node.attributes.get('type', '').lower()

			# Native HTML5 inputs with compound components or strict formats
			if input_type in {'date', 'time', 'datetime-local', 'month', 'week', 'color', 'range'}:
				return True

			# Detect jQuery/Bootstrap datepickers (text inputs with datepicker plugins)
			if input_type in {'text', ''}:
				# Check for common datepicker indicators
				class_attr = element_node.attributes.get('class', '').lower()
				if any(
					indicator in class_attr
					for indicator in ['datepicker', 'daterangepicker', 'datetimepicker', 'bootstrap-datepicker']
				):
					return True

				# Check for data attributes indicating datepickers
				if any(attr in element_node.attributes for attr in ['data-datepicker', 'data-date-format', 'data-provide']):
					return True

		return False

	async def _set_value_directly(self, element_node: EnhancedDOMTreeNode, text: str, object_id: str, cdp_session) -> None:
		"""
		Set element value directly using JavaScript for inputs that don't support typing.

		This is used for:
		- Date/time inputs where character-by-character typing doesn't work
		- jQuery datepickers that need direct value assignment
		- Color/range inputs that need specific formats
		- Any input with custom plugins that intercept typing

		After setting the value, we dispatch comprehensive events to ensure all frameworks
		and plugins recognize the change (React, Vue, Angular, jQuery, etc.)
		"""
		try:
			# Set the value using JavaScript with comprehensive event dispatching
			# callFunctionOn expects a function body (not a self-invoking function)
			set_value_js = f"""
			function() {{
				// Store old value for comparison
				const oldValue = this.value;

				// REACT-COMPATIBLE VALUE SETTING:
				// React uses Object.getOwnPropertyDescriptor to track input changes
				// We need to use the native setter to bypass React's tracking and then trigger events
				const nativeInputValueSetter = Object.getOwnPropertyDescriptor(
					window.HTMLInputElement.prototype,
					'value'
				).set;

				// Set the value using the native setter (bypasses React's control)
				nativeInputValueSetter.call(this, {json.dumps(text)});

				// Dispatch comprehensive events to ensure all frameworks detect the change
				// Order matters: focus -> input -> change -> blur (mimics user interaction)

				// 1. Focus event (in case element isn't focused)
				this.dispatchEvent(new FocusEvent('focus', {{ bubbles: true }}));

				// 2. Input event (CRITICAL for React onChange)
				// React listens to 'input' events on the document and checks for value changes
				const inputEvent = new Event('input', {{ bubbles: true, cancelable: true }});
				this.dispatchEvent(inputEvent);

				// 3. Change event (for form handling, traditional listeners)
				const changeEvent = new Event('change', {{ bubbles: true, cancelable: true }});
				this.dispatchEvent(changeEvent);

				// 4. Blur event (triggers final validation in some libraries)
				this.dispatchEvent(new FocusEvent('blur', {{ bubbles: true }}));

				// 5. jQuery-specific events (if jQuery is present)
				if (typeof jQuery !== 'undefined' && jQuery.fn) {{
					try {{
						jQuery(this).trigger('change');
						// Trigger datepicker-specific events if it's a datepicker
						if (jQuery(this).data('datepicker')) {{
							jQuery(this).datepicker('update');
						}}
					}} catch (e) {{
						// jQuery not available or error, continue anyway
					}}
				}}

				return this.value;
			}}
			"""

			result = await cdp_session.cdp_client.send.Runtime.callFunctionOn(
				params={
					'objectId': object_id,
					'functionDeclaration': set_value_js,
					'returnByValue': True,
				},
				session_id=cdp_session.session_id,
			)

			# Verify the value was set correctly
			if 'result' in result and 'value' in result['result']:
				actual_value = result['result']['value']
				self.logger.debug(f'âœ… Value set directly to: "{actual_value}"')
			else:
				self.logger.warning('âš ï¸ Could not verify value was set correctly')

		except Exception as e:
			self.logger.error(f'âŒ Failed to set value directly: {e}')
			raise

	async def _input_text_element_node_impl(
		self, element_node: EnhancedDOMTreeNode, text: str, clear: bool = True, is_sensitive: bool = False
	) -> dict | None:
		"""
		Input text into an element using pure CDP with improved focus fallbacks.

		For date/time inputs, uses direct value assignment instead of typing.
		"""

		try:
			# Get CDP client
			cdp_client = self.browser_session.cdp_client

			# Get the correct session ID for the element's iframe
			# session_id = await self._get_session_id_for_element(element_node)

			# cdp_session = await self.browser_session.get_or_create_cdp_session(target_id=element_node.target_id, focus=True)
			cdp_session = await self.browser_session.cdp_client_for_node(element_node)

			# Get element info
			backend_node_id = element_node.backend_node_id

			# Track coordinates for metadata
			input_coordinates = None

			# Scroll element into view
			try:
				await cdp_session.cdp_client.send.DOM.scrollIntoViewIfNeeded(
					params={'backendNodeId': backend_node_id}, session_id=cdp_session.session_id
				)
				await asyncio.sleep(0.01)
			except Exception as e:
				# Node detached errors are common with shadow DOM and dynamic content
				# The element can still be interacted with even if scrolling fails
				error_str = str(e)
				if 'Node is detached from document' in error_str or 'detached from document' in error_str:
					self.logger.debug(
						f'Element node temporarily detached during scroll (common with shadow DOM), continuing: {element_node}'
					)
				else:
					self.logger.debug(f'Failed to scroll element {element_node} into view before typing: {type(e).__name__}: {e}')

			# Get object ID for the element
			result = await cdp_client.send.DOM.resolveNode(
				params={'backendNodeId': backend_node_id},
				session_id=cdp_session.session_id,
			)
			assert 'object' in result and 'objectId' in result['object'], (
				'Failed to find DOM element based on backendNodeId, maybe page content changed?'
			)
			object_id = result['object']['objectId']

			# Get current coordinates using unified method
			coords = await self.browser_session.get_element_coordinates(backend_node_id, cdp_session)
			if coords:
				center_x = coords.x + coords.width / 2
				center_y = coords.y + coords.height / 2

				# Check for occlusion before using coordinates for focus
				is_occluded = await self._check_element_occlusion(backend_node_id, center_x, center_y, cdp_session)

				if is_occluded:
					self.logger.debug('ğŸš« Input element is occluded, skipping coordinate-based focus')
					input_coordinates = None  # Force fallback to CDP-only focus
				else:
					input_coordinates = {'input_x': center_x, 'input_y': center_y}
					self.logger.debug(f'Using unified coordinates: x={center_x:.1f}, y={center_y:.1f}')
			else:
				input_coordinates = None
				self.logger.debug('No coordinates found for element')

			# Ensure we have a valid object_id before proceeding
			if not object_id:
				raise ValueError('Could not get object_id for element')

			# Step 1: Focus the element using simple strategy
			focused_successfully = await self._focus_element_simple(
				backend_node_id=backend_node_id, object_id=object_id, cdp_session=cdp_session, input_coordinates=input_coordinates
			)

			# Step 2: Check if this element requires direct value assignment (date/time inputs)
			requires_direct_assignment = self._requires_direct_value_assignment(element_node)

			if requires_direct_assignment:
				# Date/time inputs: use direct value assignment instead of typing
				self.logger.debug(
					f'ğŸ¯ Element type={element_node.attributes.get("type")} requires direct value assignment, setting value directly'
				)
				await self._set_value_directly(element_node, text, object_id, cdp_session)

				# Return input coordinates for metadata
				return input_coordinates

			# Step 3: Clear existing text if requested (only for regular inputs that support typing)
			if clear:
				cleared_successfully = await self._clear_text_field(object_id=object_id, cdp_session=cdp_session)
				if not cleared_successfully:
					self.logger.warning('âš ï¸ Text field clearing failed, typing may append to existing text')

			# Step 4: Type the text character by character using proper human-like key events
			# This emulates exactly how a human would type, which modern websites expect
			if is_sensitive:
				# Note: sensitive_key_name is not passed to this low-level method,
				# but we could extend the signature if needed for more granular logging
				self.logger.debug('ğŸ¯ Typing <sensitive> character by character')
			else:
				self.logger.debug(f'ğŸ¯ Typing text character by character: "{text}"')

			for i, char in enumerate(text):
				# Handle newline characters as Enter key
				if char == '\n':
					# Send proper Enter key sequence
					await cdp_session.cdp_client.send.Input.dispatchKeyEvent(
						params={
							'type': 'keyDown',
							'key': 'Enter',
							'code': 'Enter',
							'windowsVirtualKeyCode': 13,
						},
						session_id=cdp_session.session_id,
					)

					# Small delay to emulate human typing speed
					await asyncio.sleep(0.001)

					# Send char event with carriage return
					await cdp_session.cdp_client.send.Input.dispatchKeyEvent(
						params={
							'type': 'char',
							'text': '\r',
							'key': 'Enter',
						},
						session_id=cdp_session.session_id,
					)

					# Send keyUp event
					await cdp_session.cdp_client.send.Input.dispatchKeyEvent(
						params={
							'type': 'keyUp',
							'key': 'Enter',
							'code': 'Enter',
							'windowsVirtualKeyCode': 13,
						},
						session_id=cdp_session.session_id,
					)
				else:
					# Handle regular characters
					# Get proper modifiers, VK code, and base key for the character
					modifiers, vk_code, base_key = self._get_char_modifiers_and_vk(char)
					key_code = self._get_key_code_for_char(base_key)

					# self.logger.debug(f'ğŸ¯ Typing character {i + 1}/{len(text)}: "{char}" (base_key: {base_key}, code: {key_code}, modifiers: {modifiers}, vk: {vk_code})')

					# Step 1: Send keyDown event (NO text parameter)
					await cdp_session.cdp_client.send.Input.dispatchKeyEvent(
						params={
							'type': 'keyDown',
							'key': base_key,
							'code': key_code,
							'modifiers': modifiers,
							'windowsVirtualKeyCode': vk_code,
						},
						session_id=cdp_session.session_id,
					)

					# Small delay to emulate human typing speed
					await asyncio.sleep(0.005)

					# Step 2: Send char event (WITH text parameter) - this is crucial for text input
					await cdp_session.cdp_client.send.Input.dispatchKeyEvent(
						params={
							'type': 'char',
							'text': char,
							'key': char,
						},
						session_id=cdp_session.session_id,
					)

					# Step 3: Send keyUp event (NO text parameter)
					await cdp_session.cdp_client.send.Input.dispatchKeyEvent(
						params={
							'type': 'keyUp',
							'key': base_key,
							'code': key_code,
							'modifiers': modifiers,
							'windowsVirtualKeyCode': vk_code,
						},
						session_id=cdp_session.session_id,
					)

				# Small delay between characters to look human (realistic typing speed)
				await asyncio.sleep(0.001)

			# Step 4: Trigger framework-aware DOM events after typing completion
			# Modern JavaScript frameworks (React, Vue, Angular) rely on these events
			# to update their internal state and trigger re-renders
			await self._trigger_framework_events(object_id=object_id, cdp_session=cdp_session)

			# Return coordinates metadata if available
			return input_coordinates

		except Exception as e:
			self.logger.error(f'Failed to input text via CDP: {type(e).__name__}: {e}')
			raise BrowserError(f'Failed to input text into element: {repr(element_node)}')

	async def _trigger_framework_events(self, object_id: str, cdp_session) -> None:
		"""
		Trigger framework-aware DOM events after text input completion.

		This is critical for modern JavaScript frameworks (React, Vue, Angular, etc.)
		that rely on DOM events to update their internal state and trigger re-renders.

		Args:
			object_id: CDP object ID of the input element
			cdp_session: CDP session for the element's context
		"""
		try:
			# Execute JavaScript to trigger comprehensive event sequence
			framework_events_script = """
			function() {
				// Find the target element (available as 'this' when using objectId)
				const element = this;
				if (!element) return false;

				// Ensure element is focused
				element.focus();

				// Comprehensive event sequence for maximum framework compatibility
				const events = [
					// Input event - primary event for React controlled components
					{ type: 'input', bubbles: true, cancelable: true },
					// Change event - important for form validation and Vue v-model
					{ type: 'change', bubbles: true, cancelable: true },
					// Blur event - triggers validation in many frameworks
					{ type: 'blur', bubbles: true, cancelable: true }
				];

				let success = true;

				events.forEach(eventConfig => {
					try {
						const event = new Event(eventConfig.type, {
							bubbles: eventConfig.bubbles,
							cancelable: eventConfig.cancelable
						});

						// Special handling for InputEvent (more specific than Event)
						if (eventConfig.type === 'input') {
							const inputEvent = new InputEvent('input', {
								bubbles: true,
								cancelable: true,
								data: element.value,
								inputType: 'insertText'
							});
							element.dispatchEvent(inputEvent);
						} else {
							element.dispatchEvent(event);
						}
					} catch (e) {
						success = false;
						console.warn('Framework event dispatch failed:', eventConfig.type, e);
					}
				});

				// Special React synthetic event handling
				// React uses internal fiber properties for event system
				if (element._reactInternalFiber || element._reactInternalInstance || element.__reactInternalInstance) {
					try {
						// Trigger React's synthetic event system
						const syntheticInputEvent = new InputEvent('input', {
							bubbles: true,
							cancelable: true,
							data: element.value
						});

						// Force React to process this as a synthetic event
						Object.defineProperty(syntheticInputEvent, 'isTrusted', { value: true });
						element.dispatchEvent(syntheticInputEvent);
					} catch (e) {
						console.warn('React synthetic event failed:', e);
					}
				}

				// Special Vue reactivity trigger
				// Vue uses __vueParentComponent or __vue__ for component access
				if (element.__vue__ || element._vnode || element.__vueParentComponent) {
					try {
						// Vue often needs explicit input event with proper timing
						const vueEvent = new Event('input', { bubbles: true });
						setTimeout(() => element.dispatchEvent(vueEvent), 0);
					} catch (e) {
						console.warn('Vue reactivity trigger failed:', e);
					}
				}

				return success;
			}
			"""

			# Execute the framework events script
			result = await cdp_session.cdp_client.send.Runtime.callFunctionOn(
				params={
					'objectId': object_id,
					'functionDeclaration': framework_events_script,
					'returnByValue': True,
				},
				session_id=cdp_session.session_id,
			)

			success = result.get('result', {}).get('value', False)
			if success:
				self.logger.debug('âœ… Framework events triggered successfully')
			else:
				self.logger.warning('âš ï¸ Failed to trigger framework events')

		except Exception as e:
			self.logger.warning(f'âš ï¸ Failed to trigger framework events: {type(e).__name__}: {e}')
			# Don't raise - framework events are a best-effort enhancement

	async def _scroll_with_cdp_gesture(self, pixels: int) -> bool:
		"""
		Scroll using CDP Input.synthesizeScrollGesture to simulate realistic scroll gesture.

		Args:
			pixels: Number of pixels to scroll (positive = down, negative = up)

		Returns:
			True if successful, False if failed
		"""
		try:
			# Get focused CDP session using public API (validates and waits for recovery if needed)
			cdp_session = await self.browser_session.get_or_create_cdp_session()
			cdp_client = cdp_session.cdp_client
			session_id = cdp_session.session_id

			# Get viewport dimensions from cached value if available
			if self.browser_session._original_viewport_size:
				viewport_width, viewport_height = self.browser_session._original_viewport_size
			else:
				# Fallback: query layout metrics
				layout_metrics = await cdp_client.send.Page.getLayoutMetrics(session_id=session_id)
				viewport_width = layout_metrics['layoutViewport']['clientWidth']
				viewport_height = layout_metrics['layoutViewport']['clientHeight']

			# Calculate center of viewport
			center_x = viewport_width / 2
			center_y = viewport_height / 2

			# For scroll gesture, positive yDistance scrolls up, negative scrolls down
			# (opposite of mouseWheel deltaY convention)
			y_distance = -pixels

			# Synthesize scroll gesture with faster speed
			await cdp_client.send.Input.synthesizeScrollGesture(
				params={
					'x': center_x,
					'y': center_y,
					'xDistance': 0,
					'yDistance': y_distance,
					'speed': 1200,  # pixels per second (faster than default 800)
				},
				session_id=session_id,
			)

			self.logger.debug(f'ğŸ“„ Scrolled via CDP gesture: {pixels}px')
			return True

		except Exception as e:
			# Not critical - JavaScript fallback will handle scrolling
			self.logger.debug(f'CDP gesture scroll failed ({type(e).__name__}: {e}), falling back to JS')
			return False

	async def _scroll_element_container(self, element_node, pixels: int) -> bool:
		"""Try to scroll an element's container using CDP."""
		try:
			cdp_session = await self.browser_session.cdp_client_for_node(element_node)

			# Check if this is an iframe - if so, scroll its content directly
			if element_node.tag_name and element_node.tag_name.upper() == 'IFRAME':
				# For iframes, we need to scroll the content document, not the iframe element itself
				# Use JavaScript to directly scroll the iframe's content
				backend_node_id = element_node.backend_node_id

				# Resolve the node to get an object ID
				result = await cdp_session.cdp_client.send.DOM.resolveNode(
					params={'backendNodeId': backend_node_id},
					session_id=cdp_session.session_id,
				)

				if 'object' in result and 'objectId' in result['object']:
					object_id = result['object']['objectId']

					# Scroll the iframe's content directly
					scroll_result = await cdp_session.cdp_client.send.Runtime.callFunctionOn(
						params={
							'functionDeclaration': f"""
								function() {{
									try {{
										const doc = this.contentDocument || this.contentWindow.document;
										if (doc) {{
											const scrollElement = doc.documentElement || doc.body;
											if (scrollElement) {{
												const oldScrollTop = scrollElement.scrollTop;
												scrollElement.scrollTop += {pixels};
												const newScrollTop = scrollElement.scrollTop;
												return {{
													success: true,
													oldScrollTop: oldScrollTop,
													newScrollTop: newScrollTop,
													scrolled: newScrollTop - oldScrollTop
												}};
											}}
										}}
										return {{success: false, error: 'Could not access iframe content'}};
									}} catch (e) {{
										return {{success: false, error: e.toString()}};
									}}
								}}
							""",
							'objectId': object_id,
							'returnByValue': True,
						},
						session_id=cdp_session.session_id,
					)

					if scroll_result and 'result' in scroll_result and 'value' in scroll_result['result']:
						result_value = scroll_result['result']['value']
						if result_value.get('success'):
							self.logger.debug(f'Successfully scrolled iframe content by {result_value.get("scrolled", 0)}px')
							return True
						else:
							self.logger.debug(f'Failed to scroll iframe: {result_value.get("error", "Unknown error")}')

			# For non-iframe elements, use the standard mouse wheel approach
			# Get element bounds to know where to scroll
			backend_node_id = element_node.backend_node_id
			box_model = await cdp_session.cdp_client.send.DOM.getBoxModel(
				params={'backendNodeId': backend_node_id}, session_id=cdp_session.session_id
			)
			content_quad = box_model['model']['content']

			# Calculate center point
			center_x = (content_quad[0] + content_quad[2] + content_quad[4] + content_quad[6]) / 4
			center_y = (content_quad[1] + content_quad[3] + content_quad[5] + content_quad[7]) / 4

			# Dispatch mouse wheel event at element location
			await cdp_session.cdp_client.send.Input.dispatchMouseEvent(
				params={
					'type': 'mouseWheel',
					'x': center_x,
					'y': center_y,
					'deltaX': 0,
					'deltaY': pixels,
				},
				session_id=cdp_session.session_id,
			)

			return True
		except Exception as e:
			self.logger.debug(f'Failed to scroll element container via CDP: {e}')
			return False

	async def _get_session_id_for_element(self, element_node: EnhancedDOMTreeNode) -> str | None:
		"""Get the appropriate CDP session ID for an element based on its frame."""
		if element_node.frame_id:
			# Element is in an iframe, need to get session for that frame
			try:
				all_targets = self.browser_session.session_manager.get_all_targets()

				# Find the target for this frame
				for target_id, target in all_targets.items():
					if target.target_type == 'iframe' and element_node.frame_id in str(target_id):
						# Create temporary session for iframe target without switching focus
						temp_session = await self.browser_session.get_or_create_cdp_session(target_id, focus=False)
						return temp_session.session_id

				# If frame not found in targets, use main target session
				self.logger.debug(f'Frame {element_node.frame_id} not found in targets, using main session')
			except Exception as e:
				self.logger.debug(f'Error getting frame session: {e}, using main session')

		# Use main target session - get_or_create_cdp_session validates focus automatically
		cdp_session = await self.browser_session.get_or_create_cdp_session()
		return cdp_session.session_id

	async def on_GoBackEvent(self, event: GoBackEvent) -> None:
		"""Handle navigate back request with CDP."""
		cdp_session = await self.browser_session.get_or_create_cdp_session()
		try:
			# Get CDP client and session

			# Get navigation history
			history = await cdp_session.cdp_client.send.Page.getNavigationHistory(session_id=cdp_session.session_id)
			current_index = history['currentIndex']
			entries = history['entries']

			# Check if we can go back
			if current_index <= 0:
				self.logger.warning('âš ï¸ Cannot go back - no previous entry in history')
				return

			# Navigate to the previous entry
			previous_entry_id = entries[current_index - 1]['id']
			await cdp_session.cdp_client.send.Page.navigateToHistoryEntry(
				params={'entryId': previous_entry_id}, session_id=cdp_session.session_id
			)

			# Wait for navigation
			await asyncio.sleep(0.5)
			# Navigation is handled by BrowserSession via events

			self.logger.info(f'ğŸ”™ Navigated back to {entries[current_index - 1]["url"]}')
		except Exception as e:
			raise

	async def on_GoForwardEvent(self, event: GoForwardEvent) -> None:
		"""Handle navigate forward request with CDP."""
		cdp_session = await self.browser_session.get_or_create_cdp_session()
		try:
			# Get navigation history
			history = await cdp_session.cdp_client.send.Page.getNavigationHistory(session_id=cdp_session.session_id)
			current_index = history['currentIndex']
			entries = history['entries']

			# Check if we can go forward
			if current_index >= len(entries) - 1:
				self.logger.warning('âš ï¸ Cannot go forward - no next entry in history')
				return

			# Navigate to the next entry
			next_entry_id = entries[current_index + 1]['id']
			await cdp_session.cdp_client.send.Page.navigateToHistoryEntry(
				params={'entryId': next_entry_id}, session_id=cdp_session.session_id
			)

			# Wait for navigation
			await asyncio.sleep(0.5)
			# Navigation is handled by BrowserSession via events

			self.logger.info(f'ğŸ”œ Navigated forward to {entries[current_index + 1]["url"]}')
		except Exception as e:
			raise

	async def on_RefreshEvent(self, event: RefreshEvent) -> None:
		"""Handle target refresh request with CDP."""
		cdp_session = await self.browser_session.get_or_create_cdp_session()
		try:
			# Reload the target
			await cdp_session.cdp_client.send.Page.reload(session_id=cdp_session.session_id)

			# Wait for reload
			await asyncio.sleep(1.0)

			# Note: We don't clear cached state here - let the next state fetch rebuild as needed

			# Navigation is handled by BrowserSession via events

			self.logger.info('ğŸ”„ Target refreshed')
		except Exception as e:
			raise

	@observe_debug(ignore_input=True, ignore_output=True, name='wait_event_handler')
	async def on_WaitEvent(self, event: WaitEvent) -> None:
		"""Handle wait request."""
		try:
			# Cap wait time at maximum
			actual_seconds = min(max(event.seconds, 0), event.max_seconds)
			if actual_seconds != event.seconds:
				self.logger.info(f'ğŸ•’ Waiting for {actual_seconds} seconds (capped from {event.seconds}s)')
			else:
				self.logger.info(f'ğŸ•’ Waiting for {actual_seconds} seconds')

			await asyncio.sleep(actual_seconds)
		except Exception as e:
			raise

	async def _dispatch_key_event(self, cdp_session, event_type: str, key: str, modifiers: int = 0) -> None:
		"""Helper to dispatch a keyboard event with proper key codes."""
		code, vk_code = get_key_info(key)
		params: DispatchKeyEventParameters = {
			'type': event_type,
			'key': key,
			'code': code,
		}
		if modifiers:
			params['modifiers'] = modifiers
		if vk_code is not None:
			params['windowsVirtualKeyCode'] = vk_code
		await cdp_session.cdp_client.send.Input.dispatchKeyEvent(params=params, session_id=cdp_session.session_id)

	async def on_SendKeysEvent(self, event: SendKeysEvent) -> None:
		"""Handle send keys request with CDP."""
		cdp_session = await self.browser_session.get_or_create_cdp_session(focus=True)
		try:
			# Normalize key names from common aliases
			key_aliases = {
				'ctrl': 'Control',
				'control': 'Control',
				'alt': 'Alt',
				'option': 'Alt',
				'meta': 'Meta',
				'cmd': 'Meta',
				'command': 'Meta',
				'shift': 'Shift',
				'enter': 'Enter',
				'return': 'Enter',
				'tab': 'Tab',
				'delete': 'Delete',
				'backspace': 'Backspace',
				'escape': 'Escape',
				'esc': 'Escape',
				'space': ' ',
				'up': 'ArrowUp',
				'down': 'ArrowDown',
				'left': 'ArrowLeft',
				'right': 'ArrowRight',
				'pageup': 'PageUp',
				'pagedown': 'PageDown',
				'home': 'Home',
				'end': 'End',
			}

			# Parse and normalize the key string
			keys = event.keys
			if '+' in keys:
				# Handle key combinations like "ctrl+a"
				parts = keys.split('+')
				normalized_parts = []
				for part in parts:
					part_lower = part.strip().lower()
					normalized = key_aliases.get(part_lower, part)
					normalized_parts.append(normalized)
				normalized_keys = '+'.join(normalized_parts)
			else:
				# Single key
				keys_lower = keys.strip().lower()
				normalized_keys = key_aliases.get(keys_lower, keys)

			# Handle key combinations like "Control+A"
			if '+' in normalized_keys:
				parts = normalized_keys.split('+')
				modifiers = parts[:-1]
				main_key = parts[-1]

				# Calculate modifier bitmask
				modifier_value = 0
				modifier_map = {'Alt': 1, 'Control': 2, 'Meta': 4, 'Shift': 8}
				for mod in modifiers:
					modifier_value |= modifier_map.get(mod, 0)

				# Press modifier keys
				for mod in modifiers:
					await self._dispatch_key_event(cdp_session, 'keyDown', mod)

				# Press main key with modifiers bitmask
				await self._dispatch_key_event(cdp_session, 'keyDown', main_key, modifier_value)

				await self._dispatch_key_event(cdp_session, 'keyUp', main_key, modifier_value)

				# Release modifier keys
				for mod in reversed(modifiers):
					await self._dispatch_key_event(cdp_session, 'keyUp', mod)
			else:
				# Check if this is a text string or special key
				special_keys = {
					'Enter',
					'Tab',
					'Delete',
					'Backspace',
					'Escape',
					'ArrowUp',
					'ArrowDown',
					'ArrowLeft',
					'ArrowRight',
					'PageUp',
					'PageDown',
					'Home',
					'End',
					'Control',
					'Alt',
					'Meta',
					'Shift',
					'F1',
					'F2',
					'F3',
					'F4',
					'F5',
					'F6',
					'F7',
					'F8',
					'F9',
					'F10',
					'F11',
					'F12',
				}

				# If it's a special key, use original logic
				if normalized_keys in special_keys:
					await self._dispatch_key_event(cdp_session, 'keyDown', normalized_keys)
					# For Enter key, also dispatch a char event to trigger keypress listeners
					if normalized_keys == 'Enter':
						await cdp_session.cdp_client.send.Input.dispatchKeyEvent(
							params={
								'type': 'char',
								'text': '\r',
								'key': 'Enter',
							},
							session_id=cdp_session.session_id,
						)
					await self._dispatch_key_event(cdp_session, 'keyUp', normalized_keys)
				else:
					# It's text (single character or string) - send each character as text input
					# This is crucial for text to appear in focused input fields
					for char in normalized_keys:
						# Special-case newline characters to dispatch as Enter
						if char in ('\n', '\r'):
							await cdp_session.cdp_client.send.Input.dispatchKeyEvent(
								params={
									'type': 'rawKeyDown',
									'windowsVirtualKeyCode': 13,
									'unmodifiedText': '\r',
									'text': '\r',
								},
								session_id=cdp_session.session_id,
							)
							await cdp_session.cdp_client.send.Input.dispatchKeyEvent(
								params={
									'type': 'char',
									'windowsVirtualKeyCode': 13,
									'unmodifiedText': '\r',
									'text': '\r',
								},
								session_id=cdp_session.session_id,
							)
							await cdp_session.cdp_client.send.Input.dispatchKeyEvent(
								params={
									'type': 'keyUp',
									'windowsVirtualKeyCode': 13,
									'unmodifiedText': '\r',
									'text': '\r',
								},
								session_id=cdp_session.session_id,
							)
							continue

						# Get proper modifiers and key info for the character
						modifiers, vk_code, base_key = self._get_char_modifiers_and_vk(char)
						key_code = self._get_key_code_for_char(base_key)

						# Send keyDown
						await cdp_session.cdp_client.send.Input.dispatchKeyEvent(
							params={
								'type': 'keyDown',
								'key': base_key,
								'code': key_code,
								'modifiers': modifiers,
								'windowsVirtualKeyCode': vk_code,
							},
							session_id=cdp_session.session_id,
						)

						# Send char event with text - this is what makes text appear in input fields
						await cdp_session.cdp_client.send.Input.dispatchKeyEvent(
							params={
								'type': 'char',
								'text': char,
								'key': char,
							},
							session_id=cdp_session.session_id,
						)

						# Send keyUp
						await cdp_session.cdp_client.send.Input.dispatchKeyEvent(
							params={
								'type': 'keyUp',
								'key': base_key,
								'code': key_code,
								'modifiers': modifiers,
								'windowsVirtualKeyCode': vk_code,
							},
							session_id=cdp_session.session_id,
						)

			self.logger.info(f'âŒ¨ï¸ Sent keys: {event.keys}')

			# Note: We don't clear cached state on Enter; multi_act will detect DOM changes
			# and rebuild explicitly. We still wait briefly for potential navigation.
			if 'enter' in event.keys.lower() or 'return' in event.keys.lower():
				await asyncio.sleep(0.1)
		except Exception as e:
			raise

	async def on_UploadFileEvent(self, event: UploadFileEvent) -> None:
		"""Handle file upload request with CDP."""
		try:
			# Use the provided node
			element_node = event.node
			index_for_logging = element_node.backend_node_id or 'unknown'

			# Check if it's a file input
			if not self.browser_session.is_file_input(element_node):
				msg = f'Upload failed - element {index_for_logging} is not a file input.'
				raise BrowserError(message=msg, long_term_memory=msg)

			# Get CDP client and session
			cdp_client = self.browser_session.cdp_client
			session_id = await self._get_session_id_for_element(element_node)

			# Set file(s) to upload
			backend_node_id = element_node.backend_node_id
			await cdp_client.send.DOM.setFileInputFiles(
				params={
					'files': [event.file_path],
					'backendNodeId': backend_node_id,
				},
				session_id=session_id,
			)

			self.logger.info(f'ğŸ“ Uploaded file {event.file_path} to element {index_for_logging}')
		except Exception as e:
			raise

	async def on_ScrollToTextEvent(self, event: ScrollToTextEvent) -> None:
		"""Handle scroll to text request with CDP. Raises exception if text not found."""

		# TODO: handle looking for text inside cross-origin iframes as well

		# Get focused CDP session using public API (validates and waits for recovery if needed)
		cdp_session = await self.browser_session.get_or_create_cdp_session()
		cdp_client = cdp_session.cdp_client
		session_id = cdp_session.session_id

		# Enable DOM
		await cdp_client.send.DOM.enable(session_id=session_id)

		# Get document
		doc = await cdp_client.send.DOM.getDocument(params={'depth': -1}, session_id=session_id)
		root_node_id = doc['root']['nodeId']

		# Search for text using XPath
		search_queries = [
			f'//*[contains(text(), "{event.text}")]',
			f'//*[contains(., "{event.text}")]',
			f'//*[@*[contains(., "{event.text}")]]',
		]

		found = False
		for query in search_queries:
			try:
				# Perform search
				search_result = await cdp_client.send.DOM.performSearch(params={'query': query}, session_id=session_id)
				search_id = search_result['searchId']
				result_count = search_result['resultCount']

				if result_count > 0:
					# Get the first match
					node_ids = await cdp_client.send.DOM.getSearchResults(
						params={'searchId': search_id, 'fromIndex': 0, 'toIndex': 1},
						session_id=session_id,
					)

					if node_ids['nodeIds']:
						node_id = node_ids['nodeIds'][0]

						# Scroll the element into view
						await cdp_client.send.DOM.scrollIntoViewIfNeeded(params={'nodeId': node_id}, session_id=session_id)

						found = True
						self.logger.debug(f'ğŸ“œ Scrolled to text: "{event.text}"')
						break

				# Clean up search
				await cdp_client.send.DOM.discardSearchResults(params={'searchId': search_id}, session_id=session_id)
			except Exception as e:
				self.logger.debug(f'Search query failed: {query}, error: {e}')
				continue

		if not found:
			# Fallback: Try JavaScript search
			js_result = await cdp_client.send.Runtime.evaluate(
				params={
					'expression': f'''
							(() => {{
								const walker = document.createTreeWalker(
									document.body,
									NodeFilter.SHOW_TEXT,
									null,
									false
								);
								let node;
								while (node = walker.nextNode()) {{
									if (node.textContent.includes("{event.text}")) {{
										node.parentElement.scrollIntoView({{behavior: 'smooth', block: 'center'}});
										return true;
									}}
								}}
								return false;
							}})()
						'''
				},
				session_id=session_id,
			)

			if js_result.get('result', {}).get('value'):
				self.logger.debug(f'ğŸ“œ Scrolled to text: "{event.text}" (via JS)')
				return None
			else:
				self.logger.warning(f'âš ï¸ Text not found: "{event.text}"')
				raise BrowserError(f'Text not found: "{event.text}"', details={'text': event.text})

		# If we got here and found is True, return None (success)
		if found:
			return None
		else:
			raise BrowserError(f'Text not found: "{event.text}"', details={'text': event.text})

	async def on_GetDropdownOptionsEvent(self, event: GetDropdownOptionsEvent) -> dict[str, str]:
		"""Handle get dropdown options request with CDP."""
		try:
			# Use the provided node
			element_node = event.node
			index_for_logging = element_node.backend_node_id or 'unknown'

			# Get CDP session for this node
			cdp_session = await self.browser_session.cdp_client_for_node(element_node)

			# Convert node to object ID for CDP operations
			try:
				object_result = await cdp_session.cdp_client.send.DOM.resolveNode(
					params={'backendNodeId': element_node.backend_node_id}, session_id=cdp_session.session_id
				)
				remote_object = object_result.get('object', {})
				object_id = remote_object.get('objectId')
				if not object_id:
					raise ValueError('Could not get object ID from resolved node')
			except Exception as e:
				raise ValueError(f'Failed to resolve node to object: {e}') from e

			# Use JavaScript to extract dropdown options
			options_script = """
			function() {
				const startElement = this;

				// Function to check if an element is a dropdown and extract options
				function checkDropdownElement(element) {
					// Check if it's a native select element
					if (element.tagName.toLowerCase() === 'select') {
						return {
							type: 'select',
							options: Array.from(element.options).map((opt, idx) => ({
								text: opt.text.trim(),
								value: opt.value,
								index: idx,
								selected: opt.selected
							})),
							id: element.id || '',
							name: element.name || '',
							source: 'target'
						};
					}

					// Check if it's an ARIA dropdown/menu
					const role = element.getAttribute('role');
					if (role === 'menu' || role === 'listbox' || role === 'combobox') {
						// Find all menu items/options
						const menuItems = element.querySelectorAll('[role="menuitem"], [role="option"]');
						const options = [];

						menuItems.forEach((item, idx) => {
							const text = item.textContent ? item.textContent.trim() : '';
							if (text) {
								options.push({
									text: text,
									value: item.getAttribute('data-value') || text,
									index: idx,
									selected: item.getAttribute('aria-selected') === 'true' || item.classList.contains('selected')
								});
							}
						});

						return {
							type: 'aria',
							options: options,
							id: element.id || '',
							name: element.getAttribute('aria-label') || '',
							source: 'target'
						};
					}

					// Check if it's a Semantic UI dropdown or similar
					if (element.classList.contains('dropdown') || element.classList.contains('ui')) {
						const menuItems = element.querySelectorAll('.item, .option, [data-value]');
						const options = [];

						menuItems.forEach((item, idx) => {
							const text = item.textContent ? item.textContent.trim() : '';
							if (text) {
								options.push({
									text: text,
									value: item.getAttribute('data-value') || text,
									index: idx,
									selected: item.classList.contains('selected') || item.classList.contains('active')
								});
							}
						});

						if (options.length > 0) {
							return {
								type: 'custom',
								options: options,
								id: element.id || '',
								name: element.getAttribute('aria-label') || '',
								source: 'target'
							};
						}
					}

					return null;
				}

				// Function to recursively search children up to specified depth
				function searchChildrenForDropdowns(element, maxDepth, currentDepth = 0) {
					if (currentDepth >= maxDepth) return null;

					// Check all direct children
					for (let child of element.children) {
						// Check if this child is a dropdown
						const result = checkDropdownElement(child);
						if (result) {
							result.source = `child-depth-${currentDepth + 1}`;
							return result;
						}

						// Recursively check this child's children
						const childResult = searchChildrenForDropdowns(child, maxDepth, currentDepth + 1);
						if (childResult) {
							return childResult;
						}
					}

					return null;
				}

				// First check the target element itself
				let dropdownResult = checkDropdownElement(startElement);
				if (dropdownResult) {
					return dropdownResult;
				}

				// If target element is not a dropdown, search children up to depth 4
				dropdownResult = searchChildrenForDropdowns(startElement, 4);
				if (dropdownResult) {
					return dropdownResult;
				}

				return {
					error: `Element and its children (depth 4) are not recognizable dropdown types (tag: ${startElement.tagName}, role: ${startElement.getAttribute('role')}, classes: ${startElement.className})`
				};
			}
			"""

			result = await cdp_session.cdp_client.send.Runtime.callFunctionOn(
				params={
					'functionDeclaration': options_script,
					'objectId': object_id,
					'returnByValue': True,
				},
				session_id=cdp_session.session_id,
			)

			dropdown_data = result.get('result', {}).get('value', {})

			if dropdown_data.get('error'):
				raise BrowserError(message=dropdown_data['error'], long_term_memory=dropdown_data['error'])

			if not dropdown_data.get('options'):
				msg = f'No options found in dropdown at index {index_for_logging}'
				return {
					'error': msg,
					'short_term_memory': msg,
					'long_term_memory': msg,
					'backend_node_id': str(index_for_logging),
				}

			# Format options for display
			formatted_options = []
			for opt in dropdown_data['options']:
				# Use JSON encoding to ensure exact string matching
				encoded_text = json.dumps(opt['text'])
				status = ' (selected)' if opt.get('selected') else ''
				formatted_options.append(f'{opt["index"]}: text={encoded_text}, value={json.dumps(opt["value"])}{status}')

			dropdown_type = dropdown_data.get('type', 'select')
			element_info = f'Index: {index_for_logging}, Type: {dropdown_type}, ID: {dropdown_data.get("id", "none")}, Name: {dropdown_data.get("name", "none")}'
			source_info = dropdown_data.get('source', 'unknown')

			if source_info == 'target':
				msg = f'Found {dropdown_type} dropdown ({element_info}):\n' + '\n'.join(formatted_options)
			else:
				msg = f'Found {dropdown_type} dropdown in {source_info} ({element_info}):\n' + '\n'.join(formatted_options)
			msg += (
				f'\n\nUse the exact text or value string (without quotes) in select_dropdown(index={index_for_logging}, text=...)'
			)

			if source_info == 'target':
				self.logger.info(f'ğŸ“‹ Found {len(dropdown_data["options"])} dropdown options for index {index_for_logging}')
			else:
				self.logger.info(
					f'ğŸ“‹ Found {len(dropdown_data["options"])} dropdown options for index {index_for_logging} in {source_info}'
				)

			# Create structured memory for the response
			short_term_memory = msg
			long_term_memory = f'Got dropdown options for index {index_for_logging}'

			# Return the dropdown data as a dict with structured memory
			return {
				'type': dropdown_type,
				'options': json.dumps(dropdown_data['options']),  # Convert list to JSON string for dict[str, str] type
				'element_info': element_info,
				'source': source_info,
				'formatted_options': '\n'.join(formatted_options),
				'message': msg,
				'short_term_memory': short_term_memory,
				'long_term_memory': long_term_memory,
				'backend_node_id': str(index_for_logging),
			}

		except BrowserError:
			# Re-raise BrowserError as-is to preserve structured memory
			raise
		except TimeoutError:
			msg = f'Failed to get dropdown options for index {index_for_logging} due to timeout.'
			self.logger.error(msg)
			raise BrowserError(message=msg, long_term_memory=msg)
		except Exception as e:
			msg = 'Failed to get dropdown options'
			error_msg = f'{msg}: {str(e)}'
			self.logger.error(error_msg)
			raise BrowserError(
				message=error_msg, long_term_memory=f'Failed to get dropdown options for index {index_for_logging}.'
			)

	async def on_SelectDropdownOptionEvent(self, event: SelectDropdownOptionEvent) -> dict[str, str]:
		"""Handle select dropdown option request with CDP."""
		try:
			# Use the provided node
			element_node = event.node
			index_for_logging = element_node.backend_node_id or 'unknown'
			target_text = event.text

			# Get CDP session for this node
			cdp_session = await self.browser_session.cdp_client_for_node(element_node)

			# Convert node to object ID for CDP operations
			try:
				object_result = await cdp_session.cdp_client.send.DOM.resolveNode(
					params={'backendNodeId': element_node.backend_node_id}, session_id=cdp_session.session_id
				)
				remote_object = object_result.get('object', {})
				object_id = remote_object.get('objectId')
				if not object_id:
					raise ValueError('Could not get object ID from resolved node')
			except Exception as e:
				raise ValueError(f'Failed to resolve node to object: {e}') from e

			try:
				# Use JavaScript to select the option
				selection_script = """
				function(targetText) {
					const startElement = this;

					// Function to attempt selection on a dropdown element
					function attemptSelection(element) {
						// Handle native select elements
						if (element.tagName.toLowerCase() === 'select') {
							const options = Array.from(element.options);
							const targetTextLower = targetText.toLowerCase();

							for (const option of options) {
								const optionTextLower = option.text.trim().toLowerCase();
								const optionValueLower = option.value.toLowerCase();

								// Match against both text and value (case-insensitive)
								if (optionTextLower === targetTextLower || optionValueLower === targetTextLower) {
									// Focus the element FIRST (important for Svelte/Vue/React and other reactive frameworks)
									// This simulates the user focusing on the dropdown before changing it
									element.focus();

									// Then set the value
									element.value = option.value;
									option.selected = true;

									// Trigger all necessary events for reactive frameworks
									// 1. input event - critical for Vue's v-model and Svelte's bind:value
									const inputEvent = new Event('input', { bubbles: true, cancelable: true });
									element.dispatchEvent(inputEvent);

									// 2. change event - traditional form validation and framework reactivity
									const changeEvent = new Event('change', { bubbles: true, cancelable: true });
									element.dispatchEvent(changeEvent);

									// 3. blur event - completes the interaction, triggers validation
									element.blur();

									return {
										success: true,
										message: `Selected option: ${option.text.trim()} (value: ${option.value})`,
										value: option.value
									};
								}
							}

							// Return available options as separate field
							const availableOptions = options.map(opt => ({
								text: opt.text.trim(),
								value: opt.value
							}));

							return {
								success: false,
								error: `Option with text or value '${targetText}' not found in select element`,
								availableOptions: availableOptions
							};
						}

						// Handle ARIA dropdowns/menus
						const role = element.getAttribute('role');
						if (role === 'menu' || role === 'listbox' || role === 'combobox') {
							const menuItems = element.querySelectorAll('[role="menuitem"], [role="option"]');
							const targetTextLower = targetText.toLowerCase();

							for (const item of menuItems) {
								if (item.textContent) {
									const itemTextLower = item.textContent.trim().toLowerCase();
									const itemValueLower = (item.getAttribute('data-value') || '').toLowerCase();

									// Match against both text and data-value (case-insensitive)
									if (itemTextLower === targetTextLower || itemValueLower === targetTextLower) {
										// Clear previous selections
										menuItems.forEach(mi => {
											mi.setAttribute('aria-selected', 'false');
											mi.classList.remove('selected');
										});

										// Select this item
										item.setAttribute('aria-selected', 'true');
										item.classList.add('selected');

										// Trigger click and change events
										item.click();
										const clickEvent = new MouseEvent('click', { view: window, bubbles: true, cancelable: true });
										item.dispatchEvent(clickEvent);

										return {
											success: true,
											message: `Selected ARIA menu item: ${item.textContent.trim()}`
										};
									}
								}
							}

							// Return available options as separate field
							const availableOptions = Array.from(menuItems).map(item => ({
								text: item.textContent ? item.textContent.trim() : '',
								value: item.getAttribute('data-value') || ''
							})).filter(opt => opt.text || opt.value);

							return {
								success: false,
								error: `Menu item with text or value '${targetText}' not found`,
								availableOptions: availableOptions
							};
						}

						// Handle Semantic UI or custom dropdowns
						if (element.classList.contains('dropdown') || element.classList.contains('ui')) {
							const menuItems = element.querySelectorAll('.item, .option, [data-value]');
							const targetTextLower = targetText.toLowerCase();

							for (const item of menuItems) {
								if (item.textContent) {
									const itemTextLower = item.textContent.trim().toLowerCase();
									const itemValueLower = (item.getAttribute('data-value') || '').toLowerCase();

									// Match against both text and data-value (case-insensitive)
									if (itemTextLower === targetTextLower || itemValueLower === targetTextLower) {
										// Clear previous selections
										menuItems.forEach(mi => {
											mi.classList.remove('selected', 'active');
										});

										// Select this item
										item.classList.add('selected', 'active');

										// Update dropdown text if there's a text element
										const textElement = element.querySelector('.text');
										if (textElement) {
											textElement.textContent = item.textContent.trim();
										}

										// Trigger click and change events
										item.click();
										const clickEvent = new MouseEvent('click', { view: window, bubbles: true, cancelable: true });
										item.dispatchEvent(clickEvent);

										// Also dispatch on the main dropdown element
										const dropdownChangeEvent = new Event('change', { bubbles: true });
										element.dispatchEvent(dropdownChangeEvent);

										return {
											success: true,
											message: `Selected custom dropdown item: ${item.textContent.trim()}`
										};
									}
								}
							}

							// Return available options as separate field
							const availableOptions = Array.from(menuItems).map(item => ({
								text: item.textContent ? item.textContent.trim() : '',
								value: item.getAttribute('data-value') || ''
							})).filter(opt => opt.text || opt.value);

							return {
								success: false,
								error: `Custom dropdown item with text or value '${targetText}' not found`,
								availableOptions: availableOptions
							};
						}

						return null; // Not a dropdown element
					}

					// Function to recursively search children for dropdowns
					function searchChildrenForSelection(element, maxDepth, currentDepth = 0) {
						if (currentDepth >= maxDepth) return null;

						// Check all direct children
						for (let child of element.children) {
							// Try selection on this child
							const result = attemptSelection(child);
							if (result && result.success) {
								return result;
							}

							// Recursively check this child's children
							const childResult = searchChildrenForSelection(child, maxDepth, currentDepth + 1);
							if (childResult && childResult.success) {
								return childResult;
							}
						}

						return null;
					}

					// First try the target element itself
					let selectionResult = attemptSelection(startElement);
					if (selectionResult) {
						// If attemptSelection returned a result (success or failure), use it
						// Don't search children if we found a dropdown element but selection failed
						return selectionResult;
					}

					// Only search children if target element is not a dropdown element
					selectionResult = searchChildrenForSelection(startElement, 4);
					if (selectionResult && selectionResult.success) {
						return selectionResult;
					}

					return {
						success: false,
						error: `Element and its children (depth 4) do not contain a dropdown with option '${targetText}' (tag: ${startElement.tagName}, role: ${startElement.getAttribute('role')}, classes: ${startElement.className})`
					};
				}
				"""

				result = await cdp_session.cdp_client.send.Runtime.callFunctionOn(
					params={
						'functionDeclaration': selection_script,
						'arguments': [{'value': target_text}],
						'objectId': object_id,
						'returnByValue': True,
					},
					session_id=cdp_session.session_id,
				)

				selection_result = result.get('result', {}).get('value', {})

				if selection_result.get('success'):
					msg = selection_result.get('message', f'Selected option: {target_text}')
					self.logger.debug(f'{msg}')

					# Return the result as a dict
					return {
						'success': 'true',
						'message': msg,
						'value': selection_result.get('value', target_text),
						'backend_node_id': str(index_for_logging),
					}
				else:
					error_msg = selection_result.get('error', f'Failed to select option: {target_text}')
					available_options = selection_result.get('availableOptions', [])
					self.logger.error(f'âŒ {error_msg}')
					self.logger.debug(f'Available options from JavaScript: {available_options}')

					# If we have available options, return structured error data
					if available_options:
						# Format options for short_term_memory (simple bulleted list)
						short_term_options = []
						for opt in available_options:
							if isinstance(opt, dict):
								text = opt.get('text', '').strip()
								value = opt.get('value', '').strip()
								if text:
									short_term_options.append(f'- {text}')
								elif value:
									short_term_options.append(f'- {value}')
							elif isinstance(opt, str):
								short_term_options.append(f'- {opt}')

						if short_term_options:
							short_term_memory = 'Available dropdown options  are:\n' + '\n'.join(short_term_options)
							long_term_memory = (
								f"Couldn't select the dropdown option as '{target_text}' is not one of the available options."
							)

							# Return error result with structured memory instead of raising exception
							return {
								'success': 'false',
								'error': error_msg,
								'short_term_memory': short_term_memory,
								'long_term_memory': long_term_memory,
								'backend_node_id': str(index_for_logging),
							}

					# Fallback to regular error result if no available options
					return {
						'success': 'false',
						'error': error_msg,
						'backend_node_id': str(index_for_logging),
					}

			except Exception as e:
				error_msg = f'Failed to select dropdown option: {str(e)}'
				self.logger.error(error_msg)
				raise ValueError(error_msg) from e

		except Exception as e:
			error_msg = f'Failed to select dropdown option "{target_text}" for element {index_for_logging}: {str(e)}'
			self.logger.error(error_msg)
			raise ValueError(error_msg) from e

```

---

## backend/browser-use/browser_use/browser/watchdogs/dom_watchdog.py

```py
"""DOM watchdog for browser DOM tree management using CDP."""

import asyncio
import time
from typing import TYPE_CHECKING

from browser_use.browser.events import (
	BrowserErrorEvent,
	BrowserStateRequestEvent,
	ScreenshotEvent,
	TabCreatedEvent,
)
from browser_use.browser.watchdog_base import BaseWatchdog
from browser_use.dom.service import DomService
from browser_use.dom.views import (
	EnhancedDOMTreeNode,
	SerializedDOMState,
)
from browser_use.observability import observe_debug
from browser_use.utils import create_task_with_error_handling, time_execution_async

if TYPE_CHECKING:
	from browser_use.browser.views import BrowserStateSummary, NetworkRequest, PageInfo, PaginationButton


class DOMWatchdog(BaseWatchdog):
	"""Handles DOM tree building, serialization, and element access via CDP.

	This watchdog acts as a bridge between the event-driven browser session
	and the DomService implementation, maintaining cached state and providing
	helper methods for other watchdogs.
	"""

	LISTENS_TO = [TabCreatedEvent, BrowserStateRequestEvent]
	EMITS = [BrowserErrorEvent]

	# Public properties for other watchdogs
	selector_map: dict[int, EnhancedDOMTreeNode] | None = None
	current_dom_state: SerializedDOMState | None = None
	enhanced_dom_tree: EnhancedDOMTreeNode | None = None

	# Internal DOM service
	_dom_service: DomService | None = None

	# Network tracking - maps request_id to (url, start_time, method, resource_type)
	_pending_requests: dict[str, tuple[str, float, str, str | None]] = {}

	async def on_TabCreatedEvent(self, event: TabCreatedEvent) -> None:
		# self.logger.debug('Setting up init scripts in browser')
		return None

	def _get_recent_events_str(self, limit: int = 10) -> str | None:
		"""Get the most recent events from the event bus as JSON.

		Args:
			limit: Maximum number of recent events to include

		Returns:
			JSON string of recent events or None if not available
		"""
		import json

		try:
			# Get all events from history, sorted by creation time (most recent first)
			all_events = sorted(
				self.browser_session.event_bus.event_history.values(), key=lambda e: e.event_created_at.timestamp(), reverse=True
			)

			# Take the most recent events and create JSON-serializable data
			recent_events_data = []
			for event in all_events[:limit]:
				event_data = {
					'event_type': event.event_type,
					'timestamp': event.event_created_at.isoformat(),
				}
				# Add specific fields for certain event types
				if hasattr(event, 'url'):
					event_data['url'] = getattr(event, 'url')
				if hasattr(event, 'error_message'):
					event_data['error_message'] = getattr(event, 'error_message')
				if hasattr(event, 'target_id'):
					event_data['target_id'] = getattr(event, 'target_id')
				recent_events_data.append(event_data)

			return json.dumps(recent_events_data)  # Return empty array if no events
		except Exception as e:
			self.logger.debug(f'Failed to get recent events: {e}')

		return json.dumps([])  # Return empty JSON array on error

	async def _get_pending_network_requests(self) -> list['NetworkRequest']:
		"""Get list of currently pending network requests.

		Uses document.readyState and performance API to detect pending requests.
		Filters out ads, tracking, and other noise.

		Returns:
			List of NetworkRequest objects representing currently loading resources
		"""
		from browser_use.browser.views import NetworkRequest

		try:
			# get_or_create_cdp_session() now handles focus validation automatically
			cdp_session = await self.browser_session.get_or_create_cdp_session(focus=True)

			# Use performance API to get pending requests
			js_code = """
(function() {
	const now = performance.now();
	const resources = performance.getEntriesByType('resource');
	const pending = [];

	// Check document readyState
	const docLoading = document.readyState !== 'complete';

	// Common ad/tracking domains and patterns to filter out
	const adDomains = [
		// Standard ad/tracking networks
		'doubleclick.net', 'googlesyndication.com', 'googletagmanager.com',
		'facebook.net', 'analytics', 'ads', 'tracking', 'pixel',
		'hotjar.com', 'clarity.ms', 'mixpanel.com', 'segment.com',
		// Analytics platforms
		'demdex.net', 'omtrdc.net', 'adobedtm.com', 'ensighten.com',
		'newrelic.com', 'nr-data.net', 'google-analytics.com',
		// Social media trackers
		'connect.facebook.net', 'platform.twitter.com', 'platform.linkedin.com',
		// CDN/image hosts (usually not critical for functionality)
		'.cloudfront.net/image/', '.akamaized.net/image/',
		// Common tracking paths
		'/tracker/', '/collector/', '/beacon/', '/telemetry/', '/log/',
		'/events/', '/eventBatch', '/track.', '/metrics/'
	];

	// Get resources that are still loading (responseEnd is 0)
	let totalResourcesChecked = 0;
	let filteredByResponseEnd = 0;
	const allDomains = new Set();

	for (const entry of resources) {
		totalResourcesChecked++;

		// Track all domains from recent resources (for logging)
		try {
			const hostname = new URL(entry.name).hostname;
			if (hostname) allDomains.add(hostname);
		} catch (e) {}

		if (entry.responseEnd === 0) {
			filteredByResponseEnd++;
			const url = entry.name;

			// Filter out ads and tracking
			const isAd = adDomains.some(domain => url.includes(domain));
			if (isAd) continue;

			// Filter out data: URLs and very long URLs (often inline resources)
			if (url.startsWith('data:') || url.length > 500) continue;

			const loadingDuration = now - entry.startTime;

			// Skip requests that have been loading for >10 seconds (likely stuck/polling)
			if (loadingDuration > 10000) continue;

			const resourceType = entry.initiatorType || 'unknown';

			// Filter out non-critical resources (images, fonts, icons) if loading >3 seconds
			const nonCriticalTypes = ['img', 'image', 'icon', 'font'];
			if (nonCriticalTypes.includes(resourceType) && loadingDuration > 3000) continue;

			// Filter out image URLs even if type is unknown
			const isImageUrl = /\\.(jpg|jpeg|png|gif|webp|svg|ico)(\\?|$)/i.test(url);
			if (isImageUrl && loadingDuration > 3000) continue;

			pending.push({
				url: url,
				method: 'GET',
				loading_duration_ms: Math.round(loadingDuration),
				resource_type: resourceType
			});
		}
	}

	return {
		pending_requests: pending,
		document_loading: docLoading,
		document_ready_state: document.readyState,
		debug: {
			total_resources: totalResourcesChecked,
			with_response_end_zero: filteredByResponseEnd,
			after_all_filters: pending.length,
			all_domains: Array.from(allDomains)
		}
	};
})()
"""

			result = await cdp_session.cdp_client.send.Runtime.evaluate(
				params={'expression': js_code, 'returnByValue': True}, session_id=cdp_session.session_id
			)

			if result.get('result', {}).get('type') == 'object':
				data = result['result'].get('value', {})
				pending = data.get('pending_requests', [])
				doc_state = data.get('document_ready_state', 'unknown')
				doc_loading = data.get('document_loading', False)
				debug_info = data.get('debug', {})

				# Get all domains that had recent activity (from JS)
				all_domains = debug_info.get('all_domains', [])
				all_domains_str = ', '.join(sorted(all_domains)[:5]) if all_domains else 'none'
				if len(all_domains) > 5:
					all_domains_str += f' +{len(all_domains) - 5} more'

				# Debug logging
				self.logger.debug(
					f'ğŸ” Network check: document.readyState={doc_state}, loading={doc_loading}, '
					f'total_resources={debug_info.get("total_resources", 0)}, '
					f'responseEnd=0: {debug_info.get("with_response_end_zero", 0)}, '
					f'after_filters={len(pending)}, domains=[{all_domains_str}]'
				)

				# Convert to NetworkRequest objects
				network_requests = []
				for req in pending[:20]:  # Limit to 20 to avoid overwhelming the context
					network_requests.append(
						NetworkRequest(
							url=req['url'],
							method=req.get('method', 'GET'),
							loading_duration_ms=req.get('loading_duration_ms', 0.0),
							resource_type=req.get('resource_type'),
						)
					)

				return network_requests

		except Exception as e:
			self.logger.debug(f'Failed to get pending network requests: {e}')

		return []

	@observe_debug(ignore_input=True, ignore_output=True, name='browser_state_request_event')
	async def on_BrowserStateRequestEvent(self, event: BrowserStateRequestEvent) -> 'BrowserStateSummary':
		"""Handle browser state request by coordinating DOM building and screenshot capture.

		This is the main entry point for getting the complete browser state.

		Args:
			event: The browser state request event with options

		Returns:
			Complete BrowserStateSummary with DOM, screenshot, and target info
		"""
		from browser_use.browser.views import BrowserStateSummary, PageInfo

		self.logger.debug('ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: STARTING browser state request')
		page_url = await self.browser_session.get_current_page_url()
		self.logger.debug(f'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: Got page URL: {page_url}')

		# Get focused session for logging (validation already done by get_current_page_url)
		if self.browser_session.agent_focus_target_id:
			self.logger.debug(f'Current page URL: {page_url}, target_id: {self.browser_session.agent_focus_target_id}')

		# check if we should skip DOM tree build for pointless pages
		not_a_meaningful_website = page_url.lower().split(':', 1)[0] not in ('http', 'https')

		# Check for pending network requests BEFORE waiting (so we can see what's loading)
		pending_requests_before_wait = []
		if not not_a_meaningful_website:
			try:
				pending_requests_before_wait = await self._get_pending_network_requests()
				if pending_requests_before_wait:
					self.logger.debug(f'ğŸ” Found {len(pending_requests_before_wait)} pending requests before stability wait')
			except Exception as e:
				self.logger.debug(f'Failed to get pending requests before wait: {e}')
		pending_requests = pending_requests_before_wait
		# Wait for page stability using browser profile settings (main branch pattern)
		if not not_a_meaningful_website:
			self.logger.debug('ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: â³ Waiting for page stability...')
			try:
				if pending_requests_before_wait:
					# Reduced from 1s to 0.3s for faster DOM builds while still allowing critical resources to load
					await asyncio.sleep(0.3)
				self.logger.debug('ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: âœ… Page stability complete')
			except Exception as e:
				self.logger.warning(
					f'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: Network waiting failed: {e}, continuing anyway...'
				)

		# Get tabs info once at the beginning for all paths
		self.logger.debug('ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: Getting tabs info...')
		tabs_info = await self.browser_session.get_tabs()
		self.logger.debug(f'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: Got {len(tabs_info)} tabs')
		self.logger.debug(f'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: Tabs info: {tabs_info}')

		# Get viewport / scroll position info, remember changing scroll position should invalidate selector_map cache because it only includes visible elements
		# cdp_session = await self.browser_session.get_or_create_cdp_session(focus=True)
		# scroll_info = await cdp_session.cdp_client.send.Runtime.evaluate(
		# 	params={'expression': 'JSON.stringify({y: document.body.scrollTop, x: document.body.scrollLeft, width: document.documentElement.clientWidth, height: document.documentElement.clientHeight})'},
		# 	session_id=cdp_session.session_id,
		# )
		# self.logger.debug(f'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: Got scroll info: {scroll_info["result"]}')

		try:
			# Fast path for empty pages
			if not_a_meaningful_website:
				self.logger.debug(f'âš¡ Skipping BuildDOMTree for empty target: {page_url}')
				self.logger.debug(f'ğŸ“¸ Not taking screenshot for empty page: {page_url} (non-http/https URL)')

				# Create minimal DOM state
				content = SerializedDOMState(_root=None, selector_map={})

				# Skip screenshot for empty pages
				screenshot_b64 = None

				# Try to get page info from CDP, fall back to defaults if unavailable
				try:
					page_info = await self._get_page_info()
				except Exception as e:
					self.logger.debug(f'Failed to get page info from CDP for empty page: {e}, using fallback')
					# Use default viewport dimensions
					viewport = self.browser_session.browser_profile.viewport or {'width': 1280, 'height': 720}
					page_info = PageInfo(
						viewport_width=viewport['width'],
						viewport_height=viewport['height'],
						page_width=viewport['width'],
						page_height=viewport['height'],
						scroll_x=0,
						scroll_y=0,
						pixels_above=0,
						pixels_below=0,
						pixels_left=0,
						pixels_right=0,
					)

				return BrowserStateSummary(
					dom_state=content,
					url=page_url,
					title='Empty Tab',
					tabs=tabs_info,
					screenshot=screenshot_b64,
					page_info=page_info,
					pixels_above=0,
					pixels_below=0,
					browser_errors=[],
					is_pdf_viewer=False,
					recent_events=self._get_recent_events_str() if event.include_recent_events else None,
					pending_network_requests=[],  # Empty page has no pending requests
					pagination_buttons=[],  # Empty page has no pagination
					closed_popup_messages=self.browser_session._closed_popup_messages.copy(),
				)

			# Execute DOM building and screenshot capture in parallel
			dom_task = None
			screenshot_task = None

			# Start DOM building task if requested
			if event.include_dom:
				self.logger.debug('ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: ğŸŒ³ Starting DOM tree build task...')

				previous_state = (
					self.browser_session._cached_browser_state_summary.dom_state
					if self.browser_session._cached_browser_state_summary
					else None
				)

				dom_task = create_task_with_error_handling(
					self._build_dom_tree_without_highlights(previous_state),
					name='build_dom_tree',
					logger_instance=self.logger,
					suppress_exceptions=True,
				)

			# Start clean screenshot task if requested (without JS highlights)
			if event.include_screenshot:
				self.logger.debug('ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: ğŸ“¸ Starting clean screenshot task...')
				screenshot_task = create_task_with_error_handling(
					self._capture_clean_screenshot(),
					name='capture_screenshot',
					logger_instance=self.logger,
					suppress_exceptions=True,
				)

			# Wait for both tasks to complete
			content = None
			screenshot_b64 = None

			if dom_task:
				try:
					content = await dom_task
					self.logger.debug('ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: âœ… DOM tree build completed')
				except Exception as e:
					self.logger.warning(f'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: DOM build failed: {e}, using minimal state')
					content = SerializedDOMState(_root=None, selector_map={})
			else:
				content = SerializedDOMState(_root=None, selector_map={})

			if screenshot_task:
				try:
					screenshot_b64 = await screenshot_task
					self.logger.debug('ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: âœ… Clean screenshot captured')
				except Exception as e:
					self.logger.warning(f'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: Clean screenshot failed: {e}')
					screenshot_b64 = None

			# Add browser-side highlights for user visibility
			if content and content.selector_map and self.browser_session.browser_profile.dom_highlight_elements:
				try:
					self.logger.debug('ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: ğŸ¨ Adding browser-side highlights...')
					await self.browser_session.add_highlights(content.selector_map)
					self.logger.debug(
						f'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: âœ… Added browser highlights for {len(content.selector_map)} elements'
					)
				except Exception as e:
					self.logger.warning(f'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: Browser highlighting failed: {e}')

			# Ensure we have valid content
			if not content:
				content = SerializedDOMState(_root=None, selector_map={})

			# Tabs info already fetched at the beginning

			# Get target title safely
			try:
				self.logger.debug('ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: Getting page title...')
				title = await asyncio.wait_for(self.browser_session.get_current_page_title(), timeout=1.0)
				self.logger.debug(f'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: Got title: {title}')
			except Exception as e:
				self.logger.debug(f'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: Failed to get title: {e}')
				title = 'Page'

			# Get comprehensive page info from CDP with timeout
			try:
				self.logger.debug('ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: Getting page info from CDP...')
				page_info = await asyncio.wait_for(self._get_page_info(), timeout=1.0)
				self.logger.debug(f'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: Got page info from CDP: {page_info}')
			except Exception as e:
				self.logger.debug(
					f'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: Failed to get page info from CDP: {e}, using fallback'
				)
				# Fallback to default viewport dimensions
				viewport = self.browser_session.browser_profile.viewport or {'width': 1280, 'height': 720}
				page_info = PageInfo(
					viewport_width=viewport['width'],
					viewport_height=viewport['height'],
					page_width=viewport['width'],
					page_height=viewport['height'],
					scroll_x=0,
					scroll_y=0,
					pixels_above=0,
					pixels_below=0,
					pixels_left=0,
					pixels_right=0,
				)

			# Check for PDF viewer
			is_pdf_viewer = page_url.endswith('.pdf') or '/pdf/' in page_url

			# Detect pagination buttons from the DOM
			pagination_buttons_data = []
			if content and content.selector_map:
				pagination_buttons_data = self._detect_pagination_buttons(content.selector_map)

			# Build and cache the browser state summary
			if screenshot_b64:
				self.logger.debug(
					f'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: ğŸ“¸ Creating BrowserStateSummary with screenshot, length: {len(screenshot_b64)}'
				)
			else:
				self.logger.debug(
					'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: ğŸ“¸ Creating BrowserStateSummary WITHOUT screenshot'
				)

			browser_state = BrowserStateSummary(
				dom_state=content,
				url=page_url,
				title=title,
				tabs=tabs_info,
				screenshot=screenshot_b64,
				page_info=page_info,
				pixels_above=0,
				pixels_below=0,
				browser_errors=[],
				is_pdf_viewer=is_pdf_viewer,
				recent_events=self._get_recent_events_str() if event.include_recent_events else None,
				pending_network_requests=pending_requests,
				pagination_buttons=pagination_buttons_data,
				closed_popup_messages=self.browser_session._closed_popup_messages.copy(),
			)

			# Cache the state
			self.browser_session._cached_browser_state_summary = browser_state

			# Cache viewport size for coordinate conversion (if llm_screenshot_size is enabled)
			if page_info:
				self.browser_session._original_viewport_size = (page_info.viewport_width, page_info.viewport_height)

			self.logger.debug('ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: âœ… COMPLETED - Returning browser state')
			return browser_state

		except Exception as e:
			self.logger.error(f'Failed to get browser state: {e}')

			# Return minimal recovery state
			return BrowserStateSummary(
				dom_state=SerializedDOMState(_root=None, selector_map={}),
				url=page_url if 'page_url' in locals() else '',
				title='Error',
				tabs=[],
				screenshot=None,
				page_info=PageInfo(
					viewport_width=1280,
					viewport_height=720,
					page_width=1280,
					page_height=720,
					scroll_x=0,
					scroll_y=0,
					pixels_above=0,
					pixels_below=0,
					pixels_left=0,
					pixels_right=0,
				),
				pixels_above=0,
				pixels_below=0,
				browser_errors=[str(e)],
				is_pdf_viewer=False,
				recent_events=None,
				pending_network_requests=[],  # Error state has no pending requests
				pagination_buttons=[],  # Error state has no pagination
				closed_popup_messages=self.browser_session._closed_popup_messages.copy()
				if hasattr(self, 'browser_session') and self.browser_session is not None
				else [],
			)

	@time_execution_async('build_dom_tree_without_highlights')
	@observe_debug(ignore_input=True, ignore_output=True, name='build_dom_tree_without_highlights')
	async def _build_dom_tree_without_highlights(self, previous_state: SerializedDOMState | None = None) -> SerializedDOMState:
		"""Build DOM tree without injecting JavaScript highlights (for parallel execution)."""
		try:
			self.logger.debug('ğŸ” DOMWatchdog._build_dom_tree_without_highlights: STARTING DOM tree build')

			# Create or reuse DOM service
			if self._dom_service is None:
				self._dom_service = DomService(
					browser_session=self.browser_session,
					logger=self.logger,
					cross_origin_iframes=self.browser_session.browser_profile.cross_origin_iframes,
					paint_order_filtering=self.browser_session.browser_profile.paint_order_filtering,
					max_iframes=self.browser_session.browser_profile.max_iframes,
					max_iframe_depth=self.browser_session.browser_profile.max_iframe_depth,
				)

			# Get serialized DOM tree using the service
			self.logger.debug('ğŸ” DOMWatchdog._build_dom_tree_without_highlights: Calling DomService.get_serialized_dom_tree...')
			start = time.time()
			self.current_dom_state, self.enhanced_dom_tree, timing_info = await self._dom_service.get_serialized_dom_tree(
				previous_cached_state=previous_state,
			)
			end = time.time()
			total_time_ms = (end - start) * 1000
			self.logger.debug(
				'ğŸ” DOMWatchdog._build_dom_tree_without_highlights: âœ… DomService.get_serialized_dom_tree completed'
			)

			# Build hierarchical timing breakdown as single multi-line string
			timing_lines = [f'â±ï¸ Total DOM tree time: {total_time_ms:.2f}ms', 'ğŸ“Š Timing breakdown:']

			# get_all_trees breakdown
			get_all_trees_ms = timing_info.get('get_all_trees_total_ms', 0)
			if get_all_trees_ms > 0:
				timing_lines.append(f'  â”œâ”€ get_all_trees: {get_all_trees_ms:.2f}ms')
				iframe_scroll_ms = timing_info.get('iframe_scroll_detection_ms', 0)
				cdp_parallel_ms = timing_info.get('cdp_parallel_calls_ms', 0)
				snapshot_proc_ms = timing_info.get('snapshot_processing_ms', 0)
				if iframe_scroll_ms > 0.01:
					timing_lines.append(f'  â”‚  â”œâ”€ iframe_scroll_detection: {iframe_scroll_ms:.2f}ms')
				if cdp_parallel_ms > 0.01:
					timing_lines.append(f'  â”‚  â”œâ”€ cdp_parallel_calls: {cdp_parallel_ms:.2f}ms')
				if snapshot_proc_ms > 0.01:
					timing_lines.append(f'  â”‚  â””â”€ snapshot_processing: {snapshot_proc_ms:.2f}ms')

			# build_ax_lookup
			build_ax_ms = timing_info.get('build_ax_lookup_ms', 0)
			if build_ax_ms > 0.01:
				timing_lines.append(f'  â”œâ”€ build_ax_lookup: {build_ax_ms:.2f}ms')

			# build_snapshot_lookup
			build_snapshot_ms = timing_info.get('build_snapshot_lookup_ms', 0)
			if build_snapshot_ms > 0.01:
				timing_lines.append(f'  â”œâ”€ build_snapshot_lookup: {build_snapshot_ms:.2f}ms')

			# construct_enhanced_tree
			construct_tree_ms = timing_info.get('construct_enhanced_tree_ms', 0)
			if construct_tree_ms > 0.01:
				timing_lines.append(f'  â”œâ”€ construct_enhanced_tree: {construct_tree_ms:.2f}ms')

			# serialize_accessible_elements breakdown
			serialize_total_ms = timing_info.get('serialize_accessible_elements_total_ms', 0)
			if serialize_total_ms > 0.01:
				timing_lines.append(f'  â”œâ”€ serialize_accessible_elements: {serialize_total_ms:.2f}ms')
				create_simp_ms = timing_info.get('create_simplified_tree_ms', 0)
				paint_order_ms = timing_info.get('calculate_paint_order_ms', 0)
				optimize_ms = timing_info.get('optimize_tree_ms', 0)
				bbox_ms = timing_info.get('bbox_filtering_ms', 0)
				assign_idx_ms = timing_info.get('assign_interactive_indices_ms', 0)
				clickable_ms = timing_info.get('clickable_detection_time_ms', 0)

				if create_simp_ms > 0.01:
					timing_lines.append(f'  â”‚  â”œâ”€ create_simplified_tree: {create_simp_ms:.2f}ms')
					if clickable_ms > 0.01:
						timing_lines.append(f'  â”‚  â”‚  â””â”€ clickable_detection: {clickable_ms:.2f}ms')
				if paint_order_ms > 0.01:
					timing_lines.append(f'  â”‚  â”œâ”€ calculate_paint_order: {paint_order_ms:.2f}ms')
				if optimize_ms > 0.01:
					timing_lines.append(f'  â”‚  â”œâ”€ optimize_tree: {optimize_ms:.2f}ms')
				if bbox_ms > 0.01:
					timing_lines.append(f'  â”‚  â”œâ”€ bbox_filtering: {bbox_ms:.2f}ms')
				if assign_idx_ms > 0.01:
					timing_lines.append(f'  â”‚  â””â”€ assign_interactive_indices: {assign_idx_ms:.2f}ms')

			# Overheads
			get_dom_overhead_ms = timing_info.get('get_dom_tree_overhead_ms', 0)
			serialize_overhead_ms = timing_info.get('serialization_overhead_ms', 0)
			get_serialized_overhead_ms = timing_info.get('get_serialized_dom_tree_overhead_ms', 0)

			if get_dom_overhead_ms > 0.1:
				timing_lines.append(f'  â”œâ”€ get_dom_tree_overhead: {get_dom_overhead_ms:.2f}ms')
			if serialize_overhead_ms > 0.1:
				timing_lines.append(f'  â”œâ”€ serialization_overhead: {serialize_overhead_ms:.2f}ms')
			if get_serialized_overhead_ms > 0.1:
				timing_lines.append(f'  â””â”€ get_serialized_dom_tree_overhead: {get_serialized_overhead_ms:.2f}ms')

			# Calculate total tracked time for validation
			main_operations_ms = (
				get_all_trees_ms
				+ build_ax_ms
				+ build_snapshot_ms
				+ construct_tree_ms
				+ serialize_total_ms
				+ get_dom_overhead_ms
				+ serialize_overhead_ms
				+ get_serialized_overhead_ms
			)
			untracked_time_ms = total_time_ms - main_operations_ms

			if untracked_time_ms > 1.0:  # Only log if significant
				timing_lines.append(f'  âš ï¸  untracked_time: {untracked_time_ms:.2f}ms')

			# Single log call with all timing info
			self.logger.debug('\n'.join(timing_lines))

			# Update selector map for other watchdogs
			self.logger.debug('ğŸ” DOMWatchdog._build_dom_tree_without_highlights: Updating selector maps...')
			self.selector_map = self.current_dom_state.selector_map
			# Update BrowserSession's cached selector map
			if self.browser_session:
				self.browser_session.update_cached_selector_map(self.selector_map)
			self.logger.debug(
				f'ğŸ” DOMWatchdog._build_dom_tree_without_highlights: âœ… Selector maps updated, {len(self.selector_map)} elements'
			)

			# Skip JavaScript highlighting injection - Python highlighting will be applied later
			self.logger.debug('ğŸ” DOMWatchdog._build_dom_tree_without_highlights: âœ… COMPLETED DOM tree build (no JS highlights)')
			return self.current_dom_state

		except Exception as e:
			self.logger.error(f'Failed to build DOM tree without highlights: {e}')
			self.event_bus.dispatch(
				BrowserErrorEvent(
					error_type='DOMBuildFailed',
					message=str(e),
				)
			)
			raise

	@time_execution_async('capture_clean_screenshot')
	@observe_debug(ignore_input=True, ignore_output=True, name='capture_clean_screenshot')
	async def _capture_clean_screenshot(self) -> str:
		"""Capture a clean screenshot without JavaScript highlights."""
		try:
			self.logger.debug('ğŸ” DOMWatchdog._capture_clean_screenshot: Capturing clean screenshot...')

			await self.browser_session.get_or_create_cdp_session(target_id=self.browser_session.agent_focus_target_id, focus=True)

			# Check if handler is registered
			handlers = self.event_bus.handlers.get('ScreenshotEvent', [])
			handler_names = [getattr(h, '__name__', str(h)) for h in handlers]
			self.logger.debug(f'ğŸ“¸ ScreenshotEvent handlers registered: {len(handlers)} - {handler_names}')

			screenshot_event = self.event_bus.dispatch(ScreenshotEvent(full_page=False))
			self.logger.debug('ğŸ“¸ Dispatched ScreenshotEvent, waiting for event to complete...')

			# Wait for the event itself to complete (this waits for all handlers)
			await screenshot_event

			# Get the single handler result
			screenshot_b64 = await screenshot_event.event_result(raise_if_any=True, raise_if_none=True)
			if screenshot_b64 is None:
				raise RuntimeError('Screenshot handler returned None')
			self.logger.debug('ğŸ” DOMWatchdog._capture_clean_screenshot: âœ… Clean screenshot captured successfully')
			return str(screenshot_b64)

		except TimeoutError:
			self.logger.warning('ğŸ“¸ Clean screenshot timed out after 6 seconds - no handler registered or slow page?')
			raise
		except Exception as e:
			self.logger.warning(f'ğŸ“¸ Clean screenshot failed: {type(e).__name__}: {e}')
			raise

	def _detect_pagination_buttons(self, selector_map: dict[int, EnhancedDOMTreeNode]) -> list['PaginationButton']:
		"""Detect pagination buttons from the DOM selector map.

		Args:
			selector_map: Dictionary mapping element indices to DOM tree nodes

		Returns:
			List of PaginationButton instances found in the DOM
		"""
		from browser_use.browser.views import PaginationButton

		pagination_buttons_data = []
		try:
			self.logger.debug('ğŸ” DOMWatchdog._detect_pagination_buttons: Detecting pagination buttons...')
			pagination_buttons_raw = DomService.detect_pagination_buttons(selector_map)
			# Convert to PaginationButton instances
			pagination_buttons_data = [
				PaginationButton(
					button_type=btn['button_type'],  # type: ignore
					backend_node_id=btn['backend_node_id'],  # type: ignore
					text=btn['text'],  # type: ignore
					selector=btn['selector'],  # type: ignore
					is_disabled=btn['is_disabled'],  # type: ignore
				)
				for btn in pagination_buttons_raw
			]
			if pagination_buttons_data:
				self.logger.debug(
					f'ğŸ” DOMWatchdog._detect_pagination_buttons: Found {len(pagination_buttons_data)} pagination buttons'
				)
		except Exception as e:
			self.logger.warning(f'ğŸ” DOMWatchdog._detect_pagination_buttons: Pagination detection failed: {e}')

		return pagination_buttons_data

	async def _get_page_info(self) -> 'PageInfo':
		"""Get comprehensive page information using a single CDP call.

		TODO: should we make this an event as well?

		Returns:
			PageInfo with all viewport, page dimensions, and scroll information
		"""

		from browser_use.browser.views import PageInfo

		# get_or_create_cdp_session() handles focus validation automatically
		cdp_session = await self.browser_session.get_or_create_cdp_session(
			target_id=self.browser_session.agent_focus_target_id, focus=True
		)

		# Get layout metrics which includes all the information we need
		metrics = await asyncio.wait_for(
			cdp_session.cdp_client.send.Page.getLayoutMetrics(session_id=cdp_session.session_id), timeout=10.0
		)

		# Extract different viewport types
		layout_viewport = metrics.get('layoutViewport', {})
		visual_viewport = metrics.get('visualViewport', {})
		css_visual_viewport = metrics.get('cssVisualViewport', {})
		css_layout_viewport = metrics.get('cssLayoutViewport', {})
		content_size = metrics.get('contentSize', {})

		# Calculate device pixel ratio to convert between device pixels and CSS pixels
		# This matches the approach in dom/service.py _get_viewport_ratio method
		css_width = css_visual_viewport.get('clientWidth', css_layout_viewport.get('clientWidth', 1280.0))
		device_width = visual_viewport.get('clientWidth', css_width)
		device_pixel_ratio = device_width / css_width if css_width > 0 else 1.0

		# For viewport dimensions, use CSS pixels (what JavaScript sees)
		# Prioritize CSS layout viewport, then fall back to layout viewport
		viewport_width = int(css_layout_viewport.get('clientWidth') or layout_viewport.get('clientWidth', 1280))
		viewport_height = int(css_layout_viewport.get('clientHeight') or layout_viewport.get('clientHeight', 720))

		# For total page dimensions, content size is typically in device pixels, so convert to CSS pixels
		# by dividing by device pixel ratio
		raw_page_width = content_size.get('width', viewport_width * device_pixel_ratio)
		raw_page_height = content_size.get('height', viewport_height * device_pixel_ratio)
		page_width = int(raw_page_width / device_pixel_ratio)
		page_height = int(raw_page_height / device_pixel_ratio)

		# For scroll position, use CSS visual viewport if available, otherwise CSS layout viewport
		# These should already be in CSS pixels
		scroll_x = int(css_visual_viewport.get('pageX') or css_layout_viewport.get('pageX', 0))
		scroll_y = int(css_visual_viewport.get('pageY') or css_layout_viewport.get('pageY', 0))

		# Calculate scroll information - pixels that are above/below/left/right of current viewport
		pixels_above = scroll_y
		pixels_below = max(0, page_height - viewport_height - scroll_y)
		pixels_left = scroll_x
		pixels_right = max(0, page_width - viewport_width - scroll_x)

		page_info = PageInfo(
			viewport_width=viewport_width,
			viewport_height=viewport_height,
			page_width=page_width,
			page_height=page_height,
			scroll_x=scroll_x,
			scroll_y=scroll_y,
			pixels_above=pixels_above,
			pixels_below=pixels_below,
			pixels_left=pixels_left,
			pixels_right=pixels_right,
		)

		return page_info

	# ========== Public Helper Methods ==========

	async def get_element_by_index(self, index: int) -> EnhancedDOMTreeNode | None:
		"""Get DOM element by index from cached selector map.

		Builds DOM if not cached.

		Returns:
			EnhancedDOMTreeNode or None if index not found
		"""
		if not self.selector_map:
			# Build DOM if not cached
			await self._build_dom_tree_without_highlights()

		return self.selector_map.get(index) if self.selector_map else None

	def clear_cache(self) -> None:
		"""Clear cached DOM state to force rebuild on next access."""
		self.selector_map = None
		self.current_dom_state = None
		self.enhanced_dom_tree = None
		# Keep the DOM service instance to reuse its CDP client connection

	def is_file_input(self, element: EnhancedDOMTreeNode) -> bool:
		"""Check if element is a file input."""
		return element.node_name.upper() == 'INPUT' and element.attributes.get('type', '').lower() == 'file'

	@staticmethod
	def is_element_visible_according_to_all_parents(node: EnhancedDOMTreeNode, html_frames: list[EnhancedDOMTreeNode]) -> bool:
		"""Check if the element is visible according to all its parent HTML frames.

		Delegates to the DomService static method.
		"""
		return DomService.is_element_visible_according_to_all_parents(node, html_frames)

	async def __aexit__(self, exc_type, exc_value, traceback):
		"""Clean up DOM service on exit."""
		if self._dom_service:
			await self._dom_service.__aexit__(exc_type, exc_value, traceback)
			self._dom_service = None

	def __del__(self):
		"""Clean up DOM service on deletion."""
		super().__del__()
		# DOM service will clean up its own CDP client
		self._dom_service = None

```

---

## backend/browser-use/browser_use/browser/watchdogs/downloads_watchdog.py

```py
"""Downloads watchdog for monitoring and handling file downloads."""

import asyncio
import json
import os
import tempfile
from pathlib import Path
from typing import TYPE_CHECKING, Any, ClassVar
from urllib.parse import urlparse

import anyio
from bubus import BaseEvent
from cdp_use.cdp.browser import DownloadProgressEvent, DownloadWillBeginEvent
from cdp_use.cdp.network import ResponseReceivedEvent
from cdp_use.cdp.target import SessionID, TargetID
from pydantic import PrivateAttr

from browser_use.browser.events import (
	BrowserLaunchEvent,
	BrowserStateRequestEvent,
	BrowserStoppedEvent,
	FileDownloadedEvent,
	NavigationCompleteEvent,
	TabClosedEvent,
	TabCreatedEvent,
)
from browser_use.browser.watchdog_base import BaseWatchdog
from browser_use.utils import create_task_with_error_handling

if TYPE_CHECKING:
	pass


class DownloadsWatchdog(BaseWatchdog):
	"""Monitors downloads and handles file download events."""

	# Events this watchdog listens to (for documentation)
	LISTENS_TO: ClassVar[list[type[BaseEvent[Any]]]] = [
		BrowserLaunchEvent,
		BrowserStateRequestEvent,
		BrowserStoppedEvent,
		TabCreatedEvent,
		TabClosedEvent,
		NavigationCompleteEvent,
	]

	# Events this watchdog emits
	EMITS: ClassVar[list[type[BaseEvent[Any]]]] = [
		FileDownloadedEvent,
	]

	# Private state
	_sessions_with_listeners: set[str] = PrivateAttr(default_factory=set)  # Track sessions that already have download listeners
	_active_downloads: dict[str, Any] = PrivateAttr(default_factory=dict)
	_pdf_viewer_cache: dict[str, bool] = PrivateAttr(default_factory=dict)  # Cache PDF viewer status by target URL
	_download_cdp_session_setup: bool = PrivateAttr(default=False)  # Track if CDP session is set up
	_download_cdp_session: Any = PrivateAttr(default=None)  # Store CDP session reference
	_cdp_event_tasks: set[asyncio.Task] = PrivateAttr(default_factory=set)  # Track CDP event handler tasks
	_cdp_downloads_info: dict[str, dict[str, Any]] = PrivateAttr(default_factory=dict)  # Map guid -> info
	_use_js_fetch_for_local: bool = PrivateAttr(default=False)  # Guard JS fetch path for local regular downloads
	_session_pdf_urls: dict[str, str] = PrivateAttr(default_factory=dict)  # URL -> path for PDFs downloaded this session
	_network_monitored_targets: set[str] = PrivateAttr(default_factory=set)  # Track targets with network monitoring enabled
	_detected_downloads: set[str] = PrivateAttr(default_factory=set)  # Track detected download URLs to avoid duplicates
	_network_callback_registered: bool = PrivateAttr(default=False)  # Track if global network callback is registered

	async def on_BrowserLaunchEvent(self, event: BrowserLaunchEvent) -> None:
		self.logger.debug(f'[DownloadsWatchdog] Received BrowserLaunchEvent, EventBus ID: {id(self.event_bus)}')
		# Ensure downloads directory exists
		downloads_path = self.browser_session.browser_profile.downloads_path
		if downloads_path:
			expanded_path = Path(downloads_path).expanduser().resolve()
			expanded_path.mkdir(parents=True, exist_ok=True)
			self.logger.debug(f'[DownloadsWatchdog] Ensured downloads directory exists: {expanded_path}')

	async def on_TabCreatedEvent(self, event: TabCreatedEvent) -> None:
		"""Monitor new tabs for downloads."""
		# logger.info(f'[DownloadsWatchdog] TabCreatedEvent received for tab {event.target_id[-4:]}: {event.url}')

		# Assert downloads path is configured (should always be set by BrowserProfile default)
		assert self.browser_session.browser_profile.downloads_path is not None, 'Downloads path must be configured'

		if event.target_id:
			# logger.info(f'[DownloadsWatchdog] Found target for tab {event.target_id}, calling attach_to_target')
			await self.attach_to_target(event.target_id)
		else:
			self.logger.warning(f'[DownloadsWatchdog] No target found for tab {event.target_id}')

	async def on_TabClosedEvent(self, event: TabClosedEvent) -> None:
		"""Stop monitoring closed tabs."""
		pass  # No cleanup needed, browser context handles target lifecycle

	async def on_BrowserStateRequestEvent(self, event: BrowserStateRequestEvent) -> None:
		"""Handle browser state request events."""
		# Use public API - automatically validates and waits for recovery if needed
		self.logger.debug(f'[DownloadsWatchdog] on_BrowserStateRequestEvent started, event_id={event.event_id[-4:]}')
		try:
			cdp_session = await self.browser_session.get_or_create_cdp_session()
		except ValueError:
			self.logger.warning(f'[DownloadsWatchdog] No valid focus, skipping BrowserStateRequestEvent {event.event_id[-4:]}')
			return  # No valid focus, skip

		self.logger.debug(
			f'[DownloadsWatchdog] About to call get_current_page_url(), target_id={cdp_session.target_id[-4:] if cdp_session.target_id else "None"}'
		)
		url = await self.browser_session.get_current_page_url()
		self.logger.debug(f'[DownloadsWatchdog] Got URL: {url[:80] if url else "None"}')

		if not url:
			self.logger.warning(f'[DownloadsWatchdog] No URL found for BrowserStateRequestEvent {event.event_id[-4:]}')
			return

		target_id = cdp_session.target_id
		self.logger.debug(f'[DownloadsWatchdog] About to dispatch NavigationCompleteEvent for target {target_id[-4:]}')
		self.event_bus.dispatch(
			NavigationCompleteEvent(
				event_type='NavigationCompleteEvent',
				url=url,
				target_id=target_id,
				event_parent_id=event.event_id,
			)
		)
		self.logger.debug('[DownloadsWatchdog] Successfully completed BrowserStateRequestEvent')

	async def on_BrowserStoppedEvent(self, event: BrowserStoppedEvent) -> None:
		"""Clean up when browser stops."""
		# Cancel all CDP event handler tasks
		for task in list(self._cdp_event_tasks):
			if not task.done():
				task.cancel()
		# Wait for all tasks to complete cancellation
		if self._cdp_event_tasks:
			await asyncio.gather(*self._cdp_event_tasks, return_exceptions=True)
		self._cdp_event_tasks.clear()

		# Clean up CDP session
		# CDP sessions are now cached and managed by BrowserSession
		self._download_cdp_session = None
		self._download_cdp_session_setup = False

		# Clear other state
		self._sessions_with_listeners.clear()
		self._active_downloads.clear()
		self._pdf_viewer_cache.clear()
		self._session_pdf_urls.clear()
		self._network_monitored_targets.clear()
		self._detected_downloads.clear()
		self._network_callback_registered = False

	async def on_NavigationCompleteEvent(self, event: NavigationCompleteEvent) -> None:
		"""Check for PDFs after navigation completes."""
		self.logger.debug(f'[DownloadsWatchdog] NavigationCompleteEvent received for {event.url}, tab #{event.target_id[-4:]}')

		# Clear PDF cache for the navigated URL since content may have changed
		if event.url in self._pdf_viewer_cache:
			del self._pdf_viewer_cache[event.url]

		# Check if auto-download is enabled
		auto_download_enabled = self._is_auto_download_enabled()
		if not auto_download_enabled:
			return

		# Note: Using network-based PDF detection that doesn't require JavaScript

		target_id = event.target_id
		self.logger.debug(f'[DownloadsWatchdog] Got target_id={target_id} for tab #{event.target_id[-4:]}')

		is_pdf = await self.check_for_pdf_viewer(target_id)

		if is_pdf:
			self.logger.debug(f'[DownloadsWatchdog] ğŸ“„ PDF detected at {event.url}, triggering auto-download...')
			download_path = await self.trigger_pdf_download(target_id)
			if not download_path:
				self.logger.warning(f'[DownloadsWatchdog] âš ï¸ PDF download failed for {event.url}')

	def _is_auto_download_enabled(self) -> bool:
		"""Check if auto-download PDFs is enabled in browser profile."""
		return self.browser_session.browser_profile.auto_download_pdfs

	async def attach_to_target(self, target_id: TargetID) -> None:
		"""Set up download monitoring for a specific target."""

		# Define CDP event handlers outside of try to avoid indentation/scope issues
		def download_will_begin_handler(event: DownloadWillBeginEvent, session_id: SessionID | None) -> None:
			self.logger.debug(f'[DownloadsWatchdog] Download will begin: {event}')
			# Cache info for later completion event handling (esp. remote browsers)
			guid = event.get('guid', '')
			try:
				suggested_filename = event.get('suggestedFilename')
				assert suggested_filename, 'CDP DownloadWillBegin missing suggestedFilename'
				self._cdp_downloads_info[guid] = {
					'url': event.get('url', ''),
					'suggested_filename': suggested_filename,
					'handled': False,
				}
			except (AssertionError, KeyError):
				pass
			# Create and track the task
			task = create_task_with_error_handling(
				self._handle_cdp_download(event, target_id, session_id),
				name='handle_cdp_download',
				logger_instance=self.logger,
				suppress_exceptions=True,
			)
			self._cdp_event_tasks.add(task)
			# Remove from set when done
			task.add_done_callback(lambda t: self._cdp_event_tasks.discard(t))

		def download_progress_handler(event: DownloadProgressEvent, session_id: SessionID | None) -> None:
			# Check if download is complete
			if event.get('state') == 'completed':
				file_path = event.get('filePath')
				guid = event.get('guid', '')
				if self.browser_session.is_local:
					if file_path:
						self.logger.debug(f'[DownloadsWatchdog] Download completed: {file_path}')
						# Track the download
						self._track_download(file_path)
						# Mark as handled to prevent fallback duplicate dispatch
						try:
							if guid in self._cdp_downloads_info:
								self._cdp_downloads_info[guid]['handled'] = True
						except (KeyError, AttributeError):
							pass
					else:
						# No local file path provided, local polling in _handle_cdp_download will handle it
						self.logger.debug(
							'[DownloadsWatchdog] No filePath in progress event (local); polling will handle detection'
						)
				else:
					# Remote browser: do not touch local filesystem. Fallback to downloadPath+suggestedFilename
					info = self._cdp_downloads_info.get(guid, {})
					try:
						suggested_filename = info.get('suggested_filename') or (Path(file_path).name if file_path else 'download')
						downloads_path = str(self.browser_session.browser_profile.downloads_path or '')
						effective_path = file_path or str(Path(downloads_path) / suggested_filename)
						file_name = Path(effective_path).name
						file_ext = Path(file_name).suffix.lower().lstrip('.')
						self.event_bus.dispatch(
							FileDownloadedEvent(
								url=info.get('url', ''),
								path=str(effective_path),
								file_name=file_name,
								file_size=0,
								file_type=file_ext if file_ext else None,
							)
						)
						self.logger.debug(f'[DownloadsWatchdog] âœ… (remote) Download completed: {effective_path}')
					finally:
						if guid in self._cdp_downloads_info:
							del self._cdp_downloads_info[guid]

		try:
			downloads_path_raw = self.browser_session.browser_profile.downloads_path
			if not downloads_path_raw:
				# logger.info(f'[DownloadsWatchdog] No downloads path configured, skipping target: {target_id}')
				return  # No downloads path configured

			# Check if we already have a download listener on this session
			# to prevent duplicate listeners from being added
			# Note: Since download listeners are set up once per browser session, not per target,
			# we just track if we've set up the browser-level listener
			if self._download_cdp_session_setup:
				self.logger.debug('[DownloadsWatchdog] Download listener already set up for browser session')
				return

			# logger.debug(f'[DownloadsWatchdog] Setting up CDP download listener for target: {target_id}')

			# Use CDP session for download events but store reference in watchdog
			if not self._download_cdp_session_setup:
				# Set up CDP session for downloads (only once per browser session)
				cdp_client = self.browser_session.cdp_client

				# Set download behavior to allow downloads and enable events
				downloads_path = self.browser_session.browser_profile.downloads_path
				if not downloads_path:
					self.logger.warning('[DownloadsWatchdog] No downloads path configured, skipping CDP download setup')
					return
				# Ensure path is properly expanded (~ -> absolute path)
				expanded_downloads_path = Path(downloads_path).expanduser().resolve()
				await cdp_client.send.Browser.setDownloadBehavior(
					params={
						'behavior': 'allow',
						'downloadPath': str(expanded_downloads_path),  # Use expanded absolute path
						'eventsEnabled': True,
					}
				)

				# Register the handlers with CDP
				cdp_client.register.Browser.downloadWillBegin(download_will_begin_handler)  # type: ignore[arg-type]
				cdp_client.register.Browser.downloadProgress(download_progress_handler)  # type: ignore[arg-type]

				self._download_cdp_session_setup = True
				self.logger.debug('[DownloadsWatchdog] Set up CDP download listeners')

			# No need to track individual targets since download listener is browser-level
			# logger.debug(f'[DownloadsWatchdog] Successfully set up CDP download listener for target: {target_id}')

		except Exception as e:
			self.logger.warning(f'[DownloadsWatchdog] Failed to set up CDP download listener for target {target_id}: {e}')

		# Set up network monitoring for this target (catches ALL download variants)
		await self._setup_network_monitoring(target_id)

	async def _setup_network_monitoring(self, target_id: TargetID) -> None:
		"""Set up network monitoring to detect PDFs and downloads from ALL sources.

		This catches:
		- Direct PDF navigation
		- PDFs in iframes
		- PDFs with embed/object tags
		- JavaScript-triggered downloads
		- Any Content-Disposition: attachment headers
		"""
		# Skip if already monitoring this target
		if target_id in self._network_monitored_targets:
			self.logger.debug(f'[DownloadsWatchdog] Network monitoring already enabled for target {target_id[-4:]}')
			return

		# Check if auto-download is enabled
		if not self._is_auto_download_enabled():
			self.logger.debug('[DownloadsWatchdog] Auto-download disabled, skipping network monitoring')
			return

		try:
			cdp_client = self.browser_session.cdp_client

			# Register the global callback once
			if not self._network_callback_registered:

				def on_response_received(event: ResponseReceivedEvent, session_id: str | None) -> None:
					"""Handle Network.responseReceived event to detect downloadable content.

					This callback is registered globally and uses session_id to determine the correct target.
					"""
					try:
						# Check if session_manager exists (may be None during browser shutdown)
						if not self.browser_session.session_manager:
							self.logger.warning('[DownloadsWatchdog] Session manager not found, skipping network monitoring')
							return

						# Look up target_id from session_id
						event_target_id = self.browser_session.session_manager.get_target_id_from_session_id(session_id)
						if not event_target_id:
							# Session not in pool - might be a stale session or not yet tracked
							return

						# Only process events for targets we're monitoring
						if event_target_id not in self._network_monitored_targets:
							return

						response = event.get('response', {})
						url = response.get('url', '')
						content_type = response.get('mimeType', '').lower()
						headers = response.get('headers', {})

						# Skip non-HTTP URLs (data:, about:, chrome-extension:, etc.)
						if not url.startswith('http'):
							return

						# Check if it's a PDF
						is_pdf = 'application/pdf' in content_type

						# Check if it's marked as download via Content-Disposition header
						content_disposition = headers.get('content-disposition', '').lower()
						is_download_attachment = 'attachment' in content_disposition

						# Filter out image/video/audio files even if marked as attachment
						# These are likely resources, not intentional downloads
						unwanted_content_types = [
							'image/',
							'video/',
							'audio/',
							'text/css',
							'text/javascript',
							'application/javascript',
							'application/x-javascript',
							'text/html',
							'application/json',
							'font/',
							'application/font',
							'application/x-font',
						]
						is_unwanted_type = any(content_type.startswith(prefix) for prefix in unwanted_content_types)
						if is_unwanted_type:
							return

						# Check URL extension to filter out obvious images/resources
						url_lower = url.lower().split('?')[0]  # Remove query params
						unwanted_extensions = [
							'.jpg',
							'.jpeg',
							'.png',
							'.gif',
							'.webp',
							'.svg',
							'.ico',
							'.css',
							'.js',
							'.woff',
							'.woff2',
							'.ttf',
							'.eot',
							'.mp4',
							'.webm',
							'.mp3',
							'.wav',
							'.ogg',
						]
						if any(url_lower.endswith(ext) for ext in unwanted_extensions):
							return

						# Only process if it's a PDF or download
						if not (is_pdf or is_download_attachment):
							return

						# Check if we've already processed this URL in this session
						if url in self._detected_downloads:
							self.logger.debug(f'[DownloadsWatchdog] Already detected download: {url[:80]}...')
							return

						# Mark as detected to avoid duplicates
						self._detected_downloads.add(url)

						# Extract filename from Content-Disposition if available
						suggested_filename = None
						if 'filename=' in content_disposition:
							# Parse filename from Content-Disposition header
							import re

							filename_match = re.search(r'filename[^;=\n]*=(([\'"]).*?\2|[^;\n]*)', content_disposition)
							if filename_match:
								suggested_filename = filename_match.group(1).strip('\'"')

						self.logger.info(f'[DownloadsWatchdog] ğŸ” Detected downloadable content via network: {url[:80]}...')
						self.logger.debug(
							f'[DownloadsWatchdog]   Content-Type: {content_type}, Is PDF: {is_pdf}, Is Attachment: {is_download_attachment}'
						)

						# Trigger download asynchronously in background (don't block event handler)
						async def download_in_background():
							try:
								download_path = await self.download_file_from_url(
									url=url,
									target_id=event_target_id,  # Use target_id from session_id lookup
									content_type=content_type,
									suggested_filename=suggested_filename,
								)

								if download_path:
									self.logger.info(f'[DownloadsWatchdog] âœ… Successfully downloaded: {download_path}')
								else:
									self.logger.warning(f'[DownloadsWatchdog] âš ï¸  Failed to download: {url[:80]}...')
							except Exception as e:
								self.logger.error(f'[DownloadsWatchdog] Error downloading in background: {type(e).__name__}: {e}')

						# Create background task
						task = create_task_with_error_handling(
							download_in_background(),
							name='download_in_background',
							logger_instance=self.logger,
							suppress_exceptions=True,
						)
						self._cdp_event_tasks.add(task)
						task.add_done_callback(lambda t: self._cdp_event_tasks.discard(t))

					except Exception as e:
						self.logger.error(f'[DownloadsWatchdog] Error in network response handler: {type(e).__name__}: {e}')

				# Register the callback globally (once)
				cdp_client.register.Network.responseReceived(on_response_received)
				self._network_callback_registered = True
				self.logger.debug('[DownloadsWatchdog] âœ… Registered global network response callback')

			# Get or create CDP session for this target
			cdp_session = await self.browser_session.get_or_create_cdp_session(target_id, focus=False)

			# Enable Network domain to monitor HTTP responses (per-target/per-session)
			await cdp_client.send.Network.enable(session_id=cdp_session.session_id)
			self.logger.debug(f'[DownloadsWatchdog] Enabled Network domain for target {target_id[-4:]}')

			# Mark this target as monitored
			self._network_monitored_targets.add(target_id)
			self.logger.debug(f'[DownloadsWatchdog] âœ… Network monitoring enabled for target {target_id[-4:]}')

		except Exception as e:
			self.logger.warning(f'[DownloadsWatchdog] Failed to set up network monitoring for target {target_id}: {e}')

	async def download_file_from_url(
		self, url: str, target_id: TargetID, content_type: str | None = None, suggested_filename: str | None = None
	) -> str | None:
		"""Generic method to download any file from a URL.

		Args:
			url: The URL to download
			target_id: The target ID for CDP session
			content_type: Optional content type (e.g., 'application/pdf')
			suggested_filename: Optional filename from Content-Disposition header

		Returns:
			Path to downloaded file, or None if download failed
		"""
		if not self.browser_session.browser_profile.downloads_path:
			self.logger.warning('[DownloadsWatchdog] No downloads path configured')
			return None

		# Check if already downloaded in this session
		if url in self._session_pdf_urls:
			existing_path = self._session_pdf_urls[url]
			self.logger.debug(f'[DownloadsWatchdog] File already downloaded in session: {existing_path}')
			return existing_path

		try:
			# Get or create CDP session for this target
			temp_session = await self.browser_session.get_or_create_cdp_session(target_id, focus=False)

			# Determine filename
			if suggested_filename:
				filename = suggested_filename
			else:
				# Extract from URL
				filename = os.path.basename(url.split('?')[0])  # Remove query params
				if not filename or '.' not in filename:
					# Fallback: use content type to determine extension
					if content_type and 'pdf' in content_type:
						filename = 'document.pdf'
					else:
						filename = 'download'

			# Ensure downloads directory exists
			downloads_dir = str(self.browser_session.browser_profile.downloads_path)
			os.makedirs(downloads_dir, exist_ok=True)

			# Generate unique filename if file exists
			final_filename = filename
			existing_files = os.listdir(downloads_dir)
			if filename in existing_files:
				base, ext = os.path.splitext(filename)
				counter = 1
				while f'{base} ({counter}){ext}' in existing_files:
					counter += 1
				final_filename = f'{base} ({counter}){ext}'
				self.logger.debug(f'[DownloadsWatchdog] File exists, using: {final_filename}')

			self.logger.debug(f'[DownloadsWatchdog] Downloading from: {url[:100]}...')

			# Download using JavaScript fetch to leverage browser cache
			escaped_url = json.dumps(url)

			result = await asyncio.wait_for(
				temp_session.cdp_client.send.Runtime.evaluate(
					params={
						'expression': f"""
				(async () => {{
					try {{
						const response = await fetch({escaped_url}, {{
							cache: 'force-cache'
						}});
						if (!response.ok) {{
							throw new Error(`HTTP error! status: ${{response.status}}`);
						}}
						const blob = await response.blob();
						const arrayBuffer = await blob.arrayBuffer();
						const uint8Array = new Uint8Array(arrayBuffer);

						return {{
							data: Array.from(uint8Array),
							responseSize: uint8Array.length
						}};
					}} catch (error) {{
						throw new Error(`Fetch failed: ${{error.message}}`);
					}}
				}})()
				""",
						'awaitPromise': True,
						'returnByValue': True,
					},
					session_id=temp_session.session_id,
				),
				timeout=15.0,  # 15 second timeout
			)

			download_result = result.get('result', {}).get('value', {})

			if download_result and download_result.get('data') and len(download_result['data']) > 0:
				download_path = os.path.join(downloads_dir, final_filename)

				# Save the file asynchronously
				async with await anyio.open_file(download_path, 'wb') as f:
					await f.write(bytes(download_result['data']))

				# Verify file was written successfully
				if os.path.exists(download_path):
					actual_size = os.path.getsize(download_path)
					self.logger.debug(f'[DownloadsWatchdog] File written: {download_path} ({actual_size} bytes)')

					# Determine file type
					file_ext = Path(final_filename).suffix.lower().lstrip('.')
					mime_type = content_type or f'application/{file_ext}'

					# Store URL->path mapping for this session
					self._session_pdf_urls[url] = download_path

					# Emit file downloaded event
					self.logger.debug(f'[DownloadsWatchdog] Dispatching FileDownloadedEvent for {final_filename}')
					self.event_bus.dispatch(
						FileDownloadedEvent(
							url=url,
							path=download_path,
							file_name=final_filename,
							file_size=actual_size,
							file_type=file_ext if file_ext else None,
							mime_type=mime_type,
							auto_download=True,
						)
					)

					return download_path
				else:
					self.logger.error(f'[DownloadsWatchdog] Failed to write file: {download_path}')
					return None
			else:
				self.logger.warning(f'[DownloadsWatchdog] No data received when downloading from {url}')
				return None

		except TimeoutError:
			self.logger.warning(f'[DownloadsWatchdog] Download timed out: {url[:80]}...')
			return None
		except Exception as e:
			self.logger.warning(f'[DownloadsWatchdog] Download failed: {type(e).__name__}: {e}')
			return None

	def _track_download(self, file_path: str) -> None:
		"""Track a completed download and dispatch the appropriate event.

		Args:
			file_path: The path to the downloaded file
		"""
		try:
			# Get file info
			path = Path(file_path)
			if path.exists():
				file_size = path.stat().st_size
				self.logger.debug(f'[DownloadsWatchdog] Tracked download: {path.name} ({file_size} bytes)')

				# Dispatch download event
				from browser_use.browser.events import FileDownloadedEvent

				self.event_bus.dispatch(
					FileDownloadedEvent(
						url=str(path),  # Use the file path as URL for local files
						path=str(path),
						file_name=path.name,
						file_size=file_size,
					)
				)
			else:
				self.logger.warning(f'[DownloadsWatchdog] Downloaded file not found: {file_path}')
		except Exception as e:
			self.logger.error(f'[DownloadsWatchdog] Error tracking download: {e}')

	async def _handle_cdp_download(
		self, event: DownloadWillBeginEvent, target_id: TargetID, session_id: SessionID | None
	) -> None:
		"""Handle a CDP Page.downloadWillBegin event."""
		downloads_dir = (
			Path(
				self.browser_session.browser_profile.downloads_path
				or f'{tempfile.gettempdir()}/browser_use_downloads.{str(self.browser_session.id)[-4:]}'
			)
			.expanduser()
			.resolve()
		)  # Ensure path is properly expanded

		# Initialize variables that may be used outside try blocks
		unique_filename = None
		file_size = 0
		expected_path = None
		download_result = None
		download_url = event.get('url', '')
		suggested_filename = event.get('suggestedFilename', 'download')
		guid = event.get('guid', '')

		try:
			self.logger.debug(f'[DownloadsWatchdog] â¬‡ï¸ File download starting: {suggested_filename} from {download_url[:100]}...')
			self.logger.debug(f'[DownloadsWatchdog] Full CDP event: {event}')

			# Since Browser.setDownloadBehavior is already configured, the browser will download the file
			# We just need to wait for it to appear in the downloads directory
			expected_path = downloads_dir / suggested_filename

			# Debug: List current directory contents
			self.logger.debug(f'[DownloadsWatchdog] Downloads directory: {downloads_dir}')
			if downloads_dir.exists():
				files_before = list(downloads_dir.iterdir())
				self.logger.debug(f'[DownloadsWatchdog] Files before download: {[f.name for f in files_before]}')

			# Try manual JavaScript fetch as a fallback for local browsers (disabled for regular local downloads)
			if self.browser_session.is_local and self._use_js_fetch_for_local:
				self.logger.debug(f'[DownloadsWatchdog] Attempting JS fetch fallback for {download_url}')

				unique_filename = None
				file_size = None
				download_result = None
				try:
					# Escape the URL for JavaScript
					import json

					escaped_url = json.dumps(download_url)

					# Get the proper session for the frame that initiated the download
					cdp_session = await self.browser_session.cdp_client_for_frame(event.get('frameId'))
					assert cdp_session

					result = await cdp_session.cdp_client.send.Runtime.evaluate(
						params={
							'expression': f"""
						(async () => {{
							try {{
								const response = await fetch({escaped_url});
								if (!response.ok) {{
									throw new Error(`HTTP error! status: ${{response.status}}`);
								}}
								const blob = await response.blob();
								const arrayBuffer = await blob.arrayBuffer();
								const uint8Array = new Uint8Array(arrayBuffer);
								return {{
									data: Array.from(uint8Array),
									size: uint8Array.length,
									contentType: response.headers.get('content-type') || 'application/octet-stream'
								}};
							}} catch (error) {{
								throw new Error(`Fetch failed: ${{error.message}}`);
							}}
						}})()
						""",
							'awaitPromise': True,
							'returnByValue': True,
						},
						session_id=cdp_session.session_id,
					)
					download_result = result.get('result', {}).get('value')

					if download_result and download_result.get('data'):
						# Save the file
						file_data = bytes(download_result['data'])
						file_size = len(file_data)

						# Ensure unique filename
						unique_filename = await self._get_unique_filename(str(downloads_dir), suggested_filename)
						final_path = downloads_dir / unique_filename

						# Write the file
						import anyio

						async with await anyio.open_file(final_path, 'wb') as f:
							await f.write(file_data)

						self.logger.debug(f'[DownloadsWatchdog] âœ… Downloaded and saved file: {final_path} ({file_size} bytes)')
						expected_path = final_path
						# Emit download event immediately
						file_ext = expected_path.suffix.lower().lstrip('.')
						file_type = file_ext if file_ext else None
						self.event_bus.dispatch(
							FileDownloadedEvent(
								url=download_url,
								path=str(expected_path),
								file_name=unique_filename or expected_path.name,
								file_size=file_size or 0,
								file_type=file_type,
								mime_type=(download_result.get('contentType') if download_result else None),
								from_cache=False,
								auto_download=False,
							)
						)
						# Mark as handled to prevent duplicate dispatch from progress/polling paths
						try:
							if guid in self._cdp_downloads_info:
								self._cdp_downloads_info[guid]['handled'] = True
						except (KeyError, AttributeError):
							pass
						self.logger.debug(
							f'[DownloadsWatchdog] âœ… File download completed via CDP: {suggested_filename} ({file_size} bytes) saved to {expected_path}'
						)
						return
					else:
						self.logger.error('[DownloadsWatchdog] âŒ No data received from fetch')

				except Exception as fetch_error:
					self.logger.error(f'[DownloadsWatchdog] âŒ Failed to download file via fetch: {fetch_error}')

			# For remote browsers, don't poll local filesystem; downloadProgress handler will emit the event
			if not self.browser_session.is_local:
				return
		except Exception as e:
			self.logger.error(f'[DownloadsWatchdog] âŒ Error handling CDP download: {type(e).__name__} {e}')

		# If we reach here, the fetch method failed, so wait for native download
		# Poll the downloads directory for new files
		self.logger.debug(f'[DownloadsWatchdog] Checking if browser auto-download saved the file for us: {suggested_filename}')

		# Get initial list of files in downloads directory
		initial_files = set()
		if Path(downloads_dir).exists():
			for f in Path(downloads_dir).iterdir():
				if f.is_file() and not f.name.startswith('.'):
					initial_files.add(f.name)

		# Poll for new files
		max_wait = 20  # seconds
		start_time = asyncio.get_event_loop().time()

		while asyncio.get_event_loop().time() - start_time < max_wait:
			await asyncio.sleep(5.0)  # Check every 5 seconds

			if Path(downloads_dir).exists():
				for file_path in Path(downloads_dir).iterdir():
					# Skip hidden files and files that were already there
					if file_path.is_file() and not file_path.name.startswith('.') and file_path.name not in initial_files:
						# Check if file has content (> 4 bytes)
						try:
							file_size = file_path.stat().st_size
							if file_size > 4:
								# Found a new download!
								self.logger.debug(
									f'[DownloadsWatchdog] âœ… Found downloaded file: {file_path} ({file_size} bytes)'
								)

								# Determine file type from extension
								file_ext = file_path.suffix.lower().lstrip('.')
								file_type = file_ext if file_ext else None

								# Dispatch download event
								# Skip if already handled by progress/JS fetch
								info = self._cdp_downloads_info.get(guid, {})
								if info.get('handled'):
									return
								self.event_bus.dispatch(
									FileDownloadedEvent(
										url=download_url,
										path=str(file_path),
										file_name=file_path.name,
										file_size=file_size,
										file_type=file_type,
									)
								)
								# Mark as handled after dispatch
								try:
									if guid in self._cdp_downloads_info:
										self._cdp_downloads_info[guid]['handled'] = True
								except (KeyError, AttributeError):
									pass
								return
						except Exception as e:
							self.logger.debug(f'[DownloadsWatchdog] Error checking file {file_path}: {e}')

		self.logger.warning(f'[DownloadsWatchdog] Download did not complete within {max_wait} seconds')

	async def _handle_download(self, download: Any) -> None:
		"""Handle a download event."""
		download_id = f'{id(download)}'
		self._active_downloads[download_id] = download
		self.logger.debug(f'[DownloadsWatchdog] â¬‡ï¸ Handling download: {download.suggested_filename} from {download.url[:100]}...')

		# Debug: Check if download is already being handled elsewhere
		failure = (
			await download.failure()
		)  # TODO: it always fails for some reason, figure out why connect_over_cdp makes accept_downloads not work
		self.logger.warning(f'[DownloadsWatchdog] âŒ Download state - canceled: {failure}, url: {download.url}')
		# logger.info(f'[DownloadsWatchdog] Active downloads count: {len(self._active_downloads)}')

		try:
			current_step = 'getting_download_info'
			# Get download info immediately
			url = download.url
			suggested_filename = download.suggested_filename

			current_step = 'determining_download_directory'
			# Determine download directory from browser profile
			downloads_dir = self.browser_session.browser_profile.downloads_path
			if not downloads_dir:
				downloads_dir = str(Path.home() / 'Downloads')
			else:
				downloads_dir = str(downloads_dir)  # Ensure it's a string

			# Check if Playwright already auto-downloaded the file (due to CDP setup)
			original_path = Path(downloads_dir) / suggested_filename
			if original_path.exists() and original_path.stat().st_size > 0:
				self.logger.debug(
					f'[DownloadsWatchdog] File already downloaded by Playwright: {original_path} ({original_path.stat().st_size} bytes)'
				)

				# Use the existing file instead of creating a duplicate
				download_path = original_path
				file_size = original_path.stat().st_size
				unique_filename = suggested_filename
			else:
				current_step = 'generating_unique_filename'
				# Ensure unique filename
				unique_filename = await self._get_unique_filename(downloads_dir, suggested_filename)
				download_path = Path(downloads_dir) / unique_filename

				self.logger.debug(f'[DownloadsWatchdog] Download started: {unique_filename} from {url[:100]}...')

				current_step = 'calling_save_as'
				# Save the download using Playwright's save_as method
				self.logger.debug(f'[DownloadsWatchdog] Saving download to: {download_path}')
				self.logger.debug(f'[DownloadsWatchdog] Download path exists: {download_path.parent.exists()}')
				self.logger.debug(f'[DownloadsWatchdog] Download path writable: {os.access(download_path.parent, os.W_OK)}')

				try:
					self.logger.debug('[DownloadsWatchdog] About to call download.save_as()...')
					await download.save_as(str(download_path))
					self.logger.debug(f'[DownloadsWatchdog] Successfully saved download to: {download_path}')
					current_step = 'save_as_completed'
				except Exception as save_error:
					self.logger.error(f'[DownloadsWatchdog] save_as() failed with error: {save_error}')
					raise save_error

				# Get file info
				file_size = download_path.stat().st_size if download_path.exists() else 0

			# Determine file type from extension
			file_ext = download_path.suffix.lower().lstrip('.')
			file_type = file_ext if file_ext else None

			# Try to get MIME type from response headers if available
			mime_type = None
			# Note: Playwright doesn't expose response headers directly from Download object

			# Check if this was a PDF auto-download
			auto_download = False
			if file_type == 'pdf':
				auto_download = self._is_auto_download_enabled()

			# Emit download event
			self.event_bus.dispatch(
				FileDownloadedEvent(
					url=url,
					path=str(download_path),
					file_name=suggested_filename,
					file_size=file_size,
					file_type=file_type,
					mime_type=mime_type,
					from_cache=False,
					auto_download=auto_download,
				)
			)

			self.logger.debug(
				f'[DownloadsWatchdog] âœ… Download completed: {suggested_filename} ({file_size} bytes) saved to {download_path}'
			)

			# File is now tracked on filesystem, no need to track in memory

		except Exception as e:
			self.logger.error(
				f'[DownloadsWatchdog] Error handling download at step "{locals().get("current_step", "unknown")}", error: {e}'
			)
			self.logger.error(
				f'[DownloadsWatchdog] Download state - URL: {download.url}, filename: {download.suggested_filename}'
			)
		finally:
			# Clean up tracking
			if download_id in self._active_downloads:
				del self._active_downloads[download_id]

	async def check_for_pdf_viewer(self, target_id: TargetID) -> bool:
		"""Check if the current target is a PDF using network-based detection.

		This method avoids JavaScript execution that can crash WebSocket connections.
		Returns True if a PDF is detected and should be downloaded.
		"""
		self.logger.debug(f'[DownloadsWatchdog] Checking if target {target_id} is PDF viewer...')

		# Use safe API - focus=False to avoid changing focus during PDF check
		try:
			session = await self.browser_session.get_or_create_cdp_session(target_id, focus=False)
		except ValueError as e:
			self.logger.warning(f'[DownloadsWatchdog] No session found for {target_id}: {e}')
			return False

		# Get URL from target
		target = self.browser_session.session_manager.get_target(target_id)
		if not target:
			self.logger.warning(f'[DownloadsWatchdog] No target found for {target_id}')
			return False
		page_url = target.url

		# Check cache first
		if page_url in self._pdf_viewer_cache:
			cached_result = self._pdf_viewer_cache[page_url]
			self.logger.debug(f'[DownloadsWatchdog] Using cached PDF check result for {page_url}: {cached_result}')
			return cached_result

		try:
			# Method 1: Check URL patterns (fastest, most reliable)
			url_is_pdf = self._check_url_for_pdf(page_url)
			if url_is_pdf:
				self.logger.debug(f'[DownloadsWatchdog] PDF detected via URL pattern: {page_url}')
				self._pdf_viewer_cache[page_url] = True
				return True
			chrome_pdf_viewer = self._is_chrome_pdf_viewer_url(page_url)
			if chrome_pdf_viewer:
				self.logger.debug(f'[DownloadsWatchdog] Chrome PDF viewer detected: {page_url}')
				self._pdf_viewer_cache[page_url] = True
				return True

			# Not a PDF
			self._pdf_viewer_cache[page_url] = False
			return False

		except Exception as e:
			self.logger.warning(f'[DownloadsWatchdog] âŒ Error checking for PDF viewer: {e}')
			self._pdf_viewer_cache[page_url] = False
			return False

	def _check_url_for_pdf(self, url: str) -> bool:
		"""Check if URL indicates a PDF file."""
		if not url:
			return False

		url_lower = url.lower()

		# Direct PDF file extensions
		if url_lower.endswith('.pdf'):
			return True

		# PDF in path
		if '.pdf' in url_lower:
			return True

		# PDF MIME type in URL parameters
		if any(
			param in url_lower
			for param in [
				'content-type=application/pdf',
				'content-type=application%2fpdf',
				'mimetype=application/pdf',
				'type=application/pdf',
			]
		):
			return True

		return False

	def _is_chrome_pdf_viewer_url(self, url: str) -> bool:
		"""Check if this is Chrome's internal PDF viewer URL."""
		if not url:
			return False

		url_lower = url.lower()

		# Chrome PDF viewer uses chrome-extension:// URLs
		if 'chrome-extension://' in url_lower and 'pdf' in url_lower:
			return True

		# Chrome PDF viewer internal URLs
		if url_lower.startswith('chrome://') and 'pdf' in url_lower:
			return True

		return False

	async def _check_network_headers_for_pdf(self, target_id: TargetID) -> bool:
		"""Infer PDF via navigation history/URL; headers are not available post-navigation in this context."""
		try:
			import asyncio

			# Get CDP session
			temp_session = await self.browser_session.get_or_create_cdp_session(target_id, focus=False)

			# Get navigation history to find the main resource
			history = await asyncio.wait_for(
				temp_session.cdp_client.send.Page.getNavigationHistory(session_id=temp_session.session_id), timeout=3.0
			)

			current_entry = history.get('entries', [])
			if current_entry:
				current_index = history.get('currentIndex', 0)
				if 0 <= current_index < len(current_entry):
					current_url = current_entry[current_index].get('url', '')

					# Check if the URL itself suggests PDF
					if self._check_url_for_pdf(current_url):
						return True

			# Note: CDP doesn't easily expose response headers for completed navigations
			# For more complex cases, we'd need to set up Network.responseReceived listeners
			# before navigation, but that's overkill for most PDF detection cases

			return False

		except Exception as e:
			self.logger.debug(f'[DownloadsWatchdog] Network headers check failed (non-critical): {e}')
			return False

	async def trigger_pdf_download(self, target_id: TargetID) -> str | None:
		"""Trigger download of a PDF from Chrome's PDF viewer.

		Returns the download path if successful, None otherwise.
		"""
		self.logger.debug(f'[DownloadsWatchdog] trigger_pdf_download called for target_id={target_id}')

		if not self.browser_session.browser_profile.downloads_path:
			self.logger.warning('[DownloadsWatchdog] âŒ No downloads path configured, cannot save PDF download')
			return None

		downloads_path = self.browser_session.browser_profile.downloads_path
		self.logger.debug(f'[DownloadsWatchdog] Downloads path: {downloads_path}')

		try:
			# Create a temporary CDP session for this target without switching focus
			import asyncio

			self.logger.debug(f'[DownloadsWatchdog] Creating CDP session for PDF download from target {target_id}')
			temp_session = await self.browser_session.get_or_create_cdp_session(target_id, focus=False)

			# Try to get the PDF URL with timeout
			result = await asyncio.wait_for(
				temp_session.cdp_client.send.Runtime.evaluate(
					params={
						'expression': """
				(() => {
					// For Chrome's PDF viewer, the actual URL is in window.location.href
					// The embed element's src is often "about:blank"
					const embedElement = document.querySelector('embed[type="application/x-google-chrome-pdf"]') ||
										document.querySelector('embed[type="application/pdf"]');
					if (embedElement) {
						// Chrome PDF viewer detected - use the page URL
						return { url: window.location.href };
					}
					// Fallback to window.location.href anyway
					return { url: window.location.href };
				})()
				""",
						'returnByValue': True,
					},
					session_id=temp_session.session_id,
				),
				timeout=5.0,  # 5 second timeout to prevent hanging
			)
			pdf_info = result.get('result', {}).get('value', {})

			pdf_url = pdf_info.get('url', '')
			if not pdf_url:
				self.logger.warning(f'[DownloadsWatchdog] âŒ Could not determine PDF URL for download {pdf_info}')
				return None

			# Generate filename from URL
			pdf_filename = os.path.basename(pdf_url.split('?')[0])  # Remove query params
			if not pdf_filename or not pdf_filename.endswith('.pdf'):
				parsed = urlparse(pdf_url)
				pdf_filename = os.path.basename(parsed.path) or 'document.pdf'
				if not pdf_filename.endswith('.pdf'):
					pdf_filename += '.pdf'

			self.logger.debug(f'[DownloadsWatchdog] Generated filename: {pdf_filename}')

			# Check if already downloaded in this session
			self.logger.debug(f'[DownloadsWatchdog] PDF_URL: {pdf_url}, session_pdf_urls: {self._session_pdf_urls}')
			if pdf_url in self._session_pdf_urls:
				existing_path = self._session_pdf_urls[pdf_url]
				self.logger.debug(f'[DownloadsWatchdog] PDF already downloaded in session: {existing_path}')
				return existing_path

			# Generate unique filename if file exists from previous run
			downloads_dir = str(self.browser_session.browser_profile.downloads_path)
			os.makedirs(downloads_dir, exist_ok=True)
			final_filename = pdf_filename
			existing_files = os.listdir(downloads_dir)
			if pdf_filename in existing_files:
				# Generate unique name with (1), (2), etc.
				base, ext = os.path.splitext(pdf_filename)
				counter = 1
				while f'{base} ({counter}){ext}' in existing_files:
					counter += 1
				final_filename = f'{base} ({counter}){ext}'
				self.logger.debug(f'[DownloadsWatchdog] File exists, using: {final_filename}')

			self.logger.debug(f'[DownloadsWatchdog] Starting PDF download from: {pdf_url[:100]}...')

			# Download using JavaScript fetch to leverage browser cache
			try:
				# Properly escape the URL to prevent JavaScript injection
				escaped_pdf_url = json.dumps(pdf_url)

				result = await asyncio.wait_for(
					temp_session.cdp_client.send.Runtime.evaluate(
						params={
							'expression': f"""
					(async () => {{
						try {{
							// Use fetch with cache: 'force-cache' to prioritize cached version
							const response = await fetch({escaped_pdf_url}, {{
								cache: 'force-cache'
							}});
							if (!response.ok) {{
								throw new Error(`HTTP error! status: ${{response.status}}`);
							}}
							const blob = await response.blob();
							const arrayBuffer = await blob.arrayBuffer();
							const uint8Array = new Uint8Array(arrayBuffer);
							
							// Check if served from cache
							const fromCache = response.headers.has('age') || 
											 !response.headers.has('date');
											 
							return {{ 
								data: Array.from(uint8Array),
								fromCache: fromCache,
								responseSize: uint8Array.length,
								transferSize: response.headers.get('content-length') || 'unknown'
							}};
						}} catch (error) {{
							throw new Error(`Fetch failed: ${{error.message}}`);
						}}
					}})()
					""",
							'awaitPromise': True,
							'returnByValue': True,
						},
						session_id=temp_session.session_id,
					),
					timeout=10.0,  # 10 second timeout for download operation
				)
				download_result = result.get('result', {}).get('value', {})

				if download_result and download_result.get('data') and len(download_result['data']) > 0:
					# Ensure downloads directory exists
					downloads_dir = str(self.browser_session.browser_profile.downloads_path)
					os.makedirs(downloads_dir, exist_ok=True)
					download_path = os.path.join(downloads_dir, final_filename)

					# Save the PDF asynchronously
					async with await anyio.open_file(download_path, 'wb') as f:
						await f.write(bytes(download_result['data']))

					# Verify file was written successfully
					if os.path.exists(download_path):
						actual_size = os.path.getsize(download_path)
						self.logger.debug(
							f'[DownloadsWatchdog] PDF file written successfully: {download_path} ({actual_size} bytes)'
						)
					else:
						self.logger.error(f'[DownloadsWatchdog] âŒ Failed to write PDF file to: {download_path}')
						return None

					# Log cache information
					cache_status = 'from cache' if download_result.get('fromCache') else 'from network'
					response_size = download_result.get('responseSize', 0)
					self.logger.debug(
						f'[DownloadsWatchdog] âœ… Auto-downloaded PDF ({cache_status}, {response_size:,} bytes): {download_path}'
					)

					# Store URL->path mapping for this session
					self._session_pdf_urls[pdf_url] = download_path

					# Emit file downloaded event
					self.logger.debug(f'[DownloadsWatchdog] Dispatching FileDownloadedEvent for {final_filename}')
					self.event_bus.dispatch(
						FileDownloadedEvent(
							url=pdf_url,
							path=download_path,
							file_name=final_filename,
							file_size=response_size,
							file_type='pdf',
							mime_type='application/pdf',
							from_cache=download_result.get('fromCache', False),
							auto_download=True,
						)
					)

					# No need to detach - session is cached
					return download_path
				else:
					self.logger.warning(f'[DownloadsWatchdog] No data received when downloading PDF from {pdf_url}')
					return None

			except Exception as e:
				self.logger.warning(f'[DownloadsWatchdog] Failed to auto-download PDF from {pdf_url}: {type(e).__name__}: {e}')
				return None

		except TimeoutError:
			self.logger.debug('[DownloadsWatchdog] PDF download operation timed out')
			return None
		except Exception as e:
			self.logger.error(f'[DownloadsWatchdog] Error in PDF download: {type(e).__name__}: {e}')
			return None

	@staticmethod
	async def _get_unique_filename(directory: str, filename: str) -> str:
		"""Generate a unique filename for downloads by appending (1), (2), etc., if a file already exists."""
		base, ext = os.path.splitext(filename)
		counter = 1
		new_filename = filename
		while os.path.exists(os.path.join(directory, new_filename)):
			new_filename = f'{base} ({counter}){ext}'
			counter += 1
		return new_filename


# Fix Pydantic circular dependency - this will be called from session.py after BrowserSession is defined

```

---

## backend/browser-use/browser_use/browser/watchdogs/local_browser_watchdog.py

```py
"""Local browser watchdog for managing browser subprocess lifecycle."""

import asyncio
import os
import shutil
import tempfile
from pathlib import Path
from typing import TYPE_CHECKING, Any, ClassVar

import psutil
from bubus import BaseEvent
from pydantic import PrivateAttr

from browser_use.browser.events import (
	BrowserKillEvent,
	BrowserLaunchEvent,
	BrowserLaunchResult,
	BrowserStopEvent,
)
from browser_use.browser.watchdog_base import BaseWatchdog
from browser_use.observability import observe_debug

if TYPE_CHECKING:
	pass


class LocalBrowserWatchdog(BaseWatchdog):
	"""Manages local browser subprocess lifecycle."""

	# Events this watchdog listens to
	LISTENS_TO: ClassVar[list[type[BaseEvent[Any]]]] = [
		BrowserLaunchEvent,
		BrowserKillEvent,
		BrowserStopEvent,
	]

	# Events this watchdog emits
	EMITS: ClassVar[list[type[BaseEvent[Any]]]] = []

	# Private state for subprocess management
	_subprocess: psutil.Process | None = PrivateAttr(default=None)
	_owns_browser_resources: bool = PrivateAttr(default=True)
	_temp_dirs_to_cleanup: list[Path] = PrivateAttr(default_factory=list)
	_original_user_data_dir: str | None = PrivateAttr(default=None)

	@observe_debug(ignore_input=True, ignore_output=True, name='browser_launch_event')
	async def on_BrowserLaunchEvent(self, event: BrowserLaunchEvent) -> BrowserLaunchResult:
		"""Launch a local browser process."""

		try:
			self.logger.debug('[LocalBrowserWatchdog] Received BrowserLaunchEvent, launching local browser...')

			# self.logger.debug('[LocalBrowserWatchdog] Calling _launch_browser...')
			process, cdp_url = await self._launch_browser()
			self._subprocess = process
			# self.logger.debug(f'[LocalBrowserWatchdog] _launch_browser returned: process={process}, cdp_url={cdp_url}')

			return BrowserLaunchResult(cdp_url=cdp_url)
		except Exception as e:
			self.logger.error(f'[LocalBrowserWatchdog] Exception in on_BrowserLaunchEvent: {e}', exc_info=True)
			raise

	async def on_BrowserKillEvent(self, event: BrowserKillEvent) -> None:
		"""Kill the local browser subprocess."""
		self.logger.debug('[LocalBrowserWatchdog] Killing local browser process')

		if self._subprocess:
			await self._cleanup_process(self._subprocess)
			self._subprocess = None

		# Clean up temp directories if any were created
		for temp_dir in self._temp_dirs_to_cleanup:
			self._cleanup_temp_dir(temp_dir)
		self._temp_dirs_to_cleanup.clear()

		# Restore original user_data_dir if it was modified
		if self._original_user_data_dir is not None:
			self.browser_session.browser_profile.user_data_dir = self._original_user_data_dir
			self._original_user_data_dir = None

		self.logger.debug('[LocalBrowserWatchdog] Browser cleanup completed')

	async def on_BrowserStopEvent(self, event: BrowserStopEvent) -> None:
		"""Listen for BrowserStopEvent and dispatch BrowserKillEvent without awaiting it."""
		if self.browser_session.is_local and self._subprocess:
			self.logger.debug('[LocalBrowserWatchdog] BrowserStopEvent received, dispatching BrowserKillEvent')
			# Dispatch BrowserKillEvent without awaiting so it gets processed after all BrowserStopEvent handlers
			self.event_bus.dispatch(BrowserKillEvent())

	@observe_debug(ignore_input=True, ignore_output=True, name='launch_browser_process')
	async def _launch_browser(self, max_retries: int = 3) -> tuple[psutil.Process, str]:
		"""Launch browser process and return (process, cdp_url).

		Handles launch errors by falling back to temporary directories if needed.

		Returns:
			Tuple of (psutil.Process, cdp_url)
		"""
		# Keep track of original user_data_dir to restore if needed
		profile = self.browser_session.browser_profile
		self._original_user_data_dir = str(profile.user_data_dir) if profile.user_data_dir else None
		self._temp_dirs_to_cleanup = []

		for attempt in range(max_retries):
			try:
				# Get launch args from profile
				launch_args = profile.get_args()

				# Add debugging port
				debug_port = self._find_free_port()
				launch_args.extend(
					[
						f'--remote-debugging-port={debug_port}',
					]
				)
				assert '--user-data-dir' in str(launch_args), (
					'User data dir must be set somewhere in launch args to a non-default path, otherwise Chrome will not let us attach via CDP'
				)

				# Get browser executable
				# Priority: custom executable > fallback paths > playwright subprocess
				if profile.executable_path:
					browser_path = profile.executable_path
					self.logger.debug(f'[LocalBrowserWatchdog] ğŸ“¦ Using custom local browser executable_path= {browser_path}')
				else:
					# self.logger.debug('[LocalBrowserWatchdog] ğŸ” Looking for local browser binary path...')
					# Try fallback paths first (system browsers preferred)
					browser_path = self._find_installed_browser_path()
					if not browser_path:
						self.logger.error(
							'[LocalBrowserWatchdog] âš ï¸ No local browser binary found, installing browser using playwright subprocess...'
						)
						browser_path = await self._install_browser_with_playwright()

				self.logger.debug(f'[LocalBrowserWatchdog] ğŸ“¦ Found local browser installed at executable_path= {browser_path}')
				if not browser_path:
					raise RuntimeError('No local Chrome/Chromium install found, and failed to install with playwright')

				# Launch browser subprocess directly
				self.logger.debug(f'[LocalBrowserWatchdog] ğŸš€ Launching browser subprocess with {len(launch_args)} args...')
				self.logger.debug(
					f'[LocalBrowserWatchdog] ğŸ“‚ user_data_dir={profile.user_data_dir}, profile_directory={profile.profile_directory}'
				)
				subprocess = await asyncio.create_subprocess_exec(
					browser_path,
					*launch_args,
					stdout=asyncio.subprocess.PIPE,
					stderr=asyncio.subprocess.PIPE,
				)
				self.logger.debug(
					f'[LocalBrowserWatchdog] ğŸ­ Browser running with browser_pid= {subprocess.pid} ğŸ”— listening on CDP port :{debug_port}'
				)

				# Convert to psutil.Process
				process = psutil.Process(subprocess.pid)

				# Wait for CDP to be ready and get the URL
				cdp_url = await self._wait_for_cdp_url(debug_port)

				# Success! Clean up only the temp dirs we created but didn't use
				currently_used_dir = str(profile.user_data_dir)
				unused_temp_dirs = [tmp_dir for tmp_dir in self._temp_dirs_to_cleanup if str(tmp_dir) != currently_used_dir]

				for tmp_dir in unused_temp_dirs:
					try:
						shutil.rmtree(tmp_dir, ignore_errors=True)
					except Exception:
						pass

				# Keep only the in-use directory for cleanup during browser kill
				if currently_used_dir and 'browseruse-tmp-' in currently_used_dir:
					self._temp_dirs_to_cleanup = [Path(currently_used_dir)]
				else:
					self._temp_dirs_to_cleanup = []

				return process, cdp_url

			except Exception as e:
				error_str = str(e).lower()

				# Check if this is a user_data_dir related error
				if any(err in error_str for err in ['singletonlock', 'user data directory', 'cannot create', 'already in use']):
					self.logger.warning(f'Browser launch failed (attempt {attempt + 1}/{max_retries}): {e}')

					if attempt < max_retries - 1:
						# Create a temporary directory for next attempt
						tmp_dir = Path(tempfile.mkdtemp(prefix='browseruse-tmp-'))
						self._temp_dirs_to_cleanup.append(tmp_dir)

						# Update profile to use temp directory
						profile.user_data_dir = str(tmp_dir)
						self.logger.debug(f'Retrying with temporary user_data_dir: {tmp_dir}')

						# Small delay before retry
						await asyncio.sleep(0.5)
						continue

				# Not a recoverable error or last attempt failed
				# Restore original user_data_dir before raising
				if self._original_user_data_dir is not None:
					profile.user_data_dir = self._original_user_data_dir

				# Clean up any temp dirs we created
				for tmp_dir in self._temp_dirs_to_cleanup:
					try:
						shutil.rmtree(tmp_dir, ignore_errors=True)
					except Exception:
						pass

				raise

		# Should not reach here, but just in case
		if self._original_user_data_dir is not None:
			profile.user_data_dir = self._original_user_data_dir
		raise RuntimeError(f'Failed to launch browser after {max_retries} attempts')

	@staticmethod
	def _find_installed_browser_path() -> str | None:
		"""Try to find browser executable from common fallback locations.

		Prioritizes:
		1. System Chrome Stable
		1. Playwright chromium
		2. Other system native browsers (Chromium -> Chrome Canary/Dev -> Brave)
		3. Playwright headless-shell fallback

		Returns:
			Path to browser executable or None if not found
		"""
		import glob
		import platform
		from pathlib import Path

		system = platform.system()
		patterns = []

		# Get playwright browsers path from environment variable if set
		playwright_path = os.environ.get('PLAYWRIGHT_BROWSERS_PATH')

		if system == 'Darwin':  # macOS
			if not playwright_path:
				playwright_path = '~/Library/Caches/ms-playwright'
			patterns = [
				'/Applications/Google Chrome.app/Contents/MacOS/Google Chrome',
				f'{playwright_path}/chromium-*/chrome-mac/Chromium.app/Contents/MacOS/Chromium',
				'/Applications/Chromium.app/Contents/MacOS/Chromium',
				'/Applications/Google Chrome Canary.app/Contents/MacOS/Google Chrome Canary',
				'/Applications/Brave Browser.app/Contents/MacOS/Brave Browser',
				f'{playwright_path}/chromium_headless_shell-*/chrome-mac/Chromium.app/Contents/MacOS/Chromium',
			]
		elif system == 'Linux':
			if not playwright_path:
				playwright_path = '~/.cache/ms-playwright'
			patterns = [
				'/usr/bin/google-chrome-stable',
				'/usr/bin/google-chrome',
				'/usr/local/bin/google-chrome',
				f'{playwright_path}/chromium-*/chrome-linux64/chrome',  # Playwright installs to chrome-linux64
				f'{playwright_path}/chromium-*/chrome-linux/chrome',
				'/usr/bin/chromium',
				'/usr/bin/chromium-browser',
				'/usr/local/bin/chromium',
				'/snap/bin/chromium',
				'/usr/bin/google-chrome-beta',
				'/usr/bin/google-chrome-dev',
				'/usr/bin/brave-browser',
				f'{playwright_path}/chromium_headless_shell-*/chrome-linux64/headless_shell',  # Playwright headless shell
				f'{playwright_path}/chromium_headless_shell-*/chrome-linux/chrome',
			]
		elif system == 'Windows':
			if not playwright_path:
				playwright_path = r'%LOCALAPPDATA%\ms-playwright'
			patterns = [
				r'C:\Program Files\Google\Chrome\Application\chrome.exe',
				r'C:\Program Files (x86)\Google\Chrome\Application\chrome.exe',
				r'%LOCALAPPDATA%\Google\Chrome\Application\chrome.exe',
				r'%PROGRAMFILES%\Google\Chrome\Application\chrome.exe',
				r'%PROGRAMFILES(X86)%\Google\Chrome\Application\chrome.exe',
				f'{playwright_path}\\chromium-*\\chrome-win\\chrome.exe',
				r'C:\Program Files\Chromium\Application\chrome.exe',
				r'C:\Program Files (x86)\Chromium\Application\chrome.exe',
				r'%LOCALAPPDATA%\Chromium\Application\chrome.exe',
				r'C:\Program Files\BraveSoftware\Brave-Browser\Application\brave.exe',
				r'C:\Program Files (x86)\BraveSoftware\Brave-Browser\Application\brave.exe',
				r'C:\Program Files (x86)\Microsoft\Edge\Application\msedge.exe',
				r'C:\Program Files\Microsoft\Edge\Application\msedge.exe',
				r'%LOCALAPPDATA%\Microsoft\Edge\Application\msedge.exe',
				f'{playwright_path}\\chromium_headless_shell-*\\chrome-win\\chrome.exe',
			]

		for pattern in patterns:
			# Expand user home directory
			expanded_pattern = Path(pattern).expanduser()

			# Handle Windows environment variables
			if system == 'Windows':
				pattern_str = str(expanded_pattern)
				for env_var in ['%LOCALAPPDATA%', '%PROGRAMFILES%', '%PROGRAMFILES(X86)%']:
					if env_var in pattern_str:
						env_key = env_var.strip('%').replace('(X86)', ' (x86)')
						env_value = os.environ.get(env_key, '')
						if env_value:
							pattern_str = pattern_str.replace(env_var, env_value)
				expanded_pattern = Path(pattern_str)

			# Convert to string for glob
			pattern_str = str(expanded_pattern)

			# Check if pattern contains wildcards
			if '*' in pattern_str:
				# Use glob to expand the pattern
				matches = glob.glob(pattern_str)
				if matches:
					# Sort matches and take the last one (alphanumerically highest version)
					matches.sort()
					browser_path = matches[-1]
					if Path(browser_path).exists() and Path(browser_path).is_file():
						return browser_path
			else:
				# Direct path check
				if expanded_pattern.exists() and expanded_pattern.is_file():
					return str(expanded_pattern)

		return None

	async def _install_browser_with_playwright(self) -> str:
		"""Get browser executable path from playwright in a subprocess to avoid thread issues."""
		import platform

		# Build command - only use --with-deps on Linux (it fails on Windows/macOS)
		cmd = ['uvx', 'playwright', 'install', 'chrome']
		if platform.system() == 'Linux':
			cmd.append('--with-deps')

		# Run in subprocess with timeout
		process = await asyncio.create_subprocess_exec(
			*cmd,
			stdout=asyncio.subprocess.PIPE,
			stderr=asyncio.subprocess.PIPE,
		)

		try:
			stdout, stderr = await asyncio.wait_for(process.communicate(), timeout=60.0)
			self.logger.debug(f'[LocalBrowserWatchdog] ğŸ“¦ Playwright install output: {stdout}')
			browser_path = self._find_installed_browser_path()
			if browser_path:
				return browser_path
			self.logger.error(f'[LocalBrowserWatchdog] âŒ Playwright local browser installation error: \n{stdout}\n{stderr}')
			raise RuntimeError('No local browser path found after: uvx playwright install chrome')
		except TimeoutError:
			# Kill the subprocess if it times out
			process.kill()
			await process.wait()
			raise RuntimeError('Timeout getting browser path from playwright')
		except Exception as e:
			# Make sure subprocess is terminated
			if process.returncode is None:
				process.kill()
				await process.wait()
			raise RuntimeError(f'Error getting browser path: {e}')

	@staticmethod
	def _find_free_port() -> int:
		"""Find a free port for the debugging interface."""
		import socket

		with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
			s.bind(('127.0.0.1', 0))
			s.listen(1)
			port = s.getsockname()[1]
		return port

	@staticmethod
	async def _wait_for_cdp_url(port: int, timeout: float = 30) -> str:
		"""Wait for the browser to start and return the CDP URL."""
		import aiohttp

		start_time = asyncio.get_event_loop().time()

		while asyncio.get_event_loop().time() - start_time < timeout:
			try:
				async with aiohttp.ClientSession() as session:
					async with session.get(f'http://127.0.0.1:{port}/json/version') as resp:
						if resp.status == 200:
							# Chrome is ready
							return f'http://127.0.0.1:{port}/'
						else:
							# Chrome is starting up and returning 502/500 errors
							await asyncio.sleep(0.1)
			except Exception:
				# Connection error - Chrome might not be ready yet
				await asyncio.sleep(0.1)

		raise TimeoutError(f'Browser did not start within {timeout} seconds')

	@staticmethod
	async def _cleanup_process(process: psutil.Process) -> None:
		"""Clean up browser process.

		Args:
			process: psutil.Process to terminate
		"""
		if not process:
			return

		try:
			# Try graceful shutdown first
			process.terminate()

			# Use async wait instead of blocking wait
			for _ in range(50):  # Wait up to 5 seconds (50 * 0.1)
				if not process.is_running():
					return
				await asyncio.sleep(0.1)

			# If still running after 5 seconds, force kill
			if process.is_running():
				process.kill()
				# Give it a moment to die
				await asyncio.sleep(0.1)

		except psutil.NoSuchProcess:
			# Process already gone
			pass
		except Exception:
			# Ignore any other errors during cleanup
			pass

	def _cleanup_temp_dir(self, temp_dir: Path | str) -> None:
		"""Clean up temporary directory.

		Args:
			temp_dir: Path to temporary directory to remove
		"""
		if not temp_dir:
			return

		try:
			temp_path = Path(temp_dir)
			# Only remove if it's actually a temp directory we created
			if 'browseruse-tmp-' in str(temp_path):
				shutil.rmtree(temp_path, ignore_errors=True)
		except Exception as e:
			self.logger.debug(f'Failed to cleanup temp dir {temp_dir}: {e}')

	@property
	def browser_pid(self) -> int | None:
		"""Get the browser process ID."""
		if self._subprocess:
			return self._subprocess.pid
		return None

	@staticmethod
	async def get_browser_pid_via_cdp(browser) -> int | None:
		"""Get the browser process ID via CDP SystemInfo.getProcessInfo.

		Args:
			browser: Playwright Browser instance

		Returns:
			Process ID or None if failed
		"""
		try:
			cdp_session = await browser.new_browser_cdp_session()
			result = await cdp_session.send('SystemInfo.getProcessInfo')
			process_info = result.get('processInfo', {})
			pid = process_info.get('id')
			await cdp_session.detach()
			return pid
		except Exception:
			# If we can't get PID via CDP, it's not critical
			return None

```

---

## backend/browser-use/browser_use/browser/watchdogs/permissions_watchdog.py

```py
"""Permissions watchdog for granting browser permissions on connection."""

from typing import TYPE_CHECKING, ClassVar

from bubus import BaseEvent

from browser_use.browser.events import BrowserConnectedEvent
from browser_use.browser.watchdog_base import BaseWatchdog

if TYPE_CHECKING:
	pass


class PermissionsWatchdog(BaseWatchdog):
	"""Grants browser permissions when browser connects."""

	# Event contracts
	LISTENS_TO: ClassVar[list[type[BaseEvent]]] = [
		BrowserConnectedEvent,
	]
	EMITS: ClassVar[list[type[BaseEvent]]] = []

	async def on_BrowserConnectedEvent(self, event: BrowserConnectedEvent) -> None:
		"""Grant permissions when browser connects."""
		permissions = self.browser_session.browser_profile.permissions

		if not permissions:
			self.logger.debug('No permissions to grant')
			return

		self.logger.debug(f'ğŸ”“ Granting browser permissions: {permissions}')

		try:
			# Grant permissions using CDP Browser.grantPermissions
			# origin=None means grant to all origins
			# Browser domain commands don't use session_id
			await self.browser_session.cdp_client.send.Browser.grantPermissions(
				params={'permissions': permissions}  # type: ignore
			)
			self.logger.debug(f'âœ… Successfully granted permissions: {permissions}')
		except Exception as e:
			self.logger.error(f'âŒ Failed to grant permissions: {str(e)}')
			# Don't raise - permissions are not critical to browser operation

```

---

## backend/browser-use/browser_use/browser/watchdogs/popups_watchdog.py

```py
"""Watchdog for handling JavaScript dialogs (alert, confirm, prompt) automatically."""

import asyncio
from typing import ClassVar

from bubus import BaseEvent
from pydantic import PrivateAttr

from browser_use.browser.events import TabCreatedEvent
from browser_use.browser.watchdog_base import BaseWatchdog


class PopupsWatchdog(BaseWatchdog):
	"""Handles JavaScript dialogs (alert, confirm, prompt) by automatically accepting them immediately."""

	# Events this watchdog listens to and emits
	LISTENS_TO: ClassVar[list[type[BaseEvent]]] = [TabCreatedEvent]
	EMITS: ClassVar[list[type[BaseEvent]]] = []

	# Track which targets have dialog handlers registered
	_dialog_listeners_registered: set[str] = PrivateAttr(default_factory=set)

	def __init__(self, **kwargs):
		super().__init__(**kwargs)
		self.logger.debug(f'ğŸš€ PopupsWatchdog initialized with browser_session={self.browser_session}, ID={id(self)}')

	async def on_TabCreatedEvent(self, event: TabCreatedEvent) -> None:
		"""Set up JavaScript dialog handling when a new tab is created."""
		target_id = event.target_id
		self.logger.debug(f'ğŸ¯ PopupsWatchdog received TabCreatedEvent for target {target_id}')

		# Skip if we've already registered for this target
		if target_id in self._dialog_listeners_registered:
			self.logger.debug(f'Already registered dialog handlers for target {target_id}')
			return

		self.logger.debug(f'ğŸ“Œ Starting dialog handler setup for target {target_id}')
		try:
			# Get all CDP sessions for this target and any child frames
			cdp_session = await self.browser_session.get_or_create_cdp_session(
				target_id, focus=False
			)  # don't auto-focus new tabs! sometimes we need to open tabs in background

			# CRITICAL: Enable Page domain to receive dialog events
			try:
				await cdp_session.cdp_client.send.Page.enable(session_id=cdp_session.session_id)
				self.logger.debug(f'âœ… Enabled Page domain for session {cdp_session.session_id[-8:]}')
			except Exception as e:
				self.logger.debug(f'Failed to enable Page domain: {e}')

			# Also register for the root CDP client to catch dialogs from any frame
			if self.browser_session._cdp_client_root:
				self.logger.debug('ğŸ“Œ Also registering handler on root CDP client')
				try:
					# Enable Page domain on root client too
					await self.browser_session._cdp_client_root.send.Page.enable()
					self.logger.debug('âœ… Enabled Page domain on root CDP client')
				except Exception as e:
					self.logger.debug(f'Failed to enable Page domain on root: {e}')

			# Set up async handler for JavaScript dialogs - accept immediately without event dispatch
			async def handle_dialog(event_data, session_id: str | None = None):
				"""Handle JavaScript dialog events - accept immediately."""
				try:
					dialog_type = event_data.get('type', 'alert')
					message = event_data.get('message', '')

					# Store the popup message in browser session for inclusion in browser state
					if message:
						formatted_message = f'[{dialog_type}] {message}'
						self.browser_session._closed_popup_messages.append(formatted_message)
						self.logger.debug(f'ğŸ“ Stored popup message: {formatted_message[:100]}')

					# Choose action based on dialog type:
					# - alert: accept=true (click OK to dismiss)
					# - confirm: accept=true (click OK to proceed - safer for automation)
					# - prompt: accept=false (click Cancel since we can't provide input)
					# - beforeunload: accept=true (allow navigation)
					should_accept = dialog_type in ('alert', 'confirm', 'beforeunload')

					action_str = 'accepting (OK)' if should_accept else 'dismissing (Cancel)'
					self.logger.info(f"ğŸ”” JavaScript {dialog_type} dialog: '{message[:100]}' - {action_str}...")

					dismissed = False

					# Approach 1: Use the session that detected the dialog (most reliable)
					if self.browser_session._cdp_client_root and session_id:
						try:
							self.logger.debug(f'ğŸ”„ Approach 1: Using detecting session {session_id[-8:]}')
							await asyncio.wait_for(
								self.browser_session._cdp_client_root.send.Page.handleJavaScriptDialog(
									params={'accept': should_accept},
									session_id=session_id,
								),
								timeout=0.5,
							)
							dismissed = True
							self.logger.info('âœ… Dialog handled successfully via detecting session')
						except (TimeoutError, Exception) as e:
							self.logger.debug(f'Approach 1 failed: {type(e).__name__}')

					# Approach 2: Try with current agent focus session
					if not dismissed and self.browser_session._cdp_client_root and self.browser_session.agent_focus_target_id:
						try:
							# Use public API with focus=False to avoid changing focus during popup dismissal
							cdp_session = await self.browser_session.get_or_create_cdp_session(
								self.browser_session.agent_focus_target_id, focus=False
							)
							self.logger.debug(f'ğŸ”„ Approach 2: Using agent focus session {cdp_session.session_id[-8:]}')
							await asyncio.wait_for(
								self.browser_session._cdp_client_root.send.Page.handleJavaScriptDialog(
									params={'accept': should_accept},
									session_id=cdp_session.session_id,
								),
								timeout=0.5,
							)
							dismissed = True
							self.logger.info('âœ… Dialog handled successfully via agent focus session')
						except (TimeoutError, Exception) as e:
							self.logger.debug(f'Approach 2 failed: {type(e).__name__}')

				except Exception as e:
					self.logger.error(f'âŒ Critical error in dialog handler: {type(e).__name__}: {e}')

			# Register handler on the specific session
			cdp_session.cdp_client.register.Page.javascriptDialogOpening(handle_dialog)  # type: ignore[arg-type]
			self.logger.debug(
				f'Successfully registered Page.javascriptDialogOpening handler for session {cdp_session.session_id}'
			)

			# Also register on root CDP client to catch dialogs from any frame
			if hasattr(self.browser_session._cdp_client_root, 'register'):
				try:
					self.browser_session._cdp_client_root.register.Page.javascriptDialogOpening(handle_dialog)  # type: ignore[arg-type]
					self.logger.debug('Successfully registered dialog handler on root CDP client for all frames')
				except Exception as root_error:
					self.logger.warning(f'Failed to register on root CDP client: {root_error}')

			# Mark this target as having dialog handling set up
			self._dialog_listeners_registered.add(target_id)

			self.logger.debug(f'Set up JavaScript dialog handling for tab {target_id}')

		except Exception as e:
			self.logger.warning(f'Failed to set up popup handling for tab {target_id}: {e}')

```

---

## backend/browser-use/browser_use/browser/watchdogs/recording_watchdog.py

```py
"""Recording Watchdog for Browser Use Sessions."""

import asyncio
from pathlib import Path
from typing import ClassVar

from bubus import BaseEvent
from cdp_use.cdp.page.events import ScreencastFrameEvent
from uuid_extensions import uuid7str

from browser_use.browser.events import BrowserConnectedEvent, BrowserStopEvent
from browser_use.browser.profile import ViewportSize
from browser_use.browser.video_recorder import VideoRecorderService
from browser_use.browser.watchdog_base import BaseWatchdog
from browser_use.utils import create_task_with_error_handling


class RecordingWatchdog(BaseWatchdog):
	"""
	Manages video recording of a browser session using CDP screencasting.
	"""

	LISTENS_TO: ClassVar[list[type[BaseEvent]]] = [BrowserConnectedEvent, BrowserStopEvent]
	EMITS: ClassVar[list[type[BaseEvent]]] = []

	_recorder: VideoRecorderService | None = None

	async def on_BrowserConnectedEvent(self, event: BrowserConnectedEvent) -> None:
		"""
		Starts video recording if it is configured in the browser profile.
		"""
		profile = self.browser_session.browser_profile
		if not profile.record_video_dir:
			return

		# Dynamically determine video size
		size = profile.record_video_size
		if not size:
			self.logger.debug('record_video_size not specified, detecting viewport size...')
			size = await self._get_current_viewport_size()

		if not size:
			self.logger.warning('Cannot start video recording: viewport size could not be determined.')
			return

		video_format = getattr(profile, 'record_video_format', 'mp4').strip('.')
		output_path = Path(profile.record_video_dir) / f'{uuid7str()}.{video_format}'

		self.logger.debug(f'Initializing video recorder for format: {video_format}')
		self._recorder = VideoRecorderService(output_path=output_path, size=size, framerate=profile.record_video_framerate)
		self._recorder.start()

		if not self._recorder._is_active:
			self._recorder = None
			return

		self.browser_session.cdp_client.register.Page.screencastFrame(self.on_screencastFrame)

		try:
			cdp_session = await self.browser_session.get_or_create_cdp_session()
			await cdp_session.cdp_client.send.Page.startScreencast(
				params={
					'format': 'png',
					'quality': 90,
					'maxWidth': size['width'],
					'maxHeight': size['height'],
					'everyNthFrame': 1,
				},
				session_id=cdp_session.session_id,
			)
			self.logger.info(f'ğŸ“¹ Started video recording to {output_path}')
		except Exception as e:
			self.logger.error(f'Failed to start screencast via CDP: {e}')
			if self._recorder:
				self._recorder.stop_and_save()
				self._recorder = None

	async def _get_current_viewport_size(self) -> ViewportSize | None:
		"""Gets the current viewport size directly from the browser via CDP."""
		try:
			cdp_session = await self.browser_session.get_or_create_cdp_session()
			metrics = await cdp_session.cdp_client.send.Page.getLayoutMetrics(session_id=cdp_session.session_id)

			# Use cssVisualViewport for the most accurate representation of the visible area
			viewport = metrics.get('cssVisualViewport', {})
			width = viewport.get('clientWidth')
			height = viewport.get('clientHeight')

			if width and height:
				self.logger.debug(f'Detected viewport size: {width}x{height}')
				return ViewportSize(width=int(width), height=int(height))
		except Exception as e:
			self.logger.warning(f'Failed to get viewport size from browser: {e}')

		return None

	def on_screencastFrame(self, event: ScreencastFrameEvent, session_id: str | None) -> None:
		"""
		Synchronous handler for incoming screencast frames.
		"""

		if not self._recorder:
			return
		self._recorder.add_frame(event['data'])
		create_task_with_error_handling(
			self._ack_screencast_frame(event, session_id),
			name='ack_screencast_frame',
			logger_instance=self.logger,
			suppress_exceptions=True,
		)

	async def _ack_screencast_frame(self, event: ScreencastFrameEvent, session_id: str | None) -> None:
		"""
		Asynchronously acknowledges a screencast frame.
		"""
		try:
			await self.browser_session.cdp_client.send.Page.screencastFrameAck(
				params={'sessionId': event['sessionId']}, session_id=session_id
			)
		except Exception as e:
			self.logger.debug(f'Failed to acknowledge screencast frame: {e}')

	async def on_BrowserStopEvent(self, event: BrowserStopEvent) -> None:
		"""
		Stops the video recording and finalizes the video file.
		"""
		if self._recorder:
			recorder = self._recorder
			self._recorder = None

			self.logger.debug('Stopping video recording and saving file...')
			loop = asyncio.get_event_loop()
			await loop.run_in_executor(None, recorder.stop_and_save)

```

---

## backend/browser-use/browser_use/browser/watchdogs/screenshot_watchdog.py

```py
"""Screenshot watchdog for handling screenshot requests using CDP."""

from typing import TYPE_CHECKING, Any, ClassVar

from bubus import BaseEvent
from cdp_use.cdp.page import CaptureScreenshotParameters

from browser_use.browser.events import ScreenshotEvent
from browser_use.browser.views import BrowserError
from browser_use.browser.watchdog_base import BaseWatchdog
from browser_use.observability import observe_debug

if TYPE_CHECKING:
	pass


class ScreenshotWatchdog(BaseWatchdog):
	"""Handles screenshot requests using CDP."""

	# Events this watchdog listens to
	LISTENS_TO: ClassVar[list[type[BaseEvent[Any]]]] = [ScreenshotEvent]

	# Events this watchdog emits
	EMITS: ClassVar[list[type[BaseEvent[Any]]]] = []

	@observe_debug(ignore_input=True, ignore_output=True, name='screenshot_event_handler')
	async def on_ScreenshotEvent(self, event: ScreenshotEvent) -> str:
		"""Handle screenshot request using CDP.

		Args:
			event: ScreenshotEvent with optional full_page and clip parameters

		Returns:
			Dict with 'screenshot' key containing base64-encoded screenshot or None
		"""
		self.logger.debug('[ScreenshotWatchdog] Handler START - on_ScreenshotEvent called')
		try:
			# Validate focused target is a top-level page (not iframe/worker)
			# CDP Page.captureScreenshot only works on page/tab targets
			focused_target = self.browser_session.get_focused_target()

			if focused_target and focused_target.target_type in ('page', 'tab'):
				target_id = focused_target.target_id
			else:
				# Focused target is iframe/worker/missing - fall back to any page target
				target_type_str = focused_target.target_type if focused_target else 'None'
				self.logger.warning(f'[ScreenshotWatchdog] Focused target is {target_type_str}, falling back to page target')
				page_targets = self.browser_session.get_page_targets()
				if not page_targets:
					raise BrowserError('[ScreenshotWatchdog] No page targets available for screenshot')
				target_id = page_targets[-1].target_id

			cdp_session = await self.browser_session.get_or_create_cdp_session(target_id, focus=False)

			# Prepare screenshot parameters
			params = CaptureScreenshotParameters(format='png', captureBeyondViewport=False)

			# Take screenshot using CDP
			self.logger.debug(f'[ScreenshotWatchdog] Taking screenshot with params: {params}')
			result = await cdp_session.cdp_client.send.Page.captureScreenshot(params=params, session_id=cdp_session.session_id)

			# Return base64-encoded screenshot data
			if result and 'data' in result:
				self.logger.debug('[ScreenshotWatchdog] Screenshot captured successfully')
				return result['data']

			raise BrowserError('[ScreenshotWatchdog] Screenshot result missing data')
		except Exception as e:
			self.logger.error(f'[ScreenshotWatchdog] Screenshot failed: {e}')
			raise
		finally:
			# Try to remove highlights even on failure
			try:
				await self.browser_session.remove_highlights()
			except Exception:
				pass

```

---

## backend/browser-use/browser_use/browser/watchdogs/security_watchdog.py

```py
"""Security watchdog for enforcing URL access policies."""

from typing import TYPE_CHECKING, ClassVar

from bubus import BaseEvent

from browser_use.browser.events import (
	BrowserErrorEvent,
	NavigateToUrlEvent,
	NavigationCompleteEvent,
	TabCreatedEvent,
)
from browser_use.browser.watchdog_base import BaseWatchdog

if TYPE_CHECKING:
	pass

# Track if we've shown the glob warning
_GLOB_WARNING_SHOWN = False


class SecurityWatchdog(BaseWatchdog):
	"""Monitors and enforces security policies for URL access."""

	# Event contracts
	LISTENS_TO: ClassVar[list[type[BaseEvent]]] = [
		NavigateToUrlEvent,
		NavigationCompleteEvent,
		TabCreatedEvent,
	]
	EMITS: ClassVar[list[type[BaseEvent]]] = [
		BrowserErrorEvent,
	]

	async def on_NavigateToUrlEvent(self, event: NavigateToUrlEvent) -> None:
		"""Check if navigation URL is allowed before navigation starts."""
		# Security check BEFORE navigation
		if not self._is_url_allowed(event.url):
			self.logger.warning(f'â›”ï¸ Blocking navigation to disallowed URL: {event.url}')
			self.event_bus.dispatch(
				BrowserErrorEvent(
					error_type='NavigationBlocked',
					message=f'Navigation blocked to disallowed URL: {event.url}',
					details={'url': event.url, 'reason': 'not_in_allowed_domains'},
				)
			)
			# Stop event propagation by raising exception
			raise ValueError(f'Navigation to {event.url} blocked by security policy')

	async def on_NavigationCompleteEvent(self, event: NavigationCompleteEvent) -> None:
		"""Check if navigated URL is allowed (catches redirects to blocked domains)."""
		# Check if the navigated URL is allowed (in case of redirects)
		if not self._is_url_allowed(event.url):
			self.logger.warning(f'â›”ï¸ Navigation to non-allowed URL detected: {event.url}')

			# Dispatch browser error
			self.event_bus.dispatch(
				BrowserErrorEvent(
					error_type='NavigationBlocked',
					message=f'Navigation blocked to non-allowed URL: {event.url} - redirecting to about:blank',
					details={'url': event.url, 'target_id': event.target_id},
				)
			)
			# Navigate to about:blank to keep session alive
			# Agent will see the error and can continue with other tasks
			try:
				session = await self.browser_session.get_or_create_cdp_session(target_id=event.target_id)
				await session.cdp_client.send.Page.navigate(params={'url': 'about:blank'}, session_id=session.session_id)
				self.logger.info(f'â›”ï¸ Navigated to about:blank after blocked URL: {event.url}')
			except Exception as e:
				pass
				self.logger.error(f'â›”ï¸ Failed to navigate to about:blank: {type(e).__name__} {e}')

	async def on_TabCreatedEvent(self, event: TabCreatedEvent) -> None:
		"""Check if new tab URL is allowed."""
		if not self._is_url_allowed(event.url):
			self.logger.warning(f'â›”ï¸ New tab created with disallowed URL: {event.url}')

			# Dispatch error and try to close the tab
			self.event_bus.dispatch(
				BrowserErrorEvent(
					error_type='TabCreationBlocked',
					message=f'Tab created with non-allowed URL: {event.url}',
					details={'url': event.url, 'target_id': event.target_id},
				)
			)

			# Try to close the offending tab
			try:
				await self.browser_session._cdp_close_page(event.target_id)
				self.logger.info(f'â›”ï¸ Closed new tab with non-allowed URL: {event.url}')
			except Exception as e:
				self.logger.error(f'â›”ï¸ Failed to close new tab with non-allowed URL: {type(e).__name__} {e}')

	def _is_root_domain(self, domain: str) -> bool:
		"""Check if a domain is a root domain (no subdomain present).

		Simple heuristic: only add www for domains with exactly 1 dot (domain.tld).
		For complex cases like country TLDs or subdomains, users should configure explicitly.

		Args:
			domain: The domain to check

		Returns:
			True if it's a simple root domain, False otherwise
		"""
		# Skip if it contains wildcards or protocol
		if '*' in domain or '://' in domain:
			return False

		return domain.count('.') == 1

	def _log_glob_warning(self) -> None:
		"""Log a warning about glob patterns in allowed_domains."""
		global _GLOB_WARNING_SHOWN
		if not _GLOB_WARNING_SHOWN:
			_GLOB_WARNING_SHOWN = True
			self.logger.warning(
				'âš ï¸ Using glob patterns in allowed_domains. '
				'Note: Patterns like "*.example.com" will match both subdomains AND the main domain.'
			)

	def _get_domain_variants(self, host: str) -> tuple[str, str]:
		"""Get both variants of a domain (with and without www prefix).

		Args:
			host: The hostname to process

		Returns:
			Tuple of (original_host, variant_host)
			- If host starts with www., variant is without www.
			- Otherwise, variant is with www. prefix
		"""
		if host.startswith('www.'):
			return (host, host[4:])  # ('www.example.com', 'example.com')
		else:
			return (host, f'www.{host}')  # ('example.com', 'www.example.com')

	def _is_ip_address(self, host: str) -> bool:
		"""Check if a hostname is an IP address (IPv4 or IPv6).

		Args:
			host: The hostname to check

		Returns:
			True if the host is an IP address, False otherwise
		"""
		import ipaddress

		try:
			# Try to parse as IP address (handles both IPv4 and IPv6)
			ipaddress.ip_address(host)
			return True
		except ValueError:
			return False
		except Exception:
			return False

	def _is_url_allowed(self, url: str) -> bool:
		"""Check if a URL is allowed based on the allowed_domains configuration.

		Args:
			url: The URL to check

		Returns:
			True if the URL is allowed, False otherwise
		"""

		# Always allow internal browser targets (before any other checks)
		if url in ['about:blank', 'chrome://new-tab-page/', 'chrome://new-tab-page', 'chrome://newtab/']:
			return True

		# Parse the URL to extract components
		from urllib.parse import urlparse

		try:
			parsed = urlparse(url)
		except Exception:
			# Invalid URL
			return False

		# Allow data: and blob: URLs (they don't have hostnames)
		if parsed.scheme in ['data', 'blob']:
			return True

		# Get the actual host (domain)
		host = parsed.hostname
		if not host:
			return False

		# Check if IP addresses should be blocked (before domain checks)
		if self.browser_session.browser_profile.block_ip_addresses:
			if self._is_ip_address(host):
				return False

		# If no allowed_domains specified, allow all URLs
		if (
			not self.browser_session.browser_profile.allowed_domains
			and not self.browser_session.browser_profile.prohibited_domains
		):
			return True

		# Check allowed domains (fast path for sets, slow path for lists with patterns)
		if self.browser_session.browser_profile.allowed_domains:
			allowed_domains = self.browser_session.browser_profile.allowed_domains

			if isinstance(allowed_domains, set):
				# Fast path: O(1) exact hostname match - check both www and non-www variants
				host_variant, host_alt = self._get_domain_variants(host)
				return host_variant in allowed_domains or host_alt in allowed_domains
			else:
				# Slow path: O(n) pattern matching for lists
				for pattern in allowed_domains:
					if self._is_url_match(url, host, parsed.scheme, pattern):
						return True
				return False

		# Check prohibited domains (fast path for sets, slow path for lists with patterns)
		if self.browser_session.browser_profile.prohibited_domains:
			prohibited_domains = self.browser_session.browser_profile.prohibited_domains

			if isinstance(prohibited_domains, set):
				# Fast path: O(1) exact hostname match - check both www and non-www variants
				host_variant, host_alt = self._get_domain_variants(host)
				return host_variant not in prohibited_domains and host_alt not in prohibited_domains
			else:
				# Slow path: O(n) pattern matching for lists
				for pattern in prohibited_domains:
					if self._is_url_match(url, host, parsed.scheme, pattern):
						return False
				return True

		return True

	def _is_url_match(self, url: str, host: str, scheme: str, pattern: str) -> bool:
		"""Check if a URL matches a pattern."""

		# Full URL for matching (scheme + host)
		full_url_pattern = f'{scheme}://{host}'

		# Handle glob patterns
		if '*' in pattern:
			self._log_glob_warning()
			import fnmatch

			# Check if pattern matches the host
			if pattern.startswith('*.'):
				# Pattern like *.example.com should match subdomains and main domain
				domain_part = pattern[2:]  # Remove *.
				if host == domain_part or host.endswith('.' + domain_part):
					# Only match http/https URLs for domain-only patterns
					if scheme in ['http', 'https']:
						return True
			elif pattern.endswith('/*'):
				# Pattern like brave://* or http*://example.com/*
				if fnmatch.fnmatch(url, pattern):
					return True
			else:
				# Use fnmatch for other glob patterns
				if fnmatch.fnmatch(
					full_url_pattern if '://' in pattern else host,
					pattern,
				):
					return True
		else:
			# Exact match
			if '://' in pattern:
				# Full URL pattern
				if url.startswith(pattern):
					return True
			else:
				# Domain-only pattern (case-insensitive comparison)
				if host.lower() == pattern.lower():
					return True
				# If pattern is a root domain, also check www subdomain
				if self._is_root_domain(pattern) and host.lower() == f'www.{pattern.lower()}':
					return True

		return False

```

---

## backend/browser-use/browser_use/browser/watchdogs/storage_state_watchdog.py

```py
"""Storage state watchdog for managing browser cookies and storage persistence."""

import asyncio
import json
import os
from pathlib import Path
from typing import Any, ClassVar

from bubus import BaseEvent
from cdp_use.cdp.network import Cookie
from pydantic import Field, PrivateAttr

from browser_use.browser.events import (
	BrowserConnectedEvent,
	BrowserStopEvent,
	LoadStorageStateEvent,
	SaveStorageStateEvent,
	StorageStateLoadedEvent,
	StorageStateSavedEvent,
)
from browser_use.browser.watchdog_base import BaseWatchdog
from browser_use.utils import create_task_with_error_handling


class StorageStateWatchdog(BaseWatchdog):
	"""Monitors and persists browser storage state including cookies and localStorage."""

	# Event contracts
	LISTENS_TO: ClassVar[list[type[BaseEvent]]] = [
		BrowserConnectedEvent,
		BrowserStopEvent,
		SaveStorageStateEvent,
		LoadStorageStateEvent,
	]
	EMITS: ClassVar[list[type[BaseEvent]]] = [
		StorageStateSavedEvent,
		StorageStateLoadedEvent,
	]

	# Configuration
	auto_save_interval: float = Field(default=30.0)  # Auto-save every 30 seconds
	save_on_change: bool = Field(default=True)  # Save immediately when cookies change

	# Private state
	_monitoring_task: asyncio.Task | None = PrivateAttr(default=None)
	_last_cookie_state: list[dict] = PrivateAttr(default_factory=list)
	_save_lock: asyncio.Lock = PrivateAttr(default_factory=asyncio.Lock)

	async def on_BrowserConnectedEvent(self, event: BrowserConnectedEvent) -> None:
		"""Start monitoring when browser starts."""
		self.logger.debug('[StorageStateWatchdog] ğŸª Initializing auth/cookies sync <-> with storage_state.json file')

		# Start monitoring
		await self._start_monitoring()

		# Automatically load storage state after browser start
		await self.event_bus.dispatch(LoadStorageStateEvent())

	async def on_BrowserStopEvent(self, event: BrowserStopEvent) -> None:
		"""Stop monitoring when browser stops."""
		self.logger.debug('[StorageStateWatchdog] Stopping storage_state monitoring')
		await self._stop_monitoring()

	async def on_SaveStorageStateEvent(self, event: SaveStorageStateEvent) -> None:
		"""Handle storage state save request."""
		# Use provided path or fall back to profile default
		path = event.path
		if path is None:
			# Use profile default path if available
			if self.browser_session.browser_profile.storage_state:
				path = str(self.browser_session.browser_profile.storage_state)
			else:
				path = None  # Skip saving if no path available
		await self._save_storage_state(path)

	async def on_LoadStorageStateEvent(self, event: LoadStorageStateEvent) -> None:
		"""Handle storage state load request."""
		# Use provided path or fall back to profile default
		path = event.path
		if path is None:
			# Use profile default path if available
			if self.browser_session.browser_profile.storage_state:
				path = str(self.browser_session.browser_profile.storage_state)
			else:
				path = None  # Skip loading if no path available
		await self._load_storage_state(path)

	async def _start_monitoring(self) -> None:
		"""Start the monitoring task."""
		if self._monitoring_task and not self._monitoring_task.done():
			return

		assert self.browser_session.cdp_client is not None

		self._monitoring_task = create_task_with_error_handling(
			self._monitor_storage_changes(), name='monitor_storage_changes', logger_instance=self.logger, suppress_exceptions=True
		)
		# self.logger'[StorageStateWatchdog] Started storage monitoring task')

	async def _stop_monitoring(self) -> None:
		"""Stop the monitoring task."""
		if self._monitoring_task and not self._monitoring_task.done():
			self._monitoring_task.cancel()
			try:
				await self._monitoring_task
			except asyncio.CancelledError:
				pass
			# self.logger.debug('[StorageStateWatchdog] Stopped storage monitoring task')

	async def _check_for_cookie_changes_cdp(self, event: dict) -> None:
		"""Check if a CDP network event indicates cookie changes.

		This would be called by Network.responseReceivedExtraInfo events
		if we set up CDP event listeners.
		"""
		try:
			# Check for Set-Cookie headers in the response
			headers = event.get('headers', {})
			if 'set-cookie' in headers or 'Set-Cookie' in headers:
				self.logger.debug('[StorageStateWatchdog] Cookie change detected via CDP')

				# If save on change is enabled, trigger save immediately
				if self.save_on_change:
					await self._save_storage_state()
		except Exception as e:
			self.logger.warning(f'[StorageStateWatchdog] Error checking for cookie changes: {e}')

	async def _monitor_storage_changes(self) -> None:
		"""Periodically check for storage changes and auto-save."""
		while True:
			try:
				await asyncio.sleep(self.auto_save_interval)

				# Check if cookies have changed
				if await self._have_cookies_changed():
					self.logger.debug('[StorageStateWatchdog] Detected changes to sync with storage_state.json')
					await self._save_storage_state()

			except asyncio.CancelledError:
				break
			except Exception as e:
				self.logger.error(f'[StorageStateWatchdog] Error in monitoring loop: {e}')

	async def _have_cookies_changed(self) -> bool:
		"""Check if cookies have changed since last save."""
		if not self.browser_session.cdp_client:
			return False

		try:
			# Get current cookies using CDP
			current_cookies = await self.browser_session._cdp_get_cookies()

			# Convert to comparable format, using .get() for optional fields
			current_cookie_set = {
				(c.get('name', ''), c.get('domain', ''), c.get('path', '')): c.get('value', '') for c in current_cookies
			}

			last_cookie_set = {
				(c.get('name', ''), c.get('domain', ''), c.get('path', '')): c.get('value', '') for c in self._last_cookie_state
			}

			return current_cookie_set != last_cookie_set
		except Exception as e:
			self.logger.debug(f'[StorageStateWatchdog] Error comparing cookies: {e}')
			return False

	async def _save_storage_state(self, path: str | None = None) -> None:
		"""Save browser storage state to file."""
		async with self._save_lock:
			# Check if CDP client is available
			assert await self.browser_session.get_or_create_cdp_session(target_id=None)

			save_path = path or self.browser_session.browser_profile.storage_state
			if not save_path:
				return

			# Skip saving if the storage state is already a dict (indicates it was loaded from memory)
			# We only save to file if it started as a file path
			if isinstance(save_path, dict):
				self.logger.debug('[StorageStateWatchdog] Storage state is already a dict, skipping file save')
				return

			try:
				# Get current storage state using CDP
				storage_state = await self.browser_session._cdp_get_storage_state()

				# Update our last known state
				self._last_cookie_state = storage_state.get('cookies', []).copy()

				# Convert path to Path object
				json_path = Path(save_path).expanduser().resolve()
				json_path.parent.mkdir(parents=True, exist_ok=True)

				# Merge with existing state if file exists
				merged_state = storage_state
				if json_path.exists():
					try:
						existing_state = json.loads(json_path.read_text())
						merged_state = self._merge_storage_states(existing_state, dict(storage_state))
					except Exception as e:
						self.logger.error(f'[StorageStateWatchdog] Failed to merge with existing state: {e}')

				# Write atomically
				temp_path = json_path.with_suffix('.json.tmp')
				temp_path.write_text(json.dumps(merged_state, indent=4))

				# Backup existing file
				if json_path.exists():
					backup_path = json_path.with_suffix('.json.bak')
					json_path.replace(backup_path)

				# Move temp to final
				temp_path.replace(json_path)

				# Emit success event
				self.event_bus.dispatch(
					StorageStateSavedEvent(
						path=str(json_path),
						cookies_count=len(merged_state.get('cookies', [])),
						origins_count=len(merged_state.get('origins', [])),
					)
				)

				self.logger.debug(
					f'[StorageStateWatchdog] Saved storage state to {json_path} '
					f'({len(merged_state.get("cookies", []))} cookies, '
					f'{len(merged_state.get("origins", []))} origins)'
				)

			except Exception as e:
				self.logger.error(f'[StorageStateWatchdog] Failed to save storage state: {e}')

	async def _load_storage_state(self, path: str | None = None) -> None:
		"""Load browser storage state from file."""
		if not self.browser_session.cdp_client:
			self.logger.warning('[StorageStateWatchdog] No CDP client available for loading')
			return

		load_path = path or self.browser_session.browser_profile.storage_state
		if not load_path or not os.path.exists(str(load_path)):
			return

		try:
			# Read the storage state file asynchronously
			import anyio

			content = await anyio.Path(str(load_path)).read_text()
			storage = json.loads(content)

			# Apply cookies if present
			if 'cookies' in storage and storage['cookies']:
				await self.browser_session._cdp_set_cookies(storage['cookies'])
				self._last_cookie_state = storage['cookies'].copy()
				self.logger.debug(f'[StorageStateWatchdog] Added {len(storage["cookies"])} cookies from storage state')

			# Apply origins (localStorage/sessionStorage) if present
			if 'origins' in storage and storage['origins']:
				for origin in storage['origins']:
					if 'localStorage' in origin:
						for item in origin['localStorage']:
							script = f"""
								window.localStorage.setItem({json.dumps(item['name'])}, {json.dumps(item['value'])});
							"""
							await self.browser_session._cdp_add_init_script(script)
					if 'sessionStorage' in origin:
						for item in origin['sessionStorage']:
							script = f"""
								window.sessionStorage.setItem({json.dumps(item['name'])}, {json.dumps(item['value'])});
							"""
							await self.browser_session._cdp_add_init_script(script)
				self.logger.debug(
					f'[StorageStateWatchdog] Applied localStorage/sessionStorage from {len(storage["origins"])} origins'
				)

			self.event_bus.dispatch(
				StorageStateLoadedEvent(
					path=str(load_path),
					cookies_count=len(storage.get('cookies', [])),
					origins_count=len(storage.get('origins', [])),
				)
			)

			self.logger.debug(f'[StorageStateWatchdog] Loaded storage state from: {load_path}')

		except Exception as e:
			self.logger.error(f'[StorageStateWatchdog] Failed to load storage state: {e}')

	@staticmethod
	def _merge_storage_states(existing: dict[str, Any], new: dict[str, Any]) -> dict[str, Any]:
		"""Merge two storage states, with new values taking precedence."""
		merged = existing.copy()

		# Merge cookies
		existing_cookies = {(c['name'], c['domain'], c['path']): c for c in existing.get('cookies', [])}

		for cookie in new.get('cookies', []):
			key = (cookie['name'], cookie['domain'], cookie['path'])
			existing_cookies[key] = cookie

		merged['cookies'] = list(existing_cookies.values())

		# Merge origins
		existing_origins = {origin['origin']: origin for origin in existing.get('origins', [])}

		for origin in new.get('origins', []):
			existing_origins[origin['origin']] = origin

		merged['origins'] = list(existing_origins.values())

		return merged

	async def get_current_cookies(self) -> list[dict[str, Any]]:
		"""Get current cookies using CDP."""
		if not self.browser_session.cdp_client:
			return []

		try:
			cookies = await self.browser_session._cdp_get_cookies()
			# Cookie is a TypedDict, cast to dict for compatibility
			return [dict(cookie) for cookie in cookies]
		except Exception as e:
			self.logger.error(f'[StorageStateWatchdog] Failed to get cookies: {e}')
			return []

	async def add_cookies(self, cookies: list[dict[str, Any]]) -> None:
		"""Add cookies using CDP."""
		if not self.browser_session.cdp_client:
			self.logger.warning('[StorageStateWatchdog] No CDP client available for adding cookies')
			return

		try:
			# Convert dicts to Cookie objects
			cookie_objects = [Cookie(**cookie_dict) if isinstance(cookie_dict, dict) else cookie_dict for cookie_dict in cookies]
			# Set cookies using CDP
			await self.browser_session._cdp_set_cookies(cookie_objects)
			self.logger.debug(f'[StorageStateWatchdog] Added {len(cookies)} cookies')
		except Exception as e:
			self.logger.error(f'[StorageStateWatchdog] Failed to add cookies: {e}')

```

---

## backend/browser-use/browser_use/cli.py

```py
# pyright: reportMissingImports=false

# Check for MCP mode early to prevent logging initialization
import sys

if '--mcp' in sys.argv:
	import logging
	import os

	os.environ['BROWSER_USE_LOGGING_LEVEL'] = 'critical'
	os.environ['BROWSER_USE_SETUP_LOGGING'] = 'false'
	logging.disable(logging.CRITICAL)

# Special case: install command doesn't need CLI dependencies
if len(sys.argv) > 1 and sys.argv[1] == 'install':
	import platform
	import subprocess

	print('ğŸ“¦ Installing Chromium browser + system dependencies...')
	print('â³ This may take a few minutes...\n')

	# Build command - only use --with-deps on Linux (it fails on Windows/macOS)
	cmd = ['uvx', 'playwright', 'install', 'chromium']
	if platform.system() == 'Linux':
		cmd.append('--with-deps')
	cmd.append('--no-shell')

	result = subprocess.run(cmd)

	if result.returncode == 0:
		print('\nâœ… Installation complete!')
		print('ğŸš€ Ready to use! Run: uvx browser-use')
	else:
		print('\nâŒ Installation failed')
		sys.exit(1)
	sys.exit(0)

# Check for init subcommand early to avoid loading TUI dependencies
if 'init' in sys.argv:
	from browser_use.init_cmd import INIT_TEMPLATES
	from browser_use.init_cmd import main as init_main

	# Check if --template or -t flag is present without a value
	# If so, just remove it and let init_main handle interactive mode
	if '--template' in sys.argv or '-t' in sys.argv:
		try:
			template_idx = sys.argv.index('--template') if '--template' in sys.argv else sys.argv.index('-t')
			template = sys.argv[template_idx + 1] if template_idx + 1 < len(sys.argv) else None

			# If template is not provided or is another flag, remove the flag and use interactive mode
			if not template or template.startswith('-'):
				if '--template' in sys.argv:
					sys.argv.remove('--template')
				else:
					sys.argv.remove('-t')
		except (ValueError, IndexError):
			pass

	# Remove 'init' from sys.argv so click doesn't see it as an unexpected argument
	sys.argv.remove('init')
	init_main()
	sys.exit(0)

# Check for --template flag early to avoid loading TUI dependencies
if '--template' in sys.argv:
	from pathlib import Path

	import click

	from browser_use.init_cmd import INIT_TEMPLATES

	# Parse template and output from sys.argv
	try:
		template_idx = sys.argv.index('--template')
		template = sys.argv[template_idx + 1] if template_idx + 1 < len(sys.argv) else None
	except (ValueError, IndexError):
		template = None

	# If template is not provided or is another flag, use interactive mode
	if not template or template.startswith('-'):
		# Redirect to init command with interactive template selection
		from browser_use.init_cmd import main as init_main

		# Remove --template from sys.argv
		sys.argv.remove('--template')
		init_main()
		sys.exit(0)

	# Validate template name
	if template not in INIT_TEMPLATES:
		click.echo(f'âŒ Invalid template. Choose from: {", ".join(INIT_TEMPLATES.keys())}', err=True)
		sys.exit(1)

	# Check for --output flag
	output = None
	if '--output' in sys.argv or '-o' in sys.argv:
		try:
			output_idx = sys.argv.index('--output') if '--output' in sys.argv else sys.argv.index('-o')
			output = sys.argv[output_idx + 1] if output_idx + 1 < len(sys.argv) else None
		except (ValueError, IndexError):
			pass

	# Check for --force flag
	force = '--force' in sys.argv or '-f' in sys.argv

	# Determine output path
	output_path = Path(output) if output else Path.cwd() / f'browser_use_{template}.py'

	# Read and write template
	try:
		templates_dir = Path(__file__).parent / 'cli_templates'
		template_file = INIT_TEMPLATES[template]['file']
		template_path = templates_dir / template_file
		content = template_path.read_text(encoding='utf-8')

		# Write file with safety checks
		if output_path.exists() and not force:
			click.echo(f'âš ï¸  File already exists: {output_path}')
			if not click.confirm('Overwrite?', default=False):
				click.echo('âŒ Cancelled')
				sys.exit(1)

		output_path.parent.mkdir(parents=True, exist_ok=True)
		output_path.write_text(content, encoding='utf-8')

		click.echo(f'âœ… Created {output_path}')
		click.echo('\nNext steps:')
		click.echo('  1. Install browser-use:')
		click.echo('     uv pip install browser-use')
		click.echo('  2. Set up your API key in .env file or environment:')
		click.echo('     BROWSER_USE_API_KEY=your-key')
		click.echo('     (Get your key at https://cloud.browser-use.com/new-api-key)')
		click.echo('  3. Run your script:')
		click.echo(f'     python {output_path.name}')
	except Exception as e:
		click.echo(f'âŒ Error: {e}', err=True)
		sys.exit(1)

	sys.exit(0)

import asyncio
import json
import logging
import os
import time
from pathlib import Path
from typing import Any

from dotenv import load_dotenv

from browser_use.llm.anthropic.chat import ChatAnthropic
from browser_use.llm.google.chat import ChatGoogle
from browser_use.llm.openai.chat import ChatOpenAI

load_dotenv()

from browser_use import Agent, Controller
from browser_use.agent.views import AgentSettings
from browser_use.browser import BrowserProfile, BrowserSession
from browser_use.logging_config import addLoggingLevel
from browser_use.telemetry import CLITelemetryEvent, ProductTelemetry
from browser_use.utils import get_browser_use_version

try:
	import click
	from textual import events
	from textual.app import App, ComposeResult
	from textual.binding import Binding
	from textual.containers import Container, HorizontalGroup, VerticalScroll
	from textual.widgets import Footer, Header, Input, Label, Link, RichLog, Static
except ImportError:
	print('âš ï¸ CLI addon is not installed. Please install it with: `pip install "browser-use[cli]"` and try again.')
	sys.exit(1)


try:
	import readline

	READLINE_AVAILABLE = True
except ImportError:
	# readline not available on Windows by default
	READLINE_AVAILABLE = False


os.environ['BROWSER_USE_LOGGING_LEVEL'] = 'result'

from browser_use.config import CONFIG

# Set USER_DATA_DIR now that CONFIG is imported
USER_DATA_DIR = CONFIG.BROWSER_USE_PROFILES_DIR / 'cli'

# Ensure directories exist
CONFIG.BROWSER_USE_CONFIG_FILE.parent.mkdir(parents=True, exist_ok=True)
USER_DATA_DIR.mkdir(parents=True, exist_ok=True)

# Default User settings
MAX_HISTORY_LENGTH = 100

# Directory setup will happen in functions that need CONFIG


# Logo components with styling for rich panels
BROWSER_LOGO = """
				   [white]   ++++++   +++++++++   [/]                                
				   [white] +++     +++++     +++  [/]                                
				   [white] ++    ++++   ++    ++  [/]                                
				   [white] ++  +++       +++  ++  [/]                                
				   [white]   ++++          +++    [/]                                
				   [white]  +++             +++   [/]                                
				   [white] +++               +++  [/]                                
				   [white] ++   +++      +++  ++  [/]                                
				   [white] ++    ++++   ++    ++  [/]                                
				   [white] +++     ++++++    +++  [/]                                
				   [white]   ++++++    +++++++    [/]                                

[white]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—    â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—[/]     [darkorange]â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—[/]
[white]â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—[/]    [darkorange]â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•[/]
[white]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•[/]    [darkorange]â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—[/]  
[white]â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—[/]    [darkorange]â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•[/]  
[white]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘[/]    [darkorange]â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—[/]
[white]â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â• â•šâ•â•â•â•â•â•  â•šâ•â•â•â•šâ•â•â• â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•[/]     [darkorange]â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•[/]
"""


# Common UI constants
TEXTUAL_BORDER_STYLES = {'logo': 'blue', 'info': 'blue', 'input': 'orange3', 'working': 'yellow', 'completion': 'green'}


def get_default_config() -> dict[str, Any]:
	"""Return default configuration dictionary using the new config system."""
	# Load config from the new config system
	config_data = CONFIG.load_config()

	# Extract browser profile, llm, and agent configs
	browser_profile = config_data.get('browser_profile', {})
	llm_config = config_data.get('llm', {})
	agent_config = config_data.get('agent', {})

	return {
		'model': {
			'name': llm_config.get('model'),
			'temperature': llm_config.get('temperature', 0.0),
			'api_keys': {
				'OPENAI_API_KEY': llm_config.get('api_key', CONFIG.OPENAI_API_KEY),
				'ANTHROPIC_API_KEY': CONFIG.ANTHROPIC_API_KEY,
				'GOOGLE_API_KEY': CONFIG.GOOGLE_API_KEY,
				'DEEPSEEK_API_KEY': CONFIG.DEEPSEEK_API_KEY,
				'GROK_API_KEY': CONFIG.GROK_API_KEY,
			},
		},
		'agent': agent_config,
		'browser': {
			'headless': browser_profile.get('headless', True),
			'keep_alive': browser_profile.get('keep_alive', True),
			'ignore_https_errors': browser_profile.get('ignore_https_errors', False),
			'user_data_dir': browser_profile.get('user_data_dir'),
			'allowed_domains': browser_profile.get('allowed_domains'),
			'wait_between_actions': browser_profile.get('wait_between_actions'),
			'is_mobile': browser_profile.get('is_mobile'),
			'device_scale_factor': browser_profile.get('device_scale_factor'),
			'disable_security': browser_profile.get('disable_security'),
		},
		'command_history': [],
	}


def load_user_config() -> dict[str, Any]:
	"""Load user configuration using the new config system."""
	# Just get the default config which already loads from the new system
	config = get_default_config()

	# Load command history from a separate file if it exists
	history_file = CONFIG.BROWSER_USE_CONFIG_DIR / 'command_history.json'
	if history_file.exists():
		try:
			with open(history_file) as f:
				config['command_history'] = json.load(f)
		except (FileNotFoundError, json.JSONDecodeError):
			config['command_history'] = []

	return config


def save_user_config(config: dict[str, Any]) -> None:
	"""Save command history only (config is saved via the new system)."""
	# Only save command history to a separate file
	if 'command_history' in config and isinstance(config['command_history'], list):
		# Ensure command history doesn't exceed maximum length
		history = config['command_history']
		if len(history) > MAX_HISTORY_LENGTH:
			history = history[-MAX_HISTORY_LENGTH:]

		# Save to separate history file
		history_file = CONFIG.BROWSER_USE_CONFIG_DIR / 'command_history.json'
		with open(history_file, 'w') as f:
			json.dump(history, f, indent=2)


def update_config_with_click_args(config: dict[str, Any], ctx: click.Context) -> dict[str, Any]:
	"""Update configuration with command-line arguments."""
	# Ensure required sections exist
	if 'model' not in config:
		config['model'] = {}
	if 'browser' not in config:
		config['browser'] = {}

	# Update configuration with command-line args if provided
	if ctx.params.get('model'):
		config['model']['name'] = ctx.params['model']
	if ctx.params.get('headless') is not None:
		config['browser']['headless'] = ctx.params['headless']
	if ctx.params.get('window_width'):
		config['browser']['window_width'] = ctx.params['window_width']
	if ctx.params.get('window_height'):
		config['browser']['window_height'] = ctx.params['window_height']
	if ctx.params.get('user_data_dir'):
		config['browser']['user_data_dir'] = ctx.params['user_data_dir']
	if ctx.params.get('profile_directory'):
		config['browser']['profile_directory'] = ctx.params['profile_directory']
	if ctx.params.get('cdp_url'):
		config['browser']['cdp_url'] = ctx.params['cdp_url']

	# Consolidated proxy dict
	proxy: dict[str, str] = {}
	if ctx.params.get('proxy_url'):
		proxy['server'] = ctx.params['proxy_url']
	if ctx.params.get('no_proxy'):
		# Store as comma-separated list string to match Chrome flag
		proxy['bypass'] = ','.join([p.strip() for p in ctx.params['no_proxy'].split(',') if p.strip()])
	if ctx.params.get('proxy_username'):
		proxy['username'] = ctx.params['proxy_username']
	if ctx.params.get('proxy_password'):
		proxy['password'] = ctx.params['proxy_password']
	if proxy:
		config['browser']['proxy'] = proxy

	return config


def setup_readline_history(history: list[str]) -> None:
	"""Set up readline with command history."""
	if not READLINE_AVAILABLE:
		return

	# Add history items to readline
	for item in history:
		readline.add_history(item)


def get_llm(config: dict[str, Any]):
	"""Get the language model based on config and available API keys."""
	model_config = config.get('model', {})
	model_name = model_config.get('name')
	temperature = model_config.get('temperature', 0.0)

	# Get API key from config or environment
	api_key = model_config.get('api_keys', {}).get('OPENAI_API_KEY') or CONFIG.OPENAI_API_KEY

	if model_name:
		if model_name.startswith('gpt'):
			if not api_key and not CONFIG.OPENAI_API_KEY:
				print('âš ï¸  OpenAI API key not found. Please update your config or set OPENAI_API_KEY environment variable.')
				sys.exit(1)
			return ChatOpenAI(model=model_name, temperature=temperature, api_key=api_key or CONFIG.OPENAI_API_KEY)
		elif model_name.startswith('claude'):
			if not CONFIG.ANTHROPIC_API_KEY:
				print('âš ï¸  Anthropic API key not found. Please update your config or set ANTHROPIC_API_KEY environment variable.')
				sys.exit(1)
			return ChatAnthropic(model=model_name, temperature=temperature)
		elif model_name.startswith('gemini'):
			if not CONFIG.GOOGLE_API_KEY:
				print('âš ï¸  Google API key not found. Please update your config or set GOOGLE_API_KEY environment variable.')
				sys.exit(1)
			return ChatGoogle(model=model_name, temperature=temperature)
		elif model_name.startswith('oci'):
			# OCI models require additional configuration
			print(
				'âš ï¸  OCI models require manual configuration. Please use the ChatOCIRaw class directly with your OCI credentials.'
			)
			sys.exit(1)

	# Auto-detect based on available API keys
	if api_key or CONFIG.OPENAI_API_KEY:
		return ChatOpenAI(model='gpt-5-mini', temperature=temperature, api_key=api_key or CONFIG.OPENAI_API_KEY)
	elif CONFIG.ANTHROPIC_API_KEY:
		return ChatAnthropic(model='claude-4-sonnet', temperature=temperature)
	elif CONFIG.GOOGLE_API_KEY:
		return ChatGoogle(model='gemini-2.5-pro', temperature=temperature)
	else:
		print(
			'âš ï¸  No API keys found. Please update your config or set one of: OPENAI_API_KEY, ANTHROPIC_API_KEY, or GOOGLE_API_KEY.'
		)
		sys.exit(1)


class RichLogHandler(logging.Handler):
	"""Custom logging handler that redirects logs to a RichLog widget."""

	def __init__(self, rich_log: RichLog):
		super().__init__()
		self.rich_log = rich_log

	def emit(self, record):
		try:
			msg = self.format(record)
			self.rich_log.write(msg)
		except Exception:
			self.handleError(record)


class BrowserUseApp(App):
	"""Browser-use TUI application."""

	# Make it an inline app instead of fullscreen
	# MODES = {"light"}  # Ensure app is inline, not fullscreen

	CSS = """
	#main-container {
		height: 100%;
		layout: vertical;
	}
	
	#logo-panel, #links-panel, #paths-panel, #info-panels {
		border: solid $primary;
		margin: 0 0 0 0; 
		padding: 0;
	}
	
	#info-panels {
		display: none;
		layout: vertical;
		height: auto;
		min-height: 5;
		margin: 0 0 1 0;
	}
	
	#top-panels {
		layout: horizontal;
		height: auto;
		width: 100%;
	}
	
	#browser-panel, #model-panel {
		width: 1fr;
		height: 100%;
		padding: 1;
		border-right: solid $primary;
	}
	
	#model-panel {
		border-right: none;
	}
	
	#tasks-panel {
		height: auto;
		max-height: 10;
		overflow-y: scroll;
		padding: 1;
		border-top: solid $primary;
	}
	
	#browser-info, #model-info, #tasks-info {
		height: auto;
		margin: 0;
		padding: 0;
		background: transparent;
		overflow-y: auto;
		min-height: 3;
	}
	
	#three-column-container {
		height: 1fr;
		layout: horizontal;
		width: 100%;
		display: none;
	}
	
	#main-output-column {
		width: 1fr;
		height: 100%;
		border: solid $primary;
		padding: 0;
		margin: 0 1 0 0;
	}
	
	#events-column {
		width: 1fr;
		height: 100%;
		border: solid $warning;
		padding: 0;
		margin: 0 1 0 0;
	}
	
	#cdp-column {
		width: 1fr;
		height: 100%;
		border: solid $accent;
		padding: 0;
		margin: 0;
	}
	
	#main-output-log, #events-log, #cdp-log {
		height: 100%;
		overflow-y: scroll;
		background: $surface;
		color: $text;
		width: 100%;
		padding: 1;
	}
	
	#events-log {
		color: $warning;
	}
	
	#cdp-log {
		color: $accent-lighten-2;
	}
	
	#logo-panel {
		width: 100%;
		height: auto;
		content-align: center middle;
		text-align: center;
	}
	
	#links-panel {
		width: 100%;
		padding: 1;
		border: solid $primary;
		height: auto;
	}
	
	.link-white {
		color: white;
	}
	
	.link-purple {
		color: purple;
	}
	
	.link-magenta {
		color: magenta;
	}
	
	.link-green {
		color: green;
	}

	HorizontalGroup {
		height: auto;
	}
	
	.link-label {
		width: auto;
	}
	
	.link-url {
		width: auto;
	}
	
	.link-row {
		width: 100%;
		height: auto;
	}
	
	#paths-panel {
		color: $text-muted;
	}
	
	#task-input-container {
		border: solid $accent;
		padding: 1;
		margin-bottom: 1;
		height: auto;
		dock: bottom;
	}
	
	#task-label {
		color: $accent;
		padding-bottom: 1;
	}
	
	#task-input {
		width: 100%;
	}
	"""

	BINDINGS = [
		Binding('ctrl+c', 'quit', 'Quit', priority=True, show=True),
		Binding('ctrl+q', 'quit', 'Quit', priority=True),
		Binding('ctrl+d', 'quit', 'Quit', priority=True),
		Binding('up', 'input_history_prev', 'Previous command', show=False),
		Binding('down', 'input_history_next', 'Next command', show=False),
	]

	def __init__(self, config: dict[str, Any], *args, **kwargs):
		super().__init__(*args, **kwargs)
		self.config = config
		self.browser_session: BrowserSession | None = None  # Will be set before app.run_async()
		self.controller: Controller | None = None  # Will be set before app.run_async()
		self.agent: Agent | None = None
		self.llm: Any | None = None  # Will be set before app.run_async()
		self.task_history = config.get('command_history', [])
		# Track current position in history for up/down navigation
		self.history_index = len(self.task_history)
		# Initialize telemetry
		self._telemetry = ProductTelemetry()
		# Store for event bus handler
		self._event_bus_handler_id = None
		self._event_bus_handler_func = None
		# Timer for info panel updates
		self._info_panel_timer = None

	def setup_richlog_logging(self) -> None:
		"""Set up logging to redirect to RichLog widget instead of stdout."""
		# Try to add RESULT level if it doesn't exist
		try:
			addLoggingLevel('RESULT', 35)
		except AttributeError:
			pass  # Level already exists, which is fine

		# Get the main output RichLog widget
		rich_log = self.query_one('#main-output-log', RichLog)

		# Create and set up the custom handler
		log_handler = RichLogHandler(rich_log)
		log_type = os.getenv('BROWSER_USE_LOGGING_LEVEL', 'result').lower()

		class BrowserUseFormatter(logging.Formatter):
			def format(self, record):
				# if isinstance(record.name, str) and record.name.startswith('browser_use.'):
				# 	record.name = record.name.split('.')[-2]
				return super().format(record)

		# Set up the formatter based on log type
		if log_type == 'result':
			log_handler.setLevel('RESULT')
			log_handler.setFormatter(BrowserUseFormatter('%(message)s'))
		else:
			log_handler.setFormatter(BrowserUseFormatter('%(levelname)-8s [%(name)s] %(message)s'))

		# Configure root logger - Replace ALL handlers, not just stdout handlers
		root = logging.getLogger()

		# Clear all existing handlers to prevent output to stdout/stderr
		root.handlers = []
		root.addHandler(log_handler)

		# Set log level based on environment variable
		if log_type == 'result':
			root.setLevel('RESULT')
		elif log_type == 'debug':
			root.setLevel(logging.DEBUG)
		else:
			root.setLevel(logging.INFO)

		# Configure browser_use logger and all its sub-loggers
		browser_use_logger = logging.getLogger('browser_use')
		browser_use_logger.propagate = False  # Don't propagate to root logger
		browser_use_logger.handlers = [log_handler]  # Replace any existing handlers
		browser_use_logger.setLevel(root.level)

		# Also ensure agent loggers go to the main output
		# Use a wildcard pattern to catch all agent-related loggers
		for logger_name in ['browser_use.Agent', 'browser_use.controller', 'browser_use.agent', 'browser_use.agent.service']:
			agent_logger = logging.getLogger(logger_name)
			agent_logger.propagate = False
			agent_logger.handlers = [log_handler]
			agent_logger.setLevel(root.level)

		# Also catch any dynamically created agent loggers with task IDs
		for name, logger in logging.Logger.manager.loggerDict.items():
			if isinstance(name, str) and 'browser_use.Agent' in name:
				if isinstance(logger, logging.Logger):
					logger.propagate = False
					logger.handlers = [log_handler]
					logger.setLevel(root.level)

		# Silence third-party loggers but keep them using our handler
		for logger_name in [
			'WDM',
			'httpx',
			'selenium',
			'playwright',
			'urllib3',
			'asyncio',
			'openai',
			'httpcore',
			'charset_normalizer',
			'anthropic._base_client',
			'PIL.PngImagePlugin',
			'trafilatura.htmlprocessing',
			'trafilatura',
			'groq',
			'portalocker',
			'portalocker.utils',
		]:
			third_party = logging.getLogger(logger_name)
			third_party.setLevel(logging.ERROR)
			third_party.propagate = False
			third_party.handlers = [log_handler]  # Use our handler to prevent stdout/stderr leakage

	def on_mount(self) -> None:
		"""Set up components when app is mounted."""
		# We'll use a file logger since stdout is now controlled by Textual
		logger = logging.getLogger('browser_use.on_mount')
		logger.debug('on_mount() method started')

		# Step 1: Set up custom logging to RichLog
		logger.debug('Setting up RichLog logging...')
		try:
			self.setup_richlog_logging()
			logger.debug('RichLog logging set up successfully')
		except Exception as e:
			logger.error(f'Error setting up RichLog logging: {str(e)}', exc_info=True)
			raise RuntimeError(f'Failed to set up RichLog logging: {str(e)}')

		# Step 2: Set up input history
		logger.debug('Setting up readline history...')
		try:
			if READLINE_AVAILABLE and self.task_history:
				for item in self.task_history:
					readline.add_history(item)
				logger.debug(f'Added {len(self.task_history)} items to readline history')
			else:
				logger.debug('No readline history to set up')
		except Exception as e:
			logger.error(f'Error setting up readline history: {str(e)}', exc_info=False)
			# Non-critical, continue

		# Step 3: Focus the input field
		logger.debug('Focusing input field...')
		try:
			input_field = self.query_one('#task-input', Input)
			input_field.focus()
			logger.debug('Input field focused')
		except Exception as e:
			logger.error(f'Error focusing input field: {str(e)}', exc_info=True)
			# Non-critical, continue

		# Step 5: Setup CDP logger and event bus listener if browser session is available
		logger.debug('Setting up CDP logging and event bus listener...')
		try:
			self.setup_cdp_logger()
			if self.browser_session:
				self.setup_event_bus_listener()
			logger.debug('CDP logging and event bus setup complete')
		except Exception as e:
			logger.error(f'Error setting up CDP logging/event bus: {str(e)}', exc_info=True)
			# Non-critical, continue

		# Capture telemetry for CLI start
		self._telemetry.capture(
			CLITelemetryEvent(
				version=get_browser_use_version(),
				action='start',
				mode='interactive',
				model=self.llm.model if self.llm and hasattr(self.llm, 'model') else None,
				model_provider=self.llm.provider if self.llm and hasattr(self.llm, 'provider') else None,
			)
		)

		logger.debug('on_mount() completed successfully')

	def on_input_key_up(self, event: events.Key) -> None:
		"""Handle up arrow key in the input field."""
		# For textual key events, we need to check focus manually
		input_field = self.query_one('#task-input', Input)
		if not input_field.has_focus:
			return

		# Only process if we have history
		if not self.task_history:
			return

		# Move back in history if possible
		if self.history_index > 0:
			self.history_index -= 1
			task_input = self.query_one('#task-input', Input)
			task_input.value = self.task_history[self.history_index]
			# Move cursor to end of text
			task_input.cursor_position = len(task_input.value)

		# Prevent default behavior (cursor movement)
		event.prevent_default()
		event.stop()

	def on_input_key_down(self, event: events.Key) -> None:
		"""Handle down arrow key in the input field."""
		# For textual key events, we need to check focus manually
		input_field = self.query_one('#task-input', Input)
		if not input_field.has_focus:
			return

		# Only process if we have history
		if not self.task_history:
			return

		# Move forward in history or clear input if at the end
		if self.history_index < len(self.task_history) - 1:
			self.history_index += 1
			task_input = self.query_one('#task-input', Input)
			task_input.value = self.task_history[self.history_index]
			# Move cursor to end of text
			task_input.cursor_position = len(task_input.value)
		elif self.history_index == len(self.task_history) - 1:
			# At the end of history, go to "new line" state
			self.history_index += 1
			self.query_one('#task-input', Input).value = ''

		# Prevent default behavior (cursor movement)
		event.prevent_default()
		event.stop()

	async def on_key(self, event: events.Key) -> None:
		"""Handle key events at the app level to ensure graceful exit."""
		# Handle Ctrl+C, Ctrl+D, and Ctrl+Q for app exit
		if event.key == 'ctrl+c' or event.key == 'ctrl+d' or event.key == 'ctrl+q':
			await self.action_quit()
			event.stop()
			event.prevent_default()

	def on_input_submitted(self, event: Input.Submitted) -> None:
		"""Handle task input submission."""
		if event.input.id == 'task-input':
			task = event.input.value
			if not task.strip():
				return

			# Add to history if it's new
			if task.strip() and (not self.task_history or task != self.task_history[-1]):
				self.task_history.append(task)
				self.config['command_history'] = self.task_history
				save_user_config(self.config)

			# Reset history index to point past the end of history
			self.history_index = len(self.task_history)

			# Hide logo, links, and paths panels
			self.hide_intro_panels()

			# Process the task
			self.run_task(task)

			# Clear the input
			event.input.value = ''

	def hide_intro_panels(self) -> None:
		"""Hide the intro panels, show info panels and the three-column view."""
		try:
			# Get the panels
			logo_panel = self.query_one('#logo-panel')
			links_panel = self.query_one('#links-panel')
			paths_panel = self.query_one('#paths-panel')
			info_panels = self.query_one('#info-panels')
			three_column = self.query_one('#three-column-container')

			# Hide intro panels if they're visible and show info panels + three-column view
			if logo_panel.display:
				logging.debug('Hiding intro panels and showing info panels + three-column view')

				logo_panel.display = False
				links_panel.display = False
				paths_panel.display = False

				# Show info panels and three-column container
				info_panels.display = True
				three_column.display = True

				# Start updating info panels
				self.update_info_panels()

				logging.debug('Info panels and three-column view should now be visible')
		except Exception as e:
			logging.error(f'Error in hide_intro_panels: {str(e)}')

	def setup_event_bus_listener(self) -> None:
		"""Setup listener for browser session event bus."""
		if not self.browser_session or not self.browser_session.event_bus:
			return

		# Clean up any existing handler before registering a new one
		if self._event_bus_handler_func is not None:
			try:
				# Remove handler from the event bus's internal handlers dict
				if hasattr(self.browser_session.event_bus, 'handlers'):
					# Find and remove our handler function from all event patterns
					for event_type, handler_list in list(self.browser_session.event_bus.handlers.items()):
						# Remove our specific handler function object
						if self._event_bus_handler_func in handler_list:
							handler_list.remove(self._event_bus_handler_func)
							logging.debug(f'Removed old handler from event type: {event_type}')
			except Exception as e:
				logging.debug(f'Error cleaning up event bus handler: {e}')
			self._event_bus_handler_func = None
			self._event_bus_handler_id = None

		try:
			# Get the events log widget
			events_log = self.query_one('#events-log', RichLog)
		except Exception:
			# Widget not ready yet
			return

		# Create handler to log all events
		def log_event(event):
			event_name = event.__class__.__name__
			# Format event data nicely
			try:
				if hasattr(event, 'model_dump'):
					event_data = event.model_dump(exclude_unset=True)
					# Remove large fields
					if 'screenshot' in event_data:
						event_data['screenshot'] = '<bytes>'
					if 'dom_state' in event_data:
						event_data['dom_state'] = '<truncated>'
					event_str = str(event_data) if event_data else ''
				else:
					event_str = str(event)

				# Truncate long strings
				if len(event_str) > 200:
					event_str = event_str[:200] + '...'

				events_log.write(f'[yellow]â†’ {event_name}[/] {event_str}')
			except Exception as e:
				events_log.write(f'[red]â†’ {event_name}[/] (error formatting: {e})')

		# Store the handler function before registering it
		self._event_bus_handler_func = log_event
		self._event_bus_handler_id = id(log_event)

		# Register wildcard handler for all events
		self.browser_session.event_bus.on('*', log_event)
		logging.debug(f'Registered new event bus handler with id: {self._event_bus_handler_id}')

	def setup_cdp_logger(self) -> None:
		"""Setup CDP message logger to capture already-transformed CDP logs."""
		# No need to configure levels - setup_logging() already handles that
		# We just need to capture the transformed logs and route them to the CDP pane

		# Get the CDP log widget
		cdp_log = self.query_one('#cdp-log', RichLog)

		# Create custom handler for CDP logging
		class CDPLogHandler(logging.Handler):
			def __init__(self, rich_log: RichLog):
				super().__init__()
				self.rich_log = rich_log

			def emit(self, record):
				try:
					msg = self.format(record)
					# Truncate very long messages
					if len(msg) > 300:
						msg = msg[:300] + '...'
					# Color code by level
					if record.levelno >= logging.ERROR:
						self.rich_log.write(f'[red]{msg}[/]')
					elif record.levelno >= logging.WARNING:
						self.rich_log.write(f'[yellow]{msg}[/]')
					else:
						self.rich_log.write(f'[cyan]{msg}[/]')
				except Exception:
					self.handleError(record)

		# Setup handler for cdp_use loggers
		cdp_handler = CDPLogHandler(cdp_log)
		cdp_handler.setFormatter(logging.Formatter('%(message)s'))
		cdp_handler.setLevel(logging.DEBUG)

		# Route CDP logs to the CDP pane
		# These are already transformed by cdp_use and at the right level from setup_logging
		for logger_name in ['websockets.client', 'cdp_use', 'cdp_use.client', 'cdp_use.cdp', 'cdp_use.cdp.registry']:
			logger = logging.getLogger(logger_name)
			# Add our handler (don't replace - keep existing console handler too)
			if cdp_handler not in logger.handlers:
				logger.addHandler(cdp_handler)

	def scroll_to_input(self) -> None:
		"""Scroll to the input field to ensure it's visible."""
		input_container = self.query_one('#task-input-container')
		input_container.scroll_visible()

	def run_task(self, task: str) -> None:
		"""Launch the task in a background worker."""
		# Create or update the agent
		agent_settings = AgentSettings.model_validate(self.config.get('agent', {}))

		# Get the logger
		logger = logging.getLogger('browser_use.app')

		# Make sure intro is hidden and log is ready
		self.hide_intro_panels()

		# Clear the main output log to start fresh
		rich_log = self.query_one('#main-output-log', RichLog)
		rich_log.clear()

		if self.agent is None:
			if not self.llm:
				raise RuntimeError('LLM not initialized')
			self.agent = Agent(
				task=task,
				llm=self.llm,
				controller=self.controller if self.controller else Controller(),
				browser_session=self.browser_session,
				source='cli',
				**agent_settings.model_dump(),
			)
			# Update our browser_session reference to point to the agent's
			if hasattr(self.agent, 'browser_session'):
				self.browser_session = self.agent.browser_session
				# Set up event bus listener (will clean up any old handler first)
				self.setup_event_bus_listener()
		else:
			self.agent.add_new_task(task)

		# Let the agent run in the background
		async def agent_task_worker() -> None:
			logger.debug('\nğŸš€ Working on task: %s', task)

			# Set flags to indicate the agent is running
			if self.agent:
				self.agent.running = True  # type: ignore
				self.agent.last_response_time = 0  # type: ignore

			# Panel updates are already happening via the timer in update_info_panels

			task_start_time = time.time()
			error_msg = None

			try:
				# Capture telemetry for message sent
				self._telemetry.capture(
					CLITelemetryEvent(
						version=get_browser_use_version(),
						action='message_sent',
						mode='interactive',
						model=self.llm.model if self.llm and hasattr(self.llm, 'model') else None,
						model_provider=self.llm.provider if self.llm and hasattr(self.llm, 'provider') else None,
					)
				)

				# Run the agent task, redirecting output to RichLog through our handler
				if self.agent:
					await self.agent.run()
			except Exception as e:
				error_msg = str(e)
				logger.error('\nError running agent: %s', str(e))
			finally:
				# Clear the running flag
				if self.agent:
					self.agent.running = False  # type: ignore

				# Capture telemetry for task completion
				duration = time.time() - task_start_time
				self._telemetry.capture(
					CLITelemetryEvent(
						version=get_browser_use_version(),
						action='task_completed' if error_msg is None else 'error',
						mode='interactive',
						model=self.llm.model if self.llm and hasattr(self.llm, 'model') else None,
						model_provider=self.llm.provider if self.llm and hasattr(self.llm, 'provider') else None,
						duration_seconds=duration,
						error_message=error_msg,
					)
				)

				logger.debug('\nâœ… Task completed!')

				# Make sure the task input container is visible
				task_input_container = self.query_one('#task-input-container')
				task_input_container.display = True

				# Refocus the input field
				input_field = self.query_one('#task-input', Input)
				input_field.focus()

				# Ensure the input is visible by scrolling to it
				self.call_after_refresh(self.scroll_to_input)

		# Run the worker
		self.run_worker(agent_task_worker, name='agent_task')

	def action_input_history_prev(self) -> None:
		"""Navigate to the previous item in command history."""
		# Only process if we have history and input is focused
		input_field = self.query_one('#task-input', Input)
		if not input_field.has_focus or not self.task_history:
			return

		# Move back in history if possible
		if self.history_index > 0:
			self.history_index -= 1
			input_field.value = self.task_history[self.history_index]
			# Move cursor to end of text
			input_field.cursor_position = len(input_field.value)

	def action_input_history_next(self) -> None:
		"""Navigate to the next item in command history or clear input."""
		# Only process if we have history and input is focused
		input_field = self.query_one('#task-input', Input)
		if not input_field.has_focus or not self.task_history:
			return

		# Move forward in history or clear input if at the end
		if self.history_index < len(self.task_history) - 1:
			self.history_index += 1
			input_field.value = self.task_history[self.history_index]
			# Move cursor to end of text
			input_field.cursor_position = len(input_field.value)
		elif self.history_index == len(self.task_history) - 1:
			# At the end of history, go to "new line" state
			self.history_index += 1
			input_field.value = ''

	async def action_quit(self) -> None:
		"""Quit the application and clean up resources."""
		# Note: We don't need to close the browser session here because:
		# 1. If an agent exists, it already called browser_session.stop() in its run() method
		# 2. If keep_alive=True (default), we want to leave the browser running anyway
		# This prevents the duplicate "stop() called" messages in the logs

		# Flush telemetry before exiting
		self._telemetry.flush()

		# Exit the application
		self.exit()
		print('\nTry running tasks on our cloud: https://browser-use.com')

	def compose(self) -> ComposeResult:
		"""Create the UI layout."""
		yield Header()

		# Main container for app content
		with Container(id='main-container'):
			# Logo panel
			yield Static(BROWSER_LOGO, id='logo-panel', markup=True)

			# Links panel with URLs
			with Container(id='links-panel'):
				with HorizontalGroup(classes='link-row'):
					yield Static('Run at scale on cloud:    [blink]â˜ï¸[/]  ', markup=True, classes='link-label')
					yield Link('https://browser-use.com', url='https://browser-use.com', classes='link-white link-url')

				yield Static('')  # Empty line

				with HorizontalGroup(classes='link-row'):
					yield Static('Chat & share on Discord:  ğŸš€ ', markup=True, classes='link-label')
					yield Link(
						'https://discord.gg/ESAUZAdxXY', url='https://discord.gg/ESAUZAdxXY', classes='link-purple link-url'
					)

				with HorizontalGroup(classes='link-row'):
					yield Static('Get prompt inspiration:   ğŸ¦¸ ', markup=True, classes='link-label')
					yield Link(
						'https://github.com/browser-use/awesome-prompts',
						url='https://github.com/browser-use/awesome-prompts',
						classes='link-magenta link-url',
					)

				with HorizontalGroup(classes='link-row'):
					yield Static('[dim]Report any issues:[/]        ğŸ› ', markup=True, classes='link-label')
					yield Link(
						'https://github.com/browser-use/browser-use/issues',
						url='https://github.com/browser-use/browser-use/issues',
						classes='link-green link-url',
					)

			# Paths panel
			yield Static(
				f' âš™ï¸  Settings saved to:              {str(CONFIG.BROWSER_USE_CONFIG_FILE.resolve()).replace(str(Path.home()), "~")}\n'
				f' ğŸ“ Outputs & recordings saved to:  {str(Path(".").resolve()).replace(str(Path.home()), "~")}',
				id='paths-panel',
				markup=True,
			)

			# Info panels (hidden by default, shown when task starts)
			with Container(id='info-panels'):
				# Top row with browser and model panels side by side
				with Container(id='top-panels'):
					# Browser panel
					with Container(id='browser-panel'):
						yield RichLog(id='browser-info', markup=True, highlight=True, wrap=True)

					# Model panel
					with Container(id='model-panel'):
						yield RichLog(id='model-info', markup=True, highlight=True, wrap=True)

				# Tasks panel (full width, below browser and model)
				with VerticalScroll(id='tasks-panel'):
					yield RichLog(id='tasks-info', markup=True, highlight=True, wrap=True, auto_scroll=True)

			# Three-column container (hidden by default)
			with Container(id='three-column-container'):
				# Column 1: Main output
				with VerticalScroll(id='main-output-column'):
					yield RichLog(highlight=True, markup=True, id='main-output-log', wrap=True, auto_scroll=True)

				# Column 2: Event bus events
				with VerticalScroll(id='events-column'):
					yield RichLog(highlight=True, markup=True, id='events-log', wrap=True, auto_scroll=True)

				# Column 3: CDP messages
				with VerticalScroll(id='cdp-column'):
					yield RichLog(highlight=True, markup=True, id='cdp-log', wrap=True, auto_scroll=True)

			# Task input container (now at the bottom)
			with Container(id='task-input-container'):
				yield Label('ğŸ” What would you like me to do on the web?', id='task-label')
				yield Input(placeholder='Enter your task...', id='task-input')

		yield Footer()

	def update_info_panels(self) -> None:
		"""Update all information panels with current state."""
		try:
			# Update actual content
			self.update_browser_panel()
			self.update_model_panel()
			self.update_tasks_panel()
		except Exception as e:
			logging.error(f'Error in update_info_panels: {str(e)}')
		finally:
			# Always schedule the next update - will update at 1-second intervals
			# This ensures continuous updates even if agent state changes
			self.set_timer(1.0, self.update_info_panels)

	def update_browser_panel(self) -> None:
		"""Update browser information panel with details about the browser."""
		browser_info = self.query_one('#browser-info', RichLog)
		browser_info.clear()

		# Try to use the agent's browser session if available
		browser_session = self.browser_session
		if hasattr(self, 'agent') and self.agent and hasattr(self.agent, 'browser_session'):
			browser_session = self.agent.browser_session

		if browser_session:
			try:
				# Check if browser session has a CDP client
				if not hasattr(browser_session, 'cdp_client') or browser_session.cdp_client is None:
					browser_info.write('[yellow]Browser session created, waiting for browser to launch...[/]')
					return

				# Update our reference if we're using the agent's session
				if browser_session != self.browser_session:
					self.browser_session = browser_session

				# Get basic browser info from browser_profile
				browser_type = 'Chromium'
				headless = browser_session.browser_profile.headless

				# Determine connection type based on config
				connection_type = 'playwright'  # Default
				if browser_session.cdp_url:
					connection_type = 'CDP'
				elif browser_session.browser_profile.executable_path:
					connection_type = 'user-provided'

				# Get window size details from browser_profile
				window_width = None
				window_height = None
				if browser_session.browser_profile.viewport:
					window_width = browser_session.browser_profile.viewport.width
					window_height = browser_session.browser_profile.viewport.height

				# Try to get browser PID
				browser_pid = 'Unknown'
				connected = False
				browser_status = '[red]Disconnected[/]'

				try:
					# Check if browser PID is available
					# Check if we have a CDP client
					if browser_session.cdp_client is not None:
						connected = True
						browser_status = '[green]Connected[/]'
						browser_pid = 'N/A'
				except Exception as e:
					browser_pid = f'Error: {str(e)}'

				# Display browser information
				browser_info.write(f'[bold cyan]Chromium[/] Browser ({browser_status})')
				browser_info.write(
					f'Type: [yellow]{connection_type}[/] [{"green" if not headless else "red"}]{" (headless)" if headless else ""}[/]'
				)
				browser_info.write(f'PID: [dim]{browser_pid}[/]')
				browser_info.write(f'CDP Port: {browser_session.cdp_url}')

				if window_width and window_height:
					browser_info.write(f'Window: [blue]{window_width}[/] Ã— [blue]{window_height}[/]')

				# Include additional information about the browser if needed
				if connected and hasattr(self, 'agent') and self.agent:
					try:
						# Show when the browser was connected
						timestamp = int(time.time())
						current_time = time.strftime('%H:%M:%S', time.localtime(timestamp))
						browser_info.write(f'Last updated: [dim]{current_time}[/]')
					except Exception:
						pass

					# Show the agent's current page URL if available
					if browser_session.agent_focus_target_id:
						target = browser_session.session_manager.get_focused_target()
						target_url = target.url if target else 'about:blank'
						current_url = target_url.replace('https://', '').replace('http://', '').replace('www.', '')[:36] + 'â€¦'
						browser_info.write(f'ğŸ‘ï¸  [green]{current_url}[/]')
			except Exception as e:
				browser_info.write(f'[red]Error updating browser info: {str(e)}[/]')
		else:
			browser_info.write('[red]Browser not initialized[/]')

	def update_model_panel(self) -> None:
		"""Update model information panel with details about the LLM."""
		model_info = self.query_one('#model-info', RichLog)
		model_info.clear()

		if self.llm:
			# Get model details
			model_name = 'Unknown'
			if hasattr(self.llm, 'model_name'):
				model_name = self.llm.model_name
			elif hasattr(self.llm, 'model'):
				model_name = self.llm.model

			# Show model name
			if self.agent:
				temp_str = f'{self.llm.temperature}ÂºC ' if self.llm.temperature else ''
				vision_str = '+ vision ' if self.agent.settings.use_vision else ''
				model_info.write(
					f'[white]LLM:[/] [blue]{self.llm.__class__.__name__} [yellow]{model_name}[/] {temp_str}{vision_str}'
				)
			else:
				model_info.write(f'[white]LLM:[/] [blue]{self.llm.__class__.__name__} [yellow]{model_name}[/]')

			# Show token usage statistics if agent exists and has history
			if self.agent and hasattr(self.agent, 'state') and hasattr(self.agent.state, 'history'):
				# Calculate tokens per step
				num_steps = len(self.agent.history.history)

				# Get the last step metadata to show the most recent LLM response time
				if num_steps > 0 and self.agent.history.history[-1].metadata:
					last_step = self.agent.history.history[-1]
					if last_step.metadata:
						step_duration = last_step.metadata.duration_seconds
					else:
						step_duration = 0

				# Show total duration
				total_duration = self.agent.history.total_duration_seconds()
				if total_duration > 0:
					model_info.write(f'[white]Total Duration:[/] [magenta]{total_duration:.2f}s[/]')

					# Calculate response time metrics
					model_info.write(f'[white]Last Step Duration:[/] [magenta]{step_duration:.2f}s[/]')

				# Add current state information
				if hasattr(self.agent, 'running'):
					if getattr(self.agent, 'running', False):
						model_info.write('[yellow]LLM is thinking[blink]...[/][/]')
					elif hasattr(self.agent, 'state') and hasattr(self.agent.state, 'paused') and self.agent.state.paused:
						model_info.write('[orange]LLM paused[/]')
		else:
			model_info.write('[red]Model not initialized[/]')

	def update_tasks_panel(self) -> None:
		"""Update tasks information panel with details about the tasks and steps hierarchy."""
		tasks_info = self.query_one('#tasks-info', RichLog)
		tasks_info.clear()

		if self.agent:
			# Check if agent has tasks
			task_history = []
			message_history = []

			# Try to extract tasks by looking at message history
			if hasattr(self.agent, '_message_manager') and self.agent._message_manager:
				message_history = self.agent._message_manager.state.history.get_messages()

				# Extract original task(s)
				original_tasks = []
				for msg in message_history:
					if hasattr(msg, 'content'):
						content = msg.content
						if isinstance(content, str) and 'Your ultimate task is:' in content:
							task_text = content.split('"""')[1].strip()
							original_tasks.append(task_text)

				if original_tasks:
					tasks_info.write('[bold green]TASK:[/]')
					for i, task in enumerate(original_tasks, 1):
						# Only show latest task if multiple task changes occurred
						if i == len(original_tasks):
							tasks_info.write(f'[white]{task}[/]')
					tasks_info.write('')

			# Get current state information
			current_step = self.agent.state.n_steps if hasattr(self.agent, 'state') else 0

			# Get all agent history items
			history_items = []
			if hasattr(self.agent, 'state') and hasattr(self.agent.state, 'history'):
				history_items = self.agent.history.history

				if history_items:
					tasks_info.write('[bold yellow]STEPS:[/]')

					for idx, item in enumerate(history_items, 1):
						# Determine step status
						step_style = '[green]âœ“[/]'

						# For the current step, show it as in progress
						if idx == current_step:
							step_style = '[yellow]âŸ³[/]'

						# Check if this step had an error
						if item.result and any(result.error for result in item.result):
							step_style = '[red]âœ—[/]'

						# Show step number
						tasks_info.write(f'{step_style} Step {idx}/{current_step}')

						# Show goal if available
						if item.model_output and hasattr(item.model_output, 'current_state'):
							# Show goal for this step
							goal = item.model_output.current_state.next_goal
							if goal:
								# Take just the first line for display
								goal_lines = goal.strip().split('\n')
								goal_summary = goal_lines[0]
								tasks_info.write(f'   [cyan]Goal:[/] {goal_summary}')

							# Show evaluation of previous goal (feedback)
							eval_prev = item.model_output.current_state.evaluation_previous_goal
							if eval_prev and idx > 1:  # Only show for steps after the first
								eval_lines = eval_prev.strip().split('\n')
								eval_summary = eval_lines[0]
								eval_summary = eval_summary.replace('Success', 'âœ… ').replace('Failed', 'âŒ ').strip()
								tasks_info.write(f'   [tan]Evaluation:[/] {eval_summary}')

						# Show actions taken in this step
						if item.model_output and item.model_output.action:
							tasks_info.write('   [purple]Actions:[/]')
							for action_idx, action in enumerate(item.model_output.action, 1):
								action_type = action.__class__.__name__
								if hasattr(action, 'model_dump'):
									# For proper actions, show the action type
									action_dict = action.model_dump(exclude_unset=True)
									if action_dict:
										action_name = list(action_dict.keys())[0]
										tasks_info.write(f'     {action_idx}. [blue]{action_name}[/]')

						# Show results or errors from this step
						if item.result:
							for result in item.result:
								if result.error:
									error_text = result.error
									tasks_info.write(f'   [red]Error:[/] {error_text}')
								elif result.extracted_content:
									content = result.extracted_content
									tasks_info.write(f'   [green]Result:[/] {content}')

						# Add a space between steps for readability
						tasks_info.write('')

			# If agent is actively running, show a status indicator
			if hasattr(self.agent, 'running') and getattr(self.agent, 'running', False):
				tasks_info.write('[yellow]Agent is actively working[blink]...[/][/]')
			elif hasattr(self.agent, 'state') and hasattr(self.agent.state, 'paused') and self.agent.state.paused:
				tasks_info.write('[orange]Agent is paused (press Enter to resume)[/]')
		else:
			tasks_info.write('[dim]Agent not initialized[/]')

		# Force scroll to bottom
		tasks_panel = self.query_one('#tasks-panel')
		tasks_panel.scroll_end(animate=False)


async def run_prompt_mode(prompt: str, ctx: click.Context, debug: bool = False):
	"""Run browser-use in non-interactive mode with a single prompt."""
	# Import and call setup_logging to ensure proper initialization
	from browser_use.logging_config import setup_logging

	# Set up logging to only show results by default
	os.environ['BROWSER_USE_LOGGING_LEVEL'] = 'result'

	# Re-run setup_logging to apply the new log level
	setup_logging()

	# The logging is now properly configured by setup_logging()
	# No need to manually configure handlers since setup_logging() handles it

	# Initialize telemetry
	telemetry = ProductTelemetry()
	start_time = time.time()
	error_msg = None

	try:
		# Load config
		config = load_user_config()
		config = update_config_with_click_args(config, ctx)

		# Get LLM
		llm = get_llm(config)

		# Capture telemetry for CLI start in oneshot mode
		telemetry.capture(
			CLITelemetryEvent(
				version=get_browser_use_version(),
				action='start',
				mode='oneshot',
				model=llm.model if hasattr(llm, 'model') else None,
				model_provider=llm.__class__.__name__ if llm else None,
			)
		)

		# Get agent settings from config
		agent_settings = AgentSettings.model_validate(config.get('agent', {}))

		# Create browser session with config parameters
		browser_config = config.get('browser', {})
		# Remove None values from browser_config
		browser_config = {k: v for k, v in browser_config.items() if v is not None}
		# Create BrowserProfile with user_data_dir
		profile = BrowserProfile(user_data_dir=str(USER_DATA_DIR), **browser_config)
		browser_session = BrowserSession(
			browser_profile=profile,
		)

		# Create and run agent
		agent = Agent(
			task=prompt,
			llm=llm,
			browser_session=browser_session,
			source='cli',
			**agent_settings.model_dump(),
		)

		await agent.run()

		# Ensure the browser session is fully stopped
		# The agent's close() method only kills the browser if keep_alive=False,
		# but we need to ensure all background tasks are stopped regardless
		if browser_session:
			try:
				# Kill the browser session to stop all background tasks
				await browser_session.kill()
			except Exception:
				# Ignore errors during cleanup
				pass

		# Capture telemetry for successful completion
		telemetry.capture(
			CLITelemetryEvent(
				version=get_browser_use_version(),
				action='task_completed',
				mode='oneshot',
				model=llm.model if hasattr(llm, 'model') else None,
				model_provider=llm.__class__.__name__ if llm else None,
				duration_seconds=time.time() - start_time,
			)
		)

	except Exception as e:
		error_msg = str(e)
		# Capture telemetry for error
		telemetry.capture(
			CLITelemetryEvent(
				version=get_browser_use_version(),
				action='error',
				mode='oneshot',
				model=llm.model if hasattr(llm, 'model') else None,
				model_provider=llm.__class__.__name__ if llm and 'llm' in locals() else None,
				duration_seconds=time.time() - start_time,
				error_message=error_msg,
			)
		)
		if debug:
			import traceback

			traceback.print_exc()
		else:
			print(f'Error: {str(e)}', file=sys.stderr)
		sys.exit(1)
	finally:
		# Ensure telemetry is flushed
		telemetry.flush()

		# Give a brief moment for cleanup to complete
		await asyncio.sleep(0.1)

		# Cancel any remaining tasks to ensure clean exit
		tasks = [t for t in asyncio.all_tasks() if t != asyncio.current_task()]
		for task in tasks:
			task.cancel()

		# Wait for all tasks to be cancelled
		if tasks:
			await asyncio.gather(*tasks, return_exceptions=True)


async def textual_interface(config: dict[str, Any]):
	"""Run the Textual interface."""
	# Prevent browser_use from setting up logging at import time
	os.environ['BROWSER_USE_SETUP_LOGGING'] = 'false'

	logger = logging.getLogger('browser_use.startup')

	# Set up logging for Textual UI - prevent any logging to stdout
	def setup_textual_logging():
		# Replace all handlers with null handler
		root_logger = logging.getLogger()
		for handler in root_logger.handlers:
			root_logger.removeHandler(handler)

		# Add null handler to ensure no output to stdout/stderr
		null_handler = logging.NullHandler()
		root_logger.addHandler(null_handler)
		logger.debug('Logging configured for Textual UI')

	logger.debug('Setting up Browser, Controller, and LLM...')

	# Step 1: Initialize BrowserSession with config
	logger.debug('Initializing BrowserSession...')
	try:
		# Get browser config from the config dict
		browser_config = config.get('browser', {})

		logger.info('Browser type: chromium')  # BrowserSession only supports chromium
		if browser_config.get('executable_path'):
			logger.info(f'Browser binary: {browser_config["executable_path"]}')
		if browser_config.get('headless'):
			logger.info('Browser mode: headless')
		else:
			logger.info('Browser mode: visible')

		# Create BrowserSession directly with config parameters
		# Remove None values from browser_config
		browser_config = {k: v for k, v in browser_config.items() if v is not None}
		# Create BrowserProfile with user_data_dir
		profile = BrowserProfile(user_data_dir=str(USER_DATA_DIR), **browser_config)
		browser_session = BrowserSession(
			browser_profile=profile,
		)
		logger.debug('BrowserSession initialized successfully')

		# Set up FIFO logging pipes for streaming logs to UI
		try:
			from browser_use.logging_config import setup_log_pipes

			setup_log_pipes(session_id=browser_session.id)
			logger.debug(f'FIFO logging pipes set up for session {browser_session.id[-4:]}')
		except Exception as e:
			logger.debug(f'Could not set up FIFO logging pipes: {e}')

		# Browser version logging not available with CDP implementation
	except Exception as e:
		logger.error(f'Error initializing BrowserSession: {str(e)}', exc_info=True)
		raise RuntimeError(f'Failed to initialize BrowserSession: {str(e)}')

	# Step 3: Initialize Controller
	logger.debug('Initializing Controller...')
	try:
		controller = Controller()
		logger.debug('Controller initialized successfully')
	except Exception as e:
		logger.error(f'Error initializing Controller: {str(e)}', exc_info=True)
		raise RuntimeError(f'Failed to initialize Controller: {str(e)}')

	# Step 4: Get LLM
	logger.debug('Getting LLM...')
	try:
		# Ensure setup_logging is not called when importing modules
		os.environ['BROWSER_USE_SETUP_LOGGING'] = 'false'
		llm = get_llm(config)
		# Log LLM details
		model_name = getattr(llm, 'model_name', None) or getattr(llm, 'model', 'Unknown model')
		provider = llm.__class__.__name__
		temperature = getattr(llm, 'temperature', 0.0)
		logger.info(f'LLM: {provider} ({model_name}), temperature: {temperature}')
		logger.debug(f'LLM initialized successfully: {provider}')
	except Exception as e:
		logger.error(f'Error getting LLM: {str(e)}', exc_info=True)
		raise RuntimeError(f'Failed to initialize LLM: {str(e)}')

	logger.debug('Initializing BrowserUseApp instance...')
	try:
		app = BrowserUseApp(config)
		# Pass the initialized components to the app
		app.browser_session = browser_session
		app.controller = controller
		app.llm = llm

		# Set up event bus listener now that browser session is available
		# Note: This needs to be called before run_async() but after browser_session is set
		# We'll defer this to on_mount() since it needs the widgets to be available

		# Configure logging for Textual UI before going fullscreen
		setup_textual_logging()

		# Log browser and model configuration that will be used
		browser_type = 'Chromium'  # BrowserSession only supports Chromium
		model_name = config.get('model', {}).get('name', 'auto-detected')
		headless = config.get('browser', {}).get('headless', False)
		headless_str = 'headless' if headless else 'visible'

		logger.info(f'Preparing {browser_type} browser ({headless_str}) with {model_name} LLM')

		logger.debug('Starting Textual app with run_async()...')
		# No more logging after this point as we're in fullscreen mode
		await app.run_async()
	except Exception as e:
		logger.error(f'Error in textual_interface: {str(e)}', exc_info=True)
		# Note: We don't close the browser session here to avoid duplicate stop() calls
		# The browser session will be cleaned up by its __del__ method if needed
		raise


async def run_auth_command():
	"""Run the authentication command with dummy task in UI."""
	import asyncio
	import os

	from browser_use.sync.auth import DeviceAuthClient

	print('ğŸ” Browser Use Cloud Authentication')
	print('=' * 40)

	# Ensure cloud sync is enabled (should be default, but make sure)
	os.environ['BROWSER_USE_CLOUD_SYNC'] = 'true'

	auth_client = DeviceAuthClient()

	print('ğŸ” Debug: Checking authentication status...')
	print(f'    API Token: {"âœ… Present" if auth_client.api_token else "âŒ Missing"}')
	print(f'    User ID: {auth_client.user_id}')
	print(f'    Is Authenticated: {auth_client.is_authenticated}')
	if auth_client.auth_config.authorized_at:
		print(f'    Authorized at: {auth_client.auth_config.authorized_at}')
	print()

	# Check if already authenticated
	if auth_client.is_authenticated:
		print('âœ… Already authenticated!')
		print(f'   User ID: {auth_client.user_id}')
		print(f'   Authenticated at: {auth_client.auth_config.authorized_at}')

		# Show cloud URL if possible
		frontend_url = CONFIG.BROWSER_USE_CLOUD_UI_URL or auth_client.base_url.replace('//api.', '//cloud.')
		print(f'\nğŸŒ View your runs at: {frontend_url}')
		return

	print('ğŸš€ Starting authentication flow...')
	print('   This will open a browser window for you to sign in.')
	print()

	# Initialize variables for exception handling
	task_id = None
	sync_service = None

	try:
		# Create authentication flow with dummy task
		from uuid_extensions import uuid7str

		from browser_use.agent.cloud_events import (
			CreateAgentSessionEvent,
			CreateAgentStepEvent,
			CreateAgentTaskEvent,
			UpdateAgentTaskEvent,
		)
		from browser_use.sync.service import CloudSync

		# IDs for our session and task
		session_id = uuid7str()
		task_id = uuid7str()

		# Create special sync service that allows auth events
		sync_service = CloudSync(allow_session_events_for_auth=True)
		sync_service.set_auth_flow_active()  # Explicitly enable auth flow
		sync_service.session_id = session_id  # Set session ID for auth context
		sync_service.auth_client = auth_client  # Use the same auth client instance!

		# 1. Create session (like main branch does at start)
		session_event = CreateAgentSessionEvent(
			id=session_id,
			user_id=auth_client.temp_user_id,
			browser_session_id=uuid7str(),
			browser_session_live_url='',
			browser_session_cdp_url='',
			device_id=auth_client.device_id,
			browser_state={
				'viewport': {'width': 1280, 'height': 720},
				'user_agent': None,
				'headless': True,
				'initial_url': None,
				'final_url': None,
				'total_pages_visited': 0,
				'session_duration_seconds': 0,
			},
			browser_session_data={
				'cookies': [],
				'secrets': {},
				'allowed_domains': [],
			},
		)
		await sync_service.handle_event(session_event)

		# Brief delay to ensure session is created in backend before sending task
		await asyncio.sleep(0.5)

		# 2. Create task (like main branch does at start)
		task_event = CreateAgentTaskEvent(
			id=task_id,
			agent_session_id=session_id,
			llm_model='auth-flow',
			task='ğŸ” Complete authentication and join the browser-use community',
			user_id=auth_client.temp_user_id,
			device_id=auth_client.device_id,
			done_output=None,
			user_feedback_type=None,
			user_comment=None,
			gif_url=None,
		)
		await sync_service.handle_event(task_event)

		# Longer delay to ensure task is created in backend before sending step event
		await asyncio.sleep(1.0)

		# 3. Run authentication with timeout
		print('â³ Waiting for authentication... (this may take up to 2 minutes for testing)')
		print('   Complete the authentication in your browser, then this will continue automatically.')
		print()

		try:
			print('ğŸ”§ Debug: Starting authentication process...')
			print(f'    Original auth client authenticated: {auth_client.is_authenticated}')
			print(f'    Sync service auth client authenticated: {sync_service.auth_client.is_authenticated}')
			print(f'    Same auth client? {auth_client is sync_service.auth_client}')
			print(f'    Session ID: {sync_service.session_id}')

			# Create a task to show periodic status updates
			async def show_auth_progress():
				for i in range(1, 25):  # Show updates every 5 seconds for 2 minutes
					await asyncio.sleep(5)
					fresh_check = DeviceAuthClient()
					print(f'â±ï¸  Waiting for authentication... ({i * 5}s elapsed)')
					print(f'    Status: {"âœ… Authenticated" if fresh_check.is_authenticated else "â³ Still waiting"}')
					if fresh_check.is_authenticated:
						print('ğŸ‰ Authentication detected! Completing...')
						break

			# Run authentication and progress updates concurrently
			auth_start_time = asyncio.get_event_loop().time()
			from browser_use.utils import create_task_with_error_handling

			auth_task = create_task_with_error_handling(
				sync_service.authenticate(show_instructions=True), name='sync_authenticate'
			)
			progress_task = create_task_with_error_handling(
				show_auth_progress(), name='show_auth_progress', suppress_exceptions=True
			)

			# Wait for authentication to complete, with timeout
			success = await asyncio.wait_for(auth_task, timeout=120.0)  # 2 minutes for initial testing
			progress_task.cancel()  # Stop the progress updates

			auth_duration = asyncio.get_event_loop().time() - auth_start_time
			print(f'ğŸ”§ Debug: Authentication returned: {success} (took {auth_duration:.1f}s)')

		except TimeoutError:
			print('â±ï¸ Authentication timed out after 2 minutes.')
			print('   Checking if authentication completed in background...')

			# Create a fresh auth client to check current status
			fresh_auth_client = DeviceAuthClient()
			print('ğŸ”§ Debug: Fresh auth client check:')
			print(f'    API Token: {"âœ… Present" if fresh_auth_client.api_token else "âŒ Missing"}')
			print(f'    Is Authenticated: {fresh_auth_client.is_authenticated}')

			if fresh_auth_client.is_authenticated:
				print('âœ… Authentication was successful!')
				success = True
				# Update the sync service's auth client
				sync_service.auth_client = fresh_auth_client
			else:
				print('âŒ Authentication not completed. Please try again.')
				success = False
		except Exception as e:
			print(f'âŒ Authentication error: {type(e).__name__}: {e}')
			import traceback

			print(f'ğŸ“„ Full traceback: {traceback.format_exc()}')
			success = False

		if success:
			# 4. Send step event to show progress (like main branch during execution)
			# Use the sync service's auth client which has the updated user_id
			step_event = CreateAgentStepEvent(
				# Remove explicit ID - let it auto-generate to avoid backend validation issues
				user_id=auth_client.temp_user_id,  # Use same temp user_id as task for consistency
				device_id=auth_client.device_id,  # Use consistent device_id
				agent_task_id=task_id,
				step=1,
				actions=[
					{
						'click': {
							'coordinate': [800, 400],
							'description': 'Click on Star button',
							'success': True,
						},
						'done': {
							'success': True,
							'text': 'â­ Starred browser-use/browser-use repository! Welcome to the community!',
						},
					}
				],
				next_goal='â­ Star browser-use GitHub repository to join the community',
				evaluation_previous_goal='Authentication completed successfully',
				memory='User authenticated with Browser Use Cloud and is now part of the community',
				screenshot_url=None,
				url='https://github.com/browser-use/browser-use',
			)
			print('ğŸ“¤ Sending dummy step event...')
			await sync_service.handle_event(step_event)

			# Small delay to ensure step is processed before completion
			await asyncio.sleep(0.5)

			# 5. Complete task (like main branch does at end)
			completion_event = UpdateAgentTaskEvent(
				id=task_id,
				user_id=auth_client.temp_user_id,  # Use same temp user_id as task for consistency
				device_id=auth_client.device_id,  # Use consistent device_id
				done_output="ğŸ‰ Welcome to Browser Use! You're now authenticated and part of our community. â­ Your future tasks will sync to the cloud automatically.",
				user_feedback_type=None,
				user_comment=None,
				gif_url=None,
			)
			await sync_service.handle_event(completion_event)

			print('ğŸ‰ Authentication successful!')
			print('   Future browser-use runs will now sync to the cloud.')
		else:
			# Failed - still complete the task with failure message
			completion_event = UpdateAgentTaskEvent(
				id=task_id,
				user_id=auth_client.temp_user_id,  # Still temp user since auth failed
				device_id=auth_client.device_id,
				done_output='âŒ Authentication failed. Please try again.',
				user_feedback_type=None,
				user_comment=None,
				gif_url=None,
			)
			await sync_service.handle_event(completion_event)

			print('âŒ Authentication failed.')
			print('   Please try again or check your internet connection.')

	except Exception as e:
		print(f'âŒ Authentication error: {e}')
		# Still try to complete the task in UI with error message
		if task_id and sync_service:
			try:
				from browser_use.agent.cloud_events import UpdateAgentTaskEvent

				completion_event = UpdateAgentTaskEvent(
					id=task_id,
					user_id=auth_client.temp_user_id,
					device_id=auth_client.device_id,
					done_output=f'âŒ Authentication error: {e}',
					user_feedback_type=None,
					user_comment=None,
					gif_url=None,
				)
				await sync_service.handle_event(completion_event)
			except Exception:
				pass  # Don't fail if we can't send the error event
		sys.exit(1)


@click.group(invoke_without_command=True)
@click.option('--version', is_flag=True, help='Print version and exit')
@click.option(
	'--template',
	type=click.Choice(['default', 'advanced', 'tools'], case_sensitive=False),
	help='Generate a template file (default, advanced, or tools)',
)
@click.option('--output', '-o', type=click.Path(), help='Output file path for template (default: browser_use_<template>.py)')
@click.option('--force', '-f', is_flag=True, help='Overwrite existing files without asking')
@click.option('--model', type=str, help='Model to use (e.g., gpt-5-mini, claude-4-sonnet, gemini-2.5-flash)')
@click.option('--debug', is_flag=True, help='Enable verbose startup logging')
@click.option('--headless', is_flag=True, help='Run browser in headless mode', default=None)
@click.option('--window-width', type=int, help='Browser window width')
@click.option('--window-height', type=int, help='Browser window height')
@click.option(
	'--user-data-dir', type=str, help='Path to Chrome user data directory (e.g. ~/Library/Application Support/Google/Chrome)'
)
@click.option('--profile-directory', type=str, help='Chrome profile directory name (e.g. "Default", "Profile 1")')
@click.option('--cdp-url', type=str, help='Connect to existing Chrome via CDP URL (e.g. http://localhost:9222)')
@click.option('--proxy-url', type=str, help='Proxy server for Chromium traffic (e.g. http://host:8080 or socks5://host:1080)')
@click.option('--no-proxy', type=str, help='Comma-separated hosts to bypass proxy (e.g. localhost,127.0.0.1,*.internal)')
@click.option('--proxy-username', type=str, help='Proxy auth username')
@click.option('--proxy-password', type=str, help='Proxy auth password')
@click.option('-p', '--prompt', type=str, help='Run a single task without the TUI (headless mode)')
@click.option('--mcp', is_flag=True, help='Run as MCP server (exposes JSON RPC via stdin/stdout)')
@click.pass_context
def main(ctx: click.Context, debug: bool = False, **kwargs):
	"""Browser Use - AI Agent for Web Automation

	Run without arguments to start the interactive TUI.

	Examples:
	  uvx browser-use --template default
	  uvx browser-use --template advanced --output my_script.py
	"""

	# Handle template generation
	if kwargs.get('template'):
		_run_template_generation(kwargs['template'], kwargs.get('output'), kwargs.get('force', False))
		return

	if ctx.invoked_subcommand is None:
		# No subcommand, run the main interface
		run_main_interface(ctx, debug, **kwargs)


def run_main_interface(ctx: click.Context, debug: bool = False, **kwargs):
	"""Run the main browser-use interface"""

	if kwargs['version']:
		from importlib.metadata import version

		print(version('browser-use'))
		sys.exit(0)

	# Check if MCP server mode is activated
	if kwargs.get('mcp'):
		# Capture telemetry for MCP server mode via CLI (suppress any logging from this)
		try:
			telemetry = ProductTelemetry()
			telemetry.capture(
				CLITelemetryEvent(
					version=get_browser_use_version(),
					action='start',
					mode='mcp_server',
				)
			)
		except Exception:
			# Ignore telemetry errors in MCP mode to prevent any stdout contamination
			pass
		# Run as MCP server
		from browser_use.mcp.server import main as mcp_main

		asyncio.run(mcp_main())
		return

	# Check if prompt mode is activated
	if kwargs.get('prompt'):
		# Set environment variable for prompt mode before running
		os.environ['BROWSER_USE_LOGGING_LEVEL'] = 'result'
		# Run in non-interactive mode
		asyncio.run(run_prompt_mode(kwargs['prompt'], ctx, debug))
		return

	# Configure console logging
	console_handler = logging.StreamHandler(sys.stdout)
	console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', '%H:%M:%S'))

	# Configure root logger
	root_logger = logging.getLogger()
	root_logger.setLevel(logging.INFO if not debug else logging.DEBUG)
	root_logger.addHandler(console_handler)

	logger = logging.getLogger('browser_use.startup')
	logger.info('Starting Browser-Use initialization')
	if debug:
		logger.debug(f'System info: Python {sys.version.split()[0]}, Platform: {sys.platform}')

	logger.debug('Loading environment variables from .env file...')
	load_dotenv()
	logger.debug('Environment variables loaded')

	# Load user configuration
	logger.debug('Loading user configuration...')
	try:
		config = load_user_config()
		logger.debug(f'User configuration loaded from {CONFIG.BROWSER_USE_CONFIG_FILE}')
	except Exception as e:
		logger.error(f'Error loading user configuration: {str(e)}', exc_info=True)
		print(f'Error loading configuration: {str(e)}')
		sys.exit(1)

	# Update config with command-line arguments
	logger.debug('Updating configuration with command line arguments...')
	try:
		config = update_config_with_click_args(config, ctx)
		logger.debug('Configuration updated')
	except Exception as e:
		logger.error(f'Error updating config with command line args: {str(e)}', exc_info=True)
		print(f'Error updating configuration: {str(e)}')
		sys.exit(1)

	# Save updated config
	logger.debug('Saving user configuration...')
	try:
		save_user_config(config)
		logger.debug('Configuration saved')
	except Exception as e:
		logger.error(f'Error saving user configuration: {str(e)}', exc_info=True)
		print(f'Error saving configuration: {str(e)}')
		sys.exit(1)

	# Setup handlers for console output before entering Textual UI
	logger.debug('Setting up handlers for Textual UI...')

	# Log browser and model configuration that will be used
	browser_type = 'Chromium'  # BrowserSession only supports Chromium
	model_name = config.get('model', {}).get('name', 'auto-detected')
	headless = config.get('browser', {}).get('headless', False)
	headless_str = 'headless' if headless else 'visible'

	logger.info(f'Preparing {browser_type} browser ({headless_str}) with {model_name} LLM')

	try:
		# Run the Textual UI interface - now all the initialization happens before we go fullscreen
		logger.debug('Starting Textual UI interface...')
		asyncio.run(textual_interface(config))
	except Exception as e:
		# Restore console logging for error reporting
		root_logger.setLevel(logging.INFO)
		for handler in root_logger.handlers:
			root_logger.removeHandler(handler)
		root_logger.addHandler(console_handler)

		logger.error(f'Error initializing Browser-Use: {str(e)}', exc_info=debug)
		print(f'\nError launching Browser-Use: {str(e)}')
		if debug:
			import traceback

			traceback.print_exc()
		sys.exit(1)


@main.command()
def auth():
	"""Authenticate with Browser Use Cloud to sync your runs"""
	asyncio.run(run_auth_command())


@main.command()
def install():
	"""Install Chromium browser with system dependencies"""
	import platform
	import subprocess

	print('ğŸ“¦ Installing Chromium browser + system dependencies...')
	print('â³ This may take a few minutes...\n')

	# Build command - only use --with-deps on Linux (it fails on Windows/macOS)
	cmd = ['uvx', 'playwright', 'install', 'chromium']
	if platform.system() == 'Linux':
		cmd.append('--with-deps')
	cmd.append('--no-shell')

	result = subprocess.run(cmd)

	if result.returncode == 0:
		print('\nâœ… Installation complete!')
		print('ğŸš€ Ready to use! Run: uvx browser-use')
	else:
		print('\nâŒ Installation failed')
		sys.exit(1)


# ============================================================================
# Template Generation - Generate template files
# ============================================================================

# Template metadata
INIT_TEMPLATES = {
	'default': {
		'file': 'default_template.py',
		'description': 'Simplest setup - capable of any web task with minimal configuration',
	},
	'advanced': {
		'file': 'advanced_template.py',
		'description': 'All configuration options shown with defaults',
	},
	'tools': {
		'file': 'tools_template.py',
		'description': 'Custom action examples - extend the agent with your own functions',
	},
}


def _run_template_generation(template: str, output: str | None, force: bool):
	"""Generate a template file (called from main CLI)."""
	# Determine output path
	if output:
		output_path = Path(output)
	else:
		output_path = Path.cwd() / f'browser_use_{template}.py'

	# Read template file
	try:
		templates_dir = Path(__file__).parent / 'cli_templates'
		template_file = INIT_TEMPLATES[template]['file']
		template_path = templates_dir / template_file
		content = template_path.read_text(encoding='utf-8')
	except Exception as e:
		click.echo(f'âŒ Error reading template: {e}', err=True)
		sys.exit(1)

	# Write file
	if _write_init_file(output_path, content, force):
		click.echo(f'âœ… Created {output_path}')
		click.echo('\nNext steps:')
		click.echo('  1. Install browser-use:')
		click.echo('     uv pip install browser-use')
		click.echo('  2. Set up your API key in .env file or environment:')
		click.echo('     BROWSER_USE_API_KEY=your-key')
		click.echo('     (Get your key at https://cloud.browser-use.com/new-api-key)')
		click.echo('  3. Run your script:')
		click.echo(f'     python {output_path.name}')
	else:
		sys.exit(1)


def _write_init_file(output_path: Path, content: str, force: bool = False) -> bool:
	"""Write content to a file, with safety checks."""
	# Check if file already exists
	if output_path.exists() and not force:
		click.echo(f'âš ï¸  File already exists: {output_path}')
		if not click.confirm('Overwrite?', default=False):
			click.echo('âŒ Cancelled')
			return False

	# Ensure parent directory exists
	output_path.parent.mkdir(parents=True, exist_ok=True)

	# Write file
	try:
		output_path.write_text(content, encoding='utf-8')
		return True
	except Exception as e:
		click.echo(f'âŒ Error writing file: {e}', err=True)
		return False


@main.command('init')
@click.option(
	'--template',
	'-t',
	type=click.Choice(['default', 'advanced', 'tools'], case_sensitive=False),
	help='Template to use',
)
@click.option(
	'--output',
	'-o',
	type=click.Path(),
	help='Output file path (default: browser_use_<template>.py)',
)
@click.option(
	'--force',
	'-f',
	is_flag=True,
	help='Overwrite existing files without asking',
)
@click.option(
	'--list',
	'-l',
	'list_templates',
	is_flag=True,
	help='List available templates',
)
def init(
	template: str | None,
	output: str | None,
	force: bool,
	list_templates: bool,
):
	"""
	Generate a browser-use template file to get started quickly.

	Examples:

	\b
	# Interactive mode - prompts for template selection
	uvx browser-use init

	\b
	# Generate default template
	uvx browser-use init --template default

	\b
	# Generate advanced template with custom filename
	uvx browser-use init --template advanced --output my_script.py

	\b
	# List available templates
	uvx browser-use init --list
	"""

	# Handle --list flag
	if list_templates:
		click.echo('Available templates:\n')
		for name, info in INIT_TEMPLATES.items():
			click.echo(f'  {name:12} - {info["description"]}')
		return

	# Interactive template selection if not provided
	if not template:
		click.echo('Available templates:\n')
		for name, info in INIT_TEMPLATES.items():
			click.echo(f'  {name:12} - {info["description"]}')
		click.echo()

		template = click.prompt(
			'Which template would you like to use?',
			type=click.Choice(['default', 'advanced', 'tools'], case_sensitive=False),
			default='default',
		)

	# Template is guaranteed to be set at this point (either from option or prompt)
	assert template is not None

	# Determine output path
	if output:
		output_path = Path(output)
	else:
		output_path = Path.cwd() / f'browser_use_{template}.py'

	# Read template file
	try:
		templates_dir = Path(__file__).parent / 'cli_templates'
		template_file = INIT_TEMPLATES[template]['file']
		template_path = templates_dir / template_file
		content = template_path.read_text(encoding='utf-8')
	except Exception as e:
		click.echo(f'âŒ Error reading template: {e}', err=True)
		sys.exit(1)

	# Write file
	if _write_init_file(output_path, content, force):
		click.echo(f'âœ… Created {output_path}')
		click.echo('\nNext steps:')
		click.echo('  1. Install browser-use:')
		click.echo('     uv pip install browser-use')
		click.echo('  2. Set up your API key in .env file or environment:')
		click.echo('     BROWSER_USE_API_KEY=your-key')
		click.echo('     (Get your key at https://cloud.browser-use.com/new-api-key)')
		click.echo('  3. Run your script:')
		click.echo(f'     python {output_path.name}')
	else:
		sys.exit(1)


if __name__ == '__main__':
	main()

```

---

## backend/browser-use/browser_use/code_use/__init__.py

```py
"""Code-use mode - Jupyter notebook-like code execution for browser automation."""

from browser_use.code_use.namespace import create_namespace
from browser_use.code_use.notebook_export import export_to_ipynb, session_to_python_script
from browser_use.code_use.service import CodeAgent
from browser_use.code_use.views import CodeCell, ExecutionStatus, NotebookSession

__all__ = [
	'CodeAgent',
	'create_namespace',
	'export_to_ipynb',
	'session_to_python_script',
	'CodeCell',
	'ExecutionStatus',
	'NotebookSession',
]

```

---

## backend/browser-use/browser_use/code_use/formatting.py

```py
"""Browser state formatting helpers for code-use agent."""

import logging
from typing import Any

from browser_use.browser.session import BrowserSession
from browser_use.browser.views import BrowserStateSummary

logger = logging.getLogger(__name__)


async def format_browser_state_for_llm(
	state: BrowserStateSummary,
	namespace: dict[str, Any],
	browser_session: BrowserSession,
) -> str:
	"""
	Format browser state summary for LLM consumption in code-use mode.

	Args:
		state: Browser state summary from browser_session.get_browser_state_summary()
		namespace: The code execution namespace (for showing available variables)
		browser_session: Browser session for additional checks (jQuery, etc.)

	Returns:
		Formatted browser state text for LLM
	"""
	assert state.dom_state is not None
	dom_state = state.dom_state

	# Use eval_representation (compact serializer for code agents)
	dom_html = dom_state.eval_representation()
	if dom_html == '':
		dom_html = 'Empty DOM tree (you might have to wait for the page to load)'

	# Format with URL and title header
	lines = ['## Browser State']
	lines.append(f'**URL:** {state.url}')
	lines.append(f'**Title:** {state.title}')
	lines.append('')

	# Add tabs info if multiple tabs exist
	if len(state.tabs) > 1:
		lines.append('**Tabs:**')
		current_target_candidates = []
		# Find tabs that match current URL and title
		for tab in state.tabs:
			if tab.url == state.url and tab.title == state.title:
				current_target_candidates.append(tab.target_id)
		current_target_id = current_target_candidates[0] if len(current_target_candidates) == 1 else None

		for tab in state.tabs:
			is_current = ' (current)' if tab.target_id == current_target_id else ''
			lines.append(f'  - Tab {tab.target_id[-4:]}: {tab.url} - {tab.title[:30]}{is_current}')
		lines.append('')

	# Add page scroll info if available
	if state.page_info:
		pi = state.page_info
		pages_above = pi.pixels_above / pi.viewport_height if pi.viewport_height > 0 else 0
		pages_below = pi.pixels_below / pi.viewport_height if pi.viewport_height > 0 else 0
		total_pages = pi.page_height / pi.viewport_height if pi.viewport_height > 0 else 0

		scroll_info = f'**Page:** {pages_above:.1f} pages above, {pages_below:.1f} pages below'
		if total_pages > 1.2:  # Only mention total if significantly > 1 page
			scroll_info += f', {total_pages:.1f} total pages'
		lines.append(scroll_info)
		lines.append('')

	# Add network loading info if there are pending requests
	if state.pending_network_requests:
		# Remove duplicates by URL (keep first occurrence with earliest duration)
		seen_urls = set()
		unique_requests = []
		for req in state.pending_network_requests:
			if req.url not in seen_urls:
				seen_urls.add(req.url)
				unique_requests.append(req)

		lines.append(f'**â³ Loading:** {len(unique_requests)} network requests still loading')
		# Show up to 20 unique requests with truncated URLs (30 chars max)
		for req in unique_requests[:20]:
			duration_sec = req.loading_duration_ms / 1000
			url_display = req.url if len(req.url) <= 30 else req.url[:27] + '...'
			logger.info(f'  - [{duration_sec:.1f}s] {url_display}')
			lines.append(f'  - [{duration_sec:.1f}s] {url_display}')
		if len(unique_requests) > 20:
			lines.append(f'  - ... and {len(unique_requests) - 20} more')
		lines.append('**Tip:** Content may still be loading. Consider waiting with `await asyncio.sleep(1)` if data is missing.')
		lines.append('')

	# Add available variables and functions BEFORE DOM structure
	# Show useful utilities (json, asyncio, etc.) and user-defined vars, but hide system objects
	skip_vars = {
		'browser',
		'file_system',  # System objects
		'np',
		'pd',
		'plt',
		'numpy',
		'pandas',
		'matplotlib',
		'requests',
		'BeautifulSoup',
		'bs4',
		'pypdf',
		'PdfReader',
		'wait',
	}

	# Highlight code block variables separately from regular variables
	code_block_vars = []
	regular_vars = []
	tracked_code_blocks = namespace.get('_code_block_vars', set())
	for name in namespace.keys():
		# Skip private vars and system objects/actions
		if not name.startswith('_') and name not in skip_vars:
			if name in tracked_code_blocks:
				code_block_vars.append(name)
			else:
				regular_vars.append(name)

	# Sort for consistent display
	available_vars_sorted = sorted(regular_vars)
	code_block_vars_sorted = sorted(code_block_vars)

	# Build available line with code blocks and variables
	parts = []
	if code_block_vars_sorted:
		# Show detailed info for code block variables
		code_block_details = []
		for var_name in code_block_vars_sorted:
			value = namespace.get(var_name)
			if value is not None:
				type_name = type(value).__name__
				value_str = str(value) if not isinstance(value, str) else value

				# Check if it's a function (starts with "(function" or "(async function")
				is_function = value_str.strip().startswith('(function') or value_str.strip().startswith('(async function')

				if is_function:
					# For functions, only show name and type
					detail = f'{var_name}({type_name})'
				else:
					# For non-functions, show first and last 20 chars
					first_20 = value_str[:20].replace('\n', '\\n').replace('\t', '\\t')
					last_20 = value_str[-20:].replace('\n', '\\n').replace('\t', '\\t') if len(value_str) > 20 else ''

					if last_20 and first_20 != last_20:
						detail = f'{var_name}({type_name}): "{first_20}...{last_20}"'
					else:
						detail = f'{var_name}({type_name}): "{first_20}"'
				code_block_details.append(detail)

		parts.append(f'**Code block variables:** {" | ".join(code_block_details)}')
	if available_vars_sorted:
		parts.append(f'**Variables:** {", ".join(available_vars_sorted)}')

	lines.append(f'**Available:** {" | ".join(parts)}')
	lines.append('')

	# Add DOM structure
	lines.append('**DOM Structure:**')

	# Add scroll position hints for DOM
	if state.page_info:
		pi = state.page_info
		pages_above = pi.pixels_above / pi.viewport_height if pi.viewport_height > 0 else 0
		pages_below = pi.pixels_below / pi.viewport_height if pi.viewport_height > 0 else 0

		if pages_above > 0:
			dom_html = f'... {pages_above:.1f} pages above \n{dom_html}'
		else:
			dom_html = '[Start of page]\n' + dom_html

		if pages_below <= 0:
			dom_html += '\n[End of page]'

	# Truncate DOM if too long and notify LLM
	max_dom_length = 60000
	if len(dom_html) > max_dom_length:
		lines.append(dom_html[:max_dom_length])
		lines.append(
			f'\n[DOM truncated after {max_dom_length} characters. Full page contains {len(dom_html)} characters total. Use evaluate to explore more.]'
		)
	else:
		lines.append(dom_html)

	browser_state_text = '\n'.join(lines)
	return browser_state_text

```

---

## backend/browser-use/browser_use/code_use/namespace.py

```py
"""Namespace initialization for code-use mode.

This module creates a namespace with all browser tools available as functions,
similar to a Jupyter notebook environment.
"""

import asyncio
import csv
import datetime
import json
import logging
import re
from pathlib import Path
from typing import Any

import requests

from browser_use.browser import BrowserSession
from browser_use.filesystem.file_system import FileSystem
from browser_use.llm.base import BaseChatModel
from browser_use.tools.service import CodeAgentTools, Tools

logger = logging.getLogger(__name__)

# Try to import optional data science libraries
try:
	import numpy as np  # type: ignore

	NUMPY_AVAILABLE = True
except ImportError:
	NUMPY_AVAILABLE = False

try:
	import pandas as pd  # type: ignore

	PANDAS_AVAILABLE = True
except ImportError:
	PANDAS_AVAILABLE = False

try:
	import matplotlib.pyplot as plt  # type: ignore

	MATPLOTLIB_AVAILABLE = True
except ImportError:
	MATPLOTLIB_AVAILABLE = False

try:
	from bs4 import BeautifulSoup  # type: ignore

	BS4_AVAILABLE = True
except ImportError:
	BS4_AVAILABLE = False

try:
	from pypdf import PdfReader  # type: ignore

	PYPDF_AVAILABLE = True
except ImportError:
	PYPDF_AVAILABLE = False

try:
	from tabulate import tabulate  # type: ignore

	TABULATE_AVAILABLE = True
except ImportError:
	TABULATE_AVAILABLE = False


def _strip_js_comments(js_code: str) -> str:
	"""
	Remove JavaScript comments before CDP evaluation.
	CDP's Runtime.evaluate doesn't handle comments in all contexts.

	Args:
		js_code: JavaScript code potentially containing comments

	Returns:
		JavaScript code with comments stripped
	"""
	# Remove multi-line comments (/* ... */)
	js_code = re.sub(r'/\*.*?\*/', '', js_code, flags=re.DOTALL)

	# Remove single-line comments - only lines that START with // (after whitespace)
	# This avoids breaking XPath strings, URLs, regex patterns, etc.
	js_code = re.sub(r'^\s*//.*$', '', js_code, flags=re.MULTILINE)

	return js_code


class EvaluateError(Exception):
	"""Special exception raised by evaluate() to stop Python execution immediately."""

	pass


async def validate_task_completion(
	task: str,
	output: str | None,
	llm: BaseChatModel,
) -> tuple[bool, str]:
	"""
	Validate if task is truly complete by asking LLM without system prompt or history.

	Args:
		task: The original task description
		output: The output from the done() call
		llm: The LLM to use for validation

	Returns:
		Tuple of (is_complete, reasoning)
	"""
	from browser_use.llm.messages import UserMessage

	# Build validation prompt
	validation_prompt = f"""You are a task completion validator. Analyze if the agent has truly completed the user's task.

**Original Task:**
{task}

**Agent's Output:**
{output[:100000] if output else '(No output provided)'}

**Your Task:**
Determine if the agent has successfully completed the user's task. Consider:
1. Has the agent delivered what the user requested?
2. If data extraction was requested, is there actual data?
3. If the task is impossible (e.g., localhost website, login required but no credentials), is it truly impossible?
4. Could the agent continue and make meaningful progress?

**Response Format:**
Reasoning: [Your analysis of whether the task is complete]
Verdict: [YES or NO]

YES = Task is complete OR truly impossible to complete
NO = Agent should continue working"""

	try:
		# Call LLM with just the validation prompt (no system prompt, no history)
		response = await llm.ainvoke([UserMessage(content=validation_prompt)])
		response_text = response.completion

		# Parse the response
		reasoning = ''
		verdict = 'NO'

		# Extract reasoning and verdict
		lines = response_text.split('\n')
		for line in lines:
			if line.strip().lower().startswith('reasoning:'):
				reasoning = line.split(':', 1)[1].strip()
			elif line.strip().lower().startswith('verdict:'):
				verdict_text = line.split(':', 1)[1].strip().upper()
				if 'YES' in verdict_text:
					verdict = 'YES'
				elif 'NO' in verdict_text:
					verdict = 'NO'

		# If we couldn't parse, try to find YES/NO in the response
		if not reasoning:
			reasoning = response_text

		is_complete = verdict == 'YES'

		logger.info(f'Task validation: {verdict}')
		logger.debug(f'Validation reasoning: {reasoning}')

		return is_complete, reasoning

	except Exception as e:
		logger.warning(f'Failed to validate task completion: {e}')
		# On error, assume the agent knows what they're doing
		return True, f'Validation failed: {e}'


async def evaluate(code: str, browser_session: BrowserSession) -> Any:
	"""
	Execute JavaScript code in the browser and return the result.

	Args:
		code: JavaScript code to execute (must be wrapped in IIFE)

	Returns:
		The result of the JavaScript execution

	Raises:
		EvaluateError: If JavaScript execution fails. This stops Python execution immediately.

	Example:
		result = await evaluate('''
		(function(){
			return Array.from(document.querySelectorAll('.product')).map(p => ({
				name: p.querySelector('.name').textContent,
				price: p.querySelector('.price').textContent
			}))
		})()
		''')
	"""
	# Strip JavaScript comments before CDP evaluation (CDP doesn't support them in all contexts)
	code = _strip_js_comments(code)

	cdp_session = await browser_session.get_or_create_cdp_session()

	try:
		# Execute JavaScript with proper error handling
		result = await cdp_session.cdp_client.send.Runtime.evaluate(
			params={'expression': code, 'returnByValue': True, 'awaitPromise': True},
			session_id=cdp_session.session_id,
		)

		# Check for JavaScript execution errors
		if result.get('exceptionDetails'):
			exception = result['exceptionDetails']
			error_text = exception.get('text', 'Unknown error')

			# Try to get more details from the exception
			error_details = []
			if 'exception' in exception:
				exc_obj = exception['exception']
				if 'description' in exc_obj:
					error_details.append(exc_obj['description'])
				elif 'value' in exc_obj:
					error_details.append(str(exc_obj['value']))

			# Build comprehensive error message with full CDP context
			error_msg = f'JavaScript execution error: {error_text}'
			if error_details:
				error_msg += f'\nDetails: {" | ".join(error_details)}'

			# Raise special exception that will stop Python execution immediately
			raise EvaluateError(error_msg)

		# Get the result data
		result_data = result.get('result', {})

		# Get the actual value
		value = result_data.get('value')

		# Return the value directly
		if value is None:
			return None if 'value' in result_data else 'undefined'
		elif isinstance(value, (dict, list)):
			# Complex objects - already deserialized by returnByValue
			return value
		else:
			# Primitive values
			return value

	except EvaluateError:
		# Re-raise EvaluateError as-is to stop Python execution
		raise
	except Exception as e:
		# Wrap other exceptions in EvaluateError
		raise EvaluateError(f'Failed to execute JavaScript: {type(e).__name__}: {e}') from e


def create_namespace(
	browser_session: BrowserSession,
	tools: Tools | None = None,
	page_extraction_llm: BaseChatModel | None = None,
	file_system: FileSystem | None = None,
	available_file_paths: list[str] | None = None,
	sensitive_data: dict[str, str | dict[str, str]] | None = None,
) -> dict[str, Any]:
	"""
	Create a namespace with all browser tools available as functions.

	This function creates a dictionary of functions that can be used to interact
	with the browser, similar to a Jupyter notebook environment.

	Args:
		browser_session: The browser session to use
		tools: Optional Tools instance (will create default if not provided)
		page_extraction_llm: Optional LLM for page extraction
		file_system: Optional file system for file operations
		available_file_paths: Optional list of available file paths
		sensitive_data: Optional sensitive data dictionary

	Returns:
		Dictionary containing all available functions and objects

	Example:
		namespace = create_namespace(browser_session)
		await namespace['navigate'](url='https://google.com')
		result = await namespace['evaluate']('document.title')
	"""
	if tools is None:
		# Use CodeAgentTools with default exclusions optimized for code-use mode
		# For code-use, we keep: navigate, evaluate, wait, done
		# and exclude: most browser interaction, file system actions (use Python instead)
		tools = CodeAgentTools()

	if available_file_paths is None:
		available_file_paths = []

	namespace: dict[str, Any] = {
		# Core objects
		'browser': browser_session,
		'file_system': file_system,
		# Standard library modules (always available)
		'json': json,
		'asyncio': asyncio,
		'Path': Path,
		'csv': csv,
		're': re,
		'datetime': datetime,
		'requests': requests,
	}

	# Add optional data science libraries if available
	if NUMPY_AVAILABLE:
		namespace['np'] = np
		namespace['numpy'] = np
	if PANDAS_AVAILABLE:
		namespace['pd'] = pd
		namespace['pandas'] = pd
	if MATPLOTLIB_AVAILABLE:
		namespace['plt'] = plt
		namespace['matplotlib'] = plt
	if BS4_AVAILABLE:
		namespace['BeautifulSoup'] = BeautifulSoup
		namespace['bs4'] = BeautifulSoup
	if PYPDF_AVAILABLE:
		namespace['PdfReader'] = PdfReader
		namespace['pypdf'] = PdfReader
	if TABULATE_AVAILABLE:
		namespace['tabulate'] = tabulate

	# Track failed evaluate() calls to detect repeated failed approaches
	if '_evaluate_failures' not in namespace:
		namespace['_evaluate_failures'] = []

	# Add custom evaluate function that returns values directly
	async def evaluate_wrapper(
		code: str | None = None, variables: dict[str, Any] | None = None, *_args: Any, **kwargs: Any
	) -> Any:
		# Handle both positional and keyword argument styles
		if code is None:
			# Check if code was passed as keyword arg
			code = kwargs.get('code', kwargs.get('js_code', kwargs.get('expression', '')))
		# Extract variables if passed as kwarg
		if variables is None:
			variables = kwargs.get('variables')

		if not code:
			raise ValueError('No JavaScript code provided to evaluate()')

		# Inject variables if provided
		if variables:
			vars_json = json.dumps(variables)
			stripped = code.strip()

			# Check if code is already a function expression expecting params
			# Pattern: (function(params) { ... }) or (async function(params) { ... })
			if re.match(r'\((?:async\s+)?function\s*\(\s*\w+\s*\)', stripped):
				# Already expects params, wrap to call it with our variables
				code = f'(function(){{ const params = {vars_json}; return {stripped}(params); }})()'
			else:
				# Not a parameterized function, inject params in scope
				# Check if already wrapped in IIFE (including arrow function IIFEs)
				is_wrapped = (
					(stripped.startswith('(function()') and '})()' in stripped[-10:])
					or (stripped.startswith('(async function()') and '})()' in stripped[-10:])
					or (stripped.startswith('(() =>') and ')()' in stripped[-10:])
					or (stripped.startswith('(async () =>') and ')()' in stripped[-10:])
				)
				if is_wrapped:
					# Already wrapped, inject params at the start
					# Try to match regular function IIFE
					match = re.match(r'(\((?:async\s+)?function\s*\(\s*\)\s*\{)', stripped)
					if match:
						prefix = match.group(1)
						rest = stripped[len(prefix) :]
						code = f'{prefix} const params = {vars_json}; {rest}'
					else:
						# Try to match arrow function IIFE
						# Patterns: (() => expr)() or (() => { ... })() or (async () => ...)()
						arrow_match = re.match(r'(\((?:async\s+)?\(\s*\)\s*=>\s*\{)', stripped)
						if arrow_match:
							# Arrow function with block body: (() => { ... })()
							prefix = arrow_match.group(1)
							rest = stripped[len(prefix) :]
							code = f'{prefix} const params = {vars_json}; {rest}'
						else:
							# Arrow function with expression body or fallback: wrap in outer function
							code = f'(function(){{ const params = {vars_json}; return {stripped}; }})()'
				else:
					# Not wrapped, wrap with params
					code = f'(function(){{ const params = {vars_json}; {code} }})()'
					# Skip auto-wrap below
					return await evaluate(code, browser_session)

		# Auto-wrap in IIFE if not already wrapped (and no variables were injected)
		if not variables:
			stripped = code.strip()
			# Check for regular function IIFEs, async function IIFEs, and arrow function IIFEs
			is_wrapped = (
				(stripped.startswith('(function()') and '})()' in stripped[-10:])
				or (stripped.startswith('(async function()') and '})()' in stripped[-10:])
				or (stripped.startswith('(() =>') and ')()' in stripped[-10:])
				or (stripped.startswith('(async () =>') and ')()' in stripped[-10:])
			)
			if not is_wrapped:
				code = f'(function(){{{code}}})()'

		# Execute and track failures
		try:
			result = await evaluate(code, browser_session)

			# Print result structure for debugging
			if isinstance(result, list) and result and isinstance(result[0], dict):
				result_preview = f'list of dicts - len={len(result)}, example 1:\n'
				sample_result = result[0]
				for key, value in list(sample_result.items())[:10]:
					value_str = str(value)[:10] if not isinstance(value, (int, float, bool, type(None))) else str(value)
					result_preview += f'  {key}: {value_str}...\n'
				if len(sample_result) > 10:
					result_preview += f'  ... {len(sample_result) - 10} more keys'
				print(result_preview)

			elif isinstance(result, list):
				if len(result) == 0:
					print('type=list, len=0')
				else:
					result_preview = str(result)[:100]
					print(f'type=list, len={len(result)}, preview={result_preview}...')
			elif isinstance(result, dict):
				result_preview = f'type=dict, len={len(result)}, sample keys:\n'
				for key, value in list(result.items())[:10]:
					value_str = str(value)[:10] if not isinstance(value, (int, float, bool, type(None))) else str(value)
					result_preview += f'  {key}: {value_str}...\n'
				if len(result) > 10:
					result_preview += f'  ... {len(result) - 10} more keys'
				print(result_preview)

			else:
				print(f'type={type(result).__name__}, value={repr(result)[:50]}')

			return result
		except Exception as e:
			# Track errors for pattern detection
			namespace['_evaluate_failures'].append({'error': str(e), 'type': 'exception'})
			raise

	namespace['evaluate'] = evaluate_wrapper

	# Add get_selector_from_index helper for code_use mode
	async def get_selector_from_index_wrapper(index: int) -> str:
		"""
		Get the CSS selector for an element by its interactive index.

		This allows you to use the element's index from the browser state to get
		its CSS selector for use in JavaScript evaluate() calls.

		Args:
			index: The interactive index from the browser state (e.g., [123])

		Returns:
			str: CSS selector that can be used in JavaScript

		Example:
			selector = await get_selector_from_index(123)
			await evaluate(f'''
			(function(){{
				const el = document.querySelector({json.dumps(selector)});
				if (el) el.click();
			}})()
			''')
		"""
		from browser_use.dom.utils import generate_css_selector_for_element

		# Get element by index from browser session
		node = await browser_session.get_element_by_index(index)
		if node is None:
			msg = f'Element index {index} not available - page may have changed. Try refreshing browser state.'
			logger.warning(f'âš ï¸ {msg}')
			raise RuntimeError(msg)

		# Check if element is in shadow DOM
		shadow_hosts = []
		current = node.parent_node
		while current:
			if current.shadow_root_type is not None:
				# This is a shadow host
				host_tag = current.tag_name.lower()
				host_id = current.attributes.get('id', '') if current.attributes else ''
				host_desc = f'{host_tag}#{host_id}' if host_id else host_tag
				shadow_hosts.insert(0, host_desc)
			current = current.parent_node

		# Check if in iframe
		in_iframe = False
		current = node.parent_node
		while current:
			if current.tag_name.lower() == 'iframe':
				in_iframe = True
				break
			current = current.parent_node

		# Use the robust selector generation function (now handles special chars in IDs)
		selector = generate_css_selector_for_element(node)

		# Log shadow DOM/iframe info if detected
		if shadow_hosts:
			shadow_path = ' > '.join(shadow_hosts)
			logger.info(f'Element [{index}] is inside Shadow DOM. Path: {shadow_path}')
			logger.info(f'    Selector: {selector}')
			logger.info(
				f'    To access: document.querySelector("{shadow_hosts[0].split("#")[0]}").shadowRoot.querySelector("{selector}")'
			)
		if in_iframe:
			logger.info(f"Element [{index}] is inside an iframe. Regular querySelector won't work.")

		if selector:
			return selector

		# Fallback: just use tag name if available
		if node.tag_name:
			return node.tag_name.lower()

		raise ValueError(f'Could not generate selector for element index {index}')

	namespace['get_selector_from_index'] = get_selector_from_index_wrapper

	# Inject all tools as functions into the namespace
	# Skip 'evaluate' since we have a custom implementation above
	for action_name, action in tools.registry.registry.actions.items():
		if action_name == 'evaluate':
			continue  # Skip - use custom evaluate that returns Python objects directly
		param_model = action.param_model
		action_function = action.function

		# Create a closure to capture the current action_name, param_model, and action_function
		def make_action_wrapper(act_name, par_model, act_func):
			async def action_wrapper(*args, **kwargs):
				# Convert positional args to kwargs based on param model fields
				if args:
					# Get the field names from the pydantic model
					field_names = list(par_model.model_fields.keys())
					for i, arg in enumerate(args):
						if i < len(field_names):
							kwargs[field_names[i]] = arg

				# Create params from kwargs
				try:
					params = par_model(**kwargs)
				except Exception as e:
					raise ValueError(f'Invalid parameters for {act_name}: {e}') from e

				# Special validation for done() - enforce minimal code cell
				if act_name == 'done':
					consecutive_failures = namespace.get('_consecutive_errors')
					if consecutive_failures and consecutive_failures > 3:
						pass

					else:
						# Check if there are multiple Python blocks in this response
						all_blocks = namespace.get('_all_code_blocks', {})
						python_blocks = [k for k in sorted(all_blocks.keys()) if k.startswith('python_')]

						if len(python_blocks) > 1:
							msg = (
								'done() should be the ONLY code block in the response.\n'
								'You have multiple Python blocks in this response. Consider calling done() in a separate response '
								'Now verify the last output and if it satisfies the task, call done(), else continue working.'
							)
							print(msg)

						# Get the current cell code from namespace (injected by service.py before execution)
						current_code = namespace.get('_current_cell_code')
						if current_code and isinstance(current_code, str):
							# Count non-empty, non-comment lines
							lines = [line.strip() for line in current_code.strip().split('\n')]
							code_lines = [line for line in lines if line and not line.startswith('#')]

							# Check if the line above await done() contains an if block
							done_line_index = -1
							for i, line in enumerate(reversed(code_lines)):
								if 'await done()' in line or 'await done(' in line:
									done_line_index = len(code_lines) - 1 - i
									break

							has_if_above = False
							has_else_above = False
							has_elif_above = False
							if done_line_index > 0:
								line_above = code_lines[done_line_index - 1]
								has_if_above = line_above.strip().startswith('if ') and line_above.strip().endswith(':')
								has_else_above = line_above.strip().startswith('else:')
								has_elif_above = line_above.strip().startswith('elif ')
							if has_if_above or has_else_above or has_elif_above:
								msg = (
									'done() should be called individually after verifying the result from any logic.\n'
									'Consider validating your output first, THEN call done() in a final step without if/else/elif blocks only if the task is truly complete.'
								)
								logger.error(msg)
								print(msg)
								raise RuntimeError(msg)

				# Build special context
				special_context = {
					'browser_session': browser_session,
					'page_extraction_llm': page_extraction_llm,
					'available_file_paths': available_file_paths,
					'has_sensitive_data': False,  # Can be handled separately if needed
					'file_system': file_system,
				}

				# Execute the action
				result = await act_func(params=params, **special_context)

				# For code-use mode, we want to return the result directly
				# not wrapped in ActionResult
				if hasattr(result, 'extracted_content'):
					# Special handling for done action - mark task as complete
					if act_name == 'done' and hasattr(result, 'is_done') and result.is_done:
						namespace['_task_done'] = True
						# Store the extracted content as the final result
						if result.extracted_content:
							namespace['_task_result'] = result.extracted_content
						# Store the self-reported success status
						if hasattr(result, 'success'):
							namespace['_task_success'] = result.success

					# If there's extracted content, return it
					if result.extracted_content:
						return result.extracted_content
					# If there's an error, raise it
					if result.error:
						raise RuntimeError(result.error)
					# Otherwise return None
					return None
				return result

			return action_wrapper

		# Rename 'input' to 'input_text' to avoid shadowing Python's built-in input()
		namespace_action_name = 'input_text' if action_name == 'input' else action_name

		# Add the wrapper to the namespace
		namespace[namespace_action_name] = make_action_wrapper(action_name, param_model, action_function)

	return namespace


def get_namespace_documentation(namespace: dict[str, Any]) -> str:
	"""
	Generate documentation for all available functions in the namespace.

	Args:
		namespace: The namespace dictionary

	Returns:
		Markdown-formatted documentation string
	"""
	docs = ['# Available Functions\n']

	# Document each function
	for name, obj in sorted(namespace.items()):
		if callable(obj) and not name.startswith('_'):
			# Get function signature and docstring
			if hasattr(obj, '__doc__') and obj.__doc__:
				docs.append(f'## {name}\n')
				docs.append(f'{obj.__doc__}\n')

	return '\n'.join(docs)

```

---

## backend/browser-use/browser_use/code_use/notebook_export.py

```py
"""Export code-use session to Jupyter notebook format."""

import json
import re
from pathlib import Path

from browser_use.code_use.service import CodeAgent

from .views import CellType, NotebookExport


def export_to_ipynb(agent: CodeAgent, output_path: str | Path) -> Path:
	"""
	Export a NotebookSession to a Jupyter notebook (.ipynb) file.
	Now includes JavaScript code blocks that were stored in the namespace.

	Args:
		session: The NotebookSession to export
		output_path: Path where to save the notebook file
		agent: Optional CodeAgent instance to access namespace for JavaScript blocks

	Returns:
		Path to the saved notebook file

	Example:
		``\`python
	        session = await agent.run()
	        notebook_path = export_to_ipynb(agent, 'my_automation.ipynb')
	        print(f'Notebook saved to {notebook_path}')
		``\`
	"""
	output_path = Path(output_path)

	# Create notebook structure
	notebook = NotebookExport(
		metadata={
			'kernelspec': {'display_name': 'Python 3', 'language': 'python', 'name': 'python3'},
			'language_info': {
				'name': 'python',
				'version': '3.11.0',
				'mimetype': 'text/x-python',
				'codemirror_mode': {'name': 'ipython', 'version': 3},
				'pygments_lexer': 'ipython3',
				'nbconvert_exporter': 'python',
				'file_extension': '.py',
			},
		}
	)

	# Add setup cell at the beginning with proper type hints
	setup_code = """import asyncio
import json
from typing import Any
from browser_use import BrowserSession
from browser_use.code_use import create_namespace

# Initialize browser and namespace
browser = BrowserSession()
await browser.start()

# Create namespace with all browser control functions
namespace: dict[str, Any] = create_namespace(browser)

# Import all functions into the current namespace
globals().update(namespace)

# Type hints for better IDE support (these are now available globally)
# navigate, click, input, evaluate, search, extract, scroll, done, etc.

print("Browser-use environment initialized!")
print("Available functions: navigate, click, input, evaluate, search, extract, done, etc.")"""

	setup_cell = {
		'cell_type': 'code',
		'metadata': {},
		'source': setup_code.split('\n'),
		'execution_count': None,
		'outputs': [],
	}
	notebook.cells.append(setup_cell)

	# Add JavaScript code blocks as variables FIRST
	if hasattr(agent, 'namespace') and agent.namespace:
		# Look for JavaScript variables in the namespace
		code_block_vars = agent.namespace.get('_code_block_vars', set())

		for var_name in sorted(code_block_vars):
			var_value = agent.namespace.get(var_name)
			if isinstance(var_value, str) and var_value.strip():
				# Check if this looks like JavaScript code
				# Look for common JS patterns
				js_patterns = [
					r'function\s+\w+\s*\(',
					r'\(\s*function\s*\(\)',
					r'=>\s*{',
					r'document\.',
					r'Array\.from\(',
					r'\.querySelector',
					r'\.textContent',
					r'\.innerHTML',
					r'return\s+',
					r'console\.log',
					r'window\.',
					r'\.map\(',
					r'\.filter\(',
					r'\.forEach\(',
				]

				is_js = any(re.search(pattern, var_value, re.IGNORECASE) for pattern in js_patterns)

				if is_js:
					# Create a code cell with the JavaScript variable
					js_cell = {
						'cell_type': 'code',
						'metadata': {},
						'source': [f'# JavaScript Code Block: {var_name}\n', f'{var_name} = """{var_value}"""'],
						'execution_count': None,
						'outputs': [],
					}
					notebook.cells.append(js_cell)

	# Convert cells
	python_cell_count = 0
	for cell in agent.session.cells:
		notebook_cell: dict = {
			'cell_type': cell.cell_type.value,
			'metadata': {},
			'source': cell.source.splitlines(keepends=True),
		}

		if cell.cell_type == CellType.CODE:
			python_cell_count += 1
			notebook_cell['execution_count'] = cell.execution_count
			notebook_cell['outputs'] = []

			# Add output if available
			if cell.output:
				notebook_cell['outputs'].append(
					{
						'output_type': 'stream',
						'name': 'stdout',
						'text': cell.output.split('\n'),
					}
				)

			# Add error if available
			if cell.error:
				notebook_cell['outputs'].append(
					{
						'output_type': 'error',
						'ename': 'Error',
						'evalue': cell.error.split('\n')[0] if cell.error else '',
						'traceback': cell.error.split('\n') if cell.error else [],
					}
				)

			# Add browser state as a separate output
			if cell.browser_state:
				notebook_cell['outputs'].append(
					{
						'output_type': 'stream',
						'name': 'stdout',
						'text': [f'Browser State:\n{cell.browser_state}'],
					}
				)

		notebook.cells.append(notebook_cell)

	# Write to file
	output_path.parent.mkdir(parents=True, exist_ok=True)
	with open(output_path, 'w', encoding='utf-8') as f:
		json.dump(notebook.model_dump(), f, indent=2, ensure_ascii=False)

	return output_path


def session_to_python_script(agent: CodeAgent) -> str:
	"""
	Convert a CodeAgent session to a Python script.
	Now includes JavaScript code blocks that were stored in the namespace.

	Args:
		agent: The CodeAgent instance to convert

	Returns:
		Python script as a string

	Example:
		``\`python
	        await agent.run()
	        script = session_to_python_script(agent)
	        print(script)
		``\`
	"""
	lines = []

	lines.append('# Generated from browser-use code-use session\n')
	lines.append('import asyncio\n')
	lines.append('import json\n')
	lines.append('from browser_use import BrowserSession\n')
	lines.append('from browser_use.code_use import create_namespace\n\n')

	lines.append('async def main():\n')
	lines.append('\t# Initialize browser and namespace\n')
	lines.append('\tbrowser = BrowserSession()\n')
	lines.append('\tawait browser.start()\n\n')
	lines.append('\t# Create namespace with all browser control functions\n')
	lines.append('\tnamespace = create_namespace(browser)\n\n')
	lines.append('\t# Extract functions from namespace for direct access\n')
	lines.append('\tnavigate = namespace["navigate"]\n')
	lines.append('\tclick = namespace["click"]\n')
	lines.append('\tinput_text = namespace["input"]\n')
	lines.append('\tevaluate = namespace["evaluate"]\n')
	lines.append('\tsearch = namespace["search"]\n')
	lines.append('\textract = namespace["extract"]\n')
	lines.append('\tscroll = namespace["scroll"]\n')
	lines.append('\tdone = namespace["done"]\n')
	lines.append('\tgo_back = namespace["go_back"]\n')
	lines.append('\twait = namespace["wait"]\n')
	lines.append('\tscreenshot = namespace["screenshot"]\n')
	lines.append('\tfind_text = namespace["find_text"]\n')
	lines.append('\tswitch_tab = namespace["switch"]\n')
	lines.append('\tclose_tab = namespace["close"]\n')
	lines.append('\tdropdown_options = namespace["dropdown_options"]\n')
	lines.append('\tselect_dropdown = namespace["select_dropdown"]\n')
	lines.append('\tupload_file = namespace["upload_file"]\n')
	lines.append('\tsend_keys = namespace["send_keys"]\n\n')

	# Add JavaScript code blocks as variables FIRST
	if hasattr(agent, 'namespace') and agent.namespace:
		code_block_vars = agent.namespace.get('_code_block_vars', set())

		for var_name in sorted(code_block_vars):
			var_value = agent.namespace.get(var_name)
			if isinstance(var_value, str) and var_value.strip():
				# Check if this looks like JavaScript code
				js_patterns = [
					r'function\s+\w+\s*\(',
					r'\(\s*function\s*\(\)',
					r'=>\s*{',
					r'document\.',
					r'Array\.from\(',
					r'\.querySelector',
					r'\.textContent',
					r'\.innerHTML',
					r'return\s+',
					r'console\.log',
					r'window\.',
					r'\.map\(',
					r'\.filter\(',
					r'\.forEach\(',
				]

				is_js = any(re.search(pattern, var_value, re.IGNORECASE) for pattern in js_patterns)

				if is_js:
					lines.append(f'\t# JavaScript Code Block: {var_name}\n')
					lines.append(f'\t{var_name} = """{var_value}"""\n\n')

	for i, cell in enumerate(agent.session.cells):
		if cell.cell_type == CellType.CODE:
			lines.append(f'\t# Cell {i + 1}\n')

			# Indent each line of source
			source_lines = cell.source.split('\n')
			for line in source_lines:
				if line.strip():  # Only add non-empty lines
					lines.append(f'\t{line}\n')

			lines.append('\n')

	lines.append('\tawait browser.stop()\n\n')
	lines.append("if __name__ == '__main__':\n")
	lines.append('\tasyncio.run(main())\n')

	return ''.join(lines)

```

---

## backend/browser-use/browser_use/code_use/service.py

```py
"""Code-use agent service - Jupyter notebook-like code execution for browser automation."""

import asyncio
import datetime
import html
import json
import logging
import re
import traceback
from pathlib import Path
from typing import Any

from uuid_extensions import uuid7str

from browser_use.browser import BrowserSession
from browser_use.browser.profile import BrowserProfile
from browser_use.dom.service import DomService
from browser_use.filesystem.file_system import FileSystem
from browser_use.llm.base import BaseChatModel
from browser_use.llm.messages import (
	AssistantMessage,
	BaseMessage,
	ContentPartImageParam,
	ContentPartTextParam,
	ImageURL,
	UserMessage,
)
from browser_use.screenshots.service import ScreenshotService
from browser_use.telemetry.service import ProductTelemetry
from browser_use.telemetry.views import AgentTelemetryEvent
from browser_use.tokens.service import TokenCost
from browser_use.tokens.views import UsageSummary
from browser_use.tools.service import CodeAgentTools, Tools
from browser_use.utils import get_browser_use_version

from .formatting import format_browser_state_for_llm
from .namespace import EvaluateError, create_namespace
from .utils import detect_token_limit_issue, extract_code_blocks, extract_url_from_task, truncate_message_content
from .views import (
	CellType,
	CodeAgentHistory,
	CodeAgentHistoryList,
	CodeAgentModelOutput,
	CodeAgentResult,
	CodeAgentState,
	CodeAgentStepMetadata,
	ExecutionStatus,
	NotebookSession,
)

logger = logging.getLogger(__name__)


class CodeAgent:
	"""
	Agent that executes Python code in a notebook-like environment for browser automation.

	This agent provides a Jupyter notebook-like interface where the LLM writes Python code
	that gets executed in a persistent namespace with browser control functions available.
	"""

	def __init__(
		self,
		task: str,
		# Optional parameters
		llm: BaseChatModel | None = None,
		browser_session: BrowserSession | None = None,
		browser: BrowserSession | None = None,  # Alias for browser_session
		tools: Tools | None = None,
		controller: Tools | None = None,  # Alias for tools
		# Agent settings
		page_extraction_llm: BaseChatModel | None = None,
		file_system: FileSystem | None = None,
		available_file_paths: list[str] | None = None,
		sensitive_data: dict[str, str | dict[str, str]] | None = None,
		max_steps: int = 100,
		max_failures: int = 8,
		max_validations: int = 0,
		use_vision: bool = True,
		calculate_cost: bool = False,
		demo_mode: bool | None = None,
		**kwargs,
	):
		"""
		Initialize the code-use agent.

		Args:
			task: The task description for the agent
			browser_session: Optional browser session (will be created if not provided) [DEPRECATED: use browser]
			browser: Optional browser session (cleaner API)
			tools: Optional Tools instance (will create default if not provided)
			controller: Optional Tools instance
			page_extraction_llm: Optional LLM for page extraction
			file_system: Optional file system for file operations
			available_file_paths: Optional list of available file paths
			sensitive_data: Optional sensitive data dictionary
			max_steps: Maximum number of execution steps
			max_failures: Maximum consecutive errors before termination (default: 8)
			max_validations: Maximum number of times to run the validator agent (default: 0)
			use_vision: Whether to include screenshots in LLM messages (default: True)
			calculate_cost: Whether to calculate token costs (default: False)
			demo_mode: Enable the in-browser demo panel for live logging (default: False)
			llm: Optional ChatBrowserUse LLM instance (will create default if not provided)
			**kwargs: Additional keyword arguments for compatibility (ignored)
		"""
		# Log and ignore unknown kwargs for compatibility
		if kwargs:
			logger.debug(f'Ignoring additional kwargs for CodeAgent compatibility: {list(kwargs.keys())}')

		if llm is None:
			try:
				from browser_use import ChatBrowserUse

				llm = ChatBrowserUse()
				logger.debug('CodeAgent using ChatBrowserUse')
			except Exception as e:
				raise RuntimeError(f'Failed to initialize CodeAgent LLM: {e}')

		if 'ChatBrowserUse' not in llm.__class__.__name__:
			raise ValueError('This agent works only with ChatBrowserUse.')

		# Handle browser vs browser_session parameter (browser takes precedence)
		if browser and browser_session:
			raise ValueError('Cannot specify both "browser" and "browser_session" parameters. Use "browser" for the cleaner API.')
		browser_session = browser or browser_session

		# Handle controller vs tools parameter (controller takes precedence)
		if controller and tools:
			raise ValueError('Cannot specify both "controller" and "tools" parameters. Use "controller" for the cleaner API.')
		tools = controller or tools

		# Store browser_profile for creating browser session if needed
		self._demo_mode_enabled = False
		if browser_session is None:
			profile_kwargs: dict[str, Any] = {}
			if demo_mode is not None:
				profile_kwargs['demo_mode'] = demo_mode
			self._browser_profile_for_init = BrowserProfile(**profile_kwargs)
		else:
			self._browser_profile_for_init = None

		self.task = task
		self.llm = llm
		self.browser_session = browser_session
		if self.browser_session:
			if demo_mode is not None and self.browser_session.browser_profile.demo_mode != demo_mode:
				self.browser_session.browser_profile = self.browser_session.browser_profile.model_copy(
					update={'demo_mode': demo_mode}
				)
			self._demo_mode_enabled = bool(self.browser_session.browser_profile.demo_mode)
		self.tools = tools or CodeAgentTools()
		self.page_extraction_llm = page_extraction_llm
		self.file_system = file_system if file_system is not None else FileSystem(base_dir='./')
		self.available_file_paths = available_file_paths or []
		self.sensitive_data = sensitive_data
		self.max_steps = max_steps
		self.max_failures = max_failures
		self.max_validations = max_validations
		self.use_vision = use_vision

		self.session = NotebookSession()
		self.namespace: dict[str, Any] = {}
		self._llm_messages: list[BaseMessage] = []  # Internal LLM conversation history
		self.complete_history: list[CodeAgentHistory] = []  # Type-safe history with model_output and result
		self.dom_service: DomService | None = None
		self._last_browser_state_text: str | None = None  # Track last browser state text
		self._last_screenshot: str | None = None  # Track last screenshot (base64)
		self._consecutive_errors = 0  # Track consecutive errors for auto-termination
		self._validation_count = 0  # Track number of validator runs
		self._last_llm_usage: Any | None = None  # Track last LLM call usage stats
		self._step_start_time = 0.0  # Track step start time for duration calculation
		self.usage_summary: UsageSummary | None = None  # Track usage summary across run for history property
		self._sample_output_added = False  # Track whether preview cell already created

		# Initialize screenshot service for eval tracking
		self.id = uuid7str()
		timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
		base_tmp = Path('/tmp')
		self.agent_directory = base_tmp / f'browser_use_code_agent_{self.id}_{timestamp}'
		self.screenshot_service = ScreenshotService(agent_directory=self.agent_directory)

		# Initialize token cost service for usage tracking
		self.token_cost_service = TokenCost(include_cost=calculate_cost)
		self.token_cost_service.register_llm(llm)
		if page_extraction_llm:
			self.token_cost_service.register_llm(page_extraction_llm)

		# Set version and source for telemetry
		self.version = get_browser_use_version()
		try:
			package_root = Path(__file__).parent.parent.parent
			repo_files = ['.git', 'README.md', 'docs', 'examples']
			if all(Path(package_root / file).exists() for file in repo_files):
				self.source = 'git'
			else:
				self.source = 'pip'
		except Exception:
			self.source = 'unknown'

		# Telemetry
		self.telemetry = ProductTelemetry()

	async def run(self, max_steps: int | None = None) -> NotebookSession:
		"""
		Run the agent to complete the task.

		Args:
			max_steps: Optional override for maximum number of steps (uses __init__ value if not provided)

		Returns:
			The notebook session with all executed cells
		"""
		# Use override if provided, otherwise use value from __init__
		steps_to_run = max_steps if max_steps is not None else self.max_steps
		self.max_steps = steps_to_run
		# Start browser if not provided
		if self.browser_session is None:
			assert self._browser_profile_for_init is not None
			self.browser_session = BrowserSession(browser_profile=self._browser_profile_for_init)
			await self.browser_session.start()

		if self.browser_session:
			self._demo_mode_enabled = bool(self.browser_session.browser_profile.demo_mode)
			if self._demo_mode_enabled and getattr(self.browser_session.browser_profile, 'headless', False):
				logger.warning('Demo mode is enabled but the browser is headless=True; set headless=False to view the panel.')
			if self._demo_mode_enabled:
				await self._demo_mode_log(f'Started CodeAgent task: {self.task}', 'info', {'tag': 'task'})

		# Initialize DOM service with cross-origin iframe support enabled
		self.dom_service = DomService(
			browser_session=self.browser_session,
			cross_origin_iframes=True,  # Enable for code-use agent to access forms in iframes
		)

		# Create namespace with all tools
		self.namespace = create_namespace(
			browser_session=self.browser_session,
			tools=self.tools,
			page_extraction_llm=self.page_extraction_llm,
			file_system=self.file_system,
			available_file_paths=self.available_file_paths,
			sensitive_data=self.sensitive_data,
		)

		# Initialize conversation with task
		self._llm_messages.append(UserMessage(content=f'Task: {self.task}'))

		# Track agent run error for telemetry
		agent_run_error: str | None = None
		should_delay_close = False

		# Extract URL from task and navigate if found
		initial_url = extract_url_from_task(self.task)
		if initial_url:
			try:
				logger.info(f'Extracted URL from task, navigating to: {initial_url}')
				# Use the navigate action from namespace
				await self.namespace['navigate'](initial_url)
				# Wait for page load
				await asyncio.sleep(2)

				# Record this navigation as a cell in the notebook
				nav_code = f"await navigate('{initial_url}')"
				cell = self.session.add_cell(source=nav_code)
				cell.status = ExecutionStatus.SUCCESS
				cell.execution_count = self.session.increment_execution_count()
				cell.output = f'Navigated to {initial_url}'

				# Get browser state after navigation for the cell
				if self.dom_service:
					try:
						browser_state_text, _ = await self._get_browser_state()
						cell.browser_state = browser_state_text
					except Exception as state_error:
						logger.debug(f'Failed to capture browser state for initial navigation cell: {state_error}')

			except Exception as e:
				logger.warning(f'Failed to navigate to extracted URL {initial_url}: {e}')
				# Record failed navigation as error cell
				nav_code = f"await navigate('{initial_url}')"
				cell = self.session.add_cell(source=nav_code)
				cell.status = ExecutionStatus.ERROR
				cell.execution_count = self.session.increment_execution_count()
				cell.error = str(e)

		# Get initial browser state before first LLM call
		if self.browser_session and self.dom_service:
			try:
				browser_state_text, screenshot = await self._get_browser_state()
				self._last_browser_state_text = browser_state_text
				self._last_screenshot = screenshot
			except Exception as e:
				logger.warning(f'Failed to get initial browser state: {e}')

		# Main execution loop
		for step in range(self.max_steps):
			logger.info(f'\n\n\n\n\n\n\nStep {step + 1}/{self.max_steps}')

			# Start timing this step
			self._step_start_time = datetime.datetime.now().timestamp()

			# Check if we're approaching the step limit or error limit and inject warning
			steps_remaining = self.max_steps - step - 1
			errors_remaining = self.max_failures - self._consecutive_errors

			should_warn = (
				steps_remaining <= 1  # Last step or next to last
				or errors_remaining <= 1  # One more error will terminate
				or (steps_remaining <= 2 and self._consecutive_errors >= 2)  # Close to both limits
			)

			if should_warn:
				warning_message = (
					f'\n\nâš ï¸ CRITICAL WARNING: You are approaching execution limits!\n'
					f'- Steps remaining: {steps_remaining + 1}\n'
					f'- Consecutive errors: {self._consecutive_errors}/{self.max_failures}\n\n'
					f'YOU MUST call done() in your NEXT response, even if the task is incomplete:\n'
					f"- Set success=False if you couldn't complete the task\n"
					f'- Return EVERYTHING you found so far (partial data is better than nothing)\n'
					f"- Include any variables you've stored (products, all_data, etc.)\n"
					f"- Explain what worked and what didn't\n\n"
					f'Without done(), the user will receive NOTHING.'
				)
				self._llm_messages.append(UserMessage(content=warning_message))

			try:
				# Fetch fresh browser state right before LLM call (only if not already set)
				if not self._last_browser_state_text and self.browser_session and self.dom_service:
					try:
						logger.debug('ğŸ” Fetching browser state before LLM call...')
						browser_state_text, screenshot = await self._get_browser_state()
						self._last_browser_state_text = browser_state_text
						self._last_screenshot = screenshot

						# # Log browser state
						# if len(browser_state_text) > 2000:
						# 	logger.info(
						# 		f'Browser state (before LLM):\n{browser_state_text[:2000]}...\n[Truncated, full state {len(browser_state_text)} chars sent to LLM]'
						# 	)
						# else:
						# 	logger.info(f'Browser state (before LLM):\n{browser_state_text}')
					except Exception as e:
						logger.warning(f'Failed to get browser state before LLM call: {e}')

				# Get code from LLM (this also adds to self._llm_messages)
				try:
					code, full_llm_response = await self._get_code_from_llm(step_number=step + 1)
				except Exception as llm_error:
					# LLM call failed - count as consecutive error and retry
					self._consecutive_errors += 1
					logger.warning(
						f'LLM call failed (consecutive errors: {self._consecutive_errors}/{self.max_failures}), retrying: {llm_error}'
					)
					await self._demo_mode_log(
						f'LLM call failed: {llm_error}',
						'error',
						{'step': step + 1},
					)

					# Check if we've hit the consecutive error limit
					if self._consecutive_errors >= self.max_failures:
						logger.error(f'Terminating: {self.max_failures} consecutive LLM failures')
						break

					await asyncio.sleep(1)  # Brief pause before retry
					continue

				if not code or code.strip() == '':
					# If task is already done, empty code is fine (LLM explaining completion)
					if self._is_task_done():
						logger.info('Task already marked as done, LLM provided explanation without code')
						# Add the text response to history as a non-code step
						await self._add_step_to_complete_history(
							model_output_code='',
							full_llm_response=full_llm_response,
							output=full_llm_response,  # Treat the explanation as output
							error=None,
							screenshot_path=await self._capture_screenshot(step + 1),
						)
						break  # Exit the loop since task is done

					logger.warning('LLM returned empty code')
					self._consecutive_errors += 1

					# new state
					if self.browser_session and self.dom_service:
						try:
							browser_state_text, screenshot = await self._get_browser_state()
							self._last_browser_state_text = browser_state_text
							self._last_screenshot = screenshot
						except Exception as e:
							logger.warning(f'Failed to get new browser state: {e}')
					continue

				# Execute code blocks sequentially if multiple python blocks exist
				# This allows JS/bash blocks to be injected into namespace before Python code uses them
				all_blocks = self.namespace.get('_all_code_blocks', {})
				python_blocks = [k for k in sorted(all_blocks.keys()) if k.startswith('python_')]

				if len(python_blocks) > 1:
					# Multiple Python blocks - execute each sequentially
					output = None
					error = None

					for i, block_key in enumerate(python_blocks):
						logger.info(f'Executing Python block {i + 1}/{len(python_blocks)}')
						block_code = all_blocks[block_key]
						block_output, block_error, _ = await self._execute_code(block_code)

						# Accumulate outputs
						if block_output:
							output = (output or '') + block_output
						if block_error:
							error = block_error
							# Stop on first error
							break
				else:
					# Single Python block - execute normally
					output, error, _ = await self._execute_code(code)

				# Track consecutive errors
				if error:
					self._consecutive_errors += 1
					logger.warning(f'Consecutive errors: {self._consecutive_errors}/{self.max_failures}')

					# Check if we've hit the consecutive error limit
					if self._consecutive_errors >= self.max_failures:
						logger.error(
							f'Terminating: {self.max_failures} consecutive errors reached. The agent is unable to make progress.'
						)
						await self._demo_mode_log(
							f'Terminating after {self.max_failures} consecutive errors without progress.',
							'error',
							{'step': step + 1},
						)
						# Add termination message to complete history before breaking
						await self._add_step_to_complete_history(
							model_output_code=code,
							full_llm_response=f'[Terminated after {self.max_failures} consecutive errors]',
							output=None,
							error=f'Auto-terminated: {self.max_failures} consecutive errors without progress',
							screenshot_path=None,
						)
						break
				else:
					# Reset consecutive error counter on success
					self._consecutive_errors = 0

				# Check if task is done - validate completion first if not at limits
				if self._is_task_done():
					# Get the final result from namespace (from done() call)
					final_result: str | None = self.namespace.get('_task_result')  # type: ignore[assignment]

					# Check if we should validate (not at step/error limits and under max validations)
					steps_remaining = self.max_steps - step - 1
					should_validate = (
						self._validation_count < self.max_validations  # Haven't exceeded max validations
						and steps_remaining >= 4  # At least 4 steps away from limit
						and self._consecutive_errors < 3  # Not close to error limit (8 consecutive)
					)

					if should_validate:
						self._validation_count += 1
						logger.info('Validating task completion with LLM...')
						from .namespace import validate_task_completion

						is_complete, reasoning = await validate_task_completion(
							task=self.task,
							output=final_result,
							llm=self.llm,
						)

						if not is_complete:
							# Task not truly complete - inject feedback and continue
							logger.warning('Validator: Task not complete, continuing...')
							validation_feedback = (
								f'\n\nâš ï¸ VALIDATOR FEEDBACK:\n'
								f'Your done() call was rejected. The task is NOT complete yet.\n\n'
								f'Validation reasoning:\n{reasoning}\n\n'
								f'You must continue working on the task. Analyze what is missing and complete it.\n'
								f'Do NOT call done() again until the task is truly finished.'
							)

							# Clear the done flag so execution continues
							self.namespace['_task_done'] = False
							self.namespace.pop('_task_result', None)
							self.namespace.pop('_task_success', None)

							# Add validation feedback to LLM messages
							self._llm_messages.append(UserMessage(content=validation_feedback))

							# Don't override output - let execution continue normally
						else:
							logger.info('Validator: Task complete')
							# Override output with done message for final step
							if final_result:
								output = final_result
					else:
						# At limits - skip validation and accept done()
						if self._validation_count >= self.max_validations:
							logger.info(
								f'Reached max validations ({self.max_validations}) - skipping validation and accepting done()'
							)
						else:
							logger.info('At step/error limits - skipping validation')
						if final_result:
							output = final_result

				if output:
					# Check if this is the final done() output
					if self._is_task_done():
						# Show done() output more prominently
						logger.info(
							f'âœ“ Task completed - Final output from done():\n{output[:300] if len(output) > 300 else output}'
						)
						# Also show files_to_display if they exist in namespace
						attachments: list[str] | None = self.namespace.get('_task_attachments')  # type: ignore[assignment]
						if attachments:
							logger.info(f'Files displayed: {", ".join(attachments)}')
					else:
						logger.info(f'Code output:\n{output}')

				# Browser state is now only logged when fetched before LLM call (not after execution)

				# Take screenshot for eval tracking
				screenshot_path = await self._capture_screenshot(step + 1)

				# Add step to complete_history for eval system
				await self._add_step_to_complete_history(
					model_output_code=code,
					full_llm_response=full_llm_response,
					output=output,
					error=error,
					screenshot_path=screenshot_path,
				)

				# Check if task is done (after validation)
				if self._is_task_done():
					# Get the final result from namespace
					final_result: str | None = self.namespace.get('_task_result', output)  # type: ignore[assignment]
					logger.info('Task completed successfully')
					if final_result:
						logger.info(f'Final result: {final_result}')
						self._add_sample_output_cell(final_result)
					if self._demo_mode_enabled:
						await self._demo_mode_log(
							f'Final Result: {final_result or "Task completed"}',
							'success',
							{'tag': 'task'},
						)
					should_delay_close = True
					break
				# If validation rejected done(), continue to next iteration
				# The feedback message has already been added to _llm_messages

				# Add result to LLM messages for next iteration (without browser state)
				result_message = self._format_execution_result(code, output, error, current_step=step + 1)
				truncated_result = truncate_message_content(result_message)
				self._llm_messages.append(UserMessage(content=truncated_result))

			except Exception as e:
				logger.error(f'Error in step {step + 1}: {e}')
				traceback.print_exc()
				break
		else:
			# Loop completed without break - max_steps reached
			logger.warning(f'Maximum steps ({self.max_steps}) reached without task completion')
			await self._demo_mode_log(
				f'Maximum steps ({self.max_steps}) reached without completing the task.',
				'error',
				{'tag': 'task'},
			)

		# If task is not done, capture the last step's output as partial result
		if not self._is_task_done() and self.complete_history:
			# Get the last step's output/error and use it as final extracted_content
			last_step = self.complete_history[-1]
			last_result = last_step.result[0] if last_step.result else None
			last_output = last_result.extracted_content if last_result else None
			last_error = last_result.error if last_result else None

			# Build a partial result message from the last step
			partial_result_parts = []
			partial_result_parts.append(f'Task incomplete - reached step limit ({self.max_steps} steps).')
			partial_result_parts.append('Last step output:')

			if last_output:
				partial_result_parts.append(f'\nOutput: {last_output}')
			if last_error:
				partial_result_parts.append(f'\nError: {last_error}')

			# Add any accumulated variables that might contain useful data
			data_vars = []
			for var_name in sorted(self.namespace.keys()):
				if not var_name.startswith('_') and var_name not in {'json', 'asyncio', 'csv', 're', 'datetime', 'Path'}:
					var_value = self.namespace[var_name]
					# Check if it's a list or dict that might contain collected data
					if isinstance(var_value, (list, dict)) and var_value:
						data_vars.append(f'  - {var_name}: {type(var_value).__name__} with {len(var_value)} items')

			if data_vars:
				partial_result_parts.append('\nVariables in namespace that may contain partial data:')
				partial_result_parts.extend(data_vars)

			partial_result = '\n'.join(partial_result_parts)

			# Update the last step's extracted_content with this partial result
			if last_result:
				last_result.extracted_content = partial_result
				last_result.is_done = False
				last_result.success = False

			logger.info(f'\nPartial result captured from last step:\n{partial_result}')
			if self._demo_mode_enabled:
				await self._demo_mode_log(f'Partial result:\n{partial_result}', 'error', {'tag': 'task'})

		# Log final summary if task was completed
		if self._is_task_done():
			logger.info('\n' + '=' * 60)
			logger.info('TASK COMPLETED SUCCESSFULLY')
			logger.info('=' * 60)
			final_result: str | None = self.namespace.get('_task_result')  # type: ignore[assignment]
			if final_result:
				logger.info(f'\nFinal Output:\n{final_result}')
				self._add_sample_output_cell(final_result)

			attachments: list[str] | None = self.namespace.get('_task_attachments')  # type: ignore[assignment]
			if attachments:
				logger.info(f'\nFiles Attached:\n{chr(10).join(attachments)}')
			logger.info('=' * 60 + '\n')
			if self._demo_mode_enabled and not should_delay_close:
				await self._demo_mode_log(
					f'Final Result: {final_result or "Task completed"}',
					'success',
					{'tag': 'task'},
				)
				should_delay_close = True

		# Auto-close browser if keep_alive is False
		if should_delay_close and self._demo_mode_enabled:
			await asyncio.sleep(30)
		await self.close()

		# Store usage summary for history property
		self.usage_summary = await self.token_cost_service.get_usage_summary()

		# Log token usage summary
		await self.token_cost_service.log_usage_summary()

		# Log telemetry event
		try:
			self._log_agent_event(max_steps=self.max_steps, agent_run_error=agent_run_error)
		except Exception as log_e:
			logger.error(f'Failed to log telemetry event: {log_e}', exc_info=True)

		# Store history data in session for history property
		self.session._complete_history = self.complete_history
		self.session._usage_summary = self.usage_summary

		return self.session

	async def _get_code_from_llm(self, step_number: int | None = None) -> tuple[str, str]:
		"""Get Python code from the LLM.

		Returns:
			Tuple of (extracted_code, full_llm_response)
		"""
		# Prepare messages for this request
		# Include browser state as separate message if available (not accumulated in history)
		messages_to_send = self._llm_messages.copy()

		if self._last_browser_state_text:
			# Create message with optional screenshot
			if self.use_vision and self._last_screenshot:
				# Build content with text + screenshot
				content_parts: list[ContentPartTextParam | ContentPartImageParam] = [
					ContentPartTextParam(text=self._last_browser_state_text)
				]

				# Add screenshot
				content_parts.append(
					ContentPartImageParam(
						image_url=ImageURL(
							url=f'data:image/png;base64,{self._last_screenshot}',
							media_type='image/png',
							detail='auto',
						),
					)
				)

				messages_to_send.append(UserMessage(content=content_parts))
			else:
				# Text only
				messages_to_send.append(UserMessage(content=self._last_browser_state_text))

			# Clear browser state after including it so it's only in this request
			self._last_browser_state_text = None
			self._last_screenshot = None

		# Call LLM with message history (including temporary browser state message)
		response = await self.llm.ainvoke(messages_to_send)

		# Store usage stats from this LLM call
		self._last_llm_usage = response.usage

		# Log the LLM's raw output for debugging
		logger.info(f'LLM Response:\n{response.completion}')
		await self._demo_mode_log(
			f'LLM Response:\n{response.completion}',
			'thought',
			{'step': step_number} if step_number else None,
		)

		# Check for token limit or repetition issues
		max_tokens = getattr(self.llm, 'max_tokens', None)
		completion_tokens = response.usage.completion_tokens if response.usage else None
		is_problematic, issue_message = detect_token_limit_issue(
			completion=response.completion,
			completion_tokens=completion_tokens,
			max_tokens=max_tokens,
			stop_reason=response.stop_reason,
		)

		if is_problematic:
			logger.warning(f'Token limit issue detected: {issue_message}')
			# Don't add the bad response to history
			# Instead, inject a system message prompting recovery
			recovery_prompt = (
				f'Your previous response hit a token limit or became repetitive: {issue_message}\n\n'
				'Please write a SHORT plan (2 sentences) for what to do next, then execute ONE simple action.'
			)
			self._llm_messages.append(UserMessage(content=recovery_prompt))
			# Return a controlled error message instead of corrupted code
			return '', f'[Token limit error: {issue_message}]'

		# Store the full response
		full_response = response.completion

		# Extract code blocks from response
		# Support multiple code block types: python, js, bash, markdown
		code_blocks = extract_code_blocks(response.completion)

		# Inject non-python blocks into namespace as variables
		# Track which variables are code blocks for browser state display
		if '_code_block_vars' not in self.namespace:
			self.namespace['_code_block_vars'] = set()

		for block_type, block_content in code_blocks.items():
			if not block_type.startswith('python'):
				# Store js, bash, markdown blocks (and named variants) as variables in namespace
				self.namespace[block_type] = block_content
				self.namespace['_code_block_vars'].add(block_type)
				print(f'â†’ Code block variable: {block_type} (str, {len(block_content)} chars)')
				logger.debug(f'Injected {block_type} block into namespace ({len(block_content)} chars)')

		# Store all code blocks for sequential execution
		self.namespace['_all_code_blocks'] = code_blocks

		# Get Python code if it exists
		# If no python block exists and no other code blocks exist, return empty string to skip execution
		# This prevents treating plain text explanations as code
		code = code_blocks.get('python', response.completion)

		# Add to LLM messages (truncate for history to save context)
		truncated_completion = truncate_message_content(response.completion)
		self._llm_messages.append(AssistantMessage(content=truncated_completion))

		return code, full_response

	def _print_variable_info(self, var_name: str, value: Any) -> None:
		"""Print compact info about a variable assignment."""
		# Skip built-in modules and known imports
		skip_names = {
			'json',
			'asyncio',
			'csv',
			're',
			'datetime',
			'Path',
			'pd',
			'np',
			'plt',
			'requests',
			'BeautifulSoup',
			'PdfReader',
			'browser',
			'file_system',
		}
		if var_name in skip_names:
			return

		# Skip code block variables (already printed)
		if '_code_block_vars' in self.namespace and var_name in self.namespace.get('_code_block_vars', set()):
			return

		# Print compact variable info
		if isinstance(value, (list, dict)):
			preview = str(value)[:100]
			print(f'â†’ Variable: {var_name} ({type(value).__name__}, len={len(value)}, preview={preview}...)')
		elif isinstance(value, str) and len(value) > 50:
			print(f'â†’ Variable: {var_name} (str, {len(value)} chars, preview={value[:50]}...)')
		elif callable(value):
			print(f'â†’ Variable: {var_name} (function)')
		else:
			print(f'â†’ Variable: {var_name} ({type(value).__name__}, value={repr(value)[:50]})')

	async def _execute_code(self, code: str) -> tuple[str | None, str | None, str | None]:
		"""
		Execute Python code in the namespace.

		Args:
			code: The Python code to execute

		Returns:
			Tuple of (output, error, browser_state)
		"""
		# Create new cell
		cell = self.session.add_cell(source=code)
		cell.status = ExecutionStatus.RUNNING
		cell.execution_count = self.session.increment_execution_count()

		output = None
		error = None
		browser_state = None

		try:
			# Capture output
			import ast
			import io
			import sys

			old_stdout = sys.stdout
			sys.stdout = io.StringIO()

			try:
				# Add asyncio to namespace if not already there
				if 'asyncio' not in self.namespace:
					self.namespace['asyncio'] = asyncio

				# Store the current code in namespace for done() validation
				self.namespace['_current_cell_code'] = code
				# Store consecutive errors count for done() validation
				self.namespace['_consecutive_errors'] = self._consecutive_errors

				# Check if code contains await expressions - if so, wrap in async function
				# This mimics how Jupyter/IPython handles top-level await
				try:
					tree = ast.parse(code, mode='exec')
					has_await = any(isinstance(node, (ast.Await, ast.AsyncWith, ast.AsyncFor)) for node in ast.walk(tree))
				except SyntaxError:
					# If parse fails, let exec handle the error
					has_await = False

				if has_await:
					# When code has await, we must wrap in async function
					# To make variables persist naturally (like Jupyter without needing 'global'):
					# 1. Extract all assigned variable names from the code
					# 2. Inject 'global' declarations for variables that already exist in namespace
					# 3. Extract user's explicit global declarations and pre-define those vars
					# 4. Return locals() so we can update namespace with new variables

					# Find all variable names being assigned + user's explicit globals
					try:
						assigned_names = set()
						user_global_names = set()

						for node in ast.walk(tree):
							if isinstance(node, ast.Assign):
								for target in node.targets:
									if isinstance(target, ast.Name):
										assigned_names.add(target.id)
							elif isinstance(node, ast.AugAssign) and isinstance(node.target, ast.Name):
								assigned_names.add(node.target.id)
							elif isinstance(node, (ast.AnnAssign, ast.NamedExpr)):
								if hasattr(node, 'target') and isinstance(node.target, ast.Name):
									assigned_names.add(node.target.id)
							elif isinstance(node, ast.Global):
								# Track user's explicit global declarations
								user_global_names.update(node.names)

						# Pre-define any user-declared globals that don't exist yet
						# This prevents NameError when user writes "global foo" before "foo = ..."
						for name in user_global_names:
							if name not in self.namespace:
								self.namespace[name] = None

						# Filter to only existing namespace vars (like Jupyter does)
						# Include both: assigned vars that exist + user's explicit globals
						existing_vars = {name for name in (assigned_names | user_global_names) if name in self.namespace}
					except Exception as e:
						existing_vars = set()

					# Build global declaration if needed
					global_decl = ''
					has_global_decl = False
					if existing_vars:
						vars_str = ', '.join(sorted(existing_vars))
						global_decl = f'    global {vars_str}\n'
						has_global_decl = True

					indented_code = '\n'.join('    ' + line if line.strip() else line for line in code.split('\n'))
					wrapped_code = f"""async def __code_exec__():
{global_decl}{indented_code}
    # Return locals so we can update the namespace
    return locals()

__code_exec_coro__ = __code_exec__()
"""
					# Store whether we added a global declaration (needed for error line mapping)
					self.namespace['_has_global_decl'] = has_global_decl

					# Compile and execute wrapper at module level
					compiled_code = compile(wrapped_code, '<code>', 'exec')
					exec(compiled_code, self.namespace, self.namespace)

					# Get and await the coroutine, then update namespace with new/modified variables
					coro = self.namespace.get('__code_exec_coro__')
					if coro:
						result_locals = await coro
						# Update namespace with all variables from the function's locals
						# This makes variable assignments persist across cells
						if result_locals:
							for key, value in result_locals.items():
								if not key.startswith('_'):
									self.namespace[key] = value
									# Variable info is tracked in "Available" section, no need for verbose inline output

						# Clean up temporary variables
						self.namespace.pop('__code_exec_coro__', None)
						self.namespace.pop('__code_exec__', None)
				else:
					# No await - execute directly at module level for natural variable scoping
					# This means x = x + 10 will work without needing 'global x'

					# Track variables before execution
					vars_before = set(self.namespace.keys())

					compiled_code = compile(code, '<code>', 'exec')
					exec(compiled_code, self.namespace, self.namespace)

					# Track newly created/modified variables (info shown in "Available" section)
					vars_after = set(self.namespace.keys())
					new_vars = vars_after - vars_before

				# Get output
				output_value = sys.stdout.getvalue()
				if output_value:
					output = output_value

			finally:
				sys.stdout = old_stdout

			# Wait 2 seconds for page to stabilize after code execution
			await asyncio.sleep(0.5)

			# Note: Browser state is now fetched right before LLM call instead of after each execution
			# This reduces unnecessary state fetches for operations that don't affect the browser

			cell.status = ExecutionStatus.SUCCESS
			cell.output = output
			cell.browser_state = None  # Will be captured in next iteration before LLM call

		except Exception as e:
			# Handle EvaluateError specially - JavaScript execution failed
			if isinstance(e, EvaluateError):
				error = str(e)
				cell.status = ExecutionStatus.ERROR
				cell.error = error
				logger.error(f'Code execution error: {error}')

				await asyncio.sleep(1)

				# Browser state will be fetched before next LLM call
				# Return immediately - do not continue executing code
				return output, error, None

			# Handle NameError specially - check for code block variable confusion
			if isinstance(e, NameError):
				error_msg = str(e)
				cell.status = ExecutionStatus.ERROR
				cell.error = error

				# Browser state will be fetched before next LLM call
				await asyncio.sleep(0.5)
				return output, error, None

			# For syntax errors and common parsing errors, show just the error message
			# without the full traceback to keep output clean
			if isinstance(e, SyntaxError):
				error_msg = e.msg if e.msg else str(e)
				error = f'{type(e).__name__}: {error_msg}'

				# Detect common f-string issues with JSON/JavaScript code
				if 'unterminated' in error_msg.lower() and 'string' in error_msg.lower() and code:
					# Check if code contains f-strings with potential JSON/JS content
					has_fstring = bool(re.search(r'\bf["\']', code))
					has_json_pattern = bool(re.search(r'json\.dumps|"[^"]*\{[^"]*\}[^"]*"|\'[^\']*\{[^\']*\}[^\']*\'', code))
					has_js_pattern = bool(re.search(r'evaluate\(|await evaluate', code))

					if has_fstring and (has_json_pattern or has_js_pattern):
						error += (
							'\n\nğŸ’¡ TIP: Detected f-string with JSON/JavaScript code containing {}.\n'
							'   Use separate ``\`js or ``\`markdown blocks instead of f-strings to avoid escaping issues.\n'
							'   If your code block needs ``\` inside it, wrap with 4+ backticks: ``\``markdown code`\n'
						)

				# Detect and provide helpful hints for common string literal errors
				if 'unterminated' in error_msg.lower() and 'string' in error_msg.lower():
					# Detect what type of string literal is unterminated
					is_triple = 'triple-quoted' in error_msg.lower()
					msg_lower = error_msg.lower()

					# Detect prefix type from error message
					if 'f-string' in msg_lower and 'raw' in msg_lower:
						prefix = 'rf or fr'
						desc = 'raw f-string'
					elif 'f-string' in msg_lower:
						prefix = 'f'
						desc = 'f-string'
					elif 'raw' in msg_lower and 'bytes' in msg_lower:
						prefix = 'rb or br'
						desc = 'raw bytes'
					elif 'raw' in msg_lower:
						prefix = 'r'
						desc = 'raw string'
					elif 'bytes' in msg_lower:
						prefix = 'b'
						desc = 'bytes'
					else:
						prefix = ''
						desc = 'string'

					# Build hint based on triple-quoted vs single/double quoted
					if is_triple:
						if prefix:
							hint = f"Hint: Unterminated {prefix}'''...''' or {prefix}\"\"\"...\"\" ({desc}). Check for missing closing quotes or unescaped quotes inside."
						else:
							hint = "Hint: Unterminated '''...''' or \"\"\"...\"\" detected. Check for missing closing quotes or unescaped quotes inside."
						hint += '\n      If you need ``\` inside your string, use a ``\``markdown varname` code block with 4+ backticks instead.'
					else:
						if prefix:
							hint = f'Hint: Unterminated {prefix}\'...\' or {prefix}"..." ({desc}). Check for missing closing quote or unescaped quotes inside.'
						else:
							hint = 'Hint: Unterminated \'...\' or "..." detected. Check for missing closing quote or unescaped quotes inside the string.'
					error += f'\n{hint}'

				# Show the problematic line from the code
				if e.text:
					error += f'\n{e.text}'
				elif e.lineno and code:
					# If e.text is empty, extract the line from the code
					lines = code.split('\n')
					if 0 < e.lineno <= len(lines):
						error += f'\n{lines[e.lineno - 1]}'

			else:
				# For other errors, try to extract useful information
				error_str = str(e)
				error = f'{type(e).__name__}: {error_str}' if error_str else f'{type(e).__name__} occurred'

				# For RuntimeError or other exceptions, try to extract traceback info
				# to show which line in the user's code actually failed
				if hasattr(e, '__traceback__'):
					# Walk the traceback to find the frame with '<code>' filename
					tb = e.__traceback__
					user_code_lineno = None
					while tb is not None:
						frame = tb.tb_frame
						if frame.f_code.co_filename == '<code>':
							# Found the frame executing user code
							# Get the line number from the traceback
							user_code_lineno = tb.tb_lineno
							break
						tb = tb.tb_next

			cell.status = ExecutionStatus.ERROR
			cell.error = error
			logger.error(f'Code execution error: {error}')

			await asyncio.sleep(1)

			# Browser state will be fetched before next LLM call

		return output, error, None

	async def _get_browser_state(self) -> tuple[str, str | None]:
		"""Get the current browser state as text with ultra-minimal DOM structure for code agents.

		Returns:
			Tuple of (browser_state_text, screenshot_base64)
		"""
		if not self.browser_session or not self.dom_service:
			return 'Browser state not available', None

		try:
			# Get full browser state including screenshot if use_vision is enabled
			include_screenshot = True
			state = await self.browser_session.get_browser_state_summary(include_screenshot=include_screenshot)

			# Format browser state with namespace context
			browser_state_text = await format_browser_state_for_llm(
				state=state, namespace=self.namespace, browser_session=self.browser_session
			)

			screenshot = state.screenshot if include_screenshot else None
			return browser_state_text, screenshot

		except Exception as e:
			logger.error(f'Failed to get browser state: {e}')
			return f'Error getting browser state: {e}', None

	def _format_execution_result(self, code: str, output: str | None, error: str | None, current_step: int | None = None) -> str:
		"""Format the execution result for the LLM (without browser state)."""
		result = []

		# Add step progress header if step number provided
		if current_step is not None:
			progress_header = f'Step {current_step}/{self.max_steps} executed'
			# Add consecutive failure tracking if there are errors
			if error and self._consecutive_errors > 0:
				progress_header += f' | Consecutive failures: {self._consecutive_errors}/{self.max_failures}'
			result.append(progress_header)

		if error:
			result.append(f'Error: {error}')

		if output:
			# Truncate output if too long
			if len(output) > 10000:
				output = output[:9950] + '\n[Truncated after 10000 characters]'
			result.append(f'Output: {output}')
		if len(result) == 0:
			result.append('Executed')
		return '\n'.join(result)

	def _is_task_done(self) -> bool:
		"""Check if the task is marked as done in the namespace."""
		# Check if 'done' was called by looking for a special marker in namespace
		return self.namespace.get('_task_done', False)

	async def _capture_screenshot(self, step_number: int) -> str | None:
		"""Capture and store screenshot for eval tracking."""
		if not self.browser_session:
			return None

		try:
			# Get browser state summary which includes screenshot
			state = await self.browser_session.get_browser_state_summary(include_screenshot=True)
			if state and state.screenshot:
				# Store screenshot using screenshot service
				screenshot_path = await self.screenshot_service.store_screenshot(state.screenshot, step_number)
				return str(screenshot_path) if screenshot_path else None
		except Exception as e:
			logger.warning(f'Failed to capture screenshot for step {step_number}: {e}')
			return None

	async def _add_step_to_complete_history(
		self,
		model_output_code: str,
		full_llm_response: str,
		output: str | None,
		error: str | None,
		screenshot_path: str | None,
	) -> None:
		"""Add a step to complete_history using type-safe models."""
		# Get current browser URL and title for state
		url: str | None = None
		title: str | None = None
		if self.browser_session:
			try:
				url = await self.browser_session.get_current_page_url()
				# Get title from browser
				cdp_session = await self.browser_session.get_or_create_cdp_session()
				result = await cdp_session.cdp_client.send.Runtime.evaluate(
					params={'expression': 'document.title', 'returnByValue': True},
					session_id=cdp_session.session_id,
				)
				title = result.get('result', {}).get('value')
			except Exception as e:
				logger.debug(f'Failed to get browser URL/title for history: {e}')

		# Check if this is a done result
		is_done = self._is_task_done()

		# Get self-reported success from done() call if task is done
		self_reported_success: bool | None = None
		if is_done:
			task_success = self.namespace.get('_task_success')
			self_reported_success = task_success if isinstance(task_success, bool) else None

		# Create result entry using typed model
		result_entry = CodeAgentResult(
			extracted_content=output if output else None,
			error=error if error else None,
			is_done=is_done,
			success=self_reported_success,
		)

		# Create state entry using typed model
		state_entry = CodeAgentState(url=url, title=title, screenshot_path=screenshot_path)

		# Create metadata entry using typed model
		step_end_time = datetime.datetime.now().timestamp()
		metadata_entry = CodeAgentStepMetadata(
			input_tokens=self._last_llm_usage.prompt_tokens if self._last_llm_usage else None,
			output_tokens=self._last_llm_usage.completion_tokens if self._last_llm_usage else None,
			step_start_time=self._step_start_time,
			step_end_time=step_end_time,
		)

		# Create model output entry using typed model (if there's code to track)
		model_output_entry: CodeAgentModelOutput | None = None
		if model_output_code or full_llm_response:
			model_output_entry = CodeAgentModelOutput(
				model_output=model_output_code if model_output_code else '',
				full_response=full_llm_response if full_llm_response else '',
			)

		# Create history entry using typed model
		history_entry = CodeAgentHistory(
			model_output=model_output_entry,
			result=[result_entry],
			state=state_entry,
			metadata=metadata_entry,
			screenshot_path=screenshot_path,  # Keep for backward compatibility
		)

		self.complete_history.append(history_entry)
		await self._demo_mode_log_step(history_entry)

	async def _demo_mode_log(self, message: str, level: str = 'info', metadata: dict[str, Any] | None = None) -> None:
		if not (self._demo_mode_enabled and message and self.browser_session):
			return
		try:
			await self.browser_session.send_demo_mode_log(
				message=message,
				level=level,
				metadata=metadata or {},
			)
		except Exception as exc:
			logger.debug(f'[DemoMode] Failed to send log: {exc}')

	async def _demo_mode_log_step(self, history_entry: CodeAgentHistory) -> None:
		if not self._demo_mode_enabled:
			return
		step_number = len(self.complete_history)
		result = history_entry.result[0] if history_entry.result else None
		if not result:
			return
		level = 'error' if result.error else 'success' if result.success else 'info'
		message_parts = [f'Step {step_number}:']
		if result.error:
			message_parts.append(f'Error: {result.error}')
		if result.extracted_content:
			message_parts.append(result.extracted_content)
		elif result.success:
			message_parts.append('Marked done.')
		else:
			message_parts.append('Executed.')
		await self._demo_mode_log(
			' '.join(message_parts).strip(),
			level,
			{'step': step_number, 'url': history_entry.state.url if history_entry.state else None},
		)

	def _add_sample_output_cell(self, final_result: Any | None) -> None:
		if self._sample_output_added or final_result is None:
			return

		sample_content: str | None = None

		def _extract_sample(data: Any) -> Any | None:
			if isinstance(data, list) and data:
				return data[0]
			if isinstance(data, dict) and data:
				first_key = next(iter(data))
				return {first_key: data[first_key]}
			return data if isinstance(data, (str, int, float, bool)) else None

		data: Any | None = None
		if isinstance(final_result, str):
			try:
				data = json.loads(final_result)
			except Exception:
				sample_content = final_result.strip()
		elif isinstance(final_result, (list, dict)):
			data = final_result

		if data is not None:
			sample = _extract_sample(data)
			if isinstance(sample, (dict, list)):
				try:
					sample_content = json.dumps(sample, indent=2, ensure_ascii=False)
				except Exception:
					sample_content = str(sample)
			elif sample is not None:
				sample_content = str(sample)

		if not sample_content:
			return

		sample_cell = self.session.add_cell(source='# Sample output preview')
		sample_cell.cell_type = CellType.MARKDOWN
		sample_cell.status = ExecutionStatus.SUCCESS
		sample_cell.execution_count = None
		escaped = html.escape(sample_content)
		sample_cell.output = f'<pre>{escaped}</pre>'

		self._sample_output_added = True

	def _log_agent_event(self, max_steps: int, agent_run_error: str | None = None) -> None:
		"""Send the agent event for this run to telemetry."""
		from urllib.parse import urlparse

		token_summary = self.token_cost_service.get_usage_tokens_for_model(self.llm.model)

		# For CodeAgent, we don't have action history like Agent does
		# Instead we track the code execution cells
		action_history_data: list[list[dict[str, Any]] | None] = []
		for step in self.complete_history:
			# Extract code from model_output if available (type-safe access)
			if step.model_output and step.model_output.full_response:
				code = step.model_output.full_response
				# Represent each code cell as a simple action entry
				action_history_data.append([{'llm_response': code}])
			else:
				action_history_data.append(None)

		# Get final result from the last step or namespace (type-safe)
		final_result: Any = self.namespace.get('_task_result')
		final_result_str: str | None = final_result if isinstance(final_result, str) else None

		# Get URLs visited from complete_history (type-safe access)
		urls_visited: list[str] = []
		for step in self.complete_history:
			if step.state.url and step.state.url not in urls_visited:
				urls_visited.append(step.state.url)

		# Get errors from complete_history (type-safe access)
		errors: list[str] = []
		for step in self.complete_history:
			for result in step.result:
				if result.error:
					errors.append(result.error)

		# Determine success from task completion status (type-safe)
		is_done = self._is_task_done()
		task_success: Any = self.namespace.get('_task_success')
		self_reported_success: bool | None = task_success if isinstance(task_success, bool) else (False if is_done else None)

		self.telemetry.capture(
			AgentTelemetryEvent(
				task=self.task,
				model=self.llm.model,
				model_provider=self.llm.provider,
				max_steps=max_steps,
				max_actions_per_step=1,  # CodeAgent executes one code cell per step
				use_vision=self.use_vision,
				version=self.version,
				source=self.source,
				cdp_url=urlparse(self.browser_session.cdp_url).hostname
				if self.browser_session and self.browser_session.cdp_url
				else None,
				agent_type='code',  # CodeAgent identifier
				action_errors=errors,
				action_history=action_history_data,
				urls_visited=urls_visited,
				steps=len(self.complete_history),
				total_input_tokens=token_summary.prompt_tokens,
				total_output_tokens=token_summary.completion_tokens,
				prompt_cached_tokens=token_summary.prompt_cached_tokens,
				total_tokens=token_summary.total_tokens,
				total_duration_seconds=sum(step.metadata.duration_seconds for step in self.complete_history if step.metadata),
				success=self_reported_success,
				final_result_response=final_result_str,
				error_message=agent_run_error,
			)
		)

	def screenshot_paths(self, n_last: int | None = None) -> list[str | None]:
		"""
		Get screenshot paths from complete_history for eval system.

		Args:
			n_last: Optional number of last screenshots to return

		Returns:
			List of screenshot file paths (or None for missing screenshots)
		"""
		paths = [step.screenshot_path for step in self.complete_history]

		if n_last is not None:
			return paths[-n_last:] if len(paths) > n_last else paths

		return paths

	@property
	def message_manager(self) -> Any:
		"""
		Compatibility property for eval system.
		Returns a mock object with last_input_messages attribute.
		"""

		class MockMessageManager:
			def __init__(self, llm_messages: list[BaseMessage]) -> None:
				# Convert code-use LLM messages to format expected by eval system
				self.last_input_messages = llm_messages

		return MockMessageManager(self._llm_messages)

	@property
	def history(self) -> CodeAgentHistoryList:
		"""
		Compatibility property for eval system.
		Returns a CodeAgentHistoryList object with history attribute containing complete_history.
		This is what the eval system expects when it does: agent_history = agent.history
		"""
		return CodeAgentHistoryList(self.complete_history, self.usage_summary)

	async def close(self) -> None:
		"""Close the browser session."""
		if self.browser_session:
			# Check if we should close the browser based on keep_alive setting
			if not self.browser_session.browser_profile.keep_alive:
				await self.browser_session.kill()
			else:
				logger.debug('Browser keep_alive is True, not closing browser session')

	async def __aenter__(self) -> 'CodeAgent':
		"""Async context manager entry."""
		return self

	async def __aexit__(self, exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: Any) -> None:
		"""Async context manager exit."""
		await self.close()

```

---

## backend/browser-use/browser_use/code_use/utils.py

```py
"""Utility functions for code-use agent."""

import re


def truncate_message_content(content: str, max_length: int = 10000) -> str:
	"""Truncate message content to max_length characters for history."""
	if len(content) <= max_length:
		return content
	# Truncate and add marker
	return content[:max_length] + f'\n\n[... truncated {len(content) - max_length} characters for history]'


def detect_token_limit_issue(
	completion: str,
	completion_tokens: int | None,
	max_tokens: int | None,
	stop_reason: str | None,
) -> tuple[bool, str | None]:
	"""
	Detect if the LLM response hit token limits or is repetitive garbage.

	Returns: (is_problematic, error_message)
	"""
	# Check 1: Stop reason indicates max_tokens
	if stop_reason == 'max_tokens':
		return True, f'Response terminated due to max_tokens limit (stop_reason: {stop_reason})'

	# Check 2: Used 90%+ of max_tokens (if we have both values)
	if completion_tokens is not None and max_tokens is not None and max_tokens > 0:
		usage_ratio = completion_tokens / max_tokens
		if usage_ratio >= 0.9:
			return True, f'Response used {usage_ratio:.1%} of max_tokens ({completion_tokens}/{max_tokens})'

	# Check 3: Last 6 characters repeat 40+ times (repetitive garbage)
	if len(completion) >= 6:
		last_6 = completion[-6:]
		repetition_count = completion.count(last_6)
		if repetition_count >= 40:
			return True, f'Repetitive output detected: last 6 chars "{last_6}" appears {repetition_count} times'

	return False, None


def extract_url_from_task(task: str) -> str | None:
	"""Extract URL from task string using naive pattern matching."""
	# Remove email addresses from task before looking for URLs
	task_without_emails = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '', task)

	# Look for common URL patterns
	patterns = [
		r'https?://[^\s<>"\']+',  # Full URLs with http/https
		r'(?:www\.)?[a-zA-Z0-9-]+(?:\.[a-zA-Z0-9-]+)*\.[a-zA-Z]{2,}(?:/[^\s<>"\']*)?',  # Domain names with subdomains and optional paths
	]

	found_urls = []
	for pattern in patterns:
		matches = re.finditer(pattern, task_without_emails)
		for match in matches:
			url = match.group(0)

			# Remove trailing punctuation that's not part of URLs
			url = re.sub(r'[.,;:!?()\[\]]+$', '', url)
			# Add https:// if missing
			if not url.startswith(('http://', 'https://')):
				url = 'https://' + url
			found_urls.append(url)

	unique_urls = list(set(found_urls))
	# If multiple URLs found, skip auto-navigation to avoid ambiguity
	if len(unique_urls) > 1:
		return None

	# If exactly one URL found, return it
	if len(unique_urls) == 1:
		return unique_urls[0]

	return None


def extract_code_blocks(text: str) -> dict[str, str]:
	"""Extract all code blocks from markdown response.

	Supports:
	- ``\`python, ``\`js, ``\`javascript, ``\`bash, ``\`markdown, ``\`md
	- Named blocks: ``\`js variable_name â†’ saved as 'variable_name' in namespace
	- Nested blocks: Use 4+ backticks for outer block when inner content has 3 backticks

	Returns dict mapping block_name -> content

	Note: Python blocks are NO LONGER COMBINED. Each python block executes separately
	to allow sequential execution with JS/bash blocks in between.
	"""
	# Pattern to match code blocks with language identifier and optional variable name
	# Matches: ``\`lang\n or ``\`lang varname\n or ``\``+lang\n (4+ backticks for nested blocks)
	# Uses non-greedy matching and backreferences to match opening/closing backticks
	pattern = r'(`{3,})(\w+)(?:\s+(\w+))?\n(.*?)\1(?:\n|$)'
	matches = re.findall(pattern, text, re.DOTALL)

	blocks: dict[str, str] = {}
	python_block_counter = 0

	for backticks, lang, var_name, content in matches:
		lang = lang.lower()

		# Normalize language names
		if lang in ('javascript', 'js'):
			lang_normalized = 'js'
		elif lang in ('markdown', 'md'):
			lang_normalized = 'markdown'
		elif lang in ('sh', 'shell'):
			lang_normalized = 'bash'
		elif lang == 'python':
			lang_normalized = 'python'
		else:
			# Unknown language, skip
			continue

		# Only process supported types
		if lang_normalized in ('python', 'js', 'bash', 'markdown'):
			content = content.rstrip()  # Only strip trailing whitespace, preserve leading for indentation
			if content:
				# Determine the key to use
				if var_name:
					# Named block - use the variable name
					block_key = var_name
					blocks[block_key] = content
				elif lang_normalized == 'python':
					# Unnamed Python blocks - give each a unique key to preserve order
					block_key = f'python_{python_block_counter}'
					blocks[block_key] = content
					python_block_counter += 1
				else:
					# Other unnamed blocks (js, bash, markdown) - keep last one only
					blocks[lang_normalized] = content

	# If we have multiple python blocks, mark the first one as 'python' for backward compat
	if python_block_counter > 0:
		blocks['python'] = blocks['python_0']

	# Fallback: if no python block but there's generic ``\` block, treat as python
	if python_block_counter == 0 and 'python' not in blocks:
		generic_pattern = r'``\`\n(.*?)``\`'
		generic_matches = re.findall(generic_pattern, text, re.DOTALL)
		if generic_matches:
			combined = '\n\n'.join(m.strip() for m in generic_matches if m.strip())
			if combined:
				blocks['python'] = combined

	return blocks

```

---

## backend/browser-use/browser_use/code_use/views.py

```py
"""Data models for code-use mode."""

from __future__ import annotations

import json
from enum import Enum
from pathlib import Path
from typing import Any

from pydantic import BaseModel, ConfigDict, Field, PrivateAttr
from uuid_extensions import uuid7str

from browser_use.tokens.views import UsageSummary


class CellType(str, Enum):
	"""Type of notebook cell."""

	CODE = 'code'
	MARKDOWN = 'markdown'


class ExecutionStatus(str, Enum):
	"""Execution status of a cell."""

	PENDING = 'pending'
	RUNNING = 'running'
	SUCCESS = 'success'
	ERROR = 'error'


class CodeCell(BaseModel):
	"""Represents a code cell in the notebook-like execution."""

	model_config = ConfigDict(extra='forbid')

	id: str = Field(default_factory=uuid7str)
	cell_type: CellType = CellType.CODE
	source: str = Field(description='The code to execute')
	output: str | None = Field(default=None, description='The output of the code execution')
	execution_count: int | None = Field(default=None, description='The execution count')
	status: ExecutionStatus = Field(default=ExecutionStatus.PENDING)
	error: str | None = Field(default=None, description='Error message if execution failed')
	browser_state: str | None = Field(default=None, description='Browser state after execution')


class NotebookSession(BaseModel):
	"""Represents a notebook-like session."""

	model_config = ConfigDict(extra='forbid')

	id: str = Field(default_factory=uuid7str)
	cells: list[CodeCell] = Field(default_factory=list)
	current_execution_count: int = Field(default=0)
	namespace: dict[str, Any] = Field(default_factory=dict, description='Current namespace state')
	_complete_history: list[CodeAgentHistory] = PrivateAttr(default_factory=list)
	_usage_summary: UsageSummary | None = PrivateAttr(default=None)

	def add_cell(self, source: str) -> CodeCell:
		"""Add a new code cell to the session."""
		cell = CodeCell(source=source)
		self.cells.append(cell)
		return cell

	def get_cell(self, cell_id: str) -> CodeCell | None:
		"""Get a cell by ID."""
		for cell in self.cells:
			if cell.id == cell_id:
				return cell
		return None

	def get_latest_cell(self) -> CodeCell | None:
		"""Get the most recently added cell."""
		if self.cells:
			return self.cells[-1]
		return None

	def increment_execution_count(self) -> int:
		"""Increment and return the execution count."""
		self.current_execution_count += 1
		return self.current_execution_count

	@property
	def history(self) -> CodeAgentHistoryList:
		"""Get the history as an AgentHistoryList-compatible object."""
		return CodeAgentHistoryList(self._complete_history, self._usage_summary)


class NotebookExport(BaseModel):
	"""Export format for Jupyter notebook."""

	model_config = ConfigDict(extra='forbid')

	nbformat: int = Field(default=4)
	nbformat_minor: int = Field(default=5)
	metadata: dict[str, Any] = Field(default_factory=dict)
	cells: list[dict[str, Any]] = Field(default_factory=list)


class CodeAgentModelOutput(BaseModel):
	"""Model output for CodeAgent - contains the code and full LLM response."""

	model_config = ConfigDict(extra='forbid')

	model_output: str = Field(description='The extracted code from the LLM response')
	full_response: str = Field(description='The complete LLM response including any text/reasoning')


class CodeAgentResult(BaseModel):
	"""Result of executing a code cell in CodeAgent."""

	model_config = ConfigDict(extra='forbid')

	extracted_content: str | None = Field(default=None, description='Output from code execution')
	error: str | None = Field(default=None, description='Error message if execution failed')
	is_done: bool = Field(default=False, description='Whether task is marked as done')
	success: bool | None = Field(default=None, description='Self-reported success from done() call')


class CodeAgentState(BaseModel):
	"""State information for a CodeAgent step."""

	model_config = ConfigDict(extra='forbid', arbitrary_types_allowed=True)

	url: str | None = Field(default=None, description='Current page URL')
	title: str | None = Field(default=None, description='Current page title')
	screenshot_path: str | None = Field(default=None, description='Path to screenshot file')

	def get_screenshot(self) -> str | None:
		"""Load screenshot from disk and return as base64 string."""
		if not self.screenshot_path:
			return None

		import base64
		from pathlib import Path

		path_obj = Path(self.screenshot_path)
		if not path_obj.exists():
			return None

		try:
			with open(path_obj, 'rb') as f:
				screenshot_data = f.read()
			return base64.b64encode(screenshot_data).decode('utf-8')
		except Exception:
			return None


class CodeAgentStepMetadata(BaseModel):
	"""Metadata for a single CodeAgent step including timing and token information."""

	model_config = ConfigDict(extra='forbid')

	input_tokens: int | None = Field(default=None, description='Number of input tokens used')
	output_tokens: int | None = Field(default=None, description='Number of output tokens used')
	step_start_time: float = Field(description='Step start timestamp (Unix time)')
	step_end_time: float = Field(description='Step end timestamp (Unix time)')

	@property
	def duration_seconds(self) -> float:
		"""Calculate step duration in seconds."""
		return self.step_end_time - self.step_start_time


class CodeAgentHistory(BaseModel):
	"""History item for CodeAgent actions."""

	model_config = ConfigDict(extra='forbid', arbitrary_types_allowed=True)

	model_output: CodeAgentModelOutput | None = Field(default=None, description='LLM output for this step')
	result: list[CodeAgentResult] = Field(default_factory=list, description='Results from code execution')
	state: CodeAgentState = Field(description='Browser state at this step')
	metadata: CodeAgentStepMetadata | None = Field(default=None, description='Step timing and token metadata')
	screenshot_path: str | None = Field(default=None, description='Legacy field for screenshot path')

	def model_dump(self, **kwargs) -> dict[str, Any]:
		"""Custom serialization for CodeAgentHistory."""
		return {
			'model_output': self.model_output.model_dump() if self.model_output else None,
			'result': [r.model_dump() for r in self.result],
			'state': self.state.model_dump(),
			'metadata': self.metadata.model_dump() if self.metadata else None,
			'screenshot_path': self.screenshot_path,
		}


class CodeAgentHistoryList:
	"""Compatibility wrapper for CodeAgentHistory that provides AgentHistoryList-like API."""

	def __init__(self, complete_history: list[CodeAgentHistory], usage_summary: UsageSummary | None) -> None:
		"""Initialize with CodeAgent history data."""
		self._complete_history = complete_history
		self._usage_summary = usage_summary

	@property
	def history(self) -> list[CodeAgentHistory]:
		"""Get the raw history list."""
		return self._complete_history

	@property
	def usage(self) -> UsageSummary | None:
		"""Get the usage summary."""
		return self._usage_summary

	def __len__(self) -> int:
		"""Return the number of history items."""
		return len(self._complete_history)

	def __str__(self) -> str:
		"""Representation of the CodeAgentHistoryList object."""
		return f'CodeAgentHistoryList(steps={len(self._complete_history)}, action_results={len(self.action_results())})'

	def __repr__(self) -> str:
		"""Representation of the CodeAgentHistoryList object."""
		return self.__str__()

	def final_result(self) -> None | str:
		"""Final result from history."""
		if self._complete_history and self._complete_history[-1].result:
			return self._complete_history[-1].result[-1].extracted_content
		return None

	def is_done(self) -> bool:
		"""Check if the agent is done."""
		if self._complete_history and len(self._complete_history[-1].result) > 0:
			last_result = self._complete_history[-1].result[-1]
			return last_result.is_done is True
		return False

	def is_successful(self) -> bool | None:
		"""Check if the agent completed successfully."""
		if self._complete_history and len(self._complete_history[-1].result) > 0:
			last_result = self._complete_history[-1].result[-1]
			if last_result.is_done is True:
				return last_result.success
		return None

	def errors(self) -> list[str | None]:
		"""Get all errors from history, with None for steps without errors."""
		errors = []
		for h in self._complete_history:
			step_errors = [r.error for r in h.result if r.error]
			# each step can have only one error
			errors.append(step_errors[0] if step_errors else None)
		return errors

	def has_errors(self) -> bool:
		"""Check if the agent has any non-None errors."""
		return any(error is not None for error in self.errors())

	def urls(self) -> list[str | None]:
		"""Get all URLs from history."""
		return [h.state.url if h.state.url is not None else None for h in self._complete_history]

	def screenshot_paths(self, n_last: int | None = None, return_none_if_not_screenshot: bool = True) -> list[str | None]:
		"""Get all screenshot paths from history."""
		if n_last == 0:
			return []
		if n_last is None:
			if return_none_if_not_screenshot:
				return [h.state.screenshot_path if h.state.screenshot_path is not None else None for h in self._complete_history]
			else:
				return [h.state.screenshot_path for h in self._complete_history if h.state.screenshot_path is not None]
		else:
			if return_none_if_not_screenshot:
				return [
					h.state.screenshot_path if h.state.screenshot_path is not None else None
					for h in self._complete_history[-n_last:]
				]
			else:
				return [h.state.screenshot_path for h in self._complete_history[-n_last:] if h.state.screenshot_path is not None]

	def screenshots(self, n_last: int | None = None, return_none_if_not_screenshot: bool = True) -> list[str | None]:
		"""Get all screenshots from history as base64 strings."""
		if n_last == 0:
			return []
		history_items = self._complete_history if n_last is None else self._complete_history[-n_last:]
		screenshots = []
		for item in history_items:
			screenshot_b64 = item.state.get_screenshot()
			if screenshot_b64:
				screenshots.append(screenshot_b64)
			else:
				if return_none_if_not_screenshot:
					screenshots.append(None)
		return screenshots

	def action_results(self) -> list[CodeAgentResult]:
		"""Get all results from history."""
		results = []
		for h in self._complete_history:
			results.extend([r for r in h.result if r])
		return results

	def extracted_content(self) -> list[str]:
		"""Get all extracted content from history."""
		content = []
		for h in self._complete_history:
			content.extend([r.extracted_content for r in h.result if r.extracted_content])
		return content

	def number_of_steps(self) -> int:
		"""Get the number of steps in the history."""
		return len(self._complete_history)

	def total_duration_seconds(self) -> float:
		"""Get total duration of all steps in seconds."""
		total = 0.0
		for h in self._complete_history:
			if h.metadata:
				total += h.metadata.duration_seconds
		return total

	def last_action(self) -> None | dict:
		"""Last action in history - returns the last code execution."""
		if self._complete_history and self._complete_history[-1].model_output:
			return {
				'execute_code': {
					'code': self._complete_history[-1].model_output.model_output,
					'full_response': self._complete_history[-1].model_output.full_response,
				}
			}
		return None

	def action_names(self) -> list[str]:
		"""Get all action names from history - returns 'execute_code' for each code execution."""
		action_names = []
		for action in self.model_actions():
			actions = list(action.keys())
			if actions:
				action_names.append(actions[0])
		return action_names

	def model_thoughts(self) -> list[Any]:
		"""Get all thoughts from history - returns model_output for CodeAgent."""
		return [h.model_output for h in self._complete_history if h.model_output]

	def model_outputs(self) -> list[CodeAgentModelOutput]:
		"""Get all model outputs from history."""
		return [h.model_output for h in self._complete_history if h.model_output]

	def model_actions(self) -> list[dict]:
		"""Get all actions from history - returns code execution actions with their code."""
		actions = []
		for h in self._complete_history:
			if h.model_output:
				# Create one action dict per result (code execution)
				for _ in h.result:
					action_dict = {
						'execute_code': {
							'code': h.model_output.model_output,
							'full_response': h.model_output.full_response,
						}
					}
					actions.append(action_dict)
		return actions

	def action_history(self) -> list[list[dict]]:
		"""Get truncated action history grouped by step."""
		step_outputs = []
		for h in self._complete_history:
			step_actions = []
			if h.model_output:
				for result in h.result:
					action_dict = {
						'execute_code': {
							'code': h.model_output.model_output,
						},
						'result': {
							'extracted_content': result.extracted_content,
							'is_done': result.is_done,
							'success': result.success,
							'error': result.error,
						},
					}
					step_actions.append(action_dict)
			step_outputs.append(step_actions)
		return step_outputs

	def model_actions_filtered(self, include: list[str] | None = None) -> list[dict]:
		"""Get all model actions from history filtered - returns empty for CodeAgent."""
		return []

	def add_item(self, history_item: CodeAgentHistory) -> None:
		"""Add a history item to the list."""
		self._complete_history.append(history_item)

	def model_dump(self, **kwargs) -> dict[str, Any]:
		"""Custom serialization for CodeAgentHistoryList."""
		return {
			'history': [h.model_dump(**kwargs) for h in self._complete_history],
			'usage': self._usage_summary.model_dump() if self._usage_summary else None,
		}

	def save_to_file(self, filepath: str | Path, sensitive_data: dict[str, str | dict[str, str]] | None = None) -> None:
		"""Save history to JSON file."""
		try:
			Path(filepath).parent.mkdir(parents=True, exist_ok=True)
			data = self.model_dump()
			with open(filepath, 'w', encoding='utf-8') as f:
				json.dump(data, f, indent=2)
		except Exception as e:
			raise e

```

---

## backend/browser-use/browser_use/config.py

```py
"""Configuration system for browser-use with automatic migration support."""

import json
import logging
import os
from datetime import datetime
from functools import cache
from pathlib import Path
from typing import Any
from uuid import uuid4

import psutil
from pydantic import BaseModel, ConfigDict, Field
from pydantic_settings import BaseSettings, SettingsConfigDict

logger = logging.getLogger(__name__)


@cache
def is_running_in_docker() -> bool:
	"""Detect if we are running in a docker container, for the purpose of optimizing chrome launch flags (dev shm usage, gpu settings, etc.)"""
	try:
		if Path('/.dockerenv').exists() or 'docker' in Path('/proc/1/cgroup').read_text().lower():
			return True
	except Exception:
		pass

	try:
		# if init proc (PID 1) looks like uvicorn/python/uv/etc. then we're in Docker
		# if init proc (PID 1) looks like bash/systemd/init/etc. then we're probably NOT in Docker
		init_cmd = ' '.join(psutil.Process(1).cmdline())
		if ('py' in init_cmd) or ('uv' in init_cmd) or ('app' in init_cmd):
			return True
	except Exception:
		pass

	try:
		# if less than 10 total running procs, then we're almost certainly in a container
		if len(psutil.pids()) < 10:
			return True
	except Exception:
		pass

	return False


class OldConfig:
	"""Original lazy-loading configuration class for environment variables."""

	# Cache for directory creation tracking
	_dirs_created = False

	@property
	def BROWSER_USE_LOGGING_LEVEL(self) -> str:
		return os.getenv('BROWSER_USE_LOGGING_LEVEL', 'info').lower()

	@property
	def ANONYMIZED_TELEMETRY(self) -> bool:
		return os.getenv('ANONYMIZED_TELEMETRY', 'true').lower()[:1] in 'ty1'

	@property
	def BROWSER_USE_CLOUD_SYNC(self) -> bool:
		return os.getenv('BROWSER_USE_CLOUD_SYNC', str(self.ANONYMIZED_TELEMETRY)).lower()[:1] in 'ty1'

	@property
	def BROWSER_USE_CLOUD_API_URL(self) -> str:
		url = os.getenv('BROWSER_USE_CLOUD_API_URL', 'https://api.browser-use.com')
		assert '://' in url, 'BROWSER_USE_CLOUD_API_URL must be a valid URL'
		return url

	@property
	def BROWSER_USE_CLOUD_UI_URL(self) -> str:
		url = os.getenv('BROWSER_USE_CLOUD_UI_URL', '')
		# Allow empty string as default, only validate if set
		if url and '://' not in url:
			raise AssertionError('BROWSER_USE_CLOUD_UI_URL must be a valid URL if set')
		return url

	# Path configuration
	@property
	def XDG_CACHE_HOME(self) -> Path:
		return Path(os.getenv('XDG_CACHE_HOME', '~/.cache')).expanduser().resolve()

	@property
	def XDG_CONFIG_HOME(self) -> Path:
		return Path(os.getenv('XDG_CONFIG_HOME', '~/.config')).expanduser().resolve()

	@property
	def BROWSER_USE_CONFIG_DIR(self) -> Path:
		path = Path(os.getenv('BROWSER_USE_CONFIG_DIR', str(self.XDG_CONFIG_HOME / 'browseruse'))).expanduser().resolve()
		self._ensure_dirs()
		return path

	@property
	def BROWSER_USE_CONFIG_FILE(self) -> Path:
		return self.BROWSER_USE_CONFIG_DIR / 'config.json'

	@property
	def BROWSER_USE_PROFILES_DIR(self) -> Path:
		path = self.BROWSER_USE_CONFIG_DIR / 'profiles'
		self._ensure_dirs()
		return path

	@property
	def BROWSER_USE_DEFAULT_USER_DATA_DIR(self) -> Path:
		return self.BROWSER_USE_PROFILES_DIR / 'default'

	@property
	def BROWSER_USE_EXTENSIONS_DIR(self) -> Path:
		path = self.BROWSER_USE_CONFIG_DIR / 'extensions'
		self._ensure_dirs()
		return path

	def _ensure_dirs(self) -> None:
		"""Create directories if they don't exist (only once)"""
		if not self._dirs_created:
			config_dir = (
				Path(os.getenv('BROWSER_USE_CONFIG_DIR', str(self.XDG_CONFIG_HOME / 'browseruse'))).expanduser().resolve()
			)
			config_dir.mkdir(parents=True, exist_ok=True)
			(config_dir / 'profiles').mkdir(parents=True, exist_ok=True)
			(config_dir / 'extensions').mkdir(parents=True, exist_ok=True)
			self._dirs_created = True

	# LLM API key configuration
	@property
	def OPENAI_API_KEY(self) -> str:
		return os.getenv('OPENAI_API_KEY', '')

	@property
	def ANTHROPIC_API_KEY(self) -> str:
		return os.getenv('ANTHROPIC_API_KEY', '')

	@property
	def GOOGLE_API_KEY(self) -> str:
		return os.getenv('GOOGLE_API_KEY', '')

	@property
	def DEEPSEEK_API_KEY(self) -> str:
		return os.getenv('DEEPSEEK_API_KEY', '')

	@property
	def GROK_API_KEY(self) -> str:
		return os.getenv('GROK_API_KEY', '')

	@property
	def NOVITA_API_KEY(self) -> str:
		return os.getenv('NOVITA_API_KEY', '')

	@property
	def AZURE_OPENAI_ENDPOINT(self) -> str:
		return os.getenv('AZURE_OPENAI_ENDPOINT', '')

	@property
	def AZURE_OPENAI_KEY(self) -> str:
		return os.getenv('AZURE_OPENAI_KEY', '')

	@property
	def SKIP_LLM_API_KEY_VERIFICATION(self) -> bool:
		return os.getenv('SKIP_LLM_API_KEY_VERIFICATION', 'false').lower()[:1] in 'ty1'

	@property
	def DEFAULT_LLM(self) -> str:
		return os.getenv('DEFAULT_LLM', '')

	# Runtime hints
	@property
	def IN_DOCKER(self) -> bool:
		return os.getenv('IN_DOCKER', 'false').lower()[:1] in 'ty1' or is_running_in_docker()

	@property
	def IS_IN_EVALS(self) -> bool:
		return os.getenv('IS_IN_EVALS', 'false').lower()[:1] in 'ty1'

	@property
	def BROWSER_USE_VERSION_CHECK(self) -> bool:
		return os.getenv('BROWSER_USE_VERSION_CHECK', 'true').lower()[:1] in 'ty1'

	@property
	def WIN_FONT_DIR(self) -> str:
		return os.getenv('WIN_FONT_DIR', 'C:\\Windows\\Fonts')


class FlatEnvConfig(BaseSettings):
	"""All environment variables in a flat namespace."""

	model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8', case_sensitive=True, extra='allow')

	# Logging and telemetry
	BROWSER_USE_LOGGING_LEVEL: str = Field(default='info')
	CDP_LOGGING_LEVEL: str = Field(default='WARNING')
	BROWSER_USE_DEBUG_LOG_FILE: str | None = Field(default=None)
	BROWSER_USE_INFO_LOG_FILE: str | None = Field(default=None)
	ANONYMIZED_TELEMETRY: bool = Field(default=True)
	BROWSER_USE_CLOUD_SYNC: bool | None = Field(default=None)
	BROWSER_USE_CLOUD_API_URL: str = Field(default='https://api.browser-use.com')
	BROWSER_USE_CLOUD_UI_URL: str = Field(default='')

	# Path configuration
	XDG_CACHE_HOME: str = Field(default='~/.cache')
	XDG_CONFIG_HOME: str = Field(default='~/.config')
	BROWSER_USE_CONFIG_DIR: str | None = Field(default=None)

	# LLM API keys
	OPENAI_API_KEY: str = Field(default='')
	ANTHROPIC_API_KEY: str = Field(default='')
	GOOGLE_API_KEY: str = Field(default='')
	DEEPSEEK_API_KEY: str = Field(default='')
	GROK_API_KEY: str = Field(default='')
	NOVITA_API_KEY: str = Field(default='')
	AZURE_OPENAI_ENDPOINT: str = Field(default='')
	AZURE_OPENAI_KEY: str = Field(default='')
	SKIP_LLM_API_KEY_VERIFICATION: bool = Field(default=False)
	DEFAULT_LLM: str = Field(default='')

	# Runtime hints
	IN_DOCKER: bool | None = Field(default=None)
	IS_IN_EVALS: bool = Field(default=False)
	WIN_FONT_DIR: str = Field(default='C:\\Windows\\Fonts')
	BROWSER_USE_VERSION_CHECK: bool = Field(default=True)

	# MCP-specific env vars
	BROWSER_USE_CONFIG_PATH: str | None = Field(default=None)
	BROWSER_USE_HEADLESS: bool | None = Field(default=None)
	BROWSER_USE_ALLOWED_DOMAINS: str | None = Field(default=None)
	BROWSER_USE_LLM_MODEL: str | None = Field(default=None)

	# Proxy env vars
	BROWSER_USE_PROXY_URL: str | None = Field(default=None)
	BROWSER_USE_NO_PROXY: str | None = Field(default=None)
	BROWSER_USE_PROXY_USERNAME: str | None = Field(default=None)
	BROWSER_USE_PROXY_PASSWORD: str | None = Field(default=None)


class DBStyleEntry(BaseModel):
	"""Database-style entry with UUID and metadata."""

	id: str = Field(default_factory=lambda: str(uuid4()))
	default: bool = Field(default=False)
	created_at: str = Field(default_factory=lambda: datetime.utcnow().isoformat())


class BrowserProfileEntry(DBStyleEntry):
	"""Browser profile configuration entry - accepts any BrowserProfile fields."""

	model_config = ConfigDict(extra='allow')

	# Common browser profile fields for reference
	headless: bool | None = None
	user_data_dir: str | None = None
	allowed_domains: list[str] | None = None
	downloads_path: str | None = None


class LLMEntry(DBStyleEntry):
	"""LLM configuration entry."""

	api_key: str | None = None
	model: str | None = None
	temperature: float | None = None
	max_tokens: int | None = None


class AgentEntry(DBStyleEntry):
	"""Agent configuration entry."""

	max_steps: int | None = None
	use_vision: bool | None = None
	system_prompt: str | None = None


class DBStyleConfigJSON(BaseModel):
	"""New database-style configuration format."""

	browser_profile: dict[str, BrowserProfileEntry] = Field(default_factory=dict)
	llm: dict[str, LLMEntry] = Field(default_factory=dict)
	agent: dict[str, AgentEntry] = Field(default_factory=dict)


def create_default_config() -> DBStyleConfigJSON:
	"""Create a fresh default configuration."""
	logger.debug('Creating fresh default config.json')

	new_config = DBStyleConfigJSON()

	# Generate default IDs
	profile_id = str(uuid4())
	llm_id = str(uuid4())
	agent_id = str(uuid4())

	# Create default browser profile entry
	new_config.browser_profile[profile_id] = BrowserProfileEntry(id=profile_id, default=True, headless=False, user_data_dir=None)

	# Create default LLM entry
	new_config.llm[llm_id] = LLMEntry(id=llm_id, default=True, model='gpt-4.1-mini', api_key='your-openai-api-key-here')

	# Create default agent entry
	new_config.agent[agent_id] = AgentEntry(id=agent_id, default=True)

	return new_config


def load_and_migrate_config(config_path: Path) -> DBStyleConfigJSON:
	"""Load config.json or create fresh one if old format detected."""
	if not config_path.exists():
		# Create fresh config with defaults
		config_path.parent.mkdir(parents=True, exist_ok=True)
		new_config = create_default_config()
		with open(config_path, 'w') as f:
			json.dump(new_config.model_dump(), f, indent=2)
		return new_config

	try:
		with open(config_path) as f:
			data = json.load(f)

		# Check if it's already in DB-style format
		if all(key in data for key in ['browser_profile', 'llm', 'agent']) and all(
			isinstance(data.get(key, {}), dict) for key in ['browser_profile', 'llm', 'agent']
		):
			# Check if the values are DB-style entries (have UUIDs as keys)
			if data.get('browser_profile') and all(isinstance(v, dict) and 'id' in v for v in data['browser_profile'].values()):
				# Already in new format
				return DBStyleConfigJSON(**data)

		# Old format detected - delete it and create fresh config
		logger.debug(f'Old config format detected at {config_path}, creating fresh config')
		new_config = create_default_config()

		# Overwrite with new config
		with open(config_path, 'w') as f:
			json.dump(new_config.model_dump(), f, indent=2)

		logger.debug(f'Created fresh config.json at {config_path}')
		return new_config

	except Exception as e:
		logger.error(f'Failed to load config from {config_path}: {e}, creating fresh config')
		# On any error, create fresh config
		new_config = create_default_config()
		try:
			with open(config_path, 'w') as f:
				json.dump(new_config.model_dump(), f, indent=2)
		except Exception as write_error:
			logger.error(f'Failed to write fresh config: {write_error}')
		return new_config


class Config:
	"""Backward-compatible configuration class that merges all config sources.

	Re-reads environment variables on every access to maintain compatibility.
	"""

	def __init__(self):
		# Cache for directory creation tracking only
		self._dirs_created = False

	def __getattr__(self, name: str) -> Any:
		"""Dynamically proxy all attributes to fresh instances.

		This ensures env vars are re-read on every access.
		"""
		# Special handling for internal attributes
		if name.startswith('_'):
			raise AttributeError(f"'{self.__class__.__name__}' object has no attribute '{name}'")

		# Create fresh instances on every access
		old_config = OldConfig()

		# Always use old config for all attributes (it handles env vars with proper transformations)
		if hasattr(old_config, name):
			return getattr(old_config, name)

		# For new MCP-specific attributes not in old config
		env_config = FlatEnvConfig()
		if hasattr(env_config, name):
			return getattr(env_config, name)

		# Handle special methods
		if name == 'get_default_profile':
			return lambda: self._get_default_profile()
		elif name == 'get_default_llm':
			return lambda: self._get_default_llm()
		elif name == 'get_default_agent':
			return lambda: self._get_default_agent()
		elif name == 'load_config':
			return lambda: self._load_config()
		elif name == '_ensure_dirs':
			return lambda: old_config._ensure_dirs()

		raise AttributeError(f"'{self.__class__.__name__}' object has no attribute '{name}'")

	def _get_config_path(self) -> Path:
		"""Get config path from fresh env config."""
		env_config = FlatEnvConfig()
		if env_config.BROWSER_USE_CONFIG_PATH:
			return Path(env_config.BROWSER_USE_CONFIG_PATH).expanduser()
		elif env_config.BROWSER_USE_CONFIG_DIR:
			return Path(env_config.BROWSER_USE_CONFIG_DIR).expanduser() / 'config.json'
		else:
			xdg_config = Path(env_config.XDG_CONFIG_HOME).expanduser()
			return xdg_config / 'browseruse' / 'config.json'

	def _get_db_config(self) -> DBStyleConfigJSON:
		"""Load and migrate config.json."""
		config_path = self._get_config_path()
		return load_and_migrate_config(config_path)

	def _get_default_profile(self) -> dict[str, Any]:
		"""Get the default browser profile configuration."""
		db_config = self._get_db_config()
		for profile in db_config.browser_profile.values():
			if profile.default:
				return profile.model_dump(exclude_none=True)

		# Return first profile if no default
		if db_config.browser_profile:
			return next(iter(db_config.browser_profile.values())).model_dump(exclude_none=True)

		return {}

	def _get_default_llm(self) -> dict[str, Any]:
		"""Get the default LLM configuration."""
		db_config = self._get_db_config()
		for llm in db_config.llm.values():
			if llm.default:
				return llm.model_dump(exclude_none=True)

		# Return first LLM if no default
		if db_config.llm:
			return next(iter(db_config.llm.values())).model_dump(exclude_none=True)

		return {}

	def _get_default_agent(self) -> dict[str, Any]:
		"""Get the default agent configuration."""
		db_config = self._get_db_config()
		for agent in db_config.agent.values():
			if agent.default:
				return agent.model_dump(exclude_none=True)

		# Return first agent if no default
		if db_config.agent:
			return next(iter(db_config.agent.values())).model_dump(exclude_none=True)

		return {}

	def _load_config(self) -> dict[str, Any]:
		"""Load configuration with env var overrides for MCP components."""
		config = {
			'browser_profile': self._get_default_profile(),
			'llm': self._get_default_llm(),
			'agent': self._get_default_agent(),
		}

		# Fresh env config for overrides
		env_config = FlatEnvConfig()

		# Apply MCP-specific env var overrides
		if env_config.BROWSER_USE_HEADLESS is not None:
			config['browser_profile']['headless'] = env_config.BROWSER_USE_HEADLESS

		if env_config.BROWSER_USE_ALLOWED_DOMAINS:
			domains = [d.strip() for d in env_config.BROWSER_USE_ALLOWED_DOMAINS.split(',') if d.strip()]
			config['browser_profile']['allowed_domains'] = domains

		# Proxy settings (Chromium) -> consolidated `proxy` dict
		proxy_dict: dict[str, Any] = {}
		if env_config.BROWSER_USE_PROXY_URL:
			proxy_dict['server'] = env_config.BROWSER_USE_PROXY_URL
		if env_config.BROWSER_USE_NO_PROXY:
			# store bypass as comma-separated string to match Chrome flag
			proxy_dict['bypass'] = ','.join([d.strip() for d in env_config.BROWSER_USE_NO_PROXY.split(',') if d.strip()])
		if env_config.BROWSER_USE_PROXY_USERNAME:
			proxy_dict['username'] = env_config.BROWSER_USE_PROXY_USERNAME
		if env_config.BROWSER_USE_PROXY_PASSWORD:
			proxy_dict['password'] = env_config.BROWSER_USE_PROXY_PASSWORD
		if proxy_dict:
			# ensure section exists
			config.setdefault('browser_profile', {})
			config['browser_profile']['proxy'] = proxy_dict

		if env_config.OPENAI_API_KEY:
			config['llm']['api_key'] = env_config.OPENAI_API_KEY

		if env_config.BROWSER_USE_LLM_MODEL:
			config['llm']['model'] = env_config.BROWSER_USE_LLM_MODEL

		return config


# Create singleton instance
CONFIG = Config()


# Helper functions for MCP components
def load_browser_use_config() -> dict[str, Any]:
	"""Load browser-use configuration for MCP components."""
	return CONFIG.load_config()


def get_default_profile(config: dict[str, Any]) -> dict[str, Any]:
	"""Get default browser profile from config dict."""
	return config.get('browser_profile', {})


def get_default_llm(config: dict[str, Any]) -> dict[str, Any]:
	"""Get default LLM config from config dict."""
	return config.get('llm', {})

```

---

## backend/browser-use/browser_use/controller/__init__.py

```py
from browser_use.tools.service import Controller

__all__ = ['Controller']

```

---

## backend/browser-use/browser_use/dom/enhanced_snapshot.py

```py
"""
Enhanced snapshot processing for browser-use DOM tree extraction.

This module provides stateless functions for parsing Chrome DevTools Protocol (CDP) DOMSnapshot data
to extract visibility, clickability, cursor styles, and other layout information.
"""

from cdp_use.cdp.domsnapshot.commands import CaptureSnapshotReturns
from cdp_use.cdp.domsnapshot.types import (
	LayoutTreeSnapshot,
	NodeTreeSnapshot,
	RareBooleanData,
)

from browser_use.dom.views import DOMRect, EnhancedSnapshotNode

# Only the ESSENTIAL computed styles for interactivity and visibility detection
REQUIRED_COMPUTED_STYLES = [
	# Only styles actually accessed in the codebase (prevents Chrome crashes on heavy sites)
	'display',  # Used in service.py visibility detection
	'visibility',  # Used in service.py visibility detection
	'opacity',  # Used in service.py visibility detection
	'overflow',  # Used in views.py scrollability detection
	'overflow-x',  # Used in views.py scrollability detection
	'overflow-y',  # Used in views.py scrollability detection
	'cursor',  # Used in enhanced_snapshot.py cursor extraction
	'pointer-events',  # Used for clickability logic
	'position',  # Used for visibility logic
	'background-color',  # Used for visibility logic
]


def _parse_rare_boolean_data(rare_data: RareBooleanData, index: int) -> bool | None:
	"""Parse rare boolean data from snapshot - returns True if index is in the rare data."""
	return index in rare_data['index']


def _parse_computed_styles(strings: list[str], style_indices: list[int]) -> dict[str, str]:
	"""Parse computed styles from layout tree using string indices."""
	styles = {}
	for i, style_index in enumerate(style_indices):
		if i < len(REQUIRED_COMPUTED_STYLES) and 0 <= style_index < len(strings):
			styles[REQUIRED_COMPUTED_STYLES[i]] = strings[style_index]
	return styles


def build_snapshot_lookup(
	snapshot: CaptureSnapshotReturns,
	device_pixel_ratio: float = 1.0,
) -> dict[int, EnhancedSnapshotNode]:
	"""Build a lookup table of backend node ID to enhanced snapshot data with everything calculated upfront."""
	snapshot_lookup: dict[int, EnhancedSnapshotNode] = {}

	if not snapshot['documents']:
		return snapshot_lookup

	strings = snapshot['strings']

	for document in snapshot['documents']:
		nodes: NodeTreeSnapshot = document['nodes']
		layout: LayoutTreeSnapshot = document['layout']

		# Build backend node id to snapshot index lookup
		backend_node_to_snapshot_index = {}
		if 'backendNodeId' in nodes:
			for i, backend_node_id in enumerate(nodes['backendNodeId']):
				backend_node_to_snapshot_index[backend_node_id] = i

		# PERFORMANCE: Pre-build layout index map to eliminate O(nÂ²) double lookups
		# Preserve original behavior: use FIRST occurrence for duplicates
		layout_index_map = {}
		if layout and 'nodeIndex' in layout:
			for layout_idx, node_index in enumerate(layout['nodeIndex']):
				if node_index not in layout_index_map:  # Only store first occurrence
					layout_index_map[node_index] = layout_idx

		# Build snapshot lookup for each backend node id
		for backend_node_id, snapshot_index in backend_node_to_snapshot_index.items():
			is_clickable = None
			if 'isClickable' in nodes:
				is_clickable = _parse_rare_boolean_data(nodes['isClickable'], snapshot_index)

			# Find corresponding layout node
			cursor_style = None
			is_visible = None
			bounding_box = None
			computed_styles = {}

			# Look for layout tree node that corresponds to this snapshot node
			paint_order = None
			client_rects = None
			scroll_rects = None
			stacking_contexts = None
			if snapshot_index in layout_index_map:
				layout_idx = layout_index_map[snapshot_index]
				if layout_idx < len(layout.get('bounds', [])):
					# Parse bounding box
					bounds = layout['bounds'][layout_idx]
					if len(bounds) >= 4:
						# IMPORTANT: CDP coordinates are in device pixels, convert to CSS pixels
						# by dividing by the device pixel ratio
						raw_x, raw_y, raw_width, raw_height = bounds[0], bounds[1], bounds[2], bounds[3]

						# Apply device pixel ratio scaling to convert device pixels to CSS pixels
						bounding_box = DOMRect(
							x=raw_x / device_pixel_ratio,
							y=raw_y / device_pixel_ratio,
							width=raw_width / device_pixel_ratio,
							height=raw_height / device_pixel_ratio,
						)

					# Parse computed styles for this layout node
					if layout_idx < len(layout.get('styles', [])):
						style_indices = layout['styles'][layout_idx]
						computed_styles = _parse_computed_styles(strings, style_indices)
						cursor_style = computed_styles.get('cursor')

					# Extract paint order if available
					if layout_idx < len(layout.get('paintOrders', [])):
						paint_order = layout.get('paintOrders', [])[layout_idx]

					# Extract client rects if available
					client_rects_data = layout.get('clientRects', [])
					if layout_idx < len(client_rects_data):
						client_rect_data = client_rects_data[layout_idx]
						if client_rect_data and len(client_rect_data) >= 4:
							client_rects = DOMRect(
								x=client_rect_data[0],
								y=client_rect_data[1],
								width=client_rect_data[2],
								height=client_rect_data[3],
							)

					# Extract scroll rects if available
					scroll_rects_data = layout.get('scrollRects', [])
					if layout_idx < len(scroll_rects_data):
						scroll_rect_data = scroll_rects_data[layout_idx]
						if scroll_rect_data and len(scroll_rect_data) >= 4:
							scroll_rects = DOMRect(
								x=scroll_rect_data[0],
								y=scroll_rect_data[1],
								width=scroll_rect_data[2],
								height=scroll_rect_data[3],
							)

					# Extract stacking contexts if available
					if layout_idx < len(layout.get('stackingContexts', [])):
						stacking_contexts = layout.get('stackingContexts', {}).get('index', [])[layout_idx]

			snapshot_lookup[backend_node_id] = EnhancedSnapshotNode(
				is_clickable=is_clickable,
				cursor_style=cursor_style,
				bounds=bounding_box,
				clientRects=client_rects,
				scrollRects=scroll_rects,
				computed_styles=computed_styles if computed_styles else None,
				paint_order=paint_order,
				stacking_contexts=stacking_contexts,
			)

	return snapshot_lookup

```

---

## backend/browser-use/browser_use/dom/markdown_extractor.py

```py
"""
Shared markdown extraction utilities for browser content processing.

This module provides a unified interface for extracting clean markdown from browser content,
used by both the tools service and page actor.
"""

import re
from typing import TYPE_CHECKING, Any

from browser_use.dom.serializer.html_serializer import HTMLSerializer
from browser_use.dom.service import DomService

if TYPE_CHECKING:
	from browser_use.browser.session import BrowserSession
	from browser_use.browser.watchdogs.dom_watchdog import DOMWatchdog


async def extract_clean_markdown(
	browser_session: 'BrowserSession | None' = None,
	dom_service: DomService | None = None,
	target_id: str | None = None,
	extract_links: bool = False,
) -> tuple[str, dict[str, Any]]:
	"""Extract clean markdown from browser content using enhanced DOM tree.

	This unified function can extract markdown using either a browser session (for tools service)
	or a DOM service with target ID (for page actor).

	Args:
	    browser_session: Browser session to extract content from (tools service path)
	    dom_service: DOM service instance (page actor path)
	    target_id: Target ID for the page (required when using dom_service)
	    extract_links: Whether to preserve links in markdown

	Returns:
	    tuple: (clean_markdown_content, content_statistics)

	Raises:
	    ValueError: If neither browser_session nor (dom_service + target_id) are provided
	"""
	# Validate input parameters
	if browser_session is not None:
		if dom_service is not None or target_id is not None:
			raise ValueError('Cannot specify both browser_session and dom_service/target_id')
		# Browser session path (tools service)
		enhanced_dom_tree = await _get_enhanced_dom_tree_from_browser_session(browser_session)
		current_url = await browser_session.get_current_page_url()
		method = 'enhanced_dom_tree'
	elif dom_service is not None and target_id is not None:
		# DOM service path (page actor)
		# Lazy fetch all_frames inside get_dom_tree if needed (for cross-origin iframes)
		enhanced_dom_tree, _ = await dom_service.get_dom_tree(target_id=target_id, all_frames=None)
		current_url = None  # Not available via DOM service
		method = 'dom_service'
	else:
		raise ValueError('Must provide either browser_session or both dom_service and target_id')

	# Use the HTML serializer with the enhanced DOM tree
	html_serializer = HTMLSerializer(extract_links=extract_links)
	page_html = html_serializer.serialize(enhanced_dom_tree)

	original_html_length = len(page_html)

	# Use markdownify for clean markdown conversion
	from markdownify import markdownify as md

	content = md(
		page_html,
		heading_style='ATX',  # Use # style headings
		strip=['script', 'style'],  # Remove these tags
		bullets='-',  # Use - for unordered lists
		code_language='',  # Don't add language to code blocks
		escape_asterisks=False,  # Don't escape asterisks (cleaner output)
		escape_underscores=False,  # Don't escape underscores (cleaner output)
		escape_misc=False,  # Don't escape other characters (cleaner output)
		autolinks=False,  # Don't convert URLs to <> format
		default_title=False,  # Don't add default title attributes
		keep_inline_images_in=[],  # Don't keep inline images in any tags (we already filter base64 in HTML)
	)

	initial_markdown_length = len(content)

	# Minimal cleanup - markdownify already does most of the work
	content = re.sub(r'%[0-9A-Fa-f]{2}', '', content)  # Remove any remaining URL encoding

	# Apply light preprocessing to clean up excessive whitespace
	content, chars_filtered = _preprocess_markdown_content(content)

	final_filtered_length = len(content)

	# Content statistics
	stats = {
		'method': method,
		'original_html_chars': original_html_length,
		'initial_markdown_chars': initial_markdown_length,
		'filtered_chars_removed': chars_filtered,
		'final_filtered_chars': final_filtered_length,
	}

	# Add URL to stats if available
	if current_url:
		stats['url'] = current_url

	return content, stats


async def _get_enhanced_dom_tree_from_browser_session(browser_session: 'BrowserSession'):
	"""Get enhanced DOM tree from browser session via DOMWatchdog."""
	# Get the enhanced DOM tree from DOMWatchdog
	# This captures the current state of the page including dynamic content, shadow roots, etc.
	dom_watchdog: DOMWatchdog | None = browser_session._dom_watchdog
	assert dom_watchdog is not None, 'DOMWatchdog not available'

	# Use cached enhanced DOM tree if available, otherwise build it
	if dom_watchdog.enhanced_dom_tree is not None:
		return dom_watchdog.enhanced_dom_tree

	# Build the enhanced DOM tree if not cached
	await dom_watchdog._build_dom_tree_without_highlights()
	enhanced_dom_tree = dom_watchdog.enhanced_dom_tree
	assert enhanced_dom_tree is not None, 'Enhanced DOM tree not available'

	return enhanced_dom_tree


# Legacy aliases removed - all code now uses the unified extract_clean_markdown function


def _preprocess_markdown_content(content: str, max_newlines: int = 3) -> tuple[str, int]:
	"""
	Light preprocessing of markdown output - minimal cleanup with JSON blob removal.

	Args:
	    content: Markdown content to lightly filter
	    max_newlines: Maximum consecutive newlines to allow

	Returns:
	    tuple: (filtered_content, chars_filtered)
	"""
	original_length = len(content)

	# Remove JSON blobs (common in SPAs like LinkedIn, Facebook, etc.)
	# These are often embedded as `{"key":"value",...}` and can be massive
	# Match JSON objects/arrays that are at least 100 chars long
	# This catches SPA state/config data without removing small inline JSON
	content = re.sub(r'`\{["\w].*?\}`', '', content, flags=re.DOTALL)  # Remove JSON in code blocks
	content = re.sub(r'\{"\$type":[^}]{100,}\}', '', content)  # Remove JSON with $type fields (common pattern)
	content = re.sub(r'\{"[^"]{5,}":\{[^}]{100,}\}', '', content)  # Remove nested JSON objects

	# Compress consecutive newlines (4+ newlines become max_newlines)
	content = re.sub(r'\n{4,}', '\n' * max_newlines, content)

	# Remove lines that are only whitespace or very short (likely artifacts)
	lines = content.split('\n')
	filtered_lines = []
	for line in lines:
		stripped = line.strip()
		# Keep lines with substantial content
		if len(stripped) > 2:
			# Skip lines that look like JSON (start with { or [ and are very long)
			if (stripped.startswith('{') or stripped.startswith('[')) and len(stripped) > 100:
				continue
			filtered_lines.append(line)

	content = '\n'.join(filtered_lines)
	content = content.strip()

	chars_filtered = original_length - len(content)
	return content, chars_filtered

```

---

## backend/browser-use/browser_use/dom/playground/extraction.py

```py
import asyncio
import json
import os
import time

import anyio
import pyperclip
import tiktoken

from browser_use.agent.prompts import AgentMessagePrompt
from browser_use.browser import BrowserProfile, BrowserSession
from browser_use.browser.events import ClickElementEvent, TypeTextEvent
from browser_use.browser.profile import ViewportSize
from browser_use.dom.service import DomService
from browser_use.dom.views import DEFAULT_INCLUDE_ATTRIBUTES
from browser_use.filesystem.file_system import FileSystem

TIMEOUT = 60


async def test_focus_vs_all_elements():
	browser_session = BrowserSession(
		browser_profile=BrowserProfile(
			# executable_path='/Applications/Google Chrome.app/Contents/MacOS/Google Chrome',
			window_size=ViewportSize(width=1100, height=1000),
			disable_security=False,
			wait_for_network_idle_page_load_time=1,
			headless=False,
			args=['--incognito'],
			paint_order_filtering=True,
		),
	)

	# 10 Sample websites with various interactive elements
	sample_websites = [
		'https://browser-use.github.io/stress-tests/challenges/iframe-inception-level2.html',
		'https://www.google.com/travel/flights',
		'https://v0-simple-ui-test-site.vercel.app',
		'https://browser-use.github.io/stress-tests/challenges/iframe-inception-level1.html',
		'https://browser-use.github.io/stress-tests/challenges/angular-form.html',
		'https://www.google.com/travel/flights',
		'https://www.amazon.com/s?k=laptop',
		'https://github.com/trending',
		'https://www.reddit.com',
		'https://www.ycombinator.com/companies',
		'https://www.kayak.com/flights',
		'https://www.booking.com',
		'https://www.airbnb.com',
		'https://www.linkedin.com/jobs',
		'https://stackoverflow.com/questions',
	]

	# 5 Difficult websites with complex elements (iframes, canvas, dropdowns, etc.)
	difficult_websites = [
		'https://www.w3schools.com/html/tryit.asp?filename=tryhtml_iframe',  # Nested iframes
		'https://semantic-ui.com/modules/dropdown.html',  # Complex dropdowns
		'https://www.dezlearn.com/nested-iframes-example/',  # Cross-origin nested iframes
		'https://codepen.io/towc/pen/mJzOWJ',  # Canvas elements with interactions
		'https://jqueryui.com/accordion/',  # Complex accordion/dropdown widgets
		'https://v0-simple-landing-page-seven-xi.vercel.app/',  # Simple landing page with iframe
		'https://www.unesco.org/en',
	]

	# Descriptions for difficult websites
	difficult_descriptions = {
		'https://www.w3schools.com/html/tryit.asp?filename=tryhtml_iframe': 'ğŸ”¸ NESTED IFRAMES: Multiple iframe layers',
		'https://semantic-ui.com/modules/dropdown.html': 'ğŸ”¸ COMPLEX DROPDOWNS: Custom dropdown components',
		'https://www.dezlearn.com/nested-iframes-example/': 'ğŸ”¸ CROSS-ORIGIN IFRAMES: Different domain iframes',
		'https://codepen.io/towc/pen/mJzOWJ': 'ğŸ”¸ CANVAS ELEMENTS: Interactive canvas graphics',
		'https://jqueryui.com/accordion/': 'ğŸ”¸ ACCORDION WIDGETS: Collapsible content sections',
	}

	websites = sample_websites + difficult_websites
	current_website_index = 0

	def get_website_list_for_prompt() -> str:
		"""Get a compact website list for the input prompt."""
		lines = []
		lines.append('ğŸ“‹ Websites:')

		# Sample websites (1-10)
		for i, site in enumerate(sample_websites, 1):
			current_marker = ' â†' if (i - 1) == current_website_index else ''
			domain = site.replace('https://', '').split('/')[0]
			lines.append(f'  {i:2d}.{domain[:15]:<15}{current_marker}')

		# Difficult websites (11-15)
		for i, site in enumerate(difficult_websites, len(sample_websites) + 1):
			current_marker = ' â†' if (i - 1) == current_website_index else ''
			domain = site.replace('https://', '').split('/')[0]
			desc = difficult_descriptions.get(site, '')
			challenge = desc.split(': ')[1][:15] if ': ' in desc else ''
			lines.append(f'  {i:2d}.{domain[:15]:<15} ({challenge}){current_marker}')

		return '\n'.join(lines)

	await browser_session.start()

	# Show startup info
	print('\nğŸŒ BROWSER-USE DOM EXTRACTION TESTER')
	print(f'ğŸ“Š {len(websites)} websites total: {len(sample_websites)} standard + {len(difficult_websites)} complex')
	print('ğŸ”§ Controls: Type 1-15 to jump | Enter to re-run | "n" next | "q" quit')
	print('ğŸ’¾ Outputs: tmp/user_message.txt & tmp/element_tree.json\n')

	dom_service = DomService(browser_session)

	while True:
		# Cycle through websites
		if current_website_index >= len(websites):
			current_website_index = 0
			print('Cycled back to first website!')

		website = websites[current_website_index]
		# sleep 2
		await browser_session._cdp_navigate(website)
		await asyncio.sleep(1)

		last_clicked_index = None  # Track the index for text input
		while True:
			try:
				# 	all_elements_state = await dom_service.get_serialized_dom_tree()

				website_type = 'DIFFICULT' if website in difficult_websites else 'SAMPLE'
				print(f'\n{"=" * 60}')
				print(f'[{current_website_index + 1}/{len(websites)}] [{website_type}] Testing: {website}')
				if website in difficult_descriptions:
					print(f'{difficult_descriptions[website]}')
				print(f'{"=" * 60}')

				# Get/refresh the state (includes removing old highlights)
				print('\nGetting page state...')

				start_time = time.time()
				all_elements_state = await browser_session.get_browser_state_summary(True)
				end_time = time.time()
				get_state_time = end_time - start_time
				print(f'get_state_summary took {get_state_time:.2f} seconds')

				# Get detailed timing info from DOM service
				print('\nGetting detailed DOM timing...')
				serialized_state, _, timing_info = await dom_service.get_serialized_dom_tree()

				# Combine all timing info
				all_timing = {'get_state_summary_total': get_state_time, **timing_info}

				selector_map = all_elements_state.dom_state.selector_map
				total_elements = len(selector_map.keys())
				print(f'Total number of elements: {total_elements}')

				# print(all_elements_state.element_tree.clickable_elements_to_string())
				prompt = AgentMessagePrompt(
					browser_state_summary=all_elements_state,
					file_system=FileSystem(base_dir='./tmp'),
					include_attributes=DEFAULT_INCLUDE_ATTRIBUTES,
					step_info=None,
				)
				# Write the user message to a file for analysis
				user_message = prompt.get_user_message(use_vision=False).text

				# clickable_elements_str = all_elements_state.element_tree.clickable_elements_to_string()

				text_to_save = user_message

				os.makedirs('./tmp', exist_ok=True)
				async with await anyio.open_file('./tmp/user_message.txt', 'w', encoding='utf-8') as f:
					await f.write(text_to_save)

				# save pure clickable elements to a file
				if all_elements_state.dom_state._root:
					async with await anyio.open_file('./tmp/simplified_element_tree.json', 'w', encoding='utf-8') as f:
						await f.write(json.dumps(all_elements_state.dom_state._root.__json__(), indent=2))

					async with await anyio.open_file('./tmp/original_element_tree.json', 'w', encoding='utf-8') as f:
						await f.write(json.dumps(all_elements_state.dom_state._root.original_node.__json__(), indent=2))

				# copy the user message to the clipboard
				# pyperclip.copy(text_to_save)

				encoding = tiktoken.encoding_for_model('gpt-4.1-mini')
				token_count = len(encoding.encode(text_to_save))
				print(f'Token count: {token_count}')

				print('User message written to ./tmp/user_message.txt')
				print('Element tree written to ./tmp/simplified_element_tree.json')
				print('Original element tree written to ./tmp/original_element_tree.json')

				# Save timing information
				timing_text = 'ğŸ” DOM EXTRACTION PERFORMANCE ANALYSIS\n'
				timing_text += f'{"=" * 50}\n\n'
				timing_text += f'ğŸ“„ Website: {website}\n'
				timing_text += f'ğŸ“Š Total Elements: {total_elements}\n'
				timing_text += f'ğŸ¯ Token Count: {token_count}\n\n'

				timing_text += 'â±ï¸  TIMING BREAKDOWN:\n'
				timing_text += f'{"â”€" * 30}\n'
				for key, value in all_timing.items():
					timing_text += f'{key:<35}: {value * 1000:>8.2f} ms\n'

				# Calculate percentages
				total_time = all_timing.get('get_state_summary_total', 0)
				if total_time > 0 and total_elements > 0:
					timing_text += '\nğŸ“ˆ PERCENTAGE BREAKDOWN:\n'
					timing_text += f'{"â”€" * 30}\n'
					for key, value in all_timing.items():
						if key != 'get_state_summary_total':
							percentage = (value / total_time) * 100
							timing_text += f'{key:<35}: {percentage:>7.1f}%\n'

				timing_text += '\nğŸ¯ CLICKABLE DETECTION ANALYSIS:\n'
				timing_text += f'{"â”€" * 35}\n'
				clickable_time = all_timing.get('clickable_detection_time', 0)
				if clickable_time > 0 and total_elements > 0:
					avg_per_element = (clickable_time / total_elements) * 1000000  # microseconds
					timing_text += f'Total clickable detection time: {clickable_time * 1000:.2f} ms\n'
					timing_text += f'Average per element: {avg_per_element:.2f} Î¼s\n'
					timing_text += f'Clickable detection calls: ~{total_elements} (approx)\n'

				async with await anyio.open_file('./tmp/timing_analysis.txt', 'w', encoding='utf-8') as f:
					await f.write(timing_text)

				print('Timing analysis written to ./tmp/timing_analysis.txt')

				# also save all_elements_state.element_tree.clickable_elements_to_string() to a file
				# with open('./tmp/clickable_elements.json', 'w', encoding='utf-8') as f:
				# 	f.write(json.dumps(all_elements_state.element_tree.__json__(), indent=2))
				# print('Clickable elements written to ./tmp/clickable_elements.json')

				website_list = get_website_list_for_prompt()
				answer = input(
					"ğŸ® Enter: element index | 'index' click (clickable) | 'index,text' input | 'c,index' copy | Enter re-run | 'n' next | 'q' quit: "
				)

				if answer.lower() == 'q':
					return  # Exit completely
				elif answer.lower() == 'n':
					print('Moving to next website...')
					current_website_index += 1
					break  # Break inner loop to go to next website
				elif answer.strip() == '':
					print('Re-running extraction on current page state...')
					continue  # Continue inner loop to re-extract DOM without reloading page
				elif answer.strip().isdigit():
					# Click element format: index
					try:
						clicked_index = int(answer)
						if clicked_index in selector_map:
							element_node = selector_map[clicked_index]
							print(f'Clicking element {clicked_index}: {element_node.tag_name}')
							event = browser_session.event_bus.dispatch(ClickElementEvent(node=element_node))
							await event
							print('Click successful.')
					except ValueError:
						print(f"Invalid input: '{answer}'. Enter an index, 'index,text', 'c,index', or 'q'.")
					continue

				try:
					if answer.lower().startswith('c,'):
						# Copy element JSON format: c,index
						parts = answer.split(',', 1)
						if len(parts) == 2:
							try:
								target_index = int(parts[1].strip())
								if target_index in selector_map:
									element_node = selector_map[target_index]
									element_json = json.dumps(element_node.__json__(), indent=2, default=str)
									pyperclip.copy(element_json)
									print(f'Copied element {target_index} JSON to clipboard: {element_node.tag_name}')
								else:
									print(f'Invalid index: {target_index}')
							except ValueError:
								print(f'Invalid index format: {parts[1]}')
						else:
							print("Invalid input format. Use 'c,index'.")
					elif ',' in answer:
						# Input text format: index,text
						parts = answer.split(',', 1)
						if len(parts) == 2:
							try:
								target_index = int(parts[0].strip())
								text_to_input = parts[1]
								if target_index in selector_map:
									element_node = selector_map[target_index]
									print(
										f"Inputting text '{text_to_input}' into element {target_index}: {element_node.tag_name}"
									)

									event = await browser_session.event_bus.dispatch(
										TypeTextEvent(node=element_node, text=text_to_input)
									)

									print('Input successful.')
								else:
									print(f'Invalid index: {target_index}')
							except ValueError:
								print(f'Invalid index format: {parts[0]}')
						else:
							print("Invalid input format. Use 'index,text'.")

				except Exception as action_e:
					print(f'Action failed: {action_e}')

			# No explicit highlight removal here, get_state handles it at the start of the loop

			except Exception as e:
				print(f'Error in loop: {e}')
				# Optionally add a small delay before retrying
				await asyncio.sleep(1)


if __name__ == '__main__':
	asyncio.run(test_focus_vs_all_elements())
	# asyncio.run(test_process_html_file()) # Commented out the other test

```

---

## backend/browser-use/browser_use/dom/playground/multi_act.py

```py
from browser_use import Agent
from browser_use.browser import BrowserProfile, BrowserSession
from browser_use.browser.profile import ViewportSize
from browser_use.llm import ChatAzureOpenAI

# Initialize the Azure OpenAI client
llm = ChatAzureOpenAI(
	model='gpt-4.1-mini',
)


TASK = """
Go to https://browser-use.github.io/stress-tests/challenges/react-native-web-form.html and complete the React Native Web form by filling in all required fields and submitting.
"""


async def main():
	browser = BrowserSession(
		browser_profile=BrowserProfile(
			window_size=ViewportSize(width=1100, height=1000),
		)
	)

	agent = Agent(task=TASK, llm=llm)

	await agent.run()


if __name__ == '__main__':
	import asyncio

	asyncio.run(main())

```

---

## backend/browser-use/browser_use/dom/serializer/clickable_elements.py

```py
from browser_use.dom.views import EnhancedDOMTreeNode, NodeType


class ClickableElementDetector:
	@staticmethod
	def is_interactive(node: EnhancedDOMTreeNode) -> bool:
		"""Check if this node is clickable/interactive using enhanced scoring."""

		# Skip non-element nodes
		if node.node_type != NodeType.ELEMENT_NODE:
			return False

		# # if ax ignored skip
		# if node.ax_node and node.ax_node.ignored:
		# 	return False

		# remove html and body nodes
		if node.tag_name in {'html', 'body'}:
			return False

		# IFRAME elements should be interactive if they're large enough to potentially need scrolling
		# Small iframes (< 100px width or height) are unlikely to have scrollable content
		if node.tag_name and node.tag_name.upper() == 'IFRAME' or node.tag_name.upper() == 'FRAME':
			if node.snapshot_node and node.snapshot_node.bounds:
				width = node.snapshot_node.bounds.width
				height = node.snapshot_node.bounds.height
				# Only include iframes larger than 100x100px
				if width > 100 and height > 100:
					return True

		# RELAXED SIZE CHECK: Allow all elements including size 0 (they might be interactive overlays, etc.)
		# Note: Size 0 elements can still be interactive (e.g., invisible clickable overlays)
		# Visibility is determined separately by CSS styles, not just bounding box size

		# SEARCH ELEMENT DETECTION: Check for search-related classes and attributes
		if node.attributes:
			search_indicators = {
				'search',
				'magnify',
				'glass',
				'lookup',
				'find',
				'query',
				'search-icon',
				'search-btn',
				'search-button',
				'searchbox',
			}

			# Check class names for search indicators
			class_list = node.attributes.get('class', '').lower().split()
			if any(indicator in ' '.join(class_list) for indicator in search_indicators):
				return True

			# Check id for search indicators
			element_id = node.attributes.get('id', '').lower()
			if any(indicator in element_id for indicator in search_indicators):
				return True

			# Check data attributes for search functionality
			for attr_name, attr_value in node.attributes.items():
				if attr_name.startswith('data-') and any(indicator in attr_value.lower() for indicator in search_indicators):
					return True

		# Enhanced accessibility property checks - direct clear indicators only
		if node.ax_node and node.ax_node.properties:
			for prop in node.ax_node.properties:
				try:
					# aria disabled
					if prop.name == 'disabled' and prop.value:
						return False

					# aria hidden
					if prop.name == 'hidden' and prop.value:
						return False

					# Direct interactiveness indicators
					if prop.name in ['focusable', 'editable', 'settable'] and prop.value:
						return True

					# Interactive state properties (presence indicates interactive widget)
					if prop.name in ['checked', 'expanded', 'pressed', 'selected']:
						# These properties only exist on interactive elements
						return True

					# Form-related interactiveness
					if prop.name in ['required', 'autocomplete'] and prop.value:
						return True

					# Elements with keyboard shortcuts are interactive
					if prop.name == 'keyshortcuts' and prop.value:
						return True
				except (AttributeError, ValueError):
					# Skip properties we can't process
					continue

				# ENHANCED TAG CHECK: Include truly interactive elements
		# Note: 'label' removed - labels are handled by other attribute checks below - other wise labels with "for" attribute can destroy the real clickable element on apartments.com
		interactive_tags = {
			'button',
			'input',
			'select',
			'textarea',
			'a',
			'details',
			'summary',
			'option',
			'optgroup',
		}
		# Check with case-insensitive comparison
		if node.tag_name and node.tag_name.lower() in interactive_tags:
			return True

		# SVG elements need special handling - only interactive if they have explicit handlers
		# svg_tags = {'svg', 'path', 'circle', 'rect', 'polygon', 'ellipse', 'line', 'polyline', 'g'}
		# if node.tag_name in svg_tags:
		# 	# Only consider SVG elements interactive if they have:
		# 	# 1. Explicit event handlers
		# 	# 2. Interactive role attributes
		# 	# 3. Cursor pointer style
		# 	if node.attributes:
		# 		# Check for event handlers
		# 		if any(attr.startswith('on') for attr in node.attributes):
		# 			return True
		# 		# Check for interactive roles
		# 		if node.attributes.get('role') in {'button', 'link', 'menuitem'}:
		# 			return True
		# 		# Check for cursor pointer (indicating clickability)
		# 		if node.attributes.get('style') and 'cursor: pointer' in node.attributes.get('style', ''):
		# 			return True
		# 	# Otherwise, SVG elements are decorative
		# 	return False

		# Tertiary check: elements with interactive attributes
		if node.attributes:
			# Check for event handlers or interactive attributes
			interactive_attributes = {'onclick', 'onmousedown', 'onmouseup', 'onkeydown', 'onkeyup', 'tabindex'}
			if any(attr in node.attributes for attr in interactive_attributes):
				return True

			# Check for interactive ARIA roles
			if 'role' in node.attributes:
				interactive_roles = {
					'button',
					'link',
					'menuitem',
					'option',
					'radio',
					'checkbox',
					'tab',
					'textbox',
					'combobox',
					'slider',
					'spinbutton',
					'search',
					'searchbox',
				}
				if node.attributes['role'] in interactive_roles:
					return True

		# Quaternary check: accessibility tree roles
		if node.ax_node and node.ax_node.role:
			interactive_ax_roles = {
				'button',
				'link',
				'menuitem',
				'option',
				'radio',
				'checkbox',
				'tab',
				'textbox',
				'combobox',
				'slider',
				'spinbutton',
				'listbox',
				'search',
				'searchbox',
			}
			if node.ax_node.role in interactive_ax_roles:
				return True

		# ICON AND SMALL ELEMENT CHECK: Elements that might be icons
		if (
			node.snapshot_node
			and node.snapshot_node.bounds
			and 10 <= node.snapshot_node.bounds.width <= 50  # Icon-sized elements
			and 10 <= node.snapshot_node.bounds.height <= 50
		):
			# Check if this small element has interactive properties
			if node.attributes:
				# Small elements with these attributes are likely interactive icons
				icon_attributes = {'class', 'role', 'onclick', 'data-action', 'aria-label'}
				if any(attr in node.attributes for attr in icon_attributes):
					return True

		# Final fallback: cursor style indicates interactivity (for cases Chrome missed)
		if node.snapshot_node and node.snapshot_node.cursor_style and node.snapshot_node.cursor_style == 'pointer':
			return True

		return False

```

---

## backend/browser-use/browser_use/dom/serializer/code_use_serializer.py

```py
# @file purpose: Ultra-compact serializer optimized for code-use agents
# Focuses on minimal token usage while preserving essential interactive context

from browser_use.dom.utils import cap_text_length
from browser_use.dom.views import (
	EnhancedDOMTreeNode,
	NodeType,
	SimplifiedNode,
)

# Minimal but sufficient attribute list for code agents
CODE_USE_KEY_ATTRIBUTES = [
	'id',  # Essential for element selection
	'name',  # For form inputs
	'type',  # For input types
	'placeholder',  # For empty inputs
	'aria-label',  # For buttons without text
	'value',  # Current values
	'alt',  # For images
	'class',  # Keep top 2 classes for common selectors
]

# Interactive elements agent can use
INTERACTIVE_ELEMENTS = {
	'a',
	'button',
	'input',
	'textarea',
	'select',
	'form',
}

# Semantic structure elements - expanded to include more content containers
SEMANTIC_STRUCTURE = {
	'h1',
	'h2',
	'h3',
	'h4',
	'h5',
	'h6',
	'nav',
	'main',
	'header',
	'footer',
	'article',
	'section',
	'p',  # Paragraphs often contain prices and product info
	'span',  # Spans often contain prices and labels
	'div',  # Divs with useful attributes (id/class) should be shown
	'ul',
	'ol',
	'li',
	'label',
	'img',
}


class DOMCodeAgentSerializer:
	"""Optimized DOM serializer for code-use agents - balances token efficiency with context."""

	@staticmethod
	def serialize_tree(node: SimplifiedNode | None, include_attributes: list[str], depth: int = 0) -> str:
		"""
		Serialize DOM tree with smart token optimization.

		Strategy:
		- Keep top 2 CSS classes for querySelector compatibility
		- Show div/span/p elements with useful attributes or text
		- Show all interactive + semantic elements
		- Inline text up to 80 chars for better context
		"""
		if not node:
			return ''

		# Skip excluded/hidden nodes
		if hasattr(node, 'excluded_by_parent') and node.excluded_by_parent:
			return DOMCodeAgentSerializer._serialize_children(node, include_attributes, depth)

		if not node.should_display:
			return DOMCodeAgentSerializer._serialize_children(node, include_attributes, depth)

		formatted_text = []
		depth_str = '  ' * depth  # Use 2 spaces instead of tabs for compactness

		if node.original_node.node_type == NodeType.ELEMENT_NODE:
			tag = node.original_node.tag_name.lower()
			is_visible = node.original_node.snapshot_node and node.original_node.is_visible

			# Skip invisible (except iframes)
			if not is_visible and tag not in ['iframe', 'frame']:
				return DOMCodeAgentSerializer._serialize_children(node, include_attributes, depth)

			# Special handling for iframes
			if tag in ['iframe', 'frame']:
				return DOMCodeAgentSerializer._serialize_iframe(node, include_attributes, depth)

			# Build minimal attributes
			attributes_str = DOMCodeAgentSerializer._build_minimal_attributes(node.original_node)

			# Decide if element should be shown
			is_interactive = tag in INTERACTIVE_ELEMENTS
			is_semantic = tag in SEMANTIC_STRUCTURE
			has_useful_attrs = bool(attributes_str)
			has_text = DOMCodeAgentSerializer._has_direct_text(node)

			# Skip non-semantic, non-interactive containers without attributes
			if not is_interactive and not is_semantic and not has_useful_attrs and not has_text:
				return DOMCodeAgentSerializer._serialize_children(node, include_attributes, depth)

			# Collapse pointless wrappers
			if tag in {'div', 'span'} and not has_useful_attrs and not has_text and len(node.children) == 1:
				return DOMCodeAgentSerializer._serialize_children(node, include_attributes, depth)

			# Build element
			line = f'{depth_str}<{tag}'

			if attributes_str:
				line += f' {attributes_str}'

			# Inline text
			inline_text = DOMCodeAgentSerializer._get_inline_text(node)
			if inline_text:
				line += f'>{inline_text}'
			else:
				line += '>'

			formatted_text.append(line)

			# Children (only if no inline text)
			if node.children and not inline_text:
				children_text = DOMCodeAgentSerializer._serialize_children(node, include_attributes, depth + 1)
				if children_text:
					formatted_text.append(children_text)

		elif node.original_node.node_type == NodeType.TEXT_NODE:
			# Handled inline with parent
			pass

		elif node.original_node.node_type == NodeType.DOCUMENT_FRAGMENT_NODE:
			# Shadow DOM - minimal marker
			if node.children:
				formatted_text.append(f'{depth_str}#shadow')
				children_text = DOMCodeAgentSerializer._serialize_children(node, include_attributes, depth + 1)
				if children_text:
					formatted_text.append(children_text)

		return '\n'.join(formatted_text)

	@staticmethod
	def _serialize_children(node: SimplifiedNode, include_attributes: list[str], depth: int) -> str:
		"""Serialize children."""
		children_output = []
		for child in node.children:
			child_text = DOMCodeAgentSerializer.serialize_tree(child, include_attributes, depth)
			if child_text:
				children_output.append(child_text)
		return '\n'.join(children_output)

	@staticmethod
	def _build_minimal_attributes(node: EnhancedDOMTreeNode) -> str:
		"""Build minimal but useful attributes - keep top 2 classes for selectors."""
		attrs = []

		if node.attributes:
			for attr in CODE_USE_KEY_ATTRIBUTES:
				if attr in node.attributes:
					value = str(node.attributes[attr]).strip()
					if value:
						# Special handling for class - keep only first 2 classes
						if attr == 'class':
							classes = value.split()[:2]
							value = ' '.join(classes)
						# Cap at 25 chars
						value = cap_text_length(value, 25)
						attrs.append(f'{attr}="{value}"')

		return ' '.join(attrs)

	@staticmethod
	def _has_direct_text(node: SimplifiedNode) -> bool:
		"""Check if node has direct text children."""
		for child in node.children:
			if child.original_node.node_type == NodeType.TEXT_NODE:
				text = child.original_node.node_value.strip() if child.original_node.node_value else ''
				if len(text) > 1:
					return True
		return False

	@staticmethod
	def _get_inline_text(node: SimplifiedNode) -> str:
		"""Get inline text (max 80 chars for better context)."""
		text_parts = []
		for child in node.children:
			if child.original_node.node_type == NodeType.TEXT_NODE:
				text = child.original_node.node_value.strip() if child.original_node.node_value else ''
				if text and len(text) > 1:
					text_parts.append(text)

		if not text_parts:
			return ''

		combined = ' '.join(text_parts)
		return cap_text_length(combined, 40)

	@staticmethod
	def _serialize_iframe(node: SimplifiedNode, include_attributes: list[str], depth: int) -> str:
		"""Handle iframe minimally."""
		formatted_text = []
		depth_str = '  ' * depth
		tag = node.original_node.tag_name.lower()

		# Minimal iframe marker
		attributes_str = DOMCodeAgentSerializer._build_minimal_attributes(node.original_node)
		line = f'{depth_str}<{tag}'
		if attributes_str:
			line += f' {attributes_str}'
		line += '>'
		formatted_text.append(line)

		# Iframe content
		if node.original_node.content_document:
			formatted_text.append(f'{depth_str}  #iframe-content')

			# Find and serialize body content only
			for child_node in node.original_node.content_document.children_nodes or []:
				if child_node.tag_name.lower() == 'html':
					for html_child in child_node.children:
						if html_child.tag_name.lower() == 'body':
							for body_child in html_child.children:
								DOMCodeAgentSerializer._serialize_document_node(
									body_child, formatted_text, include_attributes, depth + 2
								)
							break

		return '\n'.join(formatted_text)

	@staticmethod
	def _serialize_document_node(
		dom_node: EnhancedDOMTreeNode, output: list[str], include_attributes: list[str], depth: int
	) -> None:
		"""Serialize document node without SimplifiedNode wrapper."""
		depth_str = '  ' * depth

		if dom_node.node_type == NodeType.ELEMENT_NODE:
			tag = dom_node.tag_name.lower()

			# Skip invisible
			is_visible = dom_node.snapshot_node and dom_node.is_visible
			if not is_visible:
				return

			# Check if worth showing
			is_interactive = tag in INTERACTIVE_ELEMENTS
			is_semantic = tag in SEMANTIC_STRUCTURE
			attributes_str = DOMCodeAgentSerializer._build_minimal_attributes(dom_node)

			if not is_interactive and not is_semantic and not attributes_str:
				# Skip but process children
				for child in dom_node.children:
					DOMCodeAgentSerializer._serialize_document_node(child, output, include_attributes, depth)
				return

			# Build element
			line = f'{depth_str}<{tag}'
			if attributes_str:
				line += f' {attributes_str}'

			# Get text
			text_parts = []
			for child in dom_node.children:
				if child.node_type == NodeType.TEXT_NODE and child.node_value:
					text = child.node_value.strip()
					if text and len(text) > 1:
						text_parts.append(text)

			if text_parts:
				combined = ' '.join(text_parts)
				line += f'>{cap_text_length(combined, 25)}'
			else:
				line += '>'

			output.append(line)

			# Process non-text children
			for child in dom_node.children:
				if child.node_type != NodeType.TEXT_NODE:
					DOMCodeAgentSerializer._serialize_document_node(child, output, include_attributes, depth + 1)

```

---

## backend/browser-use/browser_use/dom/serializer/eval_serializer.py

```py
# @file purpose: Concise evaluation serializer for DOM trees - optimized for LLM query writing


from browser_use.dom.utils import cap_text_length
from browser_use.dom.views import (
	EnhancedDOMTreeNode,
	NodeType,
	SimplifiedNode,
)

# Critical attributes for query writing and form interaction
# NOTE: Removed 'id' and 'class' to force more robust structural selectors
EVAL_KEY_ATTRIBUTES = [
	'id',  # Removed - can have special chars, forces structural selectors
	'class',  # Removed - can have special chars like +, forces structural selectors
	'name',
	'type',
	'placeholder',
	'aria-label',
	'role',
	'value',
	# 'href',
	'data-testid',
	'alt',  # for images
	'title',  # useful for tooltips/link context
	# State attributes (critical for form interaction)
	'checked',
	'selected',
	'disabled',
	'required',
	'readonly',
	# ARIA states
	'aria-expanded',
	'aria-pressed',
	'aria-checked',
	'aria-selected',
	'aria-invalid',
	# Validation attributes (help agents avoid brute force)
	'pattern',
	'min',
	'max',
	'minlength',
	'maxlength',
	'step',
	'aria-valuemin',
	'aria-valuemax',
	'aria-valuenow',
]

# Semantic elements that should always be shown
SEMANTIC_ELEMENTS = {
	'html',  # Always show document root
	'body',  # Always show body
	'h1',
	'h2',
	'h3',
	'h4',
	'h5',
	'h6',
	'a',
	'button',
	'input',
	'textarea',
	'select',
	'form',
	'label',
	'nav',
	'header',
	'footer',
	'main',
	'article',
	'section',
	'table',
	'thead',
	'tbody',
	'tr',
	'th',
	'td',
	'ul',
	'ol',
	'li',
	'img',
	'iframe',
	'video',
	'audio',
}

# Container elements that can be collapsed if they only wrap one child
COLLAPSIBLE_CONTAINERS = {'div', 'span', 'section', 'article'}

# SVG child elements to skip (decorative only, no interaction value)
SVG_ELEMENTS = {
	'path',
	'rect',
	'g',
	'circle',
	'ellipse',
	'line',
	'polyline',
	'polygon',
	'use',
	'defs',
	'clipPath',
	'mask',
	'pattern',
	'image',
	'text',
	'tspan',
}


class DOMEvalSerializer:
	"""Ultra-concise DOM serializer for quick LLM query writing."""

	@staticmethod
	def serialize_tree(node: SimplifiedNode | None, include_attributes: list[str], depth: int = 0) -> str:
		"""
		Serialize complete DOM tree structure for LLM understanding.

		Strategy:
		- Show ALL elements to preserve DOM structure
		- Non-interactive elements show just tag name
		- Interactive elements show full attributes + [index]
		- Self-closing tags only (no closing tags)
		"""
		if not node:
			return ''

		# Skip excluded nodes but process children
		if hasattr(node, 'excluded_by_parent') and node.excluded_by_parent:
			return DOMEvalSerializer._serialize_children(node, include_attributes, depth)

		# Skip nodes marked as should_display=False
		if not node.should_display:
			return DOMEvalSerializer._serialize_children(node, include_attributes, depth)

		formatted_text = []
		depth_str = depth * '\t'

		if node.original_node.node_type == NodeType.ELEMENT_NODE:
			tag = node.original_node.tag_name.lower()
			is_visible = node.original_node.snapshot_node and node.original_node.is_visible

			# Container elements that should be shown even if invisible (might have visible children)
			container_tags = {'html', 'body', 'div', 'main', 'section', 'article', 'aside', 'header', 'footer', 'nav'}

			# Skip invisible elements UNLESS they're containers or iframes (which might have visible children)
			if not is_visible and tag not in container_tags and tag not in ['iframe', 'frame']:
				return DOMEvalSerializer._serialize_children(node, include_attributes, depth)

			# Special handling for iframes - show them with their content
			if tag in ['iframe', 'frame']:
				return DOMEvalSerializer._serialize_iframe(node, include_attributes, depth)

			# Skip SVG elements entirely - they're just decorative graphics with no interaction value
			# Show the <svg> tag itself to indicate graphics, but don't recurse into children
			if tag == 'svg':
				line = f'{depth_str}'
				# Add [i_X] for interactive SVG elements only
				if node.is_interactive:
					line += f'[i_{node.original_node.backend_node_id}] '
				line += '<svg'
				attributes_str = DOMEvalSerializer._build_compact_attributes(node.original_node)
				if attributes_str:
					line += f' {attributes_str}'
				line += ' /> <!-- SVG content collapsed -->'
				return line

			# Skip SVG child elements entirely (path, rect, g, circle, etc.)
			if tag in SVG_ELEMENTS:
				return ''

			# Build compact attributes string
			attributes_str = DOMEvalSerializer._build_compact_attributes(node.original_node)

			# Decide if this element should be shown
			is_semantic = tag in SEMANTIC_ELEMENTS
			has_useful_attrs = bool(attributes_str)
			has_text_content = DOMEvalSerializer._has_direct_text(node)
			has_children = len(node.children) > 0

			# Build compact element representation
			line = f'{depth_str}'
			# Add backend node ID notation - [i_X] for interactive elements only
			if node.is_interactive:
				line += f'[i_{node.original_node.backend_node_id}] '
			# Non-interactive elements don't get an index notation
			line += f'<{tag}'

			if attributes_str:
				line += f' {attributes_str}'

			# Add scroll info if element is scrollable
			if node.original_node.should_show_scroll_info:
				scroll_text = node.original_node.get_scroll_info_text()
				if scroll_text:
					line += f' scroll="{scroll_text}"'

			# Add inline text if present (keep it on same line for compactness)
			inline_text = DOMEvalSerializer._get_inline_text(node)

			# For containers (html, body, div, etc.), always show children even if there's inline text
			# For other elements, inline text replaces children (more compact)
			is_container = tag in container_tags

			if inline_text and not is_container:
				line += f'>{inline_text}'
			else:
				line += ' />'

			formatted_text.append(line)

			# Process children (always for containers, only if no inline_text for others)
			if has_children and (is_container or not inline_text):
				children_text = DOMEvalSerializer._serialize_children(node, include_attributes, depth + 1)
				if children_text:
					formatted_text.append(children_text)

		elif node.original_node.node_type == NodeType.TEXT_NODE:
			# Text nodes are handled inline with their parent
			pass

		elif node.original_node.node_type == NodeType.DOCUMENT_FRAGMENT_NODE:
			# Shadow DOM - just show children directly with minimal marker
			if node.children:
				formatted_text.append(f'{depth_str}#shadow')
				children_text = DOMEvalSerializer._serialize_children(node, include_attributes, depth + 1)
				if children_text:
					formatted_text.append(children_text)

		return '\n'.join(formatted_text)

	@staticmethod
	def _serialize_children(node: SimplifiedNode, include_attributes: list[str], depth: int) -> str:
		"""Helper to serialize all children of a node."""
		children_output = []

		# Check if parent is a list container (ul, ol)
		is_list_container = node.original_node.node_type == NodeType.ELEMENT_NODE and node.original_node.tag_name.lower() in [
			'ul',
			'ol',
		]

		# Track list items and consecutive links
		li_count = 0
		max_list_items = 50
		consecutive_link_count = 0
		max_consecutive_links = 50
		total_links_skipped = 0

		for child in node.children:
			# Get tag name for this child
			current_tag = None
			if child.original_node.node_type == NodeType.ELEMENT_NODE:
				current_tag = child.original_node.tag_name.lower()

			# If we're in a list container and this child is an li element
			if is_list_container and current_tag == 'li':
				li_count += 1
				# Skip li elements after the 5th one
				if li_count > max_list_items:
					continue

			# Track consecutive anchor tags (links)
			if current_tag == 'a':
				consecutive_link_count += 1
				# Skip links after the 5th consecutive one
				if consecutive_link_count > max_consecutive_links:
					total_links_skipped += 1
					continue
			else:
				# Reset counter when we hit a non-link element
				# But first add truncation message if we skipped links
				if total_links_skipped > 0:
					depth_str = depth * '\t'
					children_output.append(f'{depth_str}... ({total_links_skipped} more links in this list)')
					total_links_skipped = 0
				consecutive_link_count = 0

			child_text = DOMEvalSerializer.serialize_tree(child, include_attributes, depth)
			if child_text:
				children_output.append(child_text)

		# Add truncation message if we skipped items at the end
		if is_list_container and li_count > max_list_items:
			depth_str = depth * '\t'
			children_output.append(
				f'{depth_str}... ({li_count - max_list_items} more items in this list (truncated) use evaluate to get more.'
			)

		# Add truncation message for links if we skipped any at the end
		if total_links_skipped > 0:
			depth_str = depth * '\t'
			children_output.append(
				f'{depth_str}... ({total_links_skipped} more links in this list) (truncated) use evaluate to get more.'
			)

		return '\n'.join(children_output)

	@staticmethod
	def _build_compact_attributes(node: EnhancedDOMTreeNode) -> str:
		"""Build ultra-compact attributes string with only key attributes."""
		attrs = []

		# Prioritize attributes that help with query writing
		if node.attributes:
			for attr in EVAL_KEY_ATTRIBUTES:
				if attr in node.attributes:
					value = str(node.attributes[attr]).strip()
					if not value:
						continue

					# Special handling for different attributes
					if attr == 'class':
						# For class, limit to first 2 classes to save space
						classes = value.split()[:3]
						value = ' '.join(classes)
					elif attr == 'href':
						# For href, cap at 20 chars to save space
						value = cap_text_length(value, 80)
					else:
						# Cap at 25 chars for other attributes
						value = cap_text_length(value, 80)

					attrs.append(f'{attr}="{value}"')

		# Note: We intentionally don't add role from ax_node here because:
		# 1. If role is explicitly set in HTML, it's already captured above via EVAL_KEY_ATTRIBUTES
		# 2. Inferred roles from AX tree (like link, listitem, LineBreak) are redundant with the tag name
		# 3. This reduces noise - <a href="..." role="link"> is redundant, we already know <a> is a link

		return ' '.join(attrs)

	@staticmethod
	def _has_direct_text(node: SimplifiedNode) -> bool:
		"""Check if node has direct text children (not nested in other elements)."""
		for child in node.children:
			if child.original_node.node_type == NodeType.TEXT_NODE:
				text = child.original_node.node_value.strip() if child.original_node.node_value else ''
				if len(text) > 1:
					return True
		return False

	@staticmethod
	def _get_inline_text(node: SimplifiedNode) -> str:
		"""Get text content to display inline (max 40 chars)."""
		text_parts = []
		for child in node.children:
			if child.original_node.node_type == NodeType.TEXT_NODE:
				text = child.original_node.node_value.strip() if child.original_node.node_value else ''
				if text and len(text) > 1:
					text_parts.append(text)

		if not text_parts:
			return ''

		combined = ' '.join(text_parts)
		return cap_text_length(combined, 80)

	@staticmethod
	def _serialize_iframe(node: SimplifiedNode, include_attributes: list[str], depth: int) -> str:
		"""Handle iframe serialization with content document."""
		formatted_text = []
		depth_str = depth * '\t'
		tag = node.original_node.tag_name.lower()

		# Build minimal iframe marker with key attributes
		attributes_str = DOMEvalSerializer._build_compact_attributes(node.original_node)
		line = f'{depth_str}<{tag}'
		if attributes_str:
			line += f' {attributes_str}'

		# Add scroll info for iframe content
		if node.original_node.should_show_scroll_info:
			scroll_text = node.original_node.get_scroll_info_text()
			if scroll_text:
				line += f' scroll="{scroll_text}"'

		line += ' />'
		formatted_text.append(line)

		# If iframe has content document, serialize its content
		if node.original_node.content_document:
			# Add marker for iframe content
			formatted_text.append(f'{depth_str}\t#iframe-content')

			# Process content document children
			for child_node in node.original_node.content_document.children_nodes or []:
				# Process html documents
				if child_node.tag_name.lower() == 'html':
					# Find and serialize body content only (skip head)
					for html_child in child_node.children:
						if html_child.tag_name.lower() == 'body':
							for body_child in html_child.children:
								# Recursively process body children (iframe content)
								DOMEvalSerializer._serialize_document_node(
									body_child, formatted_text, include_attributes, depth + 2, is_iframe_content=True
								)
							break  # Stop after processing body
				else:
					# Not an html element - serialize directly
					DOMEvalSerializer._serialize_document_node(
						child_node, formatted_text, include_attributes, depth + 1, is_iframe_content=True
					)

		return '\n'.join(formatted_text)

	@staticmethod
	def _serialize_document_node(
		dom_node: EnhancedDOMTreeNode,
		output: list[str],
		include_attributes: list[str],
		depth: int,
		is_iframe_content: bool = True,
	) -> None:
		"""Helper to serialize a document node without SimplifiedNode wrapper.

		Args:
			is_iframe_content: If True, be more permissive with visibility checks since
				iframe content might not have snapshot data from parent page.
		"""
		depth_str = depth * '\t'

		if dom_node.node_type == NodeType.ELEMENT_NODE:
			tag = dom_node.tag_name.lower()

			# For iframe content, be permissive - show all semantic elements even without snapshot data
			# For regular content, skip invisible elements
			if is_iframe_content:
				# Only skip if we have snapshot data AND it's explicitly invisible
				# If no snapshot data, assume visible (cross-origin iframe content)
				is_visible = (not dom_node.snapshot_node) or dom_node.is_visible
			else:
				# Regular strict visibility check
				is_visible = dom_node.snapshot_node and dom_node.is_visible

			if not is_visible:
				return

			# Check if semantic or has useful attributes
			is_semantic = tag in SEMANTIC_ELEMENTS
			attributes_str = DOMEvalSerializer._build_compact_attributes(dom_node)

			if not is_semantic and not attributes_str:
				# Skip but process children
				for child in dom_node.children:
					DOMEvalSerializer._serialize_document_node(
						child, output, include_attributes, depth, is_iframe_content=is_iframe_content
					)
				return

			# Build element line
			line = f'{depth_str}<{tag}'
			if attributes_str:
				line += f' {attributes_str}'

			# Get direct text content
			text_parts = []
			for child in dom_node.children:
				if child.node_type == NodeType.TEXT_NODE and child.node_value:
					text = child.node_value.strip()
					if text and len(text) > 1:
						text_parts.append(text)

			if text_parts:
				combined = ' '.join(text_parts)
				line += f'>{cap_text_length(combined, 100)}'
			else:
				line += ' />'

			output.append(line)

			# Process non-text children
			for child in dom_node.children:
				if child.node_type != NodeType.TEXT_NODE:
					DOMEvalSerializer._serialize_document_node(
						child, output, include_attributes, depth + 1, is_iframe_content=is_iframe_content
					)

```

---

## backend/browser-use/browser_use/dom/serializer/html_serializer.py

```py
# @file purpose: Serializes enhanced DOM trees to HTML format including shadow roots

from browser_use.dom.views import EnhancedDOMTreeNode, NodeType


class HTMLSerializer:
	"""Serializes enhanced DOM trees back to HTML format.

	This serializer reconstructs HTML from the enhanced DOM tree, including:
	- Shadow DOM content (both open and closed)
	- Iframe content documents
	- All attributes and text nodes
	- Proper HTML structure

	Unlike getOuterHTML which only captures light DOM, this captures the full
	enhanced tree including shadow roots that are crucial for modern SPAs.
	"""

	def __init__(self, extract_links: bool = False):
		"""Initialize the HTML serializer.

		Args:
			extract_links: If True, preserves all links. If False, removes href attributes.
		"""
		self.extract_links = extract_links

	def serialize(self, node: EnhancedDOMTreeNode, depth: int = 0) -> str:
		"""Serialize an enhanced DOM tree node to HTML.

		Args:
			node: The enhanced DOM tree node to serialize
			depth: Current depth for indentation (internal use)

		Returns:
			HTML string representation of the node and its descendants
		"""
		if node.node_type == NodeType.DOCUMENT_NODE:
			# Process document root - serialize all children
			parts = []
			for child in node.children_and_shadow_roots:
				child_html = self.serialize(child, depth)
				if child_html:
					parts.append(child_html)
			return ''.join(parts)

		elif node.node_type == NodeType.DOCUMENT_FRAGMENT_NODE:
			# Shadow DOM root - wrap in template with shadowrootmode attribute
			parts = []

			# Add shadow root opening
			shadow_type = node.shadow_root_type or 'open'
			parts.append(f'<template shadowroot="{shadow_type.lower()}">')

			# Serialize shadow children
			for child in node.children:
				child_html = self.serialize(child, depth + 1)
				if child_html:
					parts.append(child_html)

			# Close shadow root
			parts.append('</template>')

			return ''.join(parts)

		elif node.node_type == NodeType.ELEMENT_NODE:
			parts = []
			tag_name = node.tag_name.lower()

			# Skip non-content elements
			if tag_name in {'style', 'script', 'head', 'meta', 'link', 'title'}:
				return ''

			# Skip code tags with display:none - these often contain JSON state for SPAs
			if tag_name == 'code' and node.attributes:
				style = node.attributes.get('style', '')
				# Check if element is hidden (display:none) - likely JSON data
				if 'display:none' in style.replace(' ', '') or 'display: none' in style:
					return ''
				# Also check for bpr-guid IDs (LinkedIn's JSON data pattern)
				element_id = node.attributes.get('id', '')
				if 'bpr-guid' in element_id or 'data' in element_id or 'state' in element_id:
					return ''

			# Skip base64 inline images - these are usually placeholders or tracking pixels
			if tag_name == 'img' and node.attributes:
				src = node.attributes.get('src', '')
				if src.startswith('data:image/'):
					return ''

			# Opening tag
			parts.append(f'<{tag_name}')

			# Add attributes
			if node.attributes:
				attrs = self._serialize_attributes(node.attributes)
				if attrs:
					parts.append(' ' + attrs)

			# Handle void elements (self-closing)
			void_elements = {
				'area',
				'base',
				'br',
				'col',
				'embed',
				'hr',
				'img',
				'input',
				'link',
				'meta',
				'param',
				'source',
				'track',
				'wbr',
			}
			if tag_name in void_elements:
				parts.append(' />')
				return ''.join(parts)

			parts.append('>')

			# Handle iframe content document
			if tag_name in {'iframe', 'frame'} and node.content_document:
				# Serialize iframe content
				for child in node.content_document.children_nodes or []:
					child_html = self.serialize(child, depth + 1)
					if child_html:
						parts.append(child_html)
			else:
				# Serialize shadow roots FIRST (for declarative shadow DOM)
				if node.shadow_roots:
					for shadow_root in node.shadow_roots:
						child_html = self.serialize(shadow_root, depth + 1)
						if child_html:
							parts.append(child_html)

				# Then serialize light DOM children (for slot projection)
				for child in node.children:
					child_html = self.serialize(child, depth + 1)
					if child_html:
						parts.append(child_html)

			# Closing tag
			parts.append(f'</{tag_name}>')

			return ''.join(parts)

		elif node.node_type == NodeType.TEXT_NODE:
			# Return text content with basic HTML escaping
			if node.node_value:
				return self._escape_html(node.node_value)
			return ''

		elif node.node_type == NodeType.COMMENT_NODE:
			# Skip comments to reduce noise
			return ''

		else:
			# Unknown node type - skip
			return ''

	def _serialize_attributes(self, attributes: dict[str, str]) -> str:
		"""Serialize element attributes to HTML attribute string.

		Args:
			attributes: Dictionary of attribute names to values

		Returns:
			HTML attribute string (e.g., 'class="foo" id="bar"')
		"""
		parts = []
		for key, value in attributes.items():
			# Skip href if not extracting links
			if not self.extract_links and key == 'href':
				continue

			# Skip data-* attributes as they often contain JSON payloads
			# These are used by modern SPAs (React, Vue, Angular) for state management
			if key.startswith('data-'):
				continue

			# Handle boolean attributes
			if value == '' or value is None:
				parts.append(key)
			else:
				# Escape attribute value
				escaped_value = self._escape_attribute(value)
				parts.append(f'{key}="{escaped_value}"')

		return ' '.join(parts)

	def _escape_html(self, text: str) -> str:
		"""Escape HTML special characters in text content.

		Args:
			text: Raw text content

		Returns:
			HTML-escaped text
		"""
		return text.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')

	def _escape_attribute(self, value: str) -> str:
		"""Escape HTML special characters in attribute values.

		Args:
			value: Raw attribute value

		Returns:
			HTML-escaped attribute value
		"""
		return value.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;').replace('"', '&quot;').replace("'", '&#x27;')

```

---

## backend/browser-use/browser_use/dom/serializer/paint_order.py

```py
from collections import defaultdict
from dataclasses import dataclass

from browser_use.dom.views import SimplifiedNode

"""
Helper class for maintaining a union of rectangles (used for order of elements calculation)
"""


@dataclass(frozen=True, slots=True)
class Rect:
	"""Closed axis-aligned rectangle with (x1,y1) bottom-left, (x2,y2) top-right."""

	x1: float
	y1: float
	x2: float
	y2: float

	def __post_init__(self):
		if not (self.x1 <= self.x2 and self.y1 <= self.y2):
			return False

	# --- fast relations ----------------------------------------------------
	def area(self) -> float:
		return (self.x2 - self.x1) * (self.y2 - self.y1)

	def intersects(self, other: 'Rect') -> bool:
		return not (self.x2 <= other.x1 or other.x2 <= self.x1 or self.y2 <= other.y1 or other.y2 <= self.y1)

	def contains(self, other: 'Rect') -> bool:
		return self.x1 <= other.x1 and self.y1 <= other.y1 and self.x2 >= other.x2 and self.y2 >= other.y2


class RectUnionPure:
	"""
	Maintains a *disjoint* set of rectangles.
	No external dependencies - fine for a few thousand rectangles.
	"""

	__slots__ = ('_rects',)

	def __init__(self):
		self._rects: list[Rect] = []

	# -----------------------------------------------------------------
	def _split_diff(self, a: Rect, b: Rect) -> list[Rect]:
		r"""
		Return list of up to 4 rectangles = a \ b.
		Assumes a intersects b.
		"""
		parts = []

		# Bottom slice
		if a.y1 < b.y1:
			parts.append(Rect(a.x1, a.y1, a.x2, b.y1))
		# Top slice
		if b.y2 < a.y2:
			parts.append(Rect(a.x1, b.y2, a.x2, a.y2))

		# Middle (vertical) strip: y overlap is [max(a.y1,b.y1), min(a.y2,b.y2)]
		y_lo = max(a.y1, b.y1)
		y_hi = min(a.y2, b.y2)

		# Left slice
		if a.x1 < b.x1:
			parts.append(Rect(a.x1, y_lo, b.x1, y_hi))
		# Right slice
		if b.x2 < a.x2:
			parts.append(Rect(b.x2, y_lo, a.x2, y_hi))

		return parts

	# -----------------------------------------------------------------
	def contains(self, r: Rect) -> bool:
		"""
		True iff r is fully covered by the current union.
		"""
		if not self._rects:
			return False

		stack = [r]
		for s in self._rects:
			new_stack = []
			for piece in stack:
				if s.contains(piece):
					# piece completely gone
					continue
				if piece.intersects(s):
					new_stack.extend(self._split_diff(piece, s))
				else:
					new_stack.append(piece)
			if not new_stack:  # everything eaten â€“ covered
				return True
			stack = new_stack
		return False  # something survived

	# -----------------------------------------------------------------
	def add(self, r: Rect) -> bool:
		"""
		Insert r unless it is already covered.
		Returns True if the union grew.
		"""
		if self.contains(r):
			return False

		pending = [r]
		i = 0
		while i < len(self._rects):
			s = self._rects[i]
			new_pending = []
			changed = False
			for piece in pending:
				if piece.intersects(s):
					new_pending.extend(self._split_diff(piece, s))
					changed = True
				else:
					new_pending.append(piece)
			pending = new_pending
			if changed:
				# s unchanged; proceed with next existing rectangle
				i += 1
			else:
				i += 1

		# Any leftâ€‘over pieces are new, nonâ€‘overlapping areas
		self._rects.extend(pending)
		return True


class PaintOrderRemover:
	"""
	Calculates which elements should be removed based on the paint order parameter.
	"""

	def __init__(self, root: SimplifiedNode):
		self.root = root

	def calculate_paint_order(self) -> None:
		all_simplified_nodes_with_paint_order: list[SimplifiedNode] = []

		def collect_paint_order(node: SimplifiedNode) -> None:
			if (
				node.original_node.snapshot_node
				and node.original_node.snapshot_node.paint_order is not None
				and node.original_node.snapshot_node.bounds is not None
			):
				all_simplified_nodes_with_paint_order.append(node)

			for child in node.children:
				collect_paint_order(child)

		collect_paint_order(self.root)

		grouped_by_paint_order: defaultdict[int, list[SimplifiedNode]] = defaultdict(list)

		for node in all_simplified_nodes_with_paint_order:
			if node.original_node.snapshot_node and node.original_node.snapshot_node.paint_order is not None:
				grouped_by_paint_order[node.original_node.snapshot_node.paint_order].append(node)

		rect_union = RectUnionPure()

		for paint_order, nodes in sorted(grouped_by_paint_order.items(), key=lambda x: -x[0]):
			rects_to_add = []

			for node in nodes:
				if not node.original_node.snapshot_node or not node.original_node.snapshot_node.bounds:
					continue  # shouldn't happen by how we filter them out in the first place

				rect = Rect(
					x1=node.original_node.snapshot_node.bounds.x,
					y1=node.original_node.snapshot_node.bounds.y,
					x2=node.original_node.snapshot_node.bounds.x + node.original_node.snapshot_node.bounds.width,
					y2=node.original_node.snapshot_node.bounds.y + node.original_node.snapshot_node.bounds.height,
				)

				if rect_union.contains(rect):
					node.ignored_by_paint_order = True

				# don't add to the nodes if opacity is less then 0.95 or background-color is transparent
				if (
					node.original_node.snapshot_node.computed_styles
					and node.original_node.snapshot_node.computed_styles.get('background-color', 'rgba(0, 0, 0, 0)')
					== 'rgba(0, 0, 0, 0)'
				) or (
					node.original_node.snapshot_node.computed_styles
					and float(node.original_node.snapshot_node.computed_styles.get('opacity', '1'))
					< 0.8  # this is highly vibes based number
				):
					continue

				rects_to_add.append(rect)

			for rect in rects_to_add:
				rect_union.add(rect)

		return None

```

---

## backend/browser-use/browser_use/dom/serializer/serializer.py

```py
# @file purpose: Serializes enhanced DOM trees to string format for LLM consumption

from typing import Any

from browser_use.dom.serializer.clickable_elements import ClickableElementDetector
from browser_use.dom.serializer.paint_order import PaintOrderRemover
from browser_use.dom.utils import cap_text_length
from browser_use.dom.views import (
	DOMRect,
	DOMSelectorMap,
	EnhancedDOMTreeNode,
	NodeType,
	PropagatingBounds,
	SerializedDOMState,
	SimplifiedNode,
)

DISABLED_ELEMENTS = {'style', 'script', 'head', 'meta', 'link', 'title'}

# SVG child elements to skip (decorative only, no interaction value)
SVG_ELEMENTS = {
	'path',
	'rect',
	'g',
	'circle',
	'ellipse',
	'line',
	'polyline',
	'polygon',
	'use',
	'defs',
	'clipPath',
	'mask',
	'pattern',
	'image',
	'text',
	'tspan',
}


class DOMTreeSerializer:
	"""Serializes enhanced DOM trees to string format."""

	# Configuration - elements that propagate bounds to their children
	PROPAGATING_ELEMENTS = [
		{'tag': 'a', 'role': None},  # Any <a> tag
		{'tag': 'button', 'role': None},  # Any <button> tag
		{'tag': 'div', 'role': 'button'},  # <div role="button">
		{'tag': 'div', 'role': 'combobox'},  # <div role="combobox"> - dropdowns/selects
		{'tag': 'span', 'role': 'button'},  # <span role="button">
		{'tag': 'span', 'role': 'combobox'},  # <span role="combobox">
		{'tag': 'input', 'role': 'combobox'},  # <input role="combobox"> - autocomplete inputs
		{'tag': 'input', 'role': 'combobox'},  # <input type="text"> - text inputs with suggestions
		# {'tag': 'div', 'role': 'link'},     # <div role="link">
		# {'tag': 'span', 'role': 'link'},    # <span role="link">
	]
	DEFAULT_CONTAINMENT_THRESHOLD = 0.99  # 99% containment by default

	def __init__(
		self,
		root_node: EnhancedDOMTreeNode,
		previous_cached_state: SerializedDOMState | None = None,
		enable_bbox_filtering: bool = True,
		containment_threshold: float | None = None,
		paint_order_filtering: bool = True,
		session_id: str | None = None,
	):
		self.root_node = root_node
		self._interactive_counter = 1
		self._selector_map: DOMSelectorMap = {}
		self._previous_cached_selector_map = previous_cached_state.selector_map if previous_cached_state else None
		# Add timing tracking
		self.timing_info: dict[str, float] = {}
		# Cache for clickable element detection to avoid redundant calls
		self._clickable_cache: dict[int, bool] = {}
		# Bounding box filtering configuration
		self.enable_bbox_filtering = enable_bbox_filtering
		self.containment_threshold = containment_threshold or self.DEFAULT_CONTAINMENT_THRESHOLD
		# Paint order filtering configuration
		self.paint_order_filtering = paint_order_filtering
		# Session ID for session-specific exclude attribute
		self.session_id = session_id

	def _safe_parse_number(self, value_str: str, default: float) -> float:
		"""Parse string to float, handling negatives and decimals."""
		try:
			return float(value_str)
		except (ValueError, TypeError):
			return default

	def _safe_parse_optional_number(self, value_str: str | None) -> float | None:
		"""Parse string to float, returning None for invalid values."""
		if not value_str:
			return None
		try:
			return float(value_str)
		except (ValueError, TypeError):
			return None

	def serialize_accessible_elements(self) -> tuple[SerializedDOMState, dict[str, float]]:
		import time

		start_total = time.time()

		# Reset state
		self._interactive_counter = 1
		self._selector_map = {}
		self._semantic_groups = []
		self._clickable_cache = {}  # Clear cache for new serialization

		# Step 1: Create simplified tree (includes clickable element detection)
		start_step1 = time.time()
		simplified_tree = self._create_simplified_tree(self.root_node)
		end_step1 = time.time()
		self.timing_info['create_simplified_tree'] = end_step1 - start_step1

		# Step 2: Remove elements based on paint order
		start_step3 = time.time()
		if self.paint_order_filtering and simplified_tree:
			PaintOrderRemover(simplified_tree).calculate_paint_order()
		end_step3 = time.time()
		self.timing_info['calculate_paint_order'] = end_step3 - start_step3

		# Step 3: Optimize tree (remove unnecessary parents)
		start_step2 = time.time()
		optimized_tree = self._optimize_tree(simplified_tree)
		end_step2 = time.time()
		self.timing_info['optimize_tree'] = end_step2 - start_step2

		# Step 3: Apply bounding box filtering (NEW)
		if self.enable_bbox_filtering and optimized_tree:
			start_step3 = time.time()
			filtered_tree = self._apply_bounding_box_filtering(optimized_tree)
			end_step3 = time.time()
			self.timing_info['bbox_filtering'] = end_step3 - start_step3
		else:
			filtered_tree = optimized_tree

		# Step 4: Assign interactive indices to clickable elements
		start_step4 = time.time()
		self._assign_interactive_indices_and_mark_new_nodes(filtered_tree)
		end_step4 = time.time()
		self.timing_info['assign_interactive_indices'] = end_step4 - start_step4

		end_total = time.time()
		self.timing_info['serialize_accessible_elements_total'] = end_total - start_total

		return SerializedDOMState(_root=filtered_tree, selector_map=self._selector_map), self.timing_info

	def _add_compound_components(self, simplified: SimplifiedNode, node: EnhancedDOMTreeNode) -> None:
		"""Enhance compound controls with information from their child components."""
		# Only process elements that might have compound components
		if node.tag_name not in ['input', 'select', 'details', 'audio', 'video']:
			return

		# For input elements, check for compound input types
		if node.tag_name == 'input':
			if not node.attributes or node.attributes.get('type') not in [
				'date',
				'time',
				'datetime-local',
				'month',
				'week',
				'range',
				'number',
				'color',
				'file',
			]:
				return
		# For other elements, check if they have AX child indicators
		elif not node.ax_node or not node.ax_node.child_ids:
			return

		# Add compound component information based on element type
		element_type = node.tag_name
		input_type = node.attributes.get('type', '') if node.attributes else ''

		if element_type == 'input':
			# NOTE: For date/time inputs, we DON'T add compound components because:
			# 1. They confuse the model (seeing "Day, Month, Year" suggests DD.MM.YYYY format)
			# 2. HTML5 date/time inputs ALWAYS require ISO format (YYYY-MM-DD, HH:MM, etc.)
			# 3. The placeholder attribute clearly shows the required format
			# 4. These inputs use direct value assignment, not sequential typing
			if input_type in ['date', 'time', 'datetime-local', 'month', 'week']:
				# Skip compound components for date/time inputs - format is shown in placeholder
				pass
			elif input_type == 'range':
				# Range slider with value indicator
				min_val = node.attributes.get('min', '0') if node.attributes else '0'
				max_val = node.attributes.get('max', '100') if node.attributes else '100'

				node._compound_children.append(
					{
						'role': 'slider',
						'name': 'Value',
						'valuemin': self._safe_parse_number(min_val, 0.0),
						'valuemax': self._safe_parse_number(max_val, 100.0),
						'valuenow': None,
					}
				)
				simplified.is_compound_component = True
			elif input_type == 'number':
				# Number input with increment/decrement buttons
				min_val = node.attributes.get('min') if node.attributes else None
				max_val = node.attributes.get('max') if node.attributes else None

				node._compound_children.extend(
					[
						{'role': 'button', 'name': 'Increment', 'valuemin': None, 'valuemax': None, 'valuenow': None},
						{'role': 'button', 'name': 'Decrement', 'valuemin': None, 'valuemax': None, 'valuenow': None},
						{
							'role': 'textbox',
							'name': 'Value',
							'valuemin': self._safe_parse_optional_number(min_val),
							'valuemax': self._safe_parse_optional_number(max_val),
							'valuenow': None,
						},
					]
				)
				simplified.is_compound_component = True
			elif input_type == 'color':
				# Color picker with components
				node._compound_children.extend(
					[
						{'role': 'textbox', 'name': 'Hex Value', 'valuemin': None, 'valuemax': None, 'valuenow': None},
						{'role': 'button', 'name': 'Color Picker', 'valuemin': None, 'valuemax': None, 'valuenow': None},
					]
				)
				simplified.is_compound_component = True
			elif input_type == 'file':
				# File input with browse button
				multiple = 'multiple' in node.attributes if node.attributes else False

				# Extract current file selection state from AX tree
				current_value = 'None'  # Default to explicit "None" string for clarity
				if node.ax_node and node.ax_node.properties:
					for prop in node.ax_node.properties:
						# Try valuetext first (human-readable display like "file.pdf")
						if prop.name == 'valuetext' and prop.value:
							value_str = str(prop.value).strip()
							if value_str and value_str.lower() not in ['', 'no file chosen', 'no file selected']:
								current_value = value_str
							break
						# Also try 'value' property (may include full path)
						elif prop.name == 'value' and prop.value:
							value_str = str(prop.value).strip()
							if value_str:
								# For file inputs, value might be a full path - extract just filename
								if '\\' in value_str:
									current_value = value_str.split('\\')[-1]
								elif '/' in value_str:
									current_value = value_str.split('/')[-1]
								else:
									current_value = value_str
								break

				node._compound_children.extend(
					[
						{'role': 'button', 'name': 'Browse Files', 'valuemin': None, 'valuemax': None, 'valuenow': None},
						{
							'role': 'textbox',
							'name': f'{"Files" if multiple else "File"} Selected',
							'valuemin': None,
							'valuemax': None,
							'valuenow': current_value,  # Always shows state: filename or "None"
						},
					]
				)
				simplified.is_compound_component = True

		elif element_type == 'select':
			# Select dropdown with option list and detailed option information
			base_components = [
				{'role': 'button', 'name': 'Dropdown Toggle', 'valuemin': None, 'valuemax': None, 'valuenow': None}
			]

			# Extract option information from child nodes
			options_info = self._extract_select_options(node)
			if options_info:
				options_component = {
					'role': 'listbox',
					'name': 'Options',
					'valuemin': None,
					'valuemax': None,
					'valuenow': None,
					'options_count': options_info['count'],
					'first_options': options_info['first_options'],
				}
				if options_info['format_hint']:
					options_component['format_hint'] = options_info['format_hint']
				base_components.append(options_component)
			else:
				base_components.append(
					{'role': 'listbox', 'name': 'Options', 'valuemin': None, 'valuemax': None, 'valuenow': None}
				)

			node._compound_children.extend(base_components)
			simplified.is_compound_component = True

		elif element_type == 'details':
			# Details/summary disclosure widget
			node._compound_children.extend(
				[
					{'role': 'button', 'name': 'Toggle Disclosure', 'valuemin': None, 'valuemax': None, 'valuenow': None},
					{'role': 'region', 'name': 'Content Area', 'valuemin': None, 'valuemax': None, 'valuenow': None},
				]
			)
			simplified.is_compound_component = True

		elif element_type == 'audio':
			# Audio player controls
			node._compound_children.extend(
				[
					{'role': 'button', 'name': 'Play/Pause', 'valuemin': None, 'valuemax': None, 'valuenow': None},
					{'role': 'slider', 'name': 'Progress', 'valuemin': 0, 'valuemax': 100, 'valuenow': None},
					{'role': 'button', 'name': 'Mute', 'valuemin': None, 'valuemax': None, 'valuenow': None},
					{'role': 'slider', 'name': 'Volume', 'valuemin': 0, 'valuemax': 100, 'valuenow': None},
				]
			)
			simplified.is_compound_component = True

		elif element_type == 'video':
			# Video player controls
			node._compound_children.extend(
				[
					{'role': 'button', 'name': 'Play/Pause', 'valuemin': None, 'valuemax': None, 'valuenow': None},
					{'role': 'slider', 'name': 'Progress', 'valuemin': 0, 'valuemax': 100, 'valuenow': None},
					{'role': 'button', 'name': 'Mute', 'valuemin': None, 'valuemax': None, 'valuenow': None},
					{'role': 'slider', 'name': 'Volume', 'valuemin': 0, 'valuemax': 100, 'valuenow': None},
					{'role': 'button', 'name': 'Fullscreen', 'valuemin': None, 'valuemax': None, 'valuenow': None},
				]
			)
			simplified.is_compound_component = True

	def _extract_select_options(self, select_node: EnhancedDOMTreeNode) -> dict[str, Any] | None:
		"""Extract option information from a select element."""
		if not select_node.children:
			return None

		options = []
		option_values = []

		def extract_options_recursive(node: EnhancedDOMTreeNode) -> None:
			"""Recursively extract option elements, including from optgroups."""
			if node.tag_name.lower() == 'option':
				# Extract option text and value
				option_text = ''
				option_value = ''

				# Get value attribute if present
				if node.attributes and 'value' in node.attributes:
					option_value = str(node.attributes['value']).strip()

				# Get text content from direct child text nodes only to avoid duplication
				def get_direct_text_content(n: EnhancedDOMTreeNode) -> str:
					text = ''
					for child in n.children:
						if child.node_type == NodeType.TEXT_NODE and child.node_value:
							text += child.node_value.strip() + ' '
					return text.strip()

				option_text = get_direct_text_content(node)

				# Use text as value if no explicit value
				if not option_value and option_text:
					option_value = option_text

				if option_text or option_value:
					options.append({'text': option_text, 'value': option_value})
					option_values.append(option_value)

			elif node.tag_name.lower() == 'optgroup':
				# Process optgroup children
				for child in node.children:
					extract_options_recursive(child)
			else:
				# Process other children that might contain options
				for child in node.children:
					extract_options_recursive(child)

		# Extract all options from select children
		for child in select_node.children:
			extract_options_recursive(child)

		if not options:
			return None

		# Prepare first 4 options for display
		first_options = []
		for option in options[:4]:
			# Always use text if available, otherwise use value
			display_text = option['text'] if option['text'] else option['value']
			if display_text:
				# Limit individual option text to avoid overly long attributes
				text = display_text[:30] + ('...' if len(display_text) > 30 else '')
				first_options.append(text)

		# Add ellipsis indicator if there are more options than shown
		if len(options) > 4:
			first_options.append(f'... {len(options) - 4} more options...')

		# Try to infer format hint from option values
		format_hint = None
		if len(option_values) >= 2:
			# Check for common patterns
			if all(val.isdigit() for val in option_values[:5] if val):
				format_hint = 'numeric'
			elif all(len(val) == 2 and val.isupper() for val in option_values[:5] if val):
				format_hint = 'country/state codes'
			elif all('/' in val or '-' in val for val in option_values[:5] if val):
				format_hint = 'date/path format'
			elif any('@' in val for val in option_values[:5] if val):
				format_hint = 'email addresses'

		return {'count': len(options), 'first_options': first_options, 'format_hint': format_hint}

	def _is_interactive_cached(self, node: EnhancedDOMTreeNode) -> bool:
		"""Cached version of clickable element detection to avoid redundant calls."""

		if node.node_id not in self._clickable_cache:
			import time

			start_time = time.time()
			result = ClickableElementDetector.is_interactive(node)
			end_time = time.time()

			if 'clickable_detection_time' not in self.timing_info:
				self.timing_info['clickable_detection_time'] = 0
			self.timing_info['clickable_detection_time'] += end_time - start_time

			self._clickable_cache[node.node_id] = result

		return self._clickable_cache[node.node_id]

	def _create_simplified_tree(self, node: EnhancedDOMTreeNode, depth: int = 0) -> SimplifiedNode | None:
		"""Step 1: Create a simplified tree with enhanced element detection."""

		if node.node_type == NodeType.DOCUMENT_NODE:
			# for all cldren including shadow roots
			for child in node.children_and_shadow_roots:
				simplified_child = self._create_simplified_tree(child, depth + 1)
				if simplified_child:
					return simplified_child

			return None

		if node.node_type == NodeType.DOCUMENT_FRAGMENT_NODE:
			# ENHANCED shadow DOM processing - always include shadow content
			simplified = SimplifiedNode(original_node=node, children=[])
			for child in node.children_and_shadow_roots:
				simplified_child = self._create_simplified_tree(child, depth + 1)
				if simplified_child:
					simplified.children.append(simplified_child)

			# Always return shadow DOM fragments, even if children seem empty
			# Shadow DOM often contains the actual interactive content in SPAs
			return simplified if simplified.children else SimplifiedNode(original_node=node, children=[])

		elif node.node_type == NodeType.ELEMENT_NODE:
			# Skip non-content elements
			if node.node_name.lower() in DISABLED_ELEMENTS:
				return None

			# Skip SVG child elements entirely (path, rect, g, circle, etc.)
			if node.node_name.lower() in SVG_ELEMENTS:
				return None

			attributes = node.attributes or {}
			# Check for session-specific exclude attribute first, then fall back to legacy attribute
			exclude_attr = None
			attr_type = None
			if self.session_id:
				session_specific_attr = f'data-browser-use-exclude-{self.session_id}'
				exclude_attr = attributes.get(session_specific_attr)
				if exclude_attr:
					attr_type = 'session-specific'
			# Fall back to legacy attribute if session-specific not found
			if not exclude_attr:
				exclude_attr = attributes.get('data-browser-use-exclude')
			if isinstance(exclude_attr, str) and exclude_attr.lower() == 'true':
				return None

			if node.node_name == 'IFRAME' or node.node_name == 'FRAME':
				if node.content_document:
					simplified = SimplifiedNode(original_node=node, children=[])
					for child in node.content_document.children_nodes or []:
						simplified_child = self._create_simplified_tree(child, depth + 1)
						if simplified_child is not None:
							simplified.children.append(simplified_child)
					return simplified

			is_visible = node.is_visible
			is_scrollable = node.is_actually_scrollable
			has_shadow_content = bool(node.children_and_shadow_roots)

			# ENHANCED SHADOW DOM DETECTION: Include shadow hosts even if not visible
			is_shadow_host = any(child.node_type == NodeType.DOCUMENT_FRAGMENT_NODE for child in node.children_and_shadow_roots)

			# Override visibility for elements with validation attributes
			if not is_visible and node.attributes:
				has_validation_attrs = any(attr.startswith(('aria-', 'pseudo')) for attr in node.attributes.keys())
				if has_validation_attrs:
					is_visible = True  # Force visibility for validation elements

			# EXCEPTION: File inputs are often hidden with opacity:0 but are still functional
			# Bootstrap and other frameworks use this pattern with custom-styled file pickers
			is_file_input = (
				node.tag_name and node.tag_name.lower() == 'input' and node.attributes and node.attributes.get('type') == 'file'
			)
			if not is_visible and is_file_input:
				is_visible = True  # Force visibility for file inputs

			# Include if visible, scrollable, has children, or is shadow host
			if is_visible or is_scrollable or has_shadow_content or is_shadow_host:
				simplified = SimplifiedNode(original_node=node, children=[], is_shadow_host=is_shadow_host)

				# Process ALL children including shadow roots with enhanced logging
				for child in node.children_and_shadow_roots:
					simplified_child = self._create_simplified_tree(child, depth + 1)
					if simplified_child:
						simplified.children.append(simplified_child)

				# COMPOUND CONTROL PROCESSING: Add virtual components for compound controls
				self._add_compound_components(simplified, node)

				# SHADOW DOM SPECIAL CASE: Always include shadow hosts even if not visible
				# Many SPA frameworks (React, Vue) render content in shadow DOM
				if is_shadow_host and simplified.children:
					return simplified

				# Return if meaningful or has meaningful children
				if is_visible or is_scrollable or simplified.children:
					return simplified
		elif node.node_type == NodeType.TEXT_NODE:
			# Include meaningful text nodes
			is_visible = node.snapshot_node and node.is_visible
			if is_visible and node.node_value and node.node_value.strip() and len(node.node_value.strip()) > 1:
				return SimplifiedNode(original_node=node, children=[])

		return None

	def _optimize_tree(self, node: SimplifiedNode | None) -> SimplifiedNode | None:
		"""Step 2: Optimize tree structure."""
		if not node:
			return None

		# Process children
		optimized_children = []
		for child in node.children:
			optimized_child = self._optimize_tree(child)
			if optimized_child:
				optimized_children.append(optimized_child)

		node.children = optimized_children

		# Keep meaningful nodes
		is_visible = node.original_node.snapshot_node and node.original_node.is_visible

		# EXCEPTION: File inputs are often hidden with opacity:0 but are still functional
		is_file_input = (
			node.original_node.tag_name
			and node.original_node.tag_name.lower() == 'input'
			and node.original_node.attributes
			and node.original_node.attributes.get('type') == 'file'
		)

		if (
			is_visible  # Keep all visible nodes
			or node.original_node.is_actually_scrollable
			or node.original_node.node_type == NodeType.TEXT_NODE
			or node.children
			or is_file_input  # Keep file inputs even if not visible
		):
			return node

		return None

	def _collect_interactive_elements(self, node: SimplifiedNode, elements: list[SimplifiedNode]) -> None:
		"""Recursively collect interactive elements that are also visible."""
		is_interactive = self._is_interactive_cached(node.original_node)
		is_visible = node.original_node.snapshot_node and node.original_node.is_visible

		# Only collect elements that are both interactive AND visible
		if is_interactive and is_visible:
			elements.append(node)

		for child in node.children:
			self._collect_interactive_elements(child, elements)

	def _has_interactive_descendants(self, node: SimplifiedNode) -> bool:
		"""Check if a node has any interactive descendants (not including the node itself)."""
		# Check children for interactivity
		for child in node.children:
			# Check if child itself is interactive
			if self._is_interactive_cached(child.original_node):
				return True
			# Recursively check child's descendants
			if self._has_interactive_descendants(child):
				return True

		return False

	def _assign_interactive_indices_and_mark_new_nodes(self, node: SimplifiedNode | None) -> None:
		"""Assign interactive indices to clickable elements that are also visible."""
		if not node:
			return

		# Skip assigning index to excluded nodes, or ignored by paint order
		if not node.excluded_by_parent and not node.ignored_by_paint_order:
			# Regular interactive element assignment (including enhanced compound controls)
			is_interactive_assign = self._is_interactive_cached(node.original_node)
			is_visible = node.original_node.snapshot_node and node.original_node.is_visible
			is_scrollable = node.original_node.is_actually_scrollable

			# EXCEPTION: File inputs are often hidden with opacity:0 but are still functional
			# Bootstrap and other frameworks use this pattern with custom-styled file pickers
			is_file_input = (
				node.original_node.tag_name
				and node.original_node.tag_name.lower() == 'input'
				and node.original_node.attributes
				and node.original_node.attributes.get('type') == 'file'
			)

			# Check if scrollable container should be made interactive
			# For scrollable elements, ONLY make them interactive if they have no interactive descendants
			should_make_interactive = False
			if is_scrollable:
				# For scrollable elements, check if they have interactive children
				has_interactive_desc = self._has_interactive_descendants(node)

				# Only make scrollable container interactive if it has NO interactive descendants
				if not has_interactive_desc:
					should_make_interactive = True
			elif is_interactive_assign and (is_visible or is_file_input):
				# Non-scrollable interactive elements: make interactive if visible (or file input)
				should_make_interactive = True

			# Add to selector map if element should be interactive
			if should_make_interactive:
				# Mark node as interactive
				node.is_interactive = True
				# Store backend_node_id in selector map (model outputs backend_node_id)
				self._selector_map[node.original_node.backend_node_id] = node.original_node
				self._interactive_counter += 1

				# Mark compound components as new for visibility
				if node.is_compound_component:
					node.is_new = True
				elif self._previous_cached_selector_map:
					# Check if node is new for regular elements
					previous_backend_node_ids = {node.backend_node_id for node in self._previous_cached_selector_map.values()}
					if node.original_node.backend_node_id not in previous_backend_node_ids:
						node.is_new = True

		# Process children
		for child in node.children:
			self._assign_interactive_indices_and_mark_new_nodes(child)

	def _apply_bounding_box_filtering(self, node: SimplifiedNode | None) -> SimplifiedNode | None:
		"""Filter children contained within propagating parent bounds."""
		if not node:
			return None

		# Start with no active bounds
		self._filter_tree_recursive(node, active_bounds=None, depth=0)

		# Log statistics
		excluded_count = self._count_excluded_nodes(node)
		if excluded_count > 0:
			import logging

			logging.debug(f'BBox filtering excluded {excluded_count} nodes')

		return node

	def _filter_tree_recursive(self, node: SimplifiedNode, active_bounds: PropagatingBounds | None = None, depth: int = 0):
		"""
		Recursively filter tree with bounding box propagation.
		Bounds propagate to ALL descendants until overridden.
		"""

		# Check if this node should be excluded by active bounds
		if active_bounds and self._should_exclude_child(node, active_bounds):
			node.excluded_by_parent = True
			# Important: Still check if this node starts NEW propagation

		# Check if this node starts new propagation (even if excluded!)
		new_bounds = None
		tag = node.original_node.tag_name.lower()
		role = node.original_node.attributes.get('role') if node.original_node.attributes else None
		attributes = {
			'tag': tag,
			'role': role,
		}
		# Check if this element matches any propagating element pattern
		if self._is_propagating_element(attributes):
			# This node propagates bounds to ALL its descendants
			if node.original_node.snapshot_node and node.original_node.snapshot_node.bounds:
				new_bounds = PropagatingBounds(
					tag=tag,
					bounds=node.original_node.snapshot_node.bounds,
					node_id=node.original_node.node_id,
					depth=depth,
				)

		# Propagate to ALL children
		# Use new_bounds if this node starts propagation, otherwise continue with active_bounds
		propagate_bounds = new_bounds if new_bounds else active_bounds

		for child in node.children:
			self._filter_tree_recursive(child, propagate_bounds, depth + 1)

	def _should_exclude_child(self, node: SimplifiedNode, active_bounds: PropagatingBounds) -> bool:
		"""
		Determine if child should be excluded based on propagating bounds.
		"""

		# Never exclude text nodes - we always want to preserve text content
		if node.original_node.node_type == NodeType.TEXT_NODE:
			return False

		# Get child bounds
		if not node.original_node.snapshot_node or not node.original_node.snapshot_node.bounds:
			return False  # No bounds = can't determine containment

		child_bounds = node.original_node.snapshot_node.bounds

		# Check containment with configured threshold
		if not self._is_contained(child_bounds, active_bounds.bounds, self.containment_threshold):
			return False  # Not sufficiently contained

		# EXCEPTION RULES - Keep these even if contained:

		child_tag = node.original_node.tag_name.lower()
		child_role = node.original_node.attributes.get('role') if node.original_node.attributes else None
		child_attributes = {
			'tag': child_tag,
			'role': child_role,
		}

		# 1. Never exclude form elements (they need individual interaction)
		if child_tag in ['input', 'select', 'textarea', 'label']:
			return False

		# 2. Keep if child is also a propagating element
		# (might have stopPropagation, e.g., button in button)
		if self._is_propagating_element(child_attributes):
			return False

		# 3. Keep if has explicit onclick handler
		if node.original_node.attributes and 'onclick' in node.original_node.attributes:
			return False

		# 4. Keep if has aria-label suggesting it's independently interactive
		if node.original_node.attributes:
			aria_label = node.original_node.attributes.get('aria-label')
			if aria_label and aria_label.strip():
				# Has meaningful aria-label, likely interactive
				return False

		# 5. Keep if has role suggesting interactivity
		if node.original_node.attributes:
			role = node.original_node.attributes.get('role')
			if role in ['button', 'link', 'checkbox', 'radio', 'tab', 'menuitem', 'option']:
				return False

		# Default: exclude this child
		return True

	def _is_contained(self, child: DOMRect, parent: DOMRect, threshold: float) -> bool:
		"""
		Check if child is contained within parent bounds.

		Args:
			threshold: Percentage (0.0-1.0) of child that must be within parent
		"""
		# Calculate intersection
		x_overlap = max(0, min(child.x + child.width, parent.x + parent.width) - max(child.x, parent.x))
		y_overlap = max(0, min(child.y + child.height, parent.y + parent.height) - max(child.y, parent.y))

		intersection_area = x_overlap * y_overlap
		child_area = child.width * child.height

		if child_area == 0:
			return False  # Zero-area element

		containment_ratio = intersection_area / child_area
		return containment_ratio >= threshold

	def _count_excluded_nodes(self, node: SimplifiedNode, count: int = 0) -> int:
		"""Count how many nodes were excluded (for debugging)."""
		if hasattr(node, 'excluded_by_parent') and node.excluded_by_parent:
			count += 1
		for child in node.children:
			count = self._count_excluded_nodes(child, count)
		return count

	def _is_propagating_element(self, attributes: dict[str, str | None]) -> bool:
		"""
		Check if an element should propagate bounds based on attributes.
		If the element satisfies one of the patterns, it propagates bounds to all its children.
		"""
		keys_to_check = ['tag', 'role']
		for pattern in self.PROPAGATING_ELEMENTS:
			# Check if the element satisfies the pattern
			check = [pattern.get(key) is None or pattern.get(key) == attributes.get(key) for key in keys_to_check]
			if all(check):
				return True

		return False

	@staticmethod
	def serialize_tree(node: SimplifiedNode | None, include_attributes: list[str], depth: int = 0) -> str:
		"""Serialize the optimized tree to string format."""
		if not node:
			return ''

		# Skip rendering excluded nodes, but process their children
		if hasattr(node, 'excluded_by_parent') and node.excluded_by_parent:
			formatted_text = []
			for child in node.children:
				child_text = DOMTreeSerializer.serialize_tree(child, include_attributes, depth)
				if child_text:
					formatted_text.append(child_text)
			return '\n'.join(formatted_text)

		formatted_text = []
		depth_str = depth * '\t'
		next_depth = depth

		if node.original_node.node_type == NodeType.ELEMENT_NODE:
			# Skip displaying nodes marked as should_display=False
			if not node.should_display:
				for child in node.children:
					child_text = DOMTreeSerializer.serialize_tree(child, include_attributes, depth)
					if child_text:
						formatted_text.append(child_text)
				return '\n'.join(formatted_text)

			# Special handling for SVG elements - show the tag but collapse children
			if node.original_node.tag_name.lower() == 'svg':
				shadow_prefix = ''
				if node.is_shadow_host:
					has_closed_shadow = any(
						child.original_node.node_type == NodeType.DOCUMENT_FRAGMENT_NODE
						and child.original_node.shadow_root_type
						and child.original_node.shadow_root_type.lower() == 'closed'
						for child in node.children
					)
					shadow_prefix = '|SHADOW(closed)|' if has_closed_shadow else '|SHADOW(open)|'

				line = f'{depth_str}{shadow_prefix}'
				# Add interactive marker if clickable
				if node.is_interactive:
					new_prefix = '*' if node.is_new else ''
					line += f'{new_prefix}[{node.original_node.backend_node_id}]'
				line += '<svg'
				attributes_html_str = DOMTreeSerializer._build_attributes_string(node.original_node, include_attributes, '')
				if attributes_html_str:
					line += f' {attributes_html_str}'
				line += ' /> <!-- SVG content collapsed -->'
				formatted_text.append(line)
				# Don't process children for SVG
				return '\n'.join(formatted_text)

			# Add element if clickable, scrollable, or iframe
			is_any_scrollable = node.original_node.is_actually_scrollable or node.original_node.is_scrollable
			should_show_scroll = node.original_node.should_show_scroll_info
			if (
				node.is_interactive
				or is_any_scrollable
				or node.original_node.tag_name.upper() == 'IFRAME'
				or node.original_node.tag_name.upper() == 'FRAME'
			):
				next_depth += 1

				# Build attributes string with compound component info
				text_content = ''
				attributes_html_str = DOMTreeSerializer._build_attributes_string(
					node.original_node, include_attributes, text_content
				)

				# Add compound component information to attributes if present
				if node.original_node._compound_children:
					compound_info = []
					for child_info in node.original_node._compound_children:
						parts = []
						if child_info['name']:
							parts.append(f'name={child_info["name"]}')
						if child_info['role']:
							parts.append(f'role={child_info["role"]}')
						if child_info['valuemin'] is not None:
							parts.append(f'min={child_info["valuemin"]}')
						if child_info['valuemax'] is not None:
							parts.append(f'max={child_info["valuemax"]}')
						if child_info['valuenow'] is not None:
							parts.append(f'current={child_info["valuenow"]}')

						# Add select-specific information
						if 'options_count' in child_info and child_info['options_count'] is not None:
							parts.append(f'count={child_info["options_count"]}')
						if 'first_options' in child_info and child_info['first_options']:
							options_str = '|'.join(child_info['first_options'][:4])  # Limit to 4 options
							parts.append(f'options={options_str}')
						if 'format_hint' in child_info and child_info['format_hint']:
							parts.append(f'format={child_info["format_hint"]}')

						if parts:
							compound_info.append(f'({",".join(parts)})')

					if compound_info:
						compound_attr = f'compound_components={",".join(compound_info)}'
						if attributes_html_str:
							attributes_html_str += f' {compound_attr}'
						else:
							attributes_html_str = compound_attr

				# Build the line with shadow host indicator
				shadow_prefix = ''
				if node.is_shadow_host:
					# Check if any shadow children are closed
					has_closed_shadow = any(
						child.original_node.node_type == NodeType.DOCUMENT_FRAGMENT_NODE
						and child.original_node.shadow_root_type
						and child.original_node.shadow_root_type.lower() == 'closed'
						for child in node.children
					)
					shadow_prefix = '|SHADOW(closed)|' if has_closed_shadow else '|SHADOW(open)|'

				if should_show_scroll and not node.is_interactive:
					# Scrollable container but not clickable
					line = f'{depth_str}{shadow_prefix}|SCROLL|<{node.original_node.tag_name}'
				elif node.is_interactive:
					# Clickable (and possibly scrollable) - show backend_node_id
					new_prefix = '*' if node.is_new else ''
					scroll_prefix = '|SCROLL[' if should_show_scroll else '['
					line = f'{depth_str}{shadow_prefix}{new_prefix}{scroll_prefix}{node.original_node.backend_node_id}]<{node.original_node.tag_name}'
				elif node.original_node.tag_name.upper() == 'IFRAME':
					# Iframe element (not interactive)
					line = f'{depth_str}{shadow_prefix}|IFRAME|<{node.original_node.tag_name}'
				elif node.original_node.tag_name.upper() == 'FRAME':
					# Frame element (not interactive)
					line = f'{depth_str}{shadow_prefix}|FRAME|<{node.original_node.tag_name}'
				else:
					line = f'{depth_str}{shadow_prefix}<{node.original_node.tag_name}'

				if attributes_html_str:
					line += f' {attributes_html_str}'

				line += ' />'

				# Add scroll information only when we should show it
				if should_show_scroll:
					scroll_info_text = node.original_node.get_scroll_info_text()
					if scroll_info_text:
						line += f' ({scroll_info_text})'

				formatted_text.append(line)

		elif node.original_node.node_type == NodeType.DOCUMENT_FRAGMENT_NODE:
			# Shadow DOM representation - show clearly to LLM
			if node.original_node.shadow_root_type and node.original_node.shadow_root_type.lower() == 'closed':
				formatted_text.append(f'{depth_str}Closed Shadow')
			else:
				formatted_text.append(f'{depth_str}Open Shadow')

			next_depth += 1

			# Process shadow DOM children
			for child in node.children:
				child_text = DOMTreeSerializer.serialize_tree(child, include_attributes, next_depth)
				if child_text:
					formatted_text.append(child_text)

			# Close shadow DOM indicator
			if node.children:  # Only show close if we had content
				formatted_text.append(f'{depth_str}Shadow End')

		elif node.original_node.node_type == NodeType.TEXT_NODE:
			# Include visible text
			is_visible = node.original_node.snapshot_node and node.original_node.is_visible
			if (
				is_visible
				and node.original_node.node_value
				and node.original_node.node_value.strip()
				and len(node.original_node.node_value.strip()) > 1
			):
				clean_text = node.original_node.node_value.strip()
				formatted_text.append(f'{depth_str}{clean_text}')

		# Process children (for non-shadow elements)
		if node.original_node.node_type != NodeType.DOCUMENT_FRAGMENT_NODE:
			for child in node.children:
				child_text = DOMTreeSerializer.serialize_tree(child, include_attributes, next_depth)
				if child_text:
					formatted_text.append(child_text)

		return '\n'.join(formatted_text)

	@staticmethod
	def _build_attributes_string(node: EnhancedDOMTreeNode, include_attributes: list[str], text: str) -> str:
		"""Build the attributes string for an element."""
		attributes_to_include = {}

		# Include HTML attributes
		if node.attributes:
			attributes_to_include.update(
				{
					key: str(value).strip()
					for key, value in node.attributes.items()
					if key in include_attributes and str(value).strip() != ''
				}
			)

		# Add format hints for date/time inputs to help LLMs use the correct format
		# NOTE: These formats are standardized by HTML5 specification (ISO 8601), NOT locale-dependent
		# The browser may DISPLAY dates in locale format (MM/DD/YYYY in US, DD/MM/YYYY in EU),
		# but the .value attribute and programmatic setting ALWAYS uses these ISO formats:
		# - date: YYYY-MM-DD (e.g., "2024-03-15")
		# - time: HH:MM or HH:MM:SS (24-hour, e.g., "14:30")
		# - datetime-local: YYYY-MM-DDTHH:MM (e.g., "2024-03-15T14:30")
		# Reference: https://developer.mozilla.org/en-US/docs/Web/HTML/Element/input/date
		if node.tag_name and node.tag_name.lower() == 'input' and node.attributes:
			input_type = node.attributes.get('type', '').lower()

			# For HTML5 date/time inputs, add a highly visible "format" attribute
			# This makes it IMPOSSIBLE for the model to miss the required format
			if input_type in ['date', 'time', 'datetime-local', 'month', 'week']:
				format_map = {
					'date': 'YYYY-MM-DD',
					'time': 'HH:MM',
					'datetime-local': 'YYYY-MM-DDTHH:MM',
					'month': 'YYYY-MM',
					'week': 'YYYY-W##',
				}
				# Add format as a special attribute that appears prominently
				# This appears BEFORE placeholder in the serialized output
				attributes_to_include['format'] = format_map[input_type]

			# Only add placeholder if it doesn't already exist
			if 'placeholder' in include_attributes and 'placeholder' not in attributes_to_include:
				# Native HTML5 date/time inputs - ISO format required
				if input_type == 'date':
					attributes_to_include['placeholder'] = 'YYYY-MM-DD'
				elif input_type == 'time':
					attributes_to_include['placeholder'] = 'HH:MM'
				elif input_type == 'datetime-local':
					attributes_to_include['placeholder'] = 'YYYY-MM-DDTHH:MM'
				elif input_type == 'month':
					attributes_to_include['placeholder'] = 'YYYY-MM'
				elif input_type == 'week':
					attributes_to_include['placeholder'] = 'YYYY-W##'
				# Tel - suggest format if no pattern attribute
				elif input_type == 'tel' and 'pattern' not in attributes_to_include:
					attributes_to_include['placeholder'] = '123-456-7890'
				# jQuery/Bootstrap/AngularJS datepickers (text inputs with datepicker classes/attributes)
				elif input_type in {'text', ''}:
					class_attr = node.attributes.get('class', '').lower()

					# Check for AngularJS UI Bootstrap datepicker (uib-datepicker-popup attribute)
					# This takes precedence as it's the most specific indicator
					if 'uib-datepicker-popup' in node.attributes:
						# Extract format from uib-datepicker-popup="MM/dd/yyyy"
						date_format = node.attributes.get('uib-datepicker-popup', '')
						if date_format:
							# Use 'expected_format' for clarity - this is the required input format
							attributes_to_include['expected_format'] = date_format
							# Also keep format for consistency with HTML5 date inputs
							attributes_to_include['format'] = date_format
					# Detect jQuery/Bootstrap datepickers by class names
					elif any(indicator in class_attr for indicator in ['datepicker', 'datetimepicker', 'daterangepicker']):
						# Try to get format from data-date-format attribute
						date_format = node.attributes.get('data-date-format', '')
						if date_format:
							attributes_to_include['placeholder'] = date_format
							attributes_to_include['format'] = date_format  # Also add format for jQuery datepickers
						else:
							# Default to common US format for jQuery datepickers
							attributes_to_include['placeholder'] = 'mm/dd/yyyy'
							attributes_to_include['format'] = 'mm/dd/yyyy'
					# Also detect by data-* attributes
					elif any(attr in node.attributes for attr in ['data-datepicker']):
						date_format = node.attributes.get('data-date-format', '')
						if date_format:
							attributes_to_include['placeholder'] = date_format
							attributes_to_include['format'] = date_format
						else:
							attributes_to_include['placeholder'] = 'mm/dd/yyyy'
							attributes_to_include['format'] = 'mm/dd/yyyy'

		# Include accessibility properties
		if node.ax_node and node.ax_node.properties:
			for prop in node.ax_node.properties:
				try:
					if prop.name in include_attributes and prop.value is not None:
						# Convert boolean to lowercase string, keep others as-is
						if isinstance(prop.value, bool):
							attributes_to_include[prop.name] = str(prop.value).lower()
						else:
							prop_value_str = str(prop.value).strip()
							if prop_value_str:
								attributes_to_include[prop.name] = prop_value_str
				except (AttributeError, ValueError):
					continue

		# Special handling for form elements - ensure current value is shown
		# For text inputs, textareas, and selects, prioritize showing the current value from AX tree
		if node.tag_name and node.tag_name.lower() in ['input', 'textarea', 'select']:
			# ALWAYS check AX tree - it reflects actual typed value, DOM attribute may not update
			if node.ax_node and node.ax_node.properties:
				for prop in node.ax_node.properties:
					# Try valuetext first (human-readable display value)
					if prop.name == 'valuetext' and prop.value:
						value_str = str(prop.value).strip()
						if value_str:
							attributes_to_include['value'] = value_str
							break
					# Also try 'value' property directly
					elif prop.name == 'value' and prop.value:
						value_str = str(prop.value).strip()
						if value_str:
							attributes_to_include['value'] = value_str
							break

		if not attributes_to_include:
			return ''

		# Remove duplicate values
		ordered_keys = [key for key in include_attributes if key in attributes_to_include]

		if len(ordered_keys) > 1:
			keys_to_remove = set()
			seen_values = {}

			# Attributes that should never be removed as duplicates (they serve distinct purposes)
			protected_attrs = {'format', 'expected_format', 'placeholder', 'value', 'aria-label', 'title'}

			for key in ordered_keys:
				value = attributes_to_include[key]
				if len(value) > 5:
					if value in seen_values and key not in protected_attrs:
						keys_to_remove.add(key)
					else:
						seen_values[value] = key

			for key in keys_to_remove:
				del attributes_to_include[key]

		# Remove attributes that duplicate accessibility data
		role = node.ax_node.role if node.ax_node else None
		if role and node.node_name == role:
			attributes_to_include.pop('role', None)

		# Remove type attribute if it matches the tag name (e.g. <button type="button">)
		if 'type' in attributes_to_include and attributes_to_include['type'].lower() == node.node_name.lower():
			del attributes_to_include['type']

		# Remove invalid attribute if it's false (only show when true)
		if 'invalid' in attributes_to_include and attributes_to_include['invalid'].lower() == 'false':
			del attributes_to_include['invalid']

		boolean_attrs = {'required'}
		for attr in boolean_attrs:
			if attr in attributes_to_include and attributes_to_include[attr].lower() in {'false', '0', 'no'}:
				del attributes_to_include[attr]

		# Remove aria-expanded if we have expanded (prefer AX tree over HTML attribute)
		if 'expanded' in attributes_to_include and 'aria-expanded' in attributes_to_include:
			del attributes_to_include['aria-expanded']

		attrs_to_remove_if_text_matches = ['aria-label', 'placeholder', 'title']
		for attr in attrs_to_remove_if_text_matches:
			if attributes_to_include.get(attr) and attributes_to_include.get(attr, '').strip().lower() == text.strip().lower():
				del attributes_to_include[attr]

		if attributes_to_include:
			# Format attributes, wrapping empty values in quotes for clarity
			formatted_attrs = []
			for key, value in attributes_to_include.items():
				capped_value = cap_text_length(value, 100)
				# Show empty values as key='' instead of key=
				if not capped_value:
					formatted_attrs.append(f"{key}=''")
				else:
					formatted_attrs.append(f'{key}={capped_value}')
			return ' '.join(formatted_attrs)

		return ''

```

---

## backend/browser-use/browser_use/dom/service.py

```py
import asyncio
import logging
import time
from typing import TYPE_CHECKING

from cdp_use.cdp.accessibility.commands import GetFullAXTreeReturns
from cdp_use.cdp.accessibility.types import AXNode
from cdp_use.cdp.dom.types import Node
from cdp_use.cdp.target import TargetID

from browser_use.dom.enhanced_snapshot import (
	REQUIRED_COMPUTED_STYLES,
	build_snapshot_lookup,
)
from browser_use.dom.serializer.serializer import DOMTreeSerializer
from browser_use.dom.views import (
	DOMRect,
	EnhancedAXNode,
	EnhancedAXProperty,
	EnhancedDOMTreeNode,
	NodeType,
	SerializedDOMState,
	TargetAllTrees,
)
from browser_use.observability import observe_debug
from browser_use.utils import create_task_with_error_handling

if TYPE_CHECKING:
	from browser_use.browser.session import BrowserSession

# Note: iframe limits are now configurable via BrowserProfile.max_iframes and BrowserProfile.max_iframe_depth


class DomService:
	"""
	Service for getting the DOM tree and other DOM-related information.

	Either browser or page must be provided.

	TODO: currently we start a new websocket connection PER STEP, we should definitely keep this persistent
	"""

	logger: logging.Logger

	def __init__(
		self,
		browser_session: 'BrowserSession',
		logger: logging.Logger | None = None,
		cross_origin_iframes: bool = False,
		paint_order_filtering: bool = True,
		max_iframes: int = 100,
		max_iframe_depth: int = 5,
	):
		self.browser_session = browser_session
		self.logger = logger or browser_session.logger
		self.cross_origin_iframes = cross_origin_iframes
		self.paint_order_filtering = paint_order_filtering
		self.max_iframes = max_iframes
		self.max_iframe_depth = max_iframe_depth

	async def __aenter__(self):
		return self

	async def __aexit__(self, exc_type, exc_value, traceback):
		pass  # no need to cleanup anything, browser_session auto handles cleaning up session cache

	def _build_enhanced_ax_node(self, ax_node: AXNode) -> EnhancedAXNode:
		properties: list[EnhancedAXProperty] | None = None
		if 'properties' in ax_node and ax_node['properties']:
			properties = []
			for property in ax_node['properties']:
				try:
					# test whether property name can go into the enum (sometimes Chrome returns some random properties)
					properties.append(
						EnhancedAXProperty(
							name=property['name'],
							value=property.get('value', {}).get('value', None),
							# related_nodes=[],  # TODO: add related nodes
						)
					)
				except ValueError:
					pass

		enhanced_ax_node = EnhancedAXNode(
			ax_node_id=ax_node['nodeId'],
			ignored=ax_node['ignored'],
			role=ax_node.get('role', {}).get('value', None),
			name=ax_node.get('name', {}).get('value', None),
			description=ax_node.get('description', {}).get('value', None),
			properties=properties,
			child_ids=ax_node.get('childIds', []) if ax_node.get('childIds') else None,
		)
		return enhanced_ax_node

	async def _get_viewport_ratio(self, target_id: TargetID) -> float:
		"""Get viewport dimensions, device pixel ratio, and scroll position using CDP."""
		cdp_session = await self.browser_session.get_or_create_cdp_session(target_id=target_id, focus=False)

		try:
			# Get the layout metrics which includes the visual viewport
			metrics = await cdp_session.cdp_client.send.Page.getLayoutMetrics(session_id=cdp_session.session_id)

			visual_viewport = metrics.get('visualViewport', {})

			# IMPORTANT: Use CSS viewport instead of device pixel viewport
			# This fixes the coordinate mismatch on high-DPI displays
			css_visual_viewport = metrics.get('cssVisualViewport', {})
			css_layout_viewport = metrics.get('cssLayoutViewport', {})

			# Use CSS pixels (what JavaScript sees) instead of device pixels
			width = css_visual_viewport.get('clientWidth', css_layout_viewport.get('clientWidth', 1920.0))

			# Calculate device pixel ratio
			device_width = visual_viewport.get('clientWidth', width)
			css_width = css_visual_viewport.get('clientWidth', width)
			device_pixel_ratio = device_width / css_width if css_width > 0 else 1.0

			return float(device_pixel_ratio)
		except Exception as e:
			self.logger.debug(f'Viewport size detection failed: {e}')
			# Fallback to default viewport size
			return 1.0

	@classmethod
	def is_element_visible_according_to_all_parents(
		cls, node: EnhancedDOMTreeNode, html_frames: list[EnhancedDOMTreeNode]
	) -> bool:
		"""Check if the element is visible according to all its parent HTML frames."""

		if not node.snapshot_node:
			return False

		computed_styles = node.snapshot_node.computed_styles or {}

		display = computed_styles.get('display', '').lower()
		visibility = computed_styles.get('visibility', '').lower()
		opacity = computed_styles.get('opacity', '1')

		if display == 'none' or visibility == 'hidden':
			return False

		try:
			if float(opacity) <= 0:
				return False
		except (ValueError, TypeError):
			pass

		# Start with the element's local bounds (in its own frame's coordinate system)
		current_bounds = node.snapshot_node.bounds

		if not current_bounds:
			return False  # If there are no bounds, the element is not visible

		"""
		Reverse iterate through the html frames (that can be either iframe or document -> if it's a document frame compare if the current bounds interest with it (taking scroll into account) otherwise move the current bounds by the iframe offset)
		"""
		for frame in reversed(html_frames):
			if (
				frame.node_type == NodeType.ELEMENT_NODE
				and (frame.node_name.upper() == 'IFRAME' or frame.node_name.upper() == 'FRAME')
				and frame.snapshot_node
				and frame.snapshot_node.bounds
			):
				iframe_bounds = frame.snapshot_node.bounds

				# negate the values added in `_construct_enhanced_node`
				current_bounds.x += iframe_bounds.x
				current_bounds.y += iframe_bounds.y

			if (
				frame.node_type == NodeType.ELEMENT_NODE
				and frame.node_name == 'HTML'
				and frame.snapshot_node
				and frame.snapshot_node.scrollRects
				and frame.snapshot_node.clientRects
			):
				# For iframe content, we need to check visibility within the iframe's viewport
				# The scrollRects represent the current scroll position
				# The clientRects represent the viewport size
				# Elements are visible if they fall within the viewport after accounting for scroll

				# The viewport of the frame (what's actually visible)
				viewport_left = 0  # Viewport always starts at 0 in frame coordinates
				viewport_top = 0
				viewport_right = frame.snapshot_node.clientRects.width
				viewport_bottom = frame.snapshot_node.clientRects.height

				# Adjust element bounds by the scroll offset to get position relative to viewport
				# When scrolled down, scrollRects.y is positive, so we subtract it from element's y
				adjusted_x = current_bounds.x - frame.snapshot_node.scrollRects.x
				adjusted_y = current_bounds.y - frame.snapshot_node.scrollRects.y

				frame_intersects = (
					adjusted_x < viewport_right
					and adjusted_x + current_bounds.width > viewport_left
					and adjusted_y < viewport_bottom + 1000
					and adjusted_y + current_bounds.height > viewport_top - 1000
				)

				if not frame_intersects:
					return False

				# Keep the original coordinate adjustment to maintain consistency
				# This adjustment is needed for proper coordinate transformation
				current_bounds.x -= frame.snapshot_node.scrollRects.x
				current_bounds.y -= frame.snapshot_node.scrollRects.y

		# If we reach here, element is visible in main viewport and all containing iframes
		return True

	async def _get_ax_tree_for_all_frames(self, target_id: TargetID) -> GetFullAXTreeReturns:
		"""Recursively collect all frames and merge their accessibility trees into a single array."""

		cdp_session = await self.browser_session.get_or_create_cdp_session(target_id=target_id, focus=False)
		frame_tree = await cdp_session.cdp_client.send.Page.getFrameTree(session_id=cdp_session.session_id)

		def collect_all_frame_ids(frame_tree_node) -> list[str]:
			"""Recursively collect all frame IDs from the frame tree."""
			frame_ids = [frame_tree_node['frame']['id']]

			if 'childFrames' in frame_tree_node and frame_tree_node['childFrames']:
				for child_frame in frame_tree_node['childFrames']:
					frame_ids.extend(collect_all_frame_ids(child_frame))

			return frame_ids

		# Collect all frame IDs recursively
		all_frame_ids = collect_all_frame_ids(frame_tree['frameTree'])

		# Get accessibility tree for each frame
		ax_tree_requests = []
		for frame_id in all_frame_ids:
			ax_tree_request = cdp_session.cdp_client.send.Accessibility.getFullAXTree(
				params={'frameId': frame_id}, session_id=cdp_session.session_id
			)
			ax_tree_requests.append(ax_tree_request)

		# Wait for all requests to complete
		ax_trees = await asyncio.gather(*ax_tree_requests)

		# Merge all AX nodes into a single array
		merged_nodes: list[AXNode] = []
		for ax_tree in ax_trees:
			merged_nodes.extend(ax_tree['nodes'])

		return {'nodes': merged_nodes}

	async def _get_all_trees(self, target_id: TargetID) -> TargetAllTrees:
		cdp_session = await self.browser_session.get_or_create_cdp_session(target_id=target_id, focus=False)

		# Wait for the page to be ready first
		try:
			ready_state = await cdp_session.cdp_client.send.Runtime.evaluate(
				params={'expression': 'document.readyState'}, session_id=cdp_session.session_id
			)
		except Exception as e:
			pass  # Page might not be ready yet
		# DEBUG: Log before capturing snapshot
		self.logger.debug(f'ğŸ” DEBUG: Capturing DOM snapshot for target {target_id}')

		# Get actual scroll positions for all iframes before capturing snapshot
		start_iframe_scroll = time.time()
		iframe_scroll_positions = {}
		try:
			scroll_result = await cdp_session.cdp_client.send.Runtime.evaluate(
				params={
					'expression': """
					(() => {
						const scrollData = {};
						const iframes = document.querySelectorAll('iframe');
						iframes.forEach((iframe, index) => {
							try {
								const doc = iframe.contentDocument || iframe.contentWindow.document;
								if (doc) {
									scrollData[index] = {
										scrollTop: doc.documentElement.scrollTop || doc.body.scrollTop || 0,
										scrollLeft: doc.documentElement.scrollLeft || doc.body.scrollLeft || 0
									};
								}
							} catch (e) {
								// Cross-origin iframe, can't access
							}
						});
						return scrollData;
					})()
					""",
					'returnByValue': True,
				},
				session_id=cdp_session.session_id,
			)
			if scroll_result and 'result' in scroll_result and 'value' in scroll_result['result']:
				iframe_scroll_positions = scroll_result['result']['value']
				for idx, scroll_data in iframe_scroll_positions.items():
					self.logger.debug(
						f'ğŸ” DEBUG: Iframe {idx} actual scroll position - scrollTop={scroll_data.get("scrollTop", 0)}, scrollLeft={scroll_data.get("scrollLeft", 0)}'
					)
		except Exception as e:
			self.logger.debug(f'Failed to get iframe scroll positions: {e}')
		iframe_scroll_ms = (time.time() - start_iframe_scroll) * 1000

		# Define CDP request factories to avoid duplication
		def create_snapshot_request():
			return cdp_session.cdp_client.send.DOMSnapshot.captureSnapshot(
				params={
					'computedStyles': REQUIRED_COMPUTED_STYLES,
					'includePaintOrder': True,
					'includeDOMRects': True,
					'includeBlendedBackgroundColors': False,
					'includeTextColorOpacities': False,
				},
				session_id=cdp_session.session_id,
			)

		def create_dom_tree_request():
			return cdp_session.cdp_client.send.DOM.getDocument(
				params={'depth': -1, 'pierce': True}, session_id=cdp_session.session_id
			)

		start_cdp_calls = time.time()

		# Create initial tasks
		tasks = {
			'snapshot': create_task_with_error_handling(create_snapshot_request(), name='get_snapshot'),
			'dom_tree': create_task_with_error_handling(create_dom_tree_request(), name='get_dom_tree'),
			'ax_tree': create_task_with_error_handling(self._get_ax_tree_for_all_frames(target_id), name='get_ax_tree'),
			'device_pixel_ratio': create_task_with_error_handling(self._get_viewport_ratio(target_id), name='get_viewport_ratio'),
		}

		# Wait for all tasks with timeout
		done, pending = await asyncio.wait(tasks.values(), timeout=10.0)

		# Retry any failed or timed out tasks
		if pending:
			for task in pending:
				task.cancel()

			# Retry mapping for pending tasks
			retry_map = {
				tasks['snapshot']: lambda: create_task_with_error_handling(create_snapshot_request(), name='get_snapshot_retry'),
				tasks['dom_tree']: lambda: create_task_with_error_handling(create_dom_tree_request(), name='get_dom_tree_retry'),
				tasks['ax_tree']: lambda: create_task_with_error_handling(
					self._get_ax_tree_for_all_frames(target_id), name='get_ax_tree_retry'
				),
				tasks['device_pixel_ratio']: lambda: create_task_with_error_handling(
					self._get_viewport_ratio(target_id), name='get_viewport_ratio_retry'
				),
			}

			# Create new tasks only for the ones that didn't complete
			for key, task in tasks.items():
				if task in pending and task in retry_map:
					tasks[key] = retry_map[task]()

			# Wait again with shorter timeout
			done2, pending2 = await asyncio.wait([t for t in tasks.values() if not t.done()], timeout=2.0)

			if pending2:
				for task in pending2:
					task.cancel()

		# Extract results, tracking which ones failed
		results = {}
		failed = []
		for key, task in tasks.items():
			if task.done() and not task.cancelled():
				try:
					results[key] = task.result()
				except Exception as e:
					self.logger.warning(f'CDP request {key} failed with exception: {e}')
					failed.append(key)
			else:
				self.logger.warning(f'CDP request {key} timed out')
				failed.append(key)

		# If any required tasks failed, raise an exception
		if failed:
			raise TimeoutError(f'CDP requests failed or timed out: {", ".join(failed)}')

		snapshot = results['snapshot']
		dom_tree = results['dom_tree']
		ax_tree = results['ax_tree']
		device_pixel_ratio = results['device_pixel_ratio']
		end_cdp_calls = time.time()
		cdp_calls_ms = (end_cdp_calls - start_cdp_calls) * 1000

		# Calculate total time for _get_all_trees and overhead
		start_snapshot_processing = time.time()

		# DEBUG: Log snapshot info and limit documents to prevent explosion
		if snapshot and 'documents' in snapshot:
			original_doc_count = len(snapshot['documents'])
			# Limit to max_iframes documents to prevent iframe explosion
			if original_doc_count > self.max_iframes:
				self.logger.warning(
					f'âš ï¸ Limiting processing of {original_doc_count} iframes on page to only first {self.max_iframes} to prevent crashes!'
				)
				snapshot['documents'] = snapshot['documents'][: self.max_iframes]

			total_nodes = sum(len(doc.get('nodes', [])) for doc in snapshot['documents'])
			self.logger.debug(f'ğŸ” DEBUG: Snapshot contains {len(snapshot["documents"])} frames with {total_nodes} total nodes')
			# Log iframe-specific info
			for doc_idx, doc in enumerate(snapshot['documents']):
				if doc_idx > 0:  # Not the main document
					self.logger.debug(
						f'ğŸ” DEBUG: Iframe #{doc_idx} {doc.get("frameId", "no-frame-id")} {doc.get("url", "no-url")} has {len(doc.get("nodes", []))} nodes'
					)

		snapshot_processing_ms = (time.time() - start_snapshot_processing) * 1000

		# Return with detailed timing breakdown
		return TargetAllTrees(
			snapshot=snapshot,
			dom_tree=dom_tree,
			ax_tree=ax_tree,
			device_pixel_ratio=device_pixel_ratio,
			cdp_timing={
				'iframe_scroll_detection_ms': iframe_scroll_ms,
				'cdp_parallel_calls_ms': cdp_calls_ms,
				'snapshot_processing_ms': snapshot_processing_ms,
			},
		)

	@observe_debug(ignore_input=True, ignore_output=True, name='get_dom_tree')
	async def get_dom_tree(
		self,
		target_id: TargetID,
		all_frames: dict | None = None,
		initial_html_frames: list[EnhancedDOMTreeNode] | None = None,
		initial_total_frame_offset: DOMRect | None = None,
		iframe_depth: int = 0,
	) -> tuple[EnhancedDOMTreeNode, dict[str, float]]:
		"""Get the DOM tree for a specific target.

		Args:
			target_id: Target ID of the page to get the DOM tree for.
			all_frames: Pre-fetched frame hierarchy to avoid redundant CDP calls (optional, lazy fetch if None)
			initial_html_frames: List of HTML frame nodes encountered so far
			initial_total_frame_offset: Accumulated coordinate offset
			iframe_depth: Current depth of iframe nesting to prevent infinite recursion

		Returns:
			Tuple of (enhanced_dom_tree_node, timing_info)
		"""
		timing_info: dict[str, float] = {}
		timing_start_total = time.time()

		# Get all trees from CDP (snapshot, DOM, AX, viewport ratio)
		start_get_trees = time.time()
		trees = await self._get_all_trees(target_id)
		get_trees_ms = (time.time() - start_get_trees) * 1000
		timing_info.update(trees.cdp_timing)
		timing_info['get_all_trees_total_ms'] = get_trees_ms

		dom_tree = trees.dom_tree
		ax_tree = trees.ax_tree
		snapshot = trees.snapshot
		device_pixel_ratio = trees.device_pixel_ratio

		# Build AX tree lookup
		start_ax = time.time()
		ax_tree_lookup: dict[int, AXNode] = {
			ax_node['backendDOMNodeId']: ax_node for ax_node in ax_tree['nodes'] if 'backendDOMNodeId' in ax_node
		}
		timing_info['build_ax_lookup_ms'] = (time.time() - start_ax) * 1000

		enhanced_dom_tree_node_lookup: dict[int, EnhancedDOMTreeNode] = {}
		""" NodeId (NOT backend node id) -> enhanced dom tree node"""  # way to get the parent/content node

		# Parse snapshot data with everything calculated upfront
		start_snapshot = time.time()
		snapshot_lookup = build_snapshot_lookup(snapshot, device_pixel_ratio)
		timing_info['build_snapshot_lookup_ms'] = (time.time() - start_snapshot) * 1000

		async def _construct_enhanced_node(
			node: Node,
			html_frames: list[EnhancedDOMTreeNode] | None,
			total_frame_offset: DOMRect | None,
			all_frames: dict | None,
		) -> EnhancedDOMTreeNode:
			"""
			Recursively construct enhanced DOM tree nodes.

			Args:
				node: The DOM node to construct
				html_frames: List of HTML frame nodes encountered so far
				total_frame_offset: Accumulated coordinate translation from parent iframes (includes scroll corrections)
				all_frames: Pre-fetched frame hierarchy to avoid redundant CDP calls
			"""

			# Initialize lists if not provided
			if html_frames is None:
				html_frames = []

			# to get rid of the pointer references
			if total_frame_offset is None:
				total_frame_offset = DOMRect(x=0.0, y=0.0, width=0.0, height=0.0)
			else:
				total_frame_offset = DOMRect(
					total_frame_offset.x, total_frame_offset.y, total_frame_offset.width, total_frame_offset.height
				)

			# memoize the mf (I don't know if some nodes are duplicated)
			if node['nodeId'] in enhanced_dom_tree_node_lookup:
				return enhanced_dom_tree_node_lookup[node['nodeId']]

			ax_node = ax_tree_lookup.get(node['backendNodeId'])
			if ax_node:
				enhanced_ax_node = self._build_enhanced_ax_node(ax_node)
			else:
				enhanced_ax_node = None

			# To make attributes more readable
			attributes: dict[str, str] | None = None
			if 'attributes' in node and node['attributes']:
				attributes = {}
				for i in range(0, len(node['attributes']), 2):
					attributes[node['attributes'][i]] = node['attributes'][i + 1]

			shadow_root_type = None
			if 'shadowRootType' in node and node['shadowRootType']:
				try:
					shadow_root_type = node['shadowRootType']
				except ValueError:
					pass

			# Get snapshot data and calculate absolute position
			snapshot_data = snapshot_lookup.get(node['backendNodeId'], None)
			absolute_position = None
			if snapshot_data and snapshot_data.bounds:
				absolute_position = DOMRect(
					x=snapshot_data.bounds.x + total_frame_offset.x,
					y=snapshot_data.bounds.y + total_frame_offset.y,
					width=snapshot_data.bounds.width,
					height=snapshot_data.bounds.height,
				)

			try:
				session = await self.browser_session.get_or_create_cdp_session(target_id, focus=False)
				session_id = session.session_id
			except ValueError:
				# Target may have detached during DOM construction
				session_id = None

			dom_tree_node = EnhancedDOMTreeNode(
				node_id=node['nodeId'],
				backend_node_id=node['backendNodeId'],
				node_type=NodeType(node['nodeType']),
				node_name=node['nodeName'],
				node_value=node['nodeValue'],
				attributes=attributes or {},
				is_scrollable=node.get('isScrollable', None),
				frame_id=node.get('frameId', None),
				session_id=session_id,
				target_id=target_id,
				content_document=None,
				shadow_root_type=shadow_root_type,
				shadow_roots=None,
				parent_node=None,
				children_nodes=None,
				ax_node=enhanced_ax_node,
				snapshot_node=snapshot_data,
				is_visible=None,
				absolute_position=absolute_position,
			)

			enhanced_dom_tree_node_lookup[node['nodeId']] = dom_tree_node

			if 'parentId' in node and node['parentId']:
				dom_tree_node.parent_node = enhanced_dom_tree_node_lookup[
					node['parentId']
				]  # parents should always be in the lookup

			# Check if this is an HTML frame node and add it to the list
			updated_html_frames = html_frames.copy()
			if node['nodeType'] == NodeType.ELEMENT_NODE.value and node['nodeName'] == 'HTML' and node.get('frameId') is not None:
				updated_html_frames.append(dom_tree_node)

				# and adjust the total frame offset by scroll
				if snapshot_data and snapshot_data.scrollRects:
					total_frame_offset.x -= snapshot_data.scrollRects.x
					total_frame_offset.y -= snapshot_data.scrollRects.y
					# DEBUG: Log iframe scroll information
					self.logger.debug(
						f'ğŸ” DEBUG: HTML frame scroll - scrollY={snapshot_data.scrollRects.y}, scrollX={snapshot_data.scrollRects.x}, frameId={node.get("frameId")}, nodeId={node["nodeId"]}'
					)

			# Calculate new iframe offset for content documents, accounting for iframe scroll
			if (
				(node['nodeName'].upper() == 'IFRAME' or node['nodeName'].upper() == 'FRAME')
				and snapshot_data
				and snapshot_data.bounds
			):
				if snapshot_data.bounds:
					updated_html_frames.append(dom_tree_node)

					total_frame_offset.x += snapshot_data.bounds.x
					total_frame_offset.y += snapshot_data.bounds.y

			if 'contentDocument' in node and node['contentDocument']:
				dom_tree_node.content_document = await _construct_enhanced_node(
					node['contentDocument'], updated_html_frames, total_frame_offset, all_frames
				)
				dom_tree_node.content_document.parent_node = dom_tree_node
				# forcefully set the parent node to the content document node (helps traverse the tree)

			if 'shadowRoots' in node and node['shadowRoots']:
				dom_tree_node.shadow_roots = []
				for shadow_root in node['shadowRoots']:
					shadow_root_node = await _construct_enhanced_node(
						shadow_root, updated_html_frames, total_frame_offset, all_frames
					)
					# forcefully set the parent node to the shadow root node (helps traverse the tree)
					shadow_root_node.parent_node = dom_tree_node
					dom_tree_node.shadow_roots.append(shadow_root_node)

			if 'children' in node and node['children']:
				dom_tree_node.children_nodes = []
				# Build set of shadow root node IDs to filter them out from children
				shadow_root_node_ids = set()
				if 'shadowRoots' in node and node['shadowRoots']:
					for shadow_root in node['shadowRoots']:
						shadow_root_node_ids.add(shadow_root['nodeId'])

				for child in node['children']:
					# Skip shadow roots - they should only be in shadow_roots list
					if child['nodeId'] in shadow_root_node_ids:
						continue
					dom_tree_node.children_nodes.append(
						await _construct_enhanced_node(child, updated_html_frames, total_frame_offset, all_frames)
					)

			# Set visibility using the collected HTML frames
			dom_tree_node.is_visible = self.is_element_visible_according_to_all_parents(dom_tree_node, updated_html_frames)

			# DEBUG: Log visibility info for form elements in iframes
			if dom_tree_node.tag_name and dom_tree_node.tag_name.upper() in ['INPUT', 'SELECT', 'TEXTAREA', 'LABEL']:
				attrs = dom_tree_node.attributes or {}
				elem_id = attrs.get('id', '')
				elem_name = attrs.get('name', '')
				if (
					'city' in elem_id.lower()
					or 'city' in elem_name.lower()
					or 'state' in elem_id.lower()
					or 'state' in elem_name.lower()
					or 'zip' in elem_id.lower()
					or 'zip' in elem_name.lower()
				):
					self.logger.debug(
						f"ğŸ” DEBUG: Form element {dom_tree_node.tag_name} id='{elem_id}' name='{elem_name}' - visible={dom_tree_node.is_visible}, bounds={dom_tree_node.snapshot_node.bounds if dom_tree_node.snapshot_node else 'NO_SNAPSHOT'}"
					)

			# handle cross origin iframe (just recursively call the main function with the proper target if it exists in iframes)
			# only do this if the iframe is visible (otherwise it's not worth it)

			if (
				# TODO: hacky way to disable cross origin iframes for now
				self.cross_origin_iframes and node['nodeName'].upper() == 'IFRAME' and node.get('contentDocument', None) is None
			):  # None meaning there is no content
				# Check iframe depth to prevent infinite recursion
				if iframe_depth >= self.max_iframe_depth:
					self.logger.debug(
						f'Skipping iframe at depth {iframe_depth} to prevent infinite recursion (max depth: {self.max_iframe_depth})'
					)
				else:
					# Check if iframe is visible and large enough (>= 50px in both dimensions)
					should_process_iframe = False

					# First check if the iframe element itself is visible
					if dom_tree_node.is_visible:
						# Check iframe dimensions
						if dom_tree_node.snapshot_node and dom_tree_node.snapshot_node.bounds:
							bounds = dom_tree_node.snapshot_node.bounds
							width = bounds.width
							height = bounds.height

							# Only process if iframe is at least 50px in both dimensions
							if width >= 50 and height >= 50:
								should_process_iframe = True
								self.logger.debug(f'Processing cross-origin iframe: visible=True, width={width}, height={height}')
							else:
								self.logger.debug(
									f'Skipping small cross-origin iframe: width={width}, height={height} (needs >= 50px)'
								)
						else:
							self.logger.debug('Skipping cross-origin iframe: no bounds available')
					else:
						self.logger.debug('Skipping invisible cross-origin iframe')

					if should_process_iframe:
						# Lazy fetch all_frames only when actually needed (for cross-origin iframes)
						if all_frames is None:
							all_frames, _ = await self.browser_session.get_all_frames()

						# Use pre-fetched all_frames to find the iframe's target (no redundant CDP call)
						frame_id = node.get('frameId', None)
						if frame_id:
							frame_info = all_frames.get(frame_id)
							iframe_document_target = None
							if frame_info and frame_info.get('frameTargetId'):
								iframe_target_id = frame_info['frameTargetId']
								iframe_target = self.browser_session.session_manager.get_target(iframe_target_id)
								if iframe_target:
									iframe_document_target = {
										'targetId': iframe_target.target_id,
										'url': iframe_target.url,
										'title': iframe_target.title,
										'type': iframe_target.target_type,
									}
						else:
							iframe_document_target = None
						# if target actually exists in one of the frames, just recursively build the dom tree for it
						if iframe_document_target:
							self.logger.debug(
								f'Getting content document for iframe {node.get("frameId", None)} at depth {iframe_depth + 1}'
							)
							content_document, _ = await self.get_dom_tree(
								target_id=iframe_document_target['targetId'],
								all_frames=all_frames,
								# TODO: experiment with this values -> not sure whether the whole cross origin iframe should be ALWAYS included as soon as some part of it is visible or not.
								# Current config: if the cross origin iframe is AT ALL visible, then just include everything inside of it!
								# initial_html_frames=updated_html_frames,
								initial_total_frame_offset=total_frame_offset,
								iframe_depth=iframe_depth + 1,
							)

							dom_tree_node.content_document = content_document
							dom_tree_node.content_document.parent_node = dom_tree_node

			return dom_tree_node

		# Build enhanced DOM tree recursively
		# Note: all_frames stays None and will be lazily fetched inside _construct_enhanced_node
		# only if/when a cross-origin iframe is encountered
		start_construct = time.time()
		enhanced_dom_tree_node = await _construct_enhanced_node(
			dom_tree['root'], initial_html_frames, initial_total_frame_offset, all_frames
		)
		timing_info['construct_enhanced_tree_ms'] = (time.time() - start_construct) * 1000

		# Calculate total time for get_dom_tree
		total_get_dom_tree_ms = (time.time() - timing_start_total) * 1000
		timing_info['get_dom_tree_total_ms'] = total_get_dom_tree_ms

		# Calculate overhead in get_dom_tree (time not accounted for by sub-operations)
		tracked_sub_operations_ms = (
			timing_info.get('get_all_trees_total_ms', 0)
			+ timing_info.get('build_ax_lookup_ms', 0)
			+ timing_info.get('build_snapshot_lookup_ms', 0)
			+ timing_info.get('construct_enhanced_tree_ms', 0)
		)
		get_dom_tree_overhead_ms = total_get_dom_tree_ms - tracked_sub_operations_ms
		if get_dom_tree_overhead_ms > 0.1:
			timing_info['get_dom_tree_overhead_ms'] = get_dom_tree_overhead_ms

		return enhanced_dom_tree_node, timing_info

	@observe_debug(ignore_input=True, ignore_output=True, name='get_serialized_dom_tree')
	async def get_serialized_dom_tree(
		self, previous_cached_state: SerializedDOMState | None = None
	) -> tuple[SerializedDOMState, EnhancedDOMTreeNode, dict[str, float]]:
		"""Get the serialized DOM tree representation for LLM consumption.

		Returns:
			Tuple of (serialized_dom_state, enhanced_dom_tree_root, timing_info)
		"""
		timing_info: dict[str, float] = {}
		start_total = time.time()

		# Use current target (None means use current)
		assert self.browser_session.agent_focus_target_id is not None

		session_id = self.browser_session.id

		# Build DOM tree (includes CDP calls for snapshot, DOM, AX tree)
		# Note: all_frames is fetched lazily inside get_dom_tree only if cross-origin iframes need it
		enhanced_dom_tree, dom_tree_timing = await self.get_dom_tree(
			target_id=self.browser_session.agent_focus_target_id,
			all_frames=None,  # Lazy - will fetch if needed
		)

		# Add sub-timings from DOM tree construction
		timing_info.update(dom_tree_timing)

		# Serialize DOM tree for LLM
		start_serialize = time.time()

		serialized_dom_state, serializer_timing = DOMTreeSerializer(
			enhanced_dom_tree, previous_cached_state, paint_order_filtering=self.paint_order_filtering, session_id=session_id
		).serialize_accessible_elements()
		total_serialization_ms = (time.time() - start_serialize) * 1000

		# Add serializer sub-timings (convert to ms)
		for key, value in serializer_timing.items():
			timing_info[f'{key}_ms'] = value * 1000

		# Calculate untracked time in serialization
		tracked_serialization_ms = sum(value * 1000 for value in serializer_timing.values())
		serialization_overhead_ms = total_serialization_ms - tracked_serialization_ms
		if serialization_overhead_ms > 0.1:  # Only log if significant
			timing_info['serialization_overhead_ms'] = serialization_overhead_ms

		# Calculate total time for get_serialized_dom_tree
		total_get_serialized_dom_tree_ms = (time.time() - start_total) * 1000
		timing_info['get_serialized_dom_tree_total_ms'] = total_get_serialized_dom_tree_ms

		# Calculate overhead in get_serialized_dom_tree (time not accounted for)
		tracked_major_operations_ms = timing_info.get('get_dom_tree_total_ms', 0) + total_serialization_ms
		get_serialized_overhead_ms = total_get_serialized_dom_tree_ms - tracked_major_operations_ms
		if get_serialized_overhead_ms > 0.1:
			timing_info['get_serialized_dom_tree_overhead_ms'] = get_serialized_overhead_ms

		return serialized_dom_state, enhanced_dom_tree, timing_info

	@staticmethod
	def detect_pagination_buttons(selector_map: dict[int, EnhancedDOMTreeNode]) -> list[dict[str, str | int | bool]]:
		"""Detect pagination buttons from the selector map.

		Args:
			selector_map: Map of element indices to EnhancedDOMTreeNode

		Returns:
			List of pagination button information dicts with:
			- button_type: 'next', 'prev', 'first', 'last', 'page_number'
			- backend_node_id: Backend node ID for clicking
			- text: Button text/label
			- selector: XPath selector
			- is_disabled: Whether the button appears disabled
		"""
		pagination_buttons: list[dict[str, str | int | bool]] = []

		# Common pagination patterns to look for
		next_patterns = ['next', '>', 'Â»', 'â†’', 'siguiente', 'suivant', 'weiter', 'volgende']
		prev_patterns = ['prev', 'previous', '<', 'Â«', 'â†', 'anterior', 'prÃ©cÃ©dent', 'zurÃ¼ck', 'vorige']
		first_patterns = ['first', 'â‡¤', 'Â«', 'primera', 'premiÃ¨re', 'erste', 'eerste']
		last_patterns = ['last', 'â‡¥', 'Â»', 'Ãºltima', 'dernier', 'letzte', 'laatste']

		for index, node in selector_map.items():
			# Skip non-clickable elements
			if not node.snapshot_node or not node.snapshot_node.is_clickable:
				continue

			# Get element text and attributes
			text = node.get_all_children_text().lower().strip()
			aria_label = node.attributes.get('aria-label', '').lower()
			title = node.attributes.get('title', '').lower()
			class_name = node.attributes.get('class', '').lower()
			role = node.attributes.get('role', '').lower()

			# Combine all text sources for pattern matching
			all_text = f'{text} {aria_label} {title} {class_name}'.strip()

			# Check if it's disabled
			is_disabled = (
				node.attributes.get('disabled') == 'true'
				or node.attributes.get('aria-disabled') == 'true'
				or 'disabled' in class_name
			)

			button_type: str | None = None

			# Check for next button
			if any(pattern in all_text for pattern in next_patterns):
				button_type = 'next'
			# Check for previous button
			elif any(pattern in all_text for pattern in prev_patterns):
				button_type = 'prev'
			# Check for first button
			elif any(pattern in all_text for pattern in first_patterns):
				button_type = 'first'
			# Check for last button
			elif any(pattern in all_text for pattern in last_patterns):
				button_type = 'last'
			# Check for numeric page buttons (single or double digit)
			elif text.isdigit() and len(text) <= 2 and role in ['button', 'link', '']:
				button_type = 'page_number'

			if button_type:
				pagination_buttons.append(
					{
						'button_type': button_type,
						'backend_node_id': index,
						'text': node.get_all_children_text().strip() or aria_label or title,
						'selector': node.xpath,
						'is_disabled': is_disabled,
					}
				)

		return pagination_buttons

```

---

## backend/browser-use/browser_use/dom/utils.py

```py
def cap_text_length(text: str, max_length: int) -> str:
	"""Cap text length for display."""
	if len(text) <= max_length:
		return text
	return text[:max_length] + '...'


def generate_css_selector_for_element(enhanced_node) -> str | None:
	"""Generate a CSS selector using node properties from version 0.5.0 approach."""
	import re

	if not enhanced_node or not hasattr(enhanced_node, 'tag_name') or not enhanced_node.tag_name:
		return None

	# Get base selector from tag name (simplified since we don't have xpath in EnhancedDOMTreeNode)
	tag_name = enhanced_node.tag_name.lower().strip()
	if not tag_name or not re.match(r'^[a-zA-Z][a-zA-Z0-9-]*$', tag_name):
		return None

	css_selector = tag_name

	# Add ID if available (most specific)
	if enhanced_node.attributes and 'id' in enhanced_node.attributes:
		element_id = enhanced_node.attributes['id']
		if element_id and element_id.strip():
			element_id = element_id.strip()
			# Validate ID contains only valid characters for # selector
			if re.match(r'^[a-zA-Z][a-zA-Z0-9_-]*$', element_id):
				return f'#{element_id}'
			else:
				# For IDs with special characters ($, ., :, etc.), use attribute selector
				# Escape quotes in the ID value
				escaped_id = element_id.replace('"', '\\"')
				return f'{tag_name}[id="{escaped_id}"]'

	# Handle class attributes (from version 0.5.0 approach)
	if enhanced_node.attributes and 'class' in enhanced_node.attributes and enhanced_node.attributes['class']:
		# Define a regex pattern for valid class names in CSS
		valid_class_name_pattern = re.compile(r'^[a-zA-Z_][a-zA-Z0-9_-]*$')

		# Iterate through the class attribute values
		classes = enhanced_node.attributes['class'].split()
		for class_name in classes:
			# Skip empty class names
			if not class_name.strip():
				continue

			# Check if the class name is valid
			if valid_class_name_pattern.match(class_name):
				# Append the valid class name to the CSS selector
				css_selector += f'.{class_name}'

	# Expanded set of safe attributes that are stable and useful for selection (from v0.5.0)
	SAFE_ATTRIBUTES = {
		# Data attributes (if they're stable in your application)
		'id',
		# Standard HTML attributes
		'name',
		'type',
		'placeholder',
		# Accessibility attributes
		'aria-label',
		'aria-labelledby',
		'aria-describedby',
		'role',
		# Common form attributes
		'for',
		'autocomplete',
		'required',
		'readonly',
		# Media attributes
		'alt',
		'title',
		'src',
		# Custom stable attributes (add any application-specific ones)
		'href',
		'target',
	}

	# Always include dynamic attributes (include_dynamic_attributes=True equivalent)
	include_dynamic_attributes = True
	if include_dynamic_attributes:
		dynamic_attributes = {
			'data-id',
			'data-qa',
			'data-cy',
			'data-testid',
		}
		SAFE_ATTRIBUTES.update(dynamic_attributes)

	# Handle other attributes (from version 0.5.0 approach)
	if enhanced_node.attributes:
		for attribute, value in enhanced_node.attributes.items():
			if attribute == 'class':
				continue

			# Skip invalid attribute names
			if not attribute.strip():
				continue

			if attribute not in SAFE_ATTRIBUTES:
				continue

			# Escape special characters in attribute names
			safe_attribute = attribute.replace(':', r'\:')

			# Handle different value cases
			if value == '':
				css_selector += f'[{safe_attribute}]'
			elif any(char in value for char in '"\'<>`\n\r\t'):
				# Use contains for values with special characters
				# For newline-containing text, only use the part before the newline
				if '\n' in value:
					value = value.split('\n')[0]
				# Regex-substitute *any* whitespace with a single space, then strip.
				collapsed_value = re.sub(r'\s+', ' ', value).strip()
				# Escape embedded double-quotes.
				safe_value = collapsed_value.replace('"', '\\"')
				css_selector += f'[{safe_attribute}*="{safe_value}"]'
			else:
				css_selector += f'[{safe_attribute}="{value}"]'

	# Final validation: ensure the selector is safe and doesn't contain problematic characters
	# Note: quotes are allowed in attribute selectors like [name="value"]
	if css_selector and not any(char in css_selector for char in ['\n', '\r', '\t']):
		return css_selector

	# If we get here, the selector was problematic, return just the tag name as fallback
	return tag_name

```

---

## backend/browser-use/browser_use/dom/views.py

```py
import hashlib
from dataclasses import asdict, dataclass, field
from enum import Enum
from typing import Any

from cdp_use.cdp.accessibility.commands import GetFullAXTreeReturns
from cdp_use.cdp.accessibility.types import AXPropertyName
from cdp_use.cdp.dom.commands import GetDocumentReturns
from cdp_use.cdp.dom.types import ShadowRootType
from cdp_use.cdp.domsnapshot.commands import CaptureSnapshotReturns
from cdp_use.cdp.target.types import SessionID, TargetID, TargetInfo
from uuid_extensions import uuid7str

from browser_use.dom.utils import cap_text_length
from browser_use.observability import observe_debug

# Serializer types
DEFAULT_INCLUDE_ATTRIBUTES = [
	'title',
	'type',
	'checked',
	# 'class',
	'id',
	'name',
	'role',
	'value',
	'placeholder',
	'data-date-format',
	'alt',
	'aria-label',
	'aria-expanded',
	'data-state',
	'aria-checked',
	# ARIA value attributes for datetime/range inputs
	'aria-valuemin',
	'aria-valuemax',
	'aria-valuenow',
	'aria-placeholder',
	# Validation attributes - help agents avoid brute force attempts
	'pattern',
	'min',
	'max',
	'minlength',
	'maxlength',
	'step',
	'accept',  # File input types (e.g., accept="image/*" or accept=".pdf")
	'multiple',  # Whether multiple files/selections are allowed
	'inputmode',  # Virtual keyboard hint (numeric, tel, email, url, etc.)
	'autocomplete',  # Autocomplete behavior hint
	'data-mask',  # Input mask format (e.g., phone numbers, credit cards)
	'data-inputmask',  # Alternative input mask attribute
	'data-datepicker',  # jQuery datepicker indicator
	'format',  # Synthetic attribute for date/time input format (e.g., MM/dd/yyyy)
	'expected_format',  # Synthetic attribute for explicit expected format (e.g., AngularJS datepickers)
	'contenteditable',  # Rich text editor detection
	# Webkit shadow DOM identifiers
	'pseudo',
	# Accessibility properties from ax_node (ordered by importance for automation)
	'checked',
	'selected',
	'expanded',
	'pressed',
	'disabled',
	'invalid',  # Current validation state from AX node
	'valuemin',  # Min value from AX node (for datetime/range)
	'valuemax',  # Max value from AX node (for datetime/range)
	'valuenow',
	'keyshortcuts',
	'haspopup',
	'multiselectable',
	# Less commonly needed (uncomment if required):
	# 'readonly',
	'required',
	'valuetext',
	'level',
	'busy',
	'live',
	# Accessibility name (contains text content for StaticText elements)
	'ax_name',
]

STATIC_ATTRIBUTES = {
	'class',
	'id',
	'name',
	'type',
	'placeholder',
	'aria-label',
	'title',
	# 'aria-expanded',
	'role',
	'data-testid',
	'data-test',
	'data-cy',
	'data-selenium',
	'for',
	'required',
	'disabled',
	'readonly',
	'checked',
	'selected',
	'multiple',
	'accept',
	'href',
	'target',
	'rel',
	'aria-describedby',
	'aria-labelledby',
	'aria-controls',
	'aria-owns',
	'aria-live',
	'aria-atomic',
	'aria-busy',
	'aria-disabled',
	'aria-hidden',
	'aria-pressed',
	'aria-checked',
	'aria-selected',
	'tabindex',
	'alt',
	'src',
	'lang',
	'itemscope',
	'itemtype',
	'itemprop',
	# Webkit shadow DOM attributes
	'pseudo',
	'aria-valuemin',
	'aria-valuemax',
	'aria-valuenow',
	'aria-placeholder',
}


@dataclass
class CurrentPageTargets:
	page_session: TargetInfo
	iframe_sessions: list[TargetInfo]
	"""
	Iframe sessions are ALL the iframes sessions of all the pages (not just the current page)
	"""


@dataclass
class TargetAllTrees:
	snapshot: CaptureSnapshotReturns
	dom_tree: GetDocumentReturns
	ax_tree: GetFullAXTreeReturns
	device_pixel_ratio: float
	cdp_timing: dict[str, float]


@dataclass(slots=True)
class PropagatingBounds:
	"""Track bounds that propagate from parent elements to filter children."""

	tag: str  # The tag that started propagation ('a' or 'button')
	bounds: 'DOMRect'  # The bounding box
	node_id: int  # Node ID for debugging
	depth: int  # How deep in tree this started (for debugging)


@dataclass(slots=True)
class SimplifiedNode:
	"""Simplified tree node for optimization."""

	original_node: 'EnhancedDOMTreeNode'
	children: list['SimplifiedNode']
	should_display: bool = True
	is_interactive: bool = False  # True if element is in selector_map

	is_new: bool = False

	ignored_by_paint_order: bool = False  # More info in dom/serializer/paint_order.py
	excluded_by_parent: bool = False  # New field for bbox filtering
	is_shadow_host: bool = False  # New field for shadow DOM hosts
	is_compound_component: bool = False  # True for virtual components of compound controls

	def _clean_original_node_json(self, node_json: dict) -> dict:
		"""Recursively remove children_nodes and shadow_roots from original_node JSON."""
		# Remove the fields we don't want in SimplifiedNode serialization
		if 'children_nodes' in node_json:
			del node_json['children_nodes']
		if 'shadow_roots' in node_json:
			del node_json['shadow_roots']

		# Clean nested content_document if it exists
		if node_json.get('content_document'):
			node_json['content_document'] = self._clean_original_node_json(node_json['content_document'])

		return node_json

	def __json__(self) -> dict:
		original_node_json = self.original_node.__json__()
		# Remove children_nodes and shadow_roots to avoid duplication with SimplifiedNode.children
		cleaned_original_node_json = self._clean_original_node_json(original_node_json)
		return {
			'should_display': self.should_display,
			'is_interactive': self.is_interactive,
			'ignored_by_paint_order': self.ignored_by_paint_order,
			'excluded_by_parent': self.excluded_by_parent,
			'original_node': cleaned_original_node_json,
			'children': [c.__json__() for c in self.children],
		}


class NodeType(int, Enum):
	"""DOM node types based on the DOM specification."""

	ELEMENT_NODE = 1
	ATTRIBUTE_NODE = 2
	TEXT_NODE = 3
	CDATA_SECTION_NODE = 4
	ENTITY_REFERENCE_NODE = 5
	ENTITY_NODE = 6
	PROCESSING_INSTRUCTION_NODE = 7
	COMMENT_NODE = 8
	DOCUMENT_NODE = 9
	DOCUMENT_TYPE_NODE = 10
	DOCUMENT_FRAGMENT_NODE = 11
	NOTATION_NODE = 12


@dataclass(slots=True)
class DOMRect:
	x: float
	y: float
	width: float
	height: float

	def to_dict(self) -> dict[str, Any]:
		return {
			'x': self.x,
			'y': self.y,
			'width': self.width,
			'height': self.height,
		}

	def __json__(self) -> dict:
		return self.to_dict()


@dataclass(slots=True)
class EnhancedAXProperty:
	"""we don't need `sources` and `related_nodes` for now (not sure how to use them)

	TODO: there is probably some way to determine whether it has a value or related nodes or not, but for now it's kinda fine idk
	"""

	name: AXPropertyName
	value: str | bool | None
	# related_nodes: list[EnhancedAXRelatedNode] | None


@dataclass(slots=True)
class EnhancedAXNode:
	ax_node_id: str
	"""Not to be confused the DOM node_id. Only useful for AX node tree"""
	ignored: bool
	# we don't need ignored_reasons as we anyway ignore the node otherwise
	role: str | None
	name: str | None
	description: str | None

	properties: list[EnhancedAXProperty] | None
	child_ids: list[str] | None


@dataclass(slots=True)
class EnhancedSnapshotNode:
	"""Snapshot data extracted from DOMSnapshot for enhanced functionality."""

	is_clickable: bool | None
	cursor_style: str | None
	bounds: DOMRect | None
	"""
	Document coordinates (origin = top-left of the page, ignores current scroll).
	Equivalent JS API: layoutNode.boundingBox in the older API.
	Typical use: Quick hit-test that doesn't care about scroll position.
	"""

	clientRects: DOMRect | None
	"""
	Viewport coordinates (origin = top-left of the visible scrollport).
	Equivalent JS API: element.getClientRects() / getBoundingClientRect().
	Typical use: Pixel-perfect hit-testing on screen, taking current scroll into account.
	"""

	scrollRects: DOMRect | None
	"""
	Scrollable area of the element.
	"""

	computed_styles: dict[str, str] | None
	"""Computed styles from the layout tree"""
	paint_order: int | None
	"""Paint order from the layout tree"""
	stacking_contexts: int | None
	"""Stacking contexts from the layout tree"""


# @dataclass(slots=True)
# class SuperSelector:
# 	node_id: int
# 	backend_node_id: int
# 	frame_id: str | None
# 	target_id: TargetID

# 	node_type: NodeType
# 	node_name: str

# 	# is_visible: bool | None
# 	# is_scrollable: bool | None

# 	element_index: int | None


@dataclass(slots=True)
class EnhancedDOMTreeNode:
	"""
	Enhanced DOM tree node that contains information from AX, DOM, and Snapshot trees. It's mostly based on the types on DOM node type with enhanced data from AX and Snapshot trees.

	@dev when serializing check if the value is a valid value first!

	Learn more about the fields:
	- (DOM node) https://chromedevtools.github.io/devtools-protocol/tot/DOM/#type-BackendNode
	- (AX node) https://chromedevtools.github.io/devtools-protocol/tot/Accessibility/#type-AXNode
	- (Snapshot node) https://chromedevtools.github.io/devtools-protocol/tot/DOMSnapshot/#type-DOMNode
	"""

	# region - DOM Node data

	node_id: int
	backend_node_id: int

	node_type: NodeType
	"""Node types, defined in `NodeType` enum."""
	node_name: str
	"""Only applicable for `NodeType.ELEMENT_NODE`"""
	node_value: str
	"""this is where the value from `NodeType.TEXT_NODE` is stored usually"""
	attributes: dict[str, str]
	"""slightly changed from the original attributes to be more readable"""
	is_scrollable: bool | None
	"""
	Whether the node is scrollable.
	"""
	is_visible: bool | None
	"""
	Whether the node is visible according to the upper most frame node.
	"""

	absolute_position: DOMRect | None
	"""
	Absolute position of the node in the document according to the top-left of the page.
	"""

	# frames
	target_id: TargetID
	frame_id: str | None
	session_id: SessionID | None
	content_document: 'EnhancedDOMTreeNode | None'
	"""
	Content document is the document inside a new iframe.
	"""
	# Shadow DOM
	shadow_root_type: ShadowRootType | None
	shadow_roots: list['EnhancedDOMTreeNode'] | None
	"""
	Shadow roots are the shadow DOMs of the element.
	"""

	# Navigation
	parent_node: 'EnhancedDOMTreeNode | None'
	children_nodes: list['EnhancedDOMTreeNode'] | None

	# endregion - DOM Node data

	# region - AX Node data
	ax_node: EnhancedAXNode | None

	# endregion - AX Node data

	# region - Snapshot Node data
	snapshot_node: EnhancedSnapshotNode | None

	# endregion - Snapshot Node data

	# Compound control child components information
	_compound_children: list[dict[str, Any]] = field(default_factory=list)

	uuid: str = field(default_factory=uuid7str)

	@property
	def parent(self) -> 'EnhancedDOMTreeNode | None':
		return self.parent_node

	@property
	def children(self) -> list['EnhancedDOMTreeNode']:
		return self.children_nodes or []

	@property
	def children_and_shadow_roots(self) -> list['EnhancedDOMTreeNode']:
		"""
		Returns all children nodes, including shadow roots
		"""
		# IMPORTANT: Make a copy to avoid mutating the original children_nodes list!
		children = list(self.children_nodes) if self.children_nodes else []
		if self.shadow_roots:
			children.extend(self.shadow_roots)
		return children

	@property
	def tag_name(self) -> str:
		return self.node_name.lower()

	@property
	def xpath(self) -> str:
		"""Generate XPath for this DOM node, stopping at shadow boundaries or iframes."""
		segments = []
		current_element = self

		while current_element and (
			current_element.node_type == NodeType.ELEMENT_NODE or current_element.node_type == NodeType.DOCUMENT_FRAGMENT_NODE
		):
			# just pass through shadow roots
			if current_element.node_type == NodeType.DOCUMENT_FRAGMENT_NODE:
				current_element = current_element.parent_node
				continue

			# stop ONLY if we hit iframe
			if current_element.parent_node and current_element.parent_node.node_name.lower() == 'iframe':
				break

			position = self._get_element_position(current_element)
			tag_name = current_element.node_name.lower()
			xpath_index = f'[{position}]' if position > 0 else ''
			segments.insert(0, f'{tag_name}{xpath_index}')

			current_element = current_element.parent_node

		return '/'.join(segments)

	def _get_element_position(self, element: 'EnhancedDOMTreeNode') -> int:
		"""Get the position of an element among its siblings with the same tag name.
		Returns 0 if it's the only element of its type, otherwise returns 1-based index."""
		if not element.parent_node or not element.parent_node.children_nodes:
			return 0

		same_tag_siblings = [
			child
			for child in element.parent_node.children_nodes
			if child.node_type == NodeType.ELEMENT_NODE and child.node_name.lower() == element.node_name.lower()
		]

		if len(same_tag_siblings) <= 1:
			return 0  # No index needed if it's the only one

		try:
			# XPath is 1-indexed
			position = same_tag_siblings.index(element) + 1
			return position
		except ValueError:
			return 0

	def __json__(self) -> dict:
		"""Serializes the node and its descendants to a dictionary, omitting parent references."""
		return {
			'node_id': self.node_id,
			'backend_node_id': self.backend_node_id,
			'node_type': self.node_type.name,
			'node_name': self.node_name,
			'node_value': self.node_value,
			'is_visible': self.is_visible,
			'attributes': self.attributes,
			'is_scrollable': self.is_scrollable,
			'session_id': self.session_id,
			'target_id': self.target_id,
			'frame_id': self.frame_id,
			'content_document': self.content_document.__json__() if self.content_document else None,
			'shadow_root_type': self.shadow_root_type,
			'ax_node': asdict(self.ax_node) if self.ax_node else None,
			'snapshot_node': asdict(self.snapshot_node) if self.snapshot_node else None,
			# these two in the end, so it's easier to read json
			'shadow_roots': [r.__json__() for r in self.shadow_roots] if self.shadow_roots else [],
			'children_nodes': [c.__json__() for c in self.children_nodes] if self.children_nodes else [],
		}

	def get_all_children_text(self, max_depth: int = -1) -> str:
		text_parts = []

		def collect_text(node: EnhancedDOMTreeNode, current_depth: int) -> None:
			if max_depth != -1 and current_depth > max_depth:
				return

			# Skip this branch if we hit a highlighted element (except for the current node)
			# TODO: think whether if makese sense to add text until the next clickable element or everything from children
			# if node.node_type == NodeType.ELEMENT_NODE
			# if isinstance(node, DOMElementNode) and node != self and node.highlight_index is not None:
			# 	return

			if node.node_type == NodeType.TEXT_NODE:
				text_parts.append(node.node_value)
			elif node.node_type == NodeType.ELEMENT_NODE:
				for child in node.children:
					collect_text(child, current_depth + 1)

		collect_text(self, 0)
		return '\n'.join(text_parts).strip()

	def __repr__(self) -> str:
		"""
		@DEV ! don't display this to the LLM, it's SUPER long
		"""
		attributes = ', '.join([f'{k}={v}' for k, v in self.attributes.items()])
		is_scrollable = getattr(self, 'is_scrollable', False)
		num_children = len(self.children_nodes or [])
		return (
			f'<{self.tag_name} {attributes} is_scrollable={is_scrollable} '
			f'num_children={num_children} >{self.node_value}</{self.tag_name}>'
		)

	def llm_representation(self, max_text_length: int = 100) -> str:
		"""
		Token friendly representation of the node, used in the LLM
		"""

		return f'<{self.tag_name}>{cap_text_length(self.get_all_children_text(), max_text_length) or ""}'

	def get_meaningful_text_for_llm(self) -> str:
		"""
		Get the meaningful text content that the LLM actually sees for this element.
		This matches exactly what goes into the DOMTreeSerializer output.
		"""
		meaningful_text = ''
		if hasattr(self, 'attributes') and self.attributes:
			# Priority order: value, aria-label, title, placeholder, alt, text content
			for attr in ['value', 'aria-label', 'title', 'placeholder', 'alt']:
				if attr in self.attributes and self.attributes[attr]:
					meaningful_text = self.attributes[attr]
					break

		# Fallback to text content if no meaningful attributes
		if not meaningful_text:
			meaningful_text = self.get_all_children_text()

		return meaningful_text.strip()

	@property
	def is_actually_scrollable(self) -> bool:
		"""
		Enhanced scroll detection that combines CDP detection with CSS analysis.

		This detects scrollable elements that Chrome's CDP might miss, which is common
		in iframes and dynamically sized containers.
		"""
		# First check if CDP already detected it as scrollable
		if self.is_scrollable:
			return True

		# Enhanced detection for elements CDP missed
		if not self.snapshot_node:
			return False

		# Check scroll vs client rects - this is the most reliable indicator
		scroll_rects = self.snapshot_node.scrollRects
		client_rects = self.snapshot_node.clientRects

		if scroll_rects and client_rects:
			# Content is larger than visible area = scrollable
			has_vertical_scroll = scroll_rects.height > client_rects.height + 1  # +1 for rounding
			has_horizontal_scroll = scroll_rects.width > client_rects.width + 1

			if has_vertical_scroll or has_horizontal_scroll:
				# Also check CSS to make sure scrolling is allowed
				if self.snapshot_node.computed_styles:
					styles = self.snapshot_node.computed_styles

					overflow = styles.get('overflow', 'visible').lower()
					overflow_x = styles.get('overflow-x', overflow).lower()
					overflow_y = styles.get('overflow-y', overflow).lower()

					# Only allow scrolling if overflow is explicitly set to auto, scroll, or overlay
					# Do NOT consider 'visible' overflow as scrollable - this was causing the issue
					allows_scroll = (
						overflow in ['auto', 'scroll', 'overlay']
						or overflow_x in ['auto', 'scroll', 'overlay']
						or overflow_y in ['auto', 'scroll', 'overlay']
					)

					return allows_scroll
				else:
					# No CSS info, but content overflows - be more conservative
					# Only consider it scrollable if it's a common scrollable container element
					scrollable_tags = {'div', 'main', 'section', 'article', 'aside', 'body', 'html'}
					return self.tag_name.lower() in scrollable_tags

		return False

	@property
	def should_show_scroll_info(self) -> bool:
		"""
		Simple check: show scroll info only if this element is scrollable
		and doesn't have a scrollable parent (to avoid nested scroll spam).

		Special case for iframes: Always show scroll info since Chrome might not
		always detect iframe scrollability correctly (scrollHeight: 0 issue).
		"""
		# Special case: Always show scroll info for iframe elements
		# Even if not detected as scrollable, they might have scrollable content
		if self.tag_name.lower() == 'iframe':
			return True

		# Must be scrollable first for non-iframe elements
		if not (self.is_scrollable or self.is_actually_scrollable):
			return False

		# Always show for iframe content documents (body/html)
		if self.tag_name.lower() in {'body', 'html'}:
			return True

		# Don't show if parent is already scrollable (avoid nested spam)
		if self.parent_node and (self.parent_node.is_scrollable or self.parent_node.is_actually_scrollable):
			return False

		return True

	def _find_html_in_content_document(self) -> 'EnhancedDOMTreeNode | None':
		"""Find HTML element in iframe content document."""
		if not self.content_document:
			return None

		# Check if content document itself is HTML
		if self.content_document.tag_name.lower() == 'html':
			return self.content_document

		# Look through children for HTML element
		if self.content_document.children_nodes:
			for child in self.content_document.children_nodes:
				if child.tag_name.lower() == 'html':
					return child

		return None

	@property
	def scroll_info(self) -> dict[str, Any] | None:
		"""Calculate scroll information for this element if it's scrollable."""
		if not self.is_actually_scrollable or not self.snapshot_node:
			return None

		# Get scroll and client rects from snapshot data
		scroll_rects = self.snapshot_node.scrollRects
		client_rects = self.snapshot_node.clientRects
		bounds = self.snapshot_node.bounds

		if not scroll_rects or not client_rects:
			return None

		# Calculate scroll position and percentages
		scroll_top = scroll_rects.y
		scroll_left = scroll_rects.x

		# Total scrollable height and width
		scrollable_height = scroll_rects.height
		scrollable_width = scroll_rects.width

		# Visible (client) dimensions
		visible_height = client_rects.height
		visible_width = client_rects.width

		# Calculate how much content is above/below/left/right of current view
		content_above = max(0, scroll_top)
		content_below = max(0, scrollable_height - visible_height - scroll_top)
		content_left = max(0, scroll_left)
		content_right = max(0, scrollable_width - visible_width - scroll_left)

		# Calculate scroll percentages
		vertical_scroll_percentage = 0
		horizontal_scroll_percentage = 0

		if scrollable_height > visible_height:
			max_scroll_top = scrollable_height - visible_height
			vertical_scroll_percentage = (scroll_top / max_scroll_top) * 100 if max_scroll_top > 0 else 0

		if scrollable_width > visible_width:
			max_scroll_left = scrollable_width - visible_width
			horizontal_scroll_percentage = (scroll_left / max_scroll_left) * 100 if max_scroll_left > 0 else 0

		# Calculate pages equivalent (using visible height as page unit)
		pages_above = content_above / visible_height if visible_height > 0 else 0
		pages_below = content_below / visible_height if visible_height > 0 else 0
		total_pages = scrollable_height / visible_height if visible_height > 0 else 1

		return {
			'scroll_top': scroll_top,
			'scroll_left': scroll_left,
			'scrollable_height': scrollable_height,
			'scrollable_width': scrollable_width,
			'visible_height': visible_height,
			'visible_width': visible_width,
			'content_above': content_above,
			'content_below': content_below,
			'content_left': content_left,
			'content_right': content_right,
			'vertical_scroll_percentage': round(vertical_scroll_percentage, 1),
			'horizontal_scroll_percentage': round(horizontal_scroll_percentage, 1),
			'pages_above': round(pages_above, 1),
			'pages_below': round(pages_below, 1),
			'total_pages': round(total_pages, 1),
			'can_scroll_up': content_above > 0,
			'can_scroll_down': content_below > 0,
			'can_scroll_left': content_left > 0,
			'can_scroll_right': content_right > 0,
		}

	def get_scroll_info_text(self) -> str:
		"""Get human-readable scroll information text for this element."""
		# Special case for iframes: check content document for scroll info
		if self.tag_name.lower() == 'iframe':
			# Try to get scroll info from the HTML document inside the iframe
			if self.content_document:
				# Look for HTML element in content document
				html_element = self._find_html_in_content_document()
				if html_element and html_element.scroll_info:
					info = html_element.scroll_info
					# Provide minimal but useful scroll info
					pages_below = info.get('pages_below', 0)
					pages_above = info.get('pages_above', 0)
					v_pct = int(info.get('vertical_scroll_percentage', 0))

					if pages_below > 0 or pages_above > 0:
						return f'scroll: {pages_above:.1f}â†‘ {pages_below:.1f}â†“ {v_pct}%'

			return 'scroll'

		scroll_info = self.scroll_info
		if not scroll_info:
			return ''

		parts = []

		# Vertical scroll info (concise format)
		if scroll_info['scrollable_height'] > scroll_info['visible_height']:
			parts.append(f'{scroll_info["pages_above"]:.1f} pages above, {scroll_info["pages_below"]:.1f} pages below')

		# Horizontal scroll info (concise format)
		if scroll_info['scrollable_width'] > scroll_info['visible_width']:
			parts.append(f'horizontal {scroll_info["horizontal_scroll_percentage"]:.0f}%')

		return ' '.join(parts)

	@property
	def element_hash(self) -> int:
		return hash(self)

	def __str__(self) -> str:
		return f'[<{self.tag_name}>#{self.frame_id[-4:] if self.frame_id else "?"}:{self.backend_node_id}]'

	def __hash__(self) -> int:
		"""
		Hash the element based on its parent branch path and attributes.

		TODO: migrate this to use only backendNodeId + current SessionId
		"""

		# Get parent branch path
		parent_branch_path = self._get_parent_branch_path()
		parent_branch_path_string = '/'.join(parent_branch_path)

		attributes_string = ''.join(
			f'{k}={v}' for k, v in sorted((k, v) for k, v in self.attributes.items() if k in STATIC_ATTRIBUTES)
		)

		# Combine both for final hash
		combined_string = f'{parent_branch_path_string}|{attributes_string}'
		element_hash = hashlib.sha256(combined_string.encode()).hexdigest()

		# Convert to int for __hash__ return type - use first 16 chars and convert from hex to int
		return int(element_hash[:16], 16)

	def parent_branch_hash(self) -> int:
		"""
		Hash the element based on its parent branch path and attributes.
		"""
		parent_branch_path = self._get_parent_branch_path()
		parent_branch_path_string = '/'.join(parent_branch_path)
		element_hash = hashlib.sha256(parent_branch_path_string.encode()).hexdigest()

		return int(element_hash[:16], 16)

	def _get_parent_branch_path(self) -> list[str]:
		"""Get the parent branch path as a list of tag names from root to current element."""
		parents: list['EnhancedDOMTreeNode'] = []
		current_element: 'EnhancedDOMTreeNode | None' = self

		while current_element is not None:
			if current_element.node_type == NodeType.ELEMENT_NODE:
				parents.append(current_element)
			current_element = current_element.parent_node

		parents.reverse()
		return [parent.tag_name for parent in parents]


DOMSelectorMap = dict[int, EnhancedDOMTreeNode]


@dataclass
class SerializedDOMState:
	_root: SimplifiedNode | None
	"""Not meant to be used directly, use `llm_representation` instead"""

	selector_map: DOMSelectorMap

	@observe_debug(ignore_input=True, ignore_output=True, name='llm_representation')
	def llm_representation(
		self,
		include_attributes: list[str] | None = None,
	) -> str:
		"""Kinda ugly, but leaving this as an internal method because include_attributes are a parameter on the agent, so we need to leave it as a 2 step process"""
		from browser_use.dom.serializer.serializer import DOMTreeSerializer

		if not self._root:
			return 'Empty DOM tree (you might have to wait for the page to load)'

		include_attributes = include_attributes or DEFAULT_INCLUDE_ATTRIBUTES

		return DOMTreeSerializer.serialize_tree(self._root, include_attributes)

	@observe_debug(ignore_input=True, ignore_output=True, name='eval_representation')
	def eval_representation(
		self,
		include_attributes: list[str] | None = None,
	) -> str:
		"""
		Evaluation-focused DOM representation without interactive indexes.

		This serializer is designed for evaluation/judge contexts where:
		- No interactive indexes are needed (we're not clicking)
		- Full HTML structure should be preserved for context
		- More attribute information is helpful
		- Text content is important for understanding page structure
		"""
		from browser_use.dom.serializer.eval_serializer import DOMEvalSerializer

		if not self._root:
			return 'Empty DOM tree (you might have to wait for the page to load)'

		include_attributes = include_attributes or DEFAULT_INCLUDE_ATTRIBUTES

		return DOMEvalSerializer.serialize_tree(self._root, include_attributes)


@dataclass
class DOMInteractedElement:
	"""
	DOMInteractedElement is a class that represents a DOM element that has been interacted with.
	It is used to store the DOM element that has been interacted with and to store the DOM element that has been interacted with.

	TODO: this is a bit of a hack, we should probably have a better way to do this
	"""

	node_id: int
	backend_node_id: int
	frame_id: str | None

	node_type: NodeType
	node_value: str
	node_name: str
	attributes: dict[str, str] | None

	bounds: DOMRect | None

	x_path: str

	element_hash: int

	def to_dict(self) -> dict[str, Any]:
		return {
			'node_id': self.node_id,
			'backend_node_id': self.backend_node_id,
			'frame_id': self.frame_id,
			'node_type': self.node_type.value,
			'node_value': self.node_value,
			'node_name': self.node_name,
			'attributes': self.attributes,
			'x_path': self.x_path,
			'element_hash': self.element_hash,
			'bounds': self.bounds.to_dict() if self.bounds else None,
		}

	@classmethod
	def load_from_enhanced_dom_tree(cls, enhanced_dom_tree: EnhancedDOMTreeNode) -> 'DOMInteractedElement':
		return cls(
			node_id=enhanced_dom_tree.node_id,
			backend_node_id=enhanced_dom_tree.backend_node_id,
			frame_id=enhanced_dom_tree.frame_id,
			node_type=enhanced_dom_tree.node_type,
			node_value=enhanced_dom_tree.node_value,
			node_name=enhanced_dom_tree.node_name,
			attributes=enhanced_dom_tree.attributes,
			bounds=enhanced_dom_tree.snapshot_node.bounds if enhanced_dom_tree.snapshot_node else None,
			x_path=enhanced_dom_tree.xpath,
			element_hash=hash(enhanced_dom_tree),
		)

```

---

## backend/browser-use/browser_use/exceptions.py

```py
class LLMException(Exception):
	def __init__(self, status_code, message):
		self.status_code = status_code
		self.message = message
		super().__init__(f'Error {status_code}: {message}')

```

---

## backend/browser-use/browser_use/filesystem/__init__.py

```py

```

---

## backend/browser-use/browser_use/filesystem/file_system.py

```py
import asyncio
import base64
import os
import re
import shutil
from abc import ABC, abstractmethod
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from typing import Any

from pydantic import BaseModel, Field
from reportlab.lib.pagesizes import letter
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.platypus import Paragraph, SimpleDocTemplate, Spacer

INVALID_FILENAME_ERROR_MESSAGE = 'Error: Invalid filename format. Must be alphanumeric with supported extension.'
DEFAULT_FILE_SYSTEM_PATH = 'browseruse_agent_data'


class FileSystemError(Exception):
	"""Custom exception for file system operations that should be shown to LLM"""

	pass


class BaseFile(BaseModel, ABC):
	"""Base class for all file types"""

	name: str
	content: str = ''

	# --- Subclass must define this ---
	@property
	@abstractmethod
	def extension(self) -> str:
		"""File extension (e.g. 'txt', 'md')"""
		pass

	def write_file_content(self, content: str) -> None:
		"""Update internal content (formatted)"""
		self.update_content(content)

	def append_file_content(self, content: str) -> None:
		"""Append content to internal content"""
		self.update_content(self.content + content)

	# --- These are shared and implemented here ---

	def update_content(self, content: str) -> None:
		self.content = content

	def sync_to_disk_sync(self, path: Path) -> None:
		file_path = path / self.full_name
		file_path.write_text(self.content)

	async def sync_to_disk(self, path: Path) -> None:
		file_path = path / self.full_name
		with ThreadPoolExecutor() as executor:
			await asyncio.get_event_loop().run_in_executor(executor, lambda: file_path.write_text(self.content))

	async def write(self, content: str, path: Path) -> None:
		self.write_file_content(content)
		await self.sync_to_disk(path)

	async def append(self, content: str, path: Path) -> None:
		self.append_file_content(content)
		await self.sync_to_disk(path)

	def read(self) -> str:
		return self.content

	@property
	def full_name(self) -> str:
		return f'{self.name}.{self.extension}'

	@property
	def get_size(self) -> int:
		return len(self.content)

	@property
	def get_line_count(self) -> int:
		return len(self.content.splitlines())


class MarkdownFile(BaseFile):
	"""Markdown file implementation"""

	@property
	def extension(self) -> str:
		return 'md'


class TxtFile(BaseFile):
	"""Plain text file implementation"""

	@property
	def extension(self) -> str:
		return 'txt'


class JsonFile(BaseFile):
	"""JSON file implementation"""

	@property
	def extension(self) -> str:
		return 'json'


class CsvFile(BaseFile):
	"""CSV file implementation"""

	@property
	def extension(self) -> str:
		return 'csv'


class JsonlFile(BaseFile):
	"""JSONL (JSON Lines) file implementation"""

	@property
	def extension(self) -> str:
		return 'jsonl'


class PdfFile(BaseFile):
	"""PDF file implementation"""

	@property
	def extension(self) -> str:
		return 'pdf'

	def sync_to_disk_sync(self, path: Path) -> None:
		file_path = path / self.full_name
		try:
			# Create PDF document
			doc = SimpleDocTemplate(str(file_path), pagesize=letter)
			styles = getSampleStyleSheet()
			story = []

			# Convert markdown content to simple text and add to PDF
			# For basic implementation, we'll treat content as plain text
			# This avoids the AGPL license issue while maintaining functionality
			content_lines = self.content.split('\n')

			for line in content_lines:
				if line.strip():
					# Handle basic markdown headers
					if line.startswith('# '):
						para = Paragraph(line[2:], styles['Title'])
					elif line.startswith('## '):
						para = Paragraph(line[3:], styles['Heading1'])
					elif line.startswith('### '):
						para = Paragraph(line[4:], styles['Heading2'])
					else:
						para = Paragraph(line, styles['Normal'])
					story.append(para)
				else:
					story.append(Spacer(1, 6))

			doc.build(story)
		except Exception as e:
			raise FileSystemError(f"Error: Could not write to file '{self.full_name}'. {str(e)}")

	async def sync_to_disk(self, path: Path) -> None:
		with ThreadPoolExecutor() as executor:
			await asyncio.get_event_loop().run_in_executor(executor, lambda: self.sync_to_disk_sync(path))


class DocxFile(BaseFile):
	"""DOCX file implementation"""

	@property
	def extension(self) -> str:
		return 'docx'

	def sync_to_disk_sync(self, path: Path) -> None:
		file_path = path / self.full_name
		try:
			from docx import Document

			doc = Document()

			# Convert content to DOCX paragraphs
			content_lines = self.content.split('\n')

			for line in content_lines:
				if line.strip():
					# Handle basic markdown headers
					if line.startswith('# '):
						doc.add_heading(line[2:], level=1)
					elif line.startswith('## '):
						doc.add_heading(line[3:], level=2)
					elif line.startswith('### '):
						doc.add_heading(line[4:], level=3)
					else:
						doc.add_paragraph(line)
				else:
					doc.add_paragraph()  # Empty paragraph for spacing

			doc.save(str(file_path))
		except Exception as e:
			raise FileSystemError(f"Error: Could not write to file '{self.full_name}'. {str(e)}")

	async def sync_to_disk(self, path: Path) -> None:
		with ThreadPoolExecutor() as executor:
			await asyncio.get_event_loop().run_in_executor(executor, lambda: self.sync_to_disk_sync(path))


class FileSystemState(BaseModel):
	"""Serializable state of the file system"""

	files: dict[str, dict[str, Any]] = Field(default_factory=dict)  # full filename -> file data
	base_dir: str
	extracted_content_count: int = 0


class FileSystem:
	"""Enhanced file system with in-memory storage and multiple file type support"""

	def __init__(self, base_dir: str | Path, create_default_files: bool = True):
		# Handle the Path conversion before calling super().__init__
		self.base_dir = Path(base_dir) if isinstance(base_dir, str) else base_dir
		self.base_dir.mkdir(parents=True, exist_ok=True)

		# Create and use a dedicated subfolder for all operations
		self.data_dir = self.base_dir / DEFAULT_FILE_SYSTEM_PATH
		if self.data_dir.exists():
			# clean the data directory
			shutil.rmtree(self.data_dir)
		self.data_dir.mkdir(exist_ok=True)

		self._file_types: dict[str, type[BaseFile]] = {
			'md': MarkdownFile,
			'txt': TxtFile,
			'json': JsonFile,
			'jsonl': JsonlFile,
			'csv': CsvFile,
			'pdf': PdfFile,
			'docx': DocxFile,
		}

		self.files = {}
		if create_default_files:
			self.default_files = ['todo.md']
			self._create_default_files()

		self.extracted_content_count = 0

	def get_allowed_extensions(self) -> list[str]:
		"""Get allowed extensions"""
		return list(self._file_types.keys())

	def _get_file_type_class(self, extension: str) -> type[BaseFile] | None:
		"""Get the appropriate file class for an extension."""
		return self._file_types.get(extension.lower(), None)

	def _create_default_files(self) -> None:
		"""Create default results and todo files"""
		for full_filename in self.default_files:
			name_without_ext, extension = self._parse_filename(full_filename)
			file_class = self._get_file_type_class(extension)
			if not file_class:
				raise ValueError(f"Error: Invalid file extension '{extension}' for file '{full_filename}'.")

			file_obj = file_class(name=name_without_ext)
			self.files[full_filename] = file_obj  # Use full filename as key
			file_obj.sync_to_disk_sync(self.data_dir)

	def _is_valid_filename(self, file_name: str) -> bool:
		"""Check if filename matches the required pattern: name.extension"""
		# Build extensions pattern from _file_types
		extensions = '|'.join(self._file_types.keys())
		pattern = rf'^[a-zA-Z0-9_\-\u4e00-\u9fff]+\.({extensions})$'
		file_name_base = os.path.basename(file_name)
		return bool(re.match(pattern, file_name_base))

	def _parse_filename(self, filename: str) -> tuple[str, str]:
		"""Parse filename into name and extension. Always check _is_valid_filename first."""
		name, extension = filename.rsplit('.', 1)
		return name, extension.lower()

	def get_dir(self) -> Path:
		"""Get the file system directory"""
		return self.data_dir

	def get_file(self, full_filename: str) -> BaseFile | None:
		"""Get a file object by full filename"""
		if not self._is_valid_filename(full_filename):
			return None

		# Use full filename as key
		return self.files.get(full_filename)

	def list_files(self) -> list[str]:
		"""List all files in the system"""
		return [file_obj.full_name for file_obj in self.files.values()]

	def display_file(self, full_filename: str) -> str | None:
		"""Display file content using file-specific display method"""
		if not self._is_valid_filename(full_filename):
			return None

		file_obj = self.get_file(full_filename)
		if not file_obj:
			return None

		return file_obj.read()

	async def read_file_structured(self, full_filename: str, external_file: bool = False) -> dict[str, Any]:
		"""Read file and return structured data including images if applicable.

		Returns:
			dict with keys:
				- 'message': str - The message to display
				- 'images': list[dict] | None - Image data if file is an image: [{"name": str, "data": base64_str}]
		"""
		result: dict[str, Any] = {'message': '', 'images': None}

		if external_file:
			try:
				try:
					_, extension = self._parse_filename(full_filename)
				except Exception:
					result['message'] = (
						f'Error: Invalid filename format {full_filename}. Must be alphanumeric with a supported extension.'
					)
					return result

				if extension in ['md', 'txt', 'json', 'jsonl', 'csv']:
					import anyio

					async with await anyio.open_file(full_filename, 'r') as f:
						content = await f.read()
						result['message'] = f'Read from file {full_filename}.\n<content>\n{content}\n</content>'
						return result

				elif extension == 'docx':
					from docx import Document

					doc = Document(full_filename)
					content = '\n'.join([para.text for para in doc.paragraphs])
					result['message'] = f'Read from file {full_filename}.\n<content>\n{content}\n</content>'
					return result

				elif extension == 'pdf':
					import pypdf

					reader = pypdf.PdfReader(full_filename)
					num_pages = len(reader.pages)
					MAX_PDF_PAGES = 20
					extra_pages = num_pages - MAX_PDF_PAGES
					extracted_text = ''
					for page in reader.pages[:MAX_PDF_PAGES]:
						extracted_text += page.extract_text()
					extra_pages_text = f'{extra_pages} more pages...' if extra_pages > 0 else ''
					result['message'] = (
						f'Read from file {full_filename}.\n<content>\n{extracted_text}\n{extra_pages_text}</content>'
					)
					return result

				elif extension in ['jpg', 'jpeg', 'png']:
					import anyio

					# Read image file and convert to base64
					async with await anyio.open_file(full_filename, 'rb') as f:
						img_data = await f.read()

					base64_str = base64.b64encode(img_data).decode('utf-8')

					result['message'] = f'Read image file {full_filename}.'
					result['images'] = [{'name': os.path.basename(full_filename), 'data': base64_str}]
					return result

				else:
					result['message'] = f'Error: Cannot read file {full_filename} as {extension} extension is not supported.'
					return result

			except FileNotFoundError:
				result['message'] = f"Error: File '{full_filename}' not found."
				return result
			except PermissionError:
				result['message'] = f"Error: Permission denied to read file '{full_filename}'."
				return result
			except Exception as e:
				result['message'] = f"Error: Could not read file '{full_filename}'. {str(e)}"
				return result

		# For internal files, only non-image types are supported
		if not self._is_valid_filename(full_filename):
			result['message'] = INVALID_FILENAME_ERROR_MESSAGE
			return result

		file_obj = self.get_file(full_filename)
		if not file_obj:
			result['message'] = f"File '{full_filename}' not found."
			return result

		try:
			content = file_obj.read()
			result['message'] = f'Read from file {full_filename}.\n<content>\n{content}\n</content>'
			return result
		except FileSystemError as e:
			result['message'] = str(e)
			return result
		except Exception as e:
			result['message'] = f"Error: Could not read file '{full_filename}'. {str(e)}"
			return result

	async def read_file(self, full_filename: str, external_file: bool = False) -> str:
		"""Read file content using file-specific read method and return appropriate message to LLM.

		Note: For image files, use read_file_structured() to get image data.
		"""
		result = await self.read_file_structured(full_filename, external_file)
		return result['message']

	async def write_file(self, full_filename: str, content: str) -> str:
		"""Write content to file using file-specific write method"""
		if not self._is_valid_filename(full_filename):
			return INVALID_FILENAME_ERROR_MESSAGE

		try:
			name_without_ext, extension = self._parse_filename(full_filename)
			file_class = self._get_file_type_class(extension)
			if not file_class:
				raise ValueError(f"Error: Invalid file extension '{extension}' for file '{full_filename}'.")

			# Create or get existing file using full filename as key
			if full_filename in self.files:
				file_obj = self.files[full_filename]
			else:
				file_obj = file_class(name=name_without_ext)
				self.files[full_filename] = file_obj  # Use full filename as key

			# Use file-specific write method
			await file_obj.write(content, self.data_dir)
			return f'Data written to file {full_filename} successfully.'
		except FileSystemError as e:
			return str(e)
		except Exception as e:
			return f"Error: Could not write to file '{full_filename}'. {str(e)}"

	async def append_file(self, full_filename: str, content: str) -> str:
		"""Append content to file using file-specific append method"""
		if not self._is_valid_filename(full_filename):
			return INVALID_FILENAME_ERROR_MESSAGE

		file_obj = self.get_file(full_filename)
		if not file_obj:
			return f"File '{full_filename}' not found."

		try:
			await file_obj.append(content, self.data_dir)
			return f'Data appended to file {full_filename} successfully.'
		except FileSystemError as e:
			return str(e)
		except Exception as e:
			return f"Error: Could not append to file '{full_filename}'. {str(e)}"

	async def replace_file_str(self, full_filename: str, old_str: str, new_str: str) -> str:
		"""Replace old_str with new_str in file_name"""
		if not self._is_valid_filename(full_filename):
			return INVALID_FILENAME_ERROR_MESSAGE

		if not old_str:
			return 'Error: Cannot replace empty string. Please provide a non-empty string to replace.'

		file_obj = self.get_file(full_filename)
		if not file_obj:
			return f"File '{full_filename}' not found."

		try:
			content = file_obj.read()
			content = content.replace(old_str, new_str)
			await file_obj.write(content, self.data_dir)
			return f'Successfully replaced all occurrences of "{old_str}" with "{new_str}" in file {full_filename}'
		except FileSystemError as e:
			return str(e)
		except Exception as e:
			return f"Error: Could not replace string in file '{full_filename}'. {str(e)}"

	async def save_extracted_content(self, content: str) -> str:
		"""Save extracted content to a numbered file"""
		initial_filename = f'extracted_content_{self.extracted_content_count}'
		extracted_filename = f'{initial_filename}.md'
		file_obj = MarkdownFile(name=initial_filename)
		await file_obj.write(content, self.data_dir)
		self.files[extracted_filename] = file_obj
		self.extracted_content_count += 1
		return extracted_filename

	def describe(self) -> str:
		"""List all files with their content information using file-specific display methods"""
		DISPLAY_CHARS = 400
		description = ''

		for file_obj in self.files.values():
			# Skip todo.md from description
			if file_obj.full_name == 'todo.md':
				continue

			content = file_obj.read()

			# Handle empty files
			if not content:
				description += f'<file>\n{file_obj.full_name} - [empty file]\n</file>\n'
				continue

			lines = content.splitlines()
			line_count = len(lines)

			# For small files, display the entire content
			whole_file_description = (
				f'<file>\n{file_obj.full_name} - {line_count} lines\n<content>\n{content}\n</content>\n</file>\n'
			)
			if len(content) < int(1.5 * DISPLAY_CHARS):
				description += whole_file_description
				continue

			# For larger files, display start and end previews
			half_display_chars = DISPLAY_CHARS // 2

			# Get start preview
			start_preview = ''
			start_line_count = 0
			chars_count = 0
			for line in lines:
				if chars_count + len(line) + 1 > half_display_chars:
					break
				start_preview += line + '\n'
				chars_count += len(line) + 1
				start_line_count += 1

			# Get end preview
			end_preview = ''
			end_line_count = 0
			chars_count = 0
			for line in reversed(lines):
				if chars_count + len(line) + 1 > half_display_chars:
					break
				end_preview = line + '\n' + end_preview
				chars_count += len(line) + 1
				end_line_count += 1

			# Calculate lines in between
			middle_line_count = line_count - start_line_count - end_line_count
			if middle_line_count <= 0:
				description += whole_file_description
				continue

			start_preview = start_preview.strip('\n').rstrip()
			end_preview = end_preview.strip('\n').rstrip()

			# Format output
			if not (start_preview or end_preview):
				description += f'<file>\n{file_obj.full_name} - {line_count} lines\n<content>\n{middle_line_count} lines...\n</content>\n</file>\n'
			else:
				description += f'<file>\n{file_obj.full_name} - {line_count} lines\n<content>\n{start_preview}\n'
				description += f'... {middle_line_count} more lines ...\n'
				description += f'{end_preview}\n'
				description += '</content>\n</file>\n'

		return description.strip('\n')

	def get_todo_contents(self) -> str:
		"""Get todo file contents"""
		todo_file = self.get_file('todo.md')
		return todo_file.read() if todo_file else ''

	def get_state(self) -> FileSystemState:
		"""Get serializable state of the file system"""
		files_data = {}
		for full_filename, file_obj in self.files.items():
			files_data[full_filename] = {'type': file_obj.__class__.__name__, 'data': file_obj.model_dump()}

		return FileSystemState(
			files=files_data, base_dir=str(self.base_dir), extracted_content_count=self.extracted_content_count
		)

	def nuke(self) -> None:
		"""Delete the file system directory"""
		shutil.rmtree(self.data_dir)

	@classmethod
	def from_state(cls, state: FileSystemState) -> 'FileSystem':
		"""Restore file system from serializable state at the exact same location"""
		# Create file system without default files
		fs = cls(base_dir=Path(state.base_dir), create_default_files=False)
		fs.extracted_content_count = state.extracted_content_count

		# Restore all files
		for full_filename, file_data in state.files.items():
			file_type = file_data['type']
			file_info = file_data['data']

			# Create the appropriate file object based on type
			if file_type == 'MarkdownFile':
				file_obj = MarkdownFile(**file_info)
			elif file_type == 'TxtFile':
				file_obj = TxtFile(**file_info)
			elif file_type == 'JsonFile':
				file_obj = JsonFile(**file_info)
			elif file_type == 'JsonlFile':
				file_obj = JsonlFile(**file_info)
			elif file_type == 'CsvFile':
				file_obj = CsvFile(**file_info)
			elif file_type == 'PdfFile':
				file_obj = PdfFile(**file_info)
			elif file_type == 'DocxFile':
				file_obj = DocxFile(**file_info)
			else:
				# Skip unknown file types
				continue

			# Add to files dict and sync to disk
			fs.files[full_filename] = file_obj
			file_obj.sync_to_disk_sync(fs.data_dir)

		return fs

```

---

## backend/browser-use/browser_use/init_cmd.py

```py
"""
Standalone init command for browser-use template generation.

This module provides a minimal command-line interface for generating
browser-use templates without requiring heavy TUI dependencies.
"""

import json
import shutil
import sys
from pathlib import Path
from typing import Any
from urllib import request
from urllib.error import URLError

import click
from InquirerPy import inquirer
from InquirerPy.base.control import Choice
from InquirerPy.utils import InquirerPyStyle
from rich.console import Console
from rich.panel import Panel
from rich.text import Text

# Rich console for styled output
console = Console()

# GitHub template repository URL (for runtime fetching)
TEMPLATE_REPO_URL = 'https://raw.githubusercontent.com/browser-use/template-library/main'

# Export for backward compatibility with cli.py
# Templates are fetched at runtime via _get_template_list()
INIT_TEMPLATES: dict[str, Any] = {}


def _fetch_template_list() -> dict[str, Any] | None:
	"""
	Fetch template list from GitHub templates.json.

	Returns template dict if successful, None if failed.
	"""
	try:
		url = f'{TEMPLATE_REPO_URL}/templates.json'
		with request.urlopen(url, timeout=5) as response:
			data = response.read().decode('utf-8')
			return json.loads(data)
	except (URLError, TimeoutError, json.JSONDecodeError, Exception):
		return None


def _get_template_list() -> dict[str, Any]:
	"""
	Get template list from GitHub.

	Raises FileNotFoundError if GitHub fetch fails.
	"""
	templates = _fetch_template_list()
	if templates is not None:
		return templates
	raise FileNotFoundError('Could not fetch templates from GitHub. Check your internet connection.')


def _fetch_from_github(file_path: str) -> str | None:
	"""
	Fetch template file from GitHub.

	Returns file content if successful, None if failed.
	"""
	try:
		url = f'{TEMPLATE_REPO_URL}/{file_path}'
		with request.urlopen(url, timeout=5) as response:
			return response.read().decode('utf-8')
	except (URLError, TimeoutError, Exception):
		return None


def _fetch_binary_from_github(file_path: str) -> bytes | None:
	"""
	Fetch binary file from GitHub.

	Returns file content if successful, None if failed.
	"""
	try:
		url = f'{TEMPLATE_REPO_URL}/{file_path}'
		with request.urlopen(url, timeout=5) as response:
			return response.read()
	except (URLError, TimeoutError, Exception):
		return None


def _get_template_content(file_path: str) -> str:
	"""
	Get template file content from GitHub.

	Raises exception if fetch fails.
	"""
	content = _fetch_from_github(file_path)

	if content is not None:
		return content

	raise FileNotFoundError(f'Could not fetch template from GitHub: {file_path}')


# InquirerPy style for template selection (browser-use orange theme)
inquirer_style = InquirerPyStyle(
	{
		'pointer': '#fe750e bold',
		'highlighted': '#fe750e bold',
		'question': 'bold',
		'answer': '#fe750e bold',
		'questionmark': '#fe750e bold',
	}
)


def _get_terminal_width() -> int:
	"""Get current terminal width in columns."""
	return shutil.get_terminal_size().columns


def _format_choice(name: str, metadata: dict[str, Any], width: int, is_default: bool = False) -> str:
	"""
	Format a template choice with responsive display based on terminal width.

	Styling:
	- Featured templates get [FEATURED] prefix
	- Author name included when width allows (except for default templates)
	- Everything turns orange when highlighted (InquirerPy's built-in behavior)

	Args:
		name: Template name
		metadata: Template metadata (description, featured, author)
		width: Terminal width in columns
		is_default: Whether this is a default template (default, advanced, tools)

	Returns:
		Formatted choice string
	"""
	is_featured = metadata.get('featured', False)
	description = metadata.get('description', '')
	author_name = metadata.get('author', {}).get('name', '') if isinstance(metadata.get('author'), dict) else ''

	# Build the choice string based on terminal width
	if width > 100:
		# Wide: show everything including author (except for default templates)
		if is_featured:
			if author_name:
				return f'[FEATURED] {name} by {author_name} - {description}'
			else:
				return f'[FEATURED] {name} - {description}'
		else:
			# Non-featured templates
			if author_name and not is_default:
				return f'{name} by {author_name} - {description}'
			else:
				return f'{name} - {description}'

	elif width > 60:
		# Medium: show name and description, no author
		if is_featured:
			return f'[FEATURED] {name} - {description}'
		else:
			return f'{name} - {description}'

	else:
		# Narrow: show name only
		return name


def _write_init_file(output_path: Path, content: str, force: bool = False) -> bool:
	"""Write content to a file, with safety checks."""
	# Check if file already exists
	if output_path.exists() and not force:
		console.print(f'[yellow]âš [/yellow]  File already exists: [cyan]{output_path}[/cyan]')
		if not click.confirm('Overwrite?', default=False):
			console.print('[red]âœ—[/red] Cancelled')
			return False

	# Ensure parent directory exists
	output_path.parent.mkdir(parents=True, exist_ok=True)

	# Write file
	try:
		output_path.write_text(content, encoding='utf-8')
		return True
	except Exception as e:
		console.print(f'[red]âœ—[/red] Error writing file: {e}')
		return False


@click.command('browser-use-init')
@click.option(
	'--template',
	'-t',
	type=str,
	help='Template to use',
)
@click.option(
	'--output',
	'-o',
	type=click.Path(),
	help='Output file path (default: browser_use_<template>.py)',
)
@click.option(
	'--force',
	'-f',
	is_flag=True,
	help='Overwrite existing files without asking',
)
@click.option(
	'--list',
	'-l',
	'list_templates',
	is_flag=True,
	help='List available templates',
)
def main(
	template: str | None,
	output: str | None,
	force: bool,
	list_templates: bool,
):
	"""
	Generate a browser-use template file to get started quickly.

	Examples:

	\b
	# Interactive mode - prompts for template selection
	uvx browser-use init
	uvx browser-use init --template

	\b
	# Generate default template
	uvx browser-use init --template default

	\b
	# Generate advanced template with custom filename
	uvx browser-use init --template advanced --output my_script.py

	\b
	# List available templates
	uvx browser-use init --list
	"""

	# Fetch template list at runtime
	try:
		INIT_TEMPLATES = _get_template_list()
	except FileNotFoundError as e:
		console.print(f'[red]âœ—[/red] {e}')
		sys.exit(1)

	# Handle --list flag
	if list_templates:
		console.print('\n[bold]Available templates:[/bold]\n')
		for name, info in INIT_TEMPLATES.items():
			console.print(f'  [#fe750e]{name:12}[/#fe750e] - {info["description"]}')
		console.print()
		return

	# Interactive template selection if not provided
	if not template:
		# Get terminal width for responsive formatting
		width = _get_terminal_width()

		# Separate default and featured templates
		default_template_names = ['default', 'advanced', 'tools']
		featured_templates = [(name, info) for name, info in INIT_TEMPLATES.items() if info.get('featured', False)]
		other_templates = [
			(name, info)
			for name, info in INIT_TEMPLATES.items()
			if name not in default_template_names and not info.get('featured', False)
		]

		# Sort by last_modified_date (most recent first)
		def get_last_modified(item):
			name, info = item
			date_str = (
				info.get('author', {}).get('last_modified_date', '1970-01-01')
				if isinstance(info.get('author'), dict)
				else '1970-01-01'
			)
			return date_str

		# Sort default templates by last modified
		default_templates = [(name, INIT_TEMPLATES[name]) for name in default_template_names if name in INIT_TEMPLATES]
		default_templates.sort(key=get_last_modified, reverse=True)

		# Sort featured and other templates by last modified
		featured_templates.sort(key=get_last_modified, reverse=True)
		other_templates.sort(key=get_last_modified, reverse=True)

		# Build choices in order: defaults first, then featured, then others
		choices = []

		# Add default templates
		for i, (name, info) in enumerate(default_templates):
			formatted = _format_choice(name, info, width, is_default=True)
			choices.append(Choice(name=formatted, value=name))

		# Add featured templates
		for i, (name, info) in enumerate(featured_templates):
			formatted = _format_choice(name, info, width, is_default=False)
			choices.append(Choice(name=formatted, value=name))

		# Add other templates (if any)
		for name, info in other_templates:
			formatted = _format_choice(name, info, width, is_default=False)
			choices.append(Choice(name=formatted, value=name))

		# Use fuzzy prompt for search functionality
		# Use getattr to avoid static analysis complaining about non-exported names
		_fuzzy = getattr(inquirer, 'fuzzy')
		template = _fuzzy(
			message='Select a template (type to search):',
			choices=choices,
			style=inquirer_style,
			max_height='70%',
		).execute()

		# Handle user cancellation (Ctrl+C)
		if template is None:
			console.print('\n[red]âœ—[/red] Cancelled')
			sys.exit(1)

	# Template is guaranteed to be set at this point (either from option or prompt)
	assert template is not None

	# Create template directory
	template_dir = Path.cwd() / template
	if template_dir.exists() and not force:
		console.print(f'[yellow]âš [/yellow]  Directory already exists: [cyan]{template_dir}[/cyan]')
		if not click.confirm('Continue and overwrite files?', default=False):
			console.print('[red]âœ—[/red] Cancelled')
			sys.exit(1)

	# Create directory
	template_dir.mkdir(parents=True, exist_ok=True)

	# Determine output path
	if output:
		output_path = template_dir / Path(output)
	else:
		output_path = template_dir / 'main.py'

	# Read template file from GitHub
	try:
		template_file = INIT_TEMPLATES[template]['file']
		content = _get_template_content(template_file)
	except Exception as e:
		console.print(f'[red]âœ—[/red] Error reading template: {e}')
		sys.exit(1)

	# Write file
	if _write_init_file(output_path, content, force):
		console.print(f'\n[green]âœ“[/green] Created [cyan]{output_path}[/cyan]')

		# Generate additional files if template has a manifest
		if 'files' in INIT_TEMPLATES[template]:
			import stat

			for file_spec in INIT_TEMPLATES[template]['files']:
				source_path = file_spec['source']
				dest_name = file_spec['dest']
				dest_path = output_path.parent / dest_name
				is_binary = file_spec.get('binary', False)
				is_executable = file_spec.get('executable', False)

				# Skip if we already wrote this file (main.py)
				if dest_path == output_path:
					continue

				# Fetch and write file
				try:
					if is_binary:
						file_content = _fetch_binary_from_github(source_path)
						if file_content:
							if not dest_path.exists() or force:
								dest_path.write_bytes(file_content)
								console.print(f'[green]âœ“[/green] Created [cyan]{dest_name}[/cyan]')
						else:
							console.print(f'[yellow]âš [/yellow]  Could not fetch [cyan]{dest_name}[/cyan] from GitHub')
					else:
						file_content = _get_template_content(source_path)
						if _write_init_file(dest_path, file_content, force):
							console.print(f'[green]âœ“[/green] Created [cyan]{dest_name}[/cyan]')
							# Make executable if needed
							if is_executable and sys.platform != 'win32':
								dest_path.chmod(dest_path.stat().st_mode | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH)
				except Exception as e:
					console.print(f'[yellow]âš [/yellow]  Error generating [cyan]{dest_name}[/cyan]: {e}')

		# Create a nice panel for next steps
		next_steps = Text()

		# Display next steps from manifest if available
		if 'next_steps' in INIT_TEMPLATES[template]:
			steps = INIT_TEMPLATES[template]['next_steps']
			for i, step in enumerate(steps, 1):
				# Handle footer separately (no numbering)
				if 'footer' in step:
					next_steps.append(f'{step["footer"]}\n', style='dim italic')
					continue

				# Step title
				next_steps.append(f'\n{i}. {step["title"]}:\n', style='bold')

				# Step commands
				for cmd in step.get('commands', []):
					# Replace placeholders
					cmd = cmd.replace('{template}', template)
					cmd = cmd.replace('{output}', output_path.name)
					next_steps.append(f'   {cmd}\n', style='dim')

				# Optional note
				if 'note' in step:
					next_steps.append(f'   {step["note"]}\n', style='dim italic')

				next_steps.append('\n')
		else:
			# Default workflow for templates without custom next_steps
			next_steps.append('\n1. Navigate to project directory:\n', style='bold')
			next_steps.append(f'   cd {template}\n\n', style='dim')
			next_steps.append('2. Initialize uv project:\n', style='bold')
			next_steps.append('   uv init\n\n', style='dim')
			next_steps.append('3. Install browser-use:\n', style='bold')
			next_steps.append('   uv add browser-use\n\n', style='dim')
			next_steps.append('4. Set up your API key in .env file or environment:\n', style='bold')
			next_steps.append('   BROWSER_USE_API_KEY=your-key\n', style='dim')
			next_steps.append(
				'   (Get your key at https://cloud.browser-use.com/dashboard/settings?tab=api-keys&new)\n\n',
				style='dim italic',
			)
			next_steps.append('5. Run your script:\n', style='bold')
			next_steps.append(f'   uv run {output_path.name}\n', style='dim')

		console.print(
			Panel(
				next_steps,
				title='[bold]Next steps[/bold]',
				border_style='#fe750e',
				padding=(1, 2),
			)
		)


if __name__ == '__main__':
	main()

```

---

## backend/browser-use/browser_use/integrations/gmail/__init__.py

```py
"""
Gmail Integration for Browser Use
Provides Gmail API integration for email reading and verification code extraction.
This integration enables agents to read email content and extract verification codes themselves.
Usage:
    from browser_use.integrations.gmail import GmailService, register_gmail_actions
    # Option 1: Register Gmail actions with file-based authentication
    tools = Tools()
    register_gmail_actions(tools)
    # Option 2: Register Gmail actions with direct access token (recommended for production)
    tools = Tools()
    register_gmail_actions(tools, access_token="your_access_token_here")
    # Option 3: Use the service directly
    gmail = GmailService(access_token="your_access_token_here")
    await gmail.authenticate()
    emails = await gmail.get_recent_emails()
"""

# @file purpose: Gmail integration for 2FA email authentication and email reading

from .actions import register_gmail_actions
from .service import GmailService

__all__ = ['GmailService', 'register_gmail_actions']

```
