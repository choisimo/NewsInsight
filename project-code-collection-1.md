# Project Code Snapshot

Generated at 2025-12-22T11:56:45.034Z

---

## aws/cdk/bin/newsinsight.ts

```ts
#!/usr/bin/env node
import 'source-map-support/register';
import * as cdk from 'aws-cdk-lib';
import { NewsInsightVpcStack } from '../lib/vpc-stack';
import { NewsInsightDatabaseStack } from '../lib/database-stack';
import { NewsInsightEcrStack } from '../lib/ecr-stack';
import { NewsInsightAlbStack } from '../lib/alb-stack';
import { NewsInsightEc2Stack } from '../lib/ec2-stack';

const app = new cdk.App();

// Environment configuration
const env = {
  account: process.env.CDK_DEFAULT_ACCOUNT || process.env.AWS_ACCOUNT_ID,
  region: process.env.CDK_DEFAULT_REGION || process.env.AWS_REGION || 'ap-northeast-2',
};

const envName = app.node.tryGetContext('env') || 'dev';
const projectName = 'newsinsight';

// Tags to apply to all resources
const tags = {
  Project: 'NewsInsight',
  Environment: envName,
  ManagedBy: 'CDK',
};

// 1. VPC Stack - Network infrastructure
const vpcStack = new NewsInsightVpcStack(app, `${projectName}-vpc-${envName}`, {
  env,
  envName,
  projectName,
  tags,
});

// 2. ECR Stack - Container registry
const ecrStack = new NewsInsightEcrStack(app, `${projectName}-ecr-${envName}`, {
  env,
  envName,
  projectName,
  tags,
});

// 3. Database Stack - RDS, DocumentDB, ElastiCache
const databaseStack = new NewsInsightDatabaseStack(app, `${projectName}-db-${envName}`, {
  env,
  envName,
  projectName,
  vpc: vpcStack.vpc,
  tags,
});
databaseStack.addDependency(vpcStack);

// 4. ALB Stack - Application Load Balancer
const albStack = new NewsInsightAlbStack(app, `${projectName}-alb-${envName}`, {
  env,
  envName,
  projectName,
  vpc: vpcStack.vpc,
  tags,
});
albStack.addDependency(vpcStack);

// 5. EC2 Stack - Single EC2 instance with Docker Compose
const ec2Stack = new NewsInsightEc2Stack(app, `${projectName}-ec2-${envName}`, {
  env,
  envName,
  projectName,
  vpc: vpcStack.vpc,
  albSecurityGroupId: albStack.albSecurityGroup.securityGroupId,
  databaseSecrets: databaseStack.secrets,
  postgresEndpoint: databaseStack.postgresEndpoint,
  mongoEndpoint: databaseStack.mongoEndpoint,
  redisEndpoint: databaseStack.redisEndpoint,
  tags,
});
ec2Stack.addDependency(vpcStack);
ec2Stack.addDependency(ecrStack);
ec2Stack.addDependency(databaseStack);
ec2Stack.addDependency(albStack);

// Apply tags to all stacks
Object.entries(tags).forEach(([key, value]) => {
  cdk.Tags.of(app).add(key, value);
});

app.synth();

```

---

## aws/cdk/cdk.context.json

```json
{
  "availability-zones:account=130954244737:region=ap-northeast-2": [
    "ap-northeast-2a",
    "ap-northeast-2b",
    "ap-northeast-2c",
    "ap-northeast-2d"
  ]
}

```

---

## aws/cdk/cdk.json

```json
{
  "app": "npx ts-node --prefer-ts-exts bin/newsinsight.ts",
  "watch": {
    "include": ["**"],
    "exclude": [
      "README.md",
      "cdk*.json",
      "**/*.d.ts",
      "**/*.js",
      "tsconfig.json",
      "package*.json",
      "yarn.lock",
      "node_modules",
      "test"
    ]
  },
  "context": {
    "hostedZoneId": "Z089524820FOCVKRAPAWD",
    "domainName": "newsinsight.nodove.com",
    "@aws-cdk/aws-lambda:recognizeLayerVersion": true,
    "@aws-cdk/core:checkSecretUsage": true,
    "@aws-cdk/core:target-partitions": ["aws", "aws-cn"],
    "@aws-cdk-containers/ecs-service-extensions:enableDefaultLogDriver": true,
    "@aws-cdk/aws-ec2:uniqueImdsv2TemplateName": true,
    "@aws-cdk/aws-ecs:arnFormatIncludesClusterName": true,
    "@aws-cdk/aws-iam:minimizePolicies": true,
    "@aws-cdk/core:validateSnapshotRemovalPolicy": true,
    "@aws-cdk/aws-codepipeline:crossAccountKeyAliasStackSafeResourceName": true,
    "@aws-cdk/aws-s3:createDefaultLoggingPolicy": true,
    "@aws-cdk/aws-sns-subscriptions:restrictSqsDescryption": true,
    "@aws-cdk/aws-apigateway:disableCloudWatchRole": true,
    "@aws-cdk/core:enablePartitionLiterals": true,
    "@aws-cdk/aws-events:eventsTargetQueueSameAccount": true,
    "@aws-cdk/aws-ecs:disableExplicitDeploymentControllerForCircuitBreaker": true,
    "@aws-cdk/aws-iam:importedRoleStackSafeDefaultPolicyName": true,
    "@aws-cdk/aws-s3:serverAccessLogsUseBucketPolicy": true,
    "@aws-cdk/aws-route53-patters:useCertificate": true,
    "@aws-cdk/customresources:installLatestAwsSdkDefault": false
  }
}

```

---

## aws/cdk/cdk.out/manifest.json

```json
{
  "version": "38.0.1",
  "artifacts": {
    "newsinsight-vpc-dev.assets": {
      "type": "cdk:asset-manifest",
      "properties": {
        "file": "newsinsight-vpc-dev.assets.json",
        "requiresBootstrapStackVersion": 6,
        "bootstrapStackVersionSsmParameter": "/cdk-bootstrap/hnb659fds/version"
      }
    },
    "newsinsight-vpc-dev": {
      "type": "aws:cloudformation:stack",
      "environment": "aws://130954244737/ap-northeast-2",
      "properties": {
        "templateFile": "newsinsight-vpc-dev.template.json",
        "terminationProtection": false,
        "tags": {
          "Environment": "dev",
          "ManagedBy": "CDK",
          "Project": "NewsInsight"
        },
        "validateOnSynth": false,
        "assumeRoleArn": "arn:${AWS::Partition}:iam::130954244737:role/cdk-hnb659fds-deploy-role-130954244737-ap-northeast-2",
        "cloudFormationExecutionRoleArn": "arn:${AWS::Partition}:iam::130954244737:role/cdk-hnb659fds-cfn-exec-role-130954244737-ap-northeast-2",
        "stackTemplateAssetObjectUrl": "s3://cdk-hnb659fds-assets-130954244737-ap-northeast-2/7d9f8eac39839d221d3d3376e92ce9eb47904d92073a216e3d5255c3ba2e0c3c.json",
        "requiresBootstrapStackVersion": 6,
        "bootstrapStackVersionSsmParameter": "/cdk-bootstrap/hnb659fds/version",
        "additionalDependencies": [
          "newsinsight-vpc-dev.assets"
        ],
        "lookupRole": {
          "arn": "arn:${AWS::Partition}:iam::130954244737:role/cdk-hnb659fds-lookup-role-130954244737-ap-northeast-2",
          "requiresBootstrapStackVersion": 8,
          "bootstrapStackVersionSsmParameter": "/cdk-bootstrap/hnb659fds/version"
        }
      },
      "dependencies": [
        "newsinsight-vpc-dev.assets"
      ],
      "metadata": {
        "/newsinsight-vpc-dev": [
          {
            "type": "aws:cdk:stack-tags",
            "data": [
              {
                "Key": "Environment",
                "Value": "dev"
              },
              {
                "Key": "ManagedBy",
                "Value": "CDK"
              },
              {
                "Key": "Project",
                "Value": "NewsInsight"
              }
            ]
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpc0ED561F3"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet1/Subnet": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcPublicSubnet1SubnetDDF6BE05"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet1/RouteTable": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcPublicSubnet1RouteTable4E9167A1"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet1/RouteTableAssociation": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcPublicSubnet1RouteTableAssociation616E1830"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet1/DefaultRoute": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcPublicSubnet1DefaultRoute4C591366"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet1/EIP": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcPublicSubnet1EIP1FC2F625"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet1/NATGateway": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcPublicSubnet1NATGatewayDD254F02"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet2/Subnet": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcPublicSubnet2SubnetA9DCA868"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet2/RouteTable": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcPublicSubnet2RouteTable3A13628D"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet2/RouteTableAssociation": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcPublicSubnet2RouteTableAssociationD1260818"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet2/DefaultRoute": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcPublicSubnet2DefaultRoute46CB9321"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet1/Subnet": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcPrivateSubnet1Subnet2291D3DF"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet1/RouteTable": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcPrivateSubnet1RouteTableCBB2F095"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet1/RouteTableAssociation": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcPrivateSubnet1RouteTableAssociation65213971"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet1/DefaultRoute": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcPrivateSubnet1DefaultRoute5A4D447F"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet2/Subnet": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcPrivateSubnet2Subnet4441BE11"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet2/RouteTable": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcPrivateSubnet2RouteTableF6A23486"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet2/RouteTableAssociation": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcPrivateSubnet2RouteTableAssociation214504C9"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet2/DefaultRoute": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcPrivateSubnet2DefaultRoute3B39260B"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet1/Subnet": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcDatabaseSubnet1Subnet03B7835F"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet1/RouteTable": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcDatabaseSubnet1RouteTable179C7366"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet1/RouteTableAssociation": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcDatabaseSubnet1RouteTableAssociation111ECFF2"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet2/Subnet": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcDatabaseSubnet2SubnetD7C47EB3"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet2/RouteTable": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcDatabaseSubnet2RouteTable69871B4B"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet2/RouteTableAssociation": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcDatabaseSubnet2RouteTableAssociationC660A581"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/IGW": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcIGWB88560BE"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/VPCGW": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcVPCGW74A194B1"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/S3Endpoint/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcS3Endpoint889454DC"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/EcrEndpoint/SecurityGroup/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcEcrEndpointSecurityGroupADDF0D2E"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/EcrEndpoint/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcEcrEndpointB0029AA6"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/EcrDockerEndpoint/SecurityGroup/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcEcrDockerEndpointSecurityGroupE23FE1D4"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/EcrDockerEndpoint/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcEcrDockerEndpoint9500660C"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/CloudWatchLogsEndpoint/SecurityGroup/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcCloudWatchLogsEndpointSecurityGroupF99BA3DA"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/CloudWatchLogsEndpoint/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcCloudWatchLogsEndpoint4D520062"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/SecretsManagerEndpoint/SecurityGroup/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcSecretsManagerEndpointSecurityGroup854CAC5E"
          }
        ],
        "/newsinsight-vpc-dev/NewsInsightVpc/SecretsManagerEndpoint/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "NewsInsightVpcSecretsManagerEndpoint6C5B4F98"
          }
        ],
        "/newsinsight-vpc-dev/VpcId": [
          {
            "type": "aws:cdk:logicalId",
            "data": "VpcId"
          }
        ],
        "/newsinsight-vpc-dev/PublicSubnetIds": [
          {
            "type": "aws:cdk:logicalId",
            "data": "PublicSubnetIds"
          }
        ],
        "/newsinsight-vpc-dev/PrivateSubnetIds": [
          {
            "type": "aws:cdk:logicalId",
            "data": "PrivateSubnetIds"
          }
        ],
        "/newsinsight-vpc-dev/CDKMetadata/Default": [
          {
            "type": "aws:cdk:logicalId",
            "data": "CDKMetadata"
          }
        ],
        "/newsinsight-vpc-dev/Exports/Output{\"Fn::GetAtt\":[\"NewsInsightVpc0ED561F3\",\"CidrBlock\"]}": [
          {
            "type": "aws:cdk:logicalId",
            "data": "ExportsOutputFnGetAttNewsInsightVpc0ED561F3CidrBlock6740B7E1"
          }
        ],
        "/newsinsight-vpc-dev/Exports/Output{\"Ref\":\"NewsInsightVpc0ED561F3\"}": [
          {
            "type": "aws:cdk:logicalId",
            "data": "ExportsOutputRefNewsInsightVpc0ED561F3FDBE1BB1"
          }
        ],
        "/newsinsight-vpc-dev/Exports/Output{\"Ref\":\"NewsInsightVpcDatabaseSubnet1Subnet03B7835F\"}": [
          {
            "type": "aws:cdk:logicalId",
            "data": "ExportsOutputRefNewsInsightVpcDatabaseSubnet1Subnet03B7835F8E9E9CB2"
          }
        ],
        "/newsinsight-vpc-dev/Exports/Output{\"Ref\":\"NewsInsightVpcDatabaseSubnet2SubnetD7C47EB3\"}": [
          {
            "type": "aws:cdk:logicalId",
            "data": "ExportsOutputRefNewsInsightVpcDatabaseSubnet2SubnetD7C47EB3CCBBAC7C"
          }
        ],
        "/newsinsight-vpc-dev/Exports/Output{\"Ref\":\"NewsInsightVpcPublicSubnet1SubnetDDF6BE05\"}": [
          {
            "type": "aws:cdk:logicalId",
            "data": "ExportsOutputRefNewsInsightVpcPublicSubnet1SubnetDDF6BE05C056FD30"
          }
        ],
        "/newsinsight-vpc-dev/Exports/Output{\"Ref\":\"NewsInsightVpcPublicSubnet2SubnetA9DCA868\"}": [
          {
            "type": "aws:cdk:logicalId",
            "data": "ExportsOutputRefNewsInsightVpcPublicSubnet2SubnetA9DCA8686D3D4315"
          }
        ],
        "/newsinsight-vpc-dev/BootstrapVersion": [
          {
            "type": "aws:cdk:logicalId",
            "data": "BootstrapVersion"
          }
        ],
        "/newsinsight-vpc-dev/CheckBootstrapVersion": [
          {
            "type": "aws:cdk:logicalId",
            "data": "CheckBootstrapVersion"
          }
        ]
      },
      "displayName": "newsinsight-vpc-dev"
    },
    "newsinsight-ecr-dev.assets": {
      "type": "cdk:asset-manifest",
      "properties": {
        "file": "newsinsight-ecr-dev.assets.json",
        "requiresBootstrapStackVersion": 6,
        "bootstrapStackVersionSsmParameter": "/cdk-bootstrap/hnb659fds/version"
      }
    },
    "newsinsight-ecr-dev": {
      "type": "aws:cloudformation:stack",
      "environment": "aws://130954244737/ap-northeast-2",
      "properties": {
        "templateFile": "newsinsight-ecr-dev.template.json",
        "terminationProtection": false,
        "tags": {
          "Environment": "dev",
          "ManagedBy": "CDK",
          "Project": "NewsInsight"
        },
        "validateOnSynth": false,
        "assumeRoleArn": "arn:${AWS::Partition}:iam::130954244737:role/cdk-hnb659fds-deploy-role-130954244737-ap-northeast-2",
        "cloudFormationExecutionRoleArn": "arn:${AWS::Partition}:iam::130954244737:role/cdk-hnb659fds-cfn-exec-role-130954244737-ap-northeast-2",
        "stackTemplateAssetObjectUrl": "s3://cdk-hnb659fds-assets-130954244737-ap-northeast-2/5c4a44e7d19bfc76572765a52fd563db4540e88b4ec2be94bace17144f5d30d3.json",
        "requiresBootstrapStackVersion": 6,
        "bootstrapStackVersionSsmParameter": "/cdk-bootstrap/hnb659fds/version",
        "additionalDependencies": [
          "newsinsight-ecr-dev.assets"
        ],
        "lookupRole": {
          "arn": "arn:${AWS::Partition}:iam::130954244737:role/cdk-hnb659fds-lookup-role-130954244737-ap-northeast-2",
          "requiresBootstrapStackVersion": 8,
          "bootstrapStackVersionSsmParameter": "/cdk-bootstrap/hnb659fds/version"
        }
      },
      "dependencies": [
        "newsinsight-ecr-dev.assets"
      ],
      "metadata": {
        "/newsinsight-ecr-dev": [
          {
            "type": "aws:cdk:stack-tags",
            "data": [
              {
                "Key": "Environment",
                "Value": "dev"
              },
              {
                "Key": "ManagedBy",
                "Value": "CDK"
              },
              {
                "Key": "Project",
                "Value": "NewsInsight"
              }
            ]
          }
        ],
        "/newsinsight-ecr-dev/Repo-frontend/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "Repofrontend3A01BFBD"
          }
        ],
        "/newsinsight-ecr-dev/RepoUri-frontend": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepoUrifrontend"
          }
        ],
        "/newsinsight-ecr-dev/Repo-api-gateway/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "Repoapigateway64782533"
          }
        ],
        "/newsinsight-ecr-dev/RepoUri-api-gateway": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepoUriapigateway"
          }
        ],
        "/newsinsight-ecr-dev/Repo-collector-service/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepocollectorserviceBD635EBC"
          }
        ],
        "/newsinsight-ecr-dev/RepoUri-collector-service": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepoUricollectorservice"
          }
        ],
        "/newsinsight-ecr-dev/Repo-autonomous-crawler/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "Repoautonomouscrawler2C28B095"
          }
        ],
        "/newsinsight-ecr-dev/RepoUri-autonomous-crawler": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepoUriautonomouscrawler"
          }
        ],
        "/newsinsight-ecr-dev/Repo-browser-use-api/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "Repobrowseruseapi89FF3D9A"
          }
        ],
        "/newsinsight-ecr-dev/RepoUri-browser-use-api": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepoUribrowseruseapi"
          }
        ],
        "/newsinsight-ecr-dev/Repo-admin-dashboard/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "Repoadmindashboard8DB8FD3C"
          }
        ],
        "/newsinsight-ecr-dev/RepoUri-admin-dashboard": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepoUriadmindashboard"
          }
        ],
        "/newsinsight-ecr-dev/Repo-ai-agent-worker/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "Repoaiagentworker45974E37"
          }
        ],
        "/newsinsight-ecr-dev/RepoUri-ai-agent-worker": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepoUriaiagentworker"
          }
        ],
        "/newsinsight-ecr-dev/Repo-web-crawler/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "Repowebcrawler9872983B"
          }
        ],
        "/newsinsight-ecr-dev/RepoUri-web-crawler": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepoUriwebcrawler"
          }
        ],
        "/newsinsight-ecr-dev/Repo-embedding-server/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "Repoembeddingserver9F2A1DDB"
          }
        ],
        "/newsinsight-ecr-dev/RepoUri-embedding-server": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepoUriembeddingserver"
          }
        ],
        "/newsinsight-ecr-dev/Repo-ip-rotation/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "Repoiprotation8F90C250"
          }
        ],
        "/newsinsight-ecr-dev/RepoUri-ip-rotation": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepoUriiprotation"
          }
        ],
        "/newsinsight-ecr-dev/Repo-crawl-worker/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "Repocrawlworker15CEF027"
          }
        ],
        "/newsinsight-ecr-dev/RepoUri-crawl-worker": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepoUricrawlworker"
          }
        ],
        "/newsinsight-ecr-dev/Repo-maigret-worker/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "Repomaigretworker1D1A79A5"
          }
        ],
        "/newsinsight-ecr-dev/RepoUri-maigret-worker": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepoUrimaigretworker"
          }
        ],
        "/newsinsight-ecr-dev/Repo-bot-detector/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepobotdetectorD791269A"
          }
        ],
        "/newsinsight-ecr-dev/RepoUri-bot-detector": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepoUribotdetector"
          }
        ],
        "/newsinsight-ecr-dev/Repo-sentiment-addon/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "Reposentimentaddon60A21608"
          }
        ],
        "/newsinsight-ecr-dev/RepoUri-sentiment-addon": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepoUrisentimentaddon"
          }
        ],
        "/newsinsight-ecr-dev/Repo-factcheck-addon/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepofactcheckaddonCA82F5DC"
          }
        ],
        "/newsinsight-ecr-dev/RepoUri-factcheck-addon": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepoUrifactcheckaddon"
          }
        ],
        "/newsinsight-ecr-dev/Repo-bias-addon/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepobiasaddonDDD4DB8B"
          }
        ],
        "/newsinsight-ecr-dev/RepoUri-bias-addon": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepoUribiasaddon"
          }
        ],
        "/newsinsight-ecr-dev/Repo-newsinsight-mcp/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "Reponewsinsightmcp5EF0CA8C"
          }
        ],
        "/newsinsight-ecr-dev/RepoUri-newsinsight-mcp": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepoUrinewsinsightmcp"
          }
        ],
        "/newsinsight-ecr-dev/Repo-bias-mcp/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "Repobiasmcp676781E9"
          }
        ],
        "/newsinsight-ecr-dev/RepoUri-bias-mcp": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepoUribiasmcp"
          }
        ],
        "/newsinsight-ecr-dev/Repo-factcheck-mcp/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "Repofactcheckmcp196A0DE5"
          }
        ],
        "/newsinsight-ecr-dev/RepoUri-factcheck-mcp": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepoUrifactcheckmcp"
          }
        ],
        "/newsinsight-ecr-dev/Repo-topic-mcp/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepotopicmcpE31160AA"
          }
        ],
        "/newsinsight-ecr-dev/RepoUri-topic-mcp": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepoUritopicmcp"
          }
        ],
        "/newsinsight-ecr-dev/Repo-aiagent-mcp/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "Repoaiagentmcp0F725DDB"
          }
        ],
        "/newsinsight-ecr-dev/RepoUri-aiagent-mcp": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepoUriaiagentmcp"
          }
        ],
        "/newsinsight-ecr-dev/Repo-roboflow-mcp/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "Reporoboflowmcp13A48D96"
          }
        ],
        "/newsinsight-ecr-dev/RepoUri-roboflow-mcp": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepoUriroboflowmcp"
          }
        ],
        "/newsinsight-ecr-dev/Repo-huggingface-mcp/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "Repohuggingfacemcp583F3940"
          }
        ],
        "/newsinsight-ecr-dev/RepoUri-huggingface-mcp": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepoUrihuggingfacemcp"
          }
        ],
        "/newsinsight-ecr-dev/Repo-kaggle-mcp/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "Repokagglemcp26CB5139"
          }
        ],
        "/newsinsight-ecr-dev/RepoUri-kaggle-mcp": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepoUrikagglemcp"
          }
        ],
        "/newsinsight-ecr-dev/Repo-mltraining-mcp/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepomltrainingmcpBCB2A52B"
          }
        ],
        "/newsinsight-ecr-dev/RepoUri-mltraining-mcp": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RepoUrimltrainingmcp"
          }
        ],
        "/newsinsight-ecr-dev/CDKMetadata/Default": [
          {
            "type": "aws:cdk:logicalId",
            "data": "CDKMetadata"
          }
        ],
        "/newsinsight-ecr-dev/BootstrapVersion": [
          {
            "type": "aws:cdk:logicalId",
            "data": "BootstrapVersion"
          }
        ],
        "/newsinsight-ecr-dev/CheckBootstrapVersion": [
          {
            "type": "aws:cdk:logicalId",
            "data": "CheckBootstrapVersion"
          }
        ]
      },
      "displayName": "newsinsight-ecr-dev"
    },
    "newsinsight-db-dev.assets": {
      "type": "cdk:asset-manifest",
      "properties": {
        "file": "newsinsight-db-dev.assets.json",
        "requiresBootstrapStackVersion": 6,
        "bootstrapStackVersionSsmParameter": "/cdk-bootstrap/hnb659fds/version"
      }
    },
    "newsinsight-db-dev": {
      "type": "aws:cloudformation:stack",
      "environment": "aws://130954244737/ap-northeast-2",
      "properties": {
        "templateFile": "newsinsight-db-dev.template.json",
        "terminationProtection": false,
        "tags": {
          "Environment": "dev",
          "ManagedBy": "CDK",
          "Project": "NewsInsight"
        },
        "validateOnSynth": false,
        "assumeRoleArn": "arn:${AWS::Partition}:iam::130954244737:role/cdk-hnb659fds-deploy-role-130954244737-ap-northeast-2",
        "cloudFormationExecutionRoleArn": "arn:${AWS::Partition}:iam::130954244737:role/cdk-hnb659fds-cfn-exec-role-130954244737-ap-northeast-2",
        "stackTemplateAssetObjectUrl": "s3://cdk-hnb659fds-assets-130954244737-ap-northeast-2/28ef594edb47ed9701e38e81c99da921ff84986d744235239611b0afc2cbc477.json",
        "requiresBootstrapStackVersion": 6,
        "bootstrapStackVersionSsmParameter": "/cdk-bootstrap/hnb659fds/version",
        "additionalDependencies": [
          "newsinsight-db-dev.assets"
        ],
        "lookupRole": {
          "arn": "arn:${AWS::Partition}:iam::130954244737:role/cdk-hnb659fds-lookup-role-130954244737-ap-northeast-2",
          "requiresBootstrapStackVersion": 8,
          "bootstrapStackVersionSsmParameter": "/cdk-bootstrap/hnb659fds/version"
        }
      },
      "dependencies": [
        "newsinsight-vpc-dev",
        "newsinsight-db-dev.assets"
      ],
      "metadata": {
        "/newsinsight-db-dev": [
          {
            "type": "aws:cdk:stack-tags",
            "data": [
              {
                "Key": "Environment",
                "Value": "dev"
              },
              {
                "Key": "ManagedBy",
                "Value": "CDK"
              },
              {
                "Key": "Project",
                "Value": "NewsInsight"
              }
            ]
          }
        ],
        "/newsinsight-db-dev/DbSecurityGroup/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "DbSecurityGroupE9D701AD"
          }
        ],
        "/newsinsight-db-dev/PostgresSecret/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "PostgresSecretE142E1BC"
          }
        ],
        "/newsinsight-db-dev/PostgresSecret/Attachment/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "PostgresSecretAttachment2BFBBA34"
          }
        ],
        "/newsinsight-db-dev/MongoSecret/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "MongoSecretEC0FA3F4"
          }
        ],
        "/newsinsight-db-dev/RedisSecret/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RedisSecretCB3BD971"
          }
        ],
        "/newsinsight-db-dev/PostgresCluster/Subnets/Default": [
          {
            "type": "aws:cdk:logicalId",
            "data": "PostgresClusterSubnets41D66232"
          }
        ],
        "/newsinsight-db-dev/PostgresCluster/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "PostgresClusterE64235C6"
          }
        ],
        "/newsinsight-db-dev/PostgresCluster/Instance1": [
          {
            "type": "aws:cdk:logicalId",
            "data": "PostgresClusterInstance18CEE66B9"
          }
        ],
        "/newsinsight-db-dev/DocDbCluster/Subnets": [
          {
            "type": "aws:cdk:logicalId",
            "data": "DocDbClusterSubnetsBFD1BEB3"
          }
        ],
        "/newsinsight-db-dev/DocDbCluster/Secret/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "DocDbClusterSecretAE05C874"
          }
        ],
        "/newsinsight-db-dev/DocDbCluster/Secret/Attachment/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "DocDbClusterSecretAttachment8EB57C05"
          }
        ],
        "/newsinsight-db-dev/DocDbCluster/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "DocDbClusterB46CF5D3"
          }
        ],
        "/newsinsight-db-dev/DocDbCluster/Instance1": [
          {
            "type": "aws:cdk:logicalId",
            "data": "DocDbClusterInstance13FC488BD"
          }
        ],
        "/newsinsight-db-dev/RedisSubnetGroup": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RedisSubnetGroup"
          }
        ],
        "/newsinsight-db-dev/RedisCluster": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RedisCluster"
          }
        ],
        "/newsinsight-db-dev/PostgresEndpoint": [
          {
            "type": "aws:cdk:logicalId",
            "data": "PostgresEndpoint"
          }
        ],
        "/newsinsight-db-dev/MongoEndpoint": [
          {
            "type": "aws:cdk:logicalId",
            "data": "MongoEndpoint"
          }
        ],
        "/newsinsight-db-dev/RedisEndpoint": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RedisEndpoint"
          }
        ],
        "/newsinsight-db-dev/PostgresSecretArn": [
          {
            "type": "aws:cdk:logicalId",
            "data": "PostgresSecretArn"
          }
        ],
        "/newsinsight-db-dev/MongoSecretArn": [
          {
            "type": "aws:cdk:logicalId",
            "data": "MongoSecretArn"
          }
        ],
        "/newsinsight-db-dev/RedisSecretArn": [
          {
            "type": "aws:cdk:logicalId",
            "data": "RedisSecretArn"
          }
        ],
        "/newsinsight-db-dev/CDKMetadata/Default": [
          {
            "type": "aws:cdk:logicalId",
            "data": "CDKMetadata"
          }
        ],
        "/newsinsight-db-dev/Exports/Output{\"Ref\":\"PostgresSecretE142E1BC\"}": [
          {
            "type": "aws:cdk:logicalId",
            "data": "ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
          }
        ],
        "/newsinsight-db-dev/Exports/Output{\"Ref\":\"MongoSecretEC0FA3F4\"}": [
          {
            "type": "aws:cdk:logicalId",
            "data": "ExportsOutputRefMongoSecretEC0FA3F415029244"
          }
        ],
        "/newsinsight-db-dev/Exports/Output{\"Ref\":\"RedisSecretCB3BD971\"}": [
          {
            "type": "aws:cdk:logicalId",
            "data": "ExportsOutputRefRedisSecretCB3BD9716380542C"
          }
        ],
        "/newsinsight-db-dev/Exports/Output{\"Fn::GetAtt\":[\"PostgresClusterE64235C6\",\"Endpoint.Address\"]}": [
          {
            "type": "aws:cdk:logicalId",
            "data": "ExportsOutputFnGetAttPostgresClusterE64235C6EndpointAddress8684A102"
          }
        ],
        "/newsinsight-db-dev/Exports/Output{\"Fn::GetAtt\":[\"DocDbClusterB46CF5D3\",\"Endpoint\"]}": [
          {
            "type": "aws:cdk:logicalId",
            "data": "ExportsOutputFnGetAttDocDbClusterB46CF5D3EndpointD6B42638"
          }
        ],
        "/newsinsight-db-dev/Exports/Output{\"Fn::GetAtt\":[\"RedisCluster\",\"PrimaryEndPoint.Address\"]}": [
          {
            "type": "aws:cdk:logicalId",
            "data": "ExportsOutputFnGetAttRedisClusterPrimaryEndPointAddressCFB16E1D"
          }
        ],
        "/newsinsight-db-dev/BootstrapVersion": [
          {
            "type": "aws:cdk:logicalId",
            "data": "BootstrapVersion"
          }
        ],
        "/newsinsight-db-dev/CheckBootstrapVersion": [
          {
            "type": "aws:cdk:logicalId",
            "data": "CheckBootstrapVersion"
          }
        ]
      },
      "displayName": "newsinsight-db-dev"
    },
    "newsinsight-alb-dev.assets": {
      "type": "cdk:asset-manifest",
      "properties": {
        "file": "newsinsight-alb-dev.assets.json",
        "requiresBootstrapStackVersion": 6,
        "bootstrapStackVersionSsmParameter": "/cdk-bootstrap/hnb659fds/version"
      }
    },
    "newsinsight-alb-dev": {
      "type": "aws:cloudformation:stack",
      "environment": "aws://130954244737/ap-northeast-2",
      "properties": {
        "templateFile": "newsinsight-alb-dev.template.json",
        "terminationProtection": false,
        "tags": {
          "Environment": "dev",
          "ManagedBy": "CDK",
          "Project": "NewsInsight"
        },
        "validateOnSynth": false,
        "assumeRoleArn": "arn:${AWS::Partition}:iam::130954244737:role/cdk-hnb659fds-deploy-role-130954244737-ap-northeast-2",
        "cloudFormationExecutionRoleArn": "arn:${AWS::Partition}:iam::130954244737:role/cdk-hnb659fds-cfn-exec-role-130954244737-ap-northeast-2",
        "stackTemplateAssetObjectUrl": "s3://cdk-hnb659fds-assets-130954244737-ap-northeast-2/492a690240c23b656f6ae5ee9afc1bc81782bd9ecfebed6bd8eb7d734e900452.json",
        "requiresBootstrapStackVersion": 6,
        "bootstrapStackVersionSsmParameter": "/cdk-bootstrap/hnb659fds/version",
        "additionalDependencies": [
          "newsinsight-alb-dev.assets"
        ],
        "lookupRole": {
          "arn": "arn:${AWS::Partition}:iam::130954244737:role/cdk-hnb659fds-lookup-role-130954244737-ap-northeast-2",
          "requiresBootstrapStackVersion": 8,
          "bootstrapStackVersionSsmParameter": "/cdk-bootstrap/hnb659fds/version"
        }
      },
      "dependencies": [
        "newsinsight-vpc-dev",
        "newsinsight-alb-dev.assets"
      ],
      "metadata": {
        "/newsinsight-alb-dev": [
          {
            "type": "aws:cdk:stack-tags",
            "data": [
              {
                "Key": "Environment",
                "Value": "dev"
              },
              {
                "Key": "ManagedBy",
                "Value": "CDK"
              },
              {
                "Key": "Project",
                "Value": "NewsInsight"
              }
            ]
          }
        ],
        "/newsinsight-alb-dev/AlbSecurityGroup/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "AlbSecurityGroup86A59E99"
          }
        ],
        "/newsinsight-alb-dev/Alb/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "Alb16C2F182"
          }
        ],
        "/newsinsight-alb-dev/Alb/HttpListener/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "AlbHttpListener00C8B33E"
          }
        ],
        "/newsinsight-alb-dev/Alb/HttpListener/ListenerRule-frontendRule/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "AlbHttpListenerListenerRulefrontendRuleFF284057"
          }
        ],
        "/newsinsight-alb-dev/Alb/HttpListener/ListenerRule-api-gatewayRule/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "AlbHttpListenerListenerRuleapigatewayRule3E8115B3"
          }
        ],
        "/newsinsight-alb-dev/Alb/HttpListener/ListenerRule-admin-dashboardRule/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "AlbHttpListenerListenerRuleadmindashboardRuleA3BF1C9E"
          }
        ],
        "/newsinsight-alb-dev/TG-frontend/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "TGfrontend5DB1B90D"
          }
        ],
        "/newsinsight-alb-dev/TG-api-gateway/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "TGapigatewayD7113A21"
          }
        ],
        "/newsinsight-alb-dev/TG-admin-dashboard/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "TGadmindashboard10526AD8"
          }
        ],
        "/newsinsight-alb-dev/AlbDnsName": [
          {
            "type": "aws:cdk:logicalId",
            "data": "AlbDnsName"
          }
        ],
        "/newsinsight-alb-dev/AlbArn": [
          {
            "type": "aws:cdk:logicalId",
            "data": "AlbArn"
          }
        ],
        "/newsinsight-alb-dev/HttpListenerArn": [
          {
            "type": "aws:cdk:logicalId",
            "data": "HttpListenerArn"
          }
        ],
        "/newsinsight-alb-dev/CDKMetadata/Default": [
          {
            "type": "aws:cdk:logicalId",
            "data": "CDKMetadata"
          }
        ],
        "/newsinsight-alb-dev/Exports/Output{\"Fn::GetAtt\":[\"AlbSecurityGroup86A59E99\",\"GroupId\"]}": [
          {
            "type": "aws:cdk:logicalId",
            "data": "ExportsOutputFnGetAttAlbSecurityGroup86A59E99GroupIdE3A37BC7"
          }
        ],
        "/newsinsight-alb-dev/BootstrapVersion": [
          {
            "type": "aws:cdk:logicalId",
            "data": "BootstrapVersion"
          }
        ],
        "/newsinsight-alb-dev/CheckBootstrapVersion": [
          {
            "type": "aws:cdk:logicalId",
            "data": "CheckBootstrapVersion"
          }
        ]
      },
      "displayName": "newsinsight-alb-dev"
    },
    "newsinsight-ec2-dev.assets": {
      "type": "cdk:asset-manifest",
      "properties": {
        "file": "newsinsight-ec2-dev.assets.json",
        "requiresBootstrapStackVersion": 6,
        "bootstrapStackVersionSsmParameter": "/cdk-bootstrap/hnb659fds/version"
      }
    },
    "newsinsight-ec2-dev": {
      "type": "aws:cloudformation:stack",
      "environment": "aws://130954244737/ap-northeast-2",
      "properties": {
        "templateFile": "newsinsight-ec2-dev.template.json",
        "terminationProtection": false,
        "tags": {
          "Environment": "dev",
          "ManagedBy": "CDK",
          "Project": "NewsInsight"
        },
        "validateOnSynth": false,
        "assumeRoleArn": "arn:${AWS::Partition}:iam::130954244737:role/cdk-hnb659fds-deploy-role-130954244737-ap-northeast-2",
        "cloudFormationExecutionRoleArn": "arn:${AWS::Partition}:iam::130954244737:role/cdk-hnb659fds-cfn-exec-role-130954244737-ap-northeast-2",
        "stackTemplateAssetObjectUrl": "s3://cdk-hnb659fds-assets-130954244737-ap-northeast-2/f252d68ab43fad345e92d9560c652a14eeac8575bddde87763a7d664c908c8d6.json",
        "requiresBootstrapStackVersion": 6,
        "bootstrapStackVersionSsmParameter": "/cdk-bootstrap/hnb659fds/version",
        "additionalDependencies": [
          "newsinsight-ec2-dev.assets"
        ],
        "lookupRole": {
          "arn": "arn:${AWS::Partition}:iam::130954244737:role/cdk-hnb659fds-lookup-role-130954244737-ap-northeast-2",
          "requiresBootstrapStackVersion": 8,
          "bootstrapStackVersionSsmParameter": "/cdk-bootstrap/hnb659fds/version"
        }
      },
      "dependencies": [
        "newsinsight-vpc-dev",
        "newsinsight-ecr-dev",
        "newsinsight-db-dev",
        "newsinsight-alb-dev",
        "newsinsight-ec2-dev.assets"
      ],
      "metadata": {
        "/newsinsight-ec2-dev": [
          {
            "type": "aws:cdk:stack-tags",
            "data": [
              {
                "Key": "Environment",
                "Value": "dev"
              },
              {
                "Key": "ManagedBy",
                "Value": "CDK"
              },
              {
                "Key": "Project",
                "Value": "NewsInsight"
              }
            ]
          }
        ],
        "/newsinsight-ec2-dev/Ec2SecurityGroup/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "Ec2SecurityGroup55889913"
          }
        ],
        "/newsinsight-ec2-dev/Ec2Role/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "Ec2Role2FD9A272"
          }
        ],
        "/newsinsight-ec2-dev/Ec2Role/DefaultPolicy/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "Ec2RoleDefaultPolicy99BC91F7"
          }
        ],
        "/newsinsight-ec2-dev/KeyPair/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "KeyPair1622897B"
          }
        ],
        "/newsinsight-ec2-dev/Instance/InstanceProfile": [
          {
            "type": "aws:cdk:logicalId",
            "data": "InstanceInstanceProfileAB5AEF02"
          }
        ],
        "/newsinsight-ec2-dev/Instance/Resource": [
          {
            "type": "aws:cdk:logicalId",
            "data": "InstanceC1063A87"
          }
        ],
        "/newsinsight-ec2-dev/SsmParameterValue:--aws--service--ami-amazon-linux-latest--al2023-ami-kernel-6.1-x86_64:C96584B6-F00A-464E-AD19-53AFF4B05118.Parameter": [
          {
            "type": "aws:cdk:logicalId",
            "data": "SsmParameterValueawsserviceamiamazonlinuxlatestal2023amikernel61x8664C96584B6F00A464EAD1953AFF4B05118Parameter"
          }
        ],
        "/newsinsight-ec2-dev/InstanceId": [
          {
            "type": "aws:cdk:logicalId",
            "data": "InstanceId"
          }
        ],
        "/newsinsight-ec2-dev/InstancePublicIp": [
          {
            "type": "aws:cdk:logicalId",
            "data": "InstancePublicIp"
          }
        ],
        "/newsinsight-ec2-dev/InstancePrivateIp": [
          {
            "type": "aws:cdk:logicalId",
            "data": "InstancePrivateIp"
          }
        ],
        "/newsinsight-ec2-dev/KeyPairId": [
          {
            "type": "aws:cdk:logicalId",
            "data": "KeyPairId"
          }
        ],
        "/newsinsight-ec2-dev/SSHCommand": [
          {
            "type": "aws:cdk:logicalId",
            "data": "SSHCommand"
          }
        ],
        "/newsinsight-ec2-dev/CDKMetadata/Default": [
          {
            "type": "aws:cdk:logicalId",
            "data": "CDKMetadata"
          }
        ],
        "/newsinsight-ec2-dev/BootstrapVersion": [
          {
            "type": "aws:cdk:logicalId",
            "data": "BootstrapVersion"
          }
        ],
        "/newsinsight-ec2-dev/CheckBootstrapVersion": [
          {
            "type": "aws:cdk:logicalId",
            "data": "CheckBootstrapVersion"
          }
        ]
      },
      "displayName": "newsinsight-ec2-dev"
    },
    "Tree": {
      "type": "cdk:tree",
      "properties": {
        "file": "tree.json"
      }
    }
  }
}
```

---

## aws/cdk/cdk.out/newsinsight-alb-dev.assets.json

```json
{
  "version": "38.0.1",
  "files": {
    "492a690240c23b656f6ae5ee9afc1bc81782bd9ecfebed6bd8eb7d734e900452": {
      "source": {
        "path": "newsinsight-alb-dev.template.json",
        "packaging": "file"
      },
      "destinations": {
        "130954244737-ap-northeast-2": {
          "bucketName": "cdk-hnb659fds-assets-130954244737-ap-northeast-2",
          "objectKey": "492a690240c23b656f6ae5ee9afc1bc81782bd9ecfebed6bd8eb7d734e900452.json",
          "region": "ap-northeast-2",
          "assumeRoleArn": "arn:${AWS::Partition}:iam::130954244737:role/cdk-hnb659fds-file-publishing-role-130954244737-ap-northeast-2"
        }
      }
    }
  },
  "dockerImages": {}
}
```

---

## aws/cdk/cdk.out/newsinsight-alb-dev.template.json

```json
{
 "Resources": {
  "AlbSecurityGroup86A59E99": {
   "Type": "AWS::EC2::SecurityGroup",
   "Properties": {
    "GroupDescription": "Security group for Application Load Balancer",
    "GroupName": "newsinsight-alb-sg-dev",
    "SecurityGroupEgress": [
     {
      "CidrIp": "0.0.0.0/0",
      "Description": "Allow all outbound traffic by default",
      "IpProtocol": "-1"
     }
    ],
    "SecurityGroupIngress": [
     {
      "CidrIp": "0.0.0.0/0",
      "Description": "HTTP from anywhere",
      "FromPort": 80,
      "IpProtocol": "tcp",
      "ToPort": 80
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "VpcId": {
     "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpc0ED561F3FDBE1BB1"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-alb-dev/AlbSecurityGroup/Resource"
   }
  },
  "Alb16C2F182": {
   "Type": "AWS::ElasticLoadBalancingV2::LoadBalancer",
   "Properties": {
    "LoadBalancerAttributes": [
     {
      "Key": "deletion_protection.enabled",
      "Value": "false"
     }
    ],
    "Name": "newsinsight-alb-dev",
    "Scheme": "internet-facing",
    "SecurityGroups": [
     {
      "Fn::GetAtt": [
       "AlbSecurityGroup86A59E99",
       "GroupId"
      ]
     }
    ],
    "Subnets": [
     {
      "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPublicSubnet1SubnetDDF6BE05C056FD30"
     },
     {
      "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPublicSubnet2SubnetA9DCA8686D3D4315"
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "Type": "application"
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-alb-dev/Alb/Resource"
   }
  },
  "AlbHttpListener00C8B33E": {
   "Type": "AWS::ElasticLoadBalancingV2::Listener",
   "Properties": {
    "DefaultActions": [
     {
      "FixedResponseConfig": {
       "ContentType": "text/plain",
       "MessageBody": "Not Found",
       "StatusCode": "404"
      },
      "Type": "fixed-response"
     }
    ],
    "LoadBalancerArn": {
     "Ref": "Alb16C2F182"
    },
    "Port": 80,
    "Protocol": "HTTP"
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-alb-dev/Alb/HttpListener/Resource"
   }
  },
  "AlbHttpListenerListenerRulefrontendRuleFF284057": {
   "Type": "AWS::ElasticLoadBalancingV2::ListenerRule",
   "Properties": {
    "Actions": [
     {
      "TargetGroupArn": {
       "Ref": "TGfrontend5DB1B90D"
      },
      "Type": "forward"
     }
    ],
    "Conditions": [
     {
      "Field": "path-pattern",
      "PathPatternConfig": {
       "Values": [
        "/*"
       ]
      }
     }
    ],
    "ListenerArn": {
     "Ref": "AlbHttpListener00C8B33E"
    },
    "Priority": 100
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-alb-dev/Alb/HttpListener/ListenerRule-frontendRule/Resource"
   }
  },
  "AlbHttpListenerListenerRuleapigatewayRule3E8115B3": {
   "Type": "AWS::ElasticLoadBalancingV2::ListenerRule",
   "Properties": {
    "Actions": [
     {
      "TargetGroupArn": {
       "Ref": "TGapigatewayD7113A21"
      },
      "Type": "forward"
     }
    ],
    "Conditions": [
     {
      "Field": "path-pattern",
      "PathPatternConfig": {
       "Values": [
        "/api/*"
       ]
      }
     }
    ],
    "ListenerArn": {
     "Ref": "AlbHttpListener00C8B33E"
    },
    "Priority": 10
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-alb-dev/Alb/HttpListener/ListenerRule-api-gatewayRule/Resource"
   }
  },
  "AlbHttpListenerListenerRuleadmindashboardRuleA3BF1C9E": {
   "Type": "AWS::ElasticLoadBalancingV2::ListenerRule",
   "Properties": {
    "Actions": [
     {
      "TargetGroupArn": {
       "Ref": "TGadmindashboard10526AD8"
      },
      "Type": "forward"
     }
    ],
    "Conditions": [
     {
      "Field": "path-pattern",
      "PathPatternConfig": {
       "Values": [
        "/admin/*"
       ]
      }
     }
    ],
    "ListenerArn": {
     "Ref": "AlbHttpListener00C8B33E"
    },
    "Priority": 20
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-alb-dev/Alb/HttpListener/ListenerRule-admin-dashboardRule/Resource"
   }
  },
  "TGfrontend5DB1B90D": {
   "Type": "AWS::ElasticLoadBalancingV2::TargetGroup",
   "Properties": {
    "HealthCheckIntervalSeconds": 30,
    "HealthCheckPath": "/health",
    "HealthCheckTimeoutSeconds": 10,
    "HealthyThresholdCount": 2,
    "Name": "newsinsight-frontend-dev",
    "Port": 8080,
    "Protocol": "HTTP",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TargetGroupAttributes": [
     {
      "Key": "deregistration_delay.timeout_seconds",
      "Value": "30"
     },
     {
      "Key": "stickiness.enabled",
      "Value": "false"
     }
    ],
    "TargetType": "instance",
    "UnhealthyThresholdCount": 3,
    "VpcId": {
     "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpc0ED561F3FDBE1BB1"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-alb-dev/TG-frontend/Resource"
   }
  },
  "TGapigatewayD7113A21": {
   "Type": "AWS::ElasticLoadBalancingV2::TargetGroup",
   "Properties": {
    "HealthCheckIntervalSeconds": 30,
    "HealthCheckPath": "/actuator/health",
    "HealthCheckTimeoutSeconds": 10,
    "HealthyThresholdCount": 2,
    "Name": "newsinsight-api-gateway-dev",
    "Port": 8000,
    "Protocol": "HTTP",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TargetGroupAttributes": [
     {
      "Key": "deregistration_delay.timeout_seconds",
      "Value": "30"
     },
     {
      "Key": "stickiness.enabled",
      "Value": "false"
     }
    ],
    "TargetType": "instance",
    "UnhealthyThresholdCount": 3,
    "VpcId": {
     "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpc0ED561F3FDBE1BB1"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-alb-dev/TG-api-gateway/Resource"
   }
  },
  "TGadmindashboard10526AD8": {
   "Type": "AWS::ElasticLoadBalancingV2::TargetGroup",
   "Properties": {
    "HealthCheckIntervalSeconds": 30,
    "HealthCheckPath": "/health",
    "HealthCheckTimeoutSeconds": 10,
    "HealthyThresholdCount": 2,
    "Name": "newsinsight-admin-dashboard-dev",
    "Port": 8888,
    "Protocol": "HTTP",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TargetGroupAttributes": [
     {
      "Key": "deregistration_delay.timeout_seconds",
      "Value": "30"
     },
     {
      "Key": "stickiness.enabled",
      "Value": "false"
     }
    ],
    "TargetType": "instance",
    "UnhealthyThresholdCount": 3,
    "VpcId": {
     "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpc0ED561F3FDBE1BB1"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-alb-dev/TG-admin-dashboard/Resource"
   }
  },
  "CDKMetadata": {
   "Type": "AWS::CDK::Metadata",
   "Properties": {
    "Analytics": "v2:deflate64:H4sIAAAAAAAA/22OywrCMBBFv8V9O9q4cK0u3AhC616m6SjTxqQkE0VK/11aH1RwNfdxGK6CbLWExQzvIdVVkxouoSsEdZPgPZw60gq6gnT0LI+dd7FNtmf7E/QJGQzC2jisSjRoNdvLTUG3blvDGoWd3TusNmNHfvjw46ccByH7Zj76T59HQ1Nm9BPuiP5C8h08sX0/BIcobZRB5RRc9Jr6xLqKoA7zm1KgMshmdWBOfbTCV4L8dZ+H7pJhMQEAAA=="
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-alb-dev/CDKMetadata/Default"
   }
  }
 },
 "Outputs": {
  "AlbDnsName": {
   "Description": "ALB DNS Name",
   "Value": {
    "Fn::GetAtt": [
     "Alb16C2F182",
     "DNSName"
    ]
   },
   "Export": {
    "Name": "newsinsight-alb-dns-dev"
   }
  },
  "AlbArn": {
   "Description": "ALB ARN",
   "Value": {
    "Ref": "Alb16C2F182"
   },
   "Export": {
    "Name": "newsinsight-alb-arn-dev"
   }
  },
  "HttpListenerArn": {
   "Description": "HTTP Listener ARN",
   "Value": {
    "Ref": "AlbHttpListener00C8B33E"
   },
   "Export": {
    "Name": "newsinsight-http-listener-dev"
   }
  },
  "ExportsOutputFnGetAttAlbSecurityGroup86A59E99GroupIdE3A37BC7": {
   "Value": {
    "Fn::GetAtt": [
     "AlbSecurityGroup86A59E99",
     "GroupId"
    ]
   },
   "Export": {
    "Name": "newsinsight-alb-dev:ExportsOutputFnGetAttAlbSecurityGroup86A59E99GroupIdE3A37BC7"
   }
  }
 },
 "Parameters": {
  "BootstrapVersion": {
   "Type": "AWS::SSM::Parameter::Value<String>",
   "Default": "/cdk-bootstrap/hnb659fds/version",
   "Description": "Version of the CDK Bootstrap resources in this environment, automatically retrieved from SSM Parameter Store. [cdk:skip]"
  }
 },
 "Rules": {
  "CheckBootstrapVersion": {
   "Assertions": [
    {
     "Assert": {
      "Fn::Not": [
       {
        "Fn::Contains": [
         [
          "1",
          "2",
          "3",
          "4",
          "5"
         ],
         {
          "Ref": "BootstrapVersion"
         }
        ]
       }
      ]
     },
     "AssertDescription": "CDK bootstrap stack version 6 required. Please run 'cdk bootstrap' with a recent version of the CDK CLI."
    }
   ]
  }
 }
}
```

---

## aws/cdk/cdk.out/newsinsight-db-dev.assets.json

```json
{
  "version": "38.0.1",
  "files": {
    "28ef594edb47ed9701e38e81c99da921ff84986d744235239611b0afc2cbc477": {
      "source": {
        "path": "newsinsight-db-dev.template.json",
        "packaging": "file"
      },
      "destinations": {
        "130954244737-ap-northeast-2": {
          "bucketName": "cdk-hnb659fds-assets-130954244737-ap-northeast-2",
          "objectKey": "28ef594edb47ed9701e38e81c99da921ff84986d744235239611b0afc2cbc477.json",
          "region": "ap-northeast-2",
          "assumeRoleArn": "arn:${AWS::Partition}:iam::130954244737:role/cdk-hnb659fds-file-publishing-role-130954244737-ap-northeast-2"
        }
      }
    }
  },
  "dockerImages": {}
}
```

---

## aws/cdk/cdk.out/newsinsight-db-dev.template.json

```json
{
 "Resources": {
  "DbSecurityGroupE9D701AD": {
   "Type": "AWS::EC2::SecurityGroup",
   "Properties": {
    "GroupDescription": "Security group for databases",
    "GroupName": "newsinsight-db-sg-dev",
    "SecurityGroupEgress": [
     {
      "CidrIp": "0.0.0.0/0",
      "Description": "Allow all outbound traffic by default",
      "IpProtocol": "-1"
     }
    ],
    "SecurityGroupIngress": [
     {
      "CidrIp": {
       "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputFnGetAttNewsInsightVpc0ED561F3CidrBlock6740B7E1"
      },
      "Description": "PostgreSQL from VPC",
      "FromPort": 5432,
      "IpProtocol": "tcp",
      "ToPort": 5432
     },
     {
      "CidrIp": {
       "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputFnGetAttNewsInsightVpc0ED561F3CidrBlock6740B7E1"
      },
      "Description": "MongoDB from VPC",
      "FromPort": 27017,
      "IpProtocol": "tcp",
      "ToPort": 27017
     },
     {
      "CidrIp": {
       "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputFnGetAttNewsInsightVpc0ED561F3CidrBlock6740B7E1"
      },
      "Description": "Redis from VPC",
      "FromPort": 6379,
      "IpProtocol": "tcp",
      "ToPort": 6379
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "VpcId": {
     "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpc0ED561F3FDBE1BB1"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-db-dev/DbSecurityGroup/Resource"
   }
  },
  "PostgresSecretE142E1BC": {
   "Type": "AWS::SecretsManager::Secret",
   "Properties": {
    "Description": "PostgreSQL credentials",
    "GenerateSecretString": {
     "ExcludePunctuation": true,
     "GenerateStringKey": "password",
     "PasswordLength": 32,
     "SecretStringTemplate": "{\"username\":\"newsinsight\",\"database\":\"newsinsight\"}"
    },
    "Name": "newsinsight/dev/postgres",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-db-dev/PostgresSecret/Resource"
   }
  },
  "PostgresSecretAttachment2BFBBA34": {
   "Type": "AWS::SecretsManager::SecretTargetAttachment",
   "Properties": {
    "SecretId": {
     "Ref": "PostgresSecretE142E1BC"
    },
    "TargetId": {
     "Ref": "PostgresClusterE64235C6"
    },
    "TargetType": "AWS::RDS::DBCluster"
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-db-dev/PostgresSecret/Attachment/Resource"
   }
  },
  "MongoSecretEC0FA3F4": {
   "Type": "AWS::SecretsManager::Secret",
   "Properties": {
    "Description": "MongoDB credentials",
    "GenerateSecretString": {
     "ExcludePunctuation": true,
     "GenerateStringKey": "password",
     "PasswordLength": 32,
     "SecretStringTemplate": "{\"username\":\"newsinsight\"}"
    },
    "Name": "newsinsight/dev/mongo",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-db-dev/MongoSecret/Resource"
   }
  },
  "RedisSecretCB3BD971": {
   "Type": "AWS::SecretsManager::Secret",
   "Properties": {
    "Description": "Redis auth token",
    "GenerateSecretString": {
     "ExcludePunctuation": true,
     "PasswordLength": 32
    },
    "Name": "newsinsight/dev/redis",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-db-dev/RedisSecret/Resource"
   }
  },
  "PostgresClusterSubnets41D66232": {
   "Type": "AWS::RDS::DBSubnetGroup",
   "Properties": {
    "DBSubnetGroupDescription": "Subnets for PostgresCluster database",
    "SubnetIds": [
     {
      "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcDatabaseSubnet1Subnet03B7835F8E9E9CB2"
     },
     {
      "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcDatabaseSubnet2SubnetD7C47EB3CCBBAC7C"
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-db-dev/PostgresCluster/Subnets/Default"
   }
  },
  "PostgresClusterE64235C6": {
   "Type": "AWS::RDS::DBCluster",
   "Properties": {
    "BackupRetentionPeriod": 7,
    "CopyTagsToSnapshot": true,
    "DBClusterIdentifier": "newsinsight-postgres-dev",
    "DBClusterParameterGroupName": "default.aurora-postgresql15",
    "DBSubnetGroupName": {
     "Ref": "PostgresClusterSubnets41D66232"
    },
    "DatabaseName": "newsinsight",
    "DeletionProtection": false,
    "Engine": "aurora-postgresql",
    "EngineVersion": "15.8",
    "MasterUserPassword": {
     "Fn::Join": [
      "",
      [
       "{{resolve:secretsmanager:",
       {
        "Ref": "PostgresSecretE142E1BC"
       },
       ":SecretString:password::}}"
      ]
     ]
    },
    "MasterUsername": {
     "Fn::Join": [
      "",
      [
       "{{resolve:secretsmanager:",
       {
        "Ref": "PostgresSecretE142E1BC"
       },
       ":SecretString:username::}}"
      ]
     ]
    },
    "Port": 5432,
    "StorageEncrypted": true,
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "VpcSecurityGroupIds": [
     {
      "Fn::GetAtt": [
       "DbSecurityGroupE9D701AD",
       "GroupId"
      ]
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-db-dev/PostgresCluster/Resource"
   }
  },
  "PostgresClusterInstance18CEE66B9": {
   "Type": "AWS::RDS::DBInstance",
   "Properties": {
    "DBClusterIdentifier": {
     "Ref": "PostgresClusterE64235C6"
    },
    "DBInstanceClass": "db.t4g.medium",
    "DBInstanceIdentifier": "newsinsight-postgres-devinstance1",
    "DBSubnetGroupName": {
     "Ref": "PostgresClusterSubnets41D66232"
    },
    "Engine": "aurora-postgresql",
    "PubliclyAccessible": false,
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-db-dev/PostgresCluster/Instance1"
   }
  },
  "DocDbClusterSubnetsBFD1BEB3": {
   "Type": "AWS::DocDB::DBSubnetGroup",
   "Properties": {
    "DBSubnetGroupDescription": "Subnets for DocDbCluster database",
    "SubnetIds": [
     {
      "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcDatabaseSubnet1Subnet03B7835F8E9E9CB2"
     },
     {
      "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcDatabaseSubnet2SubnetD7C47EB3CCBBAC7C"
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-db-dev/DocDbCluster/Subnets"
   }
  },
  "DocDbClusterSecretAE05C874": {
   "Type": "AWS::SecretsManager::Secret",
   "Properties": {
    "Description": {
     "Fn::Join": [
      "",
      [
       "Generated by the CDK for stack: ",
       {
        "Ref": "AWS::StackName"
       }
      ]
     ]
    },
    "GenerateSecretString": {
     "ExcludeCharacters": "\"@/",
     "GenerateStringKey": "password",
     "PasswordLength": 41,
     "SecretStringTemplate": "{\"username\":\"newsinsight\"}"
    },
    "Name": "newsinsight/dev/docdb",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-db-dev/DocDbCluster/Secret/Resource"
   }
  },
  "DocDbClusterSecretAttachment8EB57C05": {
   "Type": "AWS::SecretsManager::SecretTargetAttachment",
   "Properties": {
    "SecretId": {
     "Ref": "DocDbClusterSecretAE05C874"
    },
    "TargetId": {
     "Ref": "DocDbClusterB46CF5D3"
    },
    "TargetType": "AWS::DocDB::DBCluster"
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-db-dev/DocDbCluster/Secret/Attachment/Resource"
   }
  },
  "DocDbClusterB46CF5D3": {
   "Type": "AWS::DocDB::DBCluster",
   "Properties": {
    "BackupRetentionPeriod": 7,
    "DBClusterIdentifier": "newsinsight-docdb-dev",
    "DBSubnetGroupName": {
     "Ref": "DocDbClusterSubnetsBFD1BEB3"
    },
    "DeletionProtection": false,
    "MasterUserPassword": {
     "Fn::Join": [
      "",
      [
       "{{resolve:secretsmanager:",
       {
        "Ref": "DocDbClusterSecretAE05C874"
       },
       ":SecretString:password::}}"
      ]
     ]
    },
    "MasterUsername": {
     "Fn::Join": [
      "",
      [
       "{{resolve:secretsmanager:",
       {
        "Ref": "DocDbClusterSecretAE05C874"
       },
       ":SecretString:username::}}"
      ]
     ]
    },
    "StorageEncrypted": true,
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "VpcSecurityGroupIds": [
     {
      "Fn::GetAtt": [
       "DbSecurityGroupE9D701AD",
       "GroupId"
      ]
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-db-dev/DocDbCluster/Resource"
   }
  },
  "DocDbClusterInstance13FC488BD": {
   "Type": "AWS::DocDB::DBInstance",
   "Properties": {
    "DBClusterIdentifier": {
     "Ref": "DocDbClusterB46CF5D3"
    },
    "DBInstanceClass": "db.t4g.medium",
    "DBInstanceIdentifier": "newsinsight-docdb-devinstance1",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-db-dev/DocDbCluster/Instance1"
   }
  },
  "RedisSubnetGroup": {
   "Type": "AWS::ElastiCache::SubnetGroup",
   "Properties": {
    "CacheSubnetGroupName": "newsinsight-redis-subnet-dev",
    "Description": "Subnet group for Redis",
    "SubnetIds": [
     {
      "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcDatabaseSubnet1Subnet03B7835F8E9E9CB2"
     },
     {
      "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcDatabaseSubnet2SubnetD7C47EB3CCBBAC7C"
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-db-dev/RedisSubnetGroup"
   }
  },
  "RedisCluster": {
   "Type": "AWS::ElastiCache::ReplicationGroup",
   "Properties": {
    "AtRestEncryptionEnabled": true,
    "AuthToken": {
     "Fn::Join": [
      "",
      [
       "{{resolve:secretsmanager:",
       {
        "Ref": "RedisSecretCB3BD971"
       },
       ":SecretString:::}}"
      ]
     ]
    },
    "AutomaticFailoverEnabled": false,
    "CacheNodeType": "cache.t4g.medium",
    "CacheSubnetGroupName": "newsinsight-redis-subnet-dev",
    "Engine": "redis",
    "EngineVersion": "7.0",
    "MultiAZEnabled": false,
    "NumCacheClusters": 1,
    "ReplicationGroupDescription": "newsinsight Redis cluster",
    "ReplicationGroupId": "newsinsight-redis-dev",
    "SecurityGroupIds": [
     {
      "Fn::GetAtt": [
       "DbSecurityGroupE9D701AD",
       "GroupId"
      ]
     }
    ],
    "SnapshotRetentionLimit": 1,
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TransitEncryptionEnabled": true
   },
   "DependsOn": [
    "RedisSubnetGroup"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-db-dev/RedisCluster"
   }
  },
  "CDKMetadata": {
   "Type": "AWS::CDK::Metadata",
   "Properties": {
    "Analytics": "v2:deflate64:H4sIAAAAAAAA/3WQQU/DMAyFf8vuaaDhwBk2Ce2E1HJHrmu2bG1SxQ4TivLfURcGjIqT7edPT882ur6/07crOHGF/bEabKdTK4BHBSd+TYRGp5YwBisfT8HHSa3f3JWQFRMGEh7BwY7CmQ8kX+DclfICYUfyIAK4H8n9Av5usgo967QBgQ6Y1kNkoaDa2DmS7xSbx4VwIc/D1rGAQ8qq99h3S7+FxwX4OeBfSxqAxSLgnnSaD7mO0tA0WASx3pUnZdUQ+xiQ5vVzlClKAYualfM96QPfvBujTa3r1YGtrUJ0YkfSTamfpsAUmbABAAA="
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-db-dev/CDKMetadata/Default"
   }
  }
 },
 "Outputs": {
  "PostgresEndpoint": {
   "Description": "PostgreSQL endpoint",
   "Value": {
    "Fn::GetAtt": [
     "PostgresClusterE64235C6",
     "Endpoint.Address"
    ]
   },
   "Export": {
    "Name": "newsinsight-postgres-endpoint-dev"
   }
  },
  "MongoEndpoint": {
   "Description": "DocumentDB endpoint",
   "Value": {
    "Fn::GetAtt": [
     "DocDbClusterB46CF5D3",
     "Endpoint"
    ]
   },
   "Export": {
    "Name": "newsinsight-mongo-endpoint-dev"
   }
  },
  "RedisEndpoint": {
   "Description": "Redis endpoint",
   "Value": {
    "Fn::GetAtt": [
     "RedisCluster",
     "PrimaryEndPoint.Address"
    ]
   },
   "Export": {
    "Name": "newsinsight-redis-endpoint-dev"
   }
  },
  "PostgresSecretArn": {
   "Description": "PostgreSQL secret ARN",
   "Value": {
    "Ref": "PostgresSecretE142E1BC"
   },
   "Export": {
    "Name": "newsinsight-postgres-secret-dev"
   }
  },
  "MongoSecretArn": {
   "Description": "MongoDB secret ARN",
   "Value": {
    "Ref": "MongoSecretEC0FA3F4"
   },
   "Export": {
    "Name": "newsinsight-mongo-secret-dev"
   }
  },
  "RedisSecretArn": {
   "Description": "Redis secret ARN",
   "Value": {
    "Ref": "RedisSecretCB3BD971"
   },
   "Export": {
    "Name": "newsinsight-redis-secret-dev"
   }
  },
  "ExportsOutputRefPostgresSecretE142E1BCDFDB523F": {
   "Value": {
    "Ref": "PostgresSecretE142E1BC"
   },
   "Export": {
    "Name": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
   }
  },
  "ExportsOutputRefMongoSecretEC0FA3F415029244": {
   "Value": {
    "Ref": "MongoSecretEC0FA3F4"
   },
   "Export": {
    "Name": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
   }
  },
  "ExportsOutputRefRedisSecretCB3BD9716380542C": {
   "Value": {
    "Ref": "RedisSecretCB3BD971"
   },
   "Export": {
    "Name": "newsinsight-db-dev:ExportsOutputRefRedisSecretCB3BD9716380542C"
   }
  },
  "ExportsOutputFnGetAttPostgresClusterE64235C6EndpointAddress8684A102": {
   "Value": {
    "Fn::GetAtt": [
     "PostgresClusterE64235C6",
     "Endpoint.Address"
    ]
   },
   "Export": {
    "Name": "newsinsight-db-dev:ExportsOutputFnGetAttPostgresClusterE64235C6EndpointAddress8684A102"
   }
  },
  "ExportsOutputFnGetAttDocDbClusterB46CF5D3EndpointD6B42638": {
   "Value": {
    "Fn::GetAtt": [
     "DocDbClusterB46CF5D3",
     "Endpoint"
    ]
   },
   "Export": {
    "Name": "newsinsight-db-dev:ExportsOutputFnGetAttDocDbClusterB46CF5D3EndpointD6B42638"
   }
  },
  "ExportsOutputFnGetAttRedisClusterPrimaryEndPointAddressCFB16E1D": {
   "Value": {
    "Fn::GetAtt": [
     "RedisCluster",
     "PrimaryEndPoint.Address"
    ]
   },
   "Export": {
    "Name": "newsinsight-db-dev:ExportsOutputFnGetAttRedisClusterPrimaryEndPointAddressCFB16E1D"
   }
  }
 },
 "Parameters": {
  "BootstrapVersion": {
   "Type": "AWS::SSM::Parameter::Value<String>",
   "Default": "/cdk-bootstrap/hnb659fds/version",
   "Description": "Version of the CDK Bootstrap resources in this environment, automatically retrieved from SSM Parameter Store. [cdk:skip]"
  }
 },
 "Rules": {
  "CheckBootstrapVersion": {
   "Assertions": [
    {
     "Assert": {
      "Fn::Not": [
       {
        "Fn::Contains": [
         [
          "1",
          "2",
          "3",
          "4",
          "5"
         ],
         {
          "Ref": "BootstrapVersion"
         }
        ]
       }
      ]
     },
     "AssertDescription": "CDK bootstrap stack version 6 required. Please run 'cdk bootstrap' with a recent version of the CDK CLI."
    }
   ]
  }
 }
}
```

---

## aws/cdk/cdk.out/newsinsight-ec2-dev.assets.json

```json
{
  "version": "38.0.1",
  "files": {
    "f252d68ab43fad345e92d9560c652a14eeac8575bddde87763a7d664c908c8d6": {
      "source": {
        "path": "newsinsight-ec2-dev.template.json",
        "packaging": "file"
      },
      "destinations": {
        "130954244737-ap-northeast-2": {
          "bucketName": "cdk-hnb659fds-assets-130954244737-ap-northeast-2",
          "objectKey": "f252d68ab43fad345e92d9560c652a14eeac8575bddde87763a7d664c908c8d6.json",
          "region": "ap-northeast-2",
          "assumeRoleArn": "arn:${AWS::Partition}:iam::130954244737:role/cdk-hnb659fds-file-publishing-role-130954244737-ap-northeast-2"
        }
      }
    }
  },
  "dockerImages": {}
}
```

---

## aws/cdk/cdk.out/newsinsight-ec2-dev.template.json

```json
{
 "Resources": {
  "Ec2SecurityGroup55889913": {
   "Type": "AWS::EC2::SecurityGroup",
   "Properties": {
    "GroupDescription": "Security group for NewsInsight EC2 instance",
    "GroupName": "newsinsight-ec2-sg-dev",
    "SecurityGroupEgress": [
     {
      "CidrIp": "0.0.0.0/0",
      "Description": "Allow all outbound traffic by default",
      "IpProtocol": "-1"
     }
    ],
    "SecurityGroupIngress": [
     {
      "CidrIp": "0.0.0.0/0",
      "Description": "SSH access",
      "FromPort": 22,
      "IpProtocol": "tcp",
      "ToPort": 22
     },
     {
      "Description": "Traffic from ALB",
      "FromPort": 8000,
      "IpProtocol": "tcp",
      "SourceSecurityGroupId": {
       "Fn::ImportValue": "newsinsight-alb-dev:ExportsOutputFnGetAttAlbSecurityGroup86A59E99GroupIdE3A37BC7"
      },
      "ToPort": 9000
     },
     {
      "CidrIp": {
       "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputFnGetAttNewsInsightVpc0ED561F3CidrBlock6740B7E1"
      },
      "Description": "Internal VPC traffic",
      "FromPort": 0,
      "IpProtocol": "tcp",
      "ToPort": 65535
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "VpcId": {
     "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpc0ED561F3FDBE1BB1"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ec2-dev/Ec2SecurityGroup/Resource"
   }
  },
  "Ec2Role2FD9A272": {
   "Type": "AWS::IAM::Role",
   "Properties": {
    "AssumeRolePolicyDocument": {
     "Statement": [
      {
       "Action": "sts:AssumeRole",
       "Effect": "Allow",
       "Principal": {
        "Service": "ec2.amazonaws.com"
       }
      }
     ],
     "Version": "2012-10-17"
    },
    "ManagedPolicyArns": [
     {
      "Fn::Join": [
       "",
       [
        "arn:",
        {
         "Ref": "AWS::Partition"
        },
        ":iam::aws:policy/AmazonSSMManagedInstanceCore"
       ]
      ]
     },
     {
      "Fn::Join": [
       "",
       [
        "arn:",
        {
         "Ref": "AWS::Partition"
        },
        ":iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
       ]
      ]
     },
     {
      "Fn::Join": [
       "",
       [
        "arn:",
        {
         "Ref": "AWS::Partition"
        },
        ":iam::aws:policy/CloudWatchAgentServerPolicy"
       ]
      ]
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ec2-dev/Ec2Role/Resource"
   }
  },
  "Ec2RoleDefaultPolicy99BC91F7": {
   "Type": "AWS::IAM::Policy",
   "Properties": {
    "PolicyDocument": {
     "Statement": [
      {
       "Action": [
        "secretsmanager:DescribeSecret",
        "secretsmanager:GetSecretValue"
       ],
       "Effect": "Allow",
       "Resource": [
        {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
        },
        {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
        },
        {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefRedisSecretCB3BD9716380542C"
        }
       ]
      }
     ],
     "Version": "2012-10-17"
    },
    "PolicyName": "Ec2RoleDefaultPolicy99BC91F7",
    "Roles": [
     {
      "Ref": "Ec2Role2FD9A272"
     }
    ]
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ec2-dev/Ec2Role/DefaultPolicy/Resource"
   }
  },
  "KeyPair1622897B": {
   "Type": "AWS::EC2::KeyPair",
   "Properties": {
    "KeyFormat": "pem",
    "KeyName": "newsinsight-keypair-dev",
    "KeyType": "rsa",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ec2-dev/KeyPair/Resource"
   }
  },
  "InstanceInstanceProfileAB5AEF02": {
   "Type": "AWS::IAM::InstanceProfile",
   "Properties": {
    "Roles": [
     {
      "Ref": "Ec2Role2FD9A272"
     }
    ]
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ec2-dev/Instance/InstanceProfile"
   }
  },
  "InstanceC1063A87": {
   "Type": "AWS::EC2::Instance",
   "Properties": {
    "AvailabilityZone": "ap-northeast-2a",
    "BlockDeviceMappings": [
     {
      "DeviceName": "/dev/xvda",
      "Ebs": {
       "Encrypted": true,
       "VolumeSize": 50,
       "VolumeType": "gp3"
      }
     }
    ],
    "IamInstanceProfile": {
     "Ref": "InstanceInstanceProfileAB5AEF02"
    },
    "ImageId": {
     "Ref": "SsmParameterValueawsserviceamiamazonlinuxlatestal2023amikernel61x8664C96584B6F00A464EAD1953AFF4B05118Parameter"
    },
    "InstanceType": "t3.large",
    "KeyName": {
     "Ref": "KeyPair1622897B"
    },
    "SecurityGroupIds": [
     {
      "Fn::GetAtt": [
       "Ec2SecurityGroup55889913",
       "GroupId"
      ]
     }
    ],
    "SubnetId": {
     "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPublicSubnet1SubnetDDF6BE05C056FD30"
    },
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Name",
      "Value": "newsinsight-instance-dev"
     },
     {
      "Key": "Project",
      "Value": "newsinsight"
     }
    ],
    "UserData": {
     "Fn::Base64": {
      "Fn::Join": [
       "",
       [
        "#!/bin/bash\n#!/bin/bash\nset -ex\n\n# System updates\nyum update -y\n\n# Install Docker\nyum install -y docker git\nsystemctl start docker\nsystemctl enable docker\nusermod -aG docker ec2-user\n\n# Install Docker Compose\ncurl -L \"https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\nchmod +x /usr/local/bin/docker-compose\nln -sf /usr/local/bin/docker-compose /usr/bin/docker-compose\n\n# Install AWS CLI v2\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nyum install -y unzip\nunzip -q awscliv2.zip\n./aws/install --update\nrm -rf aws awscliv2.zip\n\n# Create app directory\nmkdir -p /home/ec2-user/newsinsight\nchown ec2-user:ec2-user /home/ec2-user/newsinsight\n\n# Set environment variables\necho 'export POSTGRES_HOST=",
        {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttPostgresClusterE64235C6EndpointAddress8684A102"
        },
        "' >> /etc/profile.d/newsinsight.sh\necho 'export MONGO_HOST=",
        {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttDocDbClusterB46CF5D3EndpointD6B42638"
        },
        "' >> /etc/profile.d/newsinsight.sh\necho 'export REDIS_HOST=",
        {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttRedisClusterPrimaryEndPointAddressCFB16E1D"
        },
        "' >> /etc/profile.d/newsinsight.sh\necho 'export AWS_REGION=ap-northeast-2' >> /etc/profile.d/newsinsight.sh\necho 'export ENVIRONMENT=dev' >> /etc/profile.d/newsinsight.sh\necho 'export ECR_REGISTRY=130954244737.dkr.ecr.ap-northeast-2.amazonaws.com' >> /etc/profile.d/newsinsight.sh\nchmod +x /etc/profile.d/newsinsight.sh\n\n# Login to ECR\naws ecr get-login-password --region ap-northeast-2 | docker login --username AWS --password-stdin 130954244737.dkr.ecr.ap-northeast-2.amazonaws.com\n\n# Create docker-compose.yml\ncat > /home/ec2-user/newsinsight/docker-compose.yml << 'COMPOSE_EOF'\nversion: \"3.8\"\n\nservices:\n  frontend:\n    image: ${ECR_REGISTRY}/newsinsight/frontend:latest\n    ports:\n      - \"8080:8080\"\n    environment:\n      - NODE_ENV=production\n      - API_GATEWAY_URL=http://api-gateway:8000\n    restart: unless-stopped\n\n  api-gateway:\n    image: ${ECR_REGISTRY}/newsinsight/api-gateway:latest\n    ports:\n      - \"8000:8000\"\n    environment:\n      - SPRING_PROFILES_ACTIVE=${ENVIRONMENT}\n      - POSTGRES_HOST=${POSTGRES_HOST}\n      - POSTGRES_PORT=5432\n      - REDIS_HOST=${REDIS_HOST}\n      - REDIS_PORT=6379\n    restart: unless-stopped\n\n  admin-dashboard:\n    image: ${ECR_REGISTRY}/newsinsight/admin-dashboard:latest\n    ports:\n      - \"8888:8888\"\n    environment:\n      - POSTGRES_HOST=${POSTGRES_HOST}\n      - MONGO_HOST=${MONGO_HOST}\n    restart: unless-stopped\n\nnetworks:\n  default:\n    name: newsinsight-network\nCOMPOSE_EOF\n\n# Create .env file\ncat > /home/ec2-user/newsinsight/.env << ENV_EOF\nECR_REGISTRY=130954244737.dkr.ecr.ap-northeast-2.amazonaws.com\nPOSTGRES_HOST=",
        {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttPostgresClusterE64235C6EndpointAddress8684A102"
        },
        "\nMONGO_HOST=",
        {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttDocDbClusterB46CF5D3EndpointD6B42638"
        },
        "\nREDIS_HOST=",
        {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttRedisClusterPrimaryEndPointAddressCFB16E1D"
        },
        "\nENVIRONMENT=dev\nENV_EOF\n\nchown -R ec2-user:ec2-user /home/ec2-user/newsinsight\n\n# Create startup script\ncat > /home/ec2-user/newsinsight/start.sh << 'START_EOF'\n#!/bin/bash\ncd /home/ec2-user/newsinsight\nsource /etc/profile.d/newsinsight.sh\naws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $ECR_REGISTRY\ndocker-compose pull\ndocker-compose up -d\nSTART_EOF\nchmod +x /home/ec2-user/newsinsight/start.sh\nchown ec2-user:ec2-user /home/ec2-user/newsinsight/start.sh\n\necho \"NewsInsight EC2 setup complete!\" > /var/log/newsinsight-setup.log"
       ]
      ]
     }
    }
   },
   "DependsOn": [
    "Ec2RoleDefaultPolicy99BC91F7",
    "Ec2Role2FD9A272"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ec2-dev/Instance/Resource"
   }
  },
  "CDKMetadata": {
   "Type": "AWS::CDK::Metadata",
   "Properties": {
    "Analytics": "v2:deflate64:H4sIAAAAAAAA/1WNzQ6CMBCEn4V7WaUefAAPxniwgQcwtSzJQmlJfySk4d0NIDGeZvbbyQyH4nyCYyZHn6u6yzW9IFVBqo7J0T8TKg6pQhUdhenqbBzYpTH/4I6TkOSWx25vxgdpFC5s9zMj2UMqrV7xqsJqUtNy/tyeF842pHGeWYneRre1CelkjwHXuUcMQwxr2zcyM2NrhNYf3pwDL6DIWk+Uu2gC9Qjlph+lTHbQ9wAAAA=="
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ec2-dev/CDKMetadata/Default"
   }
  }
 },
 "Parameters": {
  "SsmParameterValueawsserviceamiamazonlinuxlatestal2023amikernel61x8664C96584B6F00A464EAD1953AFF4B05118Parameter": {
   "Type": "AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>",
   "Default": "/aws/service/ami-amazon-linux-latest/al2023-ami-kernel-6.1-x86_64"
  },
  "BootstrapVersion": {
   "Type": "AWS::SSM::Parameter::Value<String>",
   "Default": "/cdk-bootstrap/hnb659fds/version",
   "Description": "Version of the CDK Bootstrap resources in this environment, automatically retrieved from SSM Parameter Store. [cdk:skip]"
  }
 },
 "Outputs": {
  "InstanceId": {
   "Description": "EC2 Instance ID",
   "Value": {
    "Ref": "InstanceC1063A87"
   },
   "Export": {
    "Name": "newsinsight-instance-id-dev"
   }
  },
  "InstancePublicIp": {
   "Description": "EC2 Public IP",
   "Value": {
    "Fn::GetAtt": [
     "InstanceC1063A87",
     "PublicIp"
    ]
   },
   "Export": {
    "Name": "newsinsight-instance-ip-dev"
   }
  },
  "InstancePrivateIp": {
   "Description": "EC2 Private IP",
   "Value": {
    "Fn::GetAtt": [
     "InstanceC1063A87",
     "PrivateIp"
    ]
   },
   "Export": {
    "Name": "newsinsight-instance-private-ip-dev"
   }
  },
  "KeyPairId": {
   "Description": "Key Pair ID (retrieve private key from SSM Parameter Store)",
   "Value": {
    "Fn::GetAtt": [
     "KeyPair1622897B",
     "KeyPairId"
    ]
   },
   "Export": {
    "Name": "newsinsight-keypair-id-dev"
   }
  },
  "SSHCommand": {
   "Description": "SSH command template",
   "Value": {
    "Fn::Join": [
     "",
     [
      "ssh -i ",
      {
       "Ref": "KeyPair1622897B"
      },
      ".pem ec2-user@<PUBLIC_IP>"
     ]
    ]
   }
  }
 },
 "Rules": {
  "CheckBootstrapVersion": {
   "Assertions": [
    {
     "Assert": {
      "Fn::Not": [
       {
        "Fn::Contains": [
         [
          "1",
          "2",
          "3",
          "4",
          "5"
         ],
         {
          "Ref": "BootstrapVersion"
         }
        ]
       }
      ]
     },
     "AssertDescription": "CDK bootstrap stack version 6 required. Please run 'cdk bootstrap' with a recent version of the CDK CLI."
    }
   ]
  }
 }
}
```

---

## aws/cdk/cdk.out/newsinsight-ecr-dev.assets.json

```json
{
  "version": "38.0.1",
  "files": {
    "5c4a44e7d19bfc76572765a52fd563db4540e88b4ec2be94bace17144f5d30d3": {
      "source": {
        "path": "newsinsight-ecr-dev.template.json",
        "packaging": "file"
      },
      "destinations": {
        "130954244737-ap-northeast-2": {
          "bucketName": "cdk-hnb659fds-assets-130954244737-ap-northeast-2",
          "objectKey": "5c4a44e7d19bfc76572765a52fd563db4540e88b4ec2be94bace17144f5d30d3.json",
          "region": "ap-northeast-2",
          "assumeRoleArn": "arn:${AWS::Partition}:iam::130954244737:role/cdk-hnb659fds-file-publishing-role-130954244737-ap-northeast-2"
        }
      }
    }
  },
  "dockerImages": {}
}
```

---

## aws/cdk/cdk.out/newsinsight-ecr-dev.template.json

```json
{
 "Resources": {
  "Repofrontend3A01BFBD": {
   "Type": "AWS::ECR::Repository",
   "Properties": {
    "ImageScanningConfiguration": {
     "ScanOnPush": true
    },
    "ImageTagMutability": "MUTABLE",
    "LifecyclePolicy": {
     "LifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
    },
    "RepositoryName": "newsinsight/frontend",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecr-dev/Repo-frontend/Resource"
   }
  },
  "Repoapigateway64782533": {
   "Type": "AWS::ECR::Repository",
   "Properties": {
    "ImageScanningConfiguration": {
     "ScanOnPush": true
    },
    "ImageTagMutability": "MUTABLE",
    "LifecyclePolicy": {
     "LifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
    },
    "RepositoryName": "newsinsight/api-gateway",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecr-dev/Repo-api-gateway/Resource"
   }
  },
  "RepocollectorserviceBD635EBC": {
   "Type": "AWS::ECR::Repository",
   "Properties": {
    "ImageScanningConfiguration": {
     "ScanOnPush": true
    },
    "ImageTagMutability": "MUTABLE",
    "LifecyclePolicy": {
     "LifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
    },
    "RepositoryName": "newsinsight/collector-service",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecr-dev/Repo-collector-service/Resource"
   }
  },
  "Repoautonomouscrawler2C28B095": {
   "Type": "AWS::ECR::Repository",
   "Properties": {
    "ImageScanningConfiguration": {
     "ScanOnPush": true
    },
    "ImageTagMutability": "MUTABLE",
    "LifecyclePolicy": {
     "LifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
    },
    "RepositoryName": "newsinsight/autonomous-crawler",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecr-dev/Repo-autonomous-crawler/Resource"
   }
  },
  "Repobrowseruseapi89FF3D9A": {
   "Type": "AWS::ECR::Repository",
   "Properties": {
    "ImageScanningConfiguration": {
     "ScanOnPush": true
    },
    "ImageTagMutability": "MUTABLE",
    "LifecyclePolicy": {
     "LifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
    },
    "RepositoryName": "newsinsight/browser-use-api",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecr-dev/Repo-browser-use-api/Resource"
   }
  },
  "Repoadmindashboard8DB8FD3C": {
   "Type": "AWS::ECR::Repository",
   "Properties": {
    "ImageScanningConfiguration": {
     "ScanOnPush": true
    },
    "ImageTagMutability": "MUTABLE",
    "LifecyclePolicy": {
     "LifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
    },
    "RepositoryName": "newsinsight/admin-dashboard",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecr-dev/Repo-admin-dashboard/Resource"
   }
  },
  "Repoaiagentworker45974E37": {
   "Type": "AWS::ECR::Repository",
   "Properties": {
    "ImageScanningConfiguration": {
     "ScanOnPush": true
    },
    "ImageTagMutability": "MUTABLE",
    "LifecyclePolicy": {
     "LifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
    },
    "RepositoryName": "newsinsight/ai-agent-worker",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecr-dev/Repo-ai-agent-worker/Resource"
   }
  },
  "Repowebcrawler9872983B": {
   "Type": "AWS::ECR::Repository",
   "Properties": {
    "ImageScanningConfiguration": {
     "ScanOnPush": true
    },
    "ImageTagMutability": "MUTABLE",
    "LifecyclePolicy": {
     "LifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
    },
    "RepositoryName": "newsinsight/web-crawler",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecr-dev/Repo-web-crawler/Resource"
   }
  },
  "Repoembeddingserver9F2A1DDB": {
   "Type": "AWS::ECR::Repository",
   "Properties": {
    "ImageScanningConfiguration": {
     "ScanOnPush": true
    },
    "ImageTagMutability": "MUTABLE",
    "LifecyclePolicy": {
     "LifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
    },
    "RepositoryName": "newsinsight/embedding-server",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecr-dev/Repo-embedding-server/Resource"
   }
  },
  "Repoiprotation8F90C250": {
   "Type": "AWS::ECR::Repository",
   "Properties": {
    "ImageScanningConfiguration": {
     "ScanOnPush": true
    },
    "ImageTagMutability": "MUTABLE",
    "LifecyclePolicy": {
     "LifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
    },
    "RepositoryName": "newsinsight/ip-rotation",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecr-dev/Repo-ip-rotation/Resource"
   }
  },
  "Repocrawlworker15CEF027": {
   "Type": "AWS::ECR::Repository",
   "Properties": {
    "ImageScanningConfiguration": {
     "ScanOnPush": true
    },
    "ImageTagMutability": "MUTABLE",
    "LifecyclePolicy": {
     "LifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
    },
    "RepositoryName": "newsinsight/crawl-worker",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecr-dev/Repo-crawl-worker/Resource"
   }
  },
  "Repomaigretworker1D1A79A5": {
   "Type": "AWS::ECR::Repository",
   "Properties": {
    "ImageScanningConfiguration": {
     "ScanOnPush": true
    },
    "ImageTagMutability": "MUTABLE",
    "LifecyclePolicy": {
     "LifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
    },
    "RepositoryName": "newsinsight/maigret-worker",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecr-dev/Repo-maigret-worker/Resource"
   }
  },
  "RepobotdetectorD791269A": {
   "Type": "AWS::ECR::Repository",
   "Properties": {
    "ImageScanningConfiguration": {
     "ScanOnPush": true
    },
    "ImageTagMutability": "MUTABLE",
    "LifecyclePolicy": {
     "LifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
    },
    "RepositoryName": "newsinsight/bot-detector",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecr-dev/Repo-bot-detector/Resource"
   }
  },
  "Reposentimentaddon60A21608": {
   "Type": "AWS::ECR::Repository",
   "Properties": {
    "ImageScanningConfiguration": {
     "ScanOnPush": true
    },
    "ImageTagMutability": "MUTABLE",
    "LifecyclePolicy": {
     "LifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
    },
    "RepositoryName": "newsinsight/sentiment-addon",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecr-dev/Repo-sentiment-addon/Resource"
   }
  },
  "RepofactcheckaddonCA82F5DC": {
   "Type": "AWS::ECR::Repository",
   "Properties": {
    "ImageScanningConfiguration": {
     "ScanOnPush": true
    },
    "ImageTagMutability": "MUTABLE",
    "LifecyclePolicy": {
     "LifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
    },
    "RepositoryName": "newsinsight/factcheck-addon",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecr-dev/Repo-factcheck-addon/Resource"
   }
  },
  "RepobiasaddonDDD4DB8B": {
   "Type": "AWS::ECR::Repository",
   "Properties": {
    "ImageScanningConfiguration": {
     "ScanOnPush": true
    },
    "ImageTagMutability": "MUTABLE",
    "LifecyclePolicy": {
     "LifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
    },
    "RepositoryName": "newsinsight/bias-addon",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecr-dev/Repo-bias-addon/Resource"
   }
  },
  "Reponewsinsightmcp5EF0CA8C": {
   "Type": "AWS::ECR::Repository",
   "Properties": {
    "ImageScanningConfiguration": {
     "ScanOnPush": true
    },
    "ImageTagMutability": "MUTABLE",
    "LifecyclePolicy": {
     "LifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
    },
    "RepositoryName": "newsinsight/newsinsight-mcp",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecr-dev/Repo-newsinsight-mcp/Resource"
   }
  },
  "Repobiasmcp676781E9": {
   "Type": "AWS::ECR::Repository",
   "Properties": {
    "ImageScanningConfiguration": {
     "ScanOnPush": true
    },
    "ImageTagMutability": "MUTABLE",
    "LifecyclePolicy": {
     "LifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
    },
    "RepositoryName": "newsinsight/bias-mcp",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecr-dev/Repo-bias-mcp/Resource"
   }
  },
  "Repofactcheckmcp196A0DE5": {
   "Type": "AWS::ECR::Repository",
   "Properties": {
    "ImageScanningConfiguration": {
     "ScanOnPush": true
    },
    "ImageTagMutability": "MUTABLE",
    "LifecyclePolicy": {
     "LifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
    },
    "RepositoryName": "newsinsight/factcheck-mcp",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecr-dev/Repo-factcheck-mcp/Resource"
   }
  },
  "RepotopicmcpE31160AA": {
   "Type": "AWS::ECR::Repository",
   "Properties": {
    "ImageScanningConfiguration": {
     "ScanOnPush": true
    },
    "ImageTagMutability": "MUTABLE",
    "LifecyclePolicy": {
     "LifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
    },
    "RepositoryName": "newsinsight/topic-mcp",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecr-dev/Repo-topic-mcp/Resource"
   }
  },
  "Repoaiagentmcp0F725DDB": {
   "Type": "AWS::ECR::Repository",
   "Properties": {
    "ImageScanningConfiguration": {
     "ScanOnPush": true
    },
    "ImageTagMutability": "MUTABLE",
    "LifecyclePolicy": {
     "LifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
    },
    "RepositoryName": "newsinsight/aiagent-mcp",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecr-dev/Repo-aiagent-mcp/Resource"
   }
  },
  "Reporoboflowmcp13A48D96": {
   "Type": "AWS::ECR::Repository",
   "Properties": {
    "ImageScanningConfiguration": {
     "ScanOnPush": true
    },
    "ImageTagMutability": "MUTABLE",
    "LifecyclePolicy": {
     "LifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
    },
    "RepositoryName": "newsinsight/roboflow-mcp",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecr-dev/Repo-roboflow-mcp/Resource"
   }
  },
  "Repohuggingfacemcp583F3940": {
   "Type": "AWS::ECR::Repository",
   "Properties": {
    "ImageScanningConfiguration": {
     "ScanOnPush": true
    },
    "ImageTagMutability": "MUTABLE",
    "LifecyclePolicy": {
     "LifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
    },
    "RepositoryName": "newsinsight/huggingface-mcp",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecr-dev/Repo-huggingface-mcp/Resource"
   }
  },
  "Repokagglemcp26CB5139": {
   "Type": "AWS::ECR::Repository",
   "Properties": {
    "ImageScanningConfiguration": {
     "ScanOnPush": true
    },
    "ImageTagMutability": "MUTABLE",
    "LifecyclePolicy": {
     "LifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
    },
    "RepositoryName": "newsinsight/kaggle-mcp",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecr-dev/Repo-kaggle-mcp/Resource"
   }
  },
  "RepomltrainingmcpBCB2A52B": {
   "Type": "AWS::ECR::Repository",
   "Properties": {
    "ImageScanningConfiguration": {
     "ScanOnPush": true
    },
    "ImageTagMutability": "MUTABLE",
    "LifecyclePolicy": {
     "LifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
    },
    "RepositoryName": "newsinsight/mltraining-mcp",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecr-dev/Repo-mltraining-mcp/Resource"
   }
  },
  "CDKMetadata": {
   "Type": "AWS::CDK::Metadata",
   "Properties": {
    "Analytics": "v2:deflate64:H4sIAAAAAAAA/zPSMzQ31jNQTCwv1k1OydbNyUzSqw4uSUzO1kksL45PTS7Sqw5KLcgvzizJL6rUcU7LQ/BqQVz/0pKC0hKIRHF+aVFyaq1OXn5Kql5WsX6ZkZGekaGeoWJWcWamblFpXklmbqpeEIQGALO922l6AAAA"
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecr-dev/CDKMetadata/Default"
   }
  }
 },
 "Outputs": {
  "RepoUrifrontend": {
   "Description": "ECR URI for frontend",
   "Value": {
    "Fn::Join": [
     "",
     [
      {
       "Fn::Select": [
        4,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repofrontend3A01BFBD",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".dkr.ecr.",
      {
       "Fn::Select": [
        3,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repofrontend3A01BFBD",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".",
      {
       "Ref": "AWS::URLSuffix"
      },
      "/",
      {
       "Ref": "Repofrontend3A01BFBD"
      }
     ]
    ]
   },
   "Export": {
    "Name": "newsinsight-ecr-frontend-dev"
   }
  },
  "RepoUriapigateway": {
   "Description": "ECR URI for api-gateway",
   "Value": {
    "Fn::Join": [
     "",
     [
      {
       "Fn::Select": [
        4,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repoapigateway64782533",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".dkr.ecr.",
      {
       "Fn::Select": [
        3,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repoapigateway64782533",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".",
      {
       "Ref": "AWS::URLSuffix"
      },
      "/",
      {
       "Ref": "Repoapigateway64782533"
      }
     ]
    ]
   },
   "Export": {
    "Name": "newsinsight-ecr-api-gateway-dev"
   }
  },
  "RepoUricollectorservice": {
   "Description": "ECR URI for collector-service",
   "Value": {
    "Fn::Join": [
     "",
     [
      {
       "Fn::Select": [
        4,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "RepocollectorserviceBD635EBC",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".dkr.ecr.",
      {
       "Fn::Select": [
        3,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "RepocollectorserviceBD635EBC",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".",
      {
       "Ref": "AWS::URLSuffix"
      },
      "/",
      {
       "Ref": "RepocollectorserviceBD635EBC"
      }
     ]
    ]
   },
   "Export": {
    "Name": "newsinsight-ecr-collector-service-dev"
   }
  },
  "RepoUriautonomouscrawler": {
   "Description": "ECR URI for autonomous-crawler",
   "Value": {
    "Fn::Join": [
     "",
     [
      {
       "Fn::Select": [
        4,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repoautonomouscrawler2C28B095",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".dkr.ecr.",
      {
       "Fn::Select": [
        3,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repoautonomouscrawler2C28B095",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".",
      {
       "Ref": "AWS::URLSuffix"
      },
      "/",
      {
       "Ref": "Repoautonomouscrawler2C28B095"
      }
     ]
    ]
   },
   "Export": {
    "Name": "newsinsight-ecr-autonomous-crawler-dev"
   }
  },
  "RepoUribrowseruseapi": {
   "Description": "ECR URI for browser-use-api",
   "Value": {
    "Fn::Join": [
     "",
     [
      {
       "Fn::Select": [
        4,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repobrowseruseapi89FF3D9A",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".dkr.ecr.",
      {
       "Fn::Select": [
        3,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repobrowseruseapi89FF3D9A",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".",
      {
       "Ref": "AWS::URLSuffix"
      },
      "/",
      {
       "Ref": "Repobrowseruseapi89FF3D9A"
      }
     ]
    ]
   },
   "Export": {
    "Name": "newsinsight-ecr-browser-use-api-dev"
   }
  },
  "RepoUriadmindashboard": {
   "Description": "ECR URI for admin-dashboard",
   "Value": {
    "Fn::Join": [
     "",
     [
      {
       "Fn::Select": [
        4,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repoadmindashboard8DB8FD3C",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".dkr.ecr.",
      {
       "Fn::Select": [
        3,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repoadmindashboard8DB8FD3C",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".",
      {
       "Ref": "AWS::URLSuffix"
      },
      "/",
      {
       "Ref": "Repoadmindashboard8DB8FD3C"
      }
     ]
    ]
   },
   "Export": {
    "Name": "newsinsight-ecr-admin-dashboard-dev"
   }
  },
  "RepoUriaiagentworker": {
   "Description": "ECR URI for ai-agent-worker",
   "Value": {
    "Fn::Join": [
     "",
     [
      {
       "Fn::Select": [
        4,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repoaiagentworker45974E37",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".dkr.ecr.",
      {
       "Fn::Select": [
        3,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repoaiagentworker45974E37",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".",
      {
       "Ref": "AWS::URLSuffix"
      },
      "/",
      {
       "Ref": "Repoaiagentworker45974E37"
      }
     ]
    ]
   },
   "Export": {
    "Name": "newsinsight-ecr-ai-agent-worker-dev"
   }
  },
  "RepoUriwebcrawler": {
   "Description": "ECR URI for web-crawler",
   "Value": {
    "Fn::Join": [
     "",
     [
      {
       "Fn::Select": [
        4,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repowebcrawler9872983B",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".dkr.ecr.",
      {
       "Fn::Select": [
        3,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repowebcrawler9872983B",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".",
      {
       "Ref": "AWS::URLSuffix"
      },
      "/",
      {
       "Ref": "Repowebcrawler9872983B"
      }
     ]
    ]
   },
   "Export": {
    "Name": "newsinsight-ecr-web-crawler-dev"
   }
  },
  "RepoUriembeddingserver": {
   "Description": "ECR URI for embedding-server",
   "Value": {
    "Fn::Join": [
     "",
     [
      {
       "Fn::Select": [
        4,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repoembeddingserver9F2A1DDB",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".dkr.ecr.",
      {
       "Fn::Select": [
        3,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repoembeddingserver9F2A1DDB",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".",
      {
       "Ref": "AWS::URLSuffix"
      },
      "/",
      {
       "Ref": "Repoembeddingserver9F2A1DDB"
      }
     ]
    ]
   },
   "Export": {
    "Name": "newsinsight-ecr-embedding-server-dev"
   }
  },
  "RepoUriiprotation": {
   "Description": "ECR URI for ip-rotation",
   "Value": {
    "Fn::Join": [
     "",
     [
      {
       "Fn::Select": [
        4,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repoiprotation8F90C250",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".dkr.ecr.",
      {
       "Fn::Select": [
        3,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repoiprotation8F90C250",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".",
      {
       "Ref": "AWS::URLSuffix"
      },
      "/",
      {
       "Ref": "Repoiprotation8F90C250"
      }
     ]
    ]
   },
   "Export": {
    "Name": "newsinsight-ecr-ip-rotation-dev"
   }
  },
  "RepoUricrawlworker": {
   "Description": "ECR URI for crawl-worker",
   "Value": {
    "Fn::Join": [
     "",
     [
      {
       "Fn::Select": [
        4,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repocrawlworker15CEF027",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".dkr.ecr.",
      {
       "Fn::Select": [
        3,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repocrawlworker15CEF027",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".",
      {
       "Ref": "AWS::URLSuffix"
      },
      "/",
      {
       "Ref": "Repocrawlworker15CEF027"
      }
     ]
    ]
   },
   "Export": {
    "Name": "newsinsight-ecr-crawl-worker-dev"
   }
  },
  "RepoUrimaigretworker": {
   "Description": "ECR URI for maigret-worker",
   "Value": {
    "Fn::Join": [
     "",
     [
      {
       "Fn::Select": [
        4,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repomaigretworker1D1A79A5",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".dkr.ecr.",
      {
       "Fn::Select": [
        3,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repomaigretworker1D1A79A5",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".",
      {
       "Ref": "AWS::URLSuffix"
      },
      "/",
      {
       "Ref": "Repomaigretworker1D1A79A5"
      }
     ]
    ]
   },
   "Export": {
    "Name": "newsinsight-ecr-maigret-worker-dev"
   }
  },
  "RepoUribotdetector": {
   "Description": "ECR URI for bot-detector",
   "Value": {
    "Fn::Join": [
     "",
     [
      {
       "Fn::Select": [
        4,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "RepobotdetectorD791269A",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".dkr.ecr.",
      {
       "Fn::Select": [
        3,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "RepobotdetectorD791269A",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".",
      {
       "Ref": "AWS::URLSuffix"
      },
      "/",
      {
       "Ref": "RepobotdetectorD791269A"
      }
     ]
    ]
   },
   "Export": {
    "Name": "newsinsight-ecr-bot-detector-dev"
   }
  },
  "RepoUrisentimentaddon": {
   "Description": "ECR URI for sentiment-addon",
   "Value": {
    "Fn::Join": [
     "",
     [
      {
       "Fn::Select": [
        4,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Reposentimentaddon60A21608",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".dkr.ecr.",
      {
       "Fn::Select": [
        3,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Reposentimentaddon60A21608",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".",
      {
       "Ref": "AWS::URLSuffix"
      },
      "/",
      {
       "Ref": "Reposentimentaddon60A21608"
      }
     ]
    ]
   },
   "Export": {
    "Name": "newsinsight-ecr-sentiment-addon-dev"
   }
  },
  "RepoUrifactcheckaddon": {
   "Description": "ECR URI for factcheck-addon",
   "Value": {
    "Fn::Join": [
     "",
     [
      {
       "Fn::Select": [
        4,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "RepofactcheckaddonCA82F5DC",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".dkr.ecr.",
      {
       "Fn::Select": [
        3,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "RepofactcheckaddonCA82F5DC",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".",
      {
       "Ref": "AWS::URLSuffix"
      },
      "/",
      {
       "Ref": "RepofactcheckaddonCA82F5DC"
      }
     ]
    ]
   },
   "Export": {
    "Name": "newsinsight-ecr-factcheck-addon-dev"
   }
  },
  "RepoUribiasaddon": {
   "Description": "ECR URI for bias-addon",
   "Value": {
    "Fn::Join": [
     "",
     [
      {
       "Fn::Select": [
        4,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "RepobiasaddonDDD4DB8B",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".dkr.ecr.",
      {
       "Fn::Select": [
        3,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "RepobiasaddonDDD4DB8B",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".",
      {
       "Ref": "AWS::URLSuffix"
      },
      "/",
      {
       "Ref": "RepobiasaddonDDD4DB8B"
      }
     ]
    ]
   },
   "Export": {
    "Name": "newsinsight-ecr-bias-addon-dev"
   }
  },
  "RepoUrinewsinsightmcp": {
   "Description": "ECR URI for newsinsight-mcp",
   "Value": {
    "Fn::Join": [
     "",
     [
      {
       "Fn::Select": [
        4,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Reponewsinsightmcp5EF0CA8C",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".dkr.ecr.",
      {
       "Fn::Select": [
        3,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Reponewsinsightmcp5EF0CA8C",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".",
      {
       "Ref": "AWS::URLSuffix"
      },
      "/",
      {
       "Ref": "Reponewsinsightmcp5EF0CA8C"
      }
     ]
    ]
   },
   "Export": {
    "Name": "newsinsight-ecr-newsinsight-mcp-dev"
   }
  },
  "RepoUribiasmcp": {
   "Description": "ECR URI for bias-mcp",
   "Value": {
    "Fn::Join": [
     "",
     [
      {
       "Fn::Select": [
        4,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repobiasmcp676781E9",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".dkr.ecr.",
      {
       "Fn::Select": [
        3,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repobiasmcp676781E9",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".",
      {
       "Ref": "AWS::URLSuffix"
      },
      "/",
      {
       "Ref": "Repobiasmcp676781E9"
      }
     ]
    ]
   },
   "Export": {
    "Name": "newsinsight-ecr-bias-mcp-dev"
   }
  },
  "RepoUrifactcheckmcp": {
   "Description": "ECR URI for factcheck-mcp",
   "Value": {
    "Fn::Join": [
     "",
     [
      {
       "Fn::Select": [
        4,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repofactcheckmcp196A0DE5",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".dkr.ecr.",
      {
       "Fn::Select": [
        3,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repofactcheckmcp196A0DE5",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".",
      {
       "Ref": "AWS::URLSuffix"
      },
      "/",
      {
       "Ref": "Repofactcheckmcp196A0DE5"
      }
     ]
    ]
   },
   "Export": {
    "Name": "newsinsight-ecr-factcheck-mcp-dev"
   }
  },
  "RepoUritopicmcp": {
   "Description": "ECR URI for topic-mcp",
   "Value": {
    "Fn::Join": [
     "",
     [
      {
       "Fn::Select": [
        4,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "RepotopicmcpE31160AA",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".dkr.ecr.",
      {
       "Fn::Select": [
        3,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "RepotopicmcpE31160AA",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".",
      {
       "Ref": "AWS::URLSuffix"
      },
      "/",
      {
       "Ref": "RepotopicmcpE31160AA"
      }
     ]
    ]
   },
   "Export": {
    "Name": "newsinsight-ecr-topic-mcp-dev"
   }
  },
  "RepoUriaiagentmcp": {
   "Description": "ECR URI for aiagent-mcp",
   "Value": {
    "Fn::Join": [
     "",
     [
      {
       "Fn::Select": [
        4,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repoaiagentmcp0F725DDB",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".dkr.ecr.",
      {
       "Fn::Select": [
        3,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repoaiagentmcp0F725DDB",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".",
      {
       "Ref": "AWS::URLSuffix"
      },
      "/",
      {
       "Ref": "Repoaiagentmcp0F725DDB"
      }
     ]
    ]
   },
   "Export": {
    "Name": "newsinsight-ecr-aiagent-mcp-dev"
   }
  },
  "RepoUriroboflowmcp": {
   "Description": "ECR URI for roboflow-mcp",
   "Value": {
    "Fn::Join": [
     "",
     [
      {
       "Fn::Select": [
        4,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Reporoboflowmcp13A48D96",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".dkr.ecr.",
      {
       "Fn::Select": [
        3,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Reporoboflowmcp13A48D96",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".",
      {
       "Ref": "AWS::URLSuffix"
      },
      "/",
      {
       "Ref": "Reporoboflowmcp13A48D96"
      }
     ]
    ]
   },
   "Export": {
    "Name": "newsinsight-ecr-roboflow-mcp-dev"
   }
  },
  "RepoUrihuggingfacemcp": {
   "Description": "ECR URI for huggingface-mcp",
   "Value": {
    "Fn::Join": [
     "",
     [
      {
       "Fn::Select": [
        4,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repohuggingfacemcp583F3940",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".dkr.ecr.",
      {
       "Fn::Select": [
        3,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repohuggingfacemcp583F3940",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".",
      {
       "Ref": "AWS::URLSuffix"
      },
      "/",
      {
       "Ref": "Repohuggingfacemcp583F3940"
      }
     ]
    ]
   },
   "Export": {
    "Name": "newsinsight-ecr-huggingface-mcp-dev"
   }
  },
  "RepoUrikagglemcp": {
   "Description": "ECR URI for kaggle-mcp",
   "Value": {
    "Fn::Join": [
     "",
     [
      {
       "Fn::Select": [
        4,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repokagglemcp26CB5139",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".dkr.ecr.",
      {
       "Fn::Select": [
        3,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "Repokagglemcp26CB5139",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".",
      {
       "Ref": "AWS::URLSuffix"
      },
      "/",
      {
       "Ref": "Repokagglemcp26CB5139"
      }
     ]
    ]
   },
   "Export": {
    "Name": "newsinsight-ecr-kaggle-mcp-dev"
   }
  },
  "RepoUrimltrainingmcp": {
   "Description": "ECR URI for mltraining-mcp",
   "Value": {
    "Fn::Join": [
     "",
     [
      {
       "Fn::Select": [
        4,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "RepomltrainingmcpBCB2A52B",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".dkr.ecr.",
      {
       "Fn::Select": [
        3,
        {
         "Fn::Split": [
          ":",
          {
           "Fn::GetAtt": [
            "RepomltrainingmcpBCB2A52B",
            "Arn"
           ]
          }
         ]
        }
       ]
      },
      ".",
      {
       "Ref": "AWS::URLSuffix"
      },
      "/",
      {
       "Ref": "RepomltrainingmcpBCB2A52B"
      }
     ]
    ]
   },
   "Export": {
    "Name": "newsinsight-ecr-mltraining-mcp-dev"
   }
  }
 },
 "Parameters": {
  "BootstrapVersion": {
   "Type": "AWS::SSM::Parameter::Value<String>",
   "Default": "/cdk-bootstrap/hnb659fds/version",
   "Description": "Version of the CDK Bootstrap resources in this environment, automatically retrieved from SSM Parameter Store. [cdk:skip]"
  }
 },
 "Rules": {
  "CheckBootstrapVersion": {
   "Assertions": [
    {
     "Assert": {
      "Fn::Not": [
       {
        "Fn::Contains": [
         [
          "1",
          "2",
          "3",
          "4",
          "5"
         ],
         {
          "Ref": "BootstrapVersion"
         }
        ]
       }
      ]
     },
     "AssertDescription": "CDK bootstrap stack version 6 required. Please run 'cdk bootstrap' with a recent version of the CDK CLI."
    }
   ]
  }
 }
}
```

---

## aws/cdk/cdk.out/newsinsight-ecs-dev.assets.json

```json
{
  "version": "38.0.1",
  "files": {
    "8ec1424559237f319dd80fc27bcddcfc77e116587821d900d3894c776b5098af": {
      "source": {
        "path": "newsinsight-ecs-dev.template.json",
        "packaging": "file"
      },
      "destinations": {
        "130954244737-ap-northeast-2": {
          "bucketName": "cdk-hnb659fds-assets-130954244737-ap-northeast-2",
          "objectKey": "8ec1424559237f319dd80fc27bcddcfc77e116587821d900d3894c776b5098af.json",
          "region": "ap-northeast-2",
          "assumeRoleArn": "arn:${AWS::Partition}:iam::130954244737:role/cdk-hnb659fds-file-publishing-role-130954244737-ap-northeast-2"
        }
      }
    }
  },
  "dockerImages": {}
}
```

---

## aws/cdk/cdk.out/newsinsight-ecs-dev.template.json

```json
{
 "Resources": {
  "ClusterEB0386A7": {
   "Type": "AWS::ECS::Cluster",
   "Properties": {
    "ClusterName": "newsinsight-cluster-dev",
    "ClusterSettings": [
     {
      "Name": "containerInsights",
      "Value": "disabled"
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Cluster/Resource"
   }
  },
  "Namespace9B63B8C8": {
   "Type": "AWS::ServiceDiscovery::PrivateDnsNamespace",
   "Properties": {
    "Description": "NewsInsight internal service discovery",
    "Name": "newsinsight.local",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "Vpc": {
     "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpc0ED561F3FDBE1BB1"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Namespace/Resource"
   }
  },
  "ServiceSg2632F788": {
   "Type": "AWS::EC2::SecurityGroup",
   "Properties": {
    "GroupDescription": "Security group for ECS services",
    "GroupName": "newsinsight-service-sg-dev",
    "SecurityGroupEgress": [
     {
      "CidrIp": "0.0.0.0/0",
      "Description": "Allow all outbound traffic by default",
      "IpProtocol": "-1"
     }
    ],
    "SecurityGroupIngress": [
     {
      "CidrIp": {
       "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputFnGetAttNewsInsightVpc0ED561F3CidrBlock6740B7E1"
      },
      "Description": "Internal VPC traffic",
      "FromPort": 0,
      "IpProtocol": "tcp",
      "ToPort": 65535
     },
     {
      "Description": "Traffic from ALB",
      "FromPort": 0,
      "IpProtocol": "tcp",
      "SourceSecurityGroupId": {
       "Fn::ImportValue": "newsinsight-alb-dev:ExportsOutputFnGetAttAlbSecurityGroup86A59E99GroupIdE3A37BC7"
      },
      "ToPort": 65535
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "VpcId": {
     "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpc0ED561F3FDBE1BB1"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/ServiceSg/Resource"
   }
  },
  "ServiceSgfromnewsinsightalbdevAlbSecurityGroup64D24E92808094F9DB03": {
   "Type": "AWS::EC2::SecurityGroupIngress",
   "Properties": {
    "Description": "Load balancer to target",
    "FromPort": 8080,
    "GroupId": {
     "Fn::GetAtt": [
      "ServiceSg2632F788",
      "GroupId"
     ]
    },
    "IpProtocol": "tcp",
    "SourceSecurityGroupId": {
     "Fn::ImportValue": "newsinsight-alb-dev:ExportsOutputFnGetAttAlbSecurityGroup86A59E99GroupIdE3A37BC7"
    },
    "ToPort": 8080
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/ServiceSg/from newsinsightalbdevAlbSecurityGroup64D24E92:8080"
   }
  },
  "ServiceSgfromnewsinsightalbdevAlbSecurityGroup64D24E928000565C77B4": {
   "Type": "AWS::EC2::SecurityGroupIngress",
   "Properties": {
    "Description": "Load balancer to target",
    "FromPort": 8000,
    "GroupId": {
     "Fn::GetAtt": [
      "ServiceSg2632F788",
      "GroupId"
     ]
    },
    "IpProtocol": "tcp",
    "SourceSecurityGroupId": {
     "Fn::ImportValue": "newsinsight-alb-dev:ExportsOutputFnGetAttAlbSecurityGroup86A59E99GroupIdE3A37BC7"
    },
    "ToPort": 8000
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/ServiceSg/from newsinsightalbdevAlbSecurityGroup64D24E92:8000"
   }
  },
  "ServiceSgfromnewsinsightalbdevAlbSecurityGroup64D24E9288882D631ECA": {
   "Type": "AWS::EC2::SecurityGroupIngress",
   "Properties": {
    "Description": "Load balancer to target",
    "FromPort": 8888,
    "GroupId": {
     "Fn::GetAtt": [
      "ServiceSg2632F788",
      "GroupId"
     ]
    },
    "IpProtocol": "tcp",
    "SourceSecurityGroupId": {
     "Fn::ImportValue": "newsinsight-alb-dev:ExportsOutputFnGetAttAlbSecurityGroup86A59E99GroupIdE3A37BC7"
    },
    "ToPort": 8888
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/ServiceSg/from newsinsightalbdevAlbSecurityGroup64D24E92:8888"
   }
  },
  "ExecutionRole605A040B": {
   "Type": "AWS::IAM::Role",
   "Properties": {
    "AssumeRolePolicyDocument": {
     "Statement": [
      {
       "Action": "sts:AssumeRole",
       "Effect": "Allow",
       "Principal": {
        "Service": "ecs-tasks.amazonaws.com"
       }
      }
     ],
     "Version": "2012-10-17"
    },
    "ManagedPolicyArns": [
     {
      "Fn::Join": [
       "",
       [
        "arn:",
        {
         "Ref": "AWS::Partition"
        },
        ":iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy"
       ]
      ]
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/ExecutionRole/Resource"
   }
  },
  "ExecutionRoleDefaultPolicyA5B92313": {
   "Type": "AWS::IAM::Policy",
   "Properties": {
    "PolicyDocument": {
     "Statement": [
      {
       "Action": [
        "secretsmanager:DescribeSecret",
        "secretsmanager:GetSecretValue"
       ],
       "Effect": "Allow",
       "Resource": [
        {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
        },
        {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
        },
        {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefRedisSecretCB3BD9716380542C"
        }
       ]
      },
      {
       "Action": [
        "ecr:BatchCheckLayerAvailability",
        "ecr:BatchGetImage",
        "ecr:GetDownloadUrlForLayer"
       ],
       "Effect": "Allow",
       "Resource": [
        {
         "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepoadmindashboard8DB8FD3CArnDACF9417"
        },
        {
         "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepoaiagentmcp0F725DDBArn4667959C"
        },
        {
         "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepoapigateway64782533Arn195B7386"
        },
        {
         "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepoautonomouscrawler2C28B095ArnFDDE5673"
        },
        {
         "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepobiasaddonDDD4DB8BArn2D0CFE8D"
        },
        {
         "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepobiasmcp676781E9Arn284502B0"
        },
        {
         "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepobotdetectorD791269AArn20D2BD38"
        },
        {
         "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepobrowseruseapi89FF3D9AArn28ECA372"
        },
        {
         "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepocollectorserviceBD635EBCArn94D07D97"
        },
        {
         "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepoembeddingserver9F2A1DDBArn73189998"
        },
        {
         "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepofactcheckaddonCA82F5DCArn7463D4FC"
        },
        {
         "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepofactcheckmcp196A0DE5Arn5878A1A2"
        },
        {
         "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepofrontend3A01BFBDArn609FC158"
        },
        {
         "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttReponewsinsightmcp5EF0CA8CArnCB03D924"
        },
        {
         "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttReposentimentaddon60A21608ArnE16A9985"
        },
        {
         "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepotopicmcpE31160AAArn2F9AC21C"
        }
       ]
      },
      {
       "Action": "ecr:GetAuthorizationToken",
       "Effect": "Allow",
       "Resource": "*"
      },
      {
       "Action": [
        "logs:CreateLogStream",
        "logs:PutLogEvents"
       ],
       "Effect": "Allow",
       "Resource": [
        {
         "Fn::GetAtt": [
          "LogGroupadmindashboard30E0633B",
          "Arn"
         ]
        },
        {
         "Fn::GetAtt": [
          "LogGroupaiagentmcp22A33121",
          "Arn"
         ]
        },
        {
         "Fn::GetAtt": [
          "LogGroupapigatewayF4404BF5",
          "Arn"
         ]
        },
        {
         "Fn::GetAtt": [
          "LogGroupautonomouscrawler4D362341",
          "Arn"
         ]
        },
        {
         "Fn::GetAtt": [
          "LogGroupbiasaddon2891F993",
          "Arn"
         ]
        },
        {
         "Fn::GetAtt": [
          "LogGroupbiasmcp18501486",
          "Arn"
         ]
        },
        {
         "Fn::GetAtt": [
          "LogGroupbotdetector8EBD76F3",
          "Arn"
         ]
        },
        {
         "Fn::GetAtt": [
          "LogGroupbrowseruseapiC75B0534",
          "Arn"
         ]
        },
        {
         "Fn::GetAtt": [
          "LogGroupcollectorserviceFAE751EF",
          "Arn"
         ]
        },
        {
         "Fn::GetAtt": [
          "LogGroupembeddingserverE55FA114",
          "Arn"
         ]
        },
        {
         "Fn::GetAtt": [
          "LogGroupfactcheckaddonCEF1EF63",
          "Arn"
         ]
        },
        {
         "Fn::GetAtt": [
          "LogGroupfactcheckmcp74C0999F",
          "Arn"
         ]
        },
        {
         "Fn::GetAtt": [
          "LogGroupfrontend7D57FB6C",
          "Arn"
         ]
        },
        {
         "Fn::GetAtt": [
          "LogGroupnewsinsightmcp83D7554A",
          "Arn"
         ]
        },
        {
         "Fn::GetAtt": [
          "LogGroupsentimentaddonFB260201",
          "Arn"
         ]
        },
        {
         "Fn::GetAtt": [
          "LogGrouptopicmcp948F08BC",
          "Arn"
         ]
        }
       ]
      }
     ],
     "Version": "2012-10-17"
    },
    "PolicyName": "ExecutionRoleDefaultPolicyA5B92313",
    "Roles": [
     {
      "Ref": "ExecutionRole605A040B"
     }
    ]
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/ExecutionRole/DefaultPolicy/Resource"
   }
  },
  "TaskRole30FC0FBB": {
   "Type": "AWS::IAM::Role",
   "Properties": {
    "AssumeRolePolicyDocument": {
     "Statement": [
      {
       "Action": "sts:AssumeRole",
       "Effect": "Allow",
       "Principal": {
        "Service": "ecs-tasks.amazonaws.com"
       }
      }
     ],
     "Version": "2012-10-17"
    },
    "ManagedPolicyArns": [
     {
      "Fn::Join": [
       "",
       [
        "arn:",
        {
         "Ref": "AWS::Partition"
        },
        ":iam::aws:policy/AmazonS3ReadOnlyAccess"
       ]
      ]
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/TaskRole/Resource"
   }
  },
  "LogGroupfrontend7D57FB6C": {
   "Type": "AWS::Logs::LogGroup",
   "Properties": {
    "LogGroupName": "/ecs/newsinsight/dev/frontend",
    "RetentionInDays": 7,
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/LogGroup-frontend/Resource"
   }
  },
  "TaskDeffrontend5190A524": {
   "Type": "AWS::ECS::TaskDefinition",
   "Properties": {
    "ContainerDefinitions": [
     {
      "Environment": [
       {
        "Name": "ENVIRONMENT",
        "Value": "dev"
       },
       {
        "Name": "NODE_ENV",
        "Value": "development"
       },
       {
        "Name": "POSTGRES_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttPostgresClusterE64235C6EndpointAddress8684A102"
        }
       },
       {
        "Name": "POSTGRES_PORT",
        "Value": "5432"
       },
       {
        "Name": "MONGO_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttDocDbClusterB46CF5D3EndpointD6B42638"
        }
       },
       {
        "Name": "MONGO_PORT",
        "Value": "27017"
       },
       {
        "Name": "REDIS_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttRedisClusterPrimaryEndPointAddressCFB16E1D"
        }
       },
       {
        "Name": "REDIS_PORT",
        "Value": "6379"
       },
       {
        "Name": "API_GATEWAY_URL",
        "Value": "http://api-gateway.newsinsight.local:8000"
       },
       {
        "Name": "COLLECTOR_SERVICE_URL",
        "Value": "http://collector-service.newsinsight.local:8081"
       },
       {
        "Name": "BROWSER_USE_API_URL",
        "Value": "http://browser-use-api.newsinsight.local:8500"
       },
       {
        "Name": "AUTONOMOUS_CRAWLER_URL",
        "Value": "http://autonomous-crawler.newsinsight.local:8030"
       },
       {
        "Name": "ADMIN_DASHBOARD_URL",
        "Value": "http://admin-dashboard.newsinsight.local:8888"
       },
       {
        "Name": "NEWSINSIGHT_MCP_URL",
        "Value": "http://newsinsight-mcp.newsinsight.local:5000"
       },
       {
        "Name": "BIAS_MCP_URL",
        "Value": "http://bias-mcp.newsinsight.local:5001"
       },
       {
        "Name": "FACTCHECK_MCP_URL",
        "Value": "http://factcheck-mcp.newsinsight.local:5002"
       },
       {
        "Name": "TOPIC_MCP_URL",
        "Value": "http://topic-mcp.newsinsight.local:5003"
       },
       {
        "Name": "AIAGENT_MCP_URL",
        "Value": "http://aiagent-mcp.newsinsight.local:5004"
       }
      ],
      "Essential": true,
      "Image": {
       "Fn::Join": [
        "",
        [
         {
          "Fn::Select": [
           4,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepofrontend3A01BFBDArn609FC158"
             }
            ]
           }
          ]
         },
         ".dkr.ecr.",
         {
          "Fn::Select": [
           3,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepofrontend3A01BFBDArn609FC158"
             }
            ]
           }
          ]
         },
         ".",
         {
          "Ref": "AWS::URLSuffix"
         },
         "/",
         {
          "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputRefRepofrontend3A01BFBDD8A7C9B3"
         },
         ":latest"
        ]
       ]
      },
      "LogConfiguration": {
       "LogDriver": "awslogs",
       "Options": {
        "awslogs-group": {
         "Ref": "LogGroupfrontend7D57FB6C"
        },
        "awslogs-stream-prefix": "frontend",
        "awslogs-region": "ap-northeast-2"
       }
      },
      "Name": "frontend",
      "PortMappings": [
       {
        "ContainerPort": 8080,
        "Protocol": "tcp"
       }
      ],
      "Secrets": [
       {
        "Name": "POSTGRES_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "POSTGRES_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "REDIS_PASSWORD",
        "ValueFrom": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefRedisSecretCB3BD9716380542C"
        }
       }
      ]
     }
    ],
    "Cpu": "256",
    "ExecutionRoleArn": {
     "Fn::GetAtt": [
      "ExecutionRole605A040B",
      "Arn"
     ]
    },
    "Family": "newsinsight-frontend-dev",
    "Memory": "512",
    "NetworkMode": "awsvpc",
    "RequiresCompatibilities": [
     "FARGATE"
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskRoleArn": {
     "Fn::GetAtt": [
      "TaskRole30FC0FBB",
      "Arn"
     ]
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/TaskDef-frontend/Resource"
   }
  },
  "ServicefrontendService9F390A1B": {
   "Type": "AWS::ECS::Service",
   "Properties": {
    "Cluster": {
     "Ref": "ClusterEB0386A7"
    },
    "DeploymentConfiguration": {
     "Alarms": {
      "AlarmNames": [],
      "Enable": false,
      "Rollback": false
     },
     "DeploymentCircuitBreaker": {
      "Enable": true,
      "Rollback": true
     },
     "MaximumPercent": 200,
     "MinimumHealthyPercent": 50
    },
    "DesiredCount": 1,
    "EnableECSManagedTags": false,
    "HealthCheckGracePeriodSeconds": 60,
    "LaunchType": "FARGATE",
    "LoadBalancers": [
     {
      "ContainerName": "frontend",
      "ContainerPort": 8080,
      "TargetGroupArn": {
       "Fn::ImportValue": "newsinsight-alb-dev:ExportsOutputRefTGfrontend5DB1B90D2624EC19"
      }
     }
    ],
    "NetworkConfiguration": {
     "AwsvpcConfiguration": {
      "AssignPublicIp": "DISABLED",
      "SecurityGroups": [
       {
        "Fn::GetAtt": [
         "ServiceSg2632F788",
         "GroupId"
        ]
       }
      ],
      "Subnets": [
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet1Subnet2291D3DF12E84F6D"
       },
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet2Subnet4441BE119DFCFE3A"
       }
      ]
     }
    },
    "ServiceName": "newsinsight-frontend-dev",
    "ServiceRegistries": [
     {
      "RegistryArn": {
       "Fn::GetAtt": [
        "ServicefrontendCloudmapServiceA26B6BBF",
        "Arn"
       ]
      }
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskDefinition": {
     "Ref": "TaskDeffrontend5190A524"
    }
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-frontend/Service"
   }
  },
  "ServicefrontendCloudmapServiceA26B6BBF": {
   "Type": "AWS::ServiceDiscovery::Service",
   "Properties": {
    "DnsConfig": {
     "DnsRecords": [
      {
       "TTL": 60,
       "Type": "A"
      }
     ],
     "NamespaceId": {
      "Fn::GetAtt": [
       "Namespace9B63B8C8",
       "Id"
      ]
     },
     "RoutingPolicy": "MULTIVALUE"
    },
    "HealthCheckCustomConfig": {
     "FailureThreshold": 1
    },
    "Name": "frontend",
    "NamespaceId": {
     "Fn::GetAtt": [
      "Namespace9B63B8C8",
      "Id"
     ]
    },
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-frontend/CloudmapService/Resource"
   }
  },
  "LogGroupapigatewayF4404BF5": {
   "Type": "AWS::Logs::LogGroup",
   "Properties": {
    "LogGroupName": "/ecs/newsinsight/dev/api-gateway",
    "RetentionInDays": 7,
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/LogGroup-api-gateway/Resource"
   }
  },
  "TaskDefapigatewayDDD73712": {
   "Type": "AWS::ECS::TaskDefinition",
   "Properties": {
    "ContainerDefinitions": [
     {
      "Environment": [
       {
        "Name": "ENVIRONMENT",
        "Value": "dev"
       },
       {
        "Name": "NODE_ENV",
        "Value": "development"
       },
       {
        "Name": "POSTGRES_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttPostgresClusterE64235C6EndpointAddress8684A102"
        }
       },
       {
        "Name": "POSTGRES_PORT",
        "Value": "5432"
       },
       {
        "Name": "MONGO_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttDocDbClusterB46CF5D3EndpointD6B42638"
        }
       },
       {
        "Name": "MONGO_PORT",
        "Value": "27017"
       },
       {
        "Name": "REDIS_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttRedisClusterPrimaryEndPointAddressCFB16E1D"
        }
       },
       {
        "Name": "REDIS_PORT",
        "Value": "6379"
       },
       {
        "Name": "API_GATEWAY_URL",
        "Value": "http://api-gateway.newsinsight.local:8000"
       },
       {
        "Name": "COLLECTOR_SERVICE_URL",
        "Value": "http://collector-service.newsinsight.local:8081"
       },
       {
        "Name": "BROWSER_USE_API_URL",
        "Value": "http://browser-use-api.newsinsight.local:8500"
       },
       {
        "Name": "AUTONOMOUS_CRAWLER_URL",
        "Value": "http://autonomous-crawler.newsinsight.local:8030"
       },
       {
        "Name": "ADMIN_DASHBOARD_URL",
        "Value": "http://admin-dashboard.newsinsight.local:8888"
       },
       {
        "Name": "NEWSINSIGHT_MCP_URL",
        "Value": "http://newsinsight-mcp.newsinsight.local:5000"
       },
       {
        "Name": "BIAS_MCP_URL",
        "Value": "http://bias-mcp.newsinsight.local:5001"
       },
       {
        "Name": "FACTCHECK_MCP_URL",
        "Value": "http://factcheck-mcp.newsinsight.local:5002"
       },
       {
        "Name": "TOPIC_MCP_URL",
        "Value": "http://topic-mcp.newsinsight.local:5003"
       },
       {
        "Name": "AIAGENT_MCP_URL",
        "Value": "http://aiagent-mcp.newsinsight.local:5004"
       },
       {
        "Name": "SPRING_PROFILES_ACTIVE",
        "Value": "dev"
       },
       {
        "Name": "CONSUL_ENABLED",
        "Value": "false"
       }
      ],
      "Essential": true,
      "Image": {
       "Fn::Join": [
        "",
        [
         {
          "Fn::Select": [
           4,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepoapigateway64782533Arn195B7386"
             }
            ]
           }
          ]
         },
         ".dkr.ecr.",
         {
          "Fn::Select": [
           3,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepoapigateway64782533Arn195B7386"
             }
            ]
           }
          ]
         },
         ".",
         {
          "Ref": "AWS::URLSuffix"
         },
         "/",
         {
          "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputRefRepoapigateway647825332C14E294"
         },
         ":latest"
        ]
       ]
      },
      "LogConfiguration": {
       "LogDriver": "awslogs",
       "Options": {
        "awslogs-group": {
         "Ref": "LogGroupapigatewayF4404BF5"
        },
        "awslogs-stream-prefix": "api-gateway",
        "awslogs-region": "ap-northeast-2"
       }
      },
      "Name": "api-gateway",
      "PortMappings": [
       {
        "ContainerPort": 8000,
        "Protocol": "tcp"
       }
      ],
      "Secrets": [
       {
        "Name": "POSTGRES_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "POSTGRES_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "REDIS_PASSWORD",
        "ValueFrom": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefRedisSecretCB3BD9716380542C"
        }
       }
      ]
     }
    ],
    "Cpu": "512",
    "ExecutionRoleArn": {
     "Fn::GetAtt": [
      "ExecutionRole605A040B",
      "Arn"
     ]
    },
    "Family": "newsinsight-api-gateway-dev",
    "Memory": "1024",
    "NetworkMode": "awsvpc",
    "RequiresCompatibilities": [
     "FARGATE"
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskRoleArn": {
     "Fn::GetAtt": [
      "TaskRole30FC0FBB",
      "Arn"
     ]
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/TaskDef-api-gateway/Resource"
   }
  },
  "ServiceapigatewayServiceDF8BD105": {
   "Type": "AWS::ECS::Service",
   "Properties": {
    "Cluster": {
     "Ref": "ClusterEB0386A7"
    },
    "DeploymentConfiguration": {
     "Alarms": {
      "AlarmNames": [],
      "Enable": false,
      "Rollback": false
     },
     "DeploymentCircuitBreaker": {
      "Enable": true,
      "Rollback": true
     },
     "MaximumPercent": 200,
     "MinimumHealthyPercent": 50
    },
    "DesiredCount": 1,
    "EnableECSManagedTags": false,
    "HealthCheckGracePeriodSeconds": 60,
    "LaunchType": "FARGATE",
    "LoadBalancers": [
     {
      "ContainerName": "api-gateway",
      "ContainerPort": 8000,
      "TargetGroupArn": {
       "Fn::ImportValue": "newsinsight-alb-dev:ExportsOutputRefTGapigatewayD7113A2151240BB4"
      }
     }
    ],
    "NetworkConfiguration": {
     "AwsvpcConfiguration": {
      "AssignPublicIp": "DISABLED",
      "SecurityGroups": [
       {
        "Fn::GetAtt": [
         "ServiceSg2632F788",
         "GroupId"
        ]
       }
      ],
      "Subnets": [
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet1Subnet2291D3DF12E84F6D"
       },
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet2Subnet4441BE119DFCFE3A"
       }
      ]
     }
    },
    "ServiceName": "newsinsight-api-gateway-dev",
    "ServiceRegistries": [
     {
      "RegistryArn": {
       "Fn::GetAtt": [
        "ServiceapigatewayCloudmapServiceF60E9EF9",
        "Arn"
       ]
      }
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskDefinition": {
     "Ref": "TaskDefapigatewayDDD73712"
    }
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-api-gateway/Service"
   }
  },
  "ServiceapigatewayCloudmapServiceF60E9EF9": {
   "Type": "AWS::ServiceDiscovery::Service",
   "Properties": {
    "DnsConfig": {
     "DnsRecords": [
      {
       "TTL": 60,
       "Type": "A"
      }
     ],
     "NamespaceId": {
      "Fn::GetAtt": [
       "Namespace9B63B8C8",
       "Id"
      ]
     },
     "RoutingPolicy": "MULTIVALUE"
    },
    "HealthCheckCustomConfig": {
     "FailureThreshold": 1
    },
    "Name": "api-gateway",
    "NamespaceId": {
     "Fn::GetAtt": [
      "Namespace9B63B8C8",
      "Id"
     ]
    },
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-api-gateway/CloudmapService/Resource"
   }
  },
  "LogGroupcollectorserviceFAE751EF": {
   "Type": "AWS::Logs::LogGroup",
   "Properties": {
    "LogGroupName": "/ecs/newsinsight/dev/collector-service",
    "RetentionInDays": 7,
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/LogGroup-collector-service/Resource"
   }
  },
  "TaskDefcollectorservice48D29AEF": {
   "Type": "AWS::ECS::TaskDefinition",
   "Properties": {
    "ContainerDefinitions": [
     {
      "Environment": [
       {
        "Name": "ENVIRONMENT",
        "Value": "dev"
       },
       {
        "Name": "NODE_ENV",
        "Value": "development"
       },
       {
        "Name": "POSTGRES_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttPostgresClusterE64235C6EndpointAddress8684A102"
        }
       },
       {
        "Name": "POSTGRES_PORT",
        "Value": "5432"
       },
       {
        "Name": "MONGO_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttDocDbClusterB46CF5D3EndpointD6B42638"
        }
       },
       {
        "Name": "MONGO_PORT",
        "Value": "27017"
       },
       {
        "Name": "REDIS_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttRedisClusterPrimaryEndPointAddressCFB16E1D"
        }
       },
       {
        "Name": "REDIS_PORT",
        "Value": "6379"
       },
       {
        "Name": "API_GATEWAY_URL",
        "Value": "http://api-gateway.newsinsight.local:8000"
       },
       {
        "Name": "COLLECTOR_SERVICE_URL",
        "Value": "http://collector-service.newsinsight.local:8081"
       },
       {
        "Name": "BROWSER_USE_API_URL",
        "Value": "http://browser-use-api.newsinsight.local:8500"
       },
       {
        "Name": "AUTONOMOUS_CRAWLER_URL",
        "Value": "http://autonomous-crawler.newsinsight.local:8030"
       },
       {
        "Name": "ADMIN_DASHBOARD_URL",
        "Value": "http://admin-dashboard.newsinsight.local:8888"
       },
       {
        "Name": "NEWSINSIGHT_MCP_URL",
        "Value": "http://newsinsight-mcp.newsinsight.local:5000"
       },
       {
        "Name": "BIAS_MCP_URL",
        "Value": "http://bias-mcp.newsinsight.local:5001"
       },
       {
        "Name": "FACTCHECK_MCP_URL",
        "Value": "http://factcheck-mcp.newsinsight.local:5002"
       },
       {
        "Name": "TOPIC_MCP_URL",
        "Value": "http://topic-mcp.newsinsight.local:5003"
       },
       {
        "Name": "AIAGENT_MCP_URL",
        "Value": "http://aiagent-mcp.newsinsight.local:5004"
       }
      ],
      "Essential": true,
      "Image": {
       "Fn::Join": [
        "",
        [
         {
          "Fn::Select": [
           4,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepocollectorserviceBD635EBCArn94D07D97"
             }
            ]
           }
          ]
         },
         ".dkr.ecr.",
         {
          "Fn::Select": [
           3,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepocollectorserviceBD635EBCArn94D07D97"
             }
            ]
           }
          ]
         },
         ".",
         {
          "Ref": "AWS::URLSuffix"
         },
         "/",
         {
          "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputRefRepocollectorserviceBD635EBC8634961A"
         },
         ":latest"
        ]
       ]
      },
      "LogConfiguration": {
       "LogDriver": "awslogs",
       "Options": {
        "awslogs-group": {
         "Ref": "LogGroupcollectorserviceFAE751EF"
        },
        "awslogs-stream-prefix": "collector-service",
        "awslogs-region": "ap-northeast-2"
       }
      },
      "Name": "collector-service",
      "PortMappings": [
       {
        "ContainerPort": 8081,
        "Protocol": "tcp"
       }
      ],
      "Secrets": [
       {
        "Name": "POSTGRES_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "POSTGRES_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "REDIS_PASSWORD",
        "ValueFrom": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefRedisSecretCB3BD9716380542C"
        }
       }
      ]
     }
    ],
    "Cpu": "512",
    "ExecutionRoleArn": {
     "Fn::GetAtt": [
      "ExecutionRole605A040B",
      "Arn"
     ]
    },
    "Family": "newsinsight-collector-service-dev",
    "Memory": "1024",
    "NetworkMode": "awsvpc",
    "RequiresCompatibilities": [
     "FARGATE"
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskRoleArn": {
     "Fn::GetAtt": [
      "TaskRole30FC0FBB",
      "Arn"
     ]
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/TaskDef-collector-service/Resource"
   }
  },
  "ServicecollectorserviceService6A506F75": {
   "Type": "AWS::ECS::Service",
   "Properties": {
    "Cluster": {
     "Ref": "ClusterEB0386A7"
    },
    "DeploymentConfiguration": {
     "Alarms": {
      "AlarmNames": [],
      "Enable": false,
      "Rollback": false
     },
     "DeploymentCircuitBreaker": {
      "Enable": true,
      "Rollback": true
     },
     "MaximumPercent": 200,
     "MinimumHealthyPercent": 50
    },
    "DesiredCount": 1,
    "EnableECSManagedTags": false,
    "LaunchType": "FARGATE",
    "NetworkConfiguration": {
     "AwsvpcConfiguration": {
      "AssignPublicIp": "DISABLED",
      "SecurityGroups": [
       {
        "Fn::GetAtt": [
         "ServiceSg2632F788",
         "GroupId"
        ]
       }
      ],
      "Subnets": [
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet1Subnet2291D3DF12E84F6D"
       },
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet2Subnet4441BE119DFCFE3A"
       }
      ]
     }
    },
    "ServiceName": "newsinsight-collector-service-dev",
    "ServiceRegistries": [
     {
      "RegistryArn": {
       "Fn::GetAtt": [
        "ServicecollectorserviceCloudmapService2C032341",
        "Arn"
       ]
      }
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskDefinition": {
     "Ref": "TaskDefcollectorservice48D29AEF"
    }
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-collector-service/Service"
   }
  },
  "ServicecollectorserviceCloudmapService2C032341": {
   "Type": "AWS::ServiceDiscovery::Service",
   "Properties": {
    "DnsConfig": {
     "DnsRecords": [
      {
       "TTL": 60,
       "Type": "A"
      }
     ],
     "NamespaceId": {
      "Fn::GetAtt": [
       "Namespace9B63B8C8",
       "Id"
      ]
     },
     "RoutingPolicy": "MULTIVALUE"
    },
    "HealthCheckCustomConfig": {
     "FailureThreshold": 1
    },
    "Name": "collector-service",
    "NamespaceId": {
     "Fn::GetAtt": [
      "Namespace9B63B8C8",
      "Id"
     ]
    },
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-collector-service/CloudmapService/Resource"
   }
  },
  "LogGroupadmindashboard30E0633B": {
   "Type": "AWS::Logs::LogGroup",
   "Properties": {
    "LogGroupName": "/ecs/newsinsight/dev/admin-dashboard",
    "RetentionInDays": 7,
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/LogGroup-admin-dashboard/Resource"
   }
  },
  "TaskDefadmindashboard517E748F": {
   "Type": "AWS::ECS::TaskDefinition",
   "Properties": {
    "ContainerDefinitions": [
     {
      "Environment": [
       {
        "Name": "ENVIRONMENT",
        "Value": "dev"
       },
       {
        "Name": "NODE_ENV",
        "Value": "development"
       },
       {
        "Name": "POSTGRES_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttPostgresClusterE64235C6EndpointAddress8684A102"
        }
       },
       {
        "Name": "POSTGRES_PORT",
        "Value": "5432"
       },
       {
        "Name": "MONGO_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttDocDbClusterB46CF5D3EndpointD6B42638"
        }
       },
       {
        "Name": "MONGO_PORT",
        "Value": "27017"
       },
       {
        "Name": "REDIS_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttRedisClusterPrimaryEndPointAddressCFB16E1D"
        }
       },
       {
        "Name": "REDIS_PORT",
        "Value": "6379"
       },
       {
        "Name": "API_GATEWAY_URL",
        "Value": "http://api-gateway.newsinsight.local:8000"
       },
       {
        "Name": "COLLECTOR_SERVICE_URL",
        "Value": "http://collector-service.newsinsight.local:8081"
       },
       {
        "Name": "BROWSER_USE_API_URL",
        "Value": "http://browser-use-api.newsinsight.local:8500"
       },
       {
        "Name": "AUTONOMOUS_CRAWLER_URL",
        "Value": "http://autonomous-crawler.newsinsight.local:8030"
       },
       {
        "Name": "ADMIN_DASHBOARD_URL",
        "Value": "http://admin-dashboard.newsinsight.local:8888"
       },
       {
        "Name": "NEWSINSIGHT_MCP_URL",
        "Value": "http://newsinsight-mcp.newsinsight.local:5000"
       },
       {
        "Name": "BIAS_MCP_URL",
        "Value": "http://bias-mcp.newsinsight.local:5001"
       },
       {
        "Name": "FACTCHECK_MCP_URL",
        "Value": "http://factcheck-mcp.newsinsight.local:5002"
       },
       {
        "Name": "TOPIC_MCP_URL",
        "Value": "http://topic-mcp.newsinsight.local:5003"
       },
       {
        "Name": "AIAGENT_MCP_URL",
        "Value": "http://aiagent-mcp.newsinsight.local:5004"
       }
      ],
      "Essential": true,
      "Image": {
       "Fn::Join": [
        "",
        [
         {
          "Fn::Select": [
           4,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepoadmindashboard8DB8FD3CArnDACF9417"
             }
            ]
           }
          ]
         },
         ".dkr.ecr.",
         {
          "Fn::Select": [
           3,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepoadmindashboard8DB8FD3CArnDACF9417"
             }
            ]
           }
          ]
         },
         ".",
         {
          "Ref": "AWS::URLSuffix"
         },
         "/",
         {
          "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputRefRepoadmindashboard8DB8FD3CDC7C67B6"
         },
         ":latest"
        ]
       ]
      },
      "LogConfiguration": {
       "LogDriver": "awslogs",
       "Options": {
        "awslogs-group": {
         "Ref": "LogGroupadmindashboard30E0633B"
        },
        "awslogs-stream-prefix": "admin-dashboard",
        "awslogs-region": "ap-northeast-2"
       }
      },
      "Name": "admin-dashboard",
      "PortMappings": [
       {
        "ContainerPort": 8888,
        "Protocol": "tcp"
       }
      ],
      "Secrets": [
       {
        "Name": "POSTGRES_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "POSTGRES_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "REDIS_PASSWORD",
        "ValueFrom": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefRedisSecretCB3BD9716380542C"
        }
       }
      ]
     }
    ],
    "Cpu": "256",
    "ExecutionRoleArn": {
     "Fn::GetAtt": [
      "ExecutionRole605A040B",
      "Arn"
     ]
    },
    "Family": "newsinsight-admin-dashboard-dev",
    "Memory": "512",
    "NetworkMode": "awsvpc",
    "RequiresCompatibilities": [
     "FARGATE"
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskRoleArn": {
     "Fn::GetAtt": [
      "TaskRole30FC0FBB",
      "Arn"
     ]
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/TaskDef-admin-dashboard/Resource"
   }
  },
  "ServiceadmindashboardServiceA65BFCE1": {
   "Type": "AWS::ECS::Service",
   "Properties": {
    "Cluster": {
     "Ref": "ClusterEB0386A7"
    },
    "DeploymentConfiguration": {
     "Alarms": {
      "AlarmNames": [],
      "Enable": false,
      "Rollback": false
     },
     "DeploymentCircuitBreaker": {
      "Enable": true,
      "Rollback": true
     },
     "MaximumPercent": 200,
     "MinimumHealthyPercent": 50
    },
    "DesiredCount": 1,
    "EnableECSManagedTags": false,
    "HealthCheckGracePeriodSeconds": 60,
    "LaunchType": "FARGATE",
    "LoadBalancers": [
     {
      "ContainerName": "admin-dashboard",
      "ContainerPort": 8888,
      "TargetGroupArn": {
       "Fn::ImportValue": "newsinsight-alb-dev:ExportsOutputRefTGadmindashboard10526AD81ED53695"
      }
     }
    ],
    "NetworkConfiguration": {
     "AwsvpcConfiguration": {
      "AssignPublicIp": "DISABLED",
      "SecurityGroups": [
       {
        "Fn::GetAtt": [
         "ServiceSg2632F788",
         "GroupId"
        ]
       }
      ],
      "Subnets": [
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet1Subnet2291D3DF12E84F6D"
       },
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet2Subnet4441BE119DFCFE3A"
       }
      ]
     }
    },
    "ServiceName": "newsinsight-admin-dashboard-dev",
    "ServiceRegistries": [
     {
      "RegistryArn": {
       "Fn::GetAtt": [
        "ServiceadmindashboardCloudmapService792CCD9C",
        "Arn"
       ]
      }
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskDefinition": {
     "Ref": "TaskDefadmindashboard517E748F"
    }
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-admin-dashboard/Service"
   }
  },
  "ServiceadmindashboardCloudmapService792CCD9C": {
   "Type": "AWS::ServiceDiscovery::Service",
   "Properties": {
    "DnsConfig": {
     "DnsRecords": [
      {
       "TTL": 60,
       "Type": "A"
      }
     ],
     "NamespaceId": {
      "Fn::GetAtt": [
       "Namespace9B63B8C8",
       "Id"
      ]
     },
     "RoutingPolicy": "MULTIVALUE"
    },
    "HealthCheckCustomConfig": {
     "FailureThreshold": 1
    },
    "Name": "admin-dashboard",
    "NamespaceId": {
     "Fn::GetAtt": [
      "Namespace9B63B8C8",
      "Id"
     ]
    },
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-admin-dashboard/CloudmapService/Resource"
   }
  },
  "LogGroupautonomouscrawler4D362341": {
   "Type": "AWS::Logs::LogGroup",
   "Properties": {
    "LogGroupName": "/ecs/newsinsight/dev/autonomous-crawler",
    "RetentionInDays": 7,
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/LogGroup-autonomous-crawler/Resource"
   }
  },
  "TaskDefautonomouscrawler9608A356": {
   "Type": "AWS::ECS::TaskDefinition",
   "Properties": {
    "ContainerDefinitions": [
     {
      "Environment": [
       {
        "Name": "ENVIRONMENT",
        "Value": "dev"
       },
       {
        "Name": "NODE_ENV",
        "Value": "development"
       },
       {
        "Name": "POSTGRES_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttPostgresClusterE64235C6EndpointAddress8684A102"
        }
       },
       {
        "Name": "POSTGRES_PORT",
        "Value": "5432"
       },
       {
        "Name": "MONGO_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttDocDbClusterB46CF5D3EndpointD6B42638"
        }
       },
       {
        "Name": "MONGO_PORT",
        "Value": "27017"
       },
       {
        "Name": "REDIS_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttRedisClusterPrimaryEndPointAddressCFB16E1D"
        }
       },
       {
        "Name": "REDIS_PORT",
        "Value": "6379"
       },
       {
        "Name": "API_GATEWAY_URL",
        "Value": "http://api-gateway.newsinsight.local:8000"
       },
       {
        "Name": "COLLECTOR_SERVICE_URL",
        "Value": "http://collector-service.newsinsight.local:8081"
       },
       {
        "Name": "BROWSER_USE_API_URL",
        "Value": "http://browser-use-api.newsinsight.local:8500"
       },
       {
        "Name": "AUTONOMOUS_CRAWLER_URL",
        "Value": "http://autonomous-crawler.newsinsight.local:8030"
       },
       {
        "Name": "ADMIN_DASHBOARD_URL",
        "Value": "http://admin-dashboard.newsinsight.local:8888"
       },
       {
        "Name": "NEWSINSIGHT_MCP_URL",
        "Value": "http://newsinsight-mcp.newsinsight.local:5000"
       },
       {
        "Name": "BIAS_MCP_URL",
        "Value": "http://bias-mcp.newsinsight.local:5001"
       },
       {
        "Name": "FACTCHECK_MCP_URL",
        "Value": "http://factcheck-mcp.newsinsight.local:5002"
       },
       {
        "Name": "TOPIC_MCP_URL",
        "Value": "http://topic-mcp.newsinsight.local:5003"
       },
       {
        "Name": "AIAGENT_MCP_URL",
        "Value": "http://aiagent-mcp.newsinsight.local:5004"
       }
      ],
      "Essential": true,
      "Image": {
       "Fn::Join": [
        "",
        [
         {
          "Fn::Select": [
           4,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepoautonomouscrawler2C28B095ArnFDDE5673"
             }
            ]
           }
          ]
         },
         ".dkr.ecr.",
         {
          "Fn::Select": [
           3,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepoautonomouscrawler2C28B095ArnFDDE5673"
             }
            ]
           }
          ]
         },
         ".",
         {
          "Ref": "AWS::URLSuffix"
         },
         "/",
         {
          "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputRefRepoautonomouscrawler2C28B095743EBAFA"
         },
         ":latest"
        ]
       ]
      },
      "LogConfiguration": {
       "LogDriver": "awslogs",
       "Options": {
        "awslogs-group": {
         "Ref": "LogGroupautonomouscrawler4D362341"
        },
        "awslogs-stream-prefix": "autonomous-crawler",
        "awslogs-region": "ap-northeast-2"
       }
      },
      "Name": "autonomous-crawler",
      "PortMappings": [
       {
        "ContainerPort": 8030,
        "Protocol": "tcp"
       }
      ],
      "Secrets": [
       {
        "Name": "POSTGRES_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "POSTGRES_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "REDIS_PASSWORD",
        "ValueFrom": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefRedisSecretCB3BD9716380542C"
        }
       }
      ]
     }
    ],
    "Cpu": "1024",
    "ExecutionRoleArn": {
     "Fn::GetAtt": [
      "ExecutionRole605A040B",
      "Arn"
     ]
    },
    "Family": "newsinsight-autonomous-crawler-dev",
    "Memory": "2048",
    "NetworkMode": "awsvpc",
    "RequiresCompatibilities": [
     "FARGATE"
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskRoleArn": {
     "Fn::GetAtt": [
      "TaskRole30FC0FBB",
      "Arn"
     ]
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/TaskDef-autonomous-crawler/Resource"
   }
  },
  "ServiceautonomouscrawlerServiceBBEC85D9": {
   "Type": "AWS::ECS::Service",
   "Properties": {
    "Cluster": {
     "Ref": "ClusterEB0386A7"
    },
    "DeploymentConfiguration": {
     "Alarms": {
      "AlarmNames": [],
      "Enable": false,
      "Rollback": false
     },
     "DeploymentCircuitBreaker": {
      "Enable": true,
      "Rollback": true
     },
     "MaximumPercent": 200,
     "MinimumHealthyPercent": 50
    },
    "DesiredCount": 1,
    "EnableECSManagedTags": false,
    "LaunchType": "FARGATE",
    "NetworkConfiguration": {
     "AwsvpcConfiguration": {
      "AssignPublicIp": "DISABLED",
      "SecurityGroups": [
       {
        "Fn::GetAtt": [
         "ServiceSg2632F788",
         "GroupId"
        ]
       }
      ],
      "Subnets": [
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet1Subnet2291D3DF12E84F6D"
       },
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet2Subnet4441BE119DFCFE3A"
       }
      ]
     }
    },
    "ServiceName": "newsinsight-autonomous-crawler-dev",
    "ServiceRegistries": [
     {
      "RegistryArn": {
       "Fn::GetAtt": [
        "ServiceautonomouscrawlerCloudmapServiceC4274B69",
        "Arn"
       ]
      }
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskDefinition": {
     "Ref": "TaskDefautonomouscrawler9608A356"
    }
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-autonomous-crawler/Service"
   }
  },
  "ServiceautonomouscrawlerCloudmapServiceC4274B69": {
   "Type": "AWS::ServiceDiscovery::Service",
   "Properties": {
    "DnsConfig": {
     "DnsRecords": [
      {
       "TTL": 60,
       "Type": "A"
      }
     ],
     "NamespaceId": {
      "Fn::GetAtt": [
       "Namespace9B63B8C8",
       "Id"
      ]
     },
     "RoutingPolicy": "MULTIVALUE"
    },
    "HealthCheckCustomConfig": {
     "FailureThreshold": 1
    },
    "Name": "autonomous-crawler",
    "NamespaceId": {
     "Fn::GetAtt": [
      "Namespace9B63B8C8",
      "Id"
     ]
    },
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-autonomous-crawler/CloudmapService/Resource"
   }
  },
  "LogGroupbrowseruseapiC75B0534": {
   "Type": "AWS::Logs::LogGroup",
   "Properties": {
    "LogGroupName": "/ecs/newsinsight/dev/browser-use-api",
    "RetentionInDays": 7,
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/LogGroup-browser-use-api/Resource"
   }
  },
  "TaskDefbrowseruseapi5E0FAE3B": {
   "Type": "AWS::ECS::TaskDefinition",
   "Properties": {
    "ContainerDefinitions": [
     {
      "Environment": [
       {
        "Name": "ENVIRONMENT",
        "Value": "dev"
       },
       {
        "Name": "NODE_ENV",
        "Value": "development"
       },
       {
        "Name": "POSTGRES_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttPostgresClusterE64235C6EndpointAddress8684A102"
        }
       },
       {
        "Name": "POSTGRES_PORT",
        "Value": "5432"
       },
       {
        "Name": "MONGO_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttDocDbClusterB46CF5D3EndpointD6B42638"
        }
       },
       {
        "Name": "MONGO_PORT",
        "Value": "27017"
       },
       {
        "Name": "REDIS_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttRedisClusterPrimaryEndPointAddressCFB16E1D"
        }
       },
       {
        "Name": "REDIS_PORT",
        "Value": "6379"
       },
       {
        "Name": "API_GATEWAY_URL",
        "Value": "http://api-gateway.newsinsight.local:8000"
       },
       {
        "Name": "COLLECTOR_SERVICE_URL",
        "Value": "http://collector-service.newsinsight.local:8081"
       },
       {
        "Name": "BROWSER_USE_API_URL",
        "Value": "http://browser-use-api.newsinsight.local:8500"
       },
       {
        "Name": "AUTONOMOUS_CRAWLER_URL",
        "Value": "http://autonomous-crawler.newsinsight.local:8030"
       },
       {
        "Name": "ADMIN_DASHBOARD_URL",
        "Value": "http://admin-dashboard.newsinsight.local:8888"
       },
       {
        "Name": "NEWSINSIGHT_MCP_URL",
        "Value": "http://newsinsight-mcp.newsinsight.local:5000"
       },
       {
        "Name": "BIAS_MCP_URL",
        "Value": "http://bias-mcp.newsinsight.local:5001"
       },
       {
        "Name": "FACTCHECK_MCP_URL",
        "Value": "http://factcheck-mcp.newsinsight.local:5002"
       },
       {
        "Name": "TOPIC_MCP_URL",
        "Value": "http://topic-mcp.newsinsight.local:5003"
       },
       {
        "Name": "AIAGENT_MCP_URL",
        "Value": "http://aiagent-mcp.newsinsight.local:5004"
       }
      ],
      "Essential": true,
      "Image": {
       "Fn::Join": [
        "",
        [
         {
          "Fn::Select": [
           4,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepobrowseruseapi89FF3D9AArn28ECA372"
             }
            ]
           }
          ]
         },
         ".dkr.ecr.",
         {
          "Fn::Select": [
           3,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepobrowseruseapi89FF3D9AArn28ECA372"
             }
            ]
           }
          ]
         },
         ".",
         {
          "Ref": "AWS::URLSuffix"
         },
         "/",
         {
          "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputRefRepobrowseruseapi89FF3D9AF96F4B39"
         },
         ":latest"
        ]
       ]
      },
      "LogConfiguration": {
       "LogDriver": "awslogs",
       "Options": {
        "awslogs-group": {
         "Ref": "LogGroupbrowseruseapiC75B0534"
        },
        "awslogs-stream-prefix": "browser-use-api",
        "awslogs-region": "ap-northeast-2"
       }
      },
      "Name": "browser-use-api",
      "PortMappings": [
       {
        "ContainerPort": 8500,
        "Protocol": "tcp"
       }
      ],
      "Secrets": [
       {
        "Name": "POSTGRES_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "POSTGRES_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "REDIS_PASSWORD",
        "ValueFrom": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefRedisSecretCB3BD9716380542C"
        }
       }
      ]
     }
    ],
    "Cpu": "1024",
    "ExecutionRoleArn": {
     "Fn::GetAtt": [
      "ExecutionRole605A040B",
      "Arn"
     ]
    },
    "Family": "newsinsight-browser-use-api-dev",
    "Memory": "2048",
    "NetworkMode": "awsvpc",
    "RequiresCompatibilities": [
     "FARGATE"
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskRoleArn": {
     "Fn::GetAtt": [
      "TaskRole30FC0FBB",
      "Arn"
     ]
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/TaskDef-browser-use-api/Resource"
   }
  },
  "ServicebrowseruseapiService8A5AEC05": {
   "Type": "AWS::ECS::Service",
   "Properties": {
    "Cluster": {
     "Ref": "ClusterEB0386A7"
    },
    "DeploymentConfiguration": {
     "Alarms": {
      "AlarmNames": [],
      "Enable": false,
      "Rollback": false
     },
     "DeploymentCircuitBreaker": {
      "Enable": true,
      "Rollback": true
     },
     "MaximumPercent": 200,
     "MinimumHealthyPercent": 50
    },
    "DesiredCount": 1,
    "EnableECSManagedTags": false,
    "LaunchType": "FARGATE",
    "NetworkConfiguration": {
     "AwsvpcConfiguration": {
      "AssignPublicIp": "DISABLED",
      "SecurityGroups": [
       {
        "Fn::GetAtt": [
         "ServiceSg2632F788",
         "GroupId"
        ]
       }
      ],
      "Subnets": [
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet1Subnet2291D3DF12E84F6D"
       },
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet2Subnet4441BE119DFCFE3A"
       }
      ]
     }
    },
    "ServiceName": "newsinsight-browser-use-api-dev",
    "ServiceRegistries": [
     {
      "RegistryArn": {
       "Fn::GetAtt": [
        "ServicebrowseruseapiCloudmapService60213A5F",
        "Arn"
       ]
      }
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskDefinition": {
     "Ref": "TaskDefbrowseruseapi5E0FAE3B"
    }
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-browser-use-api/Service"
   }
  },
  "ServicebrowseruseapiCloudmapService60213A5F": {
   "Type": "AWS::ServiceDiscovery::Service",
   "Properties": {
    "DnsConfig": {
     "DnsRecords": [
      {
       "TTL": 60,
       "Type": "A"
      }
     ],
     "NamespaceId": {
      "Fn::GetAtt": [
       "Namespace9B63B8C8",
       "Id"
      ]
     },
     "RoutingPolicy": "MULTIVALUE"
    },
    "HealthCheckCustomConfig": {
     "FailureThreshold": 1
    },
    "Name": "browser-use-api",
    "NamespaceId": {
     "Fn::GetAtt": [
      "Namespace9B63B8C8",
      "Id"
     ]
    },
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-browser-use-api/CloudmapService/Resource"
   }
  },
  "LogGroupbotdetector8EBD76F3": {
   "Type": "AWS::Logs::LogGroup",
   "Properties": {
    "LogGroupName": "/ecs/newsinsight/dev/bot-detector",
    "RetentionInDays": 7,
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/LogGroup-bot-detector/Resource"
   }
  },
  "TaskDefbotdetectorB27CC838": {
   "Type": "AWS::ECS::TaskDefinition",
   "Properties": {
    "ContainerDefinitions": [
     {
      "Environment": [
       {
        "Name": "ENVIRONMENT",
        "Value": "dev"
       },
       {
        "Name": "NODE_ENV",
        "Value": "development"
       },
       {
        "Name": "POSTGRES_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttPostgresClusterE64235C6EndpointAddress8684A102"
        }
       },
       {
        "Name": "POSTGRES_PORT",
        "Value": "5432"
       },
       {
        "Name": "MONGO_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttDocDbClusterB46CF5D3EndpointD6B42638"
        }
       },
       {
        "Name": "MONGO_PORT",
        "Value": "27017"
       },
       {
        "Name": "REDIS_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttRedisClusterPrimaryEndPointAddressCFB16E1D"
        }
       },
       {
        "Name": "REDIS_PORT",
        "Value": "6379"
       },
       {
        "Name": "API_GATEWAY_URL",
        "Value": "http://api-gateway.newsinsight.local:8000"
       },
       {
        "Name": "COLLECTOR_SERVICE_URL",
        "Value": "http://collector-service.newsinsight.local:8081"
       },
       {
        "Name": "BROWSER_USE_API_URL",
        "Value": "http://browser-use-api.newsinsight.local:8500"
       },
       {
        "Name": "AUTONOMOUS_CRAWLER_URL",
        "Value": "http://autonomous-crawler.newsinsight.local:8030"
       },
       {
        "Name": "ADMIN_DASHBOARD_URL",
        "Value": "http://admin-dashboard.newsinsight.local:8888"
       },
       {
        "Name": "NEWSINSIGHT_MCP_URL",
        "Value": "http://newsinsight-mcp.newsinsight.local:5000"
       },
       {
        "Name": "BIAS_MCP_URL",
        "Value": "http://bias-mcp.newsinsight.local:5001"
       },
       {
        "Name": "FACTCHECK_MCP_URL",
        "Value": "http://factcheck-mcp.newsinsight.local:5002"
       },
       {
        "Name": "TOPIC_MCP_URL",
        "Value": "http://topic-mcp.newsinsight.local:5003"
       },
       {
        "Name": "AIAGENT_MCP_URL",
        "Value": "http://aiagent-mcp.newsinsight.local:5004"
       }
      ],
      "Essential": true,
      "Image": {
       "Fn::Join": [
        "",
        [
         {
          "Fn::Select": [
           4,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepobotdetectorD791269AArn20D2BD38"
             }
            ]
           }
          ]
         },
         ".dkr.ecr.",
         {
          "Fn::Select": [
           3,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepobotdetectorD791269AArn20D2BD38"
             }
            ]
           }
          ]
         },
         ".",
         {
          "Ref": "AWS::URLSuffix"
         },
         "/",
         {
          "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputRefRepobotdetectorD791269A03D0CDBC"
         },
         ":latest"
        ]
       ]
      },
      "LogConfiguration": {
       "LogDriver": "awslogs",
       "Options": {
        "awslogs-group": {
         "Ref": "LogGroupbotdetector8EBD76F3"
        },
        "awslogs-stream-prefix": "bot-detector",
        "awslogs-region": "ap-northeast-2"
       }
      },
      "Name": "bot-detector",
      "PortMappings": [
       {
        "ContainerPort": 8041,
        "Protocol": "tcp"
       }
      ],
      "Secrets": [
       {
        "Name": "POSTGRES_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "POSTGRES_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "REDIS_PASSWORD",
        "ValueFrom": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefRedisSecretCB3BD9716380542C"
        }
       }
      ]
     }
    ],
    "Cpu": "512",
    "ExecutionRoleArn": {
     "Fn::GetAtt": [
      "ExecutionRole605A040B",
      "Arn"
     ]
    },
    "Family": "newsinsight-bot-detector-dev",
    "Memory": "1024",
    "NetworkMode": "awsvpc",
    "RequiresCompatibilities": [
     "FARGATE"
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskRoleArn": {
     "Fn::GetAtt": [
      "TaskRole30FC0FBB",
      "Arn"
     ]
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/TaskDef-bot-detector/Resource"
   }
  },
  "ServicebotdetectorServiceBD09EE98": {
   "Type": "AWS::ECS::Service",
   "Properties": {
    "Cluster": {
     "Ref": "ClusterEB0386A7"
    },
    "DeploymentConfiguration": {
     "Alarms": {
      "AlarmNames": [],
      "Enable": false,
      "Rollback": false
     },
     "DeploymentCircuitBreaker": {
      "Enable": true,
      "Rollback": true
     },
     "MaximumPercent": 200,
     "MinimumHealthyPercent": 50
    },
    "DesiredCount": 1,
    "EnableECSManagedTags": false,
    "LaunchType": "FARGATE",
    "NetworkConfiguration": {
     "AwsvpcConfiguration": {
      "AssignPublicIp": "DISABLED",
      "SecurityGroups": [
       {
        "Fn::GetAtt": [
         "ServiceSg2632F788",
         "GroupId"
        ]
       }
      ],
      "Subnets": [
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet1Subnet2291D3DF12E84F6D"
       },
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet2Subnet4441BE119DFCFE3A"
       }
      ]
     }
    },
    "ServiceName": "newsinsight-bot-detector-dev",
    "ServiceRegistries": [
     {
      "RegistryArn": {
       "Fn::GetAtt": [
        "ServicebotdetectorCloudmapService33856316",
        "Arn"
       ]
      }
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskDefinition": {
     "Ref": "TaskDefbotdetectorB27CC838"
    }
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-bot-detector/Service"
   }
  },
  "ServicebotdetectorCloudmapService33856316": {
   "Type": "AWS::ServiceDiscovery::Service",
   "Properties": {
    "DnsConfig": {
     "DnsRecords": [
      {
       "TTL": 60,
       "Type": "A"
      }
     ],
     "NamespaceId": {
      "Fn::GetAtt": [
       "Namespace9B63B8C8",
       "Id"
      ]
     },
     "RoutingPolicy": "MULTIVALUE"
    },
    "HealthCheckCustomConfig": {
     "FailureThreshold": 1
    },
    "Name": "bot-detector",
    "NamespaceId": {
     "Fn::GetAtt": [
      "Namespace9B63B8C8",
      "Id"
     ]
    },
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-bot-detector/CloudmapService/Resource"
   }
  },
  "LogGroupembeddingserverE55FA114": {
   "Type": "AWS::Logs::LogGroup",
   "Properties": {
    "LogGroupName": "/ecs/newsinsight/dev/embedding-server",
    "RetentionInDays": 7,
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/LogGroup-embedding-server/Resource"
   }
  },
  "TaskDefembeddingserver60AA534A": {
   "Type": "AWS::ECS::TaskDefinition",
   "Properties": {
    "ContainerDefinitions": [
     {
      "Environment": [
       {
        "Name": "ENVIRONMENT",
        "Value": "dev"
       },
       {
        "Name": "NODE_ENV",
        "Value": "development"
       },
       {
        "Name": "POSTGRES_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttPostgresClusterE64235C6EndpointAddress8684A102"
        }
       },
       {
        "Name": "POSTGRES_PORT",
        "Value": "5432"
       },
       {
        "Name": "MONGO_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttDocDbClusterB46CF5D3EndpointD6B42638"
        }
       },
       {
        "Name": "MONGO_PORT",
        "Value": "27017"
       },
       {
        "Name": "REDIS_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttRedisClusterPrimaryEndPointAddressCFB16E1D"
        }
       },
       {
        "Name": "REDIS_PORT",
        "Value": "6379"
       },
       {
        "Name": "API_GATEWAY_URL",
        "Value": "http://api-gateway.newsinsight.local:8000"
       },
       {
        "Name": "COLLECTOR_SERVICE_URL",
        "Value": "http://collector-service.newsinsight.local:8081"
       },
       {
        "Name": "BROWSER_USE_API_URL",
        "Value": "http://browser-use-api.newsinsight.local:8500"
       },
       {
        "Name": "AUTONOMOUS_CRAWLER_URL",
        "Value": "http://autonomous-crawler.newsinsight.local:8030"
       },
       {
        "Name": "ADMIN_DASHBOARD_URL",
        "Value": "http://admin-dashboard.newsinsight.local:8888"
       },
       {
        "Name": "NEWSINSIGHT_MCP_URL",
        "Value": "http://newsinsight-mcp.newsinsight.local:5000"
       },
       {
        "Name": "BIAS_MCP_URL",
        "Value": "http://bias-mcp.newsinsight.local:5001"
       },
       {
        "Name": "FACTCHECK_MCP_URL",
        "Value": "http://factcheck-mcp.newsinsight.local:5002"
       },
       {
        "Name": "TOPIC_MCP_URL",
        "Value": "http://topic-mcp.newsinsight.local:5003"
       },
       {
        "Name": "AIAGENT_MCP_URL",
        "Value": "http://aiagent-mcp.newsinsight.local:5004"
       }
      ],
      "Essential": true,
      "Image": {
       "Fn::Join": [
        "",
        [
         {
          "Fn::Select": [
           4,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepoembeddingserver9F2A1DDBArn73189998"
             }
            ]
           }
          ]
         },
         ".dkr.ecr.",
         {
          "Fn::Select": [
           3,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepoembeddingserver9F2A1DDBArn73189998"
             }
            ]
           }
          ]
         },
         ".",
         {
          "Ref": "AWS::URLSuffix"
         },
         "/",
         {
          "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputRefRepoembeddingserver9F2A1DDB7BCB9520"
         },
         ":latest"
        ]
       ]
      },
      "LogConfiguration": {
       "LogDriver": "awslogs",
       "Options": {
        "awslogs-group": {
         "Ref": "LogGroupembeddingserverE55FA114"
        },
        "awslogs-stream-prefix": "embedding-server",
        "awslogs-region": "ap-northeast-2"
       }
      },
      "Name": "embedding-server",
      "PortMappings": [
       {
        "ContainerPort": 8011,
        "Protocol": "tcp"
       }
      ],
      "Secrets": [
       {
        "Name": "POSTGRES_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "POSTGRES_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "REDIS_PASSWORD",
        "ValueFrom": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefRedisSecretCB3BD9716380542C"
        }
       }
      ]
     }
    ],
    "Cpu": "2048",
    "ExecutionRoleArn": {
     "Fn::GetAtt": [
      "ExecutionRole605A040B",
      "Arn"
     ]
    },
    "Family": "newsinsight-embedding-server-dev",
    "Memory": "4096",
    "NetworkMode": "awsvpc",
    "RequiresCompatibilities": [
     "FARGATE"
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskRoleArn": {
     "Fn::GetAtt": [
      "TaskRole30FC0FBB",
      "Arn"
     ]
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/TaskDef-embedding-server/Resource"
   }
  },
  "ServiceembeddingserverService08C7F7F1": {
   "Type": "AWS::ECS::Service",
   "Properties": {
    "Cluster": {
     "Ref": "ClusterEB0386A7"
    },
    "DeploymentConfiguration": {
     "Alarms": {
      "AlarmNames": [],
      "Enable": false,
      "Rollback": false
     },
     "DeploymentCircuitBreaker": {
      "Enable": true,
      "Rollback": true
     },
     "MaximumPercent": 200,
     "MinimumHealthyPercent": 50
    },
    "DesiredCount": 1,
    "EnableECSManagedTags": false,
    "LaunchType": "FARGATE",
    "NetworkConfiguration": {
     "AwsvpcConfiguration": {
      "AssignPublicIp": "DISABLED",
      "SecurityGroups": [
       {
        "Fn::GetAtt": [
         "ServiceSg2632F788",
         "GroupId"
        ]
       }
      ],
      "Subnets": [
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet1Subnet2291D3DF12E84F6D"
       },
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet2Subnet4441BE119DFCFE3A"
       }
      ]
     }
    },
    "ServiceName": "newsinsight-embedding-server-dev",
    "ServiceRegistries": [
     {
      "RegistryArn": {
       "Fn::GetAtt": [
        "ServiceembeddingserverCloudmapServiceFC7096E4",
        "Arn"
       ]
      }
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskDefinition": {
     "Ref": "TaskDefembeddingserver60AA534A"
    }
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-embedding-server/Service"
   }
  },
  "ServiceembeddingserverCloudmapServiceFC7096E4": {
   "Type": "AWS::ServiceDiscovery::Service",
   "Properties": {
    "DnsConfig": {
     "DnsRecords": [
      {
       "TTL": 60,
       "Type": "A"
      }
     ],
     "NamespaceId": {
      "Fn::GetAtt": [
       "Namespace9B63B8C8",
       "Id"
      ]
     },
     "RoutingPolicy": "MULTIVALUE"
    },
    "HealthCheckCustomConfig": {
     "FailureThreshold": 1
    },
    "Name": "embedding-server",
    "NamespaceId": {
     "Fn::GetAtt": [
      "Namespace9B63B8C8",
      "Id"
     ]
    },
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-embedding-server/CloudmapService/Resource"
   }
  },
  "LogGroupsentimentaddonFB260201": {
   "Type": "AWS::Logs::LogGroup",
   "Properties": {
    "LogGroupName": "/ecs/newsinsight/dev/sentiment-addon",
    "RetentionInDays": 7,
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/LogGroup-sentiment-addon/Resource"
   }
  },
  "TaskDefsentimentaddon99644034": {
   "Type": "AWS::ECS::TaskDefinition",
   "Properties": {
    "ContainerDefinitions": [
     {
      "Environment": [
       {
        "Name": "ENVIRONMENT",
        "Value": "dev"
       },
       {
        "Name": "NODE_ENV",
        "Value": "development"
       },
       {
        "Name": "POSTGRES_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttPostgresClusterE64235C6EndpointAddress8684A102"
        }
       },
       {
        "Name": "POSTGRES_PORT",
        "Value": "5432"
       },
       {
        "Name": "MONGO_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttDocDbClusterB46CF5D3EndpointD6B42638"
        }
       },
       {
        "Name": "MONGO_PORT",
        "Value": "27017"
       },
       {
        "Name": "REDIS_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttRedisClusterPrimaryEndPointAddressCFB16E1D"
        }
       },
       {
        "Name": "REDIS_PORT",
        "Value": "6379"
       },
       {
        "Name": "API_GATEWAY_URL",
        "Value": "http://api-gateway.newsinsight.local:8000"
       },
       {
        "Name": "COLLECTOR_SERVICE_URL",
        "Value": "http://collector-service.newsinsight.local:8081"
       },
       {
        "Name": "BROWSER_USE_API_URL",
        "Value": "http://browser-use-api.newsinsight.local:8500"
       },
       {
        "Name": "AUTONOMOUS_CRAWLER_URL",
        "Value": "http://autonomous-crawler.newsinsight.local:8030"
       },
       {
        "Name": "ADMIN_DASHBOARD_URL",
        "Value": "http://admin-dashboard.newsinsight.local:8888"
       },
       {
        "Name": "NEWSINSIGHT_MCP_URL",
        "Value": "http://newsinsight-mcp.newsinsight.local:5000"
       },
       {
        "Name": "BIAS_MCP_URL",
        "Value": "http://bias-mcp.newsinsight.local:5001"
       },
       {
        "Name": "FACTCHECK_MCP_URL",
        "Value": "http://factcheck-mcp.newsinsight.local:5002"
       },
       {
        "Name": "TOPIC_MCP_URL",
        "Value": "http://topic-mcp.newsinsight.local:5003"
       },
       {
        "Name": "AIAGENT_MCP_URL",
        "Value": "http://aiagent-mcp.newsinsight.local:5004"
       }
      ],
      "Essential": true,
      "Image": {
       "Fn::Join": [
        "",
        [
         {
          "Fn::Select": [
           4,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttReposentimentaddon60A21608ArnE16A9985"
             }
            ]
           }
          ]
         },
         ".dkr.ecr.",
         {
          "Fn::Select": [
           3,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttReposentimentaddon60A21608ArnE16A9985"
             }
            ]
           }
          ]
         },
         ".",
         {
          "Ref": "AWS::URLSuffix"
         },
         "/",
         {
          "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputRefReposentimentaddon60A216081DA0C31A"
         },
         ":latest"
        ]
       ]
      },
      "LogConfiguration": {
       "LogDriver": "awslogs",
       "Options": {
        "awslogs-group": {
         "Ref": "LogGroupsentimentaddonFB260201"
        },
        "awslogs-stream-prefix": "sentiment-addon",
        "awslogs-region": "ap-northeast-2"
       }
      },
      "Name": "sentiment-addon",
      "PortMappings": [
       {
        "ContainerPort": 8100,
        "Protocol": "tcp"
       }
      ],
      "Secrets": [
       {
        "Name": "POSTGRES_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "POSTGRES_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "REDIS_PASSWORD",
        "ValueFrom": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefRedisSecretCB3BD9716380542C"
        }
       }
      ]
     }
    ],
    "Cpu": "512",
    "ExecutionRoleArn": {
     "Fn::GetAtt": [
      "ExecutionRole605A040B",
      "Arn"
     ]
    },
    "Family": "newsinsight-sentiment-addon-dev",
    "Memory": "1024",
    "NetworkMode": "awsvpc",
    "RequiresCompatibilities": [
     "FARGATE"
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskRoleArn": {
     "Fn::GetAtt": [
      "TaskRole30FC0FBB",
      "Arn"
     ]
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/TaskDef-sentiment-addon/Resource"
   }
  },
  "ServicesentimentaddonServiceA89B8032": {
   "Type": "AWS::ECS::Service",
   "Properties": {
    "Cluster": {
     "Ref": "ClusterEB0386A7"
    },
    "DeploymentConfiguration": {
     "Alarms": {
      "AlarmNames": [],
      "Enable": false,
      "Rollback": false
     },
     "DeploymentCircuitBreaker": {
      "Enable": true,
      "Rollback": true
     },
     "MaximumPercent": 200,
     "MinimumHealthyPercent": 50
    },
    "DesiredCount": 1,
    "EnableECSManagedTags": false,
    "LaunchType": "FARGATE",
    "NetworkConfiguration": {
     "AwsvpcConfiguration": {
      "AssignPublicIp": "DISABLED",
      "SecurityGroups": [
       {
        "Fn::GetAtt": [
         "ServiceSg2632F788",
         "GroupId"
        ]
       }
      ],
      "Subnets": [
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet1Subnet2291D3DF12E84F6D"
       },
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet2Subnet4441BE119DFCFE3A"
       }
      ]
     }
    },
    "ServiceName": "newsinsight-sentiment-addon-dev",
    "ServiceRegistries": [
     {
      "RegistryArn": {
       "Fn::GetAtt": [
        "ServicesentimentaddonCloudmapService88F0351A",
        "Arn"
       ]
      }
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskDefinition": {
     "Ref": "TaskDefsentimentaddon99644034"
    }
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-sentiment-addon/Service"
   }
  },
  "ServicesentimentaddonCloudmapService88F0351A": {
   "Type": "AWS::ServiceDiscovery::Service",
   "Properties": {
    "DnsConfig": {
     "DnsRecords": [
      {
       "TTL": 60,
       "Type": "A"
      }
     ],
     "NamespaceId": {
      "Fn::GetAtt": [
       "Namespace9B63B8C8",
       "Id"
      ]
     },
     "RoutingPolicy": "MULTIVALUE"
    },
    "HealthCheckCustomConfig": {
     "FailureThreshold": 1
    },
    "Name": "sentiment-addon",
    "NamespaceId": {
     "Fn::GetAtt": [
      "Namespace9B63B8C8",
      "Id"
     ]
    },
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-sentiment-addon/CloudmapService/Resource"
   }
  },
  "LogGroupfactcheckaddonCEF1EF63": {
   "Type": "AWS::Logs::LogGroup",
   "Properties": {
    "LogGroupName": "/ecs/newsinsight/dev/factcheck-addon",
    "RetentionInDays": 7,
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/LogGroup-factcheck-addon/Resource"
   }
  },
  "TaskDeffactcheckaddon2450A1AA": {
   "Type": "AWS::ECS::TaskDefinition",
   "Properties": {
    "ContainerDefinitions": [
     {
      "Environment": [
       {
        "Name": "ENVIRONMENT",
        "Value": "dev"
       },
       {
        "Name": "NODE_ENV",
        "Value": "development"
       },
       {
        "Name": "POSTGRES_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttPostgresClusterE64235C6EndpointAddress8684A102"
        }
       },
       {
        "Name": "POSTGRES_PORT",
        "Value": "5432"
       },
       {
        "Name": "MONGO_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttDocDbClusterB46CF5D3EndpointD6B42638"
        }
       },
       {
        "Name": "MONGO_PORT",
        "Value": "27017"
       },
       {
        "Name": "REDIS_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttRedisClusterPrimaryEndPointAddressCFB16E1D"
        }
       },
       {
        "Name": "REDIS_PORT",
        "Value": "6379"
       },
       {
        "Name": "API_GATEWAY_URL",
        "Value": "http://api-gateway.newsinsight.local:8000"
       },
       {
        "Name": "COLLECTOR_SERVICE_URL",
        "Value": "http://collector-service.newsinsight.local:8081"
       },
       {
        "Name": "BROWSER_USE_API_URL",
        "Value": "http://browser-use-api.newsinsight.local:8500"
       },
       {
        "Name": "AUTONOMOUS_CRAWLER_URL",
        "Value": "http://autonomous-crawler.newsinsight.local:8030"
       },
       {
        "Name": "ADMIN_DASHBOARD_URL",
        "Value": "http://admin-dashboard.newsinsight.local:8888"
       },
       {
        "Name": "NEWSINSIGHT_MCP_URL",
        "Value": "http://newsinsight-mcp.newsinsight.local:5000"
       },
       {
        "Name": "BIAS_MCP_URL",
        "Value": "http://bias-mcp.newsinsight.local:5001"
       },
       {
        "Name": "FACTCHECK_MCP_URL",
        "Value": "http://factcheck-mcp.newsinsight.local:5002"
       },
       {
        "Name": "TOPIC_MCP_URL",
        "Value": "http://topic-mcp.newsinsight.local:5003"
       },
       {
        "Name": "AIAGENT_MCP_URL",
        "Value": "http://aiagent-mcp.newsinsight.local:5004"
       }
      ],
      "Essential": true,
      "Image": {
       "Fn::Join": [
        "",
        [
         {
          "Fn::Select": [
           4,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepofactcheckaddonCA82F5DCArn7463D4FC"
             }
            ]
           }
          ]
         },
         ".dkr.ecr.",
         {
          "Fn::Select": [
           3,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepofactcheckaddonCA82F5DCArn7463D4FC"
             }
            ]
           }
          ]
         },
         ".",
         {
          "Ref": "AWS::URLSuffix"
         },
         "/",
         {
          "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputRefRepofactcheckaddonCA82F5DCEE00F7C6"
         },
         ":latest"
        ]
       ]
      },
      "LogConfiguration": {
       "LogDriver": "awslogs",
       "Options": {
        "awslogs-group": {
         "Ref": "LogGroupfactcheckaddonCEF1EF63"
        },
        "awslogs-stream-prefix": "factcheck-addon",
        "awslogs-region": "ap-northeast-2"
       }
      },
      "Name": "factcheck-addon",
      "PortMappings": [
       {
        "ContainerPort": 8101,
        "Protocol": "tcp"
       }
      ],
      "Secrets": [
       {
        "Name": "POSTGRES_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "POSTGRES_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "REDIS_PASSWORD",
        "ValueFrom": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefRedisSecretCB3BD9716380542C"
        }
       }
      ]
     }
    ],
    "Cpu": "512",
    "ExecutionRoleArn": {
     "Fn::GetAtt": [
      "ExecutionRole605A040B",
      "Arn"
     ]
    },
    "Family": "newsinsight-factcheck-addon-dev",
    "Memory": "1024",
    "NetworkMode": "awsvpc",
    "RequiresCompatibilities": [
     "FARGATE"
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskRoleArn": {
     "Fn::GetAtt": [
      "TaskRole30FC0FBB",
      "Arn"
     ]
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/TaskDef-factcheck-addon/Resource"
   }
  },
  "ServicefactcheckaddonService8B4EE486": {
   "Type": "AWS::ECS::Service",
   "Properties": {
    "Cluster": {
     "Ref": "ClusterEB0386A7"
    },
    "DeploymentConfiguration": {
     "Alarms": {
      "AlarmNames": [],
      "Enable": false,
      "Rollback": false
     },
     "DeploymentCircuitBreaker": {
      "Enable": true,
      "Rollback": true
     },
     "MaximumPercent": 200,
     "MinimumHealthyPercent": 50
    },
    "DesiredCount": 1,
    "EnableECSManagedTags": false,
    "LaunchType": "FARGATE",
    "NetworkConfiguration": {
     "AwsvpcConfiguration": {
      "AssignPublicIp": "DISABLED",
      "SecurityGroups": [
       {
        "Fn::GetAtt": [
         "ServiceSg2632F788",
         "GroupId"
        ]
       }
      ],
      "Subnets": [
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet1Subnet2291D3DF12E84F6D"
       },
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet2Subnet4441BE119DFCFE3A"
       }
      ]
     }
    },
    "ServiceName": "newsinsight-factcheck-addon-dev",
    "ServiceRegistries": [
     {
      "RegistryArn": {
       "Fn::GetAtt": [
        "ServicefactcheckaddonCloudmapService871A7D5F",
        "Arn"
       ]
      }
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskDefinition": {
     "Ref": "TaskDeffactcheckaddon2450A1AA"
    }
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-factcheck-addon/Service"
   }
  },
  "ServicefactcheckaddonCloudmapService871A7D5F": {
   "Type": "AWS::ServiceDiscovery::Service",
   "Properties": {
    "DnsConfig": {
     "DnsRecords": [
      {
       "TTL": 60,
       "Type": "A"
      }
     ],
     "NamespaceId": {
      "Fn::GetAtt": [
       "Namespace9B63B8C8",
       "Id"
      ]
     },
     "RoutingPolicy": "MULTIVALUE"
    },
    "HealthCheckCustomConfig": {
     "FailureThreshold": 1
    },
    "Name": "factcheck-addon",
    "NamespaceId": {
     "Fn::GetAtt": [
      "Namespace9B63B8C8",
      "Id"
     ]
    },
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-factcheck-addon/CloudmapService/Resource"
   }
  },
  "LogGroupbiasaddon2891F993": {
   "Type": "AWS::Logs::LogGroup",
   "Properties": {
    "LogGroupName": "/ecs/newsinsight/dev/bias-addon",
    "RetentionInDays": 7,
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/LogGroup-bias-addon/Resource"
   }
  },
  "TaskDefbiasaddonA3A05EAD": {
   "Type": "AWS::ECS::TaskDefinition",
   "Properties": {
    "ContainerDefinitions": [
     {
      "Environment": [
       {
        "Name": "ENVIRONMENT",
        "Value": "dev"
       },
       {
        "Name": "NODE_ENV",
        "Value": "development"
       },
       {
        "Name": "POSTGRES_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttPostgresClusterE64235C6EndpointAddress8684A102"
        }
       },
       {
        "Name": "POSTGRES_PORT",
        "Value": "5432"
       },
       {
        "Name": "MONGO_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttDocDbClusterB46CF5D3EndpointD6B42638"
        }
       },
       {
        "Name": "MONGO_PORT",
        "Value": "27017"
       },
       {
        "Name": "REDIS_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttRedisClusterPrimaryEndPointAddressCFB16E1D"
        }
       },
       {
        "Name": "REDIS_PORT",
        "Value": "6379"
       },
       {
        "Name": "API_GATEWAY_URL",
        "Value": "http://api-gateway.newsinsight.local:8000"
       },
       {
        "Name": "COLLECTOR_SERVICE_URL",
        "Value": "http://collector-service.newsinsight.local:8081"
       },
       {
        "Name": "BROWSER_USE_API_URL",
        "Value": "http://browser-use-api.newsinsight.local:8500"
       },
       {
        "Name": "AUTONOMOUS_CRAWLER_URL",
        "Value": "http://autonomous-crawler.newsinsight.local:8030"
       },
       {
        "Name": "ADMIN_DASHBOARD_URL",
        "Value": "http://admin-dashboard.newsinsight.local:8888"
       },
       {
        "Name": "NEWSINSIGHT_MCP_URL",
        "Value": "http://newsinsight-mcp.newsinsight.local:5000"
       },
       {
        "Name": "BIAS_MCP_URL",
        "Value": "http://bias-mcp.newsinsight.local:5001"
       },
       {
        "Name": "FACTCHECK_MCP_URL",
        "Value": "http://factcheck-mcp.newsinsight.local:5002"
       },
       {
        "Name": "TOPIC_MCP_URL",
        "Value": "http://topic-mcp.newsinsight.local:5003"
       },
       {
        "Name": "AIAGENT_MCP_URL",
        "Value": "http://aiagent-mcp.newsinsight.local:5004"
       }
      ],
      "Essential": true,
      "Image": {
       "Fn::Join": [
        "",
        [
         {
          "Fn::Select": [
           4,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepobiasaddonDDD4DB8BArn2D0CFE8D"
             }
            ]
           }
          ]
         },
         ".dkr.ecr.",
         {
          "Fn::Select": [
           3,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepobiasaddonDDD4DB8BArn2D0CFE8D"
             }
            ]
           }
          ]
         },
         ".",
         {
          "Ref": "AWS::URLSuffix"
         },
         "/",
         {
          "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputRefRepobiasaddonDDD4DB8B793612F8"
         },
         ":latest"
        ]
       ]
      },
      "LogConfiguration": {
       "LogDriver": "awslogs",
       "Options": {
        "awslogs-group": {
         "Ref": "LogGroupbiasaddon2891F993"
        },
        "awslogs-stream-prefix": "bias-addon",
        "awslogs-region": "ap-northeast-2"
       }
      },
      "Name": "bias-addon",
      "PortMappings": [
       {
        "ContainerPort": 8102,
        "Protocol": "tcp"
       }
      ],
      "Secrets": [
       {
        "Name": "POSTGRES_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "POSTGRES_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "REDIS_PASSWORD",
        "ValueFrom": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefRedisSecretCB3BD9716380542C"
        }
       }
      ]
     }
    ],
    "Cpu": "512",
    "ExecutionRoleArn": {
     "Fn::GetAtt": [
      "ExecutionRole605A040B",
      "Arn"
     ]
    },
    "Family": "newsinsight-bias-addon-dev",
    "Memory": "1024",
    "NetworkMode": "awsvpc",
    "RequiresCompatibilities": [
     "FARGATE"
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskRoleArn": {
     "Fn::GetAtt": [
      "TaskRole30FC0FBB",
      "Arn"
     ]
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/TaskDef-bias-addon/Resource"
   }
  },
  "ServicebiasaddonService31D8D86D": {
   "Type": "AWS::ECS::Service",
   "Properties": {
    "Cluster": {
     "Ref": "ClusterEB0386A7"
    },
    "DeploymentConfiguration": {
     "Alarms": {
      "AlarmNames": [],
      "Enable": false,
      "Rollback": false
     },
     "DeploymentCircuitBreaker": {
      "Enable": true,
      "Rollback": true
     },
     "MaximumPercent": 200,
     "MinimumHealthyPercent": 50
    },
    "DesiredCount": 1,
    "EnableECSManagedTags": false,
    "LaunchType": "FARGATE",
    "NetworkConfiguration": {
     "AwsvpcConfiguration": {
      "AssignPublicIp": "DISABLED",
      "SecurityGroups": [
       {
        "Fn::GetAtt": [
         "ServiceSg2632F788",
         "GroupId"
        ]
       }
      ],
      "Subnets": [
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet1Subnet2291D3DF12E84F6D"
       },
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet2Subnet4441BE119DFCFE3A"
       }
      ]
     }
    },
    "ServiceName": "newsinsight-bias-addon-dev",
    "ServiceRegistries": [
     {
      "RegistryArn": {
       "Fn::GetAtt": [
        "ServicebiasaddonCloudmapServiceA6D4DB6E",
        "Arn"
       ]
      }
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskDefinition": {
     "Ref": "TaskDefbiasaddonA3A05EAD"
    }
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-bias-addon/Service"
   }
  },
  "ServicebiasaddonCloudmapServiceA6D4DB6E": {
   "Type": "AWS::ServiceDiscovery::Service",
   "Properties": {
    "DnsConfig": {
     "DnsRecords": [
      {
       "TTL": 60,
       "Type": "A"
      }
     ],
     "NamespaceId": {
      "Fn::GetAtt": [
       "Namespace9B63B8C8",
       "Id"
      ]
     },
     "RoutingPolicy": "MULTIVALUE"
    },
    "HealthCheckCustomConfig": {
     "FailureThreshold": 1
    },
    "Name": "bias-addon",
    "NamespaceId": {
     "Fn::GetAtt": [
      "Namespace9B63B8C8",
      "Id"
     ]
    },
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-bias-addon/CloudmapService/Resource"
   }
  },
  "LogGroupnewsinsightmcp83D7554A": {
   "Type": "AWS::Logs::LogGroup",
   "Properties": {
    "LogGroupName": "/ecs/newsinsight/dev/newsinsight-mcp",
    "RetentionInDays": 7,
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/LogGroup-newsinsight-mcp/Resource"
   }
  },
  "TaskDefnewsinsightmcpB5CD37EE": {
   "Type": "AWS::ECS::TaskDefinition",
   "Properties": {
    "ContainerDefinitions": [
     {
      "Environment": [
       {
        "Name": "ENVIRONMENT",
        "Value": "dev"
       },
       {
        "Name": "NODE_ENV",
        "Value": "development"
       },
       {
        "Name": "POSTGRES_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttPostgresClusterE64235C6EndpointAddress8684A102"
        }
       },
       {
        "Name": "POSTGRES_PORT",
        "Value": "5432"
       },
       {
        "Name": "MONGO_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttDocDbClusterB46CF5D3EndpointD6B42638"
        }
       },
       {
        "Name": "MONGO_PORT",
        "Value": "27017"
       },
       {
        "Name": "REDIS_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttRedisClusterPrimaryEndPointAddressCFB16E1D"
        }
       },
       {
        "Name": "REDIS_PORT",
        "Value": "6379"
       },
       {
        "Name": "API_GATEWAY_URL",
        "Value": "http://api-gateway.newsinsight.local:8000"
       },
       {
        "Name": "COLLECTOR_SERVICE_URL",
        "Value": "http://collector-service.newsinsight.local:8081"
       },
       {
        "Name": "BROWSER_USE_API_URL",
        "Value": "http://browser-use-api.newsinsight.local:8500"
       },
       {
        "Name": "AUTONOMOUS_CRAWLER_URL",
        "Value": "http://autonomous-crawler.newsinsight.local:8030"
       },
       {
        "Name": "ADMIN_DASHBOARD_URL",
        "Value": "http://admin-dashboard.newsinsight.local:8888"
       },
       {
        "Name": "NEWSINSIGHT_MCP_URL",
        "Value": "http://newsinsight-mcp.newsinsight.local:5000"
       },
       {
        "Name": "BIAS_MCP_URL",
        "Value": "http://bias-mcp.newsinsight.local:5001"
       },
       {
        "Name": "FACTCHECK_MCP_URL",
        "Value": "http://factcheck-mcp.newsinsight.local:5002"
       },
       {
        "Name": "TOPIC_MCP_URL",
        "Value": "http://topic-mcp.newsinsight.local:5003"
       },
       {
        "Name": "AIAGENT_MCP_URL",
        "Value": "http://aiagent-mcp.newsinsight.local:5004"
       }
      ],
      "Essential": true,
      "Image": {
       "Fn::Join": [
        "",
        [
         {
          "Fn::Select": [
           4,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttReponewsinsightmcp5EF0CA8CArnCB03D924"
             }
            ]
           }
          ]
         },
         ".dkr.ecr.",
         {
          "Fn::Select": [
           3,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttReponewsinsightmcp5EF0CA8CArnCB03D924"
             }
            ]
           }
          ]
         },
         ".",
         {
          "Ref": "AWS::URLSuffix"
         },
         "/",
         {
          "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputRefReponewsinsightmcp5EF0CA8C218ABECA"
         },
         ":latest"
        ]
       ]
      },
      "LogConfiguration": {
       "LogDriver": "awslogs",
       "Options": {
        "awslogs-group": {
         "Ref": "LogGroupnewsinsightmcp83D7554A"
        },
        "awslogs-stream-prefix": "newsinsight-mcp",
        "awslogs-region": "ap-northeast-2"
       }
      },
      "Name": "newsinsight-mcp",
      "PortMappings": [
       {
        "ContainerPort": 5000,
        "Protocol": "tcp"
       }
      ],
      "Secrets": [
       {
        "Name": "POSTGRES_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "POSTGRES_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "REDIS_PASSWORD",
        "ValueFrom": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefRedisSecretCB3BD9716380542C"
        }
       }
      ]
     }
    ],
    "Cpu": "256",
    "ExecutionRoleArn": {
     "Fn::GetAtt": [
      "ExecutionRole605A040B",
      "Arn"
     ]
    },
    "Family": "newsinsight-newsinsight-mcp-dev",
    "Memory": "512",
    "NetworkMode": "awsvpc",
    "RequiresCompatibilities": [
     "FARGATE"
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskRoleArn": {
     "Fn::GetAtt": [
      "TaskRole30FC0FBB",
      "Arn"
     ]
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/TaskDef-newsinsight-mcp/Resource"
   }
  },
  "ServicenewsinsightmcpService2CA27FFA": {
   "Type": "AWS::ECS::Service",
   "Properties": {
    "Cluster": {
     "Ref": "ClusterEB0386A7"
    },
    "DeploymentConfiguration": {
     "Alarms": {
      "AlarmNames": [],
      "Enable": false,
      "Rollback": false
     },
     "DeploymentCircuitBreaker": {
      "Enable": true,
      "Rollback": true
     },
     "MaximumPercent": 200,
     "MinimumHealthyPercent": 50
    },
    "DesiredCount": 1,
    "EnableECSManagedTags": false,
    "LaunchType": "FARGATE",
    "NetworkConfiguration": {
     "AwsvpcConfiguration": {
      "AssignPublicIp": "DISABLED",
      "SecurityGroups": [
       {
        "Fn::GetAtt": [
         "ServiceSg2632F788",
         "GroupId"
        ]
       }
      ],
      "Subnets": [
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet1Subnet2291D3DF12E84F6D"
       },
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet2Subnet4441BE119DFCFE3A"
       }
      ]
     }
    },
    "ServiceName": "newsinsight-newsinsight-mcp-dev",
    "ServiceRegistries": [
     {
      "RegistryArn": {
       "Fn::GetAtt": [
        "ServicenewsinsightmcpCloudmapService2100E124",
        "Arn"
       ]
      }
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskDefinition": {
     "Ref": "TaskDefnewsinsightmcpB5CD37EE"
    }
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-newsinsight-mcp/Service"
   }
  },
  "ServicenewsinsightmcpCloudmapService2100E124": {
   "Type": "AWS::ServiceDiscovery::Service",
   "Properties": {
    "DnsConfig": {
     "DnsRecords": [
      {
       "TTL": 60,
       "Type": "A"
      }
     ],
     "NamespaceId": {
      "Fn::GetAtt": [
       "Namespace9B63B8C8",
       "Id"
      ]
     },
     "RoutingPolicy": "MULTIVALUE"
    },
    "HealthCheckCustomConfig": {
     "FailureThreshold": 1
    },
    "Name": "newsinsight-mcp",
    "NamespaceId": {
     "Fn::GetAtt": [
      "Namespace9B63B8C8",
      "Id"
     ]
    },
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-newsinsight-mcp/CloudmapService/Resource"
   }
  },
  "LogGroupbiasmcp18501486": {
   "Type": "AWS::Logs::LogGroup",
   "Properties": {
    "LogGroupName": "/ecs/newsinsight/dev/bias-mcp",
    "RetentionInDays": 7,
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/LogGroup-bias-mcp/Resource"
   }
  },
  "TaskDefbiasmcpE22FF622": {
   "Type": "AWS::ECS::TaskDefinition",
   "Properties": {
    "ContainerDefinitions": [
     {
      "Environment": [
       {
        "Name": "ENVIRONMENT",
        "Value": "dev"
       },
       {
        "Name": "NODE_ENV",
        "Value": "development"
       },
       {
        "Name": "POSTGRES_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttPostgresClusterE64235C6EndpointAddress8684A102"
        }
       },
       {
        "Name": "POSTGRES_PORT",
        "Value": "5432"
       },
       {
        "Name": "MONGO_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttDocDbClusterB46CF5D3EndpointD6B42638"
        }
       },
       {
        "Name": "MONGO_PORT",
        "Value": "27017"
       },
       {
        "Name": "REDIS_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttRedisClusterPrimaryEndPointAddressCFB16E1D"
        }
       },
       {
        "Name": "REDIS_PORT",
        "Value": "6379"
       },
       {
        "Name": "API_GATEWAY_URL",
        "Value": "http://api-gateway.newsinsight.local:8000"
       },
       {
        "Name": "COLLECTOR_SERVICE_URL",
        "Value": "http://collector-service.newsinsight.local:8081"
       },
       {
        "Name": "BROWSER_USE_API_URL",
        "Value": "http://browser-use-api.newsinsight.local:8500"
       },
       {
        "Name": "AUTONOMOUS_CRAWLER_URL",
        "Value": "http://autonomous-crawler.newsinsight.local:8030"
       },
       {
        "Name": "ADMIN_DASHBOARD_URL",
        "Value": "http://admin-dashboard.newsinsight.local:8888"
       },
       {
        "Name": "NEWSINSIGHT_MCP_URL",
        "Value": "http://newsinsight-mcp.newsinsight.local:5000"
       },
       {
        "Name": "BIAS_MCP_URL",
        "Value": "http://bias-mcp.newsinsight.local:5001"
       },
       {
        "Name": "FACTCHECK_MCP_URL",
        "Value": "http://factcheck-mcp.newsinsight.local:5002"
       },
       {
        "Name": "TOPIC_MCP_URL",
        "Value": "http://topic-mcp.newsinsight.local:5003"
       },
       {
        "Name": "AIAGENT_MCP_URL",
        "Value": "http://aiagent-mcp.newsinsight.local:5004"
       }
      ],
      "Essential": true,
      "Image": {
       "Fn::Join": [
        "",
        [
         {
          "Fn::Select": [
           4,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepobiasmcp676781E9Arn284502B0"
             }
            ]
           }
          ]
         },
         ".dkr.ecr.",
         {
          "Fn::Select": [
           3,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepobiasmcp676781E9Arn284502B0"
             }
            ]
           }
          ]
         },
         ".",
         {
          "Ref": "AWS::URLSuffix"
         },
         "/",
         {
          "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputRefRepobiasmcp676781E986BBFE39"
         },
         ":latest"
        ]
       ]
      },
      "LogConfiguration": {
       "LogDriver": "awslogs",
       "Options": {
        "awslogs-group": {
         "Ref": "LogGroupbiasmcp18501486"
        },
        "awslogs-stream-prefix": "bias-mcp",
        "awslogs-region": "ap-northeast-2"
       }
      },
      "Name": "bias-mcp",
      "PortMappings": [
       {
        "ContainerPort": 5001,
        "Protocol": "tcp"
       }
      ],
      "Secrets": [
       {
        "Name": "POSTGRES_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "POSTGRES_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "REDIS_PASSWORD",
        "ValueFrom": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefRedisSecretCB3BD9716380542C"
        }
       }
      ]
     }
    ],
    "Cpu": "256",
    "ExecutionRoleArn": {
     "Fn::GetAtt": [
      "ExecutionRole605A040B",
      "Arn"
     ]
    },
    "Family": "newsinsight-bias-mcp-dev",
    "Memory": "512",
    "NetworkMode": "awsvpc",
    "RequiresCompatibilities": [
     "FARGATE"
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskRoleArn": {
     "Fn::GetAtt": [
      "TaskRole30FC0FBB",
      "Arn"
     ]
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/TaskDef-bias-mcp/Resource"
   }
  },
  "ServicebiasmcpService796D0C13": {
   "Type": "AWS::ECS::Service",
   "Properties": {
    "Cluster": {
     "Ref": "ClusterEB0386A7"
    },
    "DeploymentConfiguration": {
     "Alarms": {
      "AlarmNames": [],
      "Enable": false,
      "Rollback": false
     },
     "DeploymentCircuitBreaker": {
      "Enable": true,
      "Rollback": true
     },
     "MaximumPercent": 200,
     "MinimumHealthyPercent": 50
    },
    "DesiredCount": 1,
    "EnableECSManagedTags": false,
    "LaunchType": "FARGATE",
    "NetworkConfiguration": {
     "AwsvpcConfiguration": {
      "AssignPublicIp": "DISABLED",
      "SecurityGroups": [
       {
        "Fn::GetAtt": [
         "ServiceSg2632F788",
         "GroupId"
        ]
       }
      ],
      "Subnets": [
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet1Subnet2291D3DF12E84F6D"
       },
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet2Subnet4441BE119DFCFE3A"
       }
      ]
     }
    },
    "ServiceName": "newsinsight-bias-mcp-dev",
    "ServiceRegistries": [
     {
      "RegistryArn": {
       "Fn::GetAtt": [
        "ServicebiasmcpCloudmapServiceB12A3443",
        "Arn"
       ]
      }
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskDefinition": {
     "Ref": "TaskDefbiasmcpE22FF622"
    }
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-bias-mcp/Service"
   }
  },
  "ServicebiasmcpCloudmapServiceB12A3443": {
   "Type": "AWS::ServiceDiscovery::Service",
   "Properties": {
    "DnsConfig": {
     "DnsRecords": [
      {
       "TTL": 60,
       "Type": "A"
      }
     ],
     "NamespaceId": {
      "Fn::GetAtt": [
       "Namespace9B63B8C8",
       "Id"
      ]
     },
     "RoutingPolicy": "MULTIVALUE"
    },
    "HealthCheckCustomConfig": {
     "FailureThreshold": 1
    },
    "Name": "bias-mcp",
    "NamespaceId": {
     "Fn::GetAtt": [
      "Namespace9B63B8C8",
      "Id"
     ]
    },
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-bias-mcp/CloudmapService/Resource"
   }
  },
  "LogGroupfactcheckmcp74C0999F": {
   "Type": "AWS::Logs::LogGroup",
   "Properties": {
    "LogGroupName": "/ecs/newsinsight/dev/factcheck-mcp",
    "RetentionInDays": 7,
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/LogGroup-factcheck-mcp/Resource"
   }
  },
  "TaskDeffactcheckmcpB0E8594F": {
   "Type": "AWS::ECS::TaskDefinition",
   "Properties": {
    "ContainerDefinitions": [
     {
      "Environment": [
       {
        "Name": "ENVIRONMENT",
        "Value": "dev"
       },
       {
        "Name": "NODE_ENV",
        "Value": "development"
       },
       {
        "Name": "POSTGRES_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttPostgresClusterE64235C6EndpointAddress8684A102"
        }
       },
       {
        "Name": "POSTGRES_PORT",
        "Value": "5432"
       },
       {
        "Name": "MONGO_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttDocDbClusterB46CF5D3EndpointD6B42638"
        }
       },
       {
        "Name": "MONGO_PORT",
        "Value": "27017"
       },
       {
        "Name": "REDIS_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttRedisClusterPrimaryEndPointAddressCFB16E1D"
        }
       },
       {
        "Name": "REDIS_PORT",
        "Value": "6379"
       },
       {
        "Name": "API_GATEWAY_URL",
        "Value": "http://api-gateway.newsinsight.local:8000"
       },
       {
        "Name": "COLLECTOR_SERVICE_URL",
        "Value": "http://collector-service.newsinsight.local:8081"
       },
       {
        "Name": "BROWSER_USE_API_URL",
        "Value": "http://browser-use-api.newsinsight.local:8500"
       },
       {
        "Name": "AUTONOMOUS_CRAWLER_URL",
        "Value": "http://autonomous-crawler.newsinsight.local:8030"
       },
       {
        "Name": "ADMIN_DASHBOARD_URL",
        "Value": "http://admin-dashboard.newsinsight.local:8888"
       },
       {
        "Name": "NEWSINSIGHT_MCP_URL",
        "Value": "http://newsinsight-mcp.newsinsight.local:5000"
       },
       {
        "Name": "BIAS_MCP_URL",
        "Value": "http://bias-mcp.newsinsight.local:5001"
       },
       {
        "Name": "FACTCHECK_MCP_URL",
        "Value": "http://factcheck-mcp.newsinsight.local:5002"
       },
       {
        "Name": "TOPIC_MCP_URL",
        "Value": "http://topic-mcp.newsinsight.local:5003"
       },
       {
        "Name": "AIAGENT_MCP_URL",
        "Value": "http://aiagent-mcp.newsinsight.local:5004"
       }
      ],
      "Essential": true,
      "Image": {
       "Fn::Join": [
        "",
        [
         {
          "Fn::Select": [
           4,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepofactcheckmcp196A0DE5Arn5878A1A2"
             }
            ]
           }
          ]
         },
         ".dkr.ecr.",
         {
          "Fn::Select": [
           3,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepofactcheckmcp196A0DE5Arn5878A1A2"
             }
            ]
           }
          ]
         },
         ".",
         {
          "Ref": "AWS::URLSuffix"
         },
         "/",
         {
          "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputRefRepofactcheckmcp196A0DE50CA42BA1"
         },
         ":latest"
        ]
       ]
      },
      "LogConfiguration": {
       "LogDriver": "awslogs",
       "Options": {
        "awslogs-group": {
         "Ref": "LogGroupfactcheckmcp74C0999F"
        },
        "awslogs-stream-prefix": "factcheck-mcp",
        "awslogs-region": "ap-northeast-2"
       }
      },
      "Name": "factcheck-mcp",
      "PortMappings": [
       {
        "ContainerPort": 5002,
        "Protocol": "tcp"
       }
      ],
      "Secrets": [
       {
        "Name": "POSTGRES_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "POSTGRES_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "REDIS_PASSWORD",
        "ValueFrom": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefRedisSecretCB3BD9716380542C"
        }
       }
      ]
     }
    ],
    "Cpu": "256",
    "ExecutionRoleArn": {
     "Fn::GetAtt": [
      "ExecutionRole605A040B",
      "Arn"
     ]
    },
    "Family": "newsinsight-factcheck-mcp-dev",
    "Memory": "512",
    "NetworkMode": "awsvpc",
    "RequiresCompatibilities": [
     "FARGATE"
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskRoleArn": {
     "Fn::GetAtt": [
      "TaskRole30FC0FBB",
      "Arn"
     ]
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/TaskDef-factcheck-mcp/Resource"
   }
  },
  "ServicefactcheckmcpServiceC72EB134": {
   "Type": "AWS::ECS::Service",
   "Properties": {
    "Cluster": {
     "Ref": "ClusterEB0386A7"
    },
    "DeploymentConfiguration": {
     "Alarms": {
      "AlarmNames": [],
      "Enable": false,
      "Rollback": false
     },
     "DeploymentCircuitBreaker": {
      "Enable": true,
      "Rollback": true
     },
     "MaximumPercent": 200,
     "MinimumHealthyPercent": 50
    },
    "DesiredCount": 1,
    "EnableECSManagedTags": false,
    "LaunchType": "FARGATE",
    "NetworkConfiguration": {
     "AwsvpcConfiguration": {
      "AssignPublicIp": "DISABLED",
      "SecurityGroups": [
       {
        "Fn::GetAtt": [
         "ServiceSg2632F788",
         "GroupId"
        ]
       }
      ],
      "Subnets": [
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet1Subnet2291D3DF12E84F6D"
       },
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet2Subnet4441BE119DFCFE3A"
       }
      ]
     }
    },
    "ServiceName": "newsinsight-factcheck-mcp-dev",
    "ServiceRegistries": [
     {
      "RegistryArn": {
       "Fn::GetAtt": [
        "ServicefactcheckmcpCloudmapService3A4DA9C0",
        "Arn"
       ]
      }
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskDefinition": {
     "Ref": "TaskDeffactcheckmcpB0E8594F"
    }
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-factcheck-mcp/Service"
   }
  },
  "ServicefactcheckmcpCloudmapService3A4DA9C0": {
   "Type": "AWS::ServiceDiscovery::Service",
   "Properties": {
    "DnsConfig": {
     "DnsRecords": [
      {
       "TTL": 60,
       "Type": "A"
      }
     ],
     "NamespaceId": {
      "Fn::GetAtt": [
       "Namespace9B63B8C8",
       "Id"
      ]
     },
     "RoutingPolicy": "MULTIVALUE"
    },
    "HealthCheckCustomConfig": {
     "FailureThreshold": 1
    },
    "Name": "factcheck-mcp",
    "NamespaceId": {
     "Fn::GetAtt": [
      "Namespace9B63B8C8",
      "Id"
     ]
    },
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-factcheck-mcp/CloudmapService/Resource"
   }
  },
  "LogGrouptopicmcp948F08BC": {
   "Type": "AWS::Logs::LogGroup",
   "Properties": {
    "LogGroupName": "/ecs/newsinsight/dev/topic-mcp",
    "RetentionInDays": 7,
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/LogGroup-topic-mcp/Resource"
   }
  },
  "TaskDeftopicmcpDFB35BA3": {
   "Type": "AWS::ECS::TaskDefinition",
   "Properties": {
    "ContainerDefinitions": [
     {
      "Environment": [
       {
        "Name": "ENVIRONMENT",
        "Value": "dev"
       },
       {
        "Name": "NODE_ENV",
        "Value": "development"
       },
       {
        "Name": "POSTGRES_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttPostgresClusterE64235C6EndpointAddress8684A102"
        }
       },
       {
        "Name": "POSTGRES_PORT",
        "Value": "5432"
       },
       {
        "Name": "MONGO_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttDocDbClusterB46CF5D3EndpointD6B42638"
        }
       },
       {
        "Name": "MONGO_PORT",
        "Value": "27017"
       },
       {
        "Name": "REDIS_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttRedisClusterPrimaryEndPointAddressCFB16E1D"
        }
       },
       {
        "Name": "REDIS_PORT",
        "Value": "6379"
       },
       {
        "Name": "API_GATEWAY_URL",
        "Value": "http://api-gateway.newsinsight.local:8000"
       },
       {
        "Name": "COLLECTOR_SERVICE_URL",
        "Value": "http://collector-service.newsinsight.local:8081"
       },
       {
        "Name": "BROWSER_USE_API_URL",
        "Value": "http://browser-use-api.newsinsight.local:8500"
       },
       {
        "Name": "AUTONOMOUS_CRAWLER_URL",
        "Value": "http://autonomous-crawler.newsinsight.local:8030"
       },
       {
        "Name": "ADMIN_DASHBOARD_URL",
        "Value": "http://admin-dashboard.newsinsight.local:8888"
       },
       {
        "Name": "NEWSINSIGHT_MCP_URL",
        "Value": "http://newsinsight-mcp.newsinsight.local:5000"
       },
       {
        "Name": "BIAS_MCP_URL",
        "Value": "http://bias-mcp.newsinsight.local:5001"
       },
       {
        "Name": "FACTCHECK_MCP_URL",
        "Value": "http://factcheck-mcp.newsinsight.local:5002"
       },
       {
        "Name": "TOPIC_MCP_URL",
        "Value": "http://topic-mcp.newsinsight.local:5003"
       },
       {
        "Name": "AIAGENT_MCP_URL",
        "Value": "http://aiagent-mcp.newsinsight.local:5004"
       }
      ],
      "Essential": true,
      "Image": {
       "Fn::Join": [
        "",
        [
         {
          "Fn::Select": [
           4,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepotopicmcpE31160AAArn2F9AC21C"
             }
            ]
           }
          ]
         },
         ".dkr.ecr.",
         {
          "Fn::Select": [
           3,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepotopicmcpE31160AAArn2F9AC21C"
             }
            ]
           }
          ]
         },
         ".",
         {
          "Ref": "AWS::URLSuffix"
         },
         "/",
         {
          "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputRefRepotopicmcpE31160AA6A09C9AB"
         },
         ":latest"
        ]
       ]
      },
      "LogConfiguration": {
       "LogDriver": "awslogs",
       "Options": {
        "awslogs-group": {
         "Ref": "LogGrouptopicmcp948F08BC"
        },
        "awslogs-stream-prefix": "topic-mcp",
        "awslogs-region": "ap-northeast-2"
       }
      },
      "Name": "topic-mcp",
      "PortMappings": [
       {
        "ContainerPort": 5003,
        "Protocol": "tcp"
       }
      ],
      "Secrets": [
       {
        "Name": "POSTGRES_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "POSTGRES_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "REDIS_PASSWORD",
        "ValueFrom": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefRedisSecretCB3BD9716380542C"
        }
       }
      ]
     }
    ],
    "Cpu": "256",
    "ExecutionRoleArn": {
     "Fn::GetAtt": [
      "ExecutionRole605A040B",
      "Arn"
     ]
    },
    "Family": "newsinsight-topic-mcp-dev",
    "Memory": "512",
    "NetworkMode": "awsvpc",
    "RequiresCompatibilities": [
     "FARGATE"
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskRoleArn": {
     "Fn::GetAtt": [
      "TaskRole30FC0FBB",
      "Arn"
     ]
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/TaskDef-topic-mcp/Resource"
   }
  },
  "ServicetopicmcpService01B08368": {
   "Type": "AWS::ECS::Service",
   "Properties": {
    "Cluster": {
     "Ref": "ClusterEB0386A7"
    },
    "DeploymentConfiguration": {
     "Alarms": {
      "AlarmNames": [],
      "Enable": false,
      "Rollback": false
     },
     "DeploymentCircuitBreaker": {
      "Enable": true,
      "Rollback": true
     },
     "MaximumPercent": 200,
     "MinimumHealthyPercent": 50
    },
    "DesiredCount": 1,
    "EnableECSManagedTags": false,
    "LaunchType": "FARGATE",
    "NetworkConfiguration": {
     "AwsvpcConfiguration": {
      "AssignPublicIp": "DISABLED",
      "SecurityGroups": [
       {
        "Fn::GetAtt": [
         "ServiceSg2632F788",
         "GroupId"
        ]
       }
      ],
      "Subnets": [
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet1Subnet2291D3DF12E84F6D"
       },
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet2Subnet4441BE119DFCFE3A"
       }
      ]
     }
    },
    "ServiceName": "newsinsight-topic-mcp-dev",
    "ServiceRegistries": [
     {
      "RegistryArn": {
       "Fn::GetAtt": [
        "ServicetopicmcpCloudmapService31B4DCE2",
        "Arn"
       ]
      }
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskDefinition": {
     "Ref": "TaskDeftopicmcpDFB35BA3"
    }
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-topic-mcp/Service"
   }
  },
  "ServicetopicmcpCloudmapService31B4DCE2": {
   "Type": "AWS::ServiceDiscovery::Service",
   "Properties": {
    "DnsConfig": {
     "DnsRecords": [
      {
       "TTL": 60,
       "Type": "A"
      }
     ],
     "NamespaceId": {
      "Fn::GetAtt": [
       "Namespace9B63B8C8",
       "Id"
      ]
     },
     "RoutingPolicy": "MULTIVALUE"
    },
    "HealthCheckCustomConfig": {
     "FailureThreshold": 1
    },
    "Name": "topic-mcp",
    "NamespaceId": {
     "Fn::GetAtt": [
      "Namespace9B63B8C8",
      "Id"
     ]
    },
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-topic-mcp/CloudmapService/Resource"
   }
  },
  "LogGroupaiagentmcp22A33121": {
   "Type": "AWS::Logs::LogGroup",
   "Properties": {
    "LogGroupName": "/ecs/newsinsight/dev/aiagent-mcp",
    "RetentionInDays": 7,
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete",
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/LogGroup-aiagent-mcp/Resource"
   }
  },
  "TaskDefaiagentmcp0B7D3326": {
   "Type": "AWS::ECS::TaskDefinition",
   "Properties": {
    "ContainerDefinitions": [
     {
      "Environment": [
       {
        "Name": "ENVIRONMENT",
        "Value": "dev"
       },
       {
        "Name": "NODE_ENV",
        "Value": "development"
       },
       {
        "Name": "POSTGRES_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttPostgresClusterE64235C6EndpointAddress8684A102"
        }
       },
       {
        "Name": "POSTGRES_PORT",
        "Value": "5432"
       },
       {
        "Name": "MONGO_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttDocDbClusterB46CF5D3EndpointD6B42638"
        }
       },
       {
        "Name": "MONGO_PORT",
        "Value": "27017"
       },
       {
        "Name": "REDIS_HOST",
        "Value": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttRedisClusterPrimaryEndPointAddressCFB16E1D"
        }
       },
       {
        "Name": "REDIS_PORT",
        "Value": "6379"
       },
       {
        "Name": "API_GATEWAY_URL",
        "Value": "http://api-gateway.newsinsight.local:8000"
       },
       {
        "Name": "COLLECTOR_SERVICE_URL",
        "Value": "http://collector-service.newsinsight.local:8081"
       },
       {
        "Name": "BROWSER_USE_API_URL",
        "Value": "http://browser-use-api.newsinsight.local:8500"
       },
       {
        "Name": "AUTONOMOUS_CRAWLER_URL",
        "Value": "http://autonomous-crawler.newsinsight.local:8030"
       },
       {
        "Name": "ADMIN_DASHBOARD_URL",
        "Value": "http://admin-dashboard.newsinsight.local:8888"
       },
       {
        "Name": "NEWSINSIGHT_MCP_URL",
        "Value": "http://newsinsight-mcp.newsinsight.local:5000"
       },
       {
        "Name": "BIAS_MCP_URL",
        "Value": "http://bias-mcp.newsinsight.local:5001"
       },
       {
        "Name": "FACTCHECK_MCP_URL",
        "Value": "http://factcheck-mcp.newsinsight.local:5002"
       },
       {
        "Name": "TOPIC_MCP_URL",
        "Value": "http://topic-mcp.newsinsight.local:5003"
       },
       {
        "Name": "AIAGENT_MCP_URL",
        "Value": "http://aiagent-mcp.newsinsight.local:5004"
       }
      ],
      "Essential": true,
      "Image": {
       "Fn::Join": [
        "",
        [
         {
          "Fn::Select": [
           4,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepoaiagentmcp0F725DDBArn4667959C"
             }
            ]
           }
          ]
         },
         ".dkr.ecr.",
         {
          "Fn::Select": [
           3,
           {
            "Fn::Split": [
             ":",
             {
              "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputFnGetAttRepoaiagentmcp0F725DDBArn4667959C"
             }
            ]
           }
          ]
         },
         ".",
         {
          "Ref": "AWS::URLSuffix"
         },
         "/",
         {
          "Fn::ImportValue": "newsinsight-ecr-dev:ExportsOutputRefRepoaiagentmcp0F725DDB17473D70"
         },
         ":latest"
        ]
       ]
      },
      "LogConfiguration": {
       "LogDriver": "awslogs",
       "Options": {
        "awslogs-group": {
         "Ref": "LogGroupaiagentmcp22A33121"
        },
        "awslogs-stream-prefix": "aiagent-mcp",
        "awslogs-region": "ap-northeast-2"
       }
      },
      "Name": "aiagent-mcp",
      "PortMappings": [
       {
        "ContainerPort": 5004,
        "Protocol": "tcp"
       }
      ],
      "Secrets": [
       {
        "Name": "POSTGRES_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "POSTGRES_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_PASSWORD",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":password::"
          ]
         ]
        }
       },
       {
        "Name": "MONGO_USER",
        "ValueFrom": {
         "Fn::Join": [
          "",
          [
           {
            "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
           },
           ":username::"
          ]
         ]
        }
       },
       {
        "Name": "REDIS_PASSWORD",
        "ValueFrom": {
         "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefRedisSecretCB3BD9716380542C"
        }
       }
      ]
     }
    ],
    "Cpu": "256",
    "ExecutionRoleArn": {
     "Fn::GetAtt": [
      "ExecutionRole605A040B",
      "Arn"
     ]
    },
    "Family": "newsinsight-aiagent-mcp-dev",
    "Memory": "512",
    "NetworkMode": "awsvpc",
    "RequiresCompatibilities": [
     "FARGATE"
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskRoleArn": {
     "Fn::GetAtt": [
      "TaskRole30FC0FBB",
      "Arn"
     ]
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/TaskDef-aiagent-mcp/Resource"
   }
  },
  "ServiceaiagentmcpService1ADDCCC6": {
   "Type": "AWS::ECS::Service",
   "Properties": {
    "Cluster": {
     "Ref": "ClusterEB0386A7"
    },
    "DeploymentConfiguration": {
     "Alarms": {
      "AlarmNames": [],
      "Enable": false,
      "Rollback": false
     },
     "DeploymentCircuitBreaker": {
      "Enable": true,
      "Rollback": true
     },
     "MaximumPercent": 200,
     "MinimumHealthyPercent": 50
    },
    "DesiredCount": 1,
    "EnableECSManagedTags": false,
    "LaunchType": "FARGATE",
    "NetworkConfiguration": {
     "AwsvpcConfiguration": {
      "AssignPublicIp": "DISABLED",
      "SecurityGroups": [
       {
        "Fn::GetAtt": [
         "ServiceSg2632F788",
         "GroupId"
        ]
       }
      ],
      "Subnets": [
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet1Subnet2291D3DF12E84F6D"
       },
       {
        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPrivateSubnet2Subnet4441BE119DFCFE3A"
       }
      ]
     }
    },
    "ServiceName": "newsinsight-aiagent-mcp-dev",
    "ServiceRegistries": [
     {
      "RegistryArn": {
       "Fn::GetAtt": [
        "ServiceaiagentmcpCloudmapService2724E5CF",
        "Arn"
       ]
      }
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "TaskDefinition": {
     "Ref": "TaskDefaiagentmcp0B7D3326"
    }
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-aiagent-mcp/Service"
   }
  },
  "ServiceaiagentmcpCloudmapService2724E5CF": {
   "Type": "AWS::ServiceDiscovery::Service",
   "Properties": {
    "DnsConfig": {
     "DnsRecords": [
      {
       "TTL": 60,
       "Type": "A"
      }
     ],
     "NamespaceId": {
      "Fn::GetAtt": [
       "Namespace9B63B8C8",
       "Id"
      ]
     },
     "RoutingPolicy": "MULTIVALUE"
    },
    "HealthCheckCustomConfig": {
     "FailureThreshold": 1
    },
    "Name": "aiagent-mcp",
    "NamespaceId": {
     "Fn::GetAtt": [
      "Namespace9B63B8C8",
      "Id"
     ]
    },
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "DependsOn": [
    "TaskRole30FC0FBB"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/Service-aiagent-mcp/CloudmapService/Resource"
   }
  },
  "CDKMetadata": {
   "Type": "AWS::CDK::Metadata",
   "Properties": {
    "Analytics": "v2:deflate64:H4sIAAAAAAAA/32O0WrDMAxFv6XvjrZkD/2Alo3B2ErS9+E5alCT2EGyU4Lxv48m6Rij7EmHI92LCsi3T/C40RfJTN1mHX1BrLw2rdIX+YxoBOKuC+KR1e5kb/isudEej1raPZ7Ikidnrwd/jbNek0X+5dZshTySwWtoxaRkgZrEuBF5gnhgGrXHvZV33aMMeknc0/cK0RQQKzSByU8v7MKwrP8Tr7ZhFEmKdA+xdN1cOc+D68hM8wMzJdW5RiC+uean68YpqRLFBV5e+gh+CH5uWm1S1tUIZ3kYiwKKHPLNWYgyDtZTj1Au8xuMrUP0owEAAA=="
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-ecs-dev/CDKMetadata/Default"
   }
  }
 },
 "Outputs": {
  "ClusterArn": {
   "Description": "ECS Cluster ARN",
   "Value": {
    "Fn::GetAtt": [
     "ClusterEB0386A7",
     "Arn"
    ]
   },
   "Export": {
    "Name": "newsinsight-cluster-arn-dev"
   }
  },
  "NamespaceArn": {
   "Description": "Service Discovery Namespace ARN",
   "Value": {
    "Fn::GetAtt": [
     "Namespace9B63B8C8",
     "Arn"
    ]
   },
   "Export": {
    "Name": "newsinsight-namespace-arn-dev"
   }
  }
 },
 "Parameters": {
  "BootstrapVersion": {
   "Type": "AWS::SSM::Parameter::Value<String>",
   "Default": "/cdk-bootstrap/hnb659fds/version",
   "Description": "Version of the CDK Bootstrap resources in this environment, automatically retrieved from SSM Parameter Store. [cdk:skip]"
  }
 },
 "Rules": {
  "CheckBootstrapVersion": {
   "Assertions": [
    {
     "Assert": {
      "Fn::Not": [
       {
        "Fn::Contains": [
         [
          "1",
          "2",
          "3",
          "4",
          "5"
         ],
         {
          "Ref": "BootstrapVersion"
         }
        ]
       }
      ]
     },
     "AssertDescription": "CDK bootstrap stack version 6 required. Please run 'cdk bootstrap' with a recent version of the CDK CLI."
    }
   ]
  }
 }
}
```

---

## aws/cdk/cdk.out/newsinsight-vpc-dev.assets.json

```json
{
  "version": "38.0.1",
  "files": {
    "7d9f8eac39839d221d3d3376e92ce9eb47904d92073a216e3d5255c3ba2e0c3c": {
      "source": {
        "path": "newsinsight-vpc-dev.template.json",
        "packaging": "file"
      },
      "destinations": {
        "130954244737-ap-northeast-2": {
          "bucketName": "cdk-hnb659fds-assets-130954244737-ap-northeast-2",
          "objectKey": "7d9f8eac39839d221d3d3376e92ce9eb47904d92073a216e3d5255c3ba2e0c3c.json",
          "region": "ap-northeast-2",
          "assumeRoleArn": "arn:${AWS::Partition}:iam::130954244737:role/cdk-hnb659fds-file-publishing-role-130954244737-ap-northeast-2"
        }
      }
    }
  },
  "dockerImages": {}
}
```

---

## aws/cdk/cdk.out/newsinsight-vpc-dev.template.json

```json
{
 "Resources": {
  "NewsInsightVpc0ED561F3": {
   "Type": "AWS::EC2::VPC",
   "Properties": {
    "CidrBlock": "10.0.0.0/16",
    "EnableDnsHostnames": true,
    "EnableDnsSupport": true,
    "InstanceTenancy": "default",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Name",
      "Value": "newsinsight-vpc-dev"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/Resource"
   }
  },
  "NewsInsightVpcPublicSubnet1SubnetDDF6BE05": {
   "Type": "AWS::EC2::Subnet",
   "Properties": {
    "AvailabilityZone": "ap-northeast-2a",
    "CidrBlock": "10.0.0.0/24",
    "MapPublicIpOnLaunch": true,
    "Tags": [
     {
      "Key": "aws-cdk:subnet-name",
      "Value": "Public"
     },
     {
      "Key": "aws-cdk:subnet-type",
      "Value": "Public"
     },
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Name",
      "Value": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet1"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "VpcId": {
     "Ref": "NewsInsightVpc0ED561F3"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet1/Subnet"
   }
  },
  "NewsInsightVpcPublicSubnet1RouteTable4E9167A1": {
   "Type": "AWS::EC2::RouteTable",
   "Properties": {
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Name",
      "Value": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet1"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "VpcId": {
     "Ref": "NewsInsightVpc0ED561F3"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet1/RouteTable"
   }
  },
  "NewsInsightVpcPublicSubnet1RouteTableAssociation616E1830": {
   "Type": "AWS::EC2::SubnetRouteTableAssociation",
   "Properties": {
    "RouteTableId": {
     "Ref": "NewsInsightVpcPublicSubnet1RouteTable4E9167A1"
    },
    "SubnetId": {
     "Ref": "NewsInsightVpcPublicSubnet1SubnetDDF6BE05"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet1/RouteTableAssociation"
   }
  },
  "NewsInsightVpcPublicSubnet1DefaultRoute4C591366": {
   "Type": "AWS::EC2::Route",
   "Properties": {
    "DestinationCidrBlock": "0.0.0.0/0",
    "GatewayId": {
     "Ref": "NewsInsightVpcIGWB88560BE"
    },
    "RouteTableId": {
     "Ref": "NewsInsightVpcPublicSubnet1RouteTable4E9167A1"
    }
   },
   "DependsOn": [
    "NewsInsightVpcVPCGW74A194B1"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet1/DefaultRoute"
   }
  },
  "NewsInsightVpcPublicSubnet1EIP1FC2F625": {
   "Type": "AWS::EC2::EIP",
   "Properties": {
    "Domain": "vpc",
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Name",
      "Value": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet1"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet1/EIP"
   }
  },
  "NewsInsightVpcPublicSubnet1NATGatewayDD254F02": {
   "Type": "AWS::EC2::NatGateway",
   "Properties": {
    "AllocationId": {
     "Fn::GetAtt": [
      "NewsInsightVpcPublicSubnet1EIP1FC2F625",
      "AllocationId"
     ]
    },
    "SubnetId": {
     "Ref": "NewsInsightVpcPublicSubnet1SubnetDDF6BE05"
    },
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Name",
      "Value": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet1"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "DependsOn": [
    "NewsInsightVpcPublicSubnet1DefaultRoute4C591366",
    "NewsInsightVpcPublicSubnet1RouteTableAssociation616E1830"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet1/NATGateway"
   }
  },
  "NewsInsightVpcPublicSubnet2SubnetA9DCA868": {
   "Type": "AWS::EC2::Subnet",
   "Properties": {
    "AvailabilityZone": "ap-northeast-2b",
    "CidrBlock": "10.0.1.0/24",
    "MapPublicIpOnLaunch": true,
    "Tags": [
     {
      "Key": "aws-cdk:subnet-name",
      "Value": "Public"
     },
     {
      "Key": "aws-cdk:subnet-type",
      "Value": "Public"
     },
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Name",
      "Value": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet2"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "VpcId": {
     "Ref": "NewsInsightVpc0ED561F3"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet2/Subnet"
   }
  },
  "NewsInsightVpcPublicSubnet2RouteTable3A13628D": {
   "Type": "AWS::EC2::RouteTable",
   "Properties": {
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Name",
      "Value": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet2"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "VpcId": {
     "Ref": "NewsInsightVpc0ED561F3"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet2/RouteTable"
   }
  },
  "NewsInsightVpcPublicSubnet2RouteTableAssociationD1260818": {
   "Type": "AWS::EC2::SubnetRouteTableAssociation",
   "Properties": {
    "RouteTableId": {
     "Ref": "NewsInsightVpcPublicSubnet2RouteTable3A13628D"
    },
    "SubnetId": {
     "Ref": "NewsInsightVpcPublicSubnet2SubnetA9DCA868"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet2/RouteTableAssociation"
   }
  },
  "NewsInsightVpcPublicSubnet2DefaultRoute46CB9321": {
   "Type": "AWS::EC2::Route",
   "Properties": {
    "DestinationCidrBlock": "0.0.0.0/0",
    "GatewayId": {
     "Ref": "NewsInsightVpcIGWB88560BE"
    },
    "RouteTableId": {
     "Ref": "NewsInsightVpcPublicSubnet2RouteTable3A13628D"
    }
   },
   "DependsOn": [
    "NewsInsightVpcVPCGW74A194B1"
   ],
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet2/DefaultRoute"
   }
  },
  "NewsInsightVpcPrivateSubnet1Subnet2291D3DF": {
   "Type": "AWS::EC2::Subnet",
   "Properties": {
    "AvailabilityZone": "ap-northeast-2a",
    "CidrBlock": "10.0.2.0/24",
    "MapPublicIpOnLaunch": false,
    "Tags": [
     {
      "Key": "aws-cdk:subnet-name",
      "Value": "Private"
     },
     {
      "Key": "aws-cdk:subnet-type",
      "Value": "Private"
     },
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Name",
      "Value": "newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet1"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "VpcId": {
     "Ref": "NewsInsightVpc0ED561F3"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet1/Subnet"
   }
  },
  "NewsInsightVpcPrivateSubnet1RouteTableCBB2F095": {
   "Type": "AWS::EC2::RouteTable",
   "Properties": {
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Name",
      "Value": "newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet1"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "VpcId": {
     "Ref": "NewsInsightVpc0ED561F3"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet1/RouteTable"
   }
  },
  "NewsInsightVpcPrivateSubnet1RouteTableAssociation65213971": {
   "Type": "AWS::EC2::SubnetRouteTableAssociation",
   "Properties": {
    "RouteTableId": {
     "Ref": "NewsInsightVpcPrivateSubnet1RouteTableCBB2F095"
    },
    "SubnetId": {
     "Ref": "NewsInsightVpcPrivateSubnet1Subnet2291D3DF"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet1/RouteTableAssociation"
   }
  },
  "NewsInsightVpcPrivateSubnet1DefaultRoute5A4D447F": {
   "Type": "AWS::EC2::Route",
   "Properties": {
    "DestinationCidrBlock": "0.0.0.0/0",
    "NatGatewayId": {
     "Ref": "NewsInsightVpcPublicSubnet1NATGatewayDD254F02"
    },
    "RouteTableId": {
     "Ref": "NewsInsightVpcPrivateSubnet1RouteTableCBB2F095"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet1/DefaultRoute"
   }
  },
  "NewsInsightVpcPrivateSubnet2Subnet4441BE11": {
   "Type": "AWS::EC2::Subnet",
   "Properties": {
    "AvailabilityZone": "ap-northeast-2b",
    "CidrBlock": "10.0.3.0/24",
    "MapPublicIpOnLaunch": false,
    "Tags": [
     {
      "Key": "aws-cdk:subnet-name",
      "Value": "Private"
     },
     {
      "Key": "aws-cdk:subnet-type",
      "Value": "Private"
     },
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Name",
      "Value": "newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet2"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "VpcId": {
     "Ref": "NewsInsightVpc0ED561F3"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet2/Subnet"
   }
  },
  "NewsInsightVpcPrivateSubnet2RouteTableF6A23486": {
   "Type": "AWS::EC2::RouteTable",
   "Properties": {
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Name",
      "Value": "newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet2"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "VpcId": {
     "Ref": "NewsInsightVpc0ED561F3"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet2/RouteTable"
   }
  },
  "NewsInsightVpcPrivateSubnet2RouteTableAssociation214504C9": {
   "Type": "AWS::EC2::SubnetRouteTableAssociation",
   "Properties": {
    "RouteTableId": {
     "Ref": "NewsInsightVpcPrivateSubnet2RouteTableF6A23486"
    },
    "SubnetId": {
     "Ref": "NewsInsightVpcPrivateSubnet2Subnet4441BE11"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet2/RouteTableAssociation"
   }
  },
  "NewsInsightVpcPrivateSubnet2DefaultRoute3B39260B": {
   "Type": "AWS::EC2::Route",
   "Properties": {
    "DestinationCidrBlock": "0.0.0.0/0",
    "NatGatewayId": {
     "Ref": "NewsInsightVpcPublicSubnet1NATGatewayDD254F02"
    },
    "RouteTableId": {
     "Ref": "NewsInsightVpcPrivateSubnet2RouteTableF6A23486"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet2/DefaultRoute"
   }
  },
  "NewsInsightVpcDatabaseSubnet1Subnet03B7835F": {
   "Type": "AWS::EC2::Subnet",
   "Properties": {
    "AvailabilityZone": "ap-northeast-2a",
    "CidrBlock": "10.0.4.0/24",
    "MapPublicIpOnLaunch": false,
    "Tags": [
     {
      "Key": "aws-cdk:subnet-name",
      "Value": "Database"
     },
     {
      "Key": "aws-cdk:subnet-type",
      "Value": "Isolated"
     },
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Name",
      "Value": "newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet1"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "VpcId": {
     "Ref": "NewsInsightVpc0ED561F3"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet1/Subnet"
   }
  },
  "NewsInsightVpcDatabaseSubnet1RouteTable179C7366": {
   "Type": "AWS::EC2::RouteTable",
   "Properties": {
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Name",
      "Value": "newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet1"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "VpcId": {
     "Ref": "NewsInsightVpc0ED561F3"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet1/RouteTable"
   }
  },
  "NewsInsightVpcDatabaseSubnet1RouteTableAssociation111ECFF2": {
   "Type": "AWS::EC2::SubnetRouteTableAssociation",
   "Properties": {
    "RouteTableId": {
     "Ref": "NewsInsightVpcDatabaseSubnet1RouteTable179C7366"
    },
    "SubnetId": {
     "Ref": "NewsInsightVpcDatabaseSubnet1Subnet03B7835F"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet1/RouteTableAssociation"
   }
  },
  "NewsInsightVpcDatabaseSubnet2SubnetD7C47EB3": {
   "Type": "AWS::EC2::Subnet",
   "Properties": {
    "AvailabilityZone": "ap-northeast-2b",
    "CidrBlock": "10.0.5.0/24",
    "MapPublicIpOnLaunch": false,
    "Tags": [
     {
      "Key": "aws-cdk:subnet-name",
      "Value": "Database"
     },
     {
      "Key": "aws-cdk:subnet-type",
      "Value": "Isolated"
     },
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Name",
      "Value": "newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet2"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "VpcId": {
     "Ref": "NewsInsightVpc0ED561F3"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet2/Subnet"
   }
  },
  "NewsInsightVpcDatabaseSubnet2RouteTable69871B4B": {
   "Type": "AWS::EC2::RouteTable",
   "Properties": {
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Name",
      "Value": "newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet2"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "VpcId": {
     "Ref": "NewsInsightVpc0ED561F3"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet2/RouteTable"
   }
  },
  "NewsInsightVpcDatabaseSubnet2RouteTableAssociationC660A581": {
   "Type": "AWS::EC2::SubnetRouteTableAssociation",
   "Properties": {
    "RouteTableId": {
     "Ref": "NewsInsightVpcDatabaseSubnet2RouteTable69871B4B"
    },
    "SubnetId": {
     "Ref": "NewsInsightVpcDatabaseSubnet2SubnetD7C47EB3"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet2/RouteTableAssociation"
   }
  },
  "NewsInsightVpcIGWB88560BE": {
   "Type": "AWS::EC2::InternetGateway",
   "Properties": {
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Name",
      "Value": "newsinsight-vpc-dev"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ]
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/IGW"
   }
  },
  "NewsInsightVpcVPCGW74A194B1": {
   "Type": "AWS::EC2::VPCGatewayAttachment",
   "Properties": {
    "InternetGatewayId": {
     "Ref": "NewsInsightVpcIGWB88560BE"
    },
    "VpcId": {
     "Ref": "NewsInsightVpc0ED561F3"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/VPCGW"
   }
  },
  "NewsInsightVpcS3Endpoint889454DC": {
   "Type": "AWS::EC2::VPCEndpoint",
   "Properties": {
    "RouteTableIds": [
     {
      "Ref": "NewsInsightVpcPrivateSubnet1RouteTableCBB2F095"
     },
     {
      "Ref": "NewsInsightVpcPrivateSubnet2RouteTableF6A23486"
     },
     {
      "Ref": "NewsInsightVpcPublicSubnet1RouteTable4E9167A1"
     },
     {
      "Ref": "NewsInsightVpcPublicSubnet2RouteTable3A13628D"
     },
     {
      "Ref": "NewsInsightVpcDatabaseSubnet1RouteTable179C7366"
     },
     {
      "Ref": "NewsInsightVpcDatabaseSubnet2RouteTable69871B4B"
     }
    ],
    "ServiceName": {
     "Fn::Join": [
      "",
      [
       "com.amazonaws.",
       {
        "Ref": "AWS::Region"
       },
       ".s3"
      ]
     ]
    },
    "VpcEndpointType": "Gateway",
    "VpcId": {
     "Ref": "NewsInsightVpc0ED561F3"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/S3Endpoint/Resource"
   }
  },
  "NewsInsightVpcEcrEndpointSecurityGroupADDF0D2E": {
   "Type": "AWS::EC2::SecurityGroup",
   "Properties": {
    "GroupDescription": "newsinsight-vpc-dev/NewsInsightVpc/EcrEndpoint/SecurityGroup",
    "SecurityGroupEgress": [
     {
      "CidrIp": "0.0.0.0/0",
      "Description": "Allow all outbound traffic by default",
      "IpProtocol": "-1"
     }
    ],
    "SecurityGroupIngress": [
     {
      "CidrIp": {
       "Fn::GetAtt": [
        "NewsInsightVpc0ED561F3",
        "CidrBlock"
       ]
      },
      "Description": {
       "Fn::Join": [
        "",
        [
         "from ",
         {
          "Fn::GetAtt": [
           "NewsInsightVpc0ED561F3",
           "CidrBlock"
          ]
         },
         ":443"
        ]
       ]
      },
      "FromPort": 443,
      "IpProtocol": "tcp",
      "ToPort": 443
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Name",
      "Value": "newsinsight-vpc-dev"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "VpcId": {
     "Ref": "NewsInsightVpc0ED561F3"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/EcrEndpoint/SecurityGroup/Resource"
   }
  },
  "NewsInsightVpcEcrEndpointB0029AA6": {
   "Type": "AWS::EC2::VPCEndpoint",
   "Properties": {
    "PrivateDnsEnabled": true,
    "SecurityGroupIds": [
     {
      "Fn::GetAtt": [
       "NewsInsightVpcEcrEndpointSecurityGroupADDF0D2E",
       "GroupId"
      ]
     }
    ],
    "ServiceName": "com.amazonaws.ap-northeast-2.ecr.api",
    "SubnetIds": [
     {
      "Ref": "NewsInsightVpcPrivateSubnet1Subnet2291D3DF"
     },
     {
      "Ref": "NewsInsightVpcPrivateSubnet2Subnet4441BE11"
     }
    ],
    "VpcEndpointType": "Interface",
    "VpcId": {
     "Ref": "NewsInsightVpc0ED561F3"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/EcrEndpoint/Resource"
   }
  },
  "NewsInsightVpcEcrDockerEndpointSecurityGroupE23FE1D4": {
   "Type": "AWS::EC2::SecurityGroup",
   "Properties": {
    "GroupDescription": "newsinsight-vpc-dev/NewsInsightVpc/EcrDockerEndpoint/SecurityGroup",
    "SecurityGroupEgress": [
     {
      "CidrIp": "0.0.0.0/0",
      "Description": "Allow all outbound traffic by default",
      "IpProtocol": "-1"
     }
    ],
    "SecurityGroupIngress": [
     {
      "CidrIp": {
       "Fn::GetAtt": [
        "NewsInsightVpc0ED561F3",
        "CidrBlock"
       ]
      },
      "Description": {
       "Fn::Join": [
        "",
        [
         "from ",
         {
          "Fn::GetAtt": [
           "NewsInsightVpc0ED561F3",
           "CidrBlock"
          ]
         },
         ":443"
        ]
       ]
      },
      "FromPort": 443,
      "IpProtocol": "tcp",
      "ToPort": 443
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Name",
      "Value": "newsinsight-vpc-dev"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "VpcId": {
     "Ref": "NewsInsightVpc0ED561F3"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/EcrDockerEndpoint/SecurityGroup/Resource"
   }
  },
  "NewsInsightVpcEcrDockerEndpoint9500660C": {
   "Type": "AWS::EC2::VPCEndpoint",
   "Properties": {
    "PrivateDnsEnabled": true,
    "SecurityGroupIds": [
     {
      "Fn::GetAtt": [
       "NewsInsightVpcEcrDockerEndpointSecurityGroupE23FE1D4",
       "GroupId"
      ]
     }
    ],
    "ServiceName": "com.amazonaws.ap-northeast-2.ecr.dkr",
    "SubnetIds": [
     {
      "Ref": "NewsInsightVpcPrivateSubnet1Subnet2291D3DF"
     },
     {
      "Ref": "NewsInsightVpcPrivateSubnet2Subnet4441BE11"
     }
    ],
    "VpcEndpointType": "Interface",
    "VpcId": {
     "Ref": "NewsInsightVpc0ED561F3"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/EcrDockerEndpoint/Resource"
   }
  },
  "NewsInsightVpcCloudWatchLogsEndpointSecurityGroupF99BA3DA": {
   "Type": "AWS::EC2::SecurityGroup",
   "Properties": {
    "GroupDescription": "newsinsight-vpc-dev/NewsInsightVpc/CloudWatchLogsEndpoint/SecurityGroup",
    "SecurityGroupEgress": [
     {
      "CidrIp": "0.0.0.0/0",
      "Description": "Allow all outbound traffic by default",
      "IpProtocol": "-1"
     }
    ],
    "SecurityGroupIngress": [
     {
      "CidrIp": {
       "Fn::GetAtt": [
        "NewsInsightVpc0ED561F3",
        "CidrBlock"
       ]
      },
      "Description": {
       "Fn::Join": [
        "",
        [
         "from ",
         {
          "Fn::GetAtt": [
           "NewsInsightVpc0ED561F3",
           "CidrBlock"
          ]
         },
         ":443"
        ]
       ]
      },
      "FromPort": 443,
      "IpProtocol": "tcp",
      "ToPort": 443
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Name",
      "Value": "newsinsight-vpc-dev"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "VpcId": {
     "Ref": "NewsInsightVpc0ED561F3"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/CloudWatchLogsEndpoint/SecurityGroup/Resource"
   }
  },
  "NewsInsightVpcCloudWatchLogsEndpoint4D520062": {
   "Type": "AWS::EC2::VPCEndpoint",
   "Properties": {
    "PrivateDnsEnabled": true,
    "SecurityGroupIds": [
     {
      "Fn::GetAtt": [
       "NewsInsightVpcCloudWatchLogsEndpointSecurityGroupF99BA3DA",
       "GroupId"
      ]
     }
    ],
    "ServiceName": "com.amazonaws.ap-northeast-2.logs",
    "SubnetIds": [
     {
      "Ref": "NewsInsightVpcPrivateSubnet1Subnet2291D3DF"
     },
     {
      "Ref": "NewsInsightVpcPrivateSubnet2Subnet4441BE11"
     }
    ],
    "VpcEndpointType": "Interface",
    "VpcId": {
     "Ref": "NewsInsightVpc0ED561F3"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/CloudWatchLogsEndpoint/Resource"
   }
  },
  "NewsInsightVpcSecretsManagerEndpointSecurityGroup854CAC5E": {
   "Type": "AWS::EC2::SecurityGroup",
   "Properties": {
    "GroupDescription": "newsinsight-vpc-dev/NewsInsightVpc/SecretsManagerEndpoint/SecurityGroup",
    "SecurityGroupEgress": [
     {
      "CidrIp": "0.0.0.0/0",
      "Description": "Allow all outbound traffic by default",
      "IpProtocol": "-1"
     }
    ],
    "SecurityGroupIngress": [
     {
      "CidrIp": {
       "Fn::GetAtt": [
        "NewsInsightVpc0ED561F3",
        "CidrBlock"
       ]
      },
      "Description": {
       "Fn::Join": [
        "",
        [
         "from ",
         {
          "Fn::GetAtt": [
           "NewsInsightVpc0ED561F3",
           "CidrBlock"
          ]
         },
         ":443"
        ]
       ]
      },
      "FromPort": 443,
      "IpProtocol": "tcp",
      "ToPort": 443
     }
    ],
    "Tags": [
     {
      "Key": "Environment",
      "Value": "dev"
     },
     {
      "Key": "ManagedBy",
      "Value": "CDK"
     },
     {
      "Key": "Name",
      "Value": "newsinsight-vpc-dev"
     },
     {
      "Key": "Project",
      "Value": "NewsInsight"
     }
    ],
    "VpcId": {
     "Ref": "NewsInsightVpc0ED561F3"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/SecretsManagerEndpoint/SecurityGroup/Resource"
   }
  },
  "NewsInsightVpcSecretsManagerEndpoint6C5B4F98": {
   "Type": "AWS::EC2::VPCEndpoint",
   "Properties": {
    "PrivateDnsEnabled": true,
    "SecurityGroupIds": [
     {
      "Fn::GetAtt": [
       "NewsInsightVpcSecretsManagerEndpointSecurityGroup854CAC5E",
       "GroupId"
      ]
     }
    ],
    "ServiceName": "com.amazonaws.ap-northeast-2.secretsmanager",
    "SubnetIds": [
     {
      "Ref": "NewsInsightVpcPrivateSubnet1Subnet2291D3DF"
     },
     {
      "Ref": "NewsInsightVpcPrivateSubnet2Subnet4441BE11"
     }
    ],
    "VpcEndpointType": "Interface",
    "VpcId": {
     "Ref": "NewsInsightVpc0ED561F3"
    }
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/NewsInsightVpc/SecretsManagerEndpoint/Resource"
   }
  },
  "CDKMetadata": {
   "Type": "AWS::CDK::Metadata",
   "Properties": {
    "Analytics": "v2:deflate64:H4sIAAAAAAAA/02PzWrDMBCEnyV3Wa3VQ88hhJBLa+zia1mvN1SJsxLSKiaEvHvxD3ZOM/sxDLNG558f+n0DfcywvWSdbfSjEsCLgj7+Ehr9qD2q3YnrYqeK1HQWq9QwycBWV7ok9ANNRytf2TZGhxbEOl7Cg9kfi0G+QA4g1MNdFcHeQGgtPrJQYFoC05L52ooA/l2JRc2k9rjn1jvLMkeXc2w6AdJrpiJMwcr9EFzy4/RX8FQlRZcCjmu/k/g0fTvTp2LXkj7Ht5sx2uQ635yjtVlILPZKupz0H8Cf3VRmAQAA"
   },
   "Metadata": {
    "aws:cdk:path": "newsinsight-vpc-dev/CDKMetadata/Default"
   }
  }
 },
 "Outputs": {
  "VpcId": {
   "Description": "VPC ID",
   "Value": {
    "Ref": "NewsInsightVpc0ED561F3"
   },
   "Export": {
    "Name": "newsinsight-vpc-id-dev"
   }
  },
  "PublicSubnetIds": {
   "Description": "Public Subnet IDs",
   "Value": {
    "Fn::Join": [
     "",
     [
      {
       "Ref": "NewsInsightVpcPublicSubnet1SubnetDDF6BE05"
      },
      ",",
      {
       "Ref": "NewsInsightVpcPublicSubnet2SubnetA9DCA868"
      }
     ]
    ]
   },
   "Export": {
    "Name": "newsinsight-public-subnets-dev"
   }
  },
  "PrivateSubnetIds": {
   "Description": "Private Subnet IDs",
   "Value": {
    "Fn::Join": [
     "",
     [
      {
       "Ref": "NewsInsightVpcPrivateSubnet1Subnet2291D3DF"
      },
      ",",
      {
       "Ref": "NewsInsightVpcPrivateSubnet2Subnet4441BE11"
      }
     ]
    ]
   },
   "Export": {
    "Name": "newsinsight-private-subnets-dev"
   }
  },
  "ExportsOutputFnGetAttNewsInsightVpc0ED561F3CidrBlock6740B7E1": {
   "Value": {
    "Fn::GetAtt": [
     "NewsInsightVpc0ED561F3",
     "CidrBlock"
    ]
   },
   "Export": {
    "Name": "newsinsight-vpc-dev:ExportsOutputFnGetAttNewsInsightVpc0ED561F3CidrBlock6740B7E1"
   }
  },
  "ExportsOutputRefNewsInsightVpc0ED561F3FDBE1BB1": {
   "Value": {
    "Ref": "NewsInsightVpc0ED561F3"
   },
   "Export": {
    "Name": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpc0ED561F3FDBE1BB1"
   }
  },
  "ExportsOutputRefNewsInsightVpcDatabaseSubnet1Subnet03B7835F8E9E9CB2": {
   "Value": {
    "Ref": "NewsInsightVpcDatabaseSubnet1Subnet03B7835F"
   },
   "Export": {
    "Name": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcDatabaseSubnet1Subnet03B7835F8E9E9CB2"
   }
  },
  "ExportsOutputRefNewsInsightVpcDatabaseSubnet2SubnetD7C47EB3CCBBAC7C": {
   "Value": {
    "Ref": "NewsInsightVpcDatabaseSubnet2SubnetD7C47EB3"
   },
   "Export": {
    "Name": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcDatabaseSubnet2SubnetD7C47EB3CCBBAC7C"
   }
  },
  "ExportsOutputRefNewsInsightVpcPublicSubnet1SubnetDDF6BE05C056FD30": {
   "Value": {
    "Ref": "NewsInsightVpcPublicSubnet1SubnetDDF6BE05"
   },
   "Export": {
    "Name": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPublicSubnet1SubnetDDF6BE05C056FD30"
   }
  },
  "ExportsOutputRefNewsInsightVpcPublicSubnet2SubnetA9DCA8686D3D4315": {
   "Value": {
    "Ref": "NewsInsightVpcPublicSubnet2SubnetA9DCA868"
   },
   "Export": {
    "Name": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPublicSubnet2SubnetA9DCA8686D3D4315"
   }
  }
 },
 "Parameters": {
  "BootstrapVersion": {
   "Type": "AWS::SSM::Parameter::Value<String>",
   "Default": "/cdk-bootstrap/hnb659fds/version",
   "Description": "Version of the CDK Bootstrap resources in this environment, automatically retrieved from SSM Parameter Store. [cdk:skip]"
  }
 },
 "Rules": {
  "CheckBootstrapVersion": {
   "Assertions": [
    {
     "Assert": {
      "Fn::Not": [
       {
        "Fn::Contains": [
         [
          "1",
          "2",
          "3",
          "4",
          "5"
         ],
         {
          "Ref": "BootstrapVersion"
         }
        ]
       }
      ]
     },
     "AssertDescription": "CDK bootstrap stack version 6 required. Please run 'cdk bootstrap' with a recent version of the CDK CLI."
    }
   ]
  }
 }
}
```

---

## aws/cdk/cdk.out/tree.json

```json
{
  "version": "tree-0.1",
  "tree": {
    "id": "App",
    "path": "",
    "children": {
      "newsinsight-vpc-dev": {
        "id": "newsinsight-vpc-dev",
        "path": "newsinsight-vpc-dev",
        "children": {
          "NewsInsightVpc": {
            "id": "NewsInsightVpc",
            "path": "newsinsight-vpc-dev/NewsInsightVpc",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-vpc-dev/NewsInsightVpc/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::EC2::VPC",
                  "aws:cdk:cloudformation:props": {
                    "cidrBlock": "10.0.0.0/16",
                    "enableDnsHostnames": true,
                    "enableDnsSupport": true,
                    "instanceTenancy": "default",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Name",
                        "value": "newsinsight-vpc-dev"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ec2.CfnVPC",
                  "version": "2.173.0"
                }
              },
              "PublicSubnet1": {
                "id": "PublicSubnet1",
                "path": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet1",
                "children": {
                  "Subnet": {
                    "id": "Subnet",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet1/Subnet",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::EC2::Subnet",
                      "aws:cdk:cloudformation:props": {
                        "availabilityZone": "ap-northeast-2a",
                        "cidrBlock": "10.0.0.0/24",
                        "mapPublicIpOnLaunch": true,
                        "tags": [
                          {
                            "key": "aws-cdk:subnet-name",
                            "value": "Public"
                          },
                          {
                            "key": "aws-cdk:subnet-type",
                            "value": "Public"
                          },
                          {
                            "key": "Environment",
                            "value": "dev"
                          },
                          {
                            "key": "ManagedBy",
                            "value": "CDK"
                          },
                          {
                            "key": "Name",
                            "value": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet1"
                          },
                          {
                            "key": "Project",
                            "value": "NewsInsight"
                          }
                        ],
                        "vpcId": {
                          "Ref": "NewsInsightVpc0ED561F3"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.CfnSubnet",
                      "version": "2.173.0"
                    }
                  },
                  "Acl": {
                    "id": "Acl",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet1/Acl",
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.Resource",
                      "version": "2.173.0"
                    }
                  },
                  "RouteTable": {
                    "id": "RouteTable",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet1/RouteTable",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::EC2::RouteTable",
                      "aws:cdk:cloudformation:props": {
                        "tags": [
                          {
                            "key": "Environment",
                            "value": "dev"
                          },
                          {
                            "key": "ManagedBy",
                            "value": "CDK"
                          },
                          {
                            "key": "Name",
                            "value": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet1"
                          },
                          {
                            "key": "Project",
                            "value": "NewsInsight"
                          }
                        ],
                        "vpcId": {
                          "Ref": "NewsInsightVpc0ED561F3"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.CfnRouteTable",
                      "version": "2.173.0"
                    }
                  },
                  "RouteTableAssociation": {
                    "id": "RouteTableAssociation",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet1/RouteTableAssociation",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::EC2::SubnetRouteTableAssociation",
                      "aws:cdk:cloudformation:props": {
                        "routeTableId": {
                          "Ref": "NewsInsightVpcPublicSubnet1RouteTable4E9167A1"
                        },
                        "subnetId": {
                          "Ref": "NewsInsightVpcPublicSubnet1SubnetDDF6BE05"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.CfnSubnetRouteTableAssociation",
                      "version": "2.173.0"
                    }
                  },
                  "DefaultRoute": {
                    "id": "DefaultRoute",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet1/DefaultRoute",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::EC2::Route",
                      "aws:cdk:cloudformation:props": {
                        "destinationCidrBlock": "0.0.0.0/0",
                        "gatewayId": {
                          "Ref": "NewsInsightVpcIGWB88560BE"
                        },
                        "routeTableId": {
                          "Ref": "NewsInsightVpcPublicSubnet1RouteTable4E9167A1"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.CfnRoute",
                      "version": "2.173.0"
                    }
                  },
                  "EIP": {
                    "id": "EIP",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet1/EIP",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::EC2::EIP",
                      "aws:cdk:cloudformation:props": {
                        "domain": "vpc",
                        "tags": [
                          {
                            "key": "Environment",
                            "value": "dev"
                          },
                          {
                            "key": "ManagedBy",
                            "value": "CDK"
                          },
                          {
                            "key": "Name",
                            "value": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet1"
                          },
                          {
                            "key": "Project",
                            "value": "NewsInsight"
                          }
                        ]
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.CfnEIP",
                      "version": "2.173.0"
                    }
                  },
                  "NATGateway": {
                    "id": "NATGateway",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet1/NATGateway",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::EC2::NatGateway",
                      "aws:cdk:cloudformation:props": {
                        "allocationId": {
                          "Fn::GetAtt": [
                            "NewsInsightVpcPublicSubnet1EIP1FC2F625",
                            "AllocationId"
                          ]
                        },
                        "subnetId": {
                          "Ref": "NewsInsightVpcPublicSubnet1SubnetDDF6BE05"
                        },
                        "tags": [
                          {
                            "key": "Environment",
                            "value": "dev"
                          },
                          {
                            "key": "ManagedBy",
                            "value": "CDK"
                          },
                          {
                            "key": "Name",
                            "value": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet1"
                          },
                          {
                            "key": "Project",
                            "value": "NewsInsight"
                          }
                        ]
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.CfnNatGateway",
                      "version": "2.173.0"
                    }
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ec2.PublicSubnet",
                  "version": "2.173.0"
                }
              },
              "PublicSubnet2": {
                "id": "PublicSubnet2",
                "path": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet2",
                "children": {
                  "Subnet": {
                    "id": "Subnet",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet2/Subnet",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::EC2::Subnet",
                      "aws:cdk:cloudformation:props": {
                        "availabilityZone": "ap-northeast-2b",
                        "cidrBlock": "10.0.1.0/24",
                        "mapPublicIpOnLaunch": true,
                        "tags": [
                          {
                            "key": "aws-cdk:subnet-name",
                            "value": "Public"
                          },
                          {
                            "key": "aws-cdk:subnet-type",
                            "value": "Public"
                          },
                          {
                            "key": "Environment",
                            "value": "dev"
                          },
                          {
                            "key": "ManagedBy",
                            "value": "CDK"
                          },
                          {
                            "key": "Name",
                            "value": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet2"
                          },
                          {
                            "key": "Project",
                            "value": "NewsInsight"
                          }
                        ],
                        "vpcId": {
                          "Ref": "NewsInsightVpc0ED561F3"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.CfnSubnet",
                      "version": "2.173.0"
                    }
                  },
                  "Acl": {
                    "id": "Acl",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet2/Acl",
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.Resource",
                      "version": "2.173.0"
                    }
                  },
                  "RouteTable": {
                    "id": "RouteTable",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet2/RouteTable",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::EC2::RouteTable",
                      "aws:cdk:cloudformation:props": {
                        "tags": [
                          {
                            "key": "Environment",
                            "value": "dev"
                          },
                          {
                            "key": "ManagedBy",
                            "value": "CDK"
                          },
                          {
                            "key": "Name",
                            "value": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet2"
                          },
                          {
                            "key": "Project",
                            "value": "NewsInsight"
                          }
                        ],
                        "vpcId": {
                          "Ref": "NewsInsightVpc0ED561F3"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.CfnRouteTable",
                      "version": "2.173.0"
                    }
                  },
                  "RouteTableAssociation": {
                    "id": "RouteTableAssociation",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet2/RouteTableAssociation",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::EC2::SubnetRouteTableAssociation",
                      "aws:cdk:cloudformation:props": {
                        "routeTableId": {
                          "Ref": "NewsInsightVpcPublicSubnet2RouteTable3A13628D"
                        },
                        "subnetId": {
                          "Ref": "NewsInsightVpcPublicSubnet2SubnetA9DCA868"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.CfnSubnetRouteTableAssociation",
                      "version": "2.173.0"
                    }
                  },
                  "DefaultRoute": {
                    "id": "DefaultRoute",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/PublicSubnet2/DefaultRoute",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::EC2::Route",
                      "aws:cdk:cloudformation:props": {
                        "destinationCidrBlock": "0.0.0.0/0",
                        "gatewayId": {
                          "Ref": "NewsInsightVpcIGWB88560BE"
                        },
                        "routeTableId": {
                          "Ref": "NewsInsightVpcPublicSubnet2RouteTable3A13628D"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.CfnRoute",
                      "version": "2.173.0"
                    }
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ec2.PublicSubnet",
                  "version": "2.173.0"
                }
              },
              "PrivateSubnet1": {
                "id": "PrivateSubnet1",
                "path": "newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet1",
                "children": {
                  "Subnet": {
                    "id": "Subnet",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet1/Subnet",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::EC2::Subnet",
                      "aws:cdk:cloudformation:props": {
                        "availabilityZone": "ap-northeast-2a",
                        "cidrBlock": "10.0.2.0/24",
                        "mapPublicIpOnLaunch": false,
                        "tags": [
                          {
                            "key": "aws-cdk:subnet-name",
                            "value": "Private"
                          },
                          {
                            "key": "aws-cdk:subnet-type",
                            "value": "Private"
                          },
                          {
                            "key": "Environment",
                            "value": "dev"
                          },
                          {
                            "key": "ManagedBy",
                            "value": "CDK"
                          },
                          {
                            "key": "Name",
                            "value": "newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet1"
                          },
                          {
                            "key": "Project",
                            "value": "NewsInsight"
                          }
                        ],
                        "vpcId": {
                          "Ref": "NewsInsightVpc0ED561F3"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.CfnSubnet",
                      "version": "2.173.0"
                    }
                  },
                  "Acl": {
                    "id": "Acl",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet1/Acl",
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.Resource",
                      "version": "2.173.0"
                    }
                  },
                  "RouteTable": {
                    "id": "RouteTable",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet1/RouteTable",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::EC2::RouteTable",
                      "aws:cdk:cloudformation:props": {
                        "tags": [
                          {
                            "key": "Environment",
                            "value": "dev"
                          },
                          {
                            "key": "ManagedBy",
                            "value": "CDK"
                          },
                          {
                            "key": "Name",
                            "value": "newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet1"
                          },
                          {
                            "key": "Project",
                            "value": "NewsInsight"
                          }
                        ],
                        "vpcId": {
                          "Ref": "NewsInsightVpc0ED561F3"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.CfnRouteTable",
                      "version": "2.173.0"
                    }
                  },
                  "RouteTableAssociation": {
                    "id": "RouteTableAssociation",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet1/RouteTableAssociation",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::EC2::SubnetRouteTableAssociation",
                      "aws:cdk:cloudformation:props": {
                        "routeTableId": {
                          "Ref": "NewsInsightVpcPrivateSubnet1RouteTableCBB2F095"
                        },
                        "subnetId": {
                          "Ref": "NewsInsightVpcPrivateSubnet1Subnet2291D3DF"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.CfnSubnetRouteTableAssociation",
                      "version": "2.173.0"
                    }
                  },
                  "DefaultRoute": {
                    "id": "DefaultRoute",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet1/DefaultRoute",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::EC2::Route",
                      "aws:cdk:cloudformation:props": {
                        "destinationCidrBlock": "0.0.0.0/0",
                        "natGatewayId": {
                          "Ref": "NewsInsightVpcPublicSubnet1NATGatewayDD254F02"
                        },
                        "routeTableId": {
                          "Ref": "NewsInsightVpcPrivateSubnet1RouteTableCBB2F095"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.CfnRoute",
                      "version": "2.173.0"
                    }
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ec2.PrivateSubnet",
                  "version": "2.173.0"
                }
              },
              "PrivateSubnet2": {
                "id": "PrivateSubnet2",
                "path": "newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet2",
                "children": {
                  "Subnet": {
                    "id": "Subnet",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet2/Subnet",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::EC2::Subnet",
                      "aws:cdk:cloudformation:props": {
                        "availabilityZone": "ap-northeast-2b",
                        "cidrBlock": "10.0.3.0/24",
                        "mapPublicIpOnLaunch": false,
                        "tags": [
                          {
                            "key": "aws-cdk:subnet-name",
                            "value": "Private"
                          },
                          {
                            "key": "aws-cdk:subnet-type",
                            "value": "Private"
                          },
                          {
                            "key": "Environment",
                            "value": "dev"
                          },
                          {
                            "key": "ManagedBy",
                            "value": "CDK"
                          },
                          {
                            "key": "Name",
                            "value": "newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet2"
                          },
                          {
                            "key": "Project",
                            "value": "NewsInsight"
                          }
                        ],
                        "vpcId": {
                          "Ref": "NewsInsightVpc0ED561F3"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.CfnSubnet",
                      "version": "2.173.0"
                    }
                  },
                  "Acl": {
                    "id": "Acl",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet2/Acl",
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.Resource",
                      "version": "2.173.0"
                    }
                  },
                  "RouteTable": {
                    "id": "RouteTable",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet2/RouteTable",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::EC2::RouteTable",
                      "aws:cdk:cloudformation:props": {
                        "tags": [
                          {
                            "key": "Environment",
                            "value": "dev"
                          },
                          {
                            "key": "ManagedBy",
                            "value": "CDK"
                          },
                          {
                            "key": "Name",
                            "value": "newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet2"
                          },
                          {
                            "key": "Project",
                            "value": "NewsInsight"
                          }
                        ],
                        "vpcId": {
                          "Ref": "NewsInsightVpc0ED561F3"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.CfnRouteTable",
                      "version": "2.173.0"
                    }
                  },
                  "RouteTableAssociation": {
                    "id": "RouteTableAssociation",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet2/RouteTableAssociation",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::EC2::SubnetRouteTableAssociation",
                      "aws:cdk:cloudformation:props": {
                        "routeTableId": {
                          "Ref": "NewsInsightVpcPrivateSubnet2RouteTableF6A23486"
                        },
                        "subnetId": {
                          "Ref": "NewsInsightVpcPrivateSubnet2Subnet4441BE11"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.CfnSubnetRouteTableAssociation",
                      "version": "2.173.0"
                    }
                  },
                  "DefaultRoute": {
                    "id": "DefaultRoute",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/PrivateSubnet2/DefaultRoute",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::EC2::Route",
                      "aws:cdk:cloudformation:props": {
                        "destinationCidrBlock": "0.0.0.0/0",
                        "natGatewayId": {
                          "Ref": "NewsInsightVpcPublicSubnet1NATGatewayDD254F02"
                        },
                        "routeTableId": {
                          "Ref": "NewsInsightVpcPrivateSubnet2RouteTableF6A23486"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.CfnRoute",
                      "version": "2.173.0"
                    }
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ec2.PrivateSubnet",
                  "version": "2.173.0"
                }
              },
              "DatabaseSubnet1": {
                "id": "DatabaseSubnet1",
                "path": "newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet1",
                "children": {
                  "Subnet": {
                    "id": "Subnet",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet1/Subnet",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::EC2::Subnet",
                      "aws:cdk:cloudformation:props": {
                        "availabilityZone": "ap-northeast-2a",
                        "cidrBlock": "10.0.4.0/24",
                        "mapPublicIpOnLaunch": false,
                        "tags": [
                          {
                            "key": "aws-cdk:subnet-name",
                            "value": "Database"
                          },
                          {
                            "key": "aws-cdk:subnet-type",
                            "value": "Isolated"
                          },
                          {
                            "key": "Environment",
                            "value": "dev"
                          },
                          {
                            "key": "ManagedBy",
                            "value": "CDK"
                          },
                          {
                            "key": "Name",
                            "value": "newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet1"
                          },
                          {
                            "key": "Project",
                            "value": "NewsInsight"
                          }
                        ],
                        "vpcId": {
                          "Ref": "NewsInsightVpc0ED561F3"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.CfnSubnet",
                      "version": "2.173.0"
                    }
                  },
                  "Acl": {
                    "id": "Acl",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet1/Acl",
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.Resource",
                      "version": "2.173.0"
                    }
                  },
                  "RouteTable": {
                    "id": "RouteTable",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet1/RouteTable",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::EC2::RouteTable",
                      "aws:cdk:cloudformation:props": {
                        "tags": [
                          {
                            "key": "Environment",
                            "value": "dev"
                          },
                          {
                            "key": "ManagedBy",
                            "value": "CDK"
                          },
                          {
                            "key": "Name",
                            "value": "newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet1"
                          },
                          {
                            "key": "Project",
                            "value": "NewsInsight"
                          }
                        ],
                        "vpcId": {
                          "Ref": "NewsInsightVpc0ED561F3"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.CfnRouteTable",
                      "version": "2.173.0"
                    }
                  },
                  "RouteTableAssociation": {
                    "id": "RouteTableAssociation",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet1/RouteTableAssociation",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::EC2::SubnetRouteTableAssociation",
                      "aws:cdk:cloudformation:props": {
                        "routeTableId": {
                          "Ref": "NewsInsightVpcDatabaseSubnet1RouteTable179C7366"
                        },
                        "subnetId": {
                          "Ref": "NewsInsightVpcDatabaseSubnet1Subnet03B7835F"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.CfnSubnetRouteTableAssociation",
                      "version": "2.173.0"
                    }
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ec2.PrivateSubnet",
                  "version": "2.173.0"
                }
              },
              "DatabaseSubnet2": {
                "id": "DatabaseSubnet2",
                "path": "newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet2",
                "children": {
                  "Subnet": {
                    "id": "Subnet",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet2/Subnet",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::EC2::Subnet",
                      "aws:cdk:cloudformation:props": {
                        "availabilityZone": "ap-northeast-2b",
                        "cidrBlock": "10.0.5.0/24",
                        "mapPublicIpOnLaunch": false,
                        "tags": [
                          {
                            "key": "aws-cdk:subnet-name",
                            "value": "Database"
                          },
                          {
                            "key": "aws-cdk:subnet-type",
                            "value": "Isolated"
                          },
                          {
                            "key": "Environment",
                            "value": "dev"
                          },
                          {
                            "key": "ManagedBy",
                            "value": "CDK"
                          },
                          {
                            "key": "Name",
                            "value": "newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet2"
                          },
                          {
                            "key": "Project",
                            "value": "NewsInsight"
                          }
                        ],
                        "vpcId": {
                          "Ref": "NewsInsightVpc0ED561F3"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.CfnSubnet",
                      "version": "2.173.0"
                    }
                  },
                  "Acl": {
                    "id": "Acl",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet2/Acl",
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.Resource",
                      "version": "2.173.0"
                    }
                  },
                  "RouteTable": {
                    "id": "RouteTable",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet2/RouteTable",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::EC2::RouteTable",
                      "aws:cdk:cloudformation:props": {
                        "tags": [
                          {
                            "key": "Environment",
                            "value": "dev"
                          },
                          {
                            "key": "ManagedBy",
                            "value": "CDK"
                          },
                          {
                            "key": "Name",
                            "value": "newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet2"
                          },
                          {
                            "key": "Project",
                            "value": "NewsInsight"
                          }
                        ],
                        "vpcId": {
                          "Ref": "NewsInsightVpc0ED561F3"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.CfnRouteTable",
                      "version": "2.173.0"
                    }
                  },
                  "RouteTableAssociation": {
                    "id": "RouteTableAssociation",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/DatabaseSubnet2/RouteTableAssociation",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::EC2::SubnetRouteTableAssociation",
                      "aws:cdk:cloudformation:props": {
                        "routeTableId": {
                          "Ref": "NewsInsightVpcDatabaseSubnet2RouteTable69871B4B"
                        },
                        "subnetId": {
                          "Ref": "NewsInsightVpcDatabaseSubnet2SubnetD7C47EB3"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.CfnSubnetRouteTableAssociation",
                      "version": "2.173.0"
                    }
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ec2.PrivateSubnet",
                  "version": "2.173.0"
                }
              },
              "IGW": {
                "id": "IGW",
                "path": "newsinsight-vpc-dev/NewsInsightVpc/IGW",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::EC2::InternetGateway",
                  "aws:cdk:cloudformation:props": {
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Name",
                        "value": "newsinsight-vpc-dev"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ec2.CfnInternetGateway",
                  "version": "2.173.0"
                }
              },
              "VPCGW": {
                "id": "VPCGW",
                "path": "newsinsight-vpc-dev/NewsInsightVpc/VPCGW",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::EC2::VPCGatewayAttachment",
                  "aws:cdk:cloudformation:props": {
                    "internetGatewayId": {
                      "Ref": "NewsInsightVpcIGWB88560BE"
                    },
                    "vpcId": {
                      "Ref": "NewsInsightVpc0ED561F3"
                    }
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ec2.CfnVPCGatewayAttachment",
                  "version": "2.173.0"
                }
              },
              "S3Endpoint": {
                "id": "S3Endpoint",
                "path": "newsinsight-vpc-dev/NewsInsightVpc/S3Endpoint",
                "children": {
                  "Resource": {
                    "id": "Resource",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/S3Endpoint/Resource",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::EC2::VPCEndpoint",
                      "aws:cdk:cloudformation:props": {
                        "routeTableIds": [
                          {
                            "Ref": "NewsInsightVpcPrivateSubnet1RouteTableCBB2F095"
                          },
                          {
                            "Ref": "NewsInsightVpcPrivateSubnet2RouteTableF6A23486"
                          },
                          {
                            "Ref": "NewsInsightVpcPublicSubnet1RouteTable4E9167A1"
                          },
                          {
                            "Ref": "NewsInsightVpcPublicSubnet2RouteTable3A13628D"
                          },
                          {
                            "Ref": "NewsInsightVpcDatabaseSubnet1RouteTable179C7366"
                          },
                          {
                            "Ref": "NewsInsightVpcDatabaseSubnet2RouteTable69871B4B"
                          }
                        ],
                        "serviceName": {
                          "Fn::Join": [
                            "",
                            [
                              "com.amazonaws.",
                              {
                                "Ref": "AWS::Region"
                              },
                              ".s3"
                            ]
                          ]
                        },
                        "vpcEndpointType": "Gateway",
                        "vpcId": {
                          "Ref": "NewsInsightVpc0ED561F3"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.CfnVPCEndpoint",
                      "version": "2.173.0"
                    }
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ec2.GatewayVpcEndpoint",
                  "version": "2.173.0"
                }
              },
              "EcrEndpoint": {
                "id": "EcrEndpoint",
                "path": "newsinsight-vpc-dev/NewsInsightVpc/EcrEndpoint",
                "children": {
                  "SecurityGroup": {
                    "id": "SecurityGroup",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/EcrEndpoint/SecurityGroup",
                    "children": {
                      "Resource": {
                        "id": "Resource",
                        "path": "newsinsight-vpc-dev/NewsInsightVpc/EcrEndpoint/SecurityGroup/Resource",
                        "attributes": {
                          "aws:cdk:cloudformation:type": "AWS::EC2::SecurityGroup",
                          "aws:cdk:cloudformation:props": {
                            "groupDescription": "newsinsight-vpc-dev/NewsInsightVpc/EcrEndpoint/SecurityGroup",
                            "securityGroupEgress": [
                              {
                                "cidrIp": "0.0.0.0/0",
                                "description": "Allow all outbound traffic by default",
                                "ipProtocol": "-1"
                              }
                            ],
                            "securityGroupIngress": [
                              {
                                "cidrIp": {
                                  "Fn::GetAtt": [
                                    "NewsInsightVpc0ED561F3",
                                    "CidrBlock"
                                  ]
                                },
                                "ipProtocol": "tcp",
                                "fromPort": 443,
                                "toPort": 443,
                                "description": {
                                  "Fn::Join": [
                                    "",
                                    [
                                      "from ",
                                      {
                                        "Fn::GetAtt": [
                                          "NewsInsightVpc0ED561F3",
                                          "CidrBlock"
                                        ]
                                      },
                                      ":443"
                                    ]
                                  ]
                                }
                              }
                            ],
                            "tags": [
                              {
                                "key": "Environment",
                                "value": "dev"
                              },
                              {
                                "key": "ManagedBy",
                                "value": "CDK"
                              },
                              {
                                "key": "Name",
                                "value": "newsinsight-vpc-dev"
                              },
                              {
                                "key": "Project",
                                "value": "NewsInsight"
                              }
                            ],
                            "vpcId": {
                              "Ref": "NewsInsightVpc0ED561F3"
                            }
                          }
                        },
                        "constructInfo": {
                          "fqn": "aws-cdk-lib.aws_ec2.CfnSecurityGroup",
                          "version": "2.173.0"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.SecurityGroup",
                      "version": "2.173.0"
                    }
                  },
                  "Resource": {
                    "id": "Resource",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/EcrEndpoint/Resource",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::EC2::VPCEndpoint",
                      "aws:cdk:cloudformation:props": {
                        "privateDnsEnabled": true,
                        "securityGroupIds": [
                          {
                            "Fn::GetAtt": [
                              "NewsInsightVpcEcrEndpointSecurityGroupADDF0D2E",
                              "GroupId"
                            ]
                          }
                        ],
                        "serviceName": "com.amazonaws.ap-northeast-2.ecr.api",
                        "subnetIds": [
                          {
                            "Ref": "NewsInsightVpcPrivateSubnet1Subnet2291D3DF"
                          },
                          {
                            "Ref": "NewsInsightVpcPrivateSubnet2Subnet4441BE11"
                          }
                        ],
                        "vpcEndpointType": "Interface",
                        "vpcId": {
                          "Ref": "NewsInsightVpc0ED561F3"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.CfnVPCEndpoint",
                      "version": "2.173.0"
                    }
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ec2.InterfaceVpcEndpoint",
                  "version": "2.173.0"
                }
              },
              "EcrDockerEndpoint": {
                "id": "EcrDockerEndpoint",
                "path": "newsinsight-vpc-dev/NewsInsightVpc/EcrDockerEndpoint",
                "children": {
                  "SecurityGroup": {
                    "id": "SecurityGroup",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/EcrDockerEndpoint/SecurityGroup",
                    "children": {
                      "Resource": {
                        "id": "Resource",
                        "path": "newsinsight-vpc-dev/NewsInsightVpc/EcrDockerEndpoint/SecurityGroup/Resource",
                        "attributes": {
                          "aws:cdk:cloudformation:type": "AWS::EC2::SecurityGroup",
                          "aws:cdk:cloudformation:props": {
                            "groupDescription": "newsinsight-vpc-dev/NewsInsightVpc/EcrDockerEndpoint/SecurityGroup",
                            "securityGroupEgress": [
                              {
                                "cidrIp": "0.0.0.0/0",
                                "description": "Allow all outbound traffic by default",
                                "ipProtocol": "-1"
                              }
                            ],
                            "securityGroupIngress": [
                              {
                                "cidrIp": {
                                  "Fn::GetAtt": [
                                    "NewsInsightVpc0ED561F3",
                                    "CidrBlock"
                                  ]
                                },
                                "ipProtocol": "tcp",
                                "fromPort": 443,
                                "toPort": 443,
                                "description": {
                                  "Fn::Join": [
                                    "",
                                    [
                                      "from ",
                                      {
                                        "Fn::GetAtt": [
                                          "NewsInsightVpc0ED561F3",
                                          "CidrBlock"
                                        ]
                                      },
                                      ":443"
                                    ]
                                  ]
                                }
                              }
                            ],
                            "tags": [
                              {
                                "key": "Environment",
                                "value": "dev"
                              },
                              {
                                "key": "ManagedBy",
                                "value": "CDK"
                              },
                              {
                                "key": "Name",
                                "value": "newsinsight-vpc-dev"
                              },
                              {
                                "key": "Project",
                                "value": "NewsInsight"
                              }
                            ],
                            "vpcId": {
                              "Ref": "NewsInsightVpc0ED561F3"
                            }
                          }
                        },
                        "constructInfo": {
                          "fqn": "aws-cdk-lib.aws_ec2.CfnSecurityGroup",
                          "version": "2.173.0"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.SecurityGroup",
                      "version": "2.173.0"
                    }
                  },
                  "Resource": {
                    "id": "Resource",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/EcrDockerEndpoint/Resource",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::EC2::VPCEndpoint",
                      "aws:cdk:cloudformation:props": {
                        "privateDnsEnabled": true,
                        "securityGroupIds": [
                          {
                            "Fn::GetAtt": [
                              "NewsInsightVpcEcrDockerEndpointSecurityGroupE23FE1D4",
                              "GroupId"
                            ]
                          }
                        ],
                        "serviceName": "com.amazonaws.ap-northeast-2.ecr.dkr",
                        "subnetIds": [
                          {
                            "Ref": "NewsInsightVpcPrivateSubnet1Subnet2291D3DF"
                          },
                          {
                            "Ref": "NewsInsightVpcPrivateSubnet2Subnet4441BE11"
                          }
                        ],
                        "vpcEndpointType": "Interface",
                        "vpcId": {
                          "Ref": "NewsInsightVpc0ED561F3"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.CfnVPCEndpoint",
                      "version": "2.173.0"
                    }
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ec2.InterfaceVpcEndpoint",
                  "version": "2.173.0"
                }
              },
              "CloudWatchLogsEndpoint": {
                "id": "CloudWatchLogsEndpoint",
                "path": "newsinsight-vpc-dev/NewsInsightVpc/CloudWatchLogsEndpoint",
                "children": {
                  "SecurityGroup": {
                    "id": "SecurityGroup",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/CloudWatchLogsEndpoint/SecurityGroup",
                    "children": {
                      "Resource": {
                        "id": "Resource",
                        "path": "newsinsight-vpc-dev/NewsInsightVpc/CloudWatchLogsEndpoint/SecurityGroup/Resource",
                        "attributes": {
                          "aws:cdk:cloudformation:type": "AWS::EC2::SecurityGroup",
                          "aws:cdk:cloudformation:props": {
                            "groupDescription": "newsinsight-vpc-dev/NewsInsightVpc/CloudWatchLogsEndpoint/SecurityGroup",
                            "securityGroupEgress": [
                              {
                                "cidrIp": "0.0.0.0/0",
                                "description": "Allow all outbound traffic by default",
                                "ipProtocol": "-1"
                              }
                            ],
                            "securityGroupIngress": [
                              {
                                "cidrIp": {
                                  "Fn::GetAtt": [
                                    "NewsInsightVpc0ED561F3",
                                    "CidrBlock"
                                  ]
                                },
                                "ipProtocol": "tcp",
                                "fromPort": 443,
                                "toPort": 443,
                                "description": {
                                  "Fn::Join": [
                                    "",
                                    [
                                      "from ",
                                      {
                                        "Fn::GetAtt": [
                                          "NewsInsightVpc0ED561F3",
                                          "CidrBlock"
                                        ]
                                      },
                                      ":443"
                                    ]
                                  ]
                                }
                              }
                            ],
                            "tags": [
                              {
                                "key": "Environment",
                                "value": "dev"
                              },
                              {
                                "key": "ManagedBy",
                                "value": "CDK"
                              },
                              {
                                "key": "Name",
                                "value": "newsinsight-vpc-dev"
                              },
                              {
                                "key": "Project",
                                "value": "NewsInsight"
                              }
                            ],
                            "vpcId": {
                              "Ref": "NewsInsightVpc0ED561F3"
                            }
                          }
                        },
                        "constructInfo": {
                          "fqn": "aws-cdk-lib.aws_ec2.CfnSecurityGroup",
                          "version": "2.173.0"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.SecurityGroup",
                      "version": "2.173.0"
                    }
                  },
                  "Resource": {
                    "id": "Resource",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/CloudWatchLogsEndpoint/Resource",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::EC2::VPCEndpoint",
                      "aws:cdk:cloudformation:props": {
                        "privateDnsEnabled": true,
                        "securityGroupIds": [
                          {
                            "Fn::GetAtt": [
                              "NewsInsightVpcCloudWatchLogsEndpointSecurityGroupF99BA3DA",
                              "GroupId"
                            ]
                          }
                        ],
                        "serviceName": "com.amazonaws.ap-northeast-2.logs",
                        "subnetIds": [
                          {
                            "Ref": "NewsInsightVpcPrivateSubnet1Subnet2291D3DF"
                          },
                          {
                            "Ref": "NewsInsightVpcPrivateSubnet2Subnet4441BE11"
                          }
                        ],
                        "vpcEndpointType": "Interface",
                        "vpcId": {
                          "Ref": "NewsInsightVpc0ED561F3"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.CfnVPCEndpoint",
                      "version": "2.173.0"
                    }
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ec2.InterfaceVpcEndpoint",
                  "version": "2.173.0"
                }
              },
              "SecretsManagerEndpoint": {
                "id": "SecretsManagerEndpoint",
                "path": "newsinsight-vpc-dev/NewsInsightVpc/SecretsManagerEndpoint",
                "children": {
                  "SecurityGroup": {
                    "id": "SecurityGroup",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/SecretsManagerEndpoint/SecurityGroup",
                    "children": {
                      "Resource": {
                        "id": "Resource",
                        "path": "newsinsight-vpc-dev/NewsInsightVpc/SecretsManagerEndpoint/SecurityGroup/Resource",
                        "attributes": {
                          "aws:cdk:cloudformation:type": "AWS::EC2::SecurityGroup",
                          "aws:cdk:cloudformation:props": {
                            "groupDescription": "newsinsight-vpc-dev/NewsInsightVpc/SecretsManagerEndpoint/SecurityGroup",
                            "securityGroupEgress": [
                              {
                                "cidrIp": "0.0.0.0/0",
                                "description": "Allow all outbound traffic by default",
                                "ipProtocol": "-1"
                              }
                            ],
                            "securityGroupIngress": [
                              {
                                "cidrIp": {
                                  "Fn::GetAtt": [
                                    "NewsInsightVpc0ED561F3",
                                    "CidrBlock"
                                  ]
                                },
                                "ipProtocol": "tcp",
                                "fromPort": 443,
                                "toPort": 443,
                                "description": {
                                  "Fn::Join": [
                                    "",
                                    [
                                      "from ",
                                      {
                                        "Fn::GetAtt": [
                                          "NewsInsightVpc0ED561F3",
                                          "CidrBlock"
                                        ]
                                      },
                                      ":443"
                                    ]
                                  ]
                                }
                              }
                            ],
                            "tags": [
                              {
                                "key": "Environment",
                                "value": "dev"
                              },
                              {
                                "key": "ManagedBy",
                                "value": "CDK"
                              },
                              {
                                "key": "Name",
                                "value": "newsinsight-vpc-dev"
                              },
                              {
                                "key": "Project",
                                "value": "NewsInsight"
                              }
                            ],
                            "vpcId": {
                              "Ref": "NewsInsightVpc0ED561F3"
                            }
                          }
                        },
                        "constructInfo": {
                          "fqn": "aws-cdk-lib.aws_ec2.CfnSecurityGroup",
                          "version": "2.173.0"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.SecurityGroup",
                      "version": "2.173.0"
                    }
                  },
                  "Resource": {
                    "id": "Resource",
                    "path": "newsinsight-vpc-dev/NewsInsightVpc/SecretsManagerEndpoint/Resource",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::EC2::VPCEndpoint",
                      "aws:cdk:cloudformation:props": {
                        "privateDnsEnabled": true,
                        "securityGroupIds": [
                          {
                            "Fn::GetAtt": [
                              "NewsInsightVpcSecretsManagerEndpointSecurityGroup854CAC5E",
                              "GroupId"
                            ]
                          }
                        ],
                        "serviceName": "com.amazonaws.ap-northeast-2.secretsmanager",
                        "subnetIds": [
                          {
                            "Ref": "NewsInsightVpcPrivateSubnet1Subnet2291D3DF"
                          },
                          {
                            "Ref": "NewsInsightVpcPrivateSubnet2Subnet4441BE11"
                          }
                        ],
                        "vpcEndpointType": "Interface",
                        "vpcId": {
                          "Ref": "NewsInsightVpc0ED561F3"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_ec2.CfnVPCEndpoint",
                      "version": "2.173.0"
                    }
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ec2.InterfaceVpcEndpoint",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ec2.Vpc",
              "version": "2.173.0"
            }
          },
          "VpcId": {
            "id": "VpcId",
            "path": "newsinsight-vpc-dev/VpcId",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "PublicSubnetIds": {
            "id": "PublicSubnetIds",
            "path": "newsinsight-vpc-dev/PublicSubnetIds",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "PrivateSubnetIds": {
            "id": "PrivateSubnetIds",
            "path": "newsinsight-vpc-dev/PrivateSubnetIds",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "CDKMetadata": {
            "id": "CDKMetadata",
            "path": "newsinsight-vpc-dev/CDKMetadata",
            "children": {
              "Default": {
                "id": "Default",
                "path": "newsinsight-vpc-dev/CDKMetadata/Default",
                "constructInfo": {
                  "fqn": "aws-cdk-lib.CfnResource",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "constructs.Construct",
              "version": "10.4.4"
            }
          },
          "Exports": {
            "id": "Exports",
            "path": "newsinsight-vpc-dev/Exports",
            "children": {
              "Output{\"Fn::GetAtt\":[\"NewsInsightVpc0ED561F3\",\"CidrBlock\"]}": {
                "id": "Output{\"Fn::GetAtt\":[\"NewsInsightVpc0ED561F3\",\"CidrBlock\"]}",
                "path": "newsinsight-vpc-dev/Exports/Output{\"Fn::GetAtt\":[\"NewsInsightVpc0ED561F3\",\"CidrBlock\"]}",
                "constructInfo": {
                  "fqn": "aws-cdk-lib.CfnOutput",
                  "version": "2.173.0"
                }
              },
              "Output{\"Ref\":\"NewsInsightVpc0ED561F3\"}": {
                "id": "Output{\"Ref\":\"NewsInsightVpc0ED561F3\"}",
                "path": "newsinsight-vpc-dev/Exports/Output{\"Ref\":\"NewsInsightVpc0ED561F3\"}",
                "constructInfo": {
                  "fqn": "aws-cdk-lib.CfnOutput",
                  "version": "2.173.0"
                }
              },
              "Output{\"Ref\":\"NewsInsightVpcDatabaseSubnet1Subnet03B7835F\"}": {
                "id": "Output{\"Ref\":\"NewsInsightVpcDatabaseSubnet1Subnet03B7835F\"}",
                "path": "newsinsight-vpc-dev/Exports/Output{\"Ref\":\"NewsInsightVpcDatabaseSubnet1Subnet03B7835F\"}",
                "constructInfo": {
                  "fqn": "aws-cdk-lib.CfnOutput",
                  "version": "2.173.0"
                }
              },
              "Output{\"Ref\":\"NewsInsightVpcDatabaseSubnet2SubnetD7C47EB3\"}": {
                "id": "Output{\"Ref\":\"NewsInsightVpcDatabaseSubnet2SubnetD7C47EB3\"}",
                "path": "newsinsight-vpc-dev/Exports/Output{\"Ref\":\"NewsInsightVpcDatabaseSubnet2SubnetD7C47EB3\"}",
                "constructInfo": {
                  "fqn": "aws-cdk-lib.CfnOutput",
                  "version": "2.173.0"
                }
              },
              "Output{\"Ref\":\"NewsInsightVpcPublicSubnet1SubnetDDF6BE05\"}": {
                "id": "Output{\"Ref\":\"NewsInsightVpcPublicSubnet1SubnetDDF6BE05\"}",
                "path": "newsinsight-vpc-dev/Exports/Output{\"Ref\":\"NewsInsightVpcPublicSubnet1SubnetDDF6BE05\"}",
                "constructInfo": {
                  "fqn": "aws-cdk-lib.CfnOutput",
                  "version": "2.173.0"
                }
              },
              "Output{\"Ref\":\"NewsInsightVpcPublicSubnet2SubnetA9DCA868\"}": {
                "id": "Output{\"Ref\":\"NewsInsightVpcPublicSubnet2SubnetA9DCA868\"}",
                "path": "newsinsight-vpc-dev/Exports/Output{\"Ref\":\"NewsInsightVpcPublicSubnet2SubnetA9DCA868\"}",
                "constructInfo": {
                  "fqn": "aws-cdk-lib.CfnOutput",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "constructs.Construct",
              "version": "10.4.4"
            }
          },
          "BootstrapVersion": {
            "id": "BootstrapVersion",
            "path": "newsinsight-vpc-dev/BootstrapVersion",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnParameter",
              "version": "2.173.0"
            }
          },
          "CheckBootstrapVersion": {
            "id": "CheckBootstrapVersion",
            "path": "newsinsight-vpc-dev/CheckBootstrapVersion",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnRule",
              "version": "2.173.0"
            }
          }
        },
        "constructInfo": {
          "fqn": "aws-cdk-lib.Stack",
          "version": "2.173.0"
        }
      },
      "newsinsight-ecr-dev": {
        "id": "newsinsight-ecr-dev",
        "path": "newsinsight-ecr-dev",
        "children": {
          "Repo-frontend": {
            "id": "Repo-frontend",
            "path": "newsinsight-ecr-dev/Repo-frontend",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-ecr-dev/Repo-frontend/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::ECR::Repository",
                  "aws:cdk:cloudformation:props": {
                    "imageScanningConfiguration": {
                      "scanOnPush": true
                    },
                    "imageTagMutability": "MUTABLE",
                    "lifecyclePolicy": {
                      "lifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
                    },
                    "repositoryName": "newsinsight/frontend",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ecr.CfnRepository",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ecr.Repository",
              "version": "2.173.0"
            }
          },
          "RepoUri-frontend": {
            "id": "RepoUri-frontend",
            "path": "newsinsight-ecr-dev/RepoUri-frontend",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "Repo-api-gateway": {
            "id": "Repo-api-gateway",
            "path": "newsinsight-ecr-dev/Repo-api-gateway",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-ecr-dev/Repo-api-gateway/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::ECR::Repository",
                  "aws:cdk:cloudformation:props": {
                    "imageScanningConfiguration": {
                      "scanOnPush": true
                    },
                    "imageTagMutability": "MUTABLE",
                    "lifecyclePolicy": {
                      "lifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
                    },
                    "repositoryName": "newsinsight/api-gateway",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ecr.CfnRepository",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ecr.Repository",
              "version": "2.173.0"
            }
          },
          "RepoUri-api-gateway": {
            "id": "RepoUri-api-gateway",
            "path": "newsinsight-ecr-dev/RepoUri-api-gateway",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "Repo-collector-service": {
            "id": "Repo-collector-service",
            "path": "newsinsight-ecr-dev/Repo-collector-service",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-ecr-dev/Repo-collector-service/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::ECR::Repository",
                  "aws:cdk:cloudformation:props": {
                    "imageScanningConfiguration": {
                      "scanOnPush": true
                    },
                    "imageTagMutability": "MUTABLE",
                    "lifecyclePolicy": {
                      "lifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
                    },
                    "repositoryName": "newsinsight/collector-service",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ecr.CfnRepository",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ecr.Repository",
              "version": "2.173.0"
            }
          },
          "RepoUri-collector-service": {
            "id": "RepoUri-collector-service",
            "path": "newsinsight-ecr-dev/RepoUri-collector-service",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "Repo-autonomous-crawler": {
            "id": "Repo-autonomous-crawler",
            "path": "newsinsight-ecr-dev/Repo-autonomous-crawler",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-ecr-dev/Repo-autonomous-crawler/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::ECR::Repository",
                  "aws:cdk:cloudformation:props": {
                    "imageScanningConfiguration": {
                      "scanOnPush": true
                    },
                    "imageTagMutability": "MUTABLE",
                    "lifecyclePolicy": {
                      "lifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
                    },
                    "repositoryName": "newsinsight/autonomous-crawler",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ecr.CfnRepository",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ecr.Repository",
              "version": "2.173.0"
            }
          },
          "RepoUri-autonomous-crawler": {
            "id": "RepoUri-autonomous-crawler",
            "path": "newsinsight-ecr-dev/RepoUri-autonomous-crawler",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "Repo-browser-use-api": {
            "id": "Repo-browser-use-api",
            "path": "newsinsight-ecr-dev/Repo-browser-use-api",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-ecr-dev/Repo-browser-use-api/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::ECR::Repository",
                  "aws:cdk:cloudformation:props": {
                    "imageScanningConfiguration": {
                      "scanOnPush": true
                    },
                    "imageTagMutability": "MUTABLE",
                    "lifecyclePolicy": {
                      "lifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
                    },
                    "repositoryName": "newsinsight/browser-use-api",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ecr.CfnRepository",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ecr.Repository",
              "version": "2.173.0"
            }
          },
          "RepoUri-browser-use-api": {
            "id": "RepoUri-browser-use-api",
            "path": "newsinsight-ecr-dev/RepoUri-browser-use-api",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "Repo-admin-dashboard": {
            "id": "Repo-admin-dashboard",
            "path": "newsinsight-ecr-dev/Repo-admin-dashboard",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-ecr-dev/Repo-admin-dashboard/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::ECR::Repository",
                  "aws:cdk:cloudformation:props": {
                    "imageScanningConfiguration": {
                      "scanOnPush": true
                    },
                    "imageTagMutability": "MUTABLE",
                    "lifecyclePolicy": {
                      "lifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
                    },
                    "repositoryName": "newsinsight/admin-dashboard",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ecr.CfnRepository",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ecr.Repository",
              "version": "2.173.0"
            }
          },
          "RepoUri-admin-dashboard": {
            "id": "RepoUri-admin-dashboard",
            "path": "newsinsight-ecr-dev/RepoUri-admin-dashboard",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "Repo-ai-agent-worker": {
            "id": "Repo-ai-agent-worker",
            "path": "newsinsight-ecr-dev/Repo-ai-agent-worker",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-ecr-dev/Repo-ai-agent-worker/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::ECR::Repository",
                  "aws:cdk:cloudformation:props": {
                    "imageScanningConfiguration": {
                      "scanOnPush": true
                    },
                    "imageTagMutability": "MUTABLE",
                    "lifecyclePolicy": {
                      "lifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
                    },
                    "repositoryName": "newsinsight/ai-agent-worker",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ecr.CfnRepository",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ecr.Repository",
              "version": "2.173.0"
            }
          },
          "RepoUri-ai-agent-worker": {
            "id": "RepoUri-ai-agent-worker",
            "path": "newsinsight-ecr-dev/RepoUri-ai-agent-worker",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "Repo-web-crawler": {
            "id": "Repo-web-crawler",
            "path": "newsinsight-ecr-dev/Repo-web-crawler",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-ecr-dev/Repo-web-crawler/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::ECR::Repository",
                  "aws:cdk:cloudformation:props": {
                    "imageScanningConfiguration": {
                      "scanOnPush": true
                    },
                    "imageTagMutability": "MUTABLE",
                    "lifecyclePolicy": {
                      "lifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
                    },
                    "repositoryName": "newsinsight/web-crawler",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ecr.CfnRepository",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ecr.Repository",
              "version": "2.173.0"
            }
          },
          "RepoUri-web-crawler": {
            "id": "RepoUri-web-crawler",
            "path": "newsinsight-ecr-dev/RepoUri-web-crawler",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "Repo-embedding-server": {
            "id": "Repo-embedding-server",
            "path": "newsinsight-ecr-dev/Repo-embedding-server",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-ecr-dev/Repo-embedding-server/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::ECR::Repository",
                  "aws:cdk:cloudformation:props": {
                    "imageScanningConfiguration": {
                      "scanOnPush": true
                    },
                    "imageTagMutability": "MUTABLE",
                    "lifecyclePolicy": {
                      "lifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
                    },
                    "repositoryName": "newsinsight/embedding-server",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ecr.CfnRepository",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ecr.Repository",
              "version": "2.173.0"
            }
          },
          "RepoUri-embedding-server": {
            "id": "RepoUri-embedding-server",
            "path": "newsinsight-ecr-dev/RepoUri-embedding-server",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "Repo-ip-rotation": {
            "id": "Repo-ip-rotation",
            "path": "newsinsight-ecr-dev/Repo-ip-rotation",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-ecr-dev/Repo-ip-rotation/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::ECR::Repository",
                  "aws:cdk:cloudformation:props": {
                    "imageScanningConfiguration": {
                      "scanOnPush": true
                    },
                    "imageTagMutability": "MUTABLE",
                    "lifecyclePolicy": {
                      "lifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
                    },
                    "repositoryName": "newsinsight/ip-rotation",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ecr.CfnRepository",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ecr.Repository",
              "version": "2.173.0"
            }
          },
          "RepoUri-ip-rotation": {
            "id": "RepoUri-ip-rotation",
            "path": "newsinsight-ecr-dev/RepoUri-ip-rotation",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "Repo-crawl-worker": {
            "id": "Repo-crawl-worker",
            "path": "newsinsight-ecr-dev/Repo-crawl-worker",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-ecr-dev/Repo-crawl-worker/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::ECR::Repository",
                  "aws:cdk:cloudformation:props": {
                    "imageScanningConfiguration": {
                      "scanOnPush": true
                    },
                    "imageTagMutability": "MUTABLE",
                    "lifecyclePolicy": {
                      "lifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
                    },
                    "repositoryName": "newsinsight/crawl-worker",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ecr.CfnRepository",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ecr.Repository",
              "version": "2.173.0"
            }
          },
          "RepoUri-crawl-worker": {
            "id": "RepoUri-crawl-worker",
            "path": "newsinsight-ecr-dev/RepoUri-crawl-worker",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "Repo-maigret-worker": {
            "id": "Repo-maigret-worker",
            "path": "newsinsight-ecr-dev/Repo-maigret-worker",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-ecr-dev/Repo-maigret-worker/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::ECR::Repository",
                  "aws:cdk:cloudformation:props": {
                    "imageScanningConfiguration": {
                      "scanOnPush": true
                    },
                    "imageTagMutability": "MUTABLE",
                    "lifecyclePolicy": {
                      "lifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
                    },
                    "repositoryName": "newsinsight/maigret-worker",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ecr.CfnRepository",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ecr.Repository",
              "version": "2.173.0"
            }
          },
          "RepoUri-maigret-worker": {
            "id": "RepoUri-maigret-worker",
            "path": "newsinsight-ecr-dev/RepoUri-maigret-worker",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "Repo-bot-detector": {
            "id": "Repo-bot-detector",
            "path": "newsinsight-ecr-dev/Repo-bot-detector",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-ecr-dev/Repo-bot-detector/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::ECR::Repository",
                  "aws:cdk:cloudformation:props": {
                    "imageScanningConfiguration": {
                      "scanOnPush": true
                    },
                    "imageTagMutability": "MUTABLE",
                    "lifecyclePolicy": {
                      "lifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
                    },
                    "repositoryName": "newsinsight/bot-detector",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ecr.CfnRepository",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ecr.Repository",
              "version": "2.173.0"
            }
          },
          "RepoUri-bot-detector": {
            "id": "RepoUri-bot-detector",
            "path": "newsinsight-ecr-dev/RepoUri-bot-detector",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "Repo-sentiment-addon": {
            "id": "Repo-sentiment-addon",
            "path": "newsinsight-ecr-dev/Repo-sentiment-addon",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-ecr-dev/Repo-sentiment-addon/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::ECR::Repository",
                  "aws:cdk:cloudformation:props": {
                    "imageScanningConfiguration": {
                      "scanOnPush": true
                    },
                    "imageTagMutability": "MUTABLE",
                    "lifecyclePolicy": {
                      "lifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
                    },
                    "repositoryName": "newsinsight/sentiment-addon",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ecr.CfnRepository",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ecr.Repository",
              "version": "2.173.0"
            }
          },
          "RepoUri-sentiment-addon": {
            "id": "RepoUri-sentiment-addon",
            "path": "newsinsight-ecr-dev/RepoUri-sentiment-addon",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "Repo-factcheck-addon": {
            "id": "Repo-factcheck-addon",
            "path": "newsinsight-ecr-dev/Repo-factcheck-addon",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-ecr-dev/Repo-factcheck-addon/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::ECR::Repository",
                  "aws:cdk:cloudformation:props": {
                    "imageScanningConfiguration": {
                      "scanOnPush": true
                    },
                    "imageTagMutability": "MUTABLE",
                    "lifecyclePolicy": {
                      "lifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
                    },
                    "repositoryName": "newsinsight/factcheck-addon",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ecr.CfnRepository",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ecr.Repository",
              "version": "2.173.0"
            }
          },
          "RepoUri-factcheck-addon": {
            "id": "RepoUri-factcheck-addon",
            "path": "newsinsight-ecr-dev/RepoUri-factcheck-addon",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "Repo-bias-addon": {
            "id": "Repo-bias-addon",
            "path": "newsinsight-ecr-dev/Repo-bias-addon",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-ecr-dev/Repo-bias-addon/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::ECR::Repository",
                  "aws:cdk:cloudformation:props": {
                    "imageScanningConfiguration": {
                      "scanOnPush": true
                    },
                    "imageTagMutability": "MUTABLE",
                    "lifecyclePolicy": {
                      "lifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
                    },
                    "repositoryName": "newsinsight/bias-addon",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ecr.CfnRepository",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ecr.Repository",
              "version": "2.173.0"
            }
          },
          "RepoUri-bias-addon": {
            "id": "RepoUri-bias-addon",
            "path": "newsinsight-ecr-dev/RepoUri-bias-addon",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "Repo-newsinsight-mcp": {
            "id": "Repo-newsinsight-mcp",
            "path": "newsinsight-ecr-dev/Repo-newsinsight-mcp",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-ecr-dev/Repo-newsinsight-mcp/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::ECR::Repository",
                  "aws:cdk:cloudformation:props": {
                    "imageScanningConfiguration": {
                      "scanOnPush": true
                    },
                    "imageTagMutability": "MUTABLE",
                    "lifecyclePolicy": {
                      "lifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
                    },
                    "repositoryName": "newsinsight/newsinsight-mcp",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ecr.CfnRepository",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ecr.Repository",
              "version": "2.173.0"
            }
          },
          "RepoUri-newsinsight-mcp": {
            "id": "RepoUri-newsinsight-mcp",
            "path": "newsinsight-ecr-dev/RepoUri-newsinsight-mcp",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "Repo-bias-mcp": {
            "id": "Repo-bias-mcp",
            "path": "newsinsight-ecr-dev/Repo-bias-mcp",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-ecr-dev/Repo-bias-mcp/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::ECR::Repository",
                  "aws:cdk:cloudformation:props": {
                    "imageScanningConfiguration": {
                      "scanOnPush": true
                    },
                    "imageTagMutability": "MUTABLE",
                    "lifecyclePolicy": {
                      "lifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
                    },
                    "repositoryName": "newsinsight/bias-mcp",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ecr.CfnRepository",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ecr.Repository",
              "version": "2.173.0"
            }
          },
          "RepoUri-bias-mcp": {
            "id": "RepoUri-bias-mcp",
            "path": "newsinsight-ecr-dev/RepoUri-bias-mcp",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "Repo-factcheck-mcp": {
            "id": "Repo-factcheck-mcp",
            "path": "newsinsight-ecr-dev/Repo-factcheck-mcp",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-ecr-dev/Repo-factcheck-mcp/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::ECR::Repository",
                  "aws:cdk:cloudformation:props": {
                    "imageScanningConfiguration": {
                      "scanOnPush": true
                    },
                    "imageTagMutability": "MUTABLE",
                    "lifecyclePolicy": {
                      "lifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
                    },
                    "repositoryName": "newsinsight/factcheck-mcp",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ecr.CfnRepository",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ecr.Repository",
              "version": "2.173.0"
            }
          },
          "RepoUri-factcheck-mcp": {
            "id": "RepoUri-factcheck-mcp",
            "path": "newsinsight-ecr-dev/RepoUri-factcheck-mcp",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "Repo-topic-mcp": {
            "id": "Repo-topic-mcp",
            "path": "newsinsight-ecr-dev/Repo-topic-mcp",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-ecr-dev/Repo-topic-mcp/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::ECR::Repository",
                  "aws:cdk:cloudformation:props": {
                    "imageScanningConfiguration": {
                      "scanOnPush": true
                    },
                    "imageTagMutability": "MUTABLE",
                    "lifecyclePolicy": {
                      "lifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
                    },
                    "repositoryName": "newsinsight/topic-mcp",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ecr.CfnRepository",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ecr.Repository",
              "version": "2.173.0"
            }
          },
          "RepoUri-topic-mcp": {
            "id": "RepoUri-topic-mcp",
            "path": "newsinsight-ecr-dev/RepoUri-topic-mcp",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "Repo-aiagent-mcp": {
            "id": "Repo-aiagent-mcp",
            "path": "newsinsight-ecr-dev/Repo-aiagent-mcp",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-ecr-dev/Repo-aiagent-mcp/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::ECR::Repository",
                  "aws:cdk:cloudformation:props": {
                    "imageScanningConfiguration": {
                      "scanOnPush": true
                    },
                    "imageTagMutability": "MUTABLE",
                    "lifecyclePolicy": {
                      "lifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
                    },
                    "repositoryName": "newsinsight/aiagent-mcp",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ecr.CfnRepository",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ecr.Repository",
              "version": "2.173.0"
            }
          },
          "RepoUri-aiagent-mcp": {
            "id": "RepoUri-aiagent-mcp",
            "path": "newsinsight-ecr-dev/RepoUri-aiagent-mcp",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "Repo-roboflow-mcp": {
            "id": "Repo-roboflow-mcp",
            "path": "newsinsight-ecr-dev/Repo-roboflow-mcp",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-ecr-dev/Repo-roboflow-mcp/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::ECR::Repository",
                  "aws:cdk:cloudformation:props": {
                    "imageScanningConfiguration": {
                      "scanOnPush": true
                    },
                    "imageTagMutability": "MUTABLE",
                    "lifecyclePolicy": {
                      "lifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
                    },
                    "repositoryName": "newsinsight/roboflow-mcp",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ecr.CfnRepository",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ecr.Repository",
              "version": "2.173.0"
            }
          },
          "RepoUri-roboflow-mcp": {
            "id": "RepoUri-roboflow-mcp",
            "path": "newsinsight-ecr-dev/RepoUri-roboflow-mcp",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "Repo-huggingface-mcp": {
            "id": "Repo-huggingface-mcp",
            "path": "newsinsight-ecr-dev/Repo-huggingface-mcp",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-ecr-dev/Repo-huggingface-mcp/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::ECR::Repository",
                  "aws:cdk:cloudformation:props": {
                    "imageScanningConfiguration": {
                      "scanOnPush": true
                    },
                    "imageTagMutability": "MUTABLE",
                    "lifecyclePolicy": {
                      "lifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
                    },
                    "repositoryName": "newsinsight/huggingface-mcp",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ecr.CfnRepository",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ecr.Repository",
              "version": "2.173.0"
            }
          },
          "RepoUri-huggingface-mcp": {
            "id": "RepoUri-huggingface-mcp",
            "path": "newsinsight-ecr-dev/RepoUri-huggingface-mcp",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "Repo-kaggle-mcp": {
            "id": "Repo-kaggle-mcp",
            "path": "newsinsight-ecr-dev/Repo-kaggle-mcp",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-ecr-dev/Repo-kaggle-mcp/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::ECR::Repository",
                  "aws:cdk:cloudformation:props": {
                    "imageScanningConfiguration": {
                      "scanOnPush": true
                    },
                    "imageTagMutability": "MUTABLE",
                    "lifecyclePolicy": {
                      "lifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
                    },
                    "repositoryName": "newsinsight/kaggle-mcp",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ecr.CfnRepository",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ecr.Repository",
              "version": "2.173.0"
            }
          },
          "RepoUri-kaggle-mcp": {
            "id": "RepoUri-kaggle-mcp",
            "path": "newsinsight-ecr-dev/RepoUri-kaggle-mcp",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "Repo-mltraining-mcp": {
            "id": "Repo-mltraining-mcp",
            "path": "newsinsight-ecr-dev/Repo-mltraining-mcp",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-ecr-dev/Repo-mltraining-mcp/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::ECR::Repository",
                  "aws:cdk:cloudformation:props": {
                    "imageScanningConfiguration": {
                      "scanOnPush": true
                    },
                    "imageTagMutability": "MUTABLE",
                    "lifecyclePolicy": {
                      "lifecyclePolicyText": "{\"rules\":[{\"rulePriority\":1,\"description\":\"Keep last 10 images\",\"selection\":{\"tagStatus\":\"any\",\"countType\":\"imageCountMoreThan\",\"countNumber\":10},\"action\":{\"type\":\"expire\"}}]}"
                    },
                    "repositoryName": "newsinsight/mltraining-mcp",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ecr.CfnRepository",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ecr.Repository",
              "version": "2.173.0"
            }
          },
          "RepoUri-mltraining-mcp": {
            "id": "RepoUri-mltraining-mcp",
            "path": "newsinsight-ecr-dev/RepoUri-mltraining-mcp",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "CDKMetadata": {
            "id": "CDKMetadata",
            "path": "newsinsight-ecr-dev/CDKMetadata",
            "children": {
              "Default": {
                "id": "Default",
                "path": "newsinsight-ecr-dev/CDKMetadata/Default",
                "constructInfo": {
                  "fqn": "aws-cdk-lib.CfnResource",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "constructs.Construct",
              "version": "10.4.4"
            }
          },
          "BootstrapVersion": {
            "id": "BootstrapVersion",
            "path": "newsinsight-ecr-dev/BootstrapVersion",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnParameter",
              "version": "2.173.0"
            }
          },
          "CheckBootstrapVersion": {
            "id": "CheckBootstrapVersion",
            "path": "newsinsight-ecr-dev/CheckBootstrapVersion",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnRule",
              "version": "2.173.0"
            }
          }
        },
        "constructInfo": {
          "fqn": "aws-cdk-lib.Stack",
          "version": "2.173.0"
        }
      },
      "newsinsight-db-dev": {
        "id": "newsinsight-db-dev",
        "path": "newsinsight-db-dev",
        "children": {
          "DbSecurityGroup": {
            "id": "DbSecurityGroup",
            "path": "newsinsight-db-dev/DbSecurityGroup",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-db-dev/DbSecurityGroup/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::EC2::SecurityGroup",
                  "aws:cdk:cloudformation:props": {
                    "groupDescription": "Security group for databases",
                    "groupName": "newsinsight-db-sg-dev",
                    "securityGroupEgress": [
                      {
                        "cidrIp": "0.0.0.0/0",
                        "description": "Allow all outbound traffic by default",
                        "ipProtocol": "-1"
                      }
                    ],
                    "securityGroupIngress": [
                      {
                        "cidrIp": {
                          "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputFnGetAttNewsInsightVpc0ED561F3CidrBlock6740B7E1"
                        },
                        "ipProtocol": "tcp",
                        "fromPort": 5432,
                        "toPort": 5432,
                        "description": "PostgreSQL from VPC"
                      },
                      {
                        "cidrIp": {
                          "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputFnGetAttNewsInsightVpc0ED561F3CidrBlock6740B7E1"
                        },
                        "ipProtocol": "tcp",
                        "fromPort": 27017,
                        "toPort": 27017,
                        "description": "MongoDB from VPC"
                      },
                      {
                        "cidrIp": {
                          "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputFnGetAttNewsInsightVpc0ED561F3CidrBlock6740B7E1"
                        },
                        "ipProtocol": "tcp",
                        "fromPort": 6379,
                        "toPort": 6379,
                        "description": "Redis from VPC"
                      }
                    ],
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ],
                    "vpcId": {
                      "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpc0ED561F3FDBE1BB1"
                    }
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ec2.CfnSecurityGroup",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ec2.SecurityGroup",
              "version": "2.173.0"
            }
          },
          "PostgresSecret": {
            "id": "PostgresSecret",
            "path": "newsinsight-db-dev/PostgresSecret",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-db-dev/PostgresSecret/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::SecretsManager::Secret",
                  "aws:cdk:cloudformation:props": {
                    "description": "PostgreSQL credentials",
                    "generateSecretString": {
                      "secretStringTemplate": "{\"username\":\"newsinsight\",\"database\":\"newsinsight\"}",
                      "generateStringKey": "password",
                      "excludePunctuation": true,
                      "passwordLength": 32
                    },
                    "name": "newsinsight/dev/postgres",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_secretsmanager.CfnSecret",
                  "version": "2.173.0"
                }
              },
              "Attachment": {
                "id": "Attachment",
                "path": "newsinsight-db-dev/PostgresSecret/Attachment",
                "children": {
                  "Resource": {
                    "id": "Resource",
                    "path": "newsinsight-db-dev/PostgresSecret/Attachment/Resource",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::SecretsManager::SecretTargetAttachment",
                      "aws:cdk:cloudformation:props": {
                        "secretId": {
                          "Ref": "PostgresSecretE142E1BC"
                        },
                        "targetId": {
                          "Ref": "PostgresClusterE64235C6"
                        },
                        "targetType": "AWS::RDS::DBCluster"
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_secretsmanager.CfnSecretTargetAttachment",
                      "version": "2.173.0"
                    }
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_secretsmanager.SecretTargetAttachment",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_secretsmanager.Secret",
              "version": "2.173.0"
            }
          },
          "MongoSecret": {
            "id": "MongoSecret",
            "path": "newsinsight-db-dev/MongoSecret",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-db-dev/MongoSecret/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::SecretsManager::Secret",
                  "aws:cdk:cloudformation:props": {
                    "description": "MongoDB credentials",
                    "generateSecretString": {
                      "secretStringTemplate": "{\"username\":\"newsinsight\"}",
                      "generateStringKey": "password",
                      "excludePunctuation": true,
                      "passwordLength": 32
                    },
                    "name": "newsinsight/dev/mongo",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_secretsmanager.CfnSecret",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_secretsmanager.Secret",
              "version": "2.173.0"
            }
          },
          "RedisSecret": {
            "id": "RedisSecret",
            "path": "newsinsight-db-dev/RedisSecret",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-db-dev/RedisSecret/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::SecretsManager::Secret",
                  "aws:cdk:cloudformation:props": {
                    "description": "Redis auth token",
                    "generateSecretString": {
                      "excludePunctuation": true,
                      "passwordLength": 32
                    },
                    "name": "newsinsight/dev/redis",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_secretsmanager.CfnSecret",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_secretsmanager.Secret",
              "version": "2.173.0"
            }
          },
          "PostgresCluster": {
            "id": "PostgresCluster",
            "path": "newsinsight-db-dev/PostgresCluster",
            "children": {
              "Subnets": {
                "id": "Subnets",
                "path": "newsinsight-db-dev/PostgresCluster/Subnets",
                "children": {
                  "Default": {
                    "id": "Default",
                    "path": "newsinsight-db-dev/PostgresCluster/Subnets/Default",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::RDS::DBSubnetGroup",
                      "aws:cdk:cloudformation:props": {
                        "dbSubnetGroupDescription": "Subnets for PostgresCluster database",
                        "subnetIds": [
                          {
                            "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcDatabaseSubnet1Subnet03B7835F8E9E9CB2"
                          },
                          {
                            "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcDatabaseSubnet2SubnetD7C47EB3CCBBAC7C"
                          }
                        ],
                        "tags": [
                          {
                            "key": "Environment",
                            "value": "dev"
                          },
                          {
                            "key": "ManagedBy",
                            "value": "CDK"
                          },
                          {
                            "key": "Project",
                            "value": "NewsInsight"
                          }
                        ]
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_rds.CfnDBSubnetGroup",
                      "version": "2.173.0"
                    }
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_rds.SubnetGroup",
                  "version": "2.173.0"
                }
              },
              "AuroraPostgreSqlDatabaseClusterEngineDefaultParameterGroup": {
                "id": "AuroraPostgreSqlDatabaseClusterEngineDefaultParameterGroup",
                "path": "newsinsight-db-dev/PostgresCluster/AuroraPostgreSqlDatabaseClusterEngineDefaultParameterGroup",
                "constructInfo": {
                  "fqn": "aws-cdk-lib.Resource",
                  "version": "2.173.0"
                }
              },
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-db-dev/PostgresCluster/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::RDS::DBCluster",
                  "aws:cdk:cloudformation:props": {
                    "backupRetentionPeriod": 7,
                    "copyTagsToSnapshot": true,
                    "databaseName": "newsinsight",
                    "dbClusterIdentifier": "newsinsight-postgres-dev",
                    "dbClusterParameterGroupName": "default.aurora-postgresql15",
                    "dbSubnetGroupName": {
                      "Ref": "PostgresClusterSubnets41D66232"
                    },
                    "deletionProtection": false,
                    "engine": "aurora-postgresql",
                    "engineVersion": "15.8",
                    "masterUsername": {
                      "Fn::Join": [
                        "",
                        [
                          "{{resolve:secretsmanager:",
                          {
                            "Ref": "PostgresSecretE142E1BC"
                          },
                          ":SecretString:username::}}"
                        ]
                      ]
                    },
                    "masterUserPassword": {
                      "Fn::Join": [
                        "",
                        [
                          "{{resolve:secretsmanager:",
                          {
                            "Ref": "PostgresSecretE142E1BC"
                          },
                          ":SecretString:password::}}"
                        ]
                      ]
                    },
                    "port": 5432,
                    "storageEncrypted": true,
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ],
                    "vpcSecurityGroupIds": [
                      {
                        "Fn::GetAtt": [
                          "DbSecurityGroupE9D701AD",
                          "GroupId"
                        ]
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_rds.CfnDBCluster",
                  "version": "2.173.0"
                }
              },
              "Instance1": {
                "id": "Instance1",
                "path": "newsinsight-db-dev/PostgresCluster/Instance1",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::RDS::DBInstance",
                  "aws:cdk:cloudformation:props": {
                    "dbClusterIdentifier": {
                      "Ref": "PostgresClusterE64235C6"
                    },
                    "dbInstanceClass": "db.t4g.medium",
                    "dbInstanceIdentifier": "newsinsight-postgres-devinstance1",
                    "dbSubnetGroupName": {
                      "Ref": "PostgresClusterSubnets41D66232"
                    },
                    "engine": "aurora-postgresql",
                    "publiclyAccessible": false,
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_rds.CfnDBInstance",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_rds.DatabaseCluster",
              "version": "2.173.0"
            }
          },
          "DocDbCluster": {
            "id": "DocDbCluster",
            "path": "newsinsight-db-dev/DocDbCluster",
            "children": {
              "Subnets": {
                "id": "Subnets",
                "path": "newsinsight-db-dev/DocDbCluster/Subnets",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::DocDB::DBSubnetGroup",
                  "aws:cdk:cloudformation:props": {
                    "dbSubnetGroupDescription": "Subnets for DocDbCluster database",
                    "subnetIds": [
                      {
                        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcDatabaseSubnet1Subnet03B7835F8E9E9CB2"
                      },
                      {
                        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcDatabaseSubnet2SubnetD7C47EB3CCBBAC7C"
                      }
                    ],
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_docdb.CfnDBSubnetGroup",
                  "version": "2.173.0"
                }
              },
              "Secret": {
                "id": "Secret",
                "path": "newsinsight-db-dev/DocDbCluster/Secret",
                "children": {
                  "Resource": {
                    "id": "Resource",
                    "path": "newsinsight-db-dev/DocDbCluster/Secret/Resource",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::SecretsManager::Secret",
                      "aws:cdk:cloudformation:props": {
                        "description": {
                          "Fn::Join": [
                            "",
                            [
                              "Generated by the CDK for stack: ",
                              {
                                "Ref": "AWS::StackName"
                              }
                            ]
                          ]
                        },
                        "generateSecretString": {
                          "passwordLength": 41,
                          "secretStringTemplate": "{\"username\":\"newsinsight\"}",
                          "generateStringKey": "password",
                          "excludeCharacters": "\"@/"
                        },
                        "name": "newsinsight/dev/docdb",
                        "tags": [
                          {
                            "key": "Environment",
                            "value": "dev"
                          },
                          {
                            "key": "ManagedBy",
                            "value": "CDK"
                          },
                          {
                            "key": "Project",
                            "value": "NewsInsight"
                          }
                        ]
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_secretsmanager.CfnSecret",
                      "version": "2.173.0"
                    }
                  },
                  "Attachment": {
                    "id": "Attachment",
                    "path": "newsinsight-db-dev/DocDbCluster/Secret/Attachment",
                    "children": {
                      "Resource": {
                        "id": "Resource",
                        "path": "newsinsight-db-dev/DocDbCluster/Secret/Attachment/Resource",
                        "attributes": {
                          "aws:cdk:cloudformation:type": "AWS::SecretsManager::SecretTargetAttachment",
                          "aws:cdk:cloudformation:props": {
                            "secretId": {
                              "Ref": "DocDbClusterSecretAE05C874"
                            },
                            "targetId": {
                              "Ref": "DocDbClusterB46CF5D3"
                            },
                            "targetType": "AWS::DocDB::DBCluster"
                          }
                        },
                        "constructInfo": {
                          "fqn": "aws-cdk-lib.aws_secretsmanager.CfnSecretTargetAttachment",
                          "version": "2.173.0"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_secretsmanager.SecretTargetAttachment",
                      "version": "2.173.0"
                    }
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_docdb.DatabaseSecret",
                  "version": "2.173.0"
                }
              },
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-db-dev/DocDbCluster/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::DocDB::DBCluster",
                  "aws:cdk:cloudformation:props": {
                    "backupRetentionPeriod": 7,
                    "dbClusterIdentifier": "newsinsight-docdb-dev",
                    "dbSubnetGroupName": {
                      "Ref": "DocDbClusterSubnetsBFD1BEB3"
                    },
                    "deletionProtection": false,
                    "masterUsername": {
                      "Fn::Join": [
                        "",
                        [
                          "{{resolve:secretsmanager:",
                          {
                            "Ref": "DocDbClusterSecretAE05C874"
                          },
                          ":SecretString:username::}}"
                        ]
                      ]
                    },
                    "masterUserPassword": {
                      "Fn::Join": [
                        "",
                        [
                          "{{resolve:secretsmanager:",
                          {
                            "Ref": "DocDbClusterSecretAE05C874"
                          },
                          ":SecretString:password::}}"
                        ]
                      ]
                    },
                    "storageEncrypted": true,
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ],
                    "vpcSecurityGroupIds": [
                      {
                        "Fn::GetAtt": [
                          "DbSecurityGroupE9D701AD",
                          "GroupId"
                        ]
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_docdb.CfnDBCluster",
                  "version": "2.173.0"
                }
              },
              "Instance1": {
                "id": "Instance1",
                "path": "newsinsight-db-dev/DocDbCluster/Instance1",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::DocDB::DBInstance",
                  "aws:cdk:cloudformation:props": {
                    "dbClusterIdentifier": {
                      "Ref": "DocDbClusterB46CF5D3"
                    },
                    "dbInstanceClass": "db.t4g.medium",
                    "dbInstanceIdentifier": "newsinsight-docdb-devinstance1",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_docdb.CfnDBInstance",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_docdb.DatabaseCluster",
              "version": "2.173.0"
            }
          },
          "RedisSubnetGroup": {
            "id": "RedisSubnetGroup",
            "path": "newsinsight-db-dev/RedisSubnetGroup",
            "attributes": {
              "aws:cdk:cloudformation:type": "AWS::ElastiCache::SubnetGroup",
              "aws:cdk:cloudformation:props": {
                "cacheSubnetGroupName": "newsinsight-redis-subnet-dev",
                "description": "Subnet group for Redis",
                "subnetIds": [
                  {
                    "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcDatabaseSubnet1Subnet03B7835F8E9E9CB2"
                  },
                  {
                    "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcDatabaseSubnet2SubnetD7C47EB3CCBBAC7C"
                  }
                ],
                "tags": [
                  {
                    "key": "Environment",
                    "value": "dev"
                  },
                  {
                    "key": "ManagedBy",
                    "value": "CDK"
                  },
                  {
                    "key": "Project",
                    "value": "NewsInsight"
                  }
                ]
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_elasticache.CfnSubnetGroup",
              "version": "2.173.0"
            }
          },
          "RedisCluster": {
            "id": "RedisCluster",
            "path": "newsinsight-db-dev/RedisCluster",
            "attributes": {
              "aws:cdk:cloudformation:type": "AWS::ElastiCache::ReplicationGroup",
              "aws:cdk:cloudformation:props": {
                "atRestEncryptionEnabled": true,
                "authToken": {
                  "Fn::Join": [
                    "",
                    [
                      "{{resolve:secretsmanager:",
                      {
                        "Ref": "RedisSecretCB3BD971"
                      },
                      ":SecretString:::}}"
                    ]
                  ]
                },
                "automaticFailoverEnabled": false,
                "cacheNodeType": "cache.t4g.medium",
                "cacheSubnetGroupName": "newsinsight-redis-subnet-dev",
                "engine": "redis",
                "engineVersion": "7.0",
                "multiAzEnabled": false,
                "numCacheClusters": 1,
                "replicationGroupDescription": "newsinsight Redis cluster",
                "replicationGroupId": "newsinsight-redis-dev",
                "securityGroupIds": [
                  {
                    "Fn::GetAtt": [
                      "DbSecurityGroupE9D701AD",
                      "GroupId"
                    ]
                  }
                ],
                "snapshotRetentionLimit": 1,
                "tags": [
                  {
                    "key": "Environment",
                    "value": "dev"
                  },
                  {
                    "key": "ManagedBy",
                    "value": "CDK"
                  },
                  {
                    "key": "Project",
                    "value": "NewsInsight"
                  }
                ],
                "transitEncryptionEnabled": true
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_elasticache.CfnReplicationGroup",
              "version": "2.173.0"
            }
          },
          "PostgresEndpoint": {
            "id": "PostgresEndpoint",
            "path": "newsinsight-db-dev/PostgresEndpoint",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "MongoEndpoint": {
            "id": "MongoEndpoint",
            "path": "newsinsight-db-dev/MongoEndpoint",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "RedisEndpoint": {
            "id": "RedisEndpoint",
            "path": "newsinsight-db-dev/RedisEndpoint",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "PostgresSecretArn": {
            "id": "PostgresSecretArn",
            "path": "newsinsight-db-dev/PostgresSecretArn",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "MongoSecretArn": {
            "id": "MongoSecretArn",
            "path": "newsinsight-db-dev/MongoSecretArn",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "RedisSecretArn": {
            "id": "RedisSecretArn",
            "path": "newsinsight-db-dev/RedisSecretArn",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "CDKMetadata": {
            "id": "CDKMetadata",
            "path": "newsinsight-db-dev/CDKMetadata",
            "children": {
              "Default": {
                "id": "Default",
                "path": "newsinsight-db-dev/CDKMetadata/Default",
                "constructInfo": {
                  "fqn": "aws-cdk-lib.CfnResource",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "constructs.Construct",
              "version": "10.4.4"
            }
          },
          "Exports": {
            "id": "Exports",
            "path": "newsinsight-db-dev/Exports",
            "children": {
              "Output{\"Ref\":\"PostgresSecretE142E1BC\"}": {
                "id": "Output{\"Ref\":\"PostgresSecretE142E1BC\"}",
                "path": "newsinsight-db-dev/Exports/Output{\"Ref\":\"PostgresSecretE142E1BC\"}",
                "constructInfo": {
                  "fqn": "aws-cdk-lib.CfnOutput",
                  "version": "2.173.0"
                }
              },
              "Output{\"Ref\":\"MongoSecretEC0FA3F4\"}": {
                "id": "Output{\"Ref\":\"MongoSecretEC0FA3F4\"}",
                "path": "newsinsight-db-dev/Exports/Output{\"Ref\":\"MongoSecretEC0FA3F4\"}",
                "constructInfo": {
                  "fqn": "aws-cdk-lib.CfnOutput",
                  "version": "2.173.0"
                }
              },
              "Output{\"Ref\":\"RedisSecretCB3BD971\"}": {
                "id": "Output{\"Ref\":\"RedisSecretCB3BD971\"}",
                "path": "newsinsight-db-dev/Exports/Output{\"Ref\":\"RedisSecretCB3BD971\"}",
                "constructInfo": {
                  "fqn": "aws-cdk-lib.CfnOutput",
                  "version": "2.173.0"
                }
              },
              "Output{\"Fn::GetAtt\":[\"PostgresClusterE64235C6\",\"Endpoint.Address\"]}": {
                "id": "Output{\"Fn::GetAtt\":[\"PostgresClusterE64235C6\",\"Endpoint.Address\"]}",
                "path": "newsinsight-db-dev/Exports/Output{\"Fn::GetAtt\":[\"PostgresClusterE64235C6\",\"Endpoint.Address\"]}",
                "constructInfo": {
                  "fqn": "aws-cdk-lib.CfnOutput",
                  "version": "2.173.0"
                }
              },
              "Output{\"Fn::GetAtt\":[\"DocDbClusterB46CF5D3\",\"Endpoint\"]}": {
                "id": "Output{\"Fn::GetAtt\":[\"DocDbClusterB46CF5D3\",\"Endpoint\"]}",
                "path": "newsinsight-db-dev/Exports/Output{\"Fn::GetAtt\":[\"DocDbClusterB46CF5D3\",\"Endpoint\"]}",
                "constructInfo": {
                  "fqn": "aws-cdk-lib.CfnOutput",
                  "version": "2.173.0"
                }
              },
              "Output{\"Fn::GetAtt\":[\"RedisCluster\",\"PrimaryEndPoint.Address\"]}": {
                "id": "Output{\"Fn::GetAtt\":[\"RedisCluster\",\"PrimaryEndPoint.Address\"]}",
                "path": "newsinsight-db-dev/Exports/Output{\"Fn::GetAtt\":[\"RedisCluster\",\"PrimaryEndPoint.Address\"]}",
                "constructInfo": {
                  "fqn": "aws-cdk-lib.CfnOutput",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "constructs.Construct",
              "version": "10.4.4"
            }
          },
          "BootstrapVersion": {
            "id": "BootstrapVersion",
            "path": "newsinsight-db-dev/BootstrapVersion",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnParameter",
              "version": "2.173.0"
            }
          },
          "CheckBootstrapVersion": {
            "id": "CheckBootstrapVersion",
            "path": "newsinsight-db-dev/CheckBootstrapVersion",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnRule",
              "version": "2.173.0"
            }
          }
        },
        "constructInfo": {
          "fqn": "aws-cdk-lib.Stack",
          "version": "2.173.0"
        }
      },
      "newsinsight-alb-dev": {
        "id": "newsinsight-alb-dev",
        "path": "newsinsight-alb-dev",
        "children": {
          "AlbSecurityGroup": {
            "id": "AlbSecurityGroup",
            "path": "newsinsight-alb-dev/AlbSecurityGroup",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-alb-dev/AlbSecurityGroup/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::EC2::SecurityGroup",
                  "aws:cdk:cloudformation:props": {
                    "groupDescription": "Security group for Application Load Balancer",
                    "groupName": "newsinsight-alb-sg-dev",
                    "securityGroupEgress": [
                      {
                        "cidrIp": "0.0.0.0/0",
                        "description": "Allow all outbound traffic by default",
                        "ipProtocol": "-1"
                      }
                    ],
                    "securityGroupIngress": [
                      {
                        "cidrIp": "0.0.0.0/0",
                        "ipProtocol": "tcp",
                        "fromPort": 80,
                        "toPort": 80,
                        "description": "HTTP from anywhere"
                      }
                    ],
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ],
                    "vpcId": {
                      "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpc0ED561F3FDBE1BB1"
                    }
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ec2.CfnSecurityGroup",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ec2.SecurityGroup",
              "version": "2.173.0"
            }
          },
          "Alb": {
            "id": "Alb",
            "path": "newsinsight-alb-dev/Alb",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-alb-dev/Alb/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::ElasticLoadBalancingV2::LoadBalancer",
                  "aws:cdk:cloudformation:props": {
                    "loadBalancerAttributes": [
                      {
                        "key": "deletion_protection.enabled",
                        "value": "false"
                      }
                    ],
                    "name": "newsinsight-alb-dev",
                    "scheme": "internet-facing",
                    "securityGroups": [
                      {
                        "Fn::GetAtt": [
                          "AlbSecurityGroup86A59E99",
                          "GroupId"
                        ]
                      }
                    ],
                    "subnets": [
                      {
                        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPublicSubnet1SubnetDDF6BE05C056FD30"
                      },
                      {
                        "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPublicSubnet2SubnetA9DCA8686D3D4315"
                      }
                    ],
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ],
                    "type": "application"
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_elasticloadbalancingv2.CfnLoadBalancer",
                  "version": "2.173.0"
                }
              },
              "HttpListener": {
                "id": "HttpListener",
                "path": "newsinsight-alb-dev/Alb/HttpListener",
                "children": {
                  "Resource": {
                    "id": "Resource",
                    "path": "newsinsight-alb-dev/Alb/HttpListener/Resource",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::ElasticLoadBalancingV2::Listener",
                      "aws:cdk:cloudformation:props": {
                        "defaultActions": [
                          {
                            "type": "fixed-response",
                            "fixedResponseConfig": {
                              "statusCode": "404",
                              "contentType": "text/plain",
                              "messageBody": "Not Found"
                            }
                          }
                        ],
                        "loadBalancerArn": {
                          "Ref": "Alb16C2F182"
                        },
                        "port": 80,
                        "protocol": "HTTP"
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_elasticloadbalancingv2.CfnListener",
                      "version": "2.173.0"
                    }
                  },
                  "ListenerRule-frontendRule": {
                    "id": "ListenerRule-frontendRule",
                    "path": "newsinsight-alb-dev/Alb/HttpListener/ListenerRule-frontendRule",
                    "children": {
                      "Resource": {
                        "id": "Resource",
                        "path": "newsinsight-alb-dev/Alb/HttpListener/ListenerRule-frontendRule/Resource",
                        "attributes": {
                          "aws:cdk:cloudformation:type": "AWS::ElasticLoadBalancingV2::ListenerRule",
                          "aws:cdk:cloudformation:props": {
                            "actions": [
                              {
                                "type": "forward",
                                "targetGroupArn": {
                                  "Ref": "TGfrontend5DB1B90D"
                                }
                              }
                            ],
                            "conditions": [
                              {
                                "field": "path-pattern",
                                "pathPatternConfig": {
                                  "values": [
                                    "/*"
                                  ]
                                }
                              }
                            ],
                            "listenerArn": {
                              "Ref": "AlbHttpListener00C8B33E"
                            },
                            "priority": 100
                          }
                        },
                        "constructInfo": {
                          "fqn": "aws-cdk-lib.aws_elasticloadbalancingv2.CfnListenerRule",
                          "version": "2.173.0"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_elasticloadbalancingv2.ApplicationListenerRule",
                      "version": "2.173.0"
                    }
                  },
                  "ListenerRule-api-gatewayRule": {
                    "id": "ListenerRule-api-gatewayRule",
                    "path": "newsinsight-alb-dev/Alb/HttpListener/ListenerRule-api-gatewayRule",
                    "children": {
                      "Resource": {
                        "id": "Resource",
                        "path": "newsinsight-alb-dev/Alb/HttpListener/ListenerRule-api-gatewayRule/Resource",
                        "attributes": {
                          "aws:cdk:cloudformation:type": "AWS::ElasticLoadBalancingV2::ListenerRule",
                          "aws:cdk:cloudformation:props": {
                            "actions": [
                              {
                                "type": "forward",
                                "targetGroupArn": {
                                  "Ref": "TGapigatewayD7113A21"
                                }
                              }
                            ],
                            "conditions": [
                              {
                                "field": "path-pattern",
                                "pathPatternConfig": {
                                  "values": [
                                    "/api/*"
                                  ]
                                }
                              }
                            ],
                            "listenerArn": {
                              "Ref": "AlbHttpListener00C8B33E"
                            },
                            "priority": 10
                          }
                        },
                        "constructInfo": {
                          "fqn": "aws-cdk-lib.aws_elasticloadbalancingv2.CfnListenerRule",
                          "version": "2.173.0"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_elasticloadbalancingv2.ApplicationListenerRule",
                      "version": "2.173.0"
                    }
                  },
                  "ListenerRule-admin-dashboardRule": {
                    "id": "ListenerRule-admin-dashboardRule",
                    "path": "newsinsight-alb-dev/Alb/HttpListener/ListenerRule-admin-dashboardRule",
                    "children": {
                      "Resource": {
                        "id": "Resource",
                        "path": "newsinsight-alb-dev/Alb/HttpListener/ListenerRule-admin-dashboardRule/Resource",
                        "attributes": {
                          "aws:cdk:cloudformation:type": "AWS::ElasticLoadBalancingV2::ListenerRule",
                          "aws:cdk:cloudformation:props": {
                            "actions": [
                              {
                                "type": "forward",
                                "targetGroupArn": {
                                  "Ref": "TGadmindashboard10526AD8"
                                }
                              }
                            ],
                            "conditions": [
                              {
                                "field": "path-pattern",
                                "pathPatternConfig": {
                                  "values": [
                                    "/admin/*"
                                  ]
                                }
                              }
                            ],
                            "listenerArn": {
                              "Ref": "AlbHttpListener00C8B33E"
                            },
                            "priority": 20
                          }
                        },
                        "constructInfo": {
                          "fqn": "aws-cdk-lib.aws_elasticloadbalancingv2.CfnListenerRule",
                          "version": "2.173.0"
                        }
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_elasticloadbalancingv2.ApplicationListenerRule",
                      "version": "2.173.0"
                    }
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_elasticloadbalancingv2.ApplicationListener",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_elasticloadbalancingv2.ApplicationLoadBalancer",
              "version": "2.173.0"
            }
          },
          "TG-frontend": {
            "id": "TG-frontend",
            "path": "newsinsight-alb-dev/TG-frontend",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-alb-dev/TG-frontend/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::ElasticLoadBalancingV2::TargetGroup",
                  "aws:cdk:cloudformation:props": {
                    "healthCheckIntervalSeconds": 30,
                    "healthCheckPath": "/health",
                    "healthCheckTimeoutSeconds": 10,
                    "healthyThresholdCount": 2,
                    "name": "newsinsight-frontend-dev",
                    "port": 8080,
                    "protocol": "HTTP",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ],
                    "targetGroupAttributes": [
                      {
                        "key": "deregistration_delay.timeout_seconds",
                        "value": "30"
                      },
                      {
                        "key": "stickiness.enabled",
                        "value": "false"
                      }
                    ],
                    "targetType": "instance",
                    "unhealthyThresholdCount": 3,
                    "vpcId": {
                      "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpc0ED561F3FDBE1BB1"
                    }
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_elasticloadbalancingv2.CfnTargetGroup",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_elasticloadbalancingv2.ApplicationTargetGroup",
              "version": "2.173.0"
            }
          },
          "TG-api-gateway": {
            "id": "TG-api-gateway",
            "path": "newsinsight-alb-dev/TG-api-gateway",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-alb-dev/TG-api-gateway/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::ElasticLoadBalancingV2::TargetGroup",
                  "aws:cdk:cloudformation:props": {
                    "healthCheckIntervalSeconds": 30,
                    "healthCheckPath": "/actuator/health",
                    "healthCheckTimeoutSeconds": 10,
                    "healthyThresholdCount": 2,
                    "name": "newsinsight-api-gateway-dev",
                    "port": 8000,
                    "protocol": "HTTP",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ],
                    "targetGroupAttributes": [
                      {
                        "key": "deregistration_delay.timeout_seconds",
                        "value": "30"
                      },
                      {
                        "key": "stickiness.enabled",
                        "value": "false"
                      }
                    ],
                    "targetType": "instance",
                    "unhealthyThresholdCount": 3,
                    "vpcId": {
                      "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpc0ED561F3FDBE1BB1"
                    }
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_elasticloadbalancingv2.CfnTargetGroup",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_elasticloadbalancingv2.ApplicationTargetGroup",
              "version": "2.173.0"
            }
          },
          "TG-admin-dashboard": {
            "id": "TG-admin-dashboard",
            "path": "newsinsight-alb-dev/TG-admin-dashboard",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-alb-dev/TG-admin-dashboard/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::ElasticLoadBalancingV2::TargetGroup",
                  "aws:cdk:cloudformation:props": {
                    "healthCheckIntervalSeconds": 30,
                    "healthCheckPath": "/health",
                    "healthCheckTimeoutSeconds": 10,
                    "healthyThresholdCount": 2,
                    "name": "newsinsight-admin-dashboard-dev",
                    "port": 8888,
                    "protocol": "HTTP",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ],
                    "targetGroupAttributes": [
                      {
                        "key": "deregistration_delay.timeout_seconds",
                        "value": "30"
                      },
                      {
                        "key": "stickiness.enabled",
                        "value": "false"
                      }
                    ],
                    "targetType": "instance",
                    "unhealthyThresholdCount": 3,
                    "vpcId": {
                      "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpc0ED561F3FDBE1BB1"
                    }
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_elasticloadbalancingv2.CfnTargetGroup",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_elasticloadbalancingv2.ApplicationTargetGroup",
              "version": "2.173.0"
            }
          },
          "AlbDnsName": {
            "id": "AlbDnsName",
            "path": "newsinsight-alb-dev/AlbDnsName",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "AlbArn": {
            "id": "AlbArn",
            "path": "newsinsight-alb-dev/AlbArn",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "HttpListenerArn": {
            "id": "HttpListenerArn",
            "path": "newsinsight-alb-dev/HttpListenerArn",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "CDKMetadata": {
            "id": "CDKMetadata",
            "path": "newsinsight-alb-dev/CDKMetadata",
            "children": {
              "Default": {
                "id": "Default",
                "path": "newsinsight-alb-dev/CDKMetadata/Default",
                "constructInfo": {
                  "fqn": "aws-cdk-lib.CfnResource",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "constructs.Construct",
              "version": "10.4.4"
            }
          },
          "Exports": {
            "id": "Exports",
            "path": "newsinsight-alb-dev/Exports",
            "children": {
              "Output{\"Fn::GetAtt\":[\"AlbSecurityGroup86A59E99\",\"GroupId\"]}": {
                "id": "Output{\"Fn::GetAtt\":[\"AlbSecurityGroup86A59E99\",\"GroupId\"]}",
                "path": "newsinsight-alb-dev/Exports/Output{\"Fn::GetAtt\":[\"AlbSecurityGroup86A59E99\",\"GroupId\"]}",
                "constructInfo": {
                  "fqn": "aws-cdk-lib.CfnOutput",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "constructs.Construct",
              "version": "10.4.4"
            }
          },
          "BootstrapVersion": {
            "id": "BootstrapVersion",
            "path": "newsinsight-alb-dev/BootstrapVersion",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnParameter",
              "version": "2.173.0"
            }
          },
          "CheckBootstrapVersion": {
            "id": "CheckBootstrapVersion",
            "path": "newsinsight-alb-dev/CheckBootstrapVersion",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnRule",
              "version": "2.173.0"
            }
          }
        },
        "constructInfo": {
          "fqn": "aws-cdk-lib.Stack",
          "version": "2.173.0"
        }
      },
      "newsinsight-ec2-dev": {
        "id": "newsinsight-ec2-dev",
        "path": "newsinsight-ec2-dev",
        "children": {
          "Ec2SecurityGroup": {
            "id": "Ec2SecurityGroup",
            "path": "newsinsight-ec2-dev/Ec2SecurityGroup",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-ec2-dev/Ec2SecurityGroup/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::EC2::SecurityGroup",
                  "aws:cdk:cloudformation:props": {
                    "groupDescription": "Security group for NewsInsight EC2 instance",
                    "groupName": "newsinsight-ec2-sg-dev",
                    "securityGroupEgress": [
                      {
                        "cidrIp": "0.0.0.0/0",
                        "description": "Allow all outbound traffic by default",
                        "ipProtocol": "-1"
                      }
                    ],
                    "securityGroupIngress": [
                      {
                        "cidrIp": "0.0.0.0/0",
                        "ipProtocol": "tcp",
                        "fromPort": 22,
                        "toPort": 22,
                        "description": "SSH access"
                      },
                      {
                        "sourceSecurityGroupId": {
                          "Fn::ImportValue": "newsinsight-alb-dev:ExportsOutputFnGetAttAlbSecurityGroup86A59E99GroupIdE3A37BC7"
                        },
                        "ipProtocol": "tcp",
                        "fromPort": 8000,
                        "toPort": 9000,
                        "description": "Traffic from ALB"
                      },
                      {
                        "cidrIp": {
                          "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputFnGetAttNewsInsightVpc0ED561F3CidrBlock6740B7E1"
                        },
                        "ipProtocol": "tcp",
                        "fromPort": 0,
                        "toPort": 65535,
                        "description": "Internal VPC traffic"
                      }
                    ],
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ],
                    "vpcId": {
                      "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpc0ED561F3FDBE1BB1"
                    }
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ec2.CfnSecurityGroup",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ec2.SecurityGroup",
              "version": "2.173.0"
            }
          },
          "Ec2Role": {
            "id": "Ec2Role",
            "path": "newsinsight-ec2-dev/Ec2Role",
            "children": {
              "ImportEc2Role": {
                "id": "ImportEc2Role",
                "path": "newsinsight-ec2-dev/Ec2Role/ImportEc2Role",
                "constructInfo": {
                  "fqn": "aws-cdk-lib.Resource",
                  "version": "2.173.0"
                }
              },
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-ec2-dev/Ec2Role/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::IAM::Role",
                  "aws:cdk:cloudformation:props": {
                    "assumeRolePolicyDocument": {
                      "Statement": [
                        {
                          "Action": "sts:AssumeRole",
                          "Effect": "Allow",
                          "Principal": {
                            "Service": "ec2.amazonaws.com"
                          }
                        }
                      ],
                      "Version": "2012-10-17"
                    },
                    "managedPolicyArns": [
                      {
                        "Fn::Join": [
                          "",
                          [
                            "arn:",
                            {
                              "Ref": "AWS::Partition"
                            },
                            ":iam::aws:policy/AmazonSSMManagedInstanceCore"
                          ]
                        ]
                      },
                      {
                        "Fn::Join": [
                          "",
                          [
                            "arn:",
                            {
                              "Ref": "AWS::Partition"
                            },
                            ":iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
                          ]
                        ]
                      },
                      {
                        "Fn::Join": [
                          "",
                          [
                            "arn:",
                            {
                              "Ref": "AWS::Partition"
                            },
                            ":iam::aws:policy/CloudWatchAgentServerPolicy"
                          ]
                        ]
                      }
                    ],
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_iam.CfnRole",
                  "version": "2.173.0"
                }
              },
              "DefaultPolicy": {
                "id": "DefaultPolicy",
                "path": "newsinsight-ec2-dev/Ec2Role/DefaultPolicy",
                "children": {
                  "Resource": {
                    "id": "Resource",
                    "path": "newsinsight-ec2-dev/Ec2Role/DefaultPolicy/Resource",
                    "attributes": {
                      "aws:cdk:cloudformation:type": "AWS::IAM::Policy",
                      "aws:cdk:cloudformation:props": {
                        "policyDocument": {
                          "Statement": [
                            {
                              "Action": [
                                "secretsmanager:DescribeSecret",
                                "secretsmanager:GetSecretValue"
                              ],
                              "Effect": "Allow",
                              "Resource": [
                                {
                                  "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefMongoSecretEC0FA3F415029244"
                                },
                                {
                                  "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefPostgresSecretE142E1BCDFDB523F"
                                },
                                {
                                  "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputRefRedisSecretCB3BD9716380542C"
                                }
                              ]
                            }
                          ],
                          "Version": "2012-10-17"
                        },
                        "policyName": "Ec2RoleDefaultPolicy99BC91F7",
                        "roles": [
                          {
                            "Ref": "Ec2Role2FD9A272"
                          }
                        ]
                      }
                    },
                    "constructInfo": {
                      "fqn": "aws-cdk-lib.aws_iam.CfnPolicy",
                      "version": "2.173.0"
                    }
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_iam.Policy",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_iam.Role",
              "version": "2.173.0"
            }
          },
          "KeyPair": {
            "id": "KeyPair",
            "path": "newsinsight-ec2-dev/KeyPair",
            "children": {
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-ec2-dev/KeyPair/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::EC2::KeyPair",
                  "aws:cdk:cloudformation:props": {
                    "keyFormat": "pem",
                    "keyName": "newsinsight-keypair-dev",
                    "keyType": "rsa",
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Project",
                        "value": "NewsInsight"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ec2.CfnKeyPair",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ec2.KeyPair",
              "version": "2.173.0"
            }
          },
          "Instance": {
            "id": "Instance",
            "path": "newsinsight-ec2-dev/Instance",
            "children": {
              "InstanceProfile": {
                "id": "InstanceProfile",
                "path": "newsinsight-ec2-dev/Instance/InstanceProfile",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::IAM::InstanceProfile",
                  "aws:cdk:cloudformation:props": {
                    "roles": [
                      {
                        "Ref": "Ec2Role2FD9A272"
                      }
                    ]
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_iam.CfnInstanceProfile",
                  "version": "2.173.0"
                }
              },
              "Resource": {
                "id": "Resource",
                "path": "newsinsight-ec2-dev/Instance/Resource",
                "attributes": {
                  "aws:cdk:cloudformation:type": "AWS::EC2::Instance",
                  "aws:cdk:cloudformation:props": {
                    "availabilityZone": "ap-northeast-2a",
                    "blockDeviceMappings": [
                      {
                        "deviceName": "/dev/xvda",
                        "ebs": {
                          "encrypted": true,
                          "volumeSize": 50,
                          "volumeType": "gp3"
                        }
                      }
                    ],
                    "iamInstanceProfile": {
                      "Ref": "InstanceInstanceProfileAB5AEF02"
                    },
                    "imageId": {
                      "Ref": "SsmParameterValueawsserviceamiamazonlinuxlatestal2023amikernel61x8664C96584B6F00A464EAD1953AFF4B05118Parameter"
                    },
                    "instanceType": "t3.large",
                    "keyName": {
                      "Ref": "KeyPair1622897B"
                    },
                    "securityGroupIds": [
                      {
                        "Fn::GetAtt": [
                          "Ec2SecurityGroup55889913",
                          "GroupId"
                        ]
                      }
                    ],
                    "subnetId": {
                      "Fn::ImportValue": "newsinsight-vpc-dev:ExportsOutputRefNewsInsightVpcPublicSubnet1SubnetDDF6BE05C056FD30"
                    },
                    "tags": [
                      {
                        "key": "Environment",
                        "value": "dev"
                      },
                      {
                        "key": "ManagedBy",
                        "value": "CDK"
                      },
                      {
                        "key": "Name",
                        "value": "newsinsight-instance-dev"
                      },
                      {
                        "key": "Project",
                        "value": "newsinsight"
                      }
                    ],
                    "userData": {
                      "Fn::Base64": {
                        "Fn::Join": [
                          "",
                          [
                            "#!/bin/bash\n#!/bin/bash\nset -ex\n\n# System updates\nyum update -y\n\n# Install Docker\nyum install -y docker git\nsystemctl start docker\nsystemctl enable docker\nusermod -aG docker ec2-user\n\n# Install Docker Compose\ncurl -L \"https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\nchmod +x /usr/local/bin/docker-compose\nln -sf /usr/local/bin/docker-compose /usr/bin/docker-compose\n\n# Install AWS CLI v2\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nyum install -y unzip\nunzip -q awscliv2.zip\n./aws/install --update\nrm -rf aws awscliv2.zip\n\n# Create app directory\nmkdir -p /home/ec2-user/newsinsight\nchown ec2-user:ec2-user /home/ec2-user/newsinsight\n\n# Set environment variables\necho 'export POSTGRES_HOST=",
                            {
                              "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttPostgresClusterE64235C6EndpointAddress8684A102"
                            },
                            "' >> /etc/profile.d/newsinsight.sh\necho 'export MONGO_HOST=",
                            {
                              "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttDocDbClusterB46CF5D3EndpointD6B42638"
                            },
                            "' >> /etc/profile.d/newsinsight.sh\necho 'export REDIS_HOST=",
                            {
                              "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttRedisClusterPrimaryEndPointAddressCFB16E1D"
                            },
                            "' >> /etc/profile.d/newsinsight.sh\necho 'export AWS_REGION=ap-northeast-2' >> /etc/profile.d/newsinsight.sh\necho 'export ENVIRONMENT=dev' >> /etc/profile.d/newsinsight.sh\necho 'export ECR_REGISTRY=130954244737.dkr.ecr.ap-northeast-2.amazonaws.com' >> /etc/profile.d/newsinsight.sh\nchmod +x /etc/profile.d/newsinsight.sh\n\n# Login to ECR\naws ecr get-login-password --region ap-northeast-2 | docker login --username AWS --password-stdin 130954244737.dkr.ecr.ap-northeast-2.amazonaws.com\n\n# Create docker-compose.yml\ncat > /home/ec2-user/newsinsight/docker-compose.yml << 'COMPOSE_EOF'\nversion: \"3.8\"\n\nservices:\n  frontend:\n    image: ${ECR_REGISTRY}/newsinsight/frontend:latest\n    ports:\n      - \"8080:8080\"\n    environment:\n      - NODE_ENV=production\n      - API_GATEWAY_URL=http://api-gateway:8000\n    restart: unless-stopped\n\n  api-gateway:\n    image: ${ECR_REGISTRY}/newsinsight/api-gateway:latest\n    ports:\n      - \"8000:8000\"\n    environment:\n      - SPRING_PROFILES_ACTIVE=${ENVIRONMENT}\n      - POSTGRES_HOST=${POSTGRES_HOST}\n      - POSTGRES_PORT=5432\n      - REDIS_HOST=${REDIS_HOST}\n      - REDIS_PORT=6379\n    restart: unless-stopped\n\n  admin-dashboard:\n    image: ${ECR_REGISTRY}/newsinsight/admin-dashboard:latest\n    ports:\n      - \"8888:8888\"\n    environment:\n      - POSTGRES_HOST=${POSTGRES_HOST}\n      - MONGO_HOST=${MONGO_HOST}\n    restart: unless-stopped\n\nnetworks:\n  default:\n    name: newsinsight-network\nCOMPOSE_EOF\n\n# Create .env file\ncat > /home/ec2-user/newsinsight/.env << ENV_EOF\nECR_REGISTRY=130954244737.dkr.ecr.ap-northeast-2.amazonaws.com\nPOSTGRES_HOST=",
                            {
                              "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttPostgresClusterE64235C6EndpointAddress8684A102"
                            },
                            "\nMONGO_HOST=",
                            {
                              "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttDocDbClusterB46CF5D3EndpointD6B42638"
                            },
                            "\nREDIS_HOST=",
                            {
                              "Fn::ImportValue": "newsinsight-db-dev:ExportsOutputFnGetAttRedisClusterPrimaryEndPointAddressCFB16E1D"
                            },
                            "\nENVIRONMENT=dev\nENV_EOF\n\nchown -R ec2-user:ec2-user /home/ec2-user/newsinsight\n\n# Create startup script\ncat > /home/ec2-user/newsinsight/start.sh << 'START_EOF'\n#!/bin/bash\ncd /home/ec2-user/newsinsight\nsource /etc/profile.d/newsinsight.sh\naws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $ECR_REGISTRY\ndocker-compose pull\ndocker-compose up -d\nSTART_EOF\nchmod +x /home/ec2-user/newsinsight/start.sh\nchown ec2-user:ec2-user /home/ec2-user/newsinsight/start.sh\n\necho \"NewsInsight EC2 setup complete!\" > /var/log/newsinsight-setup.log"
                          ]
                        ]
                      }
                    }
                  }
                },
                "constructInfo": {
                  "fqn": "aws-cdk-lib.aws_ec2.CfnInstance",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "aws-cdk-lib.aws_ec2.Instance",
              "version": "2.173.0"
            }
          },
          "SsmParameterValue:--aws--service--ami-amazon-linux-latest--al2023-ami-kernel-6.1-x86_64:C96584B6-F00A-464E-AD19-53AFF4B05118.Parameter": {
            "id": "SsmParameterValue:--aws--service--ami-amazon-linux-latest--al2023-ami-kernel-6.1-x86_64:C96584B6-F00A-464E-AD19-53AFF4B05118.Parameter",
            "path": "newsinsight-ec2-dev/SsmParameterValue:--aws--service--ami-amazon-linux-latest--al2023-ami-kernel-6.1-x86_64:C96584B6-F00A-464E-AD19-53AFF4B05118.Parameter",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnParameter",
              "version": "2.173.0"
            }
          },
          "SsmParameterValue:--aws--service--ami-amazon-linux-latest--al2023-ami-kernel-6.1-x86_64:C96584B6-F00A-464E-AD19-53AFF4B05118": {
            "id": "SsmParameterValue:--aws--service--ami-amazon-linux-latest--al2023-ami-kernel-6.1-x86_64:C96584B6-F00A-464E-AD19-53AFF4B05118",
            "path": "newsinsight-ec2-dev/SsmParameterValue:--aws--service--ami-amazon-linux-latest--al2023-ami-kernel-6.1-x86_64:C96584B6-F00A-464E-AD19-53AFF4B05118",
            "constructInfo": {
              "fqn": "aws-cdk-lib.Resource",
              "version": "2.173.0"
            }
          },
          "InstanceId": {
            "id": "InstanceId",
            "path": "newsinsight-ec2-dev/InstanceId",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "InstancePublicIp": {
            "id": "InstancePublicIp",
            "path": "newsinsight-ec2-dev/InstancePublicIp",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "InstancePrivateIp": {
            "id": "InstancePrivateIp",
            "path": "newsinsight-ec2-dev/InstancePrivateIp",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "KeyPairId": {
            "id": "KeyPairId",
            "path": "newsinsight-ec2-dev/KeyPairId",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "SSHCommand": {
            "id": "SSHCommand",
            "path": "newsinsight-ec2-dev/SSHCommand",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnOutput",
              "version": "2.173.0"
            }
          },
          "CDKMetadata": {
            "id": "CDKMetadata",
            "path": "newsinsight-ec2-dev/CDKMetadata",
            "children": {
              "Default": {
                "id": "Default",
                "path": "newsinsight-ec2-dev/CDKMetadata/Default",
                "constructInfo": {
                  "fqn": "aws-cdk-lib.CfnResource",
                  "version": "2.173.0"
                }
              }
            },
            "constructInfo": {
              "fqn": "constructs.Construct",
              "version": "10.4.4"
            }
          },
          "BootstrapVersion": {
            "id": "BootstrapVersion",
            "path": "newsinsight-ec2-dev/BootstrapVersion",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnParameter",
              "version": "2.173.0"
            }
          },
          "CheckBootstrapVersion": {
            "id": "CheckBootstrapVersion",
            "path": "newsinsight-ec2-dev/CheckBootstrapVersion",
            "constructInfo": {
              "fqn": "aws-cdk-lib.CfnRule",
              "version": "2.173.0"
            }
          }
        },
        "constructInfo": {
          "fqn": "aws-cdk-lib.Stack",
          "version": "2.173.0"
        }
      },
      "Tree": {
        "id": "Tree",
        "path": "Tree",
        "constructInfo": {
          "fqn": "constructs.Construct",
          "version": "10.4.4"
        }
      }
    },
    "constructInfo": {
      "fqn": "aws-cdk-lib.App",
      "version": "2.173.0"
    }
  }
}
```

---

## aws/cdk/lib/alb-stack.ts

```ts
import * as cdk from 'aws-cdk-lib';
import * as ec2 from 'aws-cdk-lib/aws-ec2';
import * as elbv2 from 'aws-cdk-lib/aws-elasticloadbalancingv2';
import { Construct } from 'constructs';

export interface AlbStackProps extends cdk.StackProps {
  envName: string;
  projectName: string;
  vpc: ec2.Vpc;
  tags?: { [key: string]: string };
}

export class NewsInsightAlbStack extends cdk.Stack {
  public readonly alb: elbv2.ApplicationLoadBalancer;
  public readonly httpListener: elbv2.ApplicationListener;
  public readonly albSecurityGroup: ec2.SecurityGroup;
  // Expose Target Groups so ECS Stack can attach services to them
  public readonly targetGroups: { [key: string]: elbv2.ApplicationTargetGroup } = {};

  constructor(scope: Construct, id: string, props: AlbStackProps) {
    super(scope, id, props);

    const { envName, projectName, vpc } = props;

    // Security Group for ALB
    this.albSecurityGroup = new ec2.SecurityGroup(this, 'AlbSecurityGroup', {
      vpc,
      description: 'Security group for Application Load Balancer',
      securityGroupName: `${projectName}-alb-sg-${envName}`,
    });

    this.albSecurityGroup.addIngressRule(
      ec2.Peer.anyIpv4(),
      ec2.Port.tcp(80),
      'HTTP from anywhere'
    );

    // Application Load Balancer
    this.alb = new elbv2.ApplicationLoadBalancer(this, 'Alb', {
      vpc,
      internetFacing: true,
      loadBalancerName: `${projectName}-alb-${envName}`,
      securityGroup: this.albSecurityGroup,
      vpcSubnets: { subnetType: ec2.SubnetType.PUBLIC },
    });

    // HTTP Listener with default 404 response
    this.httpListener = this.alb.addListener('HttpListener', {
      port: 80,
      defaultAction: elbv2.ListenerAction.fixedResponse(404, {
        contentType: 'text/plain',
        messageBody: 'Not Found',
      }),
    });

    // === Define Load Balanced Services & Create Target Groups ===
    const lbServices = [
      {
        name: 'frontend',
        port: 8080,
        healthCheckPath: '/health',
        pathPattern: '/*',
        priority: 100,
      },
      {
        name: 'api-gateway',
        port: 8000,
        healthCheckPath: '/actuator/health',
        pathPattern: '/api/*',
        priority: 10,
      },
      {
        name: 'admin-dashboard',
        port: 8888,
        healthCheckPath: '/health',
        pathPattern: '/admin/*',
        priority: 20,
      }
    ];

    for (const service of lbServices) {
      const targetGroup = new elbv2.ApplicationTargetGroup(this, `TG-${service.name}`, {
        vpc,
        targetGroupName: `${projectName}-${service.name}-${envName}`.substring(0, 32),
        port: service.port,
        protocol: elbv2.ApplicationProtocol.HTTP,
        targetType: elbv2.TargetType.INSTANCE,
        deregistrationDelay: cdk.Duration.seconds(30),
        healthCheck: {
          path: service.healthCheckPath,
          healthyThresholdCount: 2,
          unhealthyThresholdCount: 3,
          interval: cdk.Duration.seconds(30),
          timeout: cdk.Duration.seconds(10),
        },
      });

      this.httpListener.addTargetGroups(`ListenerRule-${service.name}`, {
        targetGroups: [targetGroup],
        conditions: [
          elbv2.ListenerCondition.pathPatterns([service.pathPattern]),
        ],
        priority: service.priority,
      });

      // Store in public property for ECS Stack
      this.targetGroups[service.name] = targetGroup;
    }

    // Outputs
    new cdk.CfnOutput(this, 'AlbDnsName', {
      value: this.alb.loadBalancerDnsName,
      description: 'ALB DNS Name',
      exportName: `${projectName}-alb-dns-${envName}`,
    });

    new cdk.CfnOutput(this, 'AlbArn', {
      value: this.alb.loadBalancerArn,
      description: 'ALB ARN',
      exportName: `${projectName}-alb-arn-${envName}`,
    });

    new cdk.CfnOutput(this, 'HttpListenerArn', {
      value: this.httpListener.listenerArn,
      description: 'HTTP Listener ARN',
      exportName: `${projectName}-http-listener-${envName}`,
    });
  }
}

```

---

## aws/cdk/lib/database-stack.ts

```ts
import * as cdk from 'aws-cdk-lib';
import * as ec2 from 'aws-cdk-lib/aws-ec2';
import * as rds from 'aws-cdk-lib/aws-rds';
import * as docdb from 'aws-cdk-lib/aws-docdb';
import * as elasticache from 'aws-cdk-lib/aws-elasticache';
import * as secretsmanager from 'aws-cdk-lib/aws-secretsmanager';
import { Construct } from 'constructs';

export interface DatabaseStackProps extends cdk.StackProps {
  envName: string;
  projectName: string;
  vpc: ec2.Vpc;
  tags?: { [key: string]: string };
}

export class NewsInsightDatabaseStack extends cdk.Stack {
  public readonly secrets: {
    postgres: secretsmanager.Secret;
    mongo: secretsmanager.Secret;
    redis: secretsmanager.Secret;
  };
  public readonly postgresEndpoint: string;
  public readonly mongoEndpoint: string;
  public readonly redisEndpoint: string;

  constructor(scope: Construct, id: string, props: DatabaseStackProps) {
    super(scope, id, props);

    const { envName, projectName, vpc } = props;
    const isProd = envName === 'prod';

    // Security Groups
    const dbSecurityGroup = new ec2.SecurityGroup(this, 'DbSecurityGroup', {
      vpc,
      description: 'Security group for databases',
      securityGroupName: `${projectName}-db-sg-${envName}`,
    });

    // Allow internal VPC access
    dbSecurityGroup.addIngressRule(
      ec2.Peer.ipv4(vpc.vpcCidrBlock),
      ec2.Port.tcp(5432),
      'PostgreSQL from VPC'
    );
    dbSecurityGroup.addIngressRule(
      ec2.Peer.ipv4(vpc.vpcCidrBlock),
      ec2.Port.tcp(27017),
      'MongoDB from VPC'
    );
    dbSecurityGroup.addIngressRule(
      ec2.Peer.ipv4(vpc.vpcCidrBlock),
      ec2.Port.tcp(6379),
      'Redis from VPC'
    );

    // ============================================================
    // Secrets
    // ============================================================
    const postgresSecret = new secretsmanager.Secret(this, 'PostgresSecret', {
      secretName: `${projectName}/${envName}/postgres`,
      description: 'PostgreSQL credentials',
      generateSecretString: {
        secretStringTemplate: JSON.stringify({
          username: 'newsinsight',
          database: 'newsinsight',
        }),
        generateStringKey: 'password',
        excludePunctuation: true,
        passwordLength: 32,
      },
    });

    const mongoSecret = new secretsmanager.Secret(this, 'MongoSecret', {
      secretName: `${projectName}/${envName}/mongo`,
      description: 'MongoDB credentials',
      generateSecretString: {
        secretStringTemplate: JSON.stringify({
          username: 'newsinsight',
        }),
        generateStringKey: 'password',
        excludePunctuation: true,
        passwordLength: 32,
      },
    });

    const redisSecret = new secretsmanager.Secret(this, 'RedisSecret', {
      secretName: `${projectName}/${envName}/redis`,
      description: 'Redis auth token',
      generateSecretString: {
        excludePunctuation: true,
        passwordLength: 32,
      },
    });

    this.secrets = {
      postgres: postgresSecret,
      mongo: mongoSecret,
      redis: redisSecret,
    };

    // ============================================================
    // PostgreSQL (RDS with pgvector extension)
    // ============================================================
    const postgresCluster = new rds.DatabaseCluster(this, 'PostgresCluster', {
      engine: rds.DatabaseClusterEngine.auroraPostgres({
        version: rds.AuroraPostgresEngineVersion.VER_15_8,
      }),
      credentials: rds.Credentials.fromSecret(postgresSecret),
      defaultDatabaseName: 'newsinsight',
      instanceProps: {
        vpc,
        vpcSubnets: { subnetType: ec2.SubnetType.PRIVATE_ISOLATED },
        instanceType: isProd 
          ? ec2.InstanceType.of(ec2.InstanceClass.R6G, ec2.InstanceSize.LARGE)
          : ec2.InstanceType.of(ec2.InstanceClass.T4G, ec2.InstanceSize.MEDIUM),
        securityGroups: [dbSecurityGroup],
      },
      instances: isProd ? 2 : 1,
      storageEncrypted: true,
      backup: {
        retention: cdk.Duration.days(isProd ? 30 : 7),
      },
      removalPolicy: isProd ? cdk.RemovalPolicy.RETAIN : cdk.RemovalPolicy.DESTROY,
      deletionProtection: isProd,
      clusterIdentifier: `${projectName}-postgres-${envName}`,
    });

    this.postgresEndpoint = postgresCluster.clusterEndpoint.hostname;

    // ============================================================
    // DocumentDB (MongoDB compatible)
    // ============================================================
    const docDbCluster = new docdb.DatabaseCluster(this, 'DocDbCluster', {
      masterUser: {
        username: 'newsinsight',
        secretName: `${projectName}/${envName}/docdb`,
      },
      vpc,
      vpcSubnets: { subnetType: ec2.SubnetType.PRIVATE_ISOLATED },
      instanceType: isProd
        ? ec2.InstanceType.of(ec2.InstanceClass.R6G, ec2.InstanceSize.LARGE)
        : ec2.InstanceType.of(ec2.InstanceClass.T4G, ec2.InstanceSize.MEDIUM),
      instances: isProd ? 2 : 1,
      securityGroup: dbSecurityGroup,
      storageEncrypted: true,
      backup: {
        retention: cdk.Duration.days(isProd ? 30 : 7),
      },
      removalPolicy: isProd ? cdk.RemovalPolicy.RETAIN : cdk.RemovalPolicy.DESTROY,
      deletionProtection: isProd,
      dbClusterName: `${projectName}-docdb-${envName}`,
    });

    this.mongoEndpoint = docDbCluster.clusterEndpoint.hostname;

    // ============================================================
    // ElastiCache Redis
    // ============================================================
    const redisSubnetGroup = new elasticache.CfnSubnetGroup(this, 'RedisSubnetGroup', {
      description: 'Subnet group for Redis',
      subnetIds: vpc.isolatedSubnets.map(s => s.subnetId),
      cacheSubnetGroupName: `${projectName}-redis-subnet-${envName}`,
    });

    const redisCluster = new elasticache.CfnReplicationGroup(this, 'RedisCluster', {
      replicationGroupDescription: `${projectName} Redis cluster`,
      replicationGroupId: `${projectName}-redis-${envName}`,
      engine: 'redis',
      engineVersion: '7.0',
      cacheNodeType: isProd ? 'cache.r6g.large' : 'cache.t4g.medium',
      numCacheClusters: isProd ? 2 : 1,
      automaticFailoverEnabled: isProd,
      multiAzEnabled: isProd,
      cacheSubnetGroupName: redisSubnetGroup.cacheSubnetGroupName,
      securityGroupIds: [dbSecurityGroup.securityGroupId],
      atRestEncryptionEnabled: true,
      transitEncryptionEnabled: true,
      authToken: redisSecret.secretValue.unsafeUnwrap(),
      snapshotRetentionLimit: isProd ? 7 : 1,
    });
    redisCluster.addDependency(redisSubnetGroup);

    this.redisEndpoint = redisCluster.attrPrimaryEndPointAddress;

    // ============================================================
    // Outputs
    // ============================================================
    new cdk.CfnOutput(this, 'PostgresEndpoint', {
      value: this.postgresEndpoint,
      description: 'PostgreSQL endpoint',
      exportName: `${projectName}-postgres-endpoint-${envName}`,
    });

    new cdk.CfnOutput(this, 'MongoEndpoint', {
      value: this.mongoEndpoint,
      description: 'DocumentDB endpoint',
      exportName: `${projectName}-mongo-endpoint-${envName}`,
    });

    new cdk.CfnOutput(this, 'RedisEndpoint', {
      value: this.redisEndpoint,
      description: 'Redis endpoint',
      exportName: `${projectName}-redis-endpoint-${envName}`,
    });

    new cdk.CfnOutput(this, 'PostgresSecretArn', {
      value: postgresSecret.secretArn,
      description: 'PostgreSQL secret ARN',
      exportName: `${projectName}-postgres-secret-${envName}`,
    });

    new cdk.CfnOutput(this, 'MongoSecretArn', {
      value: mongoSecret.secretArn,
      description: 'MongoDB secret ARN',
      exportName: `${projectName}-mongo-secret-${envName}`,
    });

    new cdk.CfnOutput(this, 'RedisSecretArn', {
      value: redisSecret.secretArn,
      description: 'Redis secret ARN',
      exportName: `${projectName}-redis-secret-${envName}`,
    });
  }
}

```

---

## aws/cdk/lib/ec2-stack.ts

```ts
import * as cdk from 'aws-cdk-lib';
import * as ec2 from 'aws-cdk-lib/aws-ec2';
import * as iam from 'aws-cdk-lib/aws-iam';
import * as secretsmanager from 'aws-cdk-lib/aws-secretsmanager';
import { Construct } from 'constructs';

export interface Ec2StackProps extends cdk.StackProps {
  envName: string;
  projectName: string;
  vpc: ec2.Vpc;
  albSecurityGroupId: string;
  databaseSecrets: {
    postgres: secretsmanager.ISecret;
    mongo: secretsmanager.ISecret;
    redis: secretsmanager.ISecret;
  };
  postgresEndpoint: string;
  mongoEndpoint: string;
  redisEndpoint: string;
  tags?: { [key: string]: string };
}

export class NewsInsightEc2Stack extends cdk.Stack {
  public readonly instance: ec2.Instance;
  public readonly instanceId: string;

  constructor(scope: Construct, id: string, props: Ec2StackProps) {
    super(scope, id, props);

    const { envName, projectName, vpc, albSecurityGroupId,
            databaseSecrets, postgresEndpoint, mongoEndpoint, redisEndpoint } = props;

    // Security Group for EC2
    const ec2SecurityGroup = new ec2.SecurityGroup(this, 'Ec2SecurityGroup', {
      vpc,
      description: 'Security group for NewsInsight EC2 instance',
      securityGroupName: `${projectName}-ec2-sg-${envName}`,
      allowAllOutbound: true,
    });

    // Allow SSH
    ec2SecurityGroup.addIngressRule(
      ec2.Peer.anyIpv4(),
      ec2.Port.tcp(22),
      'SSH access'
    );

    // Allow traffic from ALB
    ec2SecurityGroup.addIngressRule(
      ec2.Peer.securityGroupId(albSecurityGroupId),
      ec2.Port.tcpRange(8000, 9000),
      'Traffic from ALB'
    );

    // Allow all internal VPC traffic
    ec2SecurityGroup.addIngressRule(
      ec2.Peer.ipv4(vpc.vpcCidrBlock),
      ec2.Port.allTcp(),
      'Internal VPC traffic'
    );

    // IAM Role for EC2
    const ec2Role = new iam.Role(this, 'Ec2Role', {
      assumedBy: new iam.ServicePrincipal('ec2.amazonaws.com'),
      managedPolicies: [
        iam.ManagedPolicy.fromAwsManagedPolicyName('AmazonSSMManagedInstanceCore'),
        iam.ManagedPolicy.fromAwsManagedPolicyName('AmazonEC2ContainerRegistryReadOnly'),
        iam.ManagedPolicy.fromAwsManagedPolicyName('CloudWatchAgentServerPolicy'),
      ],
    });

    // Grant secrets access
    databaseSecrets.postgres.grantRead(ec2Role);
    databaseSecrets.mongo.grantRead(ec2Role);
    databaseSecrets.redis.grantRead(ec2Role);

    // User data script
    const userData = ec2.UserData.forLinux();
    userData.addCommands(
      '#!/bin/bash',
      'set -ex',
      '',
      '# System updates',
      'yum update -y',
      '',
      '# Install Docker',
      'yum install -y docker git',
      'systemctl start docker',
      'systemctl enable docker',
      'usermod -aG docker ec2-user',
      '',
      '# Install Docker Compose',
      'curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose',
      'chmod +x /usr/local/bin/docker-compose',
      'ln -sf /usr/local/bin/docker-compose /usr/bin/docker-compose',
      '',
      '# Install AWS CLI v2',
      'curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"',
      'yum install -y unzip',
      'unzip -q awscliv2.zip',
      './aws/install --update',
      'rm -rf aws awscliv2.zip',
      '',
      '# Create app directory',
      'mkdir -p /home/ec2-user/newsinsight',
      'chown ec2-user:ec2-user /home/ec2-user/newsinsight',
      '',
      '# Set environment variables',
      `echo 'export POSTGRES_HOST=${postgresEndpoint}' >> /etc/profile.d/newsinsight.sh`,
      `echo 'export MONGO_HOST=${mongoEndpoint}' >> /etc/profile.d/newsinsight.sh`,
      `echo 'export REDIS_HOST=${redisEndpoint}' >> /etc/profile.d/newsinsight.sh`,
      `echo 'export AWS_REGION=${this.region}' >> /etc/profile.d/newsinsight.sh`,
      `echo 'export ENVIRONMENT=${envName}' >> /etc/profile.d/newsinsight.sh`,
      `echo 'export ECR_REGISTRY=${this.account}.dkr.ecr.${this.region}.amazonaws.com' >> /etc/profile.d/newsinsight.sh`,
      'chmod +x /etc/profile.d/newsinsight.sh',
      '',
      '# Login to ECR',
      `aws ecr get-login-password --region ${this.region} | docker login --username AWS --password-stdin ${this.account}.dkr.ecr.${this.region}.amazonaws.com`,
      '',
      '# Create docker-compose.yml',
      'cat > /home/ec2-user/newsinsight/docker-compose.yml << \'COMPOSE_EOF\'',
      'version: "3.8"',
      '',
      'services:',
      '  frontend:',
      '    image: ${ECR_REGISTRY}/newsinsight/frontend:latest',
      '    ports:',
      '      - "8080:8080"',
      '    environment:',
      '      - NODE_ENV=production',
      '      - API_GATEWAY_URL=http://api-gateway:8000',
      '    restart: unless-stopped',
      '',
      '  api-gateway:',
      '    image: ${ECR_REGISTRY}/newsinsight/api-gateway:latest',
      '    ports:',
      '      - "8000:8000"',
      '    environment:',
      '      - SPRING_PROFILES_ACTIVE=${ENVIRONMENT}',
      '      - POSTGRES_HOST=${POSTGRES_HOST}',
      '      - POSTGRES_PORT=5432',
      '      - REDIS_HOST=${REDIS_HOST}',
      '      - REDIS_PORT=6379',
      '    restart: unless-stopped',
      '',
      '  admin-dashboard:',
      '    image: ${ECR_REGISTRY}/newsinsight/admin-dashboard:latest',
      '    ports:',
      '      - "8888:8888"',
      '    environment:',
      '      - POSTGRES_HOST=${POSTGRES_HOST}',
      '      - MONGO_HOST=${MONGO_HOST}',
      '    restart: unless-stopped',
      '',
      'networks:',
      '  default:',
      '    name: newsinsight-network',
      'COMPOSE_EOF',
      '',
      '# Create .env file',
      'cat > /home/ec2-user/newsinsight/.env << ENV_EOF',
      `ECR_REGISTRY=${this.account}.dkr.ecr.${this.region}.amazonaws.com`,
      `POSTGRES_HOST=${postgresEndpoint}`,
      `MONGO_HOST=${mongoEndpoint}`,
      `REDIS_HOST=${redisEndpoint}`,
      `ENVIRONMENT=${envName}`,
      'ENV_EOF',
      '',
      'chown -R ec2-user:ec2-user /home/ec2-user/newsinsight',
      '',
      '# Create startup script',
      'cat > /home/ec2-user/newsinsight/start.sh << \'START_EOF\'',
      '#!/bin/bash',
      'cd /home/ec2-user/newsinsight',
      'source /etc/profile.d/newsinsight.sh',
      'aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $ECR_REGISTRY',
      'docker-compose pull',
      'docker-compose up -d',
      'START_EOF',
      'chmod +x /home/ec2-user/newsinsight/start.sh',
      'chown ec2-user:ec2-user /home/ec2-user/newsinsight/start.sh',
      '',
      'echo "NewsInsight EC2 setup complete!" > /var/log/newsinsight-setup.log',
    );

    // Create Key Pair
    const keyPair = new ec2.KeyPair(this, 'KeyPair', {
      keyPairName: `${projectName}-keypair-${envName}`,
      type: ec2.KeyPairType.RSA,
    });

    // EC2 Instance
    this.instance = new ec2.Instance(this, 'Instance', {
      vpc,
      vpcSubnets: { subnetType: ec2.SubnetType.PUBLIC },
      instanceType: ec2.InstanceType.of(ec2.InstanceClass.T3, ec2.InstanceSize.LARGE),
      machineImage: ec2.MachineImage.latestAmazonLinux2023(),
      securityGroup: ec2SecurityGroup,
      role: ec2Role,
      userData,
      keyPair,
      blockDevices: [
        {
          deviceName: '/dev/xvda',
          volume: ec2.BlockDeviceVolume.ebs(50, {
            volumeType: ec2.EbsDeviceVolumeType.GP3,
            encrypted: true,
          }),
        },
      ],
      instanceName: `${projectName}-instance-${envName}`,
    });

    this.instanceId = this.instance.instanceId;

    // Add tags
    cdk.Tags.of(this.instance).add('Project', projectName);
    cdk.Tags.of(this.instance).add('Environment', envName);

    // Outputs
    new cdk.CfnOutput(this, 'InstanceId', {
      value: this.instance.instanceId,
      description: 'EC2 Instance ID',
      exportName: `${projectName}-instance-id-${envName}`,
    });

    new cdk.CfnOutput(this, 'InstancePublicIp', {
      value: this.instance.instancePublicIp,
      description: 'EC2 Public IP',
      exportName: `${projectName}-instance-ip-${envName}`,
    });

    new cdk.CfnOutput(this, 'InstancePrivateIp', {
      value: this.instance.instancePrivateIp,
      description: 'EC2 Private IP',
      exportName: `${projectName}-instance-private-ip-${envName}`,
    });

    new cdk.CfnOutput(this, 'KeyPairId', {
      value: keyPair.keyPairId,
      description: 'Key Pair ID (retrieve private key from SSM Parameter Store)',
      exportName: `${projectName}-keypair-id-${envName}`,
    });

    new cdk.CfnOutput(this, 'SSHCommand', {
      value: `ssh -i ${keyPair.keyPairName}.pem ec2-user@<PUBLIC_IP>`,
      description: 'SSH command template',
    });
  }
}

```

---

## aws/cdk/lib/ecr-stack.ts

```ts
import * as cdk from 'aws-cdk-lib';
import * as ecr from 'aws-cdk-lib/aws-ecr';
import { Construct } from 'constructs';

export interface EcrStackProps extends cdk.StackProps {
  envName: string;
  projectName: string;
  tags?: { [key: string]: string };
}

// List of all services that need ECR repositories
const SERVICES = [
  'frontend',
  'api-gateway',
  'collector-service',
  'autonomous-crawler',
  'browser-use-api',
  'admin-dashboard',
  'ai-agent-worker',
  'web-crawler',
  'embedding-server',
  'ip-rotation',
  'crawl-worker',
  'maigret-worker',
  'bot-detector',
  // ML Addons
  'sentiment-addon',
  'factcheck-addon',
  'bias-addon',
  // MCP Servers
  'newsinsight-mcp',
  'bias-mcp',
  'factcheck-mcp',
  'topic-mcp',
  'aiagent-mcp',
  'roboflow-mcp',
  'huggingface-mcp',
  'kaggle-mcp',
  'mltraining-mcp',
];

export class NewsInsightEcrStack extends cdk.Stack {
  public readonly repositories: { [key: string]: ecr.Repository } = {};

  constructor(scope: Construct, id: string, props: EcrStackProps) {
    super(scope, id, props);

    const { envName, projectName } = props;

    // Create ECR repository for each service
    SERVICES.forEach((serviceName) => {
      const repoName = `${projectName}/${serviceName}`;
      const repo = new ecr.Repository(this, `Repo-${serviceName}`, {
        repositoryName: repoName,
        removalPolicy: envName === 'prod' ? cdk.RemovalPolicy.RETAIN : cdk.RemovalPolicy.DESTROY,
        imageScanOnPush: true,
        imageTagMutability: ecr.TagMutability.MUTABLE,
        lifecycleRules: [
          {
            description: 'Keep last 10 images',
            maxImageCount: 10,
            rulePriority: 1,
            tagStatus: ecr.TagStatus.ANY,
          },
        ],
      });

      this.repositories[serviceName] = repo;

      // Output repository URI
      new cdk.CfnOutput(this, `RepoUri-${serviceName}`, {
        value: repo.repositoryUri,
        description: `ECR URI for ${serviceName}`,
        exportName: `${projectName}-ecr-${serviceName}-${envName}`,
      });
    });
  }
}

```

---

## aws/cdk/lib/ecs-stack.ts

```ts
import * as cdk from 'aws-cdk-lib';
import * as ec2 from 'aws-cdk-lib/aws-ec2';
import * as ecs from 'aws-cdk-lib/aws-ecs';
import * as ecr from 'aws-cdk-lib/aws-ecr';
import * as elbv2 from 'aws-cdk-lib/aws-elasticloadbalancingv2';
import * as logs from 'aws-cdk-lib/aws-logs';
import * as iam from 'aws-cdk-lib/aws-iam';
import * as secretsmanager from 'aws-cdk-lib/aws-secretsmanager';
import * as servicediscovery from 'aws-cdk-lib/aws-servicediscovery';
import { Construct } from 'constructs';

export interface EcsStackProps extends cdk.StackProps {
  envName: string;
  projectName: string;
  vpc: ec2.Vpc;
  repositories: { [key: string]: ecr.Repository };
  databaseSecrets: {
    postgres: secretsmanager.Secret;
    mongo: secretsmanager.Secret;
    redis: secretsmanager.Secret;
  };
  postgresEndpoint: string;
  mongoEndpoint: string;
  redisEndpoint: string;
  alb: elbv2.ApplicationLoadBalancer;
  // New: Accept Target Groups from ALB Stack (replaces httpsListener/httpListener)
  targetGroups: { [key: string]: elbv2.ApplicationTargetGroup };
  tags?: { [key: string]: string };
}

// Service configurations
interface ServiceConfig {
  name: string;
  port: number;
  cpu: number;
  memory: number;
  desiredCount: number;
  healthCheckPath: string;
  environment?: { [key: string]: string };
  pathPattern?: string; // Kept for reference
  priority?: number;    // Kept for reference
  dependsOn?: string[];
}

export class NewsInsightEcsStack extends cdk.Stack {
  private cluster: ecs.Cluster;
  private namespace: servicediscovery.PrivateDnsNamespace;
  private services: { [key: string]: ecs.FargateService } = {};

  constructor(scope: Construct, id: string, props: EcsStackProps) {
    super(scope, id, props);

    const { envName, projectName, vpc, repositories, databaseSecrets, 
            postgresEndpoint, mongoEndpoint, redisEndpoint,
            alb, targetGroups } = props;

    const isProd = envName === 'prod';

    // ECS Cluster
    this.cluster = new ecs.Cluster(this, 'Cluster', {
      vpc,
      clusterName: `${projectName}-cluster-${envName}`,
      containerInsights: isProd,
    });

    // Cloud Map namespace for service discovery
    this.namespace = new servicediscovery.PrivateDnsNamespace(this, 'Namespace', {
      name: `${projectName}.local`,
      vpc,
      description: 'NewsInsight internal service discovery',
    });

    // Security group for services
    const serviceSecurityGroup = new ec2.SecurityGroup(this, 'ServiceSg', {
      vpc,
      description: 'Security group for ECS services',
      securityGroupName: `${projectName}-service-sg-${envName}`,
    });

    // Allow internal communication
    serviceSecurityGroup.addIngressRule(
      ec2.Peer.ipv4(vpc.vpcCidrBlock),
      ec2.Port.allTcp(),
      'Internal VPC traffic'
    );

    // Allow from ALB
    serviceSecurityGroup.addIngressRule(
      ec2.Peer.securityGroupId(alb.connections.securityGroups[0].securityGroupId),
      ec2.Port.allTcp(),
      'Traffic from ALB'
    );

    // Task execution role
    const executionRole = new iam.Role(this, 'ExecutionRole', {
      assumedBy: new iam.ServicePrincipal('ecs-tasks.amazonaws.com'),
      managedPolicies: [
        iam.ManagedPolicy.fromAwsManagedPolicyName('service-role/AmazonECSTaskExecutionRolePolicy'),
      ],
    });

    // Grant access to secrets
    databaseSecrets.postgres.grantRead(executionRole);
    databaseSecrets.mongo.grantRead(executionRole);
    databaseSecrets.redis.grantRead(executionRole);

    // Task role
    const taskRole = new iam.Role(this, 'TaskRole', {
      assumedBy: new iam.ServicePrincipal('ecs-tasks.amazonaws.com'),
    });

    // Add necessary permissions
    taskRole.addManagedPolicy(
      iam.ManagedPolicy.fromAwsManagedPolicyName('AmazonS3ReadOnlyAccess')
    );

    // Service configurations
    const serviceConfigs: ServiceConfig[] = [
      // Frontend
      {
        name: 'frontend',
        port: 8080,
        cpu: 256,
        memory: 512,
        desiredCount: isProd ? 2 : 1,
        healthCheckPath: '/health',
        pathPattern: '/*',
        priority: 100,
      },
      // API Gateway
      {
        name: 'api-gateway',
        port: 8000,
        cpu: 512,
        memory: 1024,
        desiredCount: isProd ? 2 : 1,
        healthCheckPath: '/actuator/health',
        pathPattern: '/api/*',
        priority: 10,
        environment: {
          SPRING_PROFILES_ACTIVE: envName,
          CONSUL_ENABLED: 'false',
        },
      },
      // Collector Service
      {
        name: 'collector-service',
        port: 8081,
        cpu: 512,
        memory: 1024,
        desiredCount: isProd ? 2 : 1,
        healthCheckPath: '/actuator/health',
      },
      // Admin Dashboard
      {
        name: 'admin-dashboard',
        port: 8888,
        cpu: 256,
        memory: 512,
        desiredCount: 1,
        healthCheckPath: '/health',
        pathPattern: '/admin/*',
        priority: 20,
      },
      // Autonomous Crawler
      {
        name: 'autonomous-crawler',
        port: 8030,
        cpu: 1024,
        memory: 2048,
        desiredCount: isProd ? 2 : 1,
        healthCheckPath: '/health',
      },
      // Browser Use API
      {
        name: 'browser-use-api',
        port: 8500,
        cpu: 1024,
        memory: 2048,
        desiredCount: isProd ? 2 : 1,
        healthCheckPath: '/health',
      },
      // Bot Detector
      {
        name: 'bot-detector',
        port: 8041,
        cpu: 512,
        memory: 1024,
        desiredCount: 1,
        healthCheckPath: '/health',
      },
      // Embedding Server
      {
        name: 'embedding-server',
        port: 8011,
        cpu: 2048,
        memory: 4096,
        desiredCount: 1,
        healthCheckPath: '/health',
      },
      // ML Addons
      {
        name: 'sentiment-addon',
        port: 8100,
        cpu: 512,
        memory: 1024,
        desiredCount: 1,
        healthCheckPath: '/health',
      },
      {
        name: 'factcheck-addon',
        port: 8101,
        cpu: 512,
        memory: 1024,
        desiredCount: 1,
        healthCheckPath: '/health',
      },
      {
        name: 'bias-addon',
        port: 8102,
        cpu: 512,
        memory: 1024,
        desiredCount: 1,
        healthCheckPath: '/health',
      },
      // MCP Servers
      {
        name: 'newsinsight-mcp',
        port: 5000,
        cpu: 256,
        memory: 512,
        desiredCount: 1,
        healthCheckPath: '/health',
      },
      {
        name: 'bias-mcp',
        port: 5001,
        cpu: 256,
        memory: 512,
        desiredCount: 1,
        healthCheckPath: '/health',
      },
      {
        name: 'factcheck-mcp',
        port: 5002,
        cpu: 256,
        memory: 512,
        desiredCount: 1,
        healthCheckPath: '/health',
      },
      {
        name: 'topic-mcp',
        port: 5003,
        cpu: 256,
        memory: 512,
        desiredCount: 1,
        healthCheckPath: '/health',
      },
      {
        name: 'aiagent-mcp',
        port: 5004,
        cpu: 256,
        memory: 512,
        desiredCount: 1,
        healthCheckPath: '/health',
      },
    ];

    // Create services
    for (const config of serviceConfigs) {
      const service = this.createService(
        config,
        {
          projectName,
          envName,
          isProd,
          repositories,
          databaseSecrets,
          postgresEndpoint,
          mongoEndpoint,
          redisEndpoint,
          serviceSecurityGroup,
          executionRole,
          taskRole,
          targetGroups, // Pass TGs from ALB stack
        }
      );
      this.services[config.name] = service;
    }

    // Outputs
    new cdk.CfnOutput(this, 'ClusterArn', {
      value: this.cluster.clusterArn,
      description: 'ECS Cluster ARN',
      exportName: `${projectName}-cluster-arn-${envName}`,
    });

    new cdk.CfnOutput(this, 'NamespaceArn', {
      value: this.namespace.namespaceArn,
      description: 'Service Discovery Namespace ARN',
      exportName: `${projectName}-namespace-arn-${envName}`,
    });
  }

  private createService(
    config: ServiceConfig,
    context: {
      projectName: string;
      envName: string;
      isProd: boolean;
      repositories: { [key: string]: ecr.Repository };
      databaseSecrets: {
        postgres: secretsmanager.Secret;
        mongo: secretsmanager.Secret;
        redis: secretsmanager.Secret;
      };
      postgresEndpoint: string;
      mongoEndpoint: string;
      redisEndpoint: string;
      serviceSecurityGroup: ec2.SecurityGroup;
      executionRole: iam.Role;
      taskRole: iam.Role;
      targetGroups: { [key: string]: elbv2.ApplicationTargetGroup };
    }
  ): ecs.FargateService {
    const { projectName, envName, isProd, repositories, databaseSecrets,
            postgresEndpoint, mongoEndpoint, redisEndpoint,
            serviceSecurityGroup, executionRole, taskRole, targetGroups } = context;

    // Log group
    const logGroup = new logs.LogGroup(this, `LogGroup-${config.name}`, {
      logGroupName: `/ecs/${projectName}/${envName}/${config.name}`,
      retention: isProd ? logs.RetentionDays.ONE_MONTH : logs.RetentionDays.ONE_WEEK,
      removalPolicy: isProd ? cdk.RemovalPolicy.RETAIN : cdk.RemovalPolicy.DESTROY,
    });

    // Task definition
    const taskDef = new ecs.FargateTaskDefinition(this, `TaskDef-${config.name}`, {
      family: `${projectName}-${config.name}-${envName}`,
      cpu: config.cpu,
      memoryLimitMiB: config.memory,
      executionRole,
      taskRole,
    });

    // Container
    const repo = repositories[config.name];
    taskDef.addContainer(`Container-${config.name}`, {
      containerName: config.name,
      image: repo 
        ? ecs.ContainerImage.fromEcrRepository(repo, 'latest')
        : ecs.ContainerImage.fromRegistry(`newsinsight/${config.name}:latest`),
      logging: ecs.LogDrivers.awsLogs({
        logGroup,
        streamPrefix: config.name,
      }),
      environment: {
        // Common environment variables
        ENVIRONMENT: envName,
        NODE_ENV: envName === 'prod' ? 'production' : 'development',
        // Database endpoints
        POSTGRES_HOST: postgresEndpoint,
        POSTGRES_PORT: '5432',
        MONGO_HOST: mongoEndpoint,
        MONGO_PORT: '27017',
        REDIS_HOST: redisEndpoint,
        REDIS_PORT: '6379',
        // Service discovery URLs
        API_GATEWAY_URL: `http://api-gateway.${projectName}.local:8000`,
        COLLECTOR_SERVICE_URL: `http://collector-service.${projectName}.local:8081`,
        BROWSER_USE_API_URL: `http://browser-use-api.${projectName}.local:8500`,
        AUTONOMOUS_CRAWLER_URL: `http://autonomous-crawler.${projectName}.local:8030`,
        ADMIN_DASHBOARD_URL: `http://admin-dashboard.${projectName}.local:8888`,
        // MCP URLs
        NEWSINSIGHT_MCP_URL: `http://newsinsight-mcp.${projectName}.local:5000`,
        BIAS_MCP_URL: `http://bias-mcp.${projectName}.local:5001`,
        FACTCHECK_MCP_URL: `http://factcheck-mcp.${projectName}.local:5002`,
        TOPIC_MCP_URL: `http://topic-mcp.${projectName}.local:5003`,
        AIAGENT_MCP_URL: `http://aiagent-mcp.${projectName}.local:5004`,
        // Custom environment
        ...config.environment,
      },
      secrets: {
        // Database credentials from Secrets Manager
        POSTGRES_PASSWORD: ecs.Secret.fromSecretsManager(databaseSecrets.postgres, 'password'),
        POSTGRES_USER: ecs.Secret.fromSecretsManager(databaseSecrets.postgres, 'username'),
        MONGO_PASSWORD: ecs.Secret.fromSecretsManager(databaseSecrets.mongo, 'password'),
        MONGO_USER: ecs.Secret.fromSecretsManager(databaseSecrets.mongo, 'username'),
        REDIS_PASSWORD: ecs.Secret.fromSecretsManager(databaseSecrets.redis),
      },
      portMappings: [
        {
          containerPort: config.port,
          protocol: ecs.Protocol.TCP,
        },
      ],
    });

    // Service
    const service = new ecs.FargateService(this, `Service-${config.name}`, {
      cluster: this.cluster,
      taskDefinition: taskDef,
      serviceName: `${projectName}-${config.name}-${envName}`,
      desiredCount: config.desiredCount,
      assignPublicIp: false,
      vpcSubnets: { subnetType: ec2.SubnetType.PRIVATE_WITH_EGRESS },
      securityGroups: [serviceSecurityGroup],
      cloudMapOptions: {
        name: config.name,
        cloudMapNamespace: this.namespace,
        dnsRecordType: servicediscovery.DnsRecordType.A,
      },
      circuitBreaker: { rollback: true },
      minHealthyPercent: isProd ? 100 : 50,
      maxHealthyPercent: 200,
    });

    // Auto-scaling
    if (isProd) {
      const scaling = service.autoScaleTaskCount({
        minCapacity: config.desiredCount,
        maxCapacity: config.desiredCount * 3,
      });

      scaling.scaleOnCpuUtilization(`CpuScaling-${config.name}`, {
        targetUtilizationPercent: 70,
        scaleInCooldown: cdk.Duration.seconds(60),
        scaleOutCooldown: cdk.Duration.seconds(60),
      });

      scaling.scaleOnMemoryUtilization(`MemoryScaling-${config.name}`, {
        targetUtilizationPercent: 80,
        scaleInCooldown: cdk.Duration.seconds(60),
        scaleOutCooldown: cdk.Duration.seconds(60),
      });
    }

    // Attach to ALB target group if one exists for this service
    // Target groups are now created in ALB stack to avoid circular dependency
    if (targetGroups[config.name]) {
      service.attachToApplicationTargetGroup(targetGroups[config.name]);
    }

    return service;
  }
}

```

---

## aws/cdk/lib/vpc-stack.ts

```ts
import * as cdk from 'aws-cdk-lib';
import * as ec2 from 'aws-cdk-lib/aws-ec2';
import { Construct } from 'constructs';

export interface VpcStackProps extends cdk.StackProps {
  envName: string;
  projectName: string;
  tags?: { [key: string]: string };
}

export class NewsInsightVpcStack extends cdk.Stack {
  public readonly vpc: ec2.Vpc;

  constructor(scope: Construct, id: string, props: VpcStackProps) {
    super(scope, id, props);

    const { envName, projectName } = props;

    // Create VPC with public and private subnets across 2 AZs
    this.vpc = new ec2.Vpc(this, 'NewsInsightVpc', {
      vpcName: `${projectName}-vpc-${envName}`,
      maxAzs: 2,
      ipAddresses: ec2.IpAddresses.cidr('10.0.0.0/16'),
      natGateways: envName === 'prod' ? 2 : 1,
      subnetConfiguration: [
        {
          name: 'Public',
          subnetType: ec2.SubnetType.PUBLIC,
          cidrMask: 24,
        },
        {
          name: 'Private',
          subnetType: ec2.SubnetType.PRIVATE_WITH_EGRESS,
          cidrMask: 24,
        },
        {
          name: 'Database',
          subnetType: ec2.SubnetType.PRIVATE_ISOLATED,
          cidrMask: 24,
        },
      ],
    });

    // VPC Endpoints for AWS services (reduces NAT Gateway costs)
    this.vpc.addGatewayEndpoint('S3Endpoint', {
      service: ec2.GatewayVpcEndpointAwsService.S3,
    });

    this.vpc.addInterfaceEndpoint('EcrEndpoint', {
      service: ec2.InterfaceVpcEndpointAwsService.ECR,
    });

    this.vpc.addInterfaceEndpoint('EcrDockerEndpoint', {
      service: ec2.InterfaceVpcEndpointAwsService.ECR_DOCKER,
    });

    this.vpc.addInterfaceEndpoint('CloudWatchLogsEndpoint', {
      service: ec2.InterfaceVpcEndpointAwsService.CLOUDWATCH_LOGS,
    });

    this.vpc.addInterfaceEndpoint('SecretsManagerEndpoint', {
      service: ec2.InterfaceVpcEndpointAwsService.SECRETS_MANAGER,
    });

    // Output VPC ID
    new cdk.CfnOutput(this, 'VpcId', {
      value: this.vpc.vpcId,
      description: 'VPC ID',
      exportName: `${projectName}-vpc-id-${envName}`,
    });

    // Output public subnet IDs
    new cdk.CfnOutput(this, 'PublicSubnetIds', {
      value: this.vpc.publicSubnets.map(s => s.subnetId).join(','),
      description: 'Public Subnet IDs',
      exportName: `${projectName}-public-subnets-${envName}`,
    });

    // Output private subnet IDs
    new cdk.CfnOutput(this, 'PrivateSubnetIds', {
      value: this.vpc.privateSubnets.map(s => s.subnetId).join(','),
      description: 'Private Subnet IDs',
      exportName: `${projectName}-private-subnets-${envName}`,
    });
  }
}

```

---

## aws/cdk/package-lock.json

```json
{
  "name": "newsinsight-aws-cdk",
  "version": "1.0.0",
  "lockfileVersion": 3,
  "requires": true,
  "packages": {
    "": {
      "name": "newsinsight-aws-cdk",
      "version": "1.0.0",
      "dependencies": {
        "aws-cdk-lib": "2.173.0",
        "constructs": "^10.0.0",
        "source-map-support": "^0.5.21"
      },
      "bin": {
        "newsinsight": "bin/newsinsight.js"
      },
      "devDependencies": {
        "@types/jest": "^29.5.12",
        "@types/node": "20.14.9",
        "aws-cdk": "2.173.0",
        "jest": "^29.7.0",
        "ts-jest": "^29.1.5",
        "ts-node": "^10.9.2",
        "typescript": "~5.5.3"
      }
    },
    "node_modules/@aws-cdk/asset-awscli-v1": {
      "version": "2.2.258",
      "resolved": "https://registry.npmjs.org/@aws-cdk/asset-awscli-v1/-/asset-awscli-v1-2.2.258.tgz",
      "integrity": "sha512-TL3I9cIue0bAsuwrmjgjAQaEH6JL09y49FVQMDhrz4jJ2iPKuHtdrYd7ydm02t1YZdPZE2M0VNj6VD4fGIFpvw==",
      "license": "Apache-2.0"
    },
    "node_modules/@aws-cdk/asset-kubectl-v20": {
      "version": "2.1.4",
      "resolved": "https://registry.npmjs.org/@aws-cdk/asset-kubectl-v20/-/asset-kubectl-v20-2.1.4.tgz",
      "integrity": "sha512-Ps2MkmjYgMyflagqQ4dgTElc7Vwpqj8spw8dQVFiSeaaMPsuDSNsPax3/HjuDuwqsmLdaCZc6umlxYLpL0kYDA==",
      "license": "Apache-2.0"
    },
    "node_modules/@aws-cdk/asset-node-proxy-agent-v6": {
      "version": "2.1.0",
      "resolved": "https://registry.npmjs.org/@aws-cdk/asset-node-proxy-agent-v6/-/asset-node-proxy-agent-v6-2.1.0.tgz",
      "integrity": "sha512-7bY3J8GCVxLupn/kNmpPc5VJz8grx+4RKfnnJiO1LG+uxkZfANZG3RMHhE+qQxxwkyQ9/MfPtTpf748UhR425A==",
      "license": "Apache-2.0"
    },
    "node_modules/@aws-cdk/cloud-assembly-schema": {
      "version": "38.0.1",
      "resolved": "https://registry.npmjs.org/@aws-cdk/cloud-assembly-schema/-/cloud-assembly-schema-38.0.1.tgz",
      "integrity": "sha512-KvPe+NMWAulfNVwY7jenFhzhuLhLqJ/OPy5jx7wUstbjnYnjRVLpUHPU3yCjXFE0J8cuJVdx95BJ4rOs66Pi9w==",
      "bundleDependencies": [
        "jsonschema",
        "semver"
      ],
      "license": "Apache-2.0",
      "dependencies": {
        "jsonschema": "^1.4.1",
        "semver": "^7.6.3"
      }
    },
    "node_modules/@aws-cdk/cloud-assembly-schema/node_modules/jsonschema": {
      "version": "1.4.1",
      "inBundle": true,
      "license": "MIT",
      "engines": {
        "node": "*"
      }
    },
    "node_modules/@aws-cdk/cloud-assembly-schema/node_modules/semver": {
      "version": "7.6.3",
      "inBundle": true,
      "license": "ISC",
      "bin": {
        "semver": "bin/semver.js"
      },
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/@babel/code-frame": {
      "version": "7.27.1",
      "resolved": "https://registry.npmjs.org/@babel/code-frame/-/code-frame-7.27.1.tgz",
      "integrity": "sha512-cjQ7ZlQ0Mv3b47hABuTevyTuYN4i+loJKGeV9flcCgIK37cCXRh+L1bd3iBHlynerhQ7BhCkn2BPbQUL+rGqFg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-validator-identifier": "^7.27.1",
        "js-tokens": "^4.0.0",
        "picocolors": "^1.1.1"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/compat-data": {
      "version": "7.28.5",
      "resolved": "https://registry.npmjs.org/@babel/compat-data/-/compat-data-7.28.5.tgz",
      "integrity": "sha512-6uFXyCayocRbqhZOB+6XcuZbkMNimwfVGFji8CTZnCzOHVGvDqzvitu1re2AU5LROliz7eQPhB8CpAMvnx9EjA==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/core": {
      "version": "7.28.5",
      "resolved": "https://registry.npmjs.org/@babel/core/-/core-7.28.5.tgz",
      "integrity": "sha512-e7jT4DxYvIDLk1ZHmU/m/mB19rex9sv0c2ftBtjSBv+kVM/902eh0fINUzD7UwLLNR+jU585GxUJ8/EBfAM5fw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/code-frame": "^7.27.1",
        "@babel/generator": "^7.28.5",
        "@babel/helper-compilation-targets": "^7.27.2",
        "@babel/helper-module-transforms": "^7.28.3",
        "@babel/helpers": "^7.28.4",
        "@babel/parser": "^7.28.5",
        "@babel/template": "^7.27.2",
        "@babel/traverse": "^7.28.5",
        "@babel/types": "^7.28.5",
        "@jridgewell/remapping": "^2.3.5",
        "convert-source-map": "^2.0.0",
        "debug": "^4.1.0",
        "gensync": "^1.0.0-beta.2",
        "json5": "^2.2.3",
        "semver": "^6.3.1"
      },
      "engines": {
        "node": ">=6.9.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/babel"
      }
    },
    "node_modules/@babel/generator": {
      "version": "7.28.5",
      "resolved": "https://registry.npmjs.org/@babel/generator/-/generator-7.28.5.tgz",
      "integrity": "sha512-3EwLFhZ38J4VyIP6WNtt2kUdW9dokXA9Cr4IVIFHuCpZ3H8/YFOl5JjZHisrn1fATPBmKKqXzDFvh9fUwHz6CQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/parser": "^7.28.5",
        "@babel/types": "^7.28.5",
        "@jridgewell/gen-mapping": "^0.3.12",
        "@jridgewell/trace-mapping": "^0.3.28",
        "jsesc": "^3.0.2"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helper-compilation-targets": {
      "version": "7.27.2",
      "resolved": "https://registry.npmjs.org/@babel/helper-compilation-targets/-/helper-compilation-targets-7.27.2.tgz",
      "integrity": "sha512-2+1thGUUWWjLTYTHZWK1n8Yga0ijBz1XAhUXcKy81rd5g6yh7hGqMp45v7cadSbEHc9G3OTv45SyneRN3ps4DQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/compat-data": "^7.27.2",
        "@babel/helper-validator-option": "^7.27.1",
        "browserslist": "^4.24.0",
        "lru-cache": "^5.1.1",
        "semver": "^6.3.1"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helper-globals": {
      "version": "7.28.0",
      "resolved": "https://registry.npmjs.org/@babel/helper-globals/-/helper-globals-7.28.0.tgz",
      "integrity": "sha512-+W6cISkXFa1jXsDEdYA8HeevQT/FULhxzR99pxphltZcVaugps53THCeiWA8SguxxpSp3gKPiuYfSWopkLQ4hw==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helper-module-imports": {
      "version": "7.27.1",
      "resolved": "https://registry.npmjs.org/@babel/helper-module-imports/-/helper-module-imports-7.27.1.tgz",
      "integrity": "sha512-0gSFWUPNXNopqtIPQvlD5WgXYI5GY2kP2cCvoT8kczjbfcfuIljTbcWrulD1CIPIX2gt1wghbDy08yE1p+/r3w==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/traverse": "^7.27.1",
        "@babel/types": "^7.27.1"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helper-module-transforms": {
      "version": "7.28.3",
      "resolved": "https://registry.npmjs.org/@babel/helper-module-transforms/-/helper-module-transforms-7.28.3.tgz",
      "integrity": "sha512-gytXUbs8k2sXS9PnQptz5o0QnpLL51SwASIORY6XaBKF88nsOT0Zw9szLqlSGQDP/4TljBAD5y98p2U1fqkdsw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-module-imports": "^7.27.1",
        "@babel/helper-validator-identifier": "^7.27.1",
        "@babel/traverse": "^7.28.3"
      },
      "engines": {
        "node": ">=6.9.0"
      },
      "peerDependencies": {
        "@babel/core": "^7.0.0"
      }
    },
    "node_modules/@babel/helper-plugin-utils": {
      "version": "7.27.1",
      "resolved": "https://registry.npmjs.org/@babel/helper-plugin-utils/-/helper-plugin-utils-7.27.1.tgz",
      "integrity": "sha512-1gn1Up5YXka3YYAHGKpbideQ5Yjf1tDa9qYcgysz+cNCXukyLl6DjPXhD3VRwSb8c0J9tA4b2+rHEZtc6R0tlw==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helper-string-parser": {
      "version": "7.27.1",
      "resolved": "https://registry.npmjs.org/@babel/helper-string-parser/-/helper-string-parser-7.27.1.tgz",
      "integrity": "sha512-qMlSxKbpRlAridDExk92nSobyDdpPijUq2DW6oDnUqd0iOGxmQjyqhMIihI9+zv4LPyZdRje2cavWPbCbWm3eA==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helper-validator-identifier": {
      "version": "7.28.5",
      "resolved": "https://registry.npmjs.org/@babel/helper-validator-identifier/-/helper-validator-identifier-7.28.5.tgz",
      "integrity": "sha512-qSs4ifwzKJSV39ucNjsvc6WVHs6b7S03sOh2OcHF9UHfVPqWWALUsNUVzhSBiItjRZoLHx7nIarVjqKVusUZ1Q==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helper-validator-option": {
      "version": "7.27.1",
      "resolved": "https://registry.npmjs.org/@babel/helper-validator-option/-/helper-validator-option-7.27.1.tgz",
      "integrity": "sha512-YvjJow9FxbhFFKDSuFnVCe2WxXk1zWc22fFePVNEaWJEu8IrZVlda6N0uHwzZrUM1il7NC9Mlp4MaJYbYd9JSg==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helpers": {
      "version": "7.28.4",
      "resolved": "https://registry.npmjs.org/@babel/helpers/-/helpers-7.28.4.tgz",
      "integrity": "sha512-HFN59MmQXGHVyYadKLVumYsA9dBFun/ldYxipEjzA4196jpLZd8UjEEBLkbEkvfYreDqJhZxYAWFPtrfhNpj4w==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/template": "^7.27.2",
        "@babel/types": "^7.28.4"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/parser": {
      "version": "7.28.5",
      "resolved": "https://registry.npmjs.org/@babel/parser/-/parser-7.28.5.tgz",
      "integrity": "sha512-KKBU1VGYR7ORr3At5HAtUQ+TV3SzRCXmA/8OdDZiLDBIZxVyzXuztPjfLd3BV1PRAQGCMWWSHYhL0F8d5uHBDQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/types": "^7.28.5"
      },
      "bin": {
        "parser": "bin/babel-parser.js"
      },
      "engines": {
        "node": ">=6.0.0"
      }
    },
    "node_modules/@babel/plugin-syntax-async-generators": {
      "version": "7.8.4",
      "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-async-generators/-/plugin-syntax-async-generators-7.8.4.tgz",
      "integrity": "sha512-tycmZxkGfZaxhMRbXlPXuVFpdWlXpir2W4AMhSJgRKzk/eDlIXOhb2LHWoLpDF7TEHylV5zNhykX6KAgHJmTNw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-plugin-utils": "^7.8.0"
      },
      "peerDependencies": {
        "@babel/core": "^7.0.0-0"
      }
    },
    "node_modules/@babel/plugin-syntax-bigint": {
      "version": "7.8.3",
      "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-bigint/-/plugin-syntax-bigint-7.8.3.tgz",
      "integrity": "sha512-wnTnFlG+YxQm3vDxpGE57Pj0srRU4sHE/mDkt1qv2YJJSeUAec2ma4WLUnUPeKjyrfntVwe/N6dCXpU+zL3Npg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-plugin-utils": "^7.8.0"
      },
      "peerDependencies": {
        "@babel/core": "^7.0.0-0"
      }
    },
    "node_modules/@babel/plugin-syntax-class-properties": {
      "version": "7.12.13",
      "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-class-properties/-/plugin-syntax-class-properties-7.12.13.tgz",
      "integrity": "sha512-fm4idjKla0YahUNgFNLCB0qySdsoPiZP3iQE3rky0mBUtMZ23yDJ9SJdg6dXTSDnulOVqiF3Hgr9nbXvXTQZYA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-plugin-utils": "^7.12.13"
      },
      "peerDependencies": {
        "@babel/core": "^7.0.0-0"
      }
    },
    "node_modules/@babel/plugin-syntax-class-static-block": {
      "version": "7.14.5",
      "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-class-static-block/-/plugin-syntax-class-static-block-7.14.5.tgz",
      "integrity": "sha512-b+YyPmr6ldyNnM6sqYeMWE+bgJcJpO6yS4QD7ymxgH34GBPNDM/THBh8iunyvKIZztiwLH4CJZ0RxTk9emgpjw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-plugin-utils": "^7.14.5"
      },
      "engines": {
        "node": ">=6.9.0"
      },
      "peerDependencies": {
        "@babel/core": "^7.0.0-0"
      }
    },
    "node_modules/@babel/plugin-syntax-import-attributes": {
      "version": "7.27.1",
      "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-import-attributes/-/plugin-syntax-import-attributes-7.27.1.tgz",
      "integrity": "sha512-oFT0FrKHgF53f4vOsZGi2Hh3I35PfSmVs4IBFLFj4dnafP+hIWDLg3VyKmUHfLoLHlyxY4C7DGtmHuJgn+IGww==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-plugin-utils": "^7.27.1"
      },
      "engines": {
        "node": ">=6.9.0"
      },
      "peerDependencies": {
        "@babel/core": "^7.0.0-0"
      }
    },
    "node_modules/@babel/plugin-syntax-import-meta": {
      "version": "7.10.4",
      "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-import-meta/-/plugin-syntax-import-meta-7.10.4.tgz",
      "integrity": "sha512-Yqfm+XDx0+Prh3VSeEQCPU81yC+JWZ2pDPFSS4ZdpfZhp4MkFMaDC1UqseovEKwSUpnIL7+vK+Clp7bfh0iD7g==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-plugin-utils": "^7.10.4"
      },
      "peerDependencies": {
        "@babel/core": "^7.0.0-0"
      }
    },
    "node_modules/@babel/plugin-syntax-json-strings": {
      "version": "7.8.3",
      "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-json-strings/-/plugin-syntax-json-strings-7.8.3.tgz",
      "integrity": "sha512-lY6kdGpWHvjoe2vk4WrAapEuBR69EMxZl+RoGRhrFGNYVK8mOPAW8VfbT/ZgrFbXlDNiiaxQnAtgVCZ6jv30EA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-plugin-utils": "^7.8.0"
      },
      "peerDependencies": {
        "@babel/core": "^7.0.0-0"
      }
    },
    "node_modules/@babel/plugin-syntax-jsx": {
      "version": "7.27.1",
      "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-jsx/-/plugin-syntax-jsx-7.27.1.tgz",
      "integrity": "sha512-y8YTNIeKoyhGd9O0Jiyzyyqk8gdjnumGTQPsz0xOZOQ2RmkVJeZ1vmmfIvFEKqucBG6axJGBZDE/7iI5suUI/w==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-plugin-utils": "^7.27.1"
      },
      "engines": {
        "node": ">=6.9.0"
      },
      "peerDependencies": {
        "@babel/core": "^7.0.0-0"
      }
    },
    "node_modules/@babel/plugin-syntax-logical-assignment-operators": {
      "version": "7.10.4",
      "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-logical-assignment-operators/-/plugin-syntax-logical-assignment-operators-7.10.4.tgz",
      "integrity": "sha512-d8waShlpFDinQ5MtvGU9xDAOzKH47+FFoney2baFIoMr952hKOLp1HR7VszoZvOsV/4+RRszNY7D17ba0te0ig==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-plugin-utils": "^7.10.4"
      },
      "peerDependencies": {
        "@babel/core": "^7.0.0-0"
      }
    },
    "node_modules/@babel/plugin-syntax-nullish-coalescing-operator": {
      "version": "7.8.3",
      "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-nullish-coalescing-operator/-/plugin-syntax-nullish-coalescing-operator-7.8.3.tgz",
      "integrity": "sha512-aSff4zPII1u2QD7y+F8oDsz19ew4IGEJg9SVW+bqwpwtfFleiQDMdzA/R+UlWDzfnHFCxxleFT0PMIrR36XLNQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-plugin-utils": "^7.8.0"
      },
      "peerDependencies": {
        "@babel/core": "^7.0.0-0"
      }
    },
    "node_modules/@babel/plugin-syntax-numeric-separator": {
      "version": "7.10.4",
      "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-numeric-separator/-/plugin-syntax-numeric-separator-7.10.4.tgz",
      "integrity": "sha512-9H6YdfkcK/uOnY/K7/aA2xpzaAgkQn37yzWUMRK7OaPOqOpGS1+n0H5hxT9AUw9EsSjPW8SVyMJwYRtWs3X3ug==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-plugin-utils": "^7.10.4"
      },
      "peerDependencies": {
        "@babel/core": "^7.0.0-0"
      }
    },
    "node_modules/@babel/plugin-syntax-object-rest-spread": {
      "version": "7.8.3",
      "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-object-rest-spread/-/plugin-syntax-object-rest-spread-7.8.3.tgz",
      "integrity": "sha512-XoqMijGZb9y3y2XskN+P1wUGiVwWZ5JmoDRwx5+3GmEplNyVM2s2Dg8ILFQm8rWM48orGy5YpI5Bl8U1y7ydlA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-plugin-utils": "^7.8.0"
      },
      "peerDependencies": {
        "@babel/core": "^7.0.0-0"
      }
    },
    "node_modules/@babel/plugin-syntax-optional-catch-binding": {
      "version": "7.8.3",
      "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-optional-catch-binding/-/plugin-syntax-optional-catch-binding-7.8.3.tgz",
      "integrity": "sha512-6VPD0Pc1lpTqw0aKoeRTMiB+kWhAoT24PA+ksWSBrFtl5SIRVpZlwN3NNPQjehA2E/91FV3RjLWoVTglWcSV3Q==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-plugin-utils": "^7.8.0"
      },
      "peerDependencies": {
        "@babel/core": "^7.0.0-0"
      }
    },
    "node_modules/@babel/plugin-syntax-optional-chaining": {
      "version": "7.8.3",
      "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-optional-chaining/-/plugin-syntax-optional-chaining-7.8.3.tgz",
      "integrity": "sha512-KoK9ErH1MBlCPxV0VANkXW2/dw4vlbGDrFgz8bmUsBGYkFRcbRwMh6cIJubdPrkxRwuGdtCk0v/wPTKbQgBjkg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-plugin-utils": "^7.8.0"
      },
      "peerDependencies": {
        "@babel/core": "^7.0.0-0"
      }
    },
    "node_modules/@babel/plugin-syntax-private-property-in-object": {
      "version": "7.14.5",
      "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-private-property-in-object/-/plugin-syntax-private-property-in-object-7.14.5.tgz",
      "integrity": "sha512-0wVnp9dxJ72ZUJDV27ZfbSj6iHLoytYZmh3rFcxNnvsJF3ktkzLDZPy/mA17HGsaQT3/DQsWYX1f1QGWkCoVUg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-plugin-utils": "^7.14.5"
      },
      "engines": {
        "node": ">=6.9.0"
      },
      "peerDependencies": {
        "@babel/core": "^7.0.0-0"
      }
    },
    "node_modules/@babel/plugin-syntax-top-level-await": {
      "version": "7.14.5",
      "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-top-level-await/-/plugin-syntax-top-level-await-7.14.5.tgz",
      "integrity": "sha512-hx++upLv5U1rgYfwe1xBQUhRmU41NEvpUvrp8jkrSCdvGSnM5/qdRMtylJ6PG5OFkBaHkbTAKTnd3/YyESRHFw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-plugin-utils": "^7.14.5"
      },
      "engines": {
        "node": ">=6.9.0"
      },
      "peerDependencies": {
        "@babel/core": "^7.0.0-0"
      }
    },
    "node_modules/@babel/plugin-syntax-typescript": {
      "version": "7.27.1",
      "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-typescript/-/plugin-syntax-typescript-7.27.1.tgz",
      "integrity": "sha512-xfYCBMxveHrRMnAWl1ZlPXOZjzkN82THFvLhQhFXFt81Z5HnN+EtUkZhv/zcKpmT3fzmWZB0ywiBrbC3vogbwQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-plugin-utils": "^7.27.1"
      },
      "engines": {
        "node": ">=6.9.0"
      },
      "peerDependencies": {
        "@babel/core": "^7.0.0-0"
      }
    },
    "node_modules/@babel/template": {
      "version": "7.27.2",
      "resolved": "https://registry.npmjs.org/@babel/template/-/template-7.27.2.tgz",
      "integrity": "sha512-LPDZ85aEJyYSd18/DkjNh4/y1ntkE5KwUHWTiqgRxruuZL2F1yuHligVHLvcHY2vMHXttKFpJn6LwfI7cw7ODw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/code-frame": "^7.27.1",
        "@babel/parser": "^7.27.2",
        "@babel/types": "^7.27.1"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/traverse": {
      "version": "7.28.5",
      "resolved": "https://registry.npmjs.org/@babel/traverse/-/traverse-7.28.5.tgz",
      "integrity": "sha512-TCCj4t55U90khlYkVV/0TfkJkAkUg3jZFA3Neb7unZT8CPok7iiRfaX0F+WnqWqt7OxhOn0uBKXCw4lbL8W0aQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/code-frame": "^7.27.1",
        "@babel/generator": "^7.28.5",
        "@babel/helper-globals": "^7.28.0",
        "@babel/parser": "^7.28.5",
        "@babel/template": "^7.27.2",
        "@babel/types": "^7.28.5",
        "debug": "^4.3.1"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/types": {
      "version": "7.28.5",
      "resolved": "https://registry.npmjs.org/@babel/types/-/types-7.28.5.tgz",
      "integrity": "sha512-qQ5m48eI/MFLQ5PxQj4PFaprjyCTLI37ElWMmNs0K8Lk3dVeOdNpB3ks8jc7yM5CDmVC73eMVk/trk3fgmrUpA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-string-parser": "^7.27.1",
        "@babel/helper-validator-identifier": "^7.28.5"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@bcoe/v8-coverage": {
      "version": "0.2.3",
      "resolved": "https://registry.npmjs.org/@bcoe/v8-coverage/-/v8-coverage-0.2.3.tgz",
      "integrity": "sha512-0hYQ8SB4Db5zvZB4axdMHGwEaQjkZzFjQiN9LVYvIFB2nSUHW9tYpxWriPrWDASIxiaXax83REcLxuSdnGPZtw==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@cspotcode/source-map-support": {
      "version": "0.8.1",
      "resolved": "https://registry.npmjs.org/@cspotcode/source-map-support/-/source-map-support-0.8.1.tgz",
      "integrity": "sha512-IchNf6dN4tHoMFIn/7OE8LWZ19Y6q/67Bmf6vnGREv8RSbBVb9LPJxEcnwrcwX6ixSvaiGoomAUvu4YSxXrVgw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jridgewell/trace-mapping": "0.3.9"
      },
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@cspotcode/source-map-support/node_modules/@jridgewell/trace-mapping": {
      "version": "0.3.9",
      "resolved": "https://registry.npmjs.org/@jridgewell/trace-mapping/-/trace-mapping-0.3.9.tgz",
      "integrity": "sha512-3Belt6tdc8bPgAtbcmdtNJlirVoTmEb5e2gC94PnkwEW9jI6CAHUeoG85tjWP5WquqfavoMtMwiG4P926ZKKuQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jridgewell/resolve-uri": "^3.0.3",
        "@jridgewell/sourcemap-codec": "^1.4.10"
      }
    },
    "node_modules/@istanbuljs/load-nyc-config": {
      "version": "1.1.0",
      "resolved": "https://registry.npmjs.org/@istanbuljs/load-nyc-config/-/load-nyc-config-1.1.0.tgz",
      "integrity": "sha512-VjeHSlIzpv/NyD3N0YuHfXOPDIixcA1q2ZV98wsMqcYlPmv2n3Yb2lYP9XMElnaFVXg5A7YLTeLu6V84uQDjmQ==",
      "dev": true,
      "license": "ISC",
      "dependencies": {
        "camelcase": "^5.3.1",
        "find-up": "^4.1.0",
        "get-package-type": "^0.1.0",
        "js-yaml": "^3.13.1",
        "resolve-from": "^5.0.0"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/@istanbuljs/schema": {
      "version": "0.1.3",
      "resolved": "https://registry.npmjs.org/@istanbuljs/schema/-/schema-0.1.3.tgz",
      "integrity": "sha512-ZXRY4jNvVgSVQ8DL3LTcakaAtXwTVUxE81hslsyD2AtoXW/wVob10HkOJ1X/pAlcI7D+2YoZKg5do8G/w6RYgA==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/@jest/console": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/@jest/console/-/console-29.7.0.tgz",
      "integrity": "sha512-5Ni4CU7XHQi32IJ398EEP4RrB8eV09sXP2ROqD4bksHrnTree52PsxvX8tpL8LvTZ3pFzXyPbNQReSN41CAhOg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jest/types": "^29.6.3",
        "@types/node": "*",
        "chalk": "^4.0.0",
        "jest-message-util": "^29.7.0",
        "jest-util": "^29.7.0",
        "slash": "^3.0.0"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/@jest/core": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/@jest/core/-/core-29.7.0.tgz",
      "integrity": "sha512-n7aeXWKMnGtDA48y8TLWJPJmLmmZ642Ceo78cYWEpiD7FzDgmNDV/GCVRorPABdXLJZ/9wzzgZAlHjXjxDHGsg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jest/console": "^29.7.0",
        "@jest/reporters": "^29.7.0",
        "@jest/test-result": "^29.7.0",
        "@jest/transform": "^29.7.0",
        "@jest/types": "^29.6.3",
        "@types/node": "*",
        "ansi-escapes": "^4.2.1",
        "chalk": "^4.0.0",
        "ci-info": "^3.2.0",
        "exit": "^0.1.2",
        "graceful-fs": "^4.2.9",
        "jest-changed-files": "^29.7.0",
        "jest-config": "^29.7.0",
        "jest-haste-map": "^29.7.0",
        "jest-message-util": "^29.7.0",
        "jest-regex-util": "^29.6.3",
        "jest-resolve": "^29.7.0",
        "jest-resolve-dependencies": "^29.7.0",
        "jest-runner": "^29.7.0",
        "jest-runtime": "^29.7.0",
        "jest-snapshot": "^29.7.0",
        "jest-util": "^29.7.0",
        "jest-validate": "^29.7.0",
        "jest-watcher": "^29.7.0",
        "micromatch": "^4.0.4",
        "pretty-format": "^29.7.0",
        "slash": "^3.0.0",
        "strip-ansi": "^6.0.0"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      },
      "peerDependencies": {
        "node-notifier": "^8.0.1 || ^9.0.0 || ^10.0.0"
      },
      "peerDependenciesMeta": {
        "node-notifier": {
          "optional": true
        }
      }
    },
    "node_modules/@jest/environment": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/@jest/environment/-/environment-29.7.0.tgz",
      "integrity": "sha512-aQIfHDq33ExsN4jP1NWGXhxgQ/wixs60gDiKO+XVMd8Mn0NWPWgc34ZQDTb2jKaUWQ7MuwoitXAsN2XVXNMpAw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jest/fake-timers": "^29.7.0",
        "@jest/types": "^29.6.3",
        "@types/node": "*",
        "jest-mock": "^29.7.0"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/@jest/expect": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/@jest/expect/-/expect-29.7.0.tgz",
      "integrity": "sha512-8uMeAMycttpva3P1lBHB8VciS9V0XAr3GymPpipdyQXbBcuhkLQOSe8E/p92RyAdToS6ZD1tFkX+CkhoECE0dQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "expect": "^29.7.0",
        "jest-snapshot": "^29.7.0"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/@jest/expect-utils": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/@jest/expect-utils/-/expect-utils-29.7.0.tgz",
      "integrity": "sha512-GlsNBWiFQFCVi9QVSx7f5AgMeLxe9YCCs5PuP2O2LdjDAA8Jh9eX7lA1Jq/xdXw3Wb3hyvlFNfZIfcRetSzYcA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "jest-get-type": "^29.6.3"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/@jest/fake-timers": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/@jest/fake-timers/-/fake-timers-29.7.0.tgz",
      "integrity": "sha512-q4DH1Ha4TTFPdxLsqDXK1d3+ioSL7yL5oCMJZgDYm6i+6CygW5E5xVr/D1HdsGxjt1ZWSfUAs9OxSB/BNelWrQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jest/types": "^29.6.3",
        "@sinonjs/fake-timers": "^10.0.2",
        "@types/node": "*",
        "jest-message-util": "^29.7.0",
        "jest-mock": "^29.7.0",
        "jest-util": "^29.7.0"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/@jest/globals": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/@jest/globals/-/globals-29.7.0.tgz",
      "integrity": "sha512-mpiz3dutLbkW2MNFubUGUEVLkTGiqW6yLVTA+JbP6fI6J5iL9Y0Nlg8k95pcF8ctKwCS7WVxteBs29hhfAotzQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jest/environment": "^29.7.0",
        "@jest/expect": "^29.7.0",
        "@jest/types": "^29.6.3",
        "jest-mock": "^29.7.0"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/@jest/reporters": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/@jest/reporters/-/reporters-29.7.0.tgz",
      "integrity": "sha512-DApq0KJbJOEzAFYjHADNNxAE3KbhxQB1y5Kplb5Waqw6zVbuWatSnMjE5gs8FUgEPmNsnZA3NCWl9NG0ia04Pg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@bcoe/v8-coverage": "^0.2.3",
        "@jest/console": "^29.7.0",
        "@jest/test-result": "^29.7.0",
        "@jest/transform": "^29.7.0",
        "@jest/types": "^29.6.3",
        "@jridgewell/trace-mapping": "^0.3.18",
        "@types/node": "*",
        "chalk": "^4.0.0",
        "collect-v8-coverage": "^1.0.0",
        "exit": "^0.1.2",
        "glob": "^7.1.3",
        "graceful-fs": "^4.2.9",
        "istanbul-lib-coverage": "^3.0.0",
        "istanbul-lib-instrument": "^6.0.0",
        "istanbul-lib-report": "^3.0.0",
        "istanbul-lib-source-maps": "^4.0.0",
        "istanbul-reports": "^3.1.3",
        "jest-message-util": "^29.7.0",
        "jest-util": "^29.7.0",
        "jest-worker": "^29.7.0",
        "slash": "^3.0.0",
        "string-length": "^4.0.1",
        "strip-ansi": "^6.0.0",
        "v8-to-istanbul": "^9.0.1"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      },
      "peerDependencies": {
        "node-notifier": "^8.0.1 || ^9.0.0 || ^10.0.0"
      },
      "peerDependenciesMeta": {
        "node-notifier": {
          "optional": true
        }
      }
    },
    "node_modules/@jest/schemas": {
      "version": "29.6.3",
      "resolved": "https://registry.npmjs.org/@jest/schemas/-/schemas-29.6.3.tgz",
      "integrity": "sha512-mo5j5X+jIZmJQveBKeS/clAueipV7KgiX1vMgCxam1RNYiqE1w62n0/tJJnHtjW8ZHcQco5gY85jA3mi0L+nSA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@sinclair/typebox": "^0.27.8"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/@jest/source-map": {
      "version": "29.6.3",
      "resolved": "https://registry.npmjs.org/@jest/source-map/-/source-map-29.6.3.tgz",
      "integrity": "sha512-MHjT95QuipcPrpLM+8JMSzFx6eHp5Bm+4XeFDJlwsvVBjmKNiIAvasGK2fxz2WbGRlnvqehFbh07MMa7n3YJnw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jridgewell/trace-mapping": "^0.3.18",
        "callsites": "^3.0.0",
        "graceful-fs": "^4.2.9"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/@jest/test-result": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/@jest/test-result/-/test-result-29.7.0.tgz",
      "integrity": "sha512-Fdx+tv6x1zlkJPcWXmMDAG2HBnaR9XPSd5aDWQVsfrZmLVT3lU1cwyxLgRmXR9yrq4NBoEm9BMsfgFzTQAbJYA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jest/console": "^29.7.0",
        "@jest/types": "^29.6.3",
        "@types/istanbul-lib-coverage": "^2.0.0",
        "collect-v8-coverage": "^1.0.0"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/@jest/test-sequencer": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/@jest/test-sequencer/-/test-sequencer-29.7.0.tgz",
      "integrity": "sha512-GQwJ5WZVrKnOJuiYiAF52UNUJXgTZx1NHjFSEB0qEMmSZKAkdMoIzw/Cj6x6NF4AvV23AUqDpFzQkN/eYCYTxw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jest/test-result": "^29.7.0",
        "graceful-fs": "^4.2.9",
        "jest-haste-map": "^29.7.0",
        "slash": "^3.0.0"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/@jest/transform": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/@jest/transform/-/transform-29.7.0.tgz",
      "integrity": "sha512-ok/BTPFzFKVMwO5eOHRrvnBVHdRy9IrsrW1GpMaQ9MCnilNLXQKmAX8s1YXDFaai9xJpac2ySzV0YeRRECr2Vw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/core": "^7.11.6",
        "@jest/types": "^29.6.3",
        "@jridgewell/trace-mapping": "^0.3.18",
        "babel-plugin-istanbul": "^6.1.1",
        "chalk": "^4.0.0",
        "convert-source-map": "^2.0.0",
        "fast-json-stable-stringify": "^2.1.0",
        "graceful-fs": "^4.2.9",
        "jest-haste-map": "^29.7.0",
        "jest-regex-util": "^29.6.3",
        "jest-util": "^29.7.0",
        "micromatch": "^4.0.4",
        "pirates": "^4.0.4",
        "slash": "^3.0.0",
        "write-file-atomic": "^4.0.2"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/@jest/types": {
      "version": "29.6.3",
      "resolved": "https://registry.npmjs.org/@jest/types/-/types-29.6.3.tgz",
      "integrity": "sha512-u3UPsIilWKOM3F9CXtrG8LEJmNxwoCQC/XVj4IKYXvvpx7QIi/Kg1LI5uDmDpKlac62NUtX7eLjRh+jVZcLOzw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jest/schemas": "^29.6.3",
        "@types/istanbul-lib-coverage": "^2.0.0",
        "@types/istanbul-reports": "^3.0.0",
        "@types/node": "*",
        "@types/yargs": "^17.0.8",
        "chalk": "^4.0.0"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/@jridgewell/gen-mapping": {
      "version": "0.3.13",
      "resolved": "https://registry.npmjs.org/@jridgewell/gen-mapping/-/gen-mapping-0.3.13.tgz",
      "integrity": "sha512-2kkt/7niJ6MgEPxF0bYdQ6etZaA+fQvDcLKckhy1yIQOzaoKjBBjSj63/aLVjYE3qhRt5dvM+uUyfCg6UKCBbA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jridgewell/sourcemap-codec": "^1.5.0",
        "@jridgewell/trace-mapping": "^0.3.24"
      }
    },
    "node_modules/@jridgewell/remapping": {
      "version": "2.3.5",
      "resolved": "https://registry.npmjs.org/@jridgewell/remapping/-/remapping-2.3.5.tgz",
      "integrity": "sha512-LI9u/+laYG4Ds1TDKSJW2YPrIlcVYOwi2fUC6xB43lueCjgxV4lffOCZCtYFiH6TNOX+tQKXx97T4IKHbhyHEQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jridgewell/gen-mapping": "^0.3.5",
        "@jridgewell/trace-mapping": "^0.3.24"
      }
    },
    "node_modules/@jridgewell/resolve-uri": {
      "version": "3.1.2",
      "resolved": "https://registry.npmjs.org/@jridgewell/resolve-uri/-/resolve-uri-3.1.2.tgz",
      "integrity": "sha512-bRISgCIjP20/tbWSPWMEi54QVPRZExkuD9lJL+UIxUKtwVJA8wW1Trb1jMs1RFXo1CBTNZ/5hpC9QvmKWdopKw==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.0.0"
      }
    },
    "node_modules/@jridgewell/sourcemap-codec": {
      "version": "1.5.5",
      "resolved": "https://registry.npmjs.org/@jridgewell/sourcemap-codec/-/sourcemap-codec-1.5.5.tgz",
      "integrity": "sha512-cYQ9310grqxueWbl+WuIUIaiUaDcj7WOq5fVhEljNVgRfOUhY9fy2zTvfoqWsnebh8Sl70VScFbICvJnLKB0Og==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@jridgewell/trace-mapping": {
      "version": "0.3.31",
      "resolved": "https://registry.npmjs.org/@jridgewell/trace-mapping/-/trace-mapping-0.3.31.tgz",
      "integrity": "sha512-zzNR+SdQSDJzc8joaeP8QQoCQr8NuYx2dIIytl1QeBEZHJ9uW6hebsrYgbz8hJwUQao3TWCMtmfV8Nu1twOLAw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jridgewell/resolve-uri": "^3.1.0",
        "@jridgewell/sourcemap-codec": "^1.4.14"
      }
    },
    "node_modules/@sinclair/typebox": {
      "version": "0.27.8",
      "resolved": "https://registry.npmjs.org/@sinclair/typebox/-/typebox-0.27.8.tgz",
      "integrity": "sha512-+Fj43pSMwJs4KRrH/938Uf+uAELIgVBmQzg/q1YG10djyfA3TnrU8N8XzqCh/okZdszqBQTZf96idMfE5lnwTA==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@sinonjs/commons": {
      "version": "3.0.1",
      "resolved": "https://registry.npmjs.org/@sinonjs/commons/-/commons-3.0.1.tgz",
      "integrity": "sha512-K3mCHKQ9sVh8o1C9cxkwxaOmXoAMlDxC1mYyHrjqOWEcBjYr76t96zL2zlj5dUGZ3HSw240X1qgH3Mjf1yJWpQ==",
      "dev": true,
      "license": "BSD-3-Clause",
      "dependencies": {
        "type-detect": "4.0.8"
      }
    },
    "node_modules/@sinonjs/fake-timers": {
      "version": "10.3.0",
      "resolved": "https://registry.npmjs.org/@sinonjs/fake-timers/-/fake-timers-10.3.0.tgz",
      "integrity": "sha512-V4BG07kuYSUkTCSBHG8G8TNhM+F19jXFWnQtzj+we8DrkpSBCee9Z3Ms8yiGer/dlmhe35/Xdgyo3/0rQKg7YA==",
      "dev": true,
      "license": "BSD-3-Clause",
      "dependencies": {
        "@sinonjs/commons": "^3.0.0"
      }
    },
    "node_modules/@tsconfig/node10": {
      "version": "1.0.12",
      "resolved": "https://registry.npmjs.org/@tsconfig/node10/-/node10-1.0.12.tgz",
      "integrity": "sha512-UCYBaeFvM11aU2y3YPZ//O5Rhj+xKyzy7mvcIoAjASbigy8mHMryP5cK7dgjlz2hWxh1g5pLw084E0a/wlUSFQ==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@tsconfig/node12": {
      "version": "1.0.11",
      "resolved": "https://registry.npmjs.org/@tsconfig/node12/-/node12-1.0.11.tgz",
      "integrity": "sha512-cqefuRsh12pWyGsIoBKJA9luFu3mRxCA+ORZvA4ktLSzIuCUtWVxGIuXigEwO5/ywWFMZ2QEGKWvkZG1zDMTag==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@tsconfig/node14": {
      "version": "1.0.3",
      "resolved": "https://registry.npmjs.org/@tsconfig/node14/-/node14-1.0.3.tgz",
      "integrity": "sha512-ysT8mhdixWK6Hw3i1V2AeRqZ5WfXg1G43mqoYlM2nc6388Fq5jcXyr5mRsqViLx/GJYdoL0bfXD8nmF+Zn/Iow==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@tsconfig/node16": {
      "version": "1.0.4",
      "resolved": "https://registry.npmjs.org/@tsconfig/node16/-/node16-1.0.4.tgz",
      "integrity": "sha512-vxhUy4J8lyeyinH7Azl1pdd43GJhZH/tP2weN8TntQblOY+A0XbT8DJk1/oCPuOOyg/Ja757rG0CgHcWC8OfMA==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@types/babel__core": {
      "version": "7.20.5",
      "resolved": "https://registry.npmjs.org/@types/babel__core/-/babel__core-7.20.5.tgz",
      "integrity": "sha512-qoQprZvz5wQFJwMDqeseRXWv3rqMvhgpbXFfVyWhbx9X47POIA6i/+dXefEmZKoAgOaTdaIgNSMqMIU61yRyzA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/parser": "^7.20.7",
        "@babel/types": "^7.20.7",
        "@types/babel__generator": "*",
        "@types/babel__template": "*",
        "@types/babel__traverse": "*"
      }
    },
    "node_modules/@types/babel__generator": {
      "version": "7.27.0",
      "resolved": "https://registry.npmjs.org/@types/babel__generator/-/babel__generator-7.27.0.tgz",
      "integrity": "sha512-ufFd2Xi92OAVPYsy+P4n7/U7e68fex0+Ee8gSG9KX7eo084CWiQ4sdxktvdl0bOPupXtVJPY19zk6EwWqUQ8lg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/types": "^7.0.0"
      }
    },
    "node_modules/@types/babel__template": {
      "version": "7.4.4",
      "resolved": "https://registry.npmjs.org/@types/babel__template/-/babel__template-7.4.4.tgz",
      "integrity": "sha512-h/NUaSyG5EyxBIp8YRxo4RMe2/qQgvyowRwVMzhYhBCONbW8PUsg4lkFMrhgZhUe5z3L3MiLDuvyJ/CaPa2A8A==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/parser": "^7.1.0",
        "@babel/types": "^7.0.0"
      }
    },
    "node_modules/@types/babel__traverse": {
      "version": "7.28.0",
      "resolved": "https://registry.npmjs.org/@types/babel__traverse/-/babel__traverse-7.28.0.tgz",
      "integrity": "sha512-8PvcXf70gTDZBgt9ptxJ8elBeBjcLOAcOtoO/mPJjtji1+CdGbHgm77om1GrsPxsiE+uXIpNSK64UYaIwQXd4Q==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/types": "^7.28.2"
      }
    },
    "node_modules/@types/graceful-fs": {
      "version": "4.1.9",
      "resolved": "https://registry.npmjs.org/@types/graceful-fs/-/graceful-fs-4.1.9.tgz",
      "integrity": "sha512-olP3sd1qOEe5dXTSaFvQG+02VdRXcdytWLAZsAq1PecU8uqQAhkrnbli7DagjtXKW/Bl7YJbUsa8MPcuc8LHEQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@types/node": "*"
      }
    },
    "node_modules/@types/istanbul-lib-coverage": {
      "version": "2.0.6",
      "resolved": "https://registry.npmjs.org/@types/istanbul-lib-coverage/-/istanbul-lib-coverage-2.0.6.tgz",
      "integrity": "sha512-2QF/t/auWm0lsy8XtKVPG19v3sSOQlJe/YHZgfjb/KBBHOGSV+J2q/S671rcq9uTBrLAXmZpqJiaQbMT+zNU1w==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@types/istanbul-lib-report": {
      "version": "3.0.3",
      "resolved": "https://registry.npmjs.org/@types/istanbul-lib-report/-/istanbul-lib-report-3.0.3.tgz",
      "integrity": "sha512-NQn7AHQnk/RSLOxrBbGyJM/aVQ+pjj5HCgasFxc0K/KhoATfQ/47AyUl15I2yBUpihjmas+a+VJBOqecrFH+uA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@types/istanbul-lib-coverage": "*"
      }
    },
    "node_modules/@types/istanbul-reports": {
      "version": "3.0.4",
      "resolved": "https://registry.npmjs.org/@types/istanbul-reports/-/istanbul-reports-3.0.4.tgz",
      "integrity": "sha512-pk2B1NWalF9toCRu6gjBzR69syFjP4Od8WRAX+0mmf9lAjCRicLOWc+ZrxZHx/0XRjotgkF9t6iaMJ+aXcOdZQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@types/istanbul-lib-report": "*"
      }
    },
    "node_modules/@types/jest": {
      "version": "29.5.14",
      "resolved": "https://registry.npmjs.org/@types/jest/-/jest-29.5.14.tgz",
      "integrity": "sha512-ZN+4sdnLUbo8EVvVc2ao0GFW6oVrQRPn4K2lglySj7APvSrgzxHiNNK99us4WDMi57xxA2yggblIAMNhXOotLQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "expect": "^29.0.0",
        "pretty-format": "^29.0.0"
      }
    },
    "node_modules/@types/node": {
      "version": "20.14.9",
      "resolved": "https://registry.npmjs.org/@types/node/-/node-20.14.9.tgz",
      "integrity": "sha512-06OCtnTXtWOZBJlRApleWndH4JsRVs1pDCc8dLSQp+7PpUpX3ePdHyeNSFTeSe7FtKyQkrlPvHwJOW3SLd8Oyg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "undici-types": "~5.26.4"
      }
    },
    "node_modules/@types/stack-utils": {
      "version": "2.0.3",
      "resolved": "https://registry.npmjs.org/@types/stack-utils/-/stack-utils-2.0.3.tgz",
      "integrity": "sha512-9aEbYZ3TbYMznPdcdr3SmIrLXwC/AKZXQeCf9Pgao5CKb8CyHuEX5jzWPTkvregvhRJHcpRO6BFoGW9ycaOkYw==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@types/yargs": {
      "version": "17.0.35",
      "resolved": "https://registry.npmjs.org/@types/yargs/-/yargs-17.0.35.tgz",
      "integrity": "sha512-qUHkeCyQFxMXg79wQfTtfndEC+N9ZZg76HJftDJp+qH2tV7Gj4OJi7l+PiWwJ+pWtW8GwSmqsDj/oymhrTWXjg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@types/yargs-parser": "*"
      }
    },
    "node_modules/@types/yargs-parser": {
      "version": "21.0.3",
      "resolved": "https://registry.npmjs.org/@types/yargs-parser/-/yargs-parser-21.0.3.tgz",
      "integrity": "sha512-I4q9QU9MQv4oEOz4tAHJtNz1cwuLxn2F3xcc2iV5WdqLPpUnj30aUuxt1mAxYTG+oe8CZMV/+6rU4S4gRDzqtQ==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/acorn": {
      "version": "8.15.0",
      "resolved": "https://registry.npmjs.org/acorn/-/acorn-8.15.0.tgz",
      "integrity": "sha512-NZyJarBfL7nWwIq+FDL6Zp/yHEhePMNnnJ0y3qfieCrmNvYct8uvtiV41UvlSe6apAfk0fY1FbWx+NwfmpvtTg==",
      "dev": true,
      "license": "MIT",
      "bin": {
        "acorn": "bin/acorn"
      },
      "engines": {
        "node": ">=0.4.0"
      }
    },
    "node_modules/acorn-walk": {
      "version": "8.3.4",
      "resolved": "https://registry.npmjs.org/acorn-walk/-/acorn-walk-8.3.4.tgz",
      "integrity": "sha512-ueEepnujpqee2o5aIYnvHU6C0A42MNdsIDeqy5BydrkuC5R1ZuUFnm27EeFJGoEHJQgn3uleRvmTXaJgfXbt4g==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "acorn": "^8.11.0"
      },
      "engines": {
        "node": ">=0.4.0"
      }
    },
    "node_modules/ansi-escapes": {
      "version": "4.3.2",
      "resolved": "https://registry.npmjs.org/ansi-escapes/-/ansi-escapes-4.3.2.tgz",
      "integrity": "sha512-gKXj5ALrKWQLsYG9jlTRmR/xKluxHV+Z9QEwNIgCfM1/uwPMCuzVVnh5mwTd+OuBZcwSIMbqssNWRm1lE51QaQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "type-fest": "^0.21.3"
      },
      "engines": {
        "node": ">=8"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/ansi-regex": {
      "version": "5.0.1",
      "resolved": "https://registry.npmjs.org/ansi-regex/-/ansi-regex-5.0.1.tgz",
      "integrity": "sha512-quJQXlTSUGL2LH9SUXo8VwsY4soanhgo6LNSm84E1LBcE8s3O0wpdiRzyR9z/ZZJMlMWv37qOOb9pdJlMUEKFQ==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/ansi-styles": {
      "version": "4.3.0",
      "resolved": "https://registry.npmjs.org/ansi-styles/-/ansi-styles-4.3.0.tgz",
      "integrity": "sha512-zbB9rCJAT1rbjiVDb2hqKFHNYLxgtk8NURxZ3IZwD3F6NtxbXZQCnnSi1Lkx+IDohdPlFp222wVALIheZJQSEg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "color-convert": "^2.0.1"
      },
      "engines": {
        "node": ">=8"
      },
      "funding": {
        "url": "https://github.com/chalk/ansi-styles?sponsor=1"
      }
    },
    "node_modules/anymatch": {
      "version": "3.1.3",
      "resolved": "https://registry.npmjs.org/anymatch/-/anymatch-3.1.3.tgz",
      "integrity": "sha512-KMReFUr0B4t+D+OBkjR3KYqvocp2XaSzO55UcB6mgQMd3KbcE+mWTyvVV7D/zsdEbNnV6acZUutkiHQXvTr1Rw==",
      "dev": true,
      "license": "ISC",
      "dependencies": {
        "normalize-path": "^3.0.0",
        "picomatch": "^2.0.4"
      },
      "engines": {
        "node": ">= 8"
      }
    },
    "node_modules/arg": {
      "version": "4.1.3",
      "resolved": "https://registry.npmjs.org/arg/-/arg-4.1.3.tgz",
      "integrity": "sha512-58S9QDqG0Xx27YwPSt9fJxivjYl432YCwfDMfZ+71RAqUrZef7LrKQZ3LHLOwCS4FLNBplP533Zx895SeOCHvA==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/argparse": {
      "version": "1.0.10",
      "resolved": "https://registry.npmjs.org/argparse/-/argparse-1.0.10.tgz",
      "integrity": "sha512-o5Roy6tNG4SL/FOkCAN6RzjiakZS25RLYFrcMttJqbdd8BWrnA+fGz57iN5Pb06pvBGvl5gQ0B48dJlslXvoTg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "sprintf-js": "~1.0.2"
      }
    },
    "node_modules/aws-cdk": {
      "version": "2.173.0",
      "resolved": "https://registry.npmjs.org/aws-cdk/-/aws-cdk-2.173.0.tgz",
      "integrity": "sha512-riRGKSo5dzB0MSbdkZwXRC2t//dI220bgEUfVISilcEafBKj+BPzFBd/eNKuP/dEaS31njkCwtYrS7V7/lV4hQ==",
      "dev": true,
      "license": "Apache-2.0",
      "bin": {
        "cdk": "bin/cdk"
      },
      "engines": {
        "node": ">= 14.15.0"
      },
      "optionalDependencies": {
        "fsevents": "2.3.2"
      }
    },
    "node_modules/aws-cdk-lib": {
      "version": "2.173.0",
      "resolved": "https://registry.npmjs.org/aws-cdk-lib/-/aws-cdk-lib-2.173.0.tgz",
      "integrity": "sha512-Da1JUwG8eL+chRSB+c2I4dRf54DWe/wmWKj9CBthNdsE9XCB8odyEcMpmgBC+R160o7ioYY2DBsAaKIIRa9XQw==",
      "bundleDependencies": [
        "@balena/dockerignore",
        "case",
        "fs-extra",
        "ignore",
        "jsonschema",
        "minimatch",
        "punycode",
        "semver",
        "table",
        "yaml",
        "mime-types"
      ],
      "license": "Apache-2.0",
      "dependencies": {
        "@aws-cdk/asset-awscli-v1": "^2.2.208",
        "@aws-cdk/asset-kubectl-v20": "^2.1.3",
        "@aws-cdk/asset-node-proxy-agent-v6": "^2.1.0",
        "@aws-cdk/cloud-assembly-schema": "^38.0.1",
        "@balena/dockerignore": "^1.0.2",
        "case": "1.6.3",
        "fs-extra": "^11.2.0",
        "ignore": "^5.3.2",
        "jsonschema": "^1.4.1",
        "mime-types": "^2.1.35",
        "minimatch": "^3.1.2",
        "punycode": "^2.3.1",
        "semver": "^7.6.3",
        "table": "^6.8.2",
        "yaml": "1.10.2"
      },
      "engines": {
        "node": ">= 14.15.0"
      },
      "peerDependencies": {
        "constructs": "^10.0.0"
      }
    },
    "node_modules/aws-cdk-lib/node_modules/@balena/dockerignore": {
      "version": "1.0.2",
      "inBundle": true,
      "license": "Apache-2.0"
    },
    "node_modules/aws-cdk-lib/node_modules/ajv": {
      "version": "8.17.1",
      "inBundle": true,
      "license": "MIT",
      "dependencies": {
        "fast-deep-equal": "^3.1.3",
        "fast-uri": "^3.0.1",
        "json-schema-traverse": "^1.0.0",
        "require-from-string": "^2.0.2"
      },
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/epoberezkin"
      }
    },
    "node_modules/aws-cdk-lib/node_modules/ansi-regex": {
      "version": "5.0.1",
      "inBundle": true,
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/aws-cdk-lib/node_modules/ansi-styles": {
      "version": "4.3.0",
      "inBundle": true,
      "license": "MIT",
      "dependencies": {
        "color-convert": "^2.0.1"
      },
      "engines": {
        "node": ">=8"
      },
      "funding": {
        "url": "https://github.com/chalk/ansi-styles?sponsor=1"
      }
    },
    "node_modules/aws-cdk-lib/node_modules/astral-regex": {
      "version": "2.0.0",
      "inBundle": true,
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/aws-cdk-lib/node_modules/balanced-match": {
      "version": "1.0.2",
      "inBundle": true,
      "license": "MIT"
    },
    "node_modules/aws-cdk-lib/node_modules/brace-expansion": {
      "version": "1.1.11",
      "inBundle": true,
      "license": "MIT",
      "dependencies": {
        "balanced-match": "^1.0.0",
        "concat-map": "0.0.1"
      }
    },
    "node_modules/aws-cdk-lib/node_modules/case": {
      "version": "1.6.3",
      "inBundle": true,
      "license": "(MIT OR GPL-3.0-or-later)",
      "engines": {
        "node": ">= 0.8.0"
      }
    },
    "node_modules/aws-cdk-lib/node_modules/color-convert": {
      "version": "2.0.1",
      "inBundle": true,
      "license": "MIT",
      "dependencies": {
        "color-name": "~1.1.4"
      },
      "engines": {
        "node": ">=7.0.0"
      }
    },
    "node_modules/aws-cdk-lib/node_modules/color-name": {
      "version": "1.1.4",
      "inBundle": true,
      "license": "MIT"
    },
    "node_modules/aws-cdk-lib/node_modules/concat-map": {
      "version": "0.0.1",
      "inBundle": true,
      "license": "MIT"
    },
    "node_modules/aws-cdk-lib/node_modules/emoji-regex": {
      "version": "8.0.0",
      "inBundle": true,
      "license": "MIT"
    },
    "node_modules/aws-cdk-lib/node_modules/fast-deep-equal": {
      "version": "3.1.3",
      "inBundle": true,
      "license": "MIT"
    },
    "node_modules/aws-cdk-lib/node_modules/fast-uri": {
      "version": "3.0.3",
      "inBundle": true,
      "license": "BSD-3-Clause"
    },
    "node_modules/aws-cdk-lib/node_modules/fs-extra": {
      "version": "11.2.0",
      "inBundle": true,
      "license": "MIT",
      "dependencies": {
        "graceful-fs": "^4.2.0",
        "jsonfile": "^6.0.1",
        "universalify": "^2.0.0"
      },
      "engines": {
        "node": ">=14.14"
      }
    },
    "node_modules/aws-cdk-lib/node_modules/graceful-fs": {
      "version": "4.2.11",
      "inBundle": true,
      "license": "ISC"
    },
    "node_modules/aws-cdk-lib/node_modules/ignore": {
      "version": "5.3.2",
      "inBundle": true,
      "license": "MIT",
      "engines": {
        "node": ">= 4"
      }
    },
    "node_modules/aws-cdk-lib/node_modules/is-fullwidth-code-point": {
      "version": "3.0.0",
      "inBundle": true,
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/aws-cdk-lib/node_modules/json-schema-traverse": {
      "version": "1.0.0",
      "inBundle": true,
      "license": "MIT"
    },
    "node_modules/aws-cdk-lib/node_modules/jsonfile": {
      "version": "6.1.0",
      "inBundle": true,
      "license": "MIT",
      "dependencies": {
        "universalify": "^2.0.0"
      },
      "optionalDependencies": {
        "graceful-fs": "^4.1.6"
      }
    },
    "node_modules/aws-cdk-lib/node_modules/jsonschema": {
      "version": "1.4.1",
      "inBundle": true,
      "license": "MIT",
      "engines": {
        "node": "*"
      }
    },
    "node_modules/aws-cdk-lib/node_modules/lodash.truncate": {
      "version": "4.4.2",
      "inBundle": true,
      "license": "MIT"
    },
    "node_modules/aws-cdk-lib/node_modules/mime-db": {
      "version": "1.52.0",
      "inBundle": true,
      "license": "MIT",
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/aws-cdk-lib/node_modules/mime-types": {
      "version": "2.1.35",
      "inBundle": true,
      "license": "MIT",
      "dependencies": {
        "mime-db": "1.52.0"
      },
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/aws-cdk-lib/node_modules/minimatch": {
      "version": "3.1.2",
      "inBundle": true,
      "license": "ISC",
      "dependencies": {
        "brace-expansion": "^1.1.7"
      },
      "engines": {
        "node": "*"
      }
    },
    "node_modules/aws-cdk-lib/node_modules/punycode": {
      "version": "2.3.1",
      "inBundle": true,
      "license": "MIT",
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/aws-cdk-lib/node_modules/require-from-string": {
      "version": "2.0.2",
      "inBundle": true,
      "license": "MIT",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/aws-cdk-lib/node_modules/semver": {
      "version": "7.6.3",
      "inBundle": true,
      "license": "ISC",
      "bin": {
        "semver": "bin/semver.js"
      },
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/aws-cdk-lib/node_modules/slice-ansi": {
      "version": "4.0.0",
      "inBundle": true,
      "license": "MIT",
      "dependencies": {
        "ansi-styles": "^4.0.0",
        "astral-regex": "^2.0.0",
        "is-fullwidth-code-point": "^3.0.0"
      },
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/chalk/slice-ansi?sponsor=1"
      }
    },
    "node_modules/aws-cdk-lib/node_modules/string-width": {
      "version": "4.2.3",
      "inBundle": true,
      "license": "MIT",
      "dependencies": {
        "emoji-regex": "^8.0.0",
        "is-fullwidth-code-point": "^3.0.0",
        "strip-ansi": "^6.0.1"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/aws-cdk-lib/node_modules/strip-ansi": {
      "version": "6.0.1",
      "inBundle": true,
      "license": "MIT",
      "dependencies": {
        "ansi-regex": "^5.0.1"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/aws-cdk-lib/node_modules/table": {
      "version": "6.8.2",
      "inBundle": true,
      "license": "BSD-3-Clause",
      "dependencies": {
        "ajv": "^8.0.1",
        "lodash.truncate": "^4.4.2",
        "slice-ansi": "^4.0.0",
        "string-width": "^4.2.3",
        "strip-ansi": "^6.0.1"
      },
      "engines": {
        "node": ">=10.0.0"
      }
    },
    "node_modules/aws-cdk-lib/node_modules/universalify": {
      "version": "2.0.1",
      "inBundle": true,
      "license": "MIT",
      "engines": {
        "node": ">= 10.0.0"
      }
    },
    "node_modules/aws-cdk-lib/node_modules/yaml": {
      "version": "1.10.2",
      "inBundle": true,
      "license": "ISC",
      "engines": {
        "node": ">= 6"
      }
    },
    "node_modules/babel-jest": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/babel-jest/-/babel-jest-29.7.0.tgz",
      "integrity": "sha512-BrvGY3xZSwEcCzKvKsCi2GgHqDqsYkOP4/by5xCgIwGXQxIEh+8ew3gmrE1y7XRR6LHZIj6yLYnUi/mm2KXKBg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jest/transform": "^29.7.0",
        "@types/babel__core": "^7.1.14",
        "babel-plugin-istanbul": "^6.1.1",
        "babel-preset-jest": "^29.6.3",
        "chalk": "^4.0.0",
        "graceful-fs": "^4.2.9",
        "slash": "^3.0.0"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      },
      "peerDependencies": {
        "@babel/core": "^7.8.0"
      }
    },
    "node_modules/babel-plugin-istanbul": {
      "version": "6.1.1",
      "resolved": "https://registry.npmjs.org/babel-plugin-istanbul/-/babel-plugin-istanbul-6.1.1.tgz",
      "integrity": "sha512-Y1IQok9821cC9onCx5otgFfRm7Lm+I+wwxOx738M/WLPZ9Q42m4IG5W0FNX8WLL2gYMZo3JkuXIH2DOpWM+qwA==",
      "dev": true,
      "license": "BSD-3-Clause",
      "dependencies": {
        "@babel/helper-plugin-utils": "^7.0.0",
        "@istanbuljs/load-nyc-config": "^1.0.0",
        "@istanbuljs/schema": "^0.1.2",
        "istanbul-lib-instrument": "^5.0.4",
        "test-exclude": "^6.0.0"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/babel-plugin-istanbul/node_modules/istanbul-lib-instrument": {
      "version": "5.2.1",
      "resolved": "https://registry.npmjs.org/istanbul-lib-instrument/-/istanbul-lib-instrument-5.2.1.tgz",
      "integrity": "sha512-pzqtp31nLv/XFOzXGuvhCb8qhjmTVo5vjVk19XE4CRlSWz0KoeJ3bw9XsA7nOp9YBf4qHjwBxkDzKcME/J29Yg==",
      "dev": true,
      "license": "BSD-3-Clause",
      "dependencies": {
        "@babel/core": "^7.12.3",
        "@babel/parser": "^7.14.7",
        "@istanbuljs/schema": "^0.1.2",
        "istanbul-lib-coverage": "^3.2.0",
        "semver": "^6.3.0"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/babel-plugin-jest-hoist": {
      "version": "29.6.3",
      "resolved": "https://registry.npmjs.org/babel-plugin-jest-hoist/-/babel-plugin-jest-hoist-29.6.3.tgz",
      "integrity": "sha512-ESAc/RJvGTFEzRwOTT4+lNDk/GNHMkKbNzsvT0qKRfDyyYTskxB5rnU2njIDYVxXCBHHEI1c0YwHob3WaYujOg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/template": "^7.3.3",
        "@babel/types": "^7.3.3",
        "@types/babel__core": "^7.1.14",
        "@types/babel__traverse": "^7.0.6"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/babel-preset-current-node-syntax": {
      "version": "1.2.0",
      "resolved": "https://registry.npmjs.org/babel-preset-current-node-syntax/-/babel-preset-current-node-syntax-1.2.0.tgz",
      "integrity": "sha512-E/VlAEzRrsLEb2+dv8yp3bo4scof3l9nR4lrld+Iy5NyVqgVYUJnDAmunkhPMisRI32Qc4iRiz425d8vM++2fg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/plugin-syntax-async-generators": "^7.8.4",
        "@babel/plugin-syntax-bigint": "^7.8.3",
        "@babel/plugin-syntax-class-properties": "^7.12.13",
        "@babel/plugin-syntax-class-static-block": "^7.14.5",
        "@babel/plugin-syntax-import-attributes": "^7.24.7",
        "@babel/plugin-syntax-import-meta": "^7.10.4",
        "@babel/plugin-syntax-json-strings": "^7.8.3",
        "@babel/plugin-syntax-logical-assignment-operators": "^7.10.4",
        "@babel/plugin-syntax-nullish-coalescing-operator": "^7.8.3",
        "@babel/plugin-syntax-numeric-separator": "^7.10.4",
        "@babel/plugin-syntax-object-rest-spread": "^7.8.3",
        "@babel/plugin-syntax-optional-catch-binding": "^7.8.3",
        "@babel/plugin-syntax-optional-chaining": "^7.8.3",
        "@babel/plugin-syntax-private-property-in-object": "^7.14.5",
        "@babel/plugin-syntax-top-level-await": "^7.14.5"
      },
      "peerDependencies": {
        "@babel/core": "^7.0.0 || ^8.0.0-0"
      }
    },
    "node_modules/babel-preset-jest": {
      "version": "29.6.3",
      "resolved": "https://registry.npmjs.org/babel-preset-jest/-/babel-preset-jest-29.6.3.tgz",
      "integrity": "sha512-0B3bhxR6snWXJZtR/RliHTDPRgn1sNHOR0yVtq/IiQFyuOVjFS+wuio/R4gSNkyYmKmJB4wGZv2NZanmKmTnNA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "babel-plugin-jest-hoist": "^29.6.3",
        "babel-preset-current-node-syntax": "^1.0.0"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      },
      "peerDependencies": {
        "@babel/core": "^7.0.0"
      }
    },
    "node_modules/balanced-match": {
      "version": "1.0.2",
      "resolved": "https://registry.npmjs.org/balanced-match/-/balanced-match-1.0.2.tgz",
      "integrity": "sha512-3oSeUO0TMV67hN1AmbXsK4yaqU7tjiHlbxRDZOpH0KW9+CeX4bRAaX0Anxt0tx2MrpRpWwQaPwIlISEJhYU5Pw==",
      "license": "MIT"
    },
    "node_modules/baseline-browser-mapping": {
      "version": "2.9.11",
      "resolved": "https://registry.npmjs.org/baseline-browser-mapping/-/baseline-browser-mapping-2.9.11.tgz",
      "integrity": "sha512-Sg0xJUNDU1sJNGdfGWhVHX0kkZ+HWcvmVymJbj6NSgZZmW/8S9Y2HQ5euytnIgakgxN6papOAWiwDo1ctFDcoQ==",
      "dev": true,
      "license": "Apache-2.0",
      "bin": {
        "baseline-browser-mapping": "dist/cli.js"
      }
    },
    "node_modules/brace-expansion": {
      "version": "1.1.12",
      "resolved": "https://registry.npmjs.org/brace-expansion/-/brace-expansion-1.1.12.tgz",
      "integrity": "sha512-9T9UjW3r0UW5c1Q7GTwllptXwhvYmEzFhzMfZ9H7FQWt+uZePjZPjBP/W1ZEyZ1twGWom5/56TF4lPcqjnDHcg==",
      "license": "MIT",
      "dependencies": {
        "balanced-match": "^1.0.0",
        "concat-map": "0.0.1"
      }
    },
    "node_modules/braces": {
      "version": "3.0.3",
      "resolved": "https://registry.npmjs.org/braces/-/braces-3.0.3.tgz",
      "integrity": "sha512-yQbXgO/OSZVD2IsiLlro+7Hf6Q18EJrKSEsdoMzKePKXct3gvD8oLcOQdIzGupr5Fj+EDe8gO/lxc1BzfMpxvA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "fill-range": "^7.1.1"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/browserslist": {
      "version": "4.28.1",
      "resolved": "https://registry.npmjs.org/browserslist/-/browserslist-4.28.1.tgz",
      "integrity": "sha512-ZC5Bd0LgJXgwGqUknZY/vkUQ04r8NXnJZ3yYi4vDmSiZmC/pdSN0NbNRPxZpbtO4uAfDUAFffO8IZoM3Gj8IkA==",
      "dev": true,
      "funding": [
        {
          "type": "opencollective",
          "url": "https://opencollective.com/browserslist"
        },
        {
          "type": "tidelift",
          "url": "https://tidelift.com/funding/github/npm/browserslist"
        },
        {
          "type": "github",
          "url": "https://github.com/sponsors/ai"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "baseline-browser-mapping": "^2.9.0",
        "caniuse-lite": "^1.0.30001759",
        "electron-to-chromium": "^1.5.263",
        "node-releases": "^2.0.27",
        "update-browserslist-db": "^1.2.0"
      },
      "bin": {
        "browserslist": "cli.js"
      },
      "engines": {
        "node": "^6 || ^7 || ^8 || ^9 || ^10 || ^11 || ^12 || >=13.7"
      }
    },
    "node_modules/bs-logger": {
      "version": "0.2.6",
      "resolved": "https://registry.npmjs.org/bs-logger/-/bs-logger-0.2.6.tgz",
      "integrity": "sha512-pd8DCoxmbgc7hyPKOvxtqNcjYoOsABPQdcCUjGp3d42VR2CX1ORhk2A87oqqu5R1kk+76nsxZupkmyd+MVtCog==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "fast-json-stable-stringify": "2.x"
      },
      "engines": {
        "node": ">= 6"
      }
    },
    "node_modules/bser": {
      "version": "2.1.1",
      "resolved": "https://registry.npmjs.org/bser/-/bser-2.1.1.tgz",
      "integrity": "sha512-gQxTNE/GAfIIrmHLUE3oJyp5FO6HRBfhjnw4/wMmA63ZGDJnWBmgY/lyQBpnDUkGmAhbSe39tx2d/iTOAfglwQ==",
      "dev": true,
      "license": "Apache-2.0",
      "dependencies": {
        "node-int64": "^0.4.0"
      }
    },
    "node_modules/buffer-from": {
      "version": "1.1.2",
      "resolved": "https://registry.npmjs.org/buffer-from/-/buffer-from-1.1.2.tgz",
      "integrity": "sha512-E+XQCRwSbaaiChtv6k6Dwgc+bx+Bs6vuKJHHl5kox/BaKbhiXzqQOwK4cO22yElGp2OCmjwVhT3HmxgyPGnJfQ==",
      "license": "MIT"
    },
    "node_modules/callsites": {
      "version": "3.1.0",
      "resolved": "https://registry.npmjs.org/callsites/-/callsites-3.1.0.tgz",
      "integrity": "sha512-P8BjAsXvZS+VIDUI11hHCQEv74YT67YUi5JJFNWIqL235sBmjX4+qx9Muvls5ivyNENctx46xQLQ3aTuE7ssaQ==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/camelcase": {
      "version": "5.3.1",
      "resolved": "https://registry.npmjs.org/camelcase/-/camelcase-5.3.1.tgz",
      "integrity": "sha512-L28STB170nwWS63UjtlEOE3dldQApaJXZkOI1uMFfzf3rRuPegHaHesyee+YxQ+W6SvRDQV6UrdOdRiR153wJg==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/caniuse-lite": {
      "version": "1.0.30001761",
      "resolved": "https://registry.npmjs.org/caniuse-lite/-/caniuse-lite-1.0.30001761.tgz",
      "integrity": "sha512-JF9ptu1vP2coz98+5051jZ4PwQgd2ni8A+gYSN7EA7dPKIMf0pDlSUxhdmVOaV3/fYK5uWBkgSXJaRLr4+3A6g==",
      "dev": true,
      "funding": [
        {
          "type": "opencollective",
          "url": "https://opencollective.com/browserslist"
        },
        {
          "type": "tidelift",
          "url": "https://tidelift.com/funding/github/npm/caniuse-lite"
        },
        {
          "type": "github",
          "url": "https://github.com/sponsors/ai"
        }
      ],
      "license": "CC-BY-4.0"
    },
    "node_modules/chalk": {
      "version": "4.1.2",
      "resolved": "https://registry.npmjs.org/chalk/-/chalk-4.1.2.tgz",
      "integrity": "sha512-oKnbhFyRIXpUuez8iBMmyEa4nbj4IOQyuhc/wy9kY7/WVPcwIO9VA668Pu8RkO7+0G76SLROeyw9CpQ061i4mA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "ansi-styles": "^4.1.0",
        "supports-color": "^7.1.0"
      },
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/chalk/chalk?sponsor=1"
      }
    },
    "node_modules/char-regex": {
      "version": "1.0.2",
      "resolved": "https://registry.npmjs.org/char-regex/-/char-regex-1.0.2.tgz",
      "integrity": "sha512-kWWXztvZ5SBQV+eRgKFeh8q5sLuZY2+8WUIzlxWVTg+oGwY14qylx1KbKzHd8P6ZYkAg0xyIDU9JMHhyJMZ1jw==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/ci-info": {
      "version": "3.9.0",
      "resolved": "https://registry.npmjs.org/ci-info/-/ci-info-3.9.0.tgz",
      "integrity": "sha512-NIxF55hv4nSqQswkAeiOi1r83xy8JldOFDTWiug55KBu9Jnblncd2U6ViHmYgHf01TPZS77NJBhBMKdWj9HQMQ==",
      "dev": true,
      "funding": [
        {
          "type": "github",
          "url": "https://github.com/sponsors/sibiraj-s"
        }
      ],
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/cjs-module-lexer": {
      "version": "1.4.3",
      "resolved": "https://registry.npmjs.org/cjs-module-lexer/-/cjs-module-lexer-1.4.3.tgz",
      "integrity": "sha512-9z8TZaGM1pfswYeXrUpzPrkx8UnWYdhJclsiYMm6x/w5+nN+8Tf/LnAgfLGQCm59qAOxU8WwHEq2vNwF6i4j+Q==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/cliui": {
      "version": "8.0.1",
      "resolved": "https://registry.npmjs.org/cliui/-/cliui-8.0.1.tgz",
      "integrity": "sha512-BSeNnyus75C4//NQ9gQt1/csTXyo/8Sb+afLAkzAptFuMsod9HFokGNudZpi/oQV73hnVK+sR+5PVRMd+Dr7YQ==",
      "dev": true,
      "license": "ISC",
      "dependencies": {
        "string-width": "^4.2.0",
        "strip-ansi": "^6.0.1",
        "wrap-ansi": "^7.0.0"
      },
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/co": {
      "version": "4.6.0",
      "resolved": "https://registry.npmjs.org/co/-/co-4.6.0.tgz",
      "integrity": "sha512-QVb0dM5HvG+uaxitm8wONl7jltx8dqhfU33DcqtOZcLSVIKSDDLDi7+0LbAKiyI8hD9u42m2YxXSkMGWThaecQ==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "iojs": ">= 1.0.0",
        "node": ">= 0.12.0"
      }
    },
    "node_modules/collect-v8-coverage": {
      "version": "1.0.3",
      "resolved": "https://registry.npmjs.org/collect-v8-coverage/-/collect-v8-coverage-1.0.3.tgz",
      "integrity": "sha512-1L5aqIkwPfiodaMgQunkF1zRhNqifHBmtbbbxcr6yVxxBnliw4TDOW6NxpO8DJLgJ16OT+Y4ztZqP6p/FtXnAw==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/color-convert": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/color-convert/-/color-convert-2.0.1.tgz",
      "integrity": "sha512-RRECPsj7iu/xb5oKYcsFHSppFNnsj/52OVTRKb4zP5onXwVF3zVmmToNcOfGC+CRDpfK/U584fMg38ZHCaElKQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "color-name": "~1.1.4"
      },
      "engines": {
        "node": ">=7.0.0"
      }
    },
    "node_modules/color-name": {
      "version": "1.1.4",
      "resolved": "https://registry.npmjs.org/color-name/-/color-name-1.1.4.tgz",
      "integrity": "sha512-dOy+3AuW3a2wNbZHIuMZpTcgjGuLU/uBL/ubcZF9OXbDo8ff4O8yVp5Bf0efS8uEoYo5q4Fx7dY9OgQGXgAsQA==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/concat-map": {
      "version": "0.0.1",
      "resolved": "https://registry.npmjs.org/concat-map/-/concat-map-0.0.1.tgz",
      "integrity": "sha512-/Srv4dswyQNBfohGpz9o6Yb3Gz3SrUDqBH5rTuhGR7ahtlbYKnVxw2bCFMRljaA7EXHaXZ8wsHdodFvbkhKmqg==",
      "license": "MIT"
    },
    "node_modules/constructs": {
      "version": "10.4.4",
      "resolved": "https://registry.npmjs.org/constructs/-/constructs-10.4.4.tgz",
      "integrity": "sha512-lP0qC1oViYf1cutHo9/KQ8QL637f/W29tDmv/6sy35F5zs+MD9f66nbAAIjicwc7fwyuF3rkg6PhZh4sfvWIpA==",
      "license": "Apache-2.0"
    },
    "node_modules/convert-source-map": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/convert-source-map/-/convert-source-map-2.0.0.tgz",
      "integrity": "sha512-Kvp459HrV2FEJ1CAsi1Ku+MY3kasH19TFykTz2xWmMeq6bk2NU3XXvfJ+Q61m0xktWwt+1HSYf3JZsTms3aRJg==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/create-jest": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/create-jest/-/create-jest-29.7.0.tgz",
      "integrity": "sha512-Adz2bdH0Vq3F53KEMJOoftQFutWCukm6J24wbPWRO4k1kMY7gS7ds/uoJkNuV8wDCtWWnuwGcJwpWcih+zEW1Q==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jest/types": "^29.6.3",
        "chalk": "^4.0.0",
        "exit": "^0.1.2",
        "graceful-fs": "^4.2.9",
        "jest-config": "^29.7.0",
        "jest-util": "^29.7.0",
        "prompts": "^2.0.1"
      },
      "bin": {
        "create-jest": "bin/create-jest.js"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/create-require": {
      "version": "1.1.1",
      "resolved": "https://registry.npmjs.org/create-require/-/create-require-1.1.1.tgz",
      "integrity": "sha512-dcKFX3jn0MpIaXjisoRvexIJVEKzaq7z2rZKxf+MSr9TkdmHmsU4m2lcLojrj/FHl8mk5VxMmYA+ftRkP/3oKQ==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/cross-spawn": {
      "version": "7.0.6",
      "resolved": "https://registry.npmjs.org/cross-spawn/-/cross-spawn-7.0.6.tgz",
      "integrity": "sha512-uV2QOWP2nWzsy2aMp8aRibhi9dlzF5Hgh5SHaB9OiTGEyDTiJJyx0uy51QXdyWbtAHNua4XJzUKca3OzKUd3vA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "path-key": "^3.1.0",
        "shebang-command": "^2.0.0",
        "which": "^2.0.1"
      },
      "engines": {
        "node": ">= 8"
      }
    },
    "node_modules/debug": {
      "version": "4.4.3",
      "resolved": "https://registry.npmjs.org/debug/-/debug-4.4.3.tgz",
      "integrity": "sha512-RGwwWnwQvkVfavKVt22FGLw+xYSdzARwm0ru6DhTVA3umU5hZc28V3kO4stgYryrTlLpuvgI9GiijltAjNbcqA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "ms": "^2.1.3"
      },
      "engines": {
        "node": ">=6.0"
      },
      "peerDependenciesMeta": {
        "supports-color": {
          "optional": true
        }
      }
    },
    "node_modules/dedent": {
      "version": "1.7.1",
      "resolved": "https://registry.npmjs.org/dedent/-/dedent-1.7.1.tgz",
      "integrity": "sha512-9JmrhGZpOlEgOLdQgSm0zxFaYoQon408V1v49aqTWuXENVlnCuY9JBZcXZiCsZQWDjTm5Qf/nIvAy77mXDAjEg==",
      "dev": true,
      "license": "MIT",
      "peerDependencies": {
        "babel-plugin-macros": "^3.1.0"
      },
      "peerDependenciesMeta": {
        "babel-plugin-macros": {
          "optional": true
        }
      }
    },
    "node_modules/deepmerge": {
      "version": "4.3.1",
      "resolved": "https://registry.npmjs.org/deepmerge/-/deepmerge-4.3.1.tgz",
      "integrity": "sha512-3sUqbMEc77XqpdNO7FRyRog+eW3ph+GYCbj+rK+uYyRMuwsVy0rMiVtPn+QJlKFvWP/1PYpapqYn0Me2knFn+A==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/detect-newline": {
      "version": "3.1.0",
      "resolved": "https://registry.npmjs.org/detect-newline/-/detect-newline-3.1.0.tgz",
      "integrity": "sha512-TLz+x/vEXm/Y7P7wn1EJFNLxYpUD4TgMosxY6fAVJUnJMbupHBOncxyWUG9OpTaH9EBD7uFI5LfEgmMOc54DsA==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/diff": {
      "version": "4.0.2",
      "resolved": "https://registry.npmjs.org/diff/-/diff-4.0.2.tgz",
      "integrity": "sha512-58lmxKSA4BNyLz+HHMUzlOEpg09FV+ev6ZMe3vJihgdxzgcwZ8VoEEPmALCZG9LmqfVoNMMKpttIYTVG6uDY7A==",
      "dev": true,
      "license": "BSD-3-Clause",
      "engines": {
        "node": ">=0.3.1"
      }
    },
    "node_modules/diff-sequences": {
      "version": "29.6.3",
      "resolved": "https://registry.npmjs.org/diff-sequences/-/diff-sequences-29.6.3.tgz",
      "integrity": "sha512-EjePK1srD3P08o2j4f0ExnylqRs5B9tJjcp9t1krH2qRi8CCdsYfwe9JgSLurFBWwq4uOlipzfk5fHNvwFKr8Q==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/electron-to-chromium": {
      "version": "1.5.267",
      "resolved": "https://registry.npmjs.org/electron-to-chromium/-/electron-to-chromium-1.5.267.tgz",
      "integrity": "sha512-0Drusm6MVRXSOJpGbaSVgcQsuB4hEkMpHXaVstcPmhu5LIedxs1xNK/nIxmQIU/RPC0+1/o0AVZfBTkTNJOdUw==",
      "dev": true,
      "license": "ISC"
    },
    "node_modules/emittery": {
      "version": "0.13.1",
      "resolved": "https://registry.npmjs.org/emittery/-/emittery-0.13.1.tgz",
      "integrity": "sha512-DeWwawk6r5yR9jFgnDKYt4sLS0LmHJJi3ZOnb5/JdbYwj3nW+FxQnHIjhBKz8YLC7oRNPVM9NQ47I3CVx34eqQ==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=12"
      },
      "funding": {
        "url": "https://github.com/sindresorhus/emittery?sponsor=1"
      }
    },
    "node_modules/emoji-regex": {
      "version": "8.0.0",
      "resolved": "https://registry.npmjs.org/emoji-regex/-/emoji-regex-8.0.0.tgz",
      "integrity": "sha512-MSjYzcWNOA0ewAHpz0MxpYFvwg6yjy1NG3xteoqz644VCo/RPgnr1/GGt+ic3iJTzQ8Eu3TdM14SawnVUmGE6A==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/error-ex": {
      "version": "1.3.4",
      "resolved": "https://registry.npmjs.org/error-ex/-/error-ex-1.3.4.tgz",
      "integrity": "sha512-sqQamAnR14VgCr1A618A3sGrygcpK+HEbenA/HiEAkkUwcZIIB/tgWqHFxWgOyDh4nB4JCRimh79dR5Ywc9MDQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "is-arrayish": "^0.2.1"
      }
    },
    "node_modules/escalade": {
      "version": "3.2.0",
      "resolved": "https://registry.npmjs.org/escalade/-/escalade-3.2.0.tgz",
      "integrity": "sha512-WUj2qlxaQtO4g6Pq5c29GTcWGDyd8itL8zTlipgECz3JesAiiOKotd8JU6otB3PACgG6xkJUyVhboMS+bje/jA==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/escape-string-regexp": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/escape-string-regexp/-/escape-string-regexp-2.0.0.tgz",
      "integrity": "sha512-UpzcLCXolUWcNu5HtVMHYdXJjArjsF9C0aNnquZYY4uW/Vu0miy5YoWvbV345HauVvcAUnpRuhMMcqTcGOY2+w==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/esprima": {
      "version": "4.0.1",
      "resolved": "https://registry.npmjs.org/esprima/-/esprima-4.0.1.tgz",
      "integrity": "sha512-eGuFFw7Upda+g4p+QHvnW0RyTX/SVeJBDM/gCtMARO0cLuT2HcEKnTPvhjV6aGeqrCB/sbNop0Kszm0jsaWU4A==",
      "dev": true,
      "license": "BSD-2-Clause",
      "bin": {
        "esparse": "bin/esparse.js",
        "esvalidate": "bin/esvalidate.js"
      },
      "engines": {
        "node": ">=4"
      }
    },
    "node_modules/execa": {
      "version": "5.1.1",
      "resolved": "https://registry.npmjs.org/execa/-/execa-5.1.1.tgz",
      "integrity": "sha512-8uSpZZocAZRBAPIEINJj3Lo9HyGitllczc27Eh5YYojjMFMn8yHMDMaUHE2Jqfq05D/wucwI4JGURyXt1vchyg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "cross-spawn": "^7.0.3",
        "get-stream": "^6.0.0",
        "human-signals": "^2.1.0",
        "is-stream": "^2.0.0",
        "merge-stream": "^2.0.0",
        "npm-run-path": "^4.0.1",
        "onetime": "^5.1.2",
        "signal-exit": "^3.0.3",
        "strip-final-newline": "^2.0.0"
      },
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sindresorhus/execa?sponsor=1"
      }
    },
    "node_modules/exit": {
      "version": "0.1.2",
      "resolved": "https://registry.npmjs.org/exit/-/exit-0.1.2.tgz",
      "integrity": "sha512-Zk/eNKV2zbjpKzrsQ+n1G6poVbErQxJ0LBOJXaKZ1EViLzH+hrLu9cdXI4zw9dBQJslwBEpbQ2P1oS7nDxs6jQ==",
      "dev": true,
      "engines": {
        "node": ">= 0.8.0"
      }
    },
    "node_modules/expect": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/expect/-/expect-29.7.0.tgz",
      "integrity": "sha512-2Zks0hf1VLFYI1kbh0I5jP3KHHyCHpkfyHBzsSXRFgl/Bg9mWYfMW8oD+PdMPlEwy5HNsR9JutYy6pMeOh61nw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jest/expect-utils": "^29.7.0",
        "jest-get-type": "^29.6.3",
        "jest-matcher-utils": "^29.7.0",
        "jest-message-util": "^29.7.0",
        "jest-util": "^29.7.0"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/fast-json-stable-stringify": {
      "version": "2.1.0",
      "resolved": "https://registry.npmjs.org/fast-json-stable-stringify/-/fast-json-stable-stringify-2.1.0.tgz",
      "integrity": "sha512-lhd/wF+Lk98HZoTCtlVraHtfh5XYijIjalXck7saUtuanSDyLMxnHhSXEDJqHxD7msR8D0uCmqlkwjCV8xvwHw==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/fb-watchman": {
      "version": "2.0.2",
      "resolved": "https://registry.npmjs.org/fb-watchman/-/fb-watchman-2.0.2.tgz",
      "integrity": "sha512-p5161BqbuCaSnB8jIbzQHOlpgsPmK5rJVDfDKO91Axs5NC1uu3HRQm6wt9cd9/+GtQQIO53JdGXXoyDpTAsgYA==",
      "dev": true,
      "license": "Apache-2.0",
      "dependencies": {
        "bser": "2.1.1"
      }
    },
    "node_modules/fill-range": {
      "version": "7.1.1",
      "resolved": "https://registry.npmjs.org/fill-range/-/fill-range-7.1.1.tgz",
      "integrity": "sha512-YsGpe3WHLK8ZYi4tWDg2Jy3ebRz2rXowDxnld4bkQB00cc/1Zw9AWnC0i9ztDJitivtQvaI9KaLyKrc+hBW0yg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "to-regex-range": "^5.0.1"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/find-up": {
      "version": "4.1.0",
      "resolved": "https://registry.npmjs.org/find-up/-/find-up-4.1.0.tgz",
      "integrity": "sha512-PpOwAdQ/YlXQ2vj8a3h8IipDuYRi3wceVQQGYWxNINccq40Anw7BlsEXCMbt1Zt+OLA6Fq9suIpIWD0OsnISlw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "locate-path": "^5.0.0",
        "path-exists": "^4.0.0"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/fs.realpath": {
      "version": "1.0.0",
      "resolved": "https://registry.npmjs.org/fs.realpath/-/fs.realpath-1.0.0.tgz",
      "integrity": "sha512-OO0pH2lK6a0hZnAdau5ItzHPI6pUlvI7jMVnxUQRtw4owF2wk8lOSabtGDCTP4Ggrg2MbGnWO9X8K1t4+fGMDw==",
      "dev": true,
      "license": "ISC"
    },
    "node_modules/fsevents": {
      "version": "2.3.2",
      "resolved": "https://registry.npmjs.org/fsevents/-/fsevents-2.3.2.tgz",
      "integrity": "sha512-xiqMQR4xAeHTuB9uWm+fFRcIOgKBMiOBP+eXiyT7jsgVCq1bkVygt00oASowB7EdtpOHaaPgKt812P9ab+DDKA==",
      "dev": true,
      "hasInstallScript": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "darwin"
      ],
      "engines": {
        "node": "^8.16.0 || ^10.6.0 || >=11.0.0"
      }
    },
    "node_modules/function-bind": {
      "version": "1.1.2",
      "resolved": "https://registry.npmjs.org/function-bind/-/function-bind-1.1.2.tgz",
      "integrity": "sha512-7XHNxH7qX9xG5mIwxkhumTox/MIRNcOgDrxWsMt2pAr23WHp6MrRlN7FBSFpCpr+oVO0F744iUgR82nJMfG2SA==",
      "dev": true,
      "license": "MIT",
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/gensync": {
      "version": "1.0.0-beta.2",
      "resolved": "https://registry.npmjs.org/gensync/-/gensync-1.0.0-beta.2.tgz",
      "integrity": "sha512-3hN7NaskYvMDLQY55gnW3NQ+mesEAepTqlg+VEbj7zzqEMBVNhzcGYYeqFo/TlYz6eQiFcp1HcsCZO+nGgS8zg==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/get-caller-file": {
      "version": "2.0.5",
      "resolved": "https://registry.npmjs.org/get-caller-file/-/get-caller-file-2.0.5.tgz",
      "integrity": "sha512-DyFP3BM/3YHTQOCUL/w0OZHR0lpKeGrxotcHWcqNEdnltqFwXVfhEBQ94eIo34AfQpo0rGki4cyIiftY06h2Fg==",
      "dev": true,
      "license": "ISC",
      "engines": {
        "node": "6.* || 8.* || >= 10.*"
      }
    },
    "node_modules/get-package-type": {
      "version": "0.1.0",
      "resolved": "https://registry.npmjs.org/get-package-type/-/get-package-type-0.1.0.tgz",
      "integrity": "sha512-pjzuKtY64GYfWizNAJ0fr9VqttZkNiK2iS430LtIHzjBEr6bX8Am2zm4sW4Ro5wjWW5cAlRL1qAMTcXbjNAO2Q==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=8.0.0"
      }
    },
    "node_modules/get-stream": {
      "version": "6.0.1",
      "resolved": "https://registry.npmjs.org/get-stream/-/get-stream-6.0.1.tgz",
      "integrity": "sha512-ts6Wi+2j3jQjqi70w5AlN8DFnkSwC+MqmxEzdEALB2qXZYV3X/b1CTfgPLGJNMeAWxdPfU8FO1ms3NUfaHCPYg==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/glob": {
      "version": "7.2.3",
      "resolved": "https://registry.npmjs.org/glob/-/glob-7.2.3.tgz",
      "integrity": "sha512-nFR0zLpU2YCaRxwoCJvL6UvCH2JFyFVIvwTLsIf21AuHlMskA1hhTdk+LlYJtOlYt9v6dvszD2BGRqBL+iQK9Q==",
      "deprecated": "Glob versions prior to v9 are no longer supported",
      "dev": true,
      "license": "ISC",
      "dependencies": {
        "fs.realpath": "^1.0.0",
        "inflight": "^1.0.4",
        "inherits": "2",
        "minimatch": "^3.1.1",
        "once": "^1.3.0",
        "path-is-absolute": "^1.0.0"
      },
      "engines": {
        "node": "*"
      },
      "funding": {
        "url": "https://github.com/sponsors/isaacs"
      }
    },
    "node_modules/graceful-fs": {
      "version": "4.2.11",
      "resolved": "https://registry.npmjs.org/graceful-fs/-/graceful-fs-4.2.11.tgz",
      "integrity": "sha512-RbJ5/jmFcNNCcDV5o9eTnBLJ/HszWV0P73bc+Ff4nS/rJj+YaS6IGyiOL0VoBYX+l1Wrl3k63h/KrH+nhJ0XvQ==",
      "dev": true,
      "license": "ISC"
    },
    "node_modules/handlebars": {
      "version": "4.7.8",
      "resolved": "https://registry.npmjs.org/handlebars/-/handlebars-4.7.8.tgz",
      "integrity": "sha512-vafaFqs8MZkRrSX7sFVUdo3ap/eNiLnb4IakshzvP56X5Nr1iGKAIqdX6tMlm6HcNRIkr6AxO5jFEoJzzpT8aQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "minimist": "^1.2.5",
        "neo-async": "^2.6.2",
        "source-map": "^0.6.1",
        "wordwrap": "^1.0.0"
      },
      "bin": {
        "handlebars": "bin/handlebars"
      },
      "engines": {
        "node": ">=0.4.7"
      },
      "optionalDependencies": {
        "uglify-js": "^3.1.4"
      }
    },
    "node_modules/has-flag": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/has-flag/-/has-flag-4.0.0.tgz",
      "integrity": "sha512-EykJT/Q1KjTWctppgIAgfSO0tKVuZUjhgMr17kqTumMl6Afv3EISleU7qZUzoXDFTAHTDC4NOoG/ZxU3EvlMPQ==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/hasown": {
      "version": "2.0.2",
      "resolved": "https://registry.npmjs.org/hasown/-/hasown-2.0.2.tgz",
      "integrity": "sha512-0hJU9SCPvmMzIBdZFqNPXWa6dqh7WdH0cII9y+CyS8rG3nL48Bclra9HmKhVVUHyPWNH5Y7xDwAB7bfgSjkUMQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "function-bind": "^1.1.2"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/html-escaper": {
      "version": "2.0.2",
      "resolved": "https://registry.npmjs.org/html-escaper/-/html-escaper-2.0.2.tgz",
      "integrity": "sha512-H2iMtd0I4Mt5eYiapRdIDjp+XzelXQ0tFE4JS7YFwFevXXMmOp9myNrUvCg0D6ws8iqkRPBfKHgbwig1SmlLfg==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/human-signals": {
      "version": "2.1.0",
      "resolved": "https://registry.npmjs.org/human-signals/-/human-signals-2.1.0.tgz",
      "integrity": "sha512-B4FFZ6q/T2jhhksgkbEW3HBvWIfDW85snkQgawt07S7J5QXTk6BkNV+0yAeZrM5QpMAdYlocGoljn0sJ/WQkFw==",
      "dev": true,
      "license": "Apache-2.0",
      "engines": {
        "node": ">=10.17.0"
      }
    },
    "node_modules/import-local": {
      "version": "3.2.0",
      "resolved": "https://registry.npmjs.org/import-local/-/import-local-3.2.0.tgz",
      "integrity": "sha512-2SPlun1JUPWoM6t3F0dw0FkCF/jWY8kttcY4f599GLTSjh2OCuuhdTkJQsEcZzBqbXZGKMK2OqW1oZsjtf/gQA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "pkg-dir": "^4.2.0",
        "resolve-cwd": "^3.0.0"
      },
      "bin": {
        "import-local-fixture": "fixtures/cli.js"
      },
      "engines": {
        "node": ">=8"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/imurmurhash": {
      "version": "0.1.4",
      "resolved": "https://registry.npmjs.org/imurmurhash/-/imurmurhash-0.1.4.tgz",
      "integrity": "sha512-JmXMZ6wuvDmLiHEml9ykzqO6lwFbof0GG4IkcGaENdCRDDmMVnny7s5HsIgHCbaq0w2MyPhDqkhTUgS2LU2PHA==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=0.8.19"
      }
    },
    "node_modules/inflight": {
      "version": "1.0.6",
      "resolved": "https://registry.npmjs.org/inflight/-/inflight-1.0.6.tgz",
      "integrity": "sha512-k92I/b08q4wvFscXCLvqfsHCrjrF7yiXsQuIVvVE7N82W3+aqpzuUdBbfhWcy/FZR3/4IgflMgKLOsvPDrGCJA==",
      "deprecated": "This module is not supported, and leaks memory. Do not use it. Check out lru-cache if you want a good and tested way to coalesce async requests by a key value, which is much more comprehensive and powerful.",
      "dev": true,
      "license": "ISC",
      "dependencies": {
        "once": "^1.3.0",
        "wrappy": "1"
      }
    },
    "node_modules/inherits": {
      "version": "2.0.4",
      "resolved": "https://registry.npmjs.org/inherits/-/inherits-2.0.4.tgz",
      "integrity": "sha512-k/vGaX4/Yla3WzyMCvTQOXYeIHvqOKtnqBduzTHpzpQZzAskKMhZ2K+EnBiSM9zGSoIFeMpXKxa4dYeZIQqewQ==",
      "dev": true,
      "license": "ISC"
    },
    "node_modules/is-arrayish": {
      "version": "0.2.1",
      "resolved": "https://registry.npmjs.org/is-arrayish/-/is-arrayish-0.2.1.tgz",
      "integrity": "sha512-zz06S8t0ozoDXMG+ube26zeCTNXcKIPJZJi8hBrF4idCLms4CG9QtK7qBl1boi5ODzFpjswb5JPmHCbMpjaYzg==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/is-core-module": {
      "version": "2.16.1",
      "resolved": "https://registry.npmjs.org/is-core-module/-/is-core-module-2.16.1.tgz",
      "integrity": "sha512-UfoeMA6fIJ8wTYFEUjelnaGI67v6+N7qXJEvQuIGa99l4xsCruSYOVSQ0uPANn4dAzm8lkYPaKLrrijLq7x23w==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "hasown": "^2.0.2"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/is-fullwidth-code-point": {
      "version": "3.0.0",
      "resolved": "https://registry.npmjs.org/is-fullwidth-code-point/-/is-fullwidth-code-point-3.0.0.tgz",
      "integrity": "sha512-zymm5+u+sCsSWyD9qNaejV3DFvhCKclKdizYaJUuHA83RLjb7nSuGnddCHGv0hk+KY7BMAlsWeK4Ueg6EV6XQg==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/is-generator-fn": {
      "version": "2.1.0",
      "resolved": "https://registry.npmjs.org/is-generator-fn/-/is-generator-fn-2.1.0.tgz",
      "integrity": "sha512-cTIB4yPYL/Grw0EaSzASzg6bBy9gqCofvWN8okThAYIxKJZC+udlRAmGbM0XLeniEJSs8uEgHPGuHSe1XsOLSQ==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/is-number": {
      "version": "7.0.0",
      "resolved": "https://registry.npmjs.org/is-number/-/is-number-7.0.0.tgz",
      "integrity": "sha512-41Cifkg6e8TylSpdtTpeLVMqvSBEVzTttHvERD741+pnZ8ANv0004MRL43QKPDlK9cGvNp6NZWZUBlbGXYxxng==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=0.12.0"
      }
    },
    "node_modules/is-stream": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/is-stream/-/is-stream-2.0.1.tgz",
      "integrity": "sha512-hFoiJiTl63nn+kstHGBtewWSKnQLpyb155KHheA1l39uvtO9nWIop1p3udqPcUd/xbF1VLMO4n7OI6p7RbngDg==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=8"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/isexe": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/isexe/-/isexe-2.0.0.tgz",
      "integrity": "sha512-RHxMLp9lnKHGHRng9QFhRCMbYAcVpn69smSGcq3f36xjgVVWThj4qqLbTLlq7Ssj8B+fIQ1EuCEGI2lKsyQeIw==",
      "dev": true,
      "license": "ISC"
    },
    "node_modules/istanbul-lib-coverage": {
      "version": "3.2.2",
      "resolved": "https://registry.npmjs.org/istanbul-lib-coverage/-/istanbul-lib-coverage-3.2.2.tgz",
      "integrity": "sha512-O8dpsF+r0WV/8MNRKfnmrtCWhuKjxrq2w+jpzBL5UZKTi2LeVWnWOmWRxFlesJONmc+wLAGvKQZEOanko0LFTg==",
      "dev": true,
      "license": "BSD-3-Clause",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/istanbul-lib-instrument": {
      "version": "6.0.3",
      "resolved": "https://registry.npmjs.org/istanbul-lib-instrument/-/istanbul-lib-instrument-6.0.3.tgz",
      "integrity": "sha512-Vtgk7L/R2JHyyGW07spoFlB8/lpjiOLTjMdms6AFMraYt3BaJauod/NGrfnVG/y4Ix1JEuMRPDPEj2ua+zz1/Q==",
      "dev": true,
      "license": "BSD-3-Clause",
      "dependencies": {
        "@babel/core": "^7.23.9",
        "@babel/parser": "^7.23.9",
        "@istanbuljs/schema": "^0.1.3",
        "istanbul-lib-coverage": "^3.2.0",
        "semver": "^7.5.4"
      },
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/istanbul-lib-instrument/node_modules/semver": {
      "version": "7.7.3",
      "resolved": "https://registry.npmjs.org/semver/-/semver-7.7.3.tgz",
      "integrity": "sha512-SdsKMrI9TdgjdweUSR9MweHA4EJ8YxHn8DFaDisvhVlUOe4BF1tLD7GAj0lIqWVl+dPb/rExr0Btby5loQm20Q==",
      "dev": true,
      "license": "ISC",
      "bin": {
        "semver": "bin/semver.js"
      },
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/istanbul-lib-report": {
      "version": "3.0.1",
      "resolved": "https://registry.npmjs.org/istanbul-lib-report/-/istanbul-lib-report-3.0.1.tgz",
      "integrity": "sha512-GCfE1mtsHGOELCU8e/Z7YWzpmybrx/+dSTfLrvY8qRmaY6zXTKWn6WQIjaAFw069icm6GVMNkgu0NzI4iPZUNw==",
      "dev": true,
      "license": "BSD-3-Clause",
      "dependencies": {
        "istanbul-lib-coverage": "^3.0.0",
        "make-dir": "^4.0.0",
        "supports-color": "^7.1.0"
      },
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/istanbul-lib-source-maps": {
      "version": "4.0.1",
      "resolved": "https://registry.npmjs.org/istanbul-lib-source-maps/-/istanbul-lib-source-maps-4.0.1.tgz",
      "integrity": "sha512-n3s8EwkdFIJCG3BPKBYvskgXGoy88ARzvegkitk60NxRdwltLOTaH7CUiMRXvwYorl0Q712iEjcWB+fK/MrWVw==",
      "dev": true,
      "license": "BSD-3-Clause",
      "dependencies": {
        "debug": "^4.1.1",
        "istanbul-lib-coverage": "^3.0.0",
        "source-map": "^0.6.1"
      },
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/istanbul-reports": {
      "version": "3.2.0",
      "resolved": "https://registry.npmjs.org/istanbul-reports/-/istanbul-reports-3.2.0.tgz",
      "integrity": "sha512-HGYWWS/ehqTV3xN10i23tkPkpH46MLCIMFNCaaKNavAXTF1RkqxawEPtnjnGZ6XKSInBKkiOA5BKS+aZiY3AvA==",
      "dev": true,
      "license": "BSD-3-Clause",
      "dependencies": {
        "html-escaper": "^2.0.0",
        "istanbul-lib-report": "^3.0.0"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/jest": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/jest/-/jest-29.7.0.tgz",
      "integrity": "sha512-NIy3oAFp9shda19hy4HK0HRTWKtPJmGdnvywu01nOqNC2vZg+Z+fvJDxpMQA88eb2I9EcafcdjYgsDthnYTvGw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jest/core": "^29.7.0",
        "@jest/types": "^29.6.3",
        "import-local": "^3.0.2",
        "jest-cli": "^29.7.0"
      },
      "bin": {
        "jest": "bin/jest.js"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      },
      "peerDependencies": {
        "node-notifier": "^8.0.1 || ^9.0.0 || ^10.0.0"
      },
      "peerDependenciesMeta": {
        "node-notifier": {
          "optional": true
        }
      }
    },
    "node_modules/jest-changed-files": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/jest-changed-files/-/jest-changed-files-29.7.0.tgz",
      "integrity": "sha512-fEArFiwf1BpQ+4bXSprcDc3/x4HSzL4al2tozwVpDFpsxALjLYdyiIK4e5Vz66GQJIbXJ82+35PtysofptNX2w==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "execa": "^5.0.0",
        "jest-util": "^29.7.0",
        "p-limit": "^3.1.0"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/jest-circus": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/jest-circus/-/jest-circus-29.7.0.tgz",
      "integrity": "sha512-3E1nCMgipcTkCocFwM90XXQab9bS+GMsjdpmPrlelaxwD93Ad8iVEjX/vvHPdLPnFf+L40u+5+iutRdA1N9myw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jest/environment": "^29.7.0",
        "@jest/expect": "^29.7.0",
        "@jest/test-result": "^29.7.0",
        "@jest/types": "^29.6.3",
        "@types/node": "*",
        "chalk": "^4.0.0",
        "co": "^4.6.0",
        "dedent": "^1.0.0",
        "is-generator-fn": "^2.0.0",
        "jest-each": "^29.7.0",
        "jest-matcher-utils": "^29.7.0",
        "jest-message-util": "^29.7.0",
        "jest-runtime": "^29.7.0",
        "jest-snapshot": "^29.7.0",
        "jest-util": "^29.7.0",
        "p-limit": "^3.1.0",
        "pretty-format": "^29.7.0",
        "pure-rand": "^6.0.0",
        "slash": "^3.0.0",
        "stack-utils": "^2.0.3"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/jest-cli": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/jest-cli/-/jest-cli-29.7.0.tgz",
      "integrity": "sha512-OVVobw2IubN/GSYsxETi+gOe7Ka59EFMR/twOU3Jb2GnKKeMGJB5SGUUrEz3SFVmJASUdZUzy83sLNNQ2gZslg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jest/core": "^29.7.0",
        "@jest/test-result": "^29.7.0",
        "@jest/types": "^29.6.3",
        "chalk": "^4.0.0",
        "create-jest": "^29.7.0",
        "exit": "^0.1.2",
        "import-local": "^3.0.2",
        "jest-config": "^29.7.0",
        "jest-util": "^29.7.0",
        "jest-validate": "^29.7.0",
        "yargs": "^17.3.1"
      },
      "bin": {
        "jest": "bin/jest.js"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      },
      "peerDependencies": {
        "node-notifier": "^8.0.1 || ^9.0.0 || ^10.0.0"
      },
      "peerDependenciesMeta": {
        "node-notifier": {
          "optional": true
        }
      }
    },
    "node_modules/jest-config": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/jest-config/-/jest-config-29.7.0.tgz",
      "integrity": "sha512-uXbpfeQ7R6TZBqI3/TxCU4q4ttk3u0PJeC+E0zbfSoSjq6bJ7buBPxzQPL0ifrkY4DNu4JUdk0ImlBUYi840eQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/core": "^7.11.6",
        "@jest/test-sequencer": "^29.7.0",
        "@jest/types": "^29.6.3",
        "babel-jest": "^29.7.0",
        "chalk": "^4.0.0",
        "ci-info": "^3.2.0",
        "deepmerge": "^4.2.2",
        "glob": "^7.1.3",
        "graceful-fs": "^4.2.9",
        "jest-circus": "^29.7.0",
        "jest-environment-node": "^29.7.0",
        "jest-get-type": "^29.6.3",
        "jest-regex-util": "^29.6.3",
        "jest-resolve": "^29.7.0",
        "jest-runner": "^29.7.0",
        "jest-util": "^29.7.0",
        "jest-validate": "^29.7.0",
        "micromatch": "^4.0.4",
        "parse-json": "^5.2.0",
        "pretty-format": "^29.7.0",
        "slash": "^3.0.0",
        "strip-json-comments": "^3.1.1"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      },
      "peerDependencies": {
        "@types/node": "*",
        "ts-node": ">=9.0.0"
      },
      "peerDependenciesMeta": {
        "@types/node": {
          "optional": true
        },
        "ts-node": {
          "optional": true
        }
      }
    },
    "node_modules/jest-diff": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/jest-diff/-/jest-diff-29.7.0.tgz",
      "integrity": "sha512-LMIgiIrhigmPrs03JHpxUh2yISK3vLFPkAodPeo0+BuF7wA2FoQbkEg1u8gBYBThncu7e1oEDUfIXVuTqLRUjw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "chalk": "^4.0.0",
        "diff-sequences": "^29.6.3",
        "jest-get-type": "^29.6.3",
        "pretty-format": "^29.7.0"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/jest-docblock": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/jest-docblock/-/jest-docblock-29.7.0.tgz",
      "integrity": "sha512-q617Auw3A612guyaFgsbFeYpNP5t2aoUNLwBUbc/0kD1R4t9ixDbyFTHd1nok4epoVFpr7PmeWHrhvuV3XaJ4g==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "detect-newline": "^3.0.0"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/jest-each": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/jest-each/-/jest-each-29.7.0.tgz",
      "integrity": "sha512-gns+Er14+ZrEoC5fhOfYCY1LOHHr0TI+rQUHZS8Ttw2l7gl+80eHc/gFf2Ktkw0+SIACDTeWvpFcv3B04VembQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jest/types": "^29.6.3",
        "chalk": "^4.0.0",
        "jest-get-type": "^29.6.3",
        "jest-util": "^29.7.0",
        "pretty-format": "^29.7.0"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/jest-environment-node": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/jest-environment-node/-/jest-environment-node-29.7.0.tgz",
      "integrity": "sha512-DOSwCRqXirTOyheM+4d5YZOrWcdu0LNZ87ewUoywbcb2XR4wKgqiG8vNeYwhjFMbEkfju7wx2GYH0P2gevGvFw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jest/environment": "^29.7.0",
        "@jest/fake-timers": "^29.7.0",
        "@jest/types": "^29.6.3",
        "@types/node": "*",
        "jest-mock": "^29.7.0",
        "jest-util": "^29.7.0"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/jest-get-type": {
      "version": "29.6.3",
      "resolved": "https://registry.npmjs.org/jest-get-type/-/jest-get-type-29.6.3.tgz",
      "integrity": "sha512-zrteXnqYxfQh7l5FHyL38jL39di8H8rHoecLH3JNxH3BwOrBsNeabdap5e0I23lD4HHI8W5VFBZqG4Eaq5LNcw==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/jest-haste-map": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/jest-haste-map/-/jest-haste-map-29.7.0.tgz",
      "integrity": "sha512-fP8u2pyfqx0K1rGn1R9pyE0/KTn+G7PxktWidOBTqFPLYX0b9ksaMFkhK5vrS3DVun09pckLdlx90QthlW7AmA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jest/types": "^29.6.3",
        "@types/graceful-fs": "^4.1.3",
        "@types/node": "*",
        "anymatch": "^3.0.3",
        "fb-watchman": "^2.0.0",
        "graceful-fs": "^4.2.9",
        "jest-regex-util": "^29.6.3",
        "jest-util": "^29.7.0",
        "jest-worker": "^29.7.0",
        "micromatch": "^4.0.4",
        "walker": "^1.0.8"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      },
      "optionalDependencies": {
        "fsevents": "^2.3.2"
      }
    },
    "node_modules/jest-leak-detector": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/jest-leak-detector/-/jest-leak-detector-29.7.0.tgz",
      "integrity": "sha512-kYA8IJcSYtST2BY9I+SMC32nDpBT3J2NvWJx8+JCuCdl/CR1I4EKUJROiP8XtCcxqgTTBGJNdbB1A8XRKbTetw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "jest-get-type": "^29.6.3",
        "pretty-format": "^29.7.0"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/jest-matcher-utils": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/jest-matcher-utils/-/jest-matcher-utils-29.7.0.tgz",
      "integrity": "sha512-sBkD+Xi9DtcChsI3L3u0+N0opgPYnCRPtGcQYrgXmR+hmt/fYfWAL0xRXYU8eWOdfuLgBe0YCW3AFtnRLagq/g==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "chalk": "^4.0.0",
        "jest-diff": "^29.7.0",
        "jest-get-type": "^29.6.3",
        "pretty-format": "^29.7.0"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/jest-message-util": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/jest-message-util/-/jest-message-util-29.7.0.tgz",
      "integrity": "sha512-GBEV4GRADeP+qtB2+6u61stea8mGcOT4mCtrYISZwfu9/ISHFJ/5zOMXYbpBE9RsS5+Gb63DW4FgmnKJ79Kf6w==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/code-frame": "^7.12.13",
        "@jest/types": "^29.6.3",
        "@types/stack-utils": "^2.0.0",
        "chalk": "^4.0.0",
        "graceful-fs": "^4.2.9",
        "micromatch": "^4.0.4",
        "pretty-format": "^29.7.0",
        "slash": "^3.0.0",
        "stack-utils": "^2.0.3"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/jest-mock": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/jest-mock/-/jest-mock-29.7.0.tgz",
      "integrity": "sha512-ITOMZn+UkYS4ZFh83xYAOzWStloNzJFO2s8DWrE4lhtGD+AorgnbkiKERe4wQVBydIGPx059g6riW5Btp6Llnw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jest/types": "^29.6.3",
        "@types/node": "*",
        "jest-util": "^29.7.0"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/jest-pnp-resolver": {
      "version": "1.2.3",
      "resolved": "https://registry.npmjs.org/jest-pnp-resolver/-/jest-pnp-resolver-1.2.3.tgz",
      "integrity": "sha512-+3NpwQEnRoIBtx4fyhblQDPgJI0H1IEIkX7ShLUjPGA7TtUTvI1oiKi3SR4oBR0hQhQR80l4WAe5RrXBwWMA8w==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6"
      },
      "peerDependencies": {
        "jest-resolve": "*"
      },
      "peerDependenciesMeta": {
        "jest-resolve": {
          "optional": true
        }
      }
    },
    "node_modules/jest-regex-util": {
      "version": "29.6.3",
      "resolved": "https://registry.npmjs.org/jest-regex-util/-/jest-regex-util-29.6.3.tgz",
      "integrity": "sha512-KJJBsRCyyLNWCNBOvZyRDnAIfUiRJ8v+hOBQYGn8gDyF3UegwiP4gwRR3/SDa42g1YbVycTidUF3rKjyLFDWbg==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/jest-resolve": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/jest-resolve/-/jest-resolve-29.7.0.tgz",
      "integrity": "sha512-IOVhZSrg+UvVAshDSDtHyFCCBUl/Q3AAJv8iZ6ZjnZ74xzvwuzLXid9IIIPgTnY62SJjfuupMKZsZQRsCvxEgA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "chalk": "^4.0.0",
        "graceful-fs": "^4.2.9",
        "jest-haste-map": "^29.7.0",
        "jest-pnp-resolver": "^1.2.2",
        "jest-util": "^29.7.0",
        "jest-validate": "^29.7.0",
        "resolve": "^1.20.0",
        "resolve.exports": "^2.0.0",
        "slash": "^3.0.0"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/jest-resolve-dependencies": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/jest-resolve-dependencies/-/jest-resolve-dependencies-29.7.0.tgz",
      "integrity": "sha512-un0zD/6qxJ+S0et7WxeI3H5XSe9lTBBR7bOHCHXkKR6luG5mwDDlIzVQ0V5cZCuoTgEdcdwzTghYkTWfubi+nA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "jest-regex-util": "^29.6.3",
        "jest-snapshot": "^29.7.0"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/jest-runner": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/jest-runner/-/jest-runner-29.7.0.tgz",
      "integrity": "sha512-fsc4N6cPCAahybGBfTRcq5wFR6fpLznMg47sY5aDpsoejOcVYFb07AHuSnR0liMcPTgBsA3ZJL6kFOjPdoNipQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jest/console": "^29.7.0",
        "@jest/environment": "^29.7.0",
        "@jest/test-result": "^29.7.0",
        "@jest/transform": "^29.7.0",
        "@jest/types": "^29.6.3",
        "@types/node": "*",
        "chalk": "^4.0.0",
        "emittery": "^0.13.1",
        "graceful-fs": "^4.2.9",
        "jest-docblock": "^29.7.0",
        "jest-environment-node": "^29.7.0",
        "jest-haste-map": "^29.7.0",
        "jest-leak-detector": "^29.7.0",
        "jest-message-util": "^29.7.0",
        "jest-resolve": "^29.7.0",
        "jest-runtime": "^29.7.0",
        "jest-util": "^29.7.0",
        "jest-watcher": "^29.7.0",
        "jest-worker": "^29.7.0",
        "p-limit": "^3.1.0",
        "source-map-support": "0.5.13"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/jest-runner/node_modules/source-map-support": {
      "version": "0.5.13",
      "resolved": "https://registry.npmjs.org/source-map-support/-/source-map-support-0.5.13.tgz",
      "integrity": "sha512-SHSKFHadjVA5oR4PPqhtAVdcBWwRYVd6g6cAXnIbRiIwc2EhPrTuKUBdSLvlEKyIP3GCf89fltvcZiP9MMFA1w==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "buffer-from": "^1.0.0",
        "source-map": "^0.6.0"
      }
    },
    "node_modules/jest-runtime": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/jest-runtime/-/jest-runtime-29.7.0.tgz",
      "integrity": "sha512-gUnLjgwdGqW7B4LvOIkbKs9WGbn+QLqRQQ9juC6HndeDiezIwhDP+mhMwHWCEcfQ5RUXa6OPnFF8BJh5xegwwQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jest/environment": "^29.7.0",
        "@jest/fake-timers": "^29.7.0",
        "@jest/globals": "^29.7.0",
        "@jest/source-map": "^29.6.3",
        "@jest/test-result": "^29.7.0",
        "@jest/transform": "^29.7.0",
        "@jest/types": "^29.6.3",
        "@types/node": "*",
        "chalk": "^4.0.0",
        "cjs-module-lexer": "^1.0.0",
        "collect-v8-coverage": "^1.0.0",
        "glob": "^7.1.3",
        "graceful-fs": "^4.2.9",
        "jest-haste-map": "^29.7.0",
        "jest-message-util": "^29.7.0",
        "jest-mock": "^29.7.0",
        "jest-regex-util": "^29.6.3",
        "jest-resolve": "^29.7.0",
        "jest-snapshot": "^29.7.0",
        "jest-util": "^29.7.0",
        "slash": "^3.0.0",
        "strip-bom": "^4.0.0"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/jest-snapshot": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/jest-snapshot/-/jest-snapshot-29.7.0.tgz",
      "integrity": "sha512-Rm0BMWtxBcioHr1/OX5YCP8Uov4riHvKPknOGs804Zg9JGZgmIBkbtlxJC/7Z4msKYVbIJtfU+tKb8xlYNfdkw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/core": "^7.11.6",
        "@babel/generator": "^7.7.2",
        "@babel/plugin-syntax-jsx": "^7.7.2",
        "@babel/plugin-syntax-typescript": "^7.7.2",
        "@babel/types": "^7.3.3",
        "@jest/expect-utils": "^29.7.0",
        "@jest/transform": "^29.7.0",
        "@jest/types": "^29.6.3",
        "babel-preset-current-node-syntax": "^1.0.0",
        "chalk": "^4.0.0",
        "expect": "^29.7.0",
        "graceful-fs": "^4.2.9",
        "jest-diff": "^29.7.0",
        "jest-get-type": "^29.6.3",
        "jest-matcher-utils": "^29.7.0",
        "jest-message-util": "^29.7.0",
        "jest-util": "^29.7.0",
        "natural-compare": "^1.4.0",
        "pretty-format": "^29.7.0",
        "semver": "^7.5.3"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/jest-snapshot/node_modules/semver": {
      "version": "7.7.3",
      "resolved": "https://registry.npmjs.org/semver/-/semver-7.7.3.tgz",
      "integrity": "sha512-SdsKMrI9TdgjdweUSR9MweHA4EJ8YxHn8DFaDisvhVlUOe4BF1tLD7GAj0lIqWVl+dPb/rExr0Btby5loQm20Q==",
      "dev": true,
      "license": "ISC",
      "bin": {
        "semver": "bin/semver.js"
      },
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/jest-util": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/jest-util/-/jest-util-29.7.0.tgz",
      "integrity": "sha512-z6EbKajIpqGKU56y5KBUgy1dt1ihhQJgWzUlZHArA/+X2ad7Cb5iF+AK1EWVL/Bo7Rz9uurpqw6SiBCefUbCGA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jest/types": "^29.6.3",
        "@types/node": "*",
        "chalk": "^4.0.0",
        "ci-info": "^3.2.0",
        "graceful-fs": "^4.2.9",
        "picomatch": "^2.2.3"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/jest-validate": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/jest-validate/-/jest-validate-29.7.0.tgz",
      "integrity": "sha512-ZB7wHqaRGVw/9hST/OuFUReG7M8vKeq0/J2egIGLdvjHCmYqGARhzXmtgi+gVeZ5uXFF219aOc3Ls2yLg27tkw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jest/types": "^29.6.3",
        "camelcase": "^6.2.0",
        "chalk": "^4.0.0",
        "jest-get-type": "^29.6.3",
        "leven": "^3.1.0",
        "pretty-format": "^29.7.0"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/jest-validate/node_modules/camelcase": {
      "version": "6.3.0",
      "resolved": "https://registry.npmjs.org/camelcase/-/camelcase-6.3.0.tgz",
      "integrity": "sha512-Gmy6FhYlCY7uOElZUSbxo2UCDH8owEk996gkbrpsgGtrJLM3J7jGxl9Ic7Qwwj4ivOE5AWZWRMecDdF7hqGjFA==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/jest-watcher": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/jest-watcher/-/jest-watcher-29.7.0.tgz",
      "integrity": "sha512-49Fg7WXkU3Vl2h6LbLtMQ/HyB6rXSIX7SqvBLQmssRBGN9I0PNvPmAmCWSOY6SOvrjhI/F7/bGAv9RtnsPA03g==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jest/test-result": "^29.7.0",
        "@jest/types": "^29.6.3",
        "@types/node": "*",
        "ansi-escapes": "^4.2.1",
        "chalk": "^4.0.0",
        "emittery": "^0.13.1",
        "jest-util": "^29.7.0",
        "string-length": "^4.0.1"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/jest-worker": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/jest-worker/-/jest-worker-29.7.0.tgz",
      "integrity": "sha512-eIz2msL/EzL9UFTFFx7jBTkeZfku0yUAyZZZmJ93H2TYEiroIx2PQjEXcwYtYl8zXCxb+PAmA2hLIt/6ZEkPHw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@types/node": "*",
        "jest-util": "^29.7.0",
        "merge-stream": "^2.0.0",
        "supports-color": "^8.0.0"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/jest-worker/node_modules/supports-color": {
      "version": "8.1.1",
      "resolved": "https://registry.npmjs.org/supports-color/-/supports-color-8.1.1.tgz",
      "integrity": "sha512-MpUEN2OodtUzxvKQl72cUF7RQ5EiHsGvSsVG0ia9c5RbWGL2CI4C7EpPS8UTBIplnlzZiNuV56w+FuNxy3ty2Q==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "has-flag": "^4.0.0"
      },
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/chalk/supports-color?sponsor=1"
      }
    },
    "node_modules/js-tokens": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/js-tokens/-/js-tokens-4.0.0.tgz",
      "integrity": "sha512-RdJUflcE3cUzKiMqQgsCu06FPu9UdIJO0beYbPhHN4k6apgJtifcoCtT9bcxOpYBtpD2kCM6Sbzg4CausW/PKQ==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/js-yaml": {
      "version": "3.14.2",
      "resolved": "https://registry.npmjs.org/js-yaml/-/js-yaml-3.14.2.tgz",
      "integrity": "sha512-PMSmkqxr106Xa156c2M265Z+FTrPl+oxd/rgOQy2tijQeK5TxQ43psO1ZCwhVOSdnn+RzkzlRz/eY4BgJBYVpg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "argparse": "^1.0.7",
        "esprima": "^4.0.0"
      },
      "bin": {
        "js-yaml": "bin/js-yaml.js"
      }
    },
    "node_modules/jsesc": {
      "version": "3.1.0",
      "resolved": "https://registry.npmjs.org/jsesc/-/jsesc-3.1.0.tgz",
      "integrity": "sha512-/sM3dO2FOzXjKQhJuo0Q173wf2KOo8t4I8vHy6lF9poUp7bKT0/NHE8fPX23PwfhnykfqnC2xRxOnVw5XuGIaA==",
      "dev": true,
      "license": "MIT",
      "bin": {
        "jsesc": "bin/jsesc"
      },
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/json-parse-even-better-errors": {
      "version": "2.3.1",
      "resolved": "https://registry.npmjs.org/json-parse-even-better-errors/-/json-parse-even-better-errors-2.3.1.tgz",
      "integrity": "sha512-xyFwyhro/JEof6Ghe2iz2NcXoj2sloNsWr/XsERDK/oiPCfaNhl5ONfp+jQdAZRQQ0IJWNzH9zIZF7li91kh2w==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/json5": {
      "version": "2.2.3",
      "resolved": "https://registry.npmjs.org/json5/-/json5-2.2.3.tgz",
      "integrity": "sha512-XmOWe7eyHYH14cLdVPoyg+GOH3rYX++KpzrylJwSW98t3Nk+U8XOl8FWKOgwtzdb8lXGf6zYwDUzeHMWfxasyg==",
      "dev": true,
      "license": "MIT",
      "bin": {
        "json5": "lib/cli.js"
      },
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/kleur": {
      "version": "3.0.3",
      "resolved": "https://registry.npmjs.org/kleur/-/kleur-3.0.3.tgz",
      "integrity": "sha512-eTIzlVOSUR+JxdDFepEYcBMtZ9Qqdef+rnzWdRZuMbOywu5tO2w2N7rqjoANZ5k9vywhL6Br1VRjUIgTQx4E8w==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/leven": {
      "version": "3.1.0",
      "resolved": "https://registry.npmjs.org/leven/-/leven-3.1.0.tgz",
      "integrity": "sha512-qsda+H8jTaUaN/x5vzW2rzc+8Rw4TAQ/4KjB46IwK5VH+IlVeeeje/EoZRpiXvIqjFgK84QffqPztGI3VBLG1A==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/lines-and-columns": {
      "version": "1.2.4",
      "resolved": "https://registry.npmjs.org/lines-and-columns/-/lines-and-columns-1.2.4.tgz",
      "integrity": "sha512-7ylylesZQ/PV29jhEDl3Ufjo6ZX7gCqJr5F7PKrqc93v7fzSymt1BpwEU8nAUXs8qzzvqhbjhK5QZg6Mt/HkBg==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/locate-path": {
      "version": "5.0.0",
      "resolved": "https://registry.npmjs.org/locate-path/-/locate-path-5.0.0.tgz",
      "integrity": "sha512-t7hw9pI+WvuwNJXwk5zVHpyhIqzg2qTlklJOf0mVxGSbe3Fp2VieZcduNYjaLDoy6p9uGpQEGWG87WpMKlNq8g==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "p-locate": "^4.1.0"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/lodash.memoize": {
      "version": "4.1.2",
      "resolved": "https://registry.npmjs.org/lodash.memoize/-/lodash.memoize-4.1.2.tgz",
      "integrity": "sha512-t7j+NzmgnQzTAYXcsHYLgimltOV1MXHtlOWf6GjL9Kj8GK5FInw5JotxvbOs+IvV1/Dzo04/fCGfLVs7aXb4Ag==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/lru-cache": {
      "version": "5.1.1",
      "resolved": "https://registry.npmjs.org/lru-cache/-/lru-cache-5.1.1.tgz",
      "integrity": "sha512-KpNARQA3Iwv+jTA0utUVVbrh+Jlrr1Fv0e56GGzAFOXN7dk/FviaDW8LHmK52DlcH4WP2n6gI8vN1aesBFgo9w==",
      "dev": true,
      "license": "ISC",
      "dependencies": {
        "yallist": "^3.0.2"
      }
    },
    "node_modules/make-dir": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/make-dir/-/make-dir-4.0.0.tgz",
      "integrity": "sha512-hXdUTZYIVOt1Ex//jAQi+wTZZpUpwBj/0QsOzqegb3rGMMeJiSEu5xLHnYfBrRV4RH2+OCSOO95Is/7x1WJ4bw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "semver": "^7.5.3"
      },
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/make-dir/node_modules/semver": {
      "version": "7.7.3",
      "resolved": "https://registry.npmjs.org/semver/-/semver-7.7.3.tgz",
      "integrity": "sha512-SdsKMrI9TdgjdweUSR9MweHA4EJ8YxHn8DFaDisvhVlUOe4BF1tLD7GAj0lIqWVl+dPb/rExr0Btby5loQm20Q==",
      "dev": true,
      "license": "ISC",
      "bin": {
        "semver": "bin/semver.js"
      },
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/make-error": {
      "version": "1.3.6",
      "resolved": "https://registry.npmjs.org/make-error/-/make-error-1.3.6.tgz",
      "integrity": "sha512-s8UhlNe7vPKomQhC1qFelMokr/Sc3AgNbso3n74mVPA5LTZwkB9NlXf4XPamLxJE8h0gh73rM94xvwRT2CVInw==",
      "dev": true,
      "license": "ISC"
    },
    "node_modules/makeerror": {
      "version": "1.0.12",
      "resolved": "https://registry.npmjs.org/makeerror/-/makeerror-1.0.12.tgz",
      "integrity": "sha512-JmqCvUhmt43madlpFzG4BQzG2Z3m6tvQDNKdClZnO3VbIudJYmxsT0FNJMeiB2+JTSlTQTSbU8QdesVmwJcmLg==",
      "dev": true,
      "license": "BSD-3-Clause",
      "dependencies": {
        "tmpl": "1.0.5"
      }
    },
    "node_modules/merge-stream": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/merge-stream/-/merge-stream-2.0.0.tgz",
      "integrity": "sha512-abv/qOcuPfk3URPfDzmZU1LKmuw8kT+0nIHvKrKgFrwifol/doWcdA4ZqsWQ8ENrFKkd67Mfpo/LovbIUsbt3w==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/micromatch": {
      "version": "4.0.8",
      "resolved": "https://registry.npmjs.org/micromatch/-/micromatch-4.0.8.tgz",
      "integrity": "sha512-PXwfBhYu0hBCPw8Dn0E+WDYb7af3dSLVWKi3HGv84IdF4TyFoC0ysxFd0Goxw7nSv4T/PzEJQxsYsEiFCKo2BA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "braces": "^3.0.3",
        "picomatch": "^2.3.1"
      },
      "engines": {
        "node": ">=8.6"
      }
    },
    "node_modules/mimic-fn": {
      "version": "2.1.0",
      "resolved": "https://registry.npmjs.org/mimic-fn/-/mimic-fn-2.1.0.tgz",
      "integrity": "sha512-OqbOk5oEQeAZ8WXWydlu9HJjz9WVdEIvamMCcXmuqUYjTknH/sqsWvhQ3vgwKFRR1HpjvNBKQ37nbJgYzGqGcg==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/minimatch": {
      "version": "3.1.2",
      "resolved": "https://registry.npmjs.org/minimatch/-/minimatch-3.1.2.tgz",
      "integrity": "sha512-J7p63hRiAjw1NDEww1W7i37+ByIrOWO5XQQAzZ3VOcL0PNybwpfmV/N05zFAzwQ9USyEcX6t3UO+K5aqBQOIHw==",
      "license": "ISC",
      "dependencies": {
        "brace-expansion": "^1.1.7"
      },
      "engines": {
        "node": "*"
      }
    },
    "node_modules/minimist": {
      "version": "1.2.8",
      "resolved": "https://registry.npmjs.org/minimist/-/minimist-1.2.8.tgz",
      "integrity": "sha512-2yyAR8qBkN3YuheJanUpWC5U3bb5osDywNB8RzDVlDwDHbocAJveqqj1u8+SVD7jkWT4yvsHCpWqqWqAxb0zCA==",
      "dev": true,
      "license": "MIT",
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/ms": {
      "version": "2.1.3",
      "resolved": "https://registry.npmjs.org/ms/-/ms-2.1.3.tgz",
      "integrity": "sha512-6FlzubTLZG3J2a/NVCAleEhjzq5oxgHyaCU9yYXvcLsvoVaHJq/s5xXI6/XXP6tz7R9xAOtHnSO/tXtF3WRTlA==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/natural-compare": {
      "version": "1.4.0",
      "resolved": "https://registry.npmjs.org/natural-compare/-/natural-compare-1.4.0.tgz",
      "integrity": "sha512-OWND8ei3VtNC9h7V60qff3SVobHr996CTwgxubgyQYEpg290h9J0buyECNNJexkFm5sOajh5G116RYA1c8ZMSw==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/neo-async": {
      "version": "2.6.2",
      "resolved": "https://registry.npmjs.org/neo-async/-/neo-async-2.6.2.tgz",
      "integrity": "sha512-Yd3UES5mWCSqR+qNT93S3UoYUkqAZ9lLg8a7g9rimsWmYGK8cVToA4/sF3RrshdyV3sAGMXVUmpMYOw+dLpOuw==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/node-int64": {
      "version": "0.4.0",
      "resolved": "https://registry.npmjs.org/node-int64/-/node-int64-0.4.0.tgz",
      "integrity": "sha512-O5lz91xSOeoXP6DulyHfllpq+Eg00MWitZIbtPfoSEvqIHdl5gfcY6hYzDWnj0qD5tz52PI08u9qUvSVeUBeHw==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/node-releases": {
      "version": "2.0.27",
      "resolved": "https://registry.npmjs.org/node-releases/-/node-releases-2.0.27.tgz",
      "integrity": "sha512-nmh3lCkYZ3grZvqcCH+fjmQ7X+H0OeZgP40OierEaAptX4XofMh5kwNbWh7lBduUzCcV/8kZ+NDLCwm2iorIlA==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/normalize-path": {
      "version": "3.0.0",
      "resolved": "https://registry.npmjs.org/normalize-path/-/normalize-path-3.0.0.tgz",
      "integrity": "sha512-6eZs5Ls3WtCisHWp9S2GUy8dqkpGi4BVSz3GaqiE6ezub0512ESztXUwUB6C6IKbQkY2Pnb/mD4WYojCRwcwLA==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/npm-run-path": {
      "version": "4.0.1",
      "resolved": "https://registry.npmjs.org/npm-run-path/-/npm-run-path-4.0.1.tgz",
      "integrity": "sha512-S48WzZW777zhNIrn7gxOlISNAqi9ZC/uQFnRdbeIHhZhCA6UqpkOT8T1G7BvfdgP4Er8gF4sUbaS0i7QvIfCWw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "path-key": "^3.0.0"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/once": {
      "version": "1.4.0",
      "resolved": "https://registry.npmjs.org/once/-/once-1.4.0.tgz",
      "integrity": "sha512-lNaJgI+2Q5URQBkccEKHTQOPaXdUxnZZElQTZY0MFUAuaEqe1E+Nyvgdz/aIyNi6Z9MzO5dv1H8n58/GELp3+w==",
      "dev": true,
      "license": "ISC",
      "dependencies": {
        "wrappy": "1"
      }
    },
    "node_modules/onetime": {
      "version": "5.1.2",
      "resolved": "https://registry.npmjs.org/onetime/-/onetime-5.1.2.tgz",
      "integrity": "sha512-kbpaSSGJTWdAY5KPVeMOKXSrPtr8C8C7wodJbcsd51jRnmD+GZu8Y0VoU6Dm5Z4vWr0Ig/1NKuWRKf7j5aaYSg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "mimic-fn": "^2.1.0"
      },
      "engines": {
        "node": ">=6"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/p-limit": {
      "version": "3.1.0",
      "resolved": "https://registry.npmjs.org/p-limit/-/p-limit-3.1.0.tgz",
      "integrity": "sha512-TYOanM3wGwNGsZN2cVTYPArw454xnXj5qmWF1bEoAc4+cU/ol7GVh7odevjp1FNHduHc3KZMcFduxU5Xc6uJRQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "yocto-queue": "^0.1.0"
      },
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/p-locate": {
      "version": "4.1.0",
      "resolved": "https://registry.npmjs.org/p-locate/-/p-locate-4.1.0.tgz",
      "integrity": "sha512-R79ZZ/0wAxKGu3oYMlz8jy/kbhsNrS7SKZ7PxEHBgJ5+F2mtFW2fK2cOtBh1cHYkQsbzFV7I+EoRKe6Yt0oK7A==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "p-limit": "^2.2.0"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/p-locate/node_modules/p-limit": {
      "version": "2.3.0",
      "resolved": "https://registry.npmjs.org/p-limit/-/p-limit-2.3.0.tgz",
      "integrity": "sha512-//88mFWSJx8lxCzwdAABTJL2MyWB12+eIY7MDL2SqLmAkeKU9qxRvWuSyTjm3FUmpBEMuFfckAIqEaVGUDxb6w==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "p-try": "^2.0.0"
      },
      "engines": {
        "node": ">=6"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/p-try": {
      "version": "2.2.0",
      "resolved": "https://registry.npmjs.org/p-try/-/p-try-2.2.0.tgz",
      "integrity": "sha512-R4nPAVTAU0B9D35/Gk3uJf/7XYbQcyohSKdvAxIRSNghFl4e71hVoGnBNQz9cWaXxO2I10KTC+3jMdvvoKw6dQ==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/parse-json": {
      "version": "5.2.0",
      "resolved": "https://registry.npmjs.org/parse-json/-/parse-json-5.2.0.tgz",
      "integrity": "sha512-ayCKvm/phCGxOkYRSCM82iDwct8/EonSEgCSxWxD7ve6jHggsFl4fZVQBPRNgQoKiuV/odhFrGzQXZwbifC8Rg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/code-frame": "^7.0.0",
        "error-ex": "^1.3.1",
        "json-parse-even-better-errors": "^2.3.0",
        "lines-and-columns": "^1.1.6"
      },
      "engines": {
        "node": ">=8"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/path-exists": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/path-exists/-/path-exists-4.0.0.tgz",
      "integrity": "sha512-ak9Qy5Q7jYb2Wwcey5Fpvg2KoAc/ZIhLSLOSBmRmygPsGwkVVt0fZa0qrtMz+m6tJTAHfZQ8FnmB4MG4LWy7/w==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/path-is-absolute": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/path-is-absolute/-/path-is-absolute-1.0.1.tgz",
      "integrity": "sha512-AVbw3UJ2e9bq64vSaS9Am0fje1Pa8pbGqTTsmXfaIiMpnr5DlDhfJOuLj9Sf95ZPVDAUerDfEk88MPmPe7UCQg==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/path-key": {
      "version": "3.1.1",
      "resolved": "https://registry.npmjs.org/path-key/-/path-key-3.1.1.tgz",
      "integrity": "sha512-ojmeN0qd+y0jszEtoY48r0Peq5dwMEkIlCOu6Q5f41lfkswXuKtYrhgoTpLnyIcHm24Uhqx+5Tqm2InSwLhE6Q==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/path-parse": {
      "version": "1.0.7",
      "resolved": "https://registry.npmjs.org/path-parse/-/path-parse-1.0.7.tgz",
      "integrity": "sha512-LDJzPVEEEPR+y48z93A0Ed0yXb8pAByGWo/k5YYdYgpY2/2EsOsksJrq7lOHxryrVOn1ejG6oAp8ahvOIQD8sw==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/picocolors": {
      "version": "1.1.1",
      "resolved": "https://registry.npmjs.org/picocolors/-/picocolors-1.1.1.tgz",
      "integrity": "sha512-xceH2snhtb5M9liqDsmEw56le376mTZkEX/jEb/RxNFyegNul7eNslCXP9FDj/Lcu0X8KEyMceP2ntpaHrDEVA==",
      "dev": true,
      "license": "ISC"
    },
    "node_modules/picomatch": {
      "version": "2.3.1",
      "resolved": "https://registry.npmjs.org/picomatch/-/picomatch-2.3.1.tgz",
      "integrity": "sha512-JU3teHTNjmE2VCGFzuY8EXzCDVwEqB2a8fsIvwaStHhAWJEeVd1o1QD80CU6+ZdEXXSLbSsuLwJjkCBWqRQUVA==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=8.6"
      },
      "funding": {
        "url": "https://github.com/sponsors/jonschlinkert"
      }
    },
    "node_modules/pirates": {
      "version": "4.0.7",
      "resolved": "https://registry.npmjs.org/pirates/-/pirates-4.0.7.tgz",
      "integrity": "sha512-TfySrs/5nm8fQJDcBDuUng3VOUKsd7S+zqvbOTiGXHfxX4wK31ard+hoNuvkicM/2YFzlpDgABOevKSsB4G/FA==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">= 6"
      }
    },
    "node_modules/pkg-dir": {
      "version": "4.2.0",
      "resolved": "https://registry.npmjs.org/pkg-dir/-/pkg-dir-4.2.0.tgz",
      "integrity": "sha512-HRDzbaKjC+AOWVXxAU/x54COGeIv9eb+6CkDSQoNTt4XyWoIJvuPsXizxu/Fr23EiekbtZwmh1IcIG/l/a10GQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "find-up": "^4.0.0"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/pretty-format": {
      "version": "29.7.0",
      "resolved": "https://registry.npmjs.org/pretty-format/-/pretty-format-29.7.0.tgz",
      "integrity": "sha512-Pdlw/oPxN+aXdmM9R00JVC9WVFoCLTKJvDVLgmJ+qAffBMxsV85l/Lu7sNx4zSzPyoL2euImuEwHhOXdEgNFZQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jest/schemas": "^29.6.3",
        "ansi-styles": "^5.0.0",
        "react-is": "^18.0.0"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/pretty-format/node_modules/ansi-styles": {
      "version": "5.2.0",
      "resolved": "https://registry.npmjs.org/ansi-styles/-/ansi-styles-5.2.0.tgz",
      "integrity": "sha512-Cxwpt2SfTzTtXcfOlzGEee8O+c+MmUgGrNiBcXnuWxuFJHe6a5Hz7qwhwe5OgaSYI0IJvkLqWX1ASG+cJOkEiA==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/chalk/ansi-styles?sponsor=1"
      }
    },
    "node_modules/prompts": {
      "version": "2.4.2",
      "resolved": "https://registry.npmjs.org/prompts/-/prompts-2.4.2.tgz",
      "integrity": "sha512-NxNv/kLguCA7p3jE8oL2aEBsrJWgAakBpgmgK6lpPWV+WuOmY6r2/zbAVnP+T8bQlA0nzHXSJSJW0Hq7ylaD2Q==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "kleur": "^3.0.3",
        "sisteransi": "^1.0.5"
      },
      "engines": {
        "node": ">= 6"
      }
    },
    "node_modules/pure-rand": {
      "version": "6.1.0",
      "resolved": "https://registry.npmjs.org/pure-rand/-/pure-rand-6.1.0.tgz",
      "integrity": "sha512-bVWawvoZoBYpp6yIoQtQXHZjmz35RSVHnUOTefl8Vcjr8snTPY1wnpSPMWekcFwbxI6gtmT7rSYPFvz71ldiOA==",
      "dev": true,
      "funding": [
        {
          "type": "individual",
          "url": "https://github.com/sponsors/dubzzz"
        },
        {
          "type": "opencollective",
          "url": "https://opencollective.com/fast-check"
        }
      ],
      "license": "MIT"
    },
    "node_modules/react-is": {
      "version": "18.3.1",
      "resolved": "https://registry.npmjs.org/react-is/-/react-is-18.3.1.tgz",
      "integrity": "sha512-/LLMVyas0ljjAtoYiPqYiL8VWXzUUdThrmU5+n20DZv+a+ClRoevUzw5JxU+Ieh5/c87ytoTBV9G1FiKfNJdmg==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/require-directory": {
      "version": "2.1.1",
      "resolved": "https://registry.npmjs.org/require-directory/-/require-directory-2.1.1.tgz",
      "integrity": "sha512-fGxEI7+wsG9xrvdjsrlmL22OMTTiHRwAMroiEeMgq8gzoLC/PQr7RsRDSTLUg/bZAZtF+TVIkHc6/4RIKrui+Q==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/resolve": {
      "version": "1.22.11",
      "resolved": "https://registry.npmjs.org/resolve/-/resolve-1.22.11.tgz",
      "integrity": "sha512-RfqAvLnMl313r7c9oclB1HhUEAezcpLjz95wFH4LVuhk9JF/r22qmVP9AMmOU4vMX7Q8pN8jwNg/CSpdFnMjTQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "is-core-module": "^2.16.1",
        "path-parse": "^1.0.7",
        "supports-preserve-symlinks-flag": "^1.0.0"
      },
      "bin": {
        "resolve": "bin/resolve"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/resolve-cwd": {
      "version": "3.0.0",
      "resolved": "https://registry.npmjs.org/resolve-cwd/-/resolve-cwd-3.0.0.tgz",
      "integrity": "sha512-OrZaX2Mb+rJCpH/6CpSqt9xFVpN++x01XnN2ie9g6P5/3xelLAkXWVADpdz1IHD/KFfEXyE6V0U01OQ3UO2rEg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "resolve-from": "^5.0.0"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/resolve-from": {
      "version": "5.0.0",
      "resolved": "https://registry.npmjs.org/resolve-from/-/resolve-from-5.0.0.tgz",
      "integrity": "sha512-qYg9KP24dD5qka9J47d0aVky0N+b4fTU89LN9iDnjB5waksiC49rvMB0PrUJQGoTmH50XPiqOvAjDfaijGxYZw==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/resolve.exports": {
      "version": "2.0.3",
      "resolved": "https://registry.npmjs.org/resolve.exports/-/resolve.exports-2.0.3.tgz",
      "integrity": "sha512-OcXjMsGdhL4XnbShKpAcSqPMzQoYkYyhbEaeSko47MjRP9NfEQMhZkXL1DoFlt9LWQn4YttrdnV6X2OiyzBi+A==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/semver": {
      "version": "6.3.1",
      "resolved": "https://registry.npmjs.org/semver/-/semver-6.3.1.tgz",
      "integrity": "sha512-BR7VvDCVHO+q2xBEWskxS6DJE1qRnb7DxzUrogb71CWoSficBxYsiAGd+Kl0mmq/MprG9yArRkyrQxTO6XjMzA==",
      "license": "ISC",
      "bin": {
        "semver": "bin/semver.js"
      }
    },
    "node_modules/shebang-command": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/shebang-command/-/shebang-command-2.0.0.tgz",
      "integrity": "sha512-kHxr2zZpYtdmrN1qDjrrX/Z1rR1kG8Dx+gkpK1G4eXmvXswmcE1hTWBWYUzlraYw1/yZp6YuDY77YtvbN0dmDA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "shebang-regex": "^3.0.0"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/shebang-regex": {
      "version": "3.0.0",
      "resolved": "https://registry.npmjs.org/shebang-regex/-/shebang-regex-3.0.0.tgz",
      "integrity": "sha512-7++dFhtcx3353uBaq8DDR4NuxBetBzC7ZQOhmTQInHEd6bSrXdiEyzCvG07Z44UYdLShWUyXt5M/yhz8ekcb1A==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/signal-exit": {
      "version": "3.0.7",
      "resolved": "https://registry.npmjs.org/signal-exit/-/signal-exit-3.0.7.tgz",
      "integrity": "sha512-wnD2ZE+l+SPC/uoS0vXeE9L1+0wuaMqKlfz9AMUo38JsyLSBWSFcHR1Rri62LZc12vLr1gb3jl7iwQhgwpAbGQ==",
      "dev": true,
      "license": "ISC"
    },
    "node_modules/sisteransi": {
      "version": "1.0.5",
      "resolved": "https://registry.npmjs.org/sisteransi/-/sisteransi-1.0.5.tgz",
      "integrity": "sha512-bLGGlR1QxBcynn2d5YmDX4MGjlZvy2MRBDRNHLJ8VI6l6+9FUiyTFNJ0IveOSP0bcXgVDPRcfGqA0pjaqUpfVg==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/slash": {
      "version": "3.0.0",
      "resolved": "https://registry.npmjs.org/slash/-/slash-3.0.0.tgz",
      "integrity": "sha512-g9Q1haeby36OSStwb4ntCGGGaKsaVSjQ68fBxoQcutl5fS1vuY18H3wSt3jFyFtrkx+Kz0V1G85A4MyAdDMi2Q==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/source-map": {
      "version": "0.6.1",
      "resolved": "https://registry.npmjs.org/source-map/-/source-map-0.6.1.tgz",
      "integrity": "sha512-UjgapumWlbMhkBgzT7Ykc5YXUT46F0iKu8SGXq0bcwP5dz/h0Plj6enJqjz1Zbq2l5WaqYnrVbwWOWMyF3F47g==",
      "license": "BSD-3-Clause",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/source-map-support": {
      "version": "0.5.21",
      "resolved": "https://registry.npmjs.org/source-map-support/-/source-map-support-0.5.21.tgz",
      "integrity": "sha512-uBHU3L3czsIyYXKX88fdrGovxdSCoTGDRZ6SYXtSRxLZUzHg5P/66Ht6uoUlHu9EZod+inXhKo3qQgwXUT/y1w==",
      "license": "MIT",
      "dependencies": {
        "buffer-from": "^1.0.0",
        "source-map": "^0.6.0"
      }
    },
    "node_modules/sprintf-js": {
      "version": "1.0.3",
      "resolved": "https://registry.npmjs.org/sprintf-js/-/sprintf-js-1.0.3.tgz",
      "integrity": "sha512-D9cPgkvLlV3t3IzL0D0YLvGA9Ahk4PcvVwUbN0dSGr1aP0Nrt4AEnTUbuGvquEC0mA64Gqt1fzirlRs5ibXx8g==",
      "dev": true,
      "license": "BSD-3-Clause"
    },
    "node_modules/stack-utils": {
      "version": "2.0.6",
      "resolved": "https://registry.npmjs.org/stack-utils/-/stack-utils-2.0.6.tgz",
      "integrity": "sha512-XlkWvfIm6RmsWtNJx+uqtKLS8eqFbxUg0ZzLXqY0caEy9l7hruX8IpiDnjsLavoBgqCCR71TqWO8MaXYheJ3RQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "escape-string-regexp": "^2.0.0"
      },
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/string-length": {
      "version": "4.0.2",
      "resolved": "https://registry.npmjs.org/string-length/-/string-length-4.0.2.tgz",
      "integrity": "sha512-+l6rNN5fYHNhZZy41RXsYptCjA2Igmq4EG7kZAYFQI1E1VTXarr6ZPXBg6eq7Y6eK4FEhY6AJlyuFIb/v/S0VQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "char-regex": "^1.0.2",
        "strip-ansi": "^6.0.0"
      },
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/string-width": {
      "version": "4.2.3",
      "resolved": "https://registry.npmjs.org/string-width/-/string-width-4.2.3.tgz",
      "integrity": "sha512-wKyQRQpjJ0sIp62ErSZdGsjMJWsap5oRNihHhu6G7JVO/9jIB6UyevL+tXuOqrng8j/cxKTWyWUwvSTriiZz/g==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "emoji-regex": "^8.0.0",
        "is-fullwidth-code-point": "^3.0.0",
        "strip-ansi": "^6.0.1"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/strip-ansi": {
      "version": "6.0.1",
      "resolved": "https://registry.npmjs.org/strip-ansi/-/strip-ansi-6.0.1.tgz",
      "integrity": "sha512-Y38VPSHcqkFrCpFnQ9vuSXmquuv5oXOKpGeT6aGrr3o3Gc9AlVa6JBfUSOCnbxGGZF+/0ooI7KrPuUSztUdU5A==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "ansi-regex": "^5.0.1"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/strip-bom": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/strip-bom/-/strip-bom-4.0.0.tgz",
      "integrity": "sha512-3xurFv5tEgii33Zi8Jtp55wEIILR9eh34FAW00PZf+JnSsTmV/ioewSgQl97JHvgjoRGwPShsWm+IdrxB35d0w==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/strip-final-newline": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/strip-final-newline/-/strip-final-newline-2.0.0.tgz",
      "integrity": "sha512-BrpvfNAE3dcvq7ll3xVumzjKjZQ5tI1sEUIKr3Uoks0XUl45St3FlatVqef9prk4jRDzhW6WZg+3bk93y6pLjA==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/strip-json-comments": {
      "version": "3.1.1",
      "resolved": "https://registry.npmjs.org/strip-json-comments/-/strip-json-comments-3.1.1.tgz",
      "integrity": "sha512-6fPc+R4ihwqP6N/aIv2f1gMH8lOVtWQHoqC4yK6oSDVVocumAsfCqjkXnqiYMhmMwS/mEHLp7Vehlt3ql6lEig==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=8"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/supports-color": {
      "version": "7.2.0",
      "resolved": "https://registry.npmjs.org/supports-color/-/supports-color-7.2.0.tgz",
      "integrity": "sha512-qpCAvRl9stuOHveKsn7HncJRvv501qIacKzQlO/+Lwxc9+0q2wLyv4Dfvt80/DPn2pqOBsJdDiogXGR9+OvwRw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "has-flag": "^4.0.0"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/supports-preserve-symlinks-flag": {
      "version": "1.0.0",
      "resolved": "https://registry.npmjs.org/supports-preserve-symlinks-flag/-/supports-preserve-symlinks-flag-1.0.0.tgz",
      "integrity": "sha512-ot0WnXS9fgdkgIcePe6RHNk1WA8+muPa6cSjeR3V8K27q9BB1rTE3R1p7Hv0z1ZyAc8s6Vvv8DIyWf681MAt0w==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/test-exclude": {
      "version": "6.0.0",
      "resolved": "https://registry.npmjs.org/test-exclude/-/test-exclude-6.0.0.tgz",
      "integrity": "sha512-cAGWPIyOHU6zlmg88jwm7VRyXnMN7iV68OGAbYDk/Mh/xC/pzVPlQtY6ngoIH/5/tciuhGfvESU8GrHrcxD56w==",
      "dev": true,
      "license": "ISC",
      "dependencies": {
        "@istanbuljs/schema": "^0.1.2",
        "glob": "^7.1.4",
        "minimatch": "^3.0.4"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/tmpl": {
      "version": "1.0.5",
      "resolved": "https://registry.npmjs.org/tmpl/-/tmpl-1.0.5.tgz",
      "integrity": "sha512-3f0uOEAQwIqGuWW2MVzYg8fV/QNnc/IpuJNG837rLuczAaLVHslWHZQj4IGiEl5Hs3kkbhwL9Ab7Hrsmuj+Smw==",
      "dev": true,
      "license": "BSD-3-Clause"
    },
    "node_modules/to-regex-range": {
      "version": "5.0.1",
      "resolved": "https://registry.npmjs.org/to-regex-range/-/to-regex-range-5.0.1.tgz",
      "integrity": "sha512-65P7iz6X5yEr1cwcgvQxbbIw7Uk3gOy5dIdtZ4rDveLqhrdJP+Li/Hx6tyK0NEb+2GCyneCMJiGqrADCSNk8sQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "is-number": "^7.0.0"
      },
      "engines": {
        "node": ">=8.0"
      }
    },
    "node_modules/ts-jest": {
      "version": "29.4.6",
      "resolved": "https://registry.npmjs.org/ts-jest/-/ts-jest-29.4.6.tgz",
      "integrity": "sha512-fSpWtOO/1AjSNQguk43hb/JCo16oJDnMJf3CdEGNkqsEX3t0KX96xvyX1D7PfLCpVoKu4MfVrqUkFyblYoY4lA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "bs-logger": "^0.2.6",
        "fast-json-stable-stringify": "^2.1.0",
        "handlebars": "^4.7.8",
        "json5": "^2.2.3",
        "lodash.memoize": "^4.1.2",
        "make-error": "^1.3.6",
        "semver": "^7.7.3",
        "type-fest": "^4.41.0",
        "yargs-parser": "^21.1.1"
      },
      "bin": {
        "ts-jest": "cli.js"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || ^18.0.0 || >=20.0.0"
      },
      "peerDependencies": {
        "@babel/core": ">=7.0.0-beta.0 <8",
        "@jest/transform": "^29.0.0 || ^30.0.0",
        "@jest/types": "^29.0.0 || ^30.0.0",
        "babel-jest": "^29.0.0 || ^30.0.0",
        "jest": "^29.0.0 || ^30.0.0",
        "jest-util": "^29.0.0 || ^30.0.0",
        "typescript": ">=4.3 <6"
      },
      "peerDependenciesMeta": {
        "@babel/core": {
          "optional": true
        },
        "@jest/transform": {
          "optional": true
        },
        "@jest/types": {
          "optional": true
        },
        "babel-jest": {
          "optional": true
        },
        "esbuild": {
          "optional": true
        },
        "jest-util": {
          "optional": true
        }
      }
    },
    "node_modules/ts-jest/node_modules/semver": {
      "version": "7.7.3",
      "resolved": "https://registry.npmjs.org/semver/-/semver-7.7.3.tgz",
      "integrity": "sha512-SdsKMrI9TdgjdweUSR9MweHA4EJ8YxHn8DFaDisvhVlUOe4BF1tLD7GAj0lIqWVl+dPb/rExr0Btby5loQm20Q==",
      "dev": true,
      "license": "ISC",
      "bin": {
        "semver": "bin/semver.js"
      },
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/ts-jest/node_modules/type-fest": {
      "version": "4.41.0",
      "resolved": "https://registry.npmjs.org/type-fest/-/type-fest-4.41.0.tgz",
      "integrity": "sha512-TeTSQ6H5YHvpqVwBRcnLDCBnDOHWYu7IvGbHT6N8AOymcr9PJGjc1GTtiWZTYg0NCgYwvnYWEkVChQAr9bjfwA==",
      "dev": true,
      "license": "(MIT OR CC0-1.0)",
      "engines": {
        "node": ">=16"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/ts-node": {
      "version": "10.9.2",
      "resolved": "https://registry.npmjs.org/ts-node/-/ts-node-10.9.2.tgz",
      "integrity": "sha512-f0FFpIdcHgn8zcPSbf1dRevwt047YMnaiJM3u2w2RewrB+fob/zePZcrOyQoLMMO7aBIddLcQIEK5dYjkLnGrQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@cspotcode/source-map-support": "^0.8.0",
        "@tsconfig/node10": "^1.0.7",
        "@tsconfig/node12": "^1.0.7",
        "@tsconfig/node14": "^1.0.0",
        "@tsconfig/node16": "^1.0.2",
        "acorn": "^8.4.1",
        "acorn-walk": "^8.1.1",
        "arg": "^4.1.0",
        "create-require": "^1.1.0",
        "diff": "^4.0.1",
        "make-error": "^1.1.1",
        "v8-compile-cache-lib": "^3.0.1",
        "yn": "3.1.1"
      },
      "bin": {
        "ts-node": "dist/bin.js",
        "ts-node-cwd": "dist/bin-cwd.js",
        "ts-node-esm": "dist/bin-esm.js",
        "ts-node-script": "dist/bin-script.js",
        "ts-node-transpile-only": "dist/bin-transpile.js",
        "ts-script": "dist/bin-script-deprecated.js"
      },
      "peerDependencies": {
        "@swc/core": ">=1.2.50",
        "@swc/wasm": ">=1.2.50",
        "@types/node": "*",
        "typescript": ">=2.7"
      },
      "peerDependenciesMeta": {
        "@swc/core": {
          "optional": true
        },
        "@swc/wasm": {
          "optional": true
        }
      }
    },
    "node_modules/type-detect": {
      "version": "4.0.8",
      "resolved": "https://registry.npmjs.org/type-detect/-/type-detect-4.0.8.tgz",
      "integrity": "sha512-0fr/mIH1dlO+x7TlcMy+bIDqKPsw/70tVyeHW787goQjhmqaZe10uwLujubK9q9Lg6Fiho1KUKDYz0Z7k7g5/g==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=4"
      }
    },
    "node_modules/type-fest": {
      "version": "0.21.3",
      "resolved": "https://registry.npmjs.org/type-fest/-/type-fest-0.21.3.tgz",
      "integrity": "sha512-t0rzBq87m3fVcduHDUFhKmyyX+9eo6WQjZvf51Ea/M0Q7+T374Jp1aUiyUl0GKxp8M/OETVHSDvmkyPgvX+X2w==",
      "dev": true,
      "license": "(MIT OR CC0-1.0)",
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/typescript": {
      "version": "5.5.4",
      "resolved": "https://registry.npmjs.org/typescript/-/typescript-5.5.4.tgz",
      "integrity": "sha512-Mtq29sKDAEYP7aljRgtPOpTvOfbwRWlS6dPRzwjdE+C0R4brX/GUyhHSecbHMFLNBLcJIPt9nl9yG5TZ1weH+Q==",
      "dev": true,
      "license": "Apache-2.0",
      "bin": {
        "tsc": "bin/tsc",
        "tsserver": "bin/tsserver"
      },
      "engines": {
        "node": ">=14.17"
      }
    },
    "node_modules/uglify-js": {
      "version": "3.19.3",
      "resolved": "https://registry.npmjs.org/uglify-js/-/uglify-js-3.19.3.tgz",
      "integrity": "sha512-v3Xu+yuwBXisp6QYTcH4UbH+xYJXqnq2m/LtQVWKWzYc1iehYnLixoQDN9FH6/j9/oybfd6W9Ghwkl8+UMKTKQ==",
      "dev": true,
      "license": "BSD-2-Clause",
      "optional": true,
      "bin": {
        "uglifyjs": "bin/uglifyjs"
      },
      "engines": {
        "node": ">=0.8.0"
      }
    },
    "node_modules/undici-types": {
      "version": "5.26.5",
      "resolved": "https://registry.npmjs.org/undici-types/-/undici-types-5.26.5.tgz",
      "integrity": "sha512-JlCMO+ehdEIKqlFxk6IfVoAUVmgz7cU7zD/h9XZ0qzeosSHmUJVOzSQvvYSYWXkFXC+IfLKSIffhv0sVZup6pA==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/update-browserslist-db": {
      "version": "1.2.3",
      "resolved": "https://registry.npmjs.org/update-browserslist-db/-/update-browserslist-db-1.2.3.tgz",
      "integrity": "sha512-Js0m9cx+qOgDxo0eMiFGEueWztz+d4+M3rGlmKPT+T4IS/jP4ylw3Nwpu6cpTTP8R1MAC1kF4VbdLt3ARf209w==",
      "dev": true,
      "funding": [
        {
          "type": "opencollective",
          "url": "https://opencollective.com/browserslist"
        },
        {
          "type": "tidelift",
          "url": "https://tidelift.com/funding/github/npm/browserslist"
        },
        {
          "type": "github",
          "url": "https://github.com/sponsors/ai"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "escalade": "^3.2.0",
        "picocolors": "^1.1.1"
      },
      "bin": {
        "update-browserslist-db": "cli.js"
      },
      "peerDependencies": {
        "browserslist": ">= 4.21.0"
      }
    },
    "node_modules/v8-compile-cache-lib": {
      "version": "3.0.1",
      "resolved": "https://registry.npmjs.org/v8-compile-cache-lib/-/v8-compile-cache-lib-3.0.1.tgz",
      "integrity": "sha512-wa7YjyUGfNZngI/vtK0UHAN+lgDCxBPCylVXGp0zu59Fz5aiGtNXaq3DhIov063MorB+VfufLh3JlF2KdTK3xg==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/v8-to-istanbul": {
      "version": "9.3.0",
      "resolved": "https://registry.npmjs.org/v8-to-istanbul/-/v8-to-istanbul-9.3.0.tgz",
      "integrity": "sha512-kiGUalWN+rgBJ/1OHZsBtU4rXZOfj/7rKQxULKlIzwzQSvMJUUNgPwJEEh7gU6xEVxC0ahoOBvN2YI8GH6FNgA==",
      "dev": true,
      "license": "ISC",
      "dependencies": {
        "@jridgewell/trace-mapping": "^0.3.12",
        "@types/istanbul-lib-coverage": "^2.0.1",
        "convert-source-map": "^2.0.0"
      },
      "engines": {
        "node": ">=10.12.0"
      }
    },
    "node_modules/walker": {
      "version": "1.0.8",
      "resolved": "https://registry.npmjs.org/walker/-/walker-1.0.8.tgz",
      "integrity": "sha512-ts/8E8l5b7kY0vlWLewOkDXMmPdLcVV4GmOQLyxuSswIJsweeFZtAsMF7k1Nszz+TYBQrlYRmzOnr398y1JemQ==",
      "dev": true,
      "license": "Apache-2.0",
      "dependencies": {
        "makeerror": "1.0.12"
      }
    },
    "node_modules/which": {
      "version": "2.0.2",
      "resolved": "https://registry.npmjs.org/which/-/which-2.0.2.tgz",
      "integrity": "sha512-BLI3Tl1TW3Pvl70l3yq3Y64i+awpwXqsGBYWkkqMtnbXgrMD+yj7rhW0kuEDxzJaYXGjEW5ogapKNMEKNMjibA==",
      "dev": true,
      "license": "ISC",
      "dependencies": {
        "isexe": "^2.0.0"
      },
      "bin": {
        "node-which": "bin/node-which"
      },
      "engines": {
        "node": ">= 8"
      }
    },
    "node_modules/wordwrap": {
      "version": "1.0.0",
      "resolved": "https://registry.npmjs.org/wordwrap/-/wordwrap-1.0.0.tgz",
      "integrity": "sha512-gvVzJFlPycKc5dZN4yPkP8w7Dc37BtP1yczEneOb4uq34pXZcvrtRTmWV8W+Ume+XCxKgbjM+nevkyFPMybd4Q==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/wrap-ansi": {
      "version": "7.0.0",
      "resolved": "https://registry.npmjs.org/wrap-ansi/-/wrap-ansi-7.0.0.tgz",
      "integrity": "sha512-YVGIj2kamLSTxw6NsZjoBxfSwsn0ycdesmc4p+Q21c5zPuZ1pl+NfxVdxPtdHvmNVOQ6XSYG4AUtyt/Fi7D16Q==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "ansi-styles": "^4.0.0",
        "string-width": "^4.1.0",
        "strip-ansi": "^6.0.0"
      },
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/chalk/wrap-ansi?sponsor=1"
      }
    },
    "node_modules/wrappy": {
      "version": "1.0.2",
      "resolved": "https://registry.npmjs.org/wrappy/-/wrappy-1.0.2.tgz",
      "integrity": "sha512-l4Sp/DRseor9wL6EvV2+TuQn63dMkPjZ/sp9XkghTEbV9KlPS1xUsZ3u7/IQO4wxtcFB4bgpQPRcR3QCvezPcQ==",
      "dev": true,
      "license": "ISC"
    },
    "node_modules/write-file-atomic": {
      "version": "4.0.2",
      "resolved": "https://registry.npmjs.org/write-file-atomic/-/write-file-atomic-4.0.2.tgz",
      "integrity": "sha512-7KxauUdBmSdWnmpaGFg+ppNjKF8uNLry8LyzjauQDOVONfFLNKrKvQOxZ/VuTIcS/gge/YNahf5RIIQWTSarlg==",
      "dev": true,
      "license": "ISC",
      "dependencies": {
        "imurmurhash": "^0.1.4",
        "signal-exit": "^3.0.7"
      },
      "engines": {
        "node": "^12.13.0 || ^14.15.0 || >=16.0.0"
      }
    },
    "node_modules/y18n": {
      "version": "5.0.8",
      "resolved": "https://registry.npmjs.org/y18n/-/y18n-5.0.8.tgz",
      "integrity": "sha512-0pfFzegeDWJHJIAmTLRP2DwHjdF5s7jo9tuztdQxAhINCdvS+3nGINqPd00AphqJR/0LhANUS6/+7SCb98YOfA==",
      "dev": true,
      "license": "ISC",
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/yallist": {
      "version": "3.1.1",
      "resolved": "https://registry.npmjs.org/yallist/-/yallist-3.1.1.tgz",
      "integrity": "sha512-a4UGQaWPH59mOXUYnAG2ewncQS4i4F43Tv3JoAM+s2VDAmS9NsK8GpDMLrCHPksFT7h3K6TOoUNn2pb7RoXx4g==",
      "dev": true,
      "license": "ISC"
    },
    "node_modules/yargs": {
      "version": "17.7.2",
      "resolved": "https://registry.npmjs.org/yargs/-/yargs-17.7.2.tgz",
      "integrity": "sha512-7dSzzRQ++CKnNI/krKnYRV7JKKPUXMEh61soaHKg9mrWEhzFWhFnxPxGl+69cD1Ou63C13NUPCnmIcrvqCuM6w==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "cliui": "^8.0.1",
        "escalade": "^3.1.1",
        "get-caller-file": "^2.0.5",
        "require-directory": "^2.1.1",
        "string-width": "^4.2.3",
        "y18n": "^5.0.5",
        "yargs-parser": "^21.1.1"
      },
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/yargs-parser": {
      "version": "21.1.1",
      "resolved": "https://registry.npmjs.org/yargs-parser/-/yargs-parser-21.1.1.tgz",
      "integrity": "sha512-tVpsJW7DdjecAiFpbIB1e3qxIQsE6NoPc5/eTdrbbIC4h0LVsWhnoa3g+m2HclBIujHzsxZ4VJVA+GUuc2/LBw==",
      "dev": true,
      "license": "ISC",
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/yn": {
      "version": "3.1.1",
      "resolved": "https://registry.npmjs.org/yn/-/yn-3.1.1.tgz",
      "integrity": "sha512-Ux4ygGWsu2c7isFWe8Yu1YluJmqVhxqK2cLXNQA5AcC3QfbGNpM7fu0Y8b/z16pXLnFxZYvWhd3fhBY9DLmC6Q==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/yocto-queue": {
      "version": "0.1.0",
      "resolved": "https://registry.npmjs.org/yocto-queue/-/yocto-queue-0.1.0.tgz",
      "integrity": "sha512-rVksvsnNCdJ/ohGc6xgPwyN8eheCxsiLM8mxuE/t/mOVqJewPuO1miLpTHQiRgTKCLexL4MeAFVagts7HmNZ2Q==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    }
  }
}

```

---

## aws/cdk/package.json

```json
{
  "name": "newsinsight-aws-cdk",
  "version": "1.0.0",
  "bin": {
    "newsinsight": "bin/newsinsight.js"
  },
  "scripts": {
    "build": "tsc",
    "watch": "tsc -w",
    "test": "jest",
    "cdk": "cdk",
    "deploy": "cdk deploy --all --require-approval never",
    "deploy:prod": "cdk deploy --all --require-approval never -c env=prod",
    "destroy": "cdk destroy --all",
    "synth": "cdk synth"
  },
  "devDependencies": {
    "@types/jest": "^29.5.12",
    "@types/node": "20.14.9",
    "jest": "^29.7.0",
    "ts-jest": "^29.1.5",
    "aws-cdk": "2.173.0",
    "ts-node": "^10.9.2",
    "typescript": "~5.5.3"
  },
  "dependencies": {
    "aws-cdk-lib": "2.173.0",
    "constructs": "^10.0.0",
    "source-map-support": "^0.5.21"
  }
}

```

---

## aws/cdk/tsconfig.json

```json
{
  "compilerOptions": {
    "target": "ES2020",
    "module": "commonjs",
    "lib": ["es2020"],
    "declaration": true,
    "strict": true,
    "noImplicitAny": true,
    "strictNullChecks": true,
    "noImplicitThis": true,
    "alwaysStrict": true,
    "noUnusedLocals": false,
    "noUnusedParameters": false,
    "noImplicitReturns": true,
    "noFallthroughCasesInSwitch": false,
    "inlineSourceMap": true,
    "inlineSources": true,
    "experimentalDecorators": true,
    "strictPropertyInitialization": false,
    "typeRoots": ["./node_modules/@types"],
    "outDir": "dist",
    "rootDir": "."
  },
  "exclude": ["node_modules", "cdk.out"]
}

```

---

## aws/opencode.json

```json
{
  "$schema": "https://opencode.ai/config.json",
  "mcp": {
    "aws-core": {
      "type": "local",
      "command": [
        "/home/nodove/.local/bin/uvx",
        "awslabs.aws-documentation-mcp-server@latest"
      ],
      "environment": {
        "AWS_PROFILE": "default",
        "AWS_REGION": "ap-northeast-2",
        "FASTMCP_LOG_LEVEL": "ERROR"
      }
    },
    "aws-cdk": {
      "type": "local",
      "command": [
        "/home/nodove/.local/bin/uvx",
        "awslabs.cdk-mcp-server@latest"
      ],
      "environment": {
        "AWS_PROFILE": "default",
        "AWS_REGION": "ap-northeast-2",
        "FASTMCP_LOG_LEVEL": "ERROR"
      }
    }
  }
}

```

---

## backend/admin-dashboard/api/dependencies.py

```py
"""
FastAPI Dependencies -  
"""

import os
from functools import lru_cache
from typing import Callable

from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer

from .models.schemas import User, UserRole
from .services.audit_service import AuditService
from .services.auth_service import AuthService
from .services.document_service import DocumentService
from .services.environment_service import EnvironmentService
from .services.script_service import ScriptService
from .services.health_service import HealthService
from .services.data_source_service import DataSourceService
from .services.database_service import DatabaseService
from .services.kafka_service import KafkaService

# OAuth2 
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="/api/v1/admin/auth/token")

#   
PROJECT_ROOT = os.environ.get(
    "PROJECT_ROOT",
    os.path.dirname(
        os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
    ),
)
CONFIG_DIR = os.environ.get(
    "ADMIN_CONFIG_DIR",
    os.path.join(os.path.dirname(os.path.dirname(__file__)), "config"),
)
# SECRET_KEY    
#       (: openssl rand -hex 32)
_default_secret = "your-secret-key-change-in-production"
SECRET_KEY = os.environ.get("ADMIN_SECRET_KEY", _default_secret)

#        
if SECRET_KEY == _default_secret:
    import warnings
    warnings.warn(
        " SECURITY WARNING: Using default SECRET_KEY! "
        "Set ADMIN_SECRET_KEY environment variable in production. "
        "Generate a secure key with: openssl rand -hex 32",
        UserWarning,
    )


@lru_cache()
def get_auth_service() -> AuthService:
    """  """
    return AuthService(
        config_dir=CONFIG_DIR,
        secret_key=SECRET_KEY,
    )


@lru_cache()
def get_environment_service() -> EnvironmentService:
    """  """
    return EnvironmentService(
        project_root=PROJECT_ROOT,
        config_dir=CONFIG_DIR,
    )


@lru_cache()
def get_script_service() -> ScriptService:
    """  """
    return ScriptService(
        project_root=PROJECT_ROOT,
        config_dir=CONFIG_DIR,
    )


@lru_cache()
def get_document_service() -> DocumentService:
    """  """
    return DocumentService(
        project_root=PROJECT_ROOT,
        config_dir=CONFIG_DIR,
    )


@lru_cache()
def get_audit_service() -> AuditService:
    """  """
    return AuditService(config_dir=CONFIG_DIR)


@lru_cache()
def get_health_service() -> HealthService:
    """   """
    return HealthService(
        project_root=PROJECT_ROOT,
        config_dir=CONFIG_DIR,
    )


@lru_cache()
def get_data_source_service() -> DataSourceService:
    """   """
    return DataSourceService(
        project_root=PROJECT_ROOT,
        config_dir=CONFIG_DIR,
    )


@lru_cache()
def get_database_service() -> DatabaseService:
    """   """
    return DatabaseService(
        project_root=PROJECT_ROOT,
        config_dir=CONFIG_DIR,
    )


@lru_cache()
def get_kafka_service() -> KafkaService:
    """Kafka/Redpanda   """
    return KafkaService(
        project_root=PROJECT_ROOT,
        config_dir=CONFIG_DIR,
    )


async def get_current_user(
    token: str = Depends(oauth2_scheme),
    auth_service: AuthService = Depends(get_auth_service),
) -> User:
    """   """
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )

    token_data = auth_service.verify_token(token)
    if not token_data:
        raise credentials_exception

    user = auth_service.get_user(token_data.user_id)
    if not user:
        raise credentials_exception

    if not user.is_active:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="User account is disabled",
        )

    return user


def require_role(required_role: UserRole) -> Callable:
    """    """

    async def role_checker(
        current_user: User = Depends(get_current_user),
        auth_service: AuthService = Depends(get_auth_service),
    ) -> User:
        if not auth_service.check_permission(current_user.role, required_role):
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail=f"Requires {required_role.value} permission or higher",
            )
        return current_user

    return role_checker


# Optional authentication - returns anonymous user if not authenticated
oauth2_scheme_optional = OAuth2PasswordBearer(tokenUrl="/api/v1/admin/auth/token", auto_error=False)


async def get_current_user_optional(
    token: str = Depends(oauth2_scheme_optional),
    auth_service: AuthService = Depends(get_auth_service),
) -> User:
    """    ( -     )"""
    from datetime import datetime
    
    anonymous_user = User(
        id="anonymous",
        username="anonymous",
        email="anonymous@local",
        role=UserRole.VIEWER,
        is_active=True,
        created_at=datetime.utcnow(),
    )
    
    if not token:
        return anonymous_user

    token_data = auth_service.verify_token(token)
    if not token_data:
        return anonymous_user

    user = auth_service.get_user(token_data.user_id)
    if not user or not user.is_active:
        return anonymous_user

    return user

```

---

## backend/admin-dashboard/api/main.py

```py
"""
Admin Dashboard API - FastAPI  
"""

import os
from contextlib import asynccontextmanager
from datetime import datetime

from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from fastapi.staticfiles import StaticFiles

from .models.schemas import HealthCheck
from .routers import (
    auth,
    audit,
    documents,
    environments,
    scripts,
    public_auth,
    llm_providers,
    health_monitor,
    data_sources,
    ml_addons,
    ml_training,
    databases,
    kafka,
    config_export,
)

#  
VERSION = "1.0.0"


@asynccontextmanager
async def lifespan(app: FastAPI):
    """  """
    #  
    print(f" Admin Dashboard API v{VERSION} starting...")
    yield
    #  
    print(" Admin Dashboard API shutting down...")


# FastAPI  
app = FastAPI(
    title="NewsInsight Admin Dashboard API",
    description=" TUI/Web Admin  API",
    version=VERSION,
    docs_url="/api/v1/admin/docs",
    redoc_url="/api/v1/admin/redoc",
    openapi_url="/api/v1/admin/openapi.json",
    lifespan=lifespan,
)

# CORS 
CORS_ORIGINS = os.environ.get(
    "CORS_ORIGINS", "http://localhost:3000,http://localhost:5173,http://localhost:8080"
).split(",")

app.add_middleware(
    CORSMiddleware,
    allow_origins=CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


#   
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """  """
    return JSONResponse(
        status_code=500,
        content={
            "detail": str(exc),
            "type": type(exc).__name__,
        },
    )


# API  
API_PREFIX = "/api/v1/admin"
PUBLIC_API_PREFIX = "/api/v1"

# Admin   (/api/v1/admin/...)
app.include_router(auth.router, prefix=API_PREFIX)
app.include_router(environments.router, prefix=API_PREFIX)
app.include_router(scripts.router, prefix=API_PREFIX)
app.include_router(documents.router, prefix=API_PREFIX)
app.include_router(audit.router, prefix=API_PREFIX)
app.include_router(llm_providers.router, prefix=API_PREFIX)
app.include_router(health_monitor.router, prefix=API_PREFIX)
app.include_router(data_sources.router, prefix=API_PREFIX)
app.include_router(ml_addons.router, prefix=API_PREFIX)
app.include_router(ml_training.router, prefix=API_PREFIX)
app.include_router(databases.router, prefix=API_PREFIX)
app.include_router(kafka.router, prefix=API_PREFIX)
app.include_router(config_export.router, prefix=API_PREFIX)

#   (/api/v1/auth/...)
app.include_router(public_auth.router, prefix=PUBLIC_API_PREFIX)


#  
@app.get("/health", response_model=HealthCheck, tags=["Health"])
@app.get(f"{API_PREFIX}/health", response_model=HealthCheck, tags=["Health"])
async def health_check():
    """"""
    return HealthCheck(
        status="healthy",
        version=VERSION,
        timestamp=datetime.utcnow(),
    )


#  
@app.get("/", tags=["Root"])
async def root():
    """ """
    return {
        "name": "NewsInsight Admin Dashboard API",
        "version": VERSION,
        "docs": "/api/v1/admin/docs",
        "health": "/health",
    }


#    (Web UI)
WEB_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), "web", "dist")
if os.path.exists(WEB_DIR):
    app.mount("/", StaticFiles(directory=WEB_DIR, html=True), name="static")


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "api.main:app",
        host="0.0.0.0",
        port=int(os.environ.get("PORT", 8888)),
        reload=True,
    )

```

---

## backend/admin-dashboard/api/models/__init__.py

```py
# Admin Dashboard Models

```

---

## backend/admin-dashboard/api/models/schemas.py

```py
"""
Admin Dashboard - Pydantic Schemas
, , ,      
"""

from datetime import datetime
from enum import Enum
from typing import Any, Optional

from pydantic import BaseModel, Field


# ============================================================================
# Enums
# ============================================================================
class EnvironmentType(str, Enum):
    ZEROTRUST = "zerotrust"
    LOCAL = "local"
    GCP = "gcp"
    AWS = "aws"
    PRODUCTION = "production"
    STAGING = "staging"


class RiskLevel(str, Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class TaskStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    SUCCESS = "success"
    FAILED = "failed"
    CANCELLED = "cancelled"


class UserRole(str, Enum):
    USER = "user"  #   ()
    VIEWER = "viewer"  #  -  
    OPERATOR = "operator"  #  - 
    ADMIN = "admin"  #  -  


class ServiceStatus(str, Enum):
    UP = "up"
    DOWN = "down"
    STARTING = "starting"
    STOPPING = "stopping"
    UNKNOWN = "unknown"


# ============================================================================
# Environment / Profile Models
# ============================================================================
class EnvironmentBase(BaseModel):
    name: str = Field(..., description="  (: zerotrust, local)")
    env_type: EnvironmentType = Field(..., description=" ")
    description: Optional[str] = Field(None, description=" ")
    compose_file: str = Field(..., description="Docker Compose  ")
    env_file: Optional[str] = Field(None, description="   ")
    is_active: bool = Field(True, description=" ")
    priority: int = Field(0, description=" (  )")


class EnvironmentCreate(EnvironmentBase):
    pass


class EnvironmentUpdate(BaseModel):
    name: Optional[str] = None
    description: Optional[str] = None
    compose_file: Optional[str] = None
    env_file: Optional[str] = None
    is_active: Optional[bool] = None
    priority: Optional[int] = None


class Environment(EnvironmentBase):
    id: str = Field(..., description=" ID")
    created_at: datetime
    updated_at: datetime

    class Config:
        from_attributes = True


# ============================================================================
# Environment Variable Models
# ============================================================================
class EnvVariableBase(BaseModel):
    key: str = Field(..., description="  ")
    value: str = Field(..., description="  ")
    is_secret: bool = Field(False, description="  ")
    description: Optional[str] = Field(None, description=" ")


class EnvVariableCreate(EnvVariableBase):
    environment_id: str


class EnvVariableUpdate(BaseModel):
    value: Optional[str] = None
    is_secret: Optional[bool] = None
    description: Optional[str] = None
    comment: Optional[str] = Field(None, description=" ")


class EnvVariable(EnvVariableBase):
    id: str
    environment_id: str
    masked_value: str = Field(..., description=" ")
    created_at: datetime
    updated_at: datetime

    class Config:
        from_attributes = True


class EnvVariableHistory(BaseModel):
    id: str
    variable_id: str
    old_value: str
    new_value: str
    changed_by: str
    comment: Optional[str]
    changed_at: datetime


# ============================================================================
# Script / Task Models
# ============================================================================
class ScriptParameter(BaseModel):
    name: str = Field(..., description=" ")
    param_type: str = Field(
        "string", description="  (string, boolean, number)"
    )
    required: bool = Field(False, description=" ")
    default: Optional[Any] = Field(None, description="")
    description: Optional[str] = Field(None, description=" ")


class ScriptBase(BaseModel):
    name: str = Field(..., description=" ")
    description: Optional[str] = Field(None, description=" ")
    command: str = Field(..., description=" ")
    working_dir: Optional[str] = Field(None, description=" ")
    risk_level: RiskLevel = Field(RiskLevel.LOW, description="")
    estimated_duration: Optional[int] = Field(None, description="  ()")
    allowed_environments: list[str] = Field(
        default_factory=list, description="  "
    )
    required_role: UserRole = Field(UserRole.OPERATOR, description=" ")
    parameters: list[ScriptParameter] = Field(
        default_factory=list, description=" "
    )
    pre_hooks: list[str] = Field(default_factory=list, description="  ")
    post_hooks: list[str] = Field(default_factory=list, description="  ")
    tags: list[str] = Field(default_factory=list, description="")


class ScriptCreate(ScriptBase):
    pass


class ScriptUpdate(BaseModel):
    name: Optional[str] = None
    description: Optional[str] = None
    command: Optional[str] = None
    working_dir: Optional[str] = None
    risk_level: Optional[RiskLevel] = None
    estimated_duration: Optional[int] = None
    allowed_environments: Optional[list[str]] = None
    required_role: Optional[UserRole] = None
    parameters: Optional[list[ScriptParameter]] = None
    pre_hooks: Optional[list[str]] = None
    post_hooks: Optional[list[str]] = None
    tags: Optional[list[str]] = None


class Script(ScriptBase):
    id: str
    created_at: datetime
    updated_at: datetime

    class Config:
        from_attributes = True


# ============================================================================
# Task Execution Models
# ============================================================================
class TaskExecutionRequest(BaseModel):
    script_id: str = Field(..., description="  ID")
    environment_id: str = Field(..., description="  ID")
    parameters: dict[str, Any] = Field(
        default_factory=dict, description=" "
    )


class TaskExecution(BaseModel):
    id: str = Field(..., description=" ID")
    script_id: str
    script_name: str
    environment_id: str
    environment_name: str
    status: TaskStatus
    parameters: dict[str, Any]
    started_at: datetime
    finished_at: Optional[datetime] = None
    executed_by: str
    exit_code: Optional[int] = None
    error_message: Optional[str] = None


class TaskLog(BaseModel):
    execution_id: str
    timestamp: datetime
    level: str  # INFO, WARN, ERROR
    message: str


# ============================================================================
# Service Status Models
# ============================================================================
class ContainerInfo(BaseModel):
    name: str
    image: str
    status: ServiceStatus
    health: Optional[str] = None
    ports: list[str] = Field(default_factory=list)
    created_at: Optional[datetime] = None
    started_at: Optional[datetime] = None


class EnvironmentStatus(BaseModel):
    environment_id: str
    environment_name: str
    containers: list[ContainerInfo]
    total_containers: int
    running_containers: int
    last_deployment: Optional[datetime] = None
    deployed_by: Optional[str] = None


# ============================================================================
# Document Models
# ============================================================================
class DocumentCategory(str, Enum):
    DEPLOYMENT = "deployment"
    TROUBLESHOOTING = "troubleshooting"
    ARCHITECTURE = "architecture"
    RUNBOOK = "runbook"
    GENERAL = "general"


class DocumentBase(BaseModel):
    title: str = Field(..., description=" ")
    file_path: str = Field(..., description=" ")
    category: DocumentCategory = Field(DocumentCategory.GENERAL, description="")
    tags: list[str] = Field(default_factory=list, description="")
    related_environments: list[str] = Field(
        default_factory=list, description=" "
    )
    related_scripts: list[str] = Field(
        default_factory=list, description=" "
    )


class Document(DocumentBase):
    id: str
    content: Optional[str] = Field(None, description="Markdown ")
    last_modified: datetime

    class Config:
        from_attributes = True


# ============================================================================
# Audit Log Models
# ============================================================================
class AuditAction(str, Enum):
    LOGIN = "login"
    LOGOUT = "logout"
    VIEW = "view"
    CREATE = "create"
    UPDATE = "update"
    DELETE = "delete"
    EXECUTE = "execute"
    DEPLOY = "deploy"
    ROLLBACK = "rollback"


class AuditLog(BaseModel):
    id: str
    user_id: str
    username: str
    action: AuditAction
    resource_type: str  # environment, script, variable, etc.
    resource_id: Optional[str] = None
    resource_name: Optional[str] = None
    environment_id: Optional[str] = None
    environment_name: Optional[str] = None
    details: dict[str, Any] = Field(default_factory=dict)
    ip_address: Optional[str] = None
    user_agent: Optional[str] = None
    timestamp: datetime
    success: bool = True
    error_message: Optional[str] = None


class AuditLogFilter(BaseModel):
    user_id: Optional[str] = None
    action: Optional[AuditAction] = None
    resource_type: Optional[str] = None
    environment_id: Optional[str] = None
    start_date: Optional[datetime] = None
    end_date: Optional[datetime] = None
    success: Optional[bool] = None


# ============================================================================
# User / Auth Models
# ============================================================================
class UserBase(BaseModel):
    username: str
    email: Optional[str] = None
    role: UserRole = Field(UserRole.USER)
    is_active: bool = True


class UserCreate(UserBase):
    password: str


class UserRegister(BaseModel):
    """   """

    username: str = Field(
        ..., min_length=3, max_length=50, description=" (3-50)"
    )
    email: str = Field(..., description=" ")
    password: str = Field(..., min_length=8, description=" (8 )")


class User(UserBase):
    id: str
    created_at: datetime
    last_login: Optional[datetime] = None
    password_change_required: bool = Field(False, description="   ")

    class Config:
        from_attributes = True


class SetupStatus(BaseModel):
    """  """

    setup_required: bool = Field(..., description="   ")
    has_users: bool = Field(..., description="  ")
    is_default_admin: bool = Field(False, description="    ")


class Token(BaseModel):
    """Internal token model with both tokens (used by auth service)"""

    access_token: str
    refresh_token: str
    token_type: str = "bearer"
    expires_in: int
    refresh_expires_in: int = 604800  # 7 days in seconds


class TokenResponse(BaseModel):
    """Response model for login/refresh - refresh token is sent via HTTP-Only cookie"""

    access_token: str
    token_type: str = "bearer"
    expires_in: int


class TokenData(BaseModel):
    user_id: str
    username: str
    role: UserRole
    exp: datetime


# ============================================================================
# Response Models
# ============================================================================
class PaginatedResponse(BaseModel):
    items: list[Any]
    total: int
    page: int
    page_size: int
    total_pages: int


class HealthCheck(BaseModel):
    status: str = "healthy"
    version: str
    timestamp: datetime


# ============================================================================
# Service Health Monitoring Models
# ============================================================================
class ServiceHealthStatus(str, Enum):
    HEALTHY = "healthy"
    UNHEALTHY = "unhealthy"
    DEGRADED = "degraded"
    UNREACHABLE = "unreachable"
    UNKNOWN = "unknown"


class ServiceHealth(BaseModel):
    service_id: str = Field(..., description=" ID")
    name: str = Field(..., description=" ")
    status: ServiceHealthStatus = Field(..., description=" ")
    message: Optional[str] = Field(default=None, description=" ")
    response_time_ms: Optional[float] = Field(default=None, description=" (ms)")
    url: Optional[str] = Field(default=None, description=" URL")
    checked_at: datetime = Field(..., description=" ")
    details: Optional[dict[str, Any]] = Field(default=None, description=" ")

    class Config:
        from_attributes = True


class InfrastructureHealth(BaseModel):
    service_id: str = Field(..., description="  ID")
    name: str = Field(..., description=" ")
    status: ServiceHealthStatus = Field(..., description=" ")
    message: Optional[str] = Field(default=None, description=" ")
    port: Optional[int] = Field(default=None, description="")
    checked_at: datetime = Field(..., description=" ")
    details: Optional[dict[str, Any]] = Field(default=None, description=" ")

    class Config:
        from_attributes = True


class OverallSystemHealth(BaseModel):
    status: ServiceHealthStatus = Field(..., description="  ")
    total_services: int = Field(..., description="  ")
    healthy_services: int = Field(..., description="  ")
    unhealthy_services: int = Field(..., description="  ")
    degraded_services: int = Field(..., description="  ")
    total_infrastructure: int = Field(..., description="  ")
    healthy_infrastructure: int = Field(..., description="  ")
    average_response_time_ms: Optional[float] = Field(
        None, description="  "
    )
    services: list[ServiceHealth] = Field(
        default_factory=list, description="  "
    )
    infrastructure: list[InfrastructureHealth] = Field(
        default_factory=list, description="  "
    )
    checked_at: datetime = Field(..., description=" ")

    class Config:
        from_attributes = True


class ServiceMetrics(BaseModel):
    service_id: str
    cpu_usage_percent: Optional[float] = None
    memory_usage_mb: Optional[float] = None
    memory_limit_mb: Optional[float] = None
    request_count: Optional[int] = None
    error_count: Optional[int] = None
    avg_response_time_ms: Optional[float] = None
    collected_at: datetime

    class Config:
        from_attributes = True


class ServiceInfo(BaseModel):
    id: str = Field(..., description=" ID")
    name: str = Field(..., description=" ")
    description: Optional[str] = Field(None, description="")
    port: Optional[int] = Field(None, description="")
    healthcheck: str = Field("/health", description=" ")
    hostname: str = Field(..., description="")
    type: str = Field(..., description=" ")
    tags: list[str] = Field(default_factory=list, description="")


# ============================================================================
# Data Source Management Models
# ============================================================================
class DataSourceType(str, Enum):
    RSS = "rss"
    WEB = "web"
    API = "api"
    SOCIAL = "social"


class DataSourceStatus(str, Enum):
    ACTIVE = "active"
    INACTIVE = "inactive"
    ERROR = "error"
    TESTING = "testing"


class DataSourceBase(BaseModel):
    name: str = Field(..., description=" ")
    source_type: DataSourceType = Field(..., description=" ")
    url: str = Field(..., description=" URL")
    description: Optional[str] = Field(None, description="")
    category: Optional[str] = Field(None, description="")
    language: str = Field("ko", description="")
    is_active: bool = Field(True, description=" ")
    crawl_interval_minutes: int = Field(60, description=" ()")
    priority: int = Field(0, description="")
    config: dict[str, Any] = Field(default_factory=dict, description=" ")


class DataSourceCreate(DataSourceBase):
    pass


class DataSourceUpdate(BaseModel):
    name: Optional[str] = None
    description: Optional[str] = None
    url: Optional[str] = None
    category: Optional[str] = None
    is_active: Optional[bool] = None
    crawl_interval_minutes: Optional[int] = None
    priority: Optional[int] = None
    config: Optional[dict[str, Any]] = None


class DataSource(DataSourceBase):
    id: str = Field(..., description=" ID")
    status: DataSourceStatus = Field(
        default=DataSourceStatus.ACTIVE, description=""
    )
    last_crawled_at: Optional[datetime] = Field(
        default=None, description="  "
    )
    total_articles: int = Field(default=0, description="   ")
    success_rate: float = Field(default=100.0, description="")
    created_at: datetime
    updated_at: datetime

    class Config:
        from_attributes = True


class DataSourceStats(BaseModel):
    source_id: str
    total_crawls: int = 0
    successful_crawls: int = 0
    failed_crawls: int = 0
    total_articles: int = 0
    avg_articles_per_crawl: float = 0.0
    last_error: Optional[str] = None
    last_error_at: Optional[datetime] = None


class DataSourceTestResult(BaseModel):
    source_id: str
    success: bool
    message: str
    response_time_ms: Optional[float] = None
    sample_data: Optional[dict[str, Any]] = None
    tested_at: datetime


# ============================================================================
# Database Management Models
# ============================================================================
class DatabaseType(str, Enum):
    POSTGRESQL = "postgresql"
    MONGODB = "mongodb"
    REDIS = "redis"


class DatabaseInfo(BaseModel):
    db_type: DatabaseType
    name: str
    host: str
    port: int
    status: ServiceHealthStatus
    version: Optional[str] = None
    size_bytes: Optional[int] = None
    size_human: Optional[str] = None
    connection_count: Optional[int] = None
    max_connections: Optional[int] = None
    uptime_seconds: Optional[int] = None
    checked_at: datetime


class PostgresTableInfo(BaseModel):
    schema_name: str
    table_name: str
    row_count: int
    size_bytes: int
    size_human: str
    index_size_bytes: Optional[int] = None
    last_vacuum: Optional[datetime] = None
    last_analyze: Optional[datetime] = None


class PostgresDatabaseStats(BaseModel):
    database_name: str
    size_bytes: int
    size_human: str
    tables: list[PostgresTableInfo]
    total_tables: int
    total_rows: int
    connection_count: int
    max_connections: int
    checked_at: datetime


class MongoCollectionInfo(BaseModel):
    collection_name: str
    document_count: int
    size_bytes: int
    size_human: str
    avg_document_size_bytes: Optional[int] = None
    index_count: int
    total_index_size_bytes: Optional[int] = None


class MongoDatabaseStats(BaseModel):
    database_name: str
    size_bytes: int
    size_human: str
    collections: list[MongoCollectionInfo]
    total_collections: int
    total_documents: int
    checked_at: datetime


class RedisStats(BaseModel):
    used_memory_bytes: int
    used_memory_human: str
    max_memory_bytes: Optional[int] = None
    connected_clients: int
    total_keys: int
    expired_keys: int
    keyspace_hits: int
    keyspace_misses: int
    hit_rate: float
    uptime_seconds: int
    checked_at: datetime


# ============================================================================
# Kafka/Redpanda Management Models
# ============================================================================
class KafkaTopicInfo(BaseModel):
    name: str
    partition_count: int
    replication_factor: int
    message_count: Optional[int] = None
    size_bytes: Optional[int] = None
    retention_ms: Optional[int] = None
    is_internal: bool = False


class KafkaConsumerGroupInfo(BaseModel):
    group_id: str
    state: str
    members_count: int
    topics: list[str]
    total_lag: int
    lag_per_partition: dict[str, int]


class KafkaClusterInfo(BaseModel):
    broker_count: int
    controller_id: Optional[int] = None
    cluster_id: Optional[str] = None
    topics: list[KafkaTopicInfo]
    consumer_groups: list[KafkaConsumerGroupInfo]
    total_topics: int
    total_partitions: int
    total_messages: Optional[int] = None
    checked_at: datetime

```

---

## backend/admin-dashboard/api/routers/__init__.py

```py
# Admin Dashboard Routers
from . import (
    auth,
    audit,
    documents,
    environments,
    scripts,
    public_auth,
    llm_providers,
    health_monitor,
    data_sources,
    ml_addons,
    ml_training,
    databases,
    kafka,
    config_export,
)

```

---

## backend/admin-dashboard/api/routers/audit.py

```py
"""
Audit Router -   API 
"""

import asyncio
import json
from datetime import datetime
from typing import AsyncGenerator, Optional

from fastapi import APIRouter, Depends, HTTPException, Query
from fastapi.responses import StreamingResponse

from ..models.schemas import AuditAction, AuditLog, AuditLogFilter, UserRole
from ..dependencies import get_audit_service, get_current_user, require_role

router = APIRouter(prefix="/audit", tags=["Audit Logs"])


# ============================================
# SSE Event Stream for Real-time Activity
# ============================================


async def activity_event_generator(
    audit_service,
    last_timestamp: Optional[datetime] = None,
) -> AsyncGenerator[str, None]:
    """
    Server-Sent Events generator for real-time activity stream.
    Polls for new audit logs and sends them as events.
    """
    poll_interval = 5  # seconds
    seen_ids = set()

    # Initialize with recent logs if no timestamp provided
    if last_timestamp is None:
        recent_logs, _ = audit_service.get_logs(page=1, page_size=20)
        for log in recent_logs:
            seen_ids.add(log.id)

    while True:
        try:
            # Get recent logs
            logs, _ = audit_service.get_logs(page=1, page_size=50)

            # Find new logs
            new_logs = []
            for log in logs:
                if log.id not in seen_ids:
                    new_logs.append(log)
                    seen_ids.add(log.id)

            # Send new logs as events
            for log in reversed(new_logs):  # Send oldest first
                event_data = {
                    "eventType": "activity",
                    "timestamp": log.timestamp.isoformat(),
                    "data": {
                        "id": log.id,
                        "userId": log.user_id,
                        "username": log.username,
                        "action": log.action.value,
                        "resourceType": log.resource_type,
                        "resourceId": log.resource_id,
                        "resourceName": log.resource_name,
                        "environmentId": log.environment_id,
                        "environmentName": log.environment_name,
                        "success": log.success,
                        "errorMessage": log.error_message,
                        "timestamp": log.timestamp.isoformat(),
                    },
                }
                yield f"event: activity\ndata: {json.dumps(event_data, ensure_ascii=False)}\n\n"

            # Send heartbeat
            heartbeat_data = {
                "eventType": "heartbeat",
                "timestamp": datetime.utcnow().isoformat(),
            }
            yield f"event: heartbeat\ndata: {json.dumps(heartbeat_data)}\n\n"

            # Limit seen_ids to prevent memory growth
            if len(seen_ids) > 1000:
                seen_ids = set(list(seen_ids)[-500:])

            await asyncio.sleep(poll_interval)

        except asyncio.CancelledError:
            break
        except Exception as e:
            error_data = {"eventType": "error", "message": str(e)}
            yield f"event: error\ndata: {json.dumps(error_data)}\n\n"
            await asyncio.sleep(poll_interval)


@router.get("/activity/stream")
async def stream_activity_events(
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """
    SSE endpoint for real-time activity events.

    Returns a Server-Sent Events stream with:
    - activity: New audit log entries
    - heartbeat: Keep-alive signal (every 5 seconds)
    - error: Error notifications

    Requires: OPERATOR role or higher
    """
    return StreamingResponse(
        activity_event_generator(audit_service),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # Disable nginx buffering
        },
    )


@router.get("/activity/recent")
async def get_recent_activity(
    limit: int = Query(20, ge=1, le=100, description="  "),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """
        (SSE ).
        .
    
    Requires: OPERATOR role or higher
    """
    logs, total = audit_service.get_logs(page=1, page_size=limit)

    return {
        "activities": [
            {
                "id": log.id,
                "userId": log.user_id,
                "username": log.username,
                "action": log.action.value,
                "resourceType": log.resource_type,
                "resourceId": log.resource_id,
                "resourceName": log.resource_name,
                "environmentId": log.environment_id,
                "environmentName": log.environment_name,
                "success": log.success,
                "errorMessage": log.error_message,
                "timestamp": log.timestamp.isoformat(),
            }
            for log in logs
        ],
        "total": total,
    }


# ============================================
# Original Audit Endpoints
# ============================================


@router.get("/logs", response_model=list[AuditLog])
async def list_audit_logs(
    user_id: Optional[str] = Query(None, description=" ID "),
    action: Optional[AuditAction] = Query(None, description=" "),
    resource_type: Optional[str] = Query(None, description="  "),
    environment_id: Optional[str] = Query(None, description=" ID "),
    start_date: Optional[datetime] = Query(None, description=" "),
    end_date: Optional[datetime] = Query(None, description=" "),
    success: Optional[bool] = Query(None, description="/ "),
    page: int = Query(1, ge=1, description=" "),
    page_size: int = Query(50, ge=1, le=200, description=" "),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """   (Operator   )"""
    filter_params = AuditLogFilter(
        user_id=user_id,
        action=action,
        resource_type=resource_type,
        environment_id=environment_id,
        start_date=start_date,
        end_date=end_date,
        success=success,
    )

    logs, total = audit_service.get_logs(
        filter_params=filter_params,
        page=page,
        page_size=page_size,
    )

    return logs


@router.get("/logs/count")
async def get_audit_logs_count(
    user_id: Optional[str] = Query(None),
    action: Optional[AuditAction] = Query(None),
    resource_type: Optional[str] = Query(None),
    environment_id: Optional[str] = Query(None),
    start_date: Optional[datetime] = Query(None),
    end_date: Optional[datetime] = Query(None),
    success: Optional[bool] = Query(None),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """    """
    filter_params = AuditLogFilter(
        user_id=user_id,
        action=action,
        resource_type=resource_type,
        environment_id=environment_id,
        start_date=start_date,
        end_date=end_date,
        success=success,
    )

    _, total = audit_service.get_logs(
        filter_params=filter_params,
        page=1,
        page_size=1,
    )

    return {"total": total}


@router.get("/logs/{log_id}", response_model=AuditLog)
async def get_audit_log(
    log_id: str,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """   """
    log = audit_service.get_log_by_id(log_id)
    if not log:
        raise HTTPException(status_code=404, detail="Audit log not found")
    return log


@router.get("/users/{user_id}/activity", response_model=list[AuditLog])
async def get_user_activity(
    user_id: str,
    limit: int = Query(100, ge=1, le=500),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """     (Admin  )"""
    return audit_service.get_user_activity(user_id, limit=limit)


@router.get(
    "/resources/{resource_type}/{resource_id}/history", response_model=list[AuditLog]
)
async def get_resource_history(
    resource_type: str,
    resource_id: str,
    limit: int = Query(100, ge=1, le=500),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """   """
    return audit_service.get_resource_history(resource_type, resource_id, limit=limit)


@router.get("/statistics")
async def get_audit_statistics(
    start_date: Optional[datetime] = Query(None, description=" "),
    end_date: Optional[datetime] = Query(None, description=" "),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """   (Admin  )"""
    return audit_service.get_statistics(start_date=start_date, end_date=end_date)


@router.delete("/logs/cleanup")
async def cleanup_old_logs(
    days: int = Query(90, ge=30, le=365, description="  ()"),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """   (Admin  )"""
    deleted_count = audit_service.clear_old_logs(days=days)
    return {
        "success": True,
        "message": f"Deleted {deleted_count} old logs (older than {days} days)",
        "deleted_count": deleted_count,
    }

```

---

## backend/admin-dashboard/api/routers/auth.py

```py
"""
Auth Router - / API 
"""

import os
from typing import Optional

from fastapi import APIRouter, Cookie, Depends, HTTPException, Request, Response, status
from fastapi.security import OAuth2PasswordRequestForm
from pydantic import BaseModel

from ..models.schemas import (
    AuditAction,
    Token,
    TokenResponse,
    User,
    UserCreate,
    UserRole,
    SetupStatus,
)
from ..dependencies import (
    get_audit_service,
    get_auth_service,
    get_current_user,
    require_role,
)

router = APIRouter(prefix="/auth", tags=["Authentication"])

# Cookie settings
REFRESH_TOKEN_COOKIE_NAME = "refresh_token"
# In production, set SECURE_COOKIES=true in environment
SECURE_COOKIES = os.getenv("SECURE_COOKIES", "false").lower() == "true"
# Cookie max age: 7 days in seconds
REFRESH_TOKEN_MAX_AGE = 7 * 24 * 60 * 60


def set_refresh_token_cookie(response: Response, refresh_token: str) -> None:
    """Set refresh token as HTTP-Only cookie"""
    response.set_cookie(
        key=REFRESH_TOKEN_COOKIE_NAME,
        value=refresh_token,
        httponly=True,
        secure=SECURE_COOKIES,  # HTTPS only in production
        samesite="lax",  # CSRF protection
        max_age=REFRESH_TOKEN_MAX_AGE,
        path="/api/v1/admin/auth",  # Restrict to auth endpoints only
    )


def clear_refresh_token_cookie(response: Response) -> None:
    """Clear the refresh token cookie"""
    response.delete_cookie(
        key=REFRESH_TOKEN_COOKIE_NAME,
        path="/api/v1/admin/auth",
        httponly=True,
        secure=SECURE_COOKIES,
        samesite="lax",
    )


class LoginRequest(BaseModel):
    username: str
    password: str


class ChangePasswordRequest(BaseModel):
    old_password: str
    new_password: str


class ResetPasswordRequest(BaseModel):
    new_password: str


class UpdateUserRequest(BaseModel):
    email: Optional[str] = None
    role: Optional[UserRole] = None
    is_active: Optional[bool] = None


class RefreshTokenRequest(BaseModel):
    """Optional body for refresh - prefer using HTTP-Only cookie"""

    refresh_token: Optional[str] = None


@router.post("/login", response_model=TokenResponse)
async def login(
    response: Response,
    form_data: OAuth2PasswordRequestForm = Depends(),
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """ -   HTTP-Only  """
    user = auth_service.authenticate(form_data.username, form_data.password)

    if not user:
        #  
        audit_service.log(
            user_id="unknown",
            username=form_data.username,
            action=AuditAction.LOGIN,
            resource_type="auth",
            success=False,
            error_message="Invalid credentials",
        )
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid username or password",
            headers={"WWW-Authenticate": "Bearer"},
        )

    token = auth_service.create_access_token(user)

    # Set refresh token as HTTP-Only cookie
    set_refresh_token_cookie(response, token.refresh_token)

    #  
    audit_service.log(
        user_id=user.id,
        username=user.username,
        action=AuditAction.LOGIN,
        resource_type="auth",
        success=True,
    )

    # Return only access token in body (refresh token is in cookie)
    return TokenResponse(
        access_token=token.access_token,
        token_type=token.token_type,
        expires_in=token.expires_in,
    )


@router.post("/token", response_model=TokenResponse)
async def login_for_access_token(
    response: Response,
    form_data: OAuth2PasswordRequestForm = Depends(),
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """OAuth2    -   HTTP-Only  """
    return await login(response, form_data, auth_service, audit_service)


@router.get("/me", response_model=User)
async def get_current_user_info(
    current_user=Depends(get_current_user),
):
    """   """
    return current_user


@router.post("/refresh", response_model=TokenResponse)
async def refresh_token(
    response: Response,
    request: Optional[RefreshTokenRequest] = None,
    refresh_token_cookie: Optional[str] = Cookie(None, alias="refresh_token"),
    auth_service=Depends(get_auth_service),
):
    """     

      HTTP-Only   .
         refresh_token  ( ).

    -    HTTP-Only  
    -       (Rotation)
    """
    # Prefer cookie over body
    token_to_use = refresh_token_cookie
    if not token_to_use and request and request.refresh_token:
        token_to_use = request.refresh_token

    if not token_to_use:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Refresh token not provided",
            headers={"WWW-Authenticate": "Bearer"},
        )

    new_token = auth_service.refresh_access_token(token_to_use)

    if not new_token:
        # Clear invalid cookie
        clear_refresh_token_cookie(response)
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid or expired refresh token",
            headers={"WWW-Authenticate": "Bearer"},
        )

    # Set new refresh token as HTTP-Only cookie
    set_refresh_token_cookie(response, new_token.refresh_token)

    # Return only access token in body
    return TokenResponse(
        access_token=new_token.access_token,
        token_type=new_token.token_type,
        expires_in=new_token.expires_in,
    )


@router.post("/logout")
async def logout(
    response: Response,
    current_user=Depends(get_current_user),
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """ -        """
    #     (Redis)
    auth_service.revoke_all_user_tokens(current_user.id)

    # HTTP-Only  
    clear_refresh_token_cookie(response)

    #  
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.LOGOUT,
        resource_type="auth",
    )

    return {"success": True, "message": "Logged out successfully"}


@router.post("/change-password")
async def change_password(
    request: ChangePasswordRequest,
    current_user=Depends(get_current_user),
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """ """
    success = auth_service.change_password(
        user_id=current_user.id,
        old_password=request.old_password,
        new_password=request.new_password,
    )

    if not success:
        audit_service.log(
            user_id=current_user.id,
            username=current_user.username,
            action=AuditAction.UPDATE,
            resource_type="password",
            success=False,
            error_message="Invalid old password",
        )
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid old password",
        )

    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="password",
        success=True,
    )

    return {"success": True, "message": "Password changed successfully"}


# ============================================================================
# User Management (Admin only)
# ============================================================================
@router.get("/users", response_model=list[User])
async def list_users(
    active_only: bool = False,
    auth_service=Depends(get_auth_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """   (Admin  )"""
    return auth_service.list_users(active_only=active_only)


@router.get("/users/{user_id}", response_model=User)
async def get_user(
    user_id: str,
    auth_service=Depends(get_auth_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """  (Admin  )"""
    user = auth_service.get_user(user_id)
    if not user:
        raise HTTPException(status_code=404, detail="User not found")
    return user


@router.post("/users", response_model=User, status_code=status.HTTP_201_CREATED)
async def create_user(
    data: UserCreate,
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """  (Admin  )"""
    try:
        user = auth_service.create_user(data)

        audit_service.log(
            user_id=current_user.id,
            username=current_user.username,
            action=AuditAction.CREATE,
            resource_type="user",
            resource_id=user.id,
            resource_name=user.username,
            details={"role": data.role.value},
        )

        return user
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.patch("/users/{user_id}", response_model=User)
async def update_user(
    user_id: str,
    data: UpdateUserRequest,
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """   (Admin  )"""
    user = auth_service.update_user(
        user_id=user_id,
        email=data.email,
        role=data.role,
        is_active=data.is_active,
    )

    if not user:
        raise HTTPException(status_code=404, detail="User not found")

    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="user",
        resource_id=user_id,
        resource_name=user.username,
        details=data.model_dump(exclude_unset=True),
    )

    return user


@router.post("/users/{user_id}/reset-password")
async def reset_user_password(
    user_id: str,
    request: ResetPasswordRequest,
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """   (Admin  )"""
    user = auth_service.get_user(user_id)
    if not user:
        raise HTTPException(status_code=404, detail="User not found")

    success = auth_service.reset_password(user_id, request.new_password)

    if not success:
        raise HTTPException(status_code=500, detail="Failed to reset password")

    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="user",
        resource_id=user_id,
        resource_name=user.username,
        details={"action": "password_reset"},
    )

    return {"success": True, "message": "Password reset successfully"}


@router.delete("/users/{user_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_user(
    user_id: str,
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """  (Admin  )"""
    #    
    if user_id == current_user.id:
        raise HTTPException(status_code=400, detail="Cannot delete yourself")

    user = auth_service.get_user(user_id)
    if not user:
        raise HTTPException(status_code=404, detail="User not found")

    if not auth_service.delete_user(user_id):
        raise HTTPException(status_code=500, detail="Failed to delete user")

    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DELETE,
        resource_type="user",
        resource_id=user_id,
        resource_name=user.username,
    )


# ============================================================================
# Setup Status (Public - no auth required)
# ============================================================================
@router.get("/setup-status", response_model=SetupStatus)
async def get_setup_status(
    auth_service=Depends(get_auth_service),
):
    """    ( )

         .
    - setup_required:    
    - has_users:   
    - is_default_admin:   (admin/admin123)   
    """
    return auth_service.get_setup_status()

```

---

## backend/admin-dashboard/api/routers/config_export.py

```py
"""
Config Export/Import Router -   Export/Import API 
LLM Provider, ML Addon    JSON  
"""

import os
from datetime import datetime
from typing import Any, Dict, List, Optional

import httpx
from fastapi import APIRouter, Depends, HTTPException, UploadFile, File
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field

from ..models.schemas import (
    AuditAction,
    UserRole,
)
from ..dependencies import (
    get_audit_service,
    get_current_user,
    require_role,
)

router = APIRouter(prefix="/config-export", tags=["Config Export/Import"])


# ============================================================================
# Configuration
# ============================================================================

COLLECTOR_SERVICE_URL = os.environ.get("COLLECTOR_SERVICE_URL", "http://localhost:8081")
CRAWLER_SERVICE_URL = os.environ.get(
    "CRAWLER_SERVICE_URL", "http://autonomous-crawler:8030"
)


# ============================================================================
# Schemas
# ============================================================================


class LlmProviderExport(BaseModel):
    """LLM Provider  Export """
    providerType: str
    defaultModel: str
    baseUrl: Optional[str] = None
    enabled: bool = True
    priority: int = 100
    maxTokens: int = 4096
    temperature: float = 0.7
    timeoutMs: int = 60000
    azureDeploymentName: Optional[str] = None
    azureApiVersion: Optional[str] = None
    # API Key   export
    apiKeyMasked: Optional[str] = None


class MlAddonExport(BaseModel):
    """ML Addon  Export """
    addon_key: str
    name: str
    description: Optional[str] = None
    endpoint_url: str
    version: Optional[str] = None
    status: str = "active"
    config: Optional[Dict[str, Any]] = None


class SystemConfigExport(BaseModel):
    """   Export """
    version: str = "1.0"
    exportedAt: str
    exportedBy: Optional[str] = None
    llmProviders: List[LlmProviderExport] = []
    mlAddons: List[MlAddonExport] = []
    metadata: Optional[Dict[str, Any]] = None


class LlmProviderImport(BaseModel):
    """LLM Provider Import  (API Key  )"""
    providerType: str
    apiKey: Optional[str] = Field(None, description="API  ()")
    defaultModel: str
    baseUrl: Optional[str] = None
    enabled: bool = True
    priority: int = 100
    maxTokens: int = 4096
    temperature: float = 0.7
    timeoutMs: int = 60000
    azureDeploymentName: Optional[str] = None
    azureApiVersion: Optional[str] = None


class MlAddonImport(BaseModel):
    """ML Addon Import """
    addon_key: str
    name: str
    description: Optional[str] = None
    endpoint_url: str
    version: Optional[str] = None
    config: Optional[Dict[str, Any]] = None


class SystemConfigImport(BaseModel):
    """   Import """
    version: str = "1.0"
    llmProviders: List[LlmProviderImport] = []
    mlAddons: List[MlAddonImport] = []
    metadata: Optional[Dict[str, Any]] = None


class ImportResult(BaseModel):
    """Import """
    success: bool
    message: str
    llmProvidersImported: int = 0
    llmProvidersFailed: int = 0
    mlAddonsImported: int = 0
    mlAddonsFailed: int = 0
    errors: List[str] = []
    warnings: List[str] = []


class ImportOptions(BaseModel):
    """Import """
    overwriteExisting: bool = Field(True, description="   ")
    skipLlmProviders: bool = Field(False, description="LLM Provider ")
    skipMlAddons: bool = Field(False, description="ML Addon ")
    validateOnly: bool = Field(False, description="  ( import )")


# ============================================================================
# Helper functions
# ============================================================================


async def call_collector_service(
    method: str,
    path: str,
    params: Optional[dict] = None,
    json_data: Optional[dict] = None,
) -> dict:
    """Call the data-collection-service API"""
    url = f"{COLLECTOR_SERVICE_URL}{path}"

    async with httpx.AsyncClient(timeout=30.0) as client:
        try:
            response = await client.request(
                method=method,
                url=url,
                params=params,
                json=json_data,
            )

            if response.status_code >= 400:
                detail = (
                    response.json().get("message", response.text)
                    if response.text
                    else "Unknown error"
                )
                raise HTTPException(
                    status_code=response.status_code,
                    detail=detail,
                )

            if response.status_code == 204:
                return {}

            return response.json()
        except httpx.RequestError as e:
            raise HTTPException(
                status_code=503,
                detail=f"Collector service unavailable: {str(e)}",
            )


# ============================================================================
# Export Endpoints
# ============================================================================


@router.get("/export", response_model=SystemConfigExport)
async def export_all_config(
    include_llm: bool = True,
    include_ml: bool = True,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
       JSON Export.
    
    LLM Provider, ML Addon    .
    API Key  .
    """
    export_data = SystemConfigExport(
        version="1.0",
        exportedAt=datetime.utcnow().isoformat() + "Z",
        exportedBy=current_user.username,
        llmProviders=[],
        mlAddons=[],
        metadata={
            "source": "NewsInsight Admin Dashboard",
            "includesSecrets": False,
        },
    )

    errors = []

    # Export LLM Providers
    if include_llm:
        try:
            llm_settings = await call_collector_service(
                "GET", "/api/v1/admin/llm-providers"
            )
            for setting in llm_settings:
                export_data.llmProviders.append(
                    LlmProviderExport(
                        providerType=setting.get("providerType"),
                        defaultModel=setting.get("defaultModel", ""),
                        baseUrl=setting.get("baseUrl"),
                        enabled=setting.get("enabled", True),
                        priority=setting.get("priority", 100),
                        maxTokens=setting.get("maxTokens", 4096),
                        temperature=setting.get("temperature", 0.7),
                        timeoutMs=setting.get("timeoutMs", 60000),
                        azureDeploymentName=setting.get("azureDeploymentName"),
                        azureApiVersion=setting.get("azureApiVersion"),
                        apiKeyMasked=setting.get("maskedApiKey"),
                    )
                )
        except Exception as e:
            errors.append(f"LLM Provider export failed: {str(e)}")

    # Export ML Addons
    if include_ml:
        try:
            ml_addons = await call_collector_service(
                "GET", "/api/v1/admin/ml-addons"
            )
            for addon in ml_addons:
                export_data.mlAddons.append(
                    MlAddonExport(
                        addon_key=addon.get("addon_key"),
                        name=addon.get("name"),
                        description=addon.get("description"),
                        endpoint_url=addon.get("endpoint_url"),
                        version=addon.get("version"),
                        status=addon.get("status", "active"),
                        config=addon.get("config"),
                    )
                )
        except Exception as e:
            errors.append(f"ML Addon export failed: {str(e)}")

    if errors:
        export_data.metadata["errors"] = errors

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.VIEW,
        resource_type="system_config",
        resource_id="export",
        resource_name="System Config Export",
        details={
            "llmProviderCount": len(export_data.llmProviders),
            "mlAddonCount": len(export_data.mlAddons),
        },
    )

    return export_data


@router.get("/export/download")
async def download_config(
    include_llm: bool = True,
    include_ml: bool = True,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
     JSON  .
    
    Content-Disposition     .
    """
    export_data = await export_all_config(
        include_llm=include_llm,
        include_ml=include_ml,
        audit_service=audit_service,
        current_user=current_user,
    )

    filename = f"newsinsight-config-{datetime.utcnow().strftime('%Y%m%d-%H%M%S')}.json"

    return JSONResponse(
        content=export_data.model_dump(),
        headers={
            "Content-Disposition": f'attachment; filename="{filename}"',
            "Content-Type": "application/json",
        },
    )


# ============================================================================
# Import Endpoints
# ============================================================================


@router.post("/import", response_model=ImportResult)
async def import_config(
    config: SystemConfigImport,
    options: Optional[ImportOptions] = None,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    JSON   Import.
    
    LLM Provider, ML Addon    .
         .
    """
    if options is None:
        options = ImportOptions()

    result = ImportResult(
        success=True,
        message="Import completed",
        errors=[],
        warnings=[],
    )

    # Validate version
    if config.version not in ["1.0"]:
        result.warnings.append(f"Unknown config version: {config.version}")

    # Validate only mode
    if options.validateOnly:
        result.message = "Validation completed (dry run)"
        result.llmProvidersImported = len(config.llmProviders)
        result.mlAddonsImported = len(config.mlAddons)
        return result

    # Import LLM Providers
    if not options.skipLlmProviders:
        for provider in config.llmProviders:
            try:
                if not provider.apiKey:
                    result.warnings.append(
                        f"LLM Provider {provider.providerType}: API Key  ."
                    )
                    result.llmProvidersFailed += 1
                    continue

                await call_collector_service(
                    "PUT",
                    f"/api/v1/admin/llm-providers/{provider.providerType}",
                    json_data={
                        "providerType": provider.providerType,
                        "apiKey": provider.apiKey,
                        "defaultModel": provider.defaultModel,
                        "baseUrl": provider.baseUrl,
                        "enabled": provider.enabled,
                        "priority": provider.priority,
                        "maxTokens": provider.maxTokens,
                        "temperature": provider.temperature,
                        "timeoutMs": provider.timeoutMs,
                        "azureDeploymentName": provider.azureDeploymentName,
                        "azureApiVersion": provider.azureApiVersion,
                    },
                )
                result.llmProvidersImported += 1
            except Exception as e:
                result.errors.append(
                    f"LLM Provider {provider.providerType} import failed: {str(e)}"
                )
                result.llmProvidersFailed += 1

    # Import ML Addons
    if not options.skipMlAddons:
        # Get existing addons to check for duplicates
        existing_addons = {}
        try:
            existing = await call_collector_service(
                "GET", "/api/v1/admin/ml-addons"
            )
            existing_addons = {a.get("addon_key"): a.get("id") for a in existing}
        except Exception:
            pass

        for addon in config.mlAddons:
            try:
                if addon.addon_key in existing_addons:
                    if options.overwriteExisting:
                        # Update existing
                        addon_id = existing_addons[addon.addon_key]
                        await call_collector_service(
                            "PUT",
                            f"/api/v1/admin/ml-addons/{addon_id}",
                            json_data={
                                "name": addon.name,
                                "description": addon.description,
                                "endpoint_url": addon.endpoint_url,
                                "version": addon.version,
                                "config": addon.config,
                            },
                        )
                        result.mlAddonsImported += 1
                    else:
                        result.warnings.append(
                            f"ML Addon {addon.addon_key}:   ."
                        )
                        result.mlAddonsFailed += 1
                else:
                    # Create new
                    await call_collector_service(
                        "POST",
                        "/api/v1/admin/ml-addons",
                        json_data={
                            "addon_key": addon.addon_key,
                            "name": addon.name,
                            "description": addon.description,
                            "endpoint_url": addon.endpoint_url,
                            "version": addon.version,
                            "config": addon.config,
                        },
                    )
                    result.mlAddonsImported += 1
            except Exception as e:
                result.errors.append(
                    f"ML Addon {addon.addon_key} import failed: {str(e)}"
                )
                result.mlAddonsFailed += 1

    # Determine overall success
    total_failed = result.llmProvidersFailed + result.mlAddonsFailed
    total_imported = result.llmProvidersImported + result.mlAddonsImported

    if total_failed > 0 and total_imported == 0:
        result.success = False
        result.message = "Import failed"
    elif total_failed > 0:
        result.message = f"Import completed with {total_failed} errors"
    else:
        result.message = f"Import completed successfully ({total_imported} items)"

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.CREATE,
        resource_type="system_config",
        resource_id="import",
        resource_name="System Config Import",
        details={
            "llmProvidersImported": result.llmProvidersImported,
            "llmProvidersFailed": result.llmProvidersFailed,
            "mlAddonsImported": result.mlAddonsImported,
            "mlAddonsFailed": result.mlAddonsFailed,
            "overwriteExisting": options.overwriteExisting,
        },
    )

    return result


@router.post("/import/validate", response_model=ImportResult)
async def validate_import_config(
    config: SystemConfigImport,
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    Import    ( import ).
    """
    result = ImportResult(
        success=True,
        message="Validation completed",
        errors=[],
        warnings=[],
    )

    # Validate version
    if config.version not in ["1.0"]:
        result.warnings.append(f"Unknown config version: {config.version}")

    # Validate LLM Providers
    valid_provider_types = [
        "OPENAI", "ANTHROPIC", "GOOGLE", "OPENROUTER",
        "OLLAMA", "AZURE_OPENAI", "TOGETHER_AI", "CUSTOM"
    ]

    for i, provider in enumerate(config.llmProviders):
        if provider.providerType not in valid_provider_types:
            result.errors.append(
                f"LLM Provider [{i}]:   providerType '{provider.providerType}'"
            )
        if not provider.defaultModel:
            result.warnings.append(
                f"LLM Provider [{i}] {provider.providerType}: defaultModel ."
            )
        if not provider.apiKey:
            result.warnings.append(
                f"LLM Provider [{i}] {provider.providerType}: apiKey  import  ."
            )
        else:
            result.llmProvidersImported += 1

    # Validate ML Addons
    for i, addon in enumerate(config.mlAddons):
        if not addon.addon_key:
            result.errors.append(f"ML Addon [{i}]: addon_key .")
        elif not addon.name:
            result.errors.append(f"ML Addon [{i}] {addon.addon_key}: name .")
        elif not addon.endpoint_url:
            result.errors.append(
                f"ML Addon [{i}] {addon.addon_key}: endpoint_url ."
            )
        else:
            result.mlAddonsImported += 1

    if result.errors:
        result.success = False
        result.message = f"Validation failed with {len(result.errors)} errors"

    return result


# ============================================================================
# Template Endpoint
# ============================================================================


@router.get("/template", response_model=SystemConfigImport)
async def get_config_template(
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    Import   .
    
           .
    """
    return SystemConfigImport(
        version="1.0",
        llmProviders=[
            LlmProviderImport(
                providerType="OPENAI",
                apiKey="sk-your-api-key-here",
                defaultModel="gpt-4o",
                enabled=True,
                priority=100,
                maxTokens=4096,
                temperature=0.7,
                timeoutMs=60000,
            ),
            LlmProviderImport(
                providerType="ANTHROPIC",
                apiKey="sk-ant-your-api-key-here",
                defaultModel="claude-sonnet-4-20250514",
                enabled=True,
                priority=90,
            ),
        ],
        mlAddons=[
            MlAddonImport(
                addon_key="sentiment-addon",
                name="Sentiment Analyzer",
                description="   ",
                endpoint_url="http://sentiment-mcp:8000",
                version="1.0.0",
            ),
            MlAddonImport(
                addon_key="factcheck-addon",
                name="Fact Checker",
                description="  ",
                endpoint_url="http://factcheck-mcp:8000",
                version="1.0.0",
            ),
        ],
        metadata={
            "description": "  ",
            "instructions": "API Key   .",
        },
    )

```

---

## backend/admin-dashboard/api/routers/data_sources.py

```py
"""
Data Sources Router
   API
"""

from typing import Optional
from fastapi import APIRouter, Depends, HTTPException, Query

from ..models.schemas import (
    DataSource,
    DataSourceCreate,
    DataSourceUpdate,
    DataSourceType,
    DataSourceStatus,
    DataSourceTestResult,
    UserRole,
    AuditAction,
)
from ..dependencies import (
    get_current_user,
    require_role,
    get_data_source_service,
    get_audit_service,
)
from ..services.data_source_service import DataSourceService
from ..services.audit_service import AuditService

router = APIRouter(prefix="/data-sources", tags=["Data Sources"])


@router.get("", response_model=list[DataSource])
async def list_data_sources(
    source_type: Optional[str] = Query(None, description="  "),
    status: Optional[str] = Query(None, description=" "),
    category: Optional[str] = Query(None, description=" "),
    is_active: Optional[bool] = Query(None, description="  "),
    service: DataSourceService = Depends(get_data_source_service),
    current_user=Depends(get_current_user),
):
    """   """
    type_filter = DataSourceType(source_type) if source_type else None
    status_filter = DataSourceStatus(status) if status else None

    return service.list_sources(
        source_type=type_filter,
        status=status_filter,
        category=category,
        is_active=is_active,
    )


@router.get("/categories", response_model=list[str])
async def get_categories(
    service: DataSourceService = Depends(get_data_source_service),
    current_user=Depends(get_current_user),
):
    """   """
    return service.get_categories()


@router.get("/stats", response_model=dict)
async def get_stats(
    service: DataSourceService = Depends(get_data_source_service),
    current_user=Depends(get_current_user),
):
    """   """
    return service.get_stats()


@router.get("/{source_id}", response_model=DataSource)
async def get_data_source(
    source_id: str,
    service: DataSourceService = Depends(get_data_source_service),
    current_user=Depends(get_current_user),
):
    """   """
    source = service.get_source(source_id)
    if not source:
        raise HTTPException(status_code=404, detail="Data source not found")
    return source


@router.post("", response_model=DataSource, status_code=201)
async def create_data_source(
    data: DataSourceCreate,
    service: DataSourceService = Depends(get_data_source_service),
    audit_service: AuditService = Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """   """
    source = service.create_source(data)

    #  
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.CREATE,
        resource_type="data_source",
        resource_id=source.id,
        resource_name=source.name,
        details={"url": source.url, "type": source.source_type.value},
    )

    return source


@router.patch("/{source_id}", response_model=DataSource)
async def update_data_source(
    source_id: str,
    data: DataSourceUpdate,
    service: DataSourceService = Depends(get_data_source_service),
    audit_service: AuditService = Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """  """
    source = service.update_source(source_id, data)
    if not source:
        raise HTTPException(status_code=404, detail="Data source not found")

    #  
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="data_source",
        resource_id=source.id,
        resource_name=source.name,
        details=data.model_dump(exclude_unset=True),
    )

    return source


@router.delete("/{source_id}", status_code=204)
async def delete_data_source(
    source_id: str,
    service: DataSourceService = Depends(get_data_source_service),
    audit_service: AuditService = Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """  """
    source = service.get_source(source_id)
    if not source:
        raise HTTPException(status_code=404, detail="Data source not found")

    service.delete_source(source_id)

    #  
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DELETE,
        resource_type="data_source",
        resource_id=source_id,
        resource_name=source.name,
        details={},
    )


@router.post("/{source_id}/test", response_model=DataSourceTestResult)
async def test_data_source(
    source_id: str,
    service: DataSourceService = Depends(get_data_source_service),
    audit_service: AuditService = Depends(get_audit_service),
    current_user=Depends(get_current_user),
):
    """   """
    result = await service.test_source(source_id)

    #  
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.EXECUTE,
        resource_type="data_source",
        resource_id=source_id,
        details={"action": "test", "success": result.success},
        success=result.success,
        error_message=None if result.success else result.message,
    )

    return result


@router.post("/{source_id}/crawl")
async def trigger_crawl(
    source_id: str,
    service: DataSourceService = Depends(get_data_source_service),
    audit_service: AuditService = Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """  """
    result = await service.trigger_crawl(source_id)

    #  
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.EXECUTE,
        resource_type="data_source",
        resource_id=source_id,
        details={"action": "crawl", "success": result["success"]},
        success=result["success"],
        error_message=result.get("message") if not result["success"] else None,
    )

    return result


@router.post("/bulk/toggle-active")
async def bulk_toggle_active(
    source_ids: list[str],
    is_active: bool = Query(..., description=" "),
    service: DataSourceService = Depends(get_data_source_service),
    audit_service: AuditService = Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """    /"""
    updated = service.bulk_toggle_active(source_ids, is_active)

    #  
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="data_source",
        details={
            "action": "bulk_toggle_active",
            "source_ids": source_ids,
            "is_active": is_active,
            "updated_count": updated,
        },
    )

    return {"updated": updated}

```

---

## backend/admin-dashboard/api/routers/databases.py

```py
"""
Database Management Router
PostgreSQL, MongoDB, Redis   API
"""

from fastapi import APIRouter, Depends, HTTPException

from ..models.schemas import (
    DatabaseType,
    DatabaseInfo,
    PostgresDatabaseStats,
    MongoDatabaseStats,
    RedisStats,
    UserRole,
)
from ..dependencies import get_current_user, require_role, get_database_service
from ..services.database_service import DatabaseService

router = APIRouter(prefix="/databases", tags=["Database Management"])


@router.get("", response_model=list[DatabaseInfo])
async def list_databases(
    service: DatabaseService = Depends(get_database_service),
    current_user=Depends(get_current_user),
):
    """   """
    return await service.get_all_databases()


@router.get("/postgres/health", response_model=DatabaseInfo)
async def check_postgres_health(
    service: DatabaseService = Depends(get_database_service),
    current_user=Depends(get_current_user),
):
    """PostgreSQL  """
    return await service.get_postgres_health()


@router.get("/mongo/health", response_model=DatabaseInfo)
async def check_mongo_health(
    service: DatabaseService = Depends(get_database_service),
    current_user=Depends(get_current_user),
):
    """MongoDB  """
    return await service.get_mongo_health()


@router.get("/redis/health", response_model=DatabaseInfo)
async def check_redis_health(
    service: DatabaseService = Depends(get_database_service),
    current_user=Depends(get_current_user),
):
    """Redis  """
    return await service.get_redis_health()


@router.get("/{db_type}/health", response_model=DatabaseInfo)
async def check_database_health(
    db_type: str,
    service: DatabaseService = Depends(get_database_service),
    current_user=Depends(get_current_user),
):
    """   """
    db_type_lower = db_type.lower()

    if db_type_lower in ("postgres", "postgresql"):
        return await service.get_postgres_health()
    elif db_type_lower in ("mongo", "mongodb"):
        return await service.get_mongo_health()
    elif db_type_lower == "redis":
        return await service.get_redis_health()
    else:
        raise HTTPException(
            status_code=400,
            detail=f"Unknown database type: {db_type}. Supported: postgres, mongo, redis",
        )


@router.get("/postgres/stats", response_model=PostgresDatabaseStats)
async def get_postgres_stats(
    service: DatabaseService = Depends(get_database_service),
    current_user=Depends(get_current_user),
):
    """PostgreSQL  """
    return await service.get_postgres_stats()


@router.get("/mongo/stats", response_model=MongoDatabaseStats)
async def get_mongo_stats(
    service: DatabaseService = Depends(get_database_service),
    current_user=Depends(get_current_user),
):
    """MongoDB  """
    return await service.get_mongo_stats()


@router.get("/redis/stats", response_model=RedisStats)
async def get_redis_stats(
    service: DatabaseService = Depends(get_database_service),
    current_user=Depends(get_current_user),
):
    """Redis  """
    return await service.get_redis_stats()

```

---

## backend/admin-dashboard/api/routers/documents.py

```py
"""
Document Router -   API 
"""
from typing import Optional

from fastapi import APIRouter, Depends, HTTPException, Query

from ..models.schemas import Document, DocumentCategory, UserRole
from ..dependencies import get_current_user, get_document_service, require_role

router = APIRouter(prefix="/documents", tags=["Documents"])


@router.get("", response_model=list[Document])
async def list_documents(
    category: Optional[DocumentCategory] = Query(None, description=" "),
    tag: Optional[str] = Query(None, description=" "),
    environment: Optional[str] = Query(None, description=" "),
    search: Optional[str] = Query(None, description=""),
    doc_service=Depends(get_document_service),
    current_user=Depends(get_current_user),
):
    """  """
    docs = doc_service.list_documents(
        category=category,
        tag=tag,
        environment=environment,
        search=search,
    )
    #  content 
    for doc in docs:
        doc.content = None
    return docs


@router.get("/categories")
async def get_categories_summary(
    doc_service=Depends(get_document_service),
    current_user=Depends(get_current_user),
):
    """   """
    return doc_service.get_categories_summary()


@router.get("/tags")
async def get_tags_summary(
    doc_service=Depends(get_document_service),
    current_user=Depends(get_current_user),
):
    """   """
    return doc_service.get_tags_summary()


@router.get("/related")
async def get_related_documents(
    environment: Optional[str] = Query(None, description=" "),
    script_id: Optional[str] = Query(None, description=" ID"),
    doc_service=Depends(get_document_service),
    current_user=Depends(get_current_user),
):
    """  """
    if not environment and not script_id:
        raise HTTPException(
            status_code=400,
            detail="At least one of environment or script_id is required",
        )

    docs = doc_service.get_related_documents(
        environment=environment,
        script_id=script_id,
    )
    #  content 
    for doc in docs:
        doc.content = None
    return docs


@router.get("/{doc_id}", response_model=Document)
async def get_document(
    doc_id: str,
    doc_service=Depends(get_document_service),
    current_user=Depends(get_current_user),
):
    """   ( )"""
    doc = doc_service.get_document(doc_id)
    if not doc:
        raise HTTPException(status_code=404, detail="Document not found")
    return doc


@router.patch("/{doc_id}", response_model=Document)
async def update_document_metadata(
    doc_id: str,
    title: Optional[str] = None,
    category: Optional[DocumentCategory] = None,
    tags: Optional[list[str]] = Query(None),
    related_environments: Optional[list[str]] = Query(None),
    related_scripts: Optional[list[str]] = Query(None),
    doc_service=Depends(get_document_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """   (Admin  )"""
    doc = doc_service.update_document_metadata(
        doc_id=doc_id,
        title=title,
        category=category,
        tags=tags,
        related_environments=related_environments,
        related_scripts=related_scripts,
    )
    if not doc:
        raise HTTPException(status_code=404, detail="Document not found")
    return doc


@router.post("/refresh")
async def refresh_documents(
    doc_service=Depends(get_document_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """   (Admin  )"""
    diff = doc_service.refresh_documents()
    return {
        "success": True,
        "message": f"Documents refreshed. {diff:+d} documents changed.",
        "total": len(doc_service.documents),
    }

```

---

## backend/admin-dashboard/api/routers/environments.py

```py
"""
Environment Router -   API 
"""
from typing import Optional

from fastapi import APIRouter, Depends, HTTPException, Query, status
from fastapi.responses import StreamingResponse

from ..models.schemas import (
    AuditAction,
    Environment,
    EnvironmentCreate,
    EnvironmentStatus,
    EnvironmentUpdate,
    UserRole,
)
from ..dependencies import (
    get_audit_service,
    get_current_user,
    get_environment_service,
    require_role,
)

router = APIRouter(prefix="/environments", tags=["Environments"])


@router.get("", response_model=list[Environment])
async def list_environments(
    active_only: bool = Query(False, description="  "),
    env_service=Depends(get_environment_service),
    current_user=Depends(get_current_user),
):
    """  """
    return env_service.list_environments(active_only=active_only)


@router.get("/{env_id}", response_model=Environment)
async def get_environment(
    env_id: str,
    env_service=Depends(get_environment_service),
    current_user=Depends(get_current_user),
):
    """  """
    env = env_service.get_environment(env_id)
    if not env:
        raise HTTPException(status_code=404, detail="Environment not found")
    return env


@router.post("", response_model=Environment, status_code=status.HTTP_201_CREATED)
async def create_environment(
    data: EnvironmentCreate,
    env_service=Depends(get_environment_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """  (Admin  )"""
    env = env_service.create_environment(data)

    #  
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.CREATE,
        resource_type="environment",
        resource_id=env.id,
        resource_name=env.name,
        details={"data": data.model_dump()},
    )

    return env


@router.patch("/{env_id}", response_model=Environment)
async def update_environment(
    env_id: str,
    data: EnvironmentUpdate,
    env_service=Depends(get_environment_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """  (Admin  )"""
    env = env_service.update_environment(env_id, data)
    if not env:
        raise HTTPException(status_code=404, detail="Environment not found")

    #  
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="environment",
        resource_id=env.id,
        resource_name=env.name,
        details={"changes": data.model_dump(exclude_unset=True)},
    )

    return env


@router.delete("/{env_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_environment(
    env_id: str,
    env_service=Depends(get_environment_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """  (Admin  )"""
    env = env_service.get_environment(env_id)
    if not env:
        raise HTTPException(status_code=404, detail="Environment not found")

    if not env_service.delete_environment(env_id):
        raise HTTPException(status_code=500, detail="Failed to delete environment")

    #  
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DELETE,
        resource_type="environment",
        resource_id=env_id,
        resource_name=env.name,
    )


@router.get("/{env_id}/status", response_model=EnvironmentStatus)
async def get_environment_status(
    env_id: str,
    env_service=Depends(get_environment_service),
    current_user=Depends(get_current_user),
):
    """   ( )"""
    status = env_service.get_environment_status(env_id)
    if not status:
        raise HTTPException(status_code=404, detail="Environment not found")
    return status


@router.post("/{env_id}/up")
async def docker_compose_up(
    env_id: str,
    build: bool = Query(True, description="  "),
    env_service=Depends(get_environment_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """Docker Compose Up """
    env = env_service.get_environment(env_id)
    if not env:
        raise HTTPException(status_code=404, detail="Environment not found")

    success, output = await env_service.docker_compose_up(env_id, build=build)

    #  
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DEPLOY,
        resource_type="environment",
        resource_id=env_id,
        resource_name=env.name,
        environment_id=env_id,
        environment_name=env.name,
        details={"build": build},
        success=success,
        error_message=output if not success else None,
    )

    if not success:
        raise HTTPException(status_code=500, detail=output)

    return {"success": True, "message": "Services started successfully", "output": output}


@router.post("/{env_id}/down")
async def docker_compose_down(
    env_id: str,
    volumes: bool = Query(False, description="  "),
    env_service=Depends(get_environment_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """Docker Compose Down """
    env = env_service.get_environment(env_id)
    if not env:
        raise HTTPException(status_code=404, detail="Environment not found")

    #   Admin  
    if volumes and current_user.role != UserRole.ADMIN:
        raise HTTPException(
            status_code=403,
            detail="Admin permission required to delete volumes",
        )

    success, output = await env_service.docker_compose_down(env_id, volumes=volumes)

    #  
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.EXECUTE,
        resource_type="environment",
        resource_id=env_id,
        resource_name=env.name,
        environment_id=env_id,
        environment_name=env.name,
        details={"action": "down", "volumes": volumes},
        success=success,
        error_message=output if not success else None,
    )

    if not success:
        raise HTTPException(status_code=500, detail=output)

    return {"success": True, "message": "Services stopped successfully", "output": output}


@router.post("/{env_id}/restart")
async def docker_compose_restart(
    env_id: str,
    service: Optional[str] = Query(None, description="  "),
    env_service=Depends(get_environment_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """Docker Compose Restart """
    env = env_service.get_environment(env_id)
    if not env:
        raise HTTPException(status_code=404, detail="Environment not found")

    success, output = await env_service.docker_compose_restart(env_id, service=service)

    #  
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.EXECUTE,
        resource_type="environment",
        resource_id=env_id,
        resource_name=env.name,
        environment_id=env_id,
        environment_name=env.name,
        details={"action": "restart", "service": service},
        success=success,
        error_message=output if not success else None,
    )

    if not success:
        raise HTTPException(status_code=500, detail=output)

    return {"success": True, "message": "Services restarted successfully", "output": output}


@router.get("/{env_id}/logs/{service}")
async def get_service_logs(
    env_id: str,
    service: str,
    tail: int = Query(100, ge=1, le=1000, description="   "),
    env_service=Depends(get_environment_service),
    current_user=Depends(get_current_user),
):
    """  """
    env = env_service.get_environment(env_id)
    if not env:
        raise HTTPException(status_code=404, detail="Environment not found")

    success, output = await env_service.get_service_logs(env_id, service, tail=tail)

    if not success:
        raise HTTPException(status_code=500, detail=output)

    return {"service": service, "logs": output}

```

---

## backend/admin-dashboard/api/routers/health_monitor.py

```py
"""
Service Health Monitoring Router
       API
"""

from fastapi import APIRouter, Depends, HTTPException
from fastapi.responses import StreamingResponse
import asyncio
import json
from datetime import datetime

from ..models.schemas import (
    ServiceHealth,
    InfrastructureHealth,
    OverallSystemHealth,
    ServiceInfo,
    UserRole,
)
from ..dependencies import get_current_user, require_role, get_health_service
from ..services.health_service import HealthService

router = APIRouter(prefix="/health-monitor", tags=["Health Monitor"])


@router.get("/services", response_model=list[ServiceInfo])
async def list_services(
    service: HealthService = Depends(get_health_service),
    current_user=Depends(get_current_user),
):
    """    """
    services = service.get_all_services()
    return [
        ServiceInfo(
            id=s["id"],
            name=s["name"],
            description=s.get("description"),
            port=s.get("port"),
            healthcheck=s.get("healthcheck", "/health"),
            hostname=s["hostname"],
            type=s["type"],
            tags=s.get("tags", []),
        )
        for s in services
    ]


@router.get("/infrastructure", response_model=list[dict])
async def list_infrastructure(
    service: HealthService = Depends(get_health_service),
    current_user=Depends(get_current_user),
):
    """   """
    return service.get_infrastructure_services()


@router.get("/check/{service_id}", response_model=ServiceHealth)
async def check_service(
    service_id: str,
    service: HealthService = Depends(get_health_service),
    current_user=Depends(get_current_user),
):
    """   """
    health = await service.check_service_health(service_id)
    return health


@router.get("/check-all", response_model=list[ServiceHealth])
async def check_all_services(
    service: HealthService = Depends(get_health_service),
    current_user=Depends(get_current_user),
):
    """    ( )"""
    return await service.check_all_services_health()


@router.get("/check-infrastructure", response_model=list[InfrastructureHealth])
async def check_infrastructure(
    service: HealthService = Depends(get_health_service),
    current_user=Depends(get_current_user),
):
    """    """
    return await service.check_infrastructure_health()


@router.get("/overall", response_model=OverallSystemHealth)
async def get_overall_health(
    service: HealthService = Depends(get_health_service),
    current_user=Depends(get_current_user),
):
    """    """
    return await service.get_overall_health()


@router.get("/stream")
async def stream_health_updates(
    interval: int = 10,
    service: HealthService = Depends(get_health_service),
    current_user=Depends(get_current_user),
):
    """    (SSE)

    Args:
        interval:   (,  10,  5)
    """
    interval = max(5, min(interval, 60))  # 5-60  

    async def generate():
        while True:
            try:
                health = await service.get_overall_health()
                data = health.model_dump_json()
                yield f"data: {data}\n\n"
                await asyncio.sleep(interval)
            except Exception as e:
                error_data = json.dumps(
                    {"error": str(e), "timestamp": datetime.utcnow().isoformat()}
                )
                yield f"data: {error_data}\n\n"
                await asyncio.sleep(interval)

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )


@router.get("/last-check/{service_id}", response_model=ServiceHealth)
async def get_last_check(
    service_id: str,
    service: HealthService = Depends(get_health_service),
    current_user=Depends(get_current_user),
):
    """     ( )"""
    result = service.get_last_check(service_id)
    if not result:
        raise HTTPException(
            status_code=404,
            detail=f"No health check result found for service: {service_id}",
        )
    return result

```

---

## backend/admin-dashboard/api/routers/kafka.py

```py
"""
Kafka/Redpanda Monitoring Router
Kafka/Redpanda   API
"""

from fastapi import APIRouter, Depends, HTTPException

from ..models.schemas import (
    KafkaTopicInfo,
    KafkaConsumerGroupInfo,
    KafkaClusterInfo,
    UserRole,
)
from ..dependencies import get_current_user, require_role, get_kafka_service
from ..services.kafka_service import KafkaService

router = APIRouter(prefix="/kafka", tags=["Kafka/Redpanda Monitoring"])


@router.get("/cluster", response_model=KafkaClusterInfo)
async def get_cluster_info(
    service: KafkaService = Depends(get_kafka_service),
    current_user=Depends(get_current_user),
):
    """Kafka/Redpanda    """
    return await service.get_cluster_info()


@router.get("/topics", response_model=list[KafkaTopicInfo])
async def list_topics(
    service: KafkaService = Depends(get_kafka_service),
    current_user=Depends(get_current_user),
):
    """   """
    return await service.list_topics()


@router.get("/topics/{topic_name}", response_model=KafkaTopicInfo)
async def get_topic(
    topic_name: str,
    service: KafkaService = Depends(get_kafka_service),
    current_user=Depends(get_current_user),
):
    """   """
    topic = await service.get_topic_detail(topic_name)
    if not topic:
        raise HTTPException(status_code=404, detail=f"Topic not found: {topic_name}")
    return topic


@router.get("/consumer-groups", response_model=list[KafkaConsumerGroupInfo])
async def list_consumer_groups(
    service: KafkaService = Depends(get_kafka_service),
    current_user=Depends(get_current_user),
):
    """   """
    return await service.list_consumer_groups()


@router.get("/consumer-groups/{group_id}", response_model=KafkaConsumerGroupInfo)
async def get_consumer_group(
    group_id: str,
    service: KafkaService = Depends(get_kafka_service),
    current_user=Depends(get_current_user),
):
    """    """
    group = await service.get_consumer_group_detail(group_id)
    if not group:
        raise HTTPException(
            status_code=404, detail=f"Consumer group not found: {group_id}"
        )
    return group


@router.get("/health")
async def check_health(
    service: KafkaService = Depends(get_kafka_service),
    current_user=Depends(get_current_user),
):
    """Kafka/Redpanda  """
    return await service.check_health()

```

---

## backend/admin-dashboard/api/routers/llm_providers.py

```py
"""
LLM Provider Settings Router -   LLM  API 
"""

import os
from enum import Enum
from typing import Optional
from datetime import datetime

import httpx
from fastapi import APIRouter, Depends, HTTPException, Query, status
from pydantic import BaseModel, Field

from ..models.schemas import (
    AuditAction,
    UserRole,
)
from ..dependencies import (
    get_audit_service,
    get_current_user,
    get_current_user_optional,
    require_role,
)

router = APIRouter(prefix="/llm-providers", tags=["LLM Providers"])


# ============================================================================
# Schemas
# ============================================================================


class LlmProviderType(str, Enum):
    OPENAI = "OPENAI"
    ANTHROPIC = "ANTHROPIC"
    GOOGLE = "GOOGLE"
    OPENROUTER = "OPENROUTER"
    OLLAMA = "OLLAMA"
    AZURE_OPENAI = "AZURE_OPENAI"
    TOGETHER_AI = "TOGETHER_AI"
    CUSTOM = "CUSTOM"


class LlmProviderTypeInfo(BaseModel):
    value: LlmProviderType
    displayName: str
    description: str
    requiresApiKey: bool
    defaultBaseUrl: Optional[str] = None


class LlmProviderSettingsRequest(BaseModel):
    providerType: LlmProviderType
    apiKey: Optional[str] = Field(None, description="API  (   )")
    defaultModel: str = Field(..., description=" ")
    baseUrl: Optional[str] = Field(None, description="Base URL (Ollama/Custom)")
    enabled: bool = Field(True, description=" ")
    priority: int = Field(100, ge=1, le=999, description="")
    maxTokens: int = Field(4096, ge=1, le=128000, description=" ")
    temperature: float = Field(0.7, ge=0, le=2, description="Temperature")
    timeoutMs: int = Field(60000, ge=1000, le=300000, description=" (ms)")
    azureDeploymentName: Optional[str] = Field(
        None, description="Azure deployment name"
    )
    azureApiVersion: Optional[str] = Field(None, description="Azure API version")


class LlmProviderSettings(BaseModel):
    id: int
    providerType: LlmProviderType
    userId: Optional[str] = None  # null = global setting
    hasApiKey: bool
    maskedApiKey: Optional[str] = None
    defaultModel: str
    baseUrl: Optional[str] = None
    enabled: bool
    priority: int
    maxTokens: int
    temperature: float
    timeoutMs: int
    azureDeploymentName: Optional[str] = None
    azureApiVersion: Optional[str] = None
    lastTestedAt: Optional[datetime] = None
    lastTestSuccess: Optional[bool] = None
    createdAt: datetime
    updatedAt: datetime


class LlmTestResult(BaseModel):
    providerType: LlmProviderType
    success: bool
    message: str
    latencyMs: Optional[int] = None
    testedAt: datetime


# Provider metadata (2025 12 )
LLM_PROVIDER_TYPES = [
    LlmProviderTypeInfo(
        value=LlmProviderType.OPENAI,
        displayName="OpenAI",
        description="GPT-5, GPT-4.1, o3/o4  ",
        requiresApiKey=True,
    ),
    LlmProviderTypeInfo(
        value=LlmProviderType.ANTHROPIC,
        displayName="Anthropic",
        description="Claude 4 Sonnet/Opus/Haiku",
        requiresApiKey=True,
    ),
    LlmProviderTypeInfo(
        value=LlmProviderType.GOOGLE,
        displayName="Google AI",
        description="Gemini 3 Pro, Gemini 2.5 Pro/Flash",
        requiresApiKey=True,
    ),
    LlmProviderTypeInfo(
        value=LlmProviderType.OPENROUTER,
        displayName="OpenRouter",
        description="125+   API (  )",
        requiresApiKey=True,
    ),
    LlmProviderTypeInfo(
        value=LlmProviderType.OLLAMA,
        displayName="Ollama",
        description=" LLM  (Llama 3.2, DeepSeek R1)",
        requiresApiKey=False,
        defaultBaseUrl="http://localhost:11434",
    ),
    LlmProviderTypeInfo(
        value=LlmProviderType.AZURE_OPENAI,
        displayName="Azure OpenAI",
        description="Azure  OpenAI ",
        requiresApiKey=True,
    ),
    LlmProviderTypeInfo(
        value=LlmProviderType.TOGETHER_AI,
        displayName="Together AI",
        description="DeepSeek R1, Llama 405B   ",
        requiresApiKey=True,
        defaultBaseUrl="https://api.together.xyz/v1",
    ),
    LlmProviderTypeInfo(
        value=LlmProviderType.CUSTOM,
        displayName="Custom API",
        description="OpenAI   API",
        requiresApiKey=False,
    ),
]


# Backend service URL for data-collection-service
COLLECTOR_SERVICE_URL = os.environ.get("COLLECTOR_SERVICE_URL", "http://localhost:8081")


# ============================================================================
# Helper functions
# ============================================================================


async def call_collector_service(
    method: str,
    path: str,
    params: Optional[dict] = None,
    json_data: Optional[dict] = None,
) -> dict:
    """Call the data-collection-service API"""
    url = f"{COLLECTOR_SERVICE_URL}{path}"

    async with httpx.AsyncClient(timeout=30.0) as client:
        try:
            response = await client.request(
                method=method,
                url=url,
                params=params,
                json=json_data,
            )

            if response.status_code >= 400:
                detail = (
                    response.json().get("message", response.text)
                    if response.text
                    else "Unknown error"
                )
                raise HTTPException(
                    status_code=response.status_code,
                    detail=detail,
                )

            if response.status_code == 204:
                return {}

            return response.json()
        except httpx.RequestError as e:
            raise HTTPException(
                status_code=503,
                detail=f"Collector service unavailable: {str(e)}",
            )


# ============================================================================
# Endpoints
# ============================================================================


@router.get("/types", response_model=list[LlmProviderTypeInfo])
async def list_provider_types(
    current_user=Depends(get_current_user_optional),
):
    """LLM Provider    -  """
    return LLM_PROVIDER_TYPES


@router.get("/global", response_model=list[LlmProviderSettings])
async def list_global_settings(
    current_user=Depends(get_current_user_optional),
):
    """ LLM    -     """
    #    ADMIN     
    if current_user.id == "anonymous" or current_user.role != UserRole.ADMIN:
        return []
    result = await call_collector_service("GET", "/api/v1/admin/llm-providers")
    return result


@router.get("/global/{provider_type}", response_model=LlmProviderSettings)
async def get_global_setting(
    provider_type: LlmProviderType,
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ Provider    (Admin  )"""
    result = await call_collector_service(
        "GET", f"/api/v1/admin/llm-providers/{provider_type.value}"
    )
    return result


@router.put("/global/{provider_type}", response_model=LlmProviderSettings)
async def save_global_setting(
    provider_type: LlmProviderType,
    data: LlmProviderSettingsRequest,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ LLM  / (Admin  )"""
    # Ensure provider type matches
    if data.providerType != provider_type:
        raise HTTPException(
            status_code=400,
            detail="Provider type in path must match request body",
        )

    result = await call_collector_service(
        "PUT",
        f"/api/v1/admin/llm-providers/{provider_type.value}",
        json_data=data.model_dump(exclude_none=True),
    )

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="llm_provider",
        resource_id=provider_type.value,
        resource_name=f"Global {provider_type.value} Settings",
        details={
            "provider": provider_type.value,
            "enabled": data.enabled,
            "model": data.defaultModel,
        },
    )

    return result


@router.delete("/global/{provider_type}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_global_setting(
    provider_type: LlmProviderType,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """ LLM   (Admin  )"""
    await call_collector_service(
        "DELETE", f"/api/v1/admin/llm-providers/{provider_type.value}"
    )

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DELETE,
        resource_type="llm_provider",
        resource_id=provider_type.value,
        resource_name=f"Global {provider_type.value} Settings",
    )


@router.post("/test", response_model=LlmTestResult)
async def test_connection(
    provider_type: LlmProviderType = Query(..., description="Provider "),
    model: Optional[str] = Query(None, description=" "),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """LLM Provider   (Admin  )"""
    params = {"providerType": provider_type.value}
    if model:
        params["model"] = model

    result = await call_collector_service(
        "POST", "/api/v1/llm-providers/test", params=params
    )
    return result


@router.get("/effective", response_model=list[LlmProviderSettings])
async def get_effective_settings(
    user_id: Optional[str] = Query(None, description=" ID ( )"),
    current_user=Depends(get_current_user_optional),
):
    """ LLM   (  +  fallback) -  """
    params = {}
    if user_id:
        params["userId"] = user_id

    result = await call_collector_service(
        "GET", "/api/v1/llm-providers/effective", params=params
    )
    return result


@router.get("/enabled", response_model=list[LlmProviderSettings])
async def get_enabled_providers(
    user_id: Optional[str] = Query(None, description=" ID"),
    current_user=Depends(get_current_user_optional),
):
    """ LLM Provider   -  """
    params = {}
    if user_id:
        params["userId"] = user_id

    result = await call_collector_service(
        "GET", "/api/v1/llm-providers/enabled", params=params
    )
    return result

```

---

## backend/admin-dashboard/api/routers/ml_addons.py

```py
"""
ML Addons Router - ML   API 
"""

import os
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional

import httpx
from fastapi import APIRouter, Depends, HTTPException, Query, status
from pydantic import BaseModel, Field

from ..models.schemas import (
    AuditAction,
    UserRole,
)
from ..dependencies import (
    get_audit_service,
    get_current_user,
    require_role,
)

router = APIRouter(prefix="/ml-addons", tags=["ML Addons"])


# ============================================================================
# Configuration
# ============================================================================

CRAWLER_SERVICE_URL = os.environ.get(
    "CRAWLER_SERVICE_URL", "http://autonomous-crawler:8030"
)


# ============================================================================
# Schemas
# ============================================================================


class MLAddonType(str, Enum):
    """ML Addon """

    SENTIMENT = "sentiment"
    FACTCHECK = "factcheck"
    BIAS = "bias"


class MLAddonStatus(str, Enum):
    """ML Addon """

    HEALTHY = "healthy"
    UNHEALTHY = "unhealthy"
    UNKNOWN = "unknown"
    WARMING_UP = "warming_up"


class MLAddonInfo(BaseModel):
    """ML Addon """

    key: str
    name: str
    description: str
    endpoint: str
    status: MLAddonStatus
    features: List[str]


class MLAddonHealthResponse(BaseModel):
    """ML Addon  """

    status: str
    auto_analysis_enabled: bool
    addons: Dict[str, Dict[str, Any]]


class MLAddonStatusResponse(BaseModel):
    """ML Addon  """

    auto_analysis_enabled: bool
    parallel_analysis: bool
    addons: Dict[str, Any]


class MLAnalyzeRequest(BaseModel):
    """ML  """

    article_id: int = Field(..., description=" ID")
    title: str = Field(..., description=" ", min_length=1)
    content: str = Field(..., description=" ", min_length=10)
    source: Optional[str] = Field(default=None, description="")
    url: Optional[str] = Field(default=None, description=" URL")
    published_at: Optional[str] = Field(default=None, description="")
    addons: Optional[List[str]] = Field(
        default=None,
        description="   (sentiment, factcheck, bias). None  ",
    )
    save_to_db: bool = Field(default=True, description=" DB  ")


class MLBatchAnalyzeRequest(BaseModel):
    """ML   """

    articles: List[MLAnalyzeRequest] = Field(..., min_length=1, max_length=50)
    addons: Optional[List[str]] = None
    save_to_db: bool = True
    max_concurrent: int = Field(default=5, ge=1, le=20)


class MLAnalysisResult(BaseModel):
    """ML  """

    addon_type: str
    success: bool
    request_id: str
    results: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    latency_ms: int = 0
    analyzed_at: str


class BatchAnalysisResult(BaseModel):
    """  """

    article_id: int
    sentiment: Optional[MLAnalysisResult] = None
    factcheck: Optional[MLAnalysisResult] = None
    bias: Optional[MLAnalysisResult] = None
    total_latency_ms: int = 0
    success_count: int = 0
    failure_count: int = 0


class MLAddonDBEntry(BaseModel):
    """DB  ML Addon """

    id: int
    addon_key: str
    name: str
    description: Optional[str] = None
    endpoint_url: str
    version: Optional[str] = None
    status: str
    config: Optional[Dict[str, Any]] = None
    created_at: datetime
    updated_at: datetime


class MLAddonCreateRequest(BaseModel):
    """ML Addon  """

    addon_key: str = Field(..., description="  (: sentiment-addon)")
    name: str = Field(..., description=" ")
    description: Optional[str] = Field(None, description="")
    endpoint_url: str = Field(..., description=" URL")
    version: Optional[str] = Field(None, description="")
    config: Optional[Dict[str, Any]] = Field(None, description=" ")


class MLAddonUpdateRequest(BaseModel):
    """ML Addon  """

    name: Optional[str] = None
    description: Optional[str] = None
    endpoint_url: Optional[str] = None
    version: Optional[str] = None
    status: Optional[str] = None
    config: Optional[Dict[str, Any]] = None


# ============================================================================
# Helper functions
# ============================================================================


async def call_crawler_service(
    method: str,
    path: str,
    params: Optional[dict] = None,
    json_data: Optional[dict] = None,
    timeout: float = 60.0,
) -> dict:
    """Call the autonomous-crawler-service API"""
    url = f"{CRAWLER_SERVICE_URL}{path}"

    async with httpx.AsyncClient(timeout=timeout) as client:
        try:
            response = await client.request(
                method=method,
                url=url,
                params=params,
                json=json_data,
            )

            if response.status_code >= 400:
                detail = (
                    response.json().get("detail", response.text)
                    if response.text
                    else "Unknown error"
                )
                raise HTTPException(
                    status_code=response.status_code,
                    detail=detail,
                )

            if response.status_code == 204:
                return {}

            return response.json()
        except httpx.RequestError as e:
            raise HTTPException(
                status_code=503,
                detail=f"Crawler service unavailable: {str(e)}",
            )


# ============================================================================
# Endpoints - Health & Status
# ============================================================================


@router.get("/health", response_model=MLAddonHealthResponse)
async def ml_health_check(
    current_user=Depends(get_current_user),
):
    """
    ML  .

     ML Addon  .
    """
    result = await call_crawler_service("GET", "/ml/health")
    return result


@router.get("/status", response_model=MLAddonStatusResponse)
async def ml_status(
    current_user=Depends(get_current_user),
):
    """
    ML   .

       Addon   .
    """
    result = await call_crawler_service("GET", "/ml/status")
    return result


@router.get("/list", response_model=Dict[str, Any])
async def list_addons(
    current_user=Depends(get_current_user),
):
    """
      ML Addon .

         .
    """
    result = await call_crawler_service("GET", "/ml/addons")
    return result


# ============================================================================
# Endpoints - Analysis
# ============================================================================


@router.post("/analyze", response_model=BatchAnalysisResult)
async def analyze_article(
    request: MLAnalyzeRequest,
    current_user=Depends(get_current_user),
):
    """
      ML .

      sentiment, factcheck, bias  .
    """
    result = await call_crawler_service(
        "POST",
        "/ml/analyze",
        json_data=request.model_dump(exclude_none=True),
        timeout=120.0,
    )
    return result


@router.post("/analyze/simple")
async def analyze_text_simple(
    text: str = Query(..., min_length=10, description=" "),
    source: Optional[str] = Query(None, description=""),
    addons: Optional[str] = Query(None, description=" ( )"),
    current_user=Depends(get_current_user),
):
    """
      ML .

     ID    .
     DB  .
    """
    addon_list = addons.split(",") if addons else None

    result = await call_crawler_service(
        "POST",
        "/ml/analyze/simple",
        json_data={
            "text": text,
            "source": source,
            "addons": addon_list,
        },
        timeout=120.0,
    )
    return result


@router.post("/analyze/batch")
async def analyze_batch(
    request: MLBatchAnalyzeRequest,
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
      ML .

        . (Admin  )
    """
    result = await call_crawler_service(
        "POST",
        "/ml/analyze/batch",
        json_data=request.model_dump(exclude_none=True),
        timeout=300.0,
    )
    return result


@router.post("/analyze/url")
async def analyze_url(
    url: str = Query(..., description=" URL"),
    current_user=Depends(get_current_user),
):
    """
    URL   ML  .

    URL    sentiment, factcheck, bias  .
    """
    result = await call_crawler_service(
        "POST",
        "/ml/analyze/url",
        params={"url": url},
        timeout=120.0,
    )
    return result


# ============================================================================
# Endpoints - Configuration
# ============================================================================


@router.post("/config/toggle")
async def toggle_auto_analysis(
    enabled: bool = Query(..., description="   "),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
     ML  .

       ML   /. (Admin  )
    """
    result = await call_crawler_service(
        "POST",
        "/ml/config/toggle",
        params={"enabled": enabled},
    )

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="ml_config",
        resource_id="auto_analysis",
        resource_name="ML Auto Analysis",
        details={"enabled": enabled},
    )

    return result


# ============================================================================
# Endpoints - CRUD for ml_addon table (via collector service)
# ============================================================================

COLLECTOR_SERVICE_URL_DB = os.environ.get(
    "COLLECTOR_SERVICE_URL", "http://localhost:8081"
)


async def call_collector_service_db(
    method: str,
    path: str,
    params: Optional[dict] = None,
    json_data: Optional[dict] = None,
) -> dict:
    """Call the data-collection-service API for DB operations"""
    url = f"{COLLECTOR_SERVICE_URL_DB}{path}"

    async with httpx.AsyncClient(timeout=30.0) as client:
        try:
            response = await client.request(
                method=method,
                url=url,
                params=params,
                json=json_data,
            )

            if response.status_code >= 400:
                detail = (
                    response.json().get("message", response.text)
                    if response.text
                    else "Unknown error"
                )
                raise HTTPException(
                    status_code=response.status_code,
                    detail=detail,
                )

            if response.status_code == 204:
                return {}

            return response.json()
        except httpx.RequestError as e:
            raise HTTPException(
                status_code=503,
                detail=f"Collector service unavailable: {str(e)}",
            )


@router.get("/registered", response_model=List[MLAddonDBEntry])
async def list_registered_addons(
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    DB  ML Addon   (Admin  )
    """
    result = await call_collector_service_db("GET", "/api/v1/admin/ml-addons")
    return result


@router.get("/registered/{addon_id}", response_model=MLAddonDBEntry)
async def get_registered_addon(
    addon_id: int,
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
     ML Addon  (Admin  )
    """
    result = await call_collector_service_db(
        "GET", f"/api/v1/admin/ml-addons/{addon_id}"
    )
    return result


@router.post(
    "/registered", response_model=MLAddonDBEntry, status_code=status.HTTP_201_CREATED
)
async def create_addon(
    request: MLAddonCreateRequest,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
     ML Addon  (Admin  )
    """
    result = await call_collector_service_db(
        "POST",
        "/api/v1/admin/ml-addons",
        json_data=request.model_dump(exclude_none=True),
    )

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.CREATE,
        resource_type="ml_addon",
        resource_id=request.addon_key,
        resource_name=request.name,
        details={"endpoint": request.endpoint_url},
    )

    return result


@router.put("/registered/{addon_id}", response_model=MLAddonDBEntry)
async def update_addon(
    addon_id: int,
    request: MLAddonUpdateRequest,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    ML Addon  (Admin  )
    """
    result = await call_collector_service_db(
        "PUT",
        f"/api/v1/admin/ml-addons/{addon_id}",
        json_data=request.model_dump(exclude_none=True),
    )

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="ml_addon",
        resource_id=str(addon_id),
        resource_name=result.get("name", f"Addon {addon_id}"),
        details=request.model_dump(exclude_none=True),
    )

    return result


@router.delete("/registered/{addon_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_addon(
    addon_id: int,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    ML Addon  (Admin  )
    """
    await call_collector_service_db("DELETE", f"/api/v1/admin/ml-addons/{addon_id}")

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DELETE,
        resource_type="ml_addon",
        resource_id=str(addon_id),
        resource_name=f"Addon {addon_id}",
    )


@router.post("/registered/{addon_id}/test")
async def test_addon(
    addon_id: int,
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
    ML Addon   (Admin  )
    """
    result = await call_collector_service_db(
        "POST", f"/api/v1/admin/ml-addons/{addon_id}/test"
    )
    return result

```

---

## backend/admin-dashboard/api/routers/ml_training.py

```py
"""
ML Training Router - ML    API 
"""

import os
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional

import httpx
from fastapi import APIRouter, Depends, HTTPException, Query, status
from pydantic import BaseModel, Field

from ..models.schemas import (
    AuditAction,
    UserRole,
)
from ..dependencies import (
    get_audit_service,
    get_current_user,
    require_role,
)

router = APIRouter(prefix="/ml-training", tags=["ML Training"])


# ============================================================================
# Configuration
# ============================================================================

ML_TRAINER_URL = os.environ.get("ML_TRAINER_URL", "http://ml-trainer:8103")


# ============================================================================
# Schemas
# ============================================================================


class TrainingJobStatus(str, Enum):
    """  """

    PENDING = "pending"
    PREPARING = "preparing"
    TRAINING = "training"
    EVALUATING = "evaluating"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


class ModelType(str, Enum):
    """ """

    SENTIMENT = "sentiment"
    FACTCHECK = "factcheck"
    BIAS = "bias"
    CUSTOM = "custom"


class DatasetSource(str, Enum):
    """ """

    HUGGINGFACE = "huggingface"
    LOCAL = "local"
    URL = "url"
    DATABASE = "database"


class TrainingJobCreate(BaseModel):
    """   """

    name: str = Field(..., description=" ")
    description: Optional[str] = Field(None, description="")
    model_type: ModelType = Field(..., description=" ")
    base_model: str = Field(
        ..., description="  (: monologg/koelectra-base-v3-discriminator)"
    )

    # Dataset configuration
    dataset_source: DatasetSource = Field(
        DatasetSource.HUGGINGFACE, description=" "
    )
    dataset_name: Optional[str] = Field(
        None, description="  (HuggingFace   )"
    )
    dataset_split: str = Field("train", description=" ")
    text_column: str = Field("text", description=" ")
    label_column: str = Field("label", description=" ")

    # Training hyperparameters
    num_epochs: int = Field(3, ge=1, le=100, description=" ")
    batch_size: int = Field(16, ge=1, le=256, description=" ")
    learning_rate: float = Field(2e-5, ge=1e-7, le=1e-2, description="")
    warmup_ratio: float = Field(0.1, ge=0, le=1, description=" ")
    weight_decay: float = Field(0.01, ge=0, le=1, description=" ")
    max_seq_length: int = Field(512, ge=32, le=2048, description="  ")

    # Output configuration
    output_dir: Optional[str] = Field(None, description=" ")
    save_steps: int = Field(500, description="  ")
    eval_steps: int = Field(500, description="  ")

    # Additional options
    fp16: bool = Field(False, description="FP16  ")
    gradient_accumulation_steps: int = Field(
        1, ge=1, le=64, description="  "
    )


class TrainingJob(BaseModel):
    """ """

    id: str
    name: str
    description: Optional[str] = None
    model_type: ModelType
    base_model: str
    status: TrainingJobStatus

    # Progress
    progress: float = Field(0.0, description=" (0-100)")
    current_epoch: int = 0
    total_epochs: int = 0
    current_step: int = 0
    total_steps: int = 0

    # Metrics
    train_loss: Optional[float] = None
    eval_loss: Optional[float] = None
    eval_accuracy: Optional[float] = None
    eval_f1: Optional[float] = None

    # Timing
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    elapsed_seconds: int = 0
    estimated_remaining_seconds: Optional[int] = None

    # Output
    output_path: Optional[str] = None
    model_size_mb: Optional[float] = None

    # Error handling
    error_message: Optional[str] = None

    created_at: datetime
    updated_at: datetime


class TrainingJobList(BaseModel):
    """  """

    jobs: List[TrainingJob]
    total: int
    pending: int
    running: int
    completed: int
    failed: int


class ModelInfo(BaseModel):
    """  """

    id: str
    name: str
    model_type: ModelType
    base_model: str
    training_job_id: str

    # Performance metrics
    accuracy: Optional[float] = None
    f1_score: Optional[float] = None
    precision: Optional[float] = None
    recall: Optional[float] = None

    # Model details
    model_path: str
    size_mb: float
    num_labels: int
    label_mapping: Optional[Dict[str, int]] = None

    # Deployment
    is_deployed: bool = False
    deployed_at: Optional[datetime] = None
    addon_key: Optional[str] = None

    created_at: datetime


class ModelList(BaseModel):
    """  """

    models: List[ModelInfo]
    total: int


class DeployRequest(BaseModel):
    """  """

    addon_key: str = Field(..., description="   (: sentiment-addon)")
    replace_current: bool = Field(True, description="   ")


class TrainingMetrics(BaseModel):
    """ """

    job_id: str
    step: int
    epoch: float
    train_loss: Optional[float] = None
    eval_loss: Optional[float] = None
    eval_accuracy: Optional[float] = None
    eval_f1: Optional[float] = None
    learning_rate: Optional[float] = None
    timestamp: datetime


# ============================================================================
# Helper functions
# ============================================================================


async def call_trainer_service(
    method: str,
    path: str,
    params: Optional[dict] = None,
    json_data: Optional[dict] = None,
    timeout: float = 30.0,
) -> dict:
    """Call the ml-trainer service API"""
    url = f"{ML_TRAINER_URL}{path}"

    async with httpx.AsyncClient(timeout=timeout) as client:
        try:
            response = await client.request(
                method=method,
                url=url,
                params=params,
                json=json_data,
            )

            if response.status_code >= 400:
                detail = (
                    response.json().get("detail", response.text)
                    if response.text
                    else "Unknown error"
                )
                raise HTTPException(
                    status_code=response.status_code,
                    detail=detail,
                )

            if response.status_code == 204:
                return {}

            return response.json()
        except httpx.RequestError as e:
            raise HTTPException(
                status_code=503,
                detail=f"ML Trainer service unavailable: {str(e)}",
            )


# ============================================================================
# Endpoints - Health & Status
# ============================================================================


@router.get("/health")
async def trainer_health_check(
    current_user=Depends(get_current_user),
):
    """
    ML Trainer  .
    """
    try:
        result = await call_trainer_service("GET", "/health")
        return result
    except HTTPException as e:
        if e.status_code == 503:
            return {
                "status": "unhealthy",
                "message": "ML Trainer service is unavailable",
            }
        raise


@router.get("/status")
async def trainer_status(
    current_user=Depends(get_current_user),
):
    """
    ML Trainer   .

           .
    """
    result = await call_trainer_service("GET", "/status")
    return result


# ============================================================================
# Endpoints - Training Jobs
# ============================================================================


@router.get("/jobs", response_model=TrainingJobList)
async def list_training_jobs(
    status: Optional[TrainingJobStatus] = Query(None, description=" "),
    model_type: Optional[ModelType] = Query(None, description="  "),
    limit: int = Query(20, ge=1, le=100, description=" "),
    offset: int = Query(0, ge=0, description=""),
    current_user=Depends(get_current_user),
):
    """
       .
    """
    params = {"limit": limit, "offset": offset}
    if status:
        params["status"] = status.value
    if model_type:
        params["model_type"] = model_type.value

    result = await call_trainer_service("GET", "/jobs", params=params)
    return result


@router.get("/jobs/{job_id}", response_model=TrainingJob)
async def get_training_job(
    job_id: str,
    current_user=Depends(get_current_user),
):
    """
       .
    """
    result = await call_trainer_service("GET", f"/jobs/{job_id}")
    return result


@router.post("/jobs", response_model=TrainingJob, status_code=status.HTTP_201_CREATED)
async def create_training_job(
    request: TrainingJobCreate,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
       . (Admin  )

        .
    """
    result = await call_trainer_service(
        "POST",
        "/jobs",
        json_data=request.model_dump(exclude_none=True),
    )

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.CREATE,
        resource_type="training_job",
        resource_id=result.get("id", "unknown"),
        resource_name=request.name,
        details={
            "model_type": request.model_type.value,
            "base_model": request.base_model,
            "epochs": request.num_epochs,
        },
    )

    return result


@router.post("/jobs/{job_id}/start")
async def start_training_job(
    job_id: str,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
      . (Admin  )
    """
    result = await call_trainer_service("POST", f"/jobs/{job_id}/start", timeout=60.0)

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="training_job",
        resource_id=job_id,
        resource_name=f"Training Job {job_id}",
        details={"action": "start"},
    )

    return result


@router.post("/jobs/{job_id}/cancel")
async def cancel_training_job(
    job_id: str,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
      . (Admin  )
    """
    result = await call_trainer_service("POST", f"/jobs/{job_id}/cancel")

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="training_job",
        resource_id=job_id,
        resource_name=f"Training Job {job_id}",
        details={"action": "cancel"},
    )

    return result


@router.delete("/jobs/{job_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_training_job(
    job_id: str,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
      . (Admin  )

        .
    """
    await call_trainer_service("DELETE", f"/jobs/{job_id}")

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DELETE,
        resource_type="training_job",
        resource_id=job_id,
        resource_name=f"Training Job {job_id}",
    )


@router.get("/jobs/{job_id}/metrics", response_model=List[TrainingMetrics])
async def get_training_metrics(
    job_id: str,
    limit: int = Query(100, ge=1, le=1000, description=" "),
    current_user=Depends(get_current_user),
):
    """
       .

        loss, accuracy   .
    """
    result = await call_trainer_service(
        "GET", f"/jobs/{job_id}/metrics", params={"limit": limit}
    )
    return result


@router.get("/jobs/{job_id}/logs")
async def get_training_logs(
    job_id: str,
    lines: int = Query(100, ge=1, le=1000, description="  "),
    current_user=Depends(get_current_user),
):
    """
       .
    """
    result = await call_trainer_service(
        "GET", f"/jobs/{job_id}/logs", params={"lines": lines}
    )
    return result


# ============================================================================
# Endpoints - Models
# ============================================================================


@router.get("/models", response_model=ModelList)
async def list_models(
    model_type: Optional[ModelType] = Query(None, description="  "),
    deployed_only: bool = Query(False, description=" "),
    limit: int = Query(20, ge=1, le=100, description=" "),
    offset: int = Query(0, ge=0, description=""),
    current_user=Depends(get_current_user),
):
    """
       .
    """
    params = {"limit": limit, "offset": offset, "deployed_only": deployed_only}
    if model_type:
        params["model_type"] = model_type.value

    result = await call_trainer_service("GET", "/models", params=params)
    return result


@router.get("/models/{model_id}", response_model=ModelInfo)
async def get_model(
    model_id: str,
    current_user=Depends(get_current_user),
):
    """
      .
    """
    result = await call_trainer_service("GET", f"/models/{model_id}")
    return result


@router.post("/models/{model_id}/deploy")
async def deploy_model(
    model_id: str,
    request: DeployRequest,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
     ML Addon . (Admin  )

       ML Addon  .
    """
    result = await call_trainer_service(
        "POST",
        f"/models/{model_id}/deploy",
        json_data=request.model_dump(),
        timeout=120.0,
    )

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="ml_model",
        resource_id=model_id,
        resource_name=f"Model {model_id}",
        details={
            "action": "deploy",
            "addon_key": request.addon_key,
            "replace_current": request.replace_current,
        },
    )

    return result


@router.delete("/models/{model_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_model(
    model_id: str,
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """
      . (Admin  )

        .
    """
    await call_trainer_service("DELETE", f"/models/{model_id}")

    # Audit log
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DELETE,
        resource_type="ml_model",
        resource_id=model_id,
        resource_name=f"Model {model_id}",
    )


# ============================================================================
# Endpoints - Datasets
# ============================================================================


@router.get("/datasets")
async def list_datasets(
    source: Optional[DatasetSource] = Query(None, description="  "),
    current_user=Depends(get_current_user),
):
    """
        .
    """
    params = {}
    if source:
        params["source"] = source.value

    result = await call_trainer_service("GET", "/datasets", params=params)
    return result


@router.post("/datasets/search")
async def search_huggingface_datasets(
    query: str = Query(..., description=" "),
    task: Optional[str] = Query(
        None, description="  (text-classification )"
    ),
    language: Optional[str] = Query("ko", description=""),
    limit: int = Query(10, ge=1, le=50),
    current_user=Depends(get_current_user),
):
    """
    HuggingFace  .
    """
    params = {"query": query, "limit": limit}
    if task:
        params["task"] = task
    if language:
        params["language"] = language

    result = await call_trainer_service("POST", "/datasets/search", params=params)
    return result


@router.get("/datasets/{dataset_name}/preview")
async def preview_dataset(
    dataset_name: str,
    split: str = Query("train", description=" "),
    num_samples: int = Query(5, ge=1, le=20, description=" "),
    current_user=Depends(get_current_user),
):
    """
     .

        .
    """
    result = await call_trainer_service(
        "GET",
        f"/datasets/{dataset_name}/preview",
        params={"split": split, "num_samples": num_samples},
    )
    return result


# ============================================================================
# Endpoints - Presets
# ============================================================================


@router.get("/presets")
async def list_training_presets(
    current_user=Depends(get_current_user),
):
    """
      .

        .
    """
    presets = [
        {
            "id": "korean-sentiment-koelectra",
            "name": "  (KoELECTRA)",
            "model_type": "sentiment",
            "base_model": "monologg/koelectra-base-v3-discriminator",
            "recommended_datasets": [
                "nsmc",
                "klue/ner",
            ],
            "default_config": {
                "num_epochs": 3,
                "batch_size": 32,
                "learning_rate": 2e-5,
                "max_seq_length": 128,
            },
        },
        {
            "id": "korean-bias-kcbert",
            "name": "  (KcBERT)",
            "model_type": "bias",
            "base_model": "beomi/KcBERT-base",
            "recommended_datasets": [],
            "default_config": {
                "num_epochs": 5,
                "batch_size": 16,
                "learning_rate": 3e-5,
                "max_seq_length": 256,
            },
        },
        {
            "id": "multilingual-factcheck",
            "name": "  (mBERT)",
            "model_type": "factcheck",
            "base_model": "bert-base-multilingual-cased",
            "recommended_datasets": [],
            "default_config": {
                "num_epochs": 5,
                "batch_size": 16,
                "learning_rate": 2e-5,
                "max_seq_length": 512,
            },
        },
    ]
    return {"presets": presets, "total": len(presets)}


@router.post("/presets/{preset_id}/apply", response_model=TrainingJobCreate)
async def apply_preset(
    preset_id: str,
    dataset_name: Optional[str] = Query(
        None, description="  ( )"
    ),
    current_user=Depends(get_current_user),
):
    """
     .

         .
    """
    result = await call_trainer_service(
        "POST",
        f"/presets/{preset_id}/apply",
        params={"dataset_name": dataset_name} if dataset_name else None,
    )
    return result

```

---

## backend/admin-dashboard/api/routers/public_auth.py

```py
"""
Public Auth Router -    API ( )

- :  
- :  
-  :  
- :  
-  :  
"""

from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.security import OAuth2PasswordRequestForm
from pydantic import BaseModel, EmailStr, Field

from ..models.schemas import AuditAction, Token, User, UserRegister, UserRole
from ..dependencies import (
    get_audit_service,
    get_auth_service,
    get_current_user,
)

router = APIRouter(prefix="/auth", tags=["Public Authentication"])


class RegisterRequest(BaseModel):
    """ """

    username: str = Field(..., min_length=3, max_length=50, description="")
    email: EmailStr = Field(..., description=" ")
    password: str = Field(..., min_length=8, description=" (8 )")


class ChangePasswordRequest(BaseModel):
    """  """

    old_password: str
    new_password: str = Field(..., min_length=8, description="  (8 )")


class UserPublicResponse(BaseModel):
    """   (  )"""

    id: str
    username: str
    email: str | None
    role: UserRole
    created_at: str


# ============================================================================
# Public Endpoints (No Auth Required)
# ============================================================================
@router.post("/register", response_model=Token, status_code=status.HTTP_201_CREATED)
async def register(
    request: RegisterRequest,
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """
     ( API)

    -  3-50
    -  8 
    -    
    -     
    """
    try:
        #    (role: USER)
        user = auth_service.register_user(
            username=request.username,
            email=request.email,
            password=request.password,
        )

        #      
        token = auth_service.create_access_token(user)

        #  
        audit_service.log(
            user_id=user.id,
            username=user.username,
            action=AuditAction.CREATE,
            resource_type="user_registration",
            resource_id=user.id,
            success=True,
            details={"email": request.email},
        )

        return token

    except ValueError as e:
        #  / 
        audit_service.log(
            user_id="unknown",
            username=request.username,
            action=AuditAction.CREATE,
            resource_type="user_registration",
            success=False,
            error_message=str(e),
        )
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e),
        )


@router.post("/login", response_model=Token)
async def login(
    form_data: OAuth2PasswordRequestForm = Depends(),
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """
     ( API)

         
    """
    user = auth_service.authenticate(form_data.username, form_data.password)

    if not user:
        audit_service.log(
            user_id="unknown",
            username=form_data.username,
            action=AuditAction.LOGIN,
            resource_type="auth",
            success=False,
            error_message="Invalid credentials",
        )
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="    ",
            headers={"WWW-Authenticate": "Bearer"},
        )

    token = auth_service.create_access_token(user)

    audit_service.log(
        user_id=user.id,
        username=user.username,
        action=AuditAction.LOGIN,
        resource_type="auth",
        success=True,
    )

    return token


@router.post("/token", response_model=Token)
async def login_for_access_token(
    form_data: OAuth2PasswordRequestForm = Depends(),
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """OAuth2   """
    return await login(form_data, auth_service, audit_service)


# ============================================================================
# Protected Endpoints (Auth Required)
# ============================================================================
@router.get("/me", response_model=User)
async def get_current_user_info(
    current_user=Depends(get_current_user),
):
    """   """
    return current_user


@router.post("/logout")
async def logout(
    current_user=Depends(get_current_user),
    audit_service=Depends(get_audit_service),
):
    """"""
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.LOGOUT,
        resource_type="auth",
    )

    return {"success": True, "message": ""}


@router.post("/change-password")
async def change_password(
    request: ChangePasswordRequest,
    current_user=Depends(get_current_user),
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """ """
    success = auth_service.change_password(
        user_id=current_user.id,
        old_password=request.old_password,
        new_password=request.new_password,
    )

    if not success:
        audit_service.log(
            user_id=current_user.id,
            username=current_user.username,
            action=AuditAction.UPDATE,
            resource_type="password",
            success=False,
            error_message="Invalid old password",
        )
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="   ",
        )

    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="password",
        success=True,
    )

    return {"success": True, "message": " "}


@router.delete("/me")
async def delete_my_account(
    current_user=Depends(get_current_user),
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """
       ()

    :     .
    """
    #     
    if current_user.role == UserRole.ADMIN:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="     ",
        )

    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DELETE,
        resource_type="user_account",
        resource_id=current_user.id,
        success=True,
    )

    auth_service.delete_user(current_user.id)

    return {"success": True, "message": " "}


# ============================================================================
# Username/Email Availability Check (Public)
# ============================================================================
@router.get("/check-username/{username}")
async def check_username_availability(
    username: str,
    auth_service=Depends(get_auth_service),
):
    """    """
    exists = auth_service.get_user_by_username(username) is not None
    return {
        "username": username,
        "available": not exists,
    }


@router.get("/check-email/{email}")
async def check_email_availability(
    email: str,
    auth_service=Depends(get_auth_service),
):
    """    """
    exists = auth_service.get_user_by_email(email) is not None
    return {
        "email": email,
        "available": not exists,
    }


# ============================================================================
# Email Verification Endpoints (for Registration)
# ============================================================================
class SendVerificationRequest(BaseModel):
    """  """
    username: str = Field(..., min_length=3, max_length=50, description="")
    email: EmailStr = Field(..., description=" ")
    password: str = Field(..., min_length=8, description=" (8 )")


class VerifyEmailRequest(BaseModel):
    """    """
    email: EmailStr = Field(..., description=" ")
    code: str = Field(..., min_length=6, max_length=6, description="6  ")


class ResendVerificationRequest(BaseModel):
    """   """
    email: EmailStr = Field(..., description=" ")


@router.post("/send-verification")
async def send_verification_code(
    request: SendVerificationRequest,
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """
        ( 1)
    
    - , ,     
    -       
    - 10  6   
    """
    try:
        code = auth_service.create_email_verification(
            email=request.email,
            username=request.username,
            password=request.password,
        )
        
        # TODO:     (SMTP, SendGrid, AWS SES )
        # await send_email(
        #     to=request.email,
        #     subject="[NewsInsight]   ",
        #     body=f" : {code}\n\n10  ."
        # )
        
        audit_service.log(
            user_id="unknown",
            username=request.username,
            action=AuditAction.CREATE,
            resource_type="email_verification",
            success=True,
            details={"email": request.email},
        )
        
        # NOTE:      
        # /    DEBUG_EMAIL_CODE  
        import os
        response = {
            "success": True,
            "message": "   .",
            "email": request.email,
            "expires_in": 600,  # 10
        }
        
        #     (DEBUG_EMAIL_CODE=true  )
        if os.getenv("DEBUG_EMAIL_CODE", "false").lower() == "true":
            response["code"] = code
            
        return response
        
    except ValueError as e:
        audit_service.log(
            user_id="unknown",
            username=request.username,
            action=AuditAction.CREATE,
            resource_type="email_verification",
            success=False,
            error_message=str(e),
        )
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e),
        )


@router.post("/verify-email", response_model=Token)
async def verify_email_code(
    request: VerifyEmailRequest,
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """
           ( 2)
    
    -       
    -      
    -  5  
    """
    try:
        user = auth_service.verify_email_code(
            email=request.email,
            code=request.code,
        )
        
        #  
        token = auth_service.create_access_token(user)
        
        audit_service.log(
            user_id=user.id,
            username=user.username,
            action=AuditAction.CREATE,
            resource_type="user_registration",
            resource_id=user.id,
            success=True,
            details={"email": request.email, "verified": True},
        )
        
        return token
        
    except ValueError as e:
        audit_service.log(
            user_id="unknown",
            username="unknown",
            action=AuditAction.UPDATE,
            resource_type="email_verification",
            success=False,
            error_message=str(e),
            details={"email": request.email},
        )
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e),
        )


@router.post("/resend-verification")
async def resend_verification_code(
    request: ResendVerificationRequest,
    auth_service=Depends(get_auth_service),
    audit_service=Depends(get_audit_service),
):
    """
      
    
    -      
    -  6      
    """
    try:
        code = auth_service.resend_verification_code(request.email)
        
        # TODO:    
        
        audit_service.log(
            user_id="unknown",
            username="unknown",
            action=AuditAction.UPDATE,
            resource_type="email_verification",
            success=True,
            details={"email": request.email, "action": "resend"},
        )
        
        # NOTE:      
        import os
        response = {
            "success": True,
            "message": "  .",
            "email": request.email,
            "expires_in": 600,
        }
        
        #     (DEBUG_EMAIL_CODE=true  )
        if os.getenv("DEBUG_EMAIL_CODE", "false").lower() == "true":
            response["code"] = code
            
        return response
        
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e),
        )

```

---

## backend/admin-dashboard/api/routers/scripts.py

```py
"""
Script Router - /  API 
"""
from typing import Any, Optional

from fastapi import APIRouter, Depends, HTTPException, Query, status
from fastapi.responses import StreamingResponse

from ..models.schemas import (
    AuditAction,
    Script,
    ScriptCreate,
    ScriptUpdate,
    TaskExecution,
    TaskExecutionRequest,
    TaskStatus,
    UserRole,
)
from ..dependencies import (
    get_audit_service,
    get_current_user,
    get_current_user_optional,
    get_environment_service,
    get_script_service,
    require_role,
)

router = APIRouter(prefix="/scripts", tags=["Scripts"])


@router.get("", response_model=list[Script])
async def list_scripts(
    environment: Optional[str] = Query(None, description="  "),
    tag: Optional[str] = Query(None, description=" "),
    script_service=Depends(get_script_service),
    current_user=Depends(get_current_user_optional),
):
    """   (   ) -  """
    return script_service.list_scripts(
        environment=environment,
        tag=tag,
        role=current_user.role,
    )


# NOTE: /executions  /{script_id}     (FastAPI   )
@router.get("/executions", response_model=list[TaskExecution])
async def list_executions(
    script_id: Optional[str] = Query(None, description=" ID "),
    environment_id: Optional[str] = Query(None, description=" ID "),
    status: Optional[TaskStatus] = Query(None, description=" "),
    limit: int = Query(50, ge=1, le=200, description=" "),
    script_service=Depends(get_script_service),
    current_user=Depends(get_current_user_optional),
):
    """   -  """
    return script_service.list_executions(
        script_id=script_id,
        environment_id=environment_id,
        status=status,
        limit=limit,
    )


@router.get("/executions/{execution_id}", response_model=TaskExecution)
async def get_execution(
    execution_id: str,
    script_service=Depends(get_script_service),
    current_user=Depends(get_current_user_optional),
):
    """   -  """
    execution = script_service.get_execution(execution_id)
    if not execution:
        raise HTTPException(status_code=404, detail="Execution not found")
    return execution


@router.post("/executions/{execution_id}/cancel")
async def cancel_execution(
    execution_id: str,
    script_service=Depends(get_script_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.OPERATOR)),
):
    """   """
    execution = script_service.get_execution(execution_id)
    if not execution:
        raise HTTPException(status_code=404, detail="Execution not found")

    if execution.status != TaskStatus.RUNNING:
        raise HTTPException(status_code=400, detail="Execution is not running")

    if not script_service.cancel_execution(execution_id):
        raise HTTPException(status_code=500, detail="Failed to cancel execution")

    #  
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.EXECUTE,
        resource_type="execution",
        resource_id=execution_id,
        details={"action": "cancel"},
    )

    return {"success": True, "message": "Execution cancelled"}


@router.get("/{script_id}", response_model=Script)
async def get_script(
    script_id: str,
    script_service=Depends(get_script_service),
    current_user=Depends(get_current_user_optional),
):
    """   -  """
    script = script_service.get_script(script_id)
    if not script:
        raise HTTPException(status_code=404, detail="Script not found")
    return script


@router.post("", response_model=Script, status_code=status.HTTP_201_CREATED)
async def create_script(
    data: ScriptCreate,
    script_service=Depends(get_script_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """  (Admin  )"""
    script = script_service.create_script(data)

    #  
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.CREATE,
        resource_type="script",
        resource_id=script.id,
        resource_name=script.name,
        details={"data": data.model_dump()},
    )

    return script


@router.patch("/{script_id}", response_model=Script)
async def update_script(
    script_id: str,
    data: ScriptUpdate,
    script_service=Depends(get_script_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """  (Admin  )"""
    script = script_service.update_script(script_id, data)
    if not script:
        raise HTTPException(status_code=404, detail="Script not found")

    #  
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.UPDATE,
        resource_type="script",
        resource_id=script.id,
        resource_name=script.name,
        details={"changes": data.model_dump(exclude_unset=True)},
    )

    return script


@router.delete("/{script_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_script(
    script_id: str,
    script_service=Depends(get_script_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(require_role(UserRole.ADMIN)),
):
    """  (Admin  )"""
    script = script_service.get_script(script_id)
    if not script:
        raise HTTPException(status_code=404, detail="Script not found")

    if not script_service.delete_script(script_id):
        raise HTTPException(status_code=500, detail="Failed to delete script")

    #  
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.DELETE,
        resource_type="script",
        resource_id=script_id,
        resource_name=script.name,
    )


@router.post("/execute", response_model=TaskExecution)
async def execute_script(
    request: TaskExecutionRequest,
    script_service=Depends(get_script_service),
    env_service=Depends(get_environment_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(get_current_user),
):
    """ """
    #  
    script = script_service.get_script(request.script_id)
    if not script:
        raise HTTPException(status_code=404, detail="Script not found")

    #  
    env = env_service.get_environment(request.environment_id)
    if not env:
        raise HTTPException(status_code=404, detail="Environment not found")

    #  
    from ..services.auth_service import AuthService

    if not AuthService.check_permission(
        AuthService, current_user.role, script.required_role
    ):
        raise HTTPException(
            status_code=403,
            detail=f"Requires {script.required_role.value} permission",
        )

    #   
    if script.allowed_environments and env.name not in script.allowed_environments:
        raise HTTPException(
            status_code=400,
            detail=f"Script not allowed for environment: {env.name}",
        )

    try:
        execution = await script_service.execute_script(
            script_id=request.script_id,
            environment_name=env.name,
            compose_file=env.compose_file,
            parameters=request.parameters,
            executed_by=current_user.username,
        )

        #  
        audit_service.log(
            user_id=current_user.id,
            username=current_user.username,
            action=AuditAction.EXECUTE,
            resource_type="script",
            resource_id=script.id,
            resource_name=script.name,
            environment_id=env.id,
            environment_name=env.name,
            details={
                "execution_id": execution.id,
                "parameters": request.parameters,
            },
        )

        return execution

    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.post("/execute/stream")
async def execute_script_stream(
    request: TaskExecutionRequest,
    script_service=Depends(get_script_service),
    env_service=Depends(get_environment_service),
    audit_service=Depends(get_audit_service),
    current_user=Depends(get_current_user),
):
    """  (  )"""
    #  
    script = script_service.get_script(request.script_id)
    if not script:
        raise HTTPException(status_code=404, detail="Script not found")

    #  
    env = env_service.get_environment(request.environment_id)
    if not env:
        raise HTTPException(status_code=404, detail="Environment not found")

    #  
    role_priority = {UserRole.VIEWER: 0, UserRole.OPERATOR: 1, UserRole.ADMIN: 2}
    if role_priority.get(current_user.role, 0) < role_priority.get(
        script.required_role, 0
    ):
        raise HTTPException(
            status_code=403,
            detail=f"Requires {script.required_role.value} permission",
        )

    #   
    if script.allowed_environments and env.name not in script.allowed_environments:
        raise HTTPException(
            status_code=400,
            detail=f"Script not allowed for environment: {env.name}",
        )

    #  
    audit_service.log(
        user_id=current_user.id,
        username=current_user.username,
        action=AuditAction.EXECUTE,
        resource_type="script",
        resource_id=script.id,
        resource_name=script.name,
        environment_id=env.id,
        environment_name=env.name,
        details={"parameters": request.parameters, "streaming": True},
    )

    async def generate():
        async for line in script_service.stream_execution_output(
            script_id=request.script_id,
            environment_name=env.name,
            compose_file=env.compose_file,
            parameters=request.parameters,
            executed_by=current_user.username,
        ):
            yield line

    return StreamingResponse(
        generate(),
        media_type="text/plain",
        headers={"X-Content-Type-Options": "nosniff"},
    )


# NOTE: /executions    (/{script_id}    )

```

---

## backend/admin-dashboard/api/services/__init__.py

```py
# Admin Dashboard Services

```

---

## backend/admin-dashboard/api/services/audit_service.py

```py
"""
Audit Service -    
"""
import json
from datetime import datetime
from pathlib import Path
from typing import Any, Optional
from uuid import uuid4

from ..models.schemas import AuditAction, AuditLog, AuditLogFilter


class AuditService:
    """  """

    def __init__(self, config_dir: str, max_logs: int = 10000):
        self.config_dir = Path(config_dir)
        self.logs_file = self.config_dir / "audit_logs.jsonl"
        self.max_logs = max_logs
        self._ensure_log_file()

    def _ensure_log_file(self) -> None:
        """   """
        self.config_dir.mkdir(parents=True, exist_ok=True)
        if not self.logs_file.exists():
            self.logs_file.touch()

    def log(
        self,
        user_id: str,
        username: str,
        action: AuditAction,
        resource_type: str,
        resource_id: Optional[str] = None,
        resource_name: Optional[str] = None,
        environment_id: Optional[str] = None,
        environment_name: Optional[str] = None,
        details: Optional[dict[str, Any]] = None,
        ip_address: Optional[str] = None,
        user_agent: Optional[str] = None,
        success: bool = True,
        error_message: Optional[str] = None,
    ) -> AuditLog:
        """  """
        log_entry = AuditLog(
            id=f"audit-{uuid4().hex[:12]}",
            user_id=user_id,
            username=username,
            action=action,
            resource_type=resource_type,
            resource_id=resource_id,
            resource_name=resource_name,
            environment_id=environment_id,
            environment_name=environment_name,
            details=details or {},
            ip_address=ip_address,
            user_agent=user_agent,
            timestamp=datetime.utcnow(),
            success=success,
            error_message=error_message,
        )

        # JSONL  
        with open(self.logs_file, "a") as f:
            f.write(log_entry.model_dump_json() + "\n")

        #    
        self._rotate_if_needed()

        return log_entry

    def _rotate_if_needed(self) -> None:
        """   """
        try:
            with open(self.logs_file, "r") as f:
                lines = f.readlines()

            if len(lines) > self.max_logs:
                #    ( max_logs )
                with open(self.logs_file, "w") as f:
                    f.writelines(lines[-self.max_logs :])
        except Exception:
            pass

    def get_logs(
        self,
        filter_params: Optional[AuditLogFilter] = None,
        page: int = 1,
        page_size: int = 50,
    ) -> tuple[list[AuditLog], int]:
        """  """
        logs = []

        try:
            with open(self.logs_file, "r") as f:
                for line in f:
                    if line.strip():
                        try:
                            data = json.loads(line)
                            log = AuditLog(**data)
                            logs.append(log)
                        except (json.JSONDecodeError, Exception):
                            continue
        except FileNotFoundError:
            return [], 0

        #  
        if filter_params:
            logs = self._apply_filter(logs, filter_params)

        #  
        logs.sort(key=lambda x: x.timestamp, reverse=True)

        total = len(logs)

        # 
        start = (page - 1) * page_size
        end = start + page_size
        paginated_logs = logs[start:end]

        return paginated_logs, total

    def _apply_filter(
        self, logs: list[AuditLog], filter_params: AuditLogFilter
    ) -> list[AuditLog]:
        """ """
        filtered = logs

        if filter_params.user_id:
            filtered = [l for l in filtered if l.user_id == filter_params.user_id]

        if filter_params.action:
            filtered = [l for l in filtered if l.action == filter_params.action]

        if filter_params.resource_type:
            filtered = [
                l for l in filtered if l.resource_type == filter_params.resource_type
            ]

        if filter_params.environment_id:
            filtered = [
                l for l in filtered if l.environment_id == filter_params.environment_id
            ]

        if filter_params.start_date:
            filtered = [
                l for l in filtered if l.timestamp >= filter_params.start_date
            ]

        if filter_params.end_date:
            filtered = [l for l in filtered if l.timestamp <= filter_params.end_date]

        if filter_params.success is not None:
            filtered = [l for l in filtered if l.success == filter_params.success]

        return filtered

    def get_log_by_id(self, log_id: str) -> Optional[AuditLog]:
        """  """
        try:
            with open(self.logs_file, "r") as f:
                for line in f:
                    if line.strip():
                        try:
                            data = json.loads(line)
                            if data.get("id") == log_id:
                                return AuditLog(**data)
                        except (json.JSONDecodeError, Exception):
                            continue
        except FileNotFoundError:
            pass

        return None

    def get_user_activity(
        self, user_id: str, limit: int = 100
    ) -> list[AuditLog]:
        """   """
        logs, _ = self.get_logs(
            filter_params=AuditLogFilter(user_id=user_id),
            page=1,
            page_size=limit,
        )
        return logs

    def get_resource_history(
        self, resource_type: str, resource_id: str, limit: int = 100
    ) -> list[AuditLog]:
        """   """
        logs = []

        try:
            with open(self.logs_file, "r") as f:
                for line in f:
                    if line.strip():
                        try:
                            data = json.loads(line)
                            if (
                                data.get("resource_type") == resource_type
                                and data.get("resource_id") == resource_id
                            ):
                                logs.append(AuditLog(**data))
                        except (json.JSONDecodeError, Exception):
                            continue
        except FileNotFoundError:
            pass

        logs.sort(key=lambda x: x.timestamp, reverse=True)
        return logs[:limit]

    def get_statistics(
        self, start_date: Optional[datetime] = None, end_date: Optional[datetime] = None
    ) -> dict[str, Any]:
        """  """
        logs, total = self.get_logs(
            filter_params=AuditLogFilter(start_date=start_date, end_date=end_date),
            page=1,
            page_size=self.max_logs,
        )

        #  
        action_counts = {}
        for log in logs:
            action = log.action.value
            action_counts[action] = action_counts.get(action, 0) + 1

        #  
        user_counts = {}
        for log in logs:
            user = log.username
            user_counts[user] = user_counts.get(user, 0) + 1

        #   
        resource_counts = {}
        for log in logs:
            resource = log.resource_type
            resource_counts[resource] = resource_counts.get(resource, 0) + 1

        # / 
        success_count = sum(1 for l in logs if l.success)
        failure_count = total - success_count

        return {
            "total_logs": total,
            "action_counts": action_counts,
            "user_counts": user_counts,
            "resource_counts": resource_counts,
            "success_count": success_count,
            "failure_count": failure_count,
        }

    def clear_old_logs(self, days: int = 90) -> int:
        """  """
        cutoff = datetime.utcnow().replace(
            hour=0, minute=0, second=0, microsecond=0
        )
        from datetime import timedelta

        cutoff = cutoff - timedelta(days=days)

        kept_logs = []
        deleted_count = 0

        try:
            with open(self.logs_file, "r") as f:
                for line in f:
                    if line.strip():
                        try:
                            data = json.loads(line)
                            log_time = datetime.fromisoformat(
                                data.get("timestamp", "").replace("Z", "+00:00")
                            )
                            if log_time >= cutoff:
                                kept_logs.append(line)
                            else:
                                deleted_count += 1
                        except (json.JSONDecodeError, Exception):
                            kept_logs.append(line)

            with open(self.logs_file, "w") as f:
                f.writelines(kept_logs)

        except FileNotFoundError:
            pass

        return deleted_count

```

---

## backend/admin-dashboard/api/services/auth_service.py

```py
"""
Auth Service - /  

Refresh Token :
- Refresh token HTTP-Only, Secure  
- Redis refresh token   
- Token Rotation:     ,   
"""

import asyncio
import hashlib
import os
import random
import secrets
import string
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional
from uuid import uuid4

import yaml
from jose import JWTError, jwt

from ..models.schemas import Token, TokenData, User, UserCreate, UserRole, SetupStatus


class RedisClient:
    """  Redis """

    def __init__(self, host: str = "redis", port: int = 6379):
        self.host = host
        self.port = port
        self._reader: Optional[asyncio.StreamReader] = None
        self._writer: Optional[asyncio.StreamWriter] = None

    async def connect(self) -> bool:
        """Redis """
        try:
            self._reader, self._writer = await asyncio.wait_for(
                asyncio.open_connection(self.host, self.port), timeout=5.0
            )
            return True
        except Exception:
            return False

    async def close(self):
        """ """
        if self._writer:
            self._writer.close()
            await self._writer.wait_closed()
            self._writer = None
            self._reader = None

    async def _send_command(self, *args) -> Optional[str]:
        """Redis  """
        try:
            if not self._writer:
                if not await self.connect():
                    return None

            # RESP   
            cmd = f"*{len(args)}\r\n"
            for arg in args:
                arg_str = str(arg)
                cmd += f"${len(arg_str)}\r\n{arg_str}\r\n"

            self._writer.write(cmd.encode())
            await self._writer.drain()

            #  
            response = await asyncio.wait_for(self._reader.readline(), timeout=5.0)
            response_str = response.decode("utf-8", errors="ignore").strip()

            # Simple string (+OK)
            if response_str.startswith("+"):
                return response_str[1:]
            # Error (-ERR)
            elif response_str.startswith("-"):
                return None
            # Integer (:1)
            elif response_str.startswith(":"):
                return response_str[1:]
            # Bulk string ($5\r\nhello)
            elif response_str.startswith("$"):
                length = int(response_str[1:])
                if length == -1:
                    return None
                data = await self._reader.read(length + 2)  # +2 for \r\n
                return data[:-2].decode("utf-8", errors="ignore")
            # Null
            elif response_str == "$-1":
                return None

            return response_str
        except Exception:
            await self.close()
            return None

    async def set(self, key: str, value: str, ex: Optional[int] = None) -> bool:
        """-  ( TTL)"""
        if ex:
            result = await self._send_command("SET", key, value, "EX", str(ex))
        else:
            result = await self._send_command("SET", key, value)
        return result == "OK"

    async def get(self, key: str) -> Optional[str]:
        """  """
        return await self._send_command("GET", key)

    async def delete(self, key: str) -> bool:
        """ """
        result = await self._send_command("DEL", key)
        return result == "1"

    async def exists(self, key: str) -> bool:
        """  """
        result = await self._send_command("EXISTS", key)
        return result == "1"

    async def keys(self, pattern: str) -> list[str]:
        """  """
        try:
            if not self._writer:
                if not await self.connect():
                    return []

            cmd = f"*2\r\n$4\r\nKEYS\r\n${len(pattern)}\r\n{pattern}\r\n"
            self._writer.write(cmd.encode())
            await self._writer.drain()

            # Array  
            response = await asyncio.wait_for(self._reader.readline(), timeout=5.0)
            response_str = response.decode("utf-8", errors="ignore").strip()

            if not response_str.startswith("*"):
                return []

            count = int(response_str[1:])
            if count <= 0:
                return []

            keys = []
            for _ in range(count):
                # Bulk string length
                length_line = await self._reader.readline()
                length = int(length_line.decode().strip()[1:])
                # Bulk string value
                data = await self._reader.read(length + 2)
                keys.append(data[:-2].decode("utf-8", errors="ignore"))

            return keys
        except Exception:
            return []


class AuthService:
    """/ """

    # Redis  
    REFRESH_TOKEN_PREFIX = "refresh_token:"
    USER_TOKENS_PREFIX = "user_tokens:"

    def __init__(
        self,
        config_dir: str,
        secret_key: Optional[str] = None,
        algorithm: str = "HS256",
        access_token_expire_minutes: int = 60,
        refresh_token_expire_days: int = 7,
    ):
        self.config_dir = Path(config_dir)
        self.secret_key = secret_key or secrets.token_urlsafe(32)
        self.algorithm = algorithm
        self.access_token_expire_minutes = access_token_expire_minutes
        self.refresh_token_expire_days = refresh_token_expire_days
        self.users: dict[str, dict] = {}
        self.email_verifications: dict[str, dict] = {}

        # Redis 
        self.redis_host = os.environ.get("REDIS_HOST", "redis")
        self.redis_port = int(os.environ.get("REDIS_PORT", "6379"))
        self._redis: Optional[RedisClient] = None

        #    (Redis   )
        self._memory_tokens: dict[str, dict] = {}

        self._load_users()

    async def _get_redis(self) -> Optional[RedisClient]:
        """Redis  """
        if self._redis is None:
            self._redis = RedisClient(self.redis_host, self.redis_port)
            if not await self._redis.connect():
                self._redis = None
        return self._redis

    def _hash_token(self, token: str) -> str:
        """  (SHA256)"""
        return hashlib.sha256(token.encode()).hexdigest()

    async def _store_refresh_token(
        self, jti: str, user_id: str, token_hash: str, expires_in_seconds: int
    ) -> bool:
        """Redis refresh token """
        redis = await self._get_redis()

        data = f"{user_id}:{token_hash}:{datetime.utcnow().isoformat()}"

        if redis:
            # Redis 
            success = await redis.set(
                f"{self.REFRESH_TOKEN_PREFIX}{jti}", data, ex=expires_in_seconds
            )
            if success:
                return True

        # Redis    
        self._memory_tokens[jti] = {
            "user_id": user_id,
            "token_hash": token_hash,
            "created_at": datetime.utcnow().isoformat(),
            "expires_at": (
                datetime.utcnow() + timedelta(seconds=expires_in_seconds)
            ).isoformat(),
        }
        return True

    async def _verify_refresh_token(self, jti: str, token_hash: str) -> Optional[str]:
        """Redis refresh token ,  user_id """
        redis = await self._get_redis()

        if redis:
            data = await redis.get(f"{self.REFRESH_TOKEN_PREFIX}{jti}")
            if data:
                parts = data.split(":", 2)
                if len(parts) >= 2:
                    stored_user_id, stored_hash = parts[0], parts[1]
                    #  
                    if secrets.compare_digest(stored_hash, token_hash):
                        return stored_user_id
            return None

        # Redis    
        token_data = self._memory_tokens.get(jti)
        if token_data:
            #  
            expires_at = datetime.fromisoformat(token_data["expires_at"])
            if datetime.utcnow() > expires_at:
                del self._memory_tokens[jti]
                return None

            #  
            if secrets.compare_digest(token_data["token_hash"], token_hash):
                return token_data["user_id"]

        return None

    async def _revoke_refresh_token(self, jti: str) -> bool:
        """refresh token """
        redis = await self._get_redis()

        if redis:
            return await redis.delete(f"{self.REFRESH_TOKEN_PREFIX}{jti}")

        #  
        if jti in self._memory_tokens:
            del self._memory_tokens[jti]
            return True
        return False

    async def _revoke_all_user_tokens_async(self, user_id: str) -> int:
        """  refresh token  ()"""
        revoked = 0
        redis = await self._get_redis()

        if redis:
            keys = await redis.keys(f"{self.REFRESH_TOKEN_PREFIX}*")
            for key in keys:
                data = await redis.get(key)
                if data and data.startswith(f"{user_id}:"):
                    await redis.delete(key)
                    revoked += 1
        else:
            #  
            to_delete = [
                jti
                for jti, data in self._memory_tokens.items()
                if data.get("user_id") == user_id
            ]
            for jti in to_delete:
                del self._memory_tokens[jti]
                revoked += 1

        return revoked

    def _load_users(self) -> None:
        """  """
        users_file = self.config_dir / "users.yaml"
        if users_file.exists():
            with open(users_file) as f:
                data = yaml.safe_load(f) or {}
                self.users = data.get("users", {})
        else:
            self._create_default_admin()

    def _create_default_admin(self) -> None:
        """   """
        admin_id = f"user-{uuid4().hex[:8]}"
        now = datetime.utcnow()

        password_hash = self._hash_password("admin123")

        self.users[admin_id] = {
            "id": admin_id,
            "username": "admin",
            "email": "admin@localhost",
            "password_hash": password_hash,
            "role": UserRole.ADMIN.value,
            "is_active": True,
            "created_at": now.isoformat(),
            "last_login": None,
            "password_change_required": True,
        }

        self._save_users()

    def _save_users(self) -> None:
        """  """
        self.config_dir.mkdir(parents=True, exist_ok=True)
        users_file = self.config_dir / "users.yaml"

        data = {"users": self.users}

        with open(users_file, "w") as f:
            yaml.dump(data, f, default_flow_style=False, allow_unicode=True)

    def _hash_password(self, password: str) -> str:
        """ """
        return hashlib.sha256(password.encode()).hexdigest()

    def _verify_password(self, password: str, password_hash: str) -> bool:
        """ """
        return self._hash_password(password) == password_hash

    def authenticate(self, username: str, password: str) -> Optional[User]:
        """ """
        for user_data in self.users.values():
            if user_data.get("username") == username:
                if not user_data.get("is_active", False):
                    return None

                if self._verify_password(password, user_data.get("password_hash", "")):
                    user_data["last_login"] = datetime.utcnow().isoformat()
                    self._save_users()

                    return User(
                        id=user_data["id"],
                        username=user_data["username"],
                        email=user_data.get("email"),
                        role=UserRole(user_data["role"]),
                        is_active=user_data["is_active"],
                        created_at=datetime.fromisoformat(user_data["created_at"]),
                        last_login=datetime.fromisoformat(user_data["last_login"])
                        if user_data.get("last_login")
                        else None,
                        password_change_required=user_data.get(
                            "password_change_required", False
                        ),
                    )

        return None

    async def create_tokens(self, user: User) -> tuple[str, str, str]:
        """    

        Returns:
            tuple: (access_token, refresh_token, jti)
        """
        access_expire = datetime.utcnow() + timedelta(
            minutes=self.access_token_expire_minutes
        )
        refresh_expire = datetime.utcnow() + timedelta(
            days=self.refresh_token_expire_days
        )

        jti = secrets.token_urlsafe(32)

        # Access token ( ,  )
        access_payload = {
            "sub": user.id,
            "username": user.username,
            "role": user.role.value,
            "exp": access_expire,
            "type": "access",
        }

        # Refresh token ( , HTTP-Only )
        refresh_payload = {
            "sub": user.id,
            "exp": refresh_expire,
            "type": "refresh",
            "jti": jti,
        }

        access_token = jwt.encode(
            access_payload, self.secret_key, algorithm=self.algorithm
        )
        refresh_token = jwt.encode(
            refresh_payload, self.secret_key, algorithm=self.algorithm
        )

        # Refresh token  Redis 
        token_hash = self._hash_token(refresh_token)
        expires_in = self.refresh_token_expire_days * 24 * 60 * 60

        await self._store_refresh_token(jti, user.id, token_hash, expires_in)

        return access_token, refresh_token, jti

    def create_access_token(self, user: User) -> Token:
        """ :    ( )

        Note: refresh token create_tokens_async  
        """
        access_expire = datetime.utcnow() + timedelta(
            minutes=self.access_token_expire_minutes
        )

        access_payload = {
            "sub": user.id,
            "username": user.username,
            "role": user.role.value,
            "exp": access_expire,
            "type": "access",
        }

        access_token = jwt.encode(
            access_payload, self.secret_key, algorithm=self.algorithm
        )

        #   refresh token   
        #    create_tokens_async  
        return Token(
            access_token=access_token,
            refresh_token="",
            token_type="bearer",
            expires_in=self.access_token_expire_minutes * 60,
            refresh_expires_in=0,
        )

    async def refresh_access_token(self, refresh_token: str) -> Optional[Token]:
        """    

        -  refresh token  ( )
        -    (Token Rotation)
        -    
        """
        try:
            payload = jwt.decode(
                refresh_token, self.secret_key, algorithms=[self.algorithm]
            )

            if payload.get("type") != "refresh":
                return None

            user_id = payload.get("sub")
            jti = payload.get("jti")

            if not user_id or not jti:
                return None

            # Redis   
            token_hash = self._hash_token(refresh_token)
            verified_user_id = await self._verify_refresh_token(jti, token_hash)

            if not verified_user_id or verified_user_id != user_id:
                return None

            #  
            user = self.get_user(user_id)
            if not user or not user.is_active:
                return None

            #    (Token Rotation)
            await self._revoke_refresh_token(jti)

            #    
            new_access, new_refresh, _ = await self.create_tokens(user)

            return Token(
                access_token=new_access,
                refresh_token=new_refresh,
                token_type="bearer",
                expires_in=self.access_token_expire_minutes * 60,
                refresh_expires_in=self.refresh_token_expire_days * 24 * 60 * 60,
            )

        except JWTError:
            return None

    async def revoke_refresh_token(self, refresh_token: str) -> bool:
        """  """
        try:
            payload = jwt.decode(
                refresh_token, self.secret_key, algorithms=[self.algorithm]
            )
            jti = payload.get("jti")

            if jti:
                return await self._revoke_refresh_token(jti)
            return False
        except JWTError:
            return False

    def revoke_all_user_tokens(self, user_id: str) -> int:
        """     ( )"""
        try:
            loop = asyncio.get_event_loop()
            return loop.run_until_complete(self._revoke_all_user_tokens_async(user_id))
        except RuntimeError:
            #      
            #   
            to_delete = [
                jti
                for jti, data in self._memory_tokens.items()
                if data.get("user_id") == user_id
            ]
            for jti in to_delete:
                del self._memory_tokens[jti]
            return len(to_delete)

    async def revoke_all_user_tokens_async(self, user_id: str) -> int:
        """     ()"""
        return await self._revoke_all_user_tokens_async(user_id)

    def verify_token(self, token: str) -> Optional[TokenData]:
        """ """
        try:
            payload = jwt.decode(token, self.secret_key, algorithms=[self.algorithm])

            user_id = payload.get("sub")
            username = payload.get("username")
            role = payload.get("role")
            exp = payload.get("exp")

            if not user_id or not username or not role or not exp:
                return None

            return TokenData(
                user_id=str(user_id),
                username=str(username),
                role=UserRole(role),
                exp=datetime.fromtimestamp(float(exp)),
            )

        except JWTError:
            return None

    def get_user(self, user_id: str) -> Optional[User]:
        """ """
        user_data = self.users.get(user_id)
        if not user_data:
            return None

        return User(
            id=user_data["id"],
            username=user_data["username"],
            email=user_data.get("email"),
            role=UserRole(user_data["role"]),
            is_active=user_data["is_active"],
            created_at=datetime.fromisoformat(user_data["created_at"]),
            last_login=datetime.fromisoformat(user_data["last_login"])
            if user_data.get("last_login")
            else None,
            password_change_required=user_data.get("password_change_required", False),
        )

    def get_user_by_username(self, username: str) -> Optional[User]:
        """ """
        for user_data in self.users.values():
            if user_data.get("username") == username:
                return self.get_user(user_data["id"])
        return None

    def get_user_by_email(self, email: str) -> Optional[User]:
        """ """
        for user_data in self.users.values():
            if user_data.get("email") == email:
                return self.get_user(user_data["id"])
        return None

    def list_users(self, active_only: bool = False) -> list[User]:
        """  """
        users = []
        for user_data in self.users.values():
            if active_only and not user_data.get("is_active", False):
                continue

            users.append(
                User(
                    id=user_data["id"],
                    username=user_data["username"],
                    email=user_data.get("email"),
                    role=UserRole(user_data["role"]),
                    is_active=user_data["is_active"],
                    created_at=datetime.fromisoformat(user_data["created_at"]),
                    last_login=datetime.fromisoformat(user_data["last_login"])
                    if user_data.get("last_login")
                    else None,
                    password_change_required=user_data.get(
                        "password_change_required", False
                    ),
                )
            )

        return users

    def create_user(self, data: UserCreate) -> User:
        """ """
        if self.get_user_by_username(data.username):
            raise ValueError(f"Username already exists: {data.username}")

        user_id = f"user-{uuid4().hex[:8]}"
        now = datetime.utcnow()

        self.users[user_id] = {
            "id": user_id,
            "username": data.username,
            "email": data.email,
            "password_hash": self._hash_password(data.password),
            "role": data.role.value,
            "is_active": data.is_active,
            "created_at": now.isoformat(),
            "last_login": None,
            "password_change_required": False,
        }

        self._save_users()

        return User(
            id=user_id,
            username=data.username,
            email=data.email,
            role=data.role,
            is_active=data.is_active,
            created_at=now,
            last_login=None,
            password_change_required=False,
        )

    def register_user(self, username: str, email: str, password: str) -> User:
        """  """
        if self.get_user_by_username(username):
            raise ValueError(f"   : {username}")

        if self.get_user_by_email(email):
            raise ValueError(f"   : {email}")

        user_id = f"user-{uuid4().hex[:8]}"
        now = datetime.utcnow()

        self.users[user_id] = {
            "id": user_id,
            "username": username,
            "email": email,
            "password_hash": self._hash_password(password),
            "role": UserRole.USER.value,
            "is_active": True,
            "created_at": now.isoformat(),
            "last_login": None,
            "password_change_required": False,
        }

        self._save_users()

        return User(
            id=user_id,
            username=username,
            email=email,
            role=UserRole.USER,
            is_active=True,
            created_at=now,
            last_login=None,
            password_change_required=False,
        )

    def update_user(
        self,
        user_id: str,
        email: Optional[str] = None,
        role: Optional[UserRole] = None,
        is_active: Optional[bool] = None,
    ) -> Optional[User]:
        """  """
        user_data = self.users.get(user_id)
        if not user_data:
            return None

        if email is not None:
            user_data["email"] = email
        if role is not None:
            user_data["role"] = role.value
        if is_active is not None:
            user_data["is_active"] = is_active

        self._save_users()
        return self.get_user(user_id)

    def change_password(
        self, user_id: str, old_password: str, new_password: str
    ) -> bool:
        """ """
        user_data = self.users.get(user_id)
        if not user_data:
            return False

        if not self._verify_password(old_password, user_data.get("password_hash", "")):
            return False

        user_data["password_hash"] = self._hash_password(new_password)
        user_data["password_change_required"] = False
        self._save_users()
        return True

    def reset_password(self, user_id: str, new_password: str) -> bool:
        """  ()"""
        user_data = self.users.get(user_id)
        if not user_data:
            return False

        user_data["password_hash"] = self._hash_password(new_password)
        user_data["password_change_required"] = True
        self._save_users()
        return True

    def delete_user(self, user_id: str) -> bool:
        """ """
        if user_id in self.users:
            del self.users[user_id]
            self._save_users()
            return True
        return False

    def check_permission(self, user_role: UserRole, required_role: UserRole) -> bool:
        """ """
        role_priority = {
            UserRole.VIEWER: 0,
            UserRole.OPERATOR: 1,
            UserRole.ADMIN: 2,
        }

        user_level = role_priority.get(user_role, 0)
        required_level = role_priority.get(required_role, 0)

        return user_level >= required_level

    def get_setup_status(self) -> SetupStatus:
        """   """
        has_users = len(self.users) > 0

        is_default_admin = False
        setup_required = False

        if has_users:
            for user_data in self.users.values():
                if user_data.get("username") == "admin" and user_data.get(
                    "password_change_required", False
                ):
                    is_default_admin = True
                    setup_required = True
                    break
        else:
            setup_required = True

        return SetupStatus(
            setup_required=setup_required,
            has_users=has_users,
            is_default_admin=is_default_admin,
        )

    # ============================================================================
    # Email Verification Methods
    # ============================================================================

    def generate_verification_code(self) -> str:
        """6   """
        return "".join(random.choices(string.digits, k=6))

    def create_email_verification(
        self, email: str, username: str, password: str
    ) -> str:
        """   """
        if self.get_user_by_email(email):
            raise ValueError(f"   : {email}")

        if self.get_user_by_username(username):
            raise ValueError(f"   : {username}")

        code = self.generate_verification_code()
        expires_at = datetime.utcnow() + timedelta(minutes=10)

        self.email_verifications[email] = {
            "code": code,
            "username": username,
            "password_hash": self._hash_password(password),
            "expires_at": expires_at.isoformat(),
            "created_at": datetime.utcnow().isoformat(),
            "attempts": 0,
        }

        return code

    def verify_email_code(self, email: str, code: str) -> User:
        """      """
        verification = self.email_verifications.get(email)

        if not verification:
            raise ValueError("    .  .")

        verification["attempts"] += 1

        if verification["attempts"] > 5:
            del self.email_verifications[email]
            raise ValueError(
                "   .   ."
            )

        expires_at = datetime.fromisoformat(verification["expires_at"])
        if datetime.utcnow() > expires_at:
            del self.email_verifications[email]
            raise ValueError("  .  .")

        if verification["code"] != code:
            raise ValueError(
                f"  . ( : {5 - verification['attempts']})"
            )

        username = verification["username"]
        password_hash = verification["password_hash"]

        if self.get_user_by_email(email):
            del self.email_verifications[email]
            raise ValueError(f"   : {email}")

        if self.get_user_by_username(username):
            del self.email_verifications[email]
            raise ValueError(f"   : {username}")

        user_id = f"user-{uuid4().hex[:8]}"
        now = datetime.utcnow()

        self.users[user_id] = {
            "id": user_id,
            "username": username,
            "email": email,
            "password_hash": password_hash,
            "role": UserRole.USER.value,
            "is_active": True,
            "created_at": now.isoformat(),
            "last_login": None,
            "password_change_required": False,
            "email_verified": True,
        }

        self._save_users()
        del self.email_verifications[email]

        return User(
            id=user_id,
            username=username,
            email=email,
            role=UserRole.USER,
            is_active=True,
            created_at=now,
            last_login=None,
            password_change_required=False,
        )

    def resend_verification_code(self, email: str) -> str:
        """  """
        verification = self.email_verifications.get(email)

        if not verification:
            raise ValueError(
                "    .   ."
            )

        code = self.generate_verification_code()
        verification["code"] = code
        verification["expires_at"] = (
            datetime.utcnow() + timedelta(minutes=10)
        ).isoformat()
        verification["attempts"] = 0

        return code

```

---

## backend/admin-dashboard/api/services/data_source_service.py

```py
"""
Data Source Management Service
  CRUD   
"""

import os
import uuid
from datetime import datetime
from pathlib import Path
from typing import Optional
import json
import yaml

try:
    import httpx
except ImportError:
    httpx = None  # type: ignore

from ..models.schemas import (
    DataSource,
    DataSourceCreate,
    DataSourceUpdate,
    DataSourceType,
    DataSourceStatus,
    DataSourceStats,
    DataSourceTestResult,
)


class DataSourceService:
    """   """

    def __init__(
        self,
        project_root: str,
        config_dir: str,
        collector_service_url: Optional[str] = None,
    ):
        self.project_root = Path(project_root)
        self.config_dir = Path(config_dir)
        self.config_file = self.config_dir / "data_sources.yaml"
        self._sources: dict[str, DataSource] = {}
        self.collector_service_url = collector_service_url or os.environ.get(
            "COLLECTOR_SERVICE_URL", "http://collector-service:8081"
        )
        self._load_sources()

    def _load_sources(self) -> None:
        """    """
        if self.config_file.exists():
            with open(self.config_file, "r", encoding="utf-8") as f:
                data = yaml.safe_load(f) or {}
                for source_id, source_data in data.get("sources", {}).items():
                    try:
                        # Enum 
                        source_data["source_type"] = DataSourceType(
                            source_data.get("source_type", "rss")
                        )
                        source_data["status"] = DataSourceStatus(
                            source_data.get("status", "active")
                        )
                        # datetime 
                        for dt_field in ["created_at", "updated_at", "last_crawled_at"]:
                            if source_data.get(dt_field) and isinstance(
                                source_data[dt_field], str
                            ):
                                source_data[dt_field] = datetime.fromisoformat(
                                    source_data[dt_field]
                                )

                        self._sources[source_id] = DataSource(
                            id=source_id, **source_data
                        )
                    except Exception as e:
                        print(f"Error loading source {source_id}: {e}")

    def _save_sources(self) -> None:
        """    """
        self.config_dir.mkdir(parents=True, exist_ok=True)

        data = {"sources": {}}
        for source_id, source in self._sources.items():
            source_dict = source.model_dump()
            # Enum  
            source_dict["source_type"] = (
                source_dict["source_type"].value
                if hasattr(source_dict["source_type"], "value")
                else source_dict["source_type"]
            )
            source_dict["status"] = (
                source_dict["status"].value
                if hasattr(source_dict["status"], "value")
                else source_dict["status"]
            )
            # datetime ISO  
            for dt_field in ["created_at", "updated_at", "last_crawled_at"]:
                if source_dict.get(dt_field):
                    source_dict[dt_field] = (
                        source_dict[dt_field].isoformat()
                        if hasattr(source_dict[dt_field], "isoformat")
                        else source_dict[dt_field]
                    )
            # ID   
            del source_dict["id"]
            data["sources"][source_id] = source_dict

        with open(self.config_file, "w", encoding="utf-8") as f:
            yaml.dump(data, f, allow_unicode=True, default_flow_style=False)

    def list_sources(
        self,
        source_type: Optional[DataSourceType] = None,
        status: Optional[DataSourceStatus] = None,
        category: Optional[str] = None,
        is_active: Optional[bool] = None,
    ) -> list[DataSource]:
        """   """
        sources = list(self._sources.values())

        if source_type:
            sources = [s for s in sources if s.source_type == source_type]
        if status:
            sources = [s for s in sources if s.status == status]
        if category:
            sources = [s for s in sources if s.category == category]
        if is_active is not None:
            sources = [s for s in sources if s.is_active == is_active]

        return sorted(sources, key=lambda x: (-x.priority, x.name))

    def get_source(self, source_id: str) -> Optional[DataSource]:
        """   """
        return self._sources.get(source_id)

    def create_source(self, data: DataSourceCreate) -> DataSource:
        """   """
        source_id = str(uuid.uuid4())[:8]
        now = datetime.utcnow()

        source = DataSource(
            id=source_id,
            name=data.name,
            source_type=data.source_type,
            url=data.url,
            description=data.description,
            category=data.category,
            language=data.language,
            is_active=data.is_active,
            crawl_interval_minutes=data.crawl_interval_minutes,
            priority=data.priority,
            config=data.config,
            status=DataSourceStatus.ACTIVE
            if data.is_active
            else DataSourceStatus.INACTIVE,
            created_at=now,
            updated_at=now,
        )

        self._sources[source_id] = source
        self._save_sources()
        return source

    def update_source(
        self, source_id: str, data: DataSourceUpdate
    ) -> Optional[DataSource]:
        """  """
        source = self._sources.get(source_id)
        if not source:
            return None

        update_data = data.model_dump(exclude_unset=True)
        update_data["updated_at"] = datetime.utcnow()

        # is_active   status 
        if "is_active" in update_data:
            if update_data["is_active"]:
                update_data["status"] = DataSourceStatus.ACTIVE
            else:
                update_data["status"] = DataSourceStatus.INACTIVE

        for key, value in update_data.items():
            if hasattr(source, key):
                setattr(source, key, value)

        self._save_sources()
        return source

    def delete_source(self, source_id: str) -> bool:
        """  """
        if source_id in self._sources:
            del self._sources[source_id]
            self._save_sources()
            return True
        return False

    async def test_source(self, source_id: str) -> DataSourceTestResult:
        """   """
        source = self._sources.get(source_id)
        if not source:
            return DataSourceTestResult(
                source_id=source_id,
                success=False,
                message="Source not found",
                tested_at=datetime.utcnow(),
            )

        if httpx is None:
            return DataSourceTestResult(
                source_id=source_id,
                success=False,
                message="httpx not installed",
                tested_at=datetime.utcnow(),
            )

        #  URL  
        try:
            start_time = datetime.utcnow()
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(source.url)
                response_time = (datetime.utcnow() - start_time).total_seconds() * 1000

                if response.status_code == 200:
                    #      
                    sample_data = None
                    if source.source_type == DataSourceType.RSS:
                        sample_data = {
                            "content_type": response.headers.get(
                                "content-type", "unknown"
                            )
                        }
                    elif source.source_type == DataSourceType.API:
                        try:
                            sample_data = response.json()
                        except Exception:
                            sample_data = {"raw_length": len(response.text)}

                    #     
                    source.status = DataSourceStatus.ACTIVE
                    self._save_sources()

                    return DataSourceTestResult(
                        source_id=source_id,
                        success=True,
                        message="Connection successful",
                        response_time_ms=response_time,
                        sample_data=sample_data,
                        tested_at=datetime.utcnow(),
                    )
                else:
                    return DataSourceTestResult(
                        source_id=source_id,
                        success=False,
                        message=f"HTTP {response.status_code}",
                        response_time_ms=response_time,
                        tested_at=datetime.utcnow(),
                    )
        except Exception as e:
            #     
            source.status = DataSourceStatus.ERROR
            self._save_sources()

            return DataSourceTestResult(
                source_id=source_id,
                success=False,
                message=f"Error: {str(e)}",
                tested_at=datetime.utcnow(),
            )

    async def trigger_crawl(self, source_id: str) -> dict:
        """  """
        source = self._sources.get(source_id)
        if not source:
            return {"success": False, "message": "Source not found"}

        if not source.is_active:
            return {"success": False, "message": "Source is not active"}

        if httpx is None:
            return {"success": False, "message": "httpx not installed"}

        # Collector Service  
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    f"{self.collector_service_url}/api/v1/crawl/trigger",
                    json={
                        "source_id": source_id,
                        "source_url": source.url,
                        "source_type": source.source_type.value,
                    },
                )

                if response.status_code in [200, 202]:
                    source.last_crawled_at = datetime.utcnow()
                    self._save_sources()
                    return {"success": True, "message": "Crawl triggered successfully"}
                else:
                    return {
                        "success": False,
                        "message": f"Failed: HTTP {response.status_code}",
                    }
        except Exception as e:
            return {"success": False, "message": f"Error: {str(e)}"}

    def get_categories(self) -> list[str]:
        """   """
        categories = set()
        for source in self._sources.values():
            if source.category:
                categories.add(source.category)
        return sorted(categories)

    def get_stats(self) -> dict:
        """  """
        total = len(self._sources)
        active = sum(1 for s in self._sources.values() if s.is_active)
        by_type = {}
        by_status = {}

        for source in self._sources.values():
            type_key = source.source_type.value
            by_type[type_key] = by_type.get(type_key, 0) + 1

            status_key = source.status.value
            by_status[status_key] = by_status.get(status_key, 0) + 1

        total_articles = sum(s.total_articles for s in self._sources.values())

        return {
            "total_sources": total,
            "active_sources": active,
            "inactive_sources": total - active,
            "by_type": by_type,
            "by_status": by_status,
            "total_articles": total_articles,
        }

    def bulk_toggle_active(self, source_ids: list[str], is_active: bool) -> int:
        """  /"""
        updated = 0
        for source_id in source_ids:
            source = self._sources.get(source_id)
            if source:
                source.is_active = is_active
                source.status = (
                    DataSourceStatus.ACTIVE if is_active else DataSourceStatus.INACTIVE
                )
                source.updated_at = datetime.utcnow()
                updated += 1

        if updated > 0:
            self._save_sources()

        return updated

```

---

## backend/admin-dashboard/api/services/database_service.py

```py
"""
Database Management Service
PostgreSQL, MongoDB, Redis   
"""

import os
from datetime import datetime
from pathlib import Path
from typing import Optional
import json

try:
    import httpx
except ImportError:
    httpx = None  # type: ignore

from ..models.schemas import (
    DatabaseType,
    DatabaseInfo,
    PostgresDatabaseStats,
    PostgresTableInfo,
    MongoDatabaseStats,
    MongoCollectionInfo,
    RedisStats,
    ServiceHealthStatus,
)


def format_bytes(size_bytes: int) -> str:
    """     """
    for unit in ["B", "KB", "MB", "GB", "TB"]:
        if size_bytes < 1024.0:
            return f"{size_bytes:.2f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.2f} PB"


class DatabaseService:
    """  """

    def __init__(self, project_root: str, config_dir: str):
        self.project_root = Path(project_root)
        self.config_dir = Path(config_dir)

        #   
        self.postgres_host = os.environ.get("POSTGRES_HOST", "postgres")
        self.postgres_port = int(os.environ.get("POSTGRES_PORT", "5432"))
        self.postgres_db = os.environ.get("POSTGRES_DB", "newsinsight")
        self.postgres_user = os.environ.get("POSTGRES_USER", "postgres")
        self.postgres_password = os.environ.get("POSTGRES_PASSWORD", "postgres")

        self.mongo_host = os.environ.get("MONGO_HOST", "mongo")
        self.mongo_port = int(os.environ.get("MONGO_PORT", "27017"))
        self.mongo_db = os.environ.get("MONGO_DB", "newsinsight")

        self.redis_host = os.environ.get("REDIS_HOST", "redis")
        self.redis_port = int(os.environ.get("REDIS_PORT", "6379"))

        self.timeout = 5.0

    async def get_all_databases(self) -> list[DatabaseInfo]:
        """   """
        databases = []

        # PostgreSQL
        postgres_info = await self.get_postgres_health()
        databases.append(postgres_info)

        # MongoDB
        mongo_info = await self.get_mongo_health()
        databases.append(mongo_info)

        # Redis
        redis_info = await self.get_redis_health()
        databases.append(redis_info)

        return databases

    async def get_postgres_health(self) -> DatabaseInfo:
        """PostgreSQL  """
        try:
            # psycopg2  TCP  
            import asyncio

            reader, writer = await asyncio.wait_for(
                asyncio.open_connection(self.postgres_host, self.postgres_port),
                timeout=self.timeout,
            )
            writer.close()
            await writer.wait_closed()

            return DatabaseInfo(
                db_type=DatabaseType.POSTGRESQL,
                name=self.postgres_db,
                host=self.postgres_host,
                port=self.postgres_port,
                status=ServiceHealthStatus.HEALTHY,
                version="15.x",  #    
                checked_at=datetime.utcnow(),
            )
        except Exception as e:
            return DatabaseInfo(
                db_type=DatabaseType.POSTGRESQL,
                name=self.postgres_db,
                host=self.postgres_host,
                port=self.postgres_port,
                status=ServiceHealthStatus.UNREACHABLE,
                checked_at=datetime.utcnow(),
            )

    async def get_mongo_health(self) -> DatabaseInfo:
        """MongoDB  """
        try:
            import asyncio

            reader, writer = await asyncio.wait_for(
                asyncio.open_connection(self.mongo_host, self.mongo_port),
                timeout=self.timeout,
            )
            writer.close()
            await writer.wait_closed()

            return DatabaseInfo(
                db_type=DatabaseType.MONGODB,
                name=self.mongo_db,
                host=self.mongo_host,
                port=self.mongo_port,
                status=ServiceHealthStatus.HEALTHY,
                version="7.x",
                checked_at=datetime.utcnow(),
            )
        except Exception as e:
            return DatabaseInfo(
                db_type=DatabaseType.MONGODB,
                name=self.mongo_db,
                host=self.mongo_host,
                port=self.mongo_port,
                status=ServiceHealthStatus.UNREACHABLE,
                checked_at=datetime.utcnow(),
            )

    async def get_redis_health(self) -> DatabaseInfo:
        """Redis  """
        try:
            import asyncio

            reader, writer = await asyncio.wait_for(
                asyncio.open_connection(self.redis_host, self.redis_port),
                timeout=self.timeout,
            )

            # PING 
            writer.write(b"PING\r\n")
            await writer.drain()
            response = await asyncio.wait_for(reader.readline(), timeout=2.0)
            writer.close()
            await writer.wait_closed()

            is_healthy = b"+PONG" in response

            return DatabaseInfo(
                db_type=DatabaseType.REDIS,
                name="redis",
                host=self.redis_host,
                port=self.redis_port,
                status=ServiceHealthStatus.HEALTHY
                if is_healthy
                else ServiceHealthStatus.DEGRADED,
                version="7.x",
                checked_at=datetime.utcnow(),
            )
        except Exception as e:
            return DatabaseInfo(
                db_type=DatabaseType.REDIS,
                name="redis",
                host=self.redis_host,
                port=self.redis_port,
                status=ServiceHealthStatus.UNREACHABLE,
                checked_at=datetime.utcnow(),
            )

    async def get_postgres_stats(self) -> PostgresDatabaseStats:
        """PostgreSQL   (  )"""
        #   psycopg2  asyncpg 
        #    
        return PostgresDatabaseStats(
            database_name=self.postgres_db,
            size_bytes=0,
            size_human="N/A",
            tables=[
                PostgresTableInfo(
                    schema_name="public",
                    table_name="news_articles",
                    row_count=0,
                    size_bytes=0,
                    size_human="N/A",
                ),
                PostgresTableInfo(
                    schema_name="public",
                    table_name="news_sources",
                    row_count=0,
                    size_bytes=0,
                    size_human="N/A",
                ),
            ],
            total_tables=0,
            total_rows=0,
            connection_count=0,
            max_connections=100,
            checked_at=datetime.utcnow(),
        )

    async def get_mongo_stats(self) -> MongoDatabaseStats:
        """MongoDB  """
        #   pymongo 
        return MongoDatabaseStats(
            database_name=self.mongo_db,
            size_bytes=0,
            size_human="N/A",
            collections=[
                MongoCollectionInfo(
                    collection_name="ai_responses",
                    document_count=0,
                    size_bytes=0,
                    size_human="N/A",
                    index_count=1,
                ),
            ],
            total_collections=0,
            total_documents=0,
            checked_at=datetime.utcnow(),
        )

    async def get_redis_stats(self) -> RedisStats:
        """Redis  """
        try:
            import asyncio

            reader, writer = await asyncio.wait_for(
                asyncio.open_connection(self.redis_host, self.redis_port),
                timeout=self.timeout,
            )

            # INFO 
            writer.write(b"INFO\r\n")
            await writer.drain()

            #   (bulk string)
            response_lines = []
            while True:
                line = await asyncio.wait_for(reader.readline(), timeout=2.0)
                if not line or line == b"\r\n":
                    break
                response_lines.append(line.decode("utf-8", errors="ignore").strip())

            writer.close()
            await writer.wait_closed()

            # 
            info = {}
            for line in response_lines:
                if ":" in line and not line.startswith("#"):
                    key, value = line.split(":", 1)
                    info[key] = value

            used_memory = int(info.get("used_memory", 0))
            keyspace_hits = int(info.get("keyspace_hits", 0))
            keyspace_misses = int(info.get("keyspace_misses", 0))
            total_requests = keyspace_hits + keyspace_misses
            hit_rate = (
                (keyspace_hits / total_requests * 100) if total_requests > 0 else 0.0
            )

            # DB0   
            db0_info = info.get("db0", "")
            total_keys = 0
            if db0_info:
                for part in db0_info.split(","):
                    if part.startswith("keys="):
                        total_keys = int(part.split("=")[1])
                        break

            return RedisStats(
                used_memory_bytes=used_memory,
                used_memory_human=format_bytes(used_memory),
                max_memory_bytes=int(info.get("maxmemory", 0)) or None,
                connected_clients=int(info.get("connected_clients", 0)),
                total_keys=total_keys,
                expired_keys=int(info.get("expired_keys", 0)),
                keyspace_hits=keyspace_hits,
                keyspace_misses=keyspace_misses,
                hit_rate=hit_rate,
                uptime_seconds=int(info.get("uptime_in_seconds", 0)),
                checked_at=datetime.utcnow(),
            )
        except Exception as e:
            return RedisStats(
                used_memory_bytes=0,
                used_memory_human="N/A",
                connected_clients=0,
                total_keys=0,
                expired_keys=0,
                keyspace_hits=0,
                keyspace_misses=0,
                hit_rate=0.0,
                uptime_seconds=0,
                checked_at=datetime.utcnow(),
            )

```

---

## backend/admin-dashboard/api/services/document_service.py

```py
"""
Document Service - Markdown   
"""
import os
from datetime import datetime
from pathlib import Path
from typing import Optional
from uuid import uuid4

import yaml

from ..models.schemas import Document, DocumentBase, DocumentCategory


class DocumentService:
    """  """

    def __init__(self, project_root: str, config_dir: str):
        self.project_root = Path(project_root)
        self.config_dir = Path(config_dir)
        self.docs_dirs = [
            self.project_root / "docs",
            self.project_root / "etc" / "infra-guides",
        ]
        self.documents: dict[str, Document] = {}
        self._scan_documents()

    def _scan_documents(self) -> None:
        """  """
        config_file = self.config_dir / "documents.yaml"

        #   
        existing_config = {}
        if config_file.exists():
            with open(config_file) as f:
                data = yaml.safe_load(f) or {}
                for doc_data in data.get("documents", []):
                    existing_config[doc_data.get("file_path")] = doc_data

        #  
        for docs_dir in self.docs_dirs:
            if not docs_dir.exists():
                continue

            for md_file in docs_dir.rglob("*.md"):
                file_path = str(md_file.relative_to(self.project_root))
                abs_path = str(md_file)

                #    
                if abs_path in existing_config:
                    doc_data = existing_config[abs_path]
                    doc = Document(**doc_data)
                else:
                    #   
                    doc = self._create_document_from_file(md_file)

                self.documents[doc.id] = doc

        self._save_documents()

    def _create_document_from_file(self, file_path: Path) -> Document:
        """   """
        doc_id = f"doc-{uuid4().hex[:8]}"
        rel_path = str(file_path.relative_to(self.project_root))

        #   
        title = file_path.stem.replace("_", " ").replace("-", " ").title()

        #  
        category = self._infer_category(file_path)

        #  
        tags = self._infer_tags(file_path)

        #   
        related_envs = self._infer_environments(file_path)

        #  
        stat = file_path.stat()
        last_modified = datetime.fromtimestamp(stat.st_mtime)

        return Document(
            id=doc_id,
            title=title,
            file_path=str(file_path),
            category=category,
            tags=tags,
            related_environments=related_envs,
            related_scripts=[],
            last_modified=last_modified,
        )

    def _infer_category(self, file_path: Path) -> DocumentCategory:
        """   """
        path_str = str(file_path).lower()

        if "deploy" in path_str or "deployment" in path_str:
            return DocumentCategory.DEPLOYMENT
        elif "troubleshoot" in path_str or "debug" in path_str:
            return DocumentCategory.TROUBLESHOOTING
        elif "architecture" in path_str or "overview" in path_str:
            return DocumentCategory.ARCHITECTURE
        elif "runbook" in path_str or "guide" in path_str:
            return DocumentCategory.RUNBOOK
        else:
            return DocumentCategory.GENERAL

    def _infer_tags(self, file_path: Path) -> list[str]:
        """   """
        tags = []
        path_str = str(file_path).lower()

        tag_keywords = [
            "docker",
            "kubernetes",
            "k8s",
            "consul",
            "cloudflare",
            "gcp",
            "aws",
            "api",
            "frontend",
            "backend",
            "database",
            "redis",
            "postgres",
            "mongo",
            "kafka",
        ]

        for keyword in tag_keywords:
            if keyword in path_str:
                tags.append(keyword)

        return tags

    def _infer_environments(self, file_path: Path) -> list[str]:
        """    """
        envs = []
        path_str = str(file_path).lower()

        env_keywords = {
            "zerotrust": "zerotrust",
            "local": "local",
            "gcp": "gcp",
            "aws": "aws",
            "production": "production",
            "staging": "staging",
            "pmx": "production",
        }

        for keyword, env in env_keywords.items():
            if keyword in path_str and env not in envs:
                envs.append(env)

        return envs

    def _save_documents(self) -> None:
        """   """
        self.config_dir.mkdir(parents=True, exist_ok=True)
        config_file = self.config_dir / "documents.yaml"

        data = {
            "documents": [doc.model_dump(mode="json") for doc in self.documents.values()]
        }

        # content   
        for doc_data in data["documents"]:
            doc_data.pop("content", None)

        with open(config_file, "w") as f:
            yaml.dump(data, f, default_flow_style=False, allow_unicode=True)

    def list_documents(
        self,
        category: Optional[DocumentCategory] = None,
        tag: Optional[str] = None,
        environment: Optional[str] = None,
        search: Optional[str] = None,
    ) -> list[Document]:
        """  """
        docs = list(self.documents.values())

        if category:
            docs = [d for d in docs if d.category == category]

        if tag:
            docs = [d for d in docs if tag in d.tags]

        if environment:
            docs = [d for d in docs if environment in d.related_environments]

        if search:
            search_lower = search.lower()
            docs = [
                d
                for d in docs
                if search_lower in d.title.lower()
                or any(search_lower in t.lower() for t in d.tags)
            ]

        return sorted(docs, key=lambda x: x.title)

    def get_document(self, doc_id: str) -> Optional[Document]:
        """   ( )"""
        doc = self.documents.get(doc_id)
        if not doc:
            return None

        #   
        file_path = Path(doc.file_path)
        if file_path.exists():
            try:
                with open(file_path, encoding="utf-8") as f:
                    doc.content = f.read()
            except Exception:
                doc.content = "Error reading file content"

        return doc

    def get_document_by_path(self, file_path: str) -> Optional[Document]:
        """   """
        for doc in self.documents.values():
            if doc.file_path == file_path:
                return self.get_document(doc.id)
        return None

    def update_document_metadata(
        self,
        doc_id: str,
        title: Optional[str] = None,
        category: Optional[DocumentCategory] = None,
        tags: Optional[list[str]] = None,
        related_environments: Optional[list[str]] = None,
        related_scripts: Optional[list[str]] = None,
    ) -> Optional[Document]:
        """  """
        doc = self.documents.get(doc_id)
        if not doc:
            return None

        if title is not None:
            doc.title = title
        if category is not None:
            doc.category = category
        if tags is not None:
            doc.tags = tags
        if related_environments is not None:
            doc.related_environments = related_environments
        if related_scripts is not None:
            doc.related_scripts = related_scripts

        self._save_documents()
        return doc

    def refresh_documents(self) -> int:
        """  """
        old_count = len(self.documents)
        self.documents.clear()
        self._scan_documents()
        return len(self.documents) - old_count

    def get_related_documents(
        self, environment: Optional[str] = None, script_id: Optional[str] = None
    ) -> list[Document]:
        """  """
        docs = []

        if environment:
            docs.extend(
                [d for d in self.documents.values() if environment in d.related_environments]
            )

        if script_id:
            docs.extend(
                [d for d in self.documents.values() if script_id in d.related_scripts]
            )

        #  
        seen = set()
        unique_docs = []
        for doc in docs:
            if doc.id not in seen:
                seen.add(doc.id)
                unique_docs.append(doc)

        return unique_docs

    def get_categories_summary(self) -> dict[str, int]:
        """   """
        summary = {}
        for doc in self.documents.values():
            cat = doc.category.value
            summary[cat] = summary.get(cat, 0) + 1
        return summary

    def get_tags_summary(self) -> dict[str, int]:
        """   """
        summary = {}
        for doc in self.documents.values():
            for tag in doc.tags:
                summary[tag] = summary.get(tag, 0) + 1
        return summary

```

---

## backend/admin-dashboard/api/services/environment_service.py

```py
"""
Environment Service - /  
"""
import os
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Optional
from uuid import uuid4

import yaml

from ..models.schemas import (
    ContainerInfo,
    Environment,
    EnvironmentCreate,
    EnvironmentStatus,
    EnvironmentType,
    EnvironmentUpdate,
    ServiceStatus,
)


class EnvironmentService:
    """  """

    def __init__(self, project_root: str, config_dir: str):
        self.project_root = Path(project_root)
        self.config_dir = Path(config_dir)
        self.environments: dict[str, Environment] = {}
        self._load_environments()

    def _load_environments(self) -> None:
        """    """
        config_file = self.config_dir / "environments.yaml"
        if config_file.exists():
            with open(config_file) as f:
                data = yaml.safe_load(f) or {}
                for env_data in data.get("environments", []):
                    env = Environment(**env_data)
                    self.environments[env.id] = env
        else:
            #    
            self._create_default_environments()

    def _create_default_environments(self) -> None:
        """   """
        docker_dir = self.project_root / "etc" / "docker"
        configs_dir = self.project_root / "etc" / "configs"

        default_envs = [
            {
                "id": "env-zerotrust",
                "name": "zerotrust",
                "env_type": EnvironmentType.ZEROTRUST,
                "description": "Cloudflare Zero Trust   ",
                "compose_file": str(docker_dir / "docker-compose.zerotrust.yml"),
                "env_file": str(docker_dir / ".env"),
                "is_active": True,
                "priority": 100,
                "created_at": datetime.utcnow(),
                "updated_at": datetime.utcnow(),
            },
            {
                "id": "env-production",
                "name": "production",
                "env_type": EnvironmentType.PRODUCTION,
                "description": " ",
                "compose_file": str(docker_dir / "docker-compose.production.yml"),
                "env_file": str(configs_dir / "production.env"),
                "is_active": True,
                "priority": 90,
                "created_at": datetime.utcnow(),
                "updated_at": datetime.utcnow(),
            },
            {
                "id": "env-staging",
                "name": "staging",
                "env_type": EnvironmentType.STAGING,
                "description": " ",
                "compose_file": str(docker_dir / "docker-compose.consul.yml"),
                "env_file": str(configs_dir / "staging.env"),
                "is_active": True,
                "priority": 80,
                "created_at": datetime.utcnow(),
                "updated_at": datetime.utcnow(),
            },
            {
                "id": "env-local",
                "name": "local",
                "env_type": EnvironmentType.LOCAL,
                "description": "  ",
                "compose_file": str(docker_dir / "docker-compose.consul.yml"),
                "env_file": str(configs_dir / "development.env"),
                "is_active": True,
                "priority": 70,
                "created_at": datetime.utcnow(),
                "updated_at": datetime.utcnow(),
            },
        ]

        for env_data in default_envs:
            env = Environment(**env_data)
            self.environments[env.id] = env

        self._save_environments()

    def _save_environments(self) -> None:
        """   """
        self.config_dir.mkdir(parents=True, exist_ok=True)
        config_file = self.config_dir / "environments.yaml"

        data = {
            "environments": [
                env.model_dump(mode="json") for env in self.environments.values()
            ]
        }

        with open(config_file, "w") as f:
            yaml.dump(data, f, default_flow_style=False, allow_unicode=True)

    def list_environments(self, active_only: bool = False) -> list[Environment]:
        """  """
        envs = list(self.environments.values())
        if active_only:
            envs = [e for e in envs if e.is_active]
        return sorted(envs, key=lambda x: -x.priority)

    def get_environment(self, env_id: str) -> Optional[Environment]:
        """  """
        return self.environments.get(env_id)

    def get_environment_by_name(self, name: str) -> Optional[Environment]:
        """  """
        for env in self.environments.values():
            if env.name == name:
                return env
        return None

    def create_environment(self, data: EnvironmentCreate) -> Environment:
        """ """
        env_id = f"env-{uuid4().hex[:8]}"
        now = datetime.utcnow()

        env = Environment(
            id=env_id,
            created_at=now,
            updated_at=now,
            **data.model_dump(),
        )

        self.environments[env_id] = env
        self._save_environments()
        return env

    def update_environment(
        self, env_id: str, data: EnvironmentUpdate
    ) -> Optional[Environment]:
        """ """
        env = self.environments.get(env_id)
        if not env:
            return None

        update_data = data.model_dump(exclude_unset=True)
        for key, value in update_data.items():
            setattr(env, key, value)

        env.updated_at = datetime.utcnow()
        self._save_environments()
        return env

    def delete_environment(self, env_id: str) -> bool:
        """ """
        if env_id in self.environments:
            del self.environments[env_id]
            self._save_environments()
            return True
        return False

    def get_environment_status(self, env_id: str) -> Optional[EnvironmentStatus]:
        """   """
        env = self.environments.get(env_id)
        if not env:
            return None

        containers = self._get_docker_containers(env)

        return EnvironmentStatus(
            environment_id=env.id,
            environment_name=env.name,
            containers=containers,
            total_containers=len(containers),
            running_containers=sum(
                1 for c in containers if c.status == ServiceStatus.UP
            ),
        )

    def _get_docker_containers(self, env: Environment) -> list[ContainerInfo]:
        """Docker   """
        containers = []

        if not Path(env.compose_file).exists():
            return containers

        try:
            # docker compose ps 
            result = subprocess.run(
                [
                    "docker",
                    "compose",
                    "-f",
                    env.compose_file,
                    "-p",
                    "newsinsight",
                    "ps",
                    "--format",
                    "json",
                ],
                capture_output=True,
                text=True,
                timeout=30,
            )

            if result.returncode == 0 and result.stdout.strip():
                import json

                #   JSON   
                for line in result.stdout.strip().split("\n"):
                    if line.strip():
                        try:
                            container_data = json.loads(line)
                            status = self._parse_container_status(
                                container_data.get("State", "")
                            )
                            containers.append(
                                ContainerInfo(
                                    name=container_data.get("Name", "unknown"),
                                    image=container_data.get("Image", "unknown"),
                                    status=status,
                                    health=container_data.get("Health", None),
                                    ports=self._parse_ports(
                                        container_data.get("Ports", "")
                                    ),
                                )
                            )
                        except json.JSONDecodeError:
                            continue

        except subprocess.TimeoutExpired:
            pass
        except FileNotFoundError:
            pass

        return containers

    def _parse_container_status(self, state: str) -> ServiceStatus:
        """  """
        state_lower = state.lower()
        if "running" in state_lower:
            return ServiceStatus.UP
        elif "exited" in state_lower or "dead" in state_lower:
            return ServiceStatus.DOWN
        elif "starting" in state_lower or "created" in state_lower:
            return ServiceStatus.STARTING
        elif "stopping" in state_lower or "removing" in state_lower:
            return ServiceStatus.STOPPING
        return ServiceStatus.UNKNOWN

    def _parse_ports(self, ports_str: str) -> list[str]:
        """  """
        if not ports_str:
            return []
        return [p.strip() for p in ports_str.split(",") if p.strip()]

    async def docker_compose_up(
        self, env_id: str, build: bool = True, detach: bool = True
    ) -> tuple[bool, str]:
        """Docker Compose Up """
        env = self.environments.get(env_id)
        if not env:
            return False, "Environment not found"

        cmd = ["docker", "compose", "-f", env.compose_file, "-p", "newsinsight"]

        if build:
            cmd.extend(["up", "--build"])
        else:
            cmd.extend(["up"])

        if detach:
            cmd.append("-d")

        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=600,  # 10 
                cwd=str(self.project_root),
            )
            return result.returncode == 0, result.stdout + result.stderr
        except subprocess.TimeoutExpired:
            return False, "Command timed out"
        except Exception as e:
            return False, str(e)

    async def docker_compose_down(
        self, env_id: str, volumes: bool = False
    ) -> tuple[bool, str]:
        """Docker Compose Down """
        env = self.environments.get(env_id)
        if not env:
            return False, "Environment not found"

        cmd = ["docker", "compose", "-f", env.compose_file, "-p", "newsinsight", "down"]

        if volumes:
            cmd.append("-v")

        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=120,
                cwd=str(self.project_root),
            )
            return result.returncode == 0, result.stdout + result.stderr
        except subprocess.TimeoutExpired:
            return False, "Command timed out"
        except Exception as e:
            return False, str(e)

    async def docker_compose_restart(
        self, env_id: str, service: Optional[str] = None
    ) -> tuple[bool, str]:
        """Docker Compose Restart """
        env = self.environments.get(env_id)
        if not env:
            return False, "Environment not found"

        cmd = [
            "docker",
            "compose",
            "-f",
            env.compose_file,
            "-p",
            "newsinsight",
            "restart",
        ]

        if service:
            cmd.append(service)

        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=120,
                cwd=str(self.project_root),
            )
            return result.returncode == 0, result.stdout + result.stderr
        except subprocess.TimeoutExpired:
            return False, "Command timed out"
        except Exception as e:
            return False, str(e)

    async def get_service_logs(
        self, env_id: str, service: str, tail: int = 100
    ) -> tuple[bool, str]:
        """  """
        env = self.environments.get(env_id)
        if not env:
            return False, "Environment not found"

        cmd = [
            "docker",
            "compose",
            "-f",
            env.compose_file,
            "-p",
            "newsinsight",
            "logs",
            "--tail",
            str(tail),
            service,
        ]

        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=30,
                cwd=str(self.project_root),
            )
            return result.returncode == 0, result.stdout + result.stderr
        except subprocess.TimeoutExpired:
            return False, "Command timed out"
        except Exception as e:
            return False, str(e)

```

---

## backend/admin-dashboard/api/services/health_service.py

```py
"""
Service Health Monitoring Service
     
"""

import asyncio
import os
from datetime import datetime
from pathlib import Path
from typing import Optional
import json

try:
    import httpx
except ImportError:
    httpx = None  # type: ignore

from ..models.schemas import (
    ServiceHealthStatus,
    ServiceHealth,
    InfrastructureHealth,
    OverallSystemHealth,
)


class HealthService:
    """   """

    def __init__(self, project_root: str, config_dir: str):
        self.project_root = Path(project_root)
        self.config_dir = Path(config_dir)
        self.services_config_path = (
            self.project_root / "etc" / "configs" / "services.json"
        )
        self._services_config: Optional[dict] = None
        self._last_check: dict[str, ServiceHealth] = {}
        self.timeout = 5.0  #   ()

    def _load_services_config(self) -> dict:
        """  """
        if self._services_config is None:
            if self.services_config_path.exists():
                with open(self.services_config_path, "r") as f:
                    self._services_config = json.load(f)
            else:
                self._services_config = {
                    "services": {},
                    "infrastructure": {},
                    "service_urls": {},
                }
        return self._services_config or {}

    def get_all_services(self) -> list[dict]:
        """   """
        config = self._load_services_config()
        services = []

        #  
        for service_id, service_info in config.get("services", {}).items():
            services.append(
                {
                    "id": service_id,
                    "name": service_info.get("name", service_id),
                    "description": service_info.get("description", ""),
                    "port": service_info.get("port"),
                    "healthcheck": service_info.get("healthcheck", "/health"),
                    "hostname": service_info.get("hostname", service_id),
                    "type": "service",
                    "tags": service_info.get("consul", {}).get("tags", []),
                }
            )

        # ML 
        for addon_id, addon_info in config.get("ml-addons", {}).items():
            services.append(
                {
                    "id": addon_id,
                    "name": addon_info.get("name", addon_id),
                    "description": f"ML Addon - {addon_info.get('name', addon_id)}",
                    "port": addon_info.get("port"),
                    "healthcheck": addon_info.get("healthcheck", "/health"),
                    "hostname": addon_id,
                    "type": "ml-addon",
                    "tags": ["ml", "addon"],
                }
            )

        return services

    def get_infrastructure_services(self) -> list[dict]:
        """   """
        config = self._load_services_config()
        infra_services = []

        for infra_id, infra_info in config.get("infrastructure", {}).items():
            infra_services.append(
                {
                    "id": infra_id,
                    "name": infra_id.capitalize(),
                    "port": infra_info.get("port"),
                    "image": infra_info.get("image"),
                    "healthcheck": infra_info.get("healthcheck"),
                    "type": "infrastructure",
                }
            )

        return infra_services

    async def check_service_health(self, service_id: str) -> ServiceHealth:
        """   """
        config = self._load_services_config()
        service_urls = config.get("service_urls", {})

        #   
        service_info = None
        for services_dict in [config.get("services", {}), config.get("ml-addons", {})]:
            if service_id in services_dict:
                service_info = services_dict[service_id]
                break

        if not service_info:
            return ServiceHealth(
                service_id=service_id,
                name=service_id,
                status=ServiceHealthStatus.UNKNOWN,
                message="Service not found in configuration",
                checked_at=datetime.utcnow(),
            )

        #  URL 
        base_url = service_urls.get(service_id)
        if not base_url:
            hostname = service_info.get("hostname", service_id)
            port = service_info.get("port", service_info.get("api_port"))
            base_url = f"http://{hostname}:{port}"

        healthcheck_path = service_info.get("healthcheck", "/health")
        health_url = f"{base_url}{healthcheck_path}"

        if httpx is None:
            return ServiceHealth(
                service_id=service_id,
                name=service_info.get("name", service_id),
                status=ServiceHealthStatus.UNKNOWN,
                message="httpx not installed",
                url=health_url,
                checked_at=datetime.utcnow(),
            )

        #   
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                start_time = datetime.utcnow()
                response = await client.get(health_url)
                response_time_ms = (
                    datetime.utcnow() - start_time
                ).total_seconds() * 1000

                if response.status_code == 200:
                    status = ServiceHealthStatus.HEALTHY
                    message = "Service is healthy"
                    try:
                        health_data = response.json()
                    except Exception:
                        health_data = None
                elif response.status_code >= 500:
                    status = ServiceHealthStatus.UNHEALTHY
                    message = f"Server error: {response.status_code}"
                    health_data = None
                else:
                    status = ServiceHealthStatus.DEGRADED
                    message = f"Unexpected status: {response.status_code}"
                    health_data = None

                health = ServiceHealth(
                    service_id=service_id,
                    name=service_info.get("name", service_id),
                    status=status,
                    message=message,
                    response_time_ms=response_time_ms,
                    url=health_url,
                    checked_at=datetime.utcnow(),
                    details=health_data,
                )

        except Exception as e:
            error_type = type(e).__name__
            if "Timeout" in error_type:
                status = ServiceHealthStatus.UNHEALTHY
                message = "Connection timeout"
            elif "Connect" in error_type:
                status = ServiceHealthStatus.UNREACHABLE
                message = "Connection refused - service may be down"
            else:
                status = ServiceHealthStatus.UNKNOWN
                message = f"Error: {str(e)}"

            health = ServiceHealth(
                service_id=service_id,
                name=service_info.get("name", service_id),
                status=status,
                message=message,
                url=health_url,
                checked_at=datetime.utcnow(),
            )

        self._last_check[service_id] = health
        return health

    async def check_all_services_health(self) -> list[ServiceHealth]:
        """    ()"""
        services = self.get_all_services()

        #    
        tasks = [self.check_service_health(service["id"]) for service in services]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        health_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                health_results.append(
                    ServiceHealth(
                        service_id=services[i]["id"],
                        name=services[i]["name"],
                        status=ServiceHealthStatus.UNKNOWN,
                        message=f"Error: {str(result)}",
                        checked_at=datetime.utcnow(),
                    )
                )
            else:
                health_results.append(result)

        return health_results

    async def check_infrastructure_health(self) -> list[InfrastructureHealth]:
        """   """
        infra_services = self.get_infrastructure_services()
        results = []

        for infra in infra_services:
            infra_id = infra["id"]
            port = infra["port"]

            #   
            if infra_id == "postgres":
                health = await self._check_postgres(port)
            elif infra_id == "mongo":
                health = await self._check_mongo(port)
            elif infra_id == "redis":
                health = await self._check_redis(port)
            elif infra_id == "consul":
                health = await self._check_consul(port)
            elif infra_id == "redpanda":
                health = await self._check_redpanda(port)
            else:
                health = InfrastructureHealth(
                    service_id=infra_id,
                    name=infra["name"],
                    status=ServiceHealthStatus.UNKNOWN,
                    message="Unknown infrastructure type",
                    checked_at=datetime.utcnow(),
                )

            results.append(health)

        return results

    async def _check_postgres(self, port: int) -> InfrastructureHealth:
        """PostgreSQL  """
        try:
            reader, writer = await asyncio.wait_for(
                asyncio.open_connection("postgres", port), timeout=self.timeout
            )
            writer.close()
            await writer.wait_closed()

            return InfrastructureHealth(
                service_id="postgres",
                name="PostgreSQL",
                status=ServiceHealthStatus.HEALTHY,
                message="PostgreSQL is accepting connections",
                port=port,
                checked_at=datetime.utcnow(),
            )
        except asyncio.TimeoutError:
            return InfrastructureHealth(
                service_id="postgres",
                name="PostgreSQL",
                status=ServiceHealthStatus.UNHEALTHY,
                message="Connection timeout",
                port=port,
                checked_at=datetime.utcnow(),
            )
        except Exception as e:
            return InfrastructureHealth(
                service_id="postgres",
                name="PostgreSQL",
                status=ServiceHealthStatus.UNREACHABLE,
                message=f"Connection failed: {str(e)}",
                port=port,
                checked_at=datetime.utcnow(),
            )

    async def _check_mongo(self, port: int) -> InfrastructureHealth:
        """MongoDB  """
        try:
            reader, writer = await asyncio.wait_for(
                asyncio.open_connection("mongo", port), timeout=self.timeout
            )
            writer.close()
            await writer.wait_closed()

            return InfrastructureHealth(
                service_id="mongo",
                name="MongoDB",
                status=ServiceHealthStatus.HEALTHY,
                message="MongoDB is accepting connections",
                port=port,
                checked_at=datetime.utcnow(),
            )
        except Exception as e:
            return InfrastructureHealth(
                service_id="mongo",
                name="MongoDB",
                status=ServiceHealthStatus.UNREACHABLE,
                message=f"Connection failed: {str(e)}",
                port=port,
                checked_at=datetime.utcnow(),
            )

    async def _check_redis(self, port: int) -> InfrastructureHealth:
        """Redis  """
        try:
            reader, writer = await asyncio.wait_for(
                asyncio.open_connection("redis", port), timeout=self.timeout
            )
            # Redis PING 
            writer.write(b"PING\r\n")
            await writer.drain()
            response = await asyncio.wait_for(reader.readline(), timeout=2.0)
            writer.close()
            await writer.wait_closed()

            if b"+PONG" in response:
                return InfrastructureHealth(
                    service_id="redis",
                    name="Redis",
                    status=ServiceHealthStatus.HEALTHY,
                    message="Redis is responding to PING",
                    port=port,
                    checked_at=datetime.utcnow(),
                )
            else:
                return InfrastructureHealth(
                    service_id="redis",
                    name="Redis",
                    status=ServiceHealthStatus.DEGRADED,
                    message="Redis connected but unexpected response",
                    port=port,
                    checked_at=datetime.utcnow(),
                )
        except Exception as e:
            return InfrastructureHealth(
                service_id="redis",
                name="Redis",
                status=ServiceHealthStatus.UNREACHABLE,
                message=f"Connection failed: {str(e)}",
                port=port,
                checked_at=datetime.utcnow(),
            )

    async def _check_consul(self, port: int) -> InfrastructureHealth:
        """Consul  """
        if httpx is None:
            return InfrastructureHealth(
                service_id="consul",
                name="Consul",
                status=ServiceHealthStatus.UNKNOWN,
                message="httpx not installed",
                port=port,
                checked_at=datetime.utcnow(),
            )

        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.get(f"http://consul:{port}/v1/status/leader")

                if response.status_code == 200:
                    leader = response.text.strip('"')
                    return InfrastructureHealth(
                        service_id="consul",
                        name="Consul",
                        status=ServiceHealthStatus.HEALTHY,
                        message=f"Consul leader: {leader}",
                        port=port,
                        checked_at=datetime.utcnow(),
                        details={"leader": leader},
                    )
                else:
                    return InfrastructureHealth(
                        service_id="consul",
                        name="Consul",
                        status=ServiceHealthStatus.DEGRADED,
                        message=f"Consul returned {response.status_code}",
                        port=port,
                        checked_at=datetime.utcnow(),
                    )
        except Exception as e:
            return InfrastructureHealth(
                service_id="consul",
                name="Consul",
                status=ServiceHealthStatus.UNREACHABLE,
                message=f"Connection failed: {str(e)}",
                port=port,
                checked_at=datetime.utcnow(),
            )

    async def _check_redpanda(self, port: int) -> InfrastructureHealth:
        """Redpanda/Kafka  """
        if httpx is None:
            return InfrastructureHealth(
                service_id="redpanda",
                name="Redpanda",
                status=ServiceHealthStatus.UNKNOWN,
                message="httpx not installed",
                port=port,
                checked_at=datetime.utcnow(),
            )

        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.get("http://redpanda:9644/v1/status/ready")

                if response.status_code == 200:
                    return InfrastructureHealth(
                        service_id="redpanda",
                        name="Redpanda",
                        status=ServiceHealthStatus.HEALTHY,
                        message="Redpanda is ready",
                        port=port,
                        checked_at=datetime.utcnow(),
                    )
                else:
                    return InfrastructureHealth(
                        service_id="redpanda",
                        name="Redpanda",
                        status=ServiceHealthStatus.DEGRADED,
                        message=f"Redpanda returned {response.status_code}",
                        port=port,
                        checked_at=datetime.utcnow(),
                    )
        except Exception as e:
            return InfrastructureHealth(
                service_id="redpanda",
                name="Redpanda",
                status=ServiceHealthStatus.UNREACHABLE,
                message=f"Connection failed: {str(e)}",
                port=port,
                checked_at=datetime.utcnow(),
            )

    async def get_overall_health(self) -> OverallSystemHealth:
        """    """
        services_health = await self.check_all_services_health()
        infra_health = await self.check_infrastructure_health()

        #  
        total_services = len(services_health)
        healthy_services = sum(
            1 for s in services_health if s.status == ServiceHealthStatus.HEALTHY
        )
        unhealthy_services = sum(
            1
            for s in services_health
            if s.status
            in [ServiceHealthStatus.UNHEALTHY, ServiceHealthStatus.UNREACHABLE]
        )
        degraded_services = sum(
            1 for s in services_health if s.status == ServiceHealthStatus.DEGRADED
        )

        total_infra = len(infra_health)
        healthy_infra = sum(
            1 for i in infra_health if i.status == ServiceHealthStatus.HEALTHY
        )

        #   
        if unhealthy_services > 0 or healthy_infra < total_infra:
            if healthy_services == 0:
                overall_status = ServiceHealthStatus.UNHEALTHY
            else:
                overall_status = ServiceHealthStatus.DEGRADED
        elif degraded_services > 0:
            overall_status = ServiceHealthStatus.DEGRADED
        else:
            overall_status = ServiceHealthStatus.HEALTHY

        #    
        response_times = [
            s.response_time_ms
            for s in services_health
            if s.response_time_ms is not None
        ]
        avg_response_time = (
            sum(response_times) / len(response_times) if response_times else None
        )

        return OverallSystemHealth(
            status=overall_status,
            total_services=total_services,
            healthy_services=healthy_services,
            unhealthy_services=unhealthy_services,
            degraded_services=degraded_services,
            total_infrastructure=total_infra,
            healthy_infrastructure=healthy_infra,
            average_response_time_ms=avg_response_time,
            services=services_health,
            infrastructure=infra_health,
            checked_at=datetime.utcnow(),
        )

    def get_last_check(self, service_id: str) -> Optional[ServiceHealth]:
        """    """
        return self._last_check.get(service_id)

```

---

## backend/admin-dashboard/api/services/kafka_service.py

```py
"""
Kafka/Redpanda Monitoring Service
Kafka/Redpanda   
"""

import os
from datetime import datetime
from pathlib import Path
from typing import Optional

try:
    import httpx
except ImportError:
    httpx = None  # type: ignore

from ..models.schemas import (
    KafkaTopicInfo,
    KafkaConsumerGroupInfo,
    KafkaClusterInfo,
)


class KafkaService:
    """Kafka/Redpanda  """

    def __init__(self, project_root: str, config_dir: str):
        self.project_root = Path(project_root)
        self.config_dir = Path(config_dir)

        # Redpanda Admin API  (Kafka-compatible)
        self.redpanda_host = os.environ.get("REDPANDA_HOST", "redpanda")
        self.redpanda_admin_port = int(os.environ.get("REDPANDA_ADMIN_PORT", "9644"))
        self.redpanda_kafka_port = int(os.environ.get("REDPANDA_KAFKA_PORT", "9092"))

        self.admin_api_base = f"http://{self.redpanda_host}:{self.redpanda_admin_port}"
        self.timeout = 10.0

    async def get_cluster_info(self) -> KafkaClusterInfo:
        """   """
        topics = await self.list_topics()
        consumer_groups = await self.list_consumer_groups()

        total_partitions = sum(t.partition_count for t in topics)
        total_messages = None

        #    
        broker_count = 1  # 
        controller_id = None
        cluster_id = None

        try:
            if httpx:
                async with httpx.AsyncClient(timeout=self.timeout) as client:
                    # Redpanda Admin API - brokers
                    response = await client.get(f"{self.admin_api_base}/v1/brokers")
                    if response.status_code == 200:
                        brokers = response.json()
                        broker_count = len(brokers) if isinstance(brokers, list) else 1

                    # Redpanda Admin API - cluster health
                    response = await client.get(
                        f"{self.admin_api_base}/v1/cluster/health_overview"
                    )
                    if response.status_code == 200:
                        health = response.json()
                        controller_id = health.get("controller_id")
        except Exception:
            pass

        return KafkaClusterInfo(
            broker_count=broker_count,
            controller_id=controller_id,
            cluster_id=cluster_id,
            topics=topics,
            consumer_groups=consumer_groups,
            total_topics=len(topics),
            total_partitions=total_partitions,
            total_messages=total_messages,
            checked_at=datetime.utcnow(),
        )

    async def list_topics(self) -> list[KafkaTopicInfo]:
        """   """
        topics = []

        try:
            if httpx:
                async with httpx.AsyncClient(timeout=self.timeout) as client:
                    # Redpanda Admin API - topics
                    response = await client.get(f"{self.admin_api_base}/v1/topics")
                    if response.status_code == 200:
                        topic_list = response.json()
                        for topic_data in topic_list:
                            if isinstance(topic_data, dict):
                                topics.append(
                                    KafkaTopicInfo(
                                        name=topic_data.get(
                                            "topic", topic_data.get("name", "unknown")
                                        ),
                                        partition_count=topic_data.get(
                                            "partition_count", 1
                                        ),
                                        replication_factor=topic_data.get(
                                            "replication_factor", 1
                                        ),
                                        message_count=topic_data.get("message_count"),
                                        size_bytes=topic_data.get("size_bytes"),
                                        retention_ms=topic_data.get("retention_ms"),
                                        is_internal=topic_data.get(
                                            "is_internal", False
                                        ),
                                    )
                                )
                            elif isinstance(topic_data, str):
                                #    
                                topic_detail = await self.get_topic_detail(topic_data)
                                if topic_detail:
                                    topics.append(topic_detail)
        except Exception as e:
            #      
            topics = [
                KafkaTopicInfo(
                    name="news-raw",
                    partition_count=3,
                    replication_factor=1,
                    is_internal=False,
                ),
                KafkaTopicInfo(
                    name="news-processed",
                    partition_count=3,
                    replication_factor=1,
                    is_internal=False,
                ),
                KafkaTopicInfo(
                    name="crawl-jobs",
                    partition_count=1,
                    replication_factor=1,
                    is_internal=False,
                ),
            ]

        return topics

    async def get_topic_detail(self, topic_name: str) -> Optional[KafkaTopicInfo]:
        """   """
        try:
            if httpx:
                async with httpx.AsyncClient(timeout=self.timeout) as client:
                    response = await client.get(
                        f"{self.admin_api_base}/v1/topics/{topic_name}"
                    )
                    if response.status_code == 200:
                        data = response.json()
                        return KafkaTopicInfo(
                            name=data.get("topic", topic_name),
                            partition_count=len(data.get("partitions", []))
                            or data.get("partition_count", 1),
                            replication_factor=data.get("replication_factor", 1),
                            message_count=data.get("message_count"),
                            size_bytes=data.get("size_bytes"),
                            retention_ms=data.get("retention_ms"),
                            is_internal=data.get(
                                "is_internal", topic_name.startswith("_")
                            ),
                        )
        except Exception:
            pass

        return KafkaTopicInfo(
            name=topic_name,
            partition_count=1,
            replication_factor=1,
            is_internal=topic_name.startswith("_"),
        )

    async def list_consumer_groups(self) -> list[KafkaConsumerGroupInfo]:
        """   """
        groups = []

        try:
            if httpx:
                async with httpx.AsyncClient(timeout=self.timeout) as client:
                    # Redpanda Admin API - consumer groups
                    response = await client.get(
                        f"{self.admin_api_base}/v1/consumer_groups"
                    )
                    if response.status_code == 200:
                        group_list = response.json()
                        for group_data in group_list:
                            if isinstance(group_data, dict):
                                groups.append(
                                    KafkaConsumerGroupInfo(
                                        group_id=group_data.get(
                                            "group_id",
                                            group_data.get("name", "unknown"),
                                        ),
                                        state=group_data.get("state", "Unknown"),
                                        members_count=group_data.get(
                                            "members_count",
                                            len(group_data.get("members", [])),
                                        ),
                                        topics=group_data.get("topics", []),
                                        total_lag=group_data.get("total_lag", 0),
                                        lag_per_partition=group_data.get(
                                            "lag_per_partition", {}
                                        ),
                                    )
                                )
                            elif isinstance(group_data, str):
                                #  ID  
                                group_detail = await self.get_consumer_group_detail(
                                    group_data
                                )
                                if group_detail:
                                    groups.append(group_detail)
        except Exception as e:
            #      
            groups = [
                KafkaConsumerGroupInfo(
                    group_id="news-processor",
                    state="Stable",
                    members_count=2,
                    topics=["news-raw"],
                    total_lag=0,
                    lag_per_partition={},
                ),
                KafkaConsumerGroupInfo(
                    group_id="crawler-consumer",
                    state="Stable",
                    members_count=1,
                    topics=["crawl-jobs"],
                    total_lag=0,
                    lag_per_partition={},
                ),
            ]

        return groups

    async def get_consumer_group_detail(
        self, group_id: str
    ) -> Optional[KafkaConsumerGroupInfo]:
        """    """
        try:
            if httpx:
                async with httpx.AsyncClient(timeout=self.timeout) as client:
                    response = await client.get(
                        f"{self.admin_api_base}/v1/consumer_groups/{group_id}"
                    )
                    if response.status_code == 200:
                        data = response.json()
                        return KafkaConsumerGroupInfo(
                            group_id=data.get("group_id", group_id),
                            state=data.get("state", "Unknown"),
                            members_count=len(data.get("members", [])),
                            topics=data.get("topics", []),
                            total_lag=data.get("total_lag", 0),
                            lag_per_partition=data.get("lag_per_partition", {}),
                        )
        except Exception:
            pass

        return KafkaConsumerGroupInfo(
            group_id=group_id,
            state="Unknown",
            members_count=0,
            topics=[],
            total_lag=0,
            lag_per_partition={},
        )

    async def check_health(self) -> dict:
        """Kafka/Redpanda  """
        import asyncio

        try:
            # TCP  
            reader, writer = await asyncio.wait_for(
                asyncio.open_connection(self.redpanda_host, self.redpanda_kafka_port),
                timeout=5.0,
            )
            writer.close()
            await writer.wait_closed()

            return {
                "status": "healthy",
                "host": self.redpanda_host,
                "kafka_port": self.redpanda_kafka_port,
                "admin_port": self.redpanda_admin_port,
                "checked_at": datetime.utcnow().isoformat(),
            }
        except Exception as e:
            return {
                "status": "unreachable",
                "host": self.redpanda_host,
                "kafka_port": self.redpanda_kafka_port,
                "admin_port": self.redpanda_admin_port,
                "error": str(e),
                "checked_at": datetime.utcnow().isoformat(),
            }

```

---

## backend/admin-dashboard/api/services/script_service.py

```py
"""
Script Service - /    
"""
import asyncio
import os
import re
import signal
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Any, AsyncGenerator, Optional
from uuid import uuid4

import yaml

from ..models.schemas import (
    RiskLevel,
    Script,
    ScriptCreate,
    ScriptParameter,
    ScriptUpdate,
    TaskExecution,
    TaskLog,
    TaskStatus,
    UserRole,
)


# ============================================================================
# :   
# ============================================================================

#      
DANGEROUS_COMMAND_PATTERNS = [
    #   
    r'\brm\s+(-[a-zA-Z]*f[a-zA-Z]*\s+)?(-[a-zA-Z]*r[a-zA-Z]*\s+)?/\s*$',  # rm -rf /
    r'\brm\s+(-[a-zA-Z]*r[a-zA-Z]*\s+)?(-[a-zA-Z]*f[a-zA-Z]*\s+)?/\s*$',  # rm -fr /
    r'\brm\s+-[a-zA-Z]*\s+/\s*$',  # rm -* /
    r'\brm\s+--no-preserve-root',  # rm --no-preserve-root
    r':\s*\(\)\s*\{\s*:\s*\|\s*:\s*&\s*\}\s*;',  # fork bomb :(){ :|:& };:
    r'\bdd\s+.*of=/dev/(sd[a-z]|hd[a-z]|nvme)',  # dd to disk devices
    r'\bmkfs\s+',  # format filesystem
    r'\bfdisk\s+',  # partition table manipulation
    r'\bparted\s+',  # partition manipulation
    
    #   
    r'\brm\s+.*\s+/boot\b',
    r'\brm\s+.*\s+/etc\b',
    r'\brm\s+.*\s+/usr\b',
    r'\brm\s+.*\s+/bin\b',
    r'\brm\s+.*\s+/sbin\b',
    r'\brm\s+.*\s+/lib\b',
    r'\brm\s+.*\s+/var\b',
    r'\brm\s+.*\s+/home\b',
    r'\brm\s+.*\s+/root\b',
    r'\brm\s+.*\s+/sys\b',
    r'\brm\s+.*\s+/proc\b',
    r'\brm\s+.*\s+/dev\b',
    
    #     
    r'\bchmod\s+777\s+/',  # chmod 777 /
    r'\bchmod\s+-R\s+777\s+/',  # chmod -R 777 /
    r'\bchown\s+-R\s+.*:.*\s+/',  # chown -R on root
    
    #   
    r'\bnc\s+-[a-zA-Z]*e',  # netcat with execute
    r'\bcurl\s+.*\|\s*(ba)?sh',  # curl pipe to shell
    r'\bwget\s+.*\|\s*(ba)?sh',  # wget pipe to shell
    
    #      
    r'\b(xmrig|minerd|cgminer|bfgminer)\b',
    
    #  /
    r'\bshutdown\b',
    r'\breboot\b',
    r'\bhalt\b',
    r'\bpoweroff\b',
    r'\binit\s+[06]\b',
    
    # / 
    r'\bpasswd\s+root\b',
    r'\busermod\s+-[a-zA-Z]*\s+root\b',
    r'\buserdel\s+',
    r'\bgroupdel\s+',
    
    #   
    r'\bexport\s+PATH=\s*$',  # PATH 
    r'\bexport\s+LD_PRELOAD=',  # LD_PRELOAD 
    
    #   
    r'\brm\s+.*(/var/log|\.log)',
    r'>\s*/var/log/',
    r'\btruncate\s+.*(/var/log|\.log)',
]

#   (sudo     )
DANGEROUS_WITH_SUDO = [
    r'\bsudo\s+rm\s+-[a-zA-Z]*r',
    r'\bsudo\s+rm\s+/',
    r'\bsudo\s+dd\b',
    r'\bsudo\s+mkfs\b',
    r'\bsudo\s+fdisk\b',
]


class CommandSecurityError(Exception):
    """    """
    def __init__(self, command: str, reason: str, pattern: str = None):
        self.command = command
        self.reason = reason
        self.pattern = pattern
        super().__init__(f"Security violation: {reason}")


def validate_command_security(command: str) -> tuple[bool, str]:
    """
      
    
    Args:
        command:  
        
    Returns:
        (is_safe, reason):   
        
    Raises:
        CommandSecurityError:    
    """
    #   ( ,   )
    normalized = ' '.join(command.lower().split())
    
    #   
    for pattern in DANGEROUS_COMMAND_PATTERNS:
        if re.search(pattern, normalized, re.IGNORECASE):
            raise CommandSecurityError(
                command=command,
                reason=f"   :        .",
                pattern=pattern
            )
    
    # sudo +    
    for pattern in DANGEROUS_WITH_SUDO:
        if re.search(pattern, normalized, re.IGNORECASE):
            raise CommandSecurityError(
                command=command,
                reason=f"sudo     .    .",
                pattern=pattern
            )
    
    return True, "OK"


class ScriptService:
    """    """

    def __init__(self, project_root: str, config_dir: str):
        self.project_root = Path(project_root)
        self.config_dir = Path(config_dir)
        self.scripts: dict[str, Script] = {}
        self.executions: dict[str, TaskExecution] = {}
        self.running_processes: dict[str, subprocess.Popen] = {}
        self._load_scripts()

    def _load_scripts(self) -> None:
        """    """
        config_file = self.config_dir / "scripts.yaml"
        if config_file.exists():
            with open(config_file) as f:
                data = yaml.safe_load(f) or {}
                for script_data in data.get("scripts", []):
                    # parameters ScriptParameter  
                    if "parameters" in script_data:
                        script_data["parameters"] = [
                            ScriptParameter(**p) if isinstance(p, dict) else p
                            for p in script_data["parameters"]
                        ]
                    script = Script(**script_data)
                    self.scripts[script.id] = script
        else:
            self._create_default_scripts()

    def _create_default_scripts(self) -> None:
        """   """
        scripts_dir = self.project_root / "scripts"
        now = datetime.utcnow()

        default_scripts = [
            {
                "id": "script-start",
                "name": " ",
                "description": "  Docker Compose  .",
                "command": "docker compose -f {compose_file} -p newsinsight up -d",
                "working_dir": str(self.project_root),
                "risk_level": RiskLevel.LOW,
                "estimated_duration": 120,
                "allowed_environments": ["zerotrust", "local", "production", "staging"],
                "required_role": UserRole.OPERATOR,
                "parameters": [
                    ScriptParameter(
                        name="build",
                        param_type="boolean",
                        required=False,
                        default=True,
                        description="  ",
                    ),
                ],
                "tags": ["docker", "deploy"],
                "created_at": now,
                "updated_at": now,
            },
            {
                "id": "script-stop",
                "name": " ",
                "description": "  Docker Compose  .",
                "command": "docker compose -f {compose_file} -p newsinsight down",
                "working_dir": str(self.project_root),
                "risk_level": RiskLevel.MEDIUM,
                "estimated_duration": 60,
                "allowed_environments": ["zerotrust", "local", "production", "staging"],
                "required_role": UserRole.OPERATOR,
                "parameters": [],
                "tags": ["docker", "stop"],
                "created_at": now,
                "updated_at": now,
            },
            {
                "id": "script-restart",
                "name": " ",
                "description": "  Docker Compose  .",
                "command": "docker compose -f {compose_file} -p newsinsight restart {service}",
                "working_dir": str(self.project_root),
                "risk_level": RiskLevel.MEDIUM,
                "estimated_duration": 60,
                "allowed_environments": ["zerotrust", "local", "production", "staging"],
                "required_role": UserRole.OPERATOR,
                "parameters": [
                    ScriptParameter(
                        name="service",
                        param_type="string",
                        required=False,
                        default="",
                        description="   (  )",
                    ),
                ],
                "tags": ["docker", "restart"],
                "created_at": now,
                "updated_at": now,
            },
            {
                "id": "script-full-cleanup",
                "name": "  (Full Cleanup)",
                "description": ", , ,   .    !",
                "command": """docker compose -f {compose_file} -p newsinsight down -v && \
docker builder prune -f && \
docker image prune -f && \
docker volume prune -f""",
                "working_dir": str(self.project_root),
                "risk_level": RiskLevel.CRITICAL,
                "estimated_duration": 180,
                "allowed_environments": ["local", "staging"],
                "required_role": UserRole.ADMIN,
                "parameters": [],
                "tags": ["docker", "cleanup", "dangerous"],
                "created_at": now,
                "updated_at": now,
            },
            {
                "id": "script-status",
                "name": "  ",
                "description": "     .",
                "command": "docker compose -f {compose_file} -p newsinsight ps -a",
                "working_dir": str(self.project_root),
                "risk_level": RiskLevel.LOW,
                "estimated_duration": 5,
                "allowed_environments": ["zerotrust", "local", "production", "staging"],
                "required_role": UserRole.VIEWER,
                "parameters": [],
                "tags": ["docker", "status"],
                "created_at": now,
                "updated_at": now,
            },
            {
                "id": "script-logs",
                "name": "  ",
                "description": "   .",
                "command": "docker compose -f {compose_file} -p newsinsight logs --tail {tail} {service}",
                "working_dir": str(self.project_root),
                "risk_level": RiskLevel.LOW,
                "estimated_duration": 10,
                "allowed_environments": ["zerotrust", "local", "production", "staging"],
                "required_role": UserRole.VIEWER,
                "parameters": [
                    ScriptParameter(
                        name="service",
                        param_type="string",
                        required=True,
                        description="   ",
                    ),
                    ScriptParameter(
                        name="tail",
                        param_type="number",
                        required=False,
                        default=100,
                        description="   ",
                    ),
                ],
                "tags": ["docker", "logs"],
                "created_at": now,
                "updated_at": now,
            },
            {
                "id": "script-build-push",
                "name": "   ",
                "description": "Docker    .",
                "command": str(scripts_dir / "build-and-push.sh"),
                "working_dir": str(self.project_root),
                "risk_level": RiskLevel.MEDIUM,
                "estimated_duration": 300,
                "allowed_environments": ["production", "staging"],
                "required_role": UserRole.OPERATOR,
                "parameters": [
                    ScriptParameter(
                        name="tag",
                        param_type="string",
                        required=False,
                        default="latest",
                        description=" ",
                    ),
                ],
                "tags": ["docker", "build", "ci"],
                "created_at": now,
                "updated_at": now,
            },
            {
                "id": "script-health-check",
                "name": "",
                "description": "    .",
                "command": """docker compose -f {compose_file} -p newsinsight ps --format json | \
python3 -c "import sys,json; [print(f'{json.loads(l).get(\"Name\")}: {json.loads(l).get(\"Health\", \"N/A\")}') for l in sys.stdin if l.strip()]" """,
                "working_dir": str(self.project_root),
                "risk_level": RiskLevel.LOW,
                "estimated_duration": 10,
                "allowed_environments": ["zerotrust", "local", "production", "staging"],
                "required_role": UserRole.VIEWER,
                "parameters": [],
                "tags": ["health", "monitoring"],
                "created_at": now,
                "updated_at": now,
            },
        ]

        for script_data in default_scripts:
            script = Script(**script_data)
            self.scripts[script.id] = script

        self._save_scripts()

    def _save_scripts(self) -> None:
        """   """
        self.config_dir.mkdir(parents=True, exist_ok=True)
        config_file = self.config_dir / "scripts.yaml"

        data = {
            "scripts": [
                script.model_dump(mode="json") for script in self.scripts.values()
            ]
        }

        with open(config_file, "w") as f:
            yaml.dump(data, f, default_flow_style=False, allow_unicode=True)

    def list_scripts(
        self,
        environment: Optional[str] = None,
        tag: Optional[str] = None,
        role: Optional[UserRole] = None,
    ) -> list[Script]:
        """  """
        scripts = list(self.scripts.values())

        if environment:
            scripts = [
                s
                for s in scripts
                if not s.allowed_environments or environment in s.allowed_environments
            ]

        if tag:
            scripts = [s for s in scripts if tag in s.tags]

        if role:
            role_priority = {UserRole.VIEWER: 0, UserRole.OPERATOR: 1, UserRole.ADMIN: 2}
            user_level = role_priority.get(role, 0)
            scripts = [
                s for s in scripts if role_priority.get(s.required_role, 0) <= user_level
            ]

        return scripts

    def get_script(self, script_id: str) -> Optional[Script]:
        """  """
        return self.scripts.get(script_id)

    def create_script(self, data: ScriptCreate) -> Script:
        """ """
        script_id = f"script-{uuid4().hex[:8]}"
        now = datetime.utcnow()

        script = Script(
            id=script_id,
            created_at=now,
            updated_at=now,
            **data.model_dump(),
        )

        self.scripts[script_id] = script
        self._save_scripts()
        return script

    def update_script(self, script_id: str, data: ScriptUpdate) -> Optional[Script]:
        """ """
        script = self.scripts.get(script_id)
        if not script:
            return None

        update_data = data.model_dump(exclude_unset=True)
        for key, value in update_data.items():
            setattr(script, key, value)

        script.updated_at = datetime.utcnow()
        self._save_scripts()
        return script

    def delete_script(self, script_id: str) -> bool:
        """ """
        if script_id in self.scripts:
            del self.scripts[script_id]
            self._save_scripts()
            return True
        return False

    async def execute_script(
        self,
        script_id: str,
        environment_name: str,
        compose_file: str,
        parameters: dict[str, Any],
        executed_by: str,
    ) -> TaskExecution:
        """ """
        script = self.scripts.get(script_id)
        if not script:
            raise ValueError(f"Script not found: {script_id}")

        execution_id = f"exec-{uuid4().hex[:8]}"
        now = datetime.utcnow()

        #   
        command = script.command.format(
            compose_file=compose_file,
            **parameters,
        )
        
        #  :   
        try:
            validate_command_security(command)
        except CommandSecurityError as e:
            raise ValueError(f" : {e.reason}")

        execution = TaskExecution(
            id=execution_id,
            script_id=script_id,
            script_name=script.name,
            environment_id=environment_name,
            environment_name=environment_name,
            status=TaskStatus.RUNNING,
            parameters=parameters,
            started_at=now,
            executed_by=executed_by,
        )

        self.executions[execution_id] = execution

        #  
        asyncio.create_task(
            self._run_command(execution_id, command, script.working_dir)
        )

        return execution

    async def _run_command(
        self, execution_id: str, command: str, working_dir: Optional[str]
    ) -> None:
        """  ()"""
        execution = self.executions.get(execution_id)
        if not execution:
            return

        try:
            process = await asyncio.create_subprocess_shell(
                command,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.STDOUT,
                cwd=working_dir,
            )

            self.running_processes[execution_id] = process

            #  
            stdout, _ = await process.communicate()

            execution.exit_code = process.returncode
            execution.status = (
                TaskStatus.SUCCESS if process.returncode == 0 else TaskStatus.FAILED
            )
            execution.finished_at = datetime.utcnow()

            if process.returncode != 0:
                execution.error_message = stdout.decode() if stdout else "Unknown error"

        except Exception as e:
            execution.status = TaskStatus.FAILED
            execution.error_message = str(e)
            execution.finished_at = datetime.utcnow()
        finally:
            if execution_id in self.running_processes:
                del self.running_processes[execution_id]

    async def stream_execution_output(
        self,
        script_id: str,
        environment_name: str,
        compose_file: str,
        parameters: dict[str, Any],
        executed_by: str,
    ) -> AsyncGenerator[str, None]:
        """    """
        script = self.scripts.get(script_id)
        if not script:
            yield f"Error: Script not found: {script_id}\n"
            return

        execution_id = f"exec-{uuid4().hex[:8]}"
        now = datetime.utcnow()

        #   
        command = script.command.format(
            compose_file=compose_file,
            **parameters,
        )
        
        #  :   
        try:
            validate_command_security(command)
        except CommandSecurityError as e:
            yield f"[SECURITY ERROR] {e.reason}\n"
            yield f"[BLOCKED]     .\n"
            return

        execution = TaskExecution(
            id=execution_id,
            script_id=script_id,
            script_name=script.name,
            environment_id=environment_name,
            environment_name=environment_name,
            status=TaskStatus.RUNNING,
            parameters=parameters,
            started_at=now,
            executed_by=executed_by,
        )

        self.executions[execution_id] = execution

        yield f"[{now.isoformat()}] Starting: {script.name}\n"
        yield f"[{now.isoformat()}] Command: {command}\n"
        yield f"[{now.isoformat()}] Working dir: {script.working_dir}\n"
        yield "-" * 60 + "\n"

        try:
            process = await asyncio.create_subprocess_shell(
                command,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.STDOUT,
                cwd=script.working_dir,
            )

            self.running_processes[execution_id] = process

            #   
            async for line in process.stdout:
                yield line.decode()

            await process.wait()

            execution.exit_code = process.returncode
            execution.status = (
                TaskStatus.SUCCESS if process.returncode == 0 else TaskStatus.FAILED
            )
            execution.finished_at = datetime.utcnow()

            yield "-" * 60 + "\n"
            yield f"[{execution.finished_at.isoformat()}] Finished with exit code: {process.returncode}\n"
            yield f"[{execution.finished_at.isoformat()}] Status: {execution.status.value}\n"

        except Exception as e:
            execution.status = TaskStatus.FAILED
            execution.error_message = str(e)
            execution.finished_at = datetime.utcnow()
            yield f"[ERROR] {str(e)}\n"
        finally:
            if execution_id in self.running_processes:
                del self.running_processes[execution_id]

    def cancel_execution(self, execution_id: str) -> bool:
        """   """
        process = self.running_processes.get(execution_id)
        if process:
            try:
                process.terminate()
                execution = self.executions.get(execution_id)
                if execution:
                    execution.status = TaskStatus.CANCELLED
                    execution.finished_at = datetime.utcnow()
                return True
            except Exception:
                return False
        return False

    def get_execution(self, execution_id: str) -> Optional[TaskExecution]:
        """  """
        return self.executions.get(execution_id)

    def list_executions(
        self,
        script_id: Optional[str] = None,
        environment_id: Optional[str] = None,
        status: Optional[TaskStatus] = None,
        limit: int = 50,
    ) -> list[TaskExecution]:
        """  """
        executions = list(self.executions.values())

        if script_id:
            executions = [e for e in executions if e.script_id == script_id]

        if environment_id:
            executions = [e for e in executions if e.environment_id == environment_id]

        if status:
            executions = [e for e in executions if e.status == status]

        #  
        executions.sort(key=lambda x: x.started_at, reverse=True)

        return executions[:limit]

```

---

## backend/admin-dashboard/docker-compose.yml

```yml
# Admin Dashboard Docker Compose
#     Admin Dashboard .

services:
  admin-api:
    build:
      context: .
      dockerfile: Dockerfile
    image: newsinsight/admin-dashboard:local
    container_name: newsinsight-admin-api
    restart: unless-stopped
    environment:
      - PORT=8889
      - PROJECT_ROOT=/workspace
      - ADMIN_CONFIG_DIR=/app/config
      - ADMIN_SECRET_KEY=${ADMIN_SECRET_KEY:-change-this-secret-key-in-production}
      - CORS_ORIGINS=http://localhost:3001,http://localhost:8889
    ports:
      - "8889:8889"
    volumes:
      #    docker compose   
      - ../../:/workspace:ro
      # Docker   ( )
      - /var/run/docker.sock:/var/run/docker.sock
      #   
      - admin-config:/app/config
    networks:
      - admin-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8889/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  admin-web:
    image: node:20-alpine
    container_name: newsinsight-admin-web
    working_dir: /app
    volumes:
      - ./web:/app
      - admin-web-node-modules:/app/node_modules
    environment:
      - VITE_API_URL=http://admin-api:8889/api/v1/admin
    command: >
      sh -c "npm install && npm run dev -- --host 0.0.0.0"
    ports:
      - "3001:3001"
    depends_on:
      - admin-api
    networks:
      - admin-net

volumes:
  admin-config:
  admin-web-node-modules:

networks:
  admin-net:
    name: newsinsight-admin-net

```

---

## backend/api-gateway-service/bin/main/application.yml

```yml
spring:
  application:
    name: api-gateway
  
  config:
    # Consul   (optional - Consul   )
    import: ${SPRING_CONFIG_IMPORT:optional:consul:}
  
  cloud:
    consul:
      enabled: ${CONSUL_ENABLED:true}
      host: ${CONSUL_HOST:localhost}
      port: ${CONSUL_PORT:8500}
      config:
        enabled: ${CONSUL_CONFIG_ENABLED:true}
        #  Consul KV  
        prefix: config
        default-context: ${spring.application.name}
        format: PROPERTIES
        # Fail-Fast : Consul   
        fail-fast: ${CONSUL_FAIL_FAST:false}
      discovery:
        enabled: ${CONSUL_DISCOVERY_ENABLED:true}
        health-check-path: /actuator/health
        health-check-interval: 10s
        instance-id: ${spring.application.name}:${random.value}
        prefer-ip-address: true
        #    
        fail-fast: false
    inetutils:
      preferred-networks:
        - 172.20

    gateway:
      routes:
        # ===========================================
        # Collector Service Routes
        # COLLECTOR_SERVICE_URL   URL  
        # Consul  : lb://collector-service
        # Consul  : http://collector-service:8081 ()
        # ===========================================
        
        # Collector  -  API (DataController)
        - id: collector-data
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/data/**
          # NOTE: Redis RateLimiter  - Redis   
          # Redis     
          # filters:
          #   - name: RequestRateLimiter
          #     args:
          #       redis-rate-limiter.replenishRate: 100
          #       redis-rate-limiter.burstCapacity: 150

        # Collector  -   (SourceController)
        - id: collector-sources
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/sources/**

        # Collector  -   (CollectionController)
        - id: collector-collections
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/collections/**

        # Articles & Analysis -  API
        - id: articles-root
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/articles

        - id: articles-alias
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/articles/**

        - id: analysis-root
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/analysis

        - id: analysis-alias
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/analysis/**
          metadata:
            response-timeout: 300000

        # Search API -   (SSE )
        # NOTE: SSE  RateLimiter    
        - id: search-api
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/search/**

        # Search History API -   
        - id: search-history
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/search-history/**
          metadata:
            # SSE      (5)
            response-timeout: 300000

        # Search Template API - SmartSearch   
        - id: search-templates
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/search-templates/**

        # Search Jobs API -    (SearchJobController)
        # SSE  : /api/v1/jobs/stream, /api/v1/jobs/{jobId}/stream
        - id: search-jobs
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/jobs,/api/v1/jobs/**
          metadata:
            # SSE      (5)
            response-timeout: 300000

        # AutoCrawl API -    (AutoCrawlController)
        - id: autocrawl
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/autocrawl/**

        # Config API -  
        # NOTE: Gateway  FrontendConfigController  
        #   fallback  (collector-service ConfigController  )
        - id: config-api
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/config/**

        # AI Orchestration - AI   (collector-service AiOrchestrationController)
        - id: collector-ai
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/ai/**

        - id: collector-ml
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/ml/**

        # Dashboard Events - SSE   (collector-service DashboardEventsController)
        - id: collector-events
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/events/**
          metadata:
            # SSE    
            response-timeout: 300000

        # Admin Dashboard API
        # NOTE: Using direct URL fallback if not found in Consul, assuming port 8888 from main.py
        - id: admin-dashboard-api
          uri: ${ADMIN_DASHBOARD_URL:http://localhost:8888}
          predicates:
            - Path=/api/v1/admin/**
          # No StripPrefix needed because the admin API uses /api/v1/admin prefix

        # Autonomous Crawler -   
        - id: autonomous-crawler-api
          uri: ${AUTONOMOUS_CRAWLER_API_URL:${AUTONOMOUS_CRAWLER_URL:http://autonomous-crawler:8030}}
          predicates:
            - Path=/api/v1/crawler/**
          filters:
            # /api/v1/crawler/** -> /**   (3 segments: api, v1, crawler)
            - StripPrefix=3

        # Browser-Use API - AI  
        - id: browser-use-browse
          uri: ${BROWSER_USE_URL:http://localhost:8500}
          predicates:
            - Path=/api/browser-use/browse/**
          filters:
            # /api/browser-use/browse/** -> /browse/**  
            - StripPrefix=2

        - id: browser-use-jobs
          uri: ${BROWSER_USE_URL:http://localhost:8500}
          predicates:
            - Path=/api/browser-use/jobs/**
          filters:
            # /api/browser-use/jobs/** -> /jobs/**  
            - StripPrefix=2

        - id: browser-use-health
          uri: ${BROWSER_USE_URL:http://localhost:8500}
          predicates:
            - Path=/api/browser-use/health
          filters:
            # /api/browser-use/health -> /health  
            - StripPrefix=2

        # Browser-Use WebSocket
        - id: browser-use-websocket
          uri: ${BROWSER_USE_WS_URL:ws://localhost:8500}
          predicates:
            - Path=/api/browser-use/ws/**
          filters:
            # /api/browser-use/ws/** -> /ws/**  
            - StripPrefix=2

        # ML Add-ons -   (Sentiment Analysis)
        - id: ml-addon-sentiment
          uri: ${ML_ADDON_SENTIMENT_URL:http://sentiment-addon:8100}
          predicates:
            - Path=/api/ml-addons/sentiment/**
          filters:
            # /api/ml-addons/sentiment/** -> /**  
            - StripPrefix=3

        # Fact Check Chat -  
        - id: factcheck-chat
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/factcheck-chat/**
          filters:
            - StripPrefix=0

        # ML Add-ons -  (Fact Check)
        - id: ml-addon-factcheck
          uri: ${ML_ADDON_FACTCHECK_URL:http://factcheck-addon:8101}
          predicates:
            - Path=/api/ml-addons/factcheck/**
          filters:
            # /api/ml-addons/factcheck/** -> /**  
            - StripPrefix=3

        # ML Add-ons -   (Bias Analysis)
        - id: ml-addon-bias
          uri: ${ML_ADDON_BIAS_URL:http://bias-addon:8102}
          predicates:
            - Path=/api/ml-addons/bias/**
          filters:
            # /api/ml-addons/bias/** -> /**  
            - StripPrefix=3

        - id: ml-addon-bot-detector
          uri: ${ML_ADDON_BOT_DETECTOR_URL:http://bot-detector:8041}
          predicates:
            - Path=/api/ml-addons/bot-detector/**
          filters:
            - StripPrefix=3

        # ML Trainer Service -   
        - id: ml-trainer
          uri: ${ML_TRAINER_URL:http://ml-trainer:8090}
          predicates:
            - Path=/api/ml-trainer/**
          filters:
            # /api/ml-trainer/** -> /**  
            - StripPrefix=2
          metadata:
            #        
            response-timeout: 600000

        # ML Trainer SSE Stream -    
        - id: ml-trainer-stream
          uri: ${ML_TRAINER_URL:http://ml-trainer:8090}
          predicates:
            - Path=/api/ml-trainer/jobs/*/stream
          filters:
            - StripPrefix=2
          metadata:
            # SSE      (30)
            response-timeout: 1800000

        # Health Check Rewrite -  /api/actuator/health   
        - id: gateway-health
          uri: http://localhost:${PORT:8000}
          predicates:
            - Path=/api/actuator/health,/api/actuator/health/**
          filters:
            # /api/actuator/health -> /actuator/health  
            - RewritePath=/api/actuator/(?<segment>.*), /actuator/${segment}

      #  CORS 
      globalcors:
        cors-configurations:
          '[/**]':
            allowedOriginPatterns: "*"
            allowedMethods:
              - GET
              - POST
              - PUT
              - DELETE
              - PATCH
              - OPTIONS
            allowedHeaders: "*"
            allowCredentials: true
            maxAge: 3600
  
  # Redis  (Rate Limiting   )
  data:
    redis:
      host: ${REDIS_HOST:redis}
      port: ${REDIS_PORT:6379}
      password: ${REDIS_PASSWORD:}

server:
  port: ${PORT:8000}
  # Netty  - SSE/WebSocket    
  netty:
    #    (5)
    idle-timeout: 300000

# Spring Cloud Gateway HTTP Client 
spring.cloud.gateway.httpclient:
  connect-timeout: 10000
  response-timeout: 300000
  pool:
    max-idle-time: 300000

management:
  endpoints:
    web:
      exposure:
        include: health,info,env,metrics,prometheus
  endpoint:
    health:
      show-details: always

logging:
  level:
    root: INFO
    org.springframework.cloud.gateway: DEBUG
    org.springframework.cloud.gateway.handler.RoutePredicateHandlerMapping: TRACE
    org.springframework.security: DEBUG
    com.newsinsight.gateway: DEBUG

```

---

## backend/api-gateway-service/build.gradle.kts

```kts
// API Gateway   

plugins {
    java
    id("org.springframework.boot")
    id("io.spring.dependency-management")
}

dependencies {
    // Spring Cloud Gateway
    implementation("org.springframework.cloud:spring-cloud-starter-gateway")
    
    // Security (JWT )
    implementation("org.springframework.boot:spring-boot-starter-security")
    implementation("io.jsonwebtoken:jjwt-api:0.12.3")
    runtimeOnly("io.jsonwebtoken:jjwt-impl:0.12.3")
    runtimeOnly("io.jsonwebtoken:jjwt-jackson:0.12.3")
    
    // Redis for Rate Limiting
    implementation("org.springframework.boot:spring-boot-starter-data-redis-reactive")
    
    // WebFlux (Gateway Reactive )
    implementation("org.springframework.boot:spring-boot-starter-webflux")
    
    // Test
    testImplementation("org.springframework.security:spring-security-test")
    testImplementation("io.projectreactor:reactor-test")
}

tasks.named<org.springframework.boot.gradle.tasks.bundling.BootJar>("bootJar") {
    archiveBaseName.set("api-gateway")
    archiveVersion.set("1.0.0")
}

```

---

## backend/api-gateway-service/src/main/java/com/newsinsight/gateway/GatewayApplication.java

```java
package com.newsinsight.gateway;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.client.discovery.EnableDiscoveryClient;

/**
 * NewsInsight API Gateway Application
 * 
 * Spring Cloud Gateway  API Gateway 
 * - JWT /
 * - RBAC (Role-Based Access Control)
 * - Rate Limiting (Redis )
 * - Service Discovery (Consul)
 * - Dynamic Configuration (Consul KV)
 */
@SpringBootApplication
@EnableDiscoveryClient
public class GatewayApplication {

    public static void main(String[] args) {
        SpringApplication.run(GatewayApplication.class, args);
    }
}

```

---

## backend/api-gateway-service/src/main/java/com/newsinsight/gateway/config/SecurityConfig.java

```java
package com.newsinsight.gateway.config;

import org.springframework.beans.factory.annotation.Value;
import org.springframework.cloud.gateway.filter.ratelimit.KeyResolver;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.http.HttpMethod;
import org.springframework.security.config.annotation.web.reactive.EnableWebFluxSecurity;
import org.springframework.security.config.web.server.ServerHttpSecurity;
import org.springframework.security.web.server.SecurityWebFilterChain;
import org.springframework.web.cors.CorsConfiguration;
import org.springframework.web.cors.reactive.CorsConfigurationSource;
import org.springframework.web.cors.reactive.UrlBasedCorsConfigurationSource;
import reactor.core.publisher.Mono;

import java.util.Arrays;
import java.util.List;

/**
 * API Gateway Security Configuration
 * 
 *   :
 * - Public: ,   (/api/v1/auth/**)
 * - Protected:  API  (JWT  downstream  )
 * 
 * NOTE: Gateway JWT  downstream   ,
 *  /   SecurityConfig .
 * Gateway      .
 */
@Configuration
@EnableWebFluxSecurity
public class SecurityConfig {

    @Value("${security.gateway.enabled:true}")
    private boolean securityEnabled;

    @Bean
    public SecurityWebFilterChain securityWebFilterChain(ServerHttpSecurity http) {
        if (!securityEnabled) {
            //    -    !
            return http
                    .csrf(ServerHttpSecurity.CsrfSpec::disable)
                    .cors(cors -> cors.configurationSource(corsConfigurationSource()))
                    .httpBasic(ServerHttpSecurity.HttpBasicSpec::disable)
                    .formLogin(ServerHttpSecurity.FormLoginSpec::disable)
                    .authorizeExchange(exchange -> exchange.anyExchange().permitAll())
                    .build();
        }

        return http
                .csrf(ServerHttpSecurity.CsrfSpec::disable)
                .cors(cors -> cors.configurationSource(corsConfigurationSource()))
                .httpBasic(ServerHttpSecurity.HttpBasicSpec::disable)
                .formLogin(ServerHttpSecurity.FormLoginSpec::disable)
                .authorizeExchange(exchange -> exchange
                        // ========================================
                        // Public Endpoints ( )
                        // ========================================
                        // Health checks & Actuator
                        .pathMatchers("/actuator/**").permitAll()
                        .pathMatchers("/api/actuator/**").permitAll()
                        
                        // Authentication endpoints (login, register, token)
                        .pathMatchers("/api/v1/auth/login").permitAll()
                        .pathMatchers("/api/v1/auth/register").permitAll()
                        .pathMatchers("/api/v1/auth/token").permitAll()
                        .pathMatchers("/api/v1/auth/send-verification").permitAll()
                        .pathMatchers("/api/v1/auth/verify-email").permitAll()
                        .pathMatchers("/api/v1/auth/resend-verification").permitAll()
                        .pathMatchers("/api/v1/auth/check-username/**").permitAll()
                        .pathMatchers("/api/v1/auth/check-email/**").permitAll()
                        
                        // CORS preflight
                        .pathMatchers(HttpMethod.OPTIONS, "/**").permitAll()
                        
                        // ========================================
                        // Protected Endpoints (  - downstream )
                        // Gateway   ,     
                        // ========================================
                        //    , downstream   
                        .anyExchange().permitAll()
                )
                .build();
    }

    @Bean
    public CorsConfigurationSource corsConfigurationSource() {
        CorsConfiguration configuration = new CorsConfiguration();
        configuration.setAllowedOriginPatterns(List.of("*"));
        configuration.setAllowedMethods(Arrays.asList("GET", "POST", "PUT", "DELETE", "PATCH", "OPTIONS", "HEAD"));
        configuration.setAllowedHeaders(List.of("*"));
        configuration.setAllowCredentials(true);
        configuration.setMaxAge(3600L);
        
        UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource();
        source.registerCorsConfiguration("/**", configuration);
        return source;
    }

    /**
     * Rate Limiter KeyResolver
     * IP   Rate Limit 
     */
    @Bean
    public KeyResolver ipKeyResolver() {
        return exchange -> {
            var remoteAddress = exchange.getRequest().getRemoteAddress();
            String ip = remoteAddress != null
                    ? remoteAddress.getAddress().getHostAddress()
                    : "unknown";
            return Mono.just(ip);
        };
    }
}

```

---

## backend/api-gateway-service/src/main/java/com/newsinsight/gateway/controller/FrontendConfigController.java

```java
package com.newsinsight.gateway.controller;

import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Mono;

import java.util.HashMap;
import java.util.Map;
import java.util.Base64;

@RestController
@RequestMapping("/api/v1/config")
public class FrontendConfigController {

    @Value("${FRONTEND_API_BASE_URL:${API_GATEWAY_FRONTEND_API_BASE_URL:http://localhost:8112}}")
    private String frontendApiBaseUrl;

    @Value("${spring.cloud.consul.host:consul}")
    private String consulHost;

    @Value("${spring.cloud.consul.port:8500}")
    private int consulPort;

    private final WebClient webClient;

    public FrontendConfigController(WebClient.Builder webClientBuilder) {
        this.webClient = webClientBuilder.build();
    }

    @CrossOrigin(origins = "*")
    @GetMapping("/frontend")
    public ResponseEntity<Map<String, String>> getFrontendConfig() {
        Map<String, String> body = new HashMap<>();
        body.put("apiBaseUrl", frontendApiBaseUrl);
        return ResponseEntity.ok(body);
    }

    /**
     * Save AI/LLM settings to Consul KV store
     * PUT /api/v1/config/ai-settings
     */
    @CrossOrigin(origins = "*")
    @PutMapping("/ai-settings")
    public Mono<ResponseEntity<Map<String, Object>>> saveAISettings(@RequestBody Map<String, String> settings) {
        String consulUrl = "http://" + consulHost + ":" + consulPort;
        
        // Save each setting to Consul KV under config/autonomous-crawler/ prefix
        return Mono.when(
            settings.entrySet().stream()
                .map(entry -> {
                    String key = "config/autonomous-crawler/" + entry.getKey();
                    Object value = entry.getValue();
                    // Consul expects base64 encoded value for PUT
                    return webClient.put()
                        .uri(consulUrl + "/v1/kv/" + key)
                        .bodyValue(value.toString().isEmpty() ? "" : value)
                        .retrieve()
                        .bodyToMono(Boolean.class)
                        .onErrorReturn(false);
                })
                .toArray(Mono[]::new)
        ).then(Mono.fromCallable(() -> {
            Map<String, Object> response = new HashMap<>();
            response.put("success", true);
            response.put("message", "AI settings saved to Consul");
            response.put("keysCount", settings.size());
            return ResponseEntity.ok(response);
        }));
    }

    /**
     * Get AI/LLM settings from Consul KV store
     * GET /api/v1/config/ai-settings
     */
    @CrossOrigin(origins = "*")
    @GetMapping("/ai-settings")
    public Mono<ResponseEntity<Map<String, String>>> getAISettings() {
        String consulUrl = "http://" + consulHost + ":" + consulPort;
        
        return webClient.get()
            .uri(consulUrl + "/v1/kv/config/autonomous-crawler/?recurse=true")
            .retrieve()
            .bodyToMono(Object[].class)
            .map(entries -> {
                Map<String, String> settings = new HashMap<>();
                if (entries != null) {
                    for (Object entry : entries) {
                        if (entry instanceof Map) {
                            @SuppressWarnings("unchecked")
                            Map<String, Object> kvEntry = (Map<String, Object>) entry;
                            String fullKey = (String) kvEntry.get("Key");
                            String encodedValue = (String) kvEntry.get("Value");
                            
                            if (fullKey != null && encodedValue != null) {
                                // Remove prefix and decode value
                                String key = fullKey.replace("config/autonomous-crawler/", "");
                                String value = new String(Base64.getDecoder().decode(encodedValue));
                                settings.put(key, value);
                            }
                        }
                    }
                }
                return ResponseEntity.ok(settings);
            })
            .onErrorReturn(ResponseEntity.ok(new HashMap<>()));
    }
}

```

---

## backend/api-gateway-service/src/main/java/com/newsinsight/gateway/filter/JwtAuthenticationFilter.java

```java
package com.newsinsight.gateway.filter;

import io.jsonwebtoken.Claims;
import io.jsonwebtoken.Jwts;
import io.jsonwebtoken.security.Keys;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.cloud.gateway.filter.GatewayFilterChain;
import org.springframework.cloud.gateway.filter.GlobalFilter;
import org.springframework.core.Ordered;
import org.springframework.http.HttpCookie;
import org.springframework.http.HttpStatus;
import org.springframework.http.server.reactive.ServerHttpRequest;
import org.springframework.stereotype.Component;
import org.springframework.util.MultiValueMap;
import org.springframework.web.server.ServerWebExchange;
import reactor.core.publisher.Mono;

import javax.crypto.SecretKey;
import java.nio.charset.StandardCharsets;
import java.util.List;

/**
 * JWT  
 * 
 * Python FastAPI auth_middleware   
 * - Authorization  Bearer  
 * - Cookie access_token  (SSE/EventSource )
 * -   token  (fallback)
 * - JWT   (,   )
 * -       
 */
@Slf4j
@Component
public class JwtAuthenticationFilter implements GlobalFilter, Ordered {
    
    //    
    private static final List<String> PUBLIC_PATHS = List.of(
        "/health",
        "/actuator",
        "/api/v1/auth",          // Public Auth API (register, login, check-username, check-email)
        "/api/v1/articles",
        "/api/v1/analysis",
        "/api/v1/ai",
        "/api/v1/config",
        "/api/v1/sources",
        "/api/v1/collections",
        "/api/v1/data",
        "/api/v1/search",
        "/api/v1/events",        // SSE   (EventSource   )
        "/api/v1/search-history",
        "/api/v1/search-templates",
        "/api/v1/jobs",          // Search Jobs API (SSE  )
        "/api/v1/projects",      // Projects API (  )
        "/api/v1/ai",
        "/api/v1/ml",
        "/api/v1/llm-providers", // LLM Provider Settings (  )
        "/api/v1/admin",         // Admin Dashboard (  )
        "/api/v1/crawler",       // Autonomous Crawler API
        "/api/v1/autocrawl",     // AutoCrawl API (  )
        "/api/v1/factcheck-chat", // Fact Check Chat API (  )
        "/api/v1/reports",       // PDF Report Export (   )
        "/api/v1/ai",
        "/api/v1/*",
        "/api/browser-use",      // Browser-Use API (gateway path)
        "/api/ml-addons",        // ML Add-ons API (sentiment, factcheck, bias)
        "/browse",               // Browser-Use API (direct path - legacy)
        "/jobs",                 // Browser-Use Jobs (direct path - legacy)
        "/ws"                    // WebSocket (direct path - legacy)
    );
    
    @Value("${JWT_SECRET_KEY:default-secret-key-please-change-in-consul}")
    private String jwtSecretKey;
    
    @Value("${JWT_ALGORITHM:HS256}")
    private String jwtAlgorithm;
    
    @Override
    public Mono<Void> filter(ServerWebExchange exchange, GatewayFilterChain chain) {
        String path = exchange.getRequest().getPath().value();
        
        //        
        if (PUBLIC_PATHS.stream().anyMatch(path::startsWith)) {
            log.debug("Public path: {}, adding anonymous user headers", path);
            return handleAnonymousUser(exchange, chain);
        }
        
        //   (: Authorization  > Cookie > Query Parameter)
        String token = extractToken(exchange);
        
        if (token == null) {
            log.warn("No valid token found for path: {}", path);
            exchange.getResponse().setStatusCode(HttpStatus.UNAUTHORIZED);
            return exchange.getResponse().setComplete();
        }
        
        try {
            // JWT    
            SecretKey key = Keys.hmacShaKeyFor(jwtSecretKey.getBytes(StandardCharsets.UTF_8));
            Claims claims = Jwts.parser()
                    .verifyWith(key)
                    .build()
                    .parseSignedClaims(token)
                    .getPayload();
            
            //     (  )
            ServerHttpRequest mutatedRequest = exchange.getRequest().mutate()
                    .header("X-User-Id", claims.getSubject())
                    .header("X-User-Role", claims.get("role", String.class))
                    .header("X-Username", claims.get("username", String.class))
                    .build();
            
            log.debug("Authenticated user: {} with role: {}", 
                    claims.get("username", String.class), 
                    claims.get("role", String.class));
            
            return chain.filter(exchange.mutate().request(mutatedRequest).build());
            
        } catch (Exception e) {
            log.error("JWT authentication failed: {}", e.getMessage());
            exchange.getResponse().setStatusCode(HttpStatus.UNAUTHORIZED);
            return exchange.getResponse().setComplete();
        }
    }
    
    /**
     * Handle anonymous user by generating unique user ID based on session
     * This prevents data leakage between different anonymous users
     */
    private Mono<Void> handleAnonymousUser(ServerWebExchange exchange, GatewayFilterChain chain) {
        // Extract session ID from headers (sent by frontend)
        String sessionId = exchange.getRequest().getHeaders().getFirst("X-Session-Id");
        String deviceId = exchange.getRequest().getHeaders().getFirst("X-Device-Id");
        
        // Generate unique anonymous user ID based on session
        String anonymousUserId;
        if (sessionId != null && !sessionId.isBlank()) {
            anonymousUserId = "user_anon_" + sessionId;
        } else {
            // Fallback: use device ID or generate random ID
            if (deviceId != null && !deviceId.isBlank()) {
                anonymousUserId = "user_anon_" + deviceId;
            } else {
                // Last resort: generate random ID (not ideal, but prevents null)
                anonymousUserId = "user_anon_" + System.currentTimeMillis();
            }
            log.warn("No session ID provided, using fallback anonymous user ID: {}", anonymousUserId);
        }
        
        // Add anonymous user headers for downstream services
        ServerHttpRequest mutatedRequest = exchange.getRequest().mutate()
                .header("X-User-Id", anonymousUserId)
                .header("X-User-Role", "anonymous")
                .header("X-Session-Id", sessionId != null ? sessionId : "")
                .header("X-Device-Id", deviceId != null ? deviceId : "")
                .build();
        
        log.debug("Anonymous user: userId={}, sessionId={}", anonymousUserId, sessionId);
        
        return chain.filter(exchange.mutate().request(mutatedRequest).build());
    }
    
    /**
     *   JWT  
     * :
     * 1. Authorization  (Bearer token)
     * 2. Cookie (access_token)
     * 3. Query Parameter (token)
     */
    private String extractToken(ServerWebExchange exchange) {
        // 1. Authorization  
        String authHeader = exchange.getRequest().getHeaders().getFirst("Authorization");
        if (authHeader != null && authHeader.startsWith("Bearer ")) {
            log.debug("Token extracted from Authorization header");
            return authHeader.substring(7);
        }
        
        // 2. Cookie  (SSE/EventSource )
        MultiValueMap<String, HttpCookie> cookies = exchange.getRequest().getCookies();
        HttpCookie accessTokenCookie = cookies.getFirst("access_token");
        if (accessTokenCookie != null && !accessTokenCookie.getValue().isEmpty()) {
            log.debug("Token extracted from access_token cookie");
            return accessTokenCookie.getValue();
        }
        
        // 3. Query Parameter  (fallback)
        String queryToken = exchange.getRequest().getQueryParams().getFirst("token");
        if (queryToken != null && !queryToken.isEmpty()) {
            log.debug("Token extracted from query parameter");
            return queryToken;
        }
        
        return null;
    }
    
    @Override
    public int getOrder() {
        return -100; //   ( )
    }
}

```

---

## backend/api-gateway-service/src/main/java/com/newsinsight/gateway/filter/RbacFilter.java

```java
package com.newsinsight.gateway.filter;

import lombok.extern.slf4j.Slf4j;
import org.springframework.cloud.gateway.filter.GatewayFilterChain;
import org.springframework.cloud.gateway.filter.GlobalFilter;
import org.springframework.core.Ordered;
import org.springframework.http.HttpMethod;
import org.springframework.http.HttpStatus;
import org.springframework.stereotype.Component;
import org.springframework.web.server.ServerWebExchange;
import reactor.core.publisher.Mono;

import java.util.List;
import java.util.Map;

/**
 * RBAC (Role-Based Access Control) 
 * 
 * Python FastAPI rbac_middleware   
 * - HTTP     
 * -     
 */
@Slf4j
@Component
public class RbacFilter implements GlobalFilter, Ordered {
    
    //    (Python ROLE_PERMISSIONS )
    private static final Map<String, List<String>> ROLE_PERMISSIONS = Map.of(
        "admin", List.of("READ", "WRITE", "DELETE", "ADMIN"),
        "analyst", List.of("READ", "WRITE"),
        "viewer", List.of("READ"),
        "system", List.of("READ", "WRITE", "DELETE"),
        "anonymous", List.of("READ", "WRITE"),  //   -    
        "user", List.of("READ", "WRITE")        //   -    
    );
    
    // HTTP   
    private static final Map<HttpMethod, String> METHOD_PERMISSIONS = Map.of(
        HttpMethod.GET, "READ",
        HttpMethod.POST, "WRITE",
        HttpMethod.PUT, "WRITE",
        HttpMethod.PATCH, "WRITE",
        HttpMethod.DELETE, "DELETE"
    );
    
    @Override
    public Mono<Void> filter(ServerWebExchange exchange, GatewayFilterChain chain) {
        String userRole = exchange.getRequest().getHeaders().getFirst("X-User-Role");
        
        if (userRole == null) {
            //    (public endpoint)
            log.debug("No user role found, allowing request to proceed");
            return chain.filter(exchange);
        }
        
        HttpMethod method = exchange.getRequest().getMethod();
        String requiredPermission = METHOD_PERMISSIONS.get(method);
        List<String> userPermissions = ROLE_PERMISSIONS.getOrDefault(userRole, List.of());
        
        if (requiredPermission != null && !userPermissions.contains(requiredPermission)) {
            log.warn("Access denied for role: {} on method: {}. Required permission: {}", 
                    userRole, method, requiredPermission);
            exchange.getResponse().setStatusCode(HttpStatus.FORBIDDEN);
            return exchange.getResponse().setComplete();
        }
        
        log.debug("Access granted for role: {} on method: {}", userRole, method);
        return chain.filter(exchange);
    }
    
    @Override
    public int getOrder() {
        return -90; // JWT   
    }
}

```

---

## backend/api-gateway-service/src/main/resources/application.yml

```yml
spring:
  application:
    name: api-gateway
  
  config:
    # Consul   (optional - Consul   )
    import: ${SPRING_CONFIG_IMPORT:optional:consul:}
  
  cloud:
    consul:
      enabled: ${CONSUL_ENABLED:true}
      host: ${CONSUL_HOST:localhost}
      port: ${CONSUL_PORT:8500}
      config:
        enabled: ${CONSUL_CONFIG_ENABLED:true}
        #  Consul KV  
        prefix: config
        default-context: ${spring.application.name}
        format: PROPERTIES
        # Fail-Fast : Consul   
        fail-fast: ${CONSUL_FAIL_FAST:false}
      discovery:
        enabled: ${CONSUL_DISCOVERY_ENABLED:true}
        health-check-path: /actuator/health
        health-check-interval: 10s
        instance-id: ${spring.application.name}:${random.value}
        prefer-ip-address: true
        #    
        fail-fast: false
    inetutils:
      preferred-networks:
        - 172.20

    gateway:
      routes:
        # ===========================================
        # Collector Service Routes
        # COLLECTOR_SERVICE_URL   URL  
        # Consul  : lb://collector-service
        # Consul  : http://collector-service:8081 ()
        # ===========================================
        
        # Collector  -  API (DataController)
        - id: collector-data
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/data/**
          # NOTE: Redis RateLimiter  - Redis   
          # Redis     
          # filters:
          #   - name: RequestRateLimiter
          #     args:
          #       redis-rate-limiter.replenishRate: 100
          #       redis-rate-limiter.burstCapacity: 150

        # Collector  -   (SourceController)
        - id: collector-sources
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/sources/**

        # Collector  -   (CollectionController)
        - id: collector-collections
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/collections/**

        # Articles & Analysis -  API
        - id: articles-root
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/articles

        - id: articles-alias
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/articles/**

        - id: analysis-root
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/analysis

        - id: analysis-alias
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/analysis/**
          metadata:
            response-timeout: 300000

        # Search API -   (SSE )
        # NOTE: SSE  RateLimiter    
        - id: search-api
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/search/**

        # Search History API -   
        - id: search-history
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/search-history/**
          metadata:
            # SSE      (5)
            response-timeout: 300000

        # Search Template API - SmartSearch   
        - id: search-templates
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/search-templates/**

        # Reports API - PDF     (ReportController)
        - id: reports
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/reports/**
          metadata:
            # PDF        (3)
            response-timeout: 180000

        # Search Jobs API -    (SearchJobController)
        # SSE  : /api/v1/jobs/stream, /api/v1/jobs/{jobId}/stream
        - id: search-jobs
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/jobs,/api/v1/jobs/**
          metadata:
            # SSE      (5)
            response-timeout: 300000

        # AutoCrawl API -    (AutoCrawlController)
        - id: autocrawl
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/autocrawl/**

        # Projects API -   (ProjectController)
        - id: projects
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/projects/**

        # Workspace Files API -  / (WorkspaceController)
        - id: workspace-files
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/workspace/**
          metadata:
            #  /    (5)
            response-timeout: 300000

        # Config API -  
        # NOTE: Gateway  FrontendConfigController  
        #   fallback  (collector-service ConfigController  )
        - id: config-api
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/config/**

        # LLM Provider Settings - LLM   (collector-service LlmProviderSettingsController)
        - id: llm-providers
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/llm-providers/**

        # AI Orchestration - AI   (collector-service AiOrchestrationController)
        - id: collector-ai
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/ai/**

        - id: collector-ml
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/ml/**

        # Dashboard Events - SSE   (collector-service DashboardEventsController)
        - id: collector-events
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/events/**
          metadata:
            # SSE    
            response-timeout: 300000

        # Admin Dashboard API
        # NOTE: Using direct URL fallback if not found in Consul, assuming port 8888 from main.py
        - id: admin-dashboard-api
          uri: ${ADMIN_DASHBOARD_URL:http://localhost:8888}
          predicates:
            - Path=/api/v1/admin/**
          # No StripPrefix needed because the admin API uses /api/v1/admin prefix

        # Public Auth API - ,  (Admin Dashboard )
        # /api/v1/auth/register, /api/v1/auth/login, /api/v1/auth/me 
        - id: public-auth
          uri: ${ADMIN_DASHBOARD_URL:http://localhost:8888}
          predicates:
            - Path=/api/v1/auth/**
          # No StripPrefix needed because the admin API uses /api/v1/auth prefix

        # Autonomous Crawler -   
        - id: autonomous-crawler-api
          uri: ${AUTONOMOUS_CRAWLER_API_URL:${AUTONOMOUS_CRAWLER_URL:http://autonomous-crawler:8030}}
          predicates:
            - Path=/api/v1/crawler/**
          filters:
            # /api/v1/crawler/** -> /**   (3 segments: api, v1, crawler)
            - StripPrefix=3

        # Browser-Use API - AI  
        - id: browser-use-browse
          uri: ${BROWSER_USE_URL:http://localhost:8500}
          predicates:
            - Path=/api/browser-use/browse/**
          filters:
            # /api/browser-use/browse/** -> /browse/**  
            - StripPrefix=2

        - id: browser-use-jobs
          uri: ${BROWSER_USE_URL:http://localhost:8500}
          predicates:
            - Path=/api/browser-use/jobs/**
          filters:
            # /api/browser-use/jobs/** -> /jobs/**  
            - StripPrefix=2

        - id: browser-use-health
          uri: ${BROWSER_USE_URL:http://localhost:8500}
          predicates:
            - Path=/api/browser-use/health
          filters:
            # /api/browser-use/health -> /health  
            - StripPrefix=2

        # Browser-Use WebSocket
        - id: browser-use-websocket
          uri: ${BROWSER_USE_WS_URL:ws://localhost:8500}
          predicates:
            - Path=/api/browser-use/ws/**
          filters:
            # /api/browser-use/ws/** -> /ws/**  
            - StripPrefix=2

        # ML Add-ons -   (Sentiment Analysis)
        - id: ml-addon-sentiment
          uri: ${ML_ADDON_SENTIMENT_URL:http://sentiment-addon:8100}
          predicates:
            - Path=/api/ml-addons/sentiment/**
          filters:
            # /api/ml-addons/sentiment/** -> /**  
            - StripPrefix=3

        # Fact Check Chat -  
        - id: factcheck-chat
          uri: ${COLLECTOR_SERVICE_URL:http://collector-service:8081}
          predicates:
            - Path=/api/v1/factcheck-chat/**
          filters:
            - StripPrefix=0

        # ML Add-ons -  (Fact Check)
        - id: ml-addon-factcheck
          uri: ${ML_ADDON_FACTCHECK_URL:http://factcheck-addon:8101}
          predicates:
            - Path=/api/ml-addons/factcheck/**
          filters:
            # /api/ml-addons/factcheck/** -> /**  
            - StripPrefix=3

        # ML Add-ons -   (Bias Analysis)
        - id: ml-addon-bias
          uri: ${ML_ADDON_BIAS_URL:http://bias-addon:8102}
          predicates:
            - Path=/api/ml-addons/bias/**
          filters:
            # /api/ml-addons/bias/** -> /**  
            - StripPrefix=3

        - id: ml-addon-bot-detector
          uri: ${ML_ADDON_BOT_DETECTOR_URL:http://bot-detector:8041}
          predicates:
            - Path=/api/ml-addons/bot-detector/**
          filters:
            - StripPrefix=3

        # ML Trainer Service -   
        - id: ml-trainer
          uri: ${ML_TRAINER_URL:http://ml-trainer:8090}
          predicates:
            - Path=/api/ml-trainer/**
          filters:
            # /api/ml-trainer/** -> /**  
            - StripPrefix=2
          metadata:
            #        
            response-timeout: 600000

        # ML Trainer SSE Stream -    
        - id: ml-trainer-stream
          uri: ${ML_TRAINER_URL:http://ml-trainer:8090}
          predicates:
            - Path=/api/ml-trainer/jobs/*/stream
          filters:
            - StripPrefix=2
          metadata:
            # SSE      (30)
            response-timeout: 1800000

        # Health Check Rewrite -  /api/actuator/health   
        - id: gateway-health
          uri: http://localhost:${PORT:8000}
          predicates:
            - Path=/api/actuator/health,/api/actuator/health/**
          filters:
            # /api/actuator/health -> /actuator/health  
            - RewritePath=/api/actuator/(?<segment>.*), /actuator/${segment}

        # General Health Check - /api/health  
        - id: api-health
          uri: http://localhost:${PORT:8000}
          predicates:
            - Path=/api/health
          filters:
            - RewritePath=/api/health, /actuator/health

      # NOTE: CORS is handled by SecurityConfig.java - DO NOT enable globalcors
      # Having both causes duplicate Access-Control-Allow-Origin headers
      # globalcors:
      #   cors-configurations:
      #     '[/**]':
      #       allowedOriginPatterns: "*"
      #       allowedMethods:
      #         - GET
      #         - POST
      #         - PUT
      #         - DELETE
      #         - PATCH
      #         - OPTIONS
      #       allowedHeaders: "*"
      #       allowCredentials: true
      #       maxAge: 3600
  
  # Redis  (Rate Limiting   )
  data:
    redis:
      host: ${REDIS_HOST:redis}
      port: ${REDIS_PORT:6379}
      password: ${REDIS_PASSWORD:}

server:
  port: ${PORT:8000}
  # Netty  - SSE/WebSocket    
  netty:
    #    (5)
    idle-timeout: 300000

# Spring Cloud Gateway HTTP Client 
spring.cloud.gateway.httpclient:
  connect-timeout: 10000
  response-timeout: 300000
  pool:
    max-idle-time: 300000

management:
  endpoints:
    web:
      exposure:
        include: health,info,env,metrics,prometheus
  endpoint:
    health:
      show-details: always

logging:
  level:
    root: INFO
    org.springframework.cloud.gateway: DEBUG
    org.springframework.cloud.gateway.handler.RoutePredicateHandlerMapping: TRACE
    org.springframework.security: DEBUG
    com.newsinsight.gateway: DEBUG

```

---

## backend/autonomous-crawler-service/crawl-worker/main.py

```py
import asyncio
import os
import sys
import time
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
import uuid
import logging

log = logging.getLogger(__name__)

# Add shared module to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))
try:
    from shared.prometheus_metrics import (
        setup_metrics,
        track_request_time,
        track_operation,
        track_error,
        track_item_processed,
        ServiceMetrics,
    )

    METRICS_AVAILABLE = True
except ImportError:
    METRICS_AVAILABLE = False

try:
    from shared.proxy_client import ProxyRotationClient, ProxyInfo

    PROXY_CLIENT_AVAILABLE = True
except ImportError:
    PROXY_CLIENT_AVAILABLE = False
    ProxyRotationClient = None
    ProxyInfo = None

try:
    from crawl4ai import AsyncWebCrawler  # type: ignore
except Exception:
    AsyncWebCrawler = None  # fallback for environments without crawl4ai


def detect_captcha_type(content: str) -> Optional[str]:
    text = content.lower()

    cloudflare_markers = [
        "cf-turnstile",
        "turnstile",
        "challenges.cloudflare.com",
        "challenge-running",
        "cf-browser-verification",
        "checking your browser",
        "just a moment",
    ]
    if any(m in text for m in cloudflare_markers):
        return "cloudflare"

    if "recaptcha" in text or "g-recaptcha" in text:
        return "recaptcha"

    if "hcaptcha" in text or "h-captcha" in text:
        return "hcaptcha"

    if "captcha" in text:
        return "captcha"

    return None

app = FastAPI(title="Crawl4AI Worker", version="0.3.0")

# Setup Prometheus metrics
SERVICE_NAME = "crawl-worker"
if METRICS_AVAILABLE:
    setup_metrics(app, SERVICE_NAME, version="0.3.0")
    service_metrics = ServiceMetrics(SERVICE_NAME)
    # Create service-specific metrics
    crawl_requests = service_metrics.create_counter(
        "crawl_requests_total",
        "Total crawl requests",
        ["status", "js_render", "proxy_used"],
    )
    crawl_latency = service_metrics.create_histogram(
        "crawl_latency_seconds",
        "Crawl request latency",
        ["status"],
        buckets=(0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0),
    )
    batch_crawls = service_metrics.create_counter(
        "batch_crawls_total", "Total batch crawl operations", ["status"]
    )
    concurrent_crawls_gauge = service_metrics.create_gauge(
        "concurrent_crawls", "Number of concurrent crawl operations"
    )
    proxy_usage = service_metrics.create_counter(
        "proxy_usage_total", "Proxy usage statistics", ["proxy_id", "status"]
    )
else:
    service_metrics = None

# In-memory storage for batch results
batch_results: Dict[str, Dict[str, Any]] = {}

# Semaphore for concurrent crawl limit
MAX_CONCURRENT_CRAWLS = int(os.environ.get("MAX_CONCURRENT_CRAWLS", "5"))
crawl_semaphore = asyncio.Semaphore(MAX_CONCURRENT_CRAWLS)

# Proxy rotation configuration
USE_PROXY_ROTATION = os.environ.get("USE_PROXY_ROTATION", "true").lower() == "true"
PROXY_ROTATION_URL = os.environ.get("PROXY_ROTATION_URL", "http://ip-rotation:8050")

# Initialize proxy client
proxy_client: Optional["ProxyRotationClient"] = None  # type: ignore
if PROXY_CLIENT_AVAILABLE and USE_PROXY_ROTATION:
    proxy_client = ProxyRotationClient(
        base_url=PROXY_ROTATION_URL,
        timeout=5.0,
        enabled=True,
    )
    log.info(f"Proxy rotation enabled, connecting to {PROXY_ROTATION_URL}")


class CrawlRequest(BaseModel):
    url: str
    js_render: bool = False
    wait_for: Optional[str] = None  # CSS selector to wait for (optional)
    use_proxy: bool = True  # Whether to use proxy rotation


class CrawlResponse(BaseModel):
    url: str
    markdown: Optional[str] = None
    html: Optional[str] = None
    status: str
    error: Optional[str] = None
    proxy_used: Optional[str] = None
    latency_ms: Optional[int] = None


class BatchCrawlRequest(BaseModel):
    """  """

    urls: List[str] = Field(
        ..., min_length=1, max_length=50, description=" URL  ( 50)"
    )
    js_render: bool = Field(default=False, description="JavaScript  ")
    wait_for: Optional[str] = Field(default=None, description=" CSS ")
    extract_links: bool = Field(default=False, description="  ")
    use_proxy: bool = Field(default=True, description="   ")


class BatchCrawlResult(BaseModel):
    """  """

    batch_id: str
    total: int
    completed: int
    failed: int
    status: str  # "processing", "completed", "partial"
    results: List[CrawlResponse]


@app.get("/health")
@app.head("/health")
async def health():
    proxy_healthy = False
    if proxy_client:
        proxy_healthy = await proxy_client.health_check()

    return {
        "status": "ok",
        "service": "crawl-worker",
        "version": "0.3.0",
        "crawl4ai_available": AsyncWebCrawler is not None,
        "max_concurrent": MAX_CONCURRENT_CRAWLS,
        "proxy_rotation_enabled": USE_PROXY_ROTATION,
        "proxy_service_healthy": proxy_healthy,
    }


async def get_proxy_for_crawl(
    use_proxy: bool = True,
) -> tuple[Optional[str], Optional[str]]:
    """
    Get a proxy URL for crawling.

    Returns:
        Tuple of (proxy_url, proxy_id) or (None, None) if no proxy is used
    """
    if not use_proxy or not proxy_client:
        return None, None

    try:
        proxy_info = await proxy_client.get_next_proxy()
        if proxy_info:
            return proxy_info.get_proxy_url(), proxy_info.id
    except Exception as e:
        log.warning(f"Failed to get proxy: {e}")

    return None, None


async def record_proxy_result(
    proxy_id: Optional[str], success: bool, latency_ms: int = 0, error: str = ""
):
    """Record the result of a proxy-enabled request."""
    if not proxy_id or not proxy_client:
        return

    try:
        if success:
            await proxy_client.record_success(proxy_id, latency_ms)
        else:
            await proxy_client.record_failure(proxy_id, error)
    except Exception as e:
        log.debug(f"Failed to record proxy result: {e}")


async def crawl_single_url(
    url: str,
    js_render: bool = False,
    wait_for: Optional[str] = None,
    use_proxy: bool = True,
) -> CrawlResponse:
    """ URL  (   ,   )"""
    async with crawl_semaphore:
        if AsyncWebCrawler is None:
            return CrawlResponse(url=url, status="FAILED", error="crawl4ai not available")

        max_attempts = 3 if use_proxy else 1
        last_error: str | None = None
        last_proxy_id: str | None = None

        for attempt in range(max_attempts):
            start_time = time.time()
            proxy_url, proxy_id = await get_proxy_for_crawl(use_proxy)
            if proxy_id:
                last_proxy_id = proxy_id

            try:
                # Prepare crawler configuration
                crawler_kwargs = {"verbose": False}
                if proxy_url:
                    crawler_kwargs["proxy"] = proxy_url
                    log.debug(f"Using proxy {proxy_id} for {url} (attempt={attempt + 1}/{max_attempts})")

                async with AsyncWebCrawler(**crawler_kwargs) as crawler:
                    result = await crawler.arun(
                        url=url,
                        js_code=wait_for if js_render else None,
                    )

                    latency_ms = int((time.time() - start_time) * 1000)

                    if not getattr(result, "success", False):
                        error_msg = getattr(result, "error_message", "Unknown error")
                        last_error = error_msg
                        await record_proxy_result(proxy_id, False, latency_ms, error_msg)
                        continue

                    html = getattr(result, "html", None) or ""
                    markdown = getattr(result, "markdown", None) or ""
                    captcha_type = detect_captcha_type(html) or detect_captcha_type(markdown)
                    if captcha_type:
                        last_error = f"CAPTCHA detected: {captcha_type}"
                        if proxy_id and proxy_client:
                            try:
                                await proxy_client.record_captcha(proxy_id, captcha_type=captcha_type)
                            except Exception:
                                pass
                        await record_proxy_result(proxy_id, False, latency_ms, last_error)
                        continue

                    await record_proxy_result(proxy_id, True, latency_ms)
                    return CrawlResponse(
                        url=getattr(result, "url", url),
                        markdown=getattr(result, "markdown", None),
                        html=getattr(result, "html", None),
                        status="SUCCESS",
                        proxy_used=proxy_id,
                        latency_ms=latency_ms,
                    )

            except Exception as e:
                latency_ms = int((time.time() - start_time) * 1000)
                last_error = str(e)
                await record_proxy_result(proxy_id, False, latency_ms, last_error)
                continue

        return CrawlResponse(
            url=url,
            status="FAILED",
            error=last_error or "Crawl failed",
            proxy_used=last_proxy_id,
        )


@app.post("/crawl", response_model=CrawlResponse)
async def crawl_url(request: CrawlRequest):
    """ URL """
    if AsyncWebCrawler is None:
        raise HTTPException(
            status_code=500, detail="crawl4ai not available in this environment"
        )

    result = await crawl_single_url(
        request.url,
        request.js_render,
        request.wait_for,
        request.use_proxy,
    )

    if result.status == "FAILED":
        raise HTTPException(status_code=400, detail=result.error or "Crawl failed")

    return result


@app.post("/crawl/batch", response_model=BatchCrawlResult)
async def crawl_batch(request: BatchCrawlRequest):
    """
     URL  ()

     URL     .
        .
         .
    """
    if AsyncWebCrawler is None:
        raise HTTPException(
            status_code=500, detail="crawl4ai not available in this environment"
        )

    batch_id = str(uuid.uuid4())

    #   
    tasks = [
        crawl_single_url(url, request.js_render, request.wait_for, request.use_proxy)
        for url in request.urls
    ]

    results = await asyncio.gather(*tasks, return_exceptions=True)

    #  
    crawl_results: List[CrawlResponse] = []
    completed = 0
    failed = 0

    for i, result in enumerate(results):
        if isinstance(result, Exception):
            crawl_results.append(
                CrawlResponse(url=request.urls[i], status="FAILED", error=str(result))
            )
            failed += 1
        elif isinstance(result, CrawlResponse):
            crawl_results.append(result)
            if result.status == "SUCCESS":
                completed += 1
            else:
                failed += 1
        else:
            crawl_results.append(
                CrawlResponse(
                    url=request.urls[i], status="FAILED", error="Unknown error"
                )
            )
            failed += 1

    status = "completed" if failed == 0 else ("partial" if completed > 0 else "failed")

    return BatchCrawlResult(
        batch_id=batch_id,
        total=len(request.urls),
        completed=completed,
        failed=failed,
        status=status,
        results=crawl_results,
    )


@app.post("/crawl/batch/async")
async def crawl_batch_async(
    request: BatchCrawlRequest, background_tasks: BackgroundTasks
):
    """
      

     batch_id    .
     GET /crawl/batch/{batch_id} .
    """
    if AsyncWebCrawler is None:
        raise HTTPException(
            status_code=500, detail="crawl4ai not available in this environment"
        )

    batch_id = str(uuid.uuid4())

    #   
    batch_results[batch_id] = {
        "batch_id": batch_id,
        "total": len(request.urls),
        "completed": 0,
        "failed": 0,
        "status": "processing",
        "results": [],
    }

    async def run_batch():
        tasks = [
            crawl_single_url(
                url, request.js_render, request.wait_for, request.use_proxy
            )
            for url in request.urls
        ]

        results = await asyncio.gather(*tasks, return_exceptions=True)

        crawl_results = []
        completed = 0
        failed = 0

        for i, result in enumerate(results):
            if isinstance(result, Exception):
                crawl_results.append(
                    {"url": request.urls[i], "status": "FAILED", "error": str(result)}
                )
                failed += 1
            elif isinstance(result, CrawlResponse):
                crawl_results.append(result.model_dump())
                if result.status == "SUCCESS":
                    completed += 1
                else:
                    failed += 1

        status = (
            "completed" if failed == 0 else ("partial" if completed > 0 else "failed")
        )

        batch_results[batch_id] = {
            "batch_id": batch_id,
            "total": len(request.urls),
            "completed": completed,
            "failed": failed,
            "status": status,
            "results": crawl_results,
        }

    background_tasks.add_task(run_batch)

    return {
        "batch_id": batch_id,
        "total": len(request.urls),
        "status": "processing",
        "message": "  . GET /crawl/batch/{batch_id}  .",
    }


@app.get("/crawl/batch/{batch_id}")
async def get_batch_result(batch_id: str):
    """   """
    if batch_id not in batch_results:
        raise HTTPException(status_code=404, detail="Batch not found")

    return batch_results[batch_id]


@app.delete("/crawl/batch/{batch_id}")
async def delete_batch_result(batch_id: str):
    """   """
    if batch_id in batch_results:
        del batch_results[batch_id]
        return {"status": "deleted", "batch_id": batch_id}
    raise HTTPException(status_code=404, detail="Batch not found")


@app.get("/proxy/stats")
async def get_proxy_stats():
    """   """
    if not proxy_client:
        return {"error": "Proxy rotation not enabled"}

    stats = await proxy_client.get_pool_stats()
    return stats or {"error": "Failed to get stats"}


@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup on shutdown"""
    if proxy_client:
        await proxy_client.close()

```

---

## backend/autonomous-crawler-service/crawl-worker/state_store.py

```py
"""
State Storage Module for crawl-worker Service

Provides persistent storage for batch crawl results using Redis.
Falls back to in-memory storage if Redis is unavailable.
"""

import os
import json
import asyncio
from datetime import datetime
from typing import Optional, Dict, Any, List

# Redis configuration from environment
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/2")
REDIS_PREFIX = os.getenv("REDIS_PREFIX", "crawl_worker")
RESULT_TTL_HOURS = int(os.getenv("RESULT_TTL_HOURS", "24"))  # Results expire after 24 hours


class StateStore:
    """Persistent state storage with Redis backend."""
    
    def __init__(self):
        self._redis = None
        self._memory_store: Dict[str, Any] = {}
        self._using_redis = False
        self._lock = asyncio.Lock()
    
    async def connect(self) -> bool:
        """Connect to Redis."""
        try:
            import redis.asyncio as redis
            
            self._redis = redis.from_url(
                REDIS_URL,
                encoding="utf-8",
                decode_responses=True,
                socket_connect_timeout=5,
                socket_timeout=5,
            )
            
            await self._redis.ping()
            self._using_redis = True
            return True
            
        except ImportError:
            self._using_redis = False
            return False
        except Exception:
            self._using_redis = False
            return False
    
    async def disconnect(self):
        """Close Redis connection."""
        if self._redis:
            await self._redis.close()
            self._redis = None
            self._using_redis = False
    
    def _key(self, batch_id: str) -> str:
        """Generate Redis key."""
        return f"{REDIS_PREFIX}:batch:{batch_id}"
    
    async def save_batch(self, batch_id: str, data: Dict[str, Any]) -> bool:
        """Save batch result."""
        async with self._lock:
            try:
                data_json = json.dumps(data, default=str)
                self._memory_store[batch_id] = json.loads(data_json)
                
                if self._using_redis and self._redis:
                    ttl_seconds = RESULT_TTL_HOURS * 60 * 60
                    await self._redis.set(self._key(batch_id), data_json, ex=ttl_seconds)
                
                return True
            except Exception:
                return False
    
    async def load_batch(self, batch_id: str) -> Optional[Dict[str, Any]]:
        """Load batch result."""
        if batch_id in self._memory_store:
            return self._memory_store[batch_id]
        
        if self._using_redis and self._redis:
            try:
                data_json = await self._redis.get(self._key(batch_id))
                if data_json:
                    data = json.loads(data_json)
                    self._memory_store[batch_id] = data
                    return data
            except Exception:
                pass
        
        return None
    
    async def delete_batch(self, batch_id: str) -> bool:
        """Delete batch result."""
        async with self._lock:
            self._memory_store.pop(batch_id, None)
            if self._using_redis and self._redis:
                try:
                    await self._redis.delete(self._key(batch_id))
                except Exception:
                    pass
            return True
    
    @property
    def is_redis_connected(self) -> bool:
        return self._using_redis
    
    def get_memory_store(self) -> Dict[str, Any]:
        return self._memory_store


# Singleton instance
_store: Optional[StateStore] = None


async def get_state_store() -> StateStore:
    """Get or create the singleton StateStore instance."""
    global _store
    if _store is None:
        _store = StateStore()
        await _store.connect()
    return _store

```

---

## backend/autonomous-crawler-service/src/__init__.py

```py
"""Autonomous Crawler Service package."""

```

---

## backend/autonomous-crawler-service/src/api/__init__.py

```py
"""REST API module for autonomous-crawler-service."""

# Lazy imports to avoid circular dependencies
__all__ = ["app", "create_app", "SSEManager", "SSEEventType"]


def __getattr__(name):
    """Lazy import for circular dependency prevention."""
    if name == "app":
        from src.api.server import app

        return app
    elif name == "create_app":
        from src.api.server import create_app

        return create_app
    elif name == "SSEManager":
        from src.api.sse import SSEManager

        return SSEManager
    elif name == "SSEEventType":
        from src.api.sse import SSEEventType

        return SSEEventType
    raise AttributeError(f"module {__name__!r} has no attribute {name!r}")

```

---

## backend/autonomous-crawler-service/src/api/server.py

```py
"""
FastAPI REST Server for autonomous-crawler-service.

browser-agent REST API + SSE  autonomous-crawler-service .
 Kafka    .
"""

import asyncio
import hashlib
import os
import uuid
from contextlib import asynccontextmanager
from datetime import datetime, timezone
from enum import Enum
from typing import Any, Dict, List, Optional

import httpx
import structlog
from fastapi import BackgroundTasks, Depends, FastAPI, HTTPException, Query, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field, HttpUrl

from src.api.sse import (
    SSEEvent,
    SSEEventType,
    SSEManager,
    get_sse_manager,
    sse_event_generator,
)
from src.config import Settings, get_settings
from src.crawler import AutonomousCrawlerAgent
from src.state.store import StateStore, get_state_store, close_state_store

# MCP Integration
from src.mcp.router import router as mcp_router

# ML Integration
from src.ml.router import router as ml_router
from src.ml.orchestrator import init_ml_orchestrator, get_ml_orchestrator

# Authentication
from src.auth.middleware import get_current_user, require_auth, require_admin, require_operator
from src.auth.jwt_utils import JWTPayload

logger = structlog.get_logger(__name__)


# ========================================
# Configuration
# ========================================


class LLMProvider(str, Enum):
    """ LLM Provider"""

    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    GOOGLE = "google"
    OPENROUTER = "openrouter"
    OLLAMA = "ollama"
    AZURE = "azure"
    CUSTOM = "custom"


class APIConfig:
    """API  """

    #  LLM API   (fallback)
    OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY", "")
    ANTHROPIC_API_KEY: str = os.getenv("ANTHROPIC_API_KEY", "")
    GOOGLE_API_KEY: str = os.getenv("GOOGLE_API_KEY", "")

    DEFAULT_LLM_PROVIDER: str = os.getenv("DEFAULT_LLM_PROVIDER", "openai")
    DEFAULT_MODEL: str = os.getenv("DEFAULT_MODEL", "gpt-4o")

    MAX_CONCURRENT_SESSIONS: int = int(os.getenv("MAX_CONCURRENT_SESSIONS", "3"))
    DEFAULT_TIMEOUT_SEC: int = int(os.getenv("DEFAULT_TIMEOUT_SEC", "120"))

    # DB  LLM Provider  
    USE_DB_PROVIDERS: bool = os.getenv("USE_DB_PROVIDERS", "true").lower() == "true"
    COLLECTOR_SERVICE_URL: str = os.getenv("COLLECTOR_SERVICE_URL", "http://collector:8002")
    WEB_CRAWLER_URL: str = os.getenv("WEB_CRAWLER_URL", "http://web-crawler:11235")
    WEB_CRAWLER_API_TOKEN: str = os.getenv(
        "WEB_CRAWLER_API_TOKEN", os.getenv("CRAWL4AI_API_TOKEN", "")
    )


api_config = APIConfig()


# ========================================
# Request/Response Models (Pydantic)
# ========================================


class CrawlMethod(str, Enum):
    """ """

    BROWSER_AGENT = "browser_agent"
    SIMPLE_FETCH = "simple_fetch"
    JS_RENDER = "js_render"


class AgentTask(BaseModel):
    """   """

    url: HttpUrl = Field(..., description="  URL")
    task: str = Field(
        ...,
        description="  (: '     ')",
        min_length=5,
        max_length=2000,
    )

    # LLM 
    llm_provider: LLMProvider = Field(default=LLMProvider.OPENAI)
    model: Optional[str] = Field(default=None, description=" (: gpt-4o)")

    #  
    screenshot: bool = Field(default=False, description="  ")
    extract_links: bool = Field(default=True, description="   ")
    max_steps: int = Field(default=10, ge=1, le=50, description="  ")
    timeout_sec: int = Field(default=120, ge=30, le=600)

    # URL  
    auto_save_url: bool = Field(default=True, description=" URL  ")
    source_category: str = Field(default="news", description=" ")


class ExtractedLink(BaseModel):
    """ """

    url: str
    text: Optional[str] = None
    context: Optional[str] = None


class AgentAction(BaseModel):
    """  """

    step: int
    action_type: str
    target: Optional[str] = None
    value: Optional[str] = None
    timestamp: datetime


class AgentTaskResult(BaseModel):
    """  """

    task_id: str
    url: str
    status: str  # success, failed, timeout

    #  
    extracted_data: Optional[Dict[str, Any]] = None
    extracted_text: Optional[str] = None
    extracted_links: List[ExtractedLink] = []

    # 
    content_hash: Optional[str] = None
    page_title: Optional[str] = None

    #  
    steps_taken: int = 0
    actions: List[AgentAction] = []
    tokens_used: Optional[int] = None
    duration_ms: int = 0

    #  (Base64)
    screenshot_base64: Optional[str] = None

    #  
    error_message: Optional[str] = None


class BatchCrawlRequest(BaseModel):
    """  """

    urls: List[HttpUrl] = Field(..., min_length=1, max_length=20)
    task: str = Field(..., description=" ")
    llm_provider: LLMProvider = Field(default=LLMProvider.OPENAI)
    model: Optional[str] = None
    auto_save_url: bool = True


class ChatMessage(BaseModel):
    """ """

    role: str = Field(..., description=" : user, assistant, system")
    content: str = Field(..., description=" ")


class ChatRequest(BaseModel):
    """ """

    messages: List[ChatMessage] = Field(..., description=" ")
    llm_provider: LLMProvider = Field(default=LLMProvider.OPENAI)
    model: Optional[str] = Field(default=None, description=" ")
    stream: bool = Field(default=False, description="  ")
    context_url: Optional[str] = Field(default=None, description="  URL")


class ChatResponse(BaseModel):
    """ """

    message: str
    provider: str
    model: str
    tokens_used: Optional[int] = None


# ========================================
# In-Memory Storage (deprecated - kept for backward compat, StateStore is primary)
# ========================================

# Note: task_results dict is now managed by StateStore with Redis persistence
# This variable is kept for quick reference but StateStore should be used
session_semaphore: Optional[asyncio.Semaphore] = None


# ========================================
# Provider Config Fetching (DB-based)
# ========================================

_provider_config_cache: Dict[str, dict] = {}
_provider_cache_ttl: Dict[str, datetime] = {}
PROVIDER_CACHE_TTL_SECONDS = 300


async def fetch_provider_config(provider_name: str) -> Optional[Dict[str, Any]]:
    """DB LLM provider   ( )"""
    cache_key = provider_name.lower()
    now = datetime.now(timezone.utc)

    if cache_key in _provider_config_cache:
        cache_time = _provider_cache_ttl.get(cache_key)
        if cache_time and (now - cache_time).total_seconds() < PROVIDER_CACHE_TTL_SECONDS:
            return _provider_config_cache[cache_key]

    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(
                f"{api_config.COLLECTOR_SERVICE_URL}/api/v1/llm-providers/config/{provider_name}"
            )
            if response.status_code == 200:
                provider_config = response.json()
                _provider_config_cache[cache_key] = provider_config
                _provider_cache_ttl[cache_key] = now
                return provider_config
    except Exception as e:
        logger.warning("Failed to fetch provider config", provider=provider_name, error=str(e))

    return None


def clear_provider_cache():
    """Provider  """
    _provider_config_cache.clear()
    _provider_cache_ttl.clear()


# ========================================
# LLM Factory
# ========================================


def get_llm_from_env(provider: LLMProvider, model: Optional[str] = None):
    """/Settings  LLM  

    Note: Settings Consul   .
    """
    from langchain_openai import ChatOpenAI
    from langchain_anthropic import ChatAnthropic
    from langchain_google_genai import ChatGoogleGenerativeAI

    # Get settings (includes Consul-loaded config)
    settings = get_settings()

    if provider == LLMProvider.OPENAI:
        return ChatOpenAI(
            model=model or settings.llm.openai_model or "gpt-4o",
            api_key=settings.llm.openai_api_key or api_config.OPENAI_API_KEY,
            temperature=0.1,
        )
    elif provider == LLMProvider.ANTHROPIC:
        return ChatAnthropic(
            model=model or settings.llm.anthropic_model or "claude-3-5-sonnet-20241022",
            api_key=settings.llm.anthropic_api_key or api_config.ANTHROPIC_API_KEY,
            temperature=0.1,
        )
    elif provider == LLMProvider.GOOGLE:
        return ChatGoogleGenerativeAI(
            model=model or "gemini-1.5-pro",
            google_api_key=api_config.GOOGLE_API_KEY,
            temperature=0.1,
        )
    elif provider == LLMProvider.OPENROUTER:
        return ChatOpenAI(
            model=model or settings.llm.openrouter_model or "openai/gpt-4o",
            api_key=settings.llm.openrouter_api_key,
            base_url=settings.llm.openrouter_base_url or "https://openrouter.ai/api/v1",
            temperature=0.1,
        )
    elif provider == LLMProvider.OLLAMA:
        try:
            from langchain_ollama import ChatOllama

            return ChatOllama(
                model=model or settings.llm.ollama_model or "llama3.1",
                base_url=settings.llm.ollama_base_url or "http://localhost:11434",
                temperature=0.1,
            )
        except ImportError:
            raise ValueError("Ollama support requires langchain-ollama package")
    elif provider == LLMProvider.AZURE:
        try:
            from langchain_openai import AzureChatOpenAI

            azure_endpoint = settings.llm.azure_endpoint or os.getenv("AZURE_OPENAI_ENDPOINT", "")
            azure_api_key = settings.llm.azure_api_key or os.getenv("AZURE_OPENAI_API_KEY", "")
            azure_deployment = (
                model
                or settings.llm.azure_deployment_name
                or os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME", "gpt-4o")
            )
            azure_api_version = settings.llm.azure_api_version or os.getenv(
                "AZURE_OPENAI_API_VERSION", "2024-02-15-preview"
            )

            if not azure_endpoint:
                raise ValueError(
                    "Azure OpenAI requires LLM_AZURE_ENDPOINT or AZURE_OPENAI_ENDPOINT"
                )
            if not azure_api_key:
                raise ValueError("Azure OpenAI requires LLM_AZURE_API_KEY or AZURE_OPENAI_API_KEY")

            return AzureChatOpenAI(
                azure_endpoint=azure_endpoint,
                azure_deployment=azure_deployment,
                api_key=azure_api_key,
                api_version=azure_api_version,
                temperature=0.1,
            )
        except ImportError:
            raise ValueError("Azure OpenAI support requires langchain-openai package")
    elif provider == LLMProvider.CUSTOM:
        custom_base_url = settings.llm.custom_base_url or os.getenv("CUSTOM_LLM_BASE_URL", "")
        if not custom_base_url:
            raise ValueError("Custom provider requires CUSTOM_LLM_BASE_URL or LLM_CUSTOM_BASE_URL")

        custom_request_format = settings.llm.custom_request_format
        custom_response_path = settings.llm.custom_response_path

        # If custom request format is provided, use CustomRESTAPIClient for non-OpenAI-compatible APIs
        if custom_request_format:
            from src.crawler.agent import CustomRESTAPIClient

            return CustomRESTAPIClient(
                base_url=custom_base_url,
                api_key=settings.llm.custom_api_key or os.getenv("CUSTOM_LLM_API_KEY", ""),
                model=model or settings.llm.custom_model or "default",
                request_format=custom_request_format,
                response_path=custom_response_path or "reply",
                custom_headers=settings.llm.custom_headers or "{}",
                temperature=0.1,
            )

        # Fallback to OpenAI-compatible format
        return ChatOpenAI(
            model=model or settings.llm.custom_model or "default",
            api_key=settings.llm.custom_api_key or os.getenv("CUSTOM_LLM_API_KEY", "none"),
            base_url=custom_base_url,
            temperature=0.1,
        )
    else:
        raise ValueError(f"Unsupported LLM provider: {provider}")


def get_llm_from_config(provider_config: Dict[str, Any], model_override: Optional[str] = None):
    """DB   LLM  """
    from langchain_openai import ChatOpenAI
    from langchain_anthropic import ChatAnthropic
    from langchain_google_genai import ChatGoogleGenerativeAI

    provider_type = provider_config.get("providerType", "").upper()
    api_key = provider_config.get("apiKey", "")
    base_url = provider_config.get("baseUrl")
    default_model = model_override or provider_config.get("defaultModel", "")
    extra_config = provider_config.get("config", {})
    temperature = extra_config.get("temperature", 0.1)

    if provider_type == "OPENAI":
        kwargs = {
            "model": default_model or "gpt-4o",
            "api_key": api_key,
            "temperature": temperature,
        }
        if base_url and base_url != "https://api.openai.com/v1":
            kwargs["base_url"] = base_url
        return ChatOpenAI(**kwargs)

    elif provider_type == "ANTHROPIC":
        return ChatAnthropic(
            model=default_model or "claude-3-5-sonnet-20241022",
            api_key=api_key,
            temperature=temperature,
        )

    elif provider_type == "GOOGLE":
        return ChatGoogleGenerativeAI(
            model=default_model or "gemini-1.5-pro",
            google_api_key=api_key,
            temperature=temperature,
        )

    elif provider_type == "OPENROUTER":
        return ChatOpenAI(
            model=default_model or "openai/gpt-4o",
            api_key=api_key,
            base_url=base_url or "https://openrouter.ai/api/v1",
            temperature=temperature,
        )

    elif provider_type == "OLLAMA":
        try:
            from langchain_ollama import ChatOllama

            return ChatOllama(
                model=default_model or "llama3.1",
                base_url=base_url or "http://localhost:11434",
                temperature=temperature,
            )
        except ImportError:
            raise ValueError("Ollama support requires langchain-ollama package")

    elif provider_type == "AZURE":
        try:
            from langchain_openai import AzureChatOpenAI

            azure_endpoint = extra_config.get("endpoint") or base_url
            deployment_name = extra_config.get("deployment_name") or default_model
            api_version = extra_config.get("api_version", "2024-02-15-preview")

            if not azure_endpoint:
                raise ValueError("Azure provider requires endpoint")
            if not deployment_name:
                raise ValueError("Azure provider requires deployment_name")

            return AzureChatOpenAI(
                azure_endpoint=azure_endpoint,
                azure_deployment=deployment_name,
                api_key=api_key,
                api_version=api_version,
                temperature=temperature,
            )
        except ImportError:
            raise ValueError("Azure OpenAI support requires langchain-openai package")

    elif provider_type == "CUSTOM":
        if not base_url:
            raise ValueError("Custom provider requires base_url")
        return ChatOpenAI(
            model=default_model or "default",
            api_key=api_key or "none",
            base_url=base_url,
            temperature=temperature,
        )

    else:
        raise ValueError(f"Unsupported provider type: {provider_type}")


async def get_llm(provider: LLMProvider, model: Optional[str] = None):
    """LLM   - DB ,  fallback"""
    provider_name = provider.value

    if api_config.USE_DB_PROVIDERS:
        provider_config = await fetch_provider_config(provider_name)
        if provider_config:
            try:
                return get_llm_from_config(provider_config, model)
            except Exception as e:
                logger.warning(
                    "DB provider failed, using fallback",
                    provider=provider_name,
                    error=str(e),
                )

    return get_llm_from_env(provider, model)


# ========================================
# Core Agent Logic
# ========================================


async def run_browser_agent(
    request: AgentTask,
    settings: Settings,
    sse_manager: SSEManager,
) -> AgentTaskResult:
    """
    browser-use  .

     autonomous-crawler-service AutonomousCrawlerAgent 
    browser-agent API  SSE  .
    """
    global session_semaphore

    task_id = str(uuid.uuid4())
    start_time = datetime.now(timezone.utc)
    actions_log: List[AgentAction] = []

    logger.info(
        "Agent task started",
        task_id=task_id,
        url=str(request.url),
        task=request.task[:100],
    )

    # SSE:   
    await sse_manager.send_agent_event(
        SSEEventType.AGENT_START,
        task_id=task_id,
        url=str(request.url),
        message=f" : {request.task[:50]}...",
        provider=request.llm_provider.value,
    )

    try:
        if session_semaphore is None:
            session_semaphore = asyncio.Semaphore(api_config.MAX_CONCURRENT_SESSIONS)

        async with session_semaphore:
            # AutonomousCrawlerAgent 
            agent = AutonomousCrawlerAgent(settings)

            #  URL  + AI 
            # smart_search  
            crawl_result = await agent.crawl_with_camoufox(
                url=str(request.url),
                extract_content=True,
                wait_for_cloudflare=True,
            )

            if crawl_result.get("error"):
                raise Exception(crawl_result["error"])

            extracted_text = crawl_result.get("text", "")
            page_title = crawl_result.get("title", "")
            extracted_links: List[ExtractedLink] = []

            #   ()
            if request.extract_links:
                links = crawl_result.get("links", [])
                extracted_links = [
                    ExtractedLink(url=link.get("url", ""), text=link.get("text", ""))
                    for link in links[:50]
                    if link.get("url", "").startswith("http")
                ]

            content_hash = (
                hashlib.sha256((extracted_text or "").encode()).hexdigest()
                if extracted_text
                else None
            )

            duration_ms = int((datetime.now(timezone.utc) - start_time).total_seconds() * 1000)

            task_result = AgentTaskResult(
                task_id=task_id,
                url=str(request.url),
                status="success",
                extracted_text=extracted_text,
                extracted_links=extracted_links,
                content_hash=content_hash,
                page_title=page_title,
                steps_taken=1,
                actions=actions_log,
                duration_ms=duration_ms,
            )

            #  
            await agent.close()

            logger.info(
                "Agent task completed",
                task_id=task_id,
                duration_ms=duration_ms,
                links_count=len(extracted_links),
            )

            # SSE:   
            await sse_manager.send_agent_event(
                SSEEventType.AGENT_COMPLETE,
                task_id=task_id,
                url=str(request.url),
                message=f" : {len(extracted_links)}  ",
                duration_ms=duration_ms,
                links_count=len(extracted_links),
                has_content=bool(extracted_text),
            )

            return task_result

    except asyncio.TimeoutError:
        duration_ms = int((datetime.now(timezone.utc) - start_time).total_seconds() * 1000)
        logger.error("Agent task timeout", task_id=task_id, timeout=request.timeout_sec)

        await sse_manager.send_agent_event(
            SSEEventType.AGENT_ERROR,
            task_id=task_id,
            url=str(request.url),
            message=f": {request.timeout_sec} ",
            error_type="timeout",
        )

        return AgentTaskResult(
            task_id=task_id,
            url=str(request.url),
            status="timeout",
            error_message=f" {request.timeout_sec}   .",
            duration_ms=duration_ms,
        )

    except Exception as e:
        duration_ms = int((datetime.now(timezone.utc) - start_time).total_seconds() * 1000)
        logger.error("Agent task failed", task_id=task_id, error=str(e))

        await sse_manager.send_agent_event(
            SSEEventType.AGENT_ERROR,
            task_id=task_id,
            url=str(request.url),
            message=f": {str(e)[:100]}",
            error_type="exception",
            error_detail=str(e),
        )

        return AgentTaskResult(
            task_id=task_id,
            url=str(request.url),
            status="failed",
            error_message=str(e),
            duration_ms=duration_ms,
        )


# ========================================
# FastAPI App
# ========================================


@asynccontextmanager
async def lifespan(app: FastAPI):
    """   """
    logger.info("Starting autonomous-crawler REST API server")

    # Initialize StateStore (connects to Redis)
    state_store = await get_state_store()
    app.state.state_store = state_store
    logger.info(
        "StateStore initialized",
        using_redis=state_store.is_redis_connected,
        task_count=state_store.task_count,
    )

    # Initialize ML Orchestrator
    try:
        ml_orchestrator = await init_ml_orchestrator()
        app.state.ml_orchestrator = ml_orchestrator
        logger.info("ML Orchestrator initialized")
    except Exception as e:
        logger.warning("ML Orchestrator initialization failed (non-critical)", error=str(e))
        app.state.ml_orchestrator = None

    yield

    # Cleanup ML Orchestrator
    if hasattr(app.state, "ml_orchestrator") and app.state.ml_orchestrator:
        await app.state.ml_orchestrator.close()
        logger.info("ML Orchestrator closed")

    # Cleanup StateStore
    logger.info("Shutting down autonomous-crawler REST API server")
    await close_state_store()


def create_app(settings: Settings | None = None) -> FastAPI:
    """FastAPI  """
    if settings is None:
        settings = get_settings()

    app = FastAPI(
        title="Autonomous Crawler Service API",
        description="AI     - browser-agent  REST API",
        version="0.2.0",
        lifespan=lifespan,
    )

    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # Store settings in app state
    app.state.settings = settings
    app.state.sse_manager = get_sse_manager()

    # Register routes
    register_routes(app)

    return app


def register_routes(app: FastAPI):
    """API  """

    # MCP Add-on Router 
    app.include_router(mcp_router)

    # ML Analysis Router 
    app.include_router(ml_router)

    @app.get("/health")
    @app.head("/health")
    async def health(req: Request):
        """ """
        state_store: StateStore = req.app.state.state_store
        store_stats = await state_store.get_stats()

        # Check ML orchestrator status
        ml_available = (
            hasattr(req.app.state, "ml_orchestrator") and req.app.state.ml_orchestrator is not None
        )

        return {
            "status": "ok",
            "service": "autonomous-crawler-service",
            "api_version": "0.2.0",
            "features": {
                "rest_api": True,
                "sse_events": True,
                "kafka_consumer": True,
                "camoufox": True,
                "captcha_bypass": True,
                "redis_persistence": state_store.is_redis_connected,
                "ml_analysis": ml_available,
            },
            "storage": store_stats,
            "active_sessions": state_store.task_count,
            "max_sessions": api_config.MAX_CONCURRENT_SESSIONS,
        }

    @app.get("/events")
    async def sse_events(request: Request):
        """
        SSE   .

            .
        """
        sse_manager: SSEManager = request.app.state.sse_manager
        client_id, queue = await sse_manager.connect()

        return StreamingResponse(
            sse_event_generator(client_id, queue, sse_manager),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",
                "Access-Control-Allow-Origin": "*",
            },
        )

    @app.get("/events/clients")
    async def get_sse_clients(request: Request):
        """  SSE   """
        sse_manager: SSEManager = request.app.state.sse_manager
        return {
            "connected_clients": sse_manager.client_count,
            "client_ids": sse_manager.client_ids,
        }

    @app.get("/providers")
    async def get_available_providers(user: JWTPayload = Depends(require_auth())):
        """  LLM Provider   (  - API   )"""
        providers = []
        settings = get_settings()

        if settings.llm.openai_api_key or api_config.OPENAI_API_KEY:
            providers.append(
                {
                    "name": "openai",
                    "providerType": "OPENAI",
                    "defaultModel": settings.llm.openai_model or "gpt-4o",
                    "available": True,
                }
            )

        if settings.llm.anthropic_api_key or api_config.ANTHROPIC_API_KEY:
            providers.append(
                {
                    "name": "anthropic",
                    "providerType": "ANTHROPIC",
                    "defaultModel": settings.llm.anthropic_model or "claude-3-5-sonnet-20241022",
                    "available": True,
                }
            )

        if api_config.GOOGLE_API_KEY:
            providers.append(
                {
                    "name": "google",
                    "providerType": "GOOGLE",
                    "defaultModel": "gemini-1.5-pro",
                    "available": True,
                }
            )

        # OpenRouter (Settings )
        if settings.llm.openrouter_api_key or os.getenv("OPENROUTER_API_KEY"):
            providers.append(
                {
                    "name": "openrouter",
                    "providerType": "OPENROUTER",
                    "defaultModel": settings.llm.openrouter_model or "openai/gpt-4o",
                    "available": True,
                }
            )

        # Ollama ( ,   )
        providers.append(
            {
                "name": "ollama",
                "providerType": "OLLAMA",
                "defaultModel": settings.llm.ollama_model or "llama3.1",
                "available": True,
                "local": True,
            }
        )

        # Azure OpenAI (Settings )
        if (settings.llm.azure_api_key and settings.llm.azure_endpoint) or (
            os.getenv("AZURE_OPENAI_API_KEY") and os.getenv("AZURE_OPENAI_ENDPOINT")
        ):
            providers.append(
                {
                    "name": "azure",
                    "providerType": "AZURE",
                    "defaultModel": settings.llm.azure_deployment_name
                    or os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME", "gpt-4o"),
                    "available": True,
                }
            )

        # Custom (Settings )
        if settings.llm.custom_base_url or os.getenv("CUSTOM_LLM_BASE_URL"):
            providers.append(
                {
                    "name": "custom",
                    "providerType": "CUSTOM",
                    "defaultModel": settings.llm.custom_model or "default",
                    "available": True,
                }
            )

        return {
            "providers": providers,
            "defaultProvider": providers[0] if providers else None,
            "source": "environment",
            "supportedProviders": [p.value for p in LLMProvider],
        }

    @app.post("/providers/cache/clear")
    async def clear_providers_cache_endpoint():
        """Provider   """
        clear_provider_cache()
        return {"status": "ok", "message": "Provider cache cleared"}

    @app.post("/providers/test")
    async def test_provider_connection(
        provider: LLMProvider = Query(..., description=" Provider"),
        model: Optional[str] = Query(default=None, description=" "),
    ):
        """
        LLM Provider  .

           Provider   .
        """
        import time

        start_time = time.time()

        try:
            llm = await get_llm(provider, model)

            #   
            from langchain_core.messages import HumanMessage

            test_message = HumanMessage(content="Say 'Connection successful!' in one line.")
            response = await llm.ainvoke([test_message])

            elapsed_ms = int((time.time() - start_time) * 1000)

            return {
                "status": "success",
                "provider": provider.value,
                "model": model or "default",
                "response": response.content[:100] if response.content else "No response",
                "latency_ms": elapsed_ms,
                "message": " ",
            }

        except Exception as e:
            elapsed_ms = int((time.time() - start_time) * 1000)
            logger.error("Provider test failed", provider=provider.value, error=str(e))
            return {
                "status": "failed",
                "provider": provider.value,
                "model": model or "default",
                "error": str(e),
                "latency_ms": elapsed_ms,
                "message": f" : {str(e)[:100]}",
            }

    @app.get("/providers/{provider}/models")
    async def get_provider_models(
        provider: LLMProvider,
        api_key: Optional[str] = Query(
            default=None, description="API  (,   )"
        ),
        base_url: Optional[str] = Query(default=None, description="Base URL (Ollama, Custom)"),
    ):
        """
         LLM Provider      .

        - OpenAI: /v1/models API 
        - OpenRouter: /api/v1/models API 
        - Ollama: /api/tags API 
        - Anthropic, Google, Azure:    ( API )
        - Custom: /v1/models   
        """
        try:
            async with httpx.AsyncClient(timeout=15.0) as client:
                if provider == LLMProvider.OPENAI:
                    key = api_key or os.getenv("OPENAI_API_KEY", "")
                    if not key:
                        return {
                            "provider": provider.value,
                            "models": _get_static_models("openai"),
                            "source": "static",
                            "message": "API     ",
                        }

                    resp = await client.get(
                        "https://api.openai.com/v1/models",
                        headers={"Authorization": f"Bearer {key}"},
                    )
                    if resp.status_code == 200:
                        data = resp.json()
                        # GPT   (chat )
                        models = [
                            {"id": m["id"], "name": m["id"], "owned_by": m.get("owned_by", "")}
                            for m in data.get("data", [])
                            if any(prefix in m["id"] for prefix in ["gpt-", "o1-", "chatgpt-"])
                        ]
                        # : gpt-4o 
                        models.sort(key=lambda x: (0 if "gpt-4o" in x["id"] else 1, x["id"]))
                        return {
                            "provider": provider.value,
                            "models": models[:20],  #  20
                            "source": "api",
                            "total": len(models),
                        }
                    else:
                        return {
                            "provider": provider.value,
                            "models": _get_static_models("openai"),
                            "source": "static",
                            "error": f"API  : {resp.status_code}",
                        }

                elif provider == LLMProvider.OPENROUTER:
                    key = api_key or os.getenv("OPENROUTER_API_KEY", "")
                    headers = {}
                    if key:
                        headers["Authorization"] = f"Bearer {key}"

                    resp = await client.get(
                        "https://openrouter.ai/api/v1/models",
                        headers=headers,
                    )
                    if resp.status_code == 200:
                        data = resp.json()
                        all_models = data.get("data", [])

                        #    
                        priority_providers = [
                            "openai",
                            "anthropic",
                            "google",
                            "meta-llama",
                            "mistralai",
                            "deepseek",
                            "qwen",
                            "cohere",
                        ]

                        def get_provider_priority(model_id: str) -> int:
                            for i, provider in enumerate(priority_providers):
                                if model_id.startswith(provider):
                                    return i
                            return len(priority_providers)

                        #    (   )
                        sorted_models = sorted(
                            all_models,
                            key=lambda m: (
                                get_provider_priority(m["id"]),
                                #   
                                0 if m.get("pricing", {}).get("prompt", "0") != "0" else 1,
                                m["id"],
                            ),
                        )

                        models = [
                            {
                                "id": m["id"],
                                "name": m.get("name", m["id"]),
                                "context_length": m.get("context_length"),
                                "pricing": m.get("pricing"),
                            }
                            for m in sorted_models[:100]  #  100
                        ]
                        return {
                            "provider": provider.value,
                            "models": models,
                            "source": "api",
                            "total": len(all_models),
                        }
                    else:
                        return {
                            "provider": provider.value,
                            "models": _get_static_models("openrouter"),
                            "source": "static",
                            "error": f"API  : {resp.status_code}",
                        }

                elif provider == LLMProvider.OLLAMA:
                    ollama_url = base_url or os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
                    try:
                        resp = await client.get(f"{ollama_url}/api/tags")
                        if resp.status_code == 200:
                            data = resp.json()
                            models = [
                                {
                                    "id": m["name"],
                                    "name": m["name"],
                                    "size": m.get("size"),
                                    "modified_at": m.get("modified_at"),
                                }
                                for m in data.get("models", [])
                            ]
                            return {
                                "provider": provider.value,
                                "models": models,
                                "source": "api",
                                "ollama_url": ollama_url,
                            }
                        else:
                            return {
                                "provider": provider.value,
                                "models": _get_static_models("ollama"),
                                "source": "static",
                                "error": f"Ollama  : {resp.status_code}",
                            }
                    except httpx.ConnectError:
                        return {
                            "provider": provider.value,
                            "models": _get_static_models("ollama"),
                            "source": "static",
                            "error": f"Ollama    : {ollama_url}",
                        }

                elif provider == LLMProvider.ANTHROPIC:
                    # Anthropic    API 
                    return {
                        "provider": provider.value,
                        "models": _get_static_models("anthropic"),
                        "source": "static",
                        "message": "Anthropic   API  ",
                    }

                elif provider == LLMProvider.GOOGLE:
                    # Google AI - Generative Language API   
                    key = api_key or os.getenv("GOOGLE_API_KEY", "")
                    if not key:
                        return {
                            "provider": provider.value,
                            "models": _get_static_models("google"),
                            "source": "static",
                            "message": "API     ",
                        }

                    try:
                        resp = await client.get(
                            f"https://generativelanguage.googleapis.com/v1beta/models?key={key}",
                        )
                        if resp.status_code == 200:
                            data = resp.json()
                            # gemini   (generateContent  )
                            models = [
                                {
                                    "id": m["name"].replace("models/", ""),
                                    "name": m.get("displayName", m["name"].replace("models/", "")),
                                    "context_length": m.get("inputTokenLimit"),
                                    "description": m.get("description", ""),
                                }
                                for m in data.get("models", [])
                                if "generateContent" in m.get("supportedGenerationMethods", [])
                                and "gemini" in m.get("name", "").lower()
                            ]
                            #    
                            models.sort(
                                key=lambda x: (
                                    0
                                    if "2.5" in x["id"]
                                    else (
                                        1 if "2.0" in x["id"] else (2 if "1.5" in x["id"] else 3)
                                    ),
                                    0 if "pro" in x["id"].lower() else 1,
                                    x["id"],
                                )
                            )
                            return {
                                "provider": provider.value,
                                "models": models[:15],
                                "source": "api",
                                "total": len(models),
                            }
                        else:
                            return {
                                "provider": provider.value,
                                "models": _get_static_models("google"),
                                "source": "static",
                                "error": f"API  : {resp.status_code}",
                            }
                    except Exception as e:
                        return {
                            "provider": provider.value,
                            "models": _get_static_models("google"),
                            "source": "static",
                            "error": f"Google AI API : {str(e)[:50]}",
                        }

                elif provider == LLMProvider.AZURE:
                    # Azure     
                    return {
                        "provider": provider.value,
                        "models": _get_static_models("azure"),
                        "source": "static",
                        "message": "Azure OpenAI     ",
                    }

                elif provider == LLMProvider.CUSTOM:
                    custom_url = base_url or os.getenv("CUSTOM_LLM_BASE_URL", "")
                    if custom_url:
                        try:
                            resp = await client.get(f"{custom_url}/v1/models")
                            if resp.status_code == 200:
                                data = resp.json()
                                models = [
                                    {"id": m["id"], "name": m.get("id", "")}
                                    for m in data.get("data", [])
                                ]
                                return {
                                    "provider": provider.value,
                                    "models": models,
                                    "source": "api",
                                    "base_url": custom_url,
                                }
                        except Exception:
                            pass

                    return {
                        "provider": provider.value,
                        "models": [{"id": "default", "name": " "}],
                        "source": "static",
                    }

                else:
                    return {
                        "provider": provider.value,
                        "models": [],
                        "source": "unknown",
                        "error": "   Provider",
                    }

        except Exception as e:
            logger.error("Failed to fetch models", provider=provider.value, error=str(e))
            return {
                "provider": provider.value,
                "models": _get_static_models(provider.value),
                "source": "static",
                "error": str(e),
            }

    def _get_static_models(provider: str) -> List[Dict[str, Any]]:
        """    (fallback)"""
        static_models = {
            "openai": [
                {"id": "gpt-4o", "name": "GPT-4o ()"},
                {"id": "gpt-4o-mini", "name": "GPT-4o Mini ()"},
                {"id": "gpt-4.1", "name": "GPT-4.1"},
                {"id": "gpt-4.1-mini", "name": "GPT-4.1 Mini"},
                {"id": "gpt-4-turbo", "name": "GPT-4 Turbo"},
                {"id": "gpt-3.5-turbo", "name": "GPT-3.5 Turbo ()"},
                {"id": "o1", "name": "o1 ()"},
                {"id": "o1-preview", "name": "o1-preview ()"},
                {"id": "o1-mini", "name": "o1-mini (, )"},
                {"id": "o3-mini", "name": "o3-mini ( )"},
            ],
            "anthropic": [
                {"id": "claude-sonnet-4-20250514", "name": "Claude Sonnet 4 ()"},
                {"id": "claude-3-5-sonnet-20241022", "name": "Claude 3.5 Sonnet ()"},
                {"id": "claude-3-5-haiku-20241022", "name": "Claude 3.5 Haiku ()"},
                {"id": "claude-3-opus-20240229", "name": "Claude 3 Opus ()"},
            ],
            "google": [
                {"id": "gemini-2.5-flash-preview-05-20", "name": "Gemini 2.5 Flash ()"},
                {"id": "gemini-2.5-pro-preview-05-06", "name": "Gemini 2.5 Pro ()"},
                {"id": "gemini-2.0-flash", "name": "Gemini 2.0 Flash"},
                {"id": "gemini-2.0-flash-lite", "name": "Gemini 2.0 Flash Lite ()"},
                {"id": "gemini-1.5-pro", "name": "Gemini 1.5 Pro"},
                {"id": "gemini-1.5-flash", "name": "Gemini 1.5 Flash"},
            ],
            "openrouter": [
                {"id": "openai/gpt-4o", "name": "GPT-4o (OpenAI)"},
                {"id": "openai/gpt-4.1", "name": "GPT-4.1 (OpenAI)"},
                {"id": "anthropic/claude-sonnet-4", "name": "Claude Sonnet 4 (Anthropic)"},
                {"id": "anthropic/claude-3.5-sonnet", "name": "Claude 3.5 Sonnet"},
                {"id": "google/gemini-2.5-flash-preview", "name": "Gemini 2.5 Flash (Google)"},
                {"id": "google/gemini-2.0-flash-001", "name": "Gemini 2.0 Flash (Google)"},
                {"id": "google/gemini-pro-1.5", "name": "Gemini 1.5 Pro"},
                {"id": "meta-llama/llama-3.3-70b-instruct", "name": "Llama 3.3 70B"},
                {"id": "meta-llama/llama-3.1-405b-instruct", "name": "Llama 3.1 405B"},
                {"id": "deepseek/deepseek-r1", "name": "DeepSeek R1 ()"},
                {"id": "deepseek/deepseek-chat", "name": "DeepSeek Chat"},
                {"id": "qwen/qwen-2.5-72b-instruct", "name": "Qwen 2.5 72B"},
            ],
            "ollama": [
                {"id": "llama3.3", "name": "Llama 3.3 ()"},
                {"id": "llama3.2", "name": "Llama 3.2"},
                {"id": "llama3.1", "name": "Llama 3.1"},
                {"id": "llama3.1:70b", "name": "Llama 3.1 70B"},
                {"id": "mistral", "name": "Mistral"},
                {"id": "mixtral", "name": "Mixtral"},
                {"id": "codellama", "name": "Code Llama"},
                {"id": "qwen2.5", "name": "Qwen 2.5"},
                {"id": "deepseek-r1", "name": "DeepSeek R1"},
                {"id": "gemma2", "name": "Gemma 2"},
            ],
            "azure": [
                {"id": "gpt-4o", "name": "GPT-4o"},
                {"id": "gpt-4-turbo", "name": "GPT-4 Turbo"},
                {"id": "gpt-35-turbo", "name": "GPT-3.5 Turbo"},
            ],
            "custom": [
                {"id": "default", "name": " "},
            ],
        }
        return static_models.get(provider, [])

    @app.post("/agent/crawl", response_model=AgentTaskResult)
    async def agent_crawl(
        request: AgentTask,
        req: Request,
        user: JWTPayload = Depends(require_auth()),
    ):
        """
        AI    ().

        CAPTCHA     .
         : Bearer  
        """
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store

        logger.info("Agent crawl requested", user=user.username, url=str(request.url))
        result = await run_browser_agent(request, settings, sse_manager)
        await state_store.save_task(result.task_id, result)
        return result

    @app.post("/agent/crawl/async")
    async def agent_crawl_async(
        request: AgentTask,
        req: Request,
        background_tasks: BackgroundTasks,
        user: JWTPayload = Depends(require_auth()),
    ):
        """AI     ( )"""
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store
        task_id = str(uuid.uuid4())

        async def run_task():
            result = await run_browser_agent(request, settings, sse_manager)
            result.task_id = task_id
            await state_store.save_task(task_id, result)

        background_tasks.add_task(asyncio.create_task, run_task())

        return {
            "task_id": task_id,
            "status": "queued",
            "message": "  . GET /agent/task/{task_id}  .",
        }

    @app.get("/agent/task/{task_id}", response_model=AgentTaskResult)
    async def get_task_result(task_id: str, req: Request):
        """  """
        state_store: StateStore = req.app.state.state_store
        result = await state_store.load_task(task_id)
        if result is None:
            raise HTTPException(status_code=404, detail="   .")
        return result

    @app.delete("/agent/task/{task_id}")
    async def delete_task(task_id: str, req: Request):
        """  """
        state_store: StateStore = req.app.state.state_store
        result = await state_store.load_task(task_id)
        if result is not None:
            await state_store.delete_task(task_id)
            return {"status": "deleted", "task_id": task_id}
        raise HTTPException(status_code=404, detail="   .")

    @app.get("/agent/tasks")
    async def list_tasks(
        req: Request,
        limit: int = Query(default=20, le=100),
        status: Optional[str] = Query(default=None),
    ):
        """   """
        state_store: StateStore = req.app.state.state_store
        results = await state_store.list_tasks(status=status, limit=limit)

        # Sort by duration_ms (stored tasks are dicts, not Pydantic models)
        results = sorted(results, key=lambda x: x.get("duration_ms", 0), reverse=True)[:limit]

        return {"total": len(results), "tasks": results}

    @app.post("/agent/batch")
    async def batch_crawl(
        request: BatchCrawlRequest,
        req: Request,
        background_tasks: BackgroundTasks,
        user: JWTPayload = Depends(require_auth()),
    ):
        """ URL   ( )"""
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store
        batch_id = str(uuid.uuid4())
        task_ids = []

        tasks_to_run = []
        for url in request.urls:
            agent_task = AgentTask(
                url=url,
                task=request.task,
                llm_provider=request.llm_provider,
                model=request.model,
                auto_save_url=request.auto_save_url,
            )
            task_id = str(uuid.uuid4())
            task_ids.append(task_id)
            tasks_to_run.append((agent_task, task_id))

        async def run_batch():
            for agent_task, tid in tasks_to_run:
                try:
                    result = await run_browser_agent(agent_task, settings, sse_manager)
                    result.task_id = tid
                    await state_store.save_task(tid, result)
                except Exception as e:
                    error_result = AgentTaskResult(
                        task_id=tid,
                        url=str(agent_task.url),
                        status="failed",
                        error_message=str(e),
                        duration_ms=0,
                    )
                    await state_store.save_task(tid, error_result)

        background_tasks.add_task(run_batch)

        return {
            "batch_id": batch_id,
            "task_ids": task_ids,
            "total": len(task_ids),
            "message": "  .",
        }

    # ========================================
    #   
    # ========================================

    @app.post("/agent/presets/extract-article")
    async def extract_article(url: HttpUrl, req: Request, auto_save: bool = True):
        """
           .

        URL , , ,   .
        """
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store

        request = AgentTask(
            url=url,
            task="""
                  :
            1.  
            2. /
            3. 
            4.   
            5.   
            
            JSON  .
            """,
            extract_links=True,
            auto_save_url=auto_save,
            source_category="news",
        )
        result = await run_browser_agent(request, settings, sse_manager)
        await state_store.save_task(result.task_id, result)
        return result

    @app.post("/agent/presets/extract-comments")
    async def extract_comments(
        url: HttpUrl,
        req: Request,
        max_scroll: int = 5,
    ):
        """
        /  .

          .
        """
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store

        request = AgentTask(
            url=url,
            task=f"""
               :
            1.  {max_scroll}   
            2.   , , ,   
            3. ""       
            
            JSON   .
            """,
            max_steps=20,
            auto_save_url=False,
            source_category="forum",
        )
        result = await run_browser_agent(request, settings, sse_manager)
        await state_store.save_task(result.task_id, result)
        return result

    @app.post("/agent/presets/site-structure")
    async def analyze_site_structure(url: HttpUrl, req: Request):
        """
           .

             .
        """
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store

        request = AgentTask(
            url=url,
            task="""
               :
            1.    
            2.  /
            3. RSS   
            4. API   
            5.  
            
                 .
            """,
            extract_links=True,
            auto_save_url=True,
            source_category="general",
        )
        result = await run_browser_agent(request, settings, sse_manager)
        await state_store.save_task(result.task_id, result)
        return result

    @app.post("/agent/presets/news-domain-crawl")
    async def news_domain_crawl(
        url: HttpUrl,
        req: Request,
        max_pages: int = Query(default=30, ge=1, le=100, description="   "),
        max_depth: int = Query(default=2, ge=1, le=5, description="  "),
        focus_keywords: Optional[str] = Query(default=None, description="  ( )"),
    ):
        """
            .

             .
        NEWS_ONLY     .

        Args:
            url:  URL (     )
            max_pages:    
            max_depth:    
            focus_keywords:       
        """
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store

        keyword_instruction = ""
        if focus_keywords:
            keyword_instruction = (
                f"\n     : {focus_keywords}"
            )

        request = AgentTask(
            url=url,
            task=f"""
                :
            
            1.      
            2.    , , ,  
            3.  {max_pages}    
            4.    
            5. ,  ,   
            {keyword_instruction}
            
                :
            ---ARTICLE_START---
            URL: [ URL]
            TITLE: []
            AUTHOR: []
            PUBLISHED_AT: []
            CONTENT: []
            ---ARTICLE_END---
            """,
            max_steps=max_pages + 10,
            timeout_sec=min(max_pages * 15, 600),
            extract_links=True,
            auto_save_url=True,
            source_category="news",
        )
        result = await run_browser_agent(request, settings, sse_manager)
        await state_store.save_task(result.task_id, result)
        return result

    @app.post("/agent/presets/discover-rss")
    async def discover_rss(url: HttpUrl, req: Request):
        """
        RSS   .

         RSS/Atom  URL .
             URL  .
        """
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store

        request = AgentTask(
            url=url,
            task="""
              RSS  Atom  :
            
            1.   RSS/Atom   
               - <link rel="alternate" type="application/rss+xml" ...>
               - <link rel="alternate" type="application/atom+xml" ...>
            2.  RSS  :
               - /feed, /rss, /feeds, /rss.xml, /feed.xml, /atom.xml
               - /news/rss, /blog/feed 
            3.    RSS / 
            4. sitemap.xml   
            
                JSON  :
            {
                "feeds": [
                    {
                        "url": " URL",
                        "type": "rss"  "atom",
                        "title": "  ( )",
                        "category": " ( )"
                    }
                ],
                "sitemap_url": " URL ( )",
                "has_api_hints": true/false
            }
            """,
            max_steps=15,
            extract_links=True,
            auto_save_url=False,
            source_category="general",
        )
        result = await run_browser_agent(request, settings, sse_manager)
        await state_store.save_task(result.task_id, result)
        return result

    @app.post("/agent/presets/extract-news-list")
    async def extract_news_list(
        url: HttpUrl,
        req: Request,
        max_articles: int = Query(default=20, ge=1, le=50, description="   "),
    ):
        """
            .

         /    .
          , URL, ,  .
        """
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store

        request = AgentTask(
            url=url,
            task=f"""
                 :
            
            1.  {max_articles}   
            2.      :
               -  
               -  URL ( )
               - /  ( )
               - / ( )
               -   URL ( )
               - / ( )
            3.    
            4.    
            
            JSON   :
            [
                {{
                    "title": " ",
                    "url": " URL",
                    "summary": "",
                    "published_at": "",
                    "thumbnail": " URL",
                    "category": ""
                }}
            ]
            """,
            max_steps=10,
            extract_links=True,
            auto_save_url=True,
            source_category="news",
        )
        result = await run_browser_agent(request, settings, sse_manager)
        await state_store.save_task(result.task_id, result)
        return result

    @app.post("/agent/presets/monitor-breaking-news")
    async def monitor_breaking_news(
        url: HttpUrl,
        req: Request,
        keywords: Optional[str] = Query(default=None, description="  ( )"),
    ):
        """
        /   .

             .
        '', '', 'Breaking'      .
        """
        settings: Settings = req.app.state.settings
        sse_manager: SSEManager = req.app.state.sse_manager
        state_store: StateStore = req.app.state.state_store

        keyword_filter = ""
        if keywords:
            keyword_filter = f"\n     : {keywords}"

        request = AgentTask(
            url=url,
            task=f"""
               /  :
            
            1.      :
               - "", "Breaking", "", "", "flash"
               -    
               -      
            2.  1    
            3.      
            {keyword_filter}
            
               :
            {{
                "breaking_news": [
                    {{
                        "title": " ",
                        "url": " URL",
                        "published_at": " ",
                        "label": "// ",
                        "summary": "  ",
                        "full_content": " "
                    }}
                ],
                "latest_update": "  ",
                "total_found":   
            }}
            """,
            max_steps=15,
            timeout_sec=180,
            extract_links=True,
            auto_save_url=True,
            source_category="news",
        )
        result = await run_browser_agent(request, settings, sse_manager)
        await state_store.save_task(result.task_id, result)
        return result

    # ========================================
    #  
    # ========================================

    async def fetch_page_content_for_context(url: str, max_chars: int = 8000) -> Optional[str]:
        """
        URL     .

        crawl4ai        .
        """
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                # 1: crawl4ai  
                try:

                    def extract_content(payload: Any) -> Optional[str]:
                        if isinstance(payload, dict):
                            results = payload.get("results")
                            if isinstance(results, list) and results:
                                return extract_content(results[0])
                            result = payload.get("result")
                            if result is not None:
                                return extract_content(result)
                            for key in ("markdown", "text", "content"):
                                value = payload.get(key)
                                if isinstance(value, str) and value.strip():
                                    return value
                            return None
                        if isinstance(payload, str) and payload.strip():
                            return payload
                        return None

                    headers: dict[str, str] = {}
                    if api_config.WEB_CRAWLER_API_TOKEN:
                        headers["Authorization"] = f"Bearer {api_config.WEB_CRAWLER_API_TOKEN}"

                    base_url = api_config.WEB_CRAWLER_URL.rstrip("/")
                    endpoint = f"{base_url}/crawl"

                    crawl_response = await client.get(
                        endpoint, params={"url": url}, headers=headers
                    )
                    if crawl_response.status_code == 200:
                        try:
                            crawl_data = crawl_response.json()
                        except Exception:
                            crawl_data = crawl_response.text
                        content = extract_content(crawl_data)
                        if content:
                            return content[:max_chars]

                    crawl_response = await client.post(
                        endpoint,
                        json={"urls": [url], "priority": 10},
                        headers=headers,
                    )
                    if crawl_response.status_code == 200:
                        crawl_data = crawl_response.json()
                        content = extract_content(crawl_data)
                        if content:
                            return content[:max_chars]

                        task_id = crawl_data.get("task_id")
                        if task_id:
                            for status_path in (
                                f"{base_url}/task/{task_id}",
                                f"{base_url}/job/{task_id}",
                            ):
                                try:
                                    status_response = await client.get(status_path, headers=headers)
                                    if status_response.status_code == 200:
                                        status_data = status_response.json()
                                        status_content = extract_content(status_data)
                                        if status_content:
                                            return status_content[:max_chars]
                                except Exception:
                                    continue
                except Exception:
                    pass

                # 2:  HTTP  fallback
                try:
                    response = await client.get(url, follow_redirects=True)
                    if response.status_code == 200:
                        from bs4 import BeautifulSoup

                        soup = BeautifulSoup(response.text, "html.parser")

                        #   
                        for tag in soup(["script", "style", "nav", "footer", "header", "aside"]):
                            tag.decompose()

                        #   
                        text = soup.get_text(separator="\n", strip=True)
                        return text[:max_chars] if text else None
                except Exception:
                    pass

        except Exception as e:
            logger.warning("Failed to fetch context URL", url=url, error=str(e))

        return None

    def convert_to_langchain_messages(messages: List[ChatMessage]):
        """ChatMessage  LangChain  """
        from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

        langchain_messages = []
        for msg in messages:
            if msg.role == "user":
                langchain_messages.append(HumanMessage(content=msg.content))
            elif msg.role == "assistant":
                langchain_messages.append(AIMessage(content=msg.content))
            elif msg.role == "system":
                langchain_messages.append(SystemMessage(content=msg.content))
        return langchain_messages

    @app.post("/chat", response_model=ChatResponse)
    async def chat(request: ChatRequest):
        """
        AI  .

        LLM   .
        URL       .

         :
        ``\`json
        {
            "messages": [
                {"role": "user", "content": "    "}
            ],
            "context_url": "https://news.example.com/article/123"
        }
        ``\`
        """
        try:
            llm = await get_llm(request.llm_provider, request.model)
            langchain_messages = convert_to_langchain_messages(request.messages)

            # URL     
            if request.context_url:
                page_content = await fetch_page_content_for_context(request.context_url)
                if page_content:
                    from langchain_core.messages import SystemMessage

                    context_msg = SystemMessage(
                        content=f"   :\n\n{page_content}"
                    )
                    langchain_messages.insert(0, context_msg)
                    logger.debug(
                        "Context URL content added",
                        url=request.context_url,
                        content_length=len(page_content),
                    )

            response = await llm.ainvoke(langchain_messages)

            return ChatResponse(
                message=response.content,
                provider=request.llm_provider.value,
                model=request.model or api_config.DEFAULT_MODEL,
                tokens_used=getattr(response, "usage_metadata", {}).get("total_tokens"),
            )

        except Exception as e:
            logger.error("Chat failed", error=str(e))
            raise HTTPException(status_code=500, detail=f"  : {str(e)}")

    @app.post("/chat/stream")
    async def chat_stream(request: ChatRequest):
        """
        AI    (SSE).

        LLM   .
        URL       .

         :
        ``\`json
        {
            "messages": [
                {"role": "user", "content": "  "}
            ],
            "stream": true,
            "context_url": "https://docs.python.org/3/"
        }
        ``\`

         :
        - chunk:   
        - done:  
        - error:  
        """
        import json

        async def generate_stream():
            try:
                llm = await get_llm(request.llm_provider, request.model)
                langchain_messages = convert_to_langchain_messages(request.messages)

                # URL     
                if request.context_url:
                    page_content = await fetch_page_content_for_context(request.context_url)
                    if page_content:
                        from langchain_core.messages import SystemMessage

                        context_msg = SystemMessage(
                            content=f"   :\n\n{page_content}"
                        )
                        langchain_messages.insert(0, context_msg)
                        logger.debug(
                            "Context URL content added for streaming",
                            url=request.context_url,
                            content_length=len(page_content),
                        )

                #  LLM 
                full_response = ""
                async for chunk in llm.astream(langchain_messages):
                    if hasattr(chunk, "content") and chunk.content:
                        content = chunk.content
                        full_response += content
                        yield f"data: {json.dumps({'content': content, 'type': 'chunk'})}\n\n"

                #  
                yield f"data: {json.dumps({'type': 'done', 'provider': request.llm_provider.value, 'model': request.model or api_config.DEFAULT_MODEL, 'full_response': full_response})}\n\n"

                logger.info(
                    "Chat stream completed",
                    provider=request.llm_provider.value,
                    response_length=len(full_response),
                )

            except Exception as e:
                logger.error("Chat stream failed", error=str(e))
                yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"

        return StreamingResponse(
            generate_stream(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",
                "Access-Control-Allow-Origin": "*",
            },
        )


# Default app instance
app = create_app()


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "src.api.server:app",
        host="0.0.0.0",
        port=8030,
        reload=os.getenv("ENV", "development") == "development",
    )

```

---

## backend/autonomous-crawler-service/src/api/sse.py

```py
"""SSE (Server-Sent Events) Manager for real-time agent status updates."""

import asyncio
import json
import uuid
from datetime import datetime, timezone
from enum import Enum
from typing import Any, Dict

import structlog
from pydantic import BaseModel, Field

logger = structlog.get_logger(__name__)


class SSEEventType(str, Enum):
    """SSE  """

    CONNECTED = "connected"
    AGENT_START = "agent_start"
    AGENT_STEP = "agent_step"
    AGENT_COMPLETE = "agent_complete"
    AGENT_ERROR = "agent_error"
    URL_DISCOVERED = "url_discovered"
    HEALTH_UPDATE = "health_update"
    CAPTCHA_DETECTED = "captcha_detected"
    CAPTCHA_SOLVED = "captcha_solved"
    #    
    COLLECTION_START = "collection_start"
    COLLECTION_PROGRESS = "collection_progress"
    COLLECTION_COMPLETE = "collection_complete"
    COLLECTION_ERROR = "collection_error"
    COLLECTION_LOG = "collection_log"


class SSEEvent(BaseModel):
    """SSE  """

    type: SSEEventType
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    timestamp: str = Field(default_factory=lambda: datetime.now(timezone.utc).isoformat())
    data: Dict[str, Any] = {}


class SSEManager:
    """
    SSE   .

          .
    """

    def __init__(self, max_queue_size: int = 100):
        self._clients: Dict[str, asyncio.Queue] = {}
        self._lock = asyncio.Lock()
        self._max_queue_size = max_queue_size

    @property
    def client_count(self) -> int:
        """   """
        return len(self._clients)

    @property
    def client_ids(self) -> list[str]:
        """  ID """
        return list(self._clients.keys())

    async def connect(self, client_id: str | None = None) -> tuple[str, asyncio.Queue]:
        """
          .

        Args:
            client_id:  ID (  )

        Returns:
            (client_id, queue) 
        """
        if client_id is None:
            client_id = str(uuid.uuid4())

        queue: asyncio.Queue = asyncio.Queue(maxsize=self._max_queue_size)

        async with self._lock:
            self._clients[client_id] = queue

        logger.info(
            "SSE client connected",
            client_id=client_id,
            total_clients=len(self._clients),
        )

        return client_id, queue

    async def disconnect(self, client_id: str) -> None:
        """  """
        async with self._lock:
            if client_id in self._clients:
                del self._clients[client_id]
                logger.info(
                    "SSE client disconnected",
                    client_id=client_id,
                    total_clients=len(self._clients),
                )

    async def broadcast(self, event: SSEEvent) -> None:
        """
            .

        Args:
            event:  SSE 
        """
        disconnected = []

        async with self._lock:
            for client_id, queue in self._clients.items():
                try:
                    queue.put_nowait(event)
                except asyncio.QueueFull:
                    logger.warning("SSE queue full", client_id=client_id)
                except Exception as e:
                    logger.warning(
                        "SSE broadcast error",
                        client_id=client_id,
                        error=str(e),
                    )
                    disconnected.append(client_id)

            #    
            for client_id in disconnected:
                del self._clients[client_id]

    async def send_agent_event(
        self,
        event_type: SSEEventType,
        task_id: str,
        url: str,
        message: str,
        **kwargs,
    ) -> None:
        """
           .

        Args:
            event_type:  
            task_id:  ID
            url:  URL
            message: 
            **kwargs:  
        """
        event = SSEEvent(
            type=event_type,
            data={
                "task_id": task_id,
                "url": url,
                "message": message,
                **kwargs,
            },
        )
        await self.broadcast(event)

    async def send_collection_event(
        self,
        event_type: SSEEventType,
        source_name: str,
        message: str,
        level: str = "INFO",
        **kwargs,
    ) -> None:
        """
             .

        Args:
            event_type:   (COLLECTION_*)
            source_name:    (: )
            message:  
            level:   (DEBUG, INFO, WARNING, ERROR)
            **kwargs:  
        """
        event = SSEEvent(
            type=event_type,
            data={
                "source": source_name,
                "message": message,
                "level": level,
                **kwargs,
            },
        )
        await self.broadcast(event)


async def sse_event_generator(
    client_id: str,
    queue: asyncio.Queue,
    manager: SSEManager,
    heartbeat_interval: float = 30.0,
):
    """
    SSE   .

    Args:
        client_id:  ID
        queue:  
        manager: SSE 
        heartbeat_interval:   ()

    Yields:
        SSE   
    """
    try:
        #   
        connected_event = SSEEvent(
            type=SSEEventType.CONNECTED,
            data={
                "client_id": client_id,
                "message": "Autonomous Crawler SSE connected",
                "active_clients": manager.client_count,
            },
        )
        yield f"event: {connected_event.type.value}\ndata: {json.dumps(connected_event.model_dump())}\n\n"

        while True:
            try:
                #    
                event = await asyncio.wait_for(queue.get(), timeout=heartbeat_interval)
                yield f"event: {event.type.value}\ndata: {json.dumps(event.model_dump())}\n\n"
            except asyncio.TimeoutError:
                # Heartbeat 
                yield ": heartbeat\n\n"

    except asyncio.CancelledError:
        logger.info("SSE client stream cancelled", client_id=client_id)
    finally:
        await manager.disconnect(client_id)


#  
_sse_manager: SSEManager | None = None


def get_sse_manager() -> SSEManager:
    """ SSE   """
    global _sse_manager
    if _sse_manager is None:
        _sse_manager = SSEManager()
    return _sse_manager

```

---

## backend/autonomous-crawler-service/src/auth/__init__.py

```py
"""
Authentication module for autonomous-crawler-service.
"""

from .middleware import AuthMiddleware, get_current_user, require_auth
from .jwt_utils import verify_jwt_token, JWTPayload

__all__ = [
    "AuthMiddleware",
    "get_current_user",
    "require_auth",
    "verify_jwt_token",
    "JWTPayload",
]

```

---

## backend/autonomous-crawler-service/src/auth/jwt_utils.py

```py
"""
JWT Utilities for autonomous-crawler-service.
"""

import os
from dataclasses import dataclass
from typing import Optional

import jwt
import structlog

logger = structlog.get_logger(__name__)

# Secret key for JWT verification (shared with admin-dashboard)
JWT_SECRET = os.getenv("ADMIN_SECRET_KEY", "your-secret-key-change-in-production")
JWT_ALGORITHM = "HS256"


@dataclass
class JWTPayload:
    """JWT Token Payload"""
    user_id: str
    username: str
    role: str
    exp: int
    iat: int


def verify_jwt_token(token: str) -> Optional[JWTPayload]:
    """
    Verify JWT token and return payload.
    
    Args:
        token: JWT token string (without 'Bearer ' prefix)
        
    Returns:
        JWTPayload if valid, None otherwise
    """
    try:
        payload = jwt.decode(
            token,
            JWT_SECRET,
            algorithms=[JWT_ALGORITHM],
        )
        
        return JWTPayload(
            user_id=payload.get("sub", ""),
            username=payload.get("username", ""),
            role=payload.get("role", "user"),
            exp=payload.get("exp", 0),
            iat=payload.get("iat", 0),
        )
        
    except jwt.ExpiredSignatureError:
        logger.warning("JWT token expired")
        return None
    except jwt.InvalidTokenError as e:
        logger.warning("Invalid JWT token", error=str(e))
        return None
    except Exception as e:
        logger.error("JWT verification error", error=str(e))
        return None

```

---

## backend/autonomous-crawler-service/src/auth/middleware.py

```py
"""
Authentication Middleware for autonomous-crawler-service.

Provides FastAPI dependencies for JWT-based authentication.
"""

import os
from functools import wraps
from typing import Callable, Optional

import structlog
from fastapi import Depends, HTTPException, Request, status
from fastapi.security import HTTPAuthorizationCredentials, HTTPBearer

from .jwt_utils import JWTPayload, verify_jwt_token

logger = structlog.get_logger(__name__)

# Security enabled flag - set to False only in development
SECURITY_ENABLED = os.getenv("SECURITY_ENABLED", "true").lower() == "true"

# HTTP Bearer scheme for extracting tokens
bearer_scheme = HTTPBearer(auto_error=False)


class AuthMiddleware:
    """
    Authentication middleware for FastAPI.
    
    Usage:
        app.add_middleware(AuthMiddleware)
    
    Or use the dependency injection approach with get_current_user.
    """
    
    # Endpoints that don't require authentication
    PUBLIC_PATHS = {
        "/health",
        "/",
        "/docs",
        "/openapi.json",
        "/redoc",
    }
    
    # Path prefixes that don't require authentication
    PUBLIC_PREFIXES = (
        "/health",
        "/docs",
        "/openapi",
        "/redoc",
    )

    def __init__(self, app):
        self.app = app

    async def __call__(self, scope, receive, send):
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return
            
        path = scope.get("path", "")
        
        # Skip auth for public paths
        if path in self.PUBLIC_PATHS or path.startswith(self.PUBLIC_PREFIXES):
            await self.app(scope, receive, send)
            return
            
        # Skip auth if disabled
        if not SECURITY_ENABLED:
            await self.app(scope, receive, send)
            return
            
        await self.app(scope, receive, send)


async def get_current_user(
    request: Request,
    credentials: Optional[HTTPAuthorizationCredentials] = Depends(bearer_scheme),
) -> Optional[JWTPayload]:
    """
    FastAPI dependency to get the current authenticated user.
    
    Returns None if:
    - Security is disabled
    - No token is provided
    - Token is invalid
    
    Usage:
        @app.get("/protected")
        async def protected_endpoint(user: JWTPayload = Depends(get_current_user)):
            if user is None:
                raise HTTPException(status_code=401)
            return {"user": user.username}
    """
    if not SECURITY_ENABLED:
        # Return a dummy user when security is disabled
        return JWTPayload(
            user_id="dev-user",
            username="developer",
            role="admin",
            exp=0,
            iat=0,
        )
    
    if credentials is None:
        return None
        
    token = credentials.credentials
    return verify_jwt_token(token)


def require_auth(
    roles: Optional[list[str]] = None,
) -> Callable:
    """
    FastAPI dependency factory that requires authentication.
    
    Args:
        roles: Optional list of required roles (e.g., ["admin", "operator"])
        
    Usage:
        @app.get("/admin-only")
        async def admin_endpoint(user: JWTPayload = Depends(require_auth(roles=["admin"]))):
            return {"message": "Admin access granted"}
            
        @app.get("/authenticated")
        async def auth_endpoint(user: JWTPayload = Depends(require_auth())):
            return {"message": f"Hello {user.username}"}
    """
    async def dependency(
        user: Optional[JWTPayload] = Depends(get_current_user),
    ) -> JWTPayload:
        if user is None:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Authentication required",
                headers={"WWW-Authenticate": "Bearer"},
            )
        
        if roles:
            user_role = user.role.lower()
            allowed_roles = [r.lower() for r in roles]
            
            # Admin has access to everything
            if user_role == "admin":
                return user
                
            if user_role not in allowed_roles:
                raise HTTPException(
                    status_code=status.HTTP_403_FORBIDDEN,
                    detail=f"Requires one of the following roles: {', '.join(roles)}",
                )
        
        return user
    
    return dependency


def require_admin() -> Callable:
    """Shorthand for require_auth(roles=["admin"])"""
    return require_auth(roles=["admin"])


def require_operator() -> Callable:
    """Shorthand for require_auth(roles=["admin", "operator"])"""
    return require_auth(roles=["admin", "operator"])

```

---

## backend/autonomous-crawler-service/src/captcha/__init__.py

```py
"""CAPTCHA solving module using open-source solutions."""

import asyncio
import base64
import os
import tempfile
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum
from typing import Any

import structlog

logger = structlog.get_logger(__name__)


# 
# Core Types (defined first to avoid circular imports)
# 


class CaptchaType(str, Enum):
    """Types of CAPTCHAs."""

    RECAPTCHA_V2 = "recaptcha_v2"
    RECAPTCHA_V3 = "recaptcha_v3"
    HCAPTCHA = "hcaptcha"
    IMAGE = "image"
    AUDIO = "audio"
    CLOUDFLARE = "cloudflare"


@dataclass
class CaptchaSolution:
    """Result of CAPTCHA solving attempt."""

    success: bool
    token: str | None = None
    error: str | None = None
    solver_used: str | None = None
    time_ms: float = 0


class CaptchaSolver(ABC):
    """Abstract base class for CAPTCHA solvers."""

    @property
    @abstractmethod
    def name(self) -> str:
        """Solver name."""
        pass

    @abstractmethod
    async def solve(
        self,
        captcha_type: CaptchaType,
        **kwargs,
    ) -> CaptchaSolution:
        """Solve a CAPTCHA."""
        pass

    @abstractmethod
    async def health_check(self) -> bool:
        """Check if solver is available."""
        pass


# 
# Import submodules (after core types are defined)
# 

from src.captcha.stealth import (
    StealthConfig,
    EnhancedStealthConfig,
    apply_stealth_to_playwright_async,
    get_undetected_browser_args,
    get_nopecha_extension_path,
    get_stealth_browser_args_with_extensions,
)
from src.captcha.nopecha import (
    NopeCHAConfig,
    NopeCHAExtensionManager,
    NopeCHAAPI,
    solve_captcha_with_nopecha,
)
from src.captcha.undetected import (
    UndetectedConfig,
    AdvancedStealthPatcher,
    HumanBehaviorSimulator,
    create_undetected_driver,
    get_enhanced_browser_args,
)
from src.captcha.camoufox_driver import (
    CamoufoxConfig,
    CamoufoxHelper,
    create_camoufox_browser,
    create_camoufox_browser_sync,
    get_recommended_camoufox_config,
    is_camoufox_available,
)
from src.captcha.paid_solvers import (
    CapSolverClient,
    CapSolverConfig,
    TwoCaptchaClient,
    TwoCaptchaConfig,
    create_paid_solver,
)

# Re-export for convenience
__all__ = [
    # Core CAPTCHA types and solvers
    "CaptchaType",
    "CaptchaSolution",
    "CaptchaSolver",
    "CaptchaSolverOrchestrator",
    "AudioRecaptchaSolver",
    "HCaptchaChallenger",
    "CloudflareBypasser",
    # Stealth
    "StealthConfig",
    "EnhancedStealthConfig",
    "apply_stealth_to_playwright_async",
    "get_undetected_browser_args",
    "get_nopecha_extension_path",
    "get_stealth_browser_args_with_extensions",
    # NopeCHA
    "NopeCHAConfig",
    "NopeCHAExtensionManager",
    "NopeCHAAPI",
    "solve_captcha_with_nopecha",
    # Undetected ChromeDriver
    "UndetectedConfig",
    "AdvancedStealthPatcher",
    "HumanBehaviorSimulator",
    "create_undetected_driver",
    "get_enhanced_browser_args",
    # Camoufox
    "CamoufoxConfig",
    "CamoufoxHelper",
    "create_camoufox_browser",
    "create_camoufox_browser_sync",
    "get_recommended_camoufox_config",
    "is_camoufox_available",
    # Paid solvers
    "CapSolverClient",
    "CapSolverConfig",
    "TwoCaptchaClient",
    "TwoCaptchaConfig",
    "create_paid_solver",
]


class AudioRecaptchaSolver(CaptchaSolver):
    """
    reCAPTCHA solver using audio challenge (GoogleRecaptchaBypass approach).

    Uses speech recognition to solve audio challenges.
    Requires: speech_recognition, pydub, ffmpeg
    """

    def __init__(self):
        self._sr = None
        self._pydub = None

    @property
    def name(self) -> str:
        return "audio_recaptcha"

    async def _lazy_import(self):
        """Lazy import heavy dependencies."""
        if self._sr is None:
            try:
                import speech_recognition as sr
                from pydub import AudioSegment

                self._sr = sr
                self._pydub = AudioSegment
            except ImportError as e:
                logger.error(
                    "Missing dependencies for audio solver",
                    error=str(e),
                    hint="pip install SpeechRecognition pydub",
                )
                raise

    async def solve(
        self,
        captcha_type: CaptchaType,
        audio_data: bytes | None = None,
        audio_url: str | None = None,
        **kwargs,
    ) -> CaptchaSolution:
        """
        Solve reCAPTCHA using audio challenge.

        Args:
            captcha_type: Must be RECAPTCHA_V2 or AUDIO
            audio_data: Raw audio bytes
            audio_url: URL to download audio from
        """
        import time

        start = time.time()

        if captcha_type not in (CaptchaType.RECAPTCHA_V2, CaptchaType.AUDIO):
            return CaptchaSolution(
                success=False,
                error=f"Unsupported captcha type: {captcha_type}",
                solver_used=self.name,
            )

        try:
            await self._lazy_import()

            # Get audio data
            if audio_url and not audio_data:
                import httpx

                async with httpx.AsyncClient() as client:
                    resp = await client.get(audio_url)
                    audio_data = resp.content

            if not audio_data:
                return CaptchaSolution(
                    success=False,
                    error="No audio data provided",
                    solver_used=self.name,
                )

            # Convert and recognize
            text = await self._recognize_audio(audio_data)

            if text:
                return CaptchaSolution(
                    success=True,
                    token=text,
                    solver_used=self.name,
                    time_ms=(time.time() - start) * 1000,
                )
            else:
                return CaptchaSolution(
                    success=False,
                    error="Could not recognize audio",
                    solver_used=self.name,
                    time_ms=(time.time() - start) * 1000,
                )

        except Exception as e:
            return CaptchaSolution(
                success=False,
                error=str(e),
                solver_used=self.name,
                time_ms=(time.time() - start) * 1000,
            )

    async def _recognize_audio(self, audio_data: bytes) -> str | None:
        """Convert audio to text using speech recognition."""
        with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as f:
            f.write(audio_data)
            mp3_path = f.name

        wav_path = mp3_path.replace(".mp3", ".wav")

        try:
            # Convert MP3 to WAV
            audio = self._pydub.from_mp3(mp3_path)
            audio.export(wav_path, format="wav")

            # Recognize speech
            recognizer = self._sr.Recognizer()
            with self._sr.AudioFile(wav_path) as source:
                audio_data = recognizer.record(source)

            # Try Google Speech Recognition (free)
            try:
                text = recognizer.recognize_google(audio_data)
                return text
            except self._sr.UnknownValueError:
                logger.warning("Google Speech Recognition could not understand audio")
                return None

        finally:
            # Cleanup
            for path in [mp3_path, wav_path]:
                if os.path.exists(path):
                    os.remove(path)

    async def health_check(self) -> bool:
        """Check if dependencies are available."""
        try:
            await self._lazy_import()
            return True
        except Exception:
            return False


class HCaptchaChallenger(CaptchaSolver):
    """
    hCaptcha solver using hcaptcha-challenger library.

    Uses AI models (YOLO, ResNet) to solve image challenges.
    Requires: hcaptcha-challenger
    """

    def __init__(self, model_dir: str | None = None):
        self.model_dir = model_dir
        self._challenger = None

    @property
    def name(self) -> str:
        return "hcaptcha_challenger"

    async def _lazy_import(self):
        """Lazy import hcaptcha-challenger."""
        if self._challenger is None:
            try:
                from hcaptcha_challenger import AgentChallenger

                self._challenger = AgentChallenger
            except ImportError as e:
                logger.error(
                    "Missing hcaptcha-challenger",
                    error=str(e),
                    hint="pip install hcaptcha-challenger",
                )
                raise

    async def solve(
        self,
        captcha_type: CaptchaType,
        page: Any = None,  # Playwright page
        **kwargs,
    ) -> CaptchaSolution:
        """
        Solve hCaptcha on a Playwright page.

        Args:
            captcha_type: Must be HCAPTCHA
            page: Playwright page object
        """
        import time

        start = time.time()

        if captcha_type != CaptchaType.HCAPTCHA:
            return CaptchaSolution(
                success=False,
                error=f"Unsupported captcha type: {captcha_type}",
                solver_used=self.name,
            )

        if not page:
            return CaptchaSolution(
                success=False,
                error="Playwright page required",
                solver_used=self.name,
            )

        try:
            await self._lazy_import()

            challenger = self._challenger(page)
            result = await challenger.solve()

            return CaptchaSolution(
                success=result,
                solver_used=self.name,
                time_ms=(time.time() - start) * 1000,
            )

        except Exception as e:
            return CaptchaSolution(
                success=False,
                error=str(e),
                solver_used=self.name,
                time_ms=(time.time() - start) * 1000,
            )

    async def health_check(self) -> bool:
        """Check if library is available."""
        try:
            await self._lazy_import()
            return True
        except Exception:
            return False


class CloudflareBypasser(CaptchaSolver):
    """
    Cloudflare bypass using cloudscraper library.

    Handles Cloudflare's JavaScript challenges and Turnstile.
    Requires: cloudscraper
    """

    def __init__(self):
        self._scraper = None

    @property
    def name(self) -> str:
        return "cloudscraper"

    def _get_scraper(self):
        """Get or create cloudscraper instance."""
        if self._scraper is None:
            try:
                import cloudscraper

                self._scraper = cloudscraper.create_scraper(
                    browser={
                        "browser": "chrome",
                        "platform": "windows",
                        "mobile": False,
                    },
                    delay=10,
                )
            except ImportError as e:
                logger.error(
                    "Missing cloudscraper",
                    error=str(e),
                    hint="pip install cloudscraper",
                )
                raise
        return self._scraper

    async def solve(
        self,
        captcha_type: CaptchaType,
        url: str | None = None,
        **kwargs,
    ) -> CaptchaSolution:
        """
        Bypass Cloudflare protection.

        Args:
            captcha_type: Must be CLOUDFLARE
            url: URL to access through Cloudflare
        """
        import time

        start = time.time()

        if captcha_type != CaptchaType.CLOUDFLARE:
            return CaptchaSolution(
                success=False,
                error=f"Unsupported captcha type: {captcha_type}",
                solver_used=self.name,
            )

        if not url:
            return CaptchaSolution(
                success=False,
                error="URL required",
                solver_used=self.name,
            )

        try:
            scraper = self._get_scraper()

            # Execute in thread pool since cloudscraper is synchronous
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: scraper.get(url),
            )

            if response.status_code == 200:
                return CaptchaSolution(
                    success=True,
                    token=response.text[:100],  # First 100 chars as verification
                    solver_used=self.name,
                    time_ms=(time.time() - start) * 1000,
                )
            else:
                return CaptchaSolution(
                    success=False,
                    error=f"HTTP {response.status_code}",
                    solver_used=self.name,
                    time_ms=(time.time() - start) * 1000,
                )

        except Exception as e:
            return CaptchaSolution(
                success=False,
                error=str(e),
                solver_used=self.name,
                time_ms=(time.time() - start) * 1000,
            )

    async def get_session_cookies(self, url: str) -> dict[str, str]:
        """Get Cloudflare bypass cookies for a URL."""
        try:
            scraper = self._get_scraper()
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, lambda: scraper.get(url))
            return dict(scraper.cookies)
        except Exception as e:
            logger.error("Failed to get Cloudflare cookies", url=url, error=str(e))
            return {}

    async def health_check(self) -> bool:
        """Check if cloudscraper is available."""
        try:
            self._get_scraper()
            return True
        except Exception:
            return False


class CaptchaSolverOrchestrator:
    """
    Orchestrates multiple CAPTCHA solvers.

    Tries different solvers based on CAPTCHA type and availability.
    Supports both free and paid solvers, with paid solvers prioritized when configured.
    """

    def __init__(
        self,
        capsolver_api_key: str = "",
        twocaptcha_api_key: str = "",
        prefer_paid: bool = True,
        paid_timeout: float = 120.0,
    ):
        """
        Initialize CAPTCHA solver orchestrator.

        Args:
            capsolver_api_key: CapSolver API key (recommended for Turnstile)
            twocaptcha_api_key: 2Captcha API key
            prefer_paid: If True, try paid solvers first when available
            paid_timeout: Timeout for paid solver requests
        """
        self.prefer_paid = prefer_paid

        # Initialize free solvers
        free_solvers: dict[CaptchaType, list[CaptchaSolver]] = {
            CaptchaType.RECAPTCHA_V2: [AudioRecaptchaSolver()],
            CaptchaType.AUDIO: [AudioRecaptchaSolver()],
            CaptchaType.HCAPTCHA: [HCaptchaChallenger()],
            CaptchaType.CLOUDFLARE: [CloudflareBypasser()],
        }

        # Initialize paid solvers if API keys provided
        paid_solvers: dict[CaptchaType, list[CaptchaSolver]] = {}

        if capsolver_api_key:
            capsolver = CapSolverClient(
                CapSolverConfig(api_key=capsolver_api_key, timeout=paid_timeout)
            )
            # CapSolver supports all major CAPTCHA types
            for ctype in [
                CaptchaType.RECAPTCHA_V2,
                CaptchaType.RECAPTCHA_V3,
                CaptchaType.HCAPTCHA,
                CaptchaType.CLOUDFLARE,
            ]:
                if ctype not in paid_solvers:
                    paid_solvers[ctype] = []
                paid_solvers[ctype].append(capsolver)
            logger.info("CapSolver enabled for CAPTCHA solving")

        if twocaptcha_api_key:
            twocaptcha = TwoCaptchaClient(
                TwoCaptchaConfig(api_key=twocaptcha_api_key, timeout=paid_timeout)
            )
            # 2Captcha also supports all major types
            for ctype in [
                CaptchaType.RECAPTCHA_V2,
                CaptchaType.RECAPTCHA_V3,
                CaptchaType.HCAPTCHA,
                CaptchaType.CLOUDFLARE,
            ]:
                if ctype not in paid_solvers:
                    paid_solvers[ctype] = []
                paid_solvers[ctype].append(twocaptcha)
            logger.info("2Captcha enabled for CAPTCHA solving")

        # Combine solvers: paid first if prefer_paid, else free first
        self.solvers: dict[CaptchaType, list[CaptchaSolver]] = {}
        all_types = set(free_solvers.keys()) | set(paid_solvers.keys())

        for ctype in all_types:
            paid = paid_solvers.get(ctype, [])
            free = free_solvers.get(ctype, [])

            if prefer_paid:
                self.solvers[ctype] = paid + free
            else:
                self.solvers[ctype] = free + paid

        logger.info(
            "CAPTCHA solver orchestrator initialized",
            paid_solvers_enabled=bool(capsolver_api_key or twocaptcha_api_key),
            prefer_paid=prefer_paid,
            supported_types=list(self.solvers.keys()),
        )

    def add_solver(self, captcha_type: CaptchaType, solver: CaptchaSolver):
        """Add a solver for a captcha type."""
        if captcha_type not in self.solvers:
            self.solvers[captcha_type] = []
        self.solvers[captcha_type].append(solver)

    async def solve(
        self,
        captcha_type: CaptchaType,
        **kwargs,
    ) -> CaptchaSolution:
        """
        Try to solve CAPTCHA using available solvers.

        Args:
            captcha_type: Type of CAPTCHA
            **kwargs: Solver-specific arguments

        Returns:
            CaptchaSolution with result
        """
        solvers = self.solvers.get(captcha_type, [])

        if not solvers:
            return CaptchaSolution(
                success=False,
                error=f"No solver available for {captcha_type}",
            )

        errors = []
        for solver in solvers:
            try:
                # Check health first
                if not await solver.health_check():
                    errors.append(f"{solver.name}: not available")
                    continue

                result = await solver.solve(captcha_type, **kwargs)

                if result.success:
                    logger.info(
                        "CAPTCHA solved",
                        captcha_type=captcha_type.value,
                        solver=solver.name,
                        time_ms=result.time_ms,
                    )
                    return result
                else:
                    errors.append(f"{solver.name}: {result.error}")

            except Exception as e:
                errors.append(f"{solver.name}: {str(e)}")

        return CaptchaSolution(
            success=False,
            error=f"All solvers failed: {'; '.join(errors)}",
        )

    async def get_available_solvers(self) -> dict[str, list[str]]:
        """Get list of available solvers by captcha type."""
        available = {}
        for captcha_type, solvers in self.solvers.items():
            available[captcha_type.value] = []
            for solver in solvers:
                if await solver.health_check():
                    available[captcha_type.value].append(solver.name)
        return available

```

---

## backend/autonomous-crawler-service/src/captcha/camoufox_driver.py

```py
"""
Camoufox Firefox-based Anti-Detect Browser Integration.

Camoufox is a Firefox-based anti-detect browser that provides:
- Advanced fingerprint spoofing
- Human-like behavior simulation
- Cloudflare Turnstile bypass
- Compatible with Playwright API

Installation:
    pip install camoufox[geoip]
    python -m camoufox fetch
"""

import asyncio
from dataclasses import dataclass, field
from typing import Any

import structlog

logger = structlog.get_logger(__name__)


@dataclass
class CamoufoxConfig:
    """Configuration for Camoufox browser."""
    
    # Display mode
    headless: bool = True
    
    # Human-like behavior simulation
    humanize: bool = True
    humanize_level: int = 2  # 1-3, higher = more human-like
    
    # Fingerprint options
    os: str | None = None  # "windows", "macos", "linux" or None for random
    screen_width: int | None = None
    screen_height: int | None = None
    
    # Locale/timezone
    locale: str = "ko-KR"
    timezone: str = "Asia/Seoul"
    
    # Geolocation (requires geoip addon)
    geoip: bool = True
    
    # Proxy
    proxy: str | None = None
    
    # Browser settings
    block_images: bool = False
    block_webrtc: bool = True
    
    # Extra Firefox preferences
    firefox_prefs: dict[str, Any] = field(default_factory=dict)


def is_camoufox_available() -> bool:
    """Check if Camoufox is installed and available."""
    try:
        import camoufox
        return True
    except ImportError:
        return False


async def create_camoufox_browser(
    config: CamoufoxConfig | None = None,
) -> Any:
    """
    Create a Camoufox browser instance using async API.
    
    Args:
        config: Camoufox configuration
        
    Returns:
        Browser context or None if not available
    """
    if config is None:
        config = CamoufoxConfig()
    
    if not is_camoufox_available():
        logger.warning("Camoufox not installed. Install with: pip install camoufox[geoip]")
        return None
    
    try:
        from camoufox.async_api import AsyncCamoufox
        
        # Build kwargs
        kwargs = {
            "headless": config.headless,
            "humanize": config.humanize,
        }
        
        if config.os:
            kwargs["os"] = config.os
        
        if config.screen_width and config.screen_height:
            kwargs["screen"] = {"width": config.screen_width, "height": config.screen_height}
        
        if config.locale:
            kwargs["locale"] = config.locale
            
        if config.timezone:
            kwargs["timezone"] = config.timezone
        
        if config.geoip:
            kwargs["geoip"] = True
        
        if config.proxy:
            kwargs["proxy"] = {"server": config.proxy}
        
        if config.block_webrtc:
            kwargs["block_webrtc"] = True
        
        if config.block_images:
            kwargs["block_images"] = True
        
        # Apply extra Firefox preferences
        if config.firefox_prefs:
            kwargs["firefox_prefs"] = config.firefox_prefs
        
        # Create browser
        camoufox = AsyncCamoufox(**kwargs)
        browser = await camoufox.__aenter__()
        
        logger.info("Created Camoufox browser", headless=config.headless, humanize=config.humanize)
        return browser
        
    except Exception as e:
        logger.error("Failed to create Camoufox browser", error=str(e))
        return None


def create_camoufox_browser_sync(
    config: CamoufoxConfig | None = None,
) -> Any:
    """
    Create a Camoufox browser instance using sync API.
    
    Args:
        config: Camoufox configuration
        
    Returns:
        Browser context or None if not available
    """
    if config is None:
        config = CamoufoxConfig()
    
    if not is_camoufox_available():
        logger.warning("Camoufox not installed")
        return None
    
    try:
        from camoufox.sync_api import Camoufox
        
        kwargs = {
            "headless": config.headless,
            "humanize": config.humanize,
        }
        
        if config.os:
            kwargs["os"] = config.os
        
        if config.locale:
            kwargs["locale"] = config.locale
            
        if config.timezone:
            kwargs["timezone"] = config.timezone
        
        if config.geoip:
            kwargs["geoip"] = True
        
        if config.proxy:
            kwargs["proxy"] = {"server": config.proxy}
        
        if config.block_webrtc:
            kwargs["block_webrtc"] = True
        
        camoufox = Camoufox(**kwargs)
        browser = camoufox.__enter__()
        
        logger.info("Created Camoufox browser (sync)", headless=config.headless)
        return browser
        
    except Exception as e:
        logger.error("Failed to create Camoufox browser", error=str(e))
        return None


class CamoufoxHelper:
    """Helper utilities for Camoufox browser automation."""
    
    @staticmethod
    async def wait_for_cloudflare(page: Any, timeout: int = 30) -> bool:
        """
        Wait for Cloudflare challenge to complete.
        
        Camoufox handles Cloudflare automatically in most cases,
        but this provides explicit waiting if needed.
        
        Args:
            page: Camoufox page object
            timeout: Maximum wait time in seconds
            
        Returns:
            True if challenge passed, False if timeout
        """
        try:
            # Common Cloudflare challenge indicators
            challenge_selectors = [
                "#challenge-running",
                "#challenge-stage",
                ".cf-browser-verification",
                "#trk_jschal_js",
            ]
            
            start_time = asyncio.get_event_loop().time()
            
            while asyncio.get_event_loop().time() - start_time < timeout:
                # Check if any challenge elements are visible
                is_challenging = False
                
                for selector in challenge_selectors:
                    try:
                        element = await page.query_selector(selector)
                        if element:
                            is_visible = await element.is_visible()
                            if is_visible:
                                is_challenging = True
                                break
                    except Exception:
                        continue
                
                if not is_challenging:
                    # No challenge visible, likely passed
                    logger.debug("Cloudflare challenge completed")
                    return True
                
                await asyncio.sleep(0.5)
            
            logger.warning("Cloudflare challenge timeout")
            return False
            
        except Exception as e:
            logger.error("Error waiting for Cloudflare", error=str(e))
            return False
    
    @staticmethod
    async def solve_turnstile(page: Any, timeout: int = 30) -> bool:
        """
        Wait for Cloudflare Turnstile CAPTCHA to complete.
        
        Camoufox with humanize=True should handle most Turnstile
        challenges automatically.
        
        Args:
            page: Camoufox page object
            timeout: Maximum wait time in seconds
            
        Returns:
            True if solved, False if timeout
        """
        try:
            turnstile_selectors = [
                "iframe[src*='turnstile']",
                "#cf-turnstile",
                ".cf-turnstile",
            ]
            
            start_time = asyncio.get_event_loop().time()
            
            # First, wait for turnstile to appear
            turnstile_frame = None
            while asyncio.get_event_loop().time() - start_time < timeout / 2:
                for selector in turnstile_selectors:
                    try:
                        element = await page.query_selector(selector)
                        if element:
                            turnstile_frame = element
                            break
                    except Exception:
                        continue
                
                if turnstile_frame:
                    break
                    
                await asyncio.sleep(0.3)
            
            if not turnstile_frame:
                # No turnstile found, may not be needed
                logger.debug("No Turnstile CAPTCHA found")
                return True
            
            logger.debug("Turnstile CAPTCHA detected, waiting for auto-solve...")
            
            # Wait for turnstile to complete (it should auto-solve with humanize)
            # Check for success indicator or turnstile disappearing
            while asyncio.get_event_loop().time() - start_time < timeout:
                # Check if turnstile is still visible
                try:
                    is_visible = await turnstile_frame.is_visible()
                    if not is_visible:
                        logger.info("Turnstile CAPTCHA solved")
                        return True
                except Exception:
                    # Element may have been removed
                    return True
                
                # Check for success response in page
                try:
                    response = await page.evaluate("""
                        () => {
                            const input = document.querySelector('[name="cf-turnstile-response"]');
                            return input ? input.value : null;
                        }
                    """)
                    if response:
                        logger.info("Turnstile response received")
                        return True
                except Exception:
                    pass
                
                await asyncio.sleep(0.5)
            
            logger.warning("Turnstile solve timeout")
            return False
            
        except Exception as e:
            logger.error("Error solving Turnstile", error=str(e))
            return False
    
    @staticmethod
    async def extract_page_content(page: Any) -> dict[str, Any]:
        """
        Extract main content from a page.
        
        Args:
            page: Camoufox page object
            
        Returns:
            Dictionary with extracted content
        """
        try:
            content = await page.evaluate("""
                () => {
                    const result = {
                        title: document.title,
                        url: window.location.href,
                        text: '',
                        links: [],
                        images: [],
                    };
                    
                    // Get main text content
                    const article = document.querySelector('article') || 
                                   document.querySelector('main') || 
                                   document.body;
                    
                    if (article) {
                        result.text = article.innerText;
                    }
                    
                    // Get links
                    const links = document.querySelectorAll('a[href]');
                    links.forEach(link => {
                        if (link.href && link.href.startsWith('http')) {
                            result.links.push({
                                href: link.href,
                                text: link.innerText.trim().substring(0, 200)
                            });
                        }
                    });
                    
                    // Get images
                    const images = document.querySelectorAll('img[src]');
                    images.forEach(img => {
                        if (img.src && img.src.startsWith('http')) {
                            result.images.push({
                                src: img.src,
                                alt: img.alt || ''
                            });
                        }
                    });
                    
                    return result;
                }
            """)
            
            return content
            
        except Exception as e:
            logger.error("Failed to extract page content", error=str(e))
            return {"error": str(e)}


# Firefox-specific preferences for anti-detection
FIREFOX_ANTI_DETECT_PREFS = {
    # Disable WebRTC IP leak
    "media.peerconnection.enabled": False,
    "media.peerconnection.ice.no_host": True,
    "media.peerconnection.ice.default_address_only": True,
    
    # Disable tracking
    "privacy.trackingprotection.enabled": True,
    "privacy.trackingprotection.socialtracking.enabled": True,
    
    # Fingerprint resistance
    "privacy.resistFingerprinting": False,  # Camoufox handles this
    
    # Disable telemetry
    "toolkit.telemetry.enabled": False,
    "toolkit.telemetry.unified": False,
    "toolkit.telemetry.archive.enabled": False,
    
    # Disable crash reporter
    "breakpad.reportURL": "",
    "browser.crashReports.unsubmittedCheck.autoSubmit2": False,
    
    # Disable prefetch
    "network.prefetch-next": False,
    "network.dns.disablePrefetch": True,
    
    # Disable speculative connections
    "network.http.speculative-parallel-limit": 0,
    
    # Improve privacy
    "dom.battery.enabled": False,
    "geo.enabled": False,
    "media.navigator.enabled": False,
    
    # Performance
    "browser.cache.disk.enable": False,
    "browser.cache.memory.enable": True,
}


def get_recommended_camoufox_config(
    purpose: str = "general",
    headless: bool = True,
) -> CamoufoxConfig:
    """
    Get recommended Camoufox configuration for different purposes.
    
    Args:
        purpose: "general", "scraping", "turnstile", "cloudflare"
        headless: Whether to run headless
        
    Returns:
        Optimized CamoufoxConfig
    """
    base_config = CamoufoxConfig(
        headless=headless,
        humanize=True,
        humanize_level=2,
        locale="ko-KR",
        timezone="Asia/Seoul",
        geoip=True,
        block_webrtc=True,
    )
    
    if purpose == "scraping":
        base_config.block_images = True
        base_config.humanize_level = 1
    
    elif purpose == "turnstile":
        base_config.humanize_level = 3
        base_config.block_images = False
    
    elif purpose == "cloudflare":
        base_config.humanize_level = 3
        base_config.block_images = False
        base_config.firefox_prefs = FIREFOX_ANTI_DETECT_PREFS
    
    return base_config

```

---

## backend/autonomous-crawler-service/src/captcha/nopecha.py

```py
"""
NopeCHA CAPTCHA Solver Integration.

NopeCHA is a free/open-source CAPTCHA solving extension that supports:
- reCAPTCHA v2/v3
- hCaptcha
- FunCAPTCHA
- AWS WAF
- Turnstile
- Text CAPTCHA

Usage:
1. Browser extension (Chrome/Firefox)
2. API integration for headless automation
"""

import asyncio
import base64
import json
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Literal

import aiohttp
import structlog

logger = structlog.get_logger(__name__)

# NopeCHA Chrome Extension ID
NOPECHA_EXTENSION_ID = "dknlfmjaanfblgfdfebhijalfmhmjjjo"
NOPECHA_CRX_URL = f"https://clients2.google.com/service/update2/crx?response=redirect&prodversion=133&acceptformat=crx3&x=id%3D{NOPECHA_EXTENSION_ID}%26uc"


@dataclass
class NopeCHAConfig:
    """NopeCHA configuration."""
    
    # API key (optional, for faster solving via API)
    api_key: str = ""
    
    # Extension settings
    enabled: bool = True
    auto_solve: bool = True
    
    # Solve settings
    solve_delay_ms: int = 500
    max_retries: int = 3
    
    # Supported CAPTCHA types
    solve_recaptcha: bool = True
    solve_hcaptcha: bool = True
    solve_funcaptcha: bool = True
    solve_turnstile: bool = True
    solve_text: bool = True
    
    # Audio solving for accessibility (free reCAPTCHA bypass)
    use_audio_challenge: bool = True
    
    # Extension cache directory
    cache_dir: Path = field(default_factory=lambda: Path("/tmp/nopecha-extension"))


class NopeCHAExtensionManager:
    """Manage NopeCHA extension installation and configuration."""
    
    def __init__(self, config: NopeCHAConfig | None = None):
        self.config = config or NopeCHAConfig()
    
    async def download_extension(self) -> Path:
        """Download NopeCHA extension CRX file."""
        cache_dir = self.config.cache_dir
        cache_dir.mkdir(parents=True, exist_ok=True)
        
        crx_path = cache_dir / f"{NOPECHA_EXTENSION_ID}.crx"
        ext_dir = cache_dir / NOPECHA_EXTENSION_ID
        
        # Check if already downloaded and extracted
        if ext_dir.exists() and (ext_dir / "manifest.json").exists():
            logger.debug("NopeCHA extension already cached", path=str(ext_dir))
            return ext_dir
        
        # Download CRX
        logger.info("Downloading NopeCHA extension...")
        async with aiohttp.ClientSession() as session:
            async with session.get(NOPECHA_CRX_URL, allow_redirects=True) as resp:
                if resp.status == 200:
                    content = await resp.read()
                    with open(crx_path, 'wb') as f:
                        f.write(content)
                    logger.info("NopeCHA extension downloaded", path=str(crx_path))
                else:
                    raise Exception(f"Failed to download NopeCHA: HTTP {resp.status}")
        
        # Extract CRX
        await self._extract_crx(crx_path, ext_dir)
        
        # Apply configuration
        await self._configure_extension(ext_dir)
        
        return ext_dir
    
    async def _extract_crx(self, crx_path: Path, extract_dir: Path) -> None:
        """Extract CRX file to directory."""
        import zipfile
        import shutil
        
        if extract_dir.exists():
            shutil.rmtree(extract_dir)
        extract_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            with zipfile.ZipFile(crx_path, 'r') as zip_ref:
                zip_ref.extractall(extract_dir)
        except zipfile.BadZipFile:
            # CRX has a header, skip it
            with open(crx_path, 'rb') as f:
                magic = f.read(4)
                if magic != b'Cr24':
                    raise Exception("Invalid CRX format")
                
                version = int.from_bytes(f.read(4), 'little')
                if version == 2:
                    pubkey_len = int.from_bytes(f.read(4), 'little')
                    sig_len = int.from_bytes(f.read(4), 'little')
                    f.seek(16 + pubkey_len + sig_len)
                elif version == 3:
                    header_len = int.from_bytes(f.read(4), 'little')
                    f.seek(12 + header_len)
                
                zip_data = f.read()
            
            import tempfile
            with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as tmp:
                tmp.write(zip_data)
                tmp.flush()
                with zipfile.ZipFile(tmp.name, 'r') as zip_ref:
                    zip_ref.extractall(extract_dir)
                Path(tmp.name).unlink()
        
        logger.info("NopeCHA extension extracted", path=str(extract_dir))
    
    async def _configure_extension(self, ext_dir: Path) -> None:
        """Configure NopeCHA extension settings."""
        # Create settings file for extension
        settings = {
            "key": self.config.api_key,
            "enabled": self.config.enabled,
            "auto_solve": self.config.auto_solve,
            "delay": self.config.solve_delay_ms,
            "recaptcha": self.config.solve_recaptcha,
            "hcaptcha": self.config.solve_hcaptcha,
            "funcaptcha": self.config.solve_funcaptcha,
            "turnstile": self.config.solve_turnstile,
            "textcaptcha": self.config.solve_text,
            "audio": self.config.use_audio_challenge,
        }
        
        settings_path = ext_dir / "settings.json"
        with open(settings_path, 'w') as f:
            json.dump(settings, f)
        
        logger.debug("NopeCHA settings configured", settings=settings)
    
    def get_extension_path(self) -> str:
        """Get the path to the extracted extension directory."""
        ext_dir = self.config.cache_dir / NOPECHA_EXTENSION_ID
        if ext_dir.exists():
            return str(ext_dir)
        return ""


class NopeCHAAPI:
    """NopeCHA API client for programmatic CAPTCHA solving."""
    
    API_BASE = "https://api.nopecha.com"
    
    def __init__(self, api_key: str = ""):
        self.api_key = api_key
    
    async def solve_recaptcha(
        self,
        site_key: str,
        site_url: str,
        version: Literal["v2", "v3"] = "v2",
        action: str = "",
        invisible: bool = False,
    ) -> str | None:
        """
        Solve reCAPTCHA using NopeCHA API.
        
        Args:
            site_key: reCAPTCHA site key
            site_url: URL of the page with CAPTCHA
            version: reCAPTCHA version (v2 or v3)
            action: Action for v3 scoring
            invisible: Whether CAPTCHA is invisible
            
        Returns:
            Solution token or None if failed
        """
        if not self.api_key:
            logger.warning("NopeCHA API key not configured, using extension mode")
            return None
        
        payload = {
            "key": self.api_key,
            "type": "recaptcha2" if version == "v2" else "recaptcha3",
            "sitekey": site_key,
            "url": site_url,
        }
        
        if version == "v3" and action:
            payload["action"] = action
        if invisible:
            payload["invisible"] = True
        
        return await self._solve(payload)
    
    async def solve_hcaptcha(
        self,
        site_key: str,
        site_url: str,
    ) -> str | None:
        """Solve hCaptcha using NopeCHA API."""
        if not self.api_key:
            return None
        
        payload = {
            "key": self.api_key,
            "type": "hcaptcha",
            "sitekey": site_key,
            "url": site_url,
        }
        
        return await self._solve(payload)
    
    async def solve_turnstile(
        self,
        site_key: str,
        site_url: str,
    ) -> str | None:
        """Solve Cloudflare Turnstile using NopeCHA API."""
        if not self.api_key:
            return None
        
        payload = {
            "key": self.api_key,
            "type": "turnstile",
            "sitekey": site_key,
            "url": site_url,
        }
        
        return await self._solve(payload)
    
    async def solve_image_captcha(
        self,
        image_base64: str,
        captcha_type: str = "text",
    ) -> str | None:
        """Solve image-based CAPTCHA."""
        if not self.api_key:
            return None
        
        payload = {
            "key": self.api_key,
            "type": captcha_type,
            "image": image_base64,
        }
        
        return await self._solve(payload)
    
    async def _solve(self, payload: dict[str, Any]) -> str | None:
        """Send solve request to NopeCHA API."""
        try:
            async with aiohttp.ClientSession() as session:
                # Create task
                async with session.post(
                    f"{self.API_BASE}/",
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=30),
                ) as resp:
                    result = await resp.json()
                    
                    if "error" in result:
                        logger.error("NopeCHA API error", error=result.get("error"))
                        return None
                    
                    task_id = result.get("data")
                    if not task_id:
                        return None
                
                # Poll for result
                for _ in range(60):  # Max 60 seconds
                    await asyncio.sleep(1)
                    
                    async with session.get(
                        f"{self.API_BASE}/?key={self.api_key}&id={task_id}",
                        timeout=aiohttp.ClientTimeout(total=10),
                    ) as poll_resp:
                        poll_result = await poll_resp.json()
                        
                        if "error" in poll_result:
                            error = poll_result.get("error")
                            if error == "Incomplete job":
                                continue
                            logger.error("NopeCHA polling error", error=error)
                            return None
                        
                        if "data" in poll_result:
                            return poll_result["data"]
                
                logger.warning("NopeCHA solve timeout")
                return None
                
        except Exception as e:
            logger.error("NopeCHA API request failed", error=str(e))
            return None


# Helper function for quick CAPTCHA solving
async def solve_captcha_with_nopecha(
    captcha_type: Literal["recaptcha2", "recaptcha3", "hcaptcha", "turnstile"],
    site_key: str,
    site_url: str,
    api_key: str = "",
) -> str | None:
    """
    Quick helper to solve CAPTCHA using NopeCHA.
    
    Args:
        captcha_type: Type of CAPTCHA
        site_key: CAPTCHA site key
        site_url: URL of the page
        api_key: NopeCHA API key (optional)
        
    Returns:
        Solution token or None
    """
    client = NopeCHAAPI(api_key)
    
    if captcha_type == "recaptcha2":
        return await client.solve_recaptcha(site_key, site_url, "v2")
    elif captcha_type == "recaptcha3":
        return await client.solve_recaptcha(site_key, site_url, "v3")
    elif captcha_type == "hcaptcha":
        return await client.solve_hcaptcha(site_key, site_url)
    elif captcha_type == "turnstile":
        return await client.solve_turnstile(site_key, site_url)
    
    return None

```

---

## backend/autonomous-crawler-service/src/captcha/paid_solvers.py

```py
"""
Paid CAPTCHA Solver Integrations.

Integrates with reliable paid CAPTCHA solving services:
- CapSolver (https://capsolver.com) - Recommended, supports Turnstile
- 2Captcha (https://2captcha.com) - Widely used, reliable

These services are more reliable than free solutions for:
- reCAPTCHA v2/v3
- hCaptcha
- Cloudflare Turnstile
- FunCAPTCHA
- Image CAPTCHAs
"""

import asyncio
import time
from dataclasses import dataclass
from typing import Any, Literal

import httpx
import structlog

from src.captcha import CaptchaType, CaptchaSolution, CaptchaSolver

logger = structlog.get_logger(__name__)


# 
# CapSolver Integration
# 


@dataclass
class CapSolverConfig:
    """CapSolver configuration."""

    api_key: str = ""
    base_url: str = "https://api.capsolver.com"
    timeout: float = 120.0
    poll_interval: float = 3.0


class CapSolverClient(CaptchaSolver):
    """
    CapSolver CAPTCHA solving service client.

    Supports:
    - reCAPTCHA v2/v3
    - hCaptcha
    - Cloudflare Turnstile
    - FunCAPTCHA
    - Image CAPTCHA

    Docs: https://docs.capsolver.com/
    """

    def __init__(self, config: CapSolverConfig | None = None):
        self.config = config or CapSolverConfig()
        self._client: httpx.AsyncClient | None = None

    @property
    def name(self) -> str:
        return "capsolver"

    async def _get_client(self) -> httpx.AsyncClient:
        if self._client is None:
            self._client = httpx.AsyncClient(
                base_url=self.config.base_url,
                timeout=self.config.timeout,
            )
        return self._client

    async def health_check(self) -> bool:
        """Check if CapSolver API is available and key is valid."""
        if not self.config.api_key:
            return False

        try:
            client = await self._get_client()
            resp = await client.post(
                "/getBalance",
                json={"clientKey": self.config.api_key},
            )
            data = resp.json()
            if data.get("errorId") == 0:
                balance = data.get("balance", 0)
                logger.debug("CapSolver balance check", balance=balance)
                return balance > 0
            return False
        except Exception as e:
            logger.debug("CapSolver health check failed", error=str(e))
            return False

    async def solve(
        self,
        captcha_type: CaptchaType,
        site_key: str | None = None,
        site_url: str | None = None,
        page: Any = None,
        **kwargs,
    ) -> CaptchaSolution:
        """
        Solve CAPTCHA using CapSolver API.

        Args:
            captcha_type: Type of CAPTCHA
            site_key: Site key for reCAPTCHA/hCaptcha
            site_url: URL of the page with CAPTCHA
            page: Playwright page (for extracting site_key if not provided)
        """
        start_time = time.time()

        if not self.config.api_key:
            return CaptchaSolution(
                success=False,
                error="CapSolver API key not configured",
                solver_used=self.name,
            )

        try:
            # Extract site_key from page if not provided
            if not site_key and page:
                site_key = await self._extract_site_key(page, captcha_type)
                site_url = page.url

            if not site_key:
                return CaptchaSolution(
                    success=False,
                    error="Site key not provided or could not be extracted",
                    solver_used=self.name,
                )

            # Map CAPTCHA type to CapSolver task type
            task_type = self._get_task_type(captcha_type)
            if not task_type:
                return CaptchaSolution(
                    success=False,
                    error=f"Unsupported CAPTCHA type: {captcha_type}",
                    solver_used=self.name,
                )

            # Create task
            task_data = {
                "type": task_type,
                "websiteURL": site_url,
                "websiteKey": site_key,
            }

            # Add type-specific parameters
            if captcha_type == CaptchaType.RECAPTCHA_V3:
                task_data["pageAction"] = kwargs.get("action", "verify")
                task_data["minScore"] = kwargs.get("min_score", 0.7)

            client = await self._get_client()

            # Create task
            create_resp = await client.post(
                "/createTask",
                json={
                    "clientKey": self.config.api_key,
                    "task": task_data,
                },
            )
            create_data = create_resp.json()

            if create_data.get("errorId") != 0:
                return CaptchaSolution(
                    success=False,
                    error=create_data.get("errorDescription", "Unknown error"),
                    solver_used=self.name,
                    time_ms=(time.time() - start_time) * 1000,
                )

            task_id = create_data.get("taskId")
            if not task_id:
                return CaptchaSolution(
                    success=False,
                    error="No task ID returned",
                    solver_used=self.name,
                    time_ms=(time.time() - start_time) * 1000,
                )

            # Poll for result
            token = await self._poll_result(task_id)

            if token:
                return CaptchaSolution(
                    success=True,
                    token=token,
                    solver_used=self.name,
                    time_ms=(time.time() - start_time) * 1000,
                )
            else:
                return CaptchaSolution(
                    success=False,
                    error="Failed to get solution within timeout",
                    solver_used=self.name,
                    time_ms=(time.time() - start_time) * 1000,
                )

        except Exception as e:
            logger.error("CapSolver error", error=str(e))
            return CaptchaSolution(
                success=False,
                error=str(e),
                solver_used=self.name,
                time_ms=(time.time() - start_time) * 1000,
            )

    async def _poll_result(self, task_id: str) -> str | None:
        """Poll for task result."""
        client = await self._get_client()
        max_attempts = int(self.config.timeout / self.config.poll_interval)

        for _ in range(max_attempts):
            await asyncio.sleep(self.config.poll_interval)

            resp = await client.post(
                "/getTaskResult",
                json={
                    "clientKey": self.config.api_key,
                    "taskId": task_id,
                },
            )
            data = resp.json()

            if data.get("errorId") != 0:
                logger.warning("CapSolver poll error", error=data.get("errorDescription"))
                return None

            status = data.get("status")
            if status == "ready":
                solution = data.get("solution", {})
                # Different CAPTCHA types return token in different fields
                return (
                    solution.get("gRecaptchaResponse")
                    or solution.get("token")
                    or solution.get("text")
                )
            elif status == "failed":
                logger.warning("CapSolver task failed", data=data)
                return None

        return None

    def _get_task_type(self, captcha_type: CaptchaType) -> str | None:
        """Map CaptchaType to CapSolver task type."""
        mapping = {
            CaptchaType.RECAPTCHA_V2: "ReCaptchaV2TaskProxyLess",
            CaptchaType.RECAPTCHA_V3: "ReCaptchaV3TaskProxyLess",
            CaptchaType.HCAPTCHA: "HCaptchaTaskProxyLess",
            CaptchaType.CLOUDFLARE: "AntiTurnstileTaskProxyLess",
        }
        return mapping.get(captcha_type)

    async def _extract_site_key(self, page: Any, captcha_type: CaptchaType) -> str | None:
        """Extract site key from page."""
        try:
            if captcha_type in (CaptchaType.RECAPTCHA_V2, CaptchaType.RECAPTCHA_V3):
                # reCAPTCHA site key extraction
                selectors = [
                    '[data-sitekey]',
                    '.g-recaptcha[data-sitekey]',
                    '#recaptcha[data-sitekey]',
                ]
                for selector in selectors:
                    element = await page.query_selector(selector)
                    if element:
                        return await element.get_attribute("data-sitekey")

                # Try script-based extraction
                site_key = await page.evaluate("""
                    () => {
                        const scripts = document.querySelectorAll('script');
                        for (const script of scripts) {
                            const match = script.src?.match(/render=([^&]+)/);
                            if (match) return match[1];
                        }
                        return window.___grecaptcha_cfg?.clients?.[0]?.N?.sitekey || null;
                    }
                """)
                if site_key:
                    return site_key

            elif captcha_type == CaptchaType.HCAPTCHA:
                element = await page.query_selector('[data-sitekey], .h-captcha[data-sitekey]')
                if element:
                    return await element.get_attribute("data-sitekey")

            elif captcha_type == CaptchaType.CLOUDFLARE:
                # Turnstile site key
                element = await page.query_selector('[data-sitekey], .cf-turnstile[data-sitekey]')
                if element:
                    return await element.get_attribute("data-sitekey")

        except Exception as e:
            logger.debug("Failed to extract site key", error=str(e))

        return None


# 
# 2Captcha Integration
# 


@dataclass
class TwoCaptchaConfig:
    """2Captcha configuration."""

    api_key: str = ""
    base_url: str = "https://2captcha.com"
    timeout: float = 120.0
    poll_interval: float = 5.0


class TwoCaptchaClient(CaptchaSolver):
    """
    2Captcha CAPTCHA solving service client.

    Supports:
    - reCAPTCHA v2/v3
    - hCaptcha
    - Cloudflare Turnstile
    - FunCAPTCHA
    - Image CAPTCHA

    Docs: https://2captcha.com/api-docs
    """

    def __init__(self, config: TwoCaptchaConfig | None = None):
        self.config = config or TwoCaptchaConfig()

    @property
    def name(self) -> str:
        return "2captcha"

    async def health_check(self) -> bool:
        """Check if 2Captcha API is available and key is valid."""
        if not self.config.api_key:
            return False

        try:
            async with httpx.AsyncClient() as client:
                resp = await client.get(
                    f"{self.config.base_url}/res.php",
                    params={
                        "key": self.config.api_key,
                        "action": "getbalance",
                        "json": 1,
                    },
                )
                data = resp.json()
                if data.get("status") == 1:
                    balance = float(data.get("request", 0))
                    logger.debug("2Captcha balance check", balance=balance)
                    return balance > 0
                return False
        except Exception as e:
            logger.debug("2Captcha health check failed", error=str(e))
            return False

    async def solve(
        self,
        captcha_type: CaptchaType,
        site_key: str | None = None,
        site_url: str | None = None,
        page: Any = None,
        **kwargs,
    ) -> CaptchaSolution:
        """
        Solve CAPTCHA using 2Captcha API.

        Args:
            captcha_type: Type of CAPTCHA
            site_key: Site key for reCAPTCHA/hCaptcha
            site_url: URL of the page with CAPTCHA
            page: Playwright page (for extracting site_key if not provided)
        """
        start_time = time.time()

        if not self.config.api_key:
            return CaptchaSolution(
                success=False,
                error="2Captcha API key not configured",
                solver_used=self.name,
            )

        try:
            # Extract site_key from page if not provided
            if not site_key and page:
                site_key = await self._extract_site_key(page, captcha_type)
                site_url = page.url

            if not site_key:
                return CaptchaSolution(
                    success=False,
                    error="Site key not provided or could not be extracted",
                    solver_used=self.name,
                )

            # Build request parameters
            params = {
                "key": self.config.api_key,
                "json": 1,
                "pageurl": site_url,
            }

            # Add type-specific parameters
            if captcha_type in (CaptchaType.RECAPTCHA_V2, CaptchaType.RECAPTCHA_V3):
                params["method"] = "userrecaptcha"
                params["googlekey"] = site_key
                if captcha_type == CaptchaType.RECAPTCHA_V3:
                    params["version"] = "v3"
                    params["action"] = kwargs.get("action", "verify")
                    params["min_score"] = kwargs.get("min_score", 0.7)

            elif captcha_type == CaptchaType.HCAPTCHA:
                params["method"] = "hcaptcha"
                params["sitekey"] = site_key

            elif captcha_type == CaptchaType.CLOUDFLARE:
                params["method"] = "turnstile"
                params["sitekey"] = site_key

            else:
                return CaptchaSolution(
                    success=False,
                    error=f"Unsupported CAPTCHA type: {captcha_type}",
                    solver_used=self.name,
                )

            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                # Submit task
                submit_resp = await client.get(
                    f"{self.config.base_url}/in.php",
                    params=params,
                )
                submit_data = submit_resp.json()

                if submit_data.get("status") != 1:
                    return CaptchaSolution(
                        success=False,
                        error=submit_data.get("request", "Unknown error"),
                        solver_used=self.name,
                        time_ms=(time.time() - start_time) * 1000,
                    )

                task_id = submit_data.get("request")

                # Poll for result
                token = await self._poll_result(client, task_id)

                if token:
                    return CaptchaSolution(
                        success=True,
                        token=token,
                        solver_used=self.name,
                        time_ms=(time.time() - start_time) * 1000,
                    )
                else:
                    return CaptchaSolution(
                        success=False,
                        error="Failed to get solution within timeout",
                        solver_used=self.name,
                        time_ms=(time.time() - start_time) * 1000,
                    )

        except Exception as e:
            logger.error("2Captcha error", error=str(e))
            return CaptchaSolution(
                success=False,
                error=str(e),
                solver_used=self.name,
                time_ms=(time.time() - start_time) * 1000,
            )

    async def _poll_result(self, client: httpx.AsyncClient, task_id: str) -> str | None:
        """Poll for task result."""
        max_attempts = int(self.config.timeout / self.config.poll_interval)

        for _ in range(max_attempts):
            await asyncio.sleep(self.config.poll_interval)

            resp = await client.get(
                f"{self.config.base_url}/res.php",
                params={
                    "key": self.config.api_key,
                    "action": "get",
                    "id": task_id,
                    "json": 1,
                },
            )
            data = resp.json()

            if data.get("status") == 1:
                return data.get("request")
            elif data.get("request") == "CAPCHA_NOT_READY":
                continue
            else:
                logger.warning("2Captcha poll error", error=data.get("request"))
                return None

        return None

    async def _extract_site_key(self, page: Any, captcha_type: CaptchaType) -> str | None:
        """Extract site key from page (same logic as CapSolver)."""
        try:
            if captcha_type in (CaptchaType.RECAPTCHA_V2, CaptchaType.RECAPTCHA_V3):
                selectors = [
                    '[data-sitekey]',
                    '.g-recaptcha[data-sitekey]',
                    '#recaptcha[data-sitekey]',
                ]
                for selector in selectors:
                    element = await page.query_selector(selector)
                    if element:
                        return await element.get_attribute("data-sitekey")

            elif captcha_type == CaptchaType.HCAPTCHA:
                element = await page.query_selector('[data-sitekey], .h-captcha[data-sitekey]')
                if element:
                    return await element.get_attribute("data-sitekey")

            elif captcha_type == CaptchaType.CLOUDFLARE:
                element = await page.query_selector('[data-sitekey], .cf-turnstile[data-sitekey]')
                if element:
                    return await element.get_attribute("data-sitekey")

        except Exception as e:
            logger.debug("Failed to extract site key", error=str(e))

        return None


# 
# Factory function
# 


def create_paid_solver(
    provider: Literal["capsolver", "2captcha"] = "capsolver",
    api_key: str = "",
) -> CaptchaSolver:
    """
    Create a paid CAPTCHA solver instance.

    Args:
        provider: Which service to use
        api_key: API key for the service

    Returns:
        CaptchaSolver instance
    """
    if provider == "capsolver":
        return CapSolverClient(CapSolverConfig(api_key=api_key))
    elif provider == "2captcha":
        return TwoCaptchaClient(TwoCaptchaConfig(api_key=api_key))
    else:
        raise ValueError(f"Unknown CAPTCHA solver provider: {provider}")

```

---

## backend/autonomous-crawler-service/src/captcha/stealth.py

```py
"""Stealth browser configuration for bot detection bypass."""

from dataclasses import dataclass, field
from pathlib import Path
from typing import Any

import structlog

logger = structlog.get_logger(__name__)


# Extension paths for CAPTCHA bypass
EXTENSION_CACHE_DIR = Path("/tmp/browser-extensions")


@dataclass
class StealthConfig:
    """Configuration for stealth browser mode."""
    
    # User agent rotation
    user_agents: list[str] = field(default_factory=lambda: [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:134.0) Gecko/20100101 Firefox/134.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.2 Safari/605.1.15",
    ])
    
    # Viewport sizes (common resolutions)
    viewports: list[dict[str, int]] = field(default_factory=lambda: [
        {"width": 1920, "height": 1080},
        {"width": 1366, "height": 768},
        {"width": 1536, "height": 864},
        {"width": 1440, "height": 900},
    ])
    
    # Timezone/locale
    timezone: str = "Asia/Seoul"
    locale: str = "ko-KR"
    
    # Extra args for Chromium
    extra_args: list[str] = field(default_factory=lambda: [
        "--disable-blink-features=AutomationControlled",
        "--disable-features=IsolateOrigins,site-per-process",
        "--disable-dev-shm-usage",
        "--disable-accelerated-2d-canvas",
        "--no-first-run",
        "--no-zygote",
        "--disable-gpu",
        "--hide-scrollbars",
        "--mute-audio",
    ])
    
    # Webdriver navigator override
    hide_webdriver: bool = True
    
    # Random delays (ms)
    min_delay: int = 100
    max_delay: int = 500


def apply_stealth_to_playwright(page: Any, config: StealthConfig | None = None) -> None:
    """
    Apply stealth settings to a Playwright page.
    
    Uses playwright_stealth if available, otherwise applies manual patches.
    """
    if config is None:
        config = StealthConfig()
    
    try:
        from playwright_stealth import stealth_sync
        stealth_sync(page)
        logger.debug("Applied playwright_stealth")
    except ImportError:
        logger.warning("playwright_stealth not available, using manual patches")
        _apply_manual_stealth(page, config)


async def apply_stealth_to_playwright_async(page: Any, config: StealthConfig | None = None) -> None:
    """Async version of stealth application."""
    if config is None:
        config = StealthConfig()
    
    try:
        from playwright_stealth import stealth_async
        await stealth_async(page)
        logger.debug("Applied playwright_stealth (async)")
    except ImportError:
        logger.warning("playwright_stealth not available, using manual patches")
        await _apply_manual_stealth_async(page, config)


def _apply_manual_stealth(page: Any, config: StealthConfig) -> None:
    """Apply manual stealth patches."""
    # Hide webdriver
    if config.hide_webdriver:
        page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
        """)
    
    # Override navigator properties
    page.add_init_script("""
        // Override plugins
        Object.defineProperty(navigator, 'plugins', {
            get: () => [1, 2, 3, 4, 5]
        });
        
        // Override languages
        Object.defineProperty(navigator, 'languages', {
            get: () => ['ko-KR', 'ko', 'en-US', 'en']
        });
        
        // Override platform
        Object.defineProperty(navigator, 'platform', {
            get: () => 'Win32'
        });
        
        // Chrome runtime
        window.chrome = {
            runtime: {}
        };
        
        // Permissions
        const originalQuery = window.navigator.permissions.query;
        window.navigator.permissions.query = (parameters) => (
            parameters.name === 'notifications' ?
                Promise.resolve({ state: Notification.permission }) :
                originalQuery(parameters)
        );
    """)


async def _apply_manual_stealth_async(page: Any, config: StealthConfig) -> None:
    """Apply manual stealth patches (async)."""
    # Same as sync but using await
    if config.hide_webdriver:
        await page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
        """)
    
    await page.add_init_script("""
        Object.defineProperty(navigator, 'plugins', {
            get: () => [1, 2, 3, 4, 5]
        });
        
        Object.defineProperty(navigator, 'languages', {
            get: () => ['ko-KR', 'ko', 'en-US', 'en']
        });
        
        Object.defineProperty(navigator, 'platform', {
            get: () => 'Win32'
        });
        
        window.chrome = { runtime: {} };
        
        const originalQuery = window.navigator.permissions.query;
        window.navigator.permissions.query = (parameters) => (
            parameters.name === 'notifications' ?
                Promise.resolve({ state: Notification.permission }) :
                originalQuery(parameters)
        );
    """)


def get_undetected_browser_args() -> list[str]:
    """Get Chrome args for undetected browsing."""
    return [
        "--disable-blink-features=AutomationControlled",
        "--disable-features=IsolateOrigins,site-per-process",
        "--disable-infobars",
        "--disable-background-networking",
        "--disable-background-timer-throttling",
        "--disable-backgrounding-occluded-windows",
        "--disable-breakpad",
        "--disable-component-extensions-with-background-pages",
        "--disable-component-update",
        "--disable-default-apps",
        "--disable-extensions",
        "--disable-hang-monitor",
        "--disable-ipc-flooding-protection",
        "--disable-popup-blocking",
        "--disable-prompt-on-repost",
        "--disable-renderer-backgrounding",
        "--disable-sync",
        "--enable-features=NetworkService,NetworkServiceInProcess",
        "--force-color-profile=srgb",
        "--metrics-recording-only",
        "--no-first-run",
        "--password-store=basic",
        "--use-mock-keychain",
        "--ignore-certificate-errors",
    ]


async def get_nopecha_extension_path(api_key: str = "") -> str | None:
    """
    Download and configure NopeCHA extension for browser use.
    
    Args:
        api_key: Optional NopeCHA API key for faster solving
        
    Returns:
        Path to the extracted extension directory, or None if failed
    """
    try:
        from src.captcha.nopecha import NopeCHAConfig, NopeCHAExtensionManager
        
        config = NopeCHAConfig(
            api_key=api_key,
            enabled=True,
            auto_solve=True,
            use_audio_challenge=True,
            cache_dir=EXTENSION_CACHE_DIR / "nopecha",
        )
        
        manager = NopeCHAExtensionManager(config)
        ext_path = await manager.download_extension()
        
        logger.info("NopeCHA extension ready", path=str(ext_path))
        return str(ext_path)
        
    except Exception as e:
        logger.error("Failed to setup NopeCHA extension", error=str(e))
        return None


def get_stealth_browser_args_with_extensions(
    extension_paths: list[str] | None = None,
    include_docker_args: bool = False,
) -> list[str]:
    """
    Get Chrome args for stealth browsing with extension support.
    
    Args:
        extension_paths: List of paths to unpacked extensions
        include_docker_args: Include Docker-specific args
        
    Returns:
        List of Chrome arguments
    """
    args = [
        "--disable-blink-features=AutomationControlled",
        "--disable-features=IsolateOrigins,site-per-process,AutomationControlled",
        "--disable-infobars",
        "--disable-popup-blocking",
        "--disable-notifications",
        "--no-first-run",
        "--no-default-browser-check",
        "--no-service-autorun",
        "--disable-background-networking",
        "--disable-background-timer-throttling",
        "--disable-backgrounding-occluded-windows",
        "--disable-breakpad",
        "--disable-component-update",
        "--disable-default-apps",
        "--disable-hang-monitor",
        "--disable-ipc-flooding-protection",
        "--disable-renderer-backgrounding",
        "--disable-sync",
        "--disable-client-side-phishing-detection",
        "--disable-domain-reliability",
        "--metrics-recording-only",
        "--safebrowsing-disable-auto-update",
        "--enable-webgl",
        "--enable-accelerated-2d-canvas",
        "--enable-features=NetworkService,NetworkServiceInProcess",
        "--ignore-certificate-errors",
        "--ignore-ssl-errors",
        "--allow-running-insecure-content",
        "--password-store=basic",
        "--use-mock-keychain",
        "--log-level=3",
    ]
    
    # Add extension paths (requires extensions to NOT be disabled)
    if extension_paths:
        # Remove --disable-extensions if present
        args = [a for a in args if a != "--disable-extensions"]
        
        # Add load-extension args
        for ext_path in extension_paths:
            if ext_path:
                args.append(f"--load-extension={ext_path}")
                
        # Also add to disable-extensions-except
        valid_paths = [p for p in extension_paths if p]
        if valid_paths:
            args.append(f"--disable-extensions-except={','.join(valid_paths)}")
    else:
        args.append("--disable-extensions")
    
    if include_docker_args:
        args.extend([
            "--no-sandbox",
            "--disable-dev-shm-usage",
            "--disable-gpu-sandbox",
            "--disable-setuid-sandbox",
            "--no-zygote",
            "--single-process",
        ])
    
    return args


@dataclass
class EnhancedStealthConfig(StealthConfig):
    """Enhanced stealth configuration with extension support."""
    
    # NopeCHA settings - ENABLED by default
    use_nopecha: bool = True
    nopecha_api_key: str = ""
    
    # Camoufox as alternative - can be enabled via settings
    use_camoufox: bool = False
    
    # Human-like behavior - ENABLED by default
    enable_human_simulation: bool = True
    
    # Extension paths
    extension_paths: list[str] = field(default_factory=list)
    
    async def setup_extensions(self) -> None:
        """Download and configure required extensions."""
        if self.use_nopecha:
            nopecha_path = await get_nopecha_extension_path(self.nopecha_api_key)
            if nopecha_path and nopecha_path not in self.extension_paths:
                self.extension_paths.append(nopecha_path)
    
    def get_browser_args(self, include_docker: bool = False) -> list[str]:
        """Get Chrome args with configured extensions."""
        return get_stealth_browser_args_with_extensions(
            extension_paths=self.extension_paths if self.extension_paths else None,
            include_docker_args=include_docker,
        )
    
    def get_random_user_agent(self) -> str:
        """Get a random user agent from the configured list."""
        import random
        return random.choice(self.user_agents)
    
    def get_random_viewport(self) -> dict[str, int]:
        """Get a random viewport size from the configured list."""
        import random
        return random.choice(self.viewports)

```

---

## backend/autonomous-crawler-service/src/captcha/undetected.py

```py
"""
Undetected ChromeDriver Integration for Bot Detection Bypass.

This module provides:
1. Undetected ChromeDriver - Patched ChromeDriver that bypasses detection
2. Advanced Stealth - JavaScript patches to hide automation fingerprints
3. Human-like behavior simulation

Supports:
- Cloudflare
- PerimeterX
- DataDome
- Incapsula
- reCAPTCHA detection
"""

import asyncio
import random
from dataclasses import dataclass
from typing import Any

import structlog

logger = structlog.get_logger(__name__)


@dataclass
class UndetectedConfig:
    """Configuration for undetected browser mode."""
    
    # Driver settings
    driver_executable_path: str | None = None
    browser_executable_path: str | None = None
    
    # Version matching
    version_main: int | None = None  # Chrome major version
    
    # User data
    user_data_dir: str | None = None
    use_subprocess: bool = True
    
    # Stealth options
    enable_cdp_events: bool = True
    suppress_welcome: bool = True
    log_level: int = 0
    
    # Headless mode (use new headless mode)
    headless: bool = False
    use_new_headless: bool = True  # Chrome 109+ new headless mode
    
    # Window size
    window_size: tuple[int, int] = (1920, 1080)
    
    # Proxy
    proxy: str | None = None


def get_undetected_chromedriver():
    """
    Get an undetected ChromeDriver instance.
    
    Uses undetected-chromedriver library if available,
    otherwise falls back to manual patching.
    """
    try:
        import undetected_chromedriver as uc
        return uc
    except ImportError:
        logger.warning("undetected-chromedriver not installed, using manual patches")
        return None


async def create_undetected_driver(
    config: UndetectedConfig | None = None,
) -> Any:
    """
    Create an undetected ChromeDriver instance.
    
    Args:
        config: Configuration options
        
    Returns:
        Undetected ChromeDriver instance or None
    """
    if config is None:
        config = UndetectedConfig()
    
    uc = get_undetected_chromedriver()
    if uc is None:
        return None
    
    options = uc.ChromeOptions()
    
    # Basic options
    options.add_argument(f"--window-size={config.window_size[0]},{config.window_size[1]}")
    
    if config.headless:
        if config.use_new_headless:
            options.add_argument("--headless=new")
        else:
            options.add_argument("--headless")
    
    if config.proxy:
        options.add_argument(f"--proxy-server={config.proxy}")
    
    # Additional stealth arguments
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-gpu")
    options.add_argument("--disable-infobars")
    
    # Create driver
    try:
        driver = uc.Chrome(
            options=options,
            driver_executable_path=config.driver_executable_path,
            browser_executable_path=config.browser_executable_path,
            version_main=config.version_main,
            user_data_dir=config.user_data_dir,
            use_subprocess=config.use_subprocess,
            enable_cdp_events=config.enable_cdp_events,
            suppress_welcome=config.suppress_welcome,
            log_level=config.log_level,
        )
        
        logger.info("Created undetected ChromeDriver")
        return driver
        
    except Exception as e:
        logger.error("Failed to create undetected ChromeDriver", error=str(e))
        return None


class AdvancedStealthPatcher:
    """
    Advanced JavaScript stealth patches for bot detection bypass.
    
    These patches are applied via CDP or page.evaluate to hide
    automation fingerprints that bot detection systems look for.
    """
    
    # Chrome properties patch
    CHROME_RUNTIME_PATCH = """
    (() => {
        // Add chrome.runtime if missing
        if (!window.chrome) {
            window.chrome = {};
        }
        if (!window.chrome.runtime) {
            window.chrome.runtime = {
                PlatformOs: { MAC: 'mac', WIN: 'win', ANDROID: 'android', CROS: 'cros', LINUX: 'linux', OPENBSD: 'openbsd' },
                PlatformArch: { ARM: 'arm', X86_32: 'x86-32', X86_64: 'x86-64' },
                PlatformNaclArch: { ARM: 'arm', X86_32: 'x86-32', X86_64: 'x86-64' },
                RequestUpdateCheckStatus: { THROTTLED: 'throttled', NO_UPDATE: 'no_update', UPDATE_AVAILABLE: 'update_available' },
                OnInstalledReason: { INSTALL: 'install', UPDATE: 'update', CHROME_UPDATE: 'chrome_update', SHARED_MODULE_UPDATE: 'shared_module_update' },
                OnRestartRequiredReason: { APP_UPDATE: 'app_update', OS_UPDATE: 'os_update', PERIODIC: 'periodic' }
            };
        }
        
        // Add chrome.csi if missing
        if (!window.chrome.csi) {
            window.chrome.csi = function() { return {}; };
        }
        
        // Add chrome.loadTimes if missing
        if (!window.chrome.loadTimes) {
            window.chrome.loadTimes = function() {
                return {
                    commitLoadTime: Date.now() / 1000,
                    connectionInfo: 'http/1.1',
                    finishDocumentLoadTime: Date.now() / 1000,
                    finishLoadTime: Date.now() / 1000,
                    firstPaintAfterLoadTime: 0,
                    firstPaintTime: Date.now() / 1000,
                    navigationType: 'Other',
                    npnNegotiatedProtocol: 'unknown',
                    requestTime: Date.now() / 1000,
                    startLoadTime: Date.now() / 1000,
                    wasAlternateProtocolAvailable: false,
                    wasFetchedViaSpdy: false,
                    wasNpnNegotiated: false
                };
            };
        }
    })();
    """
    
    # WebDriver property patch
    WEBDRIVER_PATCH = """
    (() => {
        // Remove webdriver property
        Object.defineProperty(navigator, 'webdriver', {
            get: () => undefined,
            configurable: true
        });
        
        // Also patch the prototype
        const originalQuery = window.Navigator.prototype.hasOwnProperty;
        Object.defineProperty(Navigator.prototype, 'webdriver', {
            get: () => undefined,
            configurable: true
        });
        
        // Delete if exists
        delete navigator.webdriver;
    })();
    """
    
    # Plugins patch
    PLUGINS_PATCH = """
    (() => {
        // Mock plugins array
        const mockPlugins = [
            {
                name: 'Chrome PDF Plugin',
                filename: 'internal-pdf-viewer',
                description: 'Portable Document Format',
                length: 1,
                0: { type: 'application/pdf', suffixes: 'pdf', description: 'Portable Document Format' }
            },
            {
                name: 'Chrome PDF Viewer',
                filename: 'mhjfbmdgcfjbbpaeojofohoefgiehjai',
                description: '',
                length: 1,
                0: { type: 'application/pdf', suffixes: 'pdf', description: '' }
            },
            {
                name: 'Native Client',
                filename: 'internal-nacl-plugin',
                description: '',
                length: 2,
                0: { type: 'application/x-nacl', suffixes: '', description: 'Native Client Executable' },
                1: { type: 'application/x-pnacl', suffixes: '', description: 'Portable Native Client Executable' }
            }
        ];
        
        // Create a proper PluginArray
        const pluginArray = Object.create(PluginArray.prototype);
        mockPlugins.forEach((plugin, i) => {
            pluginArray[i] = plugin;
        });
        pluginArray.length = mockPlugins.length;
        
        // Patch namedItem and item methods
        pluginArray.item = function(index) { return this[index]; };
        pluginArray.namedItem = function(name) {
            return mockPlugins.find(p => p.name === name) || null;
        };
        pluginArray.refresh = function() {};
        
        Object.defineProperty(navigator, 'plugins', {
            get: () => pluginArray,
            configurable: true
        });
    })();
    """
    
    # Languages patch
    LANGUAGES_PATCH = """
    (() => {
        Object.defineProperty(navigator, 'languages', {
            get: () => ['ko-KR', 'ko', 'en-US', 'en'],
            configurable: true
        });
        
        Object.defineProperty(navigator, 'language', {
            get: () => 'ko-KR',
            configurable: true
        });
    })();
    """
    
    # Hardware concurrency patch
    HARDWARE_PATCH = """
    (() => {
        Object.defineProperty(navigator, 'hardwareConcurrency', {
            get: () => 8,
            configurable: true
        });
        
        Object.defineProperty(navigator, 'deviceMemory', {
            get: () => 8,
            configurable: true
        });
    })();
    """
    
    # Permissions patch
    PERMISSIONS_PATCH = """
    (() => {
        const originalQuery = window.navigator.permissions.query;
        window.navigator.permissions.query = (parameters) => {
            if (parameters.name === 'notifications') {
                return Promise.resolve({ state: Notification.permission });
            }
            return originalQuery.call(window.navigator.permissions, parameters);
        };
    })();
    """
    
    # WebGL vendor/renderer patch
    WEBGL_PATCH = """
    (() => {
        const getParameter = WebGLRenderingContext.prototype.getParameter;
        WebGLRenderingContext.prototype.getParameter = function(parameter) {
            // UNMASKED_VENDOR_WEBGL
            if (parameter === 37445) {
                return 'Intel Inc.';
            }
            // UNMASKED_RENDERER_WEBGL
            if (parameter === 37446) {
                return 'Intel Iris OpenGL Engine';
            }
            return getParameter.call(this, parameter);
        };
        
        // Also patch WebGL2
        if (typeof WebGL2RenderingContext !== 'undefined') {
            const getParameter2 = WebGL2RenderingContext.prototype.getParameter;
            WebGL2RenderingContext.prototype.getParameter = function(parameter) {
                if (parameter === 37445) {
                    return 'Intel Inc.';
                }
                if (parameter === 37446) {
                    return 'Intel Iris OpenGL Engine';
                }
                return getParameter2.call(this, parameter);
            };
        }
    })();
    """
    
    # Iframe contentWindow patch
    IFRAME_PATCH = """
    (() => {
        // Prevent iframe detection
        try {
            if (window.top === window.self) {
                Object.defineProperty(window, 'frameElement', {
                    get: () => null,
                    configurable: true
                });
            }
        } catch (e) {}
    })();
    """
    
    # Console debug patch (hide console.debug modifications)
    CONSOLE_PATCH = """
    (() => {
        // Preserve original console methods
        const originalDebug = console.debug;
        console.debug = function(...args) {
            // Filter out automation-related debug messages
            const filtered = args.filter(arg => {
                if (typeof arg === 'string') {
                    const lower = arg.toLowerCase();
                    return !lower.includes('webdriver') && 
                           !lower.includes('automation') &&
                           !lower.includes('puppeteer') &&
                           !lower.includes('playwright');
                }
                return true;
            });
            if (filtered.length > 0) {
                originalDebug.apply(console, filtered);
            }
        };
    })();
    """
    
    @classmethod
    def get_all_patches(cls) -> str:
        """Get all stealth patches combined."""
        return "\n".join([
            cls.CHROME_RUNTIME_PATCH,
            cls.WEBDRIVER_PATCH,
            cls.PLUGINS_PATCH,
            cls.LANGUAGES_PATCH,
            cls.HARDWARE_PATCH,
            cls.PERMISSIONS_PATCH,
            cls.WEBGL_PATCH,
            cls.IFRAME_PATCH,
            cls.CONSOLE_PATCH,
        ])
    
    @classmethod
    async def apply_to_page(cls, page: Any) -> None:
        """Apply all stealth patches to a Playwright page."""
        try:
            await page.add_init_script(cls.get_all_patches())
            logger.debug("Applied advanced stealth patches to page")
        except Exception as e:
            logger.error("Failed to apply stealth patches", error=str(e))


class HumanBehaviorSimulator:
    """
    Simulate human-like behavior to avoid bot detection.
    
    Includes:
    - Random delays
    - Mouse movements
    - Scroll patterns
    - Typing patterns
    """
    
    @staticmethod
    def random_delay(min_ms: int = 100, max_ms: int = 500) -> float:
        """Get random delay in seconds."""
        return random.randint(min_ms, max_ms) / 1000
    
    @staticmethod
    async def human_type(page: Any, selector: str, text: str) -> None:
        """Type text with human-like delays."""
        element = await page.query_selector(selector)
        if not element:
            return
        
        await element.click()
        await asyncio.sleep(HumanBehaviorSimulator.random_delay(50, 150))
        
        for char in text:
            await page.keyboard.type(char)
            # Variable delay between keystrokes
            delay = random.uniform(0.05, 0.15)
            if char in " .,!?":
                delay += random.uniform(0.1, 0.2)
            await asyncio.sleep(delay)
    
    @staticmethod
    async def human_click(page: Any, selector: str) -> None:
        """Click with human-like behavior."""
        element = await page.query_selector(selector)
        if not element:
            return
        
        box = await element.bounding_box()
        if not box:
            await element.click()
            return
        
        # Click at random position within element
        x = box["x"] + random.uniform(5, box["width"] - 5)
        y = box["y"] + random.uniform(5, box["height"] - 5)
        
        # Move mouse first
        await page.mouse.move(x, y)
        await asyncio.sleep(HumanBehaviorSimulator.random_delay(50, 150))
        await page.mouse.click(x, y)
    
    @staticmethod
    async def human_scroll(page: Any, direction: str = "down", amount: int = 300) -> None:
        """Scroll with human-like patterns."""
        if direction == "down":
            delta = random.randint(amount - 50, amount + 50)
        else:
            delta = -random.randint(amount - 50, amount + 50)
        
        await page.mouse.wheel(0, delta)
        await asyncio.sleep(HumanBehaviorSimulator.random_delay(200, 400))
    
    @staticmethod
    async def random_mouse_movements(page: Any, count: int = 3) -> None:
        """Make random mouse movements."""
        viewport = page.viewport_size
        if not viewport:
            return
        
        for _ in range(count):
            x = random.randint(100, viewport["width"] - 100)
            y = random.randint(100, viewport["height"] - 100)
            await page.mouse.move(x, y)
            await asyncio.sleep(HumanBehaviorSimulator.random_delay(100, 300))


def get_enhanced_browser_args(
    include_docker: bool = False,
    include_stealth: bool = True,
) -> list[str]:
    """
    Get enhanced Chrome arguments for maximum undetectability.
    
    Args:
        include_docker: Include Docker-specific args
        include_stealth: Include stealth-related args
        
    Returns:
        List of Chrome arguments
    """
    args = [
        # Core anti-detection
        "--disable-blink-features=AutomationControlled",
        "--disable-features=IsolateOrigins,site-per-process,AutomationControlled",
        
        # Disable infobars and popups
        "--disable-infobars",
        "--disable-popup-blocking",
        "--disable-notifications",
        
        # Disable extensions welcome
        "--no-first-run",
        "--no-default-browser-check",
        "--no-service-autorun",
        
        # Performance
        "--disable-background-networking",
        "--disable-background-timer-throttling",
        "--disable-backgrounding-occluded-windows",
        "--disable-breakpad",
        "--disable-component-update",
        "--disable-default-apps",
        "--disable-hang-monitor",
        "--disable-ipc-flooding-protection",
        "--disable-renderer-backgrounding",
        "--disable-sync",
        
        # Privacy/security that helps with detection
        "--disable-client-side-phishing-detection",
        "--disable-domain-reliability",
        "--metrics-recording-only",
        "--safebrowsing-disable-auto-update",
        
        # WebRTC IP leak prevention
        "--disable-webrtc-apm-in-audio-service",
        "--disable-webrtc-encryption",
        "--disable-webrtc-hw-decoding",
        "--disable-webrtc-hw-encoding",
        "--force-webrtc-ip-handling-policy=disable_non_proxied_udp",
        
        # GPU (often checked by detection systems)
        "--enable-webgl",
        "--enable-accelerated-2d-canvas",
        "--enable-features=NetworkService,NetworkServiceInProcess",
        
        # Misc
        "--ignore-certificate-errors",
        "--ignore-ssl-errors",
        "--allow-running-insecure-content",
        "--password-store=basic",
        "--use-mock-keychain",
        "--log-level=3",
    ]
    
    if include_docker:
        args.extend([
            "--no-sandbox",
            "--disable-dev-shm-usage",
            "--disable-gpu-sandbox",
            "--disable-setuid-sandbox",
            "--no-zygote",
            "--single-process",
        ])
    
    return args

```

---

## backend/autonomous-crawler-service/src/config/__init__.py

```py
"""Configuration module for autonomous-crawler-service."""

from .settings import Settings, get_settings
from .consul import (
    load_config_from_consul,
    wait_for_consul,
    check_consul_health,
    CONSUL_ENABLED,
)

__all__ = [
    "Settings",
    "get_settings",
    "load_config_from_consul",
    "wait_for_consul",
    "check_consul_health",
    "CONSUL_ENABLED",
]

```

---

## backend/autonomous-crawler-service/src/config/consul.py

```py
"""Consul KV configuration loader for autonomous-crawler service.

Loads configuration with the following precedence:
1. Consul KV (highest priority) - key path: config/autonomous-crawler/{KEY}
2. Environment Variables - {KEY}
3. Error if required key not found

Usage:
    from src.config.consul import load_config_from_consul

    # Load all config from Consul and apply to environment
    consul_keys, env_keys = load_config_from_consul()
    
    # Then use Settings as normal
    from src.config import get_settings
    settings = get_settings()
"""

import base64
import os
from typing import Any

import httpx
import structlog

logger = structlog.get_logger(__name__)

# Consul configuration
CONSUL_HOST = os.getenv("CONSUL_HOST", "localhost")
CONSUL_PORT = os.getenv("CONSUL_PORT", "8500")
CONSUL_HTTP_TOKEN = os.getenv("CONSUL_HTTP_TOKEN", "")
CONSUL_ENABLED = os.getenv("CONSUL_ENABLED", "true").lower() == "true"
CONSUL_SERVICE_NAME = os.getenv("CONSUL_SERVICE_NAME", "autonomous-crawler")

# Consul KV prefix for this service
CONSUL_KV_PREFIX = f"config/{CONSUL_SERVICE_NAME}/"


def get_consul_url() -> str:
    """Get the Consul HTTP API URL."""
    return f"http://{CONSUL_HOST}:{CONSUL_PORT}"


def get_consul_headers() -> dict[str, str]:
    """Get headers for Consul API requests."""
    headers = {"Accept": "application/json"}
    if CONSUL_HTTP_TOKEN:
        headers["X-Consul-Token"] = CONSUL_HTTP_TOKEN
    return headers


async def fetch_consul_kv_async(key: str) -> str | None:
    """
    Fetch a single key from Consul KV (async version).
    
    Args:
        key: The key name (without prefix)
        
    Returns:
        The value as a string, or None if not found
    """
    url = f"{get_consul_url()}/v1/kv/{CONSUL_KV_PREFIX}{key}"
    
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(url, headers=get_consul_headers(), timeout=5.0)
            
            if response.status_code == 404:
                return None
            
            response.raise_for_status()
            data = response.json()
            
            if data and len(data) > 0:
                # Consul returns base64-encoded values
                value_b64 = data[0].get("Value")
                if value_b64:
                    return base64.b64decode(value_b64).decode("utf-8")
            
            return None
            
    except httpx.HTTPError as e:
        logger.warning("Failed to fetch Consul key", key=key, error=str(e))
        return None
    except Exception as e:
        logger.warning("Error fetching Consul key", key=key, error=str(e))
        return None


def fetch_consul_kv_sync(key: str) -> str | None:
    """
    Fetch a single key from Consul KV (sync version).
    
    Args:
        key: The key name (without prefix)
        
    Returns:
        The value as a string, or None if not found
    """
    url = f"{get_consul_url()}/v1/kv/{CONSUL_KV_PREFIX}{key}"
    
    try:
        with httpx.Client() as client:
            response = client.get(url, headers=get_consul_headers(), timeout=5.0)
            
            if response.status_code == 404:
                return None
            
            response.raise_for_status()
            data = response.json()
            
            if data and len(data) > 0:
                value_b64 = data[0].get("Value")
                if value_b64:
                    return base64.b64decode(value_b64).decode("utf-8")
            
            return None
            
    except httpx.HTTPError as e:
        logger.warning("Failed to fetch Consul key", key=key, error=str(e))
        return None
    except Exception as e:
        logger.warning("Error fetching Consul key", key=key, error=str(e))
        return None


def fetch_all_consul_keys_sync() -> dict[str, str]:
    """
    Fetch all keys under the service prefix from Consul KV.
    
    Returns:
        Dictionary of key-value pairs
    """
    url = f"{get_consul_url()}/v1/kv/{CONSUL_KV_PREFIX}?recurse=true"
    
    try:
        with httpx.Client() as client:
            response = client.get(url, headers=get_consul_headers(), timeout=10.0)
            
            if response.status_code == 404:
                logger.info("No keys found in Consul", prefix=CONSUL_KV_PREFIX)
                return {}
            
            response.raise_for_status()
            data = response.json()
            
            result = {}
            for item in data or []:
                full_key = item.get("Key", "")
                value_b64 = item.get("Value")
                
                # Extract key name (remove prefix)
                if full_key.startswith(CONSUL_KV_PREFIX):
                    key_name = full_key[len(CONSUL_KV_PREFIX):]
                    if value_b64 and key_name:
                        result[key_name] = base64.b64decode(value_b64).decode("utf-8")
            
            return result
            
    except httpx.HTTPError as e:
        logger.warning("Failed to fetch Consul keys", prefix=CONSUL_KV_PREFIX, error=str(e))
        return {}
    except Exception as e:
        logger.warning("Error fetching Consul keys", prefix=CONSUL_KV_PREFIX, error=str(e))
        return {}


def load_config_from_consul() -> tuple[list[str], list[str]]:
    """
    Load configuration from Consul KV and inject into environment variables.
    
    This should be called at application startup, before Settings are loaded.
    
    Returns:
        Tuple of (consul_loaded_keys, env_loaded_keys)
    """
    if not CONSUL_ENABLED:
        logger.info("Consul configuration disabled, using environment variables only")
        return [], []
    
    logger.info(
        "Loading configuration from Consul",
        consul_url=get_consul_url(),
        service_name=CONSUL_SERVICE_NAME,
        prefix=CONSUL_KV_PREFIX,
    )
    
    # Fetch all keys from Consul
    consul_config = fetch_all_consul_keys_sync()
    
    consul_loaded_keys = []
    env_loaded_keys = []
    
    # Inject Consul values into environment (they take precedence)
    for key, value in consul_config.items():
        os.environ[key] = value
        consul_loaded_keys.append(key)
        logger.debug("Loaded from Consul", key=key)
    
    # Track which keys came from existing environment variables
    # (These are keys that weren't in Consul but exist in env)
    # Note: We only track this for logging purposes
    env_only_keys = set(os.environ.keys()) - set(consul_loaded_keys)
    
    logger.info(
        "Configuration loaded",
        consul_keys_count=len(consul_loaded_keys),
        consul_keys=consul_loaded_keys,
    )
    
    return consul_loaded_keys, list(env_only_keys)


def check_consul_health() -> bool:
    """Check if Consul is reachable and healthy."""
    url = f"{get_consul_url()}/v1/status/leader"
    
    try:
        with httpx.Client() as client:
            response = client.get(url, headers=get_consul_headers(), timeout=5.0)
            return response.status_code == 200
    except Exception:
        return False


def wait_for_consul(max_attempts: int = 30, delay: float = 2.0) -> bool:
    """
    Wait for Consul to become available.
    
    Args:
        max_attempts: Maximum number of connection attempts
        delay: Delay between attempts in seconds
        
    Returns:
        True if Consul became available, False otherwise
    """
    import time
    
    logger.info("Waiting for Consul to be ready", consul_url=get_consul_url())
    
    for attempt in range(1, max_attempts + 1):
        if check_consul_health():
            logger.info("Consul is ready", attempts=attempt)
            return True
        
        logger.info(
            "Consul not ready, retrying",
            attempt=attempt,
            max_attempts=max_attempts,
        )
        time.sleep(delay)
    
    logger.error(
        "Consul did not become ready",
        max_attempts=max_attempts,
    )
    return False


# Type coercion utilities (matching ConsulConfigLoader pattern)
def coerce_bool(value: str | bool | None) -> bool:
    """Convert string value to boolean."""
    if isinstance(value, bool):
        return value
    if value is None:
        return False
    return value.lower() in ("true", "1", "yes", "on")


def coerce_int(value: str | int | None, default: int = 0) -> int:
    """Convert string value to integer."""
    if isinstance(value, int):
        return value
    if value is None:
        return default
    try:
        return int(value)
    except ValueError:
        return default


def coerce_float(value: str | float | None, default: float = 0.0) -> float:
    """Convert string value to float."""
    if isinstance(value, float):
        return value
    if value is None:
        return default
    try:
        return float(value)
    except ValueError:
        return default


def coerce_list(value: str | list | None, separator: str = ",") -> list[str]:
    """Convert comma-separated string to list."""
    if isinstance(value, list):
        return value
    if value is None or value == "":
        return []
    return [item.strip() for item in value.split(separator) if item.strip()]

```

---

## backend/autonomous-crawler-service/src/config/settings.py

```py
"""Application settings using Pydantic Settings."""

from functools import lru_cache
from typing import Literal

from pydantic import Field
from pydantic_settings import BaseSettings, SettingsConfigDict


class KafkaSettings(BaseSettings):
    """Kafka connection settings."""

    model_config = SettingsConfigDict(env_prefix="KAFKA_")

    bootstrap_servers: str = Field(
        default="localhost:9092",
        description="Kafka bootstrap servers",
    )
    consumer_group_id: str = Field(
        default="autonomous-crawler-group",
        description="Consumer group ID",
    )
    browser_task_topic: str = Field(
        default="newsinsight.crawl.browser.tasks",
        description="Topic for browser task messages",
    )
    crawl_result_topic: str = Field(
        default="newsinsight.crawl.results",
        description="Topic for crawl result messages",
    )
    auto_offset_reset: Literal["earliest", "latest"] = Field(
        default="earliest",
        description="Auto offset reset policy",
    )
    enable_auto_commit: bool = Field(
        default=False,
        description="Enable auto commit (disabled for manual acknowledgment)",
    )
    max_poll_records: int = Field(
        default=1,
        description="Maximum records per poll (1 for sequential processing)",
    )
    session_timeout_ms: int = Field(
        default=30000,
        description="Session timeout in milliseconds",
    )
    heartbeat_interval_ms: int = Field(
        default=10000,
        description="Heartbeat interval in milliseconds",
    )


class BrowserSettings(BaseSettings):
    """Browser and AI agent settings."""

    model_config = SettingsConfigDict(env_prefix="BROWSER_")

    headless: bool = Field(
        default=True,
        description="Run browser in headless mode",
    )
    max_concurrent_sessions: int = Field(
        default=2,
        description="Maximum concurrent browser sessions",
    )
    default_timeout_seconds: int = Field(
        default=300,
        description="Default timeout for browser tasks in seconds",
    )
    max_timeout_seconds: int = Field(
        default=600,
        description="Maximum allowed timeout in seconds",
    )
    screenshot_dir: str = Field(
        default="/tmp/crawler-screenshots",
        description="Directory for storing screenshots",
    )
    user_agent: str = Field(
        default="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
        description="User agent string for browser",
    )
    is_docker_env: bool = Field(
        default=False,
        description="Whether running in Docker environment (enables no-sandbox, etc.)",
    )
    # Browser backend selection
    backend: Literal["playwright", "camoufox"] = Field(
        default="playwright",
        description="Browser backend: 'playwright' (Chrome/Chromium) or 'camoufox' (Firefox anti-detect)",
    )


class LLMSettings(BaseSettings):
    """LLM provider settings.

    Supported providers:
    - openai: OpenAI API (default)
    - anthropic: Anthropic Claude API
    - openrouter: OpenRouter (access to multiple models via single API)
    - ollama: Local Ollama server
    - custom: Custom OpenAI-compatible REST API endpoint
    """

    model_config = SettingsConfigDict(env_prefix="LLM_")

    provider: Literal["openai", "anthropic", "openrouter", "ollama", "custom"] = Field(
        default="openai",
        description="LLM provider: openai, anthropic, openrouter, ollama, or custom",
    )

    # OpenAI settings
    openai_api_key: str = Field(
        default="",
        description="OpenAI API key",
    )
    openai_model: str = Field(
        default="gpt-4o",
        description="OpenAI model to use",
    )
    openai_base_url: str = Field(
        default="",
        description="Custom OpenAI API base URL (leave empty for default)",
    )

    # Anthropic settings
    anthropic_api_key: str = Field(
        default="",
        description="Anthropic API key",
    )
    anthropic_model: str = Field(
        default="claude-3-5-sonnet-20241022",
        description="Anthropic model to use",
    )

    # OpenRouter settings (https://openrouter.ai)
    openrouter_api_key: str = Field(
        default="",
        description="OpenRouter API key",
    )
    openrouter_model: str = Field(
        default="anthropic/claude-3.5-sonnet",
        description="OpenRouter model (e.g., anthropic/claude-3.5-sonnet, openai/gpt-4o, google/gemini-pro)",
    )
    openrouter_base_url: str = Field(
        default="https://openrouter.ai/api/v1",
        description="OpenRouter API base URL",
    )

    # Ollama settings (local LLM)
    ollama_base_url: str = Field(
        default="http://localhost:11434",
        description="Ollama server URL",
    )
    ollama_model: str = Field(
        default="llama3.2",
        description="Ollama model name",
    )

    # Azure OpenAI settings
    azure_api_key: str = Field(
        default="",
        description="Azure OpenAI API key",
    )
    azure_endpoint: str = Field(
        default="",
        description="Azure OpenAI endpoint URL (e.g., https://your-resource.openai.azure.com)",
    )
    azure_deployment_name: str = Field(
        default="gpt-4o",
        description="Azure OpenAI deployment name",
    )
    azure_api_version: str = Field(
        default="2024-02-15-preview",
        description="Azure OpenAI API version",
    )

    # Custom REST API settings (supports non-OpenAI-compatible APIs)
    custom_api_key: str = Field(
        default="",
        description="API key for custom endpoint (optional if API doesn't require auth)",
    )
    custom_base_url: str = Field(
        default="",
        description="Custom REST API endpoint URL (e.g., https://workflow.nodove.com/webhook/aidove)",
    )
    custom_model: str = Field(
        default="",
        description="Model name for custom endpoint (used in request if format includes it)",
    )
    custom_request_format: str = Field(
        default="",
        description="""JSON template for custom API request body.
Use placeholders: {prompt} for user message, {session_id} for session ID, {model} for model name.
Example for AI Dove: {"chatInput": "{prompt}", "sessionId": "{session_id}"}
Example for standard: {"message": "{prompt}", "model": "{model}"}
Leave empty for OpenAI-compatible format.""",
    )
    custom_response_path: str = Field(
        default="reply",
        description="""JSON path to extract response text from API response.
Use dot notation for nested fields (e.g., 'choices.0.message.content' for OpenAI format).
For AI Dove API, use 'reply'.""",
    )
    custom_headers: str = Field(
        default="",
        description="""Custom HTTP headers as JSON object.
Example: {"X-Custom-Header": "value", "Authorization": "Bearer {api_key}"}
Use {api_key} placeholder for API key substitution.""",
    )

    # Common settings
    temperature: float = Field(
        default=0.0,
        description="LLM temperature",
    )
    max_tokens: int = Field(
        default=4096,
        description="Maximum tokens for LLM response",
    )


class SearchSettings(BaseSettings):
    """Search provider settings."""

    model_config = SettingsConfigDict(env_prefix="SEARCH_")

    # API Keys
    brave_api_key: str = Field(
        default="",
        description="Brave Search API key",
    )
    tavily_api_key: str = Field(
        default="",
        description="Tavily Search API key",
    )
    perplexity_api_key: str = Field(
        default="",
        description="Perplexity API key",
    )

    # Configuration
    timeout: float = Field(
        default=30.0,
        description="Timeout for search requests in seconds",
    )
    max_results_per_provider: int = Field(
        default=10,
        description="Maximum results per search provider",
    )
    max_total_results: int = Field(
        default=30,
        description="Maximum total aggregated results",
    )
    enable_parallel: bool = Field(
        default=True,
        description="Enable parallel search across providers",
    )

    # RRF (Reciprocal Rank Fusion) Settings
    enable_rrf: bool = Field(
        default=True,
        description="Enable RRF-based multi-strategy search for improved accuracy",
    )
    rrf_k: int = Field(
        default=60,
        ge=1,
        le=1000,
        description="RRF constant k (higher = more weight to lower ranks, default: 60)",
    )
    enable_semantic_rrf: bool = Field(
        default=True,
        description="Enable semantic similarity scoring in RRF",
    )
    enable_query_expansion: bool = Field(
        default=True,
        description="Enable LLM-based query expansion for better search accuracy",
    )
    max_expanded_queries: int = Field(
        default=5,
        ge=1,
        le=10,
        description="Maximum number of expanded queries for multi-strategy search",
    )
    cache_query_analysis: bool = Field(
        default=True,
        description="Cache query analysis results to reduce LLM calls",
    )


class StealthSettings(BaseSettings):
    """Stealth/anti-detection settings."""

    model_config = SettingsConfigDict(env_prefix="STEALTH_")

    enabled: bool = Field(
        default=True,
        description="Enable stealth mode for browser",
    )
    hide_webdriver: bool = Field(
        default=True,
        description="Hide webdriver detection flags",
    )
    hide_automation: bool = Field(
        default=True,
        description="Hide automation flags",
    )
    mask_webgl: bool = Field(
        default=True,
        description="Mask WebGL vendor/renderer",
    )
    random_user_agent: bool = Field(
        default=False,
        description="Use random user agent on each session",
    )
    # NopeCHA CAPTCHA solver extension
    use_nopecha: bool = Field(
        default=True,
        description="Enable NopeCHA extension for automatic CAPTCHA solving",
    )
    nopecha_api_key: str = Field(
        default="",
        description="NopeCHA API key (optional, for faster solving)",
    )
    # Advanced stealth patches
    use_advanced_patches: bool = Field(
        default=True,
        description="Apply advanced JavaScript patches for bot detection bypass",
    )
    # Human behavior simulation
    simulate_human_behavior: bool = Field(
        default=True,
        description="Simulate human-like mouse movements and scrolling",
    )


class CamoufoxSettings(BaseSettings):
    """Camoufox Firefox-based anti-detect browser settings."""

    model_config = SettingsConfigDict(env_prefix="CAMOUFOX_")

    enabled: bool = Field(
        default=True,
        description="Enable Camoufox as alternative browser (when BROWSER_BACKEND=camoufox)",
    )
    humanize: bool = Field(
        default=True,
        description="Enable human-like behavior simulation",
    )
    humanize_level: int = Field(
        default=2,
        ge=1,
        le=3,
        description="Humanization level (1=low, 2=medium, 3=high)",
    )
    geoip: bool = Field(
        default=True,
        description="Enable GeoIP-based fingerprint matching",
    )
    block_webrtc: bool = Field(
        default=True,
        description="Block WebRTC to prevent IP leaks",
    )
    block_images: bool = Field(
        default=False,
        description="Block images for faster loading",
    )
    locale: str = Field(
        default="ko-KR",
        description="Browser locale",
    )
    timezone: str = Field(
        default="Asia/Seoul",
        description="Browser timezone",
    )
    os_type: Literal["windows", "macos", "linux", "random"] = Field(
        default="random",
        description="OS fingerprint type",
    )


class CaptchaSettings(BaseSettings):
    """CAPTCHA solving settings."""

    model_config = SettingsConfigDict(env_prefix="CAPTCHA_")

    enabled: bool = Field(
        default=True,
        description="Enable CAPTCHA solving",
    )
    prefer_audio: bool = Field(
        default=True,
        description="Prefer audio challenges for reCAPTCHA",
    )
    cloudflare_delay: int = Field(
        default=10,
        description="Delay for Cloudflare challenge (seconds)",
    )
    max_attempts: int = Field(
        default=3,
        description="Maximum CAPTCHA solve attempts",
    )
    # Paid CAPTCHA solver settings (more reliable for search portals)
    capsolver_api_key: str = Field(
        default="",
        description="CapSolver API key (https://capsolver.com) - Recommended for Turnstile",
    )
    twocaptcha_api_key: str = Field(
        default="",
        description="2Captcha API key (https://2captcha.com)",
    )
    prefer_paid_solver: bool = Field(
        default=True,
        description="Prefer paid solvers over free ones when API key is available",
    )
    paid_solver_timeout: float = Field(
        default=120.0,
        description="Timeout for paid CAPTCHA solving (seconds)",
    )


class RedisSettings(BaseSettings):
    """Redis connection settings for task state persistence."""

    model_config = SettingsConfigDict(env_prefix="REDIS_")

    enabled: bool = Field(
        default=True,
        description="Enable Redis for task state persistence",
    )
    url: str = Field(
        default="redis://localhost:6379/4",
        description="Redis connection URL",
    )
    prefix: str = Field(
        default="autonomous_crawler",
        description="Key prefix for Redis entries",
    )
    result_ttl_hours: int = Field(
        default=48,
        description="Task result TTL in hours",
    )
    socket_timeout: float = Field(
        default=5.0,
        description="Socket timeout in seconds",
    )
    connection_timeout: float = Field(
        default=5.0,
        description="Connection timeout in seconds",
    )
    max_connections: int = Field(
        default=10,
        description="Maximum Redis connections in pool",
    )
    retry_on_timeout: bool = Field(
        default=True,
        description="Retry operations on timeout",
    )


class MetricsSettings(BaseSettings):
    """Prometheus metrics settings."""

    model_config = SettingsConfigDict(env_prefix="METRICS_")

    enabled: bool = Field(
        default=True,
        description="Enable Prometheus metrics",
    )
    port: int = Field(
        default=9090,
        description="Metrics server port",
    )
    path: str = Field(
        default="/metrics",
        description="Metrics endpoint path",
    )


class Settings(BaseSettings):
    """Main application settings."""

    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        extra="ignore",
    )

    # Service settings
    service_name: str = Field(
        default="autonomous-crawler-service",
        description="Service name",
    )
    log_level: Literal["DEBUG", "INFO", "WARNING", "ERROR"] = Field(
        default="INFO",
        description="Logging level",
    )
    log_format: Literal["json", "console"] = Field(
        default="json",
        description="Log output format",
    )

    # Nested settings
    kafka: KafkaSettings = Field(default_factory=KafkaSettings)
    browser: BrowserSettings = Field(default_factory=BrowserSettings)
    llm: LLMSettings = Field(default_factory=LLMSettings)
    search: SearchSettings = Field(default_factory=SearchSettings)
    stealth: StealthSettings = Field(default_factory=StealthSettings)
    captcha: CaptchaSettings = Field(default_factory=CaptchaSettings)
    camoufox: CamoufoxSettings = Field(default_factory=CamoufoxSettings)
    redis: RedisSettings = Field(default_factory=RedisSettings)
    metrics: MetricsSettings = Field(default_factory=MetricsSettings)


@lru_cache
def get_settings() -> Settings:
    """Get cached application settings."""
    return Settings()

```

---

## backend/autonomous-crawler-service/src/crawler/__init__.py

```py
"""Crawler module for autonomous-crawler-service."""

from .agent import AutonomousCrawlerAgent
from .policies import CrawlPolicy, get_policy_prompt
from .url_filter import should_block_url, is_likely_article_url, filter_urls, clean_url

__all__ = [
    "AutonomousCrawlerAgent",
    "CrawlPolicy",
    "get_policy_prompt",
    "should_block_url",
    "is_likely_article_url",
    "filter_urls",
    "clean_url",
]

```

---

## backend/autonomous-crawler-service/src/crawler/agent.py

```py
"""Autonomous crawler agent using browser-use with CAPTCHA bypass and proxy rotation."""

import asyncio
import json
import os
import re
import time
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Optional, TYPE_CHECKING

import httpx
import structlog
from browser_use.agent.service import Agent
from browser_use.browser.session import BrowserSession
from browser_use.browser.profile import BrowserProfile, ProxySettings
from browser_use.llm.openai.chat import ChatOpenAI
from browser_use.llm.anthropic.chat import ChatAnthropic
from pydantic import BaseModel

from src.config import Settings

# Proxy rotation client
try:
    import sys

    sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "..", ".."))
    from shared.proxy_client import ProxyRotationClient, ProxyInfo

    PROXY_CLIENT_AVAILABLE = True
except ImportError:
    PROXY_CLIENT_AVAILABLE = False
    ProxyRotationClient = None  # type: ignore
    ProxyInfo = None  # type: ignore
from src.crawler.policies import CrawlPolicy, get_policy_prompt
from src.kafka.messages import BrowserTaskMessage, CrawlResultMessage
from src.captcha.stealth import (
    StealthConfig,
    EnhancedStealthConfig,
    apply_stealth_to_playwright_async,
    get_undetected_browser_args,
    get_stealth_browser_args_with_extensions,
)
from src.captcha import (
    CaptchaSolverOrchestrator,
    CaptchaType,
    AdvancedStealthPatcher,
    HumanBehaviorSimulator,
    # Camoufox
    CamoufoxConfig,
    CamoufoxHelper,
    create_camoufox_browser,
    get_recommended_camoufox_config,
    is_camoufox_available,
)
from src.search.orchestrator import (
    ParallelSearchOrchestrator,
    RRFSearchOrchestrator,
    create_rrf_orchestrator,
)
from src.search.brave import BraveSearchProvider
from src.search.tavily import TavilySearchProvider
from src.search.perplexity import PerplexitySearchProvider
from src.search.query_analyzer import QueryAnalyzer

if TYPE_CHECKING:
    from browser_use.agent.service import Agent as BrowserUseAgent

logger = structlog.get_logger(__name__)


# =============================================================================
# Custom REST API Adapter for non-OpenAI-compatible APIs
# =============================================================================


class CustomRESTAPIClient:
    """
    Adapter for custom REST APIs with configurable request/response formats.

    Supports APIs like AI Dove that use non-standard request formats.
    Implements a minimal interface compatible with browser-use's LLM requirements.
    """

    def __init__(
        self,
        base_url: str,
        api_key: str = "",
        model: str = "",
        request_format: str = "",
        response_path: str = "reply",
        custom_headers: str = "",
        temperature: float = 0.0,
        timeout: float = 120.0,
    ):
        """
        Initialize the custom REST API client.

        Args:
            base_url: The API endpoint URL
            api_key: Optional API key for authentication
            model: Model name (used in request if format includes it)
            request_format: JSON template for request body with placeholders
            response_path: Dot-notation path to extract response from JSON
            custom_headers: JSON string of custom headers
            temperature: Temperature parameter (passed to API if supported)
            timeout: Request timeout in seconds
        """
        self.base_url = base_url.rstrip("/")
        self.api_key = api_key
        self.model = model
        self.request_format = request_format
        self.response_path = response_path
        self.custom_headers = custom_headers
        self.temperature = temperature
        self.timeout = timeout
        self._session_id = f"crawler_{int(time.time())}"

        # For browser-use compatibility
        self.provider = "custom"
        self.model_name = model or "custom"  # Required by browser-use telemetry

    def _build_headers(self) -> dict[str, str]:
        """Build HTTP headers for the request."""
        headers = {"Content-Type": "application/json"}

        # Parse custom headers if provided
        if self.custom_headers:
            try:
                custom = json.loads(self.custom_headers)
                for key, value in custom.items():
                    # Replace {api_key} placeholder
                    if isinstance(value, str):
                        value = value.replace("{api_key}", self.api_key)
                    headers[key] = value
            except json.JSONDecodeError:
                logger.warning("Failed to parse custom_headers JSON", headers=self.custom_headers)

        # Add default Authorization header if API key is provided and not in custom headers
        if self.api_key and "Authorization" not in headers:
            headers["Authorization"] = f"Bearer {self.api_key}"

        return headers

    def _build_request_body(self, prompt: str) -> dict[str, Any]:
        """Build request body from template or default format."""
        # Ensure prompt is a string
        if not isinstance(prompt, str):
            if isinstance(prompt, list):
                # Join list items into a single string
                prompt = "\n".join(str(item) for item in prompt)
            else:
                prompt = str(prompt)

        if self.request_format:
            try:
                # Escape special characters for JSON string embedding
                # This handles quotes, newlines, backslashes, etc.
                escaped_prompt = json.dumps(prompt)[
                    1:-1
                ]  # Remove surrounding quotes from json.dumps

                # Parse the template and substitute placeholders
                template = self.request_format
                template = template.replace("{prompt}", escaped_prompt)
                template = template.replace("{session_id}", self._session_id)
                template = template.replace("{model}", self.model)
                template = template.replace("{temperature}", str(self.temperature))
                return json.loads(template)
            except json.JSONDecodeError as e:
                logger.error(
                    "Failed to parse request_format JSON",
                    error=str(e),
                    template=self.request_format,
                )
                raise ValueError(f"Invalid custom_request_format JSON: {e}")

        # Default OpenAI-compatible format
        return {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": self.temperature,
        }

    def _extract_response(self, response_json: dict[str, Any]) -> str:
        """Extract response text using the configured path."""
        if not self.response_path:
            # Return full response as string
            return json.dumps(response_json)

        # Navigate the JSON path (supports dot notation and array indices)
        current = response_json
        for part in self.response_path.split("."):
            if part.isdigit():
                # Array index
                idx = int(part)
                if isinstance(current, list) and len(current) > idx:
                    current = current[idx]
                else:
                    raise KeyError(f"Array index {idx} not found in response")
            elif isinstance(current, dict):
                if part in current:
                    current = current[part]
                else:
                    raise KeyError(f"Key '{part}' not found in response: {list(current.keys())}")
            else:
                raise KeyError(f"Cannot navigate '{part}' in non-dict/list value")

        return str(current) if current is not None else ""

    async def ainvoke(self, messages: list[dict[str, Any]], config: Any = None, **kwargs) -> Any:
        """
        Async invoke the custom API (compatible with LangChain interface).

        Args:
            messages: List of message dicts with 'role' and 'content' keys
            config: Optional config object (ignored for custom API, for LangChain compatibility)
            **kwargs: Additional arguments (ignored for custom API)

        Returns:
            Response object with 'content' attribute
        """
        # Extract the last user message as the prompt
        prompt = ""
        for msg in reversed(messages):
            if isinstance(msg, dict) and msg.get("role") == "user":
                prompt = msg.get("content", "")
                break
            elif hasattr(msg, "content"):
                prompt = msg.content
                break

        if not prompt:
            # Fallback: concatenate all messages
            prompt = "\n".join(
                msg.get("content", str(msg)) if isinstance(msg, dict) else str(msg)
                for msg in messages
            )

        headers = self._build_headers()
        body = self._build_request_body(prompt)

        logger.debug(
            "Custom API request",
            url=self.base_url,
            body_keys=list(body.keys()) if isinstance(body, dict) else "raw",
        )

        async with httpx.AsyncClient(timeout=self.timeout) as client:
            response = await client.post(
                self.base_url,
                headers=headers,
                json=body,
            )

            if response.status_code != 200:
                error_text = response.text[:500]
                logger.error(
                    "Custom API error",
                    status=response.status_code,
                    error=error_text,
                )
                raise RuntimeError(f"Custom API error {response.status_code}: {error_text}")

            response_json = response.json()
            content = self._extract_response(response_json)

            logger.debug(
                "Custom API response",
                response_keys=list(response_json.keys())
                if isinstance(response_json, dict)
                else "raw",
                content_length=len(content),
            )

        # Return an object with all attributes expected by browser-use/LangChain
        class Response:
            """Response object compatible with LangChain/browser-use expectations."""

            def __init__(self, text: str):
                self.content = text
                # Usage as dict with all required fields for browser-use ChatInvokeUsage Pydantic model
                self.usage = {
                    "prompt_tokens": 0,
                    "completion_tokens": 0,
                    "total_tokens": 0,
                    "prompt_cached_tokens": 0,
                    "prompt_cache_creation_tokens": 0,
                    "prompt_image_tokens": 0,
                }
                # Additional attributes expected by browser-use
                self.response_metadata = {}
                self.completion = text  # Some frameworks expect 'completion' instead of 'content'
                self.text = text  # Alias for content
                self.message = type("Message", (), {"content": text})()  # Mock message object
                self.id = "custom-response"
                self.model = "aidove"

        return Response(content)

    def invoke(self, messages: list[dict[str, Any]], **kwargs) -> Any:
        """Sync invoke - runs async version in event loop."""
        return asyncio.run(self.ainvoke(messages, **kwargs))


# =============================================================================
# CAPTCHA Detection Hook for browser-use Agent
# =============================================================================


async def create_captcha_detection_hook(
    crawler_agent: "AutonomousCrawlerAgent",
    on_captcha_detected: callable = None,
) -> callable:
    """
    Create a hook function for browser-use Agent that detects CAPTCHAs.

    This hook is called at the start of each step to check for CAPTCHAs
    and attempt to solve them before the agent takes action.

    Args:
        crawler_agent: The AutonomousCrawlerAgent instance
        on_captcha_detected: Optional callback when CAPTCHA is detected

    Returns:
        Async hook function compatible with browser-use Agent
    """

    async def on_step_start_hook(agent: "BrowserUseAgent") -> None:
        """Hook called at the start of each browser-use step."""
        try:
            # Get the current page from browser session
            browser_session = agent.browser_session
            if not browser_session:
                return

            # Access the current page
            page = None
            try:
                # browser-use stores pages internally
                if hasattr(browser_session, "_context") and browser_session._context:
                    pages = browser_session._context.pages
                    if pages:
                        page = pages[-1]  # Get the most recent page
            except Exception:
                pass

            if not page:
                return

            # Check for CAPTCHA indicators
            captcha_detected = await _quick_captcha_check(page)

            if captcha_detected:
                logger.info("CAPTCHA detected in browser-use step, attempting to solve...")

                if on_captcha_detected:
                    await on_captcha_detected(captcha_detected)

                # Try to solve the CAPTCHA
                solved = await crawler_agent._detect_and_handle_captcha(page)

                if solved:
                    logger.info("CAPTCHA solved successfully, continuing agent step")
                else:
                    logger.warning("CAPTCHA could not be solved, agent may fail on this step")

                    # Simulate human behavior to appear more legitimate
                    if crawler_agent._stealth_config.enable_human_simulation:
                        await crawler_agent._simulate_human_behavior(page)

        except Exception as e:
            logger.debug("Error in CAPTCHA detection hook", error=str(e))

    return on_step_start_hook


async def create_stealth_hook(crawler_agent: "AutonomousCrawlerAgent") -> callable:
    """
    Create a hook function that applies stealth patches after navigation.

    This hook is called at the end of each step to re-apply stealth patches
    if the page has navigated to a new URL.
    """
    _last_url = {"value": None}

    async def on_step_end_hook(agent: "BrowserUseAgent") -> None:
        """Hook called at the end of each browser-use step."""
        try:
            browser_session = agent.browser_session
            if not browser_session:
                return

            page = None
            try:
                if hasattr(browser_session, "_context") and browser_session._context:
                    pages = browser_session._context.pages
                    if pages:
                        page = pages[-1]
            except Exception:
                pass

            if not page:
                return

            current_url = page.url

            # Only apply patches if we navigated to a new URL
            if current_url != _last_url["value"]:
                _last_url["value"] = current_url

                # Re-apply stealth patches to the new page
                await AdvancedStealthPatcher.apply_to_page(page)

                # Brief human-like delay after navigation
                if crawler_agent._stealth_config.enable_human_simulation:
                    await asyncio.sleep(0.5)

        except Exception as e:
            logger.debug("Error in stealth hook", error=str(e))

    return on_step_end_hook


async def create_url_filter_hook(
    crawler_agent: "AutonomousCrawlerAgent",
    on_blocked_url: callable = None,
) -> callable:
    """
    Create a hook function that filters out blocked URLs during navigation.

    This hook is called at the end of each step to check if the browser
    navigated to a blocked URL (login, help, marketing pages, etc.) and
    navigates back if so.

    Args:
        crawler_agent: The AutonomousCrawlerAgent instance
        on_blocked_url: Optional callback when a blocked URL is detected

    Returns:
        Async hook function compatible with browser-use Agent
    """
    from src.crawler.url_filter import should_block_url, clean_url

    _visited_urls: set[str] = set()
    _blocked_count = {"value": 0}
    _max_blocked_navigations = 5  # Give up after too many blocked pages

    async def on_step_end_hook(agent: "BrowserUseAgent") -> None:
        """Hook called at the end of each browser-use step to filter blocked URLs."""
        try:
            browser_session = agent.browser_session
            if not browser_session:
                return

            page = None
            try:
                if hasattr(browser_session, "_context") and browser_session._context:
                    pages = browser_session._context.pages
                    if pages:
                        page = pages[-1]
            except Exception:
                pass

            if not page:
                return

            current_url = page.url

            # Skip if we've already checked this URL
            if current_url in _visited_urls:
                return

            _visited_urls.add(current_url)

            # Check if this URL should be blocked
            cleaned_url = clean_url(current_url)
            if should_block_url(cleaned_url, log_reason=True):
                _blocked_count["value"] += 1

                logger.warning(
                    "Browser navigated to blocked URL, will navigate back",
                    blocked_url=current_url[:200],
                    blocked_count=_blocked_count["value"],
                )

                if on_blocked_url:
                    try:
                        await on_blocked_url(current_url)
                    except Exception:
                        pass

                # Navigate back to previous page if we haven't hit the limit
                if _blocked_count["value"] <= _max_blocked_navigations:
                    try:
                        await page.go_back(timeout=5000)
                        logger.debug("Navigated back from blocked URL")
                    except Exception as e:
                        logger.debug("Could not navigate back", error=str(e))
                else:
                    logger.warning(
                        "Too many blocked URLs encountered, stopping back navigation",
                        blocked_count=_blocked_count["value"],
                    )

        except Exception as e:
            logger.debug("Error in URL filter hook", error=str(e))

    return on_step_end_hook


async def _quick_captcha_check(page) -> str | None:
    """
    Quick check for common CAPTCHA indicators on a page.

    Returns the CAPTCHA type if detected, None otherwise.
    """
    captcha_indicators = [
        # Cloudflare
        (
            "cloudflare",
            [
                "#challenge-running",
                ".cf-browser-verification",
                "iframe[src*='turnstile']",
                "#cf-turnstile",
                "div[class*='challenge']",
            ],
        ),
        # reCAPTCHA
        (
            "recaptcha",
            [
                "iframe[src*='recaptcha']",
                ".g-recaptcha",
                "#recaptcha",
                "div[class*='recaptcha']",
            ],
        ),
        # hCaptcha
        (
            "hcaptcha",
            [
                "iframe[src*='hcaptcha']",
                ".h-captcha",
                "div[class*='hcaptcha']",
            ],
        ),
        # Generic bot detection
        (
            "bot_detection",
            [
                "text=checking your browser",
                "text=please verify you are human",
                "text=access denied",
                "text=blocked",
            ],
        ),
    ]

    for captcha_type, selectors in captcha_indicators:
        for selector in selectors:
            try:
                if selector.startswith("text="):
                    # Text-based detection
                    text = selector[5:].lower()
                    page_text = await page.inner_text("body")
                    if text in page_text.lower():
                        return captcha_type
                else:
                    # Selector-based detection
                    element = await page.query_selector(selector)
                    if element:
                        is_visible = await element.is_visible()
                        if is_visible:
                            return captcha_type
            except Exception:
                continue

    return None


class ExtractedArticle(BaseModel):
    """Extracted article content from a page."""

    url: str
    title: str
    content: str
    published_at: str | None = None
    author: str | None = None
    summary: str | None = None
    extraction_time: datetime = field(default_factory=datetime.now)


@dataclass
class CrawlSession:
    """Tracks the state of a crawling session."""

    job_id: int
    source_id: int
    seed_url: str
    max_depth: int
    max_pages: int
    budget_seconds: int
    policy: CrawlPolicy
    focus_keywords: list[str]
    excluded_domains: list[str]

    # Runtime state
    visited_urls: set[str] = field(default_factory=set)
    extracted_articles: list[CrawlResultMessage] = field(default_factory=list)
    start_time: datetime | None = None
    end_time: datetime | None = None
    error: str | None = None

    # AutoCrawl metadata (passed from BrowserTaskMessage.metadata)
    metadata: dict[str, Any] | None = None

    def is_budget_exceeded(self) -> bool:
        """Check if time budget has been exceeded."""
        if not self.start_time:
            return False
        elapsed = (datetime.now() - self.start_time).total_seconds()
        return elapsed >= self.budget_seconds

    def is_page_limit_reached(self) -> bool:
        """Check if page limit has been reached."""
        return len(self.visited_urls) >= self.max_pages

    def can_continue(self) -> bool:
        """Check if crawling can continue."""
        return not self.is_budget_exceeded() and not self.is_page_limit_reached()


class AutonomousCrawlerAgent:
    """
    AI-driven autonomous web crawler using browser-use.

    Consumes BrowserTaskMessage from Kafka and produces CrawlResultMessage
    for each extracted article.

    Supports two browser backends:
    - Playwright (Chrome/Chromium) with stealth patches and NopeCHA extension
    - Camoufox (Firefox-based) anti-detect browser with built-in fingerprint spoofing

    Integrates with IP Rotation service to:
    - Automatically rotate proxies for each crawl session
    - Report CAPTCHA encounters to help weighted proxy selection
    - Retry with different proxies when CAPTCHA solving fails
    """

    def __init__(self, settings: Settings) -> None:
        self.settings = settings
        self._browser_session: BrowserSession | None = None
        self._camoufox_browser: Any = None  # Camoufox browser instance
        self._llm = self._create_llm()
        self._search_orchestrator: ParallelSearchOrchestrator | None = None
        self._rrf_search_orchestrator: RRFSearchOrchestrator | None = None
        self._query_analyzer: QueryAnalyzer | None = None
        self._captcha_solver: CaptchaSolverOrchestrator | None = None
        self._stealth_config = EnhancedStealthConfig(
            use_nopecha=getattr(settings.stealth, "use_nopecha", True),
            nopecha_api_key=getattr(settings.stealth, "nopecha_api_key", ""),
            use_camoufox=getattr(settings.browser, "backend", "playwright") == "camoufox",
            enable_human_simulation=getattr(settings.stealth, "simulate_human_behavior", True),
        )

        # Determine browser backend
        self._use_camoufox = getattr(settings.browser, "backend", "playwright") == "camoufox"
        if self._use_camoufox and not is_camoufox_available():
            logger.warning("Camoufox requested but not available, falling back to Playwright")
            self._use_camoufox = False

        # Proxy rotation integration
        self._proxy_client: Optional[Any] = None
        self._current_proxy: Optional[Any] = None  # Current ProxyInfo
        self._use_proxy_rotation = getattr(settings, "use_proxy_rotation", True)
        self._proxy_rotation_url = getattr(
            settings,
            "proxy_rotation_url",
            os.environ.get("PROXY_ROTATION_URL", "http://ip-rotation:8050"),
        )

        # Initialize proxy client if available
        if PROXY_CLIENT_AVAILABLE and self._use_proxy_rotation:
            self._proxy_client = ProxyRotationClient(
                base_url=self._proxy_rotation_url,
                timeout=5.0,
                enabled=True,
            )
            logger.info("Proxy rotation enabled", url=self._proxy_rotation_url)

    def _create_llm(self) -> ChatOpenAI | ChatAnthropic:
        """Create the LLM instance based on settings.

        Uses browser-use's LLM classes which implement the BaseChatModel protocol
        with the required .provider property.

        Supported providers:
        - openai: OpenAI API
        - anthropic: Anthropic Claude API
        - openrouter: OpenRouter (multiple models via single API)
        - ollama: Local Ollama server
        - custom: Custom OpenAI-compatible REST API
        """
        llm_settings = self.settings.llm
        provider = llm_settings.provider.lower()

        if provider == "anthropic":
            logger.info("Using Anthropic provider", model=llm_settings.anthropic_model)
            return ChatAnthropic(
                model=llm_settings.anthropic_model,
                api_key=llm_settings.anthropic_api_key,
                temperature=llm_settings.temperature,
                max_tokens=llm_settings.max_tokens,
            )

        elif provider == "openrouter":
            # OpenRouter uses OpenAI-compatible API
            logger.info("Using OpenRouter provider", model=llm_settings.openrouter_model)
            return ChatOpenAI(
                model=llm_settings.openrouter_model,
                api_key=llm_settings.openrouter_api_key,
                base_url=llm_settings.openrouter_base_url,
                temperature=llm_settings.temperature,
                max_completion_tokens=llm_settings.max_tokens,
                default_headers={
                    "HTTP-Referer": "https://newsinsight.app",
                    "X-Title": "NewsInsight Crawler",
                },
            )

        elif provider == "ollama":
            # Ollama uses OpenAI-compatible API
            logger.info(
                "Using Ollama provider",
                model=llm_settings.ollama_model,
                base_url=llm_settings.ollama_base_url,
            )
            return ChatOpenAI(
                model=llm_settings.ollama_model,
                api_key="ollama",  # Ollama doesn't require API key but field is required
                base_url=f"{llm_settings.ollama_base_url}/v1",
                temperature=llm_settings.temperature,
                max_completion_tokens=llm_settings.max_tokens,
            )

        elif provider == "custom":
            # Custom REST API endpoint (supports non-OpenAI-compatible APIs)
            if not llm_settings.custom_base_url:
                raise ValueError("LLM_CUSTOM_BASE_URL is required when using custom provider")

            # Check if custom request format is provided (non-OpenAI-compatible API)
            if llm_settings.custom_request_format:
                logger.info(
                    "Using custom REST API provider with custom format",
                    base_url=llm_settings.custom_base_url,
                    response_path=llm_settings.custom_response_path,
                )
                return CustomRESTAPIClient(
                    base_url=llm_settings.custom_base_url,
                    api_key=llm_settings.custom_api_key,
                    model=llm_settings.custom_model,
                    request_format=llm_settings.custom_request_format,
                    response_path=llm_settings.custom_response_path,
                    custom_headers=llm_settings.custom_headers,
                    temperature=llm_settings.temperature,
                )
            else:
                # Fallback to OpenAI-compatible format
                logger.info(
                    "Using custom provider (OpenAI-compatible)",
                    model=llm_settings.custom_model,
                    base_url=llm_settings.custom_base_url,
                )
                return ChatOpenAI(
                    model=llm_settings.custom_model,
                    api_key=llm_settings.custom_api_key or "not-required",
                    base_url=llm_settings.custom_base_url,
                    temperature=llm_settings.temperature,
                    max_completion_tokens=llm_settings.max_tokens,
                )

        else:
            # Default: OpenAI
            base_url = llm_settings.openai_base_url or None
            logger.info("Using OpenAI provider", model=llm_settings.openai_model, base_url=base_url)
            kwargs = {
                "model": llm_settings.openai_model,
                "api_key": llm_settings.openai_api_key,
                "temperature": llm_settings.temperature,
                "max_completion_tokens": llm_settings.max_tokens,
            }
            if base_url:
                kwargs["base_url"] = base_url
            return ChatOpenAI(**kwargs)

    def _get_search_orchestrator(self) -> ParallelSearchOrchestrator:
        """Get or create the search orchestrator with configured providers."""
        if self._search_orchestrator is None:
            providers = []
            search_settings = self.settings.search

            # Add Brave Search if API key is configured
            if search_settings.brave_api_key:
                providers.append(BraveSearchProvider(search_settings.brave_api_key))
                logger.info("Brave Search provider enabled")

            # Add Tavily if API key is configured
            if search_settings.tavily_api_key:
                providers.append(TavilySearchProvider(search_settings.tavily_api_key))
                logger.info("Tavily Search provider enabled")

            # Add Perplexity if API key is configured
            if search_settings.perplexity_api_key:
                providers.append(PerplexitySearchProvider(search_settings.perplexity_api_key))
                logger.info("Perplexity Search provider enabled")

            if not providers:
                logger.warning("No search providers configured - API search disabled")

            self._search_orchestrator = ParallelSearchOrchestrator(
                providers=providers,
                timeout=search_settings.timeout,
                deduplicate=True,
            )

        return self._search_orchestrator

    def _get_rrf_search_orchestrator(self) -> RRFSearchOrchestrator:
        """Get or create the RRF search orchestrator with query analysis."""
        if self._rrf_search_orchestrator is None:
            providers = []
            search_settings = self.settings.search

            # Add Brave Search if API key is configured
            if search_settings.brave_api_key:
                providers.append(BraveSearchProvider(search_settings.brave_api_key))

            # Add Tavily if API key is configured
            if search_settings.tavily_api_key:
                providers.append(TavilySearchProvider(search_settings.tavily_api_key))

            # Add Perplexity if API key is configured
            if search_settings.perplexity_api_key:
                providers.append(PerplexitySearchProvider(search_settings.perplexity_api_key))

            if not providers:
                logger.warning("No search providers configured - RRF search disabled")

            # Create query analyzer with the LLM
            self._query_analyzer = QueryAnalyzer(
                llm=self._llm,
                enable_expansion=True,
                max_expanded_queries=5,
                cache_results=True,
            )

            # Get RRF settings
            rrf_k = getattr(search_settings, "rrf_k", 60)
            enable_semantic = getattr(search_settings, "enable_semantic_rrf", True)

            self._rrf_search_orchestrator = create_rrf_orchestrator(
                providers=providers,
                llm=self._llm,
                timeout=search_settings.timeout,
                rrf_k=rrf_k,
                enable_semantic=enable_semantic,
            )

            logger.info(
                "RRF Search orchestrator initialized",
                providers=len(providers),
                rrf_k=rrf_k,
                semantic_enabled=enable_semantic,
            )

        return self._rrf_search_orchestrator

    def _get_captcha_solver(self) -> CaptchaSolverOrchestrator:
        """Get or create the CAPTCHA solver orchestrator with paid solver support."""
        if self._captcha_solver is None:
            # Get paid solver API keys from settings
            captcha_settings = getattr(self.settings, "captcha", None)
            capsolver_key = (
                getattr(captcha_settings, "capsolver_api_key", "") if captcha_settings else ""
            )
            twocaptcha_key = (
                getattr(captcha_settings, "twocaptcha_api_key", "") if captcha_settings else ""
            )
            prefer_paid = (
                getattr(captcha_settings, "prefer_paid_solver", True) if captcha_settings else True
            )
            paid_timeout = (
                getattr(captcha_settings, "paid_solver_timeout", 120.0)
                if captcha_settings
                else 120.0
            )

            self._captcha_solver = CaptchaSolverOrchestrator(
                capsolver_api_key=capsolver_key,
                twocaptcha_api_key=twocaptcha_key,
                prefer_paid=prefer_paid,
                paid_timeout=paid_timeout,
            )
        return self._captcha_solver

    async def _get_browser_session(self, force_new: bool = False) -> BrowserSession:
        """Get or create the browser session with enhanced stealth configuration.

        Args:
            force_new: If True, always create a new browser session (recommended for task isolation)
        """
        # Use Camoufox if configured
        if self._use_camoufox:
            return await self._get_camoufox_session()

        # Close existing session if force_new or if session is not healthy
        if force_new and self._browser_session is not None:
            try:
                await self._browser_session.stop()
            except Exception as e:
                logger.debug("Error closing existing browser session", error=str(e))
            self._browser_session = None

        # Use Playwright with stealth
        if self._browser_session is None:
            stealth_settings = self.settings.stealth

            # Setup extensions if NopeCHA is enabled
            if self._stealth_config.use_nopecha:
                await self._stealth_config.setup_extensions()
                logger.info("NopeCHA extension configured for CAPTCHA bypass")

            # Build browser args for stealth mode
            extra_args = []
            if stealth_settings.enabled:
                if self._stealth_config.extension_paths:
                    # Use enhanced args with extension support
                    extra_args = self._stealth_config.get_browser_args(
                        include_docker=getattr(self.settings.browser, "is_docker_env", False)
                    )
                else:
                    extra_args = get_undetected_browser_args()
                logger.info(
                    "Stealth mode enabled for browser session",
                    extensions_loaded=len(self._stealth_config.extension_paths),
                )

            # Get proxy from rotation service if available
            proxy_settings = None
            if self._proxy_client:
                try:
                    self._current_proxy = await self._proxy_client.get_next_proxy()
                    if self._current_proxy:
                        proxy_settings = ProxySettings(
                            server=self._current_proxy.address,
                            username=self._current_proxy.username,
                            password=self._current_proxy.password,
                        )
                        logger.info(
                            "Proxy assigned for browser session",
                            proxy_id=self._current_proxy.id,
                            proxy_address=self._current_proxy.address,
                        )
                except Exception as e:
                    logger.warning("Failed to get proxy from rotation service", error=str(e))

            # Detect Playwright browser path in Docker environment
            executable_path = None
            if getattr(self.settings.browser, "is_docker_env", False):
                # Look for installed Chromium in Playwright's cache
                playwright_path = os.environ.get(
                    "PLAYWRIGHT_BROWSERS_PATH", os.path.expanduser("~/.cache/ms-playwright")
                )
                # Find chromium executable
                import glob

                chromium_patterns = [
                    f"{playwright_path}/chromium-*/chrome-linux64/chrome",
                    f"{playwright_path}/chromium-*/chrome-linux/chrome",
                    f"{playwright_path}/chromium_headless_shell-*/chrome-linux64/headless_shell",
                    f"{playwright_path}/chromium_headless_shell-*/chrome-linux/headless_shell",
                ]
                for pattern in chromium_patterns:
                    matches = glob.glob(pattern)
                    if matches:
                        executable_path = sorted(matches)[-1]  # Use latest version
                        logger.debug("Found Playwright browser", executable_path=executable_path)
                        break
                if not executable_path:
                    logger.warning("No Playwright browser found, browser-use will try to install")

            profile = BrowserProfile(
                headless=self.settings.browser.headless,
                disable_security=True,  # Required for some sites
                extra_chromium_args=extra_args,
                proxy=proxy_settings,  # Apply proxy from rotation service
                executable_path=executable_path,  # Use pre-installed Playwright browser
            )
            self._browser_session = BrowserSession(browser_profile=profile)
        return self._browser_session

    async def _get_camoufox_session(self) -> Any:
        """Get or create Camoufox browser session."""
        if self._camoufox_browser is None:
            camoufox_settings = getattr(self.settings, "camoufox", None)

            # Build Camoufox config
            if camoufox_settings:
                config = CamoufoxConfig(
                    headless=self.settings.browser.headless,
                    humanize=camoufox_settings.humanize,
                    humanize_level=camoufox_settings.humanize_level,
                    locale=camoufox_settings.locale,
                    timezone=camoufox_settings.timezone,
                    geoip=camoufox_settings.geoip,
                    block_webrtc=camoufox_settings.block_webrtc,
                    block_images=camoufox_settings.block_images,
                    os=camoufox_settings.os_type if camoufox_settings.os_type != "random" else None,
                )
            else:
                # Use recommended config for Cloudflare bypass
                config = get_recommended_camoufox_config(
                    purpose="cloudflare",
                    headless=self.settings.browser.headless,
                )

            self._camoufox_browser = await create_camoufox_browser(config)

            if self._camoufox_browser:
                logger.info(
                    "Camoufox browser created",
                    headless=config.headless,
                    humanize=config.humanize,
                    humanize_level=config.humanize_level,
                )
            else:
                logger.error("Failed to create Camoufox browser, falling back to Playwright")
                self._use_camoufox = False
                return await self._get_browser_session()

        return self._camoufox_browser

    async def _get_camoufox_page(self) -> Any:
        """Get a new page from Camoufox browser."""
        browser = await self._get_camoufox_session()
        if browser:
            try:
                page = await browser.new_page()
                logger.debug("Created new Camoufox page")
                return page
            except Exception as e:
                logger.error("Failed to create Camoufox page", error=str(e))
        return None

    async def _apply_page_stealth(self, page) -> None:
        """Apply advanced stealth patches to a page."""
        # Apply playwright_stealth or manual patches
        await apply_stealth_to_playwright_async(page, self._stealth_config)

        # Apply advanced stealth patches from undetected module
        await AdvancedStealthPatcher.apply_to_page(page)

        logger.debug("Applied advanced stealth patches to page")

    async def search_before_crawl(
        self,
        query: str,
        max_results: int = 20,
        use_rrf: bool = True,
    ) -> list[str]:
        """
        Perform API-based search before browser crawling.

        Returns list of URLs to visit based on search results.
        Useful for bypassing search engine CAPTCHAs.

        Args:
            query: Search query
            max_results: Maximum number of URLs to return
            use_rrf: Use RRF-based multi-strategy search for better accuracy
        """
        if use_rrf:
            return await self._search_with_rrf(query, max_results)

        # Fallback to simple parallel search
        orchestrator = self._get_search_orchestrator()

        if not orchestrator.providers:
            logger.warning("No search providers available")
            return []

        try:
            result = await orchestrator.search_news(
                query=query,
                max_results_per_provider=self.settings.search.max_results_per_provider,
            )

            urls = [r.url for r in result.results[:max_results]]

            logger.info(
                "Search completed",
                query=query,
                results_count=len(urls),
                providers_used=result.providers_used,
            )

            return urls

        except Exception as e:
            logger.error("Search failed", query=query, error=str(e))
            return []

    async def _search_with_rrf(
        self,
        query: str,
        max_results: int = 20,
    ) -> list[str]:
        """
        Perform RRF-based multi-strategy search.

        This method:
        1. Analyzes the query to understand intent and extract keywords
        2. Expands the query into multiple semantically related queries
        3. Executes parallel searches across all providers for each query variant
        4. Merges results using Reciprocal Rank Fusion algorithm
        5. Returns URLs ranked by combined relevance
        """
        orchestrator = self._get_rrf_search_orchestrator()

        if not orchestrator.providers:
            logger.warning("No search providers available for RRF search")
            return []

        try:
            result = await orchestrator.search_news_with_rrf(
                query=query,
                max_results_per_strategy=self.settings.search.max_results_per_provider,
            )

            urls = [r.url for r in result.results[:max_results]]

            logger.info(
                "RRF search completed",
                query=query,
                results_count=len(urls),
                strategies_used=result.strategies_used,
                providers_used=result.providers_used,
                query_analysis=result.query_analysis,
            )

            return urls

        except Exception as e:
            logger.error(
                "RRF search failed, falling back to simple search", query=query, error=str(e)
            )
            # Fallback to simple search
            return await self.search_before_crawl(query, max_results, use_rrf=False)

    async def close(self) -> None:
        """Close the browser and cleanup resources."""
        if self._browser_session:
            await self._browser_session.stop()
            self._browser_session = None

        if self._camoufox_browser:
            try:
                await self._camoufox_browser.close()
            except Exception as e:
                logger.debug("Error closing Camoufox browser", error=str(e))
            self._camoufox_browser = None

        if self._search_orchestrator:
            await self._search_orchestrator.close_all()
            self._search_orchestrator = None

        # Close proxy client
        if self._proxy_client:
            try:
                await self._proxy_client.close()
            except Exception as e:
                logger.debug("Error closing proxy client", error=str(e))
            self._proxy_client = None
            self._current_proxy = None

    async def crawl_with_camoufox(
        self,
        url: str,
        extract_content: bool = True,
        wait_for_cloudflare: bool = True,
    ) -> dict[str, Any]:
        """
        Crawl a URL using Camoufox browser for maximum anti-detection.

        Args:
            url: URL to crawl
            extract_content: Whether to extract page content
            wait_for_cloudflare: Whether to wait for Cloudflare challenge

        Returns:
            Dictionary with page content and metadata
        """
        page = await self._get_camoufox_page()
        if not page:
            return {"error": "Failed to create Camoufox page"}

        try:
            # Navigate to URL
            await page.goto(url, wait_until="domcontentloaded")

            # Wait for Cloudflare challenge if needed
            if wait_for_cloudflare:
                passed = await CamoufoxHelper.wait_for_cloudflare(page, timeout=30)
                if not passed:
                    logger.warning("Cloudflare challenge may not have completed", url=url)

            # Simulate human behavior
            if self._stealth_config.enable_human_simulation:
                await asyncio.sleep(1)  # Brief pause

            # Extract content
            if extract_content:
                content = await CamoufoxHelper.extract_page_content(page)
                content["success"] = True
                return content

            return {
                "success": True,
                "url": url,
                "title": await page.title(),
            }

        except Exception as e:
            logger.error("Camoufox crawl failed", url=url, error=str(e))
            return {"error": str(e), "success": False}
        finally:
            try:
                await page.close()
            except Exception:
                pass

    async def _detect_and_handle_captcha(self, page) -> bool:
        """
        Detect and attempt to handle CAPTCHAs on a page.

        Args:
            page: Playwright page object

        Returns:
            True if CAPTCHA was detected and handled (or not detected),
            False if CAPTCHA was detected but could not be handled
        """
        try:
            # Check for common CAPTCHA indicators
            captcha_selectors = {
                CaptchaType.RECAPTCHA_V2: [
                    "iframe[src*='recaptcha']",
                    ".g-recaptcha",
                    "#recaptcha",
                ],
                CaptchaType.HCAPTCHA: [
                    "iframe[src*='hcaptcha']",
                    ".h-captcha",
                ],
                CaptchaType.CLOUDFLARE: [
                    "#challenge-running",
                    ".cf-browser-verification",
                    "iframe[src*='turnstile']",
                    "#cf-turnstile",
                ],
            }

            detected_type = None
            for captcha_type, selectors in captcha_selectors.items():
                for selector in selectors:
                    try:
                        element = await page.query_selector(selector)
                        if element:
                            is_visible = await element.is_visible()
                            if is_visible:
                                detected_type = captcha_type
                                logger.info(
                                    "CAPTCHA detected", type=captcha_type.value, selector=selector
                                )
                                break
                    except Exception:
                        continue
                if detected_type:
                    break

            if not detected_type:
                return True  # No CAPTCHA detected

            # Report CAPTCHA to IP rotation service for weighted proxy selection
            if self._proxy_client and self._current_proxy:
                try:
                    await self._proxy_client.record_captcha(
                        proxy_id=self._current_proxy.id,
                        captcha_type=detected_type.value,
                    )
                    logger.info(
                        "CAPTCHA reported to IP rotation service",
                        proxy_id=self._current_proxy.id,
                        captcha_type=detected_type.value,
                    )
                except Exception as e:
                    logger.debug("Failed to report CAPTCHA to IP rotation service", error=str(e))

            # Try to solve the CAPTCHA
            solver = self._get_captcha_solver()
            result = await solver.solve(detected_type, page=page)

            if result.success:
                logger.info(
                    "CAPTCHA solved successfully",
                    type=detected_type.value,
                    solver=result.solver_used,
                    time_ms=result.time_ms,
                )
                # Wait for page to update after CAPTCHA solve
                await asyncio.sleep(2)
                return True
            else:
                logger.warning("CAPTCHA solve failed", type=detected_type.value, error=result.error)
                return False

        except Exception as e:
            logger.error("Error in CAPTCHA detection/handling", error=str(e))
            return False

    async def _simulate_human_behavior(self, page) -> None:
        """Simulate human-like behavior on a page to avoid detection."""
        try:
            # Random mouse movements
            await HumanBehaviorSimulator.random_mouse_movements(page, count=2)

            # Random scroll
            await HumanBehaviorSimulator.human_scroll(page, "down", 200)
            await asyncio.sleep(HumanBehaviorSimulator.random_delay(500, 1000))

        except Exception as e:
            logger.debug("Human behavior simulation failed", error=str(e))

    async def smart_search(
        self,
        query: str,
        max_results: int = 20,
        use_browser_fallback: bool = True,
        use_rrf: bool = True,
    ) -> list[dict[str, Any]]:
        """
        Smart search with API-first strategy and browser fallback.

        Tries API-based search first to avoid CAPTCHA, then falls back
        to browser-based search with Camoufox if APIs fail.

        Now uses RRF (Reciprocal Rank Fusion) for improved accuracy by:
        1. Analyzing query intent and extracting semantic meaning
        2. Expanding query into multiple search strategies
        3. Merging results from multiple providers and strategies

        Args:
            query: Search query
            max_results: Maximum number of results
            use_browser_fallback: Whether to try browser search if API fails
            use_rrf: Use RRF-based multi-strategy search

        Returns:
            List of search results with url, title, snippet
        """
        results = []

        # Step 1: Try API-based search (no CAPTCHA)
        if use_rrf:
            logger.info("Attempting RRF-based API search", query=query)
            try:
                orchestrator = self._get_rrf_search_orchestrator()
                if orchestrator.providers:
                    rrf_result = await orchestrator.search_news_with_rrf(
                        query=query,
                        max_results_per_strategy=self.settings.search.max_results_per_provider,
                    )

                    if rrf_result.results:
                        logger.info(
                            "RRF API search successful",
                            query=query,
                            results_count=len(rrf_result.results),
                            strategies=rrf_result.strategies_used,
                            query_analysis=rrf_result.query_analysis,
                        )
                        return [
                            {
                                "url": r.url,
                                "title": r.title,
                                "snippet": r.snippet,
                                "source": f"rrf_{r.source_provider}",
                            }
                            for r in rrf_result.results[:max_results]
                        ]
            except Exception as e:
                logger.warning("RRF search failed", error=str(e))

        # Fallback to simple API search
        logger.info("Attempting simple API-based search", query=query)
        api_urls = await self.search_before_crawl(query, max_results, use_rrf=False)

        if api_urls:
            logger.info("API search successful", query=query, results_count=len(api_urls))
            results = [{"url": url, "source": "api"} for url in api_urls]
            return results

        if not use_browser_fallback:
            logger.warning("API search failed and browser fallback disabled")
            return results

        # Step 2: Try browser search with Camoufox (best anti-detection)
        if is_camoufox_available():
            logger.info("Trying Camoufox browser search", query=query)
            camoufox_results = await self._browser_search_with_camoufox(query, max_results)
            if camoufox_results:
                return camoufox_results

        # Step 3: Try browser search with enhanced Playwright stealth
        logger.info("Trying Playwright stealth browser search", query=query)
        playwright_results = await self._browser_search_with_stealth(query, max_results)

        return playwright_results

    async def _browser_search_with_camoufox(
        self,
        query: str,
        max_results: int = 20,
    ) -> list[dict[str, Any]]:
        """
        Perform browser search using Camoufox anti-detect browser.

        Uses DuckDuckGo HTML version which is less likely to show CAPTCHA.
        """
        results = []

        try:
            page = await self._get_camoufox_page()
            if not page:
                return results

            # Use DuckDuckGo HTML version (lighter, less detection)
            search_url = f"https://html.duckduckgo.com/html/?q={query.replace(' ', '+')}"

            await page.goto(search_url, wait_until="domcontentloaded")

            # Wait for Cloudflare if present
            await CamoufoxHelper.wait_for_cloudflare(page, timeout=15)

            # Simulate human behavior
            await asyncio.sleep(1)

            # Extract search results
            result_elements = await page.query_selector_all(".result")

            for element in result_elements[:max_results]:
                try:
                    link = await element.query_selector(".result__a")
                    snippet_el = await element.query_selector(".result__snippet")

                    if link:
                        url = await link.get_attribute("href")
                        title = await link.inner_text()
                        snippet = ""
                        if snippet_el:
                            snippet = await snippet_el.inner_text()

                        if url and title:
                            results.append(
                                {
                                    "url": url,
                                    "title": title.strip(),
                                    "snippet": snippet.strip(),
                                    "source": "camoufox_duckduckgo",
                                }
                            )
                except Exception:
                    continue

            await page.close()

            if results:
                logger.info("Camoufox search successful", query=query, results_count=len(results))

        except Exception as e:
            logger.error("Camoufox search failed", query=query, error=str(e))

        return results

    async def _browser_search_with_stealth(
        self,
        query: str,
        max_results: int = 20,
    ) -> list[dict[str, Any]]:
        """
        Perform browser search using Playwright with stealth patches.

        Tries multiple search engines with different strategies.
        Prioritizes engines that are less likely to show CAPTCHAs.
        """
        results = []

        # Search engines to try (in order of CAPTCHA likelihood - least likely first)
        search_engines = [
            {
                "name": "duckduckgo_html",
                "url": f"https://html.duckduckgo.com/html/?q={query.replace(' ', '+')}",
                "result_selector": ".result",
                "link_selector": ".result__a",
                "snippet_selector": ".result__snippet",
                "wait_selector": ".result",
            },
            {
                "name": "startpage",
                "url": f"https://www.startpage.com/do/search?q={query.replace(' ', '+')}",
                "result_selector": ".w-gl__result",
                "link_selector": "a.w-gl__result-title",
                "snippet_selector": ".w-gl__description",
                "wait_selector": ".w-gl__result",
            },
            {
                "name": "ecosia",
                "url": f"https://www.ecosia.org/search?q={query.replace(' ', '+')}",
                "result_selector": "[data-test-id='mainline-result-web']",
                "link_selector": "a[data-test-id='result-link']",
                "snippet_selector": "[data-test-id='result-snippet']",
                "wait_selector": "[data-test-id='mainline-result-web']",
            },
            {
                "name": "mojeek",  # Privacy-focused, rarely uses CAPTCHA
                "url": f"https://www.mojeek.com/search?q={query.replace(' ', '+')}",
                "result_selector": ".results-standard li",
                "link_selector": "a.title",
                "snippet_selector": ".s",
                "wait_selector": ".results-standard",
            },
        ]

        browser_session = None
        context = None
        page = None

        try:
            browser_session = await self._get_browser_session()

            # Get the underlying playwright browser to create isolated context
            if hasattr(browser_session, "_browser") and browser_session._browser:
                browser = browser_session._browser

                # Create isolated context with stealth args
                context = await browser.new_context(
                    user_agent=self._stealth_config.get_random_user_agent()
                    if hasattr(self._stealth_config, "get_random_user_agent")
                    else None,
                    locale="en-US",
                    timezone_id="America/New_York",
                )
                page = await context.new_page()

                # Apply stealth patches
                await self._apply_page_stealth(page)
            else:
                logger.warning("Could not access underlying browser for stealth search")
                return results

            for engine in search_engines:
                try:
                    logger.info("Trying search engine", engine=engine["name"], query=query)

                    # Navigate to search engine
                    await page.goto(engine["url"], wait_until="domcontentloaded", timeout=15000)

                    # Wait for results to load
                    try:
                        await page.wait_for_selector(
                            engine["wait_selector"], timeout=10000, state="visible"
                        )
                    except Exception:
                        # Check if we hit a CAPTCHA
                        captcha_type = await _quick_captcha_check(page)
                        if captcha_type:
                            logger.warning(
                                "CAPTCHA detected on search engine",
                                engine=engine["name"],
                                captcha_type=captcha_type,
                            )
                            # Try to solve it
                            solved = await self._detect_and_handle_captcha(page)
                            if not solved:
                                continue  # Try next engine
                            # Wait again for results after solving
                            try:
                                await page.wait_for_selector(engine["wait_selector"], timeout=5000)
                            except Exception:
                                continue
                        else:
                            logger.debug(
                                "Results not found, trying next engine", engine=engine["name"]
                            )
                            continue

                    # Simulate human behavior
                    if self._stealth_config.enable_human_simulation:
                        await HumanBehaviorSimulator.random_mouse_movements(page, count=1)
                        await asyncio.sleep(0.3)

                    # Extract search results
                    result_elements = await page.query_selector_all(engine["result_selector"])

                    for element in result_elements[:max_results]:
                        try:
                            link = await element.query_selector(engine["link_selector"])
                            snippet_el = await element.query_selector(engine["snippet_selector"])

                            if link:
                                url = await link.get_attribute("href")
                                title = await link.inner_text()
                                snippet = ""
                                if snippet_el:
                                    snippet = await snippet_el.inner_text()

                                # Clean up URL (some engines use redirect URLs)
                                if url and title:
                                    # Skip ad/sponsored results
                                    if "ad" in url.lower() or "sponsor" in title.lower():
                                        continue

                                    results.append(
                                        {
                                            "url": url,
                                            "title": title.strip(),
                                            "snippet": snippet.strip() if snippet else "",
                                            "source": f"stealth_{engine['name']}",
                                        }
                                    )
                        except Exception as e:
                            logger.debug("Failed to extract result", error=str(e))
                            continue

                    if results:
                        logger.info(
                            "Stealth search successful",
                            engine=engine["name"],
                            query=query,
                            results_count=len(results),
                        )
                        break  # Got results, stop trying other engines

                except Exception as e:
                    logger.debug("Search engine failed", engine=engine["name"], error=str(e))
                    continue

        except Exception as e:
            logger.error("Stealth browser search failed", query=query, error=str(e))
        finally:
            # Clean up
            if page:
                try:
                    await page.close()
                except Exception:
                    pass
            if context:
                try:
                    await context.close()
                except Exception:
                    pass

        return results

    async def handle_captcha_and_retry(
        self,
        page,
        action_func,
        max_retries: int = 3,
        switch_backend_on_failure: bool = True,
        rotate_proxy_on_failure: bool = True,
    ) -> Any:
        """
        Execute an action with CAPTCHA detection and retry logic.

        If CAPTCHA is detected and cannot be solved, optionally switches
        to a different browser backend or rotates to a new proxy and retries.

        Args:
            page: Current page object
            action_func: Async function to execute
            max_retries: Maximum retry attempts
            switch_backend_on_failure: Try different browser if CAPTCHA persists
            rotate_proxy_on_failure: Get a new proxy from rotation service on failure

        Returns:
            Result of action_func or None if all retries fail
        """
        for attempt in range(max_retries):
            try:
                # Check for CAPTCHA before action
                captcha_handled = await self._detect_and_handle_captcha(page)

                if not captcha_handled:
                    logger.warning(
                        "CAPTCHA detected but not solved",
                        attempt=attempt + 1,
                        max_retries=max_retries,
                    )

                    # Try rotating to a new proxy first
                    if rotate_proxy_on_failure and self._proxy_client:
                        try:
                            new_proxy = await self._proxy_client.get_next_proxy()
                            if new_proxy and (
                                not self._current_proxy or new_proxy.id != self._current_proxy.id
                            ):
                                self._current_proxy = new_proxy
                                logger.info(
                                    "Rotating to new proxy after CAPTCHA failure",
                                    proxy_id=new_proxy.id,
                                    proxy_address=new_proxy.address,
                                    attempt=attempt + 1,
                                )
                                # Recreate browser session with new proxy
                                if self._browser_session:
                                    await self._browser_session.stop()
                                    self._browser_session = None
                                # Get new session with new proxy
                                browser_session = await self._get_browser_session()
                                if (
                                    hasattr(browser_session, "_context")
                                    and browser_session._context
                                ):
                                    pages = browser_session._context.pages
                                    if pages:
                                        page = pages[-1]
                                        continue
                        except Exception as e:
                            logger.debug("Failed to rotate proxy", error=str(e))

                    # If Camoufox available and we're not already using it, switch
                    if (
                        switch_backend_on_failure
                        and not self._use_camoufox
                        and is_camoufox_available()
                    ):
                        logger.info("Switching to Camoufox browser for better CAPTCHA bypass")
                        self._use_camoufox = True

                        # Get new page from Camoufox
                        new_page = await self._get_camoufox_page()
                        if new_page:
                            page = new_page
                            continue

                    await asyncio.sleep(2**attempt)  # Exponential backoff
                    continue

                # Execute the action
                result = await action_func(page)

                # Check for CAPTCHA after action (might have triggered one)
                await self._detect_and_handle_captcha(page)

                return result

            except Exception as e:
                logger.error("Action failed", attempt=attempt + 1, error=str(e))
                await asyncio.sleep(2**attempt)

        logger.error("All retry attempts failed")
        return None

    async def execute_task(self, task: BrowserTaskMessage) -> list[CrawlResultMessage]:
        """
        Execute a browser crawling task.

        Args:
            task: The browser task message from Kafka

        Returns:
            List of extracted crawl results
        """
        # Parse policy
        try:
            policy = CrawlPolicy(task.policy.lower()) if task.policy else CrawlPolicy.NEWS_ONLY
        except ValueError:
            policy = CrawlPolicy.NEWS_ONLY

        # Create session with metadata from task (for AutoCrawl callback)
        session = CrawlSession(
            job_id=task.job_id,
            source_id=task.source_id,
            seed_url=task.seed_url,
            max_depth=task.max_depth or 2,
            max_pages=task.max_pages or 10,
            budget_seconds=min(
                task.budget_seconds or self.settings.browser.default_timeout_seconds,
                self.settings.browser.max_timeout_seconds,
            ),
            policy=policy,
            focus_keywords=task.get_focus_keywords_list(),
            excluded_domains=task.get_excluded_domains_list(),
            metadata=task.metadata,  # Pass metadata for AutoCrawl callback
        )

        # Track proxy used for this task
        task_proxy_id = None

        logger.info(
            "Starting crawl session",
            job_id=session.job_id,
            source_id=session.source_id,
            seed_url=session.seed_url,
            policy=policy.value,
            max_pages=session.max_pages,
            budget_seconds=session.budget_seconds,
        )

        session.start_time = datetime.now()

        try:
            # Generate the system prompt based on policy
            system_prompt = get_policy_prompt(
                policy=policy,
                focus_keywords=session.focus_keywords,
                custom_prompt=task.custom_prompt,
                excluded_domains=session.excluded_domains,
            )

            # Create the task prompt
            task_prompt = self._build_task_prompt(session)

            # Get browser session and create agent (this will also get proxy)
            # Use force_new=True to ensure a fresh browser for each task (avoids CDP issues)
            browser_session = await self._get_browser_session(force_new=True)

            # Track the proxy being used for this task
            if self._current_proxy:
                task_proxy_id = self._current_proxy.id
                logger.info(
                    "Task using proxy",
                    job_id=session.job_id,
                    proxy_id=task_proxy_id,
                )

            # Create CAPTCHA detection and stealth hooks for the agent
            captcha_hook = await create_captcha_detection_hook(
                crawler_agent=self,
                on_captcha_detected=lambda ct: logger.info(
                    "CAPTCHA detected during crawl",
                    captcha_type=ct,
                    job_id=session.job_id,
                ),
            )
            stealth_hook = await create_stealth_hook(self)
            url_filter_hook = await create_url_filter_hook(
                crawler_agent=self,
                on_blocked_url=lambda url: logger.info(
                    "Blocked URL navigation detected",
                    blocked_url=url[:100],
                    job_id=session.job_id,
                ),
            )

            # Combine stealth and URL filter hooks into a single on_step_end hook
            async def combined_step_end_hook(agent: "BrowserUseAgent") -> None:
                """Combined hook for stealth patches and URL filtering."""
                await stealth_hook(agent)
                await url_filter_hook(agent)

            agent = Agent(
                task=task_prompt,
                llm=self._llm,
                browser_session=browser_session,
                max_actions_per_step=5,
                extend_system_message=system_prompt,  # Add crawl policy to system prompt
            )

            # Run the agent with timeout and CAPTCHA/stealth hooks
            try:
                result = await asyncio.wait_for(
                    agent.run(
                        max_steps=session.max_pages * 3,  # Allow multiple steps per page
                        on_step_start=captcha_hook,  # CAPTCHA detection before each step
                        on_step_end=combined_step_end_hook,  # Stealth patches + URL filter after navigation
                    ),
                    timeout=session.budget_seconds,
                )

                # Parse the agent's output to extract articles
                session.extracted_articles = self._parse_agent_output(
                    result, session.job_id, session.source_id
                )

            except asyncio.TimeoutError:
                logger.warning(
                    "Crawl session timed out",
                    job_id=session.job_id,
                    elapsed_seconds=session.budget_seconds,
                )

        except Exception as e:
            session.error = str(e)
            logger.error(
                "Crawl session failed",
                job_id=session.job_id,
                error=str(e),
                exc_info=True,
            )

        finally:
            session.end_time = datetime.now()
            elapsed = (session.end_time - session.start_time).total_seconds()

            # Clean up browser session to prevent CDP issues on next task
            # Use kill() instead of stop() for more aggressive cleanup
            if self._browser_session:
                try:
                    await self._browser_session.kill()
                    logger.debug("Browser session killed successfully")
                except Exception as e:
                    logger.debug("Error killing browser session", error=str(e))
                self._browser_session = None

            # Small delay to ensure browser process fully terminates before next task
            await asyncio.sleep(0.5)

            # Record proxy usage result to IP rotation service
            if self._proxy_client and task_proxy_id:
                try:
                    if session.error:
                        await self._proxy_client.record_failure(
                            proxy_id=task_proxy_id,
                            reason=session.error[:200],  # Truncate error message
                        )
                    else:
                        await self._proxy_client.record_success(
                            proxy_id=task_proxy_id,
                            latency_ms=int(elapsed * 1000),
                        )
                    logger.debug(
                        "Proxy usage recorded",
                        proxy_id=task_proxy_id,
                        success=session.error is None,
                    )
                except Exception as e:
                    logger.debug("Failed to record proxy usage", error=str(e))

            # Send callback if configured
            if task.callback_url:
                await self._send_callback(task, session)

        logger.info(
            "Crawl session completed",
            job_id=session.job_id,
            articles_extracted=len(session.extracted_articles),
            elapsed_seconds=elapsed,
            error=session.error,
            proxy_id=task_proxy_id,
        )

        return session.extracted_articles

    def _build_task_prompt(self, session: CrawlSession) -> str:
        """Build the task prompt for the browser-use agent."""
        prompt_parts = [
            f"Navigate to {session.seed_url} and extract article content.",
            f"",
            f"## Constraints:",
            f"- Maximum pages to visit: {session.max_pages}",
            f"- Maximum link depth: {session.max_depth}",
            f"- Time budget: {session.budget_seconds} seconds",
            f"",
            f"## Output Format:",
            f"For each article you extract, output in this exact format:",
            f"---ARTICLE_START---",
            f"URL: [the page URL]",
            f"TITLE: [the article title]",
            f"PUBLISHED_AT: [publication date in ISO format, or 'unknown']",
            f"CONTENT: [the full article text]",
            f"---ARTICLE_END---",
            f"",
            f"Extract as many relevant articles as possible within the constraints.",
        ]

        if session.focus_keywords:
            prompt_parts.append(f"Focus on articles about: {', '.join(session.focus_keywords)}")

        return "\n".join(prompt_parts)

    def _is_invalid_agent_output(self, text: str) -> bool:
        """Check if text contains raw Python object representations that shouldn't be used as article content."""
        invalid_patterns = [
            "ActionResult(",
            "AgentHistoryList(",
            "AgentHistory(",
            "include_extracted_content_only_once=",
            "include_in_memory=",
            "metadata=None",
            "is_done=",
            "extracted_content=",
        ]
        return any(pattern in text for pattern in invalid_patterns)

    def _parse_agent_output(
        self, result: Any, job_id: int, source_id: int
    ) -> list[CrawlResultMessage]:
        """Parse the agent's output to extract article data."""
        articles = []

        # Get the final output from the agent
        output_text = ""
        if hasattr(result, "final_result"):
            try:
                final_result = result.final_result
                value = final_result() if callable(final_result) else final_result
                if isinstance(value, str) and value.strip():
                    # Validate that this is actual content, not raw Python objects
                    if not self._is_invalid_agent_output(value):
                        output_text = value
                    else:
                        logger.warning(
                            "final_result contains raw Python object representation, skipping",
                            job_id=job_id,
                        )
            except Exception:
                output_text = ""

        if not output_text and hasattr(result, "history") and result.history:
            # Get the last message content
            for item in reversed(result.history):
                if hasattr(item, "result") and item.result:
                    for r in reversed(item.result):
                        extracted_content = getattr(r, "extracted_content", None)
                        if isinstance(extracted_content, str) and extracted_content.strip():
                            # Validate extracted content is not raw debug output
                            if not self._is_invalid_agent_output(extracted_content):
                                output_text = extracted_content
                                break
                            else:
                                logger.debug(
                                    "Skipping invalid extracted_content",
                                    job_id=job_id,
                                    content_preview=extracted_content[:100],
                                )
                    if output_text:
                        break

        if not output_text:
            logger.warning("No valid output from agent", job_id=job_id)
            return articles

        # Parse articles from the output
        article_pattern = r"---ARTICLE_START---(.+?)---ARTICLE_END---"
        matches = re.findall(article_pattern, output_text, re.DOTALL)

        for match in matches:
            try:
                article = self._parse_article_block(match, job_id, source_id)
                if article:
                    articles.append(article)
            except Exception as e:
                logger.warning("Failed to parse article block", error=str(e))

        # If no structured output, try to extract from unstructured text
        if not articles:
            articles = self._extract_from_unstructured(output_text, job_id, source_id)

        return articles

    def _parse_article_block(
        self, block: str, job_id: int, source_id: int
    ) -> CrawlResultMessage | None:
        """Parse a single article block from agent output."""
        lines = block.strip().split("\n")
        data: dict[str, str] = {}

        current_key = None
        current_value = []

        for line in lines:
            if line.startswith("URL:"):
                if current_key:
                    data[current_key] = "\n".join(current_value).strip()
                current_key = "url"
                current_value = [line[4:].strip()]
            elif line.startswith("TITLE:"):
                if current_key:
                    data[current_key] = "\n".join(current_value).strip()
                current_key = "title"
                current_value = [line[6:].strip()]
            elif line.startswith("PUBLISHED_AT:"):
                if current_key:
                    data[current_key] = "\n".join(current_value).strip()
                current_key = "published_at"
                current_value = [line[13:].strip()]
            elif line.startswith("CONTENT:"):
                if current_key:
                    data[current_key] = "\n".join(current_value).strip()
                current_key = "content"
                current_value = [line[8:].strip()]
            elif current_key:
                current_value.append(line)

        # Don't forget the last field
        if current_key:
            data[current_key] = "\n".join(current_value).strip()

        # Validate required fields
        if not data.get("url") or not data.get("title") or not data.get("content"):
            return None

        # Handle "unknown" published_at
        published_at = data.get("published_at")
        if published_at and published_at.lower() == "unknown":
            published_at = None

        return CrawlResultMessage(
            job_id=job_id,
            source_id=source_id,
            url=data["url"],
            title=data["title"],
            content=data["content"],
            published_at=published_at,
            metadata_json=json.dumps({"source": "browser-agent"}),
        )

    def _extract_title_from_content(self, content: str) -> str | None:
        """Extract a meaningful title from content, or return None if not possible."""
        # Skip content that contains raw Python object representations
        if self._is_invalid_agent_output(content):
            return None

        # Try to find a headline-like first line
        lines = content.strip().split("\n")
        for line in lines[:3]:  # Check first 3 lines
            line = line.strip()
            # Skip empty lines and very short lines
            if len(line) < 10:
                continue
            # Skip lines that look like metadata or code
            if any(char in line for char in ["=", "(", ")", "{", "}", "[", "]"]):
                continue
            # Found a good candidate for title
            if len(line) <= 100:
                return line
            return line[:97] + "..."

        # Fallback: use first 100 chars if content looks valid
        clean_content = content.strip()
        if len(clean_content) >= 20:
            return clean_content[:97] + "..."
        return None

    def _extract_from_unstructured(
        self, text: str, job_id: int, source_id: int
    ) -> list[CrawlResultMessage]:
        """Try to extract articles from unstructured agent output."""
        # This is a fallback for when the agent doesn't follow the exact format
        # Look for URL patterns and try to associate content
        articles = []

        # First, validate the entire text is not invalid output
        if self._is_invalid_agent_output(text):
            logger.warning(
                "Skipping unstructured extraction - text contains invalid agent output",
                job_id=job_id,
                text_preview=text[:200] if len(text) > 200 else text,
            )
            return articles

        # Simple heuristic: split by URL patterns
        url_pattern = r"(https?://[^\s]+)"
        parts = re.split(url_pattern, text)

        current_url = None
        current_content = []

        for part in parts:
            if re.match(url_pattern, part):
                # Save previous article if exists
                if current_url and current_content:
                    content = " ".join(current_content).strip()
                    if len(content) > 100:  # Minimum content length
                        # Extract a valid title from content
                        title = self._extract_title_from_content(content)
                        if title:
                            articles.append(
                                CrawlResultMessage(
                                    job_id=job_id,
                                    source_id=source_id,
                                    url=current_url,
                                    title=title,
                                    content=content,
                                    published_at=None,
                                    metadata_json=json.dumps(
                                        {"source": "browser-agent", "extraction": "unstructured"}
                                    ),
                                )
                            )
                        else:
                            logger.debug(
                                "Skipping article - could not extract valid title",
                                job_id=job_id,
                                url=current_url,
                            )
                current_url = part
                current_content = []
            else:
                current_content.append(part)

        return articles

    async def _send_callback(self, task: BrowserTaskMessage, session: CrawlSession) -> None:
        """Send completion callback to the configured URL.

        The callback payload matches the schema expected by
        AutoCrawlController.handleCrawlerCallback() in data-collection-service:
        - targetId: CrawlTarget ID from metadata
        - urlHash: URL hash from metadata
        - success: boolean indicating success/failure
        - collectedDataId: (optional) ID of saved CollectedData
        - error: error message if failed
        """
        if not task.callback_url:
            return

        # Extract AutoCrawl metadata if available
        target_id = None
        url_hash = None
        if session.metadata:
            target_id = session.metadata.get("targetId")
            url_hash = session.metadata.get("urlHash")

        # Build callback payload matching Java's CrawlerCallbackRequest
        callback_data = {
            "targetId": int(target_id) if target_id else session.job_id,
            "urlHash": url_hash,
            "success": session.error is None,
            "collectedDataId": None,  # Will be set by CrawlResultConsumer
            "error": session.error,
            # Include additional stats for debugging/monitoring
            "articlesExtracted": len(session.extracted_articles),
            "pagesVisited": len(session.visited_urls),
            "elapsedSeconds": (
                (session.end_time - session.start_time).total_seconds()
                if session.end_time and session.start_time
                else 0
            ),
        }

        headers = {"Content-Type": "application/json"}
        if task.callback_token:
            headers["Authorization"] = f"Bearer {task.callback_token}"

        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    task.callback_url,
                    json=callback_data,
                    headers=headers,
                    timeout=30.0,
                )
                logger.info(
                    "Callback sent",
                    job_id=session.job_id,
                    target_id=target_id,
                    url_hash=url_hash[:16] if url_hash else None,
                    success=callback_data["success"],
                    callback_url=task.callback_url,
                    status_code=response.status_code,
                )
        except Exception as e:
            logger.error(
                "Failed to send callback",
                job_id=session.job_id,
                callback_url=task.callback_url,
                error=str(e),
            )

```

---

## backend/autonomous-crawler-service/src/crawler/policies.py

```py
"""Crawling policies and prompt generation."""

from enum import Enum


class CrawlPolicy(Enum):
    """Exploration policies for autonomous crawling."""

    #  
    FOCUSED_TOPIC = "focused_topic"
    DOMAIN_WIDE = "domain_wide"
    NEWS_ONLY = "news_only"
    CROSS_DOMAIN = "cross_domain"
    SINGLE_PAGE = "single_page"

    #    ()
    NEWS_BREAKING = "news_breaking"  # /  
    NEWS_ARCHIVE = "news_archive"  #    
    NEWS_OPINION = "news_opinion"  # / 
    NEWS_LOCAL = "news_local"  #   


# Base system prompt for all policies
BASE_SYSTEM_PROMPT = """You are an autonomous web crawler agent specialized in extracting news and article content.
Your goal is to navigate websites, identify valuable content, and extract structured information.

## Core Behaviors:
1. Navigate to the seed URL first
2. Identify and extract article content (title, body text, publication date, author)
3. Find relevant links to other articles/pages based on the policy
4. Avoid non-content pages (login, signup, ads, social media shares)
5. Respect the page budget and depth limits

## Content Extraction Guidelines:
- Extract the main article title (usually h1 or article header)
- Extract the full article body text, preserving paragraphs
- Look for publication date in meta tags, article headers, or bylines
- Skip navigation menus, footers, sidebars, and advertisements
- If a page is not an article, briefly note what type of page it is and move on

## Navigation Rules:
- Prioritize links that appear to be article links (news headlines, blog posts)
- Avoid external links unless specifically allowed by the policy
- Skip links to media files (images, PDFs, videos)
- Skip pagination if you've already seen the content pattern

## CRITICAL: URLs to NEVER visit (skip these immediately):
- Login/Authentication: URLs containing /login, /signin, /signup, /register, /join, /membership, /auth/, /oauth, /sso/
- Account pages: URLs containing /account, /profile, /settings, /my-page, /mypage, /password, /forgot
- Help/Support: URLs containing /help, /support, /faq, /contact, /feedback, /cs/, /inquiry, /customer-service
- Legal: URLs containing /about, /about-us, /privacy, /terms, /legal, /policy, /disclaimer, /careers, /jobs
- Commerce: URLs containing /subscribe, /cart, /checkout, /payment, /order, /purchase, /shop, /store, /pricing
- Marketing: URLs containing /promo, /campaign, /landing, /offer, /deal, /coupon, /mkt/, /atrb, /ad/, /ads/
- Utility: URLs containing /search? (search results), /tag/, /sitemap, /feed, /rss, /notify, /notification
- Media files: URLs ending in .pdf, .jpg, .png, .gif, .mp4, .mp3, .zip, .exe
- Subdomains to avoid: nid.*, accounts.*, auth.*, login.*, help.*, support.*, mkt.*, notify.*, dic.*

## Output Format:
For each article extracted, use this format:
---ARTICLE_START---
URL: [article URL]
TITLE: [headline]
AUTHOR: [author name if found]
PUBLISHED_AT: [publication date in ISO format if found]
CATEGORY: [category/section if identified]
CONTENT: [full article text]
---ARTICLE_END---
"""

POLICY_PROMPTS = {
    CrawlPolicy.FOCUSED_TOPIC: """
## Policy: FOCUSED_TOPIC
Focus exclusively on content related to the specified keywords/topics.

### Specific Instructions:
- Only follow links that appear related to the focus keywords: {focus_keywords}
- Prioritize articles with titles containing the keywords
- Skip unrelated content even if it looks interesting
- Extract articles that discuss or mention the focus topics
- Look for related terms and synonyms of the focus keywords
""",
    CrawlPolicy.DOMAIN_WIDE: """
## Policy: DOMAIN_WIDE
Explore the entire domain broadly to discover all available content.

### Specific Instructions:
- Follow links to all content sections of the website
- Prioritize category/section pages that lead to more articles
- Create a broad coverage of the site's content
- Balance between depth and breadth of exploration
- Identify and visit major site sections (news, blog, articles, etc.)
""",
    CrawlPolicy.NEWS_ONLY: """
## Policy: NEWS_ONLY
Focus strictly on news articles and current events content.

### Specific Instructions:
- Only extract content that appears to be news articles
- Look for date indicators showing recent publication
- Prioritize breaking news, current events, and timely content
- Skip evergreen content, guides, and static pages
- Follow links from news sections, headlines, and latest articles
- Identify news patterns: bylines, datelines, news categories
""",
    CrawlPolicy.CROSS_DOMAIN: """
## Policy: CROSS_DOMAIN
Follow links across different domains to discover related content.

### Specific Instructions:
- You may follow external links to other websites
- Prioritize links that appear to lead to related news sources
- Respect the excluded domains list if provided
- Track which domains you've visited to ensure diversity
- Extract content from each domain you visit
- Be cautious of redirect chains and avoid loops
""",
    CrawlPolicy.SINGLE_PAGE: """
## Policy: SINGLE_PAGE
Extract content only from the seed URL without following any links.

### Specific Instructions:
- Do NOT navigate to any other pages
- Focus entirely on extracting content from the current page
- Extract all article content, metadata, and structured data
- Identify any embedded content or data on the page
- This is a single-page extraction task only
""",
    CrawlPolicy.NEWS_BREAKING: """
## Policy: NEWS_BREAKING
Priority collection of breaking news and urgent updates.

### Specific Instructions:
- Look for visual indicators of breaking news:
  - Labels: "", "Breaking", "", "", "Flash", "Urgent"
  - Red or highlighted text, special formatting
  - Pinned or featured articles at the top
- Prioritize articles published in the last few hours
- Extract the FULL content of breaking news articles
- Note the exact publication time if available
- Skip older news and evergreen content
- Look for live update sections or real-time feeds
- Mark each article as breaking: true in metadata
""",
    CrawlPolicy.NEWS_ARCHIVE: """
## Policy: NEWS_ARCHIVE
Historical article collection from archives.

### Specific Instructions:
- Navigate through pagination and archive pages
- Look for " ", "", "Load More" buttons
- Accept older publication dates (weeks, months, or years old)
- Follow links to category archives and date-based listings
- Collect articles systematically by date or category
- Note the original publication date accurately
- Skip duplicate or redirected articles
- Be patient with slower-loading archive pages
""",
    CrawlPolicy.NEWS_OPINION: """
## Policy: NEWS_OPINION
Focus on opinion pieces, editorials, and columns.

### Specific Instructions:
- Look for opinion/editorial sections:
  - "", "", "", "Opinion", "Editorial", "Column"
  - Author-focused pages with byline photos
- Identify opinion content markers:
  - Personal pronouns and subjective language
  - Author bio sections
  - Regular column series
- Extract author information prominently
- Mark content as opinion: true in metadata
- Note if the author is a regular columnist
- Skip straight news reporting
""",
    CrawlPolicy.NEWS_LOCAL: """
## Policy: NEWS_LOCAL
Local and regional news collection.

### Specific Instructions:
- Focus on local news sections:
  - "", "Local", geographic region names
  - City or province-specific categories
- Look for location markers in articles:
  - City names, district names
  - Local government references
  - Regional business news
- Prioritize community-focused stories
- Note the geographic focus of each article
- Skip national or international news
- Include local events and announcements
""",
}


def get_policy_prompt(
    policy: CrawlPolicy | str,
    focus_keywords: list[str] | None = None,
    custom_prompt: str | None = None,
    excluded_domains: list[str] | None = None,
) -> str:
    """
    Generate the full system prompt for the crawler agent.

    Args:
        policy: The crawling policy to use
        focus_keywords: Keywords for FOCUSED_TOPIC policy
        custom_prompt: Optional custom instructions to append
        excluded_domains: Domains to exclude from crawling

    Returns:
        Complete system prompt for the browser-use agent
    """
    # Convert string to enum if needed
    if isinstance(policy, str):
        try:
            policy = CrawlPolicy(policy.lower())
        except ValueError:
            policy = CrawlPolicy.NEWS_ONLY  # Default fallback

    # Build the prompt
    prompt_parts = [BASE_SYSTEM_PROMPT]

    # Add policy-specific instructions
    policy_prompt = POLICY_PROMPTS.get(policy, POLICY_PROMPTS[CrawlPolicy.NEWS_ONLY])

    # Format with focus keywords if applicable
    if policy == CrawlPolicy.FOCUSED_TOPIC and focus_keywords:
        policy_prompt = policy_prompt.format(focus_keywords=", ".join(focus_keywords))
    else:
        policy_prompt = policy_prompt.replace("{focus_keywords}", "")

    prompt_parts.append(policy_prompt)

    # Add excluded domains if any
    if excluded_domains:
        prompt_parts.append(f"""
## Excluded Domains:
Do NOT visit or follow links to these domains:
{chr(10).join(f"- {d}" for d in excluded_domains)}
""")

    # Add custom instructions if provided
    if custom_prompt:
        prompt_parts.append(f"""
## Custom Instructions:
{custom_prompt}
""")

    return "\n".join(prompt_parts)


def get_extraction_prompt(url: str) -> str:
    """
    Generate the task prompt for extracting content from a specific page.

    Args:
        url: The URL to extract content from

    Returns:
        Task prompt for content extraction
    """
    return f"""
Extract the article content from this page: {url}

Return the extracted content in the following format:
1. TITLE: The main article headline
2. CONTENT: The full article text (preserve paragraph breaks)
3. PUBLISHED_AT: The publication date if found (ISO format preferred)
4. AUTHOR: The author name if found
5. SUMMARY: A brief 2-3 sentence summary of the article

If this is not an article page, indicate what type of page it is.
"""


def get_news_list_extraction_prompt(url: str, max_articles: int = 20) -> str:
    """
    Generate prompt for extracting article list from a news section page.

    Args:
        url: The news section/category URL
        max_articles: Maximum number of articles to extract

    Returns:
        Task prompt for list extraction
    """
    return f"""
Extract the list of news articles from this page: {url}

Find up to {max_articles} news articles and return each in this format:
---ARTICLE_LINK---
TITLE: [article headline]
URL: [full article URL]
SUMMARY: [brief description or lead text if visible]
PUBLISHED_AT: [date if shown]
THUMBNAIL: [image URL if present]
---END_LINK---

Focus on:
- Main content area article links
- Skip navigation, ads, and sidebar widgets
- Include only actual news article links
- Preserve the order as shown on the page
"""


def get_rss_discovery_prompt(url: str) -> str:
    """
    Generate prompt for discovering RSS/Atom feeds.

    Args:
        url: The website URL to search for feeds

    Returns:
        Task prompt for RSS discovery
    """
    return f"""
Find all RSS and Atom feeds available on this website: {url}

Search in these locations:
1. HTML head section:
   - <link rel="alternate" type="application/rss+xml" ...>
   - <link rel="alternate" type="application/atom+xml" ...>
2. Common feed paths:
   - /feed, /rss, /atom, /feeds
   - /rss.xml, /feed.xml, /atom.xml
   - /news/rss, /blog/feed
3. Page footer or sidebar RSS icons/links
4. sitemap.xml references

Return found feeds in JSON format:
{{
    "feeds": [
        {{
            "url": "feed URL",
            "type": "rss|atom",
            "title": "feed title if known",
            "category": "category if specified"
        }}
    ],
    "sitemap_url": "sitemap URL if found",
    "robots_txt_checked": true|false
}}
"""

```

---

## backend/autonomous-crawler-service/src/crawler/url_filter.py

```py
"""URL filtering module for blocking non-content pages.

This module provides programmatic URL filtering to prevent the crawler from
visiting irrelevant pages like login, signup, help, marketing, etc.

Also includes URL liveness validation to filter out:
- Dead links (404, 500, etc.)
- Deleted/removed content pages
- LLM hallucination URLs
"""

import asyncio
import logging
import re
from typing import Optional, Tuple
from urllib.parse import urlparse, unquote

import aiohttp

logger = logging.getLogger(__name__)


# URL path patterns to block (regex patterns)
BLOCKED_PATH_PATTERNS = [
    # Authentication & Account
    r"/login",
    r"/signin",
    r"/sign-in",
    r"/signout",
    r"/sign-out",
    r"/logout",
    r"/log-out",
    r"/auth/",
    r"/oauth",
    r"/sso/",
    r"/signup",
    r"/sign-up",
    r"/register",
    r"/create-account",
    r"/join",
    r"/membership",
    r"/account",
    r"/profile",
    r"/settings",
    r"/preferences",
    r"/my-page",
    r"/mypage",
    r"/password",
    r"/forgot",
    r"/reset-password",
    # Help & Support
    r"/help",
    r"/support",
    r"/faq",
    r"/contact",
    r"/feedback",
    r"/report",
    r"/customer-service",
    r"/cs/",
    r"/inquiry",
    # Legal & Policy
    r"/about",
    r"/about-us",
    r"/team",
    r"/careers",
    r"/jobs",
    r"/privacy",
    r"/terms",
    r"/tos",
    r"/legal",
    r"/policy",
    r"/disclaimer",
    r"/copyright",
    # Commerce
    r"/subscribe",
    r"/subscription",
    r"/newsletter",
    r"/cart",
    r"/checkout",
    r"/payment",
    r"/order",
    r"/purchase",
    r"/buy",
    r"/shop",
    r"/store",
    r"/product/",
    r"/pricing",
    # Marketing & Campaigns
    r"/promo",
    r"/campaign",
    r"/landing",
    r"/offer",
    r"/deal",
    r"/coupon",
    r"/discount",
    r"/atrb",  # Attribution tracking
    r"/mkt/",
    r"/marketing/",
    r"/ad/",
    r"/ads/",
    r"/sponsor",
    # Utility Pages
    r"/search\?",  # Search result pages (not article pages)
    r"/tag/",
    r"/tags/",
    r"/category/$",  # Category index pages
    r"/sitemap",
    r"/robots\.txt",
    r"/feed$",
    r"/rss$",
    r"/atom$",
    # Social & Sharing
    r"/share",
    r"/print",
    r"/email",
    r"/bookmark",
    r"/notify",
    r"/notification",
    r"/alert",
    # Media Files
    r"\.(pdf|jpg|jpeg|png|gif|webp|svg|ico|mp4|mp3|avi|mov|zip|rar|exe|dmg)$",
    r"/download",
    r"/file/",
    r"/image/",
    r"/video/",
    r"/audio/",
    # API & Technical
    r"/api/",
    r"/ajax/",
    r"/graphql",
    r"/_next/",
    r"/_nuxt/",
    r"/static/",
    r"/assets/",
    # Language/Region Selectors
    r"/lang/",
    r"/locale/",
    r"/region/",
    # External Tools
    r"/grammar_checker",
    r"/spell_check",
    r"/translate",
    r"/calculator",
    r"/converter",
    r"/tool/",
    r"/widget/",
    # Q&A & Forums (not news articles)
    r"/qna/",
    r"/question",
    r"/answer",
    r"/forum",
    r"/board",
    r"/comment",
    r"/discussion",
    # Korean specific patterns
    r"/",
    r"/",
    r"/",
    r"/",
    r"/",
]

# Domain patterns to block entirely
BLOCKED_DOMAINS = [
    # Authentication domains
    r"^nid\.",  # Naver ID
    r"^accounts\.",
    r"^auth\.",
    r"^login\.",
    r"^sso\.",
    r"^oauth\.",
    # Marketing & Tracking
    r"^mkt\.",  # Marketing
    r"^ads\.",
    r"^ad\.",
    r"^track\.",
    r"^analytics\.",
    r"^pixel\.",
    # Notification & Messaging
    r"^notify\.",
    r"^notification\.",
    r"^mail\.",
    r"^email\.",
    r"^message\.",
    # Help & Support subdomains
    r"^help\.",
    r"^support\.",
    r"^faq\.",
    r"^cs\.",
    # Social Media (unless explicitly allowed)
    r"^facebook\.com$",
    r"^twitter\.com$",
    r"^x\.com$",
    r"^instagram\.com$",
    r"^linkedin\.com$",
    r"^tiktok\.com$",
    r"^youtube\.com$",
    # Tools & Utilities
    r"^dic\.",  # Dictionary
    r"^translate\.",
    r"^map\.",
    r"^maps\.",
    # CDN & Static
    r"^cdn\.",
    r"^static\.",
    r"^img\.",
    r"^images\.",
    r"^assets\.",
]

# URL patterns that indicate article-like content (positive signals)
ARTICLE_URL_PATTERNS = [
    r"/news/",
    r"/article/",
    r"/story/",
    r"/post/",
    r"/blog/",
    r"/\d{4}/\d{2}/\d{2}/",  # Date-based URLs like /2024/01/15/
    r"/\d{4}/\d{2}/",  # Year-month URLs
    r"/entry/",
    r"/view/",
    r"/read/",
    r"/detail/",
    r"/content/",
    r"aid=\d+",  # Article ID parameter
    r"article_id=",
    r"newsId=",
]

# Compiled regex patterns for performance
_compiled_blocked_paths: list[re.Pattern] | None = None
_compiled_blocked_domains: list[re.Pattern] | None = None
_compiled_article_patterns: list[re.Pattern] | None = None


def _compile_patterns() -> None:
    """Compile regex patterns for performance."""
    global _compiled_blocked_paths, _compiled_blocked_domains, _compiled_article_patterns

    if _compiled_blocked_paths is None:
        _compiled_blocked_paths = [
            re.compile(pattern, re.IGNORECASE) for pattern in BLOCKED_PATH_PATTERNS
        ]
    if _compiled_blocked_domains is None:
        _compiled_blocked_domains = [
            re.compile(pattern, re.IGNORECASE) for pattern in BLOCKED_DOMAINS
        ]
    if _compiled_article_patterns is None:
        _compiled_article_patterns = [
            re.compile(pattern, re.IGNORECASE) for pattern in ARTICLE_URL_PATTERNS
        ]


def clean_url(url: str) -> str:
    """Clean and normalize URL for filtering.

    Args:
        url: Raw URL string

    Returns:
        Cleaned URL string
    """
    # Remove trailing punctuation that might be attached from text parsing
    url = url.rstrip(")")
    url = url.rstrip("]")
    url = url.rstrip("*")
    url = url.rstrip(".")
    url = url.rstrip(",")

    # Decode URL-encoded characters for pattern matching
    try:
        url = unquote(url)
    except Exception:
        pass

    return url


def should_block_url(url: str, log_reason: bool = True) -> bool:
    """Check if a URL should be blocked from crawling.

    Args:
        url: The URL to check
        log_reason: Whether to log the reason for blocking

    Returns:
        True if the URL should be blocked, False otherwise
    """
    _compile_patterns()

    url = clean_url(url)

    try:
        parsed = urlparse(url)
    except Exception:
        if log_reason:
            logger.debug("URL blocked: invalid URL format - %s", url[:100])
        return True

    hostname = parsed.netloc.lower()
    path = parsed.path.lower()
    full_url_lower = url.lower()

    # Check domain patterns
    if _compiled_blocked_domains:
        for pattern in _compiled_blocked_domains:
            if pattern.search(hostname):
                if log_reason:
                    logger.debug(
                        "URL blocked: domain pattern match - %s (pattern: %s)",
                        url[:100],
                        pattern.pattern,
                    )
                return True

    # Check path patterns
    if _compiled_blocked_paths:
        for pattern in _compiled_blocked_paths:
            if pattern.search(path) or pattern.search(full_url_lower):
                if log_reason:
                    logger.debug(
                        "URL blocked: path pattern match - %s (pattern: %s)",
                        url[:100],
                        pattern.pattern,
                    )
                return True

    return False


def is_likely_article_url(url: str) -> float:
    """Calculate confidence score that URL is an article.

    Args:
        url: The URL to check

    Returns:
        Confidence score between 0.0 and 1.0
    """
    _compile_patterns()

    url = clean_url(url)
    url_lower = url.lower()

    # Start with neutral score
    score = 0.5

    # Check for article-like patterns (positive signals)
    if _compiled_article_patterns:
        for pattern in _compiled_article_patterns:
            if pattern.search(url_lower):
                score += 0.15
                break  # Only count once

    # Penalize if it matches blocked patterns
    if should_block_url(url, log_reason=False):
        score -= 0.6

    # URLs with very long query strings are often not articles
    try:
        parsed = urlparse(url)
        if len(parsed.query) > 200:
            score -= 0.2
    except Exception:
        pass

    # Clamp to 0-1 range
    return max(0.0, min(1.0, score))


def filter_urls(urls: list[str], min_article_score: float = 0.3) -> list[str]:
    """Filter a list of URLs, removing blocked ones.

    Args:
        urls: List of URLs to filter
        min_article_score: Minimum article score to include

    Returns:
        List of URLs that passed filtering
    """
    filtered = []
    for url in urls:
        if not should_block_url(url):
            if is_likely_article_url(url) >= min_article_score:
                filtered.append(url)
    return filtered


def get_url_filter_stats(urls: list[str]) -> dict:
    """Get statistics about URL filtering for a list of URLs.

    Args:
        urls: List of URLs to analyze

    Returns:
        Dictionary with filtering statistics
    """
    stats = {
        "total": len(urls),
        "blocked": 0,
        "passed": 0,
        "blocked_by_domain": 0,
        "blocked_by_path": 0,
        "high_article_score": 0,
    }

    for url in urls:
        url = clean_url(url)
        try:
            parsed = urlparse(url)
            hostname = parsed.netloc.lower()

            # Check domain
            domain_blocked = False
            for pattern in _compiled_blocked_domains or []:
                if pattern.search(hostname):
                    domain_blocked = True
                    break

            if domain_blocked:
                stats["blocked"] += 1
                stats["blocked_by_domain"] += 1
                continue

            # Check path
            if should_block_url(url, log_reason=False):
                stats["blocked"] += 1
                stats["blocked_by_path"] += 1
                continue

            stats["passed"] += 1

            if is_likely_article_url(url) >= 0.6:
                stats["high_article_score"] += 1

        except Exception:
            stats["blocked"] += 1

    return stats


# ============================================
# URL Liveness Validation (HTTP HEAD/GET Check)
# ============================================

# Patterns indicating deleted/error pages (Korean & English)
DELETED_PAGE_PATTERNS = [
    # Korean patterns
    re.compile(r"\s*[]", re.IGNORECASE),
    re.compile(r"", re.IGNORECASE),
    re.compile(r"\s*\s*", re.IGNORECASE),
    re.compile(r"\s*\s*\s*", re.IGNORECASE),
    re.compile(r"\s*\s*", re.IGNORECASE),
    re.compile(r"\s*", re.IGNORECASE),
    re.compile(r"\s*", re.IGNORECASE),
    re.compile(r"\s*", re.IGNORECASE),
    re.compile(r"\s*", re.IGNORECASE),
    re.compile(r"\s*\s*", re.IGNORECASE),
    # English patterns
    re.compile(r"page\s*not\s*found", re.IGNORECASE),
    re.compile(r"404\s*error", re.IGNORECASE),
    re.compile(r"not\s*found", re.IGNORECASE),
    re.compile(r"this\s*page\s*does\s*not\s*exist", re.IGNORECASE),
    re.compile(r"content\s*has\s*been\s*removed", re.IGNORECASE),
    re.compile(r"content\s*is\s*no\s*longer\s*available", re.IGNORECASE),
    re.compile(r"access\s*denied", re.IGNORECASE),
    re.compile(r"article\s*not\s*found", re.IGNORECASE),
    re.compile(r"post\s*has\s*been\s*deleted", re.IGNORECASE),
    re.compile(r"this\s*content\s*is\s*unavailable", re.IGNORECASE),
    re.compile(r"sorry.*couldn't find", re.IGNORECASE),
]

# LLM hallucination URL patterns
HALLUCINATION_PATTERNS = [
    re.compile(r"example\.com", re.IGNORECASE),
    re.compile(r"sample\.com", re.IGNORECASE),
    re.compile(r"test\.com", re.IGNORECASE),
    re.compile(r"fake\.(com|org|net)", re.IGNORECASE),
    re.compile(r"placeholder\.(com|org|net)", re.IGNORECASE),
    re.compile(r"/article/\d{10,}", re.IGNORECASE),  # Abnormally long article IDs
    re.compile(r"www\d+\.", re.IGNORECASE),  # www1., www2. etc.
]

# Trusted domains (skip liveness check)
TRUSTED_DOMAINS = {
    "wikipedia.org",
    "en.wikipedia.org",
    "ko.wikipedia.org",
    "scholar.google.com",
    "pubmed.ncbi.nlm.nih.gov",
    "doi.org",
    "arxiv.org",
    "nature.com",
    "science.org",
    "sciencedirect.com",
    "springer.com",
    "ncbi.nlm.nih.gov",
    "britannica.com",
    "namu.wiki",
    "kosis.kr",
}

# URL validation result cache
_url_validation_cache: dict[str, Tuple[bool, str]] = {}
_cache_max_size = 1000


def is_hallucination_url(url: str) -> bool:
    """Check if URL matches LLM hallucination patterns.
    
    Args:
        url: URL to check
        
    Returns:
        True if URL is likely a hallucination
    """
    for pattern in HALLUCINATION_PATTERNS:
        if pattern.search(url):
            logger.debug("URL matches hallucination pattern: %s", url[:100])
            return True
    return False


def is_trusted_domain(url: str) -> bool:
    """Check if URL belongs to a trusted domain.
    
    Args:
        url: URL to check
        
    Returns:
        True if URL is from a trusted domain
    """
    try:
        parsed = urlparse(url)
        hostname = parsed.netloc.lower()
        
        for trusted in TRUSTED_DOMAINS:
            if hostname == trusted or hostname.endswith("." + trusted):
                return True
    except Exception:
        pass
    return False


def is_deleted_page_content(content: str, min_length: int = 100) -> bool:
    """Check if content indicates a deleted/error page.
    
    Args:
        content: Page content to check
        min_length: Minimum content length to consider valid
        
    Returns:
        True if content appears to be from a deleted/error page
    """
    if not content or len(content.strip()) < min_length:
        # Very short content - check for error patterns
        for pattern in DELETED_PAGE_PATTERNS:
            if pattern.search(content or ""):
                return True
        # Empty or too short
        if not content or len(content.strip()) < 50:
            return True
            
    # Check for error patterns in longer content
    for pattern in DELETED_PAGE_PATTERNS:
        if pattern.search(content):
            # If content is substantial, might be false positive
            if len(content) > 500:
                continue
            return True
    
    return False


async def check_url_liveness(
    url: str,
    timeout: float = 5.0,
    use_cache: bool = True,
) -> Tuple[bool, int, Optional[str]]:
    """Check if URL is accessible via HTTP HEAD/GET request.
    
    Args:
        url: URL to check
        timeout: Request timeout in seconds
        use_cache: Whether to use cached results
        
    Returns:
        Tuple of (is_accessible, http_status_code, error_message)
    """
    # Check cache
    if use_cache and url in _url_validation_cache:
        is_valid, reason = _url_validation_cache[url]
        status = 200 if is_valid else 404
        return (is_valid, status, None if is_valid else reason)
    
    # Check for hallucination patterns first
    if is_hallucination_url(url):
        _cache_result(url, False, "Hallucination URL pattern")
        return (False, 0, "Hallucination URL pattern")
    
    # Skip liveness check for trusted domains
    if is_trusted_domain(url):
        _cache_result(url, True, None)
        return (True, 200, None)
    
    # Validate URL format
    try:
        parsed = urlparse(url)
        if not parsed.scheme or not parsed.netloc:
            _cache_result(url, False, "Invalid URL format")
            return (False, 0, "Invalid URL format")
    except Exception:
        _cache_result(url, False, "URL parse error")
        return (False, 0, "URL parse error")
    
    # Perform HTTP HEAD request
    try:
        async with aiohttp.ClientSession() as session:
            # Try HEAD first (faster)
            try:
                async with session.head(
                    url,
                    timeout=aiohttp.ClientTimeout(total=timeout),
                    allow_redirects=True,
                    headers={"User-Agent": "NewsInsight-Validator/1.0"},
                ) as response:
                    is_valid = 200 <= response.status < 400
                    _cache_result(url, is_valid, None if is_valid else f"HTTP {response.status}")
                    return (is_valid, response.status, None if is_valid else f"HTTP {response.status}")
            except aiohttp.ClientError:
                # HEAD failed, try GET
                async with session.get(
                    url,
                    timeout=aiohttp.ClientTimeout(total=timeout),
                    allow_redirects=True,
                    headers={"User-Agent": "NewsInsight-Validator/1.0"},
                ) as response:
                    is_valid = 200 <= response.status < 400
                    _cache_result(url, is_valid, None if is_valid else f"HTTP {response.status}")
                    return (is_valid, response.status, None if is_valid else f"HTTP {response.status}")
                    
    except asyncio.TimeoutError:
        _cache_result(url, False, "Timeout")
        return (False, 0, "Timeout")
    except aiohttp.ClientError as e:
        error_msg = str(e)[:100]
        _cache_result(url, False, error_msg)
        return (False, 0, error_msg)
    except Exception as e:
        error_msg = str(e)[:100]
        _cache_result(url, False, error_msg)
        return (False, 0, error_msg)


def _cache_result(url: str, is_valid: bool, reason: Optional[str]) -> None:
    """Cache URL validation result with size limit."""
    global _url_validation_cache
    
    # Clean cache if too large
    if len(_url_validation_cache) >= _cache_max_size:
        # Remove oldest entries (simple FIFO)
        keys_to_remove = list(_url_validation_cache.keys())[:_cache_max_size // 2]
        for key in keys_to_remove:
            del _url_validation_cache[key]
    
    _url_validation_cache[url] = (is_valid, reason or "")


async def validate_urls_batch(
    urls: list[str],
    timeout: float = 5.0,
    max_concurrent: int = 10,
) -> dict[str, Tuple[bool, int, Optional[str]]]:
    """Validate multiple URLs concurrently.
    
    Args:
        urls: List of URLs to validate
        timeout: Request timeout per URL
        max_concurrent: Maximum concurrent requests
        
    Returns:
        Dictionary mapping URL to (is_valid, status_code, error_message)
    """
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async def check_with_semaphore(url: str) -> Tuple[str, Tuple[bool, int, Optional[str]]]:
        async with semaphore:
            result = await check_url_liveness(url, timeout)
            return (url, result)
    
    tasks = [check_with_semaphore(url) for url in urls]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    result_dict = {}
    for item in results:
        if isinstance(item, Exception):
            continue
        url, validation_result = item
        result_dict[url] = validation_result
    
    return result_dict


async def filter_valid_urls(
    urls: list[str],
    timeout: float = 5.0,
    max_concurrent: int = 10,
) -> list[str]:
    """Filter list of URLs to keep only valid, accessible ones.
    
    Args:
        urls: List of URLs to filter
        timeout: Request timeout per URL
        max_concurrent: Maximum concurrent requests
        
    Returns:
        List of valid, accessible URLs
    """
    results = await validate_urls_batch(urls, timeout, max_concurrent)
    
    valid_urls = []
    for url in urls:
        if url in results:
            is_valid, status, error = results[url]
            if is_valid:
                valid_urls.append(url)
            else:
                logger.debug("URL filtered out: %s (status=%d, error=%s)", url[:100], status, error)
    
    logger.info("URL validation: %d valid out of %d total", len(valid_urls), len(urls))
    return valid_urls


def validate_content_quality(
    url: str,
    content: str,
    min_length: int = 100,
) -> Tuple[bool, str]:
    """Validate content quality for a given URL.
    
    Args:
        url: Source URL
        content: Page content
        min_length: Minimum acceptable content length
        
    Returns:
        Tuple of (is_valid, reason)
    """
    if not content:
        return (False, "Empty content")
    
    content_length = len(content.strip())
    
    if content_length < 50:
        return (False, f"Content too short: {content_length} chars")
    
    if is_deleted_page_content(content, min_length):
        return (False, "Deleted/error page content detected")
    
    if content_length < min_length:
        return (False, f"Content below minimum length: {content_length} < {min_length}")
    
    return (True, "Valid")


def get_cache_stats() -> dict:
    """Get URL validation cache statistics.
    
    Returns:
        Dictionary with cache statistics
    """
    valid_count = sum(1 for is_valid, _ in _url_validation_cache.values() if is_valid)
    return {
        "total_entries": len(_url_validation_cache),
        "valid_urls": valid_count,
        "invalid_urls": len(_url_validation_cache) - valid_count,
        "max_size": _cache_max_size,
    }


def clear_cache() -> None:
    """Clear URL validation cache."""
    global _url_validation_cache
    _url_validation_cache = {}
    logger.info("URL validation cache cleared")

```

---

## backend/autonomous-crawler-service/src/kafka/__init__.py

```py
"""Kafka module for autonomous-crawler-service."""

from .consumer import BrowserTaskConsumer
from .producer import CrawlResultProducer
from .messages import BrowserTaskMessage, CrawlResultMessage

__all__ = [
    "BrowserTaskConsumer",
    "CrawlResultProducer",
    "BrowserTaskMessage",
    "CrawlResultMessage",
]

```

---

## backend/autonomous-crawler-service/src/kafka/consumer.py

```py
"""Kafka consumer for browser task messages."""

import asyncio
import json
from typing import AsyncGenerator, Callable, Awaitable

import structlog
from aiokafka import AIOKafkaConsumer
from aiokafka.errors import KafkaError

from src.config import Settings
from src.kafka.messages import BrowserTaskMessage

logger = structlog.get_logger(__name__)


class BrowserTaskConsumer:
    """Async Kafka consumer for browser task messages."""

    def __init__(self, settings: Settings) -> None:
        self.settings = settings
        self._consumer: AIOKafkaConsumer | None = None
        self._running = False

    async def start(self) -> None:
        """Start the Kafka consumer with retry logic."""
        kafka_settings = self.settings.kafka

        self._consumer = AIOKafkaConsumer(
            kafka_settings.browser_task_topic,
            bootstrap_servers=kafka_settings.bootstrap_servers,
            group_id=kafka_settings.consumer_group_id,
            auto_offset_reset=kafka_settings.auto_offset_reset,
            enable_auto_commit=kafka_settings.enable_auto_commit,
            max_poll_records=kafka_settings.max_poll_records,
            session_timeout_ms=kafka_settings.session_timeout_ms,
            heartbeat_interval_ms=kafka_settings.heartbeat_interval_ms,
            value_deserializer=lambda m: json.loads(m.decode("utf-8")),
        )

        # Retry connection with exponential backoff
        max_retries = 10
        retry_delay = 2
        for attempt in range(max_retries):
            try:
                await self._consumer.start()
                self._running = True
                logger.info(
                    "Kafka consumer started",
                    topic=kafka_settings.browser_task_topic,
                    group_id=kafka_settings.consumer_group_id,
                )
                return
            except Exception as e:
                if attempt < max_retries - 1:
                    logger.warning(
                        "Failed to connect to Kafka, retrying...",
                        attempt=attempt + 1,
                        max_retries=max_retries,
                        retry_delay=retry_delay,
                        error=str(e),
                    )
                    await asyncio.sleep(retry_delay)
                    retry_delay = min(retry_delay * 2, 30)  # Max 30 seconds
                else:
                    logger.error(
                        "Failed to connect to Kafka after all retries",
                        error=str(e),
                    )
                    raise

    async def stop(self) -> None:
        """Stop the Kafka consumer."""
        self._running = False
        if self._consumer:
            await self._consumer.stop()
            logger.info("Kafka consumer stopped")

    async def consume(self) -> AsyncGenerator[BrowserTaskMessage, None]:
        """
        Consume messages from Kafka topic.

        Yields BrowserTaskMessage objects. Caller is responsible for
        committing offsets after successful processing.
        """
        if not self._consumer:
            raise RuntimeError("Consumer not started. Call start() first.")

        while self._running:
            try:
                # Get batch of messages (max_poll_records=1 means one at a time)
                result = await self._consumer.getmany(timeout_ms=1000)

                for topic_partition, messages in result.items():
                    for msg in messages:
                        try:
                            task = BrowserTaskMessage.model_validate(msg.value)
                            logger.info(
                                "Received browser task",
                                job_id=task.job_id,
                                source_id=task.source_id,
                                seed_url=task.seed_url,
                                offset=msg.offset,
                            )
                            yield task

                            # Commit after successful processing
                            await self._consumer.commit()
                            logger.debug(
                                "Committed offset",
                                offset=msg.offset,
                                partition=topic_partition.partition,
                            )

                        except Exception as e:
                            logger.error(
                                "Failed to parse browser task message",
                                error=str(e),
                                raw_value=msg.value,
                            )
                            # Still commit to avoid infinite retry on malformed messages
                            await self._consumer.commit()

            except KafkaError as e:
                logger.error("Kafka consumer error", error=str(e))
                await asyncio.sleep(1)  # Back off on errors

    async def run_with_handler(
        self,
        handler: Callable[[BrowserTaskMessage], Awaitable[None]],
    ) -> None:
        """
        Run consumer with a message handler callback.

        Args:
            handler: Async function to process each message
        """
        async for task in self.consume():
            try:
                await handler(task)
            except Exception as e:
                logger.error(
                    "Handler failed for task",
                    job_id=task.job_id,
                    error=str(e),
                    exc_info=True,
                )
                # Continue processing next messages

```

---

## backend/autonomous-crawler-service/src/kafka/messages.py

```py
"""Kafka message schemas matching Java DTOs."""

import json
from datetime import datetime
from typing import Any, Optional, Union

from pydantic import BaseModel, Field, field_validator


def parse_java_datetime(value: Any) -> datetime | None:
    """
    Parse datetime from various formats:
    - ISO-8601 string: "2025-12-17T11:29:39"
    - Java LocalDateTime array: [2025, 12, 17, 11, 29, 39, 532902301]
    - Python datetime object
    - None
    """
    if value is None:
        return None
    if isinstance(value, datetime):
        return value
    if isinstance(value, str):
        # ISO-8601 string format
        try:
            # Try with microseconds
            return datetime.fromisoformat(value.replace("Z", "+00:00"))
        except ValueError:
            # Try without timezone
            return datetime.fromisoformat(value)
    if isinstance(value, (list, tuple)) and len(value) >= 6:
        # Java LocalDateTime array format: [year, month, day, hour, minute, second, nano?]
        year, month, day, hour, minute, second = value[:6]
        microsecond = value[6] // 1000 if len(value) > 6 else 0  # nano to micro
        return datetime(year, month, day, hour, minute, second, microsecond)
    raise ValueError(f"Cannot parse datetime from: {value} (type: {type(value).__name__})")


class BrowserTaskMessage(BaseModel):
    """
    Kafka message for browser-based autonomous crawling tasks.
    Matches: com.newsinsight.collector.dto.BrowserTaskMessage
    """

    job_id: int = Field(..., alias="jobId", description="Unique job ID for tracking")
    source_id: int = Field(..., alias="sourceId", description="Data source ID")
    source_name: str | None = Field(
        default=None, alias="sourceName", description="Source name for logging/display"
    )
    seed_url: str = Field(..., alias="seedUrl", description="Seed URL to start exploration from")
    max_depth: int | None = Field(
        default=2, alias="maxDepth", description="Maximum link traversal depth"
    )
    max_pages: int | None = Field(
        default=10, alias="maxPages", description="Maximum pages to visit"
    )
    budget_seconds: int | None = Field(
        default=300, alias="budgetSeconds", description="Time budget in seconds"
    )
    policy: str | None = Field(
        default="NEWS_ONLY",
        description="Exploration policy (focused_topic, domain_wide, news_only, etc.)",
    )
    focus_keywords: str | None = Field(
        default=None, alias="focusKeywords", description="Focus keywords for FOCUSED_TOPIC policy"
    )
    custom_prompt: str | None = Field(
        default=None, alias="customPrompt", description="Custom prompt/instructions for AI agent"
    )
    capture_screenshots: bool | None = Field(
        default=False, alias="captureScreenshots", description="Whether to capture screenshots"
    )
    extract_structured: bool | None = Field(
        default=True, alias="extractStructured", description="Whether to extract structured data"
    )
    excluded_domains: str | None = Field(
        default=None, alias="excludedDomains", description="Domains to exclude"
    )
    callback_url: str | None = Field(
        default=None, alias="callbackUrl", description="Callback URL for session completion"
    )
    callback_token: str | None = Field(
        default=None, alias="callbackToken", description="Callback authentication token"
    )
    metadata: dict[str, Any] | None = Field(default=None, description="Additional metadata")
    created_at: datetime | None = Field(
        default=None, alias="createdAt", description="Task creation timestamp"
    )

    # Validator to handle Java LocalDateTime array format
    @field_validator("created_at", mode="before")
    @classmethod
    def parse_created_at(cls, value: Any) -> datetime | None:
        """Parse createdAt from Java LocalDateTime array or ISO string."""
        return parse_java_datetime(value)

    class Config:
        populate_by_name = True

    def get_excluded_domains_list(self) -> list[str]:
        """Parse excluded domains string into list."""
        if not self.excluded_domains:
            return []
        return [d.strip() for d in self.excluded_domains.split(",") if d.strip()]

    def get_focus_keywords_list(self) -> list[str]:
        """Parse focus keywords string into list."""
        if not self.focus_keywords:
            return []
        return [k.strip() for k in self.focus_keywords.split(",") if k.strip()]


class CrawlResultMessage(BaseModel):
    """
    Kafka message for crawl results.
    Matches: com.newsinsight.collector.dto.CrawlResultMessage

      Java DTO ,    metadata_json JSON .
    """

    job_id: int = Field(..., alias="jobId", description="Job ID this result belongs to")
    source_id: int = Field(..., alias="sourceId", description="Data source ID")
    title: str = Field(..., description="Article/page title")
    content: str = Field(..., description="Extracted content")
    url: str = Field(..., description="Page URL")
    published_at: str | None = Field(
        default=None, alias="publishedAt", description="Publication date as ISO string"
    )
    metadata_json: str | None = Field(
        default=None, alias="metadataJson", description="Additional metadata as JSON string"
    )

    class Config:
        populate_by_name = True

    def to_kafka_dict(self) -> dict[str, Any]:
        """Convert to Kafka-compatible dict with Java-style camelCase keys."""
        return {
            "jobId": self.job_id,
            "sourceId": self.source_id,
            "title": self.title,
            "content": self.content,
            "url": self.url,
            "publishedAt": self.published_at,
            "metadataJson": self.metadata_json,
        }


class NewsArticleMetadata(BaseModel):
    """
       .

    CrawlResultMessage.metadata_json JSON  .
    Java   JSON    .
    """

    # / 
    authors: list[str] | None = Field(default=None, description=" / ")
    author_email: str | None = Field(default=None, description=" ")

    #  
    category: str | None = Field(default=None, description="  (, ,  )")
    subcategory: str | None = Field(default=None, description=" ")
    tags: list[str] | None = Field(default=None, description=" /")

    #  
    word_count: int | None = Field(default=None, description="  ")
    reading_time_minutes: float | None = Field(default=None, description="   ()")
    language: str | None = Field(default="ko", description="  ")

    #  
    is_breaking: bool = Field(default=False, description=" ")
    is_exclusive: bool = Field(default=False, description="  ")
    is_opinion: bool = Field(default=False, description="/ ")
    has_paywall: bool = Field(default=False, description="   ")

    #  
    thumbnail_url: str | None = Field(default=None, description="  URL")
    image_urls: list[str] | None = Field(default=None, description="  URL ")
    video_urls: list[str] | None = Field(default=None, description="  URL ")

    #  
    related_article_urls: list[str] | None = Field(default=None, description="  URL ")

    #  
    source_name: str | None = Field(default=None, description="/")
    source_bias: str | None = Field(default=None, description="  ( )")

    #  
    extraction_confidence: float | None = Field(default=None, description="  (0.0-1.0)")
    missing_fields: list[str] | None = Field(default=None, description="   ")

    #  
    crawled_at: str | None = Field(default=None, description="  (ISO )")
    crawl_method: str | None = Field(default=None, description="  (ai_agent, rss, api)")

    def to_json(self) -> str:
        """JSON  """
        return self.model_dump_json(exclude_none=True)

    @classmethod
    def from_json(cls, json_str: str) -> "NewsArticleMetadata":
        """JSON  """
        return cls.model_validate_json(json_str)


class EnhancedCrawlResultMessage(CrawlResultMessage):
    """
       .

     CrawlResultMessage       .
    """

    def set_news_metadata(self, metadata: NewsArticleMetadata) -> None:
        """  """
        self.metadata_json = metadata.to_json()

    def get_news_metadata(self) -> Optional[NewsArticleMetadata]:
        """  """
        if not self.metadata_json:
            return None
        try:
            return NewsArticleMetadata.from_json(self.metadata_json)
        except Exception:
            return None

    @classmethod
    def create_news_result(
        cls,
        job_id: int,
        source_id: int,
        url: str,
        title: str,
        content: str,
        published_at: str | None = None,
        authors: list[str] | None = None,
        category: str | None = None,
        is_breaking: bool = False,
        thumbnail_url: str | None = None,
        source_name: str | None = None,
        word_count: int | None = None,
        **extra_metadata,
    ) -> "EnhancedCrawlResultMessage":
        """
             .

        Args:
            job_id:  ID
            source_id:  ID
            url:  URL
            title:  
            content:  
            published_at:  (ISO )
            authors:  
            category: 
            is_breaking:  
            thumbnail_url:  URL
            source_name: 
            word_count:  
            **extra_metadata:  

        Returns:
            EnhancedCrawlResultMessage 
        """
        #    
        if word_count is None and content:
            word_count = len(content.split())

        #  
        metadata = NewsArticleMetadata(
            authors=authors,
            category=category,
            is_breaking=is_breaking,
            thumbnail_url=thumbnail_url,
            source_name=source_name,
            word_count=word_count,
            crawled_at=datetime.utcnow().isoformat() + "Z",
            crawl_method="ai_agent",
            **extra_metadata,
        )

        #   
        result = cls(
            jobId=job_id,
            sourceId=source_id,
            url=url,
            title=title,
            content=content,
            publishedAt=published_at,
        )
        result.set_news_metadata(metadata)

        return result


class CrawlSessionCallback(BaseModel):
    """
        .

    autonomous-crawler-service    data-collection-service .
    """

    job_id: int = Field(..., alias="jobId", description=" ID")
    source_id: int = Field(..., alias="sourceId", description=" ID")
    status: str = Field(..., description="  (COMPLETED, FAILED, TIMEOUT)")
    articles_extracted: int = Field(
        default=0, alias="articlesExtracted", description="  "
    )
    pages_visited: int = Field(default=0, alias="pagesVisited", description="  ")
    elapsed_seconds: float = Field(default=0, alias="elapsedSeconds", description="  ()")
    error: str | None = Field(default=None, description="  ( )")
    captcha_encountered: bool = Field(
        default=False, alias="captchaEncountered", description="CAPTCHA  "
    )
    captcha_solved: bool = Field(
        default=False, alias="captchaSolved", description="CAPTCHA  "
    )

    class Config:
        populate_by_name = True

    def to_callback_dict(self) -> dict[str, Any]:
        """ API   """
        return {
            "jobId": self.job_id,
            "sourceId": self.source_id,
            "status": self.status,
            "articlesExtracted": self.articles_extracted,
            "pagesVisited": self.pages_visited,
            "elapsedSeconds": self.elapsed_seconds,
            "error": self.error,
            "captchaEncountered": self.captcha_encountered,
            "captchaSolved": self.captcha_solved,
        }

```

---

## backend/autonomous-crawler-service/src/kafka/producer.py

```py
"""Kafka producer for crawl result messages."""

import json
from typing import Any

import structlog
from aiokafka import AIOKafkaProducer
from aiokafka.errors import KafkaError
from tenacity import retry, stop_after_attempt, wait_exponential

from src.config import Settings
from src.kafka.messages import CrawlResultMessage

logger = structlog.get_logger(__name__)


class CrawlResultProducer:
    """Async Kafka producer for crawl result messages."""

    def __init__(self, settings: Settings) -> None:
        self.settings = settings
        self._producer: AIOKafkaProducer | None = None

    async def start(self) -> None:
        """Start the Kafka producer."""
        self._producer = AIOKafkaProducer(
            bootstrap_servers=self.settings.kafka.bootstrap_servers,
            value_serializer=lambda v: json.dumps(v, default=str).encode("utf-8"),
            # Reliability settings
            acks="all",
        )

        await self._producer.start()
        logger.info(
            "Kafka producer started",
            bootstrap_servers=self.settings.kafka.bootstrap_servers,
        )

    async def stop(self) -> None:
        """Stop the Kafka producer."""
        if self._producer:
            await self._producer.stop()
            logger.info("Kafka producer stopped")

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=10),
    )
    async def send_result(self, result: CrawlResultMessage) -> None:
        """
        Send a crawl result to Kafka.

        Args:
            result: CrawlResultMessage to send
        """
        if not self._producer:
            raise RuntimeError("Producer not started. Call start() first.")

        topic = self.settings.kafka.crawl_result_topic
        value = result.to_kafka_dict()

        try:
            # Use job_id as key for partitioning (all results for same job go to same partition)
            key = str(result.job_id).encode("utf-8")

            await self._producer.send_and_wait(
                topic=topic,
                value=value,
                key=key,
            )

            logger.info(
                "Sent crawl result",
                job_id=result.job_id,
                source_id=result.source_id,
                url=result.url,
                title=result.title[:50] if result.title else None,
            )

        except KafkaError as e:
            logger.error(
                "Failed to send crawl result",
                job_id=result.job_id,
                error=str(e),
            )
            raise

    async def send_batch(self, results: list[CrawlResultMessage]) -> tuple[int, int]:
        """
        Send multiple crawl results to Kafka.

        Args:
            results: List of CrawlResultMessage objects

        Returns:
            Tuple of (successful_count, failed_count)
        """
        success_count = 0
        fail_count = 0

        for result in results:
            try:
                await self.send_result(result)
                success_count += 1
            except Exception as e:
                logger.error(
                    "Failed to send result in batch",
                    job_id=result.job_id,
                    url=result.url,
                    error=str(e),
                )
                fail_count += 1

        logger.info(
            "Batch send completed",
            success=success_count,
            failed=fail_count,
            total=len(results),
        )

        return success_count, fail_count

```

---

## backend/autonomous-crawler-service/src/main.py

```py
"""
Main entry point for autonomous-crawler-service.

Supports two modes:
1. Kafka mode (default): Consumes tasks from Kafka, produces results to Kafka
2. API mode: REST API + SSE for direct web UI access (browser-agent integration)
3. Hybrid mode: Both Kafka consumer and REST API running together

Usage:
    # Kafka mode (default)
    python -m src.main

    # API mode only
    python -m src.main --mode api

    # Hybrid mode (both Kafka + API)
    python -m src.main --mode hybrid

    # Or via environment variable
    SERVICE_MODE=hybrid python -m src.main
"""

import argparse
import asyncio
import logging
import os
import signal
import sys
from contextlib import asynccontextmanager
from enum import Enum
from typing import AsyncGenerator, Optional

import structlog
from prometheus_client import start_http_server

from src.config import Settings, get_settings
from src.config.consul import load_config_from_consul, wait_for_consul, CONSUL_ENABLED
from src.crawler import AutonomousCrawlerAgent
from src.kafka import BrowserTaskConsumer, CrawlResultProducer
from src.kafka.messages import BrowserTaskMessage
from src.api.sse import SSEEventType, SSEManager, get_sse_manager
from src.metrics import (
    ARTICLES_EXTRACTED,
    BROWSER_SESSIONS_ACTIVE,
    KAFKA_MESSAGES_CONSUMED,
    KAFKA_MESSAGES_PRODUCED,
    TASK_DURATION,
    TASKS_COMPLETED,
    TASKS_IN_PROGRESS,
    TASKS_RECEIVED,
    init_service_info,
)


class ServiceMode(str, Enum):
    """Service operation mode"""

    KAFKA = "kafka"  # Kafka consumer only (original)
    API = "api"  # REST API only (browser-agent style)
    HYBRID = "hybrid"  # Both Kafka + REST API


def configure_logging(settings: Settings) -> None:
    """Configure structured logging."""
    processors = [
        structlog.contextvars.merge_contextvars,
        structlog.processors.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
    ]

    if settings.log_format == "json":
        processors.append(structlog.processors.JSONRenderer())
    else:
        processors.append(structlog.dev.ConsoleRenderer(colors=True))

    # Map log level string to logging module level
    log_level = getattr(logging, settings.log_level.upper(), logging.INFO)

    structlog.configure(
        processors=processors,
        wrapper_class=structlog.make_filtering_bound_logger(log_level),
        context_class=dict,
        logger_factory=structlog.PrintLoggerFactory(),
        cache_logger_on_first_use=True,
    )


class CrawlerService:
    """Main service class orchestrating the crawler components."""

    def __init__(self, settings: Settings, sse_manager: SSEManager | None = None) -> None:
        self.settings = settings
        self.logger = structlog.get_logger(__name__)
        self.consumer = BrowserTaskConsumer(settings)
        self.producer = CrawlResultProducer(settings)
        self.agent = AutonomousCrawlerAgent(settings)
        self.sse_manager = sse_manager or get_sse_manager()
        self._shutdown_event = asyncio.Event()

    async def start(self) -> None:
        """Start all service components."""
        self.logger.info("Starting autonomous-crawler-service")

        # Start metrics server
        if self.settings.metrics.enabled:
            start_http_server(self.settings.metrics.port)
            self.logger.info(
                "Metrics server started",
                port=self.settings.metrics.port,
            )

        # Initialize service info metrics
        init_service_info(
            version="0.1.0",
            llm_provider=self.settings.llm.provider,
        )

        # Start Kafka components
        await self.consumer.start()
        await self.producer.start()

        BROWSER_SESSIONS_ACTIVE.set(0)

        self.logger.info("Service started successfully")

    async def stop(self) -> None:
        """Stop all service components."""
        self.logger.info("Stopping autonomous-crawler-service")

        self._shutdown_event.set()

        await self.agent.close()
        await self.consumer.stop()
        await self.producer.stop()

        self.logger.info("Service stopped")

    async def handle_task(self, task: BrowserTaskMessage) -> None:
        """
        Handle a single browser task.

        Args:
            task: The browser task to process
        """
        import time

        start_time = time.time()
        policy = task.policy or "news_only"
        source_name = (
            task.metadata.get("source_name", f"Source-{task.source_id}")
            if task.metadata
            else f"Source-{task.source_id}"
        )

        TASKS_RECEIVED.labels(policy=policy).inc()
        TASKS_IN_PROGRESS.inc()
        BROWSER_SESSIONS_ACTIVE.inc()
        KAFKA_MESSAGES_CONSUMED.labels(topic=self.settings.kafka.browser_task_topic).inc()

        self.logger.info(
            "Processing browser task",
            job_id=task.job_id,
            source_id=task.source_id,
            seed_url=task.seed_url,
            policy=policy,
        )

        # SSE:   
        await self.sse_manager.send_collection_event(
            SSEEventType.COLLECTION_START,
            source_name=source_name,
            message=f" : {task.seed_url}",
            level="INFO",
            job_id=task.job_id,
            source_id=task.source_id,
            seed_url=task.seed_url,
            policy=policy,
        )

        status = "success"
        try:
            # Execute the crawl task
            results = await self.agent.execute_task(task)

            # Send results to Kafka
            for i, result in enumerate(results):
                await self.producer.send_result(result)
                KAFKA_MESSAGES_PRODUCED.labels(topic=self.settings.kafka.crawl_result_topic).inc()
                ARTICLES_EXTRACTED.labels(source_id=str(task.source_id)).inc()

                # SSE:    ( 5 )
                if (i + 1) % 5 == 0 or i == len(results) - 1:
                    await self.sse_manager.send_collection_event(
                        SSEEventType.COLLECTION_PROGRESS,
                        source_name=source_name,
                        message=f" {i + 1}/{len(results)}  ",
                        level="INFO",
                        job_id=task.job_id,
                        progress=i + 1,
                        total=len(results),
                    )

            self.logger.info(
                "Task completed",
                job_id=task.job_id,
                articles_extracted=len(results),
            )

            # SSE:   
            duration = time.time() - start_time
            await self.sse_manager.send_collection_event(
                SSEEventType.COLLECTION_COMPLETE,
                source_name=source_name,
                message=f" : {len(results)}  ({duration:.1f})",
                level="INFO",
                job_id=task.job_id,
                articles_extracted=len(results),
                duration_seconds=duration,
            )

        except Exception as e:
            status = "error"
            self.logger.error(
                "Task failed",
                job_id=task.job_id,
                error=str(e),
                exc_info=True,
            )

            # SSE:   
            await self.sse_manager.send_collection_event(
                SSEEventType.COLLECTION_ERROR,
                source_name=source_name,
                message=f" : {str(e)[:100]}",
                level="ERROR",
                job_id=task.job_id,
                error=str(e),
            )

        finally:
            duration = time.time() - start_time
            TASK_DURATION.labels(policy=policy).observe(duration)
            TASKS_COMPLETED.labels(policy=policy, status=status).inc()
            TASKS_IN_PROGRESS.dec()
            BROWSER_SESSIONS_ACTIVE.dec()

    async def run(self) -> None:
        """Main run loop - consume and process tasks."""
        await self.start()

        try:
            await self.consumer.run_with_handler(self.handle_task)
        except asyncio.CancelledError:
            self.logger.info("Run loop cancelled")
        finally:
            await self.stop()


async def run_api_server(settings: Settings, port: int = 8030) -> None:
    """Run the FastAPI REST API server."""
    import uvicorn
    from src.api.server import create_app

    logger = structlog.get_logger(__name__)
    logger.info("Starting REST API server", port=port)

    app = create_app(settings)

    config = uvicorn.Config(
        app,
        host="0.0.0.0",
        port=port,
        log_level="info",
        access_log=True,
    )
    server = uvicorn.Server(config)
    await server.serve()


async def run_hybrid_mode(settings: Settings, api_port: int = 8030) -> None:
    """Run both Kafka consumer and REST API server concurrently."""
    logger = structlog.get_logger(__name__)
    logger.info(
        "Starting hybrid mode",
        api_port=api_port,
        kafka_enabled=True,
    )

    # Import here to avoid circular imports
    import uvicorn
    from src.api.server import create_app

    # Create services
    service = CrawlerService(settings)
    app = create_app(settings)

    # Create uvicorn config
    config = uvicorn.Config(
        app,
        host="0.0.0.0",
        port=api_port,
        log_level="info",
        access_log=True,
    )
    api_server = uvicorn.Server(config)

    # Setup signal handling
    shutdown_event = asyncio.Event()

    async def shutdown():
        logger.info("Shutting down hybrid mode...")
        shutdown_event.set()
        await service.stop()
        api_server.should_exit = True

    loop = asyncio.get_running_loop()
    for sig in (signal.SIGINT, signal.SIGTERM):
        loop.add_signal_handler(sig, lambda: asyncio.create_task(shutdown()))

    # Start Kafka service components (without the run loop)
    await service.start()

    # Run both concurrently
    async def kafka_consumer_loop():
        try:
            await service.consumer.run_with_handler(service.handle_task)
        except asyncio.CancelledError:
            logger.info("Kafka consumer loop cancelled")

    await asyncio.gather(
        kafka_consumer_loop(),
        api_server.serve(),
        return_exceptions=True,
    )


async def main_async(mode: ServiceMode = ServiceMode.KAFKA) -> None:
    """Async main function with mode selection."""
    # Load configuration from Consul (if enabled)
    consul_keys, env_keys = [], []
    if CONSUL_ENABLED:
        # Wait for Consul to be available
        if wait_for_consul(max_attempts=30, delay=2.0):
            consul_keys, env_keys = load_config_from_consul()
        else:
            print(
                "WARNING: Consul not available, using environment variables only", file=sys.stderr
            )

    settings = get_settings()
    configure_logging(settings)

    logger = structlog.get_logger(__name__)

    api_port = int(os.getenv("API_PORT", "8030"))

    logger.info(
        "Initializing autonomous-crawler-service",
        mode=mode.value,
        kafka_servers=settings.kafka.bootstrap_servers,
        llm_provider=settings.llm.provider,
        consul_enabled=CONSUL_ENABLED,
        consul_keys_loaded=len(consul_keys),
        api_port=api_port if mode in (ServiceMode.API, ServiceMode.HYBRID) else None,
    )

    if mode == ServiceMode.KAFKA:
        # Original Kafka-only mode
        service = CrawlerService(settings)

        loop = asyncio.get_running_loop()

        def signal_handler() -> None:
            logger.info("Received shutdown signal")
            asyncio.create_task(service.stop())

        for sig in (signal.SIGINT, signal.SIGTERM):
            loop.add_signal_handler(sig, signal_handler)

        await service.run()

    elif mode == ServiceMode.API:
        # REST API only mode (browser-agent style)
        await run_api_server(settings, api_port)

    elif mode == ServiceMode.HYBRID:
        # Both Kafka and REST API
        await run_hybrid_mode(settings, api_port)


def parse_args() -> argparse.Namespace:
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Autonomous Crawler Service - AI-driven browser crawler",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Modes:
  kafka   - Kafka consumer only (default, for internal job processing)
  api     - REST API only (browser-agent style, for direct web UI access)
  hybrid  - Both Kafka consumer and REST API running together

Examples:
  python -m src.main                    # Kafka mode (default)
  python -m src.main --mode api         # REST API only
  python -m src.main --mode hybrid      # Both Kafka + REST API
  SERVICE_MODE=hybrid python -m src.main  # Via environment variable
        """,
    )
    parser.add_argument(
        "--mode",
        "-m",
        type=str,
        choices=["kafka", "api", "hybrid"],
        default=os.getenv("SERVICE_MODE", "kafka"),
        help="Service operation mode (default: kafka, or SERVICE_MODE env var)",
    )
    parser.add_argument(
        "--port",
        "-p",
        type=int,
        default=int(os.getenv("API_PORT", "8030")),
        help="API server port (default: 8030, or API_PORT env var)",
    )
    return parser.parse_args()


def main() -> None:
    """Main entry point."""
    args = parse_args()

    # Convert string mode to enum
    mode_map = {
        "kafka": ServiceMode.KAFKA,
        "api": ServiceMode.API,
        "hybrid": ServiceMode.HYBRID,
    }
    mode = mode_map.get(args.mode, ServiceMode.KAFKA)

    # Set API_PORT env var so it's accessible in main_async
    os.environ["API_PORT"] = str(args.port)

    try:
        asyncio.run(main_async(mode))
    except KeyboardInterrupt:
        print("Interrupted")
        sys.exit(0)


if __name__ == "__main__":
    main()

```

---

## backend/autonomous-crawler-service/src/mcp/__init__.py

```py
"""
MCP (Model Context Protocol) Adapter Module

MCP  ML Add-on  REST API .
"""

from src.mcp.client import MCPClient
from src.mcp.adapter import MCPAdapter, get_mcp_adapter
from src.mcp.router import router as mcp_router

__all__ = ["MCPClient", "MCPAdapter", "get_mcp_adapter", "mcp_router"]

```

---

## backend/autonomous-crawler-service/src/mcp/adapter.py

```py
"""
MCP Adapter - MCP  ML Add-on  

MCP  tool REST API  ,
ML Add-on    .
"""

import os
from typing import Any, Dict, List, Optional
from datetime import datetime
from enum import Enum

import structlog
from pydantic import BaseModel, Field

from .client import (
    BiasMCPClient,
    FactcheckMCPClient,
    TopicMCPClient,
    HuggingFaceMCPClient,
    NewsInsightMCPClient,
    MCPClient,
)

logger = structlog.get_logger(__name__)


# 
# MCP  
# 


class MCPServerConfig:
    """MCP   """

    BIAS_MCP_URL = os.environ.get("BIAS_MCP_URL", "http://bias-mcp:5001")
    FACTCHECK_MCP_URL = os.environ.get("FACTCHECK_MCP_URL", "http://factcheck-mcp:5002")
    TOPIC_MCP_URL = os.environ.get("TOPIC_MCP_URL", "http://topic-mcp:5003")
    NEWSINSIGHT_MCP_URL = os.environ.get("NEWSINSIGHT_MCP_URL", "http://newsinsight-mcp:5000")
    HUGGINGFACE_MCP_URL = os.environ.get("HUGGINGFACE_MCP_URL", "http://huggingface-mcp:5011")


# 
#   (ML Add-on )
# 


class MCPAddonCategory(str, Enum):
    """MCP Add-on """

    BIAS = "BIAS_ANALYSIS"
    FACTCHECK = "FACTCHECK"
    TOPIC = "TOPIC_CLASSIFICATION"
    SENTIMENT = "SENTIMENT"
    SUMMARIZATION = "SUMMARIZATION"
    ENTITY = "ENTITY_EXTRACTION"


class MCPAddonResponse(BaseModel):
    """MCP Add-on  """

    addon_key: str
    category: MCPAddonCategory
    success: bool
    data: Optional[Dict[str, Any]] = None
    report: Optional[str] = None
    error: Optional[str] = None
    latency_ms: int = 0
    generated_at: str = Field(default_factory=lambda: datetime.utcnow().isoformat())


class MCPAddonInfo(BaseModel):
    """MCP Add-on """

    addon_key: str
    name: str
    description: str
    category: MCPAddonCategory
    endpoint_url: str
    tools: List[str] = []
    enabled: bool = True
    health_status: str = "unknown"


# 
# MCP  
# 


class MCPAdapter:
    """MCP  ML Add-on  """

    def __init__(self):
        self.bias_client = BiasMCPClient(MCPServerConfig.BIAS_MCP_URL)
        self.factcheck_client = FactcheckMCPClient(MCPServerConfig.FACTCHECK_MCP_URL)
        self.topic_client = TopicMCPClient(MCPServerConfig.TOPIC_MCP_URL)
        self.newsinsight_client = NewsInsightMCPClient(MCPServerConfig.NEWSINSIGHT_MCP_URL)
        self.huggingface_client = HuggingFaceMCPClient(MCPServerConfig.HUGGINGFACE_MCP_URL)

        self._clients: Dict[str, MCPClient] = {
            "bias": self.bias_client,
            "factcheck": self.factcheck_client,
            "topic": self.topic_client,
            "newsinsight": self.newsinsight_client,
            "huggingface": self.huggingface_client,
        }

    # 
    # Add-on    
    # 

    async def list_addons(self) -> List[MCPAddonInfo]:
        """ MCP Add-on  """
        addons = [
            MCPAddonInfo(
                addon_key="mcp-bias",
                name="  (MCP)",
                description="  /  ",
                category=MCPAddonCategory.BIAS,
                endpoint_url=MCPServerConfig.BIAS_MCP_URL,
                tools=["get_bias_raw", "get_bias_report", "get_source_bias_list"],
            ),
            MCPAddonInfo(
                addon_key="mcp-factcheck",
                name="/ (MCP)",
                description="     ",
                category=MCPAddonCategory.FACTCHECK,
                endpoint_url=MCPServerConfig.FACTCHECK_MCP_URL,
                tools=[
                    "get_factcheck_raw",
                    "get_factcheck_report",
                    "get_source_reliability_list",
                ],
            ),
            MCPAddonInfo(
                addon_key="mcp-topic",
                name="  (MCP)",
                description=" ,   ",
                category=MCPAddonCategory.TOPIC,
                endpoint_url=MCPServerConfig.TOPIC_MCP_URL,
                tools=[
                    "get_topic_raw",
                    "get_topic_report",
                    "get_trending_topics",
                    "get_category_list",
                ],
            ),
            MCPAddonInfo(
                addon_key="mcp-sentiment",
                name="  (MCP)",
                description="  (//) ",
                category=MCPAddonCategory.SENTIMENT,
                endpoint_url=MCPServerConfig.NEWSINSIGHT_MCP_URL,
                tools=[
                    "get_sentiment_raw",
                    "get_sentiment_report",
                    "get_article_list",
                ],
            ),
            MCPAddonInfo(
                addon_key="mcp-huggingface",
                name="HuggingFace NLP (MCP)",
                description="HuggingFace   NLP ",
                category=MCPAddonCategory.SUMMARIZATION,
                endpoint_url=MCPServerConfig.HUGGINGFACE_MCP_URL,
                tools=[
                    "analyze_sentiment",
                    "summarize_article",
                    "extract_entities",
                    "extract_keywords",
                    "classify_news",
                ],
            ),
        ]
        return addons

    async def check_all_health(self) -> Dict[str, Dict[str, Any]]:
        """ MCP  """
        results = {}
        for name, client in self._clients.items():
            results[name] = await client.health_check()
        return results

    # 
    # Bias Analysis
    # 

    async def analyze_bias(
        self,
        keyword: str,
        days: int = 7,
        include_report: bool = False,
    ) -> MCPAddonResponse:
        """  """
        start_time = datetime.utcnow()

        try:
            result = await self.bias_client.get_bias_raw(keyword, days)

            if not result.get("success"):
                return MCPAddonResponse(
                    addon_key="mcp-bias",
                    category=MCPAddonCategory.BIAS,
                    success=False,
                    error=result.get("error", "Unknown error"),
                    latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
                )

            response = MCPAddonResponse(
                addon_key="mcp-bias",
                category=MCPAddonCategory.BIAS,
                success=True,
                data=result.get("data"),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

            #    
            if include_report:
                report_result = await self.bias_client.get_bias_report(keyword, days)
                if report_result.get("success"):
                    response.report = report_result.get("data")

            return response

        except Exception as e:
            logger.error("Bias analysis failed", error=str(e))
            return MCPAddonResponse(
                addon_key="mcp-bias",
                category=MCPAddonCategory.BIAS,
                success=False,
                error=str(e),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

    # 
    # Factcheck Analysis
    # 

    async def analyze_factcheck(
        self,
        keyword: str,
        days: int = 7,
        include_report: bool = False,
    ) -> MCPAddonResponse:
        """/  """
        start_time = datetime.utcnow()

        try:
            result = await self.factcheck_client.get_factcheck_raw(keyword, days)

            if not result.get("success"):
                return MCPAddonResponse(
                    addon_key="mcp-factcheck",
                    category=MCPAddonCategory.FACTCHECK,
                    success=False,
                    error=result.get("error", "Unknown error"),
                    latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
                )

            response = MCPAddonResponse(
                addon_key="mcp-factcheck",
                category=MCPAddonCategory.FACTCHECK,
                success=True,
                data=result.get("data"),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

            if include_report:
                report_result = await self.factcheck_client.get_factcheck_report(keyword, days)
                if report_result.get("success"):
                    response.report = report_result.get("data")

            return response

        except Exception as e:
            logger.error("Factcheck analysis failed", error=str(e))
            return MCPAddonResponse(
                addon_key="mcp-factcheck",
                category=MCPAddonCategory.FACTCHECK,
                success=False,
                error=str(e),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

    # 
    # Topic Analysis
    # 

    async def analyze_topic(
        self,
        keyword: Optional[str] = None,
        days: int = 7,
        include_report: bool = False,
    ) -> MCPAddonResponse:
        """  """
        start_time = datetime.utcnow()

        try:
            result = await self.topic_client.get_topic_raw(keyword, days)

            if not result.get("success"):
                return MCPAddonResponse(
                    addon_key="mcp-topic",
                    category=MCPAddonCategory.TOPIC,
                    success=False,
                    error=result.get("error", "Unknown error"),
                    latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
                )

            response = MCPAddonResponse(
                addon_key="mcp-topic",
                category=MCPAddonCategory.TOPIC,
                success=True,
                data=result.get("data"),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

            if include_report:
                report_result = await self.topic_client.get_topic_report(keyword, days)
                if report_result.get("success"):
                    response.report = report_result.get("data")

            return response

        except Exception as e:
            logger.error("Topic analysis failed", error=str(e))
            return MCPAddonResponse(
                addon_key="mcp-topic",
                category=MCPAddonCategory.TOPIC,
                success=False,
                error=str(e),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

    async def get_trending_topics(self, days: int = 1, limit: int = 10) -> MCPAddonResponse:
        """  """
        start_time = datetime.utcnow()

        try:
            result = await self.topic_client.get_trending_topics(days, limit)

            return MCPAddonResponse(
                addon_key="mcp-topic",
                category=MCPAddonCategory.TOPIC,
                success=result.get("success", False),
                data=result.get("data"),
                error=result.get("error"),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

        except Exception as e:
            logger.error("Get trending topics failed", error=str(e))
            return MCPAddonResponse(
                addon_key="mcp-topic",
                category=MCPAddonCategory.TOPIC,
                success=False,
                error=str(e),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

    # 
    # Sentiment Analysis
    # 

    async def analyze_sentiment(
        self,
        keyword: str,
        days: int = 7,
        include_report: bool = False,
    ) -> MCPAddonResponse:
        """  """
        start_time = datetime.utcnow()

        try:
            result = await self.newsinsight_client.get_sentiment_raw(keyword, days)

            if not result.get("success"):
                return MCPAddonResponse(
                    addon_key="mcp-sentiment",
                    category=MCPAddonCategory.SENTIMENT,
                    success=False,
                    error=result.get("error", "Unknown error"),
                    latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
                )

            response = MCPAddonResponse(
                addon_key="mcp-sentiment",
                category=MCPAddonCategory.SENTIMENT,
                success=True,
                data=result.get("data"),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

            if include_report:
                report_result = await self.newsinsight_client.get_sentiment_report(keyword, days)
                if report_result.get("success"):
                    response.report = report_result.get("data")

            return response

        except Exception as e:
            logger.error("Sentiment analysis failed", error=str(e))
            return MCPAddonResponse(
                addon_key="mcp-sentiment",
                category=MCPAddonCategory.SENTIMENT,
                success=False,
                error=str(e),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

    # 
    # HuggingFace NLP
    # 

    async def summarize_article(
        self,
        text: str,
        max_length: int = 150,
        min_length: int = 50,
    ) -> MCPAddonResponse:
        """ """
        start_time = datetime.utcnow()

        try:
            result = await self.huggingface_client.summarize_article(text, max_length, min_length)

            return MCPAddonResponse(
                addon_key="mcp-huggingface",
                category=MCPAddonCategory.SUMMARIZATION,
                success=result.get("success", False),
                data=result.get("data"),
                error=result.get("error"),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

        except Exception as e:
            logger.error("Summarization failed", error=str(e))
            return MCPAddonResponse(
                addon_key="mcp-huggingface",
                category=MCPAddonCategory.SUMMARIZATION,
                success=False,
                error=str(e),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

    async def extract_entities(self, text: str) -> MCPAddonResponse:
        """ """
        start_time = datetime.utcnow()

        try:
            result = await self.huggingface_client.extract_entities(text)

            return MCPAddonResponse(
                addon_key="mcp-huggingface",
                category=MCPAddonCategory.ENTITY,
                success=result.get("success", False),
                data=result.get("data"),
                error=result.get("error"),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )

        except Exception as e:
            logger.error("Entity extraction failed", error=str(e))
            return MCPAddonResponse(
                addon_key="mcp-huggingface",
                category=MCPAddonCategory.ENTITY,
                success=False,
                error=str(e),
                latency_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
            )


# 
#  
# 

_mcp_adapter: Optional[MCPAdapter] = None


def get_mcp_adapter() -> MCPAdapter:
    """MCP    """
    global _mcp_adapter
    if _mcp_adapter is None:
        _mcp_adapter = MCPAdapter()
    return _mcp_adapter

```

---

## backend/autonomous-crawler-service/src/mcp/client.py

```py
"""
MCP Client - MCP  JSON-RPC  

MCP  (bias, factcheck, topic, huggingface )
JSON-RPC  tool  .
"""

import asyncio
import json
from typing import Any, Dict, List, Optional
from datetime import datetime

import httpx
import structlog

logger = structlog.get_logger(__name__)


class MCPClient:
    """MCP  JSON-RPC """

    def __init__(
        self,
        base_url: str,
        timeout: float = 60.0,
        health_check_path: str = "/health",
    ):
        """
        Args:
            base_url: MCP   URL (: http://bias-mcp:5001)
            timeout: HTTP   ()
            health_check_path:   
        """
        self.base_url = base_url.rstrip("/")
        self.timeout = timeout
        self.health_check_path = health_check_path
        self._mcp_path = "/mcp"

    async def health_check(self) -> Dict[str, Any]:
        """ """
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                resp = await client.get(f"{self.base_url}{self.health_check_path}")
                if resp.status_code == 200:
                    return {"status": "healthy", "data": resp.json()}
                return {"status": "unhealthy", "status_code": resp.status_code}
            except Exception as e:
                return {"status": "error", "error": str(e)}

    async def call_tool(
        self,
        tool_name: str,
        arguments: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        MCP  tool .

        Args:
            tool_name:  tool  (: get_bias_raw)
            arguments: tool 

        Returns:
            tool  
        """
        # JSON-RPC 2.0   
        request_payload = {
            "jsonrpc": "2.0",
            "method": "tools/call",
            "params": {
                "name": tool_name,
                "arguments": arguments or {},
            },
            "id": int(datetime.utcnow().timestamp() * 1000),
        }

        async with httpx.AsyncClient(timeout=self.timeout) as client:
            try:
                logger.debug(
                    "Calling MCP tool",
                    url=f"{self.base_url}{self._mcp_path}",
                    tool=tool_name,
                    arguments=arguments,
                )

                resp = await client.post(
                    f"{self.base_url}{self._mcp_path}",
                    json=request_payload,
                    headers={"Content-Type": "application/json"},
                )

                if resp.status_code != 200:
                    error_text = resp.text
                    logger.error(
                        "MCP call failed",
                        tool=tool_name,
                        status_code=resp.status_code,
                        error=error_text,
                    )
                    return {
                        "success": False,
                        "error": f"HTTP {resp.status_code}: {error_text}",
                    }

                result = resp.json()

                # JSON-RPC  
                if "error" in result:
                    error = result["error"]
                    return {
                        "success": False,
                        "error": error.get("message", str(error)),
                        "code": error.get("code"),
                    }

                #  
                return {
                    "success": True,
                    "data": result.get("result"),
                }

            except httpx.TimeoutException:
                logger.error("MCP call timeout", tool=tool_name)
                return {"success": False, "error": "Request timeout"}
            except Exception as e:
                logger.error("MCP call exception", tool=tool_name, error=str(e))
                return {"success": False, "error": str(e)}

    async def list_tools(self) -> Dict[str, Any]:
        """MCP    tool  ."""
        request_payload = {
            "jsonrpc": "2.0",
            "method": "tools/list",
            "params": {},
            "id": int(datetime.utcnow().timestamp() * 1000),
        }

        async with httpx.AsyncClient(timeout=self.timeout) as client:
            try:
                resp = await client.post(
                    f"{self.base_url}{self._mcp_path}",
                    json=request_payload,
                    headers={"Content-Type": "application/json"},
                )

                if resp.status_code != 200:
                    return {"success": False, "error": f"HTTP {resp.status_code}"}

                result = resp.json()
                return {
                    "success": True,
                    "tools": result.get("result", {}).get("tools", []),
                }
            except Exception as e:
                return {"success": False, "error": str(e)}


# 
# MCP   
# 


class BiasMCPClient(MCPClient):
    """Bias Analysis MCP """

    async def get_bias_raw(self, keyword: str, days: int = 7) -> Dict[str, Any]:
        """     """
        return await self.call_tool("get_bias_raw", {"keyword": keyword, "days": days})

    async def get_bias_report(
        self, keyword: str, days: int = 7, session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """    """
        args = {"keyword": keyword, "days": days}
        if session_id:
            args["session_id"] = session_id
        return await self.call_tool("get_bias_report", args)

    async def get_source_bias_list(self) -> Dict[str, Any]:
        """    """
        return await self.call_tool("get_source_bias_list")


class FactcheckMCPClient(MCPClient):
    """Fact Check MCP """

    async def get_factcheck_raw(self, keyword: str, days: int = 7) -> Dict[str, Any]:
        """  /  """
        return await self.call_tool("get_factcheck_raw", {"keyword": keyword, "days": days})

    async def get_factcheck_report(
        self, keyword: str, days: int = 7, session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """    """
        args = {"keyword": keyword, "days": days}
        if session_id:
            args["session_id"] = session_id
        return await self.call_tool("get_factcheck_report", args)

    async def get_source_reliability_list(self) -> Dict[str, Any]:
        """    """
        return await self.call_tool("get_source_reliability_list")


class TopicMCPClient(MCPClient):
    """Topic Analysis MCP """

    async def get_topic_raw(self, keyword: Optional[str] = None, days: int = 7) -> Dict[str, Any]:
        """   """
        return await self.call_tool("get_topic_raw", {"keyword": keyword, "days": days})

    async def get_topic_report(
        self,
        keyword: Optional[str] = None,
        days: int = 7,
        session_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """    """
        args = {"keyword": keyword, "days": days}
        if session_id:
            args["session_id"] = session_id
        return await self.call_tool("get_topic_report", args)

    async def get_trending_topics(self, days: int = 1, limit: int = 10) -> Dict[str, Any]:
        """   """
        return await self.call_tool("get_trending_topics", {"days": days, "limit": limit})

    async def get_category_list(self) -> Dict[str, Any]:
        """  """
        return await self.call_tool("get_category_list")


class HuggingFaceMCPClient(MCPClient):
    """Hugging Face MCP """

    async def analyze_sentiment(self, text: str, model_id: Optional[str] = None) -> Dict[str, Any]:
        """ """
        args = {"text": text}
        if model_id:
            args["model_id"] = model_id
        return await self.call_tool("analyze_sentiment", args)

    async def summarize_article(
        self, text: str, max_length: int = 150, min_length: int = 50
    ) -> Dict[str, Any]:
        """ """
        return await self.call_tool(
            "summarize_article",
            {"text": text, "max_length": max_length, "min_length": min_length},
        )

    async def extract_entities(self, text: str) -> Dict[str, Any]:
        """ """
        return await self.call_tool("extract_entities", {"text": text})

    async def extract_keywords(self, text: str, top_k: int = 10) -> Dict[str, Any]:
        """ """
        return await self.call_tool("extract_keywords", {"text": text, "top_k": top_k})

    async def classify_news(
        self, text: str, categories: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """ """
        args: Dict[str, Any] = {"text": text}
        if categories:
            args["categories"] = categories
        return await self.call_tool("classify_news", args)


class NewsInsightMCPClient(MCPClient):
    """NewsInsight MCP """

    async def get_sentiment_raw(self, keyword: str, days: int = 7) -> Dict[str, Any]:
        """   """
        return await self.call_tool("get_sentiment_raw", {"keyword": keyword, "days": days})

    async def get_sentiment_report(
        self, keyword: str, days: int = 7, session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """    """
        args = {"keyword": keyword, "days": days}
        if session_id:
            args["session_id"] = session_id
        return await self.call_tool("get_sentiment_report", args)

    async def get_article_list(
        self, keyword: str, days: int = 7, limit: int = 50
    ) -> Dict[str, Any]:
        """  """
        return await self.call_tool(
            "get_article_list", {"keyword": keyword, "days": days, "limit": limit}
        )

    async def get_discussion_summary(self, keyword: str, days: int = 7) -> Dict[str, Any]:
        """  """
        return await self.call_tool("get_discussion_summary", {"keyword": keyword, "days": days})

```

---

## backend/autonomous-crawler-service/src/mcp/router.py

```py
"""
MCP API Router - MCP Add-on REST API 

MCP   MCP   REST API .
"""

from typing import Any, Dict, List, Optional

import structlog
from fastapi import APIRouter, HTTPException, Query
from pydantic import BaseModel, Field

from .adapter import MCPAdapter, MCPAddonResponse, MCPAddonInfo, get_mcp_adapter

logger = structlog.get_logger(__name__)

router = APIRouter(prefix="/mcp", tags=["MCP Add-ons"])


# 
# Request Models
# 


class KeywordAnalysisRequest(BaseModel):
    """   """

    keyword: str = Field(..., description=" ", min_length=1, max_length=100)
    days: int = Field(default=7, ge=1, le=90, description="  ()")
    include_report: bool = Field(default=False, description="   ")


class TextAnalysisRequest(BaseModel):
    """   """

    text: str = Field(..., description=" ", min_length=10, max_length=50000)
    max_length: int = Field(default=150, ge=50, le=500, description="  ")
    min_length: int = Field(default=50, ge=20, le=200, description="  ")


# 
# Add-on  
# 


@router.get("/addons", response_model=List[MCPAddonInfo])
async def list_mcp_addons():
    """
     MCP Add-on  .
    """
    adapter = get_mcp_adapter()
    return await adapter.list_addons()


@router.get("/health")
async def check_mcp_health():
    """
     MCP    .
    """
    adapter = get_mcp_adapter()
    results = await adapter.check_all_health()

    #   
    healthy_count = sum(1 for r in results.values() if r.get("status") == "healthy")
    total_count = len(results)

    return {
        "status": "healthy" if healthy_count == total_count else "degraded",
        "healthy": healthy_count,
        "total": total_count,
        "servers": results,
    }


# 
# Bias Analysis 
# 


@router.post("/bias/analyze", response_model=MCPAddonResponse)
async def analyze_bias(request: KeywordAnalysisRequest):
    """
        .

    - /   
    -   
    -  
    """
    adapter = get_mcp_adapter()
    return await adapter.analyze_bias(
        keyword=request.keyword,
        days=request.days,
        include_report=request.include_report,
    )


@router.get("/bias/sources")
async def get_source_bias_list():
    """
          .
    """
    adapter = get_mcp_adapter()
    result = await adapter.bias_client.get_source_bias_list()
    if not result.get("success"):
        raise HTTPException(status_code=500, detail=result.get("error"))
    return result.get("data")


# 
# Factcheck Analysis 
# 


@router.post("/factcheck/analyze", response_model=MCPAddonResponse)
async def analyze_factcheck(request: KeywordAnalysisRequest):
    """
           .

    -   
    -  
    - / 
    -   
    """
    adapter = get_mcp_adapter()
    return await adapter.analyze_factcheck(
        keyword=request.keyword,
        days=request.days,
        include_report=request.include_report,
    )


@router.get("/factcheck/sources")
async def get_source_reliability_list():
    """
         .
    """
    adapter = get_mcp_adapter()
    result = await adapter.factcheck_client.get_source_reliability_list()
    if not result.get("success"):
        raise HTTPException(status_code=500, detail=result.get("error"))
    return result.get("data")


# 
# Topic Analysis 
# 


@router.post("/topics/analyze", response_model=MCPAddonResponse)
async def analyze_topic(request: KeywordAnalysisRequest):
    """
     ( )   .

    -  / 
    -  
    -  
    -  
    """
    adapter = get_mcp_adapter()
    return await adapter.analyze_topic(
        keyword=request.keyword,
        days=request.days,
        include_report=request.include_report,
    )


@router.get("/topics/trending", response_model=MCPAddonResponse)
async def get_trending_topics(
    days: int = Query(default=1, ge=1, le=7, description="  ()"),
    limit: int = Query(default=10, ge=1, le=50, description="  "),
):
    """
     N    .

      API.
    """
    adapter = get_mcp_adapter()
    return await adapter.get_trending_topics(days=days, limit=limit)


@router.get("/topics/categories")
async def get_category_list():
    """
        .
    """
    adapter = get_mcp_adapter()
    result = await adapter.topic_client.get_category_list()
    if not result.get("success"):
        raise HTTPException(status_code=500, detail=result.get("error"))
    return result.get("data")


# 
# Sentiment Analysis 
# 


@router.post("/sentiment/analyze", response_model=MCPAddonResponse)
async def analyze_sentiment(request: KeywordAnalysisRequest):
    """
        .

    - // 
    -  
    -   
    """
    adapter = get_mcp_adapter()
    return await adapter.analyze_sentiment(
        keyword=request.keyword,
        days=request.days,
        include_report=request.include_report,
    )


# 
# HuggingFace NLP 
# 


@router.post("/nlp/summarize", response_model=MCPAddonResponse)
async def summarize_article(request: TextAnalysisRequest):
    """
     .

    HuggingFace   abstractive summarization.
    """
    adapter = get_mcp_adapter()
    return await adapter.summarize_article(
        text=request.text,
        max_length=request.max_length,
        min_length=request.min_length,
    )


@router.post("/nlp/entities", response_model=MCPAddonResponse)
async def extract_entities(text: str = Query(..., description=" ", min_length=10)):
    """
     (, ,  ) .
    """
    adapter = get_mcp_adapter()
    return await adapter.extract_entities(text)


# 
#   
# 


@router.post("/analyze/comprehensive")
async def comprehensive_analysis(request: KeywordAnalysisRequest):
    """
        .

    , , ,      .
    """
    adapter = get_mcp_adapter()

    #    
    import asyncio

    results = await asyncio.gather(
        adapter.analyze_bias(request.keyword, request.days),
        adapter.analyze_factcheck(request.keyword, request.days),
        adapter.analyze_topic(request.keyword, request.days),
        adapter.analyze_sentiment(request.keyword, request.days),
        return_exceptions=True,
    )

    #  
    analysis_results = {
        "keyword": request.keyword,
        "days": request.days,
        "bias": (
            results[0].model_dump()
            if not isinstance(results[0], Exception)
            else {"error": str(results[0])}
        ),
        "factcheck": (
            results[1].model_dump()
            if not isinstance(results[1], Exception)
            else {"error": str(results[1])}
        ),
        "topic": (
            results[2].model_dump()
            if not isinstance(results[2], Exception)
            else {"error": str(results[2])}
        ),
        "sentiment": (
            results[3].model_dump()
            if not isinstance(results[3], Exception)
            else {"error": str(results[3])}
        ),
    }

    #  
    success_count = sum(1 for r in results if not isinstance(r, Exception) and r.success)

    return {
        "success": success_count > 0,
        "success_rate": success_count / 4,
        "results": analysis_results,
    }

```

---

## backend/autonomous-crawler-service/src/metrics/__init__.py

```py
"""Prometheus metrics for autonomous-crawler-service."""

from prometheus_client import Counter, Gauge, Histogram, Info

# Service info
SERVICE_INFO = Info("crawler_service", "Autonomous crawler service information")

# ========================================
# Task metrics
# ========================================

TASKS_RECEIVED = Counter(
    "crawler_tasks_received_total",
    "Total number of crawl tasks received from Kafka",
    ["policy"],
)

TASKS_COMPLETED = Counter(
    "crawler_tasks_completed_total",
    "Total number of crawl tasks completed",
    ["policy", "status"],
)

TASKS_IN_PROGRESS = Gauge(
    "crawler_tasks_in_progress",
    "Number of crawl tasks currently in progress",
)

# API-based task metrics (browser-agent compatibility)
API_CRAWL_TASKS = Counter(
    "crawler_api_tasks_total",
    "Total API-based crawl tasks",
    ["status", "llm_provider"],
)

# ========================================
# Article extraction metrics
# ========================================

ARTICLES_EXTRACTED = Counter(
    "crawler_articles_extracted_total",
    "Total number of articles extracted",
    ["source_id"],
)

PAGES_VISITED = Counter(
    "crawler_pages_visited_total",
    "Total number of pages visited",
    ["domain"],
)

URLS_DISCOVERED = Counter(
    "crawler_urls_discovered_total",
    "Total URLs discovered by agent",
    ["category"],
)

# ========================================
# Performance metrics
# ========================================

TASK_DURATION = Histogram(
    "crawler_task_duration_seconds",
    "Time spent processing a crawl task",
    ["policy"],
    buckets=[10, 30, 60, 120, 300, 600],
)

API_CRAWL_DURATION = Histogram(
    "crawler_api_crawl_duration_seconds",
    "API crawl task duration",
    ["status"],
    buckets=[5.0, 10.0, 30.0, 60.0, 120.0, 300.0, 600.0],
)

EXTRACTION_DURATION = Histogram(
    "crawler_extraction_duration_seconds",
    "Time spent extracting content from a single page",
    buckets=[1, 2, 5, 10, 30],
)

AGENT_STEPS = Histogram(
    "crawler_agent_steps",
    "Number of steps per agent task",
    ["status"],
    buckets=[1, 2, 5, 10, 20, 30, 50],
)

# ========================================
# Browser metrics
# ========================================

BROWSER_SESSIONS_ACTIVE = Gauge(
    "crawler_browser_sessions_active",
    "Number of active browser sessions",
)

BROWSER_ERRORS = Counter(
    "crawler_browser_errors_total",
    "Total number of browser errors",
    ["error_type"],
)

# ========================================
# CAPTCHA metrics
# ========================================

CAPTCHA_DETECTED = Counter(
    "crawler_captcha_detected_total",
    "Total number of CAPTCHAs detected",
    ["type"],
)

CAPTCHA_SOLVED = Counter(
    "crawler_captcha_solved_total",
    "Total number of CAPTCHAs successfully solved",
    ["type", "method"],
)

CAPTCHA_FAILED = Counter(
    "crawler_captcha_failed_total",
    "Total number of CAPTCHA solve failures",
    ["type", "reason"],
)

# ========================================
# Kafka metrics
# ========================================

KAFKA_MESSAGES_CONSUMED = Counter(
    "crawler_kafka_messages_consumed_total",
    "Total number of Kafka messages consumed",
    ["topic"],
)

KAFKA_MESSAGES_PRODUCED = Counter(
    "crawler_kafka_messages_produced_total",
    "Total number of Kafka messages produced",
    ["topic"],
)

KAFKA_CONSUMER_LAG = Gauge(
    "crawler_kafka_consumer_lag",
    "Kafka consumer lag (messages behind)",
    ["topic", "partition"],
)

# ========================================
# SSE metrics
# ========================================

SSE_CLIENTS_CONNECTED = Gauge(
    "crawler_sse_clients_connected",
    "Number of connected SSE clients",
)

SSE_EVENTS_SENT = Counter(
    "crawler_sse_events_sent_total",
    "Total number of SSE events sent",
    ["event_type"],
)

# ========================================
# Chat metrics
# ========================================

CHAT_REQUESTS = Counter(
    "crawler_chat_requests_total",
    "Total chat requests",
    ["provider", "streaming"],
)

CHAT_TOKENS_USED = Counter(
    "crawler_chat_tokens_used_total",
    "Total tokens used in chat requests",
    ["provider"],
)


def init_service_info(version: str = "0.1.0", llm_provider: str = "openai") -> None:
    """Initialize service info metrics."""
    SERVICE_INFO.info(
        {
            "version": version,
            "llm_provider": llm_provider,
        }
    )


def track_crawl_task(status: str, llm_provider: str, duration_seconds: float, steps: int = 0):
    """Track an API crawl task completion."""
    API_CRAWL_TASKS.labels(status=status, llm_provider=llm_provider).inc()
    API_CRAWL_DURATION.labels(status=status).observe(duration_seconds)
    if steps > 0:
        AGENT_STEPS.labels(status=status).observe(steps)


def track_url_discovery(category: str, count: int = 1):
    """Track URL discovery."""
    URLS_DISCOVERED.labels(category=category).inc(count)


def track_captcha(captcha_type: str, solved: bool, method: str = "unknown", reason: str = ""):
    """Track CAPTCHA detection and resolution."""
    CAPTCHA_DETECTED.labels(type=captcha_type).inc()
    if solved:
        CAPTCHA_SOLVED.labels(type=captcha_type, method=method).inc()
    else:
        CAPTCHA_FAILED.labels(type=captcha_type, reason=reason).inc()


def track_sse_event(event_type: str):
    """Track SSE event sent."""
    SSE_EVENTS_SENT.labels(event_type=event_type).inc()


def track_chat_request(provider: str, streaming: bool, tokens: int = 0):
    """Track chat request."""
    CHAT_REQUESTS.labels(provider=provider, streaming=str(streaming).lower()).inc()
    if tokens > 0:
        CHAT_TOKENS_USED.labels(provider=provider).inc(tokens)

```

---

## backend/autonomous-crawler-service/src/ml/__init__.py

```py
"""
ML Addon Integration for autonomous-crawler-service.

    ML    .
"""

from .orchestrator import (
    MLOrchestrator,
    MLAddonType,
    MLAddonConfig,
    MLAnalysisResult,
    BatchAnalysisResult,
    get_ml_orchestrator,
    init_ml_orchestrator,
)

__all__ = [
    "MLOrchestrator",
    "MLAddonType",
    "MLAddonConfig",
    "MLAnalysisResult",
    "BatchAnalysisResult",
    "get_ml_orchestrator",
    "init_ml_orchestrator",
]

```

---

## backend/autonomous-crawler-service/src/ml/orchestrator.py

```py
"""
ML Addon Orchestrator for autonomous-crawler-service.

    ML    .
Sentiment, Factcheck, Bias     DB .

Features:
-  HTTP   ML  
-    (asyncio.gather)
-  DB  (article_analysis )
-     
-      
"""

import os
import asyncio
import uuid
from typing import Any, Dict, List, Optional, Tuple
from datetime import datetime
from enum import Enum

import structlog
import httpx
from pydantic import BaseModel, Field

logger = structlog.get_logger(__name__)


# 
# Configuration
# 


class MLAddonConfig:
    """ML Addon   """

    SENTIMENT_ADDON_URL = os.environ.get("SENTIMENT_ADDON_URL", "http://sentiment-addon:8100")
    FACTCHECK_ADDON_URL = os.environ.get("FACTCHECK_ADDON_URL", "http://factcheck-addon:8101")
    BIAS_ADDON_URL = os.environ.get("BIAS_ADDON_URL", "http://bias-addon:8102")

    # HTTP client settings
    TIMEOUT_SECONDS = int(os.environ.get("ML_ADDON_TIMEOUT", "60"))
    MAX_RETRIES = int(os.environ.get("ML_ADDON_MAX_RETRIES", "2"))
    RETRY_DELAY_SECONDS = float(os.environ.get("ML_ADDON_RETRY_DELAY", "1.0"))

    # Feature flags
    AUTO_ANALYSIS_ENABLED = os.environ.get("ML_AUTO_ANALYSIS_ENABLED", "true").lower() == "true"
    PARALLEL_ANALYSIS = os.environ.get("ML_PARALLEL_ANALYSIS", "true").lower() == "true"


# 
# Enums and Models
# 


class MLAddonType(str, Enum):
    """ML Addon """

    SENTIMENT = "sentiment"
    FACTCHECK = "factcheck"
    BIAS = "bias"


class AddonHealthStatus(str, Enum):
    """Addon """

    HEALTHY = "healthy"
    UNHEALTHY = "unhealthy"
    UNKNOWN = "unknown"
    WARMING_UP = "warming_up"


class ArticleInput(BaseModel):
    """  """

    id: Optional[int] = None
    title: Optional[str] = None
    content: Optional[str] = None
    url: Optional[str] = None
    source: Optional[str] = None
    published_at: Optional[str] = None


class MLAddonRequest(BaseModel):
    """ML Addon  """

    request_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    addon_id: str
    task: str = "article_analysis"
    input_schema_version: str = "1.0"
    article: ArticleInput
    context: Optional[Dict[str, Any]] = None
    options: Optional[Dict[str, Any]] = None


class MLAnalysisResult(BaseModel):
    """ML  """

    addon_type: MLAddonType
    success: bool
    request_id: str
    results: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    latency_ms: int = 0
    analyzed_at: str = Field(default_factory=lambda: datetime.utcnow().isoformat())


class BatchAnalysisResult(BaseModel):
    """  """

    article_id: int
    sentiment: Optional[MLAnalysisResult] = None
    factcheck: Optional[MLAnalysisResult] = None
    bias: Optional[MLAnalysisResult] = None
    total_latency_ms: int = 0
    success_count: int = 0
    failure_count: int = 0


# 
# ML Addon Client
# 


class MLAddonClient:
    """ML Addon HTTP """

    def __init__(
        self,
        addon_type: MLAddonType,
        base_url: str,
        timeout: float = MLAddonConfig.TIMEOUT_SECONDS,
    ):
        self.addon_type = addon_type
        self.base_url = base_url
        self.timeout = timeout
        self._client: Optional[httpx.AsyncClient] = None
        self._health_status = AddonHealthStatus.UNKNOWN

    async def _get_client(self) -> httpx.AsyncClient:
        """HTTP    (lazy initialization)"""
        if self._client is None or self._client.is_closed:
            self._client = httpx.AsyncClient(
                base_url=self.base_url,
                timeout=httpx.Timeout(self.timeout),
                limits=httpx.Limits(max_connections=10, max_keepalive_connections=5),
            )
        return self._client

    async def close(self):
        """  """
        if self._client and not self._client.is_closed:
            await self._client.aclose()
            self._client = None

    async def health_check(self) -> Dict[str, Any]:
        """ """
        try:
            client = await self._get_client()
            response = await client.get("/health")

            if response.status_code == 200:
                data = response.json()
                status = data.get("status", "unknown")

                if status == "healthy":
                    self._health_status = AddonHealthStatus.HEALTHY
                elif data.get("warmup_complete") is False:
                    self._health_status = AddonHealthStatus.WARMING_UP
                else:
                    self._health_status = AddonHealthStatus.HEALTHY

                return {
                    "addon_type": self.addon_type.value,
                    "status": self._health_status.value,
                    "details": data,
                }
            else:
                self._health_status = AddonHealthStatus.UNHEALTHY
                return {
                    "addon_type": self.addon_type.value,
                    "status": AddonHealthStatus.UNHEALTHY.value,
                    "error": f"HTTP {response.status_code}",
                }

        except Exception as e:
            self._health_status = AddonHealthStatus.UNHEALTHY
            logger.warning(
                f"Health check failed for {self.addon_type.value}",
                error=str(e),
                url=self.base_url,
            )
            return {
                "addon_type": self.addon_type.value,
                "status": AddonHealthStatus.UNHEALTHY.value,
                "error": str(e),
            }

    async def analyze(
        self,
        article: ArticleInput,
        options: Optional[Dict[str, Any]] = None,
        retries: int = MLAddonConfig.MAX_RETRIES,
    ) -> MLAnalysisResult:
        """  """
        start_time = datetime.utcnow()
        request_id = str(uuid.uuid4())

        request_data = MLAddonRequest(
            request_id=request_id,
            addon_id=f"{self.addon_type.value}-addon",
            article=article,
            options=options or {},
        )

        for attempt in range(retries + 1):
            try:
                client = await self._get_client()
                response = await client.post(
                    "/analyze",
                    json=request_data.model_dump(),
                )

                latency_ms = int((datetime.utcnow() - start_time).total_seconds() * 1000)

                if response.status_code == 200:
                    data = response.json()
                    return MLAnalysisResult(
                        addon_type=self.addon_type,
                        success=data.get("status") == "success",
                        request_id=request_id,
                        results=data.get("results"),
                        error=data.get("error", {}).get("message") if data.get("error") else None,
                        latency_ms=latency_ms,
                    )
                else:
                    error_msg = f"HTTP {response.status_code}: {response.text[:200]}"
                    if attempt < retries:
                        logger.warning(
                            f"Retry {attempt + 1}/{retries} for {self.addon_type.value}",
                            error=error_msg,
                        )
                        await asyncio.sleep(MLAddonConfig.RETRY_DELAY_SECONDS)
                        continue

                    return MLAnalysisResult(
                        addon_type=self.addon_type,
                        success=False,
                        request_id=request_id,
                        error=error_msg,
                        latency_ms=latency_ms,
                    )

            except httpx.TimeoutException as e:
                latency_ms = int((datetime.utcnow() - start_time).total_seconds() * 1000)
                if attempt < retries:
                    logger.warning(
                        f"Timeout retry {attempt + 1}/{retries} for {self.addon_type.value}",
                        error=str(e),
                    )
                    await asyncio.sleep(MLAddonConfig.RETRY_DELAY_SECONDS)
                    continue

                return MLAnalysisResult(
                    addon_type=self.addon_type,
                    success=False,
                    request_id=request_id,
                    error=f"Timeout after {self.timeout}s",
                    latency_ms=latency_ms,
                )

            except Exception as e:
                latency_ms = int((datetime.utcnow() - start_time).total_seconds() * 1000)
                logger.error(
                    f"Analysis failed for {self.addon_type.value}",
                    error=str(e),
                    attempt=attempt + 1,
                )

                if attempt < retries:
                    await asyncio.sleep(MLAddonConfig.RETRY_DELAY_SECONDS)
                    continue

                return MLAnalysisResult(
                    addon_type=self.addon_type,
                    success=False,
                    request_id=request_id,
                    error=str(e),
                    latency_ms=latency_ms,
                )

        # Should not reach here
        return MLAnalysisResult(
            addon_type=self.addon_type,
            success=False,
            request_id=request_id,
            error="Max retries exceeded",
            latency_ms=0,
        )

    @property
    def is_healthy(self) -> bool:
        """Addon   """
        return self._health_status in [
            AddonHealthStatus.HEALTHY,
            AddonHealthStatus.WARMING_UP,
        ]


# 
# ML Orchestrator
# 


class MLOrchestrator:
    """
    ML Addon .

        ML  
     DB .
    """

    def __init__(self, db_pool=None):
        """
        Args:
            db_pool: PostgreSQL   (asyncpg)
        """
        self.db_pool = db_pool

        # Initialize addon clients
        self.sentiment_client = MLAddonClient(
            MLAddonType.SENTIMENT, MLAddonConfig.SENTIMENT_ADDON_URL
        )
        self.factcheck_client = MLAddonClient(
            MLAddonType.FACTCHECK, MLAddonConfig.FACTCHECK_ADDON_URL
        )
        self.bias_client = MLAddonClient(MLAddonType.BIAS, MLAddonConfig.BIAS_ADDON_URL)

        self._clients = {
            MLAddonType.SENTIMENT: self.sentiment_client,
            MLAddonType.FACTCHECK: self.factcheck_client,
            MLAddonType.BIAS: self.bias_client,
        }

        self._initialized = False

    async def initialize(self):
        """   """
        if self._initialized:
            return

        logger.info("Initializing ML Orchestrator...")

        # Perform health checks
        health_results = await self.check_all_health()

        healthy_count = sum(
            1 for r in health_results.values() if r.get("status") in ["healthy", "warming_up"]
        )

        logger.info(
            f"ML Orchestrator initialized",
            healthy_addons=healthy_count,
            total_addons=len(self._clients),
            auto_analysis_enabled=MLAddonConfig.AUTO_ANALYSIS_ENABLED,
        )

        self._initialized = True

    async def close(self):
        """   """
        for client in self._clients.values():
            await client.close()

    async def check_all_health(self) -> Dict[str, Dict[str, Any]]:
        """ ML Addon """
        results = {}

        if MLAddonConfig.PARALLEL_ANALYSIS:
            health_tasks = [
                (addon_type, client.health_check()) for addon_type, client in self._clients.items()
            ]
            health_results = await asyncio.gather(
                *[task[1] for task in health_tasks], return_exceptions=True
            )

            for i, (addon_type, _) in enumerate(health_tasks):
                result = health_results[i]
                if isinstance(result, Exception):
                    results[addon_type.value] = {
                        "status": "error",
                        "error": str(result),
                    }
                else:
                    results[addon_type.value] = result
        else:
            for addon_type, client in self._clients.items():
                try:
                    results[addon_type.value] = await client.health_check()
                except Exception as e:
                    results[addon_type.value] = {
                        "status": "error",
                        "error": str(e),
                    }

        return results

    async def analyze_article(
        self,
        article_id: int,
        title: str,
        content: str,
        source: Optional[str] = None,
        url: Optional[str] = None,
        published_at: Optional[str] = None,
        addon_types: Optional[List[MLAddonType]] = None,
        save_to_db: bool = True,
    ) -> BatchAnalysisResult:
        """
          ML  .

        Args:
            article_id:  ID
            title:  
            content:  
            source: 
            url:  URL
            published_at: 
            addon_types:     (None  )
            save_to_db:  DB  

        Returns:
            BatchAnalysisResult:  
        """
        if not MLAddonConfig.AUTO_ANALYSIS_ENABLED:
            logger.debug("ML auto-analysis is disabled")
            return BatchAnalysisResult(
                article_id=article_id,
                success_count=0,
                failure_count=0,
            )

        start_time = datetime.utcnow()

        # Prepare article input
        article = ArticleInput(
            id=article_id,
            title=title,
            content=content,
            source=source,
            url=url,
            published_at=published_at,
        )

        # Determine which addons to run
        if addon_types is None:
            addon_types = list(MLAddonType)

        # Filter to only healthy addons
        active_clients = {
            addon_type: self._clients[addon_type]
            for addon_type in addon_types
            if addon_type in self._clients
        }

        if not active_clients:
            logger.warning("No ML addons available for analysis")
            return BatchAnalysisResult(
                article_id=article_id,
                success_count=0,
                failure_count=len(addon_types),
            )

        # Execute analysis
        results: Dict[MLAddonType, MLAnalysisResult] = {}

        if MLAddonConfig.PARALLEL_ANALYSIS:
            # Parallel execution
            analysis_tasks = [
                (addon_type, client.analyze(article))
                for addon_type, client in active_clients.items()
            ]
            analysis_results = await asyncio.gather(
                *[task[1] for task in analysis_tasks], return_exceptions=True
            )

            for i, (addon_type, _) in enumerate(analysis_tasks):
                result = analysis_results[i]
                if isinstance(result, Exception):
                    results[addon_type] = MLAnalysisResult(
                        addon_type=addon_type,
                        success=False,
                        request_id="",
                        error=str(result),
                    )
                elif isinstance(result, MLAnalysisResult):
                    results[addon_type] = result
        else:
            # Sequential execution
            for addon_type, client in active_clients.items():
                try:
                    results[addon_type] = await client.analyze(article)
                except Exception as e:
                    results[addon_type] = MLAnalysisResult(
                        addon_type=addon_type,
                        success=False,
                        request_id="",
                        error=str(e),
                    )

        # Calculate totals
        total_latency = int((datetime.utcnow() - start_time).total_seconds() * 1000)
        success_count = sum(1 for r in results.values() if r.success)
        failure_count = len(results) - success_count

        batch_result = BatchAnalysisResult(
            article_id=article_id,
            sentiment=results.get(MLAddonType.SENTIMENT),
            factcheck=results.get(MLAddonType.FACTCHECK),
            bias=results.get(MLAddonType.BIAS),
            total_latency_ms=total_latency,
            success_count=success_count,
            failure_count=failure_count,
        )

        # Save to database
        if save_to_db and self.db_pool:
            await self._save_results_to_db(article_id, results)

        logger.info(
            f"ML analysis completed for article {article_id}",
            success=success_count,
            failure=failure_count,
            latency_ms=total_latency,
        )

        return batch_result

    async def analyze_batch(
        self,
        articles: List[Dict[str, Any]],
        addon_types: Optional[List[MLAddonType]] = None,
        save_to_db: bool = True,
        max_concurrent: int = 5,
    ) -> List[BatchAnalysisResult]:
        """
             .

        Args:
            articles:   (dict with id, title, content, source, url)
            addon_types:   
            save_to_db: DB  
            max_concurrent:    

        Returns:
            List[BatchAnalysisResult]:   
        """
        if not articles:
            return []

        semaphore = asyncio.Semaphore(max_concurrent)

        async def analyze_with_semaphore(article: Dict[str, Any]) -> BatchAnalysisResult:
            async with semaphore:
                return await self.analyze_article(
                    article_id=article.get("id", 0),
                    title=article.get("title", ""),
                    content=article.get("content", ""),
                    source=article.get("source"),
                    url=article.get("url"),
                    published_at=article.get("published_at"),
                    addon_types=addon_types,
                    save_to_db=save_to_db,
                )

        results = await asyncio.gather(
            *[analyze_with_semaphore(a) for a in articles], return_exceptions=True
        )

        # Filter out exceptions
        valid_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(
                    f"Batch analysis failed for article",
                    article_id=articles[i].get("id"),
                    error=str(result),
                )
                valid_results.append(
                    BatchAnalysisResult(
                        article_id=articles[i].get("id", 0),
                        success_count=0,
                        failure_count=3,
                    )
                )
            else:
                valid_results.append(result)

        return valid_results

    async def _save_results_to_db(
        self,
        article_id: int,
        results: Dict[MLAddonType, MLAnalysisResult],
    ):
        """  DB """
        if not self.db_pool:
            logger.warning("No database pool configured, skipping DB save")
            return

        try:
            async with self.db_pool.acquire() as conn:
                for addon_type, result in results.items():
                    if not result.success:
                        continue

                    # Extract result data based on addon type
                    result_data = result.results or {}

                    if addon_type == MLAddonType.SENTIMENT:
                        sentiment_data = result_data.get("sentiment", {})
                        await conn.execute(
                            """
                            INSERT INTO article_analysis 
                                (article_id, addon_key, analysis_type, result_json, 
                                 score, confidence, created_at)
                            VALUES ($1, $2, $3, $4, $5, $6, NOW())
                            ON CONFLICT (article_id, addon_key) 
                            DO UPDATE SET 
                                result_json = EXCLUDED.result_json,
                                score = EXCLUDED.score,
                                confidence = EXCLUDED.confidence,
                                updated_at = NOW()
                            """,
                            article_id,
                            "sentiment-addon",
                            "sentiment",
                            result_data,
                            sentiment_data.get("score", 0),
                            sentiment_data.get("confidence", 0),
                        )

                    elif addon_type == MLAddonType.FACTCHECK:
                        factcheck_data = result_data.get("factcheck", {})
                        await conn.execute(
                            """
                            INSERT INTO article_analysis 
                                (article_id, addon_key, analysis_type, result_json, 
                                 score, confidence, created_at)
                            VALUES ($1, $2, $3, $4, $5, $6, NOW())
                            ON CONFLICT (article_id, addon_key) 
                            DO UPDATE SET 
                                result_json = EXCLUDED.result_json,
                                score = EXCLUDED.score,
                                confidence = EXCLUDED.confidence,
                                updated_at = NOW()
                            """,
                            article_id,
                            "factcheck-addon",
                            "factcheck",
                            result_data,
                            factcheck_data.get("overall_credibility", 0) / 100,
                            0.7,  # Default confidence for factcheck
                        )

                    elif addon_type == MLAddonType.BIAS:
                        bias_data = result_data.get("bias", {})
                        await conn.execute(
                            """
                            INSERT INTO article_analysis 
                                (article_id, addon_key, analysis_type, result_json, 
                                 score, confidence, created_at)
                            VALUES ($1, $2, $3, $4, $5, $6, NOW())
                            ON CONFLICT (article_id, addon_key) 
                            DO UPDATE SET 
                                result_json = EXCLUDED.result_json,
                                score = EXCLUDED.score,
                                confidence = EXCLUDED.confidence,
                                updated_at = NOW()
                            """,
                            article_id,
                            "bias-addon",
                            "bias",
                            result_data,
                            bias_data.get("overall_bias_score", 0),
                            bias_data.get("confidence", 0),
                        )

                logger.debug(f"Saved ML analysis results to DB for article {article_id}")

        except Exception as e:
            logger.error(
                f"Failed to save ML results to DB",
                article_id=article_id,
                error=str(e),
            )

    def get_addon_status(self) -> Dict[str, Any]:
        """ Addon  """
        return {
            "auto_analysis_enabled": MLAddonConfig.AUTO_ANALYSIS_ENABLED,
            "parallel_analysis": MLAddonConfig.PARALLEL_ANALYSIS,
            "addons": {
                addon_type.value: {
                    "url": client.base_url,
                    "healthy": client.is_healthy,
                    "status": client._health_status.value,
                }
                for addon_type, client in self._clients.items()
            },
        }


# 
# Singleton Instance
# 

_ml_orchestrator: Optional[MLOrchestrator] = None


def get_ml_orchestrator(db_pool=None) -> MLOrchestrator:
    """ML    """
    global _ml_orchestrator
    if _ml_orchestrator is None:
        _ml_orchestrator = MLOrchestrator(db_pool=db_pool)
    elif db_pool is not None and _ml_orchestrator.db_pool is None:
        _ml_orchestrator.db_pool = db_pool
    return _ml_orchestrator


async def init_ml_orchestrator(db_pool=None) -> MLOrchestrator:
    """ML    """
    orchestrator = get_ml_orchestrator(db_pool)
    await orchestrator.initialize()
    return orchestrator

```

---

## backend/autonomous-crawler-service/src/ml/router.py

```py
"""
ML Analysis API Router for autonomous-crawler-service.

ML Addon   REST API .
   sentiment, factcheck, bias  .
"""

import os
from typing import Any, Dict, List, Optional

import structlog
from fastapi import APIRouter, HTTPException, Query, Request
from pydantic import BaseModel, Field, HttpUrl

from src.ml.orchestrator import (
    MLOrchestrator,
    MLAddonType,
    MLAddonConfig,
    MLAnalysisResult,
    BatchAnalysisResult,
    ArticleInput,
    get_ml_orchestrator,
    init_ml_orchestrator,
)

logger = structlog.get_logger(__name__)

router = APIRouter(prefix="/ml", tags=["ML Analysis"])


# 
# Request/Response Models
# 


class MLAnalyzeRequest(BaseModel):
    """ML  """

    article_id: int = Field(..., description=" ID")
    title: str = Field(..., description=" ", min_length=1)
    content: str = Field(..., description=" ", min_length=10)
    source: Optional[str] = Field(default=None, description="")
    url: Optional[str] = Field(default=None, description=" URL")
    published_at: Optional[str] = Field(default=None, description="")

    #  
    addons: Optional[List[str]] = Field(
        default=None,
        description="   (sentiment, factcheck, bias). None  ",
    )
    save_to_db: bool = Field(default=True, description=" DB  ")


class MLBatchAnalyzeRequest(BaseModel):
    """ML   """

    articles: List[MLAnalyzeRequest] = Field(..., min_length=1, max_length=50)
    addons: Optional[List[str]] = None
    save_to_db: bool = True
    max_concurrent: int = Field(default=5, ge=1, le=20)


class MLSimpleAnalyzeRequest(BaseModel):
    """ ML   ()"""

    text: str = Field(..., description=" ", min_length=10)
    source: Optional[str] = Field(default=None, description="/")
    addons: Optional[List[str]] = Field(default=None, description="  ")


class MLAddonInfo(BaseModel):
    """ML Addon """

    type: str
    url: str
    healthy: bool
    status: str


class MLStatusResponse(BaseModel):
    """ML  """

    auto_analysis_enabled: bool
    parallel_analysis: bool
    addons: Dict[str, MLAddonInfo]


# 
# Helper Functions
# 


def parse_addon_types(addons: Optional[List[str]]) -> Optional[List[MLAddonType]]:
    """   MLAddonType  """
    if addons is None:
        return None

    addon_map = {
        "sentiment": MLAddonType.SENTIMENT,
        "factcheck": MLAddonType.FACTCHECK,
        "bias": MLAddonType.BIAS,
    }

    result = []
    for addon in addons:
        addon_type = addon_map.get(addon.lower())
        if addon_type:
            result.append(addon_type)

    return result if result else None


# 
# API Endpoints
# 


@router.get("/health")
async def ml_health_check(request: Request):
    """
    ML  .

     ML Addon  .
    """
    orchestrator = get_ml_orchestrator()
    health_results = await orchestrator.check_all_health()

    all_healthy = all(r.get("status") in ["healthy", "warming_up"] for r in health_results.values())

    return {
        "status": "healthy" if all_healthy else "degraded",
        "auto_analysis_enabled": MLAddonConfig.AUTO_ANALYSIS_ENABLED,
        "addons": health_results,
    }


@router.get("/status", response_model=MLStatusResponse)
async def ml_status(request: Request):
    """
    ML   .

       Addon   .
    """
    orchestrator = get_ml_orchestrator()
    status = orchestrator.get_addon_status()

    return MLStatusResponse(
        auto_analysis_enabled=status["auto_analysis_enabled"],
        parallel_analysis=status["parallel_analysis"],
        addons={
            k: MLAddonInfo(
                type=k,
                url=v["url"],
                healthy=v["healthy"],
                status=v["status"],
            )
            for k, v in status["addons"].items()
        },
    )


@router.post("/analyze", response_model=BatchAnalysisResult)
async def analyze_article(
    request: MLAnalyzeRequest,
    req: Request,
):
    """
      ML .

      sentiment, factcheck, bias  .

    ** :**
    ``\`json
    {
        "article_id": 12345,
        "title": " ",
        "content": "  ...",
        "source": "",
        "addons": ["sentiment", "bias"]
    }
    ``\`
    """
    orchestrator = get_ml_orchestrator()
    addon_types = parse_addon_types(request.addons)

    result = await orchestrator.analyze_article(
        article_id=request.article_id,
        title=request.title,
        content=request.content,
        source=request.source,
        url=request.url,
        published_at=request.published_at,
        addon_types=addon_types,
        save_to_db=request.save_to_db,
    )

    return result


@router.post("/analyze/simple")
async def analyze_text_simple(
    request: MLSimpleAnalyzeRequest,
    req: Request,
):
    """
      ML .

     ID    .
     DB  .

    ** :**
    ``\`json
    {
        "text": "  ...",
        "source": "",
        "addons": ["sentiment"]
    }
    ``\`
    """
    orchestrator = get_ml_orchestrator()
    addon_types = parse_addon_types(request.addons)

    #  article_id  (DB  )
    result = await orchestrator.analyze_article(
        article_id=0,
        title="",
        content=request.text,
        source=request.source,
        addon_types=addon_types,
        save_to_db=False,
    )

    #    
    return {
        "sentiment": result.sentiment.model_dump() if result.sentiment else None,
        "factcheck": result.factcheck.model_dump() if result.factcheck else None,
        "bias": result.bias.model_dump() if result.bias else None,
        "total_latency_ms": result.total_latency_ms,
        "success_count": result.success_count,
        "failure_count": result.failure_count,
    }


@router.post("/analyze/batch")
async def analyze_batch(
    request: MLBatchAnalyzeRequest,
    req: Request,
):
    """
      ML .

        .

    ** :**
    ``\`json
    {
        "articles": [
            {"article_id": 1, "title": "1", "content": "1"},
            {"article_id": 2, "title": "2", "content": "2"}
        ],
        "addons": ["sentiment", "factcheck", "bias"],
        "max_concurrent": 5
    }
    ``\`
    """
    orchestrator = get_ml_orchestrator()
    addon_types = parse_addon_types(request.addons)

    #  dict  
    articles = [
        {
            "id": a.article_id,
            "title": a.title,
            "content": a.content,
            "source": a.source,
            "url": a.url,
            "published_at": a.published_at,
        }
        for a in request.articles
    ]

    results = await orchestrator.analyze_batch(
        articles=articles,
        addon_types=addon_types,
        save_to_db=request.save_to_db,
        max_concurrent=request.max_concurrent,
    )

    #  
    total_success = sum(r.success_count for r in results)
    total_failure = sum(r.failure_count for r in results)

    return {
        "total_articles": len(results),
        "total_success": total_success,
        "total_failure": total_failure,
        "results": [
            {
                "article_id": r.article_id,
                "success_count": r.success_count,
                "failure_count": r.failure_count,
                "total_latency_ms": r.total_latency_ms,
            }
            for r in results
        ],
    }


@router.post("/analyze/url")
async def analyze_url(
    url: HttpUrl = Query(..., description=" URL"),
    req: Request = None,
):
    """
    URL   ML  .

    URL    sentiment, factcheck, bias  .
    """
    import httpx
    from bs4 import BeautifulSoup

    try:
        #  URL 
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(str(url), follow_redirects=True)
            response.raise_for_status()

        # HTML 
        soup = BeautifulSoup(response.text, "html.parser")

        #   
        for tag in soup(["script", "style", "nav", "footer", "header", "aside", "ad"]):
            tag.decompose()

        #   
        title = soup.title.string if soup.title else ""
        content = soup.get_text(separator="\n", strip=True)

        #    
        if len(content) < 100:
            raise HTTPException(
                status_code=400,
                detail="     .",
            )

        # ML  
        orchestrator = get_ml_orchestrator()
        result = await orchestrator.analyze_article(
            article_id=0,
            title=title[:500] if title else "",
            content=content[:10000],  #  10000
            url=str(url),
            save_to_db=False,
        )

        return {
            "url": str(url),
            "title": title[:200] if title else None,
            "content_length": len(content),
            "sentiment": result.sentiment.model_dump() if result.sentiment else None,
            "factcheck": result.factcheck.model_dump() if result.factcheck else None,
            "bias": result.bias.model_dump() if result.bias else None,
            "total_latency_ms": result.total_latency_ms,
        }

    except httpx.HTTPError as e:
        raise HTTPException(status_code=400, detail=f"URL  : {str(e)}")
    except Exception as e:
        logger.error("URL analysis failed", url=str(url), error=str(e))
        raise HTTPException(status_code=500, detail=f" : {str(e)}")


@router.get("/addons")
async def list_addons():
    """
      ML Addon .

         .
    """
    orchestrator = get_ml_orchestrator()
    health_results = await orchestrator.check_all_health()

    addons = [
        {
            "key": "sentiment",
            "name": "  (Sentiment Analysis)",
            "description": "  (//) . KoELECTRA  ML  .",
            "endpoint": MLAddonConfig.SENTIMENT_ADDON_URL,
            "status": health_results.get("sentiment", {}).get("status", "unknown"),
            "features": ["sentiment_score", "emotion_detection", "tone_analysis"],
        },
        {
            "key": "factcheck",
            "name": " (Fact-Check Analysis)",
            "description": "    .  ,  ,   .",
            "endpoint": MLAddonConfig.FACTCHECK_ADDON_URL,
            "status": health_results.get("factcheck", {}).get("status", "unknown"),
            "features": [
                "claim_extraction",
                "credibility_score",
                "clickbait_detection",
                "misinformation_risk",
            ],
        },
        {
            "key": "bias",
            "name": "  (Bias Analysis)",
            "description": "  /  .  ,  ,  .",
            "endpoint": MLAddonConfig.BIAS_ADDON_URL,
            "status": health_results.get("bias", {}).get("status", "unknown"),
            "features": [
                "political_lean",
                "source_bias",
                "framing_analysis",
                "objectivity_score",
            ],
        },
    ]

    return {
        "addons": addons,
        "total": len(addons),
        "auto_analysis_enabled": MLAddonConfig.AUTO_ANALYSIS_ENABLED,
    }


@router.post("/config/toggle")
async def toggle_auto_analysis(
    enabled: bool = Query(..., description="   "),
):
    """
     ML  .

       ML   /.
    ( ,    )
    """
    # Note:    
    #     ML_AUTO_ANALYSIS_ENABLED  
    MLAddonConfig.AUTO_ANALYSIS_ENABLED = enabled

    logger.info(f"ML auto-analysis toggled", enabled=enabled)

    return {
        "status": "ok",
        "auto_analysis_enabled": MLAddonConfig.AUTO_ANALYSIS_ENABLED,
        "message": f" ML  {'' if enabled else ''}.",
    }

```

---

## backend/autonomous-crawler-service/src/search/__init__.py

```py
"""Search providers package with RRF-based multi-strategy search."""

from src.search.base import SearchResult, SearchProvider
from src.search.brave import BraveSearchProvider
from src.search.tavily import TavilySearchProvider
from src.search.perplexity import PerplexitySearchProvider
from src.search.orchestrator import (
    ParallelSearchOrchestrator,
    RRFSearchOrchestrator,
    RRFSearchResult,
    AggregatedSearchResult,
    create_rrf_orchestrator,
)
from src.search.rrf import (
    ReciprocalRankFusion,
    SemanticRRF,
    RRFConfig,
    RRFResult,
    RRFMergeResult,
    create_rrf_merger,
    create_semantic_rrf_merger,
)
from src.search.query_analyzer import (
    QueryAnalyzer,
    QueryAnalysis,
    MultiStrategyQueryExpander,
)

__all__ = [
    # Base classes
    "SearchResult",
    "SearchProvider",
    # Providers
    "BraveSearchProvider",
    "TavilySearchProvider",
    "PerplexitySearchProvider",
    # Orchestrators
    "ParallelSearchOrchestrator",
    "RRFSearchOrchestrator",
    "RRFSearchResult",
    "AggregatedSearchResult",
    "create_rrf_orchestrator",
    # RRF algorithm
    "ReciprocalRankFusion",
    "SemanticRRF",
    "RRFConfig",
    "RRFResult",
    "RRFMergeResult",
    "create_rrf_merger",
    "create_semantic_rrf_merger",
    # Query analysis
    "QueryAnalyzer",
    "QueryAnalysis",
    "MultiStrategyQueryExpander",
]

```

---

## backend/autonomous-crawler-service/src/search/base.py

```py
"""Base classes for search providers."""

from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any


@dataclass
class SearchResult:
    """Unified search result structure."""
    
    title: str
    url: str
    snippet: str
    source_provider: str  # brave, tavily, perplexity, browser
    
    # Optional fields
    published_date: str | None = None
    score: float | None = None
    raw_data: dict[str, Any] = field(default_factory=dict)
    
    # Metadata
    fetched_at: datetime = field(default_factory=datetime.now)
    
    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary."""
        return {
            "title": self.title,
            "url": self.url,
            "snippet": self.snippet,
            "source_provider": self.source_provider,
            "published_date": self.published_date,
            "score": self.score,
            "fetched_at": self.fetched_at.isoformat(),
        }


class SearchProvider(ABC):
    """Abstract base class for search providers."""
    
    @property
    @abstractmethod
    def name(self) -> str:
        """Provider name."""
        pass
    
    @abstractmethod
    async def search(
        self,
        query: str,
        max_results: int = 10,
        **kwargs,
    ) -> list[SearchResult]:
        """
        Execute search query.
        
        Args:
            query: Search query string
            max_results: Maximum number of results to return
            **kwargs: Provider-specific options
            
        Returns:
            List of SearchResult objects
        """
        pass
    
    @abstractmethod
    async def health_check(self) -> bool:
        """Check if the provider is healthy and accessible."""
        pass

```

---

## backend/autonomous-crawler-service/src/search/brave.py

```py
"""Brave Search API provider."""

import httpx
import structlog
from typing import Any

from src.search.base import SearchProvider, SearchResult

logger = structlog.get_logger(__name__)


class BraveSearchProvider(SearchProvider):
    """
    Brave Search API client.
    
    Docs: https://api.search.brave.com/app/documentation/web-search/get-started
    """
    
    BASE_URL = "https://api.search.brave.com/res/v1"
    
    def __init__(self, api_key: str, timeout: float = 30.0):
        self.api_key = api_key
        self.timeout = timeout
        self._client: httpx.AsyncClient | None = None
    
    @property
    def name(self) -> str:
        return "brave"
    
    async def _get_client(self) -> httpx.AsyncClient:
        """Get or create HTTP client."""
        if self._client is None or self._client.is_closed:
            self._client = httpx.AsyncClient(
                timeout=self.timeout,
                headers={
                    "Accept": "application/json",
                    "Accept-Encoding": "gzip",
                    "X-Subscription-Token": self.api_key,
                },
            )
        return self._client
    
    async def close(self) -> None:
        """Close the HTTP client."""
        if self._client and not self._client.is_closed:
            await self._client.aclose()
            self._client = None
    
    async def search(
        self,
        query: str,
        max_results: int = 10,
        country: str = "kr",
        search_lang: str = "ko",
        ui_lang: str = "ko-KR",
        freshness: str | None = None,  # pd (past day), pw (past week), pm (past month)
        **kwargs,
    ) -> list[SearchResult]:
        """
        Execute Brave web search.
        
        Args:
            query: Search query
            max_results: Max results (1-20 for free tier)
            country: Country code
            search_lang: Search language
            ui_lang: UI language
            freshness: Time filter (pd, pw, pm, py)
        """
        client = await self._get_client()
        
        params: dict[str, Any] = {
            "q": query,
            "count": min(max_results, 20),  # Brave max is 20
            "country": country,
            "search_lang": search_lang,
            "ui_lang": ui_lang,
        }
        
        if freshness:
            params["freshness"] = freshness
        
        try:
            response = await client.get(
                f"{self.BASE_URL}/web/search",
                params=params,
            )
            response.raise_for_status()
            data = response.json()
            
            results: list[SearchResult] = []
            
            # Parse web results
            web_results = data.get("web", {}).get("results", [])
            for item in web_results[:max_results]:
                results.append(SearchResult(
                    title=item.get("title", ""),
                    url=item.get("url", ""),
                    snippet=item.get("description", ""),
                    source_provider=self.name,
                    published_date=item.get("age"),  # e.g., "2 hours ago"
                    score=item.get("relevancy_score"),
                    raw_data=item,
                ))
            
            logger.info(
                "Brave search completed",
                query=query,
                results_count=len(results),
            )
            
            return results
            
        except httpx.HTTPStatusError as e:
            logger.error(
                "Brave search HTTP error",
                query=query,
                status_code=e.response.status_code,
                error=str(e),
            )
            return []
        except Exception as e:
            logger.error(
                "Brave search failed",
                query=query,
                error=str(e),
            )
            return []
    
    async def search_news(
        self,
        query: str,
        max_results: int = 10,
        country: str = "kr",
        **kwargs,
    ) -> list[SearchResult]:
        """Search news specifically."""
        client = await self._get_client()
        
        params = {
            "q": query,
            "count": min(max_results, 20),
            "country": country,
            "freshness": "pw",  # Past week for news
        }
        
        try:
            response = await client.get(
                f"{self.BASE_URL}/news/search",
                params=params,
            )
            response.raise_for_status()
            data = response.json()
            
            results: list[SearchResult] = []
            news_results = data.get("results", [])
            
            for item in news_results[:max_results]:
                results.append(SearchResult(
                    title=item.get("title", ""),
                    url=item.get("url", ""),
                    snippet=item.get("description", ""),
                    source_provider=f"{self.name}_news",
                    published_date=item.get("age"),
                    raw_data=item,
                ))
            
            return results
            
        except Exception as e:
            logger.error("Brave news search failed", query=query, error=str(e))
            return []
    
    async def health_check(self) -> bool:
        """Check API health."""
        try:
            results = await self.search("test", max_results=1)
            return len(results) > 0
        except Exception:
            return False

```

---

## backend/autonomous-crawler-service/src/search/orchestrator.py

```py
"""Parallel search orchestrator for multiple providers."""

import asyncio
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Optional
from urllib.parse import urlparse

import structlog

from src.search.base import SearchProvider, SearchResult
from src.search.rrf import (
    ReciprocalRankFusion,
    SemanticRRF,
    RRFConfig,
    RRFMergeResult,
)

logger = structlog.get_logger(__name__)


@dataclass
class AggregatedSearchResult:
    """Aggregated results from multiple search providers."""

    query: str
    results: list[SearchResult]
    providers_used: list[str]
    providers_failed: list[str]
    total_results: int
    unique_urls: int
    search_time_ms: float
    timestamp: datetime = field(default_factory=datetime.now)

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary."""
        return {
            "query": self.query,
            "results": [r.to_dict() for r in self.results],
            "providers_used": self.providers_used,
            "providers_failed": self.providers_failed,
            "total_results": self.total_results,
            "unique_urls": self.unique_urls,
            "search_time_ms": self.search_time_ms,
            "timestamp": self.timestamp.isoformat(),
        }


class ParallelSearchOrchestrator:
    """
    Orchestrates parallel searches across multiple providers.

    Features:
    - Parallel execution for speed
    - Deduplication by URL
    - Result ranking and merging
    - Provider health tracking
    - Fallback strategies
    """

    def __init__(
        self,
        providers: list[SearchProvider],
        timeout: float = 30.0,
        deduplicate: bool = True,
    ):
        """
        Initialize orchestrator.

        Args:
            providers: List of search providers to use
            timeout: Timeout for each provider search
            deduplicate: Whether to deduplicate results by URL
        """
        self.providers = providers
        self.timeout = timeout
        self.deduplicate = deduplicate
        self._provider_health: dict[str, bool] = {}

    async def search(
        self,
        query: str,
        max_results_per_provider: int = 10,
        max_total_results: int = 30,
        **kwargs,
    ) -> AggregatedSearchResult:
        """
        Execute parallel search across all providers.

        Args:
            query: Search query
            max_results_per_provider: Max results from each provider
            max_total_results: Max total results after aggregation
            **kwargs: Provider-specific options

        Returns:
            AggregatedSearchResult with merged, deduplicated results
        """
        start_time = datetime.now()

        # Create tasks for all providers
        tasks = []
        provider_names = []

        for provider in self.providers:
            task = asyncio.create_task(
                self._search_with_timeout(
                    provider,
                    query,
                    max_results_per_provider,
                    **kwargs,
                )
            )
            tasks.append(task)
            provider_names.append(provider.name)

        # Wait for all tasks to complete
        results_by_provider = await asyncio.gather(*tasks, return_exceptions=True)

        # Process results
        all_results: list[SearchResult] = []
        providers_used: list[str] = []
        providers_failed: list[str] = []

        for provider_name, result in zip(provider_names, results_by_provider):
            if isinstance(result, Exception):
                logger.error(
                    "Provider search failed",
                    provider=provider_name,
                    error=str(result),
                )
                providers_failed.append(provider_name)
                self._provider_health[provider_name] = False
            elif isinstance(result, list):
                all_results.extend(result)
                if result:
                    providers_used.append(provider_name)
                    self._provider_health[provider_name] = True
                else:
                    # Empty results but no error
                    providers_used.append(provider_name)

        # Deduplicate by URL
        if self.deduplicate:
            all_results = self._deduplicate_results(all_results)

        # Rank and limit results
        all_results = self._rank_results(all_results)[:max_total_results]

        # Calculate search time
        search_time_ms = (datetime.now() - start_time).total_seconds() * 1000

        logger.info(
            "Parallel search completed",
            query=query,
            total_results=len(all_results),
            providers_used=providers_used,
            providers_failed=providers_failed,
            search_time_ms=search_time_ms,
        )

        return AggregatedSearchResult(
            query=query,
            results=all_results,
            providers_used=providers_used,
            providers_failed=providers_failed,
            total_results=len(all_results),
            unique_urls=len(set(r.url for r in all_results)),
            search_time_ms=search_time_ms,
        )

    async def _search_with_timeout(
        self,
        provider: SearchProvider,
        query: str,
        max_results: int,
        **kwargs,
    ) -> list[SearchResult]:
        """Execute provider search with timeout."""
        try:
            return await asyncio.wait_for(
                provider.search(query, max_results, **kwargs),
                timeout=self.timeout,
            )
        except asyncio.TimeoutError:
            logger.warning(
                "Provider search timed out",
                provider=provider.name,
                timeout=self.timeout,
            )
            raise

    def _deduplicate_results(
        self,
        results: list[SearchResult],
    ) -> list[SearchResult]:
        """
        Deduplicate results by URL, keeping the first occurrence.

        Also merges information from duplicate entries.
        """
        seen_urls: dict[str, SearchResult] = {}

        for result in results:
            # Normalize URL for comparison
            normalized_url = self._normalize_url(result.url)

            if normalized_url not in seen_urls:
                seen_urls[normalized_url] = result
            else:
                # Merge: keep the one with more information
                existing = seen_urls[normalized_url]
                if len(result.snippet) > len(existing.snippet):
                    # Keep longer snippet
                    result.raw_data["merged_from"] = existing.source_provider
                    seen_urls[normalized_url] = result

        return list(seen_urls.values())

    def _normalize_url(self, url: str) -> str:
        """Normalize URL for deduplication."""
        try:
            parsed = urlparse(url)
            # Remove www. prefix and trailing slash
            netloc = parsed.netloc.lower().lstrip("www.")
            path = parsed.path.rstrip("/")
            return f"{netloc}{path}"
        except Exception:
            return url.lower()

    def _rank_results(self, results: list[SearchResult]) -> list[SearchResult]:
        """
        Rank results by relevance.

        Scoring factors:
        - Provider reliability
        - Relevance score (if available)
        - Content length
        - Recency (if date available)
        """

        def score_result(result: SearchResult) -> float:
            score = 0.0

            # Provider weight
            provider_weights = {
                "tavily": 1.0,
                "brave": 0.9,
                "perplexity": 0.85,
                "brave_news": 0.95,
            }
            score += provider_weights.get(result.source_provider, 0.5) * 10

            # Relevance score if available
            if result.score:
                score += result.score * 5

            # Content length (prefer informative snippets)
            score += min(len(result.snippet) / 100, 5)

            # Title quality
            if result.title and len(result.title) > 10:
                score += 2

            return score

        return sorted(results, key=score_result, reverse=True)

    async def search_news(
        self,
        query: str,
        max_results_per_provider: int = 10,
        days: int = 7,
        **kwargs,
    ) -> AggregatedSearchResult:
        """Search for news specifically."""
        # Add news-specific parameters
        kwargs["topic"] = "news"
        kwargs["days"] = days
        kwargs["freshness"] = "pw"  # Past week for Brave
        kwargs["search_recency_filter"] = "week"  # For Perplexity

        return await self.search(
            query=query,
            max_results_per_provider=max_results_per_provider,
            **kwargs,
        )

    async def health_check_all(self) -> dict[str, bool]:
        """Check health of all providers."""
        tasks = [(provider.name, provider.health_check()) for provider in self.providers]

        results = {}
        for name, task in tasks:
            try:
                results[name] = await asyncio.wait_for(task, timeout=10.0)
            except Exception:
                results[name] = False

        self._provider_health = results
        return results

    def get_healthy_providers(self) -> list[str]:
        """Get list of healthy provider names."""
        return [name for name, healthy in self._provider_health.items() if healthy]

    async def close_all(self) -> None:
        """Close all provider connections."""
        for provider in self.providers:
            if hasattr(provider, "close"):
                await provider.close()


@dataclass
class RRFSearchResult:
    """Result from RRF-based multi-strategy search."""

    original_query: str
    results: list[SearchResult]
    strategies_used: list[str]
    providers_used: list[str]
    total_results: int
    unique_urls: int
    search_time_ms: float
    rrf_merge_result: Optional[RRFMergeResult] = None
    query_analysis: Optional[dict[str, Any]] = None
    timestamp: datetime = field(default_factory=datetime.now)

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary."""
        return {
            "original_query": self.original_query,
            "results": [r.to_dict() for r in self.results],
            "strategies_used": self.strategies_used,
            "providers_used": self.providers_used,
            "total_results": self.total_results,
            "unique_urls": self.unique_urls,
            "search_time_ms": self.search_time_ms,
            "query_analysis": self.query_analysis,
            "timestamp": self.timestamp.isoformat(),
        }


class RRFSearchOrchestrator:
    """
    Advanced search orchestrator using RRF (Reciprocal Rank Fusion)
    to combine results from multiple search strategies.

    Features:
    - Query analysis and expansion using LLM
    - Multi-strategy parallel search
    - RRF-based result fusion
    - Semantic relevance scoring
    - Provider failover and health tracking

    Strategies:
    1. Original query search
    2. Keyword-extracted search
    3. Semantically expanded queries
    4. Cross-lingual search (for non-English queries)
    """

    def __init__(
        self,
        providers: list[SearchProvider],
        query_analyzer: Optional[Any] = None,  # QueryAnalyzer
        timeout: float = 30.0,
        rrf_config: Optional[RRFConfig] = None,
        enable_semantic_rrf: bool = True,
    ):
        """
        Initialize RRF Search Orchestrator.

        Args:
            providers: List of search providers
            query_analyzer: Optional QueryAnalyzer for query expansion
            timeout: Timeout per provider search
            rrf_config: RRF algorithm configuration
            enable_semantic_rrf: Use semantic similarity in RRF scoring
        """
        self.providers = providers
        self.query_analyzer = query_analyzer
        self.timeout = timeout
        self.rrf_config = rrf_config or RRFConfig()
        self._provider_health: dict[str, bool] = {}

        # Initialize RRF merger
        if enable_semantic_rrf:
            self._rrf_merger = SemanticRRF(self.rrf_config)
        else:
            self._rrf_merger = ReciprocalRankFusion(self.rrf_config)

    async def search_with_rrf(
        self,
        query: str,
        max_results_per_strategy: int = 10,
        max_total_results: int = 30,
        enable_query_expansion: bool = True,
        context: Optional[str] = None,
        **kwargs,
    ) -> RRFSearchResult:
        """
        Execute multi-strategy search with RRF fusion.

        Args:
            query: Original search query
            max_results_per_strategy: Max results per search strategy
            max_total_results: Max final results after RRF fusion
            enable_query_expansion: Whether to expand query using analyzer
            context: Optional context for query analysis
            **kwargs: Additional provider-specific options

        Returns:
            RRFSearchResult with fused results
        """
        start_time = datetime.now()

        # Step 1: Analyze and expand query
        search_queries = [query]  # Always include original
        query_analysis_dict = None

        if enable_query_expansion and self.query_analyzer:
            try:
                analysis = await self.query_analyzer.analyze(query, context)
                search_queries = analysis.get_all_search_queries()[:5]  # Limit strategies
                query_analysis_dict = {
                    "intent": analysis.intent,
                    "language": analysis.language,
                    "keywords": analysis.keywords,
                    "expanded_queries": analysis.expanded_queries,
                    "confidence": analysis.confidence,
                }
                logger.info(
                    "Query expanded for RRF search",
                    original=query,
                    expanded_count=len(search_queries),
                    intent=analysis.intent,
                )
            except Exception as e:
                logger.warning("Query expansion failed, using original", error=str(e))

        # Step 2: Execute parallel searches for each strategy
        ranked_lists: list[tuple[str, list[SearchResult]]] = []
        all_providers_used: set[str] = set()

        # Create search tasks for each query variant
        async def search_single_query(q: str, strategy_name: str) -> tuple[str, list[SearchResult]]:
            """Execute search for a single query across all providers."""
            results: list[SearchResult] = []

            tasks = [
                self._search_provider_with_timeout(provider, q, max_results_per_strategy, **kwargs)
                for provider in self.providers
            ]

            provider_results = await asyncio.gather(*tasks, return_exceptions=True)

            for provider, result in zip(self.providers, provider_results):
                if isinstance(result, Exception):
                    logger.debug(
                        "Provider search failed for strategy",
                        provider=provider.name,
                        strategy=strategy_name,
                        error=str(result),
                    )
                elif isinstance(result, list) and result:
                    results.extend(result)
                    all_providers_used.add(provider.name)

            return (strategy_name, results)

        # Execute all strategy searches in parallel
        strategy_tasks = [
            search_single_query(q, f"strategy_{i}" if i > 0 else "original")
            for i, q in enumerate(search_queries)
        ]

        ranked_lists = await asyncio.gather(*strategy_tasks)

        # Step 3: Apply RRF fusion
        strategy_weights = self._calculate_strategy_weights(search_queries)

        if isinstance(self._rrf_merger, SemanticRRF):
            # Use semantic RRF with query relevance
            keywords = query_analysis_dict.get("keywords", []) if query_analysis_dict else None
            rrf_result = self._rrf_merger.merge_with_query_relevance(
                query=query,
                ranked_lists=list(ranked_lists),
                query_keywords=keywords,
                weights=strategy_weights,
                max_results=max_total_results,
            )
        else:
            rrf_result = self._rrf_merger.merge(
                ranked_lists=list(ranked_lists),
                weights=strategy_weights,
                max_results=max_total_results,
            )

        # Extract final results
        final_results = rrf_result.get_search_results()

        search_time_ms = (datetime.now() - start_time).total_seconds() * 1000

        logger.info(
            "RRF search completed",
            query=query,
            strategies_used=len(search_queries),
            providers_used=list(all_providers_used),
            input_results=rrf_result.total_input_results,
            final_results=len(final_results),
            search_time_ms=round(search_time_ms, 2),
        )

        return RRFSearchResult(
            original_query=query,
            results=final_results,
            strategies_used=rrf_result.strategies_used,
            providers_used=list(all_providers_used),
            total_results=len(final_results),
            unique_urls=rrf_result.unique_results,
            search_time_ms=search_time_ms,
            rrf_merge_result=rrf_result,
            query_analysis=query_analysis_dict,
        )

    async def search_news_with_rrf(
        self,
        query: str,
        max_results_per_strategy: int = 10,
        days: int = 7,
        **kwargs,
    ) -> RRFSearchResult:
        """Search for news with RRF fusion."""
        kwargs["topic"] = "news"
        kwargs["days"] = days
        kwargs["freshness"] = "pw"
        kwargs["search_recency_filter"] = "week"

        return await self.search_with_rrf(
            query=query,
            max_results_per_strategy=max_results_per_strategy,
            **kwargs,
        )

    async def _search_provider_with_timeout(
        self,
        provider: SearchProvider,
        query: str,
        max_results: int,
        **kwargs,
    ) -> list[SearchResult]:
        """Execute provider search with timeout."""
        try:
            return await asyncio.wait_for(
                provider.search(query, max_results, **kwargs),
                timeout=self.timeout,
            )
        except asyncio.TimeoutError:
            logger.debug(
                "Provider search timed out",
                provider=provider.name,
                timeout=self.timeout,
            )
            raise
        except Exception as e:
            logger.debug(
                "Provider search failed",
                provider=provider.name,
                error=str(e),
            )
            raise

    def _calculate_strategy_weights(
        self,
        queries: list[str],
    ) -> dict[str, float]:
        """
        Calculate weights for each search strategy.

        Original query gets highest weight, expanded queries get decreasing weights.
        """
        weights = {}
        for i, _ in enumerate(queries):
            strategy_name = f"strategy_{i}" if i > 0 else "original"
            # Original: 1.0, then decreasing: 0.9, 0.8, 0.7...
            weight = max(1.0 - (i * 0.1), 0.5)
            weights[strategy_name] = weight
        return weights

    async def health_check_all(self) -> dict[str, bool]:
        """Check health of all providers."""
        results = {}
        for provider in self.providers:
            try:
                results[provider.name] = await asyncio.wait_for(
                    provider.health_check(),
                    timeout=10.0,
                )
            except Exception:
                results[provider.name] = False

        self._provider_health = results
        return results

    async def close_all(self) -> None:
        """Close all provider connections."""
        for provider in self.providers:
            if hasattr(provider, "close"):
                await provider.close()


def create_rrf_orchestrator(
    providers: list[SearchProvider],
    llm: Optional[Any] = None,
    timeout: float = 30.0,
    rrf_k: int = 60,
    enable_semantic: bool = True,
) -> RRFSearchOrchestrator:
    """
    Factory function to create an RRF Search Orchestrator.

    Args:
        providers: List of search providers
        llm: Optional LLM for query analysis
        timeout: Timeout per provider
        rrf_k: RRF constant
        enable_semantic: Enable semantic similarity in scoring

    Returns:
        Configured RRFSearchOrchestrator
    """
    from src.search.query_analyzer import QueryAnalyzer

    query_analyzer = None
    if llm:
        query_analyzer = QueryAnalyzer(llm)

    rrf_config = RRFConfig(
        k=rrf_k,
        boost_exact_matches=True,
        normalize_scores=True,
    )

    return RRFSearchOrchestrator(
        providers=providers,
        query_analyzer=query_analyzer,
        timeout=timeout,
        rrf_config=rrf_config,
        enable_semantic_rrf=enable_semantic,
    )

```

---

## backend/autonomous-crawler-service/src/search/perplexity.py

```py
"""Perplexity API provider."""

import httpx
import structlog
from typing import Any, Literal

from src.search.base import SearchProvider, SearchResult

logger = structlog.get_logger(__name__)


class PerplexitySearchProvider(SearchProvider):
    """
    Perplexity API client (Sonar models for search).
    
    Docs: https://docs.perplexity.ai/api-reference/chat-completions
    """
    
    BASE_URL = "https://api.perplexity.ai"
    
    # Available models
    MODELS = {
        "sonar": "sonar",  # Lightweight, fast
        "sonar-pro": "sonar-pro",  # More comprehensive
        "sonar-reasoning": "sonar-reasoning",  # With reasoning
        "sonar-reasoning-pro": "sonar-reasoning-pro",  # Best quality
    }
    
    def __init__(
        self,
        api_key: str,
        model: str = "sonar",
        timeout: float = 60.0,
    ):
        self.api_key = api_key
        self.model = self.MODELS.get(model, model)
        self.timeout = timeout
        self._client: httpx.AsyncClient | None = None
    
    @property
    def name(self) -> str:
        return "perplexity"
    
    async def _get_client(self) -> httpx.AsyncClient:
        """Get or create HTTP client."""
        if self._client is None or self._client.is_closed:
            self._client = httpx.AsyncClient(
                timeout=self.timeout,
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json",
                },
            )
        return self._client
    
    async def close(self) -> None:
        """Close the HTTP client."""
        if self._client and not self._client.is_closed:
            await self._client.aclose()
            self._client = None
    
    async def search(
        self,
        query: str,
        max_results: int = 10,
        search_recency_filter: Literal["month", "week", "day", "hour"] | None = None,
        search_domain_filter: list[str] | None = None,
        return_citations: bool = True,
        return_related_questions: bool = False,
        **kwargs,
    ) -> list[SearchResult]:
        """
        Execute Perplexity search using Sonar models.
        
        Args:
            query: Search query
            max_results: Not directly used (Perplexity returns variable citations)
            search_recency_filter: Filter by recency (month, week, day, hour)
            search_domain_filter: List of domains to restrict search
            return_citations: Return source citations
            return_related_questions: Return related questions
        """
        client = await self._get_client()
        
        # Build system prompt for search
        system_prompt = (
            "You are a search assistant. Return factual information with sources. "
            "Focus on finding relevant web pages and their content."
        )
        
        payload: dict[str, Any] = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": query},
            ],
            "return_citations": return_citations,
            "return_related_questions": return_related_questions,
        }
        
        # Add search filters if specified
        if search_recency_filter:
            payload["search_recency_filter"] = search_recency_filter
        if search_domain_filter:
            payload["search_domain_filter"] = search_domain_filter
        
        try:
            response = await client.post(
                f"{self.BASE_URL}/chat/completions",
                json=payload,
            )
            response.raise_for_status()
            data = response.json()
            
            results: list[SearchResult] = []
            
            # Extract citations as search results
            citations = data.get("citations", [])
            content = ""
            
            if data.get("choices"):
                content = data["choices"][0].get("message", {}).get("content", "")
            
            # Parse citations into results
            for i, citation_url in enumerate(citations[:max_results]):
                # Try to extract title from the content that references this citation
                # Citations are referenced as [1], [2], etc. in the content
                results.append(SearchResult(
                    title=f"Source {i + 1}",  # Perplexity doesn't provide titles directly
                    url=citation_url,
                    snippet=content[:500] if i == 0 else "",  # Full answer as snippet for first result
                    source_provider=self.name,
                    score=1.0 - (i * 0.1),  # Assume earlier citations are more relevant
                    raw_data={
                        "full_response": content,
                        "citation_index": i,
                        "model": self.model,
                    },
                ))
            
            logger.info(
                "Perplexity search completed",
                query=query,
                citations_count=len(citations),
                model=self.model,
            )
            
            return results
            
        except httpx.HTTPStatusError as e:
            logger.error(
                "Perplexity search HTTP error",
                query=query,
                status_code=e.response.status_code,
                error=str(e),
            )
            return []
        except Exception as e:
            logger.error(
                "Perplexity search failed",
                query=query,
                error=str(e),
            )
            return []
    
    async def search_with_context(
        self,
        query: str,
        context: str | None = None,
        focus: Literal["internet", "academic", "news"] = "internet",
        **kwargs,
    ) -> tuple[str, list[SearchResult]]:
        """
        Search with context and return both answer and citations.
        
        Args:
            query: Search query
            context: Additional context to consider
            focus: Search focus area
            
        Returns:
            Tuple of (AI answer, list of SearchResult)
        """
        client = await self._get_client()
        
        messages = []
        if context:
            messages.append({
                "role": "system",
                "content": f"Context: {context}\n\nProvide a comprehensive answer with citations.",
            })
        
        messages.append({"role": "user", "content": query})
        
        payload = {
            "model": self.model,
            "messages": messages,
            "return_citations": True,
        }
        
        try:
            response = await client.post(
                f"{self.BASE_URL}/chat/completions",
                json=payload,
            )
            response.raise_for_status()
            data = response.json()
            
            answer = ""
            if data.get("choices"):
                answer = data["choices"][0].get("message", {}).get("content", "")
            
            citations = data.get("citations", [])
            results = [
                SearchResult(
                    title=f"Citation {i + 1}",
                    url=url,
                    snippet="",
                    source_provider=self.name,
                    raw_data={"citation_index": i},
                )
                for i, url in enumerate(citations)
            ]
            
            return answer, results
            
        except Exception as e:
            logger.error("Perplexity context search failed", query=query, error=str(e))
            return "", []
    
    async def health_check(self) -> bool:
        """Check API health."""
        try:
            results = await self.search("test", max_results=1)
            return True
        except Exception:
            return False

```

---

## backend/autonomous-crawler-service/src/search/query_analyzer.py

```py
"""Query analyzer for semantic search enhancement using LLM."""

import asyncio
import re
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Callable, Optional

import structlog
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import HumanMessage, SystemMessage

logger = structlog.get_logger(__name__)


class SearchStrategy(Enum):
    """Search strategy types for fallback mechanisms."""

    FULL_QUERY = "full_query"
    KEYWORDS_AND = "keywords_and"
    KEYWORDS_OR = "keywords_or"
    PRIMARY_KEYWORD = "primary_keyword"
    SEMANTIC_VARIANT = "semantic_variant"
    RELATED_TOPIC = "related_topic"
    PARTIAL_MATCH = "partial_match"


@dataclass
class FallbackStrategy:
    """Represents a fallback search strategy."""

    strategy_type: SearchStrategy
    query: str
    priority: int
    description: str
    weight: float = 1.0


@dataclass
class QueryAnalysis:
    """Result of query analysis."""

    original_query: str
    intent: str  # search_intent: news, research, factcheck, general
    language: str  # detected language
    keywords: list[str]  # extracted keywords
    primary_keyword: str  # most important keyword
    entities: list[str]  # named entities (people, places, organizations)
    expanded_queries: list[str]  # semantically expanded queries
    synonyms: dict[str, list[str]]  # word -> synonyms mapping
    search_terms: list[str]  # optimized search terms for different strategies
    fallback_strategies: list[FallbackStrategy]  # ordered fallback strategies
    confidence: float  # analysis confidence score
    metadata: dict[str, Any] = field(default_factory=dict)

    def get_all_search_queries(self) -> list[str]:
        """Get all search queries for multi-strategy search."""
        queries = [self.original_query]
        queries.extend(self.expanded_queries)
        queries.extend(self.search_terms)
        # Deduplicate while preserving order
        seen = set()
        unique = []
        for q in queries:
            if q.lower() not in seen:
                seen.add(q.lower())
                unique.append(q)
        return unique

    def get_fallback_query(self, attempt_index: int) -> Optional[str]:
        """Get fallback query by attempt index."""
        if attempt_index < len(self.fallback_strategies):
            return self.fallback_strategies[attempt_index].query
        return None


class QueryAnalyzer:
    """
    Analyzes and expands search queries using LLM for better search accuracy.

    Features:
    - Intent detection (news, research, factcheck, general)
    - Keyword extraction
    - Named entity recognition
    - Query expansion with synonyms and related terms
    - Multi-language support
    - Search term optimization
    """

    ANALYSIS_PROMPT = """You are a search query analyzer. Analyze the given query and provide structured information to improve search accuracy.

For the query: "{query}"

Respond in the following JSON format ONLY (no additional text):
{{
    "intent": "<news|research|factcheck|opinion|general>",
    "language": "<detected language code, e.g., ko, en, ja>",
    "keywords": ["<key term 1>", "<key term 2>", ...],
    "entities": ["<named entity 1>", "<named entity 2>", ...],
    "expanded_queries": [
        "<semantically related query 1>",
        "<semantically related query 2>",
        "<semantically related query 3>"
    ],
    "synonyms": {{
        "<original term>": ["<synonym 1>", "<synonym 2>"]
    }},
    "search_terms": [
        "<optimized search term for web search>",
        "<optimized search term for news search>",
        "<english translation if non-english>"
    ],
    "confidence": <0.0 to 1.0>
}}

Guidelines:
1. For ambiguous or metaphorical queries (like " " which could be about moles' attack power, a game, or slang), generate multiple interpretations
2. Extract the core semantic meaning, not just literal keywords
3. For non-English queries, include English translations in search_terms
4. Identify if the query is about specific domains (politics, sports, tech, entertainment, etc.)
5. Generate expanded_queries that capture different aspects or interpretations of the query
6. Keep search_terms concise and optimized for search engines"""

    QUICK_KEYWORDS_PROMPT = """Extract key search terms from this query. Return ONLY a JSON array of strings, no explanation.
Query: "{query}"
Example output: ["term1", "term2", "term3"]"""

    def __init__(
        self,
        llm: BaseChatModel,
        enable_expansion: bool = True,
        max_expanded_queries: int = 5,
        cache_results: bool = True,
    ):
        """
        Initialize the query analyzer.

        Args:
            llm: Language model for query analysis
            enable_expansion: Whether to enable query expansion
            max_expanded_queries: Maximum number of expanded queries
            cache_results: Whether to cache analysis results
        """
        self.llm = llm
        self.enable_expansion = enable_expansion
        self.max_expanded_queries = max_expanded_queries
        self._cache: Optional[dict[str, QueryAnalysis]] = {} if cache_results else None

    async def analyze(
        self,
        query: str,
        context: Optional[str] = None,
        force_refresh: bool = False,
    ) -> QueryAnalysis:
        """
        Analyze a search query and generate expanded search terms.

        Args:
            query: The original search query
            context: Optional context about the search domain
            force_refresh: Force re-analysis even if cached

        Returns:
            QueryAnalysis with expanded queries and keywords
        """
        # Check cache
        cache_key = f"{query}:{context or ''}"
        if self._cache is not None and not force_refresh:
            if cache_key in self._cache:
                logger.debug("Using cached query analysis", query=query)
                return self._cache[cache_key]

        try:
            analysis = await self._analyze_with_llm(query, context)

            # Cache result
            if self._cache is not None:
                self._cache[cache_key] = analysis

            logger.info(
                "Query analyzed",
                query=query,
                intent=analysis.intent,
                keywords=analysis.keywords,
                expanded_count=len(analysis.expanded_queries),
            )

            return analysis

        except Exception as e:
            logger.error("Query analysis failed, using fallback", query=query, error=str(e))
            return self._fallback_analysis(query)

    async def _analyze_with_llm(
        self,
        query: str,
        context: Optional[str] = None,
    ) -> QueryAnalysis:
        """Perform LLM-based query analysis."""
        prompt = self.ANALYSIS_PROMPT.format(query=query)

        if context:
            prompt += f"\n\nAdditional context: {context}"

        messages = [
            SystemMessage(content="You are a search query analyzer. Respond only with valid JSON."),
            HumanMessage(content=prompt),
        ]

        response = await self.llm.ainvoke(messages)
        content = response.content.strip()

        # Parse JSON response
        import json

        # Try to extract JSON from the response
        json_match = re.search(r"\{[\s\S]*\}", content)
        if json_match:
            content = json_match.group()

        try:
            data = json.loads(content)
        except json.JSONDecodeError:
            logger.warning("Failed to parse LLM response as JSON", response=content[:200])
            return self._fallback_analysis(query)

        # Validate and extract fields with defaults
        keywords = data.get("keywords", [])[:10]
        primary_keyword = self._identify_primary_keyword(keywords, query)

        analysis = QueryAnalysis(
            original_query=query,
            intent=data.get("intent", "general"),
            language=data.get("language", "unknown"),
            keywords=keywords,
            primary_keyword=primary_keyword,
            entities=data.get("entities", [])[:5],
            expanded_queries=data.get("expanded_queries", [])[: self.max_expanded_queries],
            synonyms=data.get("synonyms", {}),
            search_terms=data.get("search_terms", [])[:5],
            fallback_strategies=[],  # Will be generated below
            confidence=min(max(data.get("confidence", 0.5), 0.0), 1.0),
            metadata={"context": context} if context else {},
        )

        # Generate fallback strategies
        analysis.fallback_strategies = self._generate_fallback_strategies(analysis)

        return analysis

    async def extract_keywords_quick(self, query: str) -> list[str]:
        """
        Quick keyword extraction without full analysis.

        Useful for simple queries or when speed is critical.
        """
        try:
            prompt = self.QUICK_KEYWORDS_PROMPT.format(query=query)
            messages = [HumanMessage(content=prompt)]

            response = await self.llm.ainvoke(messages)
            content = response.content.strip()

            # Parse JSON array
            import json

            # Try to extract JSON array
            array_match = re.search(r"\[[\s\S]*\]", content)
            if array_match:
                keywords = json.loads(array_match.group())
                if isinstance(keywords, list):
                    return [str(k) for k in keywords[:10]]

            return self._extract_keywords_rule_based(query)

        except Exception as e:
            logger.debug("Quick keyword extraction failed", error=str(e))
            return self._extract_keywords_rule_based(query)

    def _fallback_analysis(self, query: str) -> QueryAnalysis:
        """Fallback analysis when LLM fails."""
        keywords = self._extract_keywords_rule_based(query)
        primary_keyword = self._identify_primary_keyword(keywords, query)
        language = self._detect_language(query)

        analysis = QueryAnalysis(
            original_query=query,
            intent="general",
            language=language,
            keywords=keywords,
            primary_keyword=primary_keyword,
            entities=[],
            expanded_queries=[],
            synonyms={},
            search_terms=keywords[:3],
            fallback_strategies=[],  # Will be generated below
            confidence=0.3,
            metadata={"fallback": True},
        )

        # Generate fallback strategies
        analysis.fallback_strategies = self._generate_fallback_strategies(analysis)

        return analysis

    def _extract_keywords_rule_based(self, query: str) -> list[str]:
        """Rule-based keyword extraction as fallback."""
        # Remove common stop words (basic implementation)
        stop_words = {
            # English
            "the",
            "a",
            "an",
            "is",
            "are",
            "was",
            "were",
            "be",
            "been",
            "being",
            "have",
            "has",
            "had",
            "do",
            "does",
            "did",
            "will",
            "would",
            "could",
            "should",
            "may",
            "might",
            "must",
            "shall",
            "can",
            "need",
            "dare",
            "ought",
            "used",
            "to",
            "of",
            "in",
            "for",
            "on",
            "with",
            "at",
            "by",
            "from",
            "as",
            "into",
            "through",
            "during",
            "before",
            "after",
            "above",
            "below",
            "between",
            "under",
            "again",
            "further",
            "then",
            "once",
            "here",
            "there",
            "when",
            "where",
            "why",
            "how",
            "all",
            "each",
            "few",
            "more",
            "most",
            "other",
            "some",
            "such",
            "no",
            "nor",
            "not",
            "only",
            "own",
            "same",
            "so",
            "than",
            "too",
            "very",
            "just",
            "and",
            "but",
            "if",
            "or",
            "because",
            "until",
            "while",
            "although",
            # Korean particles and common words
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
        }

        # Tokenize
        words = re.findall(r"[\w-]+", query.lower())

        # Filter stop words and short words
        keywords = [w for w in words if w not in stop_words and len(w) > 1]

        return keywords[:10]

    def _detect_language(self, text: str) -> str:
        """Simple language detection based on character ranges."""
        # Count character types
        korean_count = len(re.findall(r"[-]", text))
        japanese_count = len(re.findall(r"[--]", text))
        chinese_count = len(re.findall(r"[\u4e00-\u9fff]", text))
        latin_count = len(re.findall(r"[a-zA-Z]", text))

        total = korean_count + japanese_count + chinese_count + latin_count
        if total == 0:
            return "unknown"

        if korean_count / total > 0.3:
            return "ko"
        elif japanese_count / total > 0.3:
            return "ja"
        elif chinese_count / total > 0.3:
            return "zh"
        else:
            return "en"

    def _identify_primary_keyword(self, keywords: list[str], original_query: str) -> str:
        """Identify the most important keyword from the list."""
        if not keywords:
            words = original_query.split()
            return words[0] if words else original_query

        # Score-based primary keyword selection
        scores = {}
        for keyword in keywords:
            score = 0.0

            # Length weight (longer keywords are more specific)
            score += min(len(keyword) / 10.0, 1.0) * 0.3

            # Position weight (earlier keywords are often more important)
            pos = original_query.lower().find(keyword.lower())
            if pos >= 0:
                score += (1.0 - pos / len(original_query)) * 0.3

            # Uppercase start (potential proper noun)
            if keyword[0].isupper():
                score += 0.2

            # Contains numbers (specific identifier)
            if any(c.isdigit() for c in keyword):
                score += 0.1

            scores[keyword] = score

        return max(scores.items(), key=lambda x: x[1])[0] if scores else keywords[0]

    def _generate_fallback_strategies(self, analysis: QueryAnalysis) -> list[FallbackStrategy]:
        """Generate ordered fallback search strategies."""
        strategies = []
        priority = 1

        # Strategy 1: Full query
        strategies.append(
            FallbackStrategy(
                strategy_type=SearchStrategy.FULL_QUERY,
                query=analysis.original_query,
                priority=priority,
                description="  "
                if analysis.language == "ko"
                else "Search with original query",
                weight=1.0,
            )
        )
        priority += 1

        # Strategy 2: Keywords AND
        if len(analysis.keywords) > 1:
            strategies.append(
                FallbackStrategy(
                    strategy_type=SearchStrategy.KEYWORDS_AND,
                    query=" ".join(analysis.keywords),
                    priority=priority,
                    description="  "
                    if analysis.language == "ko"
                    else "Search with all keywords",
                    weight=0.9,
                )
            )
            priority += 1

        # Strategy 3: Primary keyword
        if analysis.primary_keyword:
            strategies.append(
                FallbackStrategy(
                    strategy_type=SearchStrategy.PRIMARY_KEYWORD,
                    query=analysis.primary_keyword,
                    priority=priority,
                    description="  "
                    if analysis.language == "ko"
                    else "Search with primary keyword only",
                    weight=0.85,
                )
            )
            priority += 1

        # Strategy 4: Semantic variants (expanded queries)
        for i, expanded in enumerate(analysis.expanded_queries[:3]):
            if expanded.lower() != analysis.original_query.lower():
                strategies.append(
                    FallbackStrategy(
                        strategy_type=SearchStrategy.SEMANTIC_VARIANT,
                        query=expanded,
                        priority=priority,
                        description=f"  {i + 1}: {expanded}"
                        if analysis.language == "ko"
                        else f"Variant query {i + 1}: {expanded}",
                        weight=0.8 - (i * 0.1),
                    )
                )
                priority += 1

        # Strategy 5: Keywords OR (broader search)
        if len(analysis.keywords) > 1:
            or_query = " OR ".join(analysis.keywords[:5])
            strategies.append(
                FallbackStrategy(
                    strategy_type=SearchStrategy.KEYWORDS_OR,
                    query=or_query,
                    priority=priority,
                    description=" OR  ( )"
                    if analysis.language == "ko"
                    else "Keywords OR search (broader)",
                    weight=0.7,
                )
            )
            priority += 1

        # Strategy 6: Partial match (top 2 keywords)
        if len(analysis.keywords) >= 2:
            partial_query = f"{analysis.keywords[0]} {analysis.keywords[1]}"
            strategies.append(
                FallbackStrategy(
                    strategy_type=SearchStrategy.PARTIAL_MATCH,
                    query=partial_query,
                    priority=priority,
                    description="   "
                    if analysis.language == "ko"
                    else "Top keywords partial match",
                    weight=0.65,
                )
            )
            priority += 1

        # Sort by priority
        strategies.sort(key=lambda s: s.priority)
        return strategies

    def build_no_result_message(self, analysis: QueryAnalysis) -> str:
        """Build a helpful message when no results are found."""
        if analysis.language == "ko":
            message = "   .   :\n\n"
            message += " :\n"
            message += f"- {analysis.original_query}\n"
            message += f"- {analysis.primary_keyword}\n"

            message += "\n  :\n"
            message += "1.     \n"
            message += f"2.    : {', '.join(analysis.keywords[:3])}\n"
            message += "3.    \n"

            intent_desc = {
                "news": " ",
                "research": " ",
                "factcheck": "",
                "opinion": " ",
                "general": " ",
            }
            message += f"\n : {intent_desc.get(analysis.intent, ' ')}"
        else:
            message = "Search results were difficult to find. Try the following:\n\n"
            message += "Queries attempted:\n"
            message += f"- {analysis.original_query}\n"
            message += f"- {analysis.primary_keyword}\n"

            message += "\nRecommended approaches:\n"
            message += "1. Try more specific keywords\n"
            message += f"2. Use alternative keywords: {', '.join(analysis.keywords[:3])}\n"
            message += "3. Adjust the time range\n"

            message += f"\nDetected intent: {analysis.intent}"

        return message

    def build_enhanced_task(self, analysis: QueryAnalysis, original_task: str) -> str:
        """Build an enhanced task with fallback instructions."""
        task = original_task + "\n\n"

        if analysis.language == "ko":
            task += "  (   ):\n"
            for i, strategy in enumerate(analysis.fallback_strategies[:5], 1):
                task += f'{i}. {strategy.description}: "{strategy.query}"\n'
            task += "\n:     .    ,\n"
            task += "   .   \n"
            task += "      ."
        else:
            task += "Search strategies (try in order if no results):\n"
            for i, strategy in enumerate(analysis.fallback_strategies[:5], 1):
                task += f'{i}. {strategy.description}: "{strategy.query}"\n'
            task += (
                '\nIMPORTANT: Never say "not found" or "no results". Try ALL strategies above,\n'
            )
            task += "and provide whatever relevant information you can find. Even if not an exact match,\n"
            task += "providing the most relevant information is important."

        return task

    def clear_cache(self) -> None:
        """Clear the analysis cache."""
        if self._cache is not None:
            self._cache.clear()


class MultiStrategyQueryExpander:
    """
    Expands a single query into multiple search strategies.

    Strategies:
    1. Original query (exact match)
    2. Keyword-based query (extracted keywords)
    3. Semantic expansion (related concepts)
    4. Entity-focused query (named entities)
    5. Cross-lingual query (translations)
    """

    def __init__(self, analyzer: QueryAnalyzer):
        self.analyzer = analyzer

    async def expand(
        self,
        query: str,
        max_strategies: int = 5,
        context: Optional[str] = None,
    ) -> list[dict[str, Any]]:
        """
        Expand query into multiple search strategies.

        Returns:
            List of dicts with 'query', 'strategy', and 'weight' keys
        """
        analysis = await self.analyzer.analyze(query, context)

        strategies = []

        # Strategy 1: Original query (highest weight)
        strategies.append(
            {
                "query": analysis.original_query,
                "strategy": "original",
                "weight": 1.0,
            }
        )

        # Strategy 2: Keywords joined
        if analysis.keywords:
            keyword_query = " ".join(analysis.keywords[:5])
            if keyword_query.lower() != query.lower():
                strategies.append(
                    {
                        "query": keyword_query,
                        "strategy": "keywords",
                        "weight": 0.9,
                    }
                )

        # Strategy 3: Semantic expansions
        for i, expanded in enumerate(analysis.expanded_queries[:3]):
            strategies.append(
                {
                    "query": expanded,
                    "strategy": f"semantic_{i + 1}",
                    "weight": 0.8 - (i * 0.1),
                }
            )

        # Strategy 4: Entity-focused
        if analysis.entities:
            entity_query = " ".join(analysis.entities[:3])
            if entity_query.lower() != query.lower():
                strategies.append(
                    {
                        "query": entity_query,
                        "strategy": "entities",
                        "weight": 0.7,
                    }
                )

        # Strategy 5: Search terms (often includes translations)
        for i, term in enumerate(analysis.search_terms[:2]):
            if term.lower() != query.lower():
                strategies.append(
                    {
                        "query": term,
                        "strategy": f"search_term_{i + 1}",
                        "weight": 0.75 - (i * 0.1),
                    }
                )

        # Deduplicate and limit
        seen_queries = set()
        unique_strategies = []
        for s in strategies:
            q_lower = s["query"].lower()
            if q_lower not in seen_queries:
                seen_queries.add(q_lower)
                unique_strategies.append(s)

        return unique_strategies[:max_strategies]

```

---

## backend/autonomous-crawler-service/src/search/rrf.py

```py
"""Reciprocal Rank Fusion (RRF) for multi-strategy search result merging."""

from dataclasses import dataclass, field
from typing import Any, TypeVar, Callable, Optional
from collections import defaultdict

import structlog

from src.search.base import SearchResult

logger = structlog.get_logger(__name__)

T = TypeVar("T")


@dataclass
class RRFConfig:
    """Configuration for RRF algorithm."""

    k: int = 60  # RRF constant (default: 60, higher = more weight to lower ranks)
    min_score_threshold: float = 0.0  # Minimum RRF score to include result
    boost_exact_matches: bool = True  # Boost results that appear in multiple rankings
    multi_appearance_bonus: float = 0.1  # Bonus per additional appearance
    normalize_scores: bool = True  # Normalize final scores to 0-1 range


@dataclass
class RRFResult:
    """Result with RRF scoring information."""

    item: SearchResult
    rrf_score: float
    appearances: int  # Number of rankings this item appeared in
    rank_positions: list[int]  # Original positions in each ranking
    source_strategies: list[str]  # Which strategies found this result

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary."""
        return {
            "item": self.item.to_dict(),
            "rrf_score": self.rrf_score,
            "appearances": self.appearances,
            "rank_positions": self.rank_positions,
            "source_strategies": self.source_strategies,
        }


@dataclass
class RRFMergeResult:
    """Result of RRF merge operation."""

    results: list[RRFResult]
    total_input_results: int
    unique_results: int
    strategies_used: list[str]
    processing_time_ms: float
    config: RRFConfig = field(default_factory=RRFConfig)

    def get_search_results(self) -> list[SearchResult]:
        """Get just the SearchResult items, sorted by RRF score."""
        return [r.item for r in sorted(self.results, key=lambda x: x.rrf_score, reverse=True)]

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary."""
        return {
            "results": [r.to_dict() for r in self.results],
            "total_input_results": self.total_input_results,
            "unique_results": self.unique_results,
            "strategies_used": self.strategies_used,
            "processing_time_ms": self.processing_time_ms,
        }


class ReciprocalRankFusion:
    """
    Implements Reciprocal Rank Fusion (RRF) algorithm for combining
    multiple ranked lists into a single ranking.

    RRF Score =  1 / (k + rank_i) for each ranking list

    Features:
    - Combines results from multiple search strategies
    - Handles duplicates by URL normalization
    - Boosts results that appear in multiple rankings
    - Configurable k parameter and scoring thresholds

    Reference:
    Cormack, G. V., Clarke, C. L., & Buettcher, S. (2009).
    Reciprocal rank fusion outperforms condorcet and individual rank learning methods.
    """

    def __init__(self, config: Optional[RRFConfig] = None):
        """
        Initialize RRF merger.

        Args:
            config: RRF configuration options
        """
        self.config = config or RRFConfig()

    def merge(
        self,
        ranked_lists: list[tuple[str, list[SearchResult]]],
        weights: Optional[dict[str, float]] = None,
        max_results: int = 50,
    ) -> RRFMergeResult:
        """
        Merge multiple ranked lists using RRF.

        Args:
            ranked_lists: List of (strategy_name, results) tuples
            weights: Optional weights per strategy (default: equal weights)
            max_results: Maximum number of results to return

        Returns:
            RRFMergeResult with merged and scored results
        """
        import time

        start_time = time.time()

        if not ranked_lists:
            return RRFMergeResult(
                results=[],
                total_input_results=0,
                unique_results=0,
                strategies_used=[],
                processing_time_ms=0,
                config=self.config,
            )

        # Default equal weights
        if weights is None:
            weights = {name: 1.0 for name, _ in ranked_lists}

        # Track scores and metadata per unique URL
        url_scores: dict[str, float] = defaultdict(float)
        url_items: dict[str, SearchResult] = {}
        url_appearances: dict[str, int] = defaultdict(int)
        url_ranks: dict[str, list[int]] = defaultdict(list)
        url_strategies: dict[str, list[str]] = defaultdict(list)

        total_input = 0
        strategies_used = []

        for strategy_name, results in ranked_lists:
            if not results:
                continue

            strategies_used.append(strategy_name)
            strategy_weight = weights.get(strategy_name, 1.0)

            for rank, result in enumerate(results, start=1):
                total_input += 1

                # Normalize URL for deduplication
                normalized_url = self._normalize_url(result.url)

                # Calculate RRF score for this position
                rrf_score = strategy_weight / (self.config.k + rank)

                url_scores[normalized_url] += rrf_score
                url_appearances[normalized_url] += 1
                url_ranks[normalized_url].append(rank)
                url_strategies[normalized_url].append(strategy_name)

                # Keep the result with most information
                if normalized_url not in url_items:
                    url_items[normalized_url] = result
                else:
                    existing = url_items[normalized_url]
                    # Prefer result with longer snippet or more metadata
                    if len(result.snippet) > len(existing.snippet):
                        url_items[normalized_url] = result

        # Apply multi-appearance bonus
        if self.config.boost_exact_matches:
            for url, appearances in url_appearances.items():
                if appearances > 1:
                    bonus = self.config.multi_appearance_bonus * (appearances - 1)
                    url_scores[url] += bonus

        # Normalize scores if configured
        if self.config.normalize_scores and url_scores:
            max_score = max(url_scores.values())
            if max_score > 0:
                url_scores = {url: score / max_score for url, score in url_scores.items()}

        # Build final results
        rrf_results = []
        for url, score in url_scores.items():
            if score >= self.config.min_score_threshold:
                rrf_results.append(
                    RRFResult(
                        item=url_items[url],
                        rrf_score=score,
                        appearances=url_appearances[url],
                        rank_positions=url_ranks[url],
                        source_strategies=url_strategies[url],
                    )
                )

        # Sort by RRF score (descending)
        rrf_results.sort(key=lambda x: x.rrf_score, reverse=True)

        # Limit results
        rrf_results = rrf_results[:max_results]

        processing_time = (time.time() - start_time) * 1000

        logger.info(
            "RRF merge completed",
            total_input=total_input,
            unique_results=len(rrf_results),
            strategies=strategies_used,
            processing_time_ms=round(processing_time, 2),
        )

        return RRFMergeResult(
            results=rrf_results,
            total_input_results=total_input,
            unique_results=len(rrf_results),
            strategies_used=strategies_used,
            processing_time_ms=processing_time,
            config=self.config,
        )

    def merge_with_reranking(
        self,
        ranked_lists: list[tuple[str, list[SearchResult]]],
        rerank_fn: Callable[[SearchResult], float],
        rerank_weight: float = 0.3,
        **kwargs,
    ) -> RRFMergeResult:
        """
        Merge with additional reranking based on a custom scoring function.

        Args:
            ranked_lists: List of (strategy_name, results) tuples
            rerank_fn: Function that takes a SearchResult and returns a score (0-1)
            rerank_weight: Weight for the reranking score (RRF weight = 1 - rerank_weight)
            **kwargs: Additional arguments passed to merge()

        Returns:
            RRFMergeResult with combined RRF and reranking scores
        """
        # First do standard RRF merge
        rrf_result = self.merge(ranked_lists, **kwargs)

        # Apply reranking
        for result in rrf_result.results:
            try:
                rerank_score = rerank_fn(result.item)
                # Combine RRF score with rerank score
                combined = result.rrf_score * (1 - rerank_weight) + rerank_score * rerank_weight
                result.rrf_score = combined
            except Exception as e:
                logger.debug("Reranking failed for result", url=result.item.url, error=str(e))

        # Re-sort by new scores
        rrf_result.results.sort(key=lambda x: x.rrf_score, reverse=True)

        return rrf_result

    def _normalize_url(self, url: str) -> str:
        """Normalize URL for deduplication."""
        try:
            from urllib.parse import urlparse

            parsed = urlparse(url)
            # Remove www. prefix and trailing slash
            netloc = parsed.netloc.lower().lstrip("www.")
            path = parsed.path.rstrip("/")
            # Remove common tracking parameters
            return f"{netloc}{path}"
        except Exception:
            return url.lower()


class SemanticRRF(ReciprocalRankFusion):
    """
    Extended RRF with semantic similarity scoring.

    Combines RRF with semantic similarity between query and results
    for better relevance ranking.
    """

    def __init__(
        self,
        config: Optional[RRFConfig] = None,
        similarity_weight: float = 0.2,
    ):
        """
        Initialize Semantic RRF.

        Args:
            config: RRF configuration
            similarity_weight: Weight for semantic similarity score (0-1)
        """
        super().__init__(config)
        self.similarity_weight = similarity_weight

    def merge_with_query_relevance(
        self,
        query: str,
        ranked_lists: list[tuple[str, list[SearchResult]]],
        query_keywords: Optional[list[str]] = None,
        **kwargs,
    ) -> RRFMergeResult:
        """
        Merge with query relevance scoring.

        Args:
            query: Original search query
            ranked_lists: List of (strategy_name, results) tuples
            query_keywords: Pre-extracted keywords for matching
            **kwargs: Additional arguments passed to merge()
        """
        keywords = query_keywords or self._extract_keywords(query)

        def relevance_scorer(result: SearchResult) -> float:
            """Score result based on keyword presence."""
            text = f"{result.title} {result.snippet}".lower()

            if not keywords:
                return 0.5

            matches = sum(1 for kw in keywords if kw.lower() in text)
            return min(matches / len(keywords), 1.0)

        return self.merge_with_reranking(
            ranked_lists=ranked_lists,
            rerank_fn=relevance_scorer,
            rerank_weight=self.similarity_weight,
            **kwargs,
        )

    def _extract_keywords(self, query: str) -> list[str]:
        """Simple keyword extraction."""
        import re

        # Basic tokenization
        words = re.findall(r"[\w-]+", query.lower())
        # Filter short words
        return [w for w in words if len(w) > 1]


def create_rrf_merger(
    k: int = 60,
    boost_duplicates: bool = True,
    normalize: bool = True,
) -> ReciprocalRankFusion:
    """
    Factory function to create an RRF merger with common configurations.

    Args:
        k: RRF constant (higher = more weight to lower ranks)
        boost_duplicates: Boost results appearing in multiple rankings
        normalize: Normalize final scores to 0-1

    Returns:
        Configured ReciprocalRankFusion instance
    """
    config = RRFConfig(
        k=k,
        boost_exact_matches=boost_duplicates,
        normalize_scores=normalize,
    )
    return ReciprocalRankFusion(config)


def create_semantic_rrf_merger(
    similarity_weight: float = 0.2,
    **kwargs,
) -> SemanticRRF:
    """
    Factory function to create a Semantic RRF merger.

    Args:
        similarity_weight: Weight for semantic similarity (0-1)
        **kwargs: Additional RRFConfig arguments

    Returns:
        Configured SemanticRRF instance
    """
    config = RRFConfig(**kwargs) if kwargs else None
    return SemanticRRF(config, similarity_weight=similarity_weight)

```

---

## backend/autonomous-crawler-service/src/search/tavily.py

```py
"""Tavily Search API provider."""

import httpx
import structlog
from typing import Any, Literal

from src.search.base import SearchProvider, SearchResult

logger = structlog.get_logger(__name__)


class TavilySearchProvider(SearchProvider):
    """
    Tavily Search API client.
    
    Docs: https://docs.tavily.com/docs/tavily-api/rest_api
    """
    
    BASE_URL = "https://api.tavily.com"
    
    def __init__(self, api_key: str, timeout: float = 60.0):
        self.api_key = api_key
        self.timeout = timeout
        self._client: httpx.AsyncClient | None = None
    
    @property
    def name(self) -> str:
        return "tavily"
    
    async def _get_client(self) -> httpx.AsyncClient:
        """Get or create HTTP client."""
        if self._client is None or self._client.is_closed:
            self._client = httpx.AsyncClient(
                timeout=self.timeout,
                headers={"Content-Type": "application/json"},
            )
        return self._client
    
    async def close(self) -> None:
        """Close the HTTP client."""
        if self._client and not self._client.is_closed:
            await self._client.aclose()
            self._client = None
    
    async def search(
        self,
        query: str,
        max_results: int = 10,
        search_depth: Literal["basic", "advanced"] = "basic",
        include_domains: list[str] | None = None,
        exclude_domains: list[str] | None = None,
        include_answer: bool = False,
        include_raw_content: bool = False,
        topic: Literal["general", "news"] = "general",
        days: int | None = None,  # For news topic, limit to N days
        **kwargs,
    ) -> list[SearchResult]:
        """
        Execute Tavily search.
        
        Args:
            query: Search query
            max_results: Maximum results (up to 10 for basic, 20 for advanced)
            search_depth: basic (faster) or advanced (more comprehensive)
            include_domains: List of domains to include
            exclude_domains: List of domains to exclude
            include_answer: Include AI-generated answer
            include_raw_content: Include raw HTML content
            topic: general or news
            days: For news, limit to past N days
        """
        client = await self._get_client()
        
        payload: dict[str, Any] = {
            "api_key": self.api_key,
            "query": query,
            "max_results": max_results,
            "search_depth": search_depth,
            "include_answer": include_answer,
            "include_raw_content": include_raw_content,
            "topic": topic,
        }
        
        if include_domains:
            payload["include_domains"] = include_domains
        if exclude_domains:
            payload["exclude_domains"] = exclude_domains
        if days and topic == "news":
            payload["days"] = days
        
        try:
            response = await client.post(
                f"{self.BASE_URL}/search",
                json=payload,
            )
            response.raise_for_status()
            data = response.json()
            
            results: list[SearchResult] = []
            
            for item in data.get("results", []):
                results.append(SearchResult(
                    title=item.get("title", ""),
                    url=item.get("url", ""),
                    snippet=item.get("content", ""),
                    source_provider=self.name,
                    published_date=item.get("published_date"),
                    score=item.get("score"),
                    raw_data={
                        "raw_content": item.get("raw_content"),
                        **item,
                    },
                ))
            
            # Add AI answer if available
            if include_answer and data.get("answer"):
                logger.info(
                    "Tavily AI answer generated",
                    query=query,
                    answer_preview=data["answer"][:100],
                )
            
            logger.info(
                "Tavily search completed",
                query=query,
                results_count=len(results),
                search_depth=search_depth,
            )
            
            return results
            
        except httpx.HTTPStatusError as e:
            logger.error(
                "Tavily search HTTP error",
                query=query,
                status_code=e.response.status_code,
                error=str(e),
            )
            return []
        except Exception as e:
            logger.error(
                "Tavily search failed",
                query=query,
                error=str(e),
            )
            return []
    
    async def search_news(
        self,
        query: str,
        max_results: int = 10,
        days: int = 7,
        **kwargs,
    ) -> list[SearchResult]:
        """Search news specifically."""
        return await self.search(
            query=query,
            max_results=max_results,
            topic="news",
            days=days,
            **kwargs,
        )
    
    async def extract_content(
        self,
        urls: list[str],
    ) -> list[dict[str, Any]]:
        """
        Extract content from URLs using Tavily Extract API.
        
        Args:
            urls: List of URLs to extract content from
            
        Returns:
            List of extracted content dictionaries
        """
        client = await self._get_client()
        
        try:
            response = await client.post(
                f"{self.BASE_URL}/extract",
                json={
                    "api_key": self.api_key,
                    "urls": urls,
                },
            )
            response.raise_for_status()
            data = response.json()
            
            return data.get("results", [])
            
        except Exception as e:
            logger.error("Tavily extract failed", urls=urls, error=str(e))
            return []
    
    async def health_check(self) -> bool:
        """Check API health."""
        try:
            results = await self.search("test", max_results=1)
            return True  # Tavily returns empty for simple queries but API works
        except Exception:
            return False

```

---

## backend/autonomous-crawler-service/src/state/__init__.py

```py
"""
State management module for autonomous-crawler-service.

Provides persistent storage for task results using Redis with in-memory fallback.
"""

from src.state.store import StateStore, get_state_store

__all__ = ["StateStore", "get_state_store"]

```

---

## backend/autonomous-crawler-service/src/state/store.py

```py
"""
State Storage Module for autonomous-crawler-service.

Provides persistent storage for task results using Redis.
Falls back to in-memory storage if Redis is unavailable.

Features:
- Redis backend with configurable TTL
- In-memory fallback for resilience
- Automatic state restoration on startup
- Async-safe with lock protection
"""

import asyncio
import json
from dataclasses import asdict
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional

import structlog

from src.config import get_settings

logger = structlog.get_logger(__name__)


class EnhancedJSONEncoder(json.JSONEncoder):
    """JSON encoder that handles dataclasses, enums, and datetime objects."""

    def default(self, o: Any) -> Any:
        if hasattr(o, "__dataclass_fields__"):
            return asdict(o)
        if hasattr(o, "model_dump"):
            # Pydantic v2 models
            return o.model_dump()
        if hasattr(o, "dict"):
            # Pydantic v1 models
            return o.dict()
        if isinstance(o, Enum):
            return o.value
        if isinstance(o, datetime):
            return o.isoformat()
        return super().default(o)


class StateStore:
    """
    Persistent state storage with Redis backend and in-memory fallback.

    Usage:
        store = StateStore()
        await store.connect()

        # Save task result
        await store.save_task("task-123", task_result)

        # Load task result
        result = await store.load_task("task-123")

        # List recent tasks
        tasks = await store.list_tasks(status="success", limit=10)

        # Cleanup
        await store.disconnect()
    """

    def __init__(self):
        self._redis = None
        self._memory_store: Dict[str, Any] = {}
        self._using_redis = False
        self._lock = asyncio.Lock()
        self._settings = get_settings().redis

    async def connect(self) -> bool:
        """
        Connect to Redis.

        Returns:
            True if Redis connection successful, False if falling back to memory.
        """
        if not self._settings.enabled:
            logger.info("Redis disabled in settings, using in-memory storage")
            return False

        try:
            import redis.asyncio as redis

            self._redis = redis.from_url(
                self._settings.url,
                encoding="utf-8",
                decode_responses=True,
                socket_connect_timeout=self._settings.connection_timeout,
                socket_timeout=self._settings.socket_timeout,
                max_connections=self._settings.max_connections,
                retry_on_timeout=self._settings.retry_on_timeout,
            )

            # Test connection
            await self._redis.ping()
            self._using_redis = True

            logger.info(
                "Connected to Redis",
                url=self._settings.url.split("@")[-1],  # Hide password if present
                prefix=self._settings.prefix,
            )

            # Load existing results from Redis into memory cache
            await self._load_existing_results()

            return True

        except ImportError:
            logger.warning(
                "redis package not installed, falling back to in-memory storage",
                hint="Install with: pip install redis>=5.0.0",
            )
            self._using_redis = False
            return False

        except Exception as e:
            logger.warning(
                "Failed to connect to Redis, falling back to in-memory storage",
                error=str(e),
                url=self._settings.url.split("@")[-1],
            )
            self._using_redis = False
            return False

    async def _load_existing_results(self):
        """Load existing results from Redis into memory on startup."""
        if not self._using_redis or not self._redis:
            return

        try:
            pattern = f"{self._settings.prefix}:task:*"
            cursor = 0
            loaded_count = 0

            while True:
                cursor, keys = await self._redis.scan(cursor, match=pattern, count=100)
                for key in keys:
                    task_id = key.split(":")[-1]
                    task_data = await self._redis.get(key)
                    if task_data:
                        self._memory_store[task_id] = json.loads(task_data)
                        loaded_count += 1

                if cursor == 0:
                    break

            if loaded_count > 0:
                logger.info(
                    "Restored tasks from Redis",
                    count=loaded_count,
                )

        except Exception as e:
            logger.warning(
                "Failed to load existing results from Redis",
                error=str(e),
            )

    async def disconnect(self):
        """Close Redis connection."""
        if self._redis:
            try:
                await self._redis.close()
                logger.info("Disconnected from Redis")
            except Exception as e:
                logger.warning("Error closing Redis connection", error=str(e))
            finally:
                self._redis = None
                self._using_redis = False

    def _key(self, task_id: str) -> str:
        """Generate Redis key for a task."""
        return f"{self._settings.prefix}:task:{task_id}"

    async def save_task(self, task_id: str, task_result: Any) -> bool:
        """
        Save task result to storage.

        Args:
            task_id: Unique task identifier
            task_result: Task result object (dict, dataclass, or Pydantic model)

        Returns:
            True if save successful
        """
        async with self._lock:
            try:
                # Convert to dict if needed
                if hasattr(task_result, "__dataclass_fields__"):
                    data = asdict(task_result)
                elif hasattr(task_result, "model_dump"):
                    data = task_result.model_dump()
                elif hasattr(task_result, "dict"):
                    data = task_result.dict()
                elif isinstance(task_result, dict):
                    data = task_result
                else:
                    data = task_result

                # Serialize to JSON
                data_json = json.dumps(data, cls=EnhancedJSONEncoder)

                # Store in memory cache (for fast reads)
                self._memory_store[task_id] = json.loads(data_json)

                # Store in Redis (for persistence)
                if self._using_redis and self._redis:
                    ttl_seconds = self._settings.result_ttl_hours * 60 * 60
                    await self._redis.set(self._key(task_id), data_json, ex=ttl_seconds)

                logger.debug(
                    "Task saved",
                    task_id=task_id,
                    redis=self._using_redis,
                )
                return True

            except Exception as e:
                logger.error(
                    "Failed to save task",
                    task_id=task_id,
                    error=str(e),
                )
                return False

    async def load_task(self, task_id: str) -> Optional[Dict[str, Any]]:
        """
        Load task result from storage.

        Args:
            task_id: Unique task identifier

        Returns:
            Task result dict or None if not found
        """
        # Check memory cache first (fast path)
        if task_id in self._memory_store:
            return self._memory_store[task_id]

        # Try Redis if available
        if self._using_redis and self._redis:
            try:
                data_json = await self._redis.get(self._key(task_id))
                if data_json:
                    data = json.loads(data_json)
                    # Cache in memory for faster subsequent access
                    self._memory_store[task_id] = data
                    return data
            except Exception as e:
                logger.warning(
                    "Failed to load task from Redis",
                    task_id=task_id,
                    error=str(e),
                )

        return None

    async def delete_task(self, task_id: str) -> bool:
        """
        Delete task result from storage.

        Args:
            task_id: Unique task identifier

        Returns:
            True if deletion successful
        """
        async with self._lock:
            # Remove from memory
            self._memory_store.pop(task_id, None)

            # Remove from Redis
            if self._using_redis and self._redis:
                try:
                    await self._redis.delete(self._key(task_id))
                except Exception as e:
                    logger.warning(
                        "Failed to delete task from Redis",
                        task_id=task_id,
                        error=str(e),
                    )

            logger.debug("Task deleted", task_id=task_id)
            return True

    async def list_tasks(
        self,
        status: Optional[str] = None,
        limit: int = 50,
    ) -> List[Dict[str, Any]]:
        """
        List tasks with optional filtering.

        Args:
            status: Filter by status (e.g., "success", "failed", "timeout")
            limit: Maximum number of tasks to return

        Returns:
            List of task result dicts
        """
        tasks = []
        for task_id, task_data in list(self._memory_store.items())[-limit * 2 :]:
            if status and task_data.get("status", "").lower() != status.lower():
                continue
            tasks.append(task_data)
            if len(tasks) >= limit:
                break

        return tasks

    async def get_stats(self) -> Dict[str, Any]:
        """
        Get storage statistics.

        Returns:
            Dict with storage stats
        """
        stats = {
            "using_redis": self._using_redis,
            "memory_count": len(self._memory_store),
            "redis_url": self._settings.url.split("@")[-1] if self._using_redis else None,
            "ttl_hours": self._settings.result_ttl_hours,
        }

        if self._using_redis and self._redis:
            try:
                pattern = f"{self._settings.prefix}:task:*"
                cursor = 0
                redis_count = 0
                while True:
                    cursor, keys = await self._redis.scan(cursor, match=pattern, count=100)
                    redis_count += len(keys)
                    if cursor == 0:
                        break
                stats["redis_count"] = redis_count
            except Exception:
                stats["redis_count"] = "unknown"

        return stats

    @property
    def is_redis_connected(self) -> bool:
        """Check if Redis is connected."""
        return self._using_redis

    @property
    def task_count(self) -> int:
        """Get number of tasks in memory cache."""
        return len(self._memory_store)

    def get_memory_store(self) -> Dict[str, Any]:
        """Get direct access to memory store (for backward compatibility)."""
        return self._memory_store


# Singleton instance
_store: Optional[StateStore] = None


async def get_state_store() -> StateStore:
    """
    Get or create the singleton StateStore instance.

    This ensures only one StateStore exists throughout the application lifecycle.

    Returns:
        Initialized StateStore instance
    """
    global _store
    if _store is None:
        _store = StateStore()
        await _store.connect()
    return _store


async def close_state_store():
    """Close the singleton StateStore instance."""
    global _store
    if _store is not None:
        await _store.disconnect()
        _store = None

```

---

## backend/data-collection-service/build.gradle.kts

```kts
// Collector Service   

plugins {
    java
    id("org.springframework.boot")
    id("io.spring.dependency-management")
}

dependencies {
    // Spring Boot Web
    implementation("org.springframework.boot:spring-boot-starter-web")
    
    // Spring Security
    implementation("org.springframework.boot:spring-boot-starter-security")
    
    // JWT Support
    implementation("io.jsonwebtoken:jjwt-api:0.12.3")
    runtimeOnly("io.jsonwebtoken:jjwt-impl:0.12.3")
    runtimeOnly("io.jsonwebtoken:jjwt-jackson:0.12.3")
    
    // OpenAPI / Swagger
    implementation("org.springdoc:springdoc-openapi-starter-webmvc-ui:2.3.0")
    
    // Spring Data JPA
    implementation("org.springframework.boot:spring-boot-starter-data-jpa")
    
    // Redis
    implementation("org.springframework.boot:spring-boot-starter-data-redis")
    
    // WebClient for HTTP calls (web-crawler )
    implementation("org.springframework.boot:spring-boot-starter-webflux")
    
    // Database
    runtimeOnly("org.postgresql:postgresql")
    
    // Database Migration ()
    implementation("org.flywaydb:flyway-core:10.4.1")
    implementation("org.flywaydb:flyway-database-postgresql:10.4.1")
    
    // JSON Processing
    implementation("com.fasterxml.jackson.dataformat:jackson-dataformat-xml")
    implementation("com.fasterxml.jackson.datatype:jackson-datatype-jsr310")
    
    // HTML Parsing ( )
    implementation("org.jsoup:jsoup:1.17.1")
    
    // MongoDB (AI  )
    implementation("org.springframework.boot:spring-boot-starter-data-mongodb")
    
    // RSS Feed Parsing
    implementation("com.rometools:rome:2.1.0")
    
    // Async Support
    implementation("org.springframework.boot:spring-boot-starter-aop")
    
    // Kafka (for AI integration and asynchronous messaging)
    implementation("org.springframework.kafka:spring-kafka")
    
    // PDF Generation - iText 7 (AGPL License)
    implementation("com.itextpdf:itext7-core:8.0.2")
    implementation("com.itextpdf:html2pdf:5.0.2")
    
    // Chart Generation - JFreeChart for server-side charts
    implementation("org.jfree:jfreechart:1.5.4")
    
    // Spring Boot Actuator (Health Check, Metrics)
    implementation("org.springframework.boot:spring-boot-starter-actuator")
    
    // Micrometer for Prometheus metrics
    implementation("io.micrometer:micrometer-registry-prometheus")
    
    // Caffeine Cache (Local cache fallback)
    implementation("com.github.ben-manes.caffeine:caffeine:3.1.8")
    
    // Spring Retry ( )
    implementation("org.springframework.retry:spring-retry")
    
    // Test
    testImplementation("com.h2database:h2")
    testImplementation("org.testcontainers:testcontainers:1.19.3")
    testImplementation("org.testcontainers:postgresql:1.19.3")
    testImplementation("org.testcontainers:junit-jupiter:1.19.3")
    testImplementation("org.testcontainers:mongodb:1.19.3")
}

tasks.named<org.springframework.boot.gradle.tasks.bundling.BootJar>("bootJar") {
    archiveBaseName.set("collector-service")
    archiveVersion.set("1.0.0")
}

```

---

## backend/data-collection-service/maigret-worker/main.py

```py
import asyncio
import json
import logging
import os
import subprocess
import tempfile
import uuid
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from contextlib import asynccontextmanager
import sys

from fastapi import FastAPI, HTTPException, BackgroundTasks, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

# Add shared module to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

# Try to import proxy client
try:
    from shared.proxy_client import ProxyRotationClient, ProxyInfo

    PROXY_CLIENT_AVAILABLE = True
except ImportError:
    PROXY_CLIENT_AVAILABLE = False
    ProxyRotationClient = None
    ProxyInfo = None

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


# --- Configuration ---
class AppConfig:
    """Application configuration from environment variables."""

    PORT = int(os.getenv("PORT", "8020"))
    LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")

    # Maigret configuration
    MAX_CONCURRENT_SCANS = int(os.getenv("MAX_CONCURRENT_SCANS", "3"))
    SCAN_TIMEOUT_SEC = int(os.getenv("SCAN_TIMEOUT_SEC", "300"))  # 5 minutes default
    REQUEST_DELAY_MS = int(os.getenv("REQUEST_DELAY_MS", "100"))

    # Output directory for reports
    REPORTS_DIR = Path(os.getenv("REPORTS_DIR", "/app/reports"))

    # Proxy configuration (optional)
    PROXY_URL = os.getenv("PROXY_URL", None)
    USE_TOR = os.getenv("USE_TOR", "false").lower() == "true"

    # Site filtering
    TOP_SITES_ONLY = os.getenv("TOP_SITES_ONLY", "false").lower() == "true"
    MAX_SITES = int(os.getenv("MAX_SITES", "500"))


config = AppConfig()

# Ensure reports directory exists
config.REPORTS_DIR.mkdir(parents=True, exist_ok=True)

# Semaphore for concurrent scan control
scan_semaphore = asyncio.Semaphore(config.MAX_CONCURRENT_SCANS)

# Proxy rotation configuration
USE_PROXY_ROTATION = os.getenv("USE_PROXY_ROTATION", "true").lower() == "true"
PROXY_ROTATION_URL = os.getenv("PROXY_ROTATION_URL", "http://ip-rotation:8050")

# Initialize proxy client (if available)
proxy_client = None
if PROXY_CLIENT_AVAILABLE and USE_PROXY_ROTATION:
    proxy_client = ProxyRotationClient(
        base_url=PROXY_ROTATION_URL,
        timeout=5.0,
        enabled=True,
    )
    logger.info(f"Proxy rotation enabled, connecting to {PROXY_ROTATION_URL}")

# Proxy rotation configuration
USE_PROXY_ROTATION = os.getenv("USE_PROXY_ROTATION", "true").lower() == "true"
PROXY_ROTATION_URL = os.getenv("PROXY_ROTATION_URL", "http://ip-rotation:8050")

# Initialize proxy client (if available)
proxy_client: "ProxyRotationClient | None" = None
if PROXY_CLIENT_AVAILABLE and USE_PROXY_ROTATION:
    proxy_client = ProxyRotationClient(
        base_url=PROXY_ROTATION_URL,
        timeout=5.0,
        enabled=True,
    )
    logger.info(f"Proxy rotation enabled, connecting to {PROXY_ROTATION_URL}")


# --- Enums and Models ---
class ScanStatus(str, Enum):
    PENDING = "PENDING"
    RUNNING = "RUNNING"
    COMPLETED = "COMPLETED"
    FAILED = "FAILED"
    TIMEOUT = "TIMEOUT"


class ScanRequest(BaseModel):
    """Request model for username scan."""

    username: str = Field(
        ..., min_length=1, max_length=100, description="Username to scan"
    )
    options: Optional[Dict[str, Any]] = Field(
        default=None, description="Additional Maigret options"
    )
    timeout_sec: Optional[int] = Field(
        default=None, ge=30, le=600, description="Scan timeout in seconds (30-600)"
    )
    top_sites_only: Optional[bool] = Field(
        default=None, description="Only scan top/popular sites for faster results"
    )


class AccountInfo(BaseModel):
    """Information about a discovered account."""

    site_name: str
    url: str
    username: str
    status: str = "claimed"
    tags: List[str] = Field(default_factory=list)


class ScanSummary(BaseModel):
    """Summary of scan results."""

    total_sites_checked: int
    accounts_found: int
    accounts_claimed: int
    accounts_available: int
    accounts_error: int
    scan_duration_ms: int


class ScanResult(BaseModel):
    """Complete scan result."""

    scan_id: str
    username: str
    status: ScanStatus
    summary: Optional[ScanSummary] = None
    accounts: List[AccountInfo] = Field(default_factory=list)
    raw_json_path: Optional[str] = None
    error_message: Optional[str] = None
    started_at: Optional[str] = None
    completed_at: Optional[str] = None


class ScanResponse(BaseModel):
    """Response model for scan endpoint."""

    status: str = "ok"
    scan_id: str
    message: str
    result: Optional[ScanResult] = None


class HealthResponse(BaseModel):
    """Health check response."""

    status: str
    version: str
    maigret_available: bool
    active_scans: int
    max_concurrent_scans: int
    proxy_rotation_enabled: bool = False
    proxy_service_healthy: bool = False


# --- In-memory scan tracking ---
# In production, this should be replaced with Redis or a database
active_scans: Dict[str, ScanResult] = {}


# --- Helper Functions ---
def sanitize_username(username: str) -> str:
    """Sanitize username to prevent command injection."""
    # Remove any shell-dangerous characters
    import re

    # Only allow alphanumeric, underscore, dash, dot
    sanitized = re.sub(r"[^a-zA-Z0-9_\-.]", "", username)
    if not sanitized:
        raise ValueError("Invalid username after sanitization")
    return sanitized


def build_maigret_command(
    username: str,
    output_dir: Path,
    options: Optional[Dict] = None,
    proxy_url: Optional[str] = None,
) -> List[str]:
    """Build Maigret CLI command with options.

    Args:
        username: Username to scan
        output_dir: Directory for output files
        options: Additional scan options
        proxy_url: Optional proxy URL from rotation service (takes priority)
    """
    cmd = [
        "maigret",
        username,
        "--json",
        "simple",
        "--folderoutput",
        str(output_dir),
    ]

    # Add timeout per site
    cmd.extend(["--timeout", "10"])

    # Proxy configuration (rotation proxy takes priority)
    if proxy_url:
        # Determine proxy type from URL
        if proxy_url.startswith("socks"):
            cmd.extend(["--tor-proxy", proxy_url])
        else:
            cmd.extend(["--proxy", proxy_url])
    elif config.PROXY_URL:
        cmd.extend(["--proxy", config.PROXY_URL])
    elif config.USE_TOR:
        cmd.extend(["--tor-proxy", "socks5://127.0.0.1:9050"])

    # Top sites only for faster scanning
    top_sites = (
        options.get("top_sites_only", config.TOP_SITES_ONLY)
        if options
        else config.TOP_SITES_ONLY
    )
    if top_sites:
        cmd.extend(["--top-sites", "50"])

    # Limit number of sites
    max_sites = (
        options.get("max_sites", config.MAX_SITES) if options else config.MAX_SITES
    )
    if max_sites and max_sites < 500:
        cmd.extend(["--top-sites", str(max_sites)])

    # No recursive search for basic scans
    cmd.append("--no-recursion")

    # Suppress color output and progress bar for cleaner parsing
    cmd.append("--no-color")
    cmd.append("--no-progressbar")

    return cmd


async def get_proxy_for_scan() -> tuple:
    """
    Get a proxy from the rotation service for scanning.

    Returns:
        Tuple of (proxy_url, proxy_id) or (None, None) if unavailable
    """
    if not proxy_client:
        return None, None

    try:
        proxy_info = await proxy_client.get_next_proxy()
        if proxy_info:
            return proxy_info.get_proxy_url(), proxy_info.id
    except Exception as e:
        logger.warning(f"Failed to get proxy from rotation service: {e}")

    return None, None


async def record_proxy_result(
    proxy_id: Optional[str], success: bool, latency_ms: int = 0, error: str = ""
):
    """Record the result of a proxy-enabled scan."""
    if not proxy_id or not proxy_client:
        return

    try:
        if success:
            await proxy_client.record_success(proxy_id, latency_ms)
        else:
            await proxy_client.record_failure(proxy_id, error[:200])
    except Exception as e:
        logger.debug(f"Failed to record proxy result: {e}")


def parse_maigret_json(json_path: Path) -> Dict[str, Any]:
    """Parse Maigret JSON output file."""
    try:
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data
    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse Maigret JSON: {e}")
        return {}
    except FileNotFoundError:
        logger.error(f"Maigret output file not found: {json_path}")
        return {}


def extract_accounts_from_result(
    raw_result: Dict[str, Any], username: str
) -> List[AccountInfo]:
    """Extract account information from Maigret JSON result."""
    accounts = []

    # Maigret JSON structure: { "SiteName": { "status": { "status": "Claimed" }, "url_user": "..." } }
    if isinstance(raw_result, dict):
        for site_name, site_data in raw_result.items():
            if isinstance(site_data, dict):
                # Get status from nested structure
                status_obj = site_data.get("status", {})
                if isinstance(status_obj, dict):
                    status = status_obj.get("status", "Unknown")
                    tags = status_obj.get("tags", [])
                else:
                    status = str(status_obj)
                    tags = []

                url = site_data.get("url_user", site_data.get("url", ""))

                # Only include claimed/found accounts
                if isinstance(status, str) and status.lower() in [
                    "claimed",
                    "found",
                    "detected",
                ]:
                    accounts.append(
                        AccountInfo(
                            site_name=site_name,
                            url=url or f"https://{site_name.lower()}.com/{username}",
                            username=username,
                            status="claimed",
                            tags=tags if isinstance(tags, list) else [],
                        )
                    )

    # Format 2: Array of results (fallback)
    elif isinstance(raw_result, list):
        for item in raw_result:
            if isinstance(item, dict):
                status_obj = item.get("status", {})
                if isinstance(status_obj, dict):
                    status = status_obj.get("status", "Unknown")
                else:
                    status = str(status_obj)

                if isinstance(status, str) and status.lower() in ["claimed", "found"]:
                    accounts.append(
                        AccountInfo(
                            site_name=item.get("site", item.get("name", "Unknown")),
                            url=item.get("url", ""),
                            username=username,
                            status="claimed",
                            tags=item.get("tags", []),
                        )
                    )

    return accounts


async def run_maigret_scan(
    scan_id: str, username: str, options: Optional[Dict], timeout: int
) -> ScanResult:
    """Execute Maigret scan as subprocess with proxy rotation support."""
    start_time = datetime.now()
    result = active_scans[scan_id]
    result.status = ScanStatus.RUNNING
    result.started_at = start_time.isoformat()

    # Create output directory for this scan
    scan_output_dir = config.REPORTS_DIR / f"scan_{scan_id}"
    scan_output_dir.mkdir(parents=True, exist_ok=True)

    # Get proxy from rotation service
    proxy_url, proxy_id = await get_proxy_for_scan()
    if proxy_id:
        logger.info(f"[{scan_id}] Using rotating proxy: {proxy_id}")

    try:
        # Sanitize username
        safe_username = sanitize_username(username)

        # Build command with optional proxy
        cmd = build_maigret_command(safe_username, scan_output_dir, options, proxy_url)
        logger.info(f"[{scan_id}] Running Maigret: {' '.join(cmd)}")

        # Run Maigret with timeout
        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
            cwd=str(scan_output_dir),
        )

        try:
            stdout, stderr = await asyncio.wait_for(
                process.communicate(), timeout=timeout
            )
        except asyncio.TimeoutError:
            process.kill()
            await process.wait()
            # Record proxy failure on timeout
            duration_ms = int((datetime.now() - start_time).total_seconds() * 1000)
            await record_proxy_result(proxy_id, False, duration_ms, "Scan timeout")
            result.status = ScanStatus.TIMEOUT
            result.error_message = f"Scan timed out after {timeout} seconds"
            result.completed_at = datetime.now().isoformat()
            return result

        # Log output for debugging
        stdout_text = stdout.decode("utf-8", errors="replace")
        stderr_text = stderr.decode("utf-8", errors="replace")
        if stdout_text:
            logger.info(f"[{scan_id}] Maigret stdout: {stdout_text[:500]}")
        if stderr_text:
            logger.warning(f"[{scan_id}] Maigret stderr: {stderr_text[:500]}")

        # Check for errors (but Maigret may exit non-zero even on partial success)
        if process.returncode != 0 and not any(scan_output_dir.glob("*.json")):
            logger.error(f"[{scan_id}] Maigret failed with no output: {stderr_text}")
            duration_ms = int((datetime.now() - start_time).total_seconds() * 1000)
            await record_proxy_result(proxy_id, False, duration_ms, stderr_text[:100])
            result.status = ScanStatus.FAILED
            result.error_message = (
                f"Maigret exited with code {process.returncode}: {stderr_text[:500]}"
            )
            result.completed_at = datetime.now().isoformat()
            return result

        # Find JSON output file - Maigret creates files like report_<username>_simple.json
        json_files = list(scan_output_dir.glob("*.json"))

        if json_files:
            output_path = json_files[0]  # Take the first JSON file
            logger.info(f"[{scan_id}] Found output file: {output_path}")

            raw_result = parse_maigret_json(output_path)
            accounts = extract_accounts_from_result(raw_result, safe_username)

            # Calculate summary
            end_time = datetime.now()
            duration_ms = int((end_time - start_time).total_seconds() * 1000)

            # Record proxy success
            await record_proxy_result(proxy_id, True, duration_ms)

            # Count stats from raw result
            total_checked = len(raw_result) if isinstance(raw_result, dict) else 0
            claimed = len(accounts)

            result.summary = ScanSummary(
                total_sites_checked=total_checked,
                accounts_found=claimed,
                accounts_claimed=claimed,
                accounts_available=0,
                accounts_error=0,
                scan_duration_ms=duration_ms,
            )
            result.accounts = accounts
            result.raw_json_path = str(output_path)
            result.status = ScanStatus.COMPLETED
            result.completed_at = end_time.isoformat()

            logger.info(
                f"[{scan_id}] Scan completed: {claimed} accounts found in {duration_ms}ms"
            )
        else:
            duration_ms = int((datetime.now() - start_time).total_seconds() * 1000)
            await record_proxy_result(proxy_id, False, duration_ms, "No output file")
            result.status = ScanStatus.FAILED
            result.error_message = "Maigret did not produce output file"
            result.completed_at = datetime.now().isoformat()

    except ValueError as e:
        duration_ms = int((datetime.now() - start_time).total_seconds() * 1000)
        await record_proxy_result(proxy_id, False, duration_ms, str(e))
        result.status = ScanStatus.FAILED
        result.error_message = f"Invalid input: {str(e)}"
        result.completed_at = datetime.now().isoformat()
    except Exception as e:
        logger.exception(f"[{scan_id}] Unexpected error during scan")
        duration_ms = int((datetime.now() - start_time).total_seconds() * 1000)
        await record_proxy_result(proxy_id, False, duration_ms, str(e)[:100])
        result.status = ScanStatus.FAILED
        result.error_message = f"Unexpected error: {str(e)}"
        result.completed_at = datetime.now().isoformat()

    return result


async def execute_scan_with_semaphore(
    scan_id: str, username: str, options: Optional[Dict], timeout: int
):
    """Execute scan with concurrency control."""
    async with scan_semaphore:
        await run_maigret_scan(scan_id, username, options, timeout)


# --- Lifespan Management ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan handler."""
    logger.info("Maigret Worker service starting up...")
    logger.info(f"Max concurrent scans: {config.MAX_CONCURRENT_SCANS}")
    logger.info(f"Default timeout: {config.SCAN_TIMEOUT_SEC}s")
    logger.info(f"Reports directory: {config.REPORTS_DIR}")
    logger.info(f"Proxy rotation enabled: {USE_PROXY_ROTATION}")

    # Verify Maigret is installed
    try:
        result = subprocess.run(
            ["maigret", "--version"], capture_output=True, text=True, timeout=10
        )
        logger.info(f"Maigret version: {result.stdout.strip()}")
    except Exception as e:
        logger.warning(f"Could not verify Maigret installation: {e}")

    yield

    # Cleanup proxy client on shutdown
    if proxy_client:
        await proxy_client.close()
    logger.info("Maigret Worker service shutting down...")


# --- FastAPI App ---
app = FastAPI(
    title="Maigret OSINT Username Scanner",
    description="Performs username OSINT scanning using Maigret for social media account discovery",
    version="1.0.0",
    lifespan=lifespan,
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# --- API Endpoints ---
@app.get("/health", response_model=HealthResponse)
@app.head("/health")
async def health_check():
    """Health check endpoint."""
    # Check if Maigret is available
    maigret_available = False
    try:
        result = subprocess.run(
            ["maigret", "--version"], capture_output=True, timeout=5
        )
        maigret_available = result.returncode == 0
    except Exception:
        pass

    # Count active scans
    active_count = sum(
        1
        for s in active_scans.values()
        if s.status in [ScanStatus.PENDING, ScanStatus.RUNNING]
    )

    # Check proxy service health
    proxy_healthy = False
    if proxy_client:
        proxy_healthy = await proxy_client.health_check()

    return HealthResponse(
        status="ok" if maigret_available else "degraded",
        version="1.1.0",
        maigret_available=maigret_available,
        active_scans=active_count,
        max_concurrent_scans=config.MAX_CONCURRENT_SCANS,
        proxy_rotation_enabled=USE_PROXY_ROTATION,
        proxy_service_healthy=proxy_healthy,
    )


@app.post("/scan", response_model=ScanResponse)
async def start_scan(
    request: ScanRequest, background_tasks: BackgroundTasks, req: Request
):
    """
    Start a username OSINT scan using Maigret.

    The scan runs asynchronously. Use GET /scan/{scan_id} to check status and results.
    """
    trace_id = req.headers.get("X-Trace-Id", req.headers.get("X-Request-Id", "unknown"))

    # Generate scan ID
    scan_id = str(uuid.uuid4())

    logger.info(
        f"[{trace_id}] Starting scan {scan_id} for username: {request.username}"
    )

    # Validate username early
    try:
        sanitize_username(request.username)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))

    # Determine timeout
    timeout = request.timeout_sec or config.SCAN_TIMEOUT_SEC

    # Build options
    options = request.options or {}
    if request.top_sites_only is not None:
        options["top_sites_only"] = request.top_sites_only

    # Create initial scan result
    result = ScanResult(
        scan_id=scan_id, username=request.username, status=ScanStatus.PENDING
    )
    active_scans[scan_id] = result

    # Start scan in background
    background_tasks.add_task(
        execute_scan_with_semaphore, scan_id, request.username, options, timeout
    )

    return ScanResponse(
        status="ok",
        scan_id=scan_id,
        message=f"Scan started for username '{request.username}'. Use GET /scan/{scan_id} to check status.",
        result=result,
    )


@app.post("/scan/sync", response_model=ScanResponse)
async def run_scan_sync(request: ScanRequest, req: Request):
    """
    Run a username scan synchronously and wait for results.

    This endpoint blocks until the scan completes or times out.
    Recommended for shorter scans (use top_sites_only=true for faster results).
    """
    trace_id = req.headers.get("X-Trace-Id", req.headers.get("X-Request-Id", "unknown"))

    scan_id = str(uuid.uuid4())
    logger.info(
        f"[{trace_id}] Starting sync scan {scan_id} for username: {request.username}"
    )

    # Validate username
    try:
        sanitize_username(request.username)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))

    # Determine timeout
    timeout = request.timeout_sec or config.SCAN_TIMEOUT_SEC

    # Build options - default to top_sites for sync
    options = request.options or {}
    if request.top_sites_only is not None:
        options["top_sites_only"] = request.top_sites_only
    else:
        options["top_sites_only"] = True  # Default to top sites for sync requests

    # Create scan result
    result = ScanResult(
        scan_id=scan_id, username=request.username, status=ScanStatus.PENDING
    )
    active_scans[scan_id] = result

    # Run scan with semaphore
    async with scan_semaphore:
        result = await run_maigret_scan(scan_id, request.username, options, timeout)

    if result.status == ScanStatus.COMPLETED:
        return ScanResponse(
            status="ok",
            scan_id=scan_id,
            message=f"Scan completed. Found {len(result.accounts)} accounts.",
            result=result,
        )
    else:
        return ScanResponse(
            status="error",
            scan_id=scan_id,
            message=result.error_message or "Scan failed",
            result=result,
        )


@app.get("/scan/{scan_id}", response_model=ScanResponse)
async def get_scan_status(scan_id: str):
    """Get the status and results of a scan."""
    result = active_scans.get(scan_id)

    if not result:
        raise HTTPException(status_code=404, detail=f"Scan not found: {scan_id}")

    return ScanResponse(
        status="ok",
        scan_id=scan_id,
        message=f"Scan status: {result.status.value}",
        result=result,
    )


@app.get("/scans", response_model=Dict[str, Any])
async def list_scans(status: Optional[ScanStatus] = None, limit: int = 50):
    """List recent scans, optionally filtered by status."""
    scans = list(active_scans.values())

    if status:
        scans = [s for s in scans if s.status == status]

    # Sort by started_at descending
    scans.sort(key=lambda x: x.started_at or "", reverse=True)

    return {"total": len(scans), "scans": scans[:limit]}


@app.delete("/scan/{scan_id}")
async def delete_scan(scan_id: str):
    """Delete a completed scan and its report file."""
    result = active_scans.get(scan_id)

    if not result:
        raise HTTPException(status_code=404, detail=f"Scan not found: {scan_id}")

    if result.status in [ScanStatus.PENDING, ScanStatus.RUNNING]:
        raise HTTPException(status_code=400, detail="Cannot delete a running scan")

    # Delete report file if exists
    if result.raw_json_path:
        try:
            Path(result.raw_json_path).unlink(missing_ok=True)
        except Exception as e:
            logger.warning(f"Failed to delete report file: {e}")

    del active_scans[scan_id]

    return {"status": "ok", "message": f"Scan {scan_id} deleted"}


@app.get("/report/{scan_id}")
async def get_raw_report(scan_id: str):
    """Get the raw JSON report for a completed scan."""
    result = active_scans.get(scan_id)

    if not result:
        raise HTTPException(status_code=404, detail=f"Scan not found: {scan_id}")

    if result.status != ScanStatus.COMPLETED:
        raise HTTPException(
            status_code=400, detail=f"Scan not completed: {result.status}"
        )

    if not result.raw_json_path or not Path(result.raw_json_path).exists():
        raise HTTPException(status_code=404, detail="Report file not found")

    with open(result.raw_json_path, "r", encoding="utf-8") as f:
        return json.load(f)


@app.get("/proxy/stats")
async def get_proxy_stats():
    """Get proxy pool statistics."""
    if not proxy_client:
        return {"error": "Proxy rotation not enabled", "enabled": False}

    stats = await proxy_client.get_pool_stats()
    return stats or {"error": "Failed to get stats", "enabled": True}


# --- Main Entry Point ---
if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=config.PORT,
        reload=os.getenv("ENV", "production") == "development",
    )

```

---

## backend/data-collection-service/maigret-worker/state_store.py

```py
"""
State Storage Module for maigret-worker Service

Provides persistent storage for OSINT scan results using Redis.
Falls back to in-memory storage if Redis is unavailable.
"""

import os
import json
import asyncio
from datetime import datetime
from typing import Optional, Dict, Any, List
from dataclasses import asdict
from enum import Enum

# Redis configuration from environment
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/3")
REDIS_PREFIX = os.getenv("REDIS_PREFIX", "maigret_worker")
SCAN_TTL_DAYS = int(os.getenv("SCAN_TTL_DAYS", "7"))  # Scans expire after 7 days


class EnhancedJSONEncoder(json.JSONEncoder):
    """JSON encoder that handles dataclasses and enums."""
    
    def default(self, o: Any) -> Any:
        if hasattr(o, '__dataclass_fields__'):
            return asdict(o)
        if isinstance(o, Enum):
            return o.value
        if isinstance(o, datetime):
            return o.isoformat()
        return super().default(o)


class StateStore:
    """Persistent state storage with Redis backend."""
    
    def __init__(self):
        self._redis = None
        self._memory_store: Dict[str, Any] = {}
        self._using_redis = False
        self._lock = asyncio.Lock()
    
    async def connect(self) -> bool:
        """Connect to Redis."""
        try:
            import redis.asyncio as redis
            
            self._redis = redis.from_url(
                REDIS_URL,
                encoding="utf-8",
                decode_responses=True,
                socket_connect_timeout=5,
                socket_timeout=5,
            )
            
            await self._redis.ping()
            self._using_redis = True
            
            # Load existing scans from Redis
            await self._load_existing_scans()
            
            return True
            
        except ImportError:
            self._using_redis = False
            return False
        except Exception:
            self._using_redis = False
            return False
    
    async def _load_existing_scans(self):
        """Load existing scans from Redis into memory on startup."""
        if not self._using_redis or not self._redis:
            return
        
        try:
            pattern = f"{REDIS_PREFIX}:scan:*"
            cursor = 0
            
            while True:
                cursor, keys = await self._redis.scan(cursor, match=pattern, count=100)
                for key in keys:
                    scan_id = key.split(":")[-1]
                    scan_data = await self._redis.get(key)
                    if scan_data:
                        self._memory_store[scan_id] = json.loads(scan_data)
                
                if cursor == 0:
                    break
        except Exception:
            pass
    
    async def disconnect(self):
        """Close Redis connection."""
        if self._redis:
            await self._redis.close()
            self._redis = None
            self._using_redis = False
    
    def _key(self, scan_id: str) -> str:
        """Generate Redis key."""
        return f"{REDIS_PREFIX}:scan:{scan_id}"
    
    async def save_scan(self, scan_id: str, scan_result: Any) -> bool:
        """Save scan result."""
        async with self._lock:
            try:
                # Convert to dict if needed
                if hasattr(scan_result, '__dataclass_fields__'):
                    data = asdict(scan_result)
                elif hasattr(scan_result, 'model_dump'):
                    data = scan_result.model_dump()
                elif hasattr(scan_result, 'dict'):
                    data = scan_result.dict()
                elif isinstance(scan_result, dict):
                    data = scan_result
                else:
                    data = scan_result
                
                data_json = json.dumps(data, cls=EnhancedJSONEncoder)
                self._memory_store[scan_id] = json.loads(data_json)
                
                if self._using_redis and self._redis:
                    ttl_seconds = SCAN_TTL_DAYS * 24 * 60 * 60
                    await self._redis.set(self._key(scan_id), data_json, ex=ttl_seconds)
                
                return True
            except Exception:
                return False
    
    async def load_scan(self, scan_id: str) -> Optional[Dict[str, Any]]:
        """Load scan result."""
        if scan_id in self._memory_store:
            return self._memory_store[scan_id]
        
        if self._using_redis and self._redis:
            try:
                data_json = await self._redis.get(self._key(scan_id))
                if data_json:
                    data = json.loads(data_json)
                    self._memory_store[scan_id] = data
                    return data
            except Exception:
                pass
        
        return None
    
    async def delete_scan(self, scan_id: str) -> bool:
        """Delete scan result."""
        async with self._lock:
            self._memory_store.pop(scan_id, None)
            if self._using_redis and self._redis:
                try:
                    await self._redis.delete(self._key(scan_id))
                except Exception:
                    pass
            return True
    
    async def list_scans(self, status: Optional[str] = None, limit: int = 50) -> List[Dict[str, Any]]:
        """List scans with optional filtering."""
        scans = []
        for scan_id, scan_data in list(self._memory_store.items())[-limit:]:
            if status and scan_data.get('status', '').lower() != status.lower():
                continue
            scans.append(scan_data)
        return scans
    
    @property
    def is_redis_connected(self) -> bool:
        return self._using_redis
    
    def get_memory_store(self) -> Dict[str, Any]:
        return self._memory_store


# Singleton instance
_store: Optional[StateStore] = None


async def get_state_store() -> StateStore:
    """Get or create the singleton StateStore instance."""
    global _store
    if _store is None:
        _store = StateStore()
        await _store.connect()
    return _store

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/CollectorApplication.java

```java
package com.newsinsight.collector;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.client.discovery.EnableDiscoveryClient;
import org.springframework.scheduling.annotation.EnableAsync;
import org.springframework.scheduling.annotation.EnableScheduling;

/**
 * NewsInsight Collector Service Application
 * 
 * Spring Boot    
 * -  (RSS, Web Scraping, API)  
 * -     
 * - Consul      
 * - PostgreSQL    
 */
@SpringBootApplication
@EnableDiscoveryClient
@EnableAsync
@EnableScheduling
public class CollectorApplication {

    public static void main(String[] args) {
        SpringApplication.run(CollectorApplication.class, args);
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/client/AIDoveClient.java

```java
package com.newsinsight.collector.client;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import io.netty.channel.ChannelOption;
import io.netty.handler.timeout.ReadTimeoutHandler;
import io.netty.handler.timeout.WriteTimeoutHandler;
import jakarta.annotation.PostConstruct;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.MediaType;
import org.springframework.http.client.reactive.ReactorClientHttpConnector;
import org.springframework.stereotype.Component;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Mono;
import reactor.netty.http.client.HttpClient;

import java.time.Duration;
import java.util.Map;
import java.util.concurrent.TimeUnit;

/**
 * Client for AI Dove Agent API.
 * Provides AI-powered text analysis using the self-healing AI service.
 * 
 * API Endpoint: Configurable via COLLECTOR_AIDOVE_BASE_URL or Consul KV
 * 
 * Request:
 *   - chatInput: string (required) - The message/prompt
 *   - sessionId: string (optional) - Session ID for context continuity
 * 
 * Response:
 *   - reply: string - AI response
 *   - tokens_used: integer - Tokens consumed
 *   - model: string - Model used for generation
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class AIDoveClient {

    private final ObjectMapper objectMapper;

    @Value("${collector.ai-dove.base-url:${collector.aidove.base-url:${COLLECTOR_AIDOVE_BASE_URL:https://workflow.nodove.com/webhook/aidove}}}")
    private String baseUrl;

    @Value("${collector.ai-dove.timeout-seconds:${collector.aidove.timeout-seconds:180}}")
    private int timeoutSeconds;

    @Value("${collector.ai-dove.enabled:${collector.aidove.enabled:true}}")
    private boolean enabled;

    private WebClient aiDoveWebClient;

    @PostConstruct
    public void init() {
        // Create dedicated WebClient with extended timeout for AI operations
        HttpClient httpClient = HttpClient.create()
                .option(ChannelOption.CONNECT_TIMEOUT_MILLIS, 30000) // 30s connect timeout
                .responseTimeout(Duration.ofSeconds(timeoutSeconds))
                .doOnConnected(conn -> 
                    conn.addHandlerLast(new ReadTimeoutHandler(timeoutSeconds, TimeUnit.SECONDS))
                        .addHandlerLast(new WriteTimeoutHandler(timeoutSeconds, TimeUnit.SECONDS))
                )
                .followRedirect(true);

        this.aiDoveWebClient = WebClient.builder()
                .clientConnector(new ReactorClientHttpConnector(httpClient))
                .defaultHeader("User-Agent", "NewsInsight-AIDove/1.0")
                .build();
        
        log.info("AIDoveClient initialized with timeout: {}s, baseUrl: {}", timeoutSeconds, baseUrl);
    }

    /**
     * Check if AI Dove client is enabled
     */
    public boolean isEnabled() {
        return enabled;
    }

    /**
     * Send a prompt to AI Dove and get a response.
     * 
     * @param prompt The prompt to send
     * @param sessionId Optional session ID for context continuity
     * @return The AI response
     */
    public Mono<AIDoveResponse> chat(String prompt, String sessionId) {
        if (!enabled) {
            return Mono.error(new IllegalStateException("AI Dove client is disabled"));
        }

        Map<String, Object> payload = sessionId != null
                ? Map.of("chatInput", prompt, "sessionId", sessionId)
                : Map.of("chatInput", prompt);

        return aiDoveWebClient.post()
                .uri(baseUrl)
                .contentType(MediaType.APPLICATION_JSON)
                .bodyValue(payload)
                .retrieve()
                .bodyToMono(String.class)
                .timeout(Duration.ofSeconds(timeoutSeconds))
                .map(this::parseResponse)
                .doOnError(e -> log.error("AI Dove request failed: {}", e.getMessage()));
    }

    /**
     * Stream a response from AI Dove (simulated streaming by splitting response).
     * Note: AI Dove API doesn't support true streaming, so we simulate it.
     */
    public Flux<String> chatStream(String prompt, String sessionId) {
        return chat(prompt, sessionId)
                .flatMapMany(response -> {
                    if (response.reply() == null) {
                        return Flux.empty();
                    }
                    // Split response into chunks for simulated streaming
                    String[] sentences = response.reply().split("(?<=[.!?\\n])\\s*");
                    return Flux.fromArray(sentences)
                            .delayElements(Duration.ofMillis(50));
                })
                .onErrorResume(e -> {
                    log.error("AI Dove stream failed: {}", e.getMessage());
                    return Flux.just("AI    : " + e.getMessage());
                });
    }

    private AIDoveResponse parseResponse(String json) {
        try {
            JsonNode node = objectMapper.readTree(json);
            return new AIDoveResponse(
                    node.has("reply") ? node.get("reply").asText() : null,
                    node.has("tokens_used") ? node.get("tokens_used").asInt() : 0,
                    node.has("model") ? node.get("model").asText() : "unknown"
            );
        } catch (Exception e) {
            log.error("Failed to parse AI Dove response: {}", e.getMessage());
            return new AIDoveResponse(json, 0, "unknown");
        }
    }

    /**
     * AI Dove API response
     */
    public record AIDoveResponse(
            String reply,
            int tokensUsed,
            String model
    ) {}
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/client/Crawl4aiClient.java

```java
package com.newsinsight.collector.client;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.jsoup.Jsoup;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.MediaType;
import org.springframework.stereotype.Component;
import org.springframework.web.reactive.function.client.ClientResponse;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Mono;

import java.net.URI;
import java.time.Duration;

/**
 * Lightweight client for the Crawl4AI service.
 * Tries to call /crawl at the configured base URL and extract text content.
 * If the API responds with JSON, attempts to read common fields like
 * "content", "markdown", "text", or "html". Falls back to plain text.
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class Crawl4aiClient {

    private final WebClient webClient;
    private final ObjectMapper objectMapper;

    @Value("${collector.crawler.base-url:http://web-crawler:11235}")
    private String baseUrl;

    @Value("${collector.http.timeout.read:30000}")
    private int readTimeoutMs;

    @Data
    @Builder
    @AllArgsConstructor
    public static class CrawlResult {
        private String title;
        private String content; // normalized text content
    }

    /**
     * Attempts to crawl the given URL via Crawl4AI. Returns null on failure.
     */
    public CrawlResult crawl(String targetUrl) {
        try {
            String endpoint = baseUrl.endsWith("/") ? baseUrl + "crawl" : baseUrl + "/crawl";

            Mono<CrawlResult> mono = webClient
                    .get()
                    .uri(uriBuilder -> {
                        URI uri = URI.create(endpoint);
                        return uriBuilder
                                .scheme(uri.getScheme())
                                .host(uri.getHost())
                                .port(uri.getPort())
                                .path(uri.getPath())
                                .queryParam("url", targetUrl)
                                .build();
                    })
                    .accept(MediaType.APPLICATION_JSON, MediaType.TEXT_PLAIN, MediaType.ALL)
                    .exchangeToMono(response -> handleResponse(response))
                    .timeout(Duration.ofMillis(Math.max(1000, readTimeoutMs)));

            return mono.onErrorResume(e -> {
                        log.warn("Crawl4AI request failed for {}: {}", targetUrl, e.toString());
                        return Mono.empty();
                    })
                    .block();
        } catch (Exception e) {
            log.warn("Crawl4AI client error for {}: {}", targetUrl, e.toString());
            return null;
        }
    }

    private Mono<CrawlResult> handleResponse(ClientResponse response) {
        MediaType ct = response.headers().contentType().orElse(MediaType.APPLICATION_JSON);
        if (ct.isCompatibleWith(MediaType.APPLICATION_JSON) || ct.getSubtype().contains("json")) {
            return response.bodyToMono(String.class).flatMap(body -> {
                try {
                    JsonNode node = objectMapper.readTree(body);
                    String title = textOf(node, "title");
                    String content = firstNonBlank(
                            textOf(node, "content"),
                            textOf(node, "markdown"),
                            textOf(node, "text"),
                            stripHtml(textOf(node, "html"))
                    );
                    if (isBlank(content)) return Mono.empty();
                    return Mono.just(CrawlResult.builder()
                            .title(title)
                            .content(normalize(content))
                            .build());
                } catch (Exception ex) {
                    log.debug("Failed to parse JSON from Crawl4AI: {}", ex.toString());
                    return Mono.empty();
                }
            });
        } else {
            return response.bodyToMono(String.class).map(body ->
                    CrawlResult.builder()
                            .title(null)
                            .content(normalize(stripHtml(body)))
                            .build()
            );
        }
    }

    private static boolean isBlank(String s) {
        return s == null || s.trim().isEmpty();
    }

    private static String firstNonBlank(String... values) {
        if (values == null) return null;
        for (String v : values) {
            if (!isBlank(v)) return v;
        }
        return null;
    }

    private static String normalize(String s) {
        if (s == null) return null;
        return s.replaceAll("\\s+", " ").trim();
    }

    private static String textOf(JsonNode node, String field) {
        if (node == null || node.isNull()) return null;
        JsonNode v = node.get(field);
        if (v == null || v.isNull()) return null;
        if (v.isTextual()) return v.asText();
        return v.toString();
    }

    private static String stripHtml(String html) {
        if (html == null) return null;
        try {
            return Jsoup.parse(html).text();
        } catch (Exception ignored) {
            return html;
        }
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/client/OpenAICompatibleClient.java

```java
package com.newsinsight.collector.client;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import io.netty.channel.ChannelOption;
import io.netty.handler.timeout.ReadTimeoutHandler;
import io.netty.handler.timeout.WriteTimeoutHandler;
import jakarta.annotation.PostConstruct;
import lombok.Getter;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.HttpHeaders;
import org.springframework.http.MediaType;
import org.springframework.http.client.reactive.ReactorClientHttpConnector;
import org.springframework.stereotype.Component;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Flux;
import reactor.netty.http.client.HttpClient;

import java.time.Duration;
import java.util.List;
import java.util.Map;
import java.util.concurrent.TimeUnit;

/**
 * OpenAI-compatible client that can connect to various LLM providers.
 * Supports: OpenAI, OpenRouter, Ollama, Azure OpenAI, and custom endpoints.
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class OpenAICompatibleClient {

    private final ObjectMapper objectMapper;
    private WebClient webClient;

    // OpenAI settings
    @Value("${LLM_OPENAI_API_KEY:${OPENAI_API_KEY:}}")
    private String openaiApiKey;

    @Value("${LLM_OPENAI_BASE_URL:https://api.openai.com/v1}")
    private String openaiBaseUrl;

    @Value("${LLM_OPENAI_MODEL:gpt-4o-mini}")
    private String openaiModel;

    // OpenRouter settings
    @Value("${LLM_OPENROUTER_API_KEY:${OPENROUTER_API_KEY:}}")
    private String openrouterApiKey;

    @Value("${LLM_OPENROUTER_BASE_URL:https://openrouter.ai/api/v1}")
    private String openrouterBaseUrl;

    @Value("${LLM_OPENROUTER_MODEL:anthropic/claude-3.5-sonnet}")
    private String openrouterModel;

    // Ollama settings
    @Value("${LLM_OLLAMA_BASE_URL:http://localhost:11434/v1}")
    private String ollamaBaseUrl;

    @Value("${LLM_OLLAMA_MODEL:llama3.2}")
    private String ollamaModel;

    // Azure OpenAI settings
    @Value("${LLM_AZURE_API_KEY:${AZURE_OPENAI_API_KEY:}}")
    private String azureApiKey;

    @Value("${LLM_AZURE_ENDPOINT:}")
    private String azureEndpoint;

    @Value("${LLM_AZURE_DEPLOYMENT:gpt-4o}")
    private String azureDeployment;

    @Value("${LLM_AZURE_API_VERSION:2024-02-15-preview}")
    private String azureApiVersion;

    // Custom endpoint settings
    @Value("${LLM_CUSTOM_BASE_URL:}")
    private String customBaseUrl;

    @Value("${LLM_CUSTOM_API_KEY:}")
    private String customApiKey;

    @Value("${LLM_CUSTOM_MODEL:}")
    private String customModel;

    @Value("${collector.openai.timeout-seconds:120}")
    private int timeoutSeconds;

    @Getter
    private ProviderStatus providerStatus;

    @PostConstruct
    public void init() {
        HttpClient httpClient = HttpClient.create()
                .option(ChannelOption.CONNECT_TIMEOUT_MILLIS, 30000)
                .responseTimeout(Duration.ofSeconds(timeoutSeconds))
                .doOnConnected(conn ->
                        conn.addHandlerLast(new ReadTimeoutHandler(timeoutSeconds, TimeUnit.SECONDS))
                            .addHandlerLast(new WriteTimeoutHandler(timeoutSeconds, TimeUnit.SECONDS))
                )
                .followRedirect(true);

        this.webClient = WebClient.builder()
                .clientConnector(new ReactorClientHttpConnector(httpClient))
                .defaultHeader("User-Agent", "NewsInsight-OpenAI/1.0")
                .codecs(configurer -> configurer.defaultCodecs().maxInMemorySize(10 * 1024 * 1024))
                .build();

        this.providerStatus = checkProviderStatus();
        log.info("OpenAICompatibleClient initialized - Available providers: {}", providerStatus);
    }

    /**
     * Check which providers are available
     */
    public ProviderStatus checkProviderStatus() {
        return new ProviderStatus(
                isNotBlank(openaiApiKey),
                isNotBlank(openrouterApiKey),
                true, // Ollama is always potentially available (local)
                isNotBlank(azureApiKey) && isNotBlank(azureEndpoint),
                isNotBlank(customBaseUrl)
        );
    }

    /**
     * Check if any OpenAI-compatible provider is enabled
     */
    public boolean isEnabled() {
        return providerStatus.openai() || providerStatus.openrouter() 
                || providerStatus.ollama() || providerStatus.azure() 
                || providerStatus.custom();
    }

    /**
     * Check if OpenAI is enabled
     */
    public boolean isOpenAIEnabled() {
        return isNotBlank(openaiApiKey);
    }

    /**
     * Check if OpenRouter is enabled
     */
    public boolean isOpenRouterEnabled() {
        return isNotBlank(openrouterApiKey);
    }

    /**
     * Check if Ollama is enabled (always returns true as it's local)
     */
    public boolean isOllamaEnabled() {
        return true;
    }

    /**
     * Check if Azure OpenAI is enabled
     */
    public boolean isAzureEnabled() {
        return isNotBlank(azureApiKey) && isNotBlank(azureEndpoint);
    }

    /**
     * Check if Custom endpoint is enabled
     */
    public boolean isCustomEnabled() {
        return isNotBlank(customBaseUrl);
    }

    /**
     * Stream completion from OpenAI
     */
    public Flux<String> streamFromOpenAI(String prompt) {
        if (!isOpenAIEnabled()) {
            return Flux.error(new IllegalStateException("OpenAI API key is not configured"));
        }
        return streamCompletion(openaiBaseUrl, openaiApiKey, openaiModel, prompt, "OpenAI");
    }

    /**
     * Stream completion from OpenRouter
     */
    public Flux<String> streamFromOpenRouter(String prompt) {
        if (!isOpenRouterEnabled()) {
            return Flux.error(new IllegalStateException("OpenRouter API key is not configured"));
        }
        return streamCompletion(openrouterBaseUrl, openrouterApiKey, openrouterModel, prompt, "OpenRouter");
    }

    /**
     * Stream completion from Ollama
     */
    public Flux<String> streamFromOllama(String prompt) {
        return streamCompletion(ollamaBaseUrl, null, ollamaModel, prompt, "Ollama");
    }

    /**
     * Stream completion from Azure OpenAI
     */
    public Flux<String> streamFromAzure(String prompt) {
        if (!isAzureEnabled()) {
            return Flux.error(new IllegalStateException("Azure OpenAI is not configured"));
        }
        String url = String.format("%s/openai/deployments/%s/chat/completions?api-version=%s",
                azureEndpoint, azureDeployment, azureApiVersion);
        return streamCompletionAzure(url, azureApiKey, prompt);
    }

    /**
     * Stream completion from Custom endpoint
     */
    public Flux<String> streamFromCustom(String prompt) {
        if (!isCustomEnabled()) {
            return Flux.error(new IllegalStateException("Custom endpoint is not configured"));
        }
        return streamCompletion(customBaseUrl, customApiKey, customModel, prompt, "Custom");
    }

    /**
     * Generic OpenAI-compatible streaming completion
     */
    private Flux<String> streamCompletion(String baseUrl, String apiKey, String model, String prompt, String providerName) {
        String url = baseUrl.endsWith("/") ? baseUrl + "chat/completions" : baseUrl + "/chat/completions";

        // System message to guide the AI to respond directly in report format
        String systemMessage = """
                   .       .
                "", "", ""       .
                 ( )  .
                """;

        Map<String, Object> body = Map.of(
                "model", model,
                "stream", true,
                "messages", List.of(
                        Map.of("role", "system", "content", systemMessage),
                        Map.of("role", "user", "content", prompt)
                )
        );

        log.debug("Calling {} API: {} with model {}", providerName, url, model);

        WebClient.RequestBodySpec request = webClient.post()
                .uri(url)
                .contentType(MediaType.APPLICATION_JSON)
                .accept(MediaType.TEXT_EVENT_STREAM);

        if (apiKey != null && !apiKey.isBlank()) {
            request = request.header(HttpHeaders.AUTHORIZATION, "Bearer " + apiKey);
        }

        return request
                .bodyValue(body)
                .retrieve()
                .bodyToFlux(String.class)
                .timeout(Duration.ofSeconds(timeoutSeconds))
                .doOnSubscribe(s -> log.debug("Starting {} stream request", providerName))
                .filter(chunk -> chunk != null && !chunk.isBlank() && !chunk.equals("[DONE]"))
                .mapNotNull(this::extractContent)
                .doOnComplete(() -> log.debug("{} stream completed", providerName))
                .doOnError(e -> log.error("{} stream failed: {}", providerName, e.getMessage()));
    }

    /**
     * Azure-specific streaming (uses api-key header instead of Authorization)
     */
    private Flux<String> streamCompletionAzure(String url, String apiKey, String prompt) {
        // System message to guide the AI to respond directly in report format
        String systemMessage = """
                   .       .
                "", "", ""       .
                 ( )  .
                """;

        Map<String, Object> body = Map.of(
                "stream", true,
                "messages", List.of(
                        Map.of("role", "system", "content", systemMessage),
                        Map.of("role", "user", "content", prompt)
                )
        );

        log.debug("Calling Azure OpenAI API: {}", url);

        return webClient.post()
                .uri(url)
                .header("api-key", apiKey)
                .contentType(MediaType.APPLICATION_JSON)
                .accept(MediaType.TEXT_EVENT_STREAM)
                .bodyValue(body)
                .retrieve()
                .bodyToFlux(String.class)
                .timeout(Duration.ofSeconds(timeoutSeconds))
                .doOnSubscribe(s -> log.debug("Starting Azure stream request"))
                .filter(chunk -> chunk != null && !chunk.isBlank() && !chunk.equals("[DONE]"))
                .mapNotNull(this::extractContent)
                .doOnComplete(() -> log.debug("Azure stream completed"))
                .doOnError(e -> log.error("Azure stream failed: {}", e.getMessage()));
    }

    /**
     * Extract content from SSE chunk
     */
    private String extractContent(String chunk) {
        try {
            // Handle SSE format: data: {...}
            String json = chunk.startsWith("data:") ? chunk.substring(5).trim() : chunk;
            if (json.isBlank() || json.equals("[DONE]")) {
                return null;
            }

            JsonNode node = objectMapper.readTree(json);
            JsonNode choices = node.get("choices");
            if (choices != null && choices.isArray() && !choices.isEmpty()) {
                JsonNode delta = choices.get(0).get("delta");
                if (delta != null && delta.has("content")) {
                    return delta.get("content").asText();
                }
            }
            return null;
        } catch (Exception e) {
            log.trace("Failed to parse chunk: {}", chunk);
            return null;
        }
    }

    private boolean isNotBlank(String str) {
        return str != null && !str.isBlank();
    }

    /**
     * Provider availability status
     */
    public record ProviderStatus(
            boolean openai,
            boolean openrouter,
            boolean ollama,
            boolean azure,
            boolean custom
    ) {
        @Override
        public String toString() {
            StringBuilder sb = new StringBuilder("[");
            if (openai) sb.append("OpenAI, ");
            if (openrouter) sb.append("OpenRouter, ");
            if (ollama) sb.append("Ollama, ");
            if (azure) sb.append("Azure, ");
            if (custom) sb.append("Custom, ");
            if (sb.length() > 1) {
                sb.setLength(sb.length() - 2);
            }
            sb.append("]");
            return sb.toString();
        }
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/client/PerplexityClient.java

```java
package com.newsinsight.collector.client;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import io.netty.channel.ChannelOption;
import io.netty.handler.timeout.ReadTimeoutHandler;
import io.netty.handler.timeout.WriteTimeoutHandler;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.HttpHeaders;
import org.springframework.http.MediaType;
import org.springframework.http.client.reactive.ReactorClientHttpConnector;
import org.springframework.stereotype.Component;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Flux;
import reactor.netty.http.client.HttpClient;

import com.newsinsight.collector.service.LlmProviderSettingsService;
import jakarta.annotation.PostConstruct;
import java.time.Duration;
import java.util.List;
import java.util.Map;
import java.util.concurrent.TimeUnit;

@Component
@Slf4j
public class PerplexityClient {

    private final ObjectMapper objectMapper;
    private final LlmProviderSettingsService llmProviderSettingsService;
    private WebClient perplexityWebClient;

    @Value("${PERPLEXITY_API_KEY:}")
    private String envApiKey;

    @Value("${PERPLEXITY_BASE_URL:https://api.perplexity.ai}")
    private String envBaseUrl;

    @Value("${PERPLEXITY_MODEL:llama-3.1-sonar-large-128k-online}")
    private String model;

    @Value("${collector.perplexity.timeout-seconds:120}")
    private int timeoutSeconds;

    public PerplexityClient(ObjectMapper objectMapper, LlmProviderSettingsService llmProviderSettingsService) {
        this.objectMapper = objectMapper;
        this.llmProviderSettingsService = llmProviderSettingsService;
    }
    
    /**
     * Get API key from LLM Provider Settings or fall back to environment variable
     */
    private String getApiKey() {
        // Try to get from LLM Provider Settings first
        try {
            var apiKey = llmProviderSettingsService.getGlobalApiKey(
                com.newsinsight.collector.entity.settings.LlmProviderType.PERPLEXITY);
            if (apiKey.isPresent() && !apiKey.get().isBlank()) {
                return apiKey.get();
            }
        } catch (Exception e) {
            log.debug("Failed to get Perplexity settings from database, falling back to env: {}", e.getMessage());
        }
        // Fall back to environment variable
        return envApiKey;
    }
    
    /**
     * Get base URL from LLM Provider Settings or fall back to environment variable
     */
    private String getBaseUrl() {
        try {
            var baseUrl = llmProviderSettingsService.getGlobalBaseUrl(
                com.newsinsight.collector.entity.settings.LlmProviderType.PERPLEXITY);
            if (baseUrl.isPresent() && !baseUrl.get().isBlank()) {
                return baseUrl.get();
            }
        } catch (Exception e) {
            log.debug("Failed to get Perplexity base URL from database: {}", e.getMessage());
        }
        return envBaseUrl;
    }

    @PostConstruct
    public void init() {
        // Create dedicated WebClient with longer timeout for AI streaming
        HttpClient httpClient = HttpClient.create()
                .option(ChannelOption.CONNECT_TIMEOUT_MILLIS, 30000)
                .responseTimeout(Duration.ofSeconds(timeoutSeconds))
                .doOnConnected(conn ->
                        conn.addHandlerLast(new ReadTimeoutHandler(timeoutSeconds, TimeUnit.SECONDS))
                            .addHandlerLast(new WriteTimeoutHandler(timeoutSeconds, TimeUnit.SECONDS))
                )
                .followRedirect(true);

        this.perplexityWebClient = WebClient.builder()
                .clientConnector(new ReactorClientHttpConnector(httpClient))
                .defaultHeader("User-Agent", "NewsInsight-Collector/1.0")
                .codecs(configurer -> configurer.defaultCodecs().maxInMemorySize(10 * 1024 * 1024))
                .build();

        log.info("PerplexityClient initialized with timeout: {}s, enabled: {}", timeoutSeconds, isEnabled());
    }

    /**
     * Check if Perplexity API is enabled (API key is configured)
     */
    public boolean isEnabled() {
        String apiKey = getApiKey();
        return apiKey != null && !apiKey.isBlank();
    }

    public Flux<String> streamCompletion(String prompt) {
        String apiKey = getApiKey();
        if (apiKey == null || apiKey.isBlank()) {
            return Flux.error(new IllegalStateException("Perplexity API key is not configured"));
        }

        String baseUrl = getBaseUrl();
        String url = baseUrl.endsWith("/") ? baseUrl + "chat/completions" : baseUrl + "/chat/completions";

        // System message to guide the AI to respond directly in report format
        String systemMessage = """
                   .       .
                "", "", ""       .
                 ( )  .
                """;

        Map<String, Object> body = Map.of(
                "model", model,
                "stream", true,
                "messages", List.of(
                        Map.of("role", "system", "content", systemMessage),
                        Map.of("role", "user", "content", prompt)
                )
        );

        log.debug("Calling Perplexity API: {} with timeout {}s", url, timeoutSeconds);

        return perplexityWebClient.post()
                .uri(url)
                .header(HttpHeaders.AUTHORIZATION, "Bearer " + apiKey)
                .contentType(MediaType.APPLICATION_JSON)
                .accept(MediaType.TEXT_EVENT_STREAM)
                .bodyValue(body)
                .retrieve()
                .bodyToFlux(String.class)
                .timeout(Duration.ofSeconds(timeoutSeconds))
                .doOnSubscribe(s -> log.debug("Starting Perplexity stream request"))
                .doOnNext(chunk -> log.debug("Perplexity raw chunk: {}", chunk))
                .doOnError(e -> log.error("Perplexity API error: {}", e.getMessage()))
                .doOnComplete(() -> log.debug("Perplexity stream completed"))
                .flatMap(this::extractTextFromChunk);
    }

    private Flux<String> extractTextFromChunk(String chunk) {
        if (chunk == null || chunk.isBlank()) {
            return Flux.empty();
        }

        String trimmed = chunk.trim();
        if ("[DONE]".equalsIgnoreCase(trimmed) || "data: [DONE]".equalsIgnoreCase(trimmed)) {
            return Flux.empty();
        }

        String json;
        if (trimmed.startsWith("data:")) {
            json = trimmed.substring(5).trim();
        } else {
            json = trimmed;
        }

        if (json.isEmpty()) {
            return Flux.empty();
        }

        try {
            JsonNode root = objectMapper.readTree(json);
            JsonNode choices = root.get("choices");
            if (choices == null || !choices.isArray() || choices.isEmpty()) {
                return Flux.empty();
            }

            JsonNode choice = choices.get(0);
            JsonNode delta = choice.get("delta");
            if (delta != null && delta.has("content")) {
                String text = delta.get("content").asText();
                if (text != null && !text.isEmpty()) {
                    return Flux.just(text);
                }
            }

            JsonNode message = choice.get("message");
            if (message != null && message.has("content")) {
                String text = message.get("content").asText();
                if (text != null && !text.isEmpty()) {
                    return Flux.just(text);
                }
            }
        } catch (Exception e) {
            log.warn("Failed to parse Perplexity chunk: {}", chunk, e);
        }

        return Flux.empty();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/ApiKeyMigrationRunner.java

```java
package com.newsinsight.collector.config;

import com.newsinsight.collector.entity.settings.LlmProviderSettings;
import com.newsinsight.collector.repository.LlmProviderSettingsRepository;
import com.newsinsight.collector.util.ApiKeyEncryptor;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.boot.ApplicationArguments;
import org.springframework.boot.ApplicationRunner;
import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;
import org.springframework.stereotype.Component;
import org.springframework.transaction.annotation.Transactional;

import java.util.List;
import java.util.concurrent.atomic.AtomicInteger;

/**
 * Migration runner to encrypt existing plain text API keys in the database.
 * 
 * This component runs on application startup when enabled via configuration.
 * It scans all LLM provider settings and encrypts any API keys that are
 * stored in plain text (not prefixed with "ENC:").
 * 
 * Configuration:
 * - Enable: newsinsight.encryption.migrate-on-startup=true
 * - Disable: newsinsight.encryption.migrate-on-startup=false (default)
 * 
 * The migration is idempotent - already encrypted keys are skipped.
 */
@Component
@ConditionalOnProperty(
    name = "newsinsight.encryption.migrate-on-startup",
    havingValue = "true",
    matchIfMissing = false
)
@RequiredArgsConstructor
@Slf4j
public class ApiKeyMigrationRunner implements ApplicationRunner {

    private final LlmProviderSettingsRepository repository;
    private final ApiKeyEncryptor apiKeyEncryptor;

    @Override
    @Transactional
    public void run(ApplicationArguments args) {
        log.info("Starting API key encryption migration...");
        
        List<LlmProviderSettings> allSettings = repository.findAll();
        AtomicInteger migratedCount = new AtomicInteger(0);
        AtomicInteger skippedCount = new AtomicInteger(0);
        AtomicInteger errorCount = new AtomicInteger(0);
        
        for (LlmProviderSettings settings : allSettings) {
            try {
                String apiKey = settings.getApiKey();
                
                // Skip if no API key
                if (apiKey == null || apiKey.isBlank()) {
                    skippedCount.incrementAndGet();
                    continue;
                }
                
                // Skip if already encrypted
                if (apiKeyEncryptor.isEncrypted(apiKey)) {
                    skippedCount.incrementAndGet();
                    log.debug("Skipping already encrypted key for provider: {} (id: {})", 
                            settings.getProviderType(), settings.getId());
                    continue;
                }
                
                // Encrypt the plain text API key
                String encryptedKey = apiKeyEncryptor.encrypt(apiKey);
                settings.setApiKey(encryptedKey);
                repository.save(settings);
                
                migratedCount.incrementAndGet();
                log.info("Migrated API key for provider: {} (id: {}, user: {})", 
                        settings.getProviderType(), 
                        settings.getId(),
                        settings.getUserId() != null ? settings.getUserId() : "GLOBAL");
                
            } catch (Exception e) {
                errorCount.incrementAndGet();
                log.error("Failed to migrate API key for settings id: {}, provider: {}", 
                        settings.getId(), settings.getProviderType(), e);
            }
        }
        
        log.info("API key encryption migration completed:");
        log.info("  - Total settings found: {}", allSettings.size());
        log.info("  - Successfully migrated: {}", migratedCount.get());
        log.info("  - Skipped (no key or already encrypted): {}", skippedCount.get());
        log.info("  - Errors: {}", errorCount.get());
        
        if (errorCount.get() > 0) {
            log.warn("Some API keys failed to migrate. Please check the logs for details.");
        }
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/AsyncConfig.java

```java
package com.newsinsight.collector.config;

import lombok.extern.slf4j.Slf4j;
import org.springframework.aop.interceptor.AsyncUncaughtExceptionHandler;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.scheduling.annotation.AsyncConfigurer;
import org.springframework.scheduling.annotation.EnableAsync;
import org.springframework.scheduling.annotation.EnableScheduling;
import org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor;

import java.util.concurrent.Executor;

@Configuration
@EnableAsync
@EnableScheduling
@Slf4j
public class AsyncConfig implements AsyncConfigurer {

    @Value("${async.executor.core-pool-size:5}")
    private int corePoolSize;

    @Value("${async.executor.max-pool-size:20}")
    private int maxPoolSize;

    @Value("${async.executor.queue-capacity:100}")
    private int queueCapacity;

    @Value("${async.chat-sync.core-pool-size:3}")
    private int chatSyncCorePoolSize;

    @Value("${async.chat-sync.max-pool-size:10}")
    private int chatSyncMaxPoolSize;

    @Value("${async.chat-sync.queue-capacity:50}")
    private int chatSyncQueueCapacity;

    /**
     *    
     */
    @Bean(name = "taskExecutor")
    public Executor taskExecutor() {
        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
        executor.setCorePoolSize(corePoolSize);
        executor.setMaxPoolSize(maxPoolSize);
        executor.setQueueCapacity(queueCapacity);
        executor.setThreadNamePrefix("async-collection-");
        executor.setWaitForTasksToCompleteOnShutdown(true);
        executor.setAwaitTerminationSeconds(60);
        executor.setRejectedExecutionHandler((r, e) -> 
                log.warn("Task rejected from taskExecutor: {}", r.toString()));
        executor.initialize();
        return executor;
    }

    /**
     *    
     */
    @Bean(name = "chatSyncExecutor")
    public Executor chatSyncExecutor() {
        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
        executor.setCorePoolSize(chatSyncCorePoolSize);
        executor.setMaxPoolSize(chatSyncMaxPoolSize);
        executor.setQueueCapacity(chatSyncQueueCapacity);
        executor.setThreadNamePrefix("chat-sync-");
        executor.setWaitForTasksToCompleteOnShutdown(true);
        executor.setAwaitTerminationSeconds(120); //    2
        executor.setRejectedExecutionHandler((r, e) -> 
                log.warn("Task rejected from chatSyncExecutor: {}", r.toString()));
        executor.initialize();
        return executor;
    }

    /**
     *    
     */
    @Bean(name = "embeddingExecutor")
    public Executor embeddingExecutor() {
        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
        executor.setCorePoolSize(2);
        executor.setMaxPoolSize(5);
        executor.setQueueCapacity(100);
        executor.setThreadNamePrefix("embedding-");
        executor.setWaitForTasksToCompleteOnShutdown(true);
        executor.setAwaitTerminationSeconds(180); //    3
        executor.setRejectedExecutionHandler((r, e) -> 
                log.warn("Task rejected from embeddingExecutor: {}", r.toString()));
        executor.initialize();
        return executor;
    }

    @Override
    public Executor getAsyncExecutor() {
        return taskExecutor();
    }

    @Override
    public AsyncUncaughtExceptionHandler getAsyncUncaughtExceptionHandler() {
        return (ex, method, params) -> {
            log.error("Uncaught async exception in method {}: {}", method.getName(), ex.getMessage(), ex);
        };
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/AutoCrawlInitializer.java

```java
package com.newsinsight.collector.config;

import com.newsinsight.collector.entity.autocrawl.CrawlTarget;
import com.newsinsight.collector.service.autocrawl.AutoCrawlDiscoveryService;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;
import org.springframework.boot.context.event.ApplicationReadyEvent;
import org.springframework.context.event.EventListener;
import org.springframework.stereotype.Component;

import java.util.Arrays;
import java.util.List;

/**
 * AutoCrawl  .
 * 
 *    seed URL    .
 * docker-compose      .
 */
@Component
@RequiredArgsConstructor
@Slf4j
@ConditionalOnProperty(name = "autocrawl.enabled", havingValue = "true", matchIfMissing = false)
public class AutoCrawlInitializer {

    private final AutoCrawlDiscoveryService autoCrawlDiscoveryService;

    @Value("${autocrawl.seed.enabled:true}")
    private boolean seedEnabled;

    @Value("${autocrawl.seed.urls:}")
    private String seedUrlsConfig;

    @Value("${autocrawl.seed.keywords:,,,,IT,}")
    private String seedKeywords;

    @Value("${autocrawl.seed.priority:70}")
    private int seedPriority;

    /**
     *  seed URL  (   )
     */
    private static final List<String> DEFAULT_SEED_URLS = List.of(
            //   
            "https://news.naver.com",
            "https://news.naver.com/section/100",  // 
            "https://news.naver.com/section/101",  // 
            "https://news.naver.com/section/102",  // 
            "https://news.naver.com/section/103",  // /
            "https://news.naver.com/section/104",  // 
            "https://news.naver.com/section/105",  // IT/
            
            //   
            "https://news.daum.net",
            "https://news.daum.net/politics",
            "https://news.daum.net/economic",
            "https://news.daum.net/society",
            "https://news.daum.net/culture",
            "https://news.daum.net/digital",
            
            //   
            "https://www.chosun.com",
            "https://www.donga.com",
            "https://www.joongang.co.kr",
            "https://www.hani.co.kr",
            "https://www.khan.co.kr",
            "https://www.yna.co.kr",
            
            // IT/ 
            "https://www.etnews.com",
            "https://zdnet.co.kr",
            "https://www.bloter.net"
    );

    @EventListener(ApplicationReadyEvent.class)
    public void initializeSeedUrls() {
        if (!seedEnabled) {
            log.info("[AutoCrawl Init] Seed initialization is disabled. Set AUTOCRAWL_SEED_ENABLED=true to enable.");
            return;
        }

        log.info("[AutoCrawl Init] Starting seed URL initialization...");

        try {
            List<String> seedUrls = getSeedUrls();
            
            if (seedUrls.isEmpty()) {
                log.warn("[AutoCrawl Init] No seed URLs configured");
                return;
            }

            log.info("[AutoCrawl Init] Adding {} seed URLs to crawl queue", seedUrls.size());

            List<CrawlTarget> addedTargets = autoCrawlDiscoveryService.addManualTargets(
                    seedUrls,
                    seedKeywords,
                    seedPriority
            );

            log.info("[AutoCrawl Init] Successfully added {} seed URLs to crawl queue (skipped {} duplicates)",
                    addedTargets.size(),
                    seedUrls.size() - addedTargets.size());

            //  URL 
            if (!addedTargets.isEmpty() && log.isDebugEnabled()) {
                addedTargets.forEach(target -> 
                    log.debug("[AutoCrawl Init] Added: {} (priority={})", target.getUrl(), target.getPriority())
                );
            }

        } catch (Exception e) {
            log.error("[AutoCrawl Init] Failed to initialize seed URLs: {}", e.getMessage(), e);
        }
    }

    /**
     * Seed URL  
     *    URL  ,   URL 
     */
    private List<String> getSeedUrls() {
        if (seedUrlsConfig != null && !seedUrlsConfig.isBlank()) {
            //     URL  ( )
            List<String> customUrls = Arrays.stream(seedUrlsConfig.split(","))
                    .map(String::trim)
                    .filter(url -> !url.isBlank())
                    .filter(url -> url.startsWith("http://") || url.startsWith("https://"))
                    .toList();
            
            if (!customUrls.isEmpty()) {
                log.info("[AutoCrawl Init] Using {} custom seed URLs from configuration", customUrls.size());
                return customUrls;
            }
        }

        //  seed URL 
        log.info("[AutoCrawl Init] Using {} default seed URLs", DEFAULT_SEED_URLS.size());
        return DEFAULT_SEED_URLS;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/DataSourceInitializer.java

```java
package com.newsinsight.collector.config;

import com.newsinsight.collector.entity.DataSource;
import com.newsinsight.collector.entity.SourceType;
import com.newsinsight.collector.repository.DataSourceRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.boot.CommandLineRunner;
import org.springframework.stereotype.Component;
import org.springframework.transaction.annotation.Transactional;

import java.util.ArrayList;
import java.util.List;

/**
 *     .
 * 
 *       (, ,  )
 * DB  .    .
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class DataSourceInitializer implements CommandLineRunner {

    private final DataSourceRepository dataSourceRepository;

    @Override
    @Transactional
    public void run(String... args) {
        log.info("Initializing default web search sources...");
        
        List<DataSource> defaultSources = createDefaultWebSearchSources();
        int initialized = 0;
        
        for (DataSource source : defaultSources) {
            //    ()
            if (dataSourceRepository.findByName(source.getName()).isEmpty()) {
                dataSourceRepository.save(source);
                initialized++;
                log.info("Initialized web search source: {}", source.getName());
            } else {
                log.debug("Web search source already exists: {}", source.getName());
            }
        }
        
        if (initialized > 0) {
            log.info("Initialized {} new web search sources", initialized);
        } else {
            log.info("All default web search sources already exist");
        }
        
        //       
        long activeCount = dataSourceRepository.findActiveWebSearchSources().size();
        log.info("Total active web search sources: {}", activeCount);
    }

    /**
     *     
     */
    private List<DataSource> createDefaultWebSearchSources() {
        List<DataSource> sources = new ArrayList<>();
        
        // 1.   ( )
        sources.add(DataSource.builder()
                .name(" ")
                .url("https://news.naver.com")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://search.naver.com/search.naver?where=news&query={query}")
                .searchPriority(10)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"country\":\"KR\",\"language\":\"ko\",\"portal\":\"naver\"}")
                .build());
        
        // 2.  
        sources.add(DataSource.builder()
                .name(" ")
                .url("https://news.daum.net")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://search.daum.net/search?w=news&q={query}")
                .searchPriority(20)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"country\":\"KR\",\"language\":\"ko\",\"portal\":\"daum\"}")
                .build());
        
        // 3.   ()
        sources.add(DataSource.builder()
                .name(" ")
                .url("https://news.google.com")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://news.google.com/search?q={query}&hl=ko&gl=KR")
                .searchPriority(30)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"country\":\"KR\",\"language\":\"ko\",\"portal\":\"google\"}")
                .build());
        
        // 4.   (   -    )
        sources.add(DataSource.builder()
                .name(" ")
                .url("https://news.nate.com")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://search.nate.com/search/all.html?q={query}&csn=1")
                .searchPriority(40)
                .isActive(false)
                .collectionFrequency(3600)
                .metadataJson("{\"country\":\"KR\",\"language\":\"ko\",\"portal\":\"nate\"}")
                .build());
        
        // 5.   (  )
        sources.add(DataSource.builder()
                .name(" ")
                .url("https://news.zum.com")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://search.zum.com/search.zum?method=news&query={query}")
                .searchPriority(50)
                .isActive(false)
                .collectionFrequency(3600)
                .metadataJson("{\"country\":\"KR\",\"language\":\"ko\",\"portal\":\"zum\"}")
                .build());
        
        return sources;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/DataSourceSeeder.java

```java
package com.newsinsight.collector.config;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.newsinsight.collector.entity.BrowserAgentConfig;
import com.newsinsight.collector.entity.DataSource;
import com.newsinsight.collector.entity.SourceType;
import com.newsinsight.collector.repository.DataSourceRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.boot.ApplicationArguments;
import org.springframework.boot.ApplicationRunner;
import org.springframework.context.annotation.Profile;
import org.springframework.stereotype.Component;
import org.springframework.transaction.annotation.Transactional;

import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.stream.Collectors;

/**
 * Seeds the database with default Korean news sources on application startup.
 * Only runs if the data_sources table is empty.
 * 
 * Sources can be configured via:
 * 1. application.yml (collector.data-sources.sources)
 * 2. Default hardcoded sources (if no external config provided)
 * 
 * Profiles:
 * - default: Runs automatically
 * - no-seed: Skip seeding (for production or when using external config)
 */
@Component
@Profile("!no-seed")
@RequiredArgsConstructor
@Slf4j
public class DataSourceSeeder implements ApplicationRunner {

    private final DataSourceRepository dataSourceRepository;
    private final DataSourcesConfig dataSourcesConfig;
    private final ObjectMapper objectMapper;

    @Override
    @Transactional
    public void run(ApplicationArguments args) {
        if (!dataSourcesConfig.isSeedEnabled()) {
            log.info("DataSource seeding is disabled via configuration.");
            return;
        }

        log.info("Seeding data sources...");
        
        List<DataSource> sources;
        
        // Check if external configuration is provided
        if (dataSourcesConfig.getSources() != null && !dataSourcesConfig.getSources().isEmpty()) {
            log.info("Using {} data sources from external configuration.", dataSourcesConfig.getSources().size());
            sources = dataSourcesConfig.getSources().stream()
                    .map(this::convertToDataSource)
                    .collect(Collectors.toList());
        } else {
            log.info("No external configuration found, using default Korean news sources.");
            sources = createDefaultSources();
        }

        int created = 0;
        int skipped = 0;
        for (DataSource desired : sources) {
            DataSource existing = dataSourceRepository
                    .findFirstByUrl(desired.getUrl())
                    .or(() -> dataSourceRepository.findByName(desired.getName()))
                    .orElse(null);

            if (existing == null) {
                dataSourceRepository.save(desired);
                created++;
                continue;
            }
            skipped++;
        }

        log.info(
                "Successfully seeded data sources. created={}, skipped={}, totalDesired={}",
                created,
                skipped,
                sources.size()
        );
    }

    /**
     * Convert external configuration entry to DataSource entity
     */
    private DataSource convertToDataSource(DataSourcesConfig.DataSourceEntry entry) {
        // Build metadata JSON from entry fields
        Map<String, String> metadata = new HashMap<>();
        if (entry.getRegion() != null) metadata.put("region", entry.getRegion());
        if (entry.getLanguage() != null) metadata.put("language", entry.getLanguage());
        if (entry.getReliability() != null) metadata.put("reliability", entry.getReliability());
        if (entry.getCategory() != null) metadata.put("category", entry.getCategory());
        if (entry.getStance() != null) metadata.put("stance", entry.getStance());
        
        // Merge with any additional metadata provided
        if (entry.getMetadata() != null) {
            metadata.putAll(entry.getMetadata());
        }
        
        String metadataJson;
        try {
            metadataJson = objectMapper.writeValueAsString(metadata);
        } catch (JsonProcessingException e) {
            log.warn("Failed to serialize metadata for source {}: {}", entry.getName(), e.getMessage());
            metadataJson = "{}";
        }
        
        DataSource.DataSourceBuilder builder = DataSource.builder()
                .name(entry.getName())
                .url(entry.getUrl())
                .sourceType(parseSourceType(entry.getSourceType()))
                .isActive(entry.isActive())
                .collectionFrequency(entry.getCollectionFrequency())
                .metadataJson(metadataJson);
        
        // Add search-related fields for WEB_SEARCH sources
        if (entry.getSearchUrlTemplate() != null) {
            builder.searchUrlTemplate(entry.getSearchUrlTemplate());
        }
        if (entry.getSearchPriority() != null) {
            builder.searchPriority(entry.getSearchPriority());
        }
        
        return builder.build();
    }

    private SourceType parseSourceType(String type) {
        if (type == null) return SourceType.RSS;
        try {
            return SourceType.valueOf(type.toUpperCase());
        } catch (IllegalArgumentException e) {
            log.warn("Unknown source type '{}', defaulting to RSS", type);
            return SourceType.RSS;
        }
    }

    private List<DataSource> createDefaultSources() {
        return List.of(
            // ========== Korean Major News (High Reliability) ==========
            DataSource.builder()
                .name(" (Yonhap)")
                .url("https://www.yna.co.kr/rss/news.xml")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(1800) // 30 min
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"news_agency\"}")
                .build(),
                
            DataSource.builder()
                .name("KBS ")
                .url("https://news.kbs.co.kr/rss/rss.xml")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(1800)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"broadcast\"}")
                .build(),
                
            DataSource.builder()
                .name("MBC ")
                .url("https://imnews.imbc.com/rss/news.xml")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(1800)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"broadcast\"}")
                .build(),
                
            DataSource.builder()
                .name("SBS ")
                .url("https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=01")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(1800)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"broadcast\"}")
                .build(),
                
            // ========== Korean Major Newspapers ==========
            DataSource.builder()
                .name("")
                .url("https://www.chosun.com/arc/outboundfeeds/rss/?outputType=xml")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"newspaper\",\"stance\":\"conservative\"}")
                .build(),
                
            DataSource.builder()
                .name("")
                .url("https://rss.joins.com/joins_news_list.xml")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"newspaper\",\"stance\":\"center-right\"}")
                .build(),
                
            DataSource.builder()
                .name("")
                .url("https://rss.donga.com/total.xml")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"newspaper\",\"stance\":\"conservative\"}")
                .build(),
                
            DataSource.builder()
                .name("")
                .url("https://www.hani.co.kr/rss/")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"newspaper\",\"stance\":\"progressive\"}")
                .build(),
                
            DataSource.builder()
                .name("")
                .url("https://www.khan.co.kr/rss/rssdata/total_news.xml")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"newspaper\",\"stance\":\"progressive\"}")
                .build(),
                
            // ========== Korean Business/Economy News ==========
            DataSource.builder()
                .name("")
                .url("https://www.mk.co.kr/rss/30000001/")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"business\"}")
                .build(),
                
            DataSource.builder()
                .name("")
                .url("https://www.hankyung.com/feed/all-news")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"business\"}")
                .build(),
                
            // ========== Korean IT/Tech News ==========
            DataSource.builder()
                .name("ZDNet Korea")
                .url("https://zdnet.co.kr/rss/")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"tech\"}")
                .build(),
                
            DataSource.builder()
                .name(" (ETNews)")
                .url("https://www.etnews.com/rss")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"tech\"}")
                .build(),
                
            DataSource.builder()
                .name(" (Bloter)")
                .url("https://www.bloter.net/feed")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(7200)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"tech_startup\"}")
                .build(),
                
            // ========== International News (Korean Edition) ==========
            DataSource.builder()
                .name("BBC ")
                .url("https://feeds.bbci.co.uk/korean/rss.xml")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(1800)
                .metadataJson("{\"region\":\"international\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"international\"}")
                .build(),
                
            DataSource.builder()
                .name(" (Newsis)")
                .url("https://newsis.com/RSS/")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(1800)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"news_agency\"}")
                .build(),
                
            DataSource.builder()
                .name("1")
                .url("https://www.news1.kr/rss/")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(1800)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"news_agency\"}")
                .build(),
                
            // ========== BROWSER_AGENT Sources (AI-based crawling) ==========
            //   (Browser Agent)
            DataSource.builder()
                .name("  (Browser Agent)")
                .url("https://news.naver.com/")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(true)
                .collectionFrequency(1800) // 30 min
                .browserAgentConfig(BrowserAgentConfig.forNewsExploration())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"portal\",\"crawler\":\"browser_agent\"}")
                .build(),
                
            //   (Browser Agent)
            DataSource.builder()
                .name("  (Browser Agent)")
                .url("https://news.daum.net/")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(true)
                .collectionFrequency(1800)
                .browserAgentConfig(BrowserAgentConfig.forNewsExploration())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"portal\",\"crawler\":\"browser_agent\"}")
                .build(),
                
            //    (Browser Agent)
            DataSource.builder()
                .name("   (Browser Agent)")
                .url("https://news.google.com/home?hl=ko&gl=KR&ceid=KR:ko")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(true)
                .collectionFrequency(3600)
                .browserAgentConfig(BrowserAgentConfig.forNewsExploration())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"aggregator\",\"crawler\":\"browser_agent\"}")
                .build(),
                
            //     (Browser Agent - Breaking News)
            DataSource.builder()
                .name("  (Browser Agent)")
                .url("https://datalab.naver.com/keyword/realtimeList.naver")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(true)
                .collectionFrequency(900) // 15 min -   
                .browserAgentConfig(BrowserAgentConfig.forBreakingNews())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"trending\",\"crawler\":\"browser_agent\"}")
                .build(),
                
            //  (Browser Agent - Archive mode for non-RSS content)
            DataSource.builder()
                .name(" (Browser Agent)")
                .url("https://www.chosun.com/")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(false) // RSS   
                .collectionFrequency(7200)
                .browserAgentConfig(BrowserAgentConfig.forNewsArchive())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"newspaper\",\"crawler\":\"browser_agent\",\"stance\":\"conservative\"}")
                .build(),
                
            //  (Browser Agent)
            DataSource.builder()
                .name(" (Browser Agent)")
                .url("https://www.hani.co.kr/")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(false) // RSS   
                .collectionFrequency(7200)
                .browserAgentConfig(BrowserAgentConfig.forNewsArchive())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"newspaper\",\"crawler\":\"browser_agent\",\"stance\":\"progressive\"}")
                .build(),
                
            // ========== WEB_SEARCH Sources (Portal Search Integration) ==========
            DataSource.builder()
                .name("  ")
                .url("https://search.naver.com")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://search.naver.com/search.naver?where=news&query={query}")
                .searchPriority(1)
                .isActive(true)
                .collectionFrequency(0) //    
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"portal_search\"}")
                .build(),
                
            DataSource.builder()
                .name("  ")
                .url("https://search.daum.net")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://search.daum.net/search?w=news&q={query}")
                .searchPriority(2)
                .isActive(true)
                .collectionFrequency(0)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"portal_search\"}")
                .build(),
                
            DataSource.builder()
                .name("   ()")
                .url("https://news.google.com")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://news.google.com/search?q={query}&hl=ko&gl=KR")
                .searchPriority(3)
                .isActive(true)
                .collectionFrequency(0)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"aggregator_search\"}")
                .build(),
                
            DataSource.builder()
                .name("   ()")
                .url("https://www.bing.com")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://www.bing.com/news/search?q={query}&cc=kr")
                .searchPriority(4)
                .isActive(true)
                .collectionFrequency(0)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"aggregator_search\"}")
                .build(),
                
            // ========== COMMUNITY Sources (  ) ==========
            // DCInside ()
            DataSource.builder()
                .name(" (Browser Agent)")
                .url("https://www.dcinside.com/")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(true)
                .collectionFrequency(3600)
                .browserAgentConfig(BrowserAgentConfig.forNewsExploration())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"low\",\"category\":\"community\",\"source_category\":\"community\",\"crawler\":\"browser_agent\"}")
                .build(),
                
            // Clien ()
            DataSource.builder()
                .name(" (Browser Agent)")
                .url("https://www.clien.net/service/")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(true)
                .collectionFrequency(3600)
                .browserAgentConfig(BrowserAgentConfig.forNewsExploration())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"community\",\"source_category\":\"community\",\"crawler\":\"browser_agent\"}")
                .build(),
                
            // Ruliweb ()
            DataSource.builder()
                .name(" (Browser Agent)")
                .url("https://bbs.ruliweb.com/")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(true)
                .collectionFrequency(3600)
                .browserAgentConfig(BrowserAgentConfig.forNewsExploration())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"low\",\"category\":\"community\",\"source_category\":\"community\",\"crawler\":\"browser_agent\"}")
                .build(),
                
            // Ppomppu ()
            DataSource.builder()
                .name(" (Browser Agent)")
                .url("https://www.ppomppu.co.kr/")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(true)
                .collectionFrequency(3600)
                .browserAgentConfig(BrowserAgentConfig.forNewsExploration())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"low\",\"category\":\"community\",\"source_category\":\"community\",\"crawler\":\"browser_agent\"}")
                .build(),
                
            // TheQoo ()
            DataSource.builder()
                .name(" (Browser Agent)")
                .url("https://theqoo.net/")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(true)
                .collectionFrequency(3600)
                .browserAgentConfig(BrowserAgentConfig.forNewsExploration())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"low\",\"category\":\"community\",\"source_category\":\"community\",\"crawler\":\"browser_agent\"}")
                .build(),
                
            // FMKorea ()
            DataSource.builder()
                .name(" (Browser Agent)")
                .url("https://www.fmkorea.com/")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(true)
                .collectionFrequency(3600)
                .browserAgentConfig(BrowserAgentConfig.forNewsExploration())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"low\",\"category\":\"community\",\"source_category\":\"community\",\"crawler\":\"browser_agent\"}")
                .build(),
                
            // MLB Park ()
            DataSource.builder()
                .name(" (Browser Agent)")
                .url("https://mlbpark.donga.com/")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(true)
                .collectionFrequency(3600)
                .browserAgentConfig(BrowserAgentConfig.forNewsExploration())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"low\",\"category\":\"community\",\"source_category\":\"community\",\"crawler\":\"browser_agent\"}")
                .build(),
                
            // Bobaedream ( - )
            DataSource.builder()
                .name(" (Browser Agent)")
                .url("https://www.bobaedream.co.kr/")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(true)
                .collectionFrequency(7200)
                .browserAgentConfig(BrowserAgentConfig.forNewsExploration())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"low\",\"category\":\"community\",\"source_category\":\"community\",\"crawler\":\"browser_agent\"}")
                .build(),
                
            // ========== COMMUNITY Search Sources ( ) ==========
            DataSource.builder()
                .name("Reddit ")
                .url("https://www.reddit.com")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://www.reddit.com/search/?q={query}&type=link")
                .searchPriority(5)
                .isActive(true)
                .collectionFrequency(0)
                .metadataJson("{\"region\":\"global\",\"language\":\"en\",\"reliability\":\"medium\",\"category\":\"community_search\",\"source_category\":\"community\"}")
                .build(),
                
            DataSource.builder()
                .name("Twitter/X ")
                .url("https://twitter.com")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://twitter.com/search?q={query}&f=live")
                .searchPriority(6)
                .isActive(true)
                .collectionFrequency(0)
                .metadataJson("{\"region\":\"global\",\"language\":\"multi\",\"reliability\":\"low\",\"category\":\"social_search\",\"source_category\":\"community\"}")
                .build(),
                
            // ========== BLOG Sources (/) ==========
            DataSource.builder()
                .name("  ")
                .url("https://search.naver.com")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://search.naver.com/search.naver?where=blog&query={query}")
                .searchPriority(7)
                .isActive(true)
                .collectionFrequency(0)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"low\",\"category\":\"blog_search\",\"source_category\":\"blog\"}")
                .build(),
                
            DataSource.builder()
                .name(" ")
                .url("https://brunch.co.kr")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://brunch.co.kr/search?q={query}")
                .searchPriority(8)
                .isActive(true)
                .collectionFrequency(0)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"blog_search\",\"source_category\":\"blog\"}")
                .build()
        );
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/DataSourcesConfig.java

```java
package com.newsinsight.collector.config;

import lombok.Data;
import org.springframework.boot.context.properties.ConfigurationProperties;
import org.springframework.context.annotation.Configuration;

import java.util.ArrayList;
import java.util.List;
import java.util.Map;

/**
 * Configuration for externalized data sources.
 * 
 * Data sources can be configured via application.yml or environment variables.
 * This replaces hardcoded sources in DataSourceSeeder with configurable ones.
 */
@Configuration
@ConfigurationProperties(prefix = "collector.data-sources")
@Data
public class DataSourcesConfig {

    /**
     * Enable/disable automatic seeding of data sources
     */
    private boolean seedEnabled = true;

    /**
     * List of predefined data source configurations
     */
    private List<DataSourceEntry> sources = new ArrayList<>();

    @Data
    public static class DataSourceEntry {
        /**
         * Display name for the source
         */
        private String name;

        /**
         * URL for the data source (RSS feed, API endpoint, etc.)
         */
        private String url;

        /**
         * Type of source: RSS, API, WEB_SCRAPER, WEB_SEARCH, BROWSER_AGENT
         */
        private String sourceType = "RSS";

        /**
         * Whether this source is active and should be collected
         */
        private boolean active = true;

        /**
         * Collection frequency in seconds
         */
        private int collectionFrequency = 3600;

        /**
         * Search URL template for WEB_SEARCH sources.
         * Use {query} as placeholder for the encoded search query.
         * Example: "https://search.naver.com/search.naver?where=news&query={query}"
         */
        private String searchUrlTemplate;

        /**
         * Priority for web search sources (lower = higher priority).
         */
        private Integer searchPriority = 100;

        /**
         * Additional metadata as key-value pairs
         */
        private Map<String, String> metadata;

        /**
         * Region/country code (e.g., "korea", "international")
         */
        private String region;

        /**
         * Language code (e.g., "ko", "en")
         */
        private String language = "ko";

        /**
         * Reliability level: "high", "medium", "low"
         */
        private String reliability = "medium";

        /**
         * Category: "news_agency", "broadcast", "newspaper", "business", "tech", etc.
         */
        private String category;

        /**
         * Political stance (optional): "conservative", "progressive", "center", "center-right", "center-left"
         */
        private String stance;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/KafkaConfig.java

```java
package com.newsinsight.collector.config;

import com.newsinsight.collector.dto.AiRequestMessage;
import com.newsinsight.collector.dto.AiResponseMessage;
import com.newsinsight.collector.dto.AiTaskRequestMessage;
import com.newsinsight.collector.dto.BrowserTaskMessage;
import com.newsinsight.collector.dto.CrawlCommandMessage;
import com.newsinsight.collector.dto.CrawlResultMessage;
import com.newsinsight.collector.dto.SearchHistoryMessage;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.kafka.common.serialization.StringSerializer;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;
import org.springframework.kafka.core.*;
import org.springframework.kafka.listener.CommonErrorHandler;
import org.springframework.kafka.listener.ContainerProperties;
import org.springframework.kafka.listener.DeadLetterPublishingRecoverer;
import org.springframework.kafka.listener.DefaultErrorHandler;
import org.springframework.kafka.support.ExponentialBackOffWithMaxRetries;
import org.springframework.kafka.support.serializer.JsonDeserializer;
import org.springframework.kafka.support.serializer.JsonSerializer;

import java.util.HashMap;
import java.util.Map;

/**
 * Kafka Configuration with Production-grade reliability features:
 * - Dead Letter Queue (DLQ) for failed messages
 * - Exponential backoff retry with max attempts
 * - Producer reliability settings (acks=all, retries, idempotence)
 * - Manual acknowledgment mode for consumer reliability
 * - Centralized configuration to reduce duplication
 */
@Configuration
@Slf4j
public class KafkaConfig {

    // ========== Configuration Properties ==========
    
    @Value("${spring.kafka.bootstrap-servers:localhost:9092}")
    private String bootstrapServers;

    @Value("${spring.application.name:collector-service}")
    private String applicationName;

    // Producer reliability settings
    @Value("${spring.kafka.producer.acks:all}")
    private String producerAcks;

    @Value("${spring.kafka.producer.retries:3}")
    private int producerRetries;

    @Value("${spring.kafka.producer.retry-backoff-ms:1000}")
    private int producerRetryBackoffMs;

    @Value("${spring.kafka.producer.delivery-timeout-ms:120000}")
    private int producerDeliveryTimeoutMs;

    @Value("${spring.kafka.producer.enable-idempotence:true}")
    private boolean producerIdempotence;

    // Consumer reliability settings
    @Value("${spring.kafka.consumer.max-retry-attempts:3}")
    private int consumerMaxRetryAttempts;

    @Value("${spring.kafka.consumer.retry-backoff-ms:1000}")
    private long consumerRetryBackoffMs;

    @Value("${spring.kafka.consumer.retry-max-backoff-ms:30000}")
    private long consumerRetryMaxBackoffMs;

    @Value("${spring.kafka.consumer.concurrency:1}")
    private int consumerConcurrency;

    // DLQ suffix
    private static final String DLQ_SUFFIX = ".dlq";

    // ========== Common Producer Configuration ==========

    private Map<String, Object> buildProducerProps() {
        Map<String, Object> props = new HashMap<>();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, JsonSerializer.class);
        
        // Reliability settings
        props.put(ProducerConfig.ACKS_CONFIG, producerAcks);
        props.put(ProducerConfig.RETRIES_CONFIG, producerRetries);
        props.put(ProducerConfig.RETRY_BACKOFF_MS_CONFIG, producerRetryBackoffMs);
        props.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, producerDeliveryTimeoutMs);
        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, producerIdempotence);
        
        // Batching for throughput (can be tuned)
        props.put(ProducerConfig.LINGER_MS_CONFIG, 5);
        props.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384);
        
        return props;
    }

    // ========== Common Consumer Configuration ==========

    private Map<String, Object> buildConsumerProps(String groupIdSuffix) {
        Map<String, Object> props = new HashMap<>();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, applicationName + "-" + groupIdSuffix);
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, JsonDeserializer.class);
        props.put(JsonDeserializer.TRUSTED_PACKAGES, "com.newsinsight.collector.dto");
        
        // Reliability settings
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false); // Manual ack
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 100);
        props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, 300000); // 5 minutes
        
        return props;
    }

    // ========== DLQ Producer (Generic) ==========

    @Bean
    public ProducerFactory<String, Object> dlqProducerFactory() {
        return new DefaultKafkaProducerFactory<>(buildProducerProps());
    }

    @Bean
    public KafkaTemplate<String, Object> dlqKafkaTemplate() {
        return new KafkaTemplate<>(dlqProducerFactory());
    }

    /**
     * Dead Letter Publishing Recoverer - sends failed messages to DLQ topic.
     * Topic naming convention: original-topic.dlq
     */
    @Bean
    public DeadLetterPublishingRecoverer deadLetterPublishingRecoverer() {
        return new DeadLetterPublishingRecoverer(dlqKafkaTemplate(),
                (ConsumerRecord<?, ?> record, Exception ex) -> {
                    String dlqTopic = record.topic() + DLQ_SUFFIX;
                    log.error("Sending to DLQ: topic={}, key={}, offset={}, error={}",
                            dlqTopic, record.key(), record.offset(), ex.getMessage());
                    return new TopicPartition(dlqTopic, record.partition());
                });
    }

    /**
     * Common Error Handler with exponential backoff and DLQ.
     */
    @Bean
    public CommonErrorHandler kafkaErrorHandler(DeadLetterPublishingRecoverer recoverer) {
        ExponentialBackOffWithMaxRetries backOff = new ExponentialBackOffWithMaxRetries(consumerMaxRetryAttempts);
        backOff.setInitialInterval(consumerRetryBackoffMs);
        backOff.setMaxInterval(consumerRetryMaxBackoffMs);
        backOff.setMultiplier(2.0);

        DefaultErrorHandler errorHandler = new DefaultErrorHandler(recoverer, backOff);
        
        // Log retries
        errorHandler.setRetryListeners((record, ex, attempt) -> {
            log.warn("Retry attempt {} for record: topic={}, key={}, offset={}, error={}",
                    attempt, record.topic(), record.key(), record.offset(), ex.getMessage());
        });
        
        return errorHandler;
    }

    // ========== AI Request Producer ==========

    @Bean
    public ProducerFactory<String, AiRequestMessage> aiRequestProducerFactory() {
        return new DefaultKafkaProducerFactory<>(buildProducerProps());
    }

    @Bean
    public KafkaTemplate<String, AiRequestMessage> aiRequestKafkaTemplate() {
        KafkaTemplate<String, AiRequestMessage> template = new KafkaTemplate<>(aiRequestProducerFactory());
        template.setObservationEnabled(true); // Enable metrics
        return template;
    }

    // ========== Crawl Command Producer ==========

    @Bean
    public ProducerFactory<String, CrawlCommandMessage> crawlCommandProducerFactory() {
        return new DefaultKafkaProducerFactory<>(buildProducerProps());
    }

    @Bean
    public KafkaTemplate<String, CrawlCommandMessage> crawlCommandKafkaTemplate() {
        KafkaTemplate<String, CrawlCommandMessage> template = new KafkaTemplate<>(crawlCommandProducerFactory());
        template.setObservationEnabled(true);
        return template;
    }

    // ========== Crawl Result Producer ==========

    @Bean
    public ProducerFactory<String, CrawlResultMessage> crawlResultProducerFactory() {
        return new DefaultKafkaProducerFactory<>(buildProducerProps());
    }

    @Bean
    public KafkaTemplate<String, CrawlResultMessage> crawlResultKafkaTemplate() {
        KafkaTemplate<String, CrawlResultMessage> template = new KafkaTemplate<>(crawlResultProducerFactory());
        template.setObservationEnabled(true);
        return template;
    }

    // ========== AI Task Request Producer (for Orchestration) ==========

    @Bean
    public ProducerFactory<String, AiTaskRequestMessage> aiTaskRequestProducerFactory() {
        return new DefaultKafkaProducerFactory<>(buildProducerProps());
    }

    @Bean
    public KafkaTemplate<String, AiTaskRequestMessage> aiTaskRequestKafkaTemplate() {
        KafkaTemplate<String, AiTaskRequestMessage> template = new KafkaTemplate<>(aiTaskRequestProducerFactory());
        template.setObservationEnabled(true);
        return template;
    }

    // ========== Browser Task Producer (for autonomous browser crawling) ==========

    @Bean
    public ProducerFactory<String, BrowserTaskMessage> browserTaskProducerFactory() {
        return new DefaultKafkaProducerFactory<>(buildProducerProps());
    }

    @Bean
    public KafkaTemplate<String, BrowserTaskMessage> browserTaskKafkaTemplate() {
        KafkaTemplate<String, BrowserTaskMessage> template = new KafkaTemplate<>(browserTaskProducerFactory());
        template.setObservationEnabled(true);
        return template;
    }

    // ========== AI Response Consumer ==========

    @Bean
    public ConsumerFactory<String, AiResponseMessage> aiResponseConsumerFactory() {
        return new DefaultKafkaConsumerFactory<>(
                buildConsumerProps("ai"),
                new StringDeserializer(),
                new JsonDeserializer<>(AiResponseMessage.class)
        );
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, AiResponseMessage> aiResponseKafkaListenerContainerFactory(
            CommonErrorHandler kafkaErrorHandler) {
        ConcurrentKafkaListenerContainerFactory<String, AiResponseMessage> factory =
                new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(aiResponseConsumerFactory());
        factory.setConcurrency(consumerConcurrency);
        factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.RECORD);
        factory.setCommonErrorHandler(kafkaErrorHandler);
        return factory;
    }

    // ========== Crawl Command Consumer ==========

    @Bean
    public ConsumerFactory<String, CrawlCommandMessage> crawlCommandConsumerFactory() {
        return new DefaultKafkaConsumerFactory<>(
                buildConsumerProps("crawl"),
                new StringDeserializer(),
                new JsonDeserializer<>(CrawlCommandMessage.class)
        );
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, CrawlCommandMessage> crawlCommandKafkaListenerContainerFactory(
            CommonErrorHandler kafkaErrorHandler) {
        ConcurrentKafkaListenerContainerFactory<String, CrawlCommandMessage> factory =
                new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(crawlCommandConsumerFactory());
        factory.setConcurrency(consumerConcurrency);
        factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.RECORD);
        factory.setCommonErrorHandler(kafkaErrorHandler);
        return factory;
    }

    // ========== Crawl Result Consumer ==========

    @Bean
    public ConsumerFactory<String, CrawlResultMessage> crawlResultConsumerFactory() {
        return new DefaultKafkaConsumerFactory<>(
                buildConsumerProps("crawl-result"),
                new StringDeserializer(),
                new JsonDeserializer<>(CrawlResultMessage.class)
        );
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, CrawlResultMessage> crawlResultKafkaListenerContainerFactory(
            CommonErrorHandler kafkaErrorHandler) {
        ConcurrentKafkaListenerContainerFactory<String, CrawlResultMessage> factory =
                new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(crawlResultConsumerFactory());
        factory.setConcurrency(consumerConcurrency);
        factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.RECORD);
        factory.setCommonErrorHandler(kafkaErrorHandler);
        return factory;
    }

    // ========== Search History Producer (for async persistence) ==========

    @Bean
    public ProducerFactory<String, SearchHistoryMessage> searchHistoryProducerFactory() {
        return new DefaultKafkaProducerFactory<>(buildProducerProps());
    }

    @Bean
    public KafkaTemplate<String, SearchHistoryMessage> searchHistoryKafkaTemplate() {
        KafkaTemplate<String, SearchHistoryMessage> template = new KafkaTemplate<>(searchHistoryProducerFactory());
        template.setObservationEnabled(true);
        return template;
    }

    // ========== Search History Consumer ==========

    @Bean
    public ConsumerFactory<String, SearchHistoryMessage> searchHistoryConsumerFactory() {
        return new DefaultKafkaConsumerFactory<>(
                buildConsumerProps("search-history"),
                new StringDeserializer(),
                new JsonDeserializer<>(SearchHistoryMessage.class)
        );
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, SearchHistoryMessage> searchHistoryKafkaListenerContainerFactory(
            CommonErrorHandler kafkaErrorHandler) {
        ConcurrentKafkaListenerContainerFactory<String, SearchHistoryMessage> factory =
                new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(searchHistoryConsumerFactory());
        factory.setConcurrency(consumerConcurrency);
        factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.RECORD);
        factory.setCommonErrorHandler(kafkaErrorHandler);
        return factory;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/MlAddonSeeder.java

```java
package com.newsinsight.collector.config;

import com.newsinsight.collector.entity.addon.*;
import com.newsinsight.collector.repository.MlAddonRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.boot.ApplicationArguments;
import org.springframework.boot.ApplicationRunner;
import org.springframework.context.annotation.Profile;
import org.springframework.stereotype.Component;
import org.springframework.transaction.annotation.Transactional;

import java.util.List;
import java.util.Map;

/**
 * Seeds the database with default ML Add-ons on application startup.
 * Registers sentiment, factcheck, and bias analysis add-ons if not already present.
 * 
 * Profiles:
 * - default: Runs automatically
 * - no-seed: Skip seeding (for production or when using external config)
 */
@Component
@Profile("!no-seed")
@RequiredArgsConstructor
@Slf4j
public class MlAddonSeeder implements ApplicationRunner {

    private final MlAddonRepository mlAddonRepository;

    @Value("${ml.addon.sentiment.host:sentiment-addon}")
    private String sentimentHost;

    @Value("${ml.addon.sentiment.port:8100}")
    private int sentimentPort;

    @Value("${ml.addon.factcheck.host:factcheck-addon}")
    private String factcheckHost;

    @Value("${ml.addon.factcheck.port:8101}")
    private int factcheckPort;

    @Value("${ml.addon.bias.host:bias-addon}")
    private String biasHost;

    @Value("${ml.addon.bias.port:8102}")
    private int biasPort;

    @Value("${ml.addon.botdetector.host:bot-detector}")
    private String botDetectorHost;

    @Value("${ml.addon.botdetector.port:8041}")
    private int botDetectorPort;

    @Value("${ml.addon.seed.enabled:true}")
    private boolean seedEnabled;

    @Override
    @Transactional
    public void run(ApplicationArguments args) {
        if (!seedEnabled) {
            log.info("ML Add-on seeding is disabled via configuration.");
            return;
        }

        log.info("Seeding ML Add-ons...");

        List<MlAddon> defaultAddons = createDefaultAddons();

        int created = 0;
        int skipped = 0;

        for (MlAddon addon : defaultAddons) {
            if (mlAddonRepository.existsByAddonKey(addon.getAddonKey())) {
                log.debug("ML Add-on '{}' already exists, skipping.", addon.getAddonKey());
                skipped++;
            } else {
                mlAddonRepository.save(addon);
                log.info("Created ML Add-on: {} ({})", addon.getName(), addon.getAddonKey());
                created++;
            }
        }

        log.info("Successfully seeded ML Add-ons. created={}, skipped={}, total={}", 
                created, skipped, defaultAddons.size());
    }

    private List<MlAddon> createDefaultAddons() {
        return List.of(
            // ========== Sentiment Analysis Add-on ==========
            MlAddon.builder()
                .addonKey("sentiment-v1")
                .name("  ")
                .description("   (//) . " +
                        "KoBERT/KoELECTRA       .")
                .category(AddonCategory.SENTIMENT)
                .invokeType(AddonInvokeType.HTTP_SYNC)
                .endpointUrl(String.format("http://%s:%d/analyze", sentimentHost, sentimentPort))
                .healthCheckUrl(String.format("http://%s:%d/health", sentimentHost, sentimentPort))
                .authType(AddonAuthType.NONE)
                .inputSchemaVersion("1.0")
                .outputSchemaVersion("1.0")
                .timeoutMs(30000)
                .maxQps(20)
                .maxRetries(3)
                .enabled(true)
                .priority(10)
                .healthStatus(AddonHealthStatus.UNKNOWN)
                .owner("system")
                .config(Map.of(
                    "model", "koelectra-sentiment",
                    "language", "ko",
                    "min_confidence", 0.5,
                    "include_emotions", true
                ))
                .build(),

            // ========== Fact Check Add-on ==========
            MlAddon.builder()
                .addonKey("factcheck-v1")
                .name(" ")
                .description("     . " +
                        "KoELECTRA, Sentence Transformers, KLUE BERT     .")
                .category(AddonCategory.FACTCHECK)
                .invokeType(AddonInvokeType.HTTP_SYNC)
                .endpointUrl(String.format("http://%s:%d/analyze", factcheckHost, factcheckPort))
                .healthCheckUrl(String.format("http://%s:%d/health", factcheckHost, factcheckPort))
                .authType(AddonAuthType.NONE)
                .inputSchemaVersion("1.0")
                .outputSchemaVersion("1.0")
                .timeoutMs(60000) // Factcheck may take longer due to cross-reference
                .maxQps(10)
                .maxRetries(2)
                .enabled(true)
                .priority(20)
                .healthStatus(AddonHealthStatus.UNKNOWN)
                .owner("system")
                .config(Map.of(
                    "models", List.of("koelectra", "sentence-transformers", "klue-bert"),
                    "language", "ko",
                    "extract_claims", true,
                    "cross_reference", true,
                    "min_claim_confidence", 0.6
                ))
                .build(),

            // ========== Bias Analysis Add-on ==========
            MlAddon.builder()
                .addonKey("bias-v1")
                .name(" ")
                .description("  /  . " +
                        " ,  ,       .")
                .category(AddonCategory.BIAS)
                .invokeType(AddonInvokeType.HTTP_SYNC)
                .endpointUrl(String.format("http://%s:%d/analyze", biasHost, biasPort))
                .healthCheckUrl(String.format("http://%s:%d/health", biasHost, biasPort))
                .authType(AddonAuthType.NONE)
                .inputSchemaVersion("1.0")
                .outputSchemaVersion("1.0")
                .timeoutMs(30000)
                .maxQps(15)
                .maxRetries(3)
                .enabled(true)
                .priority(30)
                .healthStatus(AddonHealthStatus.UNKNOWN)
                .owner("system")
                .config(Map.of(
                    "model", "bias-detector-ko",
                    "language", "ko",
                    "analyze_source", true,
                    "analyze_language", true,
                    "analyze_framing", true,
                    "political_spectrum", true
                ))
                .build(),

            // ========== Source Quality Add-on ==========
            MlAddon.builder()
                .addonKey("source-quality-v1")
                .name("  ")
                .description("    . " +
                        "       .")
                .category(AddonCategory.SOURCE_QUALITY)
                .invokeType(AddonInvokeType.HTTP_SYNC)
                .endpointUrl(String.format("http://%s:%d/analyze/source", biasHost, biasPort))
                .healthCheckUrl(String.format("http://%s:%d/health", biasHost, biasPort))
                .authType(AddonAuthType.NONE)
                .inputSchemaVersion("1.0")
                .outputSchemaVersion("1.0")
                .timeoutMs(15000)
                .maxQps(30)
                .maxRetries(2)
                .enabled(true)
                .priority(5) // Run early as other addons may depend on source info
                .healthStatus(AddonHealthStatus.UNKNOWN)
                .owner("system")
                .config(Map.of(
                    "include_history", true,
                    "check_domain_reputation", true,
                    "check_author", false
                ))
                .build(),

            // ========== Topic Classification Add-on ==========
            MlAddon.builder()
                .addonKey("topic-classifier-v1")
                .name(" ")
                .description("  , , , , IT   .")
                .category(AddonCategory.TOPIC_CLASSIFICATION)
                .invokeType(AddonInvokeType.HTTP_SYNC)
                .endpointUrl(String.format("http://%s:%d/analyze/topic", sentimentHost, sentimentPort))
                .healthCheckUrl(String.format("http://%s:%d/health", sentimentHost, sentimentPort))
                .authType(AddonAuthType.NONE)
                .inputSchemaVersion("1.0")
                .outputSchemaVersion("1.0")
                .timeoutMs(20000)
                .maxQps(25)
                .maxRetries(3)
                .enabled(false) // Disabled by default, enable when model is ready
                .priority(15)
                .healthStatus(AddonHealthStatus.UNKNOWN)
                .owner("system")
                .config(Map.of(
                    "model", "klue-ynat",
                    "language", "ko",
                    "categories", List.of("", "", "", "", "", "IT/", "", ""),
                    "multi_label", true
                ))
                .build(),

            // ========== Entity Extraction (NER) Add-on ==========
            MlAddon.builder()
                .addonKey("ner-v1")
                .name("  (NER)")
                .description("  , , ,    .")
                .category(AddonCategory.NER)
                .invokeType(AddonInvokeType.HTTP_SYNC)
                .endpointUrl(String.format("http://%s:%d/analyze/ner", factcheckHost, factcheckPort))
                .healthCheckUrl(String.format("http://%s:%d/health", factcheckHost, factcheckPort))
                .authType(AddonAuthType.NONE)
                .inputSchemaVersion("1.0")
                .outputSchemaVersion("1.0")
                .timeoutMs(25000)
                .maxQps(20)
                .maxRetries(3)
                .enabled(false) // Disabled by default
                .priority(8)
                .healthStatus(AddonHealthStatus.UNKNOWN)
                .owner("system")
                .config(Map.of(
                    "model", "klue-ner",
                    "language", "ko",
                    "entity_types", List.of("PERSON", "ORGANIZATION", "LOCATION", "DATE", "QUANTITY"),
                    "link_entities", true
                ))
                .build(),

            // ========== Bot Detector Add-on ==========
            MlAddon.builder()
                .addonKey("bot-detector-v1")
                .name("/AI   ")
                .description("     AI   . " +
                        "RoBERTa  OpenAI detector  .")
                .category(AddonCategory.BOT_DETECTION)
                .invokeType(AddonInvokeType.HTTP_SYNC)
                .endpointUrl(String.format("http://%s:%d/analyze", botDetectorHost, botDetectorPort))
                .healthCheckUrl(String.format("http://%s:%d/health", botDetectorHost, botDetectorPort))
                .authType(AddonAuthType.NONE)
                .inputSchemaVersion("1.0")
                .outputSchemaVersion("1.0")
                .timeoutMs(30000)
                .maxQps(15)
                .maxRetries(3)
                .enabled(true)
                .priority(25)
                .healthStatus(AddonHealthStatus.UNKNOWN)
                .owner("system")
                .config(Map.of(
                    "model", "roberta-base-openai-detector",
                    "language", "multi",
                    "threshold", 0.5,
                    "detect_ai_generated", true,
                    "detect_bot_patterns", true
                ))
                .build()
        );
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/RedisCacheConfig.java

```java
package com.newsinsight.collector.config;

import com.fasterxml.jackson.annotation.JsonTypeInfo;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.jsontype.impl.LaissezFaireSubTypeValidator;
import com.fasterxml.jackson.datatype.jsr310.JavaTimeModule;
import io.micrometer.core.instrument.MeterRegistry;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.cache.Cache;
import org.springframework.cache.CacheManager;
import org.springframework.cache.annotation.CachingConfigurer;
import org.springframework.cache.annotation.EnableCaching;
import org.springframework.cache.caffeine.CaffeineCacheManager;
import org.springframework.cache.interceptor.CacheErrorHandler;
import org.springframework.cache.support.CompositeCacheManager;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.Primary;
import org.springframework.data.redis.cache.RedisCacheConfiguration;
import org.springframework.data.redis.cache.RedisCacheManager;
import org.springframework.data.redis.connection.RedisConnectionFactory;
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.data.redis.serializer.GenericJackson2JsonRedisSerializer;
import org.springframework.data.redis.serializer.RedisSerializationContext;
import org.springframework.data.redis.serializer.StringRedisSerializer;

import java.time.Duration;
import java.util.Arrays;
import java.util.HashMap;
import java.util.Map;

/**
 * Redis  
 * 
 *    Redis    .
 * 
 * :
 * -   prefix  ( )
 * -    (Caffeine)
 * -   
 * -   
 * -   
 */
@Configuration
@EnableCaching
@Slf4j
public class RedisCacheConfig implements CachingConfigurer {

    @Value("${spring.application.name:newsinsight}")
    private String applicationName;

    @Value("${spring.data.redis.enabled:true}")
    private boolean redisEnabled;

    //  TTL 
    @Value("${cache.chat-sessions.ttl-hours:2}")
    private int chatSessionsTtlHours;

    @Value("${cache.chat-messages.ttl-minutes:30}")
    private int chatMessagesTtlMinutes;

    @Value("${cache.default.ttl-hours:24}")
    private int defaultTtlHours;

    //   
    @Value("${cache.local.max-size:1000}")
    private int localCacheMaxSize;

    @Value("${cache.local.ttl-minutes:10}")
    private int localCacheTtlMinutes;

    private final MeterRegistry meterRegistry;

    public RedisCacheConfig(MeterRegistry meterRegistry) {
        this.meterRegistry = meterRegistry;
    }

    /**
     * ObjectMapper  (  )
     */
    private ObjectMapper createCacheObjectMapper() {
        ObjectMapper mapper = new ObjectMapper();
        mapper.registerModule(new JavaTimeModule());
        mapper.activateDefaultTyping(
                LaissezFaireSubTypeValidator.instance,
                ObjectMapper.DefaultTyping.NON_FINAL,
                JsonTypeInfo.As.PROPERTY
        );
        return mapper;
    }

    /**
     * Redis   
     */
    @Bean
    public RedisCacheManager redisCacheManager(RedisConnectionFactory connectionFactory) {
        //   prefix 
        String keyPrefix = applicationName + ":cache:";

        //   
        RedisCacheConfiguration defaultConfig = RedisCacheConfiguration.defaultCacheConfig()
                .entryTtl(Duration.ofHours(defaultTtlHours))
                .prefixCacheNameWith(keyPrefix)
                .serializeKeysWith(
                        RedisSerializationContext.SerializationPair.fromSerializer(
                                new StringRedisSerializer()
                        )
                )
                .serializeValuesWith(
                        RedisSerializationContext.SerializationPair.fromSerializer(
                                new GenericJackson2JsonRedisSerializer(createCacheObjectMapper())
                        )
                )
                // null   
                .disableCachingNullValues();

        //  
        Map<String, RedisCacheConfiguration> cacheConfigurations = new HashMap<>();
        
        //   : 2
        cacheConfigurations.put("chatSessions", 
                defaultConfig.entryTtl(Duration.ofHours(chatSessionsTtlHours)));
        
        //   : 30
        cacheConfigurations.put("chatMessages", 
                defaultConfig.entryTtl(Duration.ofMinutes(chatMessagesTtlMinutes)));
        
        //    : 1
        cacheConfigurations.put("userSessions", 
                defaultConfig.entryTtl(Duration.ofHours(1)));
        
        //   : 6
        cacheConfigurations.put("factCheckResults", 
                defaultConfig.entryTtl(Duration.ofHours(6)));
        
        //    : 1
        cacheConfigurations.put("similarQuestions", 
                defaultConfig.entryTtl(Duration.ofHours(1)));
        
        //   : 5 (  )
        cacheConfigurations.put("searchResults", 
                defaultConfig.entryTtl(Duration.ofMinutes(5)));
        
        // DB   : 10
        cacheConfigurations.put("dbSearchResults", 
                defaultConfig.entryTtl(Duration.ofMinutes(10)));

        RedisCacheManager cacheManager = RedisCacheManager.builder(connectionFactory)
                .cacheDefaults(defaultConfig)
                .withInitialCacheConfigurations(cacheConfigurations)
                .enableStatistics() //  
                .build();

        log.info("Redis Cache Manager initialized with prefix: {}", keyPrefix);
        return cacheManager;
    }

    /**
     *    (Caffeine) - 
     */
    @Bean
    public CaffeineCacheManager caffeineCacheManager() {
        CaffeineCacheManager cacheManager = new CaffeineCacheManager();
        cacheManager.setCaffeine(
                com.github.benmanes.caffeine.cache.Caffeine.newBuilder()
                        .maximumSize(localCacheMaxSize)
                        .expireAfterWrite(Duration.ofMinutes(localCacheTtlMinutes))
                        .recordStats() //  
        );
        cacheManager.setCacheNames(Arrays.asList(
                "chatSessions", 
                "chatMessages", 
                "userSessions",
                "factCheckResults",
                "similarQuestions",
                "searchResults",
                "dbSearchResults"
        ));

        log.info("Caffeine Cache Manager initialized (fallback)");
        return cacheManager;
    }

    /**
     *    (Redis , Caffeine )
     */
    @Bean
    @Primary
    @Override
    public CacheManager cacheManager() {
        CompositeCacheManager compositeCacheManager = new CompositeCacheManager();
        
        // Redis   Redis  
        //   Caffeine 
        if (redisEnabled) {
            log.info("Using Redis as primary cache with Caffeine fallback");
        } else {
            log.info("Redis disabled, using Caffeine as primary cache");
        }
        
        compositeCacheManager.setFallbackToNoOpCache(false);
        return compositeCacheManager;
    }

    /**
     * Redis   Primary  
     */
    @Bean("primaryCacheManager")
    public CacheManager primaryCacheManager(RedisConnectionFactory connectionFactory) {
        return redisCacheManager(connectionFactory);
    }

    /**
     * RedisTemplate 
     */
    @Bean
    public RedisTemplate<String, Object> redisTemplate(RedisConnectionFactory connectionFactory) {
        RedisTemplate<String, Object> template = new RedisTemplate<>();
        template.setConnectionFactory(connectionFactory);
        
        //  String 
        template.setKeySerializer(new StringRedisSerializer());
        template.setHashKeySerializer(new StringRedisSerializer());
        
        //  JSON 
        GenericJackson2JsonRedisSerializer jsonSerializer = 
                new GenericJackson2JsonRedisSerializer(createCacheObjectMapper());
        template.setValueSerializer(jsonSerializer);
        template.setHashValueSerializer(jsonSerializer);
        
        template.afterPropertiesSet();
        return template;
    }

    /**
     *    - Redis      
     */
    @Override
    public CacheErrorHandler errorHandler() {
        return new CacheErrorHandler() {
            @Override
            public void handleCacheGetError(RuntimeException exception, Cache cache, Object key) {
                log.warn("Cache GET error - cache: {}, key: {}, error: {}", 
                        cache.getName(), key, exception.getMessage());
                //  
                meterRegistry.counter("cache.error", 
                        "cache", cache.getName(), 
                        "operation", "get").increment();
            }

            @Override
            public void handleCachePutError(RuntimeException exception, Cache cache, Object key, Object value) {
                log.warn("Cache PUT error - cache: {}, key: {}, error: {}", 
                        cache.getName(), key, exception.getMessage());
                meterRegistry.counter("cache.error", 
                        "cache", cache.getName(), 
                        "operation", "put").increment();
            }

            @Override
            public void handleCacheEvictError(RuntimeException exception, Cache cache, Object key) {
                log.warn("Cache EVICT error - cache: {}, key: {}, error: {}", 
                        cache.getName(), key, exception.getMessage());
                meterRegistry.counter("cache.error", 
                        "cache", cache.getName(), 
                        "operation", "evict").increment();
            }

            @Override
            public void handleCacheClearError(RuntimeException exception, Cache cache) {
                log.warn("Cache CLEAR error - cache: {}, error: {}", 
                        cache.getName(), exception.getMessage());
                meterRegistry.counter("cache.error", 
                        "cache", cache.getName(), 
                        "operation", "clear").increment();
            }
        };
    }
}

```
