# Project Code Snapshot

Generated at 2025-12-22T11:56:45.061Z

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/repository/AiSubTaskRepository.java

```java
package com.newsinsight.collector.repository;

import com.newsinsight.collector.entity.ai.AiProvider;
import com.newsinsight.collector.entity.ai.AiSubTask;
import com.newsinsight.collector.entity.ai.AiTaskStatus;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Modifying;
import org.springframework.data.jpa.repository.Query;
import org.springframework.data.repository.query.Param;
import org.springframework.stereotype.Repository;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Optional;

@Repository
public interface AiSubTaskRepository extends JpaRepository<AiSubTask, String> {

    /**
     * Find all sub-tasks for a job
     */
    List<AiSubTask> findByAiJobId(String jobId);

    /**
     * Find sub-tasks by job ID and status
     */
    List<AiSubTask> findByAiJobIdAndStatus(String jobId, AiTaskStatus status);

    /**
     * Find sub-tasks by provider
     */
    List<AiSubTask> findByProviderId(AiProvider providerId);

    /**
     * Find sub-tasks by status
     */
    Page<AiSubTask> findByStatus(AiTaskStatus status, Pageable pageable);

    /**
     * Find sub-task by job ID and provider ID
     */
    Optional<AiSubTask> findByAiJobIdAndProviderId(String jobId, AiProvider providerId);

    /**
     * Count sub-tasks by job ID and status
     */
    long countByAiJobIdAndStatus(String jobId, AiTaskStatus status);

    /**
     * Count sub-tasks by job ID
     */
    long countByAiJobId(String jobId);

    /**
     * Get status distribution for a job
     */
    @Query("SELECT t.status, COUNT(t) FROM AiSubTask t WHERE t.aiJob.id = :jobId GROUP BY t.status")
    List<Object[]> getStatusDistributionByJobId(@Param("jobId") String jobId);

    /**
     * Find pending tasks older than cutoff (for timeout)
     */
    @Query("SELECT t FROM AiSubTask t WHERE t.status IN ('PENDING', 'IN_PROGRESS') AND t.createdAt < :before")
    List<AiSubTask> findPendingTasksOlderThan(@Param("before") LocalDateTime before);

    /**
     * Mark timed out sub-tasks
     */
    @Modifying
    @Query("UPDATE AiSubTask t SET t.status = 'TIMEOUT', t.completedAt = CURRENT_TIMESTAMP " +
            "WHERE t.status IN ('PENDING', 'IN_PROGRESS') AND t.createdAt < :before")
    int markTimedOutTasks(@Param("before") LocalDateTime before);

    /**
     * Delete sub-tasks by job IDs
     */
    @Modifying
    @Query("DELETE FROM AiSubTask t WHERE t.aiJob.id IN :jobIds")
    int deleteByJobIds(@Param("jobIds") List<String> jobIds);

    /**
     * Check if all sub-tasks for a job are in terminal state
     */
    @Query("SELECT COUNT(t) = 0 FROM AiSubTask t WHERE t.aiJob.id = :jobId AND t.status IN ('PENDING', 'IN_PROGRESS')")
    boolean areAllTasksTerminal(@Param("jobId") String jobId);

    /**
     * Check if any sub-task for a job completed successfully
     */
    @Query("SELECT COUNT(t) > 0 FROM AiSubTask t WHERE t.aiJob.id = :jobId AND t.status = 'COMPLETED'")
    boolean hasCompletedTask(@Param("jobId") String jobId);

    /**
     * Check if all sub-tasks for a job completed successfully
     */
    @Query("SELECT COUNT(t) = (SELECT COUNT(t2) FROM AiSubTask t2 WHERE t2.aiJob.id = :jobId) " +
            "FROM AiSubTask t WHERE t.aiJob.id = :jobId AND t.status = 'COMPLETED'")
    boolean areAllTasksCompleted(@Param("jobId") String jobId);
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/repository/ArticleAnalysisRepository.java

```java
package com.newsinsight.collector.repository;

import com.newsinsight.collector.entity.analysis.ArticleAnalysis;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Query;
import org.springframework.data.repository.query.Param;
import org.springframework.stereotype.Repository;

import java.util.List;
import java.util.Optional;

@Repository
public interface ArticleAnalysisRepository extends JpaRepository<ArticleAnalysis, Long> {

    Optional<ArticleAnalysis> findByArticleId(Long articleId);

    List<ArticleAnalysis> findByArticleIdIn(List<Long> articleIds);

    @Query("SELECT a FROM ArticleAnalysis a WHERE a.fullyAnalyzed = false")
    List<ArticleAnalysis> findIncompleteAnalyses();

    @Query("SELECT a FROM ArticleAnalysis a WHERE a.reliabilityScore >= :minScore")
    List<ArticleAnalysis> findByReliabilityScoreGreaterThanEqual(@Param("minScore") Double minScore);

    @Query("SELECT a FROM ArticleAnalysis a WHERE a.misinfoRisk = :risk")
    List<ArticleAnalysis> findByMisinfoRisk(@Param("risk") String risk);

    @Query("SELECT a.articleId FROM ArticleAnalysis a WHERE a.articleId IN :articleIds")
    List<Long> findAnalyzedArticleIds(@Param("articleIds") List<Long> articleIds);

    boolean existsByArticleId(Long articleId);
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/repository/ArticleDiscussionRepository.java

```java
package com.newsinsight.collector.repository;

import com.newsinsight.collector.entity.analysis.ArticleDiscussion;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import java.util.List;
import java.util.Optional;

@Repository
public interface ArticleDiscussionRepository extends JpaRepository<ArticleDiscussion, Long> {

    Optional<ArticleDiscussion> findByArticleId(Long articleId);

    List<ArticleDiscussion> findByArticleIdIn(List<Long> articleIds);

    boolean existsByArticleId(Long articleId);
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/repository/BrowserJobHistoryRepository.java

```java
package com.newsinsight.collector.repository;

import com.newsinsight.collector.entity.browser.BrowserJobHistory;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Modifying;
import org.springframework.data.jpa.repository.Query;
import org.springframework.data.repository.query.Param;
import org.springframework.stereotype.Repository;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Optional;

/**
 * Repository for BrowserJobHistory entity.
 * Manages browser automation job history persistence.
 */
@Repository
public interface BrowserJobHistoryRepository extends JpaRepository<BrowserJobHistory, Long> {

    /**
     * Find by job ID
     */
    Optional<BrowserJobHistory> findByJobId(String jobId);

    /**
     * Find by user ID
     */
    Page<BrowserJobHistory> findByUserIdOrderByCreatedAtDesc(String userId, Pageable pageable);

    /**
     * Find by session ID
     */
    Page<BrowserJobHistory> findBySessionIdOrderByCreatedAtDesc(String sessionId, Pageable pageable);

    /**
     * Find by status
     */
    Page<BrowserJobHistory> findByStatus(BrowserJobHistory.BrowserJobStatus status, Pageable pageable);

    /**
     * Find by user and status
     */
    Page<BrowserJobHistory> findByUserIdAndStatus(
            String userId,
            BrowserJobHistory.BrowserJobStatus status,
            Pageable pageable
    );

    /**
     * Find active jobs (PENDING, RUNNING, WAITING_HUMAN)
     */
    @Query("""
            SELECT b FROM BrowserJobHistory b 
            WHERE b.status IN ('PENDING', 'RUNNING', 'WAITING_HUMAN')
            ORDER BY b.createdAt DESC
            """)
    List<BrowserJobHistory> findActiveJobs();

    /**
     * Find active jobs by user
     */
    @Query("""
            SELECT b FROM BrowserJobHistory b 
            WHERE b.userId = :userId 
            AND b.status IN ('PENDING', 'RUNNING', 'WAITING_HUMAN')
            ORDER BY b.createdAt DESC
            """)
    List<BrowserJobHistory> findActiveJobsByUser(@Param("userId") String userId);

    /**
     * Find by project ID
     */
    Page<BrowserJobHistory> findByProjectIdOrderByCreatedAtDesc(Long projectId, Pageable pageable);

    /**
     * Find by related search history ID
     */
    List<BrowserJobHistory> findBySearchHistoryIdOrderByCreatedAtDesc(Long searchHistoryId);

    /**
     * Update job status
     */
    @Modifying
    @Query("""
            UPDATE BrowserJobHistory b 
            SET b.status = :status, b.updatedAt = :updatedAt 
            WHERE b.jobId = :jobId
            """)
    void updateStatus(
            @Param("jobId") String jobId,
            @Param("status") BrowserJobHistory.BrowserJobStatus status,
            @Param("updatedAt") LocalDateTime updatedAt
    );

    /**
     * Count jobs by status
     */
    long countByStatus(BrowserJobHistory.BrowserJobStatus status);

    /**
     * Count jobs by user and status
     */
    long countByUserIdAndStatus(String userId, BrowserJobHistory.BrowserJobStatus status);

    /**
     * Find jobs completed within time range
     */
    Page<BrowserJobHistory> findByStatusAndCompletedAtAfter(
            BrowserJobHistory.BrowserJobStatus status,
            LocalDateTime after,
            Pageable pageable
    );

    /**
     * Delete old completed jobs (cleanup)
     */
    @Modifying
    @Query("""
            DELETE FROM BrowserJobHistory b 
            WHERE b.status IN ('COMPLETED', 'FAILED', 'CANCELLED', 'TIMEOUT') 
            AND b.completedAt < :before
            """)
    void deleteOldCompletedJobs(@Param("before") LocalDateTime before);

    /**
     * Get statistics by status
     */
    @Query("""
            SELECT b.status as status, COUNT(b) as count, AVG(b.durationMs) as avgDuration
            FROM BrowserJobHistory b
            WHERE b.createdAt > :after
            GROUP BY b.status
            """)
    List<BrowserJobStats> getStatsByStatus(@Param("after") LocalDateTime after);

    interface BrowserJobStats {
        BrowserJobHistory.BrowserJobStatus getStatus();
        Long getCount();
        Double getAvgDuration();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/repository/ChatHistoryRepository.java

```java
package com.newsinsight.collector.repository;

import com.newsinsight.collector.entity.chat.ChatHistory;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Query;
import org.springframework.stereotype.Repository;

import java.time.LocalDateTime;
import java.util.List;

/**
 * 채팅 이력 리포지토리 (PostgreSQL)
 */
@Repository
public interface ChatHistoryRepository extends JpaRepository<ChatHistory, Long> {

    /**
     * 세션 ID로 메시지 조회
     */
    List<ChatHistory> findBySessionIdOrderByCreatedAtAsc(String sessionId);

    /**
     * 사용자 ID로 메시지 조회
     */
    List<ChatHistory> findByUserIdOrderByCreatedAtDesc(String userId);

    /**
     * 메시지 ID 존재 여부 확인
     */
    boolean existsByMessageId(String messageId);

    /**
     * 특정 기간 내 메시지 조회
     */
    List<ChatHistory> findByCreatedAtBetween(LocalDateTime start, LocalDateTime end);

    /**
     * 임베딩이 필요한 메시지 조회 (assistant 메시지만)
     */
    @Query("SELECT ch FROM ChatHistory ch WHERE ch.role = 'assistant' AND ch.embeddingId IS NULL")
    List<ChatHistory> findMessagesNeedingEmbedding();
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/repository/CollectedDataRepository.java

```java
package com.newsinsight.collector.repository;

import com.newsinsight.collector.entity.CollectedData;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Query;
import org.springframework.data.repository.query.Param;
import org.springframework.stereotype.Repository;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Optional;

@Repository
public interface CollectedDataRepository extends JpaRepository<CollectedData, Long> {

    Optional<CollectedData> findByContentHash(String contentHash);

    List<CollectedData> findBySourceIdOrderByCollectedAtDesc(Long sourceId);

    Page<CollectedData> findBySourceId(Long sourceId, Pageable pageable);

    List<CollectedData> findByProcessedFalse();

    Page<CollectedData> findByProcessedFalse(Pageable pageable);

    Page<CollectedData> findByProcessed(Boolean processed, Pageable pageable);

    long countByProcessedFalse();

    @Query("SELECT COUNT(cd) FROM CollectedData cd WHERE cd.collectedAt >= :startDate")
    long countCollectedSince(@Param("startDate") LocalDateTime startDate);

    @Query("SELECT COUNT(cd) FROM CollectedData cd WHERE cd.sourceId = :sourceId")
    long countBySourceId(@Param("sourceId") Long sourceId);

    /**
     * Full-Text Search with date filter using PostgreSQL tsvector.
     * Uses plainto_tsquery for natural language queries (handles Korean well).
     * Falls back to LIKE for very short queries (1-2 chars).
     * Results are ranked by FTS relevance, then by date.
     */
    @Query(value = """
        SELECT * FROM collected_data cd
        WHERE (
            :query IS NULL OR :query = '' OR
            CASE 
                WHEN LENGTH(:query) <= 2 THEN 
                    LOWER(cd.title) LIKE LOWER(CONCAT('%', :query, '%')) 
                    OR LOWER(cd.content) LIKE LOWER(CONCAT('%', :query, '%'))
                ELSE 
                    cd.search_vector @@ plainto_tsquery('simple', :query)
            END
        )
        AND ((cd.published_date IS NOT NULL AND cd.published_date >= :since)
          OR (cd.published_date IS NULL AND cd.collected_at >= :since))
        ORDER BY 
            CASE WHEN :query IS NOT NULL AND :query != '' AND LENGTH(:query) > 2 
                 THEN ts_rank(cd.search_vector, plainto_tsquery('simple', :query)) 
                 ELSE 0 END DESC,
            COALESCE(cd.published_date, cd.collected_at) DESC
        """,
        countQuery = """
        SELECT COUNT(*) FROM collected_data cd
        WHERE (
            :query IS NULL OR :query = '' OR
            CASE 
                WHEN LENGTH(:query) <= 2 THEN 
                    LOWER(cd.title) LIKE LOWER(CONCAT('%', :query, '%')) 
                    OR LOWER(cd.content) LIKE LOWER(CONCAT('%', :query, '%'))
                ELSE 
                    cd.search_vector @@ plainto_tsquery('simple', :query)
            END
        )
        AND ((cd.published_date IS NOT NULL AND cd.published_date >= :since)
          OR (cd.published_date IS NULL AND cd.collected_at >= :since))
        """,
        nativeQuery = true)
    Page<CollectedData> searchByQueryAndSince(@Param("query") String query,
                                              @Param("since") LocalDateTime since,
                                              Pageable pageable);

    /**
     * Full-Text Search without date filter.
     * Uses plainto_tsquery for natural language queries.
     * Falls back to LIKE for very short queries (1-2 chars).
     */
    @Query(value = """
        SELECT * FROM collected_data cd
        WHERE (
            :query IS NULL OR :query = '' OR
            CASE 
                WHEN LENGTH(:query) <= 2 THEN 
                    LOWER(cd.title) LIKE LOWER(CONCAT('%', :query, '%')) 
                    OR LOWER(cd.content) LIKE LOWER(CONCAT('%', :query, '%'))
                ELSE 
                    cd.search_vector @@ plainto_tsquery('simple', :query)
            END
        )
        ORDER BY 
            CASE WHEN :query IS NOT NULL AND :query != '' AND LENGTH(:query) > 2 
                 THEN ts_rank(cd.search_vector, plainto_tsquery('simple', :query)) 
                 ELSE 0 END DESC,
            COALESCE(cd.published_date, cd.collected_at) DESC
        """,
        countQuery = """
        SELECT COUNT(*) FROM collected_data cd
        WHERE (
            :query IS NULL OR :query = '' OR
            CASE 
                WHEN LENGTH(:query) <= 2 THEN 
                    LOWER(cd.title) LIKE LOWER(CONCAT('%', :query, '%')) 
                    OR LOWER(cd.content) LIKE LOWER(CONCAT('%', :query, '%'))
                ELSE 
                    cd.search_vector @@ plainto_tsquery('simple', :query)
            END
        )
        """,
        nativeQuery = true)
    Page<CollectedData> searchByQuery(@Param("query") String query,
                                      Pageable pageable);

    /**
     * Full-Text Search with custom date range (start and end date).
     * Uses plainto_tsquery for natural language queries (handles Korean well).
     * Falls back to LIKE for very short queries (1-2 chars).
     * Results are ranked by FTS relevance, then by date.
     * 
     * @param query Search query
     * @param since Start date (inclusive)
     * @param until End date (inclusive)
     * @param pageable Pagination info
     * @return Page of matching articles within the date range
     */
    @Query(value = """
        SELECT * FROM collected_data cd
        WHERE (
            :query IS NULL OR :query = '' OR
            CASE 
                WHEN LENGTH(:query) <= 2 THEN 
                    LOWER(cd.title) LIKE LOWER(CONCAT('%', :query, '%')) 
                    OR LOWER(cd.content) LIKE LOWER(CONCAT('%', :query, '%'))
                ELSE 
                    cd.search_vector @@ plainto_tsquery('simple', :query)
            END
        )
        AND ((cd.published_date IS NOT NULL AND cd.published_date >= :since AND cd.published_date <= :until)
          OR (cd.published_date IS NULL AND cd.collected_at >= :since AND cd.collected_at <= :until))
        ORDER BY 
            CASE WHEN :query IS NOT NULL AND :query != '' AND LENGTH(:query) > 2 
                 THEN ts_rank(cd.search_vector, plainto_tsquery('simple', :query)) 
                 ELSE 0 END DESC,
            COALESCE(cd.published_date, cd.collected_at) DESC
        """,
        countQuery = """
        SELECT COUNT(*) FROM collected_data cd
        WHERE (
            :query IS NULL OR :query = '' OR
            CASE 
                WHEN LENGTH(:query) <= 2 THEN 
                    LOWER(cd.title) LIKE LOWER(CONCAT('%', :query, '%')) 
                    OR LOWER(cd.content) LIKE LOWER(CONCAT('%', :query, '%'))
                ELSE 
                    cd.search_vector @@ plainto_tsquery('simple', :query)
            END
        )
        AND ((cd.published_date IS NOT NULL AND cd.published_date >= :since AND cd.published_date <= :until)
          OR (cd.published_date IS NULL AND cd.collected_at >= :since AND cd.collected_at <= :until))
        """,
        nativeQuery = true)
    Page<CollectedData> searchByQueryAndDateRange(@Param("query") String query,
                                                  @Param("since") LocalDateTime since,
                                                  @Param("until") LocalDateTime until,
                                                  Pageable pageable);

    boolean existsByContentHash(String contentHash);
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/repository/CollectionJobRepository.java

```java
package com.newsinsight.collector.repository;

import com.newsinsight.collector.entity.CollectionJob;
import com.newsinsight.collector.entity.CollectionJob.JobStatus;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Query;
import org.springframework.data.repository.query.Param;
import org.springframework.stereotype.Repository;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Optional;

@Repository
public interface CollectionJobRepository extends JpaRepository<CollectionJob, Long> {

    List<CollectionJob> findBySourceIdOrderByCreatedAtDesc(Long sourceId);

    Page<CollectionJob> findBySourceId(Long sourceId, Pageable pageable);

    List<CollectionJob> findByStatus(JobStatus status);

    Page<CollectionJob> findByStatus(JobStatus status, Pageable pageable);

    Optional<CollectionJob> findFirstBySourceIdAndStatusOrderByCreatedAtDesc(
        Long sourceId, JobStatus status);

    @Query("SELECT cj FROM CollectionJob cj WHERE cj.status = :status " +
           "AND cj.startedAt < :threshold")
    List<CollectionJob> findStaleJobs(
        @Param("status") JobStatus status,
        @Param("threshold") LocalDateTime threshold);

    @Query("SELECT cj FROM CollectionJob cj WHERE cj.createdAt >= :startDate " +
           "ORDER BY cj.createdAt DESC")
    List<CollectionJob> findRecentJobs(@Param("startDate") LocalDateTime startDate);

    List<CollectionJob> findByStatusAndCompletedAtBefore(JobStatus status, LocalDateTime completedAt);

    long countByStatus(JobStatus status);
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/repository/CrawlEvidenceRepository.java

```java
package com.newsinsight.collector.repository;

import com.newsinsight.collector.entity.CrawlEvidence;
import com.newsinsight.collector.entity.EvidenceStance;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Modifying;
import org.springframework.data.jpa.repository.Query;
import org.springframework.data.repository.query.Param;
import org.springframework.stereotype.Repository;

import java.util.List;

@Repository
public interface CrawlEvidenceRepository extends JpaRepository<CrawlEvidence, Long> {

    /**
     * Find all evidence for a job
     */
    List<CrawlEvidence> findByJobId(String jobId);

    /**
     * Find evidence by job ID with pagination
     */
    Page<CrawlEvidence> findByJobId(String jobId, Pageable pageable);

    /**
     * Find evidence by job ID and stance
     */
    List<CrawlEvidence> findByJobIdAndStance(String jobId, EvidenceStance stance);

    /**
     * Count evidence by job ID
     */
    long countByJobId(String jobId);

    /**
     * Count evidence by stance for a job
     */
    long countByJobIdAndStance(String jobId, EvidenceStance stance);

    /**
     * Delete all evidence for a job
     */
    @Modifying
    @Query("DELETE FROM CrawlEvidence e WHERE e.jobId = :jobId")
    int deleteByJobId(@Param("jobId") String jobId);

    /**
     * Delete evidence for multiple jobs
     */
    @Modifying
    @Query("DELETE FROM CrawlEvidence e WHERE e.jobId IN :jobIds")
    int deleteByJobIdIn(@Param("jobIds") List<String> jobIds);

    /**
     * Search evidence by snippet content
     */
    @Query("SELECT e FROM CrawlEvidence e WHERE e.jobId = :jobId AND " +
            "(LOWER(e.snippet) LIKE LOWER(CONCAT('%', :keyword, '%')) OR " +
            "LOWER(e.title) LIKE LOWER(CONCAT('%', :keyword, '%')))")
    List<CrawlEvidence> searchByKeyword(
            @Param("jobId") String jobId,
            @Param("keyword") String keyword
    );

    /**
     * Get stance distribution for a job
     */
    @Query("SELECT e.stance, COUNT(e) FROM CrawlEvidence e WHERE e.jobId = :jobId GROUP BY e.stance")
    List<Object[]> getStanceDistribution(@Param("jobId") String jobId);
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/repository/CrawlJobRepository.java

```java
package com.newsinsight.collector.repository;

import com.newsinsight.collector.entity.CrawlJob;
import com.newsinsight.collector.entity.CrawlJobStatus;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Modifying;
import org.springframework.data.jpa.repository.Query;
import org.springframework.data.repository.query.Param;
import org.springframework.stereotype.Repository;

import java.time.LocalDateTime;
import java.util.List;

@Repository
public interface CrawlJobRepository extends JpaRepository<CrawlJob, String> {

    /**
     * Find jobs by status
     */
    Page<CrawlJob> findByStatus(CrawlJobStatus status, Pageable pageable);

    /**
     * Find jobs by topic containing the search term
     */
    Page<CrawlJob> findByTopicContainingIgnoreCase(String topic, Pageable pageable);

    /**
     * Find pending jobs older than a given time (for timeout handling)
     */
    @Query("SELECT j FROM CrawlJob j WHERE j.status IN :statuses AND j.createdAt < :before")
    List<CrawlJob> findByStatusInAndCreatedAtBefore(
            @Param("statuses") List<CrawlJobStatus> statuses,
            @Param("before") LocalDateTime before
    );

    /**
     * Find recent jobs by topic
     */
    @Query("SELECT j FROM CrawlJob j WHERE LOWER(j.topic) = LOWER(:topic) ORDER BY j.createdAt DESC")
    List<CrawlJob> findRecentByTopic(@Param("topic") String topic, Pageable pageable);

    /**
     * Count jobs by status
     */
    long countByStatus(CrawlJobStatus status);

    /**
     * Mark timed out jobs
     */
    @Modifying
    @Query("UPDATE CrawlJob j SET j.status = 'TIMEOUT', j.completedAt = CURRENT_TIMESTAMP " +
            "WHERE j.status IN ('PENDING', 'IN_PROGRESS') AND j.createdAt < :before")
    int markTimedOutJobs(@Param("before") LocalDateTime before);

    /**
     * Delete old completed/failed jobs
     */
    @Modifying
    @Query("DELETE FROM CrawlJob j WHERE j.status IN ('COMPLETED', 'FAILED', 'TIMEOUT', 'CANCELLED') " +
            "AND j.completedAt < :before")
    int deleteOldJobs(@Param("before") LocalDateTime before);

    /**
     * Find jobs created within a time range
     */
    Page<CrawlJob> findByCreatedAtBetween(LocalDateTime start, LocalDateTime end, Pageable pageable);
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/repository/CrawlTargetRepository.java

```java
package com.newsinsight.collector.repository;

import com.newsinsight.collector.entity.autocrawl.CrawlTarget;
import com.newsinsight.collector.entity.autocrawl.CrawlTargetStatus;
import com.newsinsight.collector.entity.autocrawl.DiscoverySource;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Modifying;
import org.springframework.data.jpa.repository.Query;
import org.springframework.data.repository.query.Param;
import org.springframework.stereotype.Repository;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Optional;

/**
 * 자동 크롤링 대상 URL 저장소.
 * 검색, 기사 분석 등에서 발견된 URL의 크롤링 대기열을 관리합니다.
 */
@Repository
public interface CrawlTargetRepository extends JpaRepository<CrawlTarget, Long> {
       

    /**
     * URL 해시로 기존 대상 조회 (중복 체크용)
     */
    Optional<CrawlTarget> findByUrlHash(String urlHash);

    /**
     * URL 해시 존재 여부 (빠른 중복 체크)
     */
    boolean existsByUrlHash(String urlHash);

    /**
     * 상태별 대상 조회
     */
    List<CrawlTarget> findByStatus(CrawlTargetStatus status);

    Page<CrawlTarget> findByStatus(CrawlTargetStatus status, Pageable pageable);

    /**
     * 대기 중인 대상을 우선순위 순으로 조회 (크롤링 큐)
     * 재시도 백오프 시간이 지난 대상만 포함
     */
    @Query("SELECT ct FROM CrawlTarget ct " +
           "WHERE ct.status = :status " +
           "AND (ct.nextAttemptAfter IS NULL OR ct.nextAttemptAfter <= :now) " +
           "ORDER BY ct.priority DESC, ct.discoveredAt ASC")
    List<CrawlTarget> findPendingTargetsOrderByPriority(
            @Param("status") CrawlTargetStatus status,
            @Param("now") LocalDateTime now,
            Pageable pageable);

    /**
     * PENDING 상태의 대상 중 크롤링 가능한 대상 조회 (우선순위 순)
     */
    default List<CrawlTarget> findReadyToCrawl(int limit) {
        return findPendingTargetsOrderByPriority(
                CrawlTargetStatus.PENDING,
                LocalDateTime.now(),
                Pageable.ofSize(limit));
    }

    /**
     * 도메인별 대상 조회
     */
    List<CrawlTarget> findByDomain(String domain);

    /**
     * 발견 출처별 대상 조회
     */
    List<CrawlTarget> findByDiscoverySource(DiscoverySource source);

    Page<CrawlTarget> findByDiscoverySource(DiscoverySource source, Pageable pageable);

    /**
     * 특정 기간 내 발견된 대상 조회
     */
    List<CrawlTarget> findByDiscoveredAtAfter(LocalDateTime since);

    /**
     * 키워드 관련 대상 조회 (LIKE 검색)
     */
    @Query("SELECT ct FROM CrawlTarget ct WHERE ct.relatedKeywords LIKE %:keyword%")
    List<CrawlTarget> findByRelatedKeywordsContaining(@Param("keyword") String keyword);

    /**
     * 상태별 카운트
     */
    long countByStatus(CrawlTargetStatus status);

    /**
     * 발견 출처별 카운트
     */
    long countByDiscoverySource(DiscoverySource source);

    /**
     * 오래된 완료/실패 대상 정리
     */
    @Modifying
    @Query("DELETE FROM CrawlTarget ct WHERE ct.status IN :statuses AND ct.updatedAt < :before")
    int deleteOldTargets(@Param("statuses") List<CrawlTargetStatus> statuses, 
                         @Param("before") LocalDateTime before);

    /**
     * 오래 대기 중인 대상 정리 (7일 이상 PENDING인 경우)
     */
    @Modifying
    @Query("UPDATE CrawlTarget ct SET ct.status = 'EXPIRED' " +
           "WHERE ct.status = 'PENDING' AND ct.discoveredAt < :before")
    int expireOldPendingTargets(@Param("before") LocalDateTime before);

    /**
     * IN_PROGRESS 상태로 오래 멈춘 대상 복구 (타임아웃)
     */
    @Modifying
    @Query("UPDATE CrawlTarget ct SET ct.status = 'PENDING', ct.retryCount = ct.retryCount + 1 " +
           "WHERE ct.status = 'IN_PROGRESS' AND ct.lastAttemptAt < :timeout")
    int recoverStuckTargets(@Param("timeout") LocalDateTime timeout);

    /**
     * 도메인별 대기 중 대상 수 (도메인별 rate limiting용)
     */
    @Query("SELECT ct.domain, COUNT(ct) FROM CrawlTarget ct " +
           "WHERE ct.status = 'PENDING' GROUP BY ct.domain ORDER BY COUNT(ct) DESC")
    List<Object[]> countPendingByDomain();

    /**
     * 최근 N일간 발견된 대상 통계
     */
    @Query("SELECT ct.discoverySource, COUNT(ct) FROM CrawlTarget ct " +
           "WHERE ct.discoveredAt > :since GROUP BY ct.discoverySource")
    List<Object[]> getDiscoveryStatsSince(@Param("since") LocalDateTime since);

    /**
     * 최근 N일간 완료된 대상 통계
     */
    @Query("SELECT DATE(ct.completedAt), COUNT(ct) FROM CrawlTarget ct " +
           "WHERE ct.status = 'COMPLETED' AND ct.completedAt > :since " +
           "GROUP BY DATE(ct.completedAt) ORDER BY DATE(ct.completedAt)")
    List<Object[]> getCompletedStatsByDateSince(@Param("since") LocalDateTime since);
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/repository/DataSourceRepository.java

```java
package com.newsinsight.collector.repository;

import com.newsinsight.collector.entity.DataSource;
import com.newsinsight.collector.entity.SourceType;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Query;
import org.springframework.stereotype.Repository;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Optional;

@Repository
public interface DataSourceRepository extends JpaRepository<DataSource, Long> {

    List<DataSource> findByIsActiveTrue();

    List<DataSource> findBySourceType(SourceType sourceType);

    List<DataSource> findByIsActiveTrueAndSourceType(SourceType sourceType);

    /**
     * Find active web search sources ordered by priority.
     * Lower priority number = higher priority.
     */
    @Query("SELECT ds FROM DataSource ds WHERE ds.isActive = true " +
           "AND ds.sourceType = 'WEB_SEARCH' " +
           "AND ds.searchUrlTemplate IS NOT NULL " +
           "ORDER BY ds.searchPriority ASC")
    List<DataSource> findActiveWebSearchSources();

    Optional<DataSource> findByName(String name);

    /**
     * Find a DataSource by URL.
     * Returns the first match if duplicates exist (to handle legacy data).
     */
    Optional<DataSource> findFirstByUrl(String url);

    @Query("SELECT ds FROM DataSource ds WHERE ds.isActive = true " +
           "AND (ds.lastCollected IS NULL OR ds.lastCollected < :threshold)")
    List<DataSource> findDueForCollection(LocalDateTime threshold);

    long countByIsActiveTrue();
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/repository/DraftSearchRepository.java

```java
package com.newsinsight.collector.repository;

import com.newsinsight.collector.entity.search.DraftSearch;
import com.newsinsight.collector.entity.search.SearchType;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Modifying;
import org.springframework.data.jpa.repository.Query;
import org.springframework.data.repository.query.Param;
import org.springframework.stereotype.Repository;

import java.time.LocalDateTime;
import java.util.List;

/**
 * Repository for DraftSearch entity.
 * Manages draft/unsaved search persistence.
 */
@Repository
public interface DraftSearchRepository extends JpaRepository<DraftSearch, Long> {

    /**
     * Find unexecuted drafts by user
     */
    List<DraftSearch> findByUserIdAndExecutedFalseOrderByCreatedAtDesc(String userId);

    /**
     * Find unexecuted drafts by session
     */
    List<DraftSearch> findBySessionIdAndExecutedFalseOrderByCreatedAtDesc(String sessionId);

    /**
     * Find drafts by user or session (for anonymous users)
     */
    @Query("""
            SELECT d FROM DraftSearch d 
            WHERE (d.userId = :userId OR d.sessionId = :sessionId)
            AND d.executed = false
            ORDER BY d.createdAt DESC
            """)
    List<DraftSearch> findUnexecutedDrafts(
            @Param("userId") String userId,
            @Param("sessionId") String sessionId,
            Pageable pageable
    );

    /**
     * Find drafts by search type
     */
    Page<DraftSearch> findBySearchTypeAndExecutedFalse(SearchType searchType, Pageable pageable);

    /**
     * Find drafts for a project
     */
    List<DraftSearch> findByProjectIdAndExecutedFalseOrderByCreatedAtDesc(Long projectId);

    /**
     * Mark draft as executed
     */
    @Modifying
    @Query("""
            UPDATE DraftSearch d 
            SET d.executed = true, d.executedAt = :executedAt, d.searchHistoryId = :searchHistoryId 
            WHERE d.id = :id
            """)
    void markExecuted(
            @Param("id") Long id,
            @Param("executedAt") LocalDateTime executedAt,
            @Param("searchHistoryId") Long searchHistoryId
    );

    /**
     * Delete old executed drafts (cleanup)
     */
    @Modifying
    @Query("DELETE FROM DraftSearch d WHERE d.executed = true AND d.executedAt < :before")
    void deleteOldExecutedDrafts(@Param("before") LocalDateTime before);

    /**
     * Delete old unexecuted drafts (cleanup)
     */
    @Modifying
    @Query("DELETE FROM DraftSearch d WHERE d.executed = false AND d.createdAt < :before")
    void deleteOldUnexecutedDrafts(@Param("before") LocalDateTime before);

    /**
     * Count unexecuted drafts by user
     */
    long countByUserIdAndExecutedFalse(String userId);

    /**
     * Find recent drafts with similar query
     */
    @Query("""
            SELECT d FROM DraftSearch d 
            WHERE d.userId = :userId 
            AND LOWER(d.query) LIKE LOWER(CONCAT('%', :query, '%'))
            AND d.executed = false
            ORDER BY d.createdAt DESC
            """)
    List<DraftSearch> findSimilarDrafts(
            @Param("userId") String userId,
            @Param("query") String query,
            Pageable pageable
    );
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/repository/FactCheckChatSessionRepository.java

```java
package com.newsinsight.collector.repository;

import com.newsinsight.collector.entity.chat.FactCheckChatSession;
import org.springframework.data.mongodb.repository.MongoRepository;
import org.springframework.stereotype.Repository;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Optional;

/**
 * 팩트체크 챗봇 세션 리포지토리 (MongoDB)
 */
@Repository
public interface FactCheckChatSessionRepository extends MongoRepository<FactCheckChatSession, String> {

    /**
     * 세션 ID로 조회
     */
    Optional<FactCheckChatSession> findBySessionId(String sessionId);

    /**
     * 사용자 ID로 세션 목록 조회
     */
    List<FactCheckChatSession> findByUserIdOrderByStartedAtDesc(String userId);

    /**
     * 상태별 세션 조회
     */
    List<FactCheckChatSession> findByStatus(FactCheckChatSession.SessionStatus status);

    /**
     * RDB 동기화가 필요한 세션 조회
     */
    List<FactCheckChatSession> findBySyncedToRdbFalseAndStatusIn(List<FactCheckChatSession.SessionStatus> statuses);

    /**
     * 벡터 DB 임베딩이 필요한 세션 조회
     */
    List<FactCheckChatSession> findByEmbeddedToVectorDbFalseAndStatusIn(List<FactCheckChatSession.SessionStatus> statuses);

    /**
     * 특정 시간 이후 활동이 없는 세션 조회 (만료 처리용)
     */
    List<FactCheckChatSession> findByStatusAndLastActivityAtBefore(
            FactCheckChatSession.SessionStatus status, 
            LocalDateTime dateTime
    );

    /**
     * 세션 ID 존재 여부 확인
     */
    boolean existsBySessionId(String sessionId);
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/repository/GeneratedReportRepository.java

```java
package com.newsinsight.collector.repository;

import com.newsinsight.collector.entity.report.GeneratedReport;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Modifying;
import org.springframework.data.jpa.repository.Query;
import org.springframework.data.repository.query.Param;
import org.springframework.stereotype.Repository;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Optional;

/**
 * Repository for GeneratedReport entity.
 * Manages report persistence and retrieval.
 */
@Repository
public interface GeneratedReportRepository extends JpaRepository<GeneratedReport, Long> {

    /**
     * Find by search history ID
     */
    List<GeneratedReport> findBySearchHistoryIdOrderByCreatedAtDesc(Long searchHistoryId);

    /**
     * Find by user ID
     */
    Page<GeneratedReport> findByUserIdOrderByCreatedAtDesc(String userId, Pageable pageable);

    /**
     * Find by project ID
     */
    Page<GeneratedReport> findByProjectIdOrderByCreatedAtDesc(Long projectId, Pageable pageable);

    /**
     * Find by report type
     */
    Page<GeneratedReport> findByReportType(GeneratedReport.ReportType reportType, Pageable pageable);

    /**
     * Find by status
     */
    Page<GeneratedReport> findByStatus(GeneratedReport.ReportStatus status, Pageable pageable);

    /**
     * Find public reports
     */
    Page<GeneratedReport> findByIsPublicTrue(Pageable pageable);

    /**
     * Find by public URL
     */
    Optional<GeneratedReport> findByPublicUrl(String publicUrl);

    /**
     * Find pending reports
     */
    @Query("SELECT r FROM GeneratedReport r WHERE r.status IN ('PENDING', 'GENERATING') ORDER BY r.createdAt")
    List<GeneratedReport> findPendingReports();

    /**
     * Find expired public reports
     */
    @Query("""
            SELECT r FROM GeneratedReport r 
            WHERE r.isPublic = true 
            AND r.shareExpiresAt < :now
            """)
    List<GeneratedReport> findExpiredPublicReports(@Param("now") LocalDateTime now);

    /**
     * Update download count
     */
    @Modifying
    @Query("""
            UPDATE GeneratedReport r 
            SET r.downloadCount = r.downloadCount + 1, r.lastDownloadedAt = :downloadedAt 
            WHERE r.id = :id
            """)
    void incrementDownloadCount(@Param("id") Long id, @Param("downloadedAt") LocalDateTime downloadedAt);

    /**
     * Update status
     */
    @Modifying
    @Query("UPDATE GeneratedReport r SET r.status = :status WHERE r.id = :id")
    void updateStatus(@Param("id") Long id, @Param("status") GeneratedReport.ReportStatus status);

    /**
     * Mark as public
     */
    @Modifying
    @Query("""
            UPDATE GeneratedReport r 
            SET r.isPublic = true, r.publicUrl = :publicUrl, r.shareExpiresAt = :expiresAt 
            WHERE r.id = :id
            """)
    void makePublic(
            @Param("id") Long id,
            @Param("publicUrl") String publicUrl,
            @Param("expiresAt") LocalDateTime expiresAt
    );

    /**
     * Revoke public access
     */
    @Modifying
    @Query("UPDATE GeneratedReport r SET r.isPublic = false, r.publicUrl = null, r.shareExpiresAt = null WHERE r.id = :id")
    void revokePublicAccess(@Param("id") Long id);

    /**
     * Count by user
     */
    long countByUserId(String userId);

    /**
     * Count by project
     */
    long countByProjectId(Long projectId);

    /**
     * Delete old reports
     */
    @Modifying
    @Query("DELETE FROM GeneratedReport r WHERE r.createdAt < :before AND r.status IN ('COMPLETED', 'FAILED', 'EXPIRED')")
    void deleteOldReports(@Param("before") LocalDateTime before);

    /**
     * Get report statistics
     */
    @Query("""
            SELECT r.reportType as reportType, COUNT(r) as count, SUM(r.downloadCount) as totalDownloads
            FROM GeneratedReport r
            WHERE r.createdAt > :after
            GROUP BY r.reportType
            """)
    List<ReportStats> getStatsByType(@Param("after") LocalDateTime after);

    interface ReportStats {
        GeneratedReport.ReportType getReportType();
        Long getCount();
        Long getTotalDownloads();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/repository/LlmProviderSettingsRepository.java

```java
package com.newsinsight.collector.repository;

import com.newsinsight.collector.entity.settings.LlmProviderSettings;
import com.newsinsight.collector.entity.settings.LlmProviderType;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Modifying;
import org.springframework.data.jpa.repository.Query;
import org.springframework.data.repository.query.Param;
import org.springframework.stereotype.Repository;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Optional;

@Repository
public interface LlmProviderSettingsRepository extends JpaRepository<LlmProviderSettings, Long> {

    // === 전역(관리자) 설정 조회 ===

    /**
     * 전역 설정 전체 조회 (userId가 null인 것들)
     */
    @Query("SELECT s FROM LlmProviderSettings s WHERE s.userId IS NULL ORDER BY s.priority ASC")
    List<LlmProviderSettings> findAllGlobalSettings();

    /**
     * 활성화된 전역 설정만 조회
     */
    @Query("SELECT s FROM LlmProviderSettings s WHERE s.userId IS NULL AND s.enabled = true ORDER BY s.priority ASC")
    List<LlmProviderSettings> findEnabledGlobalSettings();

    /**
     * 특정 Provider의 전역 설정 조회
     */
    @Query("SELECT s FROM LlmProviderSettings s WHERE s.providerType = :providerType AND s.userId IS NULL")
    Optional<LlmProviderSettings> findGlobalByProviderType(@Param("providerType") LlmProviderType providerType);

    // === 사용자별 설정 조회 ===

    /**
     * 특정 사용자의 모든 설정 조회
     */
    List<LlmProviderSettings> findByUserIdOrderByPriorityAsc(String userId);

    /**
     * 특정 사용자의 활성화된 설정만 조회
     */
    List<LlmProviderSettings> findByUserIdAndEnabledTrueOrderByPriorityAsc(String userId);

    /**
     * 특정 사용자의 특정 Provider 설정 조회
     */
    Optional<LlmProviderSettings> findByProviderTypeAndUserId(LlmProviderType providerType, String userId);

    // === 유효(effective) 설정 조회 (사용자 설정 우선, 없으면 전역) ===

    /**
     * 특정 Provider의 유효 설정 조회 (사용자 > 전역 우선순위)
     */
    @Query("SELECT s FROM LlmProviderSettings s " +
           "WHERE s.providerType = :providerType " +
           "AND (s.userId = :userId OR s.userId IS NULL) " +
           "ORDER BY CASE WHEN s.userId IS NOT NULL THEN 0 ELSE 1 END, s.priority ASC")
    List<LlmProviderSettings> findEffectiveSettings(
            @Param("providerType") LlmProviderType providerType,
            @Param("userId") String userId
    );

    /**
     * 사용자에게 유효한 모든 활성화된 설정 조회
     * 사용자 설정이 있으면 그것을, 없으면 전역 설정 반환
     */
    @Query(value = """
        SELECT DISTINCT ON (provider_type) * FROM llm_provider_settings 
        WHERE (user_id = :userId OR user_id IS NULL) 
        AND enabled = true 
        ORDER BY provider_type, 
                 CASE WHEN user_id IS NOT NULL THEN 0 ELSE 1 END, 
                 priority ASC
        """, nativeQuery = true)
    List<LlmProviderSettings> findAllEffectiveSettingsForUser(@Param("userId") String userId);

    // === 업데이트 쿼리 ===

    @Modifying
    @Query("UPDATE LlmProviderSettings s SET s.enabled = :enabled WHERE s.id = :id")
    void updateEnabled(@Param("id") Long id, @Param("enabled") Boolean enabled);

    @Modifying
    @Query("UPDATE LlmProviderSettings s SET s.lastTestedAt = :testedAt, s.lastTestSuccess = :success WHERE s.id = :id")
    void updateTestResult(@Param("id") Long id, @Param("testedAt") LocalDateTime testedAt, @Param("success") Boolean success);

    // === 존재 여부 확인 ===

    boolean existsByProviderTypeAndUserId(LlmProviderType providerType, String userId);

    @Query("SELECT CASE WHEN COUNT(s) > 0 THEN true ELSE false END FROM LlmProviderSettings s " +
           "WHERE s.providerType = :providerType AND s.userId IS NULL")
    boolean existsGlobalByProviderType(@Param("providerType") LlmProviderType providerType);

    // === 삭제 ===

    void deleteByProviderTypeAndUserId(LlmProviderType providerType, String userId);

    @Modifying
    @Query("DELETE FROM LlmProviderSettings s WHERE s.providerType = :providerType AND s.userId IS NULL")
    void deleteGlobalByProviderType(@Param("providerType") LlmProviderType providerType);

    /**
     * 특정 사용자의 모든 설정 삭제
     */
    void deleteByUserId(String userId);
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/repository/MlAddonExecutionRepository.java

```java
package com.newsinsight.collector.repository;

import com.newsinsight.collector.entity.addon.ExecutionStatus;
import com.newsinsight.collector.entity.addon.MlAddonExecution;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.data.jpa.repository.EntityGraph;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Modifying;
import org.springframework.data.jpa.repository.Query;
import org.springframework.data.repository.query.Param;
import org.springframework.stereotype.Repository;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Optional;

@Repository
public interface MlAddonExecutionRepository extends JpaRepository<MlAddonExecution, Long> {

    Optional<MlAddonExecution> findByRequestId(String requestId);

    @EntityGraph(attributePaths = {"addon"})
    List<MlAddonExecution> findByArticleId(Long articleId);

    List<MlAddonExecution> findByBatchId(String batchId);

    List<MlAddonExecution> findByStatus(ExecutionStatus status);

    @EntityGraph(attributePaths = {"addon"})
    Page<MlAddonExecution> findByStatus(ExecutionStatus status, Pageable pageable);

    @EntityGraph(attributePaths = {"addon"})
    Page<MlAddonExecution> findByAddonId(Long addonId, Pageable pageable);

    @EntityGraph(attributePaths = {"addon"})
    @Override
    Page<MlAddonExecution> findAll(Pageable pageable);

    @Query("SELECT e FROM MlAddonExecution e WHERE e.articleId = :articleId AND e.addon.addonKey = :addonKey")
    Optional<MlAddonExecution> findByArticleIdAndAddonKey(@Param("articleId") Long articleId, @Param("addonKey") String addonKey);

    @Query("SELECT e FROM MlAddonExecution e WHERE e.status = 'PENDING' AND e.createdAt < :cutoff")
    List<MlAddonExecution> findStaleExecutions(@Param("cutoff") LocalDateTime cutoff);

    @Modifying
    @Query("UPDATE MlAddonExecution e SET e.status = 'TIMEOUT' WHERE e.status IN ('PENDING', 'RUNNING') AND e.createdAt < :cutoff")
    int markTimedOutExecutions(@Param("cutoff") LocalDateTime cutoff);

    @Query("SELECT COUNT(e) FROM MlAddonExecution e WHERE e.addon.id = :addonId AND e.status = :status AND e.createdAt > :since")
    long countByAddonAndStatusSince(@Param("addonId") Long addonId, @Param("status") ExecutionStatus status, @Param("since") LocalDateTime since);

    @Query("SELECT AVG(e.latencyMs) FROM MlAddonExecution e WHERE e.addon.id = :addonId AND e.status = 'SUCCESS' AND e.createdAt > :since")
    Double getAverageLatency(@Param("addonId") Long addonId, @Param("since") LocalDateTime since);

    @Modifying
    @Query("DELETE FROM MlAddonExecution e WHERE e.createdAt < :cutoff")
    int deleteOldExecutions(@Param("cutoff") LocalDateTime cutoff);

    /**
     * 특정 시간 이후의 모든 실행 기록 조회 (오늘의 통계 계산용)
     */
    List<MlAddonExecution> findByCreatedAtAfter(LocalDateTime since);
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/repository/MlAddonRepository.java

```java
package com.newsinsight.collector.repository;

import com.newsinsight.collector.entity.addon.AddonCategory;
import com.newsinsight.collector.entity.addon.AddonHealthStatus;
import com.newsinsight.collector.entity.addon.MlAddon;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Modifying;
import org.springframework.data.jpa.repository.Query;
import org.springframework.data.repository.query.Param;
import org.springframework.stereotype.Repository;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Optional;

@Repository
public interface MlAddonRepository extends JpaRepository<MlAddon, Long> {

    Optional<MlAddon> findByAddonKey(String addonKey);

    List<MlAddon> findByEnabledTrue();

    List<MlAddon> findByCategory(AddonCategory category);

    List<MlAddon> findByCategoryAndEnabledTrue(AddonCategory category);

    List<MlAddon> findByEnabledTrueOrderByPriorityAsc();

    @Query("SELECT a FROM MlAddon a WHERE a.enabled = true AND a.category IN :categories ORDER BY a.priority ASC")
    List<MlAddon> findEnabledByCategories(@Param("categories") List<AddonCategory> categories);

    @Query("SELECT a FROM MlAddon a WHERE a.enabled = true AND a.healthStatus = :status")
    List<MlAddon> findEnabledByHealthStatus(@Param("status") AddonHealthStatus status);

    @Modifying
    @Query("UPDATE MlAddon a SET a.healthStatus = :status, a.lastHealthCheck = :checkTime WHERE a.id = :id")
    void updateHealthStatus(@Param("id") Long id, @Param("status") AddonHealthStatus status, @Param("checkTime") LocalDateTime checkTime);

    @Modifying
    @Query("UPDATE MlAddon a SET a.enabled = false WHERE a.id = :id")
    void disableAddon(@Param("id") Long id);

    @Query("SELECT a FROM MlAddon a WHERE a.enabled = true AND a.healthCheckUrl IS NOT NULL AND " +
           "(a.lastHealthCheck IS NULL OR a.lastHealthCheck < :cutoff)")
    List<MlAddon> findAddonsNeedingHealthCheck(@Param("cutoff") LocalDateTime cutoff);

    boolean existsByAddonKey(String addonKey);
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/repository/ProjectActivityLogRepository.java

```java
package com.newsinsight.collector.repository;

import com.newsinsight.collector.entity.project.ProjectActivityLog;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Modifying;
import org.springframework.data.jpa.repository.Query;
import org.springframework.data.repository.query.Param;
import org.springframework.stereotype.Repository;

import java.time.LocalDateTime;
import java.util.List;

/**
 * Repository for ProjectActivityLog entity.
 */
@Repository
public interface ProjectActivityLogRepository extends JpaRepository<ProjectActivityLog, Long> {

    /**
     * Find by project ID
     */
    Page<ProjectActivityLog> findByProjectIdOrderByCreatedAtDesc(Long projectId, Pageable pageable);

    /**
     * Find by project ID and activity type
     */
    Page<ProjectActivityLog> findByProjectIdAndActivityType(
            Long projectId,
            ProjectActivityLog.ActivityType activityType,
            Pageable pageable
    );

    /**
     * Find by user ID
     */
    Page<ProjectActivityLog> findByUserIdOrderByCreatedAtDesc(String userId, Pageable pageable);

    /**
     * Find by project and user
     */
    Page<ProjectActivityLog> findByProjectIdAndUserId(Long projectId, String userId, Pageable pageable);

    /**
     * Find recent activities for project
     */
    List<ProjectActivityLog> findTop20ByProjectIdOrderByCreatedAtDesc(Long projectId);

    /**
     * Find activities within date range
     */
    Page<ProjectActivityLog> findByProjectIdAndCreatedAtBetween(
            Long projectId,
            LocalDateTime from,
            LocalDateTime to,
            Pageable pageable
    );

    /**
     * Find by entity
     */
    List<ProjectActivityLog> findByEntityTypeAndEntityIdOrderByCreatedAtDesc(String entityType, String entityId);

    /**
     * Get activity count by type
     */
    @Query("""
            SELECT a.activityType as activityType, COUNT(a) as count
            FROM ProjectActivityLog a
            WHERE a.projectId = :projectId
            AND a.createdAt > :after
            GROUP BY a.activityType
            """)
    List<ActivityTypeCount> getActivityCountByType(@Param("projectId") Long projectId, @Param("after") LocalDateTime after);

    /**
     * Get activity count by user
     */
    @Query("""
            SELECT a.userId as userId, COUNT(a) as count
            FROM ProjectActivityLog a
            WHERE a.projectId = :projectId
            AND a.createdAt > :after
            GROUP BY a.userId
            """)
    List<UserActivityCount> getActivityCountByUser(@Param("projectId") Long projectId, @Param("after") LocalDateTime after);

    /**
     * Delete old activities
     */
    @Modifying
    @Query("DELETE FROM ProjectActivityLog a WHERE a.createdAt < :before")
    void deleteOldActivities(@Param("before") LocalDateTime before);

    /**
     * Delete by project
     */
    void deleteByProjectId(Long projectId);

    interface ActivityTypeCount {
        ProjectActivityLog.ActivityType getActivityType();
        Long getCount();
    }

    interface UserActivityCount {
        String getUserId();
        Long getCount();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/repository/ProjectItemRepository.java

```java
package com.newsinsight.collector.repository;

import com.newsinsight.collector.entity.project.ProjectItem;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Modifying;
import org.springframework.data.jpa.repository.Query;
import org.springframework.data.repository.query.Param;
import org.springframework.stereotype.Repository;

import java.time.LocalDateTime;
import java.util.List;

/**
 * Repository for ProjectItem entity.
 */
@Repository
public interface ProjectItemRepository extends JpaRepository<ProjectItem, Long> {

    /**
     * Find by project ID
     */
    Page<ProjectItem> findByProjectIdOrderByAddedAtDesc(Long projectId, Pageable pageable);

    /**
     * Find by project ID and type
     */
    Page<ProjectItem> findByProjectIdAndItemType(Long projectId, ProjectItem.ItemType itemType, Pageable pageable);

    /**
     * Find bookmarked items
     */
    Page<ProjectItem> findByProjectIdAndBookmarkedTrue(Long projectId, Pageable pageable);

    /**
     * Find unread items
     */
    Page<ProjectItem> findByProjectIdAndIsReadFalse(Long projectId, Pageable pageable);

    /**
     * Find by importance
     */
    Page<ProjectItem> findByProjectIdAndImportanceGreaterThanEqual(Long projectId, Integer minImportance, Pageable pageable);

    /**
     * Find by category
     */
    Page<ProjectItem> findByProjectIdAndCategory(Long projectId, String category, Pageable pageable);

    /**
     * Find by source ID
     */
    List<ProjectItem> findBySourceIdAndSourceType(String sourceId, String sourceType);

    /**
     * Find by URL
     */
    List<ProjectItem> findByProjectIdAndUrl(Long projectId, String url);

    /**
     * Search by title
     */
    @Query("""
            SELECT i FROM ProjectItem i 
            WHERE i.projectId = :projectId 
            AND LOWER(i.title) LIKE LOWER(CONCAT('%', :query, '%'))
            ORDER BY i.addedAt DESC
            """)
    Page<ProjectItem> searchByTitle(@Param("projectId") Long projectId, @Param("query") String query, Pageable pageable);

    /**
     * Search by content
     */
    @Query("""
            SELECT i FROM ProjectItem i 
            WHERE i.projectId = :projectId 
            AND (LOWER(i.title) LIKE LOWER(CONCAT('%', :query, '%')) 
                 OR LOWER(i.summary) LIKE LOWER(CONCAT('%', :query, '%')))
            ORDER BY i.addedAt DESC
            """)
    Page<ProjectItem> searchByContent(@Param("projectId") Long projectId, @Param("query") String query, Pageable pageable);

    /**
     * Find by tag
     */
    @Query(value = """
            SELECT * FROM project_items 
            WHERE project_id = :projectId 
            AND tags @> :tag::jsonb
            ORDER BY added_at DESC
            """, nativeQuery = true)
    Page<ProjectItem> findByTag(@Param("projectId") Long projectId, @Param("tag") String tagJson, Pageable pageable);

    /**
     * Find items within date range
     */
    Page<ProjectItem> findByProjectIdAndPublishedAtBetween(
            Long projectId,
            LocalDateTime from,
            LocalDateTime to,
            Pageable pageable
    );

    /**
     * Find recent items added
     */
    Page<ProjectItem> findByProjectIdAndAddedAtAfter(Long projectId, LocalDateTime after, Pageable pageable);

    /**
     * Mark as read
     */
    @Modifying
    @Query("UPDATE ProjectItem i SET i.isRead = true WHERE i.id = :id")
    void markAsRead(@Param("id") Long id);

    /**
     * Mark all as read for project
     */
    @Modifying
    @Query("UPDATE ProjectItem i SET i.isRead = true WHERE i.projectId = :projectId")
    void markAllAsRead(@Param("projectId") Long projectId);

    /**
     * Toggle bookmark
     */
    @Modifying
    @Query("UPDATE ProjectItem i SET i.bookmarked = NOT i.bookmarked WHERE i.id = :id")
    void toggleBookmark(@Param("id") Long id);

    /**
     * Update importance
     */
    @Modifying
    @Query("UPDATE ProjectItem i SET i.importance = :importance WHERE i.id = :id")
    void updateImportance(@Param("id") Long id, @Param("importance") Integer importance);

    /**
     * Count by project
     */
    long countByProjectId(Long projectId);

    /**
     * Count by project and type
     */
    long countByProjectIdAndItemType(Long projectId, ProjectItem.ItemType itemType);

    /**
     * Count unread by project
     */
    long countByProjectIdAndIsReadFalse(Long projectId);

    /**
     * Get distinct categories for project
     */
    @Query("SELECT DISTINCT i.category FROM ProjectItem i WHERE i.projectId = :projectId AND i.category IS NOT NULL")
    List<String> findDistinctCategories(@Param("projectId") Long projectId);

    /**
     * Get item count by date
     */
    @Query("""
            SELECT CAST(i.addedAt AS date) as date, COUNT(i) as count
            FROM ProjectItem i
            WHERE i.projectId = :projectId
            AND i.addedAt > :after
            GROUP BY CAST(i.addedAt AS date)
            ORDER BY date DESC
            """)
    List<ItemCountByDate> getItemCountByDate(@Param("projectId") Long projectId, @Param("after") LocalDateTime after);

    /**
     * Delete by project
     */
    void deleteByProjectId(Long projectId);

    interface ItemCountByDate {
        java.sql.Date getDate();
        Long getCount();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/repository/ProjectMemberRepository.java

```java
package com.newsinsight.collector.repository;

import com.newsinsight.collector.entity.project.ProjectMember;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Modifying;
import org.springframework.data.jpa.repository.Query;
import org.springframework.data.repository.query.Param;
import org.springframework.stereotype.Repository;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Optional;

/**
 * Repository for ProjectMember entity.
 */
@Repository
public interface ProjectMemberRepository extends JpaRepository<ProjectMember, Long> {

    /**
     * Find by project ID
     */
    List<ProjectMember> findByProjectIdOrderByJoinedAtDesc(Long projectId);

    /**
     * Find by project ID and status
     */
    List<ProjectMember> findByProjectIdAndStatus(Long projectId, ProjectMember.MemberStatus status);

    /**
     * Find by user ID
     */
    List<ProjectMember> findByUserIdOrderByJoinedAtDesc(String userId);

    /**
     * Find by user ID and status
     */
    List<ProjectMember> findByUserIdAndStatus(String userId, ProjectMember.MemberStatus status);

    /**
     * Find specific membership
     */
    Optional<ProjectMember> findByProjectIdAndUserId(Long projectId, String userId);

    /**
     * Find by invite token
     */
    Optional<ProjectMember> findByInviteToken(String inviteToken);

    /**
     * Find pending invitations for user
     */
    List<ProjectMember> findByUserIdAndStatusOrderByJoinedAtDesc(String userId, ProjectMember.MemberStatus status);

    /**
     * Find expired invitations
     */
    @Query("""
            SELECT m FROM ProjectMember m 
            WHERE m.status = 'PENDING' 
            AND m.inviteExpiresAt < :now
            """)
    List<ProjectMember> findExpiredInvitations(@Param("now") LocalDateTime now);

    /**
     * Find members by role
     */
    List<ProjectMember> findByProjectIdAndRole(Long projectId, ProjectMember.MemberRole role);

    /**
     * Find projects where user is a member
     */
    @Query("""
            SELECT m.projectId FROM ProjectMember m 
            WHERE m.userId = :userId 
            AND m.status = 'ACTIVE'
            """)
    List<Long> findProjectIdsByUser(@Param("userId") String userId);

    /**
     * Check if user is member of project
     */
    boolean existsByProjectIdAndUserIdAndStatus(Long projectId, String userId, ProjectMember.MemberStatus status);

    /**
     * Update role
     */
    @Modifying
    @Query("UPDATE ProjectMember m SET m.role = :role WHERE m.id = :id")
    void updateRole(@Param("id") Long id, @Param("role") ProjectMember.MemberRole role);

    /**
     * Update status
     */
    @Modifying
    @Query("UPDATE ProjectMember m SET m.status = :status WHERE m.id = :id")
    void updateStatus(@Param("id") Long id, @Param("status") ProjectMember.MemberStatus status);

    /**
     * Update last active
     */
    @Modifying
    @Query("UPDATE ProjectMember m SET m.lastActiveAt = :activeAt WHERE m.projectId = :projectId AND m.userId = :userId")
    void updateLastActive(
            @Param("projectId") Long projectId,
            @Param("userId") String userId,
            @Param("activeAt") LocalDateTime activeAt
    );

    /**
     * Count members by project
     */
    long countByProjectIdAndStatus(Long projectId, ProjectMember.MemberStatus status);

    /**
     * Delete by project
     */
    void deleteByProjectId(Long projectId);

    /**
     * Delete expired invitations
     */
    @Modifying
    @Query("DELETE FROM ProjectMember m WHERE m.status = 'PENDING' AND m.inviteExpiresAt < :now")
    void deleteExpiredInvitations(@Param("now") LocalDateTime now);
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/repository/ProjectNotificationRepository.java

```java
package com.newsinsight.collector.repository;

import com.newsinsight.collector.entity.project.ProjectNotification;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Modifying;
import org.springframework.data.jpa.repository.Query;
import org.springframework.data.repository.query.Param;
import org.springframework.stereotype.Repository;

import java.time.LocalDateTime;
import java.util.List;

/**
 * Repository for ProjectNotification entity.
 */
@Repository
public interface ProjectNotificationRepository extends JpaRepository<ProjectNotification, Long> {

    /**
     * Find by user ID ordered by created at desc
     */
    Page<ProjectNotification> findByUserIdOrderByCreatedAtDesc(String userId, Pageable pageable);

    /**
     * Find unread notifications by user ID
     */
    List<ProjectNotification> findByUserIdAndIsReadFalseOrderByCreatedAtDesc(String userId);

    /**
     * Mark notification as read
     */
    @Modifying
    @Query("UPDATE ProjectNotification n SET n.isRead = true WHERE n.id = :id")
    void markAsRead(@Param("id") Long id);

    /**
     * Mark all notifications as read for a user
     */
    @Modifying
    @Query("UPDATE ProjectNotification n SET n.isRead = true WHERE n.userId = :userId AND n.isRead = false")
    void markAllAsRead(@Param("userId") String userId);

    /**
     * Find by project ID
     */
    Page<ProjectNotification> findByProjectIdOrderByCreatedAtDesc(Long projectId, Pageable pageable);

    /**
     * Find by notification type
     */
    Page<ProjectNotification> findByProjectIdAndNotificationType(
            Long projectId,
            ProjectNotification.NotificationType notificationType,
            Pageable pageable
    );

    /**
     * Find by priority
     */
    Page<ProjectNotification> findByProjectIdAndPriority(
            Long projectId,
            ProjectNotification.NotificationPriority priority,
            Pageable pageable
    );

    /**
     * Find unread notifications for user
     */
    @Query(value = """
            SELECT * FROM project_notifications 
            WHERE project_id = :projectId 
            AND recipients @> :userIdJson::jsonb
            AND (read_by IS NULL OR NOT read_by @> :userIdJson::jsonb)
            AND (expires_at IS NULL OR expires_at > :now)
            ORDER BY created_at DESC
            """, nativeQuery = true)
    List<ProjectNotification> findUnreadForUser(
            @Param("projectId") Long projectId,
            @Param("userIdJson") String userIdJson,
            @Param("now") LocalDateTime now
    );

    /**
     * Find notifications for user across all projects
     */
    @Query(value = """
            SELECT * FROM project_notifications 
            WHERE recipients @> :userIdJson::jsonb
            AND (expires_at IS NULL OR expires_at > :now)
            ORDER BY created_at DESC
            LIMIT :limit
            """, nativeQuery = true)
    List<ProjectNotification> findForUser(
            @Param("userIdJson") String userIdJson,
            @Param("now") LocalDateTime now,
            @Param("limit") int limit
    );

    /**
     * Find unsent notifications
     */
    @Query("SELECT n FROM ProjectNotification n WHERE n.sentAt IS NULL AND n.dismissed = false ORDER BY n.createdAt")
    List<ProjectNotification> findUnsent(Pageable pageable);

    /**
     * Find expired notifications
     */
    @Query("SELECT n FROM ProjectNotification n WHERE n.expiresAt < :now")
    List<ProjectNotification> findExpired(@Param("now") LocalDateTime now);

    /**
     * Mark as sent
     */
    @Modifying
    @Query("UPDATE ProjectNotification n SET n.sentAt = :sentAt WHERE n.id = :id")
    void markAsSent(@Param("id") Long id, @Param("sentAt") LocalDateTime sentAt);

    /**
     * Dismiss notification
     */
    @Modifying
    @Query("UPDATE ProjectNotification n SET n.dismissed = true WHERE n.id = :id")
    void dismiss(@Param("id") Long id);

    /**
     * Count unread for user in project
     */
    @Query(value = """
            SELECT COUNT(*) FROM project_notifications 
            WHERE project_id = :projectId 
            AND recipients @> :userIdJson::jsonb
            AND (read_by IS NULL OR NOT read_by @> :userIdJson::jsonb)
            AND (expires_at IS NULL OR expires_at > :now)
            """, nativeQuery = true)
    long countUnreadForUser(
            @Param("projectId") Long projectId,
            @Param("userIdJson") String userIdJson,
            @Param("now") LocalDateTime now
    );

    /**
     * Delete old notifications
     */
    @Modifying
    @Query("DELETE FROM ProjectNotification n WHERE n.createdAt < :before")
    void deleteOldNotifications(@Param("before") LocalDateTime before);

    /**
     * Delete expired notifications
     */
    @Modifying
    @Query("DELETE FROM ProjectNotification n WHERE n.expiresAt < :now")
    void deleteExpiredNotifications(@Param("now") LocalDateTime now);

    /**
     * Delete by project
     */
    void deleteByProjectId(Long projectId);
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/repository/ProjectRepository.java

```java
package com.newsinsight.collector.repository;

import com.newsinsight.collector.entity.project.Project;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Modifying;
import org.springframework.data.jpa.repository.Query;
import org.springframework.data.repository.query.Param;
import org.springframework.stereotype.Repository;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Optional;

/**
 * Repository for Project entity.
 */
@Repository
public interface ProjectRepository extends JpaRepository<Project, Long> {

    /**
     * Find by owner ID
     */
    Page<Project> findByOwnerIdOrderByLastActivityAtDesc(String ownerId, Pageable pageable);

    /**
     * Find by owner ID and status
     */
    Page<Project> findByOwnerIdAndStatus(String ownerId, Project.ProjectStatus status, Pageable pageable);

    /**
     * Find by owner ID and category
     */
    Page<Project> findByOwnerIdAndCategory(String ownerId, Project.ProjectCategory category, Pageable pageable);

    /**
     * Find default project for user
     */
    Optional<Project> findByOwnerIdAndIsDefaultTrue(String ownerId);

    /**
     * Find active projects with auto-collect enabled
     */
    @Query(value = "SELECT * FROM projects p WHERE p.status = 'ACTIVE' AND p.settings IS NOT NULL AND p.settings->>'autoCollect' = 'true'", nativeQuery = true)
    List<Project> findAutoCollectEnabledProjects();

    /**
     * Find projects needing collection (based on interval)
     */
    @Query(value = """
            SELECT * FROM projects p 
            WHERE p.status = 'ACTIVE' 
            AND p.settings->>'autoCollect' = 'true'
            AND (
                p.last_collected_at IS NULL 
                OR (
                    (p.settings->>'collectInterval' = 'hourly' AND p.last_collected_at < :hourAgo)
                    OR (p.settings->>'collectInterval' = 'daily' AND p.last_collected_at < :dayAgo)
                    OR (p.settings->>'collectInterval' = 'weekly' AND p.last_collected_at < :weekAgo)
                )
            )
            """, nativeQuery = true)
    List<Project> findProjectsNeedingCollection(
            @Param("hourAgo") LocalDateTime hourAgo,
            @Param("dayAgo") LocalDateTime dayAgo,
            @Param("weekAgo") LocalDateTime weekAgo
    );

    /**
     * Find public projects
     */
    Page<Project> findByVisibility(Project.ProjectVisibility visibility, Pageable pageable);

    /**
     * Search projects by name
     */
    @Query("SELECT p FROM Project p WHERE LOWER(p.name) LIKE LOWER(CONCAT('%', :name, '%'))")
    Page<Project> searchByName(@Param("name") String name, Pageable pageable);

    /**
     * Search projects by keyword
     */
    @Query(value = """
            SELECT * FROM projects p 
            WHERE p.keywords @> :keyword::jsonb
            """, nativeQuery = true)
    List<Project> findByKeyword(@Param("keyword") String keywordJson);

    /**
     * Update last activity
     */
    @Modifying
    @Query("UPDATE Project p SET p.lastActivityAt = :activityAt WHERE p.id = :id")
    void updateLastActivity(@Param("id") Long id, @Param("activityAt") LocalDateTime activityAt);

    /**
     * Update last collected
     */
    @Modifying
    @Query("UPDATE Project p SET p.lastCollectedAt = :collectedAt WHERE p.id = :id")
    void updateLastCollected(@Param("id") Long id, @Param("collectedAt") LocalDateTime collectedAt);

    /**
     * Update status
     */
    @Modifying
    @Query("UPDATE Project p SET p.status = :status, p.updatedAt = :updatedAt WHERE p.id = :id")
    void updateStatus(
            @Param("id") Long id,
            @Param("status") Project.ProjectStatus status,
            @Param("updatedAt") LocalDateTime updatedAt
    );

    /**
     * Count by owner
     */
    long countByOwnerId(String ownerId);

    /**
     * Count by status
     */
    long countByStatus(Project.ProjectStatus status);

    /**
     * Count active projects by owner
     */
    long countByOwnerIdAndStatus(String ownerId, Project.ProjectStatus status);

    /**
     * Find inactive projects (for cleanup suggestions)
     */
    @Query("SELECT p FROM Project p WHERE p.status = com.newsinsight.collector.entity.project.Project$ProjectStatus.ACTIVE AND p.lastActivityAt < :inactiveSince ORDER BY p.lastActivityAt ASC")
    List<Project> findInactiveProjects(@Param("inactiveSince") LocalDateTime inactiveSince, Pageable pageable);
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/repository/SearchFeedbackRepository.java

```java
package com.newsinsight.collector.repository;

import com.newsinsight.collector.entity.feedback.SearchFeedback;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Query;
import org.springframework.data.repository.query.Param;
import org.springframework.stereotype.Repository;

import java.time.LocalDateTime;
import java.util.List;

/**
 * Repository for SearchFeedback entity.
 * Manages user feedback on search results.
 */
@Repository
public interface SearchFeedbackRepository extends JpaRepository<SearchFeedback, Long> {

    /**
     * Find by search history ID
     */
    List<SearchFeedback> findBySearchHistoryIdOrderByCreatedAtDesc(Long searchHistoryId);

    /**
     * Find by user ID
     */
    Page<SearchFeedback> findByUserIdOrderByCreatedAtDesc(String userId, Pageable pageable);

    /**
     * Find by feedback type
     */
    Page<SearchFeedback> findByFeedbackType(SearchFeedback.FeedbackType feedbackType, Pageable pageable);

    /**
     * Find unreviewed feedback
     */
    Page<SearchFeedback> findByReviewedFalseOrderByCreatedAtDesc(Pageable pageable);

    /**
     * Find feedback with low ratings
     */
    @Query("SELECT f FROM SearchFeedback f WHERE f.rating <= :maxRating ORDER BY f.createdAt DESC")
    Page<SearchFeedback> findLowRatedFeedback(@Param("maxRating") int maxRating, Pageable pageable);

    /**
     * Find positive feedback (thumbs up)
     */
    Page<SearchFeedback> findByThumbsUpTrueOrderByCreatedAtDesc(Pageable pageable);

    /**
     * Find negative feedback (thumbs down)
     */
    Page<SearchFeedback> findByThumbsUpFalseOrderByCreatedAtDesc(Pageable pageable);

    /**
     * Count feedback by search history
     */
    long countBySearchHistoryId(Long searchHistoryId);

    /**
     * Average rating by search history
     */
    @Query("SELECT AVG(f.rating) FROM SearchFeedback f WHERE f.searchHistoryId = :searchHistoryId AND f.rating IS NOT NULL")
    Double getAverageRatingBySearchHistory(@Param("searchHistoryId") Long searchHistoryId);

    /**
     * Get overall feedback statistics
     */
    @Query("""
            SELECT 
                COUNT(f) as totalCount,
                AVG(f.rating) as avgRating,
                AVG(f.usefulnessRating) as avgUsefulness,
                AVG(f.accuracyRating) as avgAccuracy,
                AVG(f.relevanceRating) as avgRelevance,
                SUM(CASE WHEN f.thumbsUp = true THEN 1 ELSE 0 END) as thumbsUpCount,
                SUM(CASE WHEN f.thumbsUp = false THEN 1 ELSE 0 END) as thumbsDownCount
            FROM SearchFeedback f
            WHERE f.createdAt > :after
            """)
    FeedbackStats getOverallStats(@Param("after") LocalDateTime after);

    /**
     * Get feedback stats by type
     */
    @Query("""
            SELECT f.feedbackType as feedbackType, COUNT(f) as count, AVG(f.rating) as avgRating
            FROM SearchFeedback f
            WHERE f.createdAt > :after
            GROUP BY f.feedbackType
            """)
    List<FeedbackTypeStats> getStatsByType(@Param("after") LocalDateTime after);

    /**
     * Find feedback not used for training
     */
    @Query("""
            SELECT f FROM SearchFeedback f 
            WHERE f.usedForTraining = false 
            AND f.reviewed = true
            ORDER BY f.createdAt
            """)
    List<SearchFeedback> findUnusedForTraining(Pageable pageable);

    interface FeedbackStats {
        Long getTotalCount();
        Double getAvgRating();
        Double getAvgUsefulness();
        Double getAvgAccuracy();
        Double getAvgRelevance();
        Long getThumbsUpCount();
        Long getThumbsDownCount();
    }

    interface FeedbackTypeStats {
        SearchFeedback.FeedbackType getFeedbackType();
        Long getCount();
        Double getAvgRating();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/repository/SearchHistoryRepository.java

```java
package com.newsinsight.collector.repository;

import com.newsinsight.collector.entity.search.SearchHistory;
import com.newsinsight.collector.entity.search.SearchType;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Query;
import org.springframework.data.repository.query.Param;
import org.springframework.stereotype.Repository;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Optional;

/**
 * Repository for SearchHistory entity.
 * Provides search history persistence and query operations.
 */
@Repository
public interface SearchHistoryRepository extends JpaRepository<SearchHistory, Long> {

    /**
     * Find by external ID (e.g., jobId)
     */
    Optional<SearchHistory> findByExternalId(String externalId);

    /**
     * Find by external ID containing (for jobId + suffix patterns)
     */
    List<SearchHistory> findByExternalIdContaining(String externalIdPart);

    /**
     * Find all searches by type
     */
    Page<SearchHistory> findBySearchType(SearchType searchType, Pageable pageable);

    /**
     * Find all searches by user
     */
    Page<SearchHistory> findByUserId(String userId, Pageable pageable);

    /**
     * Find searches by user and type
     */
    Page<SearchHistory> findByUserIdAndSearchType(String userId, SearchType searchType, Pageable pageable);

    /**
     * Find bookmarked searches
     */
    Page<SearchHistory> findByBookmarkedTrue(Pageable pageable);

    /**
     * Find bookmarked searches by user
     */
    Page<SearchHistory> findByUserIdAndBookmarkedTrue(String userId, Pageable pageable);

    /**
     * Find derived searches from a parent
     */
    List<SearchHistory> findByParentSearchIdOrderByCreatedAtDesc(Long parentSearchId);

    /**
     * Find searches by session
     */
    List<SearchHistory> findBySessionIdOrderByCreatedAtDesc(String sessionId);
    
    /**
     * Find searches by userId and sessionId (for anonymous user isolation)
     * This ensures that each anonymous session only sees their own data
     */
    Page<SearchHistory> findByUserIdAndSessionId(String userId, String sessionId, Pageable pageable);
    
    /**
     * Find searches by userId OR sessionId (fallback for migration)
     */
    @Query("SELECT sh FROM SearchHistory sh WHERE sh.userId = :userId OR sh.sessionId = :sessionId ORDER BY sh.createdAt DESC")
    Page<SearchHistory> findByUserIdOrSessionId(
            @Param("userId") String userId,
            @Param("sessionId") String sessionId,
            Pageable pageable
    );

    /**
     * Search by query text (case-insensitive, partial match)
     */
    @Query("SELECT sh FROM SearchHistory sh WHERE LOWER(sh.query) LIKE LOWER(CONCAT('%', :query, '%'))")
    Page<SearchHistory> searchByQuery(@Param("query") String query, Pageable pageable);

    /**
     * Search by query text and type
     */
    @Query("SELECT sh FROM SearchHistory sh WHERE LOWER(sh.query) LIKE LOWER(CONCAT('%', :query, '%')) AND sh.searchType = :searchType")
    Page<SearchHistory> searchByQueryAndType(
            @Param("query") String query,
            @Param("searchType") SearchType searchType,
            Pageable pageable
    );

    /**
     * Find recent searches within time range
     */
    Page<SearchHistory> findByCreatedAtAfter(LocalDateTime after, Pageable pageable);

    /**
     * Find recent searches by user within time range
     */
    Page<SearchHistory> findByUserIdAndCreatedAtAfter(String userId, LocalDateTime after, Pageable pageable);

    /**
     * Get search count by type
     */
    long countBySearchType(SearchType searchType);

    /**
     * Get search count by user
     */
    long countByUserId(String userId);

    /**
     * Find searches with specific tag
     */
    @Query(value = "SELECT * FROM search_history WHERE tags @> :tag::jsonb", nativeQuery = true)
    List<SearchHistory> findByTag(@Param("tag") String tagJson);

    /**
     * Delete old searches (for cleanup)
     */
    @Query("DELETE FROM SearchHistory sh WHERE sh.createdAt < :before AND sh.bookmarked = false")
    void deleteOldSearches(@Param("before") LocalDateTime before);

    /**
     * Get unique discovered URLs from recent searches
     */
    @Query(value = """
            SELECT DISTINCT jsonb_array_elements_text(discovered_urls) as url
            FROM search_history 
            WHERE discovered_urls IS NOT NULL 
            AND created_at > :after
            LIMIT :limit
            """, nativeQuery = true)
    List<String> findRecentDiscoveredUrls(@Param("after") LocalDateTime after, @Param("limit") int limit);

    /**
     * Find similar searches (by query similarity)
     */
    @Query(value = """
            SELECT * FROM search_history 
            WHERE search_type = :searchType
            AND similarity(query, :query) > 0.3
            ORDER BY similarity(query, :query) DESC
            LIMIT :limit
            """, nativeQuery = true)
    List<SearchHistory> findSimilarSearches(
            @Param("query") String query,
            @Param("searchType") String searchType,
            @Param("limit") int limit
    );

    /**
     * Get search statistics summary
     */
    @Query("""
            SELECT sh.searchType as searchType, COUNT(sh) as count, AVG(sh.resultCount) as avgResults
            FROM SearchHistory sh
            WHERE sh.createdAt > :after
            GROUP BY sh.searchType
            """)
    List<SearchStatsSummary> getSearchStatsSummary(@Param("after") LocalDateTime after);

    /**
     * Projection for search statistics
     */
    interface SearchStatsSummary {
        SearchType getSearchType();
        Long getCount();
        Double getAvgResults();
    }

    // ============ New methods for Continue Work feature ============

    /**
     * Find searches that need continuation (for "Continue Work" feature)
     * Includes: IN_PROGRESS, PARTIAL, FAILED, DRAFT, or COMPLETED but not viewed
     */
    @Query("""
            SELECT sh FROM SearchHistory sh
            WHERE (sh.userId = :userId OR sh.sessionId = :sessionId)
            AND (
                sh.completionStatus IN ('DRAFT', 'IN_PROGRESS', 'PARTIAL', 'FAILED')
                OR (sh.completionStatus = 'COMPLETED' AND sh.viewed = false)
            )
            AND sh.bookmarked = false
            AND sh.reportGenerated = false
            ORDER BY 
                CASE sh.completionStatus 
                    WHEN 'IN_PROGRESS' THEN 1
                    WHEN 'FAILED' THEN 2
                    WHEN 'DRAFT' THEN 3
                    WHEN 'PARTIAL' THEN 4
                    ELSE 5
                END,
                sh.updatedAt DESC
            """)
    List<SearchHistory> findContinueWorkItems(
            @Param("userId") String userId,
            @Param("sessionId") String sessionId,
            Pageable pageable
    );

    /**
     * Find searches by completion status
     */
    Page<SearchHistory> findByCompletionStatus(
            SearchHistory.CompletionStatus completionStatus,
            Pageable pageable
    );

    /**
     * Find searches by user and completion status
     */
    Page<SearchHistory> findByUserIdAndCompletionStatus(
            String userId,
            SearchHistory.CompletionStatus completionStatus,
            Pageable pageable
    );

    /**
     * Find unviewed completed searches
     */
    @Query("SELECT sh FROM SearchHistory sh WHERE sh.completionStatus = 'COMPLETED' AND sh.viewed = false")
    Page<SearchHistory> findUnviewedCompleted(Pageable pageable);

    /**
     * Find searches by project ID
     */
    Page<SearchHistory> findByProjectId(Long projectId, Pageable pageable);

    /**
     * Find searches by project ID and type
     */
    Page<SearchHistory> findByProjectIdAndSearchType(Long projectId, SearchType searchType, Pageable pageable);

    /**
     * Count in-progress searches by user
     */
    @Query("SELECT COUNT(sh) FROM SearchHistory sh WHERE sh.userId = :userId AND sh.completionStatus = 'IN_PROGRESS'")
    long countInProgressByUser(@Param("userId") String userId);

    /**
     * Update viewed status
     */
    @Query("UPDATE SearchHistory sh SET sh.viewed = true, sh.viewedAt = :viewedAt WHERE sh.id = :id")
    void markAsViewed(@Param("id") Long id, @Param("viewedAt") LocalDateTime viewedAt);

    /**
     * Update completion status
     */
    @Query("UPDATE SearchHistory sh SET sh.completionStatus = :status, sh.updatedAt = :updatedAt WHERE sh.id = :id")
    void updateCompletionStatus(
            @Param("id") Long id,
            @Param("status") SearchHistory.CompletionStatus status,
            @Param("updatedAt") LocalDateTime updatedAt
    );

    /**
     * Find failed searches for retry
     */
    @Query("""
            SELECT sh FROM SearchHistory sh 
            WHERE sh.completionStatus = 'FAILED' 
            AND sh.createdAt > :after
            ORDER BY sh.createdAt DESC
            """)
    List<SearchHistory> findFailedSearches(@Param("after") LocalDateTime after, Pageable pageable);
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/repository/SearchTemplateRepository.java

```java
package com.newsinsight.collector.repository;

import com.newsinsight.collector.entity.search.SearchTemplate;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Modifying;
import org.springframework.data.jpa.repository.Query;
import org.springframework.data.repository.query.Param;
import org.springframework.stereotype.Repository;

import java.util.List;

/**
 * Repository for SearchTemplate entity.
 * Provides template persistence and query operations.
 */
@Repository
public interface SearchTemplateRepository extends JpaRepository<SearchTemplate, Long> {

    /**
     * Find all templates by user
     */
    Page<SearchTemplate> findByUserId(String userId, Pageable pageable);

    /**
     * Find all templates by user (list)
     */
    List<SearchTemplate> findByUserIdOrderByCreatedAtDesc(String userId);

    /**
     * Find templates by mode
     */
    Page<SearchTemplate> findByMode(String mode, Pageable pageable);

    /**
     * Find templates by user and mode
     */
    Page<SearchTemplate> findByUserIdAndMode(String userId, String mode, Pageable pageable);

    /**
     * Find favorite templates by user
     */
    List<SearchTemplate> findByUserIdAndFavoriteTrueOrderByLastUsedAtDesc(String userId);

    /**
     * Find all favorites
     */
    Page<SearchTemplate> findByFavoriteTrue(Pageable pageable);

    /**
     * Search templates by name (case-insensitive)
     */
    @Query("SELECT st FROM SearchTemplate st WHERE LOWER(st.name) LIKE LOWER(CONCAT('%', :name, '%'))")
    Page<SearchTemplate> searchByName(@Param("name") String name, Pageable pageable);

    /**
     * Search templates by name for a specific user
     */
    @Query("SELECT st FROM SearchTemplate st WHERE st.userId = :userId AND LOWER(st.name) LIKE LOWER(CONCAT('%', :name, '%'))")
    Page<SearchTemplate> searchByNameAndUserId(@Param("name") String name, @Param("userId") String userId, Pageable pageable);

    /**
     * Search templates by query text
     */
    @Query("SELECT st FROM SearchTemplate st WHERE LOWER(st.query) LIKE LOWER(CONCAT('%', :query, '%'))")
    Page<SearchTemplate> searchByQuery(@Param("query") String query, Pageable pageable);

    /**
     * Find most used templates
     */
    @Query("SELECT st FROM SearchTemplate st WHERE st.userId = :userId ORDER BY st.useCount DESC")
    List<SearchTemplate> findMostUsedByUser(@Param("userId") String userId, Pageable pageable);

    /**
     * Find recently used templates
     */
    @Query("SELECT st FROM SearchTemplate st WHERE st.userId = :userId AND st.lastUsedAt IS NOT NULL ORDER BY st.lastUsedAt DESC")
    List<SearchTemplate> findRecentlyUsedByUser(@Param("userId") String userId, Pageable pageable);

    /**
     * Find templates created from a specific search
     */
    List<SearchTemplate> findBySourceSearchId(Long sourceSearchId);

    /**
     * Count templates by user
     */
    long countByUserId(String userId);

    /**
     * Count templates by mode
     */
    long countByMode(String mode);

    /**
     * Increment use count
     */
    @Modifying
    @Query("UPDATE SearchTemplate st SET st.useCount = st.useCount + 1, st.lastUsedAt = CURRENT_TIMESTAMP WHERE st.id = :id")
    void incrementUseCount(@Param("id") Long id);

    /**
     * Check if template with name exists for user
     */
    boolean existsByUserIdAndName(String userId, String name);
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/repository/WorkspaceFileRepository.java

```java
package com.newsinsight.collector.repository;

import com.newsinsight.collector.entity.workspace.WorkspaceFile;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Modifying;
import org.springframework.data.jpa.repository.Query;
import org.springframework.data.repository.query.Param;
import org.springframework.stereotype.Repository;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Optional;

/**
 * Repository for WorkspaceFile entity.
 */
@Repository
public interface WorkspaceFileRepository extends JpaRepository<WorkspaceFile, Long> {

    /**
     * Find by file UUID
     */
    Optional<WorkspaceFile> findByFileUuid(String fileUuid);

    /**
     * Find by file UUID and active status
     */
    @Query("SELECT f FROM WorkspaceFile f WHERE f.fileUuid = :fileUuid AND f.status = 'ACTIVE'")
    Optional<WorkspaceFile> findActiveByFileUuid(@Param("fileUuid") String fileUuid);

    /**
     * Find files by session ID
     */
    Page<WorkspaceFile> findBySessionIdAndStatusOrderByCreatedAtDesc(
            String sessionId, 
            WorkspaceFile.FileStatus status, 
            Pageable pageable
    );

    /**
     * Find files by user ID
     */
    Page<WorkspaceFile> findByUserIdAndStatusOrderByCreatedAtDesc(
            String userId, 
            WorkspaceFile.FileStatus status, 
            Pageable pageable
    );

    /**
     * Find files by project ID
     */
    Page<WorkspaceFile> findByProjectIdAndStatusOrderByCreatedAtDesc(
            Long projectId, 
            WorkspaceFile.FileStatus status, 
            Pageable pageable
    );

    /**
     * Find files by session ID and file type
     */
    Page<WorkspaceFile> findBySessionIdAndFileTypeAndStatus(
            String sessionId,
            WorkspaceFile.FileType fileType,
            WorkspaceFile.FileStatus status,
            Pageable pageable
    );

    /**
     * Find files by user ID and file type
     */
    Page<WorkspaceFile> findByUserIdAndFileTypeAndStatus(
            String userId,
            WorkspaceFile.FileType fileType,
            WorkspaceFile.FileStatus status,
            Pageable pageable
    );

    /**
     * Search files by name for session
     */
    @Query("SELECT f FROM WorkspaceFile f WHERE f.sessionId = :sessionId " +
           "AND f.status = 'ACTIVE' AND LOWER(f.originalName) LIKE LOWER(CONCAT('%', :name, '%'))")
    Page<WorkspaceFile> searchByNameForSession(
            @Param("sessionId") String sessionId,
            @Param("name") String name,
            Pageable pageable
    );

    /**
     * Search files by name for user
     */
    @Query("SELECT f FROM WorkspaceFile f WHERE f.userId = :userId " +
           "AND f.status = 'ACTIVE' AND LOWER(f.originalName) LIKE LOWER(CONCAT('%', :name, '%'))")
    Page<WorkspaceFile> searchByNameForUser(
            @Param("userId") String userId,
            @Param("name") String name,
            Pageable pageable
    );

    /**
     * Find expired files
     */
    @Query("SELECT f FROM WorkspaceFile f WHERE f.status = 'ACTIVE' AND f.expiresAt IS NOT NULL AND f.expiresAt < :now")
    List<WorkspaceFile> findExpiredFiles(@Param("now") LocalDateTime now);

    /**
     * Find files pending deletion
     */
    List<WorkspaceFile> findByStatus(WorkspaceFile.FileStatus status);

    /**
     * Find old session files (for cleanup)
     */
    @Query("SELECT f FROM WorkspaceFile f WHERE f.sessionId IS NOT NULL AND f.userId IS NULL " +
           "AND f.createdAt < :threshold AND f.status = 'ACTIVE'")
    List<WorkspaceFile> findOldSessionFiles(@Param("threshold") LocalDateTime threshold);

    /**
     * Update file status
     */
    @Modifying
    @Query("UPDATE WorkspaceFile f SET f.status = :status, f.updatedAt = :now WHERE f.id = :id")
    void updateStatus(
            @Param("id") Long id,
            @Param("status") WorkspaceFile.FileStatus status,
            @Param("now") LocalDateTime now
    );

    /**
     * Increment download count
     */
    @Modifying
    @Query("UPDATE WorkspaceFile f SET f.downloadCount = f.downloadCount + 1, " +
           "f.lastAccessedAt = :now WHERE f.id = :id")
    void incrementDownloadCount(@Param("id") Long id, @Param("now") LocalDateTime now);

    /**
     * Mark files as deleted for session
     */
    @Modifying
    @Query("UPDATE WorkspaceFile f SET f.status = 'DELETED', f.updatedAt = :now WHERE f.sessionId = :sessionId")
    void markDeletedBySessionId(@Param("sessionId") String sessionId, @Param("now") LocalDateTime now);

    /**
     * Transfer files from session to user (when user logs in)
     */
    @Modifying
    @Query("UPDATE WorkspaceFile f SET f.userId = :userId, f.updatedAt = :now WHERE f.sessionId = :sessionId")
    void transferSessionFilesToUser(
            @Param("sessionId") String sessionId,
            @Param("userId") String userId,
            @Param("now") LocalDateTime now
    );

    /**
     * Count files by session
     */
    long countBySessionIdAndStatus(String sessionId, WorkspaceFile.FileStatus status);

    /**
     * Count files by user
     */
    long countByUserIdAndStatus(String userId, WorkspaceFile.FileStatus status);

    /**
     * Sum file sizes by session
     */
    @Query("SELECT COALESCE(SUM(f.fileSize), 0) FROM WorkspaceFile f WHERE f.sessionId = :sessionId AND f.status = 'ACTIVE'")
    Long sumFileSizeBySessionId(@Param("sessionId") String sessionId);

    /**
     * Sum file sizes by user
     */
    @Query("SELECT COALESCE(SUM(f.fileSize), 0) FROM WorkspaceFile f WHERE f.userId = :userId AND f.status = 'ACTIVE'")
    Long sumFileSizeByUserId(@Param("userId") String userId);

    /**
     * Find by stored name
     */
    Optional<WorkspaceFile> findByStoredName(String storedName);

    /**
     * Check if file exists with same checksum for deduplication
     */
    @Query("SELECT f FROM WorkspaceFile f WHERE f.checksum = :checksum AND f.status = 'ACTIVE' " +
           "AND (f.sessionId = :sessionId OR f.userId = :userId)")
    Optional<WorkspaceFile> findByChecksumAndOwner(
            @Param("checksum") String checksum,
            @Param("sessionId") String sessionId,
            @Param("userId") String userId
    );
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/scheduler/AutoCrawlScheduler.java

```java
package com.newsinsight.collector.scheduler;

import com.newsinsight.collector.entity.autocrawl.DiscoverySource;
import com.newsinsight.collector.service.autocrawl.AutoCrawlDiscoveryService;
import com.newsinsight.collector.service.autocrawl.CrawlQueueService;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;
import org.springframework.scheduling.annotation.Scheduled;
import org.springframework.stereotype.Component;

import java.util.Map;

/**
 * 자동 크롤링 스케줄러.
 * 
 * 크롤링 큐를 주기적으로 처리하고, URL 발견/정리 작업을 자동화합니다.
 * 백그라운드에서 지속적으로 실행되어 실시간 크롤링을 지원합니다.
 */
@Component
@RequiredArgsConstructor
@Slf4j
@ConditionalOnProperty(name = "autocrawl.enabled", havingValue = "true", matchIfMissing = false)
public class AutoCrawlScheduler {

    private final CrawlQueueService crawlQueueService;
    private final AutoCrawlDiscoveryService autoCrawlDiscoveryService;

    @Value("${autocrawl.batch-size:10}")
    private int batchSize;

    @Value("${autocrawl.cleanup-days:7}")
    private int cleanupDays;

    @Value("${autocrawl.expire-pending-days:7}")
    private int expirePendingDays;

    // ========================================
    // 크롤링 큐 처리
    // ========================================

    /**
     * 크롤링 큐 처리 (30초마다)
     * 대기 중인 대상을 우선순위에 따라 크롤러로 분배
     */
    @Scheduled(fixedDelayString = "${autocrawl.queue-interval-ms:30000}")
    public void processQueue() {
        try {
            int dispatched = crawlQueueService.processQueue(batchSize);
            if (dispatched > 0) {
                log.info("[AutoCrawl] Dispatched {} targets for crawling", dispatched);
            }
        } catch (Exception e) {
            log.error("[AutoCrawl] Error processing queue: {}", e.getMessage(), e);
        }
    }

    /**
     * 멈춘 작업 복구 (5분마다)
     * IN_PROGRESS 상태로 오래 방치된 대상을 PENDING으로 복구
     */
    @Scheduled(fixedDelayString = "${autocrawl.recovery-interval-ms:300000}")
    public void recoverStuckTargets() {
        try {
            int recovered = crawlQueueService.recoverStuckTargets();
            if (recovered > 0) {
                log.warn("[AutoCrawl] Recovered {} stuck targets", recovered);
            }
        } catch (Exception e) {
            log.error("[AutoCrawl] Error recovering stuck targets: {}", e.getMessage(), e);
        }
    }

    // ========================================
    // 정리 작업
    // ========================================

    /**
     * 오래된 완료/실패 대상 정리 (매일 새벽 3시)
     */
    @Scheduled(cron = "${autocrawl.cleanup-cron:0 0 3 * * *}")
    public void cleanupOldTargets() {
        try {
            log.info("[AutoCrawl] Starting daily cleanup...");
            
            // 완료/실패 대상 정리
            int cleaned = crawlQueueService.cleanupOldTargets(cleanupDays);
            
            // 오래 대기 중인 대상 만료
            int expired = crawlQueueService.expireOldPendingTargets(expirePendingDays);
            
            log.info("[AutoCrawl] Daily cleanup complete: cleaned={}, expired={}", cleaned, expired);
        } catch (Exception e) {
            log.error("[AutoCrawl] Error during cleanup: {}", e.getMessage(), e);
        }
    }

    // ========================================
    // 통계 로깅
    // ========================================

    /**
     * 큐 상태 로깅 (10분마다)
     */
    @Scheduled(fixedDelayString = "${autocrawl.stats-interval-ms:600000}")
    public void logQueueStats() {
        try {
            CrawlQueueService.QueueStats stats = crawlQueueService.getQueueStats();
            
            log.info("[AutoCrawl Stats] pending={}, inProgress={}, completed={}, failed={}, " +
                     "sessionDispatched={}, sessionCompleted={}, sessionFailed={}",
                    stats.getPendingCount(),
                    stats.getInProgressCount(),
                    stats.getCompletedCount(),
                    stats.getFailedCount(),
                    stats.getTotalDispatched(),
                    stats.getTotalCompleted(),
                    stats.getTotalFailed());
            
            // 발견 출처별 통계
            Map<DiscoverySource, Long> discoveryStats = autoCrawlDiscoveryService.getDiscoveryStats();
            if (!discoveryStats.isEmpty()) {
                log.info("[AutoCrawl Stats] Discovery sources (last 7 days): {}", discoveryStats);
            }
            
        } catch (Exception e) {
            log.error("[AutoCrawl] Error logging stats: {}", e.getMessage(), e);
        }
    }

    // ========================================
    // 수동 제어용 메서드
    // ========================================

    /**
     * 수동으로 큐 처리 트리거
     */
    public int triggerQueueProcessing(int customBatchSize) {
        log.info("[AutoCrawl] Manual queue processing triggered with batch size: {}", customBatchSize);
        return crawlQueueService.processQueue(customBatchSize);
    }

    /**
     * 수동으로 정리 트리거
     */
    public void triggerCleanup() {
        log.info("[AutoCrawl] Manual cleanup triggered");
        cleanupOldTargets();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/scheduler/CollectionScheduler.java

```java
package com.newsinsight.collector.scheduler;

import com.newsinsight.collector.entity.CollectionJob;
import com.newsinsight.collector.entity.CollectionJob.JobStatus;
import com.newsinsight.collector.repository.CollectionJobRepository;
import com.newsinsight.collector.service.CollectionService;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.scheduling.annotation.Scheduled;
import org.springframework.stereotype.Component;

import java.util.List;

/**
 * 자동 크롤링 스케줄러.
 * 설정된 cron 주기에 따라 활성화된 모든 데이터 소스에 대해 자동으로 수집 작업을 시작합니다.
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class CollectionScheduler {

    private final CollectionService collectionService;
    private final CollectionJobRepository collectionJobRepository;

    @Value("${collector.scheduling.enabled:true}")
    private boolean schedulingEnabled;

    @Value("${collector.scheduling.skip-if-running:true}")
    private boolean skipIfRunning;

    /**
     * 정기 수집 스케줄러.
     * 기본값: 매 시 정각 (0 0 * * * ?)
     * 환경변수 COLLECTION_CRON으로 조정 가능.
     */
    @Scheduled(cron = "${collector.scheduling.cron:0 0 * * * ?}")
    public void scheduledCollection() {
        if (!schedulingEnabled) {
            log.debug("Scheduled collection is disabled");
            return;
        }

        // 이미 실행 중인 작업이 있으면 스킵 (중복 실행 방지)
        if (skipIfRunning && hasRunningJobs()) {
            log.info("Skipping scheduled collection: jobs already running");
            return;
        }

        log.info("Starting scheduled collection for all active sources");
        try {
            List<CollectionJob> jobs = collectionService.startCollectionForAllActive();
            log.info("Scheduled collection started {} jobs", jobs.size());
            
            if (jobs.isEmpty()) {
                log.warn("No active data sources found for scheduled collection");
            }
        } catch (Exception e) {
            log.error("Scheduled collection failed: {}", e.getMessage(), e);
        }
    }

    /**
     * 실행 중인 수집 작업이 있는지 확인.
     */
    private boolean hasRunningJobs() {
        long runningCount = collectionJobRepository.countByStatus(JobStatus.RUNNING);
        long pendingCount = collectionJobRepository.countByStatus(JobStatus.PENDING);
        return (runningCount + pendingCount) > 0;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/scheduler/MlAddonHealthScheduler.java

```java
package com.newsinsight.collector.scheduler;

import com.newsinsight.collector.entity.addon.AddonHealthStatus;
import com.newsinsight.collector.entity.addon.MlAddon;
import com.newsinsight.collector.repository.MlAddonRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;
import org.springframework.http.ResponseEntity;
import org.springframework.scheduling.annotation.Scheduled;
import org.springframework.stereotype.Component;
import org.springframework.transaction.annotation.Transactional;
import org.springframework.web.client.RestClientException;
import org.springframework.web.client.RestTemplate;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.atomic.AtomicInteger;

/**
 * ML Addon Health Check Scheduler.
 * 
 * Periodically checks the health of registered ML addons and updates their status.
 * Supports automatic disabling of unhealthy addons after consecutive failures.
 */
@Component
@RequiredArgsConstructor
@Slf4j
@ConditionalOnProperty(name = "ml.addon.health-check.enabled", havingValue = "true", matchIfMissing = true)
public class MlAddonHealthScheduler {

    private final MlAddonRepository mlAddonRepository;
    private final RestTemplate restTemplate;

    @Value("${ml.addon.health-check.timeout-ms:5000}")
    private int healthCheckTimeoutMs;

    @Value("${ml.addon.health-check.max-consecutive-failures:3}")
    private int maxConsecutiveFailures;

    @Value("${ml.addon.health-check.auto-disable:false}")
    private boolean autoDisableUnhealthy;

    @Value("${ml.addon.health-check.interval-minutes:5}")
    private int healthCheckIntervalMinutes;

    // Track consecutive failures per addon
    private final Map<Long, AtomicInteger> failureCounters = new ConcurrentHashMap<>();

    /**
     * Periodic health check for all registered ML addons.
     * Default: runs every 5 minutes.
     */
    @Scheduled(fixedDelayString = "${ml.addon.health-check.interval-ms:300000}")
    @Transactional
    public void checkAddonHealth() {
        log.debug("[MlAddonHealth] Starting health check cycle...");
        
        // Get addons that need health check
        LocalDateTime cutoff = LocalDateTime.now().minusMinutes(healthCheckIntervalMinutes);
        List<MlAddon> addonsToCheck = mlAddonRepository.findAddonsNeedingHealthCheck(cutoff);
        
        if (addonsToCheck.isEmpty()) {
            log.debug("[MlAddonHealth] No addons need health check at this time");
            return;
        }
        
        log.info("[MlAddonHealth] Checking {} addons...", addonsToCheck.size());
        
        int healthy = 0;
        int unhealthy = 0;
        int degraded = 0;
        
        for (MlAddon addon : addonsToCheck) {
            try {
                AddonHealthStatus status = checkSingleAddon(addon);
                updateAddonHealth(addon, status);
                
                switch (status) {
                    case HEALTHY -> healthy++;
                    case UNHEALTHY -> unhealthy++;
                    case DEGRADED -> degraded++;
                    default -> {} // UNKNOWN - shouldn't happen after check
                }
            } catch (Exception e) {
                log.error("[MlAddonHealth] Error checking addon {}: {}", addon.getAddonKey(), e.getMessage());
                updateAddonHealth(addon, AddonHealthStatus.UNHEALTHY);
                unhealthy++;
            }
        }
        
        log.info("[MlAddonHealth] Health check complete: healthy={}, degraded={}, unhealthy={}", 
                healthy, degraded, unhealthy);
    }

    /**
     * Check health of a single addon.
     */
    private AddonHealthStatus checkSingleAddon(MlAddon addon) {
        if (addon.getHealthCheckUrl() == null || addon.getHealthCheckUrl().isBlank()) {
            log.debug("[MlAddonHealth] Addon {} has no health check URL, marking as UNKNOWN", addon.getAddonKey());
            return AddonHealthStatus.UNKNOWN;
        }
        
        try {
            long startTime = System.currentTimeMillis();
            ResponseEntity<Map> response = restTemplate.getForEntity(
                addon.getHealthCheckUrl(), 
                Map.class
            );
            long latencyMs = System.currentTimeMillis() - startTime;
            
            if (response.getStatusCode().is2xxSuccessful()) {
                // Clear failure counter on success
                failureCounters.remove(addon.getId());
                
                // Check if response indicates healthy status
                Map<?, ?> body = response.getBody();
                if (body != null) {
                    Object status = body.get("status");
                    if ("healthy".equalsIgnoreCase(String.valueOf(status)) ||
                        "ok".equalsIgnoreCase(String.valueOf(status))) {
                        
                        // Check latency for degraded status
                        if (latencyMs > addon.getTimeoutMs() / 2) {
                            log.warn("[MlAddonHealth] Addon {} responding slowly: {}ms", 
                                    addon.getAddonKey(), latencyMs);
                            return AddonHealthStatus.DEGRADED;
                        }
                        
                        log.debug("[MlAddonHealth] Addon {} is healthy ({}ms)", addon.getAddonKey(), latencyMs);
                        return AddonHealthStatus.HEALTHY;
                    }
                }
                
                // Response OK but status field not healthy
                return AddonHealthStatus.DEGRADED;
            } else {
                // Non-2xx response
                incrementFailureCounter(addon);
                return AddonHealthStatus.UNHEALTHY;
            }
            
        } catch (RestClientException e) {
            log.warn("[MlAddonHealth] Failed to reach addon {}: {}", addon.getAddonKey(), e.getMessage());
            incrementFailureCounter(addon);
            return AddonHealthStatus.UNHEALTHY;
        }
    }

    /**
     * Update addon health status in database.
     */
    @Transactional
    protected void updateAddonHealth(MlAddon addon, AddonHealthStatus status) {
        mlAddonRepository.updateHealthStatus(addon.getId(), status, LocalDateTime.now());
        
        // Check if we should auto-disable
        if (autoDisableUnhealthy && status == AddonHealthStatus.UNHEALTHY) {
            AtomicInteger counter = failureCounters.get(addon.getId());
            if (counter != null && counter.get() >= maxConsecutiveFailures) {
                log.warn("[MlAddonHealth] Auto-disabling addon {} after {} consecutive failures", 
                        addon.getAddonKey(), counter.get());
                mlAddonRepository.disableAddon(addon.getId());
                failureCounters.remove(addon.getId());
            }
        }
    }

    /**
     * Increment failure counter for an addon.
     */
    private void incrementFailureCounter(MlAddon addon) {
        failureCounters.computeIfAbsent(addon.getId(), k -> new AtomicInteger(0)).incrementAndGet();
    }

    /**
     * Manually trigger health check for a specific addon.
     */
    @Transactional
    public AddonHealthStatus checkAddonHealthNow(Long addonId) {
        MlAddon addon = mlAddonRepository.findById(addonId)
                .orElseThrow(() -> new IllegalArgumentException("Addon not found: " + addonId));
        
        AddonHealthStatus status = checkSingleAddon(addon);
        updateAddonHealth(addon, status);
        
        return status;
    }

    /**
     * Manually trigger health check for all addons.
     */
    @Transactional
    public Map<String, Integer> checkAllAddonsNow() {
        List<MlAddon> allAddons = mlAddonRepository.findByEnabledTrue();
        
        int healthy = 0;
        int unhealthy = 0;
        int degraded = 0;
        int unknown = 0;
        
        for (MlAddon addon : allAddons) {
            AddonHealthStatus status = checkSingleAddon(addon);
            updateAddonHealth(addon, status);
            
            switch (status) {
                case HEALTHY -> healthy++;
                case UNHEALTHY -> unhealthy++;
                case DEGRADED -> degraded++;
                case UNKNOWN -> unknown++;
            }
        }
        
        return Map.of(
            "healthy", healthy,
            "unhealthy", unhealthy,
            "degraded", degraded,
            "unknown", unknown,
            "total", allAddons.size()
        );
    }

    /**
     * Get current health statistics.
     */
    public Map<String, Object> getHealthStats() {
        List<MlAddon> allAddons = mlAddonRepository.findByEnabledTrue();
        
        long healthy = allAddons.stream()
                .filter(a -> a.getHealthStatus() == AddonHealthStatus.HEALTHY)
                .count();
        long unhealthy = allAddons.stream()
                .filter(a -> a.getHealthStatus() == AddonHealthStatus.UNHEALTHY)
                .count();
        long degraded = allAddons.stream()
                .filter(a -> a.getHealthStatus() == AddonHealthStatus.DEGRADED)
                .count();
        long unknown = allAddons.stream()
                .filter(a -> a.getHealthStatus() == AddonHealthStatus.UNKNOWN)
                .count();
        
        return Map.of(
            "healthy", healthy,
            "unhealthy", unhealthy,
            "degraded", degraded,
            "unknown", unknown,
            "total", allAddons.size(),
            "healthRate", allAddons.isEmpty() ? 0.0 : (double) healthy / allAddons.size()
        );
    }

    /**
     * Reset failure counter for an addon (e.g., after manual intervention).
     */
    public void resetFailureCounter(Long addonId) {
        failureCounters.remove(addonId);
        log.info("[MlAddonHealth] Reset failure counter for addon {}", addonId);
    }

    /**
     * Log health summary periodically (every hour).
     */
    @Scheduled(cron = "0 0 * * * *")
    public void logHealthSummary() {
        Map<String, Object> stats = getHealthStats();
        log.info("[MlAddonHealth] Hourly summary: healthy={}, degraded={}, unhealthy={}, unknown={}, total={}", 
                stats.get("healthy"), 
                stats.get("degraded"), 
                stats.get("unhealthy"), 
                stats.get("unknown"),
                stats.get("total"));
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/AddonOrchestratorService.java

```java
package com.newsinsight.collector.service;

import com.newsinsight.collector.dto.addon.AddonRequest;
import com.newsinsight.collector.dto.addon.AddonResponse;
import com.newsinsight.collector.entity.CollectedData;
import com.newsinsight.collector.entity.addon.*;
import com.newsinsight.collector.entity.analysis.ArticleAnalysis;
import com.newsinsight.collector.entity.analysis.ArticleDiscussion;
import com.newsinsight.collector.repository.*;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.http.MediaType;
import org.springframework.scheduling.annotation.Async;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Mono;

import java.time.Duration;
import java.time.LocalDateTime;
import java.util.*;
import java.util.concurrent.CompletableFuture;
import java.util.stream.Collectors;

/**
 * ML Add-on Orchestrator Service.
 * 
 * 기사 분석 요청을 받아 등록된 Add-on들에게 분배하고,
 * 결과를 수집하여 ArticleAnalysis/ArticleDiscussion에 저장.
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class AddonOrchestratorService {

    private final MlAddonRepository addonRepository;
    private final MlAddonExecutionRepository executionRepository;
    private final ArticleAnalysisRepository analysisRepository;
    private final ArticleDiscussionRepository discussionRepository;
    private final CollectedDataRepository collectedDataRepository;
    private final WebClient.Builder webClientBuilder;
    private final AnalysisEventService analysisEventService;

    /**
     * 단일 기사에 대해 모든 활성화된 Add-on 실행.
     * 
     * @param articleId 분석할 기사 ID
     * @param importance 중요도 (realtime / batch)
     * @return 배치 ID
     */
    @Async
    public CompletableFuture<String> analyzeArticle(Long articleId, String importance) {
        String batchId = UUID.randomUUID().toString();
        log.info("Starting article analysis: articleId={}, batchId={}, importance={}", articleId, batchId, importance);

        // 기사 조회
        CollectedData article = collectedDataRepository.findById(articleId)
                .orElseThrow(() -> new IllegalArgumentException("Article not found: " + articleId));

        // 활성화된 Add-on 목록 조회 (우선순위 순)
        List<MlAddon> addons = addonRepository.findByEnabledTrueOrderByPriorityAsc();
        
        if (addons.isEmpty()) {
            log.warn("No enabled Add-ons found");
            return CompletableFuture.completedFuture(batchId);
        }

        // 의존성 없는 Add-on들은 병렬 실행, 의존성 있는 것들은 순차 실행
        Map<String, AddonResponse> results = new HashMap<>();
        List<MlAddon> pendingAddons = new ArrayList<>(addons);

        while (!pendingAddons.isEmpty()) {
            // 현재 실행 가능한 Add-on 찾기 (의존성이 모두 충족된 것들)
            List<MlAddon> readyAddons = pendingAddons.stream()
                    .filter(addon -> areDependenciesSatisfied(addon, results.keySet()))
                    .collect(Collectors.toList());

            if (readyAddons.isEmpty() && !pendingAddons.isEmpty()) {
                log.warn("Circular dependency or missing addon detected. Remaining: {}", 
                        pendingAddons.stream().map(MlAddon::getAddonKey).collect(Collectors.toList()));
                break;
            }

            // 병렬 실행
            List<CompletableFuture<AddonResponse>> futures = readyAddons.stream()
                    .map(addon -> executeAddon(addon, article, batchId, importance, results))
                    .collect(Collectors.toList());

            // 모든 실행 완료 대기
            CompletableFuture.allOf(futures.toArray(new CompletableFuture[0])).join();

            // 결과 수집
            for (int i = 0; i < readyAddons.size(); i++) {
                MlAddon addon = readyAddons.get(i);
                try {
                    AddonResponse response = futures.get(i).get();
                    if (response != null && "success".equals(response.getStatus())) {
                        results.put(addon.getAddonKey(), response);
                    }
                } catch (Exception e) {
                    log.error("Failed to get result for addon: {}", addon.getAddonKey(), e);
                }
            }

            pendingAddons.removeAll(readyAddons);
        }

        // 결과를 ArticleAnalysis에 저장
        saveAnalysisResults(articleId, results);

        saveDiscussionResults(articleId, results);

        log.info("Article analysis completed: articleId={}, batchId={}, addonsExecuted={}", 
                articleId, batchId, results.size());

        return CompletableFuture.completedFuture(batchId);
    }

    @Transactional
    public void saveDiscussionResults(Long articleId, Map<String, AddonResponse> results) {
        ArticleDiscussion discussion = discussionRepository.findByArticleId(articleId)
                .orElse(ArticleDiscussion.builder().articleId(articleId).build());

        List<String> analyzedBy = discussion.getAnalyzedBy() != null
                ? new ArrayList<>(discussion.getAnalyzedBy())
                : new ArrayList<>();

        boolean updated = false;

        for (Map.Entry<String, AddonResponse> entry : results.entrySet()) {
            String addonKey = entry.getKey();
            AddonResponse response = entry.getValue();

            if (response == null || response.getResults() == null) continue;

            AddonResponse.AnalysisResults r = response.getResults();
            if (r.getDiscussion() == null) continue;

            AddonResponse.DiscussionResult d = r.getDiscussion();

            if (d.getOverallSentiment() != null) discussion.setOverallSentiment(d.getOverallSentiment());
            if (d.getSentimentDistribution() != null) discussion.setSentimentDistribution(d.getSentimentDistribution());
            if (d.getStanceDistribution() != null) discussion.setStanceDistribution(d.getStanceDistribution());
            if (d.getToxicityScore() != null) discussion.setToxicityScore(d.getToxicityScore());
            if (d.getTopKeywords() != null) discussion.setTopKeywords(d.getTopKeywords());
            if (d.getTimeSeries() != null) discussion.setTimeSeries(d.getTimeSeries());

            if (d.getBotLikelihood() != null) {
                discussion.setBotLikelihoodScore(d.getBotLikelihood());
                discussion.setSuspiciousPatternDetected(d.getBotLikelihood() >= 0.7);
            }

            if (r.getRaw() != null) {
                Object totalObj = r.getRaw().get("total");
                if (totalObj instanceof Number n) {
                    discussion.setTotalCommentCount(n.intValue());
                    discussion.setAnalyzedCount(n.intValue());
                }

                Object reasonsObj = r.getRaw().get("detection_reasons");
                if (reasonsObj instanceof List<?> list) {
                    List<String> reasons = list.stream()
                            .filter(Objects::nonNull)
                            .map(Object::toString)
                            .collect(Collectors.toList());
                    discussion.setSuspiciousPatterns(reasons);
                }
            }

            analyzedBy.add(addonKey);
            updated = true;
        }

        if (!updated) return;

        discussion.setAnalyzedBy(analyzedBy.stream().distinct().collect(Collectors.toList()));
        ArticleDiscussion savedDiscussion = discussionRepository.save(discussion);

        analysisEventService.publishDiscussionComplete(articleId, savedDiscussion);
    }

    /**
     * 여러 기사 일괄 분석.
     */
    @Async
    public CompletableFuture<String> analyzeArticles(List<Long> articleIds, String importance) {
        String batchId = UUID.randomUUID().toString();
        log.info("Starting batch analysis: articleCount={}, batchId={}", articleIds.size(), batchId);

        for (Long articleId : articleIds) {
            try {
                analyzeArticle(articleId, importance).join();
            } catch (Exception e) {
                log.error("Failed to analyze article: {}", articleId, e);
            }
        }

        return CompletableFuture.completedFuture(batchId);
    }

    /**
     * 특정 카테고리의 Add-on만 실행.
     */
    @Async
    public CompletableFuture<AddonResponse> executeCategory(Long articleId, AddonCategory category) {
        List<MlAddon> addons = addonRepository.findByCategoryAndEnabledTrue(category);
        if (addons.isEmpty()) {
            log.warn("No enabled addon found for category: {}", category);
            return CompletableFuture.completedFuture(null);
        }

        CollectedData article = collectedDataRepository.findById(articleId)
                .orElseThrow(() -> new IllegalArgumentException("Article not found: " + articleId));

        // 첫 번째 활성화된 Add-on 실행
        MlAddon addon = addons.get(0);
        return executeAddon(addon, article, UUID.randomUUID().toString(), "batch", new HashMap<>());
    }

    /**
     * 개별 Add-on 실행.
     */
    private CompletableFuture<AddonResponse> executeAddon(
            MlAddon addon,
            CollectedData article,
            String batchId,
            String importance,
            Map<String, AddonResponse> previousResults
    ) {
        String requestId = UUID.randomUUID().toString();

        // 실행 기록 생성
        MlAddonExecution execution = MlAddonExecution.builder()
                .requestId(requestId)
                .batchId(batchId)
                .addon(addon)
                .articleId(article.getId())
                .importance(importance)
                .status(ExecutionStatus.PENDING)
                .build();
        executionRepository.save(execution);

        // 요청 페이로드 생성
        AddonRequest request = buildRequest(requestId, addon, article, previousResults, importance);

        // 실행 시작
        execution.markStarted();
        executionRepository.save(execution);

        // SSE 이벤트 발행: 분석 시작
        analysisEventService.publishAnalysisStarted(article.getId(), addon.getAddonKey());

        // Add-on 호출
        return callAddon(addon, request)
                .map(response -> {
                    // 성공 처리
                    execution.markSuccess(
                            response.getResults() != null ? Map.of("results", response.getResults()) : null,
                            response.getMeta() != null ? response.getMeta().getModelVersion() : null
                    );
                    addon.incrementSuccess(execution.getLatencyMs());
                    executionRepository.save(execution);
                    addonRepository.save(addon);

                    // SSE 이벤트 발행: 부분 결과
                    if (response.getResults() != null) {
                        Map<String, Object> partialResult = new HashMap<>();
                        AddonResponse.AnalysisResults r = response.getResults();
                        if (r.getSentiment() != null) {
                            partialResult.put("sentimentLabel", r.getSentiment().getLabel());
                            partialResult.put("sentimentScore", r.getSentiment().getScore());
                        }
                        if (r.getReliability() != null) {
                            partialResult.put("reliabilityScore", r.getReliability().getScore());
                            partialResult.put("reliabilityGrade", r.getReliability().getGrade());
                        }
                        if (r.getBias() != null) {
                            partialResult.put("biasLabel", r.getBias().getLabel());
                            partialResult.put("biasScore", r.getBias().getScore());
                        }
                        analysisEventService.publishPartialResult(article.getId(), addon.getAddonKey(), partialResult);
                    }

                    return response;
                })
                .onErrorResume(error -> {
                    // 실패 처리
                    execution.markFailed("EXECUTION_ERROR", error.getMessage());
                    addon.incrementFailure();
                    executionRepository.save(execution);
                    addonRepository.save(addon);
                    log.error("Addon execution failed: addon={}, error={}", addon.getAddonKey(), error.getMessage());

                    // SSE 이벤트 발행: 에러
                    analysisEventService.publishAnalysisError(article.getId(), addon.getAddonKey(), error.getMessage());

                    return Mono.empty();
                })
                .toFuture();
    }

    /**
     * Add-on HTTP 호출.
     */
    private Mono<AddonResponse> callAddon(MlAddon addon, AddonRequest request) {
        if (!addon.isHttpBased()) {
            log.warn("Non-HTTP addon not yet supported: {}", addon.getInvokeType());
            return Mono.empty();
        }

        WebClient client = webClientBuilder.build();
        
        return client.post()
                .uri(addon.getEndpointUrl())
                .contentType(MediaType.APPLICATION_JSON)
                .headers(headers -> {
                    // 인증 헤더 추가
                    if (addon.getAuthType() == AddonAuthType.API_KEY && addon.getAuthCredentials() != null) {
                        headers.set("X-API-Key", addon.getAuthCredentials());
                    } else if (addon.getAuthType() == AddonAuthType.BEARER_TOKEN && addon.getAuthCredentials() != null) {
                        headers.setBearerAuth(addon.getAuthCredentials());
                    }
                })
                .bodyValue(request)
                .retrieve()
                .bodyToMono(AddonResponse.class)
                .timeout(Duration.ofMillis(addon.getTimeoutMs()))
                .doOnSubscribe(s -> log.debug("Calling addon: {} at {}", addon.getAddonKey(), addon.getEndpointUrl()))
                .doOnSuccess(r -> log.debug("Addon response received: {}", addon.getAddonKey()));
    }

    /**
     * 요청 페이로드 생성.
     */
    private AddonRequest buildRequest(
            String requestId,
            MlAddon addon,
            CollectedData article,
            Map<String, AddonResponse> previousResults,
            String importance
    ) {
        return AddonRequest.builder()
                .requestId(requestId)
                .addonId(addon.getAddonKey())
                .task("article_analysis")
                .inputSchemaVersion(addon.getInputSchemaVersion())
                .article(AddonRequest.ArticleInput.builder()
                        .id(article.getId())
                        .title(article.getTitle())
                        .content(article.getContent())
                        .url(article.getUrl())
                        .publishedAt(article.getPublishedDate() != null ? article.getPublishedDate().toString() : null)
                        .build())
                .context(AddonRequest.AnalysisContext.builder()
                        .language("ko")
                        .country("KR")
                        .previousResults(previousResults.entrySet().stream()
                                .collect(Collectors.toMap(
                                        Map.Entry::getKey,
                                        e -> e.getValue().getResults()
                                )))
                        .build())
                .options(AddonRequest.ExecutionOptions.builder()
                        .importance(importance)
                        .timeoutMs(addon.getTimeoutMs())
                        .build())
                .build();
    }

    /**
     * 의존성 충족 여부 확인.
     */
    private boolean areDependenciesSatisfied(MlAddon addon, Set<String> completedAddons) {
        if (addon.getDependsOn() == null || addon.getDependsOn().isEmpty()) {
            return true;
        }
        return completedAddons.containsAll(addon.getDependsOn());
    }

    /**
     * 분석 결과를 ArticleAnalysis에 저장.
     */
    @Transactional
    public void saveAnalysisResults(Long articleId, Map<String, AddonResponse> results) {
        ArticleAnalysis analysis = analysisRepository.findByArticleId(articleId)
                .orElse(ArticleAnalysis.builder().articleId(articleId).build());

        List<String> analyzedBy = new ArrayList<>();
        Map<String, Boolean> analysisStatus = new HashMap<>();

        for (Map.Entry<String, AddonResponse> entry : results.entrySet()) {
            String addonKey = entry.getKey();
            AddonResponse response = entry.getValue();
            
            if (response == null || response.getResults() == null) continue;

            analyzedBy.add(addonKey);
            analysisStatus.put(addonKey, true);

            AddonResponse.AnalysisResults r = response.getResults();

            // 감정 분석 결과 저장
            if (r.getSentiment() != null) {
                analysis.setSentimentScore(r.getSentiment().getScore());
                analysis.setSentimentLabel(r.getSentiment().getLabel());
                analysis.setSentimentDistribution(r.getSentiment().getDistribution());
            }

            // 신뢰도 결과 저장
            if (r.getReliability() != null) {
                analysis.setReliabilityScore(r.getReliability().getScore());
                analysis.setReliabilityGrade(r.getReliability().getGrade());
                analysis.setReliabilityFactors(r.getReliability().getFactors());
            }

            // 편향도 결과 저장
            if (r.getBias() != null) {
                analysis.setBiasLabel(r.getBias().getLabel());
                analysis.setBiasScore(r.getBias().getScore());
                analysis.setBiasDetails(r.getBias().getDetails());
            }

            // 팩트체크 결과 저장
            if (r.getFactcheck() != null) {
                analysis.setFactcheckStatus(r.getFactcheck().getStatus());
                analysis.setFactcheckNotes(r.getFactcheck().getNotes());
            }

            // 요약 결과 저장
            if (r.getSummary() != null) {
                analysis.setSummary(r.getSummary().getAbstractiveSummary());
                analysis.setKeySentences(r.getSummary().getExtractiveSentences());
            }

            // 주제 분류 결과 저장
            if (r.getTopics() != null) {
                analysis.setTopics(r.getTopics().getLabels());
                analysis.setTopicScores(r.getTopics().getScores());
            }

            // 허위정보 결과 저장
            if (r.getMisinformation() != null) {
                analysis.setMisinfoRisk(r.getMisinformation().getRiskLevel());
                analysis.setMisinfoScore(r.getMisinformation().getScore());
            }

            // 독성 분석 결과 저장
            if (r.getToxicity() != null) {
                analysis.setToxicityScore(r.getToxicity().getScore());
            }
        }

        analysis.setAnalyzedBy(analyzedBy);
        analysis.setAnalysisStatus(analysisStatus);
        analysis.setFullyAnalyzed(!results.isEmpty());

        ArticleAnalysis savedAnalysis = analysisRepository.save(analysis);
        log.info("Saved analysis results for article: {}, addons: {}", articleId, analyzedBy);

        // SSE 이벤트 발행: 분석 완료
        analysisEventService.publishAnalysisComplete(articleId, savedAnalysis);
    }

    /**
     * Add-on 헬스체크 실행.
     */
    @Async
    public void runHealthChecks() {
        LocalDateTime cutoff = LocalDateTime.now().minusMinutes(5);
        List<MlAddon> addons = addonRepository.findAddonsNeedingHealthCheck(cutoff);

        for (MlAddon addon : addons) {
            try {
                WebClient client = webClientBuilder.build();
                client.get()
                        .uri(addon.getHealthCheckUrl())
                        .retrieve()
                        .toBodilessEntity()
                        .timeout(Duration.ofSeconds(10))
                        .subscribe(
                                response -> {
                                    addonRepository.updateHealthStatus(addon.getId(), AddonHealthStatus.HEALTHY, LocalDateTime.now());
                                    log.debug("Health check passed: {}", addon.getAddonKey());
                                },
                                error -> {
                                    addonRepository.updateHealthStatus(addon.getId(), AddonHealthStatus.UNHEALTHY, LocalDateTime.now());
                                    log.warn("Health check failed: {}", addon.getAddonKey());
                                }
                        );
            } catch (Exception e) {
                log.error("Health check error for addon: {}", addon.getAddonKey(), e);
            }
        }
    }

    /**
     * 특정 Add-on으로 직접 분석 실행 (커스텀 입력, 기사 ID 없이).
     * 프론트엔드에서 직접 호출 시 사용.
     * 
     * @param addon 실행할 Add-on
     * @param articleData 기사 데이터 (title, content, url 등)
     * @param requestId 요청 ID
     * @param importance 중요도
     * @return 분석 결과
     */
    public AddonResponse executeAddonDirect(
            MlAddon addon,
            Map<String, Object> articleData,
            String requestId,
            String importance
    ) {
        log.info("Direct addon execution: addon={}, requestId={}", addon.getAddonKey(), requestId);
        
        // 요청 페이로드 생성
        AddonRequest request = AddonRequest.builder()
                .requestId(requestId)
                .addonId(addon.getAddonKey())
                .task("direct_analysis")
                .inputSchemaVersion(addon.getInputSchemaVersion())
                .article(AddonRequest.ArticleInput.builder()
                        .id(articleData.get("id") != null ? Long.parseLong(articleData.get("id").toString()) : null)
                        .title((String) articleData.get("title"))
                        .content((String) articleData.get("content"))
                        .url((String) articleData.get("url"))
                        .source((String) articleData.get("source"))
                        .publishedAt((String) articleData.get("publishedAt"))
                        .build())
                .context(AddonRequest.AnalysisContext.builder()
                        .language("ko")
                        .country("KR")
                        .build())
                .options(AddonRequest.ExecutionOptions.builder()
                        .importance(importance)
                        .timeoutMs(addon.getTimeoutMs())
                        .build())
                .build();
        
        // 실행 기록 생성
        MlAddonExecution execution = MlAddonExecution.builder()
                .requestId(requestId)
                .batchId(null)
                .addon(addon)
                .articleId(articleData.get("id") != null ? Long.parseLong(articleData.get("id").toString()) : null)
                .importance(importance)
                .status(ExecutionStatus.PENDING)
                .build();
        executionRepository.save(execution);
        
        execution.markStarted();
        executionRepository.save(execution);
        
        try {
            // Add-on 호출 (동기)
            AddonResponse response = callAddon(addon, request)
                    .block(Duration.ofMillis(addon.getTimeoutMs() + 5000));
            
            if (response != null && "success".equals(response.getStatus())) {
                execution.markSuccess(
                        response.getResults() != null ? Map.of("results", response.getResults()) : null,
                        response.getMeta() != null ? response.getMeta().getModelVersion() : null
                );
                addon.incrementSuccess(execution.getLatencyMs());
            } else {
                String errorMsg = response != null && response.getError() != null 
                        ? response.getError().getMessage() 
                        : "Unknown error";
                execution.markFailed("ADDON_ERROR", errorMsg);
                addon.incrementFailure();
            }
            
            executionRepository.save(execution);
            addonRepository.save(addon);
            
            return response;
        } catch (Exception e) {
            execution.markFailed("EXECUTION_ERROR", e.getMessage());
            addon.incrementFailure();
            executionRepository.save(execution);
            addonRepository.save(addon);
            log.error("Direct addon execution failed: addon={}, error={}", addon.getAddonKey(), e.getMessage());
            throw new RuntimeException("Addon execution failed: " + e.getMessage(), e);
        }
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/AiMessagingService.java

```java
package com.newsinsight.collector.service;

import com.newsinsight.collector.dto.AiRequestMessage;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Service;

import java.time.OffsetDateTime;
import java.util.Map;
import java.util.UUID;

@Service
@RequiredArgsConstructor
@Slf4j
public class AiMessagingService {

    private final KafkaTemplate<String, AiRequestMessage> aiRequestKafkaTemplate;

    @Value("${collector.ai.topic.request:newsinsight.ai.requests}")
    private String requestTopic;

    @Value("${collector.ai.default-provider-id:openai}")
    private String defaultProviderId;

    @Value("${collector.ai.default-model-id:gpt-4.1}")
    private String defaultModelId;

    public String sendAnalysisRequest(String query, String window, String message, Map<String, Object> context) {
        String requestId = UUID.randomUUID().toString();
        String type = "ARTICLE_ANALYSIS";
        String effectiveWindow = (window == null || window.isBlank()) ? "7d" : window;
        AiRequestMessage payload = new AiRequestMessage(
                requestId,
                type,
                query,
                effectiveWindow,
                message,
                context,
                defaultProviderId,
                defaultModelId,
                null,
                null,
                "collector-service"
        );
        aiRequestKafkaTemplate.send(requestTopic, requestId, payload);
        log.info("Sent AI analysis request {} to topic {} at {}", requestId, requestTopic, OffsetDateTime.now());
        return requestId;
    }

    public String sendAnalysisRequestWithRole(
            String query,
            String window,
            String message,
            Map<String, Object> context,
            String agentRole,
            String outputSchema
    ) {
        String requestId = UUID.randomUUID().toString();
        String type = "ARTICLE_ANALYSIS";
        String effectiveWindow = (window == null || window.isBlank()) ? "7d" : window;
        AiRequestMessage payload = new AiRequestMessage(
                requestId,
                type,
                query,
                effectiveWindow,
                message,
                context,
                defaultProviderId,
                defaultModelId,
                agentRole,
                outputSchema,
                "collector-service"
        );
        aiRequestKafkaTemplate.send(requestTopic, requestId, payload);
        log.info("Sent AI analysis request {} (role={}) to topic {} at {}", requestId, agentRole, requestTopic, OffsetDateTime.now());
        return requestId;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/AiProviderFallbackService.java

```java
package com.newsinsight.collector.service;

import com.newsinsight.collector.client.AIDoveClient;
import com.newsinsight.collector.client.OpenAICompatibleClient;
import com.newsinsight.collector.client.PerplexityClient;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Mono;

import java.time.Duration;
import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.function.Supplier;

/**
 * AI Provider Fallback Chain Service.
 * Provides resilient AI completion by trying multiple providers in sequence.
 * 
 * Fallback order:
 * 1. Perplexity (if enabled) - Best for fact-checking with online search
 * 2. OpenAI (if enabled)
 * 3. OpenRouter (if enabled)
 * 4. Azure OpenAI (if enabled)
 * 5. AI Dove (if enabled) - n8n webhook
 * 6. Ollama (local) - Last resort
 * 7. Custom endpoint (if enabled)
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class AiProviderFallbackService {

    private final PerplexityClient perplexityClient;
    private final OpenAICompatibleClient openAICompatibleClient;
    private final AIDoveClient aiDoveClient;

    /**
     * Stream completion using fallback chain.
     * Tries each available provider in order until one succeeds.
     * 
     * @param prompt The prompt to send
     * @return Flux of response chunks
     */
    public Flux<String> streamCompletionWithFallback(String prompt) {
        List<ProviderAttempt> providers = buildProviderChain(prompt);
        
        if (providers.isEmpty()) {
            log.error("No AI providers are available");
            return Flux.just("AI 분석을 수행할 수 없습니다. 설정된 AI 제공자가 없습니다.");
        }

        log.info("AI fallback chain initialized with {} providers: {}", 
                providers.size(), 
                providers.stream().map(ProviderAttempt::name).toList());

        return tryProvidersInSequence(providers, 0);
    }

    /**
     * Get completion (non-streaming) using fallback chain.
     * Collects all chunks into a single string.
     * 
     * @param prompt The prompt to send
     * @return Mono of complete response
     */
    public Mono<String> getCompletionWithFallback(String prompt) {
        return streamCompletionWithFallback(prompt)
                .collectList()
                .map(chunks -> String.join("", chunks));
    }

    /**
     * Check which providers are currently available
     */
    public List<String> getAvailableProviders() {
        List<String> available = new ArrayList<>();
        
        if (perplexityClient.isEnabled()) {
            available.add("Perplexity");
        }
        if (openAICompatibleClient.isOpenAIEnabled()) {
            available.add("OpenAI");
        }
        if (openAICompatibleClient.isOpenRouterEnabled()) {
            available.add("OpenRouter");
        }
        if (openAICompatibleClient.isAzureEnabled()) {
            available.add("Azure OpenAI");
        }
        if (aiDoveClient.isEnabled()) {
            available.add("AI Dove");
        }
        if (openAICompatibleClient.isOllamaEnabled()) {
            available.add("Ollama");
        }
        if (openAICompatibleClient.isCustomEnabled()) {
            available.add("Custom");
        }
        
        return available;
    }

    /**
     * Check if any AI provider is available
     */
    public boolean isAnyProviderAvailable() {
        return perplexityClient.isEnabled() 
                || openAICompatibleClient.isEnabled()
                || aiDoveClient.isEnabled();
    }

    /**
     * Build the provider chain based on availability
     */
    private List<ProviderAttempt> buildProviderChain(String prompt) {
        List<ProviderAttempt> chain = new ArrayList<>();

        // 1. Perplexity - Best for fact-checking with online search capabilities
        if (perplexityClient.isEnabled()) {
            chain.add(new ProviderAttempt(
                    "Perplexity",
                    () -> perplexityClient.streamCompletion(prompt)
            ));
        }

        // 2. OpenAI
        if (openAICompatibleClient.isOpenAIEnabled()) {
            chain.add(new ProviderAttempt(
                    "OpenAI",
                    () -> openAICompatibleClient.streamFromOpenAI(prompt)
            ));
        }

        // 3. OpenRouter - Access to multiple models
        if (openAICompatibleClient.isOpenRouterEnabled()) {
            chain.add(new ProviderAttempt(
                    "OpenRouter",
                    () -> openAICompatibleClient.streamFromOpenRouter(prompt)
            ));
        }

        // 4. Azure OpenAI
        if (openAICompatibleClient.isAzureEnabled()) {
            chain.add(new ProviderAttempt(
                    "Azure OpenAI",
                    () -> openAICompatibleClient.streamFromAzure(prompt)
            ));
        }

        // 5. AI Dove (n8n webhook) - Simulated streaming
        if (aiDoveClient.isEnabled()) {
            chain.add(new ProviderAttempt(
                    "AI Dove",
                    () -> aiDoveClient.chatStream(prompt, null)
            ));
        }

        // 6. Ollama - Local LLM (always available but may not be running)
        chain.add(new ProviderAttempt(
                "Ollama",
                () -> openAICompatibleClient.streamFromOllama(prompt)
        ));

        // 7. Custom endpoint
        if (openAICompatibleClient.isCustomEnabled()) {
            chain.add(new ProviderAttempt(
                    "Custom",
                    () -> openAICompatibleClient.streamFromCustom(prompt)
            ));
        }

        return chain;
    }

    /**
     * Try providers in sequence until one succeeds
     */
    private Flux<String> tryProvidersInSequence(List<ProviderAttempt> providers, int index) {
        if (index >= providers.size()) {
            log.error("All AI providers failed");
            return Flux.just("모든 AI 제공자 연결에 실패했습니다. 나중에 다시 시도해주세요.");
        }

        ProviderAttempt current = providers.get(index);
        log.info("Attempting AI provider: {} (attempt {}/{})", current.name(), index + 1, providers.size());

        AtomicInteger chunkCount = new AtomicInteger(0);

        return current.streamSupplier().get()
                .timeout(Duration.ofSeconds(90))
                .doOnNext(chunk -> chunkCount.incrementAndGet())
                .doOnComplete(() -> {
                    if (chunkCount.get() > 0) {
                        log.info("AI provider {} completed successfully with {} chunks", 
                                current.name(), chunkCount.get());
                    }
                })
                .onErrorResume(e -> {
                    log.warn("AI provider {} failed: {}. Trying next provider...", 
                            current.name(), e.getMessage());
                    return tryProvidersInSequence(providers, index + 1);
                })
                .switchIfEmpty(Flux.defer(() -> {
                    log.warn("AI provider {} returned empty response. Trying next provider...", 
                            current.name());
                    return tryProvidersInSequence(providers, index + 1);
                }));
    }

    /**
     * Provider attempt wrapper
     */
    private record ProviderAttempt(
            String name,
            Supplier<Flux<String>> streamSupplier
    ) {}
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/AiResultConsumerService.java

```java
package com.newsinsight.collector.service;

import com.newsinsight.collector.dto.AiResponseMessage;
import com.newsinsight.collector.mongo.AiResponseDocument;
import com.newsinsight.collector.mongo.AiResponseRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Service;

import java.time.Instant;

@Service
@RequiredArgsConstructor
@Slf4j
public class AiResultConsumerService {

    private final AiResponseRepository aiResponseRepository;

    @KafkaListener(
            topics = "${collector.ai.topic.response:newsinsight.ai.responses}",
            groupId = "${spring.application.name}-ai",
            containerFactory = "aiResponseKafkaListenerContainerFactory"
    )
    public void handleAiResponse(AiResponseMessage message) {
        log.info("Received AI response requestId={} status={} model={}",
                message.requestId(), message.status(), message.modelId());

        AiResponseDocument document = new AiResponseDocument(
                message.requestId(),
                message.status(),
                message.completedAt(),
                message.providerId(),
                message.modelId(),
                message.text(),
                message.raw(),
                Instant.now()
        );

        aiResponseRepository.save(document);
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/AnalysisEventService.java

```java
package com.newsinsight.collector.service;

import com.newsinsight.collector.entity.analysis.ArticleAnalysis;
import com.newsinsight.collector.entity.analysis.ArticleDiscussion;
import lombok.extern.slf4j.Slf4j;
import org.springframework.http.codec.ServerSentEvent;
import org.springframework.stereotype.Service;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Sinks;

import java.time.Duration;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.ConcurrentHashMap;

/**
 * SSE 서비스 - 분석 결과 실시간 업데이트 알림
 * 
 * 프론트엔드가 특정 기사 ID들을 구독하면, 해당 기사의 분석이 완료될 때 알림을 받습니다.
 * 검색 결과 페이지에서 분석 중인 기사들의 상태를 실시간으로 업데이트할 때 사용합니다.
 */
@Service
@Slf4j
public class AnalysisEventService {

    // Global sink for all analysis updates (clients filter by articleId)
    private final Sinks.Many<ServerSentEvent<Object>> globalSink;
    
    // Track which article IDs are being watched
    private final Set<Long> watchedArticleIds = ConcurrentHashMap.newKeySet();
    
    // Subscriber count for cleanup
    private int subscriberCount = 0;

    public AnalysisEventService() {
        this.globalSink = Sinks.many().multicast().onBackpressureBuffer(500);
    }

    /**
     * Subscribe to analysis updates for specific article IDs.
     * 
     * @param articleIds Set of article IDs to watch
     * @return SSE event stream
     */
    public Flux<ServerSentEvent<Object>> subscribeToAnalysisUpdates(Set<Long> articleIds) {
        if (articleIds != null && !articleIds.isEmpty()) {
            watchedArticleIds.addAll(articleIds);
        }

        // Heartbeat stream
        Flux<ServerSentEvent<Object>> heartbeat = Flux.interval(Duration.ofSeconds(20))
                .map(tick -> ServerSentEvent.builder()
                        .event("heartbeat")
                        .data(Map.of("timestamp", System.currentTimeMillis()))
                        .build());

        // Filter events by article IDs if provided
        Flux<ServerSentEvent<Object>> events = globalSink.asFlux()
                .filter(event -> {
                    if (articleIds == null || articleIds.isEmpty()) {
                        return true; // No filter, receive all
                    }
                    Object data = event.data();
                    if (data instanceof Map<?, ?> dataMap) {
                        Object articleId = dataMap.get("articleId");
                        if (articleId instanceof Long id) {
                            return articleIds.contains(id);
                        }
                    }
                    return false;
                });

        return Flux.merge(heartbeat, events)
                .doOnSubscribe(sub -> {
                    subscriberCount++;
                    log.debug("New analysis updates subscriber, total: {}", subscriberCount);
                })
                .doOnCancel(() -> {
                    subscriberCount--;
                    log.debug("Analysis updates subscriber disconnected, total: {}", subscriberCount);
                    // Clean up watched IDs if no more subscribers
                    if (subscriberCount <= 0) {
                        watchedArticleIds.clear();
                    }
                });
    }

    /**
     * Publish analysis started event.
     * 
     * @param articleId The article ID
     * @param addonKey The addon that started analysis
     */
    public void publishAnalysisStarted(Long articleId, String addonKey) {
        if (!watchedArticleIds.contains(articleId)) return;

        ServerSentEvent<Object> event = ServerSentEvent.builder()
                .event("analysis_started")
                .data(Map.of(
                        "articleId", articleId,
                        "addonKey", addonKey,
                        "timestamp", System.currentTimeMillis()
                ))
                .build();

        globalSink.tryEmitNext(event);
        log.debug("Published analysis_started for article: {}, addon: {}", articleId, addonKey);
    }

    /**
     * Publish analysis progress event.
     * 
     * @param articleId The article ID
     * @param addonKey The addon processing
     * @param progress Progress percentage (0-100)
     */
    public void publishAnalysisProgress(Long articleId, String addonKey, int progress) {
        if (!watchedArticleIds.contains(articleId)) return;

        ServerSentEvent<Object> event = ServerSentEvent.builder()
                .event("analysis_progress")
                .data(Map.of(
                        "articleId", articleId,
                        "addonKey", addonKey,
                        "progress", progress,
                        "timestamp", System.currentTimeMillis()
                ))
                .build();

        globalSink.tryEmitNext(event);
    }

    /**
     * Publish partial analysis result (single addon completed).
     * 
     * @param articleId The article ID
     * @param addonKey The addon that completed
     * @param result The partial result data
     */
    public void publishPartialResult(Long articleId, String addonKey, Map<String, Object> result) {
        if (!watchedArticleIds.contains(articleId)) return;

        ServerSentEvent<Object> event = ServerSentEvent.builder()
                .event("analysis_partial")
                .data(Map.of(
                        "articleId", articleId,
                        "addonKey", addonKey,
                        "result", result,
                        "timestamp", System.currentTimeMillis()
                ))
                .build();

        globalSink.tryEmitNext(event);
        log.debug("Published partial analysis for article: {}, addon: {}", articleId, addonKey);
    }

    /**
     * Publish full analysis complete event.
     * 
     * @param articleId The article ID
     * @param analysis The complete analysis result
     */
    public void publishAnalysisComplete(Long articleId, ArticleAnalysis analysis) {
        // Build analysis map using HashMap since we have more than 10 entries
        Map<String, Object> analysisMap = new HashMap<>();
        analysisMap.put("reliabilityScore", analysis.getReliabilityScore() != null ? analysis.getReliabilityScore() : 0);
        analysisMap.put("reliabilityGrade", analysis.getReliabilityGrade() != null ? analysis.getReliabilityGrade() : "unknown");
        analysisMap.put("reliabilityColor", analysis.getReliabilityColor());
        analysisMap.put("sentimentLabel", analysis.getSentimentLabel() != null ? analysis.getSentimentLabel() : "neutral");
        analysisMap.put("sentimentScore", analysis.getSentimentScore() != null ? analysis.getSentimentScore() : 0);
        analysisMap.put("biasLabel", analysis.getBiasLabel());
        analysisMap.put("biasScore", analysis.getBiasScore());
        analysisMap.put("factcheckStatus", analysis.getFactcheckStatus());
        analysisMap.put("misinfoRisk", analysis.getMisinfoRisk());
        analysisMap.put("riskTags", analysis.getRiskTags());
        analysisMap.put("topics", analysis.getTopics());
        analysisMap.put("summary", analysis.getSummary());
        analysisMap.put("fullyAnalyzed", analysis.getFullyAnalyzed());

        ServerSentEvent<Object> event = ServerSentEvent.builder()
                .event("analysis_complete")
                .data(Map.of(
                        "articleId", articleId,
                        "analysis", analysisMap,
                        "timestamp", System.currentTimeMillis()
                ))
                .build();

        globalSink.tryEmitNext(event);
        log.info("Published analysis_complete for article: {}", articleId);
        
        // Remove from watched list
        watchedArticleIds.remove(articleId);
    }

    /**
     * Publish discussion analysis complete event.
     * 
     * @param articleId The article ID
     * @param discussion The discussion analysis result
     */
    public void publishDiscussionComplete(Long articleId, ArticleDiscussion discussion) {
        ServerSentEvent<Object> event = ServerSentEvent.builder()
                .event("discussion_complete")
                .data(Map.of(
                        "articleId", articleId,
                        "discussion", Map.of(
                                "totalCommentCount", discussion.getTotalCommentCount() != null ? discussion.getTotalCommentCount() : 0,
                                "overallSentiment", discussion.getOverallSentiment() != null ? discussion.getOverallSentiment() : "unknown",
                                "sentimentDistribution", discussion.getSentimentDistribution() != null ? discussion.getSentimentDistribution() : Map.of(),
                                "discussionQualityScore", discussion.getDiscussionQualityScore(),
                                "stanceDistribution", discussion.getStanceDistribution(),
                                "suspiciousPatternDetected", discussion.getSuspiciousPatternDetected()
                        ),
                        "timestamp", System.currentTimeMillis()
                ))
                .build();

        globalSink.tryEmitNext(event);
        log.info("Published discussion_complete for article: {}", articleId);
    }

    /**
     * Publish analysis error event.
     * 
     * @param articleId The article ID
     * @param addonKey The addon that failed
     * @param errorMessage Error description
     */
    public void publishAnalysisError(Long articleId, String addonKey, String errorMessage) {
        if (!watchedArticleIds.contains(articleId)) return;

        ServerSentEvent<Object> event = ServerSentEvent.builder()
                .event("analysis_error")
                .data(Map.of(
                        "articleId", articleId,
                        "addonKey", addonKey != null ? addonKey : "unknown",
                        "error", errorMessage,
                        "timestamp", System.currentTimeMillis()
                ))
                .build();

        globalSink.tryEmitNext(event);
        log.warn("Published analysis_error for article: {}, addon: {}", articleId, addonKey);
    }

    /**
     * Add article IDs to watch list.
     * 
     * @param articleIds Article IDs to watch
     */
    public void watchArticles(Set<Long> articleIds) {
        if (articleIds != null) {
            watchedArticleIds.addAll(articleIds);
        }
    }

    /**
     * Remove article ID from watch list.
     * 
     * @param articleId Article ID to stop watching
     */
    public void unwatchArticle(Long articleId) {
        watchedArticleIds.remove(articleId);
    }

    /**
     * Check if an article is being watched.
     * 
     * @param articleId The article ID
     * @return true if being watched
     */
    public boolean isWatched(Long articleId) {
        return watchedArticleIds.contains(articleId);
    }

    /**
     * Get count of watched articles.
     * 
     * @return Count of watched article IDs
     */
    public int getWatchedCount() {
        return watchedArticleIds.size();
    }

    /**
     * Get subscriber count.
     * 
     * @return Number of active subscribers
     */
    public int getSubscriberCount() {
        return subscriberCount;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/AnalysisService.java

```java
package com.newsinsight.collector.service;

import com.newsinsight.collector.dto.AnalysisResponseDto;
import com.newsinsight.collector.dto.ArticleDto;
import com.newsinsight.collector.dto.ArticlesResponseDto;
import com.newsinsight.collector.dto.KeywordDataDto;
import com.newsinsight.collector.dto.SentimentDataDto;
import com.newsinsight.collector.entity.CollectedData;
import com.newsinsight.collector.entity.DataSource;
import com.newsinsight.collector.repository.CollectedDataRepository;
import com.newsinsight.collector.repository.DataSourceRepository;
import lombok.RequiredArgsConstructor;
import org.jsoup.Jsoup;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.PageRequest;
import org.springframework.data.domain.Pageable;
import org.springframework.data.domain.Sort;
import org.springframework.stereotype.Service;

import java.time.LocalDateTime;
import java.time.OffsetDateTime;
import java.util.*;

@Service
@RequiredArgsConstructor
public class AnalysisService {

    private static final int MAX_KEYWORD_DOCS = 200;
    private static final int SNIPPET_MAX_LENGTH = 200;

    private static final Set<String> STOP_WORDS = Set.of(
            "the", "and", "or", "a", "an", "of", "to", "in", "on", "for", "with",
            "이", "그", "저", "에서", "으로", "에게", "하다", "되다"
    );

    private final CollectedDataRepository collectedDataRepository;
    private final DataSourceRepository dataSourceRepository;
    private final AiMessagingService aiMessagingService;

    public AnalysisResponseDto analyze(String query, String window) {
        LocalDateTime now = LocalDateTime.now();
        String effectiveWindow = window;
        LocalDateTime since;
        switch (window) {
            case "1d" -> since = now.minusDays(1);
            case "30d" -> since = now.minusDays(30);
            case "7d" -> since = now.minusDays(7);
            default -> {
                since = now.minusDays(7);
                effectiveWindow = "7d";
            }
        }

        String normalizedQuery = (query != null && !query.isBlank()) ? query : null;
        String message = normalizedQuery != null ? normalizedQuery : "";
        aiMessagingService.sendAnalysisRequest(normalizedQuery, window, message, Map.of());
        Page<CollectedData> page = collectedDataRepository.searchByQueryAndSince(normalizedQuery, since, Pageable.unpaged());
        long articleCount = page.getTotalElements();

        List<CollectedData> documents = page.getContent();

        double pos = 0.0;
        double neg = 0.0;
        double neu = 0.0;

        for (CollectedData data : documents) {
            Double quality = data.getQualityScore();
            if (quality == null) {
                neu += 1.0;
            } else if (quality >= 0.66) {
                pos += 1.0;
            } else if (quality <= 0.33) {
                neg += 1.0;
            } else {
                neu += 1.0;
            }
        }

        if (pos == 0.0 && neg == 0.0 && neu == 0.0) {
            neu = 1.0;
        }

        SentimentDataDto sentiments = new SentimentDataDto(pos, neg, neu);
        List<KeywordDataDto> topKeywords = extractTopKeywords(documents, query);

        String analyzedAt = OffsetDateTime.now().toString();

        return new AnalysisResponseDto(query, effectiveWindow, articleCount, sentiments, topKeywords, analyzedAt);
    }

    public ArticlesResponseDto searchArticles(String query, int limit) {
        int pageSize = limit > 0 ? limit : 50;
        // Note: Native query already has ORDER BY clause, so use unsorted PageRequest
        PageRequest pageRequest = PageRequest.of(0, pageSize, Sort.unsorted());
        String normalizedQuery = (query != null && !query.isBlank()) ? query : null;
        Page<CollectedData> page = collectedDataRepository.searchByQuery(normalizedQuery, pageRequest);

        List<ArticleDto> articles = page.getContent().stream()
                .map(this::toArticleDto)
                .toList();

        return new ArticlesResponseDto(query, articles, page.getTotalElements());
    }

    private ArticleDto toArticleDto(CollectedData data) {
        String id = data.getId() != null ? data.getId().toString() : null;
        String title = data.getTitle();
        DataSource source = data.getSourceId() != null
                ? dataSourceRepository.findById(data.getSourceId()).orElse(null)
                : null;
        String sourceName = source != null ? source.getName() : "Unknown";

        String publishedAt;
        if (data.getPublishedDate() != null) {
            publishedAt = data.getPublishedDate().toString();
        } else if (data.getCollectedAt() != null) {
            publishedAt = data.getCollectedAt().toString();
        } else {
            publishedAt = null;
        }

        String url = data.getUrl();
        // 원본 콘텐츠를 보존하면서 정제된 텍스트 생성
        String rawContent = data.getContent();
        String cleanedContent = cleanContent(rawContent);
        String snippet = buildSnippetFromCleanText(cleanedContent);

        return new ArticleDto(id, title, sourceName, publishedAt, url, snippet, cleanedContent);
    }

    private List<KeywordDataDto> extractTopKeywords(List<CollectedData> documents, String query) {
        if (documents == null || documents.isEmpty()) {
            return List.of();
        }

        Map<String, Integer> freq = new HashMap<>();
        int docCount = 0;

        for (CollectedData data : documents) {
            if (docCount >= MAX_KEYWORD_DOCS) {
                break;
            }
            docCount++;

            StringBuilder sb = new StringBuilder();
            if (data.getTitle() != null) {
                sb.append(data.getTitle()).append(' ');
            }
            if (data.getContent() != null) {
                sb.append(data.getContent());
            }

            String text;
            try {
                text = Jsoup.parse(sb.toString()).text();
            } catch (Exception e) {
                text = sb.toString();
            }

            text = text.toLowerCase(Locale.ROOT);
            String[] tokens = text.split("[^\\p{L}0-9]+");
            for (String token : tokens) {
                if (token == null || token.isBlank()) continue;
                if (token.length() <= 1) continue;
                if (STOP_WORDS.contains(token)) continue;
                if (query != null && token.equalsIgnoreCase(query)) continue;

                // @CHECK 
                // token이 null이 될 수 있음 
                freq.merge(token, 1, Integer::sum);
            }
        }

        if (freq.isEmpty()) {
            return query == null || query.isBlank()
                    ? List.of()
                    : List.of(new KeywordDataDto(query, 1.0));
        }

        return freq.entrySet().stream()
                .sorted(Map.Entry.<String, Integer>comparingByValue().reversed())
                .limit(10)
                .map(e -> new KeywordDataDto(e.getKey(), e.getValue()))
                .toList();
    }

    /**
     * 이미 정제된 텍스트에서 snippet 생성 (HTML 파싱 불필요)
     */
    private String buildSnippetFromCleanText(String cleanText) {
        if (cleanText == null || cleanText.isBlank()) {
            return null;
        }

        if (cleanText.length() <= SNIPPET_MAX_LENGTH) {
            return cleanText;
        }

        // 단어 경계에서 자르기
        int cut = SNIPPET_MAX_LENGTH;
        for (int i = Math.min(SNIPPET_MAX_LENGTH - 1, cleanText.length() - 1); 
             i > SNIPPET_MAX_LENGTH * 0.6 && i >= 0; i--) {
            if (Character.isWhitespace(cleanText.charAt(i))) {
                cut = i;
                break;
            }
        }

        return cleanText.substring(0, cut).trim() + "...";
    }

    /**
     * 레거시 호환성을 위한 buildSnippet (HTML 파싱 포함)
     */
    private String buildSnippet(String content) {
        if (content == null || content.isBlank()) {
            return null;
        }

        String text;
        try {
            text = Jsoup.parse(content).text();
        } catch (Exception e) {
            text = content;
        }

        text = text.replaceAll("\\s+", " ").trim();
        if (text.isEmpty()) {
            return null;
        }

        if (text.length() <= SNIPPET_MAX_LENGTH) {
            return text;
        }

        int startIdx = Math.min(SNIPPET_MAX_LENGTH - 1, text.length() - 1);
        int cut = SNIPPET_MAX_LENGTH;
        for (int i = startIdx; i > SNIPPET_MAX_LENGTH * 0.6 && i >= 0; i--) {
            if (Character.isWhitespace(text.charAt(i))) {
                cut = i;
                break;
            }
        }

        return text.substring(0, cut).trim() + "...";
    }

    /**
     * HTML 태그를 제거하고 정리된 전체 텍스트를 반환합니다.
     * snippet과 달리 길이 제한 없이 전체 내용을 반환합니다.
     * 
     * 중요: 이 메서드는 원본 텍스트 내용을 최대한 보존하며,
     * HTML 태그만 제거하고 실제 텍스트 데이터는 변경하지 않습니다.
     *
     * @param content 원본 콘텐츠 (HTML 포함 가능)
     * @return 정리된 전체 텍스트 (원본 데이터 보존)
     */
    private String cleanContent(String content) {
        if (content == null || content.isBlank()) {
            return null;
        }

        String text;
        try {
            // Jsoup을 사용하여 HTML 태그만 제거, 텍스트 내용은 보존
            text = Jsoup.parse(content).text();
        } catch (Exception e) {
            // HTML 파싱 실패 시 원본 그대로 사용
            text = content;
        }

        // 연속 공백만 정리 (실제 텍스트 내용은 변경하지 않음)
        text = text.replaceAll("\\s+", " ").trim();
        
        return text.isEmpty() ? null : text;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/ChatSyncService.java

```java
package com.newsinsight.collector.service;

import com.newsinsight.collector.entity.chat.ChatHistory;
import com.newsinsight.collector.entity.chat.FactCheckChatSession;
import com.newsinsight.collector.repository.ChatHistoryRepository;
import com.newsinsight.collector.repository.FactCheckChatSessionRepository;
import io.micrometer.core.instrument.Counter;
import io.micrometer.core.instrument.MeterRegistry;
import io.micrometer.core.instrument.Timer;
import jakarta.annotation.PostConstruct;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.scheduling.annotation.Async;
import org.springframework.scheduling.annotation.Scheduled;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.time.Duration;
import java.time.LocalDateTime;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.atomic.AtomicLong;

/**
 * 채팅 동기화 서비스
 * 
 * MongoDB → RDB 동기화 (백그라운드)
 * MongoDB → 벡터 DB 임베딩 (백그라운드)
 * 
 * 개선사항:
 * - 5분 경과 조건 추가
 * - 배치 저장 지원
 * - 동시성 제어 강화
 * - 메트릭 수집
 * - 재시도 로직
 */
@Service
@Slf4j
public class ChatSyncService {

    private final FactCheckChatSessionRepository sessionRepository;
    private final ChatHistoryRepository chatHistoryRepository;
    private final VectorEmbeddingService vectorEmbeddingService;
    private final MeterRegistry meterRegistry;

    // 설정값
    @Value("${chat.sync.min-messages:10}")
    private int minMessagesForSync;

    @Value("${chat.sync.max-idle-minutes:5}")
    private int maxIdleMinutesForSync;

    @Value("${chat.sync.batch-size:50}")
    private int batchSize;

    @Value("${chat.sync.max-retry:3}")
    private int maxRetryAttempts;

    @Value("${chat.sync.session-expire-hours:24}")
    private int sessionExpireHours;

    // 메트릭
    private Counter syncSuccessCounter;
    private Counter syncErrorCounter;
    private Counter embeddingSuccessCounter;
    private Counter embeddingErrorCounter;
    private Counter sessionExpiredCounter;
    private Timer syncDurationTimer;
    private Timer embeddingDurationTimer;
    private final AtomicLong pendingSyncGauge = new AtomicLong(0);
    private final AtomicLong pendingEmbeddingGauge = new AtomicLong(0);

    // 동시성 제어 - 진행 중인 동기화 세션 추적
    private final ConcurrentHashMap<String, LocalDateTime> syncingSessionsMap = new ConcurrentHashMap<>();
    
    // 마지막 동기화 시간 추적
    private final ConcurrentHashMap<String, LocalDateTime> lastSyncTimeMap = new ConcurrentHashMap<>();

    public ChatSyncService(
            FactCheckChatSessionRepository sessionRepository,
            ChatHistoryRepository chatHistoryRepository,
            VectorEmbeddingService vectorEmbeddingService,
            MeterRegistry meterRegistry
    ) {
        this.sessionRepository = sessionRepository;
        this.chatHistoryRepository = chatHistoryRepository;
        this.vectorEmbeddingService = vectorEmbeddingService;
        this.meterRegistry = meterRegistry;
    }

    @PostConstruct
    public void initMetrics() {
        syncSuccessCounter = Counter.builder("chat.sync.rdb.success")
                .description("Number of successful RDB syncs")
                .register(meterRegistry);
        
        syncErrorCounter = Counter.builder("chat.sync.rdb.error")
                .description("Number of failed RDB syncs")
                .register(meterRegistry);
        
        embeddingSuccessCounter = Counter.builder("chat.sync.embedding.success")
                .description("Number of successful embeddings")
                .register(meterRegistry);
        
        embeddingErrorCounter = Counter.builder("chat.sync.embedding.error")
                .description("Number of failed embeddings")
                .register(meterRegistry);
        
        sessionExpiredCounter = Counter.builder("chat.sync.sessions.expired")
                .description("Number of sessions expired")
                .register(meterRegistry);
        
        syncDurationTimer = Timer.builder("chat.sync.rdb.duration")
                .description("Time taken for RDB sync")
                .register(meterRegistry);
        
        embeddingDurationTimer = Timer.builder("chat.sync.embedding.duration")
                .description("Time taken for embedding")
                .register(meterRegistry);

        meterRegistry.gauge("chat.sync.rdb.pending", pendingSyncGauge);
        meterRegistry.gauge("chat.sync.embedding.pending", pendingEmbeddingGauge);
    }

    /**
     * 동기화가 필요한 경우 스케줄링
     * 조건:
     * 1. 메시지가 minMessagesForSync개 이상
     * 2. 마지막 동기화로부터 maxIdleMinutesForSync분 경과
     */
    public void scheduleSyncIfNeeded(FactCheckChatSession session) {
        if (session.isSyncedToRdb()) {
            return;
        }

        boolean shouldSync = false;
        String reason = "";

        // 조건 1: 메시지 개수 체크
        if (session.getMessages().size() >= minMessagesForSync) {
            shouldSync = true;
            reason = "message count >= " + minMessagesForSync;
        }

        // 조건 2: 마지막 동기화로부터 시간 경과 체크
        LocalDateTime lastSync = lastSyncTimeMap.get(session.getSessionId());
        if (lastSync != null) {
            Duration elapsed = Duration.between(lastSync, LocalDateTime.now());
            if (elapsed.toMinutes() >= maxIdleMinutesForSync) {
                shouldSync = true;
                reason = "idle time >= " + maxIdleMinutesForSync + " minutes";
            }
        } else if (session.getStartedAt() != null) {
            // 최초 동기화 - 세션 시작 후 5분 경과 시
            Duration elapsed = Duration.between(session.getStartedAt(), LocalDateTime.now());
            if (elapsed.toMinutes() >= maxIdleMinutesForSync && session.getMessages().size() > 0) {
                shouldSync = true;
                reason = "first sync after " + maxIdleMinutesForSync + " minutes";
            }
        }

        if (shouldSync) {
            log.debug("Scheduling sync for session {}: {}", session.getSessionId(), reason);
            syncSessionToRdbAsync(session);
        }
    }

    /**
     * 세션을 RDB로 동기화 (비동기)
     */
    @Async("chatSyncExecutor")
    public void syncSessionToRdbAsync(FactCheckChatSession session) {
        String sessionId = session.getSessionId();
        
        // 중복 동기화 방지
        if (syncingSessionsMap.putIfAbsent(sessionId, LocalDateTime.now()) != null) {
            log.debug("Session {} is already being synced, skipping", sessionId);
            return;
        }

        try {
            syncSessionToRdbWithRetry(session);
        } finally {
            syncingSessionsMap.remove(sessionId);
        }
    }

    /**
     * 세션을 RDB로 동기화 (재시도 포함)
     */
    private void syncSessionToRdbWithRetry(FactCheckChatSession session) {
        int attempts = 0;
        Exception lastException = null;

        while (attempts < maxRetryAttempts) {
            try {
                syncSessionToRdb(session);
                return; // 성공 시 반환
            } catch (Exception e) {
                attempts++;
                lastException = e;
                log.warn("Sync attempt {} failed for session {}: {}", 
                        attempts, session.getSessionId(), e.getMessage());
                
                if (attempts < maxRetryAttempts) {
                    try {
                        // 지수 백오프
                        Thread.sleep((long) Math.pow(2, attempts) * 1000);
                    } catch (InterruptedException ie) {
                        Thread.currentThread().interrupt();
                        break;
                    }
                }
            }
        }

        log.error("Failed to sync session {} after {} attempts", 
                session.getSessionId(), maxRetryAttempts, lastException);
        syncErrorCounter.increment();
    }

    /**
     * 세션을 RDB로 동기화 (동기)
     */
    @Transactional
    public void syncSessionToRdb(FactCheckChatSession session) {
        Timer.Sample sample = Timer.start(meterRegistry);
        
        log.info("Syncing session {} to RDB ({} messages)", 
                session.getSessionId(), session.getMessages().size());

        // 배치로 저장할 메시지 수집
        List<ChatHistory> toSave = new ArrayList<>();
        
        for (FactCheckChatSession.ChatMessage message : session.getMessages()) {
            // 이미 동기화된 메시지는 건너뛰기
            if (chatHistoryRepository.existsByMessageId(message.getMessageId())) {
                continue;
            }

            // RDB 엔티티 생성
            ChatHistory chatHistory = ChatHistory.builder()
                    .sessionId(session.getSessionId())
                    .messageId(message.getMessageId())
                    .userId(session.getUserId())
                    .role(message.getRole())
                    .content(message.getContent())
                    .messageType(message.getType() != null ? message.getType().name() : null)
                    .metadata(convertMetadata(message.getMetadata()))
                    .build();

            toSave.add(chatHistory);

            // 배치 크기에 도달하면 저장
            if (toSave.size() >= batchSize) {
                chatHistoryRepository.saveAll(toSave);
                toSave.clear();
            }
        }

        // 남은 메시지 저장
        if (!toSave.isEmpty()) {
            chatHistoryRepository.saveAll(toSave);
        }

        // 동기화 완료 플래그 업데이트
        session.setSyncedToRdb(true);
        sessionRepository.save(session);

        // 마지막 동기화 시간 업데이트
        lastSyncTimeMap.put(session.getSessionId(), LocalDateTime.now());

        sample.stop(syncDurationTimer);
        syncSuccessCounter.increment();
        
        log.info("Synced {} new messages from session {} to RDB", 
                toSave.size(), session.getSessionId());

        // 벡터 임베딩 트리거
        if (!session.isEmbeddedToVectorDb()) {
            embedSessionToVectorDbAsync(session);
        }
    }

    /**
     * 세션을 벡터 DB로 임베딩 (비동기)
     */
    @Async("chatSyncExecutor")
    public void embedSessionToVectorDbAsync(FactCheckChatSession session) {
        String sessionId = session.getSessionId();
        
        try {
            embedSessionToVectorDbWithRetry(session);
        } catch (Exception e) {
            log.error("Failed to embed session {} to vector DB: {}", 
                    sessionId, e.getMessage(), e);
            embeddingErrorCounter.increment();
        }
    }

    /**
     * 세션을 벡터 DB로 임베딩 (재시도 포함)
     */
    private void embedSessionToVectorDbWithRetry(FactCheckChatSession session) {
        int attempts = 0;
        Exception lastException = null;

        while (attempts < maxRetryAttempts) {
            try {
                embedSessionToVectorDb(session);
                return;
            } catch (Exception e) {
                attempts++;
                lastException = e;
                log.warn("Embedding attempt {} failed for session {}: {}", 
                        attempts, session.getSessionId(), e.getMessage());
                
                if (attempts < maxRetryAttempts) {
                    try {
                        Thread.sleep((long) Math.pow(2, attempts) * 1000);
                    } catch (InterruptedException ie) {
                        Thread.currentThread().interrupt();
                        break;
                    }
                }
            }
        }

        log.error("Failed to embed session {} after {} attempts", 
                session.getSessionId(), maxRetryAttempts, lastException);
        embeddingErrorCounter.increment();
    }

    /**
     * 세션을 벡터 DB로 임베딩 (동기)
     */
    @Transactional
    public void embedSessionToVectorDb(FactCheckChatSession session) {
        Timer.Sample sample = Timer.start(meterRegistry);
        
        log.info("Embedding session {} to vector DB", session.getSessionId());

        // assistant 메시지만 임베딩 (팩트체크 결과)
        List<FactCheckChatSession.ChatMessage> assistantMessages = session.getMessages().stream()
                .filter(msg -> "assistant".equals(msg.getRole()))
                .filter(msg -> msg.getContent() != null && !msg.getContent().isBlank())
                .filter(msg -> msg.getType() == FactCheckChatSession.MessageType.AI_SYNTHESIS 
                        || msg.getType() == FactCheckChatSession.MessageType.VERIFICATION
                        || msg.getType() == FactCheckChatSession.MessageType.ASSESSMENT)
                .toList();

        if (assistantMessages.isEmpty()) {
            log.debug("No assistant messages to embed for session {}", session.getSessionId());
            session.setEmbeddedToVectorDb(true);
            sessionRepository.save(session);
            return;
        }

        int embeddedCount = 0;
        for (FactCheckChatSession.ChatMessage message : assistantMessages) {
            try {
                // 벡터 임베딩 생성 및 저장
                String embeddingId = vectorEmbeddingService.embedChatMessage(
                        session.getSessionId(),
                        message.getMessageId(),
                        message.getContent(),
                        message.getMetadata()
                );

                if (embeddingId != null) {
                    // RDB에 임베딩 ID 업데이트
                    updateEmbeddingIdInRdb(session.getSessionId(), message.getMessageId(), embeddingId);
                    embeddedCount++;
                }
            } catch (Exception e) {
                log.error("Failed to embed message {}: {}", message.getMessageId(), e.getMessage());
            }
        }

        // 임베딩 완료 플래그 업데이트
        session.setEmbeddedToVectorDb(true);
        sessionRepository.save(session);

        sample.stop(embeddingDurationTimer);
        embeddingSuccessCounter.increment();

        log.info("Embedded {} messages from session {} to vector DB", 
                embeddedCount, session.getSessionId());
    }

    /**
     * RDB에 임베딩 ID 업데이트
     */
    private void updateEmbeddingIdInRdb(String sessionId, String messageId, String embeddingId) {
        chatHistoryRepository.findBySessionIdOrderByCreatedAtAsc(sessionId)
                .stream()
                .filter(ch -> ch.getMessageId().equals(messageId))
                .findFirst()
                .ifPresent(ch -> {
                    ch.setEmbeddingId(embeddingId);
                    chatHistoryRepository.save(ch);
                });
    }

    /**
     * 스케줄러: 주기적으로 동기화되지 않은 세션 처리
     */
    @Scheduled(fixedDelayString = "${chat.sync.scheduler.interval:300000}") // 기본 5분마다
    public void syncPendingSessions() {
        log.debug("Running scheduled sync for pending sessions");

        List<FactCheckChatSession.SessionStatus> targetStatuses = List.of(
                FactCheckChatSession.SessionStatus.ACTIVE,
                FactCheckChatSession.SessionStatus.COMPLETED,
                FactCheckChatSession.SessionStatus.EXPIRED
        );

        // RDB 동기화 대상 조회
        List<FactCheckChatSession> unsyncedSessions = 
                sessionRepository.findBySyncedToRdbFalseAndStatusIn(targetStatuses);
        
        pendingSyncGauge.set(unsyncedSessions.size());
        log.info("Found {} sessions to sync to RDB", unsyncedSessions.size());
        
        for (FactCheckChatSession session : unsyncedSessions) {
            // 5분 이상 경과한 세션만 동기화
            if (shouldSyncNow(session)) {
                syncSessionToRdbAsync(session);
            }
        }

        // 벡터 DB 임베딩 대상 조회
        List<FactCheckChatSession> unembeddedSessions = 
                sessionRepository.findByEmbeddedToVectorDbFalseAndStatusIn(
                        List.of(FactCheckChatSession.SessionStatus.COMPLETED,
                                FactCheckChatSession.SessionStatus.EXPIRED));
        
        pendingEmbeddingGauge.set(unembeddedSessions.size());
        log.info("Found {} sessions to embed to vector DB", unembeddedSessions.size());
        
        for (FactCheckChatSession session : unembeddedSessions) {
            // RDB 동기화 완료된 세션만 임베딩
            if (session.isSyncedToRdb()) {
                embedSessionToVectorDbAsync(session);
            }
        }
    }

    /**
     * 지금 동기화해야 하는지 확인
     */
    private boolean shouldSyncNow(FactCheckChatSession session) {
        // 완료/만료 세션은 즉시 동기화
        if (session.getStatus() != FactCheckChatSession.SessionStatus.ACTIVE) {
            return true;
        }

        // 활성 세션은 마지막 활동으로부터 5분 경과 시 동기화
        if (session.getLastActivityAt() != null) {
            Duration elapsed = Duration.between(session.getLastActivityAt(), LocalDateTime.now());
            return elapsed.toMinutes() >= maxIdleMinutesForSync;
        }

        return true;
    }

    /**
     * 스케줄러: 오래된 활성 세션 만료 처리
     */
    @Scheduled(fixedDelayString = "${chat.sync.expire.interval:3600000}") // 기본 1시간마다
    public void expireInactiveSessions() {
        LocalDateTime expiryThreshold = LocalDateTime.now().minusHours(sessionExpireHours);
        
        List<FactCheckChatSession> inactiveSessions = sessionRepository
                .findByStatusAndLastActivityAtBefore(
                        FactCheckChatSession.SessionStatus.ACTIVE, 
                        expiryThreshold
                );

        log.info("Found {} inactive sessions to expire", inactiveSessions.size());
        
        for (FactCheckChatSession session : inactiveSessions) {
            session.setStatus(FactCheckChatSession.SessionStatus.EXPIRED);
            session.setEndedAt(LocalDateTime.now());
            sessionRepository.save(session);
            
            // 만료된 세션도 동기화
            syncSessionToRdbAsync(session);
            
            sessionExpiredCounter.increment();
            log.info("Expired session: {}", session.getSessionId());
        }

        // 메모리 정리 - 오래된 추적 데이터 삭제
        cleanupTrackingData();
    }

    /**
     * 스케줄러: 동기화 상태 정리 (stuck 상태 복구)
     */
    @Scheduled(fixedDelay = 600000) // 10분마다
    public void cleanupStuckSyncs() {
        LocalDateTime stuckThreshold = LocalDateTime.now().minusMinutes(10);
        
        syncingSessionsMap.entrySet().removeIf(entry -> {
            if (entry.getValue().isBefore(stuckThreshold)) {
                log.warn("Removing stuck sync for session: {}", entry.getKey());
                return true;
            }
            return false;
        });
    }

    /**
     * 오래된 추적 데이터 정리
     */
    private void cleanupTrackingData() {
        LocalDateTime cleanupThreshold = LocalDateTime.now().minusHours(sessionExpireHours * 2);
        
        lastSyncTimeMap.entrySet().removeIf(entry -> 
                entry.getValue().isBefore(cleanupThreshold));
    }

    /**
     * 메타데이터 변환 (Object → Map)
     */
    @SuppressWarnings("unchecked")
    private Map<String, Object> convertMetadata(Object metadata) {
        if (metadata == null) {
            return new HashMap<>();
        }
        if (metadata instanceof Map) {
            return (Map<String, Object>) metadata;
        }
        // 다른 타입의 경우 빈 맵 반환
        Map<String, Object> result = new HashMap<>();
        result.put("raw", metadata.toString());
        return result;
    }

    /**
     * 동기화 통계 조회
     */
    public SyncStats getSyncStats() {
        return SyncStats.builder()
                .pendingSyncCount(pendingSyncGauge.get())
                .pendingEmbeddingCount(pendingEmbeddingGauge.get())
                .activeSyncCount(syncingSessionsMap.size())
                .build();
    }

    @lombok.Data
    @lombok.Builder
    public static class SyncStats {
        private long pendingSyncCount;
        private long pendingEmbeddingCount;
        private int activeSyncCount;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/ClaimExtractionService.java

```java
package com.newsinsight.collector.service;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.newsinsight.collector.client.AIDoveClient;
import com.newsinsight.collector.dto.ClaimExtractionRequest;
import com.newsinsight.collector.dto.ClaimExtractionResponse;
import com.newsinsight.collector.dto.ClaimExtractionResponse.ExtractedClaim;
import com.newsinsight.collector.dto.CrawledPage;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;
import org.jsoup.nodes.Element;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.MediaType;
import org.springframework.stereotype.Service;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Mono;
import reactor.core.scheduler.Schedulers;

import java.time.Duration;
import java.util.*;
import java.util.concurrent.atomic.AtomicInteger;

/**
 * Service for extracting verifiable claims from URLs.
 * 
 * Uses IntegratedCrawler for content extraction and AI Dove for claim analysis.
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class ClaimExtractionService {

    private final WebClient webClient;
    private final ObjectMapper objectMapper;
    private final AIDoveClient aiDoveClient;

    @Value("${collector.crawler.base-url:http://web-crawler:11235}")
    private String crawl4aiBaseUrl;

    @Value("${collector.claim-extraction.timeout-seconds:60}")
    private int timeoutSeconds;

    @Value("${collector.claim-extraction.max-claims:10}")
    private int defaultMaxClaims;

    @Value("${collector.claim-extraction.min-confidence:0.5}")
    private double defaultMinConfidence;

    /**
     * Extract claims from a URL.
     * 
     * Pipeline:
     * 1. Crawl the URL to get page content
     * 2. Send content to AI Dove with claim extraction prompt
     * 3. Parse and return structured claims
     */
    public Mono<ClaimExtractionResponse> extractClaims(ClaimExtractionRequest request) {
        long startTime = System.currentTimeMillis();
        String url = request.getUrl();
        int maxClaims = request.getMaxClaims() != null ? request.getMaxClaims() : defaultMaxClaims;
        double minConfidence = request.getMinConfidence() != null ? request.getMinConfidence() : defaultMinConfidence;

        log.info("Starting claim extraction for URL: {}", url);

        return crawlUrl(url)
                .flatMap(page -> {
                    if (page.content() == null || page.content().isBlank()) {
                        return Mono.just(ClaimExtractionResponse.builder()
                                .url(url)
                                .pageTitle(page.title())
                                .claims(Collections.emptyList())
                                .processingTimeMs(System.currentTimeMillis() - startTime)
                                .extractionSource(page.source())
                                .message("페이지에서 분석할 수 있는 콘텐츠를 찾지 못했습니다.")
                                .build());
                    }

                    return extractClaimsFromContent(page.content(), page.title(), maxClaims)
                            .map(claims -> {
                                // Filter by minimum confidence
                                List<ExtractedClaim> filteredClaims = claims.stream()
                                        .filter(c -> c.getConfidence() >= minConfidence)
                                        .toList();

                                return ClaimExtractionResponse.builder()
                                        .url(url)
                                        .pageTitle(page.title())
                                        .claims(filteredClaims)
                                        .processingTimeMs(System.currentTimeMillis() - startTime)
                                        .extractionSource(page.source())
                                        .message(filteredClaims.isEmpty() 
                                                ? "검증 가능한 주장을 찾지 못했습니다." 
                                                : null)
                                        .build();
                            });
                })
                .onErrorResume(e -> {
                    log.error("Claim extraction failed for URL {}: {}", url, e.getMessage());
                    return Mono.just(ClaimExtractionResponse.builder()
                            .url(url)
                            .claims(Collections.emptyList())
                            .processingTimeMs(System.currentTimeMillis() - startTime)
                            .message("주장 추출 실패: " + e.getMessage())
                            .build());
                });
    }

    /**
     * Crawl URL using multiple strategies with fallback
     */
    private Mono<CrawledPage> crawlUrl(String url) {
        // Try Crawl4AI first
        return crawlWithCrawl4AI(url)
                .switchIfEmpty(crawlDirect(url))
                .timeout(Duration.ofSeconds(timeoutSeconds))
                .doOnSuccess(page -> log.debug("Successfully crawled: {} using {}", url, page.source()));
    }

    /**
     * Crawl using Crawl4AI service
     */
    private Mono<CrawledPage> crawlWithCrawl4AI(String url) {
        String endpoint = crawl4aiBaseUrl + "/md";

        Map<String, Object> payload = Map.of(
                "url", url,
                "bypass_cache", true,
                "word_count_threshold", 50,
                "remove_overlay_elements", true,
                "process_iframes", true
        );

        return webClient.post()
                .uri(endpoint)
                .contentType(MediaType.APPLICATION_JSON)
                .bodyValue(payload)
                .retrieve()
                .bodyToMono(String.class)
                .timeout(Duration.ofSeconds(30))
                .map(response -> parseCrawl4AIResponse(url, response))
                .filter(page -> page.content() != null && !page.content().isBlank())
                .doOnSuccess(page -> log.debug("Crawl4AI success: {}", url))
                .onErrorResume(e -> {
                    log.debug("Crawl4AI failed for {}: {}", url, e.getMessage());
                    return Mono.empty();
                });
    }

    /**
     * Direct HTTP crawl using Jsoup
     */
    private Mono<CrawledPage> crawlDirect(String url) {
        return Mono.fromCallable(() -> {
                    Document doc = Jsoup.connect(url)
                            .userAgent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
                            .timeout(30000)
                            .followRedirects(true)
                            .get();

                    String title = doc.title();
                    String content = extractMainContent(doc);

                    return new CrawledPage(url, title, content, "direct", new ArrayList<>());
                })
                .subscribeOn(Schedulers.boundedElastic())
                .filter(page -> page.content() != null && page.content().length() > 100)
                .doOnSuccess(page -> log.debug("Direct crawl success: {}", url))
                .onErrorResume(e -> {
                    log.debug("Direct crawl failed for {}: {}", url, e.getMessage());
                    return Mono.empty();
                });
    }

    /**
     * Parse Crawl4AI response
     */
    private CrawledPage parseCrawl4AIResponse(String url, String response) {
        try {
            JsonNode node = objectMapper.readTree(response);
            String content = null;
            String title = null;

            if (node.has("result")) {
                JsonNode result = node.get("result");
                if (result.has("markdown")) {
                    content = result.get("markdown").asText();
                }
                if (result.has("metadata") && result.get("metadata").has("title")) {
                    title = result.get("metadata").get("title").asText();
                }
            } else if (node.has("markdown")) {
                content = node.get("markdown").asText();
            }

            // Truncate very long content
            if (content != null && content.length() > 15000) {
                content = content.substring(0, 15000) + "\n...[truncated]";
            }

            return new CrawledPage(url, title, content, "crawl4ai", new ArrayList<>());
        } catch (Exception e) {
            log.warn("Failed to parse Crawl4AI response for {}: {}", url, e.getMessage());
            return new CrawledPage(url, null, null, "crawl4ai", new ArrayList<>());
        }
    }

    /**
     * Extract main content from HTML document
     */
    private String extractMainContent(Document doc) {
        // Remove unwanted elements
        doc.select("script, style, nav, header, footer, aside, .advertisement, .ads, .sidebar, .comment, .comments").remove();

        // Try to find article content
        Element article = doc.selectFirst("article, .article, .content, .post-content, main, .main-content, .article-body, .entry-content");
        if (article != null) {
            return article.text();
        }

        // Fallback to body
        Element body = doc.body();
        return body != null ? body.text() : doc.text();
    }

    /**
     * Extract claims from content using AI Dove
     */
    private Mono<List<ExtractedClaim>> extractClaimsFromContent(String content, String title, int maxClaims) {
        if (!aiDoveClient.isEnabled()) {
            log.warn("AI Dove is disabled, cannot extract claims");
            return Mono.just(Collections.emptyList());
        }

        String prompt = buildClaimExtractionPrompt(content, title, maxClaims);

        return aiDoveClient.chat(prompt, null)
                .map(response -> parseClaimsFromAI(response.reply()))
                .onErrorResume(e -> {
                    log.error("AI claim extraction failed: {}", e.getMessage());
                    return Mono.just(Collections.emptyList());
                });
    }

    /**
     * Build prompt for claim extraction
     */
    private String buildClaimExtractionPrompt(String content, String title, int maxClaims) {
        // Truncate content if too long
        String truncatedContent = content;
        if (content.length() > 10000) {
            truncatedContent = content.substring(0, 10000) + "\n...[truncated]";
        }

        return """
                You are an expert fact-checker. Analyze the following news article and extract verifiable claims.
                
                A "verifiable claim" is a factual statement that can be confirmed or refuted through evidence.
                Do NOT include opinions, predictions, or subjective statements.
                
                For each claim, provide:
                1. The exact claim text (1-2 sentences)
                2. A confidence score (0.0-1.0) indicating how clearly stated the claim is
                3. The context where it was found (e.g., "headline", "first paragraph", "statistics section")
                4. The type of claim: "statistical" (numbers/data), "event" (something happened), "quote" (attributed statement), "general" (other factual claims)
                5. Whether it's verifiable (true/false)
                
                Return ONLY a JSON array with this structure (no other text):
                [
                  {
                    "text": "The claim text",
                    "confidence": 0.85,
                    "context": "Found in the opening paragraph",
                    "claimType": "statistical",
                    "verifiable": true
                  }
                ]
                
                Extract up to %d claims. Prioritize:
                1. Statistical claims (numbers, percentages, data)
                2. Event claims (specific things that happened)
                3. Attributed quotes (statements from named sources)
                
                Title: %s
                
                Content:
                %s
                """.formatted(maxClaims, title != null ? title : "Unknown", truncatedContent);
    }

    /**
     * Parse claims from AI response
     */
    private List<ExtractedClaim> parseClaimsFromAI(String aiResponse) {
        if (aiResponse == null || aiResponse.isBlank()) {
            return Collections.emptyList();
        }

        try {
            // Extract JSON array from response
            String json = extractJsonArray(aiResponse);
            if (json == null) {
                log.warn("No JSON array found in AI response");
                return Collections.emptyList();
            }

            JsonNode claimsArray = objectMapper.readTree(json);
            List<ExtractedClaim> claims = new ArrayList<>();
            AtomicInteger idCounter = new AtomicInteger(1);

            for (JsonNode node : claimsArray) {
                ExtractedClaim claim = ExtractedClaim.builder()
                        .id("claim-" + idCounter.getAndIncrement())
                        .text(node.has("text") ? node.get("text").asText() : "")
                        .confidence(node.has("confidence") ? node.get("confidence").asDouble() : 0.7)
                        .context(node.has("context") ? node.get("context").asText() : null)
                        .claimType(node.has("claimType") ? node.get("claimType").asText() : "general")
                        .verifiable(node.has("verifiable") ? node.get("verifiable").asBoolean() : true)
                        .build();

                // Only add claims with actual text
                if (claim.getText() != null && !claim.getText().isBlank()) {
                    claims.add(claim);
                }
            }

            log.info("Extracted {} claims from AI response", claims.size());
            return claims;
        } catch (Exception e) {
            log.warn("Failed to parse AI claims response: {}", e.getMessage());
            return Collections.emptyList();
        }
    }

    /**
     * Extract JSON array from text that may contain other content
     */
    private String extractJsonArray(String text) {
        if (text == null) return null;

        int start = text.indexOf('[');
        int end = text.lastIndexOf(']');

        if (start >= 0 && end > start) {
            return text.substring(start, end + 1);
        }
        return null;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/CollectedDataService.java

```java
package com.newsinsight.collector.service;

import com.newsinsight.collector.config.TrustScoreConfig;
import com.newsinsight.collector.entity.CollectedData;
import com.newsinsight.collector.repository.CollectedDataRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.PageRequest;
import org.springframework.data.domain.Pageable;
import org.springframework.data.domain.Sort;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.nio.charset.StandardCharsets;
import java.security.MessageDigest;
import java.security.NoSuchAlgorithmException;
import java.util.Optional;

@Service
@RequiredArgsConstructor
@Slf4j
public class CollectedDataService {

    private final CollectedDataRepository collectedDataRepository;
    private final TrustScoreConfig trustScoreConfig;

    /**
     * 중복 제거를 위한 SHA-256 콘텐츠 해시 계산
     */
    public String computeContentHash(String url, String title, String content) {
        try {
            MessageDigest digest = MessageDigest.getInstance("SHA-256");
            digest.update((url != null ? url : "").getBytes(StandardCharsets.UTF_8));
            digest.update((title != null ? title : "").getBytes(StandardCharsets.UTF_8));
            digest.update((content != null ? content : "").getBytes(StandardCharsets.UTF_8));
            
            byte[] hash = digest.digest();
            StringBuilder hexString = new StringBuilder();
            for (byte b : hash) {
                String hex = Integer.toHexString(0xff & b);
                if (hex.length() == 1) hexString.append('0');
                hexString.append(hex);
            }
            return hexString.toString();
        } catch (NoSuchAlgorithmException e) {
            throw new RuntimeException("SHA-256 algorithm not available", e);
        }
    }

    /**
     * 해시값으로 콘텐츠 존재 여부 확인
     */
    public boolean isDuplicate(String contentHash) {
        return collectedDataRepository.findByContentHash(contentHash).isPresent();
    }

    /**
     * 수집된 데이터 저장
     */
    @Transactional
    public CollectedData save(CollectedData data) {
        // 콘텐츠 해시가 비어있으면 계산하여 설정
        if (data.getContentHash() == null) {
            String hash = computeContentHash(data.getUrl(), data.getTitle(), data.getContent());
            data.setContentHash(hash);
        }
        
        // 중복 여부 확인
        if (isDuplicate(data.getContentHash())) {
            log.debug("Duplicate content detected: {}", data.getContentHash());
            data.setDuplicate(true);
        }
        
        return collectedDataRepository.save(data);
    }

    /**
     * 수집된 데이터 단건 조회 (ID)
     */
    public Optional<CollectedData> findById(Long id) {
        return collectedDataRepository.findById(id);
    }

    /**
     * 수집된 데이터 전체 조회 (페이지네이션)
     */
    public Page<CollectedData> findAll(Pageable pageable) {
        return collectedDataRepository.findAll(pageable);
    }

    /**
     * 미처리 데이터 조회
     */
    public Page<CollectedData> findUnprocessed(Pageable pageable) {
        return collectedDataRepository.findByProcessedFalse(pageable);
    }

    /**
     * 소스 ID 기준 데이터 조회
     */
    public Page<CollectedData> findBySourceId(Long sourceId, Pageable pageable) {
        return collectedDataRepository.findBySourceId(sourceId, pageable);
    }

    /**
     * 키워드 기반 검색 (제목 + 본문)
     * Note: Native query에 ORDER BY가 이미 포함되어 있으므로 Sort.unsorted() 사용
     */
    public Page<CollectedData> search(String query, Pageable pageable) {
        if (query == null || query.isBlank()) {
            return collectedDataRepository.findAll(pageable);
        }
        // Native query already has ORDER BY, create unsorted pageable to avoid duplicate ORDER BY
        Pageable unsortedPageable = PageRequest.of(pageable.getPageNumber(), pageable.getPageSize(), Sort.unsorted());
        return collectedDataRepository.searchByQuery(query.trim(), unsortedPageable);
    }

    /**
     * 키워드 기반 검색 + 처리 상태 필터
     * Note: Native query에 ORDER BY가 이미 포함되어 있으므로 Sort.unsorted() 사용
     */
    public Page<CollectedData> searchWithFilter(String query, Boolean processed, Pageable pageable) {
        if (query == null || query.isBlank()) {
            if (processed == null) {
                return collectedDataRepository.findAll(pageable);
            } else if (Boolean.FALSE.equals(processed)) {
                return collectedDataRepository.findByProcessedFalse(pageable);
            } else {
                return collectedDataRepository.findByProcessed(true, pageable);
            }
        }
        
        // Native query already has ORDER BY, create unsorted pageable to avoid duplicate ORDER BY
        Pageable unsortedPageable = PageRequest.of(pageable.getPageNumber(), pageable.getPageSize(), Sort.unsorted());
        Page<CollectedData> results = collectedDataRepository.searchByQuery(query.trim(), unsortedPageable);
        if (processed == null) {
            return results;
        }
        // Note: 효율적인 구현을 위해서는 Repository에 복합 쿼리 추가 필요
        // 현재는 검색 결과 그대로 반환 (필터링은 클라이언트에서 처리)
        return results;
    }

    /**
     * 데이터 처리 완료로 마킹
     */
    @Transactional
    public boolean markAsProcessed(Long id) {
        Optional<CollectedData> dataOpt = collectedDataRepository.findById(id);
        if (dataOpt.isEmpty()) {
            return false;
        }
        
        CollectedData data = dataOpt.get();
        data.setProcessed(true);
        collectedDataRepository.save(data);
        return true;
    }

    /**
     * 전체 수집 건수 카운트
     */
    public long countTotal() {
        return collectedDataRepository.count();
    }

    /**
     * 미처리 건수 카운트
     */
    public long countUnprocessed() {
        return collectedDataRepository.countByProcessedFalse();
    }

    /**
     * QA 지표 기반 품질 점수 계산
     */
    public double calculateQualityScore(
            Boolean httpOk,
            boolean hasContent,
            boolean duplicate,
            double semanticConsistency,
            double outlierScore) {
        
        double httpScore = httpOk == null ? 0.5 : (httpOk ? 1.0 : 0.0);
        double contentScore = hasContent ? 1.0 : 0.0;
        double duplicatePenalty = duplicate ? 1.0 : 0.0;
        double outlierPenalty = Math.max(0.0, Math.min(1.0, outlierScore));
        double sem = Math.max(0.0, Math.min(1.0, semanticConsistency));
        
        double score = 0.25 * httpScore + 0.25 * contentScore + 0.3 * sem + 
                      0.2 * (1.0 - outlierPenalty) - 0.2 * duplicatePenalty;
        
        return Math.max(0.0, Math.min(1.0, score));
    }

    /**
     * URL 도메인 기반 신뢰도 점수 계산
     * Uses externalized trust score configuration.
     */
    public double calculateTrustScore(String url, Boolean httpOk, boolean inWhitelist) {
        TrustScoreConfig.DataQuality dq = trustScoreConfig.getDataQuality();
        double base = inWhitelist ? dq.getWhitelistScore() : dq.getBaseScore();
        if (Boolean.TRUE.equals(httpOk)) {
            base += dq.getHttpOkBonus();
        }
        return Math.max(0.0, Math.min(1.0, base));
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/CollectionService.java

```java
package com.newsinsight.collector.service;

import com.newsinsight.collector.dto.BrowserTaskMessage;
import com.newsinsight.collector.dto.CollectionStatsDTO;
import com.newsinsight.collector.dto.CrawlCommandMessage;
import com.newsinsight.collector.dto.CrawlResultMessage;
import com.newsinsight.collector.entity.BrowserAgentConfig;
import com.newsinsight.collector.entity.CollectedData;
import com.newsinsight.collector.entity.CollectionJob;
import com.newsinsight.collector.entity.CollectionJob.JobStatus;
import com.newsinsight.collector.entity.DataSource;
import com.newsinsight.collector.entity.SourceType;
import com.newsinsight.collector.repository.CollectionJobRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Optional;

@Service
@RequiredArgsConstructor
@Slf4j
public class CollectionService {

    private final CollectionJobRepository collectionJobRepository;
    private final DataSourceService dataSourceService;
    private final RssFeedService rssFeedService;
    private final WebScraperService webScraperService;
    private final CollectedDataService collectedDataService;

    private final KafkaTemplate<String, CrawlCommandMessage> crawlCommandKafkaTemplate;
    private final KafkaTemplate<String, CrawlResultMessage> crawlResultKafkaTemplate;
    private final KafkaTemplate<String, BrowserTaskMessage> browserTaskKafkaTemplate;

    @Value("${collector.crawl.topic.command:newsinsight.crawl.commands}")
    private String crawlCommandTopic;

    @Value("${collector.crawl.topic.result:newsinsight.crawl.results}")
    private String crawlResultTopic;

    @Value("${collector.crawl.topic.browser-task:newsinsight.crawl.browser.tasks}")
    private String browserTaskTopic;

    @Value("${collector.browser-agent.callback-base-url:http://localhost:8081}")
    private String browserAgentCallbackBaseUrl;

    @Value("${collector.browser-agent.callback-token:}")
    private String browserAgentCallbackToken;

    /**
     * 특정 소스에 대한 수집 작업 시작
     */
    @Transactional
    public CollectionJob startCollection(Long sourceId) {
        Optional<DataSource> sourceOpt = dataSourceService.findById(sourceId);
        
        if (sourceOpt.isEmpty()) {
            throw new IllegalArgumentException("Data source not found: " + sourceId);
        }
        
        DataSource source = sourceOpt.get();
        
        if (!source.getIsActive()) {
            throw new IllegalStateException("Data source is not active: " + sourceId);
        }
        
        // 수집 작업 엔티티 생성
        CollectionJob job = CollectionJob.builder()
                .sourceId(sourceId)
                .status(JobStatus.PENDING)
                .itemsCollected(0)
                .build();
        
        job = collectionJobRepository.save(job);
        
        // 수집 작업을 비동기로 실행
        final Long jobId = job.getId();

        CrawlCommandMessage command = new CrawlCommandMessage(
                jobId,
                sourceId,
                source.getSourceType().name(),
                source.getUrl(),
                source.getName()
        );

        crawlCommandKafkaTemplate.send(crawlCommandTopic, jobId.toString(), command);
        
        return job;
    }

    /**
     * 여러 소스에 대한 수집 작업 시작
     */
    @Transactional
    public List<CollectionJob> startCollectionForSources(List<Long> sourceIds) {
        return sourceIds.stream()
                .map(this::startCollection)
                .toList();
    }

    /**
     * 활성 소스 전체에 대한 수집 작업 시작
     */
    @Transactional
    public List<CollectionJob> startCollectionForAllActive() {
        List<DataSource> activeSources = dataSourceService.findActiveSources();
        return activeSources.stream()
                .map(source -> startCollection(source.getId()))
                .toList();
    }

    /**
     * 실제 수집 로직 실행
     */
    @Transactional
    protected void executeCollection(Long jobId, DataSource source) {
        Optional<CollectionJob> jobOpt = collectionJobRepository.findById(jobId);
        
        if (jobOpt.isEmpty()) {
            log.error("Collection job not found: {}", jobId);
            return;
        }
        
        CollectionJob job = jobOpt.get();
        
        try {
            log.info("Starting collection job {} for source: {} ({})", 
                    jobId, source.getName(), source.getSourceType());
            
            // 작업 상태를 RUNNING으로 변경
            job.setStatus(JobStatus.RUNNING);
            job.setStartedAt(LocalDateTime.now());
            collectionJobRepository.save(job);
            
            // BROWSER_AGENT 타입인 경우 별도 처리
            if (source.getSourceType() == SourceType.BROWSER_AGENT) {
                executeBrowserAgentCollection(jobId, source, job);
                return;
            }
            
            // 소스 타입에 따라 데이터 수집
            List<CollectedData> collectedItems = collectFromSource(source);

            // 수집된 데이터 이벤트 발행
            int eventCount = 0;
            for (CollectedData data : collectedItems) {
                try {
                    String publishedAt = data.getPublishedDate() != null
                            ? data.getPublishedDate().toString()
                            : null;

                    CrawlResultMessage message = new CrawlResultMessage(
                            jobId,
                            data.getSourceId(),
                            data.getTitle(),
                            data.getContent(),
                            data.getUrl(),
                            publishedAt,
                            data.getMetadataJson()
                    );

                    crawlResultKafkaTemplate.send(crawlResultTopic, jobId.toString(), message);
                    eventCount++;
                } catch (Exception e) {
                    log.error("Error publishing crawl result event: {}", e.getMessage(), e);
                }
            }
            
            // 소스의 마지막 수집 시각 업데이트
            dataSourceService.updateLastCollected(source.getId(), LocalDateTime.now());
            
            // 작업 상태를 COMPLETED로 변경
            job.setStatus(JobStatus.COMPLETED);
            job.setCompletedAt(LocalDateTime.now());
            job.setItemsCollected(eventCount);
            collectionJobRepository.save(job);
            
            log.info("Completed collection job {} for source: {} - published {} crawl result events", 
                    jobId, source.getName(), eventCount);
            
        } catch (Exception e) {
            log.error("Error executing collection job {}: {}", jobId, e.getMessage(), e);
            
            // 작업 상태를 FAILED로 변경
            job.setStatus(JobStatus.FAILED);
            job.setCompletedAt(LocalDateTime.now());
            job.setErrorMessage(e.getMessage());
            collectionJobRepository.save(job);
        }
    }

    /**
     * BROWSER_AGENT 소스에 대한 비동기 수집 시작.
     * BrowserTaskMessage를 Kafka로 발행하고, 결과는 autonomous-crawler-service에서 
     * crawl.results 토픽으로 비동기 전송됨.
     */
    private void executeBrowserAgentCollection(Long jobId, DataSource source, CollectionJob job) {
        BrowserAgentConfig config = source.getEffectiveBrowserAgentConfig();
        
        String callbackUrl = browserAgentCallbackBaseUrl.endsWith("/") 
                ? browserAgentCallbackBaseUrl + "api/v1/browser-agent/callback"
                : browserAgentCallbackBaseUrl + "/api/v1/browser-agent/callback";

        BrowserTaskMessage task = BrowserTaskMessage.builder()
                .jobId(jobId)
                .sourceId(source.getId())
                .sourceName(source.getName())
                .seedUrl(source.getUrl())
                .maxDepth(config.getMaxDepth())
                .maxPages(config.getMaxPages())
                .budgetSeconds(config.getBudgetSeconds())
                .policy(config.getPolicy() != null ? config.getPolicy().getValue() : "focused_topic")
                .focusKeywords(config.getFocusKeywords())
                .customPrompt(config.getCustomPrompt())
                .captureScreenshots(config.getCaptureScreenshots())
                .extractStructured(config.getExtractStructured())
                .excludedDomains(config.getExcludedDomains())
                .callbackUrl(callbackUrl)
                .callbackToken(browserAgentCallbackToken)
                .createdAt(LocalDateTime.now())
                .build();

        browserTaskKafkaTemplate.send(browserTaskTopic, jobId.toString(), task);
        
        log.info("Published browser task for job {}: source={}, seedUrl={}, policy={}, maxDepth={}, maxPages={}",
                jobId, source.getName(), source.getUrl(), 
                config.getPolicy(), config.getMaxDepth(), config.getMaxPages());
        
        // Job은 RUNNING 상태로 유지 - 결과는 비동기로 들어옴
        // autonomous-crawler-service가 세션 완료 시 callback을 호출하거나,
        // 개별 결과를 crawl.results 토픽으로 발행
    }

    /**
     * 소스 타입에 따른 데이터 수집
     */
    private List<CollectedData> collectFromSource(DataSource source) {
        SourceType sourceType = source.getSourceType();
        
        return switch (sourceType) {
            case RSS -> rssFeedService.fetchRssFeed(source);
            case WEB -> webScraperService.scrapeWebPage(source);
            case WEB_SEARCH -> {
                log.warn("WEB_SEARCH 소스 타입은 UnifiedSearchService를 통해 처리해야 합니다: {}", source.getName());
                yield List.of();
            }
            case API -> {
                log.warn("API 소스 타입은 아직 미구현: {}", source.getName());
                yield List.of();
            }
            case WEBHOOK -> {
                log.warn("WEBHOOK 소스 타입은 수동 이벤트 기반으로, 능동 수집이 불가: {}", source.getName());
                yield List.of();
            }
            case BROWSER_AGENT -> {
                // BROWSER_AGENT는 executeBrowserAgentCollection에서 별도 처리
                log.warn("BROWSER_AGENT should be handled by executeBrowserAgentCollection: {}", source.getName());
                yield List.of();
            }
        };
    }

    /**
     * 수집 작업 단건 조회 (ID)
     */
    public Optional<CollectionJob> getJobById(Long jobId) {
        return collectionJobRepository.findById(jobId);
    }

    /**
     * 수집 작업 전체 조회 (페이지네이션)
     */
    public Page<CollectionJob> getAllJobs(Pageable pageable) {
        return collectionJobRepository.findAll(pageable);
    }

    /**
     * 상태별 수집 작업 조회
     */
    public Page<CollectionJob> getJobsByStatus(JobStatus status, Pageable pageable) {
        return collectionJobRepository.findByStatus(status, pageable);
    }

    /**
     * 수집 통계 조회
     */
    public CollectionStatsDTO getStatistics() {
        long totalSources = dataSourceService.countAll();
        long activeSources = dataSourceService.countActive();
        long totalItemsCollected = collectedDataService.countTotal();
        long unprocessedItems = collectedDataService.countUnprocessed();
        
        // 최근 수집 시각 계산
        LocalDateTime lastCollection = dataSourceService.findAll(Pageable.unpaged())
                .stream()
                .map(DataSource::getLastCollected)
                .filter(java.util.Objects::nonNull)
                .max(LocalDateTime::compareTo)
                .orElse(null);
        
        return new CollectionStatsDTO(
                totalSources,
                activeSources,
                totalItemsCollected,
                unprocessedItems, // Using unprocessed as proxy for today's count
                lastCollection
        );
    }

    /**
     * 실행 중인 수집 작업 취소
     */
    @Transactional
    public boolean cancelJob(Long jobId) {
        Optional<CollectionJob> jobOpt = collectionJobRepository.findById(jobId);
        
        if (jobOpt.isEmpty()) {
            return false;
        }
        
        CollectionJob job = jobOpt.get();
        
        if (job.getStatus() != JobStatus.RUNNING && job.getStatus() != JobStatus.PENDING) {
            return false;
        }
        
        job.setStatus(JobStatus.CANCELLED);
        job.setCompletedAt(LocalDateTime.now());
        collectionJobRepository.save(job);
        
        log.info("Cancelled collection job: {}", jobId);
        return true;
    }

    /**
     * 오래된 완료 작업 정리
     */
    @Transactional
    public int cleanupOldJobs(int daysOld) {
        LocalDateTime cutoffDate = LocalDateTime.now().minusDays(daysOld);
        List<CollectionJob> oldJobs = collectionJobRepository.findByStatusAndCompletedAtBefore(
                JobStatus.COMPLETED, cutoffDate);
        
        collectionJobRepository.deleteAll(oldJobs);
        log.info("Cleaned up {} old collection jobs", oldJobs.size());
        
        return oldJobs.size();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/CrawlCommandConsumerService.java

```java
package com.newsinsight.collector.service;

import com.newsinsight.collector.dto.CrawlCommandMessage;
import com.newsinsight.collector.entity.CollectionJob;
import com.newsinsight.collector.entity.CollectionJob.JobStatus;
import com.newsinsight.collector.entity.DataSource;
import com.newsinsight.collector.repository.CollectionJobRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.time.LocalDateTime;
import java.util.Optional;

/**
 * Kafka Consumer for crawl commands.
 * Validates job and source before delegating to CollectionService.
 * Failed messages will be retried and eventually sent to DLQ by KafkaConfig error handler.
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class CrawlCommandConsumerService {

    private final CollectionService collectionService;
    private final DataSourceService dataSourceService;
    private final CollectionJobRepository collectionJobRepository;

    @KafkaListener(
            topics = "${collector.crawl.topic.command:newsinsight.crawl.commands}",
            groupId = "${spring.application.name}-crawl",
            containerFactory = "crawlCommandKafkaListenerContainerFactory"
    )
    @Transactional
    public void handleCrawlCommand(CrawlCommandMessage command) {
        log.info("Processing crawl command: jobId={}, sourceId={}, sourceType={}, url={}",
                command.jobId(), command.sourceId(), command.sourceType(), command.url());

        // Validate job exists
        Optional<CollectionJob> jobOpt = collectionJobRepository.findById(command.jobId());
        if (jobOpt.isEmpty()) {
            log.error("CollectionJob not found: jobId={}, sourceId={}. Message will be sent to DLQ.",
                    command.jobId(), command.sourceId());
            throw new IllegalStateException("CollectionJob not found: " + command.jobId());
        }

        CollectionJob job = jobOpt.get();

        // Validate source exists
        Optional<DataSource> sourceOpt = dataSourceService.findById(command.sourceId());
        if (sourceOpt.isEmpty()) {
            String errorMsg = "DataSource not found: sourceId=" + command.sourceId();
            log.error("DataSource not found: jobId={}, sourceId={}. Marking job as FAILED.",
                    command.jobId(), command.sourceId());
            markJobFailed(job, errorMsg);
            return; // Don't retry - source doesn't exist
        }

        DataSource source = sourceOpt.get();

        // Validate source is active
        if (!source.getIsActive()) {
            String errorMsg = "DataSource is not active: sourceId=" + command.sourceId();
            log.warn("DataSource is inactive: jobId={}, sourceId={}. Marking job as FAILED.",
                    command.jobId(), command.sourceId());
            markJobFailed(job, errorMsg);
            return; // Don't retry - intentionally disabled
        }

        // Execute collection - exceptions here will trigger retry + DLQ
        log.info("Starting collection execution: jobId={}, source={}, type={}",
                command.jobId(), source.getName(), source.getSourceType());
        
        collectionService.executeCollection(command.jobId(), source);
        
        log.info("Completed crawl command: jobId={}, sourceId={}",
                command.jobId(), command.sourceId());
    }

    private void markJobFailed(CollectionJob job, String errorMessage) {
        job.setStatus(JobStatus.FAILED);
        job.setCompletedAt(LocalDateTime.now());
        job.setErrorMessage(errorMessage);
        collectionJobRepository.save(job);
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/CrawlResultConsumerService.java

```java
package com.newsinsight.collector.service;

import com.newsinsight.collector.dto.CrawlResultMessage;
import com.newsinsight.collector.entity.CollectedData;
import com.newsinsight.collector.service.autocrawl.AutoCrawlIntegrationService;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Service;

import java.time.LocalDateTime;
import java.time.OffsetDateTime;
import java.time.ZonedDateTime;
import java.time.format.DateTimeFormatter;
import java.time.format.DateTimeParseException;
import java.util.List;

/**
 * Kafka Consumer for crawl results.
 * Handles idempotency via content hash deduplication in CollectedDataService.
 * 
 * Integrates with AutoCrawl to:
 * - Trigger URL discovery from collected articles
 * - Update CrawlTarget status on completion
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class CrawlResultConsumerService {

    private final CollectedDataService collectedDataService;
    private final AutoCrawlIntegrationService autoCrawlIntegrationService;

    @Value("${autocrawl.enabled:true}")
    private boolean autoCrawlEnabled;

    /**
     * Supported date formats for parsing publishedAt from various sources.
     * Order matters - more specific formats should come first.
     */
    private static final List<DateTimeFormatter> DATE_FORMATTERS = List.of(
            DateTimeFormatter.ISO_OFFSET_DATE_TIME,      // 2025-11-29T01:20:00+09:00
            DateTimeFormatter.ISO_ZONED_DATE_TIME,       // 2025-11-29T01:20:00+09:00[Asia/Seoul]
            DateTimeFormatter.ISO_LOCAL_DATE_TIME,       // 2025-11-29T01:20:00
            DateTimeFormatter.RFC_1123_DATE_TIME,        // Fri, 29 Nov 2025 01:20:00 GMT
            DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"),
            DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ssXXX"),
            DateTimeFormatter.ofPattern("yyyy/MM/dd HH:mm:ss"),
            DateTimeFormatter.ofPattern("dd-MM-yyyy HH:mm:ss"),
            DateTimeFormatter.ofPattern("MMM dd, yyyy HH:mm:ss")
    );

    @KafkaListener(
            topics = "${collector.crawl.topic.result:newsinsight.crawl.results}",
            groupId = "${spring.application.name}-crawl-result",
            containerFactory = "crawlResultKafkaListenerContainerFactory"
    )
    public void handleCrawlResult(CrawlResultMessage message) {
        log.info("Processing crawl result: jobId={}, sourceId={}, url={}",
                message.jobId(), message.sourceId(), message.url());

        // Pre-compute content hash for idempotency check
        String contentHash = collectedDataService.computeContentHash(
                message.url(), message.title(), message.content());

        // Early duplicate detection - skip processing if already exists
        if (collectedDataService.isDuplicate(contentHash)) {
            log.info("Duplicate detected, skipping: jobId={}, url={}, hash={}",
                    message.jobId(), message.url(), contentHash.substring(0, 8));
            return;
        }

        // Parse published date with multiple format support
        LocalDateTime publishedDate = parsePublishedDate(message.publishedAt(), message.url());

        // Validate content
        boolean hasContent = message.content() != null && !message.content().isBlank() 
                && message.content().length() >= 50;

        CollectedData data = CollectedData.builder()
                .sourceId(message.sourceId())
                .title(message.title())
                .content(message.content())
                .url(message.url())
                .publishedDate(publishedDate)
                .metadataJson(message.metadataJson())
                .contentHash(contentHash)
                .processed(false)
                .hasContent(hasContent)
                .duplicate(false)
                .normalized(false) // Will be set true after normalization pipeline
                .build();

        CollectedData saved = collectedDataService.save(data);
        
        log.info("Saved crawl result: id={}, jobId={}, sourceId={}, url={}, hasContent={}, duplicate={}",
                saved.getId(), message.jobId(), message.sourceId(), message.url(), 
                hasContent, saved.getDuplicate());

        // Integrate with AutoCrawl: discover new URLs from article and notify completion
        if (autoCrawlEnabled) {
            // 1. Discover new URLs from the collected article's content
            autoCrawlIntegrationService.onArticleCollected(saved);
            
            // 2. Notify AutoCrawl of completion (update CrawlTarget status)
            // Note: For AutoCrawl-originated tasks, the callback is sent separately by autonomous-crawler
            // This is for crawl results that may be from other sources
            if (saved.getUrl() != null) {
                autoCrawlIntegrationService.onCrawlCompleted(saved.getUrl(), saved.getId());
            }
        }
    }

    /**
     * Parse publishedAt string with multiple format support.
     * Returns null if parsing fails for all formats.
     */
    private LocalDateTime parsePublishedDate(String publishedAt, String url) {
        if (publishedAt == null || publishedAt.isBlank()) {
            return null;
        }

        String trimmed = publishedAt.trim();

        for (DateTimeFormatter formatter : DATE_FORMATTERS) {
            try {
                // Try parsing as OffsetDateTime first (has timezone)
                if (formatter == DateTimeFormatter.ISO_OFFSET_DATE_TIME ||
                    formatter == DateTimeFormatter.RFC_1123_DATE_TIME) {
                    OffsetDateTime odt = OffsetDateTime.parse(trimmed, formatter);
                    return odt.toLocalDateTime();
                }
                
                // Try parsing as ZonedDateTime
                if (formatter == DateTimeFormatter.ISO_ZONED_DATE_TIME) {
                    ZonedDateTime zdt = ZonedDateTime.parse(trimmed, formatter);
                    return zdt.toLocalDateTime();
                }

                // Try parsing as LocalDateTime
                return LocalDateTime.parse(trimmed, formatter);
            } catch (DateTimeParseException e) {
                // Try next formatter
            }
        }

        log.warn("Failed to parse publishedAt with any known format: value='{}', url={}",
                publishedAt, url);
        return null;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/CrawlSearchService.java

```java
package com.newsinsight.collector.service;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.newsinsight.collector.client.AIDoveClient;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.MediaType;
import org.springframework.stereotype.Service;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Mono;

import java.net.URLEncoder;
import java.nio.charset.StandardCharsets;
import java.time.Duration;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;

/**
 * Service for crawling web content based on keyword search and analyzing with AI Dove.
 * This serves as a fallback when Perplexity API is not available.
 * 
 * Flow:
 * 1. Generate search URLs based on keywords (Google News, Naver News)
 * 2. Crawl URLs using Crawl4AI service
 * 3. Aggregate crawled content
 * 4. Analyze aggregated content with AI Dove
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class CrawlSearchService {

    private final WebClient webClient;
    private final ObjectMapper objectMapper;
    private final AIDoveClient aiDoveClient;

    @Value("${collector.crawler.base-url:http://web-crawler:11235}")
    private String crawlerBaseUrl;

    @Value("${collector.crawler.timeout-seconds:60}")
    private int crawlerTimeoutSeconds;

    @Value("${collector.crawler.max-urls:20}")
    private int maxUrls;

    /**
     * Result of a crawl operation
     */
    public record CrawlResult(
            String url,
            String title,
            String content,
            boolean success,
            String error
    ) {
        public static CrawlResult success(String url, String title, String content) {
            return new CrawlResult(url, title, content, true, null);
        }

        public static CrawlResult failure(String url, String error) {
            return new CrawlResult(url, null, null, false, error);
        }
    }

    /**
     * Search and analyze news for a given keyword.
     * Returns a streaming response of the analysis.
     *
     * @param keyword The search keyword
     * @param window Time window (1d, 7d, 30d)
     * @return Flux of analysis text chunks
     */
    public Flux<String> searchAndAnalyze(String keyword, String window) {
        if (!aiDoveClient.isEnabled()) {
            return Flux.just("AI Dove 서비스가 비활성화되어 있습니다. 관리자에게 문의하세요.");
        }

        return Flux.concat(
                Flux.just("🔍 '" + keyword + "' 관련 뉴스를 검색하고 있습니다...\n\n"),
                performSearchAndAnalysis(keyword, window)
        );
    }

    private Flux<String> performSearchAndAnalysis(String keyword, String window) {
        List<String> searchUrls = generateSearchUrls(keyword, window);

        return Flux.fromIterable(searchUrls)
                .flatMap(this::crawlUrl, 3) // Parallel crawling with concurrency 3
                .collectList()
                .flatMapMany(results -> {
                    List<CrawlResult> successfulResults = results.stream()
                            .filter(CrawlResult::success)
                            .toList();

                    if (successfulResults.isEmpty()) {
                        return Flux.just(
                                "⚠️ 뉴스 크롤링에 실패했습니다.\n\n" +
                                "검색된 URL에서 콘텐츠를 가져올 수 없습니다.\n" +
                                "잠시 후 다시 시도하거나, Browser AI Agent를 사용해 보세요."
                        );
                    }

                    String aggregatedContent = aggregateContent(successfulResults, keyword, window);

                    return Flux.concat(
                            Flux.just("📰 " + successfulResults.size() + "개의 뉴스 소스를 분석 중...\n\n"),
                            analyzeWithAIDove(aggregatedContent, keyword)
                    );
                })
                .onErrorResume(e -> {
                    log.error("Search and analyze failed for keyword '{}': {}", keyword, e.getMessage());
                    return Flux.just(
                            "❌ 분석 중 오류가 발생했습니다: " + e.getMessage() + "\n\n" +
                            "Browser AI Agent 또는 Deep AI Search를 사용해 보세요."
                    );
                });
    }

    /**
     * Generate search URLs for the given keyword.
     */
    private List<String> generateSearchUrls(String keyword, String window) {
        List<String> urls = new ArrayList<>();
        String encodedKeyword = URLEncoder.encode(keyword, StandardCharsets.UTF_8);

        // Google News search
        String googleNewsUrl = "https://news.google.com/search?q=" + encodedKeyword + "&hl=ko&gl=KR&ceid=KR:ko";
        urls.add(googleNewsUrl);

        // Naver News search
        String naverNewsUrl = "https://search.naver.com/search.naver?where=news&query=" + encodedKeyword;
        urls.add(naverNewsUrl);

        // Daum News search
        String daumNewsUrl = "https://search.daum.net/search?w=news&q=" + encodedKeyword;
        urls.add(daumNewsUrl);

        // Add time-specific search if needed
        if ("1d".equals(window)) {
            // Google News sorted by date (last 24 hours)
            String recentGoogleUrl = "https://www.google.com/search?q=" + encodedKeyword + 
                    "+site:news.google.com&tbs=qdr:d&tbm=nws";
            urls.add(recentGoogleUrl);
        } else if ("30d".equals(window)) {
            // Google News last month
            String monthlyGoogleUrl = "https://www.google.com/search?q=" + encodedKeyword + 
                    "+site:news.google.com&tbs=qdr:m&tbm=nws";
            urls.add(monthlyGoogleUrl);
        }

        return urls.stream().limit(maxUrls).toList();
    }

    /**
     * Crawl a single URL using Crawl4AI.
     */
    private Mono<CrawlResult> crawlUrl(String url) {
        log.debug("Crawling URL: {}", url);

        String crawlEndpoint = crawlerBaseUrl.endsWith("/") 
                ? crawlerBaseUrl + "md" 
                : crawlerBaseUrl + "/md";

        Map<String, Object> payload = Map.of(
                "url", url,
                "bypass_cache", true,
                "word_count_threshold", 50
        );

        return webClient.post()
                .uri(crawlEndpoint)
                .contentType(MediaType.APPLICATION_JSON)
                .bodyValue(payload)
                .retrieve()
                .bodyToMono(String.class)
                .timeout(Duration.ofSeconds(crawlerTimeoutSeconds))
                .map(response -> parseCrawlResponse(url, response))
                .onErrorResume(e -> {
                    log.warn("Failed to crawl {}: {}", url, e.getMessage());
                    return Mono.just(CrawlResult.failure(url, e.getMessage()));
                });
    }

    private CrawlResult parseCrawlResponse(String url, String response) {
        try {
            JsonNode node = objectMapper.readTree(response);

            // Try to extract markdown or content
            String content = null;
            String title = null;

            if (node.has("result")) {
                JsonNode result = node.get("result");
                if (result.has("markdown")) {
                    content = result.get("markdown").asText();
                }
                if (result.has("metadata") && result.get("metadata").has("title")) {
                    title = result.get("metadata").get("title").asText();
                }
            } else if (node.has("markdown")) {
                content = node.get("markdown").asText();
            } else if (node.has("content")) {
                content = node.get("content").asText();
            } else {
                // Fallback: use raw response if it looks like text
                content = response;
            }

            if (content == null || content.isBlank()) {
                return CrawlResult.failure(url, "No content extracted");
            }

            // Truncate very long content
            if (content.length() > 10000) {
                content = content.substring(0, 10000) + "...[truncated]";
            }

            return CrawlResult.success(url, title, content);
        } catch (Exception e) {
            log.warn("Failed to parse crawl response for {}: {}", url, e.getMessage());
            return CrawlResult.failure(url, "Failed to parse response: " + e.getMessage());
        }
    }

    /**
     * Aggregate crawled content into a single prompt for AI analysis.
     */
    private String aggregateContent(List<CrawlResult> results, String keyword, String window) {
        StringBuilder sb = new StringBuilder();
        sb.append("다음은 '").append(keyword).append("' 키워드로 검색한 뉴스 콘텐츠입니다:\n\n");

        int index = 1;
        for (CrawlResult result : results) {
            sb.append("--- 뉴스 소스 ").append(index++).append(" ---\n");
            if (result.title() != null) {
                sb.append("제목: ").append(result.title()).append("\n");
            }
            sb.append("URL: ").append(result.url()).append("\n");
            sb.append("내용:\n").append(result.content()).append("\n\n");
        }

        return sb.toString();
    }

    /**
     * Analyze aggregated content with AI Dove.
     */
    private Flux<String> analyzeWithAIDove(String aggregatedContent, String keyword) {
        String analysisPrompt = buildAnalysisPrompt(aggregatedContent, keyword);

        return aiDoveClient.chatStream(analysisPrompt, null)
                .onErrorResume(e -> {
                    log.error("AI Dove analysis failed: {}", e.getMessage());
                    return Flux.just("AI 분석 중 오류가 발생했습니다: " + e.getMessage());
                });
    }

    private String buildAnalysisPrompt(String aggregatedContent, String keyword) {
        return """
                당신은 뉴스 분석 전문가입니다. 아래 크롤링된 뉴스 콘텐츠를 분석하고 다음을 제공해 주세요:
                
                1. **핵심 요약**: '%s' 관련 주요 뉴스 흐름을 2-3문장으로 요약
                2. **주요 이슈**: bullet point로 3-5개의 핵심 이슈 정리
                3. **시장/산업 영향**: 관련 분야에 미치는 영향 분석
                4. **향후 전망**: 향후 예상되는 발전 방향
                5. **종합 의견**: 전체적인 분석 의견을 한 문단으로 정리
                
                반드시 한국어로 답변해 주세요.
                
                ---
                
                %s
                """.formatted(keyword, aggregatedContent);
    }

    /**
     * Check if the service is available.
     */
    public boolean isAvailable() {
        return aiDoveClient.isEnabled();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/DashboardEventService.java

```java
package com.newsinsight.collector.service;

import com.newsinsight.collector.dto.DashboardEventDto;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Mono;
import reactor.core.publisher.Sinks;

import java.util.HashMap;
import java.util.Map;
import java.util.concurrent.atomic.AtomicLong;

/**
 * 대시보드 실시간 이벤트 서비스.
 * SSE를 통해 클라이언트에 이벤트를 푸시하는 기능을 제공합니다.
 */
@Service
@Slf4j
public class DashboardEventService {

    private final Sinks.Many<DashboardEventDto> eventSink;
    
    // 간단한 통계 카운터 (실제 환경에서는 DB나 Redis에서 조회)
    private final AtomicLong totalCollected = new AtomicLong(0);
    private final AtomicLong activeSourceCount = new AtomicLong(0);
    private final AtomicLong todayCollected = new AtomicLong(0);

    public DashboardEventService() {
        this.eventSink = Sinks.many().multicast().onBackpressureBuffer();
    }

    /**
     * 이벤트 스트림을 구독합니다.
     * 
     * @return 이벤트 Flux
     */
    public Flux<DashboardEventDto> getEventStream() {
        return eventSink.asFlux()
                .doOnSubscribe(sub -> log.debug("New subscriber connected to event stream"))
                .doOnCancel(() -> log.debug("Subscriber disconnected from event stream"));
    }

    /**
     * 이벤트를 발행합니다.
     * 
     * @param event 발행할 이벤트
     */
    public void publishEvent(DashboardEventDto event) {
        log.debug("Publishing event: {}", event.getEventType());
        eventSink.tryEmitNext(event);
    }

    /**
     * 새 데이터 수집 이벤트를 발행합니다.
     * 
     * @param sourceId 소스 ID
     * @param count 수집된 항목 수
     */
    public void notifyNewData(String sourceId, int count) {
        totalCollected.addAndGet(count);
        todayCollected.addAndGet(count);
        
        Map<String, Object> data = new HashMap<>();
        data.put("sourceId", sourceId);
        data.put("count", count);
        data.put("totalCollected", totalCollected.get());
        
        publishEvent(DashboardEventDto.newData(
                "Collected " + count + " items from " + sourceId, 
                data
        ));
    }

    /**
     * 소스 상태 변경 이벤트를 발행합니다.
     * 
     * @param sourceId 소스 ID
     * @param status 새 상태
     */
    public void notifySourceUpdated(String sourceId, String status) {
        publishEvent(DashboardEventDto.sourceUpdated(sourceId, status));
    }

    /**
     * 현재 통계를 조회합니다.
     * 
     * @return 통계 이벤트 Mono
     */
    public Mono<DashboardEventDto> getCurrentStats() {
        Map<String, Object> stats = new HashMap<>();
        stats.put("totalCollected", totalCollected.get());
        stats.put("todayCollected", todayCollected.get());
        stats.put("activeSourceCount", activeSourceCount.get());
        stats.put("timestamp", System.currentTimeMillis());
        
        return Mono.just(DashboardEventDto.statsUpdated(stats));
    }

    /**
     * 활성 소스 수를 업데이트합니다.
     * 
     * @param count 활성 소스 수
     */
    public void updateActiveSourceCount(long count) {
        activeSourceCount.set(count);
    }

    /**
     * 일일 통계를 리셋합니다. (스케줄러에서 호출)
     */
    public void resetDailyStats() {
        todayCollected.set(0);
        log.info("Daily stats reset");
    }

    /**
     * 에러 이벤트를 발행합니다.
     * 
     * @param message 에러 메시지
     */
    public void notifyError(String message) {
        publishEvent(DashboardEventDto.error(message));
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/DataSourceService.java

```java
package com.newsinsight.collector.service;

import com.newsinsight.collector.dto.*;
import com.newsinsight.collector.entity.DataSource;
import com.newsinsight.collector.entity.SourceType;
import com.newsinsight.collector.mapper.EntityMapper;
import com.newsinsight.collector.repository.DataSourceRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Optional;
import java.util.stream.Collectors;

@Slf4j
@Service
@RequiredArgsConstructor
public class DataSourceService {

    private final DataSourceRepository dataSourceRepository;
    private final EntityMapper entityMapper;

    /**
     * 모든 데이터 소스 목록 조회
     */
    @Transactional(readOnly = true)
    public List<DataSourceDTO> getAllSources() {
        return dataSourceRepository.findAll().stream()
                .map(entityMapper::toDTO)
                .collect(Collectors.toList());
    }

    /**
     * 활성화된 데이터 소스 목록 조회
     */
    @Transactional(readOnly = true)
    public List<DataSourceDTO> getActiveSources() {
        return dataSourceRepository.findByIsActiveTrue().stream()
                .map(entityMapper::toDTO)
                .collect(Collectors.toList());
    }

    /**
     * 소스 타입별 데이터 소스 조회
     */
    @Transactional(readOnly = true)
    public List<DataSourceDTO> getSourcesByType(SourceType sourceType) {
        return dataSourceRepository.findBySourceType(sourceType).stream()
                .map(entityMapper::toDTO)
                .collect(Collectors.toList());
    }

    /**
     * 데이터 소스 단건 조회 (ID)
     */
    @Transactional(readOnly = true)
    public DataSourceDTO getSource(Long id) {
        return dataSourceRepository.findById(id)
                .map(entityMapper::toDTO)
                .orElse(null);
    }

    // findById의 Optional<DataSource> 반환 버전
    @Transactional(readOnly = true)
    public Optional<DataSource> findById(Long id) {
        return dataSourceRepository.findById(id);
    }

    /**
     * 데이터 소스 생성 (DTO 요청 기반)
     */
    @Transactional
    public DataSourceDTO createSource(DataSourceCreateRequest request) {
        DataSource source = entityMapper.toEntity(request);
        DataSource saved = dataSourceRepository.save(source);
        log.info("Created data source: id={}, name={}, type={}", 
                 saved.getId(), saved.getName(), saved.getSourceType());
        return entityMapper.toDTO(saved);
    }

    // 엔티티 직접 저장/반환 버전
    @Transactional
    public DataSource create(DataSource source) {
        DataSource saved = dataSourceRepository.save(source);
        log.info("Created data source: id={}, name={}, type={}", 
                 saved.getId(), saved.getName(), saved.getSourceType());
        return saved;
    }

    /**
     * 데이터 소스 수정 (DTO 요청 기반)
     */
    @Transactional
    public DataSourceDTO updateSource(Long id, DataSourceUpdateRequest request) {
        DataSource source = dataSourceRepository.findById(id)
                .orElseThrow(() -> new IllegalArgumentException("Data source not found: " + id));
        
        entityMapper.updateEntity(source, request);
        DataSource saved = dataSourceRepository.save(source);
        log.info("Updated data source: id={}, name={}", saved.getId(), saved.getName());
        return entityMapper.toDTO(saved);
    }

    // 엔티티 직접 수정/반환 버전
    @Transactional
    public DataSource update(Long id, DataSourceUpdateRequest request) {
        DataSource source = dataSourceRepository.findById(id)
                .orElseThrow(() -> new IllegalArgumentException("Data source not found: " + id));
        
        entityMapper.updateEntity(source, request);
        DataSource saved = dataSourceRepository.save(source);
        log.info("Updated data source: id={}, name={}", saved.getId(), saved.getName());
        return saved;
    }

    /**
     * 데이터 소스 삭제 (예외 발생)
     */
    @Transactional
    public void deleteSource(Long id) {
        if (!dataSourceRepository.existsById(id)) {
            throw new IllegalArgumentException("Data source not found: " + id);
        }
        dataSourceRepository.deleteById(id);
        log.info("Deleted data source: id={}", id);
    }

    // 삭제 결과를 boolean으로 반환하는 버전
    @Transactional
    public boolean delete(Long id) {
        if (!dataSourceRepository.existsById(id)) {
            return false;
        }
        dataSourceRepository.deleteById(id);
        log.info("Deleted data source: id={}", id);
        return true;
    }

    /**
     * 마지막 수집 시각 업데이트
     */
    @Transactional
    public void updateLastCollected(Long id, LocalDateTime timestamp) {
        DataSource source = dataSourceRepository.findById(id)
                .orElseThrow(() -> new IllegalArgumentException("Data source not found: " + id));
        source.setLastCollected(timestamp);
        dataSourceRepository.save(source);
    }

    /**
     * 수집 대상(기한 도래) 소스 조회
     */
    @Transactional(readOnly = true)
    public List<DataSource> findDueForCollection() {
        LocalDateTime threshold = LocalDateTime.now().minusSeconds(3600); // Default 1 hour
        return dataSourceRepository.findDueForCollection(threshold);
    }

    /**
     * 활성화된 소스 목록 조회
     */
    @Transactional(readOnly = true)
    public List<DataSource> findActiveSources() {
        return dataSourceRepository.findByIsActiveTrue();
    }

    // 페이징 지원 메서드
    /**
     * 모든 소스 페이징 조회
     */
    @Transactional(readOnly = true)
    public Page<DataSource> findAll(Pageable pageable) {
        return dataSourceRepository.findAll(pageable);
    }

    /**
     * 활성 소스 페이징 조회 (주의: null 포함 가능)
     */
    @Transactional(readOnly = true)
    public Page<DataSource> findAllActive(Pageable pageable) {
        return dataSourceRepository.findAll(pageable)
                .map(source -> source.getIsActive() ? source : null);
    }

    /**
     * 전체 소스 개수 조회
     */
    @Transactional(readOnly = true)
    public long countAll() {
        return dataSourceRepository.count();
    }

    /**
     * 활성 소스 개수 조회
     */
    @Transactional(readOnly = true)
    public long countActive() {
        return dataSourceRepository.findByIsActiveTrue().size();
    }

    // 엔티티 직접 저장/업데이트
    /**
     * 데이터 소스 저장/업데이트 (엔티티 직접 전달)
     */
    @Transactional
    public DataSource save(DataSource source) {
        return dataSourceRepository.save(source);
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/DeepAnalysisService.java

```java
package com.newsinsight.collector.service;

import com.newsinsight.collector.dto.DeepSearchJobDto;
import com.newsinsight.collector.dto.DeepSearchResultDto;
import com.newsinsight.collector.dto.EvidenceDto;
import com.newsinsight.collector.dto.StanceDistributionDto;
import com.newsinsight.collector.entity.CrawlEvidence;
import com.newsinsight.collector.entity.CrawlFailureReason;
import com.newsinsight.collector.entity.CrawlJob;
import com.newsinsight.collector.entity.CrawlJobStatus;
import com.newsinsight.collector.entity.EvidenceStance;
import com.newsinsight.collector.repository.CrawlEvidenceRepository;
import com.newsinsight.collector.repository.CrawlJobRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.ApplicationContext;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.PageRequest;
import org.springframework.data.domain.Pageable;
import org.springframework.data.domain.Sort;
import org.springframework.scheduling.annotation.Async;
import org.springframework.scheduling.annotation.Scheduled;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Map;
import java.util.UUID;
import java.util.stream.Collectors;

/**
 * Service for managing deep AI search operations.
 * Handles job creation, progress tracking, and result retrieval.
 * Publishes SSE events via DeepSearchEventService for real-time updates.
 * 
 * Uses IntegratedCrawlerService with multiple strategies:
 * - Crawl4AI for JS-rendered pages
 * - Browser-Use API for complex interactions
 * - Direct HTTP for simple pages
 * - Search Engines (Google, Naver, Daum) for topic-based searches
 * 
 * Results are analyzed using AIDove for evidence extraction and stance analysis.
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class DeepAnalysisService {

    private final CrawlJobRepository crawlJobRepository;
    private final CrawlEvidenceRepository crawlEvidenceRepository;
    private final DeepSearchEventService deepSearchEventService;
    private final IntegratedCrawlerService integratedCrawlerService;
    private final ApplicationContext applicationContext;

    @Value("${collector.deep-search.timeout-minutes:30}")
    private int timeoutMinutes;

    @Value("${collector.deep-search.cleanup-days:7}")
    private int cleanupDays;

    @Value("${collector.deep-search.callback-token:}")
    private String expectedCallbackToken;

    /**
     * Start a new deep search job
     */
    @Transactional
    public DeepSearchJobDto startDeepSearch(String topic, String baseUrl) {
        // Check if integrated crawler is available
        if (!integratedCrawlerService.isAvailable()) {
            throw new IllegalStateException(
                "Deep search is not available. IntegratedCrawlerService is not ready. " +
                "Please ensure at least one of the following is configured: " +
                "Crawl4AI, Browser-Use API, or AIDove."
            );
        }

        String jobId = generateJobId();

        // Create job record
        CrawlJob job = CrawlJob.builder()
                .id(jobId)
                .topic(topic)
                .baseUrl(baseUrl)
                .status(CrawlJobStatus.PENDING)
                .build();

        crawlJobRepository.save(job);
        log.info("Created deep search job: id={}, topic={}", jobId, topic);

        // Publish initial status via SSE
        deepSearchEventService.publishStatusUpdate(jobId, "PENDING", "Job created, starting search...");

        // Start integrated crawler
        applicationContext.getBean(DeepAnalysisService.class)
                .triggerIntegratedSearchAsync(jobId, topic, baseUrl);

        return toJobDto(job);
    }

    /**
     * Async method to trigger search using IntegratedCrawlerService.
     * 
     * IMPORTANT: This method uses reactive subscribe() instead of block() to ensure
     * truly non-blocking execution. The @Async annotation ensures this runs in a
     * separate thread pool, and subscribe() ensures we don't block that thread.
     */
    @Async
    public void triggerIntegratedSearchAsync(String jobId, String topic, String baseUrl) {
        log.info("Starting integrated crawl: jobId={}, topic={}", jobId, topic);
        
        // Publish progress update immediately
        deepSearchEventService.publishProgressUpdate(jobId, 10, "Starting integrated crawler...");
        updateJobStatus(jobId, CrawlJobStatus.IN_PROGRESS);
        deepSearchEventService.publishStatusUpdate(jobId, "IN_PROGRESS", "Integrated crawl in progress...");

        // Build crawl request
        IntegratedCrawlerService.CrawlRequest request;
        if (baseUrl != null && !baseUrl.isBlank()) {
            request = IntegratedCrawlerService.CrawlRequest.forUrl(topic, baseUrl);
        } else {
            request = IntegratedCrawlerService.CrawlRequest.forTopic(topic);
        }

        // Create progress callback
        IntegratedCrawlerService.CrawlProgressCallback callback = new IntegratedCrawlerService.CrawlProgressCallback() {
            @Override
            public void onProgress(int current, int total, String message) {
                // Scale progress from 10-90%
                int scaledProgress = 10 + (int)((current / (double) Math.max(total, 1)) * 80);
                deepSearchEventService.publishProgressUpdate(jobId, scaledProgress, message);
            }

            @Override
            public void onPageCrawled(com.newsinsight.collector.dto.CrawledPage page) {
                log.debug("Page crawled for job {}: {}", jobId, page.url());
            }

            @Override
            public void onEvidenceFound(EvidenceDto evidence) {
                deepSearchEventService.publishEvidence(jobId, evidence);
            }

            @Override
            public void onError(String url, String error) {
                log.warn("Crawl error for job {} at {}: {}", jobId, url, error);
            }
        };

        // Execute crawl reactively - DO NOT use .block() as it defeats async execution!
        // Use subscribe() to process results when they arrive without blocking.
        integratedCrawlerService
                .crawl(request, callback)
                .subscribe(
                        result -> handleCrawlSuccess(jobId, result),
                        error -> handleCrawlError(jobId, error)
                );
    }

    /**
     * Handle successful crawl completion
     */
    private void handleCrawlSuccess(String jobId, IntegratedCrawlerService.CrawlResult result) {
        try {
            if (result != null && !result.evidence().isEmpty()) {
                // Save evidence to database
                List<CrawlEvidence> evidenceEntities = result.evidence().stream()
                        .map(e -> CrawlEvidence.builder()
                                .jobId(jobId)
                                .url(e.getUrl())
                                .title(e.getTitle())
                                .stance(parseStance(e.getStance()))
                                .snippet(e.getSnippet())
                                .source(e.getSource())
                                .build())
                        .toList();
                crawlEvidenceRepository.saveAll(evidenceEntities);

                // Update job as completed
                CrawlJob job = crawlJobRepository.findById(jobId).orElse(null);
                if (job != null) {
                    job.markCompleted(evidenceEntities.size());
                    crawlJobRepository.save(job);
                }

                // Publish completion
                deepSearchEventService.publishProgressUpdate(jobId, 100, "Completed");
                deepSearchEventService.publishComplete(jobId, toJobDto(job));
                
                log.info("Integrated crawl completed: jobId={}, evidence={}", jobId, evidenceEntities.size());
            } else {
                // Mark as completed with no evidence
                CrawlJob job = crawlJobRepository.findById(jobId).orElse(null);
                if (job != null) {
                    job.markCompleted(0);
                    crawlJobRepository.save(job);
                }
                deepSearchEventService.publishProgressUpdate(jobId, 100, "Completed (no evidence found)");
                deepSearchEventService.publishComplete(jobId, toJobDto(job));
                log.info("Integrated crawl completed with no evidence: jobId={}", jobId);
            }
        } catch (Exception e) {
            log.error("Error handling crawl success for jobId={}: {}", jobId, e.getMessage(), e);
            handleCrawlError(jobId, e);
        }
    }

    /**
     * Handle crawl error
     */
    private void handleCrawlError(String jobId, Throwable e) {
        log.error("Integrated crawl failed: jobId={}, error={}", jobId, e.getMessage(), e);
        CrawlFailureReason failureReason = CrawlFailureReason.fromException(e);
        String errorMsg = "Crawl failed: " + e.getMessage() + " [" + failureReason.getCode() + "]";
        updateJobStatusWithReason(jobId, CrawlJobStatus.FAILED, errorMsg, failureReason);
        deepSearchEventService.publishError(jobId, errorMsg, failureReason);
    }

    /**
     * Parse stance string to EvidenceStance enum
     */
    private EvidenceStance parseStance(String stance) {
        if (stance == null) return EvidenceStance.NEUTRAL;
        return switch (stance.toLowerCase()) {
            case "pro" -> EvidenceStance.PRO;
            case "con" -> EvidenceStance.CON;
            default -> EvidenceStance.NEUTRAL;
        };
    }

    /**
     * Process callback from internal workers (for extensibility)
     * This endpoint can be used by future internal async workers if needed.
     */
    @Transactional
    public DeepSearchResultDto processInternalCallback(
            String callbackToken,
            String jobId,
            String status,
            List<EvidenceDto> evidenceList
    ) {
        // Validate callback token if configured
        if (expectedCallbackToken != null && !expectedCallbackToken.isBlank()) {
            if (!expectedCallbackToken.equals(callbackToken)) {
                log.warn("Invalid callback token received for job: {}", jobId);
                throw new SecurityException("Invalid callback token");
            }
        }

        CrawlJob job = crawlJobRepository.findById(jobId)
                .orElseThrow(() -> new IllegalArgumentException("Job not found: " + jobId));

        // Check if already processed
        if (job.getCallbackReceived()) {
            log.warn("Duplicate callback received for job: {}", jobId);
            return getSearchResult(jobId);
        }

        // Publish progress update
        deepSearchEventService.publishProgressUpdate(jobId, 70, "Processing callback, saving evidence...");

        // Process evidence
        List<CrawlEvidence> savedEvidence = List.of();
        if (evidenceList != null && !evidenceList.isEmpty()) {
            savedEvidence = evidenceList.stream()
                    .map(e -> CrawlEvidence.builder()
                            .jobId(jobId)
                            .url(e.getUrl())
                            .title(e.getTitle())
                            .stance(parseStance(e.getStance()))
                            .snippet(e.getSnippet())
                            .source(e.getSource())
                            .build())
                    .toList();
            crawlEvidenceRepository.saveAll(savedEvidence);
            
            // Publish each evidence via SSE
            int evidenceCount = 0;
            for (EvidenceDto evidence : evidenceList) {
                evidenceCount++;
                deepSearchEventService.publishEvidence(jobId, evidence);
                
                // Update progress as evidence is processed
                int progress = 70 + (int) ((evidenceCount / (double) evidenceList.size()) * 25);
                deepSearchEventService.publishProgressUpdate(jobId, progress, 
                        String.format("Processing evidence %d/%d", evidenceCount, evidenceList.size()));
            }
        }

        // Update job status
        if ("completed".equalsIgnoreCase(status)) {
            job.markCompleted(savedEvidence.size());
            // Publish completion event
            deepSearchEventService.publishProgressUpdate(jobId, 100, "Completed");
            deepSearchEventService.publishComplete(jobId, toJobDto(job));
        } else {
            job.markFailed("Worker returned status: " + status);
            // Publish error event
            deepSearchEventService.publishError(jobId, "Worker returned status: " + status);
        }
        crawlJobRepository.save(job);

        log.info("Processed internal callback for job: id={}, evidenceCount={}, status={}", 
                jobId, savedEvidence.size(), job.getStatus());

        return getSearchResult(jobId);
    }

    /**
     * Get job status
     */
    @Transactional(readOnly = true)
    public DeepSearchJobDto getJobStatus(String jobId) {
        CrawlJob job = crawlJobRepository.findById(jobId)
                .orElseThrow(() -> new IllegalArgumentException("Job not found: " + jobId));
        return toJobDto(job);
    }

    /**
     * Get full search result including evidence
     */
    @Transactional(readOnly = true)
    public DeepSearchResultDto getSearchResult(String jobId) {
        CrawlJob job = crawlJobRepository.findById(jobId)
                .orElseThrow(() -> new IllegalArgumentException("Job not found: " + jobId));

        List<CrawlEvidence> evidenceList = crawlEvidenceRepository.findByJobId(jobId);
        List<EvidenceDto> evidenceDtos = evidenceList.stream()
                .map(this::toEvidenceDto)
                .collect(Collectors.toList());

        // Calculate stance distribution
        StanceDistributionDto stanceDistribution = calculateStanceDistribution(jobId);

        return DeepSearchResultDto.builder()
                .jobId(job.getId())
                .topic(job.getTopic())
                .baseUrl(job.getBaseUrl())
                .status(job.getStatus().name())
                .evidenceCount(evidenceList.size())
                .evidence(evidenceDtos)
                .stanceDistribution(stanceDistribution)
                .createdAt(job.getCreatedAt())
                .completedAt(job.getCompletedAt())
                .errorMessage(job.getErrorMessage())
                .failureReason(job.getFailureReason() != null ? job.getFailureReason().getCode() : null)
                .failureCategory(categorizeFailureReason(job.getFailureReason()))
                .build();
    }

    /**
     * List recent jobs
     */
    @Transactional(readOnly = true)
    public Page<DeepSearchJobDto> listJobs(int page, int size, CrawlJobStatus status) {
        Pageable pageable = PageRequest.of(page, size, Sort.by(Sort.Direction.DESC, "createdAt"));

        Page<CrawlJob> jobs;
        if (status != null) {
            jobs = crawlJobRepository.findByStatus(status, pageable);
        } else {
            jobs = crawlJobRepository.findAll(pageable);
        }

        return jobs.map(this::toJobDto);
    }

    /**
     * Cancel a pending or in-progress job
     */
    @Transactional
    public DeepSearchJobDto cancelJob(String jobId) {
        CrawlJob job = crawlJobRepository.findById(jobId)
                .orElseThrow(() -> new IllegalArgumentException("Job not found: " + jobId));

        if (job.getStatus() == CrawlJobStatus.PENDING || job.getStatus() == CrawlJobStatus.IN_PROGRESS) {
            job.setStatus(CrawlJobStatus.CANCELLED);
            job.setCompletedAt(LocalDateTime.now());
            job.setFailureReason(CrawlFailureReason.JOB_CANCELLED);
            job.setErrorMessage("Job was cancelled by user");
            crawlJobRepository.save(job);
            log.info("Cancelled job: {}", jobId);
            
            // Publish cancellation via SSE
            deepSearchEventService.publishStatusUpdate(jobId, "CANCELLED", "Job was cancelled by user");
            deepSearchEventService.publishComplete(jobId, toJobDto(job));
        }

        return toJobDto(job);
    }

    /**
     * Scheduled task to timeout old pending jobs
     */
    @Scheduled(fixedDelayString = "${collector.deep-search.timeout-check-interval:300000}")
    @Transactional
    public void timeoutOldJobs() {
        LocalDateTime cutoff = LocalDateTime.now().minusMinutes(timeoutMinutes);
        
        // Find jobs that will be timed out before marking them
        List<CrawlJob> jobsToTimeout = crawlJobRepository.findByStatusInAndCreatedAtBefore(
                List.of(CrawlJobStatus.PENDING, CrawlJobStatus.IN_PROGRESS),
                cutoff
        );
        
        if (!jobsToTimeout.isEmpty()) {
            // Update each job with proper failure reason
            for (CrawlJob job : jobsToTimeout) {
                job.markTimedOut(CrawlFailureReason.TIMEOUT_JOB_OVERALL);
                crawlJobRepository.save(job);
                
                String errorMsg = "Job timed out after " + timeoutMinutes + " minutes [" + 
                        CrawlFailureReason.TIMEOUT_JOB_OVERALL.getCode() + "]";
                deepSearchEventService.publishError(job.getId(), errorMsg, CrawlFailureReason.TIMEOUT_JOB_OVERALL);
            }
            log.info("Marked {} jobs as timed out with reason: {}", jobsToTimeout.size(), 
                    CrawlFailureReason.TIMEOUT_JOB_OVERALL.getCode());
        }
    }

    /**
     * Scheduled task to cleanup old jobs
     */
    @Scheduled(cron = "${collector.deep-search.cleanup-cron:0 0 3 * * ?}")
    @Transactional
    public void cleanupOldJobs() {
        LocalDateTime cutoff = LocalDateTime.now().minusDays(cleanupDays);
        
        // Get job IDs to delete
        List<CrawlJob> oldJobs = crawlJobRepository.findByStatusInAndCreatedAtBefore(
                List.of(CrawlJobStatus.COMPLETED, CrawlJobStatus.FAILED, 
                        CrawlJobStatus.TIMEOUT, CrawlJobStatus.CANCELLED),
                cutoff
        );

        if (!oldJobs.isEmpty()) {
            List<String> jobIds = oldJobs.stream().map(CrawlJob::getId).collect(Collectors.toList());
            
            // Delete evidence first
            int evidenceDeleted = crawlEvidenceRepository.deleteByJobIdIn(jobIds);
            
            // Delete jobs
            int jobsDeleted = crawlJobRepository.deleteOldJobs(cutoff);
            
            log.info("Cleanup completed: {} jobs deleted, {} evidence records deleted", 
                    jobsDeleted, evidenceDeleted);
        }
    }

    // Helper methods

    private String generateJobId() {
        return "crawl_" + UUID.randomUUID().toString().replace("-", "").substring(0, 16);
    }

    private void updateJobStatus(String jobId, CrawlJobStatus status) {
        updateJobStatus(jobId, status, null);
    }

    private void updateJobStatus(String jobId, CrawlJobStatus status, String errorMessage) {
        updateJobStatusWithReason(jobId, status, errorMessage, null);
    }

    private void updateJobStatusWithReason(String jobId, CrawlJobStatus status, String errorMessage, CrawlFailureReason failureReason) {
        crawlJobRepository.findById(jobId).ifPresent(job -> {
            job.setStatus(status);
            if (errorMessage != null) {
                job.setErrorMessage(errorMessage);
            }
            if (failureReason != null) {
                job.setFailureReason(failureReason);
            } else if (errorMessage != null && status == CrawlJobStatus.FAILED) {
                // Auto-detect failure reason from error message
                job.setFailureReason(CrawlFailureReason.fromErrorMessage(errorMessage));
            }
            if (status == CrawlJobStatus.FAILED || status == CrawlJobStatus.TIMEOUT) {
                job.setCompletedAt(LocalDateTime.now());
            }
            crawlJobRepository.save(job);
        });
    }

    private StanceDistributionDto calculateStanceDistribution(String jobId) {
        List<Object[]> distribution = crawlEvidenceRepository.getStanceDistribution(jobId);
        
        Map<EvidenceStance, Long> counts = distribution.stream()
                .collect(Collectors.toMap(
                        arr -> (EvidenceStance) arr[0],
                        arr -> (Long) arr[1]
                ));

        long total = counts.values().stream().mapToLong(Long::longValue).sum();
        if (total == 0) total = 1; // Avoid division by zero

        long finalTotal = total;
        return StanceDistributionDto.builder()
                .pro(counts.getOrDefault(EvidenceStance.PRO, 0L))
                .con(counts.getOrDefault(EvidenceStance.CON, 0L))
                .neutral(counts.getOrDefault(EvidenceStance.NEUTRAL, 0L))
                .proRatio(counts.getOrDefault(EvidenceStance.PRO, 0L) / (double) finalTotal)
                .conRatio(counts.getOrDefault(EvidenceStance.CON, 0L) / (double) finalTotal)
                .neutralRatio(counts.getOrDefault(EvidenceStance.NEUTRAL, 0L) / (double) finalTotal)
                .build();
    }

    private DeepSearchJobDto toJobDto(CrawlJob job) {
        return DeepSearchJobDto.builder()
                .jobId(job.getId())
                .topic(job.getTopic())
                .baseUrl(job.getBaseUrl())
                .status(job.getStatus().name())
                .evidenceCount(job.getEvidenceCount())
                .errorMessage(job.getErrorMessage())
                .failureReason(job.getFailureReason() != null ? job.getFailureReason().getCode() : null)
                .failureCategory(categorizeFailureReason(job.getFailureReason()))
                .createdAt(job.getCreatedAt())
                .completedAt(job.getCompletedAt())
                .build();
    }

    /**
     * Categorize failure reason into high-level categories for frontend display
     */
    private String categorizeFailureReason(CrawlFailureReason reason) {
        if (reason == null) return null;
        
        String code = reason.getCode();
        if (code.startsWith("timeout")) return "timeout";
        if (code.contains("connection") || code.contains("dns") || code.contains("network") || code.contains("ssl")) return "network";
        if (code.contains("service") || code.contains("unavailable") || code.contains("overloaded")) return "service";
        if (code.contains("content") || code.contains("parse") || code.contains("blocked")) return "content";
        if (code.contains("ai") || code.contains("evidence") || code.contains("stance")) return "processing";
        if (code.contains("cancelled") || code.contains("callback") || code.contains("token")) return "job";
        return "unknown";
    }

    private EvidenceDto toEvidenceDto(CrawlEvidence evidence) {
        return EvidenceDto.builder()
                .id(evidence.getId())
                .url(evidence.getUrl())
                .title(evidence.getTitle())
                .stance(evidence.getStance() != null ? evidence.getStance().name().toLowerCase() : "neutral")
                .snippet(evidence.getSnippet())
                .source(evidence.getSource())
                .sourceCategory(evidence.getSourceCategory() != null ? evidence.getSourceCategory().getValue() : "news")
                .build();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/DeepOrchestrationService.java

```java
package com.newsinsight.collector.service;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.newsinsight.collector.dto.*;
import com.newsinsight.collector.entity.ai.*;
import com.newsinsight.collector.repository.AiJobRepository;
import com.newsinsight.collector.repository.AiSubTaskRepository;
import com.newsinsight.collector.service.autocrawl.AutoCrawlIntegrationService;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.PageRequest;
import org.springframework.data.domain.Pageable;
import org.springframework.data.domain.Sort;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.scheduling.annotation.Scheduled;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.time.LocalDateTime;
import java.util.*;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
import java.util.stream.Collectors;

/**
 * Orchestration service for multi-provider AI analysis.
 * Manages job lifecycle, sub-task distribution, and result aggregation.
 * 
 * AutoCrawl Integration: Notifies AutoCrawl of discovered URLs when deep analysis completes.
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class DeepOrchestrationService {

    private final AiJobRepository aiJobRepository;
    private final AiSubTaskRepository aiSubTaskRepository;
    private final KafkaTemplate<String, AiTaskRequestMessage> aiTaskRequestKafkaTemplate;
    private final ObjectMapper objectMapper;
    private final AutoCrawlIntegrationService autoCrawlIntegrationService;

    @Value("${collector.ai.orchestration.topic:ai.tasks.requests}")
    private String aiTaskRequestTopic;

    @Value("${collector.ai.orchestration.callback-base-url:${collector.deep-search.callback-base-url:http://localhost:8081}}")
    private String callbackBaseUrl;

    @Value("${collector.ai.orchestration.callback-token:${collector.deep-search.callback-token:}}")
    private String callbackToken;

    @Value("${collector.ai.orchestration.timeout-minutes:30}")
    private int timeoutMinutes;

    @Value("${collector.ai.orchestration.cleanup-days:7}")
    private int cleanupDays;

    @Value("${autocrawl.enabled:true}")
    private boolean autoCrawlEnabled;

    // URL extraction pattern for discovering URLs in AI results
    private static final Pattern URL_PATTERN = Pattern.compile(
            "https?://[\\w\\-._~:/?#\\[\\]@!$&'()*+,;=%]+",
            Pattern.CASE_INSENSITIVE
    );

    /**
     * Start a new deep analysis job with multiple AI providers.
     * 
     * @param topic The search topic
     * @param baseUrl Optional base URL for crawling
     * @param providers List of providers to use (null = default set)
     * @return Created job DTO
     */
    @Transactional
    public AiJobDto startDeepAnalysis(String topic, String baseUrl, List<AiProvider> providers) {
        // Generate job ID
        String jobId = AiJob.generateJobId();
        
        // Create job entity
        AiJob job = AiJob.builder()
                .id(jobId)
                .topic(topic)
                .baseUrl(baseUrl)
                .overallStatus(AiJobStatus.PENDING)
                .build();

        // Determine which providers to use
        List<AiProvider> targetProviders = providers != null && !providers.isEmpty()
                ? providers
                : getDefaultProviders();

        // Create sub-tasks for each provider
        for (AiProvider provider : targetProviders) {
            AiSubTask subTask = AiSubTask.create(job, provider, getTaskTypeForProvider(provider));
            log.debug("Created sub-task: {} for provider: {}", subTask.getId(), provider);
        }

        // Save job with sub-tasks
        aiJobRepository.save(job);
        log.info("Created AI job: id={}, topic={}, providers={}", jobId, topic, targetProviders);

        // Publish tasks to Kafka
        publishTasksToKafka(job);

        // Update job status to IN_PROGRESS
        job.markInProgress();
        aiJobRepository.save(job);

        return toJobDto(job);
    }

    /**
     * Start analysis with default providers
     */
    @Transactional
    public AiJobDto startDeepAnalysis(String topic, String baseUrl) {
        return startDeepAnalysis(topic, baseUrl, null);
    }

    /**
     * Handle callback from AI worker/n8n
     */
    @Transactional
    public void handleCallback(AiTaskCallbackRequest request) {
        log.info("Processing callback: jobId={}, subTaskId={}, status={}", 
                request.jobId(), request.subTaskId(), request.status());

        // Find sub-task
        AiSubTask subTask = findSubTask(request);
        if (subTask == null) {
            log.warn("Sub-task not found for callback: jobId={}, subTaskId={}, providerId={}", 
                    request.jobId(), request.subTaskId(), request.providerId());
            return;
        }

        // Check if already processed
        if (subTask.isTerminal()) {
            log.warn("Sub-task already in terminal state: {}", subTask.getId());
            return;
        }

        // Update sub-task status
        if (request.isSuccess()) {
            String resultJson = buildResultJson(request);
            subTask.markCompleted(resultJson);
            log.info("Sub-task completed: {}", subTask.getId());
        } else {
            subTask.markFailed(request.errorMessage());
            log.warn("Sub-task failed: {} - {}", subTask.getId(), request.errorMessage());
        }
        aiSubTaskRepository.save(subTask);

        // Update job overall status
        AiJob job = subTask.getAiJob();
        updateJobOverallStatus(job);
    }

    /**
     * Get job status
     */
    @Transactional(readOnly = true)
    public AiJobDto getJobStatus(String jobId) {
        AiJob job = aiJobRepository.findByIdWithSubTasks(jobId)
                .orElseThrow(() -> new IllegalArgumentException("Job not found: " + jobId));
        return toJobDto(job);
    }

    /**
     * List jobs with pagination
     */
    @Transactional(readOnly = true)
    public Page<AiJobDto> listJobs(int page, int size, AiJobStatus status) {
        Pageable pageable = PageRequest.of(page, size, Sort.by(Sort.Direction.DESC, "createdAt"));
        
        Page<AiJob> jobs = status != null
                ? aiJobRepository.findByOverallStatus(status, pageable)
                : aiJobRepository.findAll(pageable);

        return jobs.map(this::toJobDtoWithoutSubTasks);
    }

    /**
     * Cancel a job
     */
    @Transactional
    public AiJobDto cancelJob(String jobId) {
        AiJob job = aiJobRepository.findByIdWithSubTasks(jobId)
                .orElseThrow(() -> new IllegalArgumentException("Job not found: " + jobId));

        if (!job.isTerminal()) {
            job.markCancelled();
            
            // Cancel all pending sub-tasks
            for (AiSubTask subTask : job.getSubTasks()) {
                if (!subTask.isTerminal()) {
                    subTask.markCancelled();
                }
            }
            
            aiJobRepository.save(job);
            log.info("Cancelled job: {}", jobId);
        }

        return toJobDto(job);
    }

    /**
     * Retry failed sub-tasks for a job
     */
    @Transactional
    public AiJobDto retryFailedTasks(String jobId) {
        AiJob job = aiJobRepository.findByIdWithSubTasks(jobId)
                .orElseThrow(() -> new IllegalArgumentException("Job not found: " + jobId));

        List<AiSubTask> failedTasks = job.getSubTasks().stream()
                .filter(t -> t.getStatus() == AiTaskStatus.FAILED || t.getStatus() == AiTaskStatus.TIMEOUT)
                .collect(Collectors.toList());

        if (failedTasks.isEmpty()) {
            log.info("No failed tasks to retry for job: {}", jobId);
            return toJobDto(job);
        }

        // Reset failed tasks and re-publish
        for (AiSubTask task : failedTasks) {
            task.setStatus(AiTaskStatus.PENDING);
            task.setErrorMessage(null);
            task.setResultJson(null);
            task.setCompletedAt(null);
            task.incrementRetry();
            
            publishTaskToKafka(job, task);
        }

        job.setOverallStatus(AiJobStatus.IN_PROGRESS);
        job.setCompletedAt(null);
        aiJobRepository.save(job);

        log.info("Retrying {} failed tasks for job: {}", failedTasks.size(), jobId);
        return toJobDto(job);
    }

    /**
     * Scheduled task to timeout old pending jobs
     */
    @Scheduled(fixedDelayString = "${collector.ai.orchestration.timeout-check-interval:300000}")
    @Transactional
    public void timeoutOldJobs() {
        LocalDateTime cutoff = LocalDateTime.now().minusMinutes(timeoutMinutes);
        
        // Mark timed out sub-tasks
        int tasksTimedOut = aiSubTaskRepository.markTimedOutTasks(cutoff);
        
        // Mark timed out jobs
        int jobsTimedOut = aiJobRepository.markTimedOutJobs(cutoff);
        
        if (tasksTimedOut > 0 || jobsTimedOut > 0) {
            log.info("Timeout check: {} tasks, {} jobs marked as timed out", tasksTimedOut, jobsTimedOut);
        }
    }

    /**
     * Scheduled cleanup of old jobs
     */
    @Scheduled(cron = "${collector.ai.orchestration.cleanup-cron:0 0 3 * * ?}")
    @Transactional
    public void cleanupOldJobs() {
        LocalDateTime cutoff = LocalDateTime.now().minusDays(cleanupDays);
        
        // Get job IDs to delete
        List<AiJob> oldJobs = aiJobRepository.findByStatusInAndCreatedAtBefore(
                List.of(AiJobStatus.COMPLETED, AiJobStatus.FAILED, 
                        AiJobStatus.PARTIAL_SUCCESS, AiJobStatus.TIMEOUT, AiJobStatus.CANCELLED),
                cutoff
        );

        if (!oldJobs.isEmpty()) {
            List<String> jobIds = oldJobs.stream().map(AiJob::getId).collect(Collectors.toList());
            
            // Delete sub-tasks first
            int tasksDeleted = aiSubTaskRepository.deleteByJobIds(jobIds);
            
            // Delete jobs
            int jobsDeleted = aiJobRepository.deleteOldJobs(cutoff);
            
            log.info("Cleanup: {} jobs deleted, {} sub-tasks deleted", jobsDeleted, tasksDeleted);
        }
    }

    // ========== Helper Methods ==========

    private List<AiProvider> getDefaultProviders() {
        // Default set of providers for deep analysis
        return List.of(
                AiProvider.SCOUT,           // Quick reconnaissance
                AiProvider.DEEP_READER,     // In-depth analysis
                AiProvider.UNIVERSAL_AGENT  // General AI processing
        );
    }

    private String getTaskTypeForProvider(AiProvider provider) {
        return switch (provider) {
            case SCOUT -> "reconnaissance";
            case DEEP_READER -> "deep_analysis";
            case UNIVERSAL_AGENT -> "general_analysis";
            case LOCAL_QUICK -> "quick_process";
        };
    }

    private void publishTasksToKafka(AiJob job) {
        for (AiSubTask subTask : job.getSubTasks()) {
            publishTaskToKafka(job, subTask);
        }
    }

    private void publishTaskToKafka(AiJob job, AiSubTask subTask) {
        if (!subTask.getProviderId().isExternal()) {
            // LOCAL_QUICK doesn't need Kafka
            log.debug("Skipping Kafka publish for local provider: {}", subTask.getProviderId());
            return;
        }

        String callbackUrl = buildCallbackUrl();
        
        AiTaskRequestMessage message = AiTaskRequestMessage.builder()
                .jobId(job.getId())
                .subTaskId(subTask.getId())
                .providerId(subTask.getProviderId().name())
                .taskType(subTask.getTaskType())
                .topic(job.getTopic())
                .baseUrl(job.getBaseUrl())
                .payload(buildPayload(job, subTask))
                .callbackUrl(callbackUrl)
                .callbackToken(callbackToken)
                .createdAt(LocalDateTime.now())
                .build();

        try {
            aiTaskRequestKafkaTemplate.send(aiTaskRequestTopic, job.getId(), message);
            log.debug("Published task to Kafka: topic={}, jobId={}, subTaskId={}", 
                    aiTaskRequestTopic, job.getId(), subTask.getId());
        } catch (Exception e) {
            log.error("Failed to publish task to Kafka: {}", e.getMessage(), e);
            subTask.markFailed("Failed to publish to Kafka: " + e.getMessage());
            aiSubTaskRepository.save(subTask);
        }
    }

    private Map<String, Object> buildPayload(AiJob job, AiSubTask subTask) {
        Map<String, Object> payload = new HashMap<>();
        payload.put("topic", job.getTopic());
        payload.put("baseUrl", job.getBaseUrl());
        payload.put("taskType", subTask.getTaskType());
        payload.put("retryCount", subTask.getRetryCount());
        return payload;
    }

    private String buildCallbackUrl() {
        String base = callbackBaseUrl.endsWith("/") 
                ? callbackBaseUrl.substring(0, callbackBaseUrl.length() - 1) 
                : callbackBaseUrl;
        return base + "/api/v1/ai/callback";
    }

    private AiSubTask findSubTask(AiTaskCallbackRequest request) {
        // Try to find by subTaskId first
        if (request.subTaskId() != null && !request.subTaskId().isBlank()) {
            return aiSubTaskRepository.findById(request.subTaskId()).orElse(null);
        }
        
        // Fallback: find by jobId + providerId
        if (request.jobId() != null && request.providerId() != null) {
            try {
                AiProvider provider = AiProvider.valueOf(request.providerId());
                return aiSubTaskRepository.findByAiJobIdAndProviderId(request.jobId(), provider).orElse(null);
            } catch (IllegalArgumentException e) {
                log.warn("Invalid provider ID: {}", request.providerId());
            }
        }
        
        return null;
    }

    private String buildResultJson(AiTaskCallbackRequest request) {
        // If evidence is provided, include it in result
        if (request.evidence() != null && !request.evidence().isEmpty()) {
            try {
                Map<String, Object> result = new HashMap<>();
                result.put("evidence", request.evidence());
                result.put("resultJson", request.resultJson());
                return objectMapper.writeValueAsString(result);
            } catch (JsonProcessingException e) {
                log.warn("Failed to serialize evidence: {}", e.getMessage());
            }
        }
        return request.resultJson();
    }

    private void updateJobOverallStatus(AiJob job) {
        List<AiSubTask> subTasks = aiSubTaskRepository.findByAiJobId(job.getId());
        
        long total = subTasks.size();
        long completed = subTasks.stream().filter(t -> t.getStatus() == AiTaskStatus.COMPLETED).count();
        // long failed = subTasks.stream().filter(t -> t.getStatus() == AiTaskStatus.FAILED).count();
        long timeout = subTasks.stream().filter(t -> t.getStatus() == AiTaskStatus.TIMEOUT).count();
        long pending = subTasks.stream().filter(t -> 
                t.getStatus() == AiTaskStatus.PENDING || t.getStatus() == AiTaskStatus.IN_PROGRESS).count();

        AiJobStatus previousStatus = job.getOverallStatus();
        
        if (pending > 0) {
            // Still processing
            job.setOverallStatus(AiJobStatus.IN_PROGRESS);
        } else if (completed == total) {
            // All completed
            job.markCompleted();
        } else if (completed > 0) {
            // Some completed, some failed
            job.markPartialSuccess();
        } else if (timeout == total) {
            // All timed out
            job.markTimeout();
        } else {
            // All failed
            job.markFailed("All sub-tasks failed");
        }

        aiJobRepository.save(job);
        log.info("Updated job status: id={}, status={}, completed={}/{}", 
                job.getId(), job.getOverallStatus(), completed, total);

        // Notify AutoCrawl of discovered URLs when job completes (fully or partially)
        if (autoCrawlEnabled && previousStatus == AiJobStatus.IN_PROGRESS 
                && (job.getOverallStatus() == AiJobStatus.COMPLETED 
                    || job.getOverallStatus() == AiJobStatus.PARTIAL_SUCCESS)) {
            notifyAutoCrawlOfDiscoveredUrls(job, subTasks);
        }
    }

    /**
     * Extract URLs from completed sub-task results and notify AutoCrawl.
     */
    private void notifyAutoCrawlOfDiscoveredUrls(AiJob job, List<AiSubTask> subTasks) {
        try {
            Set<String> discoveredUrls = new HashSet<>();
            
            for (AiSubTask subTask : subTasks) {
                if (subTask.getStatus() == AiTaskStatus.COMPLETED && subTask.getResultJson() != null) {
                    // Extract URLs from result JSON
                    List<String> urls = extractUrlsFromText(subTask.getResultJson());
                    discoveredUrls.addAll(urls);
                }
            }

            if (!discoveredUrls.isEmpty()) {
                log.info("Deep search job {} discovered {} unique URLs, notifying AutoCrawl", 
                        job.getId(), discoveredUrls.size());
                autoCrawlIntegrationService.onDeepSearchCompleted(
                        job.getId(), 
                        job.getTopic(), 
                        new ArrayList<>(discoveredUrls)
                );
            }
        } catch (Exception e) {
            log.warn("Failed to notify AutoCrawl of discovered URLs: jobId={}, error={}", 
                    job.getId(), e.getMessage());
        }
    }

    /**
     * Extract URLs from text content.
     */
    private List<String> extractUrlsFromText(String text) {
        if (text == null || text.isBlank()) {
            return List.of();
        }
        
        Matcher matcher = URL_PATTERN.matcher(text);
        List<String> urls = new ArrayList<>();
        while (matcher.find()) {
            urls.add(matcher.group());
        }
        return urls;
    }

    private AiJobDto toJobDto(AiJob job) {
        List<AiSubTaskDto> subTaskDtos = job.getSubTasks().stream()
                .map(this::toSubTaskDto)
                .collect(Collectors.toList());

        long completed = subTaskDtos.stream()
                .filter(t -> "COMPLETED".equals(t.getStatus())).count();
        long failed = subTaskDtos.stream()
                .filter(t -> "FAILED".equals(t.getStatus()) || "TIMEOUT".equals(t.getStatus())).count();

        return AiJobDto.builder()
                .jobId(job.getId())
                .topic(job.getTopic())
                .baseUrl(job.getBaseUrl())
                .overallStatus(job.getOverallStatus().name())
                .subTasks(subTaskDtos)
                .totalTasks(subTaskDtos.size())
                .completedTasks((int) completed)
                .failedTasks((int) failed)
                .errorMessage(job.getErrorMessage())
                .createdAt(job.getCreatedAt())
                .updatedAt(job.getUpdatedAt())
                .completedAt(job.getCompletedAt())
                .build();
    }

    private AiJobDto toJobDtoWithoutSubTasks(AiJob job) {
        return AiJobDto.builder()
                .jobId(job.getId())
                .topic(job.getTopic())
                .baseUrl(job.getBaseUrl())
                .overallStatus(job.getOverallStatus().name())
                .subTasks(List.of())
                .errorMessage(job.getErrorMessage())
                .createdAt(job.getCreatedAt())
                .updatedAt(job.getUpdatedAt())
                .completedAt(job.getCompletedAt())
                .build();
    }

    private AiSubTaskDto toSubTaskDto(AiSubTask subTask) {
        return AiSubTaskDto.builder()
                .subTaskId(subTask.getId())
                .jobId(subTask.getAiJob() != null ? subTask.getAiJob().getId() : null)
                .providerId(subTask.getProviderId().name())
                .taskType(subTask.getTaskType())
                .status(subTask.getStatus().name())
                .resultJson(subTask.getResultJson())
                .errorMessage(subTask.getErrorMessage())
                .retryCount(subTask.getRetryCount())
                .createdAt(subTask.getCreatedAt())
                .updatedAt(subTask.getUpdatedAt())
                .completedAt(subTask.getCompletedAt())
                .build();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/DeepSearchEventService.java

```java
package com.newsinsight.collector.service;

import com.newsinsight.collector.dto.DeepSearchJobDto;
import com.newsinsight.collector.dto.EvidenceDto;
import com.newsinsight.collector.entity.CrawlFailureReason;
import lombok.extern.slf4j.Slf4j;
import org.springframework.http.codec.ServerSentEvent;
import org.springframework.stereotype.Service;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Sinks;

import java.time.Duration;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;

/**
 * Service for managing SSE streams for deep search jobs.
 * Each job has its own event sink that clients can subscribe to.
 */
@Service
@Slf4j
public class DeepSearchEventService {

    // Map of jobId -> Sinks for that job
    private final Map<String, Sinks.Many<ServerSentEvent<Object>>> jobSinks = new ConcurrentHashMap<>();
    
    // Timeout for inactive sinks (30 minutes)
    // @CHECK 싱크 타임아웃 설정이 필요할 것 같음
    private static final Duration SINK_TIMEOUT = Duration.ofMinutes(30);

    /**
     * Get or create a sink for a job.
     * 
     * @param jobId The job ID
     * @return The event sink for this job
     */
    private Sinks.Many<ServerSentEvent<Object>> getOrCreateSink(String jobId) {
        return jobSinks.computeIfAbsent(jobId, id -> {
            log.info("Creating new SSE sink for job: {}", id);
            return Sinks.many().multicast().onBackpressureBuffer(100);
        });
    }

    /**
     * Get the event stream for a specific job.
     * Includes heartbeats every 15 seconds to keep connection alive.
     * 
     * @param jobId The job ID to subscribe to
     * @return SSE event stream
     */
    public Flux<ServerSentEvent<Object>> getJobEventStream(String jobId) {
        Sinks.Many<ServerSentEvent<Object>> sink = getOrCreateSink(jobId);
        
        // Heartbeat stream
        Flux<ServerSentEvent<Object>> heartbeat = Flux.interval(Duration.ofSeconds(15))
                .map(tick -> ServerSentEvent.builder()
                        .event("heartbeat")
                        .data(Map.of(
                                "eventType", "heartbeat",
                                "jobId", jobId,
                                "timestamp", System.currentTimeMillis()
                        ))
                        .build());

        // Main event stream from sink
        Flux<ServerSentEvent<Object>> events = sink.asFlux();

        return Flux.merge(heartbeat, events)
                .doOnSubscribe(sub -> log.info("New SSE subscriber for job: {}", jobId))
                .doOnCancel(() -> log.info("SSE subscriber disconnected for job: {}", jobId))
                .doOnError(e -> log.error("SSE stream error for job: {}", jobId, e));
    }

    /**
     * Publish a status update event.
     * 
     * @param jobId The job ID
     * @param status Current status
     * @param message Optional message
     */
    public void publishStatusUpdate(String jobId, String status, String message) {
        Sinks.Many<ServerSentEvent<Object>> sink = jobSinks.get(jobId);
        if (sink == null) {
            log.debug("No sink found for job: {}, creating new one", jobId);
            sink = getOrCreateSink(jobId);
        }

        ServerSentEvent<Object> event = ServerSentEvent.builder()
                .event("status")
                .data(Map.of(
                        "eventType", "status",
                        "jobId", jobId,
                        "status", status,
                        "message", message != null ? message : "",
                        "timestamp", System.currentTimeMillis()
                ))
                .build();

        sink.tryEmitNext(event);
        log.debug("Published status event for job: {}, status: {}", jobId, status);
    }

    /**
     * Publish a progress update event.
     * 
     * @param jobId The job ID
     * @param progress Progress percentage (0-100)
     * @param currentStep Current step description
     */
    public void publishProgressUpdate(String jobId, int progress, String currentStep) {
        Sinks.Many<ServerSentEvent<Object>> sink = jobSinks.get(jobId);
        if (sink == null) {
            log.debug("No sink found for job: {}, creating new one for progress update", jobId);
            sink = getOrCreateSink(jobId);
        }

        ServerSentEvent<Object> event = ServerSentEvent.builder()
                .event("progress")
                .data(Map.of(
                        "eventType", "progress",
                        "jobId", jobId,
                        "progress", progress,
                        "progressMessage", currentStep,
                        "timestamp", System.currentTimeMillis()
                ))
                .build();

        sink.tryEmitNext(event);
        log.debug("Published progress event for job: {}, progress: {}%", jobId, progress);
    }

    /**
     * Publish an evidence discovered event.
     * 
     * @param jobId The job ID
     * @param evidence The evidence DTO
     */
    public void publishEvidence(String jobId, EvidenceDto evidence) {
        Sinks.Many<ServerSentEvent<Object>> sink = jobSinks.get(jobId);
        if (sink == null) {
            log.debug("No sink found for job: {}, creating new one for evidence", jobId);
            sink = getOrCreateSink(jobId);
        }

        // Get current evidence count from the sink's context or use a simple counter
        int evidenceCount = evidence.getId() != null ? evidence.getId().intValue() : 1;

        ServerSentEvent<Object> event = ServerSentEvent.builder()
                .event("evidence")
                .data(Map.of(
                        "eventType", "evidence",
                        "jobId", jobId,
                        "evidence", evidence,
                        "evidenceCount", evidenceCount,
                        "timestamp", System.currentTimeMillis()
                ))
                .build();

        sink.tryEmitNext(event);
        log.debug("Published evidence event for job: {}, url: {}", jobId, evidence.getUrl());
    }

    /**
     * Publish a job completion event.
     * 
     * @param jobId The job ID
     * @param jobDto The final job DTO
     */
    public void publishComplete(String jobId, DeepSearchJobDto jobDto) {
        Sinks.Many<ServerSentEvent<Object>> sink = jobSinks.get(jobId);
        if (sink == null) {
            log.debug("No sink found for job: {}, creating new one for completion", jobId);
            sink = getOrCreateSink(jobId);
        }

        ServerSentEvent<Object> event = ServerSentEvent.builder()
                .event("complete")
                .data(Map.of(
                        "eventType", "complete",
                        "jobId", jobId,
                        "result", jobDto,
                        "timestamp", System.currentTimeMillis()
                ))
                .build();

        sink.tryEmitNext(event);
        log.info("Published complete event for job: {}", jobId);

        // Complete the sink and schedule cleanup
        sink.tryEmitComplete();
        scheduleCleanup(jobId);
    }

    /**
     * Publish an error event.
     * 
     * @param jobId The job ID
     * @param errorMessage Error message
     */
    public void publishError(String jobId, String errorMessage) {
        publishError(jobId, errorMessage, null);
    }

    /**
     * Publish an error event with a failure reason.
     * 
     * @param jobId The job ID
     * @param errorMessage Error message
     * @param failureReason The categorized failure reason for diagnostics
     */
    public void publishError(String jobId, String errorMessage, CrawlFailureReason failureReason) {
        Sinks.Many<ServerSentEvent<Object>> sink = jobSinks.get(jobId);
        if (sink == null) {
            log.debug("No sink found for job: {}, creating new one for error", jobId);
            sink = getOrCreateSink(jobId);
        }

        // Build error data map with optional failure reason
        java.util.Map<String, Object> errorData = new java.util.HashMap<>();
        errorData.put("eventType", "error");
        errorData.put("jobId", jobId);
        errorData.put("error", errorMessage);
        errorData.put("timestamp", System.currentTimeMillis());
        
        if (failureReason != null) {
            errorData.put("failureReason", failureReason.getCode());
            errorData.put("failureCategory", categorizeFailureReason(failureReason));
            errorData.put("failureDescription", failureReason.getDescription());
        }

        ServerSentEvent<Object> event = ServerSentEvent.builder()
                .event("error")
                .data(errorData)
                .build();

        sink.tryEmitNext(event);
        log.info("Published error event for job: {}, error: {}, reason: {}", 
                jobId, errorMessage, failureReason != null ? failureReason.getCode() : "unknown");

        // Complete the sink and schedule cleanup
        sink.tryEmitComplete();
        scheduleCleanup(jobId);
    }

    /**
     * Categorize failure reason into high-level categories for frontend display
     */
    private String categorizeFailureReason(CrawlFailureReason reason) {
        if (reason == null) return "unknown";
        
        String code = reason.getCode();
        if (code.startsWith("timeout")) return "timeout";
        if (code.contains("connection") || code.contains("dns") || code.contains("network") || code.contains("ssl")) return "network";
        if (code.contains("service") || code.contains("unavailable") || code.contains("overloaded")) return "service";
        if (code.contains("content") || code.contains("parse") || code.contains("blocked")) return "content";
        if (code.contains("ai") || code.contains("evidence") || code.contains("stance")) return "processing";
        if (code.contains("cancelled") || code.contains("callback") || code.contains("token")) return "job";
        return "unknown";
    }

    /**
     * Schedule cleanup of a job's sink after a delay.
     * 
     * @param jobId The job ID to clean up
     */
    private void scheduleCleanup(String jobId) {
        // Use virtual thread or executor for delayed cleanup
        Thread.startVirtualThread(() -> {
            try {
                Thread.sleep(Duration.ofMinutes(5));
                jobSinks.remove(jobId);
                log.debug("Cleaned up sink for job: {}", jobId);
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
            }
        });
    }

    /**
     * Remove a job's sink immediately.
     * 
     * @param jobId The job ID
     */
    public void removeSink(String jobId) {
        Sinks.Many<ServerSentEvent<Object>> sink = jobSinks.remove(jobId);
        if (sink != null) {
            sink.tryEmitComplete();
            log.debug("Removed sink for job: {}", jobId);
        }
    }

    /**
     * Check if a job has an active sink.
     * 
     * @param jobId The job ID
     * @return true if there's an active sink
     */
    public boolean hasSink(String jobId) {
        return jobSinks.containsKey(jobId);
    }

    /**
     * Get the number of active sinks.
     * 
     * @return Count of active sinks
     */
    public int getActiveSinkCount() {
        return jobSinks.size();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/FactCheckChatService.java

```java
package com.newsinsight.collector.service;

import com.newsinsight.collector.controller.FactCheckChatController.ChatMessage;
import com.newsinsight.collector.entity.chat.FactCheckChatSession;
import com.newsinsight.collector.repository.FactCheckChatSessionRepository;
import com.newsinsight.collector.service.FactVerificationService.DeepAnalysisEvent;
import io.micrometer.core.annotation.Timed;
import io.micrometer.core.instrument.Counter;
import io.micrometer.core.instrument.MeterRegistry;
import io.micrometer.core.instrument.Timer;
import jakarta.annotation.PostConstruct;
import lombok.Builder;
import lombok.Data;
import lombok.extern.slf4j.Slf4j;
import org.springframework.cache.CacheManager;
import org.springframework.stereotype.Service;
import reactor.core.publisher.Flux;

import java.time.LocalDateTime;
import java.util.ArrayList;
import java.util.List;
import java.util.UUID;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.atomic.AtomicLong;
import java.util.stream.Collectors;

/**
 * 팩트체크 챗봇 서비스
 * 
 * MongoDB에 채팅 이력을 저장하고 Redis로 캐싱합니다.
 * 백그라운드에서 FactVerificationService를 호출하고
 * 결과를 SSE로 스트리밍합니다.
 * 
 * 주요 기능:
 * - 세션 관리 (생성, 조회, 종료)
 * - 메시지 처리 및 팩트체크 실행
 * - Redis 캐싱 (수동 관리)
 * - 백그라운드 동기화 트리거
 * - 메트릭 수집
 */
@Service
@Slf4j
public class FactCheckChatService {

    private static final String CACHE_NAME_SESSIONS = "chatSessions";
    private static final String CACHE_NAME_MESSAGES = "chatMessages";

    private final FactVerificationService factVerificationService;
    private final FactCheckChatSessionRepository sessionRepository;
    private final ChatSyncService chatSyncService;
    private final CacheManager cacheManager;
    private final MeterRegistry meterRegistry;

    // 메트릭
    private Counter sessionCreatedCounter;
    private Counter sessionClosedCounter;
    private Counter messageProcessedCounter;
    private Counter factCheckSuccessCounter;
    private Counter factCheckErrorCounter;
    private Timer factCheckTimer;
    private final AtomicLong activeSessionsGauge = new AtomicLong(0);

    // 진행 중인 세션 트래킹 (동시성 제어)
    private final ConcurrentHashMap<String, Boolean> processingSessions = new ConcurrentHashMap<>();

    // MongoDB 연결 실패 시 인메모리 fallback 세션 저장소
    private final ConcurrentHashMap<String, FactCheckChatSession> inMemorySessions = new ConcurrentHashMap<>();

    public FactCheckChatService(
            FactVerificationService factVerificationService,
            FactCheckChatSessionRepository sessionRepository,
            ChatSyncService chatSyncService,
            CacheManager cacheManager,
            MeterRegistry meterRegistry
    ) {
        this.factVerificationService = factVerificationService;
        this.sessionRepository = sessionRepository;
        this.chatSyncService = chatSyncService;
        this.cacheManager = cacheManager;
        this.meterRegistry = meterRegistry;
    }

    @PostConstruct
    public void initMetrics() {
        sessionCreatedCounter = Counter.builder("factcheck.chat.sessions.created")
                .description("Number of chat sessions created")
                .register(meterRegistry);
        
        sessionClosedCounter = Counter.builder("factcheck.chat.sessions.closed")
                .description("Number of chat sessions closed")
                .register(meterRegistry);
        
        messageProcessedCounter = Counter.builder("factcheck.chat.messages.processed")
                .description("Number of messages processed")
                .register(meterRegistry);
        
        factCheckSuccessCounter = Counter.builder("factcheck.chat.factcheck.success")
                .description("Number of successful fact checks")
                .register(meterRegistry);
        
        factCheckErrorCounter = Counter.builder("factcheck.chat.factcheck.error")
                .description("Number of failed fact checks")
                .register(meterRegistry);
        
        factCheckTimer = Timer.builder("factcheck.chat.factcheck.duration")
                .description("Time taken for fact check operations")
                .register(meterRegistry);

        meterRegistry.gauge("factcheck.chat.sessions.active", activeSessionsGauge);
    }

    /**
     * 세션 생성 또는 조회
     * 캐시를 수동으로 관리하여 proxy 문제 회피
     * MongoDB 연결 실패 시 인메모리 fallback 사용
     */
    @Timed(value = "factcheck.chat.session.get", description = "Time to get or create session")
    public FactCheckChatSession getOrCreateSession(String sessionId) {
        // 1. 캐시에서 먼저 조회
        FactCheckChatSession cached = getCachedSession(sessionId);
        if (cached != null) {
            log.debug("Session {} found in cache", sessionId);
            return cached;
        }

        // 2. 인메모리 fallback에서 조회
        FactCheckChatSession inMemory = inMemorySessions.get(sessionId);
        if (inMemory != null) {
            log.debug("Session {} found in in-memory fallback", sessionId);
            return inMemory;
        }

        try {
            // 3. MongoDB에서 조회
            return sessionRepository.findBySessionId(sessionId)
                    .map(session -> {
                        putSessionToCache(session);
                        return session;
                    })
                    .orElseGet(() -> {
                        // 4. 새 세션 생성 (MongoDB)
                        FactCheckChatSession session = FactCheckChatSession.builder()
                                .sessionId(sessionId)
                                .startedAt(LocalDateTime.now())
                                .lastActivityAt(LocalDateTime.now())
                                .status(FactCheckChatSession.SessionStatus.ACTIVE)
                                .messages(new ArrayList<>())
                                .build();
                        FactCheckChatSession saved = sessionRepository.save(session);
                        
                        // 메트릭 업데이트
                        sessionCreatedCounter.increment();
                        activeSessionsGauge.incrementAndGet();
                        
                        putSessionToCache(saved);
                        log.info("Created new chat session: {}", sessionId);
                        return saved;
                    });
        } catch (Exception e) {
            // MongoDB 연결 실패 시 인메모리 fallback 사용
            log.warn("MongoDB connection failed, using in-memory fallback for session {}: {}", sessionId, e.getMessage());
            return createInMemorySession(sessionId);
        }
    }

    /**
     * 인메모리 세션 생성 (MongoDB 연결 실패 시 fallback)
     */
    private FactCheckChatSession createInMemorySession(String sessionId) {
        FactCheckChatSession session = FactCheckChatSession.builder()
                .sessionId(sessionId)
                .startedAt(LocalDateTime.now())
                .lastActivityAt(LocalDateTime.now())
                .status(FactCheckChatSession.SessionStatus.ACTIVE)
                .messages(new ArrayList<>())
                .build();
        
        inMemorySessions.put(sessionId, session);
        sessionCreatedCounter.increment();
        activeSessionsGauge.incrementAndGet();
        
        log.info("Created in-memory chat session: {}", sessionId);
        return session;
    }

    /**
     * 세션 생성 (사용자 정보 포함)
     */
    public FactCheckChatSession createSession(String sessionId, String userId, String userAgent, String ipAddress) {
        FactCheckChatSession session = FactCheckChatSession.builder()
                .sessionId(sessionId)
                .userId(userId)
                .startedAt(LocalDateTime.now())
                .lastActivityAt(LocalDateTime.now())
                .status(FactCheckChatSession.SessionStatus.ACTIVE)
                .messages(new ArrayList<>())
                .metadata(FactCheckChatSession.SessionMetadata.builder()
                        .userAgent(userAgent)
                        .ipAddress(ipAddress)
                        .messageCount(0)
                        .factCheckCount(0)
                        .build())
                .build();
        
        FactCheckChatSession saved = sessionRepository.save(session);
        
        sessionCreatedCounter.increment();
        activeSessionsGauge.incrementAndGet();
        putSessionToCache(saved);
        
        log.info("Created new chat session: {} for user: {}", sessionId, userId);
        return saved;
    }

    /**
     * 사용자 메시지 처리 및 팩트체크 수행
     * 
     * @param sessionId 세션 ID
     * @param userMessage 사용자 메시지
     * @param claims 검증할 주장 목록 (선택)
     * @return 챗봇 응답 이벤트 스트림
     */
    @Timed(value = "factcheck.chat.message.process", description = "Time to process message")
    public Flux<ChatEvent> processMessage(String sessionId, String userMessage, List<String> claims) {
        log.info("Processing message for session {}: {}", sessionId, userMessage);
        
        // 중복 처리 방지
        if (processingSessions.putIfAbsent(sessionId, true) != null) {
            log.warn("Session {} is already processing a message", sessionId);
            return Flux.just(ChatEvent.builder()
                    .type("error")
                    .role("system")
                    .content("이전 요청을 처리 중입니다. 잠시 후 다시 시도해주세요.")
                    .timestamp(System.currentTimeMillis())
                    .build());
        }
        
        messageProcessedCounter.increment();
        
        // 세션 조회 또는 생성
        FactCheckChatSession session = getOrCreateSession(sessionId);
        
        // 사용자 메시지 저장
        FactCheckChatSession.ChatMessage userMsg = FactCheckChatSession.ChatMessage.builder()
                .messageId(UUID.randomUUID().toString())
                .role("user")
                .content(userMessage)
                .timestamp(System.currentTimeMillis())
                .type(FactCheckChatSession.MessageType.MESSAGE)
                .build();
        
        session.getMessages().add(userMsg);
        session.setLastActivityAt(LocalDateTime.now());
        
        // 메타데이터 업데이트
        updateSessionMetadata(session);
        
        saveSession(session);

        return Flux.create(sink -> {
            Timer.Sample sample = Timer.start(meterRegistry);
            
            // 1. 인사 메시지
            sink.next(ChatEvent.builder()
                    .type("message")
                    .role("assistant")
                    .content("안녕하세요! 팩트체크 챗봇입니다. 입력하신 내용을 분석하겠습니다.")
                    .timestamp(System.currentTimeMillis())
                    .build());

            // 2. 분석 시작 알림
            sink.next(ChatEvent.builder()
                    .type("status")
                    .role("system")
                    .content("🔍 팩트체크를 시작합니다...")
                    .phase("init")
                    .timestamp(System.currentTimeMillis())
                    .build());

            // 3. 백그라운드에서 팩트체크 실행
            executeFactCheckAsync(sessionId, userMessage, claims, sink, sample);
        });
    }

    /**
     * 백그라운드에서 팩트체크 실행
     */
    private void executeFactCheckAsync(
            String sessionId, 
            String topic, 
            List<String> claims,
            reactor.core.publisher.FluxSink<ChatEvent> sink,
            Timer.Sample timerSample
    ) {
        try {
            StringBuilder assistantResponse = new StringBuilder();
            
            // FactVerificationService 호출
            factVerificationService.analyzeAndVerify(topic, claims)
                    .doOnNext(event -> {
                        // DeepAnalysisEvent를 ChatEvent로 변환
                        ChatEvent chatEvent = convertToChatEvent(event);
                        
                        // 어시스턴트 응답 누적
                        if ("ai_synthesis".equals(event.getEventType())) {
                            assistantResponse.append(event.getMessage());
                        }
                        
                        sink.next(chatEvent);
                    })
                    .doOnComplete(() -> {
                        // 최종 응답 저장
                        if (assistantResponse.length() > 0) {
                            addToHistory(sessionId, assistantResponse.toString(), 
                                    FactCheckChatSession.MessageType.AI_SYNTHESIS);
                        }
                        
                        // 완료 메시지
                        sink.next(ChatEvent.builder()
                                .type("complete")
                                .role("system")
                                .content("✅ 팩트체크가 완료되었습니다. 추가로 궁금한 점이 있으시면 질문해주세요!")
                                .timestamp(System.currentTimeMillis())
                                .build());
                        
                        // 메트릭 기록
                        timerSample.stop(factCheckTimer);
                        factCheckSuccessCounter.increment();
                        
                        // 세션 메타데이터 업데이트
                        FactCheckChatSession session = getOrCreateSession(sessionId);
                        if (session.getMetadata() != null) {
                            Integer count = session.getMetadata().getFactCheckCount();
                            session.getMetadata().setFactCheckCount(count != null ? count + 1 : 1);
                            saveSession(session);
                        }
                        
                        // 처리 상태 해제
                        processingSessions.remove(sessionId);
                        
                        sink.complete();
                    })
                    .doOnError(error -> {
                        log.error("Fact check failed for session {}: {}", sessionId, error.getMessage());
                        
                        // 에러 메시지 저장
                        addToHistory(sessionId, "팩트체크 중 오류 발생: " + error.getMessage(), 
                                FactCheckChatSession.MessageType.ERROR);
                        
                        sink.next(ChatEvent.builder()
                                .type("error")
                                .role("system")
                                .content("❌ 팩트체크 중 오류가 발생했습니다: " + error.getMessage())
                                .timestamp(System.currentTimeMillis())
                                .build());
                        
                        // 메트릭 기록
                        timerSample.stop(factCheckTimer);
                        factCheckErrorCounter.increment();
                        
                        // 처리 상태 해제
                        processingSessions.remove(sessionId);
                        
                        sink.error(error);
                    })
                    .subscribe();
                    
        } catch (Exception e) {
            log.error("Failed to execute fact check for session {}: {}", sessionId, e.getMessage());
            factCheckErrorCounter.increment();
            processingSessions.remove(sessionId);
            sink.error(e);
        }
    }

    /**
     * DeepAnalysisEvent를 ChatEvent로 변환
     */
    private ChatEvent convertToChatEvent(DeepAnalysisEvent event) {
        ChatEvent.ChatEventBuilder builder = ChatEvent.builder()
                .type(event.getEventType())
                .role("assistant")
                .content(event.getMessage())
                .phase(event.getPhase())
                .timestamp(System.currentTimeMillis());

        // 증거 정보 추가
        if (event.getEvidence() != null && !event.getEvidence().isEmpty()) {
            builder.evidence(event.getEvidence());
        }

        // 검증 결과 추가
        if (event.getVerificationResult() != null) {
            builder.verificationResult(event.getVerificationResult());
        }

        // 신뢰도 평가 추가
        if (event.getCredibility() != null) {
            builder.credibility(event.getCredibility());
        }

        return builder.build();
    }

    /**
     * 세션 저장 (MongoDB + Redis 캐시 갱신)
     * MongoDB 연결 실패 시 인메모리에 저장
     */
    private FactCheckChatSession saveSession(FactCheckChatSession session) {
        session.setLastActivityAt(LocalDateTime.now());
        
        try {
            FactCheckChatSession saved = sessionRepository.save(session);
            
            // 캐시 업데이트
            putSessionToCache(saved);
            evictMessagesCache(session.getSessionId());
            
            // 백그라운드 동기화 트리거
            chatSyncService.scheduleSyncIfNeeded(saved);
            
            return saved;
        } catch (Exception e) {
            // MongoDB 연결 실패 시 인메모리에 저장
            log.warn("MongoDB save failed, using in-memory fallback for session {}: {}", 
                    session.getSessionId(), e.getMessage());
            inMemorySessions.put(session.getSessionId(), session);
            return session;
        }
    }

    /**
     * 이력에 메시지 추가
     */
    private void addToHistory(String sessionId, String content, FactCheckChatSession.MessageType type) {
        try {
            FactCheckChatSession session = getOrCreateSession(sessionId);
            
            FactCheckChatSession.ChatMessage message = FactCheckChatSession.ChatMessage.builder()
                    .messageId(UUID.randomUUID().toString())
                    .role("assistant")
                    .content(content)
                    .timestamp(System.currentTimeMillis())
                    .type(type)
                    .build();
            
            session.getMessages().add(message);
            saveSession(session);
            
            log.debug("Added message to history for session {}: type={}", sessionId, type);
        } catch (Exception e) {
            log.error("Failed to add message to history for session {}: {}", sessionId, e.getMessage());
        }
    }

    /**
     * 메시지 추가 및 저장 (공개 메서드)
     */
    public void addMessageToSession(String sessionId, FactCheckChatSession.ChatMessage message) {
        FactCheckChatSession session = getOrCreateSession(sessionId);
        session.getMessages().add(message);
        updateSessionMetadata(session);
        saveSession(session);
    }

    /**
     * 세션 메타데이터 업데이트
     */
    private void updateSessionMetadata(FactCheckChatSession session) {
        if (session.getMetadata() == null) {
            session.setMetadata(FactCheckChatSession.SessionMetadata.builder()
                    .messageCount(0)
                    .factCheckCount(0)
                    .build());
        }
        session.getMetadata().setMessageCount(session.getMessages().size());
    }

    /**
     * 세션 이력 조회
     */
    @Timed(value = "factcheck.chat.history.get", description = "Time to get chat history")
    public List<ChatMessage> getHistory(String sessionId) {
        // 1. 캐시에서 먼저 조회
        List<ChatMessage> cached = getCachedMessages(sessionId);
        if (cached != null) {
            log.debug("History for session {} found in cache", sessionId);
            return cached;
        }

        // 2. MongoDB에서 조회
        List<ChatMessage> history = sessionRepository.findBySessionId(sessionId)
                .map(session -> session.getMessages().stream()
                        .map(msg -> ChatMessage.builder()
                                .role(msg.getRole())
                                .content(msg.getContent())
                                .timestamp(msg.getTimestamp())
                                .build())
                        .collect(Collectors.toList()))
                .orElse(new ArrayList<>());
        
        // 캐시에 저장
        putMessagesToCache(sessionId, history);
        
        return history;
    }

    /**
     * 세션 종료
     */
    public void closeSession(String sessionId) {
        sessionRepository.findBySessionId(sessionId).ifPresent(session -> {
            session.setStatus(FactCheckChatSession.SessionStatus.COMPLETED);
            session.setEndedAt(LocalDateTime.now());
            sessionRepository.save(session);
            
            // 캐시 삭제
            evictSessionCache(sessionId);
            evictMessagesCache(sessionId);
            
            // 최종 동기화 트리거
            chatSyncService.syncSessionToRdb(session);
            
            // 메트릭 업데이트
            sessionClosedCounter.increment();
            activeSessionsGauge.decrementAndGet();
            
            // 처리 상태 정리
            processingSessions.remove(sessionId);
            
            log.info("Closed fact-check chat session: {}", sessionId);
        });
    }

    /**
     * 사용자별 세션 목록 조회
     */
    public List<FactCheckChatSession> getUserSessions(String userId) {
        return sessionRepository.findByUserIdOrderByStartedAtDesc(userId);
    }

    /**
     * 세션 상태 조회
     */
    public FactCheckChatSession.SessionStatus getSessionStatus(String sessionId) {
        return sessionRepository.findBySessionId(sessionId)
                .map(FactCheckChatSession::getStatus)
                .orElse(null);
    }

    // =====================
    // 캐시 관리 메서드
    // =====================

    private FactCheckChatSession getCachedSession(String sessionId) {
        try {
            var cache = cacheManager.getCache(CACHE_NAME_SESSIONS);
            if (cache != null) {
                var wrapper = cache.get(sessionId, FactCheckChatSession.class);
                return wrapper;
            }
        } catch (Exception e) {
            log.warn("Failed to get session from cache: {}", e.getMessage());
        }
        return null;
    }

    private void putSessionToCache(FactCheckChatSession session) {
        try {
            var cache = cacheManager.getCache(CACHE_NAME_SESSIONS);
            if (cache != null) {
                cache.put(session.getSessionId(), session);
            }
        } catch (Exception e) {
            log.warn("Failed to put session to cache: {}", e.getMessage());
        }
    }

    private void evictSessionCache(String sessionId) {
        try {
            var cache = cacheManager.getCache(CACHE_NAME_SESSIONS);
            if (cache != null) {
                cache.evict(sessionId);
            }
        } catch (Exception e) {
            log.warn("Failed to evict session from cache: {}", e.getMessage());
        }
    }

    @SuppressWarnings("unchecked")
    private List<ChatMessage> getCachedMessages(String sessionId) {
        try {
            var cache = cacheManager.getCache(CACHE_NAME_MESSAGES);
            if (cache != null) {
                return cache.get(sessionId, List.class);
            }
        } catch (Exception e) {
            log.warn("Failed to get messages from cache: {}", e.getMessage());
        }
        return null;
    }

    private void putMessagesToCache(String sessionId, List<ChatMessage> messages) {
        try {
            var cache = cacheManager.getCache(CACHE_NAME_MESSAGES);
            if (cache != null) {
                cache.put(sessionId, messages);
            }
        } catch (Exception e) {
            log.warn("Failed to put messages to cache: {}", e.getMessage());
        }
    }

    private void evictMessagesCache(String sessionId) {
        try {
            var cache = cacheManager.getCache(CACHE_NAME_MESSAGES);
            if (cache != null) {
                cache.evict(sessionId);
            }
        } catch (Exception e) {
            log.warn("Failed to evict messages from cache: {}", e.getMessage());
        }
    }

    /**
     * 챗봇 이벤트 DTO
     */
    @Data
    @Builder
    public static class ChatEvent {
        private String type;        // message, status, evidence, verification, assessment, ai_synthesis, complete, error
        private String role;        // user, assistant, system
        private String content;     // 메시지 내용
        private String phase;       // init, concepts, verification, assessment, synthesis, complete
        private Long timestamp;
        
        // 추가 데이터
        private Object evidence;
        private Object verificationResult;
        private Object credibility;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/FactVerificationService.java

```java
package com.newsinsight.collector.service;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.newsinsight.collector.client.PerplexityClient;
import com.newsinsight.collector.client.OpenAICompatibleClient;
import com.newsinsight.collector.client.AIDoveClient;
import com.newsinsight.collector.config.TrustScoreConfig;
import com.newsinsight.collector.service.factcheck.FactCheckSource;
import com.newsinsight.collector.service.factcheck.RRFEvidenceFusionService;
import com.newsinsight.collector.service.factcheck.RRFEvidenceFusionService.FusionResult;
import com.newsinsight.collector.service.search.AdvancedIntentAnalyzer;
import com.newsinsight.collector.service.search.AdvancedIntentAnalyzer.AnalyzedQuery;
import com.newsinsight.collector.service.search.AdvancedIntentAnalyzer.FallbackStrategy;
import lombok.Builder;
import lombok.Data;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.MediaType;
import org.springframework.stereotype.Service;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Mono;
import reactor.core.scheduler.Schedulers;

import java.net.URLEncoder;
import java.nio.charset.StandardCharsets;
import java.time.Duration;
import java.util.ArrayList;
import java.util.List;
import java.util.Set;
import java.util.UUID;
import java.util.concurrent.CopyOnWriteArrayList;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
import java.util.stream.Collectors;

/**
 * 심층 분석 신뢰성 검증 서비스
 * 
 * Wikipedia, 학술DB 등 신뢰할 수 있는 출처와 대조하여
 * 주장의 타당성을 검증합니다.
 */
@Service
@Slf4j
public class FactVerificationService {

    private final WebClient webClient;
    private final ObjectMapper objectMapper;
    private final PerplexityClient perplexityClient;
    private final OpenAICompatibleClient openAICompatibleClient;
    private final AIDoveClient aiDoveClient;
    private final List<FactCheckSource> factCheckSources;
    private final TrustScoreConfig trustScoreConfig;
    private final List<TrustedSource> trustedSources;
    private final AdvancedIntentAnalyzer advancedIntentAnalyzer;
    private final RRFEvidenceFusionService rrfFusionService;
    
    @Value("${collector.fact-check.rrf.enabled:true}")
    private boolean rrfEnabled;

    public FactVerificationService(
            WebClient webClient,
            ObjectMapper objectMapper,
            PerplexityClient perplexityClient,
            OpenAICompatibleClient openAICompatibleClient,
            AIDoveClient aiDoveClient,
            List<FactCheckSource> factCheckSources,
            TrustScoreConfig trustScoreConfig,
            AdvancedIntentAnalyzer advancedIntentAnalyzer,
            RRFEvidenceFusionService rrfFusionService) {
        this.webClient = webClient;
        this.objectMapper = objectMapper;
        this.perplexityClient = perplexityClient;
        this.openAICompatibleClient = openAICompatibleClient;
        this.aiDoveClient = aiDoveClient;
        this.factCheckSources = factCheckSources;
        this.trustScoreConfig = trustScoreConfig;
        this.advancedIntentAnalyzer = advancedIntentAnalyzer;
        this.rrfFusionService = rrfFusionService;
        
        // Initialize trusted sources with externalized scores
        this.trustedSources = initializeTrustedSources();
        
        log.info("FactVerificationService initialized with {} sources: {}", 
                factCheckSources.size(),
                factCheckSources.stream()
                        .map(s -> s.getSourceId() + (s.isAvailable() ? " (active)" : " (disabled)"))
                        .collect(Collectors.joining(", ")));
    }

    private List<TrustedSource> initializeTrustedSources() {
        TrustScoreConfig.TrustedSources ts = trustScoreConfig.getTrusted();
        return List.of(
                new TrustedSource("wikipedia", "위키백과", "https://ko.wikipedia.org/wiki/", ts.getWikipediaKo()),
                new TrustedSource("wikipedia_en", "Wikipedia", "https://en.wikipedia.org/wiki/", ts.getWikipediaEn()),
                new TrustedSource("britannica", "브리태니커", "https://www.britannica.com/search?query=", ts.getBritannica()),
                new TrustedSource("namu", "나무위키", "https://namu.wiki/w/", ts.getNamuWiki()),
                new TrustedSource("kosis", "통계청", "https://kosis.kr/search/search.do?query=", ts.getKosis()),
                new TrustedSource("scholar", "학술 자료", "https://scholar.google.com/scholar?q=", ts.getGoogleScholar())
        );
    }

    @Value("${collector.crawler.base-url:http://web-crawler:11235}")
    private String crawlerBaseUrl;

    @Value("${collector.fact-check.timeout-seconds:30}")
    private int timeoutSeconds;

    // ============================================
    // DTO Classes
    // ============================================

    @Data
    @Builder
    public static class VerificationResult {
        private String claimId;
        private String originalClaim;       // 원본 주장
        private VerificationStatus status;  // 검증 상태
        private Double confidenceScore;     // 신뢰도 점수 (0-1)
        private List<SourceEvidence> supportingEvidence;    // 지지 근거
        private List<SourceEvidence> contradictingEvidence; // 반박 근거
        private String verificationSummary; // 검증 요약
        private List<String> relatedConcepts; // 관련 개념
    }

    public enum VerificationStatus {
        VERIFIED,           // 검증됨 (신뢰할 수 있는 출처에서 확인)
        PARTIALLY_VERIFIED, // 부분 검증됨
        UNVERIFIED,         // 검증 불가 (정보 부족)
        DISPUTED,           // 논쟁 중 (상반된 정보 존재)
        FALSE               // 거짓으로 판명
    }

    @Data
    @Builder
    public static class SourceEvidence {
        private String sourceType;      // wikipedia, scholar, news 등
        private String sourceName;      // 출처 이름
        private String url;             // URL
        private String excerpt;         // 관련 발췌문
        private Double relevanceScore;  // 관련성 점수
        private String stance;          // support, contradict, neutral
    }

    @Data
    @Builder
    public static class DeepAnalysisResult {
        private String topic;
        private List<VerificationResult> verifiedClaims;
        private ConceptMap conceptMap;          // 개념 관계도
        private List<String> keyInsights;       // 핵심 인사이트
        private CredibilityAssessment credibility; // 전체 신뢰도 평가
        private String finalConclusion;         // 최종 결론
    }

    @Data
    @Builder
    public static class ConceptMap {
        private String mainTopic;
        private List<RelatedConcept> relatedConcepts;
        private List<ConceptLink> links;
    }

    @Data
    @Builder
    public static class RelatedConcept {
        private String name;
        private String description;
        private String wikiUrl;
        private Double relevance;
    }

    @Data
    @Builder
    public static class ConceptLink {
        private String from;
        private String to;
        private String relationship;
    }

    @Data
    @Builder
    public static class CredibilityAssessment {
        private Double overallScore;        // 전체 신뢰도 (0-1)
        private Integer verifiedCount;      // 검증된 주장 수
        private Integer totalClaims;        // 전체 주장 수
        private String riskLevel;           // low, medium, high
        private List<String> warnings;      // 주의사항
    }

    private record TrustedSource(String id, String name, String searchUrl, double trustScore) {}

    // ============================================
    // Main Verification Methods
    // ============================================

    /**
     * 매우 단순한 언어 감지: 영문 알파벳이 포함되어 있으면 영어(en),
     * 그렇지 않으면 기본적으로 한국어(ko)로 간주.
     */
    private String detectLanguage(String text) {
        if (text == null || text.isBlank()) {
            return "ko";
        }
        for (int i = 0; i < text.length(); i++) {
            char c = text.charAt(i);
            if ((c >= 'A' && c <= 'Z') || (c >= 'a' && c <= 'z')) {
                return "en";
            }
        }
        return "ko";
    }

    /**
     * Claim 목록을 하나로 합쳐서 기준 텍스트를 만들고,
     * evidence.excerpt 와의 향상된 유사도를 이용해 의미 있는 근거만 남긴다.
     * 
     * 개선사항:
     * - 더 낮은 임계값으로 더 많은 근거 수집
     * - 학술 소스에 대해서는 더 관대한 필터링
     * - 키워드 매칭 기반의 추가 필터링
     */
    private List<SourceEvidence> filterEvidenceForClaims(List<SourceEvidence> allEvidence, List<String> claims) {
        if (allEvidence == null || allEvidence.isEmpty()) {
            return List.of();
        }
        if (claims == null || claims.isEmpty()) {
            // Claim 정보가 없으면 필터링 없이 그대로 사용
            return new ArrayList<>(allEvidence);
        }

        String combinedClaims = claims.stream()
                .filter(c -> c != null && !c.isBlank())
                .collect(Collectors.joining(" "));
        if (combinedClaims.isBlank()) {
            return new ArrayList<>(allEvidence);
        }

        // 주장에서 핵심 키워드 추출
        List<String> claimKeywords = extractKeywords(combinedClaims);

        List<SourceEvidence> filtered = new ArrayList<>();
        for (SourceEvidence evidence : allEvidence) {
            if (evidence == null || evidence.getExcerpt() == null || evidence.getExcerpt().isBlank()) {
                continue;
            }
            
            // 자카드 유사도 계산
            double sim = calculateSimilarity(combinedClaims, evidence.getExcerpt());
            
            // 키워드 매칭 점수 계산
            double keywordScore = calculateKeywordMatchScore(claimKeywords, evidence.getExcerpt());
            
            // 학술 소스는 더 관대하게 필터링 (학술 DB는 기본적으로 신뢰할 수 있음)
            double threshold = "academic".equals(evidence.getSourceType()) ? 0.05 : 0.08;
            
            // 유사도 또는 키워드 매칭 중 하나라도 임계값 이상이면 포함
            if (sim >= threshold || keywordScore >= 0.2) {
                // 종합 점수로 relevanceScore 업데이트
                double combinedScore = Math.max(sim, keywordScore);
                if (evidence.getRelevanceScore() == null || evidence.getRelevanceScore() < combinedScore) {
                    evidence.setRelevanceScore(combinedScore);
                }
                filtered.add(evidence);
            }
        }

        // 관련성 점수로 정렬
        filtered.sort((a, b) -> Double.compare(
                b.getRelevanceScore() != null ? b.getRelevanceScore() : 0,
                a.getRelevanceScore() != null ? a.getRelevanceScore() : 0
        ));

        // 상위 결과만 사용 (최대 60개)
        if (filtered.size() > 60) {
            return filtered.subList(0, 60);
        }
        return filtered;
    }
    
    /**
     * 키워드 매칭 점수 계산
     */
    private double calculateKeywordMatchScore(List<String> keywords, String text) {
        if (keywords == null || keywords.isEmpty() || text == null || text.isBlank()) {
            return 0.0;
        }
        
        String lowerText = text.toLowerCase();
        int matchCount = 0;
        
        for (String keyword : keywords) {
            if (lowerText.contains(keyword.toLowerCase())) {
                matchCount++;
            }
        }
        
        return (double) matchCount / keywords.size();
    }

    /**
     * 주어진 주제에 대해 심층 분석 및 검증 수행
     */
    public Flux<DeepAnalysisEvent> analyzeAndVerify(String topic, List<String> claims) {
        log.info("Starting deep analysis and verification for topic: {}", topic);

        // Advanced Intent Analysis for better search strategies
        AnalyzedQuery analyzedTopic = advancedIntentAnalyzer.analyzeQuery(topic);
        log.info("Topic analyzed: keywords={}, primary='{}', intent={}, strategies={}",
                analyzedTopic.getKeywords().size(),
                analyzedTopic.getPrimaryKeyword(),
                analyzedTopic.getIntentType(),
                analyzedTopic.getFallbackStrategies().size());

        // 간단한 언어 감지 (영문 알파벳 포함 여부 기준)
        String language = analyzedTopic.getLanguage();

        return Flux.create(sink -> {
            // 1. 시작 이벤트
            sink.next(DeepAnalysisEvent.builder()
                    .eventType("status")
                    .phase("init")
                    .message("심층 분석을 시작합니다: " + topic)
                    .build());

            // 2. 관련 개념 수집
            sink.next(DeepAnalysisEvent.builder()
                    .eventType("status")
                    .phase("concepts")
                    .message("관련 개념을 수집하고 있습니다...")
                    .build());

            // 병렬로 모든 신뢰할 수 있는 소스에서 정보 수집 (폴백 전략 포함)
            List<SourceEvidence> allEvidence = fetchAllSourceEvidenceWithFallback(analyzedTopic, language);

            // Claim 정보가 있다면, claim과의 유사도 기반으로 근거를 1차 필터링
            List<SourceEvidence> filteredEvidence = filterEvidenceForClaims(allEvidence, claims);

            if (!filteredEvidence.isEmpty()) {
                // 소스별 통계 생성
                var sourceStats = filteredEvidence.stream()
                        .collect(Collectors.groupingBy(
                                SourceEvidence::getSourceType,
                                Collectors.counting()));
                String statsMessage = sourceStats.entrySet().stream()
                        .map(e -> e.getKey() + ": " + e.getValue() + "개")
                        .collect(Collectors.joining(", "));
                
                sink.next(DeepAnalysisEvent.builder()
                        .eventType("evidence")
                        .phase("concepts")
                        .message("신뢰할 수 있는 출처에서 " + filteredEvidence.size() + "개의 유의미한 근거를 수집했습니다. (" + statsMessage + ")")
                        .evidence(filteredEvidence)
                        .build());
            } else {
                // 결과가 없을 때 도움말 메시지
                String noResultMessage = advancedIntentAnalyzer.buildNoResultMessage(analyzedTopic);
                sink.next(DeepAnalysisEvent.builder()
                        .eventType("status")
                        .phase("concepts")
                        .message("관련 근거를 찾기 어려웠습니다.\n" + noResultMessage)
                        .build());
            }

            // 3. 각 주장에 대한 검증 (향상된 키워드 매칭)
            final List<VerificationResult> verificationResults = new ArrayList<>();
            final CredibilityAssessment[] credibilityHolder = new CredibilityAssessment[1];
            
            if (claims != null && !claims.isEmpty()) {
                sink.next(DeepAnalysisEvent.builder()
                        .eventType("status")
                        .phase("verification")
                        .message(claims.size() + "개의 주장을 검증하고 있습니다...")
                        .build());
                
                for (int i = 0; i < claims.size(); i++) {
                    String claim = claims.get(i);
                    // 향상된 claim 검증
                    VerificationResult result = verifyClaimWithIntentAnalysis(claim, filteredEvidence);
                    verificationResults.add(result);

                    sink.next(DeepAnalysisEvent.builder()
                            .eventType("verification")
                            .phase("verification")
                            .message("주장 " + (i + 1) + "/" + claims.size() + " 검증 완료")
                            .verificationResult(result)
                            .build());
                }

                // 4. 신뢰도 평가
                credibilityHolder[0] = assessCredibility(verificationResults);
                
                sink.next(DeepAnalysisEvent.builder()
                        .eventType("assessment")
                        .phase("assessment")
                        .message("신뢰도 평가 완료")
                        .credibility(credibilityHolder[0])
                        .build());
            }

            // 5. AI 기반 종합 분석 (Fallback Chain)
            int evidenceCount = filteredEvidence.size();
            
            // 증거 수에 따른 경고 메시지 생성
            String synthesisStatusMessage;
            if (evidenceCount == 0) {
                synthesisStatusMessage = "⚠️ 신뢰할 수 있는 출처에서 관련 정보를 찾지 못했습니다. 제한된 분석을 진행합니다...";
                log.warn("No evidence found for topic: {}. AI may refuse to generate content.", topic);
            } else if (evidenceCount < 3) {
                synthesisStatusMessage = "⚠️ 수집된 정보가 제한적입니다 (" + evidenceCount + "개). 제한된 분석을 진행합니다...";
                log.info("Limited evidence ({}) found for topic: {}", evidenceCount, topic);
            } else {
                synthesisStatusMessage = "AI가 수집된 " + evidenceCount + "개의 정보를 종합 분석하고 있습니다...";
            }
            
            sink.next(DeepAnalysisEvent.builder()
                    .eventType("status")
                    .phase("synthesis")
                    .message(synthesisStatusMessage)
                    .build());

            // Build provider chain and try each in sequence
            String synthesisPrompt = buildSynthesisPrompt(topic, filteredEvidence, claims);
            StringBuilder aiResponse = new StringBuilder();

            // Try AI providers in order of preference
            Flux<String> aiStream = getAiStreamWithFallback(synthesisPrompt);
            
            aiStream
                    .doOnNext(chunk -> {
                        aiResponse.append(chunk);
                        sink.next(DeepAnalysisEvent.builder()
                                .eventType("ai_synthesis")
                                .phase("synthesis")
                                .message(chunk)
                                .build());
                    })
                    .doOnComplete(() -> {
                        String conclusion = aiResponse.toString();
                        if (conclusion.isBlank()) {
                            conclusion = buildFallbackConclusion(topic, verificationResults, credibilityHolder[0]);
                        }
                        sink.next(DeepAnalysisEvent.builder()
                                .eventType("complete")
                                .phase("complete")
                                .message("심층 분석이 완료되었습니다.")
                                .finalConclusion(conclusion)
                                .build());
                        sink.complete();
                    })
                    .doOnError(e -> {
                        log.error("All AI providers failed: {}", e.getMessage());
                        // Generate fallback conclusion without AI
                        String fallbackConclusion = buildFallbackConclusion(topic, verificationResults, credibilityHolder[0]);
                        sink.next(DeepAnalysisEvent.builder()
                                .eventType("complete")
                                .phase("complete")
                                .message("분석이 완료되었습니다.")
                                .finalConclusion(fallbackConclusion)
                                .build());
                        sink.complete();
                    })
                    .subscribe();
        });
    }

    /**
     * Get AI stream with fallback chain.
     * Tries providers in order: Perplexity -> OpenAI -> OpenRouter -> Azure -> AI Dove -> Ollama
     */
    private Flux<String> getAiStreamWithFallback(String prompt) {
        List<AiProviderAttempt> providers = buildAiProviderChain(prompt);
        
        if (providers.isEmpty()) {
            log.warn("No AI providers available, returning empty stream");
            return Flux.empty();
        }

        log.info("AI synthesis using fallback chain: {}", 
                providers.stream().map(AiProviderAttempt::name).toList());

        return tryAiProvidersInSequence(providers, 0);
    }

    /**
     * Build the AI provider chain based on availability
     */
    private List<AiProviderAttempt> buildAiProviderChain(String prompt) {
        List<AiProviderAttempt> chain = new ArrayList<>();

        // 1. Perplexity - Best for fact-checking with online search
        if (perplexityClient.isEnabled()) {
            chain.add(new AiProviderAttempt("Perplexity", () -> perplexityClient.streamCompletion(prompt)));
        }

        // 2. OpenAI
        if (openAICompatibleClient.isOpenAIEnabled()) {
            chain.add(new AiProviderAttempt("OpenAI", () -> openAICompatibleClient.streamFromOpenAI(prompt)));
        }

        // 3. OpenRouter - Access to multiple models
        if (openAICompatibleClient.isOpenRouterEnabled()) {
            chain.add(new AiProviderAttempt("OpenRouter", () -> openAICompatibleClient.streamFromOpenRouter(prompt)));
        }

        // 4. Azure OpenAI
        if (openAICompatibleClient.isAzureEnabled()) {
            chain.add(new AiProviderAttempt("Azure", () -> openAICompatibleClient.streamFromAzure(prompt)));
        }

        // 5. AI Dove (n8n webhook) - Simulated streaming
        if (aiDoveClient.isEnabled()) {
            chain.add(new AiProviderAttempt("AI Dove", () -> aiDoveClient.chatStream(prompt, null)));
        }

        // 6. Ollama - Local LLM (always in chain, may fail if not running)
        chain.add(new AiProviderAttempt("Ollama", () -> openAICompatibleClient.streamFromOllama(prompt)));

        // 7. Custom endpoint
        if (openAICompatibleClient.isCustomEnabled()) {
            chain.add(new AiProviderAttempt("Custom", () -> openAICompatibleClient.streamFromCustom(prompt)));
        }

        return chain;
    }

    /**
     * Try AI providers in sequence until one succeeds
     */
    private Flux<String> tryAiProvidersInSequence(List<AiProviderAttempt> providers, int index) {
        if (index >= providers.size()) {
            log.warn("All AI providers exhausted");
            return Flux.empty();
        }

        AiProviderAttempt current = providers.get(index);
        log.info("Trying AI provider: {} ({}/{})", current.name(), index + 1, providers.size());

        return current.streamSupplier().get()
                .timeout(Duration.ofSeconds(90))
                .onErrorResume(e -> {
                    log.warn("AI provider {} failed: {}. Trying next...", current.name(), e.getMessage());
                    return tryAiProvidersInSequence(providers, index + 1);
                })
                .switchIfEmpty(Flux.defer(() -> {
                    log.warn("AI provider {} returned empty. Trying next...", current.name());
                    return tryAiProvidersInSequence(providers, index + 1);
                }));
    }

    /**
     * Build a fallback conclusion when AI is not available
     */
    private String buildFallbackConclusion(String topic, List<VerificationResult> results, CredibilityAssessment credibility) {
        StringBuilder sb = new StringBuilder();
        sb.append("## ").append(topic).append(" 분석 결과\n\n");
        
        if (results == null || results.isEmpty()) {
            // 증거가 전혀 없는 경우 - 명확한 "정보 없음" 메시지
            sb.append("""
                ### ⚠️ 검색 결과 없음
                
                죄송합니다. 이 주제에 대해 신뢰할 수 있는 출처에서 관련 정보를 찾을 수 없었습니다.
                
                **가능한 이유:**
                - 해당 주제가 존재하지 않거나 잘못된 정보일 수 있습니다
                - 아직 널리 알려지지 않은 주제일 수 있습니다
                - 검색어를 다르게 입력해 보시기 바랍니다
                
                **주의**: 확인되지 않은 정보는 제공하지 않습니다.
                """);
        } else {
            sb.append("### 검증 결과 요약\n\n");
            int verified = 0, unverified = 0, contradicted = 0;
            for (VerificationResult r : results) {
                if (r.getStatus() == null) continue;
                switch (r.getStatus()) {
                    case VERIFIED, PARTIALLY_VERIFIED -> verified++;
                    case UNVERIFIED -> unverified++;
                    case DISPUTED, FALSE -> contradicted++;
                }
            }
            sb.append(String.format("- ✅ 검증됨: %d건\n", verified));
            sb.append(String.format("- ❓ 미확인: %d건\n", unverified));
            sb.append(String.format("- ❌ 반박됨: %d건\n\n", contradicted));
            
            // 미확인 비율이 높을 경우 경고
            int total = verified + unverified + contradicted;
            if (total > 0 && (double) unverified / total > 0.5) {
                sb.append("⚠️ **주의**: 대부분의 주장이 확인되지 않았습니다. 추가 검증이 필요합니다.\n\n");
            }
        }

        if (credibility != null) {
            sb.append("### 신뢰도 평가\n");
            sb.append(String.format("- 전체 신뢰도: %.0f%%\n", credibility.getOverallScore() * 100));
            sb.append(String.format("- 위험 수준: %s\n", credibility.getRiskLevel()));
            
            if (credibility.getWarnings() != null && !credibility.getWarnings().isEmpty()) {
                sb.append("\n### ⚠️ 주의사항\n");
                for (String warning : credibility.getWarnings()) {
                    sb.append("- ").append(warning).append("\n");
                }
            }
        }

        sb.append("\n---\n*이 결과는 수집된 정보에만 기반합니다. 추가 검증을 권장합니다.*");
        return sb.toString();
    }

    /**
     * AI provider attempt wrapper
     */
    private record AiProviderAttempt(
            String name,
            java.util.function.Supplier<Flux<String>> streamSupplier
    ) {}

    // ============================================
    // Enhanced Evidence Collection with Fallback
    // ============================================

    /**
     * RRF 기반 다중 쿼리 병렬 검색으로 근거 수집
     * 
     * 의도 분석을 통해 생성된 여러 검색 쿼리를 병렬로 실행하고,
     * RRF 알고리즘을 사용하여 결과를 융합합니다.
     */
    private List<SourceEvidence> fetchAllSourceEvidenceWithFallback(AnalyzedQuery analyzedQuery, String language) {
        List<SourceEvidence> allEvidence = new CopyOnWriteArrayList<>();
        
        // RRF 기반 다중 쿼리 병렬 검색 사용
        if (rrfEnabled && rrfFusionService != null) {
            try {
                log.info("Using RRF-based multi-query parallel search for: {}", analyzedQuery.getOriginalQuery());
                
                FusionResult fusionResult = rrfFusionService
                        .searchAndFuse(analyzedQuery.getOriginalQuery(), language)
                        .block(Duration.ofSeconds(timeoutSeconds * 2));
                
                if (fusionResult != null && fusionResult.getEvidences() != null) {
                    allEvidence.addAll(fusionResult.getEvidences());
                    log.info("RRF search completed: {} queries × {} sources → {} evidences (method: {})",
                            fusionResult.getQueryCount(),
                            fusionResult.getSourceCount(),
                            fusionResult.getEvidences().size(),
                            fusionResult.getFusionMethod());
                }
            } catch (Exception e) {
                log.warn("RRF search failed, falling back to sequential search: {}", e.getMessage());
                // RRF 실패 시 기존 방식으로 폴백
                allEvidence.addAll(fetchAllSourceEvidenceSequential(analyzedQuery, language));
            }
        } else {
            // RRF 비활성화 시 기존 방식 사용
            allEvidence.addAll(fetchAllSourceEvidenceSequential(analyzedQuery, language));
        }
        
        // Wikipedia 정보 추가 (항상 포함)
        List<SourceEvidence> wikiEvidence = fetchWikipediaInfo(analyzedQuery.getOriginalQuery());
        for (SourceEvidence wiki : wikiEvidence) {
            boolean isDuplicate = allEvidence.stream()
                    .anyMatch(e -> e.getUrl() != null && e.getUrl().equals(wiki.getUrl()));
            if (!isDuplicate) {
                allEvidence.add(wiki);
            }
        }
        
        log.info("Total evidence collected: {} items", allEvidence.size());
        return new ArrayList<>(allEvidence);
    }
    
    /**
     * 기존 순차적 폴백 검색 방식 (RRF 비활성화 시 또는 폴백용)
     */
    private List<SourceEvidence> fetchAllSourceEvidenceSequential(AnalyzedQuery analyzedQuery, String language) {
        List<SourceEvidence> allEvidence = new CopyOnWriteArrayList<>();
        
        // 원본 쿼리로 먼저 시도
        String currentQuery = analyzedQuery.getOriginalQuery();
        allEvidence.addAll(fetchAllSourceEvidence(currentQuery, language));
        
        // 결과가 부족하면 폴백 전략 사용
        if (allEvidence.size() < 3 && analyzedQuery.getFallbackStrategies() != null) {
            int maxAttempts = Math.min(3, analyzedQuery.getFallbackStrategies().size());
            
            for (int i = 0; i < maxAttempts && allEvidence.size() < 5; i++) {
                FallbackStrategy strategy = analyzedQuery.getFallbackStrategies().get(i);
                log.info("Fact verification fallback attempt {}: strategy='{}', query='{}'", 
                        i + 1, strategy.getStrategyType(), strategy.getQuery());
                
                List<SourceEvidence> fallbackEvidence = fetchAllSourceEvidence(strategy.getQuery(), language);
                
                // 중복 제거하며 추가
                for (SourceEvidence evidence : fallbackEvidence) {
                    boolean isDuplicate = allEvidence.stream()
                            .anyMatch(e -> e.getUrl() != null && e.getUrl().equals(evidence.getUrl()));
                    if (!isDuplicate) {
                        allEvidence.add(evidence);
                    }
                }
            }
        }
        
        return new ArrayList<>(allEvidence);
    }

    /**
     * 향상된 Claim 검증 - Intent Analysis 사용
     */
    private VerificationResult verifyClaimWithIntentAnalysis(String claim, List<SourceEvidence> backgroundEvidence) {
        // Claim에 대한 의도 분석
        AnalyzedQuery analyzedClaim = advancedIntentAnalyzer.analyzeQuery(claim);
        List<String> keywords = analyzedClaim.getKeywords();
        String primaryKeyword = analyzedClaim.getPrimaryKeyword();

        // 배경 증거와 대조
        List<SourceEvidence> supporting = new ArrayList<>();
        List<SourceEvidence> contradicting = new ArrayList<>();

        for (SourceEvidence evidence : backgroundEvidence) {
            // 향상된 유사도 계산 - 키워드 매칭 포함
            double similarity = calculateEnhancedSimilarity(claim, evidence.getExcerpt(), keywords, primaryKeyword);
            
            if (similarity > 0.25) {  // 낮은 임계값으로 더 많은 매칭
                evidence.setRelevanceScore(similarity);
                
                // 감성 분석으로 지지/반박 구분
                if (containsContradiction(claim, evidence.getExcerpt())) {
                    evidence.setStance("contradict");
                    contradicting.add(evidence);
                } else {
                    evidence.setStance("support");
                    supporting.add(evidence);
                }
            }
        }

        // 검증 상태 결정
        VerificationStatus status;
        double confidence;

        if (!supporting.isEmpty() && contradicting.isEmpty()) {
            status = VerificationStatus.VERIFIED;
            confidence = Math.min(0.6 + supporting.size() * 0.1, 0.95);
        } else if (!supporting.isEmpty() && !contradicting.isEmpty()) {
            status = VerificationStatus.DISPUTED;
            confidence = 0.5;
        } else if (supporting.isEmpty() && !contradicting.isEmpty()) {
            status = VerificationStatus.FALSE;
            confidence = 0.3;
        } else {
            status = VerificationStatus.UNVERIFIED;
            confidence = 0.4;
        }

        String summary = generateVerificationSummary(status, supporting.size(), contradicting.size());

        return VerificationResult.builder()
                .claimId(UUID.randomUUID().toString())
                .originalClaim(claim)
                .status(status)
                .confidenceScore(confidence)
                .supportingEvidence(supporting)
                .contradictingEvidence(contradicting)
                .verificationSummary(summary)
                .relatedConcepts(keywords)
                .build();
    }

    /**
     * 향상된 유사도 계산 - 키워드 매칭 + 자카드 유사도 결합
     */
    private double calculateEnhancedSimilarity(
            String claim, 
            String evidence, 
            List<String> keywords, 
            String primaryKeyword) {
        
        if (claim == null || evidence == null) return 0;
        
        String lowerClaim = claim.toLowerCase();
        String lowerEvidence = evidence.toLowerCase();
        
        double score = 0;
        
        // 1. 기본 자카드 유사도
        double jaccardScore = calculateSimilarity(claim, evidence);
        score += jaccardScore * 0.4;
        
        // 2. 주요 키워드 매칭 (높은 가중치)
        if (primaryKeyword != null && !primaryKeyword.isBlank() && 
                lowerEvidence.contains(primaryKeyword.toLowerCase())) {
            score += 0.3;
        }
        
        // 3. 기타 키워드 매칭
        if (keywords != null && !keywords.isEmpty()) {
            int matchCount = 0;
            for (String keyword : keywords) {
                if (lowerEvidence.contains(keyword.toLowerCase())) {
                    matchCount++;
                }
            }
            score += (double) matchCount / keywords.size() * 0.3;
        }
        
        return Math.min(score, 1.0);
    }

    // ============================================
    // Wikipedia & Trusted Source Fetching
    // ============================================

    /**
     * 모든 등록된 팩트체크 소스에서 병렬로 근거를 수집합니다.
     * 실시간 데이터가 필요한 쿼리의 경우 RealtimeSearchSource와 뉴스를 우선 처리합니다.
     */
    private List<SourceEvidence> fetchAllSourceEvidence(String topic, String language) {
        List<SourceEvidence> allEvidence = new CopyOnWriteArrayList<>();
        
        // 0. 실시간 검색이 필요한지 판단하고 우선 처리
        boolean needsRealtime = isRealtimeDataRequired(topic);
        if (needsRealtime) {
            log.info("Topic '{}' requires realtime data, prioritizing realtime search and news", topic);
            
            // 실시간 검색 우선 처리
            List<SourceEvidence> realtimeEvidence = fetchRealtimeEvidence(topic, language);
            if (!realtimeEvidence.isEmpty()) {
                allEvidence.addAll(realtimeEvidence);
                log.info("Fetched {} realtime evidence items", realtimeEvidence.size());
            }
            
            // 뉴스 소스 우선 처리 (최신 정보)
            List<SourceEvidence> newsEvidence = fetchNewsEvidence(topic, language);
            if (!newsEvidence.isEmpty()) {
                allEvidence.addAll(newsEvidence);
                log.info("Fetched {} news evidence items", newsEvidence.size());
            }
        }
        
        // 1. Wikipedia 정보 수집 (실시간 데이터가 아닌 경우 우선, 실시간인 경우 나중에)
        if (!needsRealtime) {
            List<SourceEvidence> wikiEvidence = fetchWikipediaInfo(topic);
            allEvidence.addAll(wikiEvidence);
        }
        
        // 2. 추가 팩트체크 소스에서 병렬 수집 (실시간 소스와 뉴스 제외 - 이미 처리됨)
        if (factCheckSources != null && !factCheckSources.isEmpty()) {
            List<Mono<List<SourceEvidence>>> sourceFetches = factCheckSources.stream()
                    .filter(FactCheckSource::isAvailable)
                    .filter(source -> {
                        String sourceId = source.getSourceId();
                        // 실시간 데이터 필요 시 이미 처리한 소스 제외
                        if (needsRealtime && ("realtime_search".equals(sourceId) || "naver_news".equals(sourceId))) {
                            return false;
                        }
                        return true;
                    })
                    .map(source -> {
                        log.debug("Fetching evidence from source: {}", source.getSourceId());
                        return source.fetchEvidence(topic, language)
                                .collectList()
                                .timeout(Duration.ofSeconds(timeoutSeconds))
                                .doOnNext(evidences -> 
                                    log.debug("Source {} returned {} evidences", 
                                            source.getSourceId(), evidences.size()))
                                .onErrorResume(e -> {
                                    log.warn("Failed to fetch from {}: {}", 
                                            source.getSourceId(), e.getMessage());
                                    return Mono.just(List.of());
                                });
                    })
                    .toList();
            
            if (!sourceFetches.isEmpty()) {
                try {
                    List<List<SourceEvidence>> results = Flux.merge(sourceFetches)
                            .collectList()
                            .block(Duration.ofSeconds(timeoutSeconds * 2));
                    
                    if (results != null) {
                        for (List<SourceEvidence> evidences : results) {
                            allEvidence.addAll(evidences);
                        }
                    }
                } catch (Exception e) {
                    log.warn("Error during parallel evidence fetch: {}", e.getMessage());
                }
            }
        }
        
        // 3. Wikipedia 정보 추가 (실시간 데이터인 경우 마지막에 추가)
        if (needsRealtime) {
            List<SourceEvidence> wikiEvidence = fetchWikipediaInfo(topic);
            allEvidence.addAll(wikiEvidence);
        }
        
        log.info("Collected total {} evidence items for topic: {}", allEvidence.size(), topic);
        return new ArrayList<>(allEvidence);
    }
    
    /**
     * 실시간 데이터가 필요한 주제인지 판단
     * 
     * 기존 키워드 매칭의 한계를 극복하기 위해 AdvancedIntentAnalyzer의
     * 의미 기반 분석을 사용합니다. (LLM + 휴리스틱 + 의미 패턴)
     * 
     * 이를 통해:
     * - 새로운 암호화폐/자산 이름도 감지
     * - "X가 얼마야?" 같은 패턴 인식
     * - 문맥에서 시간 민감성 추론
     */
    private boolean isRealtimeDataRequired(String topic) {
        if (topic == null) return false;
        
        // AdvancedIntentAnalyzer의 의미 기반 분석 사용
        var realtimeAnalysis = advancedIntentAnalyzer.analyzeRealtimeDataNeed(topic);
        
        if (realtimeAnalysis.isNeedsRealtimeData()) {
            log.info("Realtime data required for '{}': type={}, confidence={}, reason={}",
                    topic, 
                    realtimeAnalysis.getDataType(),
                    String.format("%.2f", realtimeAnalysis.getConfidence()),
                    realtimeAnalysis.getReason());
            return true;
        }
        
        return false;
    }
    
    /**
     * 실시간 검색 소스에서 증거 수집
     */
    private List<SourceEvidence> fetchRealtimeEvidence(String topic, String language) {
        if (factCheckSources == null) return List.of();
        
        return factCheckSources.stream()
                .filter(source -> "realtime_search".equals(source.getSourceId()))
                .filter(FactCheckSource::isAvailable)
                .findFirst()
                .map(source -> {
                    try {
                        return source.fetchEvidence(topic, language)
                                .collectList()
                                .timeout(Duration.ofSeconds(timeoutSeconds))
                                .block();
                    } catch (Exception e) {
                        log.warn("Failed to fetch realtime evidence: {}", e.getMessage());
                        return List.<SourceEvidence>of();
                    }
                })
                .orElse(List.of());
    }
    
    /**
     * 뉴스 소스에서 증거 수집
     */
    private List<SourceEvidence> fetchNewsEvidence(String topic, String language) {
        if (factCheckSources == null) return List.of();
        
        return factCheckSources.stream()
                .filter(source -> "naver_news".equals(source.getSourceId()))
                .filter(FactCheckSource::isAvailable)
                .findFirst()
                .map(source -> {
                    try {
                        return source.fetchEvidence(topic, language)
                                .collectList()
                                .timeout(Duration.ofSeconds(timeoutSeconds))
                                .block();
                    } catch (Exception e) {
                        log.warn("Failed to fetch news evidence: {}", e.getMessage());
                        return List.<SourceEvidence>of();
                    }
                })
                .orElse(List.of());
    }

    private List<SourceEvidence> fetchWikipediaInfo(String topic) {
        List<SourceEvidence> evidenceList = new ArrayList<>();

        // 한국어 위키백과
        try {
            String koWikiContent = fetchWikipediaContent(topic, "ko");
            if (koWikiContent != null && !koWikiContent.isBlank()) {
                evidenceList.add(SourceEvidence.builder()
                        .sourceType("wikipedia")
                        .sourceName("위키백과")
                        .url("https://ko.wikipedia.org/wiki/" + URLEncoder.encode(topic, StandardCharsets.UTF_8))
                        .excerpt(truncateContent(koWikiContent, 500))
                        .relevanceScore(0.9)
                        .stance("neutral")
                        .build());
            }
        } catch (Exception e) {
            log.debug("Failed to fetch Korean Wikipedia: {}", e.getMessage());
        }

        // 영어 위키백과
        try {
            String enWikiContent = fetchWikipediaContent(topic, "en");
            if (enWikiContent != null && !enWikiContent.isBlank()) {
                evidenceList.add(SourceEvidence.builder()
                        .sourceType("wikipedia")
                        .sourceName("Wikipedia (EN)")
                        .url("https://en.wikipedia.org/wiki/" + URLEncoder.encode(topic, StandardCharsets.UTF_8))
                        .excerpt(truncateContent(enWikiContent, 500))
                        .relevanceScore(0.9)
                        .stance("neutral")
                        .build());
            }
        } catch (Exception e) {
            log.debug("Failed to fetch English Wikipedia: {}", e.getMessage());
        }

        return evidenceList;
    }

    private String fetchWikipediaContent(String topic, String lang) {
        try {
            String apiUrl = String.format(
                    "https://%s.wikipedia.org/api/rest_v1/page/summary/%s",
                    lang,
                    URLEncoder.encode(topic.replace(" ", "_"), StandardCharsets.UTF_8)
            );

            String response = webClient.get()
                    .uri(apiUrl)
                    .accept(MediaType.APPLICATION_JSON)
                    .retrieve()
                    .bodyToMono(String.class)
                    .timeout(Duration.ofSeconds(timeoutSeconds))
                    .block();

            if (response != null) {
                JsonNode node = objectMapper.readTree(response);
                if (node.has("extract")) {
                    return node.get("extract").asText();
                }
            }
        } catch (Exception e) {
            log.debug("Wikipedia API call failed for topic '{}' ({}): {}", topic, lang, e.getMessage());
        }
        return null;
    }

    // ============================================
    // Claim Verification
    // ============================================

    private VerificationResult verifyClaim(String claim, List<SourceEvidence> backgroundEvidence) {
        // 주장에서 핵심 키워드 추출
        List<String> keywords = extractKeywords(claim);

        // 배경 증거와 대조
        List<SourceEvidence> supporting = new ArrayList<>();
        List<SourceEvidence> contradicting = new ArrayList<>();

        for (SourceEvidence evidence : backgroundEvidence) {
            double similarity = calculateSimilarity(claim, evidence.getExcerpt());
            if (similarity > 0.3) {
                evidence.setRelevanceScore(similarity);
                // 간단한 감성 분석으로 지지/반박 구분 (실제로는 더 정교한 분석 필요)
                if (containsContradiction(claim, evidence.getExcerpt())) {
                    evidence.setStance("contradict");
                    contradicting.add(evidence);
                } else {
                    evidence.setStance("support");
                    supporting.add(evidence);
                }
            }
        }

        // 검증 상태 결정
        VerificationStatus status;
        double confidence;

        if (!supporting.isEmpty() && contradicting.isEmpty()) {
            status = VerificationStatus.VERIFIED;
            confidence = 0.8;
        } else if (!supporting.isEmpty() && !contradicting.isEmpty()) {
            status = VerificationStatus.DISPUTED;
            confidence = 0.5;
        } else if (supporting.isEmpty() && !contradicting.isEmpty()) {
            status = VerificationStatus.FALSE;
            confidence = 0.3;
        } else {
            status = VerificationStatus.UNVERIFIED;
            confidence = 0.4;
        }

        String summary = generateVerificationSummary(status, supporting.size(), contradicting.size());

        return VerificationResult.builder()
                .claimId(UUID.randomUUID().toString())
                .originalClaim(claim)
                .status(status)
                .confidenceScore(confidence)
                .supportingEvidence(supporting)
                .contradictingEvidence(contradicting)
                .verificationSummary(summary)
                .relatedConcepts(keywords)
                .build();
    }

    private List<String> extractKeywords(String text) {
        // 개선된 키워드 추출 - 명사 및 중요 단어 추출
        List<String> keywords = new ArrayList<>();
        String[] words = text.split("[\\s,\\.\\?!\\(\\)\\[\\]\"']+");
        
        for (String word : words) {
            String cleaned = word.trim().toLowerCase();
            // 최소 2글자 이상, 불용어 제외, 숫자만 있는 것 제외
            if (cleaned.length() >= 2 && !isStopWord(cleaned) && !cleaned.matches("^\\d+$")) {
                keywords.add(cleaned);
            }
        }
        
        // 중복 제거 및 우선순위 정렬 (긴 단어가 더 의미있을 가능성)
        return keywords.stream()
                .distinct()
                .sorted((a, b) -> Integer.compare(b.length(), a.length()))
                .limit(8)
                .toList();
    }

    private boolean isStopWord(String word) {
        return STOPWORDS.contains(word.toLowerCase());
    }
    
    // 확장된 불용어 목록
    private static final Set<String> STOPWORDS = Set.of(
            // 영어 불용어
            "the", "a", "an", "is", "are", "was", "were", "be", "been", "being",
            "have", "has", "had", "do", "does", "did", "will", "would", "could",
            "should", "may", "might", "must", "shall", "can", "need", "dare",
            "ought", "used", "to", "of", "in", "for", "on", "with", "at", "by",
            "from", "as", "into", "through", "during", "before", "after", "above",
            "below", "between", "under", "again", "further", "then", "once",
            "here", "there", "when", "where", "why", "how", "all", "each", "few",
            "more", "most", "other", "some", "such", "no", "nor", "not", "only",
            "own", "same", "so", "than", "too", "very", "just", "also", "now",
            "and", "but", "or", "if", "because", "until", "while", "about",
            "this", "that", "these", "those", "what", "which", "who", "whom",
            "it", "its", "they", "them", "their", "we", "us", "our", "you", "your",
            "he", "him", "his", "she", "her", "i", "me", "my",
            // 한국어 불용어
            "이", "그", "저", "는", "은", "가", "를", "을", "에", "의", "와", "과",
            "도", "만", "로", "으로", "에서", "까지", "부터", "에게", "한테",
            "것", "수", "등", "들", "및", "더", "덜", "뭐", "어디", "언제",
            "어떻게", "왜", "누구", "있다", "없다", "하다", "되다", "이다",
            "그리고", "그러나", "하지만", "그래서", "때문에", "대해", "대한",
            "관련", "관한", "통해", "위해", "따라", "인해", "있는", "없는",
            "하는", "되는", "아주", "매우", "정말", "너무", "조금", "약간",
            "진짜", "가짜", "사실", "인가요", "인가", "입니까", "일까", "나요"
    );

    private double calculateSimilarity(String text1, String text2) {
        if (text1 == null || text2 == null) return 0;
        
        // 간단한 자카드 유사도
        String[] words1 = text1.toLowerCase().split("\\s+");
        String[] words2 = text2.toLowerCase().split("\\s+");
        
        java.util.Set<String> set1 = new java.util.HashSet<>(List.of(words1));
        java.util.Set<String> set2 = new java.util.HashSet<>(List.of(words2));
        
        java.util.Set<String> intersection = new java.util.HashSet<>(set1);
        intersection.retainAll(set2);
        
        java.util.Set<String> union = new java.util.HashSet<>(set1);
        union.addAll(set2);
        
        return union.isEmpty() ? 0 : (double) intersection.size() / union.size();
    }

    private boolean containsContradiction(String claim, String evidence) {
        // 간단한 부정 표현 감지
        String lowerEvidence = evidence.toLowerCase();
        String lowerClaim = claim.toLowerCase();
        
        List<String> negativePatterns = List.of(
                "not true", "false", "incorrect", "wrong", "disputed", "controversy",
                "사실이 아", "거짓", "논쟁", "오류", "틀린", "잘못"
        );
        
        for (String pattern : negativePatterns) {
            if (lowerEvidence.contains(pattern)) {
                return true;
            }
        }
        return false;
    }

    private String generateVerificationSummary(VerificationStatus status, int supportCount, int contradictCount) {
        return switch (status) {
            case VERIFIED -> String.format("✅ 신뢰할 수 있는 %d개의 출처에서 확인되었습니다.", supportCount);
            case PARTIALLY_VERIFIED -> String.format("⚠️ 부분적으로 확인되었습니다. (지지: %d, 반박: %d)", supportCount, contradictCount);
            case UNVERIFIED -> "❓ 신뢰할 수 있는 출처에서 관련 정보를 찾을 수 없습니다.";
            case DISPUTED -> String.format("⚖️ 논쟁 중인 주장입니다. (지지: %d, 반박: %d)", supportCount, contradictCount);
            case FALSE -> String.format("❌ 신뢰할 수 있는 출처에서 반박되었습니다. (반박: %d)", contradictCount);
        };
    }

    // ============================================
    // Credibility Assessment
    // ============================================

    private CredibilityAssessment assessCredibility(List<VerificationResult> results) {
        int verified = 0;
        int disputed = 0;
        int falseClaims = 0;
        List<String> warnings = new ArrayList<>();

        for (VerificationResult result : results) {
            switch (result.getStatus()) {
                case VERIFIED, PARTIALLY_VERIFIED -> verified++;
                case DISPUTED -> {
                    disputed++;
                    warnings.add("논쟁 중: " + truncateContent(result.getOriginalClaim(), 50));
                }
                case FALSE -> {
                    falseClaims++;
                    warnings.add("주의 필요: " + truncateContent(result.getOriginalClaim(), 50));
                }
                default -> {}
            }
        }

        double score = results.isEmpty() ? 0.5 : 
                (double) verified / results.size() * 0.7 + 
                (1 - (double) falseClaims / Math.max(1, results.size())) * 0.3;

        String riskLevel;
        if (falseClaims > 0 || disputed > verified) {
            riskLevel = "high";
        } else if (disputed > 0) {
            riskLevel = "medium";
        } else {
            riskLevel = "low";
        }

        return CredibilityAssessment.builder()
                .overallScore(score)
                .verifiedCount(verified)
                .totalClaims(results.size())
                .riskLevel(riskLevel)
                .warnings(warnings)
                .build();
    }

    // ============================================
    // AI Synthesis
    // ============================================

    private String buildSynthesisPrompt(String topic, List<SourceEvidence> evidence, List<String> claims) {
        StringBuilder prompt = new StringBuilder();
        
        // 강력한 할루시네이션 방지 지침
        prompt.append("""
                당신은 팩트체커이자 심층 분석 전문가입니다.
                
                ## ⚠️ 절대 규칙 (반드시 준수)
                1. **아래 '수집된 정보' 섹션에 있는 내용만 사용하세요**
                2. **수집된 정보에 없는 내용은 절대 만들어내지 마세요 (할루시네이션 금지)**
                3. **정보가 부족하면 "관련 정보를 찾을 수 없습니다"라고 명확히 말하세요**
                4. **각 사실에는 반드시 출처를 [출처명] 형식으로 표기하세요**
                5. **수집된 정보에 없는 통계, 날짜, 수치, 순위 등을 절대 만들어내지 마세요**
                6. **존재하지 않는 출처나 URL을 만들어내지 마세요**
                7. **불확실한 정보는 "~로 추정됩니다", "~일 가능성이 있습니다"로 표현하세요**
                
                """);
        
        prompt.append("## 분석 주제\n").append(topic).append("\n\n");
        
        // 통화/단위 맥락 분석
        String currencyHint = buildCurrencyHint(topic);
        if (!currencyHint.isEmpty()) {
            prompt.append(currencyHint).append("\n");
        }

        // 수집된 증거 수에 따른 분기
        int evidenceCount = (evidence != null) ? evidence.size() : 0;
        
        if (evidenceCount == 0) {
            // 증거가 전혀 없는 경우 - 분석 거부 지시
            prompt.append("""
                ## ⚠️ 주의: 수집된 정보 없음
                신뢰할 수 있는 출처에서 이 주제에 관한 정보를 찾지 못했습니다.
                
                **이 경우 반드시 다음과 같이만 응답하세요:**
                
                ---
                ## 검색 결과
                
                죄송합니다. **"[주제]"**에 대해 신뢰할 수 있는 출처에서 관련 정보를 찾을 수 없었습니다.
                
                가능한 이유:
                - 해당 주제가 존재하지 않거나 잘못된 정보일 수 있습니다
                - 아직 널리 알려지지 않은 주제일 수 있습니다
                - 검색어를 다르게 입력해 보시기 바랍니다
                
                **주의**: 확인되지 않은 정보를 제공하지 않습니다.
                ---
                
                위 형식 외의 다른 내용을 생성하지 마세요.
                """);
        } else if (evidenceCount < 3) {
            // 증거가 부족한 경우 - 제한적 분석 지시
            prompt.append("## ⚠️ 주의: 수집된 정보 부족 (").append(evidenceCount).append("개)\n");
            prompt.append("정보가 매우 제한적이므로, **반드시 수집된 정보의 범위 내에서만** 답변하세요.\n");
            prompt.append("정보가 부족하다는 점을 응답 시작 부분에 명확히 밝히세요.\n\n");
            
            prompt.append("## 수집된 정보 (").append(evidenceCount).append("개):\n");
            for (SourceEvidence e : evidence) {
                String url = (e.getUrl() != null && !e.getUrl().isBlank()) ? " - " + e.getUrl() : "";
                prompt.append("- [").append(e.getSourceName()).append("]").append(url).append("\n");
                prompt.append("  내용: ").append(truncateContent(e.getExcerpt(), 500)).append("\n\n");
            }
        } else {
            // 충분한 증거가 있는 경우
            prompt.append("## 수집된 정보 (").append(evidenceCount).append("개):\n\n");
            
            // 실시간 검색 결과를 먼저 표시 (우선순위 높음)
            boolean hasRealtimeData = false;
            for (SourceEvidence e : evidence) {
                if ("realtime_search".equals(e.getSourceType()) || 
                    "realtime_search_citation".equals(e.getSourceType())) {
                    if (!hasRealtimeData) {
                        prompt.append("### 🔴 실시간 검색 결과 (최신 데이터 - 우선 참고)\n");
                        hasRealtimeData = true;
                    }
                    String url = (e.getUrl() != null && !e.getUrl().isBlank()) ? " - " + e.getUrl() : "";
                    prompt.append("- [").append(e.getSourceName()).append("]").append(url).append("\n");
                    prompt.append("  내용: ").append(truncateContent(e.getExcerpt(), 600)).append("\n\n");
                }
            }
            if (hasRealtimeData) {
                prompt.append("⚠️ **위 실시간 검색 결과의 가격/시세 데이터를 최우선으로 사용하세요.**\n\n");
            }
            
            // 나머지 증거 표시
            prompt.append("### 참고 자료\n");
            for (SourceEvidence e : evidence) {
                if (!"realtime_search".equals(e.getSourceType()) && 
                    !"realtime_search_citation".equals(e.getSourceType())) {
                    String url = (e.getUrl() != null && !e.getUrl().isBlank()) ? " - " + e.getUrl() : "";
                    prompt.append("- [").append(e.getSourceName()).append("]").append(url).append("\n");
                    prompt.append("  내용: ").append(truncateContent(e.getExcerpt(), 500)).append("\n\n");
                }
            }
        }

        if (claims != null && !claims.isEmpty()) {
            prompt.append("## 검증이 필요한 주장들:\n");
            for (String claim : claims) {
                prompt.append("- ").append(claim).append("\n");
            }
            prompt.append("\n");
        }

        // 증거가 충분할 때만 상세 분석 요청
        if (evidenceCount >= 3) {
            prompt.append("""
                ## 응답 형식
                위 **수집된 정보만을** 바탕으로 다음을 제공해주세요:
                
                ### 📋 사실 확인 결과
                각 주장에 대해 수집된 정보에서 확인 가능한 내용만 제시
                - ✅ 확인됨: 수집된 정보에서 직접 확인된 사실
                - ⚠️ 부분 확인: 일부만 확인되거나 추가 검증 필요
                - ❓ 확인 불가: 수집된 정보에서 확인할 수 없음
                
                ### 📚 배경 지식
                수집된 정보에서 추출한 맥락과 배경 (출처 명시 필수)
                
                ### 🔍 다양한 관점
                수집된 정보에서 발견된 서로 다른 시각 (있는 경우만)
                
                ### 📌 결론
                수집된 정보 기반의 객관적 종합 판단
                - 정보가 부족한 부분은 "추가 확인 필요"라고 명시
                
                ### ⚠️ 주의사항
                - 이 분석은 수집된 정보에 기반합니다
                - 수집되지 않은 최신 정보가 있을 수 있습니다
                
                한국어로 답변해주세요.
                """);
        } else if (evidenceCount > 0) {
            // 증거가 적을 때는 간략한 분석만 요청
            prompt.append("""
                ## 응답 형식
                **수집된 정보가 제한적입니다.** 다음 형식으로 응답하세요:
                
                ### ⚠️ 정보 부족 안내
                이 주제에 대해 신뢰할 수 있는 출처에서 제한된 정보만 수집되었습니다.
                
                ### 📋 확인된 정보
                수집된 정보에서 확인 가능한 내용만 간략히 제시 (출처 명시 필수)
                
                ### ❓ 확인 불가 사항
                현재 수집된 정보로는 확인할 수 없는 내용 목록
                
                **중요**: 수집된 정보에 없는 내용은 절대 추가하지 마세요.
                
                한국어로 답변해주세요.
                """);
        }

        return prompt.toString();
    }
    
    /**
     * 토픽에서 통화/단위 맥락을 분석하여 힌트 생성
     */
    private String buildCurrencyHint(String topic) {
        if (topic == null) return "";
        
        // 한국어 숫자 단위 + 가격 관련 키워드 감지
        boolean hasKoreanNumber = topic.matches(".*\\d+\\s*(억|만|조|천).*");
        boolean hasPriceKeyword = topic.matches(".*(가격|price|도달|목표|전망|예측).*");
        boolean hasExplicitCurrency = topic.matches(".*\\$|USD|달러|₩|KRW|원화.*");
        
        if (hasKoreanNumber && hasPriceKeyword && !hasExplicitCurrency) {
            return """
                ## 통화 단위 주의
                - 이 주제에 한국어 숫자 단위가 포함되어 있습니다
                - 단위가 명시되지 않은 금액은 **한국 원화(KRW)**일 가능성을 고려하세요
                - 예: "10억" = 10억 원 ≈ $670,000 USD
                - 가능하면 원화와 달러 양쪽 기준을 모두 분석해주세요
                """;
        }
        return "";
    }

    // ============================================
    // Utility Methods
    // ============================================

    private String truncateContent(String content, int maxLength) {
        if (content == null) return "";
        if (content.length() <= maxLength) return content;
        return content.substring(0, maxLength) + "...";
    }

    // ============================================
    // Event DTO
    // ============================================

    @Data
    @Builder
    public static class DeepAnalysisEvent {
        private String eventType;       // status, evidence, verification, assessment, ai_synthesis, complete
        private String phase;           // init, concepts, verification, assessment, synthesis, complete
        private String message;
        private List<SourceEvidence> evidence;
        private VerificationResult verificationResult;
        private CredibilityAssessment credibility;
        private String finalConclusion;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/IntegratedCrawlerService.java

```java
package com.newsinsight.collector.service;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.newsinsight.collector.client.AIDoveClient;
import com.newsinsight.collector.client.Crawl4aiClient;
import com.newsinsight.collector.dto.CrawledPage;
import com.newsinsight.collector.dto.EvidenceDto;
import com.newsinsight.collector.entity.EvidenceStance;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;
import org.jsoup.nodes.Element;
import org.jsoup.select.Elements;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.MediaType;
import org.springframework.stereotype.Service;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Mono;
import reactor.core.scheduler.Schedulers;

import java.net.URI;
import java.net.URLEncoder;
import java.nio.charset.StandardCharsets;
import java.time.Duration;
import java.util.*;
import java.util.concurrent.ConcurrentHashMap;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
import java.util.stream.Collectors;

/**
 * Integrated Crawler Service
 * 
 * Combines multiple crawling strategies for deep, comprehensive web crawling:
 * 1. Crawl4AI - Fast, efficient web scraping
 * 2. Browser-Use API - JavaScript-rendered content with AI agent
 * 3. Direct HTTP - Lightweight fallback
 * 4. Search Engines - Google, Naver, Daum news aggregation
 * 
 * Features:
 * - Multi-depth recursive crawling
 * - Link extraction and following
 * - AI-powered stance analysis
 * - Evidence collection and classification
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class IntegratedCrawlerService {

    private final WebClient webClient;
    private final ObjectMapper objectMapper;
    private final Crawl4aiClient crawl4aiClient;
    private final AIDoveClient aiDoveClient;

    @Value("${collector.crawler.base-url:http://web-crawler:11235}")
    private String crawl4aiBaseUrl;

    @Value("${collector.browser-use.base-url:http://browser-use-api:8500}")
    private String browserUseBaseUrl;

    @Value("${collector.integrated-crawler.max-depth:2}")
    private int maxDepth;

    @Value("${collector.integrated-crawler.max-pages:20}")
    private int maxPages;

    @Value("${collector.integrated-crawler.timeout-seconds:30}")
    private int timeoutSeconds;

    @Value("${collector.integrated-crawler.concurrent-crawls:5}")
    private int concurrentCrawls;

    /**
     * Crawling strategy enum
     */
    public enum CrawlStrategy {
        CRAWL4AI,       // Use Crawl4AI service
        BROWSER_USE,    // Use Browser-Use API for JS rendering
        DIRECT_HTTP,    // Direct HTTP fetch with Jsoup
        SEARCH_ENGINE   // Search engine aggregation
    }

    /**
     * Crawl request with options
     */
    public record CrawlRequest(
            String topic,
            String baseUrl,
            int maxDepth,
            int maxPages,
            Set<CrawlStrategy> strategies,
            boolean extractEvidence
    ) {
        public static CrawlRequest forTopic(String topic) {
            return new CrawlRequest(topic, null, 2, 20, 
                    EnumSet.of(CrawlStrategy.CRAWL4AI, CrawlStrategy.SEARCH_ENGINE), true);
        }

        public static CrawlRequest forUrl(String topic, String baseUrl) {
            return new CrawlRequest(topic, baseUrl, 2, 15, 
                    EnumSet.of(CrawlStrategy.CRAWL4AI, CrawlStrategy.DIRECT_HTTP), true);
        }
    }

    /**
     * Crawl result containing all collected pages and evidence
     */
    public record CrawlResult(
            String topic,
            List<CrawledPage> pages,
            List<EvidenceDto> evidence,
            Map<String, Object> metadata
    ) {}

    /**
     * Progress callback interface
     */
    public interface CrawlProgressCallback {
        void onProgress(int current, int total, String message);
        void onPageCrawled(CrawledPage page);
        void onEvidenceFound(EvidenceDto evidence);
        void onError(String url, String error);
    }

    /**
     * Perform integrated deep crawling for a topic
     */
    public Mono<CrawlResult> crawl(CrawlRequest request, CrawlProgressCallback callback) {
        log.info("Starting integrated crawl: topic={}, strategies={}", request.topic(), request.strategies());

        Set<String> visitedUrls = ConcurrentHashMap.newKeySet();
        List<CrawledPage> allPages = Collections.synchronizedList(new ArrayList<>());
        List<EvidenceDto> allEvidence = Collections.synchronizedList(new ArrayList<>());

        // Generate initial URLs based on strategies
        List<String> seedUrls = generateSeedUrls(request);
        
        if (callback != null) {
            callback.onProgress(0, seedUrls.size(), "Starting crawl with " + seedUrls.size() + " seed URLs");
        }

        return Flux.fromIterable(seedUrls)
                .flatMap(url -> crawlWithStrategies(url, request.topic(), request.strategies(), visitedUrls, 0, request.maxDepth()),
                        concurrentCrawls)
                .doOnNext(page -> {
                    allPages.add(page);
                    if (callback != null) {
                        callback.onPageCrawled(page);
                        callback.onProgress(allPages.size(), request.maxPages(), "Crawled: " + page.url());
                    }
                })
                .takeUntil(page -> allPages.size() >= request.maxPages())
                .collectList()
                .flatMap(pages -> {
                    // 크롤링 결과가 없을 경우 기본 evidence 생성
                    if (pages.isEmpty()) {
                        log.warn("No pages crawled, generating fallback evidence for topic: {}", request.topic());
                        List<EvidenceDto> fallbackEvidence = generateFallbackEvidence(request.topic(), callback);
                        allEvidence.addAll(fallbackEvidence);
                        return Mono.just(createResult(request.topic(), allPages, allEvidence));
                    }
                    
                    if (request.extractEvidence()) {
                        return extractEvidence(pages, request.topic(), callback)
                                .collectList()
                                .flatMap(evidence -> {
                                    allEvidence.addAll(evidence);
                                    // 크롤링은 했지만 evidence가 없을 경우에도 fallback 생성
                                    if (allEvidence.isEmpty()) {
                                        log.warn("Pages crawled but no evidence extracted, generating fallback for topic: {}", request.topic());
                                        List<EvidenceDto> fallbackEvidence = generateFallbackEvidence(request.topic(), callback);
                                        allEvidence.addAll(fallbackEvidence);
                                    }
                                    return Mono.just(createResult(request.topic(), allPages, allEvidence));
                                });
                    }
                    return Mono.just(createResult(request.topic(), allPages, allEvidence));
                })
                .doOnSuccess(result -> log.info("Crawl completed: topic={}, pages={}, evidence={}", 
                        request.topic(), result.pages().size(), result.evidence().size()));
    }

    /**
     * Generate fallback evidence when crawling fails
     * Uses AI Dove to generate topic analysis without external crawling
     */
    private List<EvidenceDto> generateFallbackEvidence(String topic, CrawlProgressCallback callback) {
        if (callback != null) {
            callback.onProgress(50, 100, "외부 크롤링 실패 - AI 분석으로 대체");
        }
        
        // AI Dove를 사용하여 주제 분석 시도
        if (aiDoveClient.isEnabled()) {
            try {
                String prompt = """
                    주제 '%s'에 대해 분석해주세요. 
                    다음 형식으로 JSON 배열을 반환해주세요:
                    [
                      {"title": "관점 제목", "snippet": "해당 관점에 대한 설명 (2-3문장)", "stance": "pro" 또는 "con" 또는 "neutral", "source": "AI 분석"}
                    ]
                    최소 3개, 최대 5개의 다양한 관점을 포함해주세요.
                    JSON 배열만 반환하세요.
                    """.formatted(topic);
                
                var response = aiDoveClient.chat(prompt, null).block();
                if (response != null && response.reply() != null) {
                    String json = extractJsonArray(response.reply());
                    if (json != null) {
                        JsonNode evidenceArray = objectMapper.readTree(json);
                        List<EvidenceDto> evidence = new ArrayList<>();
                        int id = 1;
                        for (JsonNode node : evidenceArray) {
                            EvidenceDto e = EvidenceDto.builder()
                                    .id((long) id++)
                                    .url("https://ai-analysis/" + topic.hashCode() + "/" + id)
                                    .title(node.has("title") ? node.get("title").asText() : "AI 분석 결과")
                                    .stance(node.has("stance") ? node.get("stance").asText().toLowerCase() : "neutral")
                                    .snippet(node.has("snippet") ? node.get("snippet").asText() : "")
                                    .source("AI 분석")
                                    .build();
                            evidence.add(e);
                            if (callback != null) {
                                callback.onEvidenceFound(e);
                            }
                        }
                        log.info("Generated {} AI fallback evidence items for topic: {}", evidence.size(), topic);
                        return evidence;
                    }
                }
            } catch (Exception e) {
                log.warn("AI fallback evidence generation failed: {}", e.getMessage());
            }
        }
        
        // AI도 실패할 경우 기본 메시지 반환
        log.warn("All evidence generation methods failed for topic: {}", topic);
        EvidenceDto defaultEvidence = EvidenceDto.builder()
                .id(1L)
                .url("https://newsinsight.local/analysis")
                .title("분석 결과 없음")
                .stance("neutral")
                .snippet("'" + topic + "'에 대한 외부 자료를 수집하지 못했습니다. 인터넷 연결 또는 외부 서비스 상태를 확인해주세요.")
                .source("시스템")
                .build();
        
        if (callback != null) {
            callback.onEvidenceFound(defaultEvidence);
        }
        
        return List.of(defaultEvidence);
    }

    /**
     * Generate seed URLs for crawling based on topic and strategies
     */
    private List<String> generateSeedUrls(CrawlRequest request) {
        List<String> urls = new ArrayList<>();
        String encodedTopic = URLEncoder.encode(request.topic(), StandardCharsets.UTF_8);

        // If base URL provided, use it
        if (request.baseUrl() != null && !request.baseUrl().isBlank()) {
            urls.add(request.baseUrl());
        }

        // Add search engine URLs
        if (request.strategies().contains(CrawlStrategy.SEARCH_ENGINE)) {
            // Google News Korea
            urls.add("https://news.google.com/search?q=" + encodedTopic + "&hl=ko&gl=KR&ceid=KR:ko");
            
            // Naver News
            urls.add("https://search.naver.com/search.naver?where=news&query=" + encodedTopic);
            
            // Daum News
            urls.add("https://search.daum.net/search?w=news&q=" + encodedTopic);
            
            // Google News English (for broader coverage)
            urls.add("https://news.google.com/search?q=" + encodedTopic + "&hl=en&gl=US&ceid=US:en");
        }

        return urls;
    }

    /**
     * Crawl URL using multiple strategies with fallback
     */
    private Flux<CrawledPage> crawlWithStrategies(String url, String topic, Set<CrawlStrategy> strategies,
                                                   Set<String> visitedUrls, int currentDepth, int maxDepth) {
        if (visitedUrls.contains(url) || currentDepth > maxDepth) {
            return Flux.empty();
        }
        visitedUrls.add(url);

        // Try strategies in order of preference
        Mono<CrawledPage> crawlMono = Mono.empty();

        if (strategies.contains(CrawlStrategy.CRAWL4AI)) {
            crawlMono = crawlMono.switchIfEmpty(crawlWithCrawl4AI(url));
        }
        if (strategies.contains(CrawlStrategy.BROWSER_USE)) {
            crawlMono = crawlMono.switchIfEmpty(crawlWithBrowserUse(url, topic));
        }
        if (strategies.contains(CrawlStrategy.DIRECT_HTTP)) {
            crawlMono = crawlMono.switchIfEmpty(crawlDirect(url));
        }

        return crawlMono
                .flux()
                .flatMap(page -> {
                    // Extract links for recursive crawling
                    if (currentDepth < maxDepth && page.content() != null) {
                        List<String> links = extractLinks(page.content(), url, topic);
                        return Flux.concat(
                                Flux.just(page),
                                Flux.fromIterable(links)
                                        .filter(link -> !visitedUrls.contains(link))
                                        .take(15) // Limit links per page (increased from 5)
                                        .flatMap(link -> crawlWithStrategies(link, topic, strategies, visitedUrls, currentDepth + 1, maxDepth))
                        );
                    }
                    return Flux.just(page);
                })
                .onErrorResume(e -> {
                    log.warn("Failed to crawl {}: {}", url, e.getMessage());
                    return Flux.empty();
                });
    }

    /**
     * Crawl using Crawl4AI service
     */
    private Mono<CrawledPage> crawlWithCrawl4AI(String url) {
        String endpoint = crawl4aiBaseUrl + "/md";

        Map<String, Object> payload = Map.of(
                "url", url,
                "bypass_cache", true,
                "word_count_threshold", 50,
                "remove_overlay_elements", true,
                "process_iframes", true
        );

        return webClient.post()
                .uri(endpoint)
                .contentType(MediaType.APPLICATION_JSON)
                .bodyValue(payload)
                .retrieve()
                .bodyToMono(String.class)
                .timeout(Duration.ofSeconds(timeoutSeconds))
                .map(response -> parseCrawl4AIResponse(url, response))
                .filter(page -> page.content() != null && !page.content().isBlank())
                .doOnSuccess(page -> log.debug("Crawl4AI success: {}", url))
                .onErrorResume(e -> {
                    log.debug("Crawl4AI failed for {}: {}", url, e.getMessage());
                    return Mono.empty();
                });
    }

    /**
     * Crawl using Browser-Use API for JavaScript-rendered content
     */
    private Mono<CrawledPage> crawlWithBrowserUse(String url, String topic) {
        String endpoint = browserUseBaseUrl + "/browse";

        Map<String, Object> payload = Map.of(
                "task", "Navigate to the URL and extract all news content related to: " + topic,
                "url", url,
                "max_steps", 5,
                "timeout_seconds", timeoutSeconds,
                "headless", true
        );

        return webClient.post()
                .uri(endpoint)
                .contentType(MediaType.APPLICATION_JSON)
                .bodyValue(payload)
                .retrieve()
                .bodyToMono(String.class)
                .timeout(Duration.ofSeconds(timeoutSeconds + 10))
                .flatMap(response -> pollBrowserUseResult(response, url))
                .doOnSuccess(page -> log.debug("Browser-Use success: {}", url))
                .onErrorResume(e -> {
                    log.debug("Browser-Use failed for {}: {}", url, e.getMessage());
                    return Mono.empty();
                });
    }

    /**
     * Poll Browser-Use job for result
     */
    private Mono<CrawledPage> pollBrowserUseResult(String initialResponse, String url) {
        try {
            JsonNode node = objectMapper.readTree(initialResponse);
            String jobId = node.has("job_id") ? node.get("job_id").asText() : null;
            
            if (jobId == null) {
                // Immediate result
                String result = node.has("result") ? node.get("result").asText() : null;
                return Mono.justOrEmpty(result)
                        .map(r -> new CrawledPage(url, "Browser-Use Result", r, "browser-use", new ArrayList<>()));
            }

            // Poll for result
            return Flux.interval(Duration.ofSeconds(2))
                    .take(15) // Max 30 seconds of polling
                    .flatMap(i -> checkBrowserUseJob(jobId))
                    .filter(status -> "completed".equals(status.status()) || "failed".equals(status.status()))
                    .next()
                    .filter(status -> "completed".equals(status.status()))
                    .map(status -> new CrawledPage(url, "Browser-Use Result", status.result(), "browser-use", new ArrayList<>()));
        } catch (Exception e) {
            return Mono.empty();
        }
    }

    private record BrowserUseJobStatus(String status, String result) {}

    private Mono<BrowserUseJobStatus> checkBrowserUseJob(String jobId) {
        return webClient.get()
                .uri(browserUseBaseUrl + "/jobs/" + jobId)
                .retrieve()
                .bodyToMono(String.class)
                .map(response -> {
                    try {
                        JsonNode node = objectMapper.readTree(response);
                        return new BrowserUseJobStatus(
                                node.has("status") ? node.get("status").asText() : "unknown",
                                node.has("result") ? node.get("result").asText() : null
                        );
                    } catch (Exception e) {
                        return new BrowserUseJobStatus("error", null);
                    }
                })
                .onErrorReturn(new BrowserUseJobStatus("error", null));
    }

    /**
     * Direct HTTP crawl using Jsoup
     */
    private Mono<CrawledPage> crawlDirect(String url) {
        return Mono.fromCallable(() -> {
                    Document doc = Jsoup.connect(url)
                            .userAgent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
                            .timeout(timeoutSeconds * 1000)
                            .followRedirects(true)
                            .get();

                    String title = doc.title();
                    String content = extractMainContent(doc);
                    List<String> links = extractDocumentLinks(doc, url);

                    return new CrawledPage(url, title, content, "direct", links);
                })
                .subscribeOn(Schedulers.boundedElastic())
                .filter(page -> page.content() != null && page.content().length() > 100)
                .doOnSuccess(page -> log.debug("Direct crawl success: {}", url))
                .onErrorResume(e -> {
                    log.debug("Direct crawl failed for {}: {}", url, e.getMessage());
                    return Mono.empty();
                });
    }

    /**
     * Parse Crawl4AI response
     */
    private CrawledPage parseCrawl4AIResponse(String url, String response) {
        try {
            JsonNode node = objectMapper.readTree(response);
            String content = null;
            String title = null;
            List<String> links = new ArrayList<>();

            if (node.has("result")) {
                JsonNode result = node.get("result");
                if (result.has("markdown")) {
                    content = result.get("markdown").asText();
                }
                if (result.has("metadata") && result.get("metadata").has("title")) {
                    title = result.get("metadata").get("title").asText();
                }
                if (result.has("links")) {
                    result.get("links").forEach(link -> {
                        if (link.has("href")) {
                            links.add(link.get("href").asText());
                        }
                    });
                }
            } else if (node.has("markdown")) {
                content = node.get("markdown").asText();
            }

            // Truncate very long content
            if (content != null && content.length() > 15000) {
                content = content.substring(0, 15000) + "\n...[truncated]";
            }

            return new CrawledPage(url, title, content, "crawl4ai", links);
        } catch (Exception e) {
            log.warn("Failed to parse Crawl4AI response for {}: {}", url, e.getMessage());
            return new CrawledPage(url, null, null, "crawl4ai", new ArrayList<>());
        }
    }

    /**
     * Extract main content from HTML document
     */
    private String extractMainContent(Document doc) {
        // Remove unwanted elements
        doc.select("script, style, nav, header, footer, aside, .advertisement, .ads, .sidebar").remove();

        // Try to find article content
        Element article = doc.selectFirst("article, .article, .content, .post-content, main, .main-content");
        if (article != null) {
            return article.text();
        }

        // Fallback to body
        Element body = doc.body();
        return body != null ? body.text() : doc.text();
    }

    /**
     * Extract links from document
     */
    private List<String> extractDocumentLinks(Document doc, String baseUrl) {
        List<String> links = new ArrayList<>();
        Elements anchors = doc.select("a[href]");

        for (Element anchor : anchors) {
            String href = anchor.absUrl("href");
            if (isValidNewsLink(href, baseUrl)) {
                links.add(href);
            }
        }

        return links.stream().distinct().limit(30).collect(Collectors.toList());
    }

    /**
     * Extract links from markdown/text content
     */
    private List<String> extractLinks(String content, String baseUrl, String topic) {
        List<String> links = new ArrayList<>();
        
        // URL pattern
        Pattern urlPattern = Pattern.compile("https?://[\\w\\-._~:/?#\\[\\]@!$&'()*+,;=%]+", Pattern.CASE_INSENSITIVE);
        Matcher matcher = urlPattern.matcher(content);

        while (matcher.find()) {
            String url = cleanUrl(matcher.group());
            if (isValidNewsLink(url, baseUrl)) {
                links.add(url);
            }
        }

        return links.stream().distinct().limit(30).collect(Collectors.toList());
    }

    /**
     * Clean extracted URLs by removing trailing punctuation
     */
    private String cleanUrl(String url) {
        if (url == null) return null;
        // Remove trailing punctuation that might be captured from markdown/text
        url = url.replaceAll("[)\\]\\*.,;:!?]+$", "");
        return url;
    }

    /**
     * Check if link is a valid news article link
     */
    private boolean isValidNewsLink(String url, String baseUrl) {
        if (url == null || url.isBlank()) return false;
        
        try {
            URI uri = URI.create(url);
            String host = uri.getHost();
            if (host == null) return false;
            
            String hostLower = host.toLowerCase();
            String pathLower = uri.getPath() != null ? uri.getPath().toLowerCase() : "";
            String urlLower = url.toLowerCase();

            // Skip blocked subdomains (authentication, marketing, help, etc.)
            if (hostLower.startsWith("nid.") ||      // Naver ID (login)
                hostLower.startsWith("accounts.") ||
                hostLower.startsWith("auth.") ||
                hostLower.startsWith("login.") ||
                hostLower.startsWith("sso.") ||
                hostLower.startsWith("mkt.") ||      // Marketing
                hostLower.startsWith("ads.") ||
                hostLower.startsWith("notify.") ||   // Notifications
                hostLower.startsWith("help.") ||     // Help/Support
                hostLower.startsWith("support.") ||
                hostLower.startsWith("dic.") ||      // Dictionary
                hostLower.startsWith("translate.") ||
                hostLower.startsWith("map.") ||
                hostLower.startsWith("maps.") ||
                hostLower.startsWith("cdn.") ||
                hostLower.startsWith("static.") ||
                hostLower.startsWith("img.") ||
                hostLower.startsWith("images.")) {
                return false;
            }

            // Skip common non-news/social media domains
            if (hostLower.contains("facebook.com") || hostLower.contains("twitter.com") ||
                hostLower.contains("instagram.com") || hostLower.contains("youtube.com") ||
                hostLower.contains("linkedin.com") || hostLower.contains("tiktok.com") ||
                hostLower.contains("x.com")) {
                return false;
            }

            // Skip non-http URLs
            String scheme = uri.getScheme();
            if (!"http".equals(scheme) && !"https".equals(scheme)) {
                return false;
            }

            // Skip blocked URL paths
            if (pathLower.contains("/login") ||
                pathLower.contains("/signin") ||
                pathLower.contains("/signup") ||
                pathLower.contains("/register") ||
                pathLower.contains("/join") ||
                pathLower.contains("/membership") ||
                pathLower.contains("/auth/") ||
                pathLower.contains("/account") ||
                pathLower.contains("/profile") ||
                pathLower.contains("/settings") ||
                pathLower.contains("/help") ||
                pathLower.contains("/support") ||
                pathLower.contains("/faq") ||
                pathLower.contains("/contact") ||
                pathLower.contains("/about") ||
                pathLower.contains("/privacy") ||
                pathLower.contains("/terms") ||
                pathLower.contains("/legal") ||
                pathLower.contains("/careers") ||
                pathLower.contains("/subscribe") ||
                pathLower.contains("/cart") ||
                pathLower.contains("/checkout") ||
                pathLower.contains("/promo") ||
                pathLower.contains("/campaign") ||
                pathLower.contains("/landing") ||
                pathLower.contains("/offer") ||
                pathLower.contains("/notify") ||
                pathLower.contains("/notification") ||
                pathLower.contains("/grammar_checker") ||
                pathLower.contains("/spell_check") ||
                pathLower.contains("/qna/") ||
                pathLower.contains("/question") ||
                pathLower.contains("/forum") ||
                pathLower.contains("/board")) {
                return false;
            }
            
            // Skip URLs with search query params (not direct article links)
            if (urlLower.contains("search.naver.com") && !urlLower.contains("where=news")) {
                return false;
            }

            // Skip media files
            if (pathLower.endsWith(".jpg") || pathLower.endsWith(".jpeg") ||
                pathLower.endsWith(".png") || pathLower.endsWith(".gif") || 
                pathLower.endsWith(".webp") || pathLower.endsWith(".svg") ||
                pathLower.endsWith(".pdf") || pathLower.endsWith(".mp4") ||
                pathLower.endsWith(".mp3") || pathLower.endsWith(".zip") ||
                pathLower.endsWith(".exe")) {
                return false;
            }

            return true;
        } catch (Exception e) {
            return false;
        }
    }

    /**
     * Extract evidence from crawled pages using AI
     */
    private Flux<EvidenceDto> extractEvidence(List<CrawledPage> pages, String topic, CrawlProgressCallback callback) {
        if (!aiDoveClient.isEnabled()) {
            log.warn("AI Dove is disabled, using simple extraction");
            return Flux.fromIterable(pages)
                    .map(page -> createSimpleEvidence(page, topic));
        }

        String aggregatedContent = aggregateContent(pages, topic);
        String prompt = buildEvidenceExtractionPrompt(aggregatedContent, topic);

        return aiDoveClient.chat(prompt, null)
                .flatMapMany(response -> parseEvidenceFromAI(response.reply(), pages, topic))
                .doOnNext(evidence -> {
                    if (callback != null) {
                        callback.onEvidenceFound(evidence);
                    }
                })
                .onErrorResume(e -> {
                    log.error("AI evidence extraction failed: {}", e.getMessage());
                    // Fallback to simple extraction
                    return Flux.fromIterable(pages)
                            .map(page -> createSimpleEvidence(page, topic));
                });
    }

    /**
     * Aggregate content from multiple pages for AI analysis
     */
    private String aggregateContent(List<CrawledPage> pages, String topic) {
        StringBuilder sb = new StringBuilder();
        sb.append("Topic: ").append(topic).append("\n\n");

        int index = 1;
        for (CrawledPage page : pages) {
            if (page.content() == null || page.content().isBlank()) continue;

            sb.append("=== Source ").append(index++).append(" ===\n");
            sb.append("URL: ").append(page.url()).append("\n");
            if (page.title() != null) {
                sb.append("Title: ").append(page.title()).append("\n");
            }
            
            // Limit content per page
            String content = page.content();
            if (content.length() > 3000) {
                content = content.substring(0, 3000) + "...[truncated]";
            }
            sb.append("Content:\n").append(content).append("\n\n");

            if (sb.length() > 20000) {
                sb.append("\n...[additional sources truncated]\n");
                break;
            }
        }

        return sb.toString();
    }

    /**
     * Build prompt for evidence extraction
     */
    private String buildEvidenceExtractionPrompt(String content, String topic) {
        return """
                You are an expert fact-checker and evidence analyst. Analyze the following news content about "%s" and extract evidence.

                For each piece of evidence, determine:
                1. The stance (pro/con/neutral) - whether it supports, opposes, or is neutral to the topic
                2. A brief snippet summarizing the key point
                3. The source URL and title

                Return your analysis as a JSON array with the following structure:
                [
                  {
                    "url": "source URL",
                    "title": "article title",
                    "stance": "pro" | "con" | "neutral",
                    "snippet": "key evidence snippet (1-2 sentences)",
                    "source": "publication name"
                  }
                ]

                Only include factual evidence, not opinions. Maximum 10 pieces of evidence.
                Respond ONLY with the JSON array, no other text.

                --- CONTENT ---
                %s
                """.formatted(topic, content);
    }

    /**
     * Parse evidence from AI response
     */
    private Flux<EvidenceDto> parseEvidenceFromAI(String aiResponse, List<CrawledPage> pages, String topic) {
        try {
            // Extract JSON array from response
            String json = extractJsonArray(aiResponse);
            if (json == null) {
                return Flux.fromIterable(pages).map(page -> createSimpleEvidence(page, topic));
            }

            JsonNode evidenceArray = objectMapper.readTree(json);
            List<EvidenceDto> evidenceList = new ArrayList<>();

            for (JsonNode node : evidenceArray) {
                EvidenceDto evidence = EvidenceDto.builder()
                        .url(node.has("url") ? node.get("url").asText() : "")
                        .title(node.has("title") ? node.get("title").asText() : "")
                        .stance(node.has("stance") ? node.get("stance").asText().toLowerCase() : "neutral")
                        .snippet(node.has("snippet") ? node.get("snippet").asText() : "")
                        .source(node.has("source") ? node.get("source").asText() : "")
                        .build();
                evidenceList.add(evidence);
            }

            return Flux.fromIterable(evidenceList);
        } catch (Exception e) {
            log.warn("Failed to parse AI evidence response: {}", e.getMessage());
            return Flux.fromIterable(pages).map(page -> createSimpleEvidence(page, topic));
        }
    }

    /**
     * Extract JSON array from text that may contain other content
     */
    private String extractJsonArray(String text) {
        if (text == null) return null;
        
        int start = text.indexOf('[');
        int end = text.lastIndexOf(']');
        
        if (start >= 0 && end > start) {
            return text.substring(start, end + 1);
        }
        return null;
    }

    /**
     * Create simple evidence from page without AI
     */
    private EvidenceDto createSimpleEvidence(CrawledPage page, String topic) {
        String snippet = page.content();
        if (snippet != null && snippet.length() > 300) {
            snippet = snippet.substring(0, 300) + "...";
        }

        return EvidenceDto.builder()
                .url(page.url())
                .title(page.title() != null ? page.title() : "Untitled")
                .stance("neutral")
                .snippet(snippet != null ? snippet : "")
                .source(extractDomain(page.url()))
                .build();
    }

    /**
     * Extract domain from URL
     */
    private String extractDomain(String url) {
        try {
            URI uri = URI.create(url);
            return uri.getHost();
        } catch (Exception e) {
            return url;
        }
    }

    /**
     * Create final result
     */
    private CrawlResult createResult(String topic, List<CrawledPage> pages, List<EvidenceDto> evidence) {
        Map<String, Object> metadata = new HashMap<>();
        metadata.put("totalPages", pages.size());
        metadata.put("totalEvidence", evidence.size());
        metadata.put("sources", pages.stream().map(CrawledPage::source).distinct().collect(Collectors.toList()));

        // Calculate stance distribution
        Map<String, Long> stanceCount = evidence.stream()
                .collect(Collectors.groupingBy(EvidenceDto::getStance, Collectors.counting()));
        metadata.put("stanceDistribution", stanceCount);

        return new CrawlResult(topic, pages, evidence, metadata);
    }

    /**
     * Check if service is available
     */
    public boolean isAvailable() {
        return true; // At least direct HTTP is always available
    }

    /**
     * Get available strategies
     */
    public Set<CrawlStrategy> getAvailableStrategies() {
        Set<CrawlStrategy> strategies = EnumSet.of(CrawlStrategy.DIRECT_HTTP, CrawlStrategy.SEARCH_ENGINE);
        
        // Check Crawl4AI
        try {
            webClient.get()
                    .uri(crawl4aiBaseUrl + "/health")
                    .retrieve()
                    .bodyToMono(String.class)
                    .timeout(Duration.ofSeconds(5))
                    .block();
            strategies.add(CrawlStrategy.CRAWL4AI);
        } catch (Exception e) {
            log.debug("Crawl4AI not available: {}", e.getMessage());
        }

        // Check Browser-Use
        try {
            webClient.get()
                    .uri(browserUseBaseUrl + "/health")
                    .retrieve()
                    .bodyToMono(String.class)
                    .timeout(Duration.ofSeconds(5))
                    .block();
            strategies.add(CrawlStrategy.BROWSER_USE);
        } catch (Exception e) {
            log.debug("Browser-Use not available: {}", e.getMessage());
        }

        return strategies;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/LlmProviderSettingsService.java

```java
package com.newsinsight.collector.service;

import com.newsinsight.collector.dto.llm.LlmProviderSettingsDto;
import com.newsinsight.collector.dto.llm.LlmProviderSettingsRequest;
import com.newsinsight.collector.dto.llm.LlmTestResult;
import com.newsinsight.collector.entity.settings.LlmProviderSettings;
import com.newsinsight.collector.entity.settings.LlmProviderType;
import com.newsinsight.collector.repository.LlmProviderSettingsRepository;
import com.newsinsight.collector.util.ApiKeyEncryptor;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.http.*;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;
import org.springframework.web.client.RestTemplate;

import java.time.LocalDateTime;
import java.util.*;
import java.util.stream.Collectors;

/**
 * LLM Provider 설정 관리 서비스.
 * 
 * 관리자 전역 설정과 사용자별 설정을 관리하며,
 * 사용자 요청 시 유효한 설정(사용자 > 전역 우선순위)을 반환.
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class LlmProviderSettingsService {

    private final LlmProviderSettingsRepository repository;
    private final RestTemplate restTemplate;
    private final ApiKeyEncryptor apiKeyEncryptor;

    // ========== 전역(관리자) 설정 관리 ==========

    /**
     * 모든 전역 설정 조회
     */
    @Transactional(readOnly = true)
    public List<LlmProviderSettingsDto> getAllGlobalSettings() {
        return repository.findAllGlobalSettings().stream()
                .map(this::toDto)
                .collect(Collectors.toList());
    }

    /**
     * 특정 Provider의 전역 설정 조회
     */
    @Transactional(readOnly = true)
    public Optional<LlmProviderSettingsDto> getGlobalSetting(LlmProviderType providerType) {
        return repository.findGlobalByProviderType(providerType)
                .map(this::toDto);
    }

    /**
     * 전역 설정 생성/업데이트
     */
    @Transactional
    public LlmProviderSettingsDto saveGlobalSetting(LlmProviderSettingsRequest request) {
        LlmProviderSettings settings = repository.findGlobalByProviderType(request.getProviderType())
                .orElse(LlmProviderSettings.builder()
                        .providerType(request.getProviderType())
                        .userId(null) // 전역 설정
                        .build());

        updateSettingsFromRequest(settings, request);
        LlmProviderSettings saved = repository.save(settings);
        log.info("Saved global LLM setting for provider: {}", request.getProviderType());
        return toDto(saved);
    }

    /**
     * 전역 설정 삭제
     */
    @Transactional
    public void deleteGlobalSetting(LlmProviderType providerType) {
        repository.deleteGlobalByProviderType(providerType);
        log.info("Deleted global LLM setting for provider: {}", providerType);
    }

    // ========== 사용자별 설정 관리 ==========

    /**
     * 사용자의 모든 개인 설정 조회
     */
    @Transactional(readOnly = true)
    public List<LlmProviderSettingsDto> getUserSettings(String userId) {
        return repository.findByUserIdOrderByPriorityAsc(userId).stream()
                .map(this::toDto)
                .collect(Collectors.toList());
    }

    /**
     * 사용자의 특정 Provider 설정 조회
     */
    @Transactional(readOnly = true)
    public Optional<LlmProviderSettingsDto> getUserSetting(String userId, LlmProviderType providerType) {
        return repository.findByProviderTypeAndUserId(providerType, userId)
                .map(this::toDto);
    }

    /**
     * 사용자 설정 생성/업데이트
     */
    @Transactional
    public LlmProviderSettingsDto saveUserSetting(String userId, LlmProviderSettingsRequest request) {
        LlmProviderSettings settings = repository.findByProviderTypeAndUserId(request.getProviderType(), userId)
                .orElse(LlmProviderSettings.builder()
                        .providerType(request.getProviderType())
                        .userId(userId)
                        .build());

        updateSettingsFromRequest(settings, request);
        LlmProviderSettings saved = repository.save(settings);
        log.info("Saved user LLM setting for user: {}, provider: {}", userId, request.getProviderType());
        return toDto(saved);
    }

    /**
     * 사용자 설정 삭제 (전역 설정으로 폴백)
     */
    @Transactional
    public void deleteUserSetting(String userId, LlmProviderType providerType) {
        repository.deleteByProviderTypeAndUserId(providerType, userId);
        log.info("Deleted user LLM setting for user: {}, provider: {}", userId, providerType);
    }

    /**
     * 사용자의 모든 설정 삭제
     */
    @Transactional
    public void deleteAllUserSettings(String userId) {
        repository.deleteByUserId(userId);
        log.info("Deleted all LLM settings for user: {}", userId);
    }

    // ========== 유효(Effective) 설정 조회 ==========

    /**
     * 사용자에게 유효한 모든 설정 조회
     * - 사용자 설정이 있으면 사용자 설정 반환
     * - 없으면 전역 설정 반환
     */
    @Transactional(readOnly = true)
    public List<LlmProviderSettingsDto> getEffectiveSettings(String userId) {
        // 전역 설정 가져오기
        Map<LlmProviderType, LlmProviderSettings> effectiveMap = new LinkedHashMap<>();
        for (LlmProviderSettings global : repository.findAllGlobalSettings()) {
            effectiveMap.put(global.getProviderType(), global);
        }

        // 사용자 설정으로 오버라이드
        if (userId != null && !userId.isBlank()) {
            for (LlmProviderSettings userSetting : repository.findByUserIdOrderByPriorityAsc(userId)) {
                effectiveMap.put(userSetting.getProviderType(), userSetting);
            }
        }

        return effectiveMap.values().stream()
                .sorted(Comparator.comparing(LlmProviderSettings::getPriority))
                .map(this::toDto)
                .collect(Collectors.toList());
    }

    /**
     * 특정 Provider의 유효 설정 조회
     */
    @Transactional(readOnly = true)
    public Optional<LlmProviderSettingsDto> getEffectiveSetting(String userId, LlmProviderType providerType) {
        // 사용자 설정 먼저 확인
        if (userId != null && !userId.isBlank()) {
            Optional<LlmProviderSettings> userSetting = repository.findByProviderTypeAndUserId(providerType, userId);
            if (userSetting.isPresent()) {
                return userSetting.map(this::toDto);
            }
        }
        // 없으면 전역 설정 반환
        return repository.findGlobalByProviderType(providerType).map(this::toDto);
    }

    /**
     * 활성화된 Provider 목록 (Fallback 체인용)
     */
    @Transactional(readOnly = true)
    public List<LlmProviderSettingsDto> getEnabledProviders(String userId) {
        return getEffectiveSettings(userId).stream()
                .filter(LlmProviderSettingsDto::getEnabled)
                .sorted(Comparator.comparing(LlmProviderSettingsDto::getPriority))
                .collect(Collectors.toList());
    }

    // ========== API 키 직접 조회 (내부 서비스용) ==========

    /**
     * 전역 설정에서 API 키 직접 조회 (실시간 검색 등 내부 서비스용)
     * API 키는 복호화하여 반환합니다.
     */
    @Transactional(readOnly = true)
    public Optional<String> getGlobalApiKey(LlmProviderType providerType) {
        return repository.findGlobalByProviderType(providerType)
                .filter(LlmProviderSettings::getEnabled)
                .map(LlmProviderSettings::getApiKey)
                .map(apiKeyEncryptor::decrypt);
    }

    /**
     * 전역 설정에서 Base URL 직접 조회
     */
    @Transactional(readOnly = true)
    public Optional<String> getGlobalBaseUrl(LlmProviderType providerType) {
        return repository.findGlobalByProviderType(providerType)
                .filter(LlmProviderSettings::getEnabled)
                .map(LlmProviderSettings::getBaseUrl);
    }

    // ========== 연결 테스트 ==========

    /**
     * Provider 연결 테스트
     */
    @Transactional
    public LlmTestResult testConnection(Long settingsId) {
        LlmProviderSettings settings = repository.findById(settingsId)
                .orElseThrow(() -> new IllegalArgumentException("Settings not found: " + settingsId));

        LlmTestResult result = performConnectionTest(settings);

        // 테스트 결과 업데이트
        repository.updateTestResult(settingsId, LocalDateTime.now(), result.isSuccess());

        return result;
    }

    /**
     * Provider 연결 테스트 (설정 객체로 직접)
     */
    public LlmTestResult testConnection(LlmProviderSettingsRequest request) {
        LlmProviderSettings settings = LlmProviderSettings.builder()
                .providerType(request.getProviderType())
                .apiKey(request.getApiKey())
                .baseUrl(request.getBaseUrl())
                .defaultModel(request.getDefaultModel())
                .azureDeploymentName(request.getAzureDeploymentName())
                .azureApiVersion(request.getAzureApiVersion())
                .build();

        return performConnectionTest(settings);
    }

    private LlmTestResult performConnectionTest(LlmProviderSettings settings) {
        try {
            String testUrl = buildTestUrl(settings);
            HttpHeaders headers = buildHeaders(settings);

            HttpEntity<String> entity = new HttpEntity<>(headers);
            ResponseEntity<String> response = restTemplate.exchange(
                    testUrl, HttpMethod.GET, entity, String.class
            );

            boolean success = response.getStatusCode().is2xxSuccessful();
            return LlmTestResult.builder()
                    .success(success)
                    .providerType(settings.getProviderType())
                    .message(success ? "Connection successful" : "Connection failed")
                    .responseTime(System.currentTimeMillis())
                    .build();

        } catch (Exception e) {
            log.warn("LLM connection test failed for {}: {}", settings.getProviderType(), e.getMessage());
            return LlmTestResult.builder()
                    .success(false)
                    .providerType(settings.getProviderType())
                    .message("Connection failed: " + e.getMessage())
                    .error(e.getMessage())
                    .build();
        }
    }

    private String buildTestUrl(LlmProviderSettings settings) {
        String baseUrl = settings.getBaseUrl() != null ? settings.getBaseUrl() 
                : settings.getProviderType().getDefaultBaseUrl();

        return switch (settings.getProviderType()) {
            case OPENAI, OPENROUTER, TOGETHER_AI -> baseUrl + "/models";
            case ANTHROPIC -> baseUrl + "/v1/messages"; // Will return 405 but proves connectivity
            case GOOGLE -> baseUrl + "/v1/models";
            case OLLAMA -> baseUrl + "/api/tags";
            case AZURE_OPENAI -> baseUrl + "/openai/deployments?api-version=" + 
                    (settings.getAzureApiVersion() != null ? settings.getAzureApiVersion() : "2024-02-01");
            case PERPLEXITY -> baseUrl + "/chat/completions";
            case BRAVE_SEARCH -> baseUrl + "/web/search";
            case TAVILY -> baseUrl + "/search";
            case CUSTOM -> baseUrl + "/health";
        };
    }

    private HttpHeaders buildHeaders(LlmProviderSettings settings) {
        HttpHeaders headers = new HttpHeaders();
        headers.setContentType(MediaType.APPLICATION_JSON);

        if (settings.getApiKey() != null && !settings.getApiKey().isBlank()) {
            // Decrypt the API key before using it in headers
            String decryptedApiKey = apiKeyEncryptor.decrypt(settings.getApiKey());
            switch (settings.getProviderType()) {
                case OPENAI, OPENROUTER -> headers.setBearerAuth(decryptedApiKey);
                case ANTHROPIC -> {
                    headers.set("x-api-key", decryptedApiKey);
                    headers.set("anthropic-version", "2023-06-01");
                }
                case GOOGLE -> headers.set("x-goog-api-key", decryptedApiKey);
                case AZURE_OPENAI -> headers.set("api-key", decryptedApiKey);
                default -> headers.setBearerAuth(decryptedApiKey);
            }
        }

        return headers;
    }

    // ========== 활성화/비활성화 ==========

    @Transactional
    public void setEnabled(Long settingsId, boolean enabled) {
        repository.updateEnabled(settingsId, enabled);
        log.info("Updated LLM settings {} enabled status to: {}", settingsId, enabled);
    }

    // ========== DTO 변환 ==========

    private LlmProviderSettingsDto toDto(LlmProviderSettings entity) {
        return LlmProviderSettingsDto.builder()
                .id(entity.getId())
                .providerType(entity.getProviderType())
                .providerDisplayName(entity.getProviderType().getDisplayName())
                .userId(entity.getUserId())
                .isGlobal(entity.isGlobal())
                .apiKeyMasked(apiKeyEncryptor.getMaskedKey(entity.getApiKey()))
                .hasApiKey(entity.getApiKey() != null && !entity.getApiKey().isBlank())
                .defaultModel(entity.getDefaultModel())
                .baseUrl(entity.getBaseUrl())
                .enabled(entity.getEnabled())
                .priority(entity.getPriority())
                .maxTokens(entity.getMaxTokens())
                .temperature(entity.getTemperature())
                .timeoutMs(entity.getTimeoutMs())
                .maxRequestsPerMinute(entity.getMaxRequestsPerMinute())
                .azureDeploymentName(entity.getAzureDeploymentName())
                .azureApiVersion(entity.getAzureApiVersion())
                .lastTestedAt(entity.getLastTestedAt())
                .lastTestSuccess(entity.getLastTestSuccess())
                .createdAt(entity.getCreatedAt())
                .updatedAt(entity.getUpdatedAt())
                .build();
    }

    private void updateSettingsFromRequest(LlmProviderSettings settings, LlmProviderSettingsRequest request) {
        if (request.getApiKey() != null) {
            // Encrypt the API key before storing
            String encryptedApiKey = apiKeyEncryptor.encrypt(request.getApiKey());
            settings.setApiKey(encryptedApiKey);
            log.debug("API key encrypted and stored for provider: {}", settings.getProviderType());
        }
        if (request.getDefaultModel() != null) {
            settings.setDefaultModel(request.getDefaultModel());
        }
        if (request.getBaseUrl() != null) {
            settings.setBaseUrl(request.getBaseUrl());
        }
        if (request.getEnabled() != null) {
            settings.setEnabled(request.getEnabled());
        }
        if (request.getPriority() != null) {
            settings.setPriority(request.getPriority());
        }
        if (request.getMaxTokens() != null) {
            settings.setMaxTokens(request.getMaxTokens());
        }
        if (request.getTemperature() != null) {
            settings.setTemperature(request.getTemperature());
        }
        if (request.getTimeoutMs() != null) {
            settings.setTimeoutMs(request.getTimeoutMs());
        }
        if (request.getMaxRequestsPerMinute() != null) {
            settings.setMaxRequestsPerMinute(request.getMaxRequestsPerMinute());
        }
        if (request.getAzureDeploymentName() != null) {
            settings.setAzureDeploymentName(request.getAzureDeploymentName());
        }
        if (request.getAzureApiVersion() != null) {
            settings.setAzureApiVersion(request.getAzureApiVersion());
        }
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/ProjectAutoCollectService.java

```java
package com.newsinsight.collector.service;

import com.newsinsight.collector.entity.project.Project;
import com.newsinsight.collector.entity.project.ProjectActivityLog;
import com.newsinsight.collector.entity.project.ProjectItem;
import com.newsinsight.collector.entity.project.ProjectNotification;
import com.newsinsight.collector.entity.search.SearchType;
import com.newsinsight.collector.repository.ProjectRepository;
import com.newsinsight.collector.service.SearchJobQueueService.SearchJobRequest;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.scheduling.annotation.Scheduled;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Map;

/**
 * Service for automatic news collection for projects.
 * Runs on a schedule to collect news for projects with auto-collect enabled.
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class ProjectAutoCollectService {

    private final ProjectRepository projectRepository;
    private final ProjectService projectService;
    private final SearchJobQueueService searchJobQueueService;
    private final UnifiedSearchService unifiedSearchService;

    /**
     * Scheduled task to process auto-collection for projects.
     * Runs every 30 minutes.
     */
    @Scheduled(fixedRate = 1800000) // 30 minutes
    @Transactional
    public void processAutoCollection() {
        log.info("Starting scheduled auto-collection processing");

        LocalDateTime now = LocalDateTime.now();
        LocalDateTime hourAgo = now.minusHours(1);
        LocalDateTime dayAgo = now.minusDays(1);
        LocalDateTime weekAgo = now.minusWeeks(1);

        List<Project> projectsToCollect = projectRepository.findProjectsNeedingCollection(
                hourAgo, dayAgo, weekAgo
        );

        log.info("Found {} projects needing collection", projectsToCollect.size());

        for (Project project : projectsToCollect) {
            try {
                collectForProject(project);
            } catch (Exception e) {
                log.error("Failed to collect for project {}: {}", project.getId(), e.getMessage(), e);
            }
        }

        log.info("Completed auto-collection processing");
    }

    /**
     * Collect news for a specific project.
     */
    @Transactional
    public void collectForProject(Project project) {
        log.info("Starting collection for project: id={}, name='{}'", project.getId(), project.getName());

        List<String> keywords = project.getKeywords();
        if (keywords == null || keywords.isEmpty()) {
            log.warn("Project {} has no keywords configured for collection", project.getId());
            return;
        }

        Project.ProjectSettings settings = project.getSettings();
        String timeWindow = settings != null && settings.getTimeWindow() != null 
                ? settings.getTimeWindow() 
                : "7d";

        // Build search query from keywords
        String query = buildSearchQuery(keywords);

        // Start search job
        SearchJobRequest jobRequest = SearchJobRequest.builder()
                .type(SearchType.UNIFIED)
                .query(query)
                .timeWindow(timeWindow)
                .userId(project.getOwnerId())
                .projectId(project.getId())
                .options(Map.of(
                        "autoCollect", true,
                        "projectName", project.getName()
                ))
                .build();

        String jobId = searchJobQueueService.startJob(jobRequest);
        log.info("Started auto-collection job: jobId={}, projectId={}", jobId, project.getId());

        // Update project last collected timestamp
        projectRepository.updateLastCollected(project.getId(), LocalDateTime.now());

        // Log activity
        projectService.logActivity(
                project.getId(),
                "system",
                ProjectActivityLog.ActivityType.AUTO_COLLECTION,
                "자동 수집 실행: " + query,
                "job",
                jobId,
                Map.of("keywords", keywords, "timeWindow", timeWindow)
        );
    }

    /**
     * Manually trigger collection for a project.
     */
    @Transactional
    public String triggerCollection(Long projectId, String userId) {
        Project project = projectRepository.findById(projectId)
                .orElseThrow(() -> new IllegalArgumentException("Project not found: " + projectId));

        log.info("Manual collection triggered: projectId={}, triggeredBy={}", projectId, userId);

        List<String> keywords = project.getKeywords();
        if (keywords == null || keywords.isEmpty()) {
            throw new IllegalStateException("Project has no keywords configured for collection");
        }

        Project.ProjectSettings settings = project.getSettings();
        String timeWindow = settings != null && settings.getTimeWindow() != null 
                ? settings.getTimeWindow() 
                : "7d";

        String query = buildSearchQuery(keywords);

        SearchJobRequest jobRequest = SearchJobRequest.builder()
                .type(SearchType.UNIFIED)
                .query(query)
                .timeWindow(timeWindow)
                .userId(userId)
                .projectId(projectId)
                .options(Map.of(
                        "manualTrigger", true,
                        "triggeredBy", userId
                ))
                .build();

        String jobId = searchJobQueueService.startJob(jobRequest);

        // Update project
        projectRepository.updateLastCollected(projectId, LocalDateTime.now());
        projectRepository.updateLastActivity(projectId, LocalDateTime.now());

        // Log activity
        projectService.logActivity(
                projectId,
                userId,
                ProjectActivityLog.ActivityType.MANUAL_COLLECTION,
                "수동 수집 실행: " + query,
                "job",
                jobId,
                Map.of("keywords", keywords, "timeWindow", timeWindow)
        );

        return jobId;
    }

    /**
     * Process search results and add to project.
     * Called by SearchJobQueueService when a project-related search completes.
     */
    @Transactional
    public void processSearchResults(Long projectId, String jobId, List<Map<String, Object>> results, String userId) {
        log.info("Processing search results for project: projectId={}, resultCount={}", projectId, results.size());

        int addedCount = 0;
        int duplicateCount = 0;

        for (Map<String, Object> result : results) {
            try {
                String url = (String) result.get("url");
                
                // Check for duplicates by URL
                List<ProjectItem> existing = projectService.getProject(projectId)
                        .map(p -> List.<ProjectItem>of()) // Simplified - would need actual check
                        .orElse(List.of());
                
                // For now, assume no duplicates check needed (would need proper implementation)
                
                ProjectService.AddItemRequest itemRequest = ProjectService.AddItemRequest.builder()
                        .itemType(ProjectItem.ItemType.ARTICLE)
                        .title((String) result.get("title"))
                        .summary((String) result.get("snippet"))
                        .url(url)
                        .thumbnailUrl((String) result.get("imageUrl"))
                        .sourceName((String) result.get("source"))
                        .sourceId(jobId)
                        .sourceType("auto_collect")
                        .publishedAt(parsePublishedAt(result.get("publishedAt")))
                        .sentiment((String) result.get("sentiment"))
                        .importance(calculateImportance(result))
                        .metadata(Map.of(
                                "jobId", jobId,
                                "autoCollected", true
                        ))
                        .build();

                projectService.addItem(projectId, itemRequest, userId != null ? userId : "system");
                addedCount++;
                
            } catch (Exception e) {
                log.warn("Failed to add result to project: {}", e.getMessage());
            }
        }

        log.info("Added {} items to project {} (duplicates: {})", addedCount, projectId, duplicateCount);

        // Notify project owner if significant results found
        if (addedCount > 0) {
            Project project = projectRepository.findById(projectId).orElse(null);
            if (project != null) {
                projectService.createNotification(
                        projectId,
                        project.getOwnerId(),
                        ProjectNotification.NotificationType.NEW_ARTICLES,
                        "새로운 기사 수집 완료",
                        String.format("%d개의 새로운 기사가 수집되었습니다.", addedCount),
                        "/projects/" + projectId + "/items"
                );
            }
        }
    }

    /**
     * Build search query from keywords.
     */
    private String buildSearchQuery(List<String> keywords) {
        if (keywords.size() == 1) {
            return keywords.get(0);
        }
        
        // Join keywords with OR for broader search
        // Could be made more sophisticated with AND/OR options
        return String.join(" OR ", keywords);
    }

    /**
     * Parse published date from result.
     */
    private LocalDateTime parsePublishedAt(Object publishedAt) {
        if (publishedAt == null) {
            return null;
        }
        
        if (publishedAt instanceof LocalDateTime) {
            return (LocalDateTime) publishedAt;
        }
        
        if (publishedAt instanceof String dateStr) {
            try {
                return LocalDateTime.parse(dateStr);
            } catch (Exception e) {
                // Try other formats
                try {
                    return LocalDateTime.parse(dateStr.replace("Z", ""));
                } catch (Exception e2) {
                    return null;
                }
            }
        }
        
        return null;
    }

    /**
     * Calculate importance score for an article.
     */
    private int calculateImportance(Map<String, Object> result) {
        int importance = 50; // Default

        // Boost for credibility score
        Object credibility = result.get("credibilityScore");
        if (credibility instanceof Number) {
            importance += ((Number) credibility).intValue() / 2;
        }

        // Boost for recent articles
        LocalDateTime publishedAt = parsePublishedAt(result.get("publishedAt"));
        if (publishedAt != null && publishedAt.isAfter(LocalDateTime.now().minusDays(1))) {
            importance += 10;
        }

        // Cap at 100
        return Math.min(importance, 100);
    }

    /**
     * Get collection status for a project.
     */
    public Map<String, Object> getCollectionStatus(Long projectId) {
        Project project = projectRepository.findById(projectId)
                .orElseThrow(() -> new IllegalArgumentException("Project not found: " + projectId));

        boolean autoCollectEnabled = project.isAutoCollectEnabled();
        LocalDateTime lastCollected = project.getLastCollectedAt();
        String interval = project.getSettings() != null ? project.getSettings().getCollectInterval() : "daily";

        LocalDateTime nextCollection = null;
        if (lastCollected != null && autoCollectEnabled) {
            nextCollection = switch (interval) {
                case "hourly" -> lastCollected.plusHours(1);
                case "weekly" -> lastCollected.plusWeeks(1);
                default -> lastCollected.plusDays(1); // daily
            };
        }

        return Map.of(
                "projectId", projectId,
                "autoCollectEnabled", autoCollectEnabled,
                "interval", interval,
                "lastCollectedAt", lastCollected != null ? lastCollected.toString() : null,
                "nextCollectionAt", nextCollection != null ? nextCollection.toString() : null,
                "keywords", project.getKeywords() != null ? project.getKeywords() : List.of()
        );
    }

    /**
     * Update auto-collect settings for a project.
     */
    @Transactional
    public void updateAutoCollectSettings(Long projectId, boolean enabled, String interval, String userId) {
        Project project = projectRepository.findById(projectId)
                .orElseThrow(() -> new IllegalArgumentException("Project not found: " + projectId));

        Project.ProjectSettings settings = project.getSettings();
        if (settings == null) {
            settings = Project.ProjectSettings.builder().build();
        }

        settings.setAutoCollect(enabled);
        if (interval != null) {
            settings.setCollectInterval(interval);
        }

        project.setSettings(settings);
        projectRepository.save(project);

        // Log activity
        projectService.logActivity(
                projectId,
                userId,
                ProjectActivityLog.ActivityType.SETTINGS_CHANGED,
                "자동 수집 설정 변경: " + (enabled ? "활성화" : "비활성화"),
                "project",
                projectId.toString(),
                Map.of("autoCollect", enabled, "interval", interval != null ? interval : settings.getCollectInterval())
        );

        log.info("Updated auto-collect settings: projectId={}, enabled={}, interval={}", 
                projectId, enabled, interval);
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/ProjectService.java

```java
package com.newsinsight.collector.service;

import com.newsinsight.collector.entity.project.*;
import com.newsinsight.collector.repository.*;
import lombok.Builder;
import lombok.Data;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.PageRequest;
import org.springframework.data.domain.Pageable;
import org.springframework.data.domain.Sort;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.time.LocalDateTime;
import java.util.*;

/**
 * Service for managing Projects.
 * Provides CRUD operations for projects, members, items, and activity logging.
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class ProjectService {

    private final ProjectRepository projectRepository;
    private final ProjectMemberRepository projectMemberRepository;
    private final ProjectItemRepository projectItemRepository;
    private final ProjectActivityLogRepository activityLogRepository;
    private final ProjectNotificationRepository notificationRepository;

    // ============================================
    // Project CRUD
    // ============================================

    /**
     * Create a new project.
     */
    @Transactional
    public Project createProject(CreateProjectRequest request) {
        Project project = Project.builder()
                .name(request.getName())
                .description(request.getDescription())
                .keywords(request.getKeywords())
                .category(request.getCategory() != null ? request.getCategory() : Project.ProjectCategory.CUSTOM)
                .status(Project.ProjectStatus.ACTIVE)
                .visibility(request.getVisibility() != null ? request.getVisibility() : Project.ProjectVisibility.PRIVATE)
                .ownerId(request.getOwnerId())
                .color(request.getColor())
                .icon(request.getIcon())
                .isDefault(request.getIsDefault() != null && request.getIsDefault())
                .settings(request.getSettings())
                .tags(request.getTags())
                .lastActivityAt(LocalDateTime.now())
                .build();

        Project saved = projectRepository.save(project);
        log.info("Created project: id={}, name='{}', owner={}", saved.getId(), saved.getName(), saved.getOwnerId());

        // Add owner as admin member
        addMember(saved.getId(), request.getOwnerId(), ProjectMember.MemberRole.ADMIN, null);

        // Log activity
        logActivity(saved.getId(), request.getOwnerId(), ProjectActivityLog.ActivityType.PROJECT_CREATED,
                "프로젝트 생성: " + saved.getName(), "project", saved.getId().toString(), null);

        return saved;
    }

    /**
     * Get project by ID.
     */
    public Optional<Project> getProject(Long id) {
        return projectRepository.findById(id);
    }

    /**
     * Get project with access check.
     */
    public Optional<Project> getProjectWithAccess(Long id, String userId) {
        Optional<Project> project = projectRepository.findById(id);
        if (project.isEmpty()) {
            return Optional.empty();
        }

        Project p = project.get();
        
        // Owner always has access
        if (p.getOwnerId().equals(userId)) {
            return project;
        }

        // Public projects are accessible to all
        if (p.getVisibility() == Project.ProjectVisibility.PUBLIC) {
            return project;
        }

        // Check membership for team projects
        if (p.getVisibility() == Project.ProjectVisibility.TEAM) {
            boolean isMember = projectMemberRepository.existsByProjectIdAndUserIdAndStatus(
                    id, userId, ProjectMember.MemberStatus.ACTIVE
            );
            if (isMember) {
                return project;
            }
        }

        return Optional.empty();
    }

    /**
     * Update project.
     */
    @Transactional
    public Project updateProject(Long id, UpdateProjectRequest request, String userId) {
        Project project = projectRepository.findById(id)
                .orElseThrow(() -> new IllegalArgumentException("Project not found: " + id));

        // Check permission
        if (!hasEditPermission(id, userId)) {
            throw new IllegalStateException("User does not have edit permission for this project");
        }

        if (request.getName() != null) {
            project.setName(request.getName());
        }
        if (request.getDescription() != null) {
            project.setDescription(request.getDescription());
        }
        if (request.getKeywords() != null) {
            project.setKeywords(request.getKeywords());
        }
        if (request.getCategory() != null) {
            project.setCategory(request.getCategory());
        }
        if (request.getVisibility() != null) {
            project.setVisibility(request.getVisibility());
        }
        if (request.getColor() != null) {
            project.setColor(request.getColor());
        }
        if (request.getIcon() != null) {
            project.setIcon(request.getIcon());
        }
        if (request.getSettings() != null) {
            project.setSettings(request.getSettings());
        }
        if (request.getTags() != null) {
            project.setTags(request.getTags());
        }

        project.touchActivity();
        Project saved = projectRepository.save(project);

        // Log activity
        logActivity(id, userId, ProjectActivityLog.ActivityType.PROJECT_UPDATED,
                "프로젝트 수정", "project", id.toString(), null);

        log.info("Updated project: id={}, name='{}'", saved.getId(), saved.getName());
        return saved;
    }

    /**
     * Update project status.
     */
    @Transactional
    public Project updateProjectStatus(Long id, Project.ProjectStatus status, String userId) {
        Project project = projectRepository.findById(id)
                .orElseThrow(() -> new IllegalArgumentException("Project not found: " + id));

        if (!project.getOwnerId().equals(userId)) {
            throw new IllegalStateException("Only owner can change project status");
        }

        project.setStatus(status);
        project.touchActivity();
        
        Project saved = projectRepository.save(project);

        // Log activity
        logActivity(id, userId, ProjectActivityLog.ActivityType.PROJECT_STATUS_CHANGED,
                "프로젝트 상태 변경: " + status, "project", id.toString(), null);

        return saved;
    }

    /**
     * Delete project.
     */
    @Transactional
    public void deleteProject(Long id, String userId) {
        Project project = projectRepository.findById(id)
                .orElseThrow(() -> new IllegalArgumentException("Project not found: " + id));

        if (!project.getOwnerId().equals(userId)) {
            throw new IllegalStateException("Only owner can delete the project");
        }

        // Delete all related data
        notificationRepository.deleteByProjectId(id);
        activityLogRepository.deleteByProjectId(id);
        projectItemRepository.deleteByProjectId(id);
        projectMemberRepository.deleteByProjectId(id);
        projectRepository.delete(project);

        log.info("Deleted project: id={}, name='{}'", id, project.getName());
    }

    /**
     * Get projects by owner.
     */
    public Page<Project> getProjectsByOwner(String ownerId, int page, int size) {
        Pageable pageable = PageRequest.of(page, size);
        return projectRepository.findByOwnerIdOrderByLastActivityAtDesc(ownerId, pageable);
    }

    /**
     * Get projects by owner and status.
     */
    public Page<Project> getProjectsByOwnerAndStatus(String ownerId, Project.ProjectStatus status, int page, int size) {
        Pageable pageable = PageRequest.of(page, size);
        return projectRepository.findByOwnerIdAndStatus(ownerId, status, pageable);
    }

    /**
     * Search projects by name.
     */
    public Page<Project> searchProjects(String name, int page, int size) {
        Pageable pageable = PageRequest.of(page, size);
        return projectRepository.searchByName(name, pageable);
    }

    /**
     * Get user's default project (create if not exists).
     */
    @Transactional
    public Project getOrCreateDefaultProject(String userId) {
        return projectRepository.findByOwnerIdAndIsDefaultTrue(userId)
                .orElseGet(() -> {
                    CreateProjectRequest request = CreateProjectRequest.builder()
                            .name("My Project")
                            .description("기본 프로젝트")
                            .ownerId(userId)
                            .isDefault(true)
                            .build();
                    return createProject(request);
                });
    }

    // ============================================
    // Member Management
    // ============================================

    /**
     * Add member to project.
     */
    @Transactional
    public ProjectMember addMember(Long projectId, String userId, ProjectMember.MemberRole role, String invitedBy) {
        // Check if already a member
        Optional<ProjectMember> existing = projectMemberRepository.findByProjectIdAndUserId(projectId, userId);
        if (existing.isPresent()) {
            ProjectMember member = existing.get();
            if (member.getStatus() == ProjectMember.MemberStatus.ACTIVE) {
                return member;
            }
            // Reactivate if previously left
            member.setStatus(ProjectMember.MemberStatus.ACTIVE);
            member.setRole(role);
            return projectMemberRepository.save(member);
        }

        ProjectMember member = ProjectMember.builder()
                .projectId(projectId)
                .userId(userId)
                .role(role)
                .status(ProjectMember.MemberStatus.ACTIVE)
                .invitedBy(invitedBy)
                .joinedAt(LocalDateTime.now())
                .lastActiveAt(LocalDateTime.now())
                .build();

        ProjectMember saved = projectMemberRepository.save(member);

        // Log activity
        if (invitedBy != null) {
            logActivity(projectId, invitedBy, ProjectActivityLog.ActivityType.MEMBER_ADDED,
                    "멤버 추가: " + userId, "member", saved.getId().toString(), null);
        }

        // Update project activity
        projectRepository.updateLastActivity(projectId, LocalDateTime.now());

        return saved;
    }

    /**
     * Invite member (creates pending invitation).
     */
    @Transactional
    public ProjectMember inviteMember(Long projectId, String userId, ProjectMember.MemberRole role, String invitedBy) {
        // Check inviter has permission
        if (!hasInvitePermission(projectId, invitedBy)) {
            throw new IllegalStateException("User does not have permission to invite members");
        }

        // Check if already invited or member
        Optional<ProjectMember> existing = projectMemberRepository.findByProjectIdAndUserId(projectId, userId);
        if (existing.isPresent()) {
            throw new IllegalStateException("User is already invited or a member");
        }

        String inviteToken = UUID.randomUUID().toString();

        ProjectMember member = ProjectMember.builder()
                .projectId(projectId)
                .userId(userId)
                .role(role)
                .status(ProjectMember.MemberStatus.PENDING)
                .invitedBy(invitedBy)
                .inviteToken(inviteToken)
                .inviteExpiresAt(LocalDateTime.now().plusDays(7))
                .build();

        ProjectMember saved = projectMemberRepository.save(member);

        // Create notification
        createNotification(projectId, userId, ProjectNotification.NotificationType.MEMBER_INVITED,
                "프로젝트 초대", "프로젝트에 초대되었습니다", null);

        log.info("Invited member: projectId={}, userId={}, invitedBy={}", projectId, userId, invitedBy);
        return saved;
    }

    /**
     * Accept invitation.
     */
    @Transactional
    public ProjectMember acceptInvitation(String inviteToken, String userId) {
        ProjectMember member = projectMemberRepository.findByInviteToken(inviteToken)
                .orElseThrow(() -> new IllegalArgumentException("Invalid or expired invitation"));

        if (!member.getUserId().equals(userId)) {
            throw new IllegalStateException("Invitation is for a different user");
        }

        if (member.getInviteExpiresAt() != null && member.getInviteExpiresAt().isBefore(LocalDateTime.now())) {
            throw new IllegalStateException("Invitation has expired");
        }

        member.setStatus(ProjectMember.MemberStatus.ACTIVE);
        member.setJoinedAt(LocalDateTime.now());
        member.setInviteToken(null);
        member.setInviteExpiresAt(null);

        ProjectMember saved = projectMemberRepository.save(member);

        // Log activity
        logActivity(member.getProjectId(), userId, ProjectActivityLog.ActivityType.MEMBER_JOINED,
                "멤버 참여", "member", saved.getId().toString(), null);

        return saved;
    }

    /**
     * Remove member from project.
     */
    @Transactional
    public void removeMember(Long projectId, String userId, String removedBy) {
        ProjectMember member = projectMemberRepository.findByProjectIdAndUserId(projectId, userId)
                .orElseThrow(() -> new IllegalArgumentException("Member not found"));

        // Check permission
        if (!canRemoveMember(projectId, removedBy, member)) {
            throw new IllegalStateException("User does not have permission to remove this member");
        }

        member.setStatus(ProjectMember.MemberStatus.LEFT);
        projectMemberRepository.save(member);

        // Log activity
        logActivity(projectId, removedBy, ProjectActivityLog.ActivityType.MEMBER_REMOVED,
                "멤버 제거: " + userId, "member", member.getId().toString(), null);

        log.info("Removed member: projectId={}, userId={}, removedBy={}", projectId, userId, removedBy);
    }

    /**
     * Update member role.
     */
    @Transactional
    public ProjectMember updateMemberRole(Long projectId, String userId, ProjectMember.MemberRole newRole, String updatedBy) {
        ProjectMember member = projectMemberRepository.findByProjectIdAndUserId(projectId, userId)
                .orElseThrow(() -> new IllegalArgumentException("Member not found"));

        // Only admin or owner can change roles
        if (!hasAdminPermission(projectId, updatedBy)) {
            throw new IllegalStateException("User does not have permission to change roles");
        }

        member.setRole(newRole);
        ProjectMember saved = projectMemberRepository.save(member);

        // Log activity
        logActivity(projectId, updatedBy, ProjectActivityLog.ActivityType.MEMBER_ROLE_CHANGED,
                "멤버 역할 변경: " + userId + " -> " + newRole, "member", member.getId().toString(), null);

        return saved;
    }

    /**
     * Get project members.
     */
    public List<ProjectMember> getMembers(Long projectId) {
        return projectMemberRepository.findByProjectIdOrderByJoinedAtDesc(projectId);
    }

    /**
     * Get active members.
     */
    public List<ProjectMember> getActiveMembers(Long projectId) {
        return projectMemberRepository.findByProjectIdAndStatus(projectId, ProjectMember.MemberStatus.ACTIVE);
    }

    // ============================================
    // Item Management
    // ============================================

    /**
     * Add item to project.
     */
    @Transactional
    public ProjectItem addItem(Long projectId, AddItemRequest request, String userId) {
        // Check permission
        if (!hasEditPermission(projectId, userId)) {
            throw new IllegalStateException("User does not have permission to add items");
        }

        ProjectItem item = ProjectItem.builder()
                .projectId(projectId)
                .itemType(request.getItemType())
                .title(request.getTitle())
                .summary(request.getSummary())
                .url(request.getUrl())
                .thumbnailUrl(request.getThumbnailUrl())
                .sourceName(request.getSourceName())
                .sourceId(request.getSourceId())
                .sourceType(request.getSourceType())
                .publishedAt(request.getPublishedAt())
                .category(request.getCategory())
                .tags(request.getTags())
                .sentimentLabel(request.getSentiment())
                .importance(request.getImportance() != null ? request.getImportance() : 50)
                .addedBy(userId)
                .addedAt(LocalDateTime.now())
                .isRead(false)
                .bookmarked(false)
                .metadata(request.getMetadata())
                .build();

        ProjectItem saved = projectItemRepository.save(item);

        // Log activity
        logActivity(projectId, userId, ProjectActivityLog.ActivityType.ITEM_ADDED,
                "아이템 추가: " + item.getTitle(), "item", saved.getId().toString(), null);

        // Update project activity
        projectRepository.updateLastActivity(projectId, LocalDateTime.now());

        return saved;
    }

    /**
     * Get project items.
     */
    public Page<ProjectItem> getItems(Long projectId, int page, int size) {
        Pageable pageable = PageRequest.of(page, size);
        return projectItemRepository.findByProjectIdOrderByAddedAtDesc(projectId, pageable);
    }

    /**
     * Get project items by type.
     */
    public Page<ProjectItem> getItemsByType(Long projectId, ProjectItem.ItemType type, int page, int size) {
        Pageable pageable = PageRequest.of(page, size);
        return projectItemRepository.findByProjectIdAndItemType(projectId, type, pageable);
    }

    /**
     * Search items.
     */
    public Page<ProjectItem> searchItems(Long projectId, String query, int page, int size) {
        Pageable pageable = PageRequest.of(page, size);
        return projectItemRepository.searchByContent(projectId, query, pageable);
    }

    /**
     * Mark item as read.
     */
    @Transactional
    public void markItemAsRead(Long itemId, String userId) {
        projectItemRepository.markAsRead(itemId);
    }

    /**
     * Toggle item bookmark.
     */
    @Transactional
    public void toggleItemBookmark(Long itemId, String userId) {
        projectItemRepository.toggleBookmark(itemId);
    }

    /**
     * Delete item.
     */
    @Transactional
    public void deleteItem(Long projectId, Long itemId, String userId) {
        if (!hasEditPermission(projectId, userId)) {
            throw new IllegalStateException("User does not have permission to delete items");
        }

        projectItemRepository.deleteById(itemId);

        logActivity(projectId, userId, ProjectActivityLog.ActivityType.ITEM_DELETED,
                "아이템 삭제", "item", itemId.toString(), null);
    }

    // ============================================
    // Activity Log
    // ============================================

    /**
     * Log activity.
     */
    @Transactional
    public ProjectActivityLog logActivity(Long projectId, String userId, ProjectActivityLog.ActivityType type,
                                          String description, String entityType, String entityId, Map<String, Object> metadata) {
        ProjectActivityLog activityLog = ProjectActivityLog.builder()
                .projectId(projectId)
                .userId(userId)
                .activityType(type)
                .description(description)
                .entityType(entityType)
                .entityId(entityId)
                .metadata(metadata)
                .build();

        return activityLogRepository.save(activityLog);
    }

    /**
     * Get project activity log.
     */
    public Page<ProjectActivityLog> getActivityLog(Long projectId, int page, int size) {
        Pageable pageable = PageRequest.of(page, size);
        return activityLogRepository.findByProjectIdOrderByCreatedAtDesc(projectId, pageable);
    }

    /**
     * Get recent activity.
     */
    public List<ProjectActivityLog> getRecentActivity(Long projectId) {
        return activityLogRepository.findTop20ByProjectIdOrderByCreatedAtDesc(projectId);
    }

    // ============================================
    // Notifications
    // ============================================

    /**
     * Create notification.
     */
    @Transactional
    public ProjectNotification createNotification(Long projectId, String userId, 
                                                   ProjectNotification.NotificationType type,
                                                   String title, String message, String actionUrl) {
        ProjectNotification notification = ProjectNotification.builder()
                .projectId(projectId)
                .userId(userId)
                .notificationType(type)
                .title(title)
                .message(message)
                .actionUrl(actionUrl)
                .isRead(false)
                .build();

        return notificationRepository.save(notification);
    }

    /**
     * Get user notifications.
     */
    public Page<ProjectNotification> getUserNotifications(String userId, int page, int size) {
        Pageable pageable = PageRequest.of(page, size);
        return notificationRepository.findByUserIdOrderByCreatedAtDesc(userId, pageable);
    }

    /**
     * Get unread notifications.
     */
    public List<ProjectNotification> getUnreadNotifications(String userId) {
        return notificationRepository.findByUserIdAndIsReadFalseOrderByCreatedAtDesc(userId);
    }

    /**
     * Mark notification as read.
     */
    @Transactional
    public void markNotificationAsRead(Long notificationId) {
        notificationRepository.markAsRead(notificationId);
    }

    /**
     * Mark all notifications as read.
     */
    @Transactional
    public void markAllNotificationsAsRead(String userId) {
        notificationRepository.markAllAsRead(userId);
    }

    // ============================================
    // Statistics
    // ============================================

    /**
     * Get project statistics.
     */
    public Map<String, Object> getProjectStats(Long projectId) {
        long itemCount = projectItemRepository.countByProjectId(projectId);
        long unreadCount = projectItemRepository.countByProjectIdAndIsReadFalse(projectId);
        long memberCount = projectMemberRepository.countByProjectIdAndStatus(projectId, ProjectMember.MemberStatus.ACTIVE);
        List<String> categories = projectItemRepository.findDistinctCategories(projectId);

        return Map.of(
                "itemCount", itemCount,
                "unreadCount", unreadCount,
                "memberCount", memberCount,
                "categories", categories
        );
    }

    // ============================================
    // Permission Helpers
    // ============================================

    private boolean hasEditPermission(Long projectId, String userId) {
        Project project = projectRepository.findById(projectId).orElse(null);
        if (project == null) return false;
        
        if (project.getOwnerId().equals(userId)) return true;

        Optional<ProjectMember> member = projectMemberRepository.findByProjectIdAndUserId(projectId, userId);
        if (member.isEmpty() || member.get().getStatus() != ProjectMember.MemberStatus.ACTIVE) {
            return false;
        }

        ProjectMember.MemberRole role = member.get().getRole();
        return role == ProjectMember.MemberRole.ADMIN || role == ProjectMember.MemberRole.EDITOR;
    }

    private boolean hasAdminPermission(Long projectId, String userId) {
        Project project = projectRepository.findById(projectId).orElse(null);
        if (project == null) return false;
        
        if (project.getOwnerId().equals(userId)) return true;

        Optional<ProjectMember> member = projectMemberRepository.findByProjectIdAndUserId(projectId, userId);
        return member.isPresent() 
                && member.get().getStatus() == ProjectMember.MemberStatus.ACTIVE
                && member.get().getRole() == ProjectMember.MemberRole.ADMIN;
    }

    private boolean hasInvitePermission(Long projectId, String userId) {
        return hasAdminPermission(projectId, userId);
    }

    private boolean canRemoveMember(Long projectId, String removedBy, ProjectMember member) {
        Project project = projectRepository.findById(projectId).orElse(null);
        if (project == null) return false;

        // Owner can remove anyone
        if (project.getOwnerId().equals(removedBy)) return true;

        // Member can remove themselves
        if (member.getUserId().equals(removedBy)) return true;

        // Admin can remove non-admin members
        if (hasAdminPermission(projectId, removedBy) && member.getRole() != ProjectMember.MemberRole.ADMIN) {
            return true;
        }

        return false;
    }

    // ============================================
    // DTOs
    // ============================================

    @Data
    @Builder
    public static class CreateProjectRequest {
        private String name;
        private String description;
        private List<String> keywords;
        private Project.ProjectCategory category;
        private Project.ProjectVisibility visibility;
        private String ownerId;
        private String color;
        private String icon;
        private Boolean isDefault;
        private Project.ProjectSettings settings;
        private List<String> tags;
    }

    @Data
    @Builder
    public static class UpdateProjectRequest {
        private String name;
        private String description;
        private List<String> keywords;
        private Project.ProjectCategory category;
        private Project.ProjectVisibility visibility;
        private String color;
        private String icon;
        private Project.ProjectSettings settings;
        private List<String> tags;
    }

    @Data
    @Builder
    public static class AddItemRequest {
        private ProjectItem.ItemType itemType;
        private String title;
        private String summary;
        private String url;
        private String thumbnailUrl;
        private String sourceName;
        private String sourceId;
        private String sourceType;
        private LocalDateTime publishedAt;
        private String category;
        private List<String> tags;
        private String sentiment;
        private Integer importance;
        private Map<String, Object> metadata;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/ProxyRotationService.java

```java
package com.newsinsight.collector.service;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.Builder;
import lombok.Data;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.MediaType;
import org.springframework.stereotype.Service;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Mono;

import java.net.InetSocketAddress;
import java.net.ProxySelector;
import java.net.URI;
import java.net.http.HttpClient;
import java.time.Duration;
import java.util.Map;
import java.util.concurrent.atomic.AtomicReference;

/**
 * IP Rotation 서비스와 통신하여 프록시를 가져오고 결과를 기록하는 서비스.
 * 
 * 429 (Too Many Requests) 또는 403 (Forbidden) 에러 시 
 * 다른 IP를 통해 재시도할 수 있도록 프록시를 제공합니다.
 * 
 * IP-rotation 서비스 API:
 * - GET /proxy/next - 다음 프록시 가져오기
 * - POST /proxy/record - 성공/실패 결과 기록
 */
@Service
@Slf4j
public class ProxyRotationService {

    private final WebClient webClient;
    private final ObjectMapper objectMapper;
    private final AtomicReference<ProxyInfo> cachedProxy = new AtomicReference<>();

    @Value("${collector.ip-rotation.enabled:true}")
    private boolean enabled;

    @Value("${collector.ip-rotation.base-url:http://ip-rotation:8050}")
    private String ipRotationBaseUrl;

    @Value("${collector.ip-rotation.timeout-seconds:5}")
    private int timeoutSeconds;

    @Value("${collector.ip-rotation.max-retries:3}")
    private int maxRetries;

    public ProxyRotationService(WebClient.Builder webClientBuilder, ObjectMapper objectMapper) {
        this.webClient = webClientBuilder.build();
        this.objectMapper = objectMapper;
    }

    /**
     * IP Rotation이 활성화되어 있는지 확인
     */
    public boolean isEnabled() {
        return enabled;
    }

    /**
     * 다음 프록시를 가져옵니다.
     * 
     * @return 프록시 정보 또는 사용 불가 시 null
     */
    public Mono<ProxyInfo> getNextProxy() {
        if (!enabled) {
            return Mono.empty();
        }

        return webClient.get()
                .uri(ipRotationBaseUrl + "/proxy/next")
                .accept(MediaType.APPLICATION_JSON)
                .retrieve()
                .bodyToMono(String.class)
                .timeout(Duration.ofSeconds(timeoutSeconds))
                .flatMap(response -> {
                    try {
                        JsonNode node = objectMapper.readTree(response);
                        
                        ProxyInfo proxy = ProxyInfo.builder()
                                .proxyId(node.path("proxyId").asText())
                                .address(node.path("address").asText())
                                .protocol(node.path("protocol").asText("http"))
                                .username(node.path("username").asText(null))
                                .password(node.path("password").asText(null))
                                .country(node.path("country").asText(null))
                                .healthStatus(node.path("healthStatus").asText("unknown"))
                                .build();
                        
                        if (proxy.getAddress() == null || proxy.getAddress().isBlank()) {
                            log.warn("IP-rotation returned empty proxy address");
                            return Mono.empty();
                        }
                        
                        cachedProxy.set(proxy);
                        log.debug("Got proxy from IP-rotation: {} ({})", proxy.getAddress(), proxy.getCountry());
                        return Mono.just(proxy);
                    } catch (Exception e) {
                        log.warn("Failed to parse proxy response: {}", e.getMessage());
                        return Mono.empty();
                    }
                })
                .onErrorResume(e -> {
                    log.warn("Failed to get proxy from IP-rotation: {}", e.getMessage());
                    return Mono.empty();
                });
    }

    /**
     * 동기적으로 다음 프록시를 가져옵니다.
     */
    public ProxyInfo getNextProxyBlocking() {
        if (!enabled) {
            return null;
        }
        
        try {
            return getNextProxy().block(Duration.ofSeconds(timeoutSeconds));
        } catch (Exception e) {
            log.warn("Failed to get proxy (blocking): {}", e.getMessage());
            return null;
        }
    }

    /**
     * 프록시 사용 결과를 기록합니다.
     * 
     * @param proxyId 프록시 ID
     * @param success 성공 여부
     * @param latencyMs 응답 시간 (밀리초)
     * @param reason 실패 사유 (실패 시)
     */
    public Mono<Void> recordResult(String proxyId, boolean success, long latencyMs, String reason) {
        if (!enabled || proxyId == null || proxyId.isBlank()) {
            return Mono.empty();
        }

        Map<String, Object> body = Map.of(
                "proxyId", proxyId,
                "success", success,
                "latencyMs", latencyMs,
                "reason", reason != null ? reason : ""
        );

        return webClient.post()
                .uri(ipRotationBaseUrl + "/proxy/record")
                .contentType(MediaType.APPLICATION_JSON)
                .bodyValue(body)
                .retrieve()
                .bodyToMono(String.class)
                .timeout(Duration.ofSeconds(timeoutSeconds))
                .doOnSuccess(r -> log.debug("Recorded proxy result: {} success={}", proxyId, success))
                .onErrorResume(e -> {
                    log.warn("Failed to record proxy result: {}", e.getMessage());
                    return Mono.empty();
                })
                .then();
    }

    /**
     * 프록시 사용 성공을 기록합니다.
     */
    public void recordSuccess(String proxyId, long latencyMs) {
        recordResult(proxyId, true, latencyMs, null).subscribe();
    }

    /**
     * 프록시 사용 실패를 기록합니다.
     */
    public void recordFailure(String proxyId, String reason) {
        recordResult(proxyId, false, 0, reason).subscribe();
    }

    /**
     * 마지막으로 가져온 프록시 정보
     */
    public ProxyInfo getCachedProxy() {
        return cachedProxy.get();
    }

    /**
     * 프록시 주소를 파싱하여 호스트와 포트를 추출합니다.
     * 
     * @param address 프록시 주소 (예: "192.168.1.1:8080" 또는 "http://proxy.example.com:3128")
     * @return InetSocketAddress 또는 파싱 실패 시 null
     */
    public InetSocketAddress parseProxyAddress(String address) {
        if (address == null || address.isBlank()) {
            return null;
        }

        try {
            // http:// 또는 https:// 프리픽스 제거
            String cleanAddress = address;
            if (cleanAddress.startsWith("http://")) {
                cleanAddress = cleanAddress.substring(7);
            } else if (cleanAddress.startsWith("https://")) {
                cleanAddress = cleanAddress.substring(8);
            }

            // host:port 분리
            int colonIndex = cleanAddress.lastIndexOf(':');
            if (colonIndex > 0 && colonIndex < cleanAddress.length() - 1) {
                String host = cleanAddress.substring(0, colonIndex);
                int port = Integer.parseInt(cleanAddress.substring(colonIndex + 1));
                return new InetSocketAddress(host, port);
            }
        } catch (Exception e) {
            log.warn("Failed to parse proxy address '{}': {}", address, e.getMessage());
        }
        return null;
    }

    /**
     * 프록시를 사용하는 HttpClient를 생성합니다.
     * 
     * @param proxyInfo 프록시 정보
     * @return HttpClient 또는 프록시 사용 불가 시 null
     */
    public HttpClient createProxiedHttpClient(ProxyInfo proxyInfo) {
        if (proxyInfo == null) {
            return null;
        }

        InetSocketAddress proxyAddress = parseProxyAddress(proxyInfo.getAddress());
        if (proxyAddress == null) {
            return null;
        }

        try {
            HttpClient.Builder builder = HttpClient.newBuilder()
                    .proxy(ProxySelector.of(proxyAddress))
                    .connectTimeout(Duration.ofSeconds(10));

            // 인증이 필요한 경우 Authenticator 설정 가능 (현재는 미구현)
            // 프록시 인증은 대부분 URL에 포함되거나 별도 처리 필요

            return builder.build();
        } catch (Exception e) {
            log.warn("Failed to create proxied HttpClient: {}", e.getMessage());
            return null;
        }
    }

    /**
     * 프록시 정보
     */
    @Data
    @Builder
    public static class ProxyInfo {
        private String proxyId;
        private String address;
        private String protocol;
        private String username;
        private String password;
        private String country;
        private String healthStatus;

        /**
         * 프록시 URL을 생성합니다 (인증 정보 포함).
         */
        public String toProxyUrl() {
            StringBuilder sb = new StringBuilder();
            sb.append(protocol != null ? protocol : "http").append("://");
            
            if (username != null && !username.isBlank()) {
                sb.append(username);
                if (password != null && !password.isBlank()) {
                    sb.append(":").append(password);
                }
                sb.append("@");
            }
            
            sb.append(address);
            return sb.toString();
        }
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/RateLimitRetryService.java

```java
package com.newsinsight.collector.service;

import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.HttpStatusCode;
import org.springframework.stereotype.Service;
import org.springframework.web.reactive.function.client.WebClient;
import org.springframework.web.reactive.function.client.WebClientResponseException;
import reactor.core.publisher.Mono;
import reactor.util.retry.Retry;

import java.net.InetSocketAddress;
import java.net.URI;
import java.net.http.HttpClient;
import java.net.http.HttpRequest;
import java.net.http.HttpResponse;
import java.time.Duration;
import java.util.Set;
import java.util.concurrent.CompletableFuture;
import java.util.function.Function;

/**
 * Rate Limit (429, 403) 에러 발생 시 IP Rotation을 통해 재시도하는 서비스.
 * 
 * 사용 방법:
 * 1. 일반 WebClient 요청 실행
 * 2. 429/403 에러 발생 시 ProxyRotationService에서 새 프록시 가져오기
 * 3. 프록시를 통해 요청 재시도
 * 4. 결과를 ProxyRotationService에 기록
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class RateLimitRetryService {

    private final ProxyRotationService proxyRotationService;
    private final WebClient webClient;

    @Value("${collector.ip-rotation.max-retries:3}")
    private int maxRetries;

    @Value("${collector.ip-rotation.retry-delay-ms:1000}")
    private long retryDelayMs;

    @Value("${collector.ip-rotation.timeout-seconds:15}")
    private int timeoutSeconds;

    // 프록시를 통해 재시도해야 하는 HTTP 상태 코드
    private static final Set<Integer> RETRYABLE_STATUS_CODES = Set.of(
            429, // Too Many Requests
            403, // Forbidden (rate limit or IP blocked)
            503  // Service Unavailable (sometimes used for rate limiting)
    );

    /**
     * GET 요청을 실행하고, Rate Limit 에러 시 프록시를 통해 재시도합니다.
     * 
     * @param url 요청 URL
     * @param headers 추가 헤더 (키-값 쌍, 예: "Authorization", "Bearer xxx")
     * @return 응답 본문
     */
    public Mono<String> executeWithRetry(String url, String... headers) {
        return executeRequest(url, headers)
                .onErrorResume(e -> {
                    if (isRetryableError(e)) {
                        log.info("Rate limit detected for URL: {}, attempting proxy retry", url);
                        return retryWithProxy(url, headers);
                    }
                    return Mono.error(e);
                });
    }

    /**
     * GET 요청을 동기적으로 실행하고, Rate Limit 에러 시 프록시를 통해 재시도합니다.
     * 
     * @param url 요청 URL
     * @param headers 추가 헤더 (키-값 쌍)
     * @return 응답 본문 또는 실패 시 null
     */
    public String executeWithRetryBlocking(String url, String... headers) {
        try {
            return executeWithRetry(url, headers)
                    .block(Duration.ofSeconds(timeoutSeconds * (maxRetries + 1)));
        } catch (Exception e) {
            log.warn("Request failed after retries: {} - {}", url, e.getMessage());
            return null;
        }
    }

    /**
     * WebClient를 사용한 기본 GET 요청
     */
    private Mono<String> executeRequest(String url, String... headers) {
        WebClient.RequestHeadersSpec<?> request = webClient.get()
                .uri(url);

        // 헤더 추가
        if (headers != null && headers.length >= 2) {
            for (int i = 0; i < headers.length - 1; i += 2) {
                request = request.header(headers[i], headers[i + 1]);
            }
        }

        return request
                .retrieve()
                .bodyToMono(String.class)
                .timeout(Duration.ofSeconds(timeoutSeconds));
    }

    /**
     * 프록시를 통한 재시도
     */
    private Mono<String> retryWithProxy(String url, String... headers) {
        if (!proxyRotationService.isEnabled()) {
            log.debug("IP rotation disabled, skipping proxy retry");
            return Mono.empty();
        }

        return Mono.defer(() -> attemptProxyRequest(url, 0, headers));
    }

    /**
     * 프록시 요청 시도 (재귀적)
     */
    private Mono<String> attemptProxyRequest(String url, int attempt, String... headers) {
        if (attempt >= maxRetries) {
            log.warn("Max proxy retries ({}) exceeded for URL: {}", maxRetries, url);
            return Mono.empty();
        }

        return proxyRotationService.getNextProxy()
                .flatMap(proxyInfo -> {
                    log.debug("Attempting proxy request (attempt {}/{}): {} via {}", 
                            attempt + 1, maxRetries, url, proxyInfo.getAddress());
                    
                    long startTime = System.currentTimeMillis();
                    
                    return executeViaProxy(url, proxyInfo, headers)
                            .doOnSuccess(response -> {
                                long latency = System.currentTimeMillis() - startTime;
                                proxyRotationService.recordSuccess(proxyInfo.getProxyId(), latency);
                                log.info("Proxy request succeeded: {} via {} ({}ms)", 
                                        url, proxyInfo.getAddress(), latency);
                            })
                            .onErrorResume(e -> {
                                String reason = e.getMessage();
                                if (e instanceof WebClientResponseException wce) {
                                    reason = "HTTP " + wce.getStatusCode().value();
                                }
                                proxyRotationService.recordFailure(proxyInfo.getProxyId(), reason);
                                log.warn("Proxy request failed (attempt {}/{}): {} via {} - {}", 
                                        attempt + 1, maxRetries, url, proxyInfo.getAddress(), reason);
                                
                                // 재시도 가능한 에러면 다음 프록시로 시도
                                if (isRetryableError(e) && attempt + 1 < maxRetries) {
                                    return Mono.delay(Duration.ofMillis(retryDelayMs))
                                            .then(attemptProxyRequest(url, attempt + 1, headers));
                                }
                                return Mono.empty();
                            });
                })
                .switchIfEmpty(Mono.defer(() -> {
                    // 프록시를 가져오지 못한 경우
                    if (attempt + 1 < maxRetries) {
                        return Mono.delay(Duration.ofMillis(retryDelayMs))
                                .then(attemptProxyRequest(url, attempt + 1, headers));
                    }
                    return Mono.empty();
                }));
    }

    /**
     * 프록시를 통해 HTTP 요청 실행 (Java HttpClient 사용)
     */
    private Mono<String> executeViaProxy(String url, ProxyRotationService.ProxyInfo proxyInfo, String... headers) {
        return Mono.fromCallable(() -> {
            InetSocketAddress proxyAddress = proxyRotationService.parseProxyAddress(proxyInfo.getAddress());
            if (proxyAddress == null) {
                throw new RuntimeException("Invalid proxy address: " + proxyInfo.getAddress());
            }

            HttpClient client = HttpClient.newBuilder()
                    .proxy(java.net.ProxySelector.of(proxyAddress))
                    .connectTimeout(Duration.ofSeconds(10))
                    .build();

            HttpRequest.Builder requestBuilder = HttpRequest.newBuilder()
                    .uri(URI.create(url))
                    .timeout(Duration.ofSeconds(timeoutSeconds))
                    .GET();

            // 헤더 추가
            if (headers != null && headers.length >= 2) {
                for (int i = 0; i < headers.length - 1; i += 2) {
                    requestBuilder.header(headers[i], headers[i + 1]);
                }
            }

            // 프록시 인증이 필요한 경우 (Basic Auth)
            if (proxyInfo.getUsername() != null && !proxyInfo.getUsername().isBlank()) {
                String auth = proxyInfo.getUsername() + ":" + 
                        (proxyInfo.getPassword() != null ? proxyInfo.getPassword() : "");
                String encodedAuth = java.util.Base64.getEncoder().encodeToString(auth.getBytes());
                requestBuilder.header("Proxy-Authorization", "Basic " + encodedAuth);
            }

            HttpResponse<String> response = client.send(
                    requestBuilder.build(),
                    HttpResponse.BodyHandlers.ofString()
            );

            int statusCode = response.statusCode();
            if (statusCode >= 200 && statusCode < 300) {
                return response.body();
            } else if (RETRYABLE_STATUS_CODES.contains(statusCode)) {
                throw new WebClientResponseException(
                        statusCode, 
                        "HTTP " + statusCode, 
                        null, null, null
                );
            } else {
                throw new RuntimeException("HTTP error: " + statusCode);
            }
        });
    }

    /**
     * 재시도 가능한 에러인지 확인
     */
    private boolean isRetryableError(Throwable e) {
        if (e instanceof WebClientResponseException wce) {
            return RETRYABLE_STATUS_CODES.contains(wce.getStatusCode().value());
        }
        String message = e.getMessage();
        if (message != null) {
            message = message.toLowerCase();
            return message.contains("429") || 
                   message.contains("403") || 
                   message.contains("too many requests") ||
                   message.contains("rate limit") ||
                   message.contains("forbidden");
        }
        return false;
    }

    /**
     * 특정 상태 코드가 재시도 가능한지 확인
     */
    public boolean isRetryableStatusCode(int statusCode) {
        return RETRYABLE_STATUS_CODES.contains(statusCode);
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/RssFeedService.java

```java
package com.newsinsight.collector.service;

import com.fasterxml.jackson.databind.ObjectMapper;
import com.newsinsight.collector.entity.CollectedData;
import com.newsinsight.collector.entity.DataSource;
import com.rometools.rome.feed.synd.SyndEntry;
import com.rometools.rome.feed.synd.SyndFeed;
import com.rometools.rome.io.SyndFeedInput;
import com.rometools.rome.io.XmlReader;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;

import java.net.HttpURLConnection;
import java.net.URL;
import java.time.LocalDateTime;
import java.time.ZoneId;
import java.util.ArrayList;
import java.util.Date;
import java.util.List;
import java.util.Map;
import java.util.stream.Collectors;

@Service
@RequiredArgsConstructor
@Slf4j
public class RssFeedService {

    private final CollectedDataService collectedDataService;
    private final ObjectMapper objectMapper;

    /**
     * 공백을 정리하여 텍스트를 정규화
     */
    private String normalizeText(String text) {
        if (text == null || text.isBlank()) {
            return "";
        }
        return text.replaceAll("\\s+", " ").trim();
    }

    /**
     * RSS 피드를 조회하고 파싱
     */
    public List<CollectedData> fetchRssFeed(DataSource source) {
        List<CollectedData> results = new ArrayList<>();
        
        try {
            log.info("Fetching RSS feed from: {}", source.getUrl());
            
            URL feedUrl = new URL(source.getUrl());
            
            // User-Agent를 설정하여 봇 차단 우회
            HttpURLConnection connection = (HttpURLConnection) feedUrl.openConnection();
            connection.setRequestProperty("User-Agent", 
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36");
            connection.setRequestProperty("Accept", "application/rss+xml, application/xml, text/xml, */*");
            connection.setConnectTimeout(10000);
            connection.setReadTimeout(30000);
            connection.setInstanceFollowRedirects(true);
            
            SyndFeedInput input = new SyndFeedInput();
            SyndFeed feed = input.build(new XmlReader(connection.getInputStream()));
            
            log.info("Found {} entries in feed: {}", feed.getEntries().size(), source.getName());
            
            for (SyndEntry entry : feed.getEntries()) {
                try {
                    CollectedData data = parseEntry(entry, source);
                    if (data != null) {
                        results.add(data);
                    }
                } catch (Exception e) {
                    log.error("Error parsing RSS entry: {}", e.getMessage(), e);
                }
            }
            
        } catch (Exception e) {
            log.error("Error fetching RSS feed from {}: {}", source.getUrl(), e.getMessage(), e);
        }
        
        return results;
    }

    /**
     * RSS 엔트리 1건을 CollectedData로 변환
     */
    private CollectedData parseEntry(SyndEntry entry, DataSource source) {
        String title = entry.getTitle();
        String description = entry.getDescription() != null ? entry.getDescription().getValue() : "";
        String link = entry.getLink();
        
        // 콘텐츠 정규화
        String content = normalizeText(description);
        
        // 콘텐츠가 너무 짧으면 스킵
        if (content.length() < 10) {
            log.debug("Skipping entry with too short content: {}", title);
            return null;
        }
        
        // 게시일 파싱
        LocalDateTime publishedDate = null;
        Date pubDate = entry.getPublishedDate() != null ? entry.getPublishedDate() : entry.getUpdatedDate();
        if (pubDate != null) {
            publishedDate = LocalDateTime.ofInstant(pubDate.toInstant(), ZoneId.systemDefault());
        }
        
        // 콘텐츠 해시 계산
        String contentHash = collectedDataService.computeContentHash(link, title, content);
        
        // 중복 여부 확인
        if (collectedDataService.isDuplicate(contentHash)) {
            log.debug("Duplicate entry detected: {}", title);
            return null;
        }
        
        // 태그/카테고리 추출
        List<String> tags = entry.getCategories() != null 
            ? entry.getCategories().stream()
                .map(cat -> cat.getName())
                .collect(Collectors.toList())
            : List.of();
        
        // 메타데이터 구성
        Map<String, Object> metadata = Map.of(
            "adapter", "rss",
            "tags", tags,
            "author", entry.getAuthor() != null ? entry.getAuthor() : "",
            "source_name", source.getName()
        );
        
        // 메타데이터를 JSON 문자열로 변환
        String metadataJson;
        try {
            metadataJson = objectMapper.writeValueAsString(metadata);
        } catch (Exception e) {
            log.warn("Failed to serialize metadata to JSON: {}", e.getMessage());
            metadataJson = "{}";
        }
        
        // CollectedData 엔티티 생성
        CollectedData data = CollectedData.builder()
                .sourceId(source.getId())
                .title(title)
                .content(content)
                .url(link)
                .publishedDate(publishedDate)
                .contentHash(contentHash)
                .metadataJson(metadataJson)
                .processed(false)
                .hasContent(true)
                .duplicate(false)
                .normalized(true)
                .build();
        
        return data;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/SearchCacheService.java

```java
package com.newsinsight.collector.service;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.stereotype.Service;

import java.nio.charset.StandardCharsets;
import java.security.MessageDigest;
import java.security.NoSuchAlgorithmException;
import java.time.Duration;
import java.util.List;
import java.util.Optional;

/**
 * Search Result Cache Service
 * 
 * Redis를 사용하여 검색 결과를 캐싱합니다.
 * - DB 검색 결과: 10분 TTL
 * - 통합 검색 결과: 5분 TTL
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class SearchCacheService {

    private final RedisTemplate<String, Object> redisTemplate;
    private final ObjectMapper objectMapper;

    private static final String DB_SEARCH_CACHE_PREFIX = "newsinsight:search:db:";
    private static final String UNIFIED_SEARCH_CACHE_PREFIX = "newsinsight:search:unified:";
    private static final Duration DB_SEARCH_TTL = Duration.ofMinutes(10);
    private static final Duration UNIFIED_SEARCH_TTL = Duration.ofMinutes(5);

    /**
     * 캐시 키 생성 (쿼리 + 윈도우 해시)
     */
    public String generateCacheKey(String query, String window) {
        String input = query.toLowerCase().trim() + ":" + (window != null ? window : "all");
        try {
            MessageDigest md = MessageDigest.getInstance("SHA-256");
            byte[] hash = md.digest(input.getBytes(StandardCharsets.UTF_8));
            StringBuilder hexString = new StringBuilder();
            for (int i = 0; i < Math.min(hash.length, 16); i++) {
                String hex = Integer.toHexString(0xff & hash[i]);
                if (hex.length() == 1) hexString.append('0');
                hexString.append(hex);
            }
            return hexString.toString();
        } catch (NoSuchAlgorithmException e) {
            // Fallback to simple hash
            return String.valueOf(input.hashCode());
        }
    }

    /**
     * DB 검색 결과 캐시 조회
     */
    @SuppressWarnings("unchecked")
    public <T> Optional<List<T>> getDbSearchResults(String query, String window, Class<T> elementType) {
        String cacheKey = DB_SEARCH_CACHE_PREFIX + generateCacheKey(query, window);
        try {
            Object cached = redisTemplate.opsForValue().get(cacheKey);
            if (cached != null) {
                log.debug("Cache HIT for DB search: query='{}', window='{}'", query, window);
                if (cached instanceof List) {
                    return Optional.of((List<T>) cached);
                }
            }
            log.debug("Cache MISS for DB search: query='{}', window='{}'", query, window);
        } catch (Exception e) {
            log.warn("Error reading from cache: {}", e.getMessage());
        }
        return Optional.empty();
    }

    /**
     * DB 검색 결과 캐시 저장
     */
    public <T> void cacheDbSearchResults(String query, String window, List<T> results) {
        if (results == null || results.isEmpty()) {
            return; // 빈 결과는 캐싱하지 않음
        }
        
        String cacheKey = DB_SEARCH_CACHE_PREFIX + generateCacheKey(query, window);
        try {
            redisTemplate.opsForValue().set(cacheKey, results, DB_SEARCH_TTL);
            log.debug("Cached DB search results: query='{}', window='{}', count={}", 
                    query, window, results.size());
        } catch (Exception e) {
            log.warn("Error caching DB search results: {}", e.getMessage());
        }
    }

    /**
     * 통합 검색 결과 캐시 조회
     */
    @SuppressWarnings("unchecked")
    public <T> Optional<T> getUnifiedSearchResults(String query, String window, Class<T> resultType) {
        String cacheKey = UNIFIED_SEARCH_CACHE_PREFIX + generateCacheKey(query, window);
        try {
            Object cached = redisTemplate.opsForValue().get(cacheKey);
            if (cached != null) {
                log.debug("Cache HIT for unified search: query='{}', window='{}'", query, window);
                return Optional.of((T) cached);
            }
            log.debug("Cache MISS for unified search: query='{}', window='{}'", query, window);
        } catch (Exception e) {
            log.warn("Error reading from cache: {}", e.getMessage());
        }
        return Optional.empty();
    }

    /**
     * 통합 검색 결과 캐시 저장
     */
    public <T> void cacheUnifiedSearchResults(String query, String window, T results) {
        if (results == null) {
            return;
        }
        
        String cacheKey = UNIFIED_SEARCH_CACHE_PREFIX + generateCacheKey(query, window);
        try {
            redisTemplate.opsForValue().set(cacheKey, results, UNIFIED_SEARCH_TTL);
            log.debug("Cached unified search results: query='{}', window='{}'", query, window);
        } catch (Exception e) {
            log.warn("Error caching unified search results: {}", e.getMessage());
        }
    }

    /**
     * 검색 캐시 무효화
     */
    public void invalidateSearchCache(String query, String window) {
        String dbKey = DB_SEARCH_CACHE_PREFIX + generateCacheKey(query, window);
        String unifiedKey = UNIFIED_SEARCH_CACHE_PREFIX + generateCacheKey(query, window);
        
        try {
            redisTemplate.delete(dbKey);
            redisTemplate.delete(unifiedKey);
            log.debug("Invalidated search cache for query='{}', window='{}'", query, window);
        } catch (Exception e) {
            log.warn("Error invalidating cache: {}", e.getMessage());
        }
    }

    /**
     * 모든 검색 캐시 클리어
     */
    public void clearAllSearchCaches() {
        try {
            redisTemplate.delete(redisTemplate.keys(DB_SEARCH_CACHE_PREFIX + "*"));
            redisTemplate.delete(redisTemplate.keys(UNIFIED_SEARCH_CACHE_PREFIX + "*"));
            log.info("Cleared all search caches");
        } catch (Exception e) {
            log.warn("Error clearing all search caches: {}", e.getMessage());
        }
    }

    /**
     * 캐시 통계 조회
     */
    public CacheStats getStats() {
        try {
            Long dbKeyCount = Optional.ofNullable(
                    redisTemplate.keys(DB_SEARCH_CACHE_PREFIX + "*")
            ).map(keys -> (long) keys.size()).orElse(0L);
            
            Long unifiedKeyCount = Optional.ofNullable(
                    redisTemplate.keys(UNIFIED_SEARCH_CACHE_PREFIX + "*")
            ).map(keys -> (long) keys.size()).orElse(0L);
            
            return new CacheStats(dbKeyCount, unifiedKeyCount);
        } catch (Exception e) {
            log.warn("Error getting cache stats: {}", e.getMessage());
            return new CacheStats(0L, 0L);
        }
    }

    public record CacheStats(Long dbSearchKeys, Long unifiedSearchKeys) {
        public Long totalKeys() {
            return dbSearchKeys + unifiedSearchKeys;
        }
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/SearchHistoryConsumerService.java

```java
package com.newsinsight.collector.service;

import com.newsinsight.collector.dto.SearchHistoryMessage;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.kafka.support.Acknowledgment;
import org.springframework.kafka.support.KafkaHeaders;
import org.springframework.messaging.handler.annotation.Header;
import org.springframework.messaging.handler.annotation.Payload;
import org.springframework.stereotype.Service;

/**
 * Kafka consumer service for search history persistence.
 * Listens to search history topic and persists records to database.
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class SearchHistoryConsumerService {

    private final SearchHistoryService searchHistoryService;

    /**
     * Consume search history messages and persist to database.
     */
    @KafkaListener(
            topics = "newsinsight.search.history",
            containerFactory = "searchHistoryKafkaListenerContainerFactory",
            groupId = "${spring.application.name:collector-service}-search-history"
    )
    public void consumeSearchHistory(
            @Payload SearchHistoryMessage message,
            @Header(KafkaHeaders.RECEIVED_KEY) String key,
            @Header(KafkaHeaders.RECEIVED_TOPIC) String topic,
            @Header(KafkaHeaders.RECEIVED_PARTITION) int partition,
            @Header(KafkaHeaders.OFFSET) long offset,
            Acknowledgment acknowledgment
    ) {
        log.debug("Received search history message: key={}, topic={}, partition={}, offset={}",
                key, topic, partition, offset);

        try {
            // Persist to database
            searchHistoryService.saveFromMessage(message);
            
            // Acknowledge successful processing
            acknowledgment.acknowledge();
            
            log.debug("Successfully processed search history: externalId={}, type={}, query='{}'",
                    message.getExternalId(), message.getSearchType(), message.getQuery());
                    
        } catch (Exception e) {
            log.error("Failed to process search history message: key={}, error={}", 
                    key, e.getMessage(), e);
            // Don't acknowledge - message will be retried or sent to DLQ
            throw e;
        }
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/SearchHistoryEventService.java

```java
package com.newsinsight.collector.service;

import com.newsinsight.collector.dto.SearchHistoryDto;
import com.newsinsight.collector.entity.search.SearchHistory;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Sinks;

import java.time.Duration;
import java.util.Map;
import java.util.concurrent.atomic.AtomicLong;

/**
 * SSE event service for real-time search history updates.
 * Broadcasts new search history entries to connected clients.
 */
@Service
@Slf4j
public class SearchHistoryEventService {

    private final Sinks.Many<SearchHistoryEventDto> eventSink;
    private final AtomicLong subscriberCount = new AtomicLong(0);

    public SearchHistoryEventService() {
        this.eventSink = Sinks.many().multicast().onBackpressureBuffer(256);
    }

    /**
     * Subscribe to the search history event stream.
     * Returns a Flux that emits events when new search history entries are saved.
     * 
     * Events include:
     * - new_search: A new search was saved
     * - updated_search: An existing search was updated
     * - deleted_search: A search was deleted
     * - heartbeat: Keep-alive signal (every 30 seconds)
     */
    public Flux<SearchHistoryEventDto> getEventStream() {
        return Flux.merge(
            eventSink.asFlux(),
            createHeartbeat()
        )
        .doOnSubscribe(sub -> {
            long count = subscriberCount.incrementAndGet();
            log.debug("New subscriber connected to search history stream. Total: {}", count);
        })
        .doOnCancel(() -> {
            long count = subscriberCount.decrementAndGet();
            log.debug("Subscriber disconnected from search history stream. Total: {}", count);
        })
        .doOnError(err -> log.error("Error in search history stream: {}", err.getMessage()));
    }

    /**
     * Get current subscriber count
     */
    public long getSubscriberCount() {
        return subscriberCount.get();
    }

    /**
     * Notify subscribers of a new search history entry.
     */
    public void notifyNewSearch(SearchHistory searchHistory) {
        if (subscriberCount.get() == 0) {
            log.debug("No subscribers, skipping event broadcast");
            return;
        }

        SearchHistoryDto dto = SearchHistoryDto.fromEntity(searchHistory);
        SearchHistoryEventDto event = new SearchHistoryEventDto(
            "new_search",
            dto,
            System.currentTimeMillis()
        );
        
        log.debug("Broadcasting new search event: type={}, query='{}'", 
                searchHistory.getSearchType(), searchHistory.getQuery());
        
        Sinks.EmitResult result = eventSink.tryEmitNext(event);
        if (result.isFailure()) {
            log.warn("Failed to emit search history event: {}", result);
        }
    }

    /**
     * Notify subscribers of an updated search history entry.
     */
    public void notifyUpdatedSearch(SearchHistory searchHistory) {
        if (subscriberCount.get() == 0) {
            return;
        }

        SearchHistoryDto dto = SearchHistoryDto.fromEntity(searchHistory);
        SearchHistoryEventDto event = new SearchHistoryEventDto(
            "updated_search",
            dto,
            System.currentTimeMillis()
        );
        
        log.debug("Broadcasting updated search event: id={}", searchHistory.getId());
        eventSink.tryEmitNext(event);
    }

    /**
     * Notify subscribers of a deleted search history entry.
     */
    public void notifyDeletedSearch(Long id) {
        if (subscriberCount.get() == 0) {
            return;
        }

        SearchHistoryEventDto event = new SearchHistoryEventDto(
            "deleted_search",
            Map.of("id", id),
            System.currentTimeMillis()
        );
        
        log.debug("Broadcasting deleted search event: id={}", id);
        eventSink.tryEmitNext(event);
    }

    /**
     * Create heartbeat events to keep connection alive.
     */
    private Flux<SearchHistoryEventDto> createHeartbeat() {
        return Flux.interval(Duration.ofSeconds(30))
            .map(tick -> new SearchHistoryEventDto(
                "heartbeat",
                Map.of("tick", tick, "subscribers", subscriberCount.get()),
                System.currentTimeMillis()
            ));
    }

    /**
     * DTO for SSE events
     */
    public record SearchHistoryEventDto(
        String eventType,
        Object data,
        long timestamp
    ) {}
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/SearchHistoryService.java

```java
package com.newsinsight.collector.service;

import com.newsinsight.collector.dto.SearchHistoryMessage;
import com.newsinsight.collector.entity.search.SearchHistory;
import com.newsinsight.collector.entity.search.SearchType;
import com.newsinsight.collector.repository.SearchHistoryRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.PageRequest;
import org.springframework.data.domain.Pageable;
import org.springframework.data.domain.Sort;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import java.util.concurrent.TimeUnit;

/**
 * Service for managing search history.
 * Provides CRUD operations and Kafka integration for async persistence.
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class SearchHistoryService {

    private final SearchHistoryRepository searchHistoryRepository;
    private final KafkaTemplate<String, SearchHistoryMessage> searchHistoryKafkaTemplate;
    private final SearchHistoryEventService searchHistoryEventService;

    // Kafka topic for search history
    public static final String SEARCH_HISTORY_TOPIC = "newsinsight.search.history";

    /**
     * Send search result to Kafka for async persistence.
     * This is the primary method to save search results asynchronously.
     * Throws IllegalStateException if Kafka send fails.
     */
    public void sendToKafka(SearchHistoryMessage message) {
        if (message.getTimestamp() == null) {
            message.setTimestamp(System.currentTimeMillis());
        }
        
        String key = message.getExternalId() != null ? message.getExternalId() : String.valueOf(message.getTimestamp());
        
        try {
            searchHistoryKafkaTemplate.send(SEARCH_HISTORY_TOPIC, key, message)
                    .get(5, TimeUnit.SECONDS);  // 동기 대기 + 5초 타임아웃
            
            log.debug("Search history sent to Kafka: key={}, topic={}", key, SEARCH_HISTORY_TOPIC);
        } catch (Exception ex) {
            log.error("Failed to send search history to Kafka: key={}, error={}", key, ex.getMessage(), ex);
            throw new IllegalStateException("Failed to queue search history for saving", ex);
        }
    }

    /**
     * Save search history directly (synchronous).
     * Use sendToKafka() for async persistence in production.
     */
    @Transactional
    public SearchHistory save(SearchHistory searchHistory) {
        return searchHistoryRepository.save(searchHistory);
    }

    /**
     * Save from Kafka message (used by consumer).
     */
    @Transactional
    public SearchHistory saveFromMessage(SearchHistoryMessage message) {
        // Check for duplicate by externalId
        if (message.getExternalId() != null) {
            Optional<SearchHistory> existing = searchHistoryRepository.findByExternalId(message.getExternalId());
            if (existing.isPresent()) {
                log.debug("Search history already exists for externalId: {}", message.getExternalId());
                return updateFromMessage(existing.get(), message);
            }
        }

        SearchHistory history = SearchHistory.builder()
                .externalId(message.getExternalId())
                .searchType(message.getSearchType())
                .query(message.getQuery())
                .timeWindow(message.getTimeWindow())
                .userId(message.getUserId())
                .sessionId(message.getSessionId())
                .parentSearchId(message.getParentSearchId())
                .depthLevel(message.getDepthLevel())
                .resultCount(message.getResultCount())
                .results(message.getResults())
                .aiSummary(message.getAiSummary())
                .discoveredUrls(message.getDiscoveredUrls())
                .factCheckResults(message.getFactCheckResults())
                .credibilityScore(message.getCredibilityScore())
                .stanceDistribution(message.getStanceDistribution())
                .metadata(message.getMetadata())
                .durationMs(message.getDurationMs())
                .errorMessage(message.getErrorMessage())
                .success(message.getSuccess())
                .build();

        SearchHistory saved = searchHistoryRepository.save(history);
        log.info("Saved search history: id={}, type={}, query='{}'", 
                saved.getId(), saved.getSearchType(), saved.getQuery());
        
        // Notify SSE subscribers
        searchHistoryEventService.notifyNewSearch(saved);
        
        return saved;
    }

    /**
     * Update existing search history from message.
     */
    private SearchHistory updateFromMessage(SearchHistory existing, SearchHistoryMessage message) {
        if (message.getResults() != null) {
            existing.setResults(message.getResults());
        }
        if (message.getResultCount() != null) {
            existing.setResultCount(message.getResultCount());
        }
        if (message.getAiSummary() != null) {
            existing.setAiSummary(message.getAiSummary());
        }
        if (message.getDiscoveredUrls() != null) {
            existing.setDiscoveredUrls(message.getDiscoveredUrls());
        }
        if (message.getFactCheckResults() != null) {
            existing.setFactCheckResults(message.getFactCheckResults());
        }
        if (message.getCredibilityScore() != null) {
            existing.setCredibilityScore(message.getCredibilityScore());
        }
        if (message.getStanceDistribution() != null) {
            existing.setStanceDistribution(message.getStanceDistribution());
        }
        if (message.getDurationMs() != null) {
            existing.setDurationMs(message.getDurationMs());
        }
        if (message.getErrorMessage() != null) {
            existing.setErrorMessage(message.getErrorMessage());
        }
        if (message.getSuccess() != null) {
            existing.setSuccess(message.getSuccess());
        }
        
        SearchHistory updated = searchHistoryRepository.save(existing);
        
        // Notify SSE subscribers of update
        searchHistoryEventService.notifyUpdatedSearch(updated);
        
        return updated;
    }

    /**
     * Find by ID.
     */
    public Optional<SearchHistory> findById(Long id) {
        return searchHistoryRepository.findById(id);
    }

    /**
     * Find by external ID (e.g., jobId).
     */
    public Optional<SearchHistory> findByExternalId(String externalId) {
        return searchHistoryRepository.findByExternalId(externalId);
    }

    /**
     * Get paginated search history.
     */
    public Page<SearchHistory> findAll(int page, int size, String sortBy, String direction) {
        Sort sort = direction.equalsIgnoreCase("ASC") 
                ? Sort.by(sortBy).ascending() 
                : Sort.by(sortBy).descending();
        Pageable pageable = PageRequest.of(page, size, sort);
        return searchHistoryRepository.findAll(pageable);
    }

    /**
     * Get search history by type.
     */
    public Page<SearchHistory> findByType(SearchType searchType, int page, int size) {
        Pageable pageable = PageRequest.of(page, size, Sort.by("createdAt").descending());
        return searchHistoryRepository.findBySearchType(searchType, pageable);
    }

    /**
     * Get search history by user.
     */
    public Page<SearchHistory> findByUser(String userId, int page, int size) {
        Pageable pageable = PageRequest.of(page, size, Sort.by("createdAt").descending());
        return searchHistoryRepository.findByUserId(userId, pageable);
    }

    /**
     * Get search history by user and type.
     */
    public Page<SearchHistory> findByUserAndType(String userId, SearchType searchType, int page, int size) {
        Pageable pageable = PageRequest.of(page, size, Sort.by("createdAt").descending());
        return searchHistoryRepository.findByUserIdAndSearchType(userId, searchType, pageable);
    }

    /**
     * Search history by query text.
     */
    public Page<SearchHistory> searchByQuery(String query, int page, int size) {
        Pageable pageable = PageRequest.of(page, size, Sort.by("createdAt").descending());
        return searchHistoryRepository.searchByQuery(query, pageable);
    }

    /**
     * Get bookmarked searches.
     */
    public Page<SearchHistory> findBookmarked(int page, int size) {
        Pageable pageable = PageRequest.of(page, size, Sort.by("createdAt").descending());
        return searchHistoryRepository.findByBookmarkedTrue(pageable);
    }

    /**
     * Get derived searches from a parent.
     */
    public List<SearchHistory> findDerivedSearches(Long parentSearchId) {
        return searchHistoryRepository.findByParentSearchIdOrderByCreatedAtDesc(parentSearchId);
    }

    /**
     * Get searches from a session.
     */
    public List<SearchHistory> findBySession(String sessionId) {
        return searchHistoryRepository.findBySessionIdOrderByCreatedAtDesc(sessionId);
    }

    /**
     * Toggle bookmark status.
     */
    @Transactional
    public SearchHistory toggleBookmark(Long id) {
        SearchHistory history = searchHistoryRepository.findById(id)
                .orElseThrow(() -> new IllegalArgumentException("Search history not found: " + id));
        history.setBookmarked(!history.getBookmarked());
        return searchHistoryRepository.save(history);
    }

    /**
     * Update tags.
     */
    @Transactional
    public SearchHistory updateTags(Long id, List<String> tags) {
        SearchHistory history = searchHistoryRepository.findById(id)
                .orElseThrow(() -> new IllegalArgumentException("Search history not found: " + id));
        history.setTags(tags);
        return searchHistoryRepository.save(history);
    }

    /**
     * Update notes.
     */
    @Transactional
    public SearchHistory updateNotes(Long id, String notes) {
        SearchHistory history = searchHistoryRepository.findById(id)
                .orElseThrow(() -> new IllegalArgumentException("Search history not found: " + id));
        history.setNotes(notes);
        return searchHistoryRepository.save(history);
    }

    /**
     * Delete search history.
     */
    @Transactional
    public void delete(Long id) {
        searchHistoryRepository.deleteById(id);
        log.info("Deleted search history: id={}", id);
        
        // Notify SSE subscribers
        searchHistoryEventService.notifyDeletedSearch(id);
    }

    /**
     * Delete old non-bookmarked searches (for cleanup).
     */
    @Transactional
    public void cleanupOldSearches(int daysOld) {
        LocalDateTime before = LocalDateTime.now().minusDays(daysOld);
        searchHistoryRepository.deleteOldSearches(before);
        log.info("Cleaned up search history older than {} days", daysOld);
    }

    /**
     * Get search statistics.
     */
    public Map<String, Object> getStatistics(int days) {
        LocalDateTime after = LocalDateTime.now().minusDays(days);
        List<SearchHistoryRepository.SearchStatsSummary> stats = 
                searchHistoryRepository.getSearchStatsSummary(after);
        
        long totalCount = stats.stream().mapToLong(SearchHistoryRepository.SearchStatsSummary::getCount).sum();
        
        return Map.of(
                "totalSearches", totalCount,
                "byType", stats,
                "period", Map.of("days", days, "since", after.toString())
        );
    }

    /**
     * Get recently discovered URLs.
     */
    public List<String> getRecentDiscoveredUrls(int days, int limit) {
        LocalDateTime after = LocalDateTime.now().minusDays(days);
        return searchHistoryRepository.findRecentDiscoveredUrls(after, limit);
    }

    /**
     * Create a derived search (for drill-down functionality).
     */
    @Transactional
    public SearchHistory createDerivedSearch(Long parentId, SearchHistoryMessage message) {
        SearchHistory parent = searchHistoryRepository.findById(parentId)
                .orElseThrow(() -> new IllegalArgumentException("Parent search not found: " + parentId));
        
        message.setParentSearchId(parentId);
        message.setDepthLevel(parent.getDepthLevel() + 1);
        message.setSessionId(parent.getSessionId());
        message.setUserId(parent.getUserId());
        
        return saveFromMessage(message);
    }

    // ============================================
    // Continue Work Feature
    // ============================================

    /**
     * Find actionable items for "Continue Work" feature.
     * Returns searches that need user attention:
     * - IN_PROGRESS: Still running
     * - FAILED: Need retry
     * - PARTIAL: Incomplete results
     * - DRAFT: Not executed yet
     * - COMPLETED but not viewed: Need review
     */
    public List<SearchHistory> findContinueWorkItems(String userId, String sessionId, int limit) {
        Pageable pageable = PageRequest.of(0, limit);
        return searchHistoryRepository.findContinueWorkItems(userId, sessionId, pageable);
    }

    /**
     * Mark search as viewed.
     */
    @Transactional
    public SearchHistory markAsViewed(Long id) {
        SearchHistory history = searchHistoryRepository.findById(id)
                .orElseThrow(() -> new IllegalArgumentException("Search history not found: " + id));
        
        history.markViewed();
        return searchHistoryRepository.save(history);
    }

    /**
     * Mark search as viewed by external ID.
     */
    @Transactional
    public SearchHistory markAsViewedByExternalId(String externalId) {
        SearchHistory history = searchHistoryRepository.findByExternalId(externalId)
                .orElseThrow(() -> new IllegalArgumentException("Search history not found for externalId: " + externalId));
        
        history.markViewed();
        return searchHistoryRepository.save(history);
    }

    /**
     * Update completion status.
     */
    @Transactional
    public SearchHistory updateCompletionStatus(Long id, SearchHistory.CompletionStatus status) {
        SearchHistory history = searchHistoryRepository.findById(id)
                .orElseThrow(() -> new IllegalArgumentException("Search history not found: " + id));
        
        history.setCompletionStatus(status);
        return searchHistoryRepository.save(history);
    }

    /**
     * Find by completion status.
     */
    public Page<SearchHistory> findByCompletionStatus(SearchHistory.CompletionStatus status, int page, int size) {
        Pageable pageable = PageRequest.of(page, size, Sort.by("updatedAt").descending());
        return searchHistoryRepository.findByCompletionStatus(status, pageable);
    }

    /**
     * Find by project ID.
     */
    public Page<SearchHistory> findByProjectId(Long projectId, int page, int size) {
        Pageable pageable = PageRequest.of(page, size, Sort.by("createdAt").descending());
        return searchHistoryRepository.findByProjectId(projectId, pageable);
    }

    /**
     * Find failed searches for potential retry.
     */
    public List<SearchHistory> findFailedSearches(int daysBack, int limit) {
        LocalDateTime after = LocalDateTime.now().minusDays(daysBack);
        Pageable pageable = PageRequest.of(0, limit);
        return searchHistoryRepository.findFailedSearches(after, pageable);
    }

    /**
     * Count in-progress searches for a user.
     */
    public long countInProgressByUser(String userId) {
        return searchHistoryRepository.countInProgressByUser(userId);
    }

    /**
     * Get continue work statistics.
     */
    public Map<String, Object> getContinueWorkStats(String userId, String sessionId) {
        List<SearchHistory> items = findContinueWorkItems(userId, sessionId, 100);
        
        long inProgress = items.stream()
                .filter(h -> h.getCompletionStatus() == SearchHistory.CompletionStatus.IN_PROGRESS)
                .count();
        long failed = items.stream()
                .filter(h -> h.getCompletionStatus() == SearchHistory.CompletionStatus.FAILED)
                .count();
        long draft = items.stream()
                .filter(h -> h.getCompletionStatus() == SearchHistory.CompletionStatus.DRAFT)
                .count();
        long partial = items.stream()
                .filter(h -> h.getCompletionStatus() == SearchHistory.CompletionStatus.PARTIAL)
                .count();
        long unviewed = items.stream()
                .filter(h -> h.getCompletionStatus() == SearchHistory.CompletionStatus.COMPLETED && !h.getViewed())
                .count();

        return Map.of(
                "total", items.size(),
                "inProgress", inProgress,
                "failed", failed,
                "draft", draft,
                "partial", partial,
                "unviewedCompleted", unviewed
        );
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/SearchJobQueueService.java

```java
package com.newsinsight.collector.service;

import com.newsinsight.collector.dto.SearchHistoryMessage;
import com.newsinsight.collector.entity.search.SearchHistory;
import com.newsinsight.collector.entity.search.SearchType;
import com.newsinsight.collector.repository.SearchHistoryRepository;
import lombok.Builder;
import lombok.Data;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.scheduling.annotation.Async;
import org.springframework.stereotype.Service;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import java.util.UUID;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.function.Consumer;

/**
 * Service for managing concurrent search jobs.
 * Enables users to run multiple searches simultaneously
 * (Unified Search, Deep Search, Fact Check, etc.)
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class SearchJobQueueService {

    private final SearchHistoryRepository searchHistoryRepository;
    private final SearchHistoryService searchHistoryService;
    private final UnifiedSearchService unifiedSearchService;
    private final DeepAnalysisService deepAnalysisService;

    // Active jobs tracked in memory
    private final Map<String, SearchJob> activeJobs = new ConcurrentHashMap<>();
    
    // Job listeners for SSE notifications
    private final Map<String, Consumer<SearchJobEvent>> jobListeners = new ConcurrentHashMap<>();

    // Executor for async job execution
    private final ExecutorService executorService = Executors.newFixedThreadPool(10);

    /**
     * Start a new search job
     */
    public String startJob(SearchJobRequest request) {
        String jobId = UUID.randomUUID().toString();
        
        SearchJob job = SearchJob.builder()
                .jobId(jobId)
                .type(request.getType())
                .query(request.getQuery())
                .timeWindow(request.getTimeWindow())
                .userId(request.getUserId())
                .sessionId(request.getSessionId())
                .projectId(request.getProjectId())
                .status(JobStatus.PENDING)
                .progress(0)
                .startedAt(LocalDateTime.now())
                .build();
        
        activeJobs.put(jobId, job);
        
        // Create initial SearchHistory entry
        SearchHistory history = SearchHistory.builder()
                .externalId(jobId)
                .searchType(request.getType())
                .query(request.getQuery())
                .timeWindow(request.getTimeWindow())
                .userId(request.getUserId())
                .sessionId(request.getSessionId())
                .projectId(request.getProjectId())
                .completionStatus(SearchHistory.CompletionStatus.IN_PROGRESS)
                .progress(0)
                .currentPhase("초기화 중...")
                .build();
        
        searchHistoryRepository.save(history);
        
        // Execute job asynchronously
        executeJobAsync(job);
        
        log.info("Started search job: id={}, type={}, query='{}'", jobId, request.getType(), request.getQuery());
        
        return jobId;
    }

    /**
     * Execute job asynchronously
     */
    @Async
    protected void executeJobAsync(SearchJob job) {
        try {
            job.setStatus(JobStatus.RUNNING);
            notifyJobUpdate(job, "started", "검색을 시작합니다");
            
            switch (job.getType()) {
                case UNIFIED -> executeUnifiedSearch(job);
                case DEEP_SEARCH -> executeDeepSearch(job);
                case FACT_CHECK -> executeFactCheck(job);
                case BROWSER_AGENT -> executeBrowserAgent(job);
                default -> throw new IllegalArgumentException("Unknown job type: " + job.getType());
            }
            
        } catch (Exception e) {
            log.error("Job execution failed: jobId={}, error={}", job.getJobId(), e.getMessage(), e);
            markJobFailed(job, e.getMessage());
        }
    }

    /**
     * Execute unified search
     */
    private void executeUnifiedSearch(SearchJob job) {
        updateJobProgress(job, 10, "데이터베이스 검색 중...");
        
        // Delegate to UnifiedSearchService
        // The service will handle SSE streaming and updates
        unifiedSearchService.executeSearchAsync(
                job.getJobId(),
                job.getQuery(),
                job.getTimeWindow(),
                null // priority URLs
        );
        
        // Note: Job completion is handled by the callback from UnifiedSearchService
    }

    /**
     * Execute deep search
     */
    private void executeDeepSearch(SearchJob job) {
        updateJobProgress(job, 10, "Deep Search 시작...");
        
        // Delegate to DeepAnalysisService
        deepAnalysisService.startDeepSearch(job.getQuery(), null);
        
        // Note: Job completion is handled by the callback from DeepAnalysisService
    }

    /**
     * Execute fact check (placeholder)
     */
    private void executeFactCheck(SearchJob job) {
        updateJobProgress(job, 10, "팩트체크 시작...");
        // TODO: Implement fact check execution
        markJobCompleted(job, Map.of("status", "not_implemented"));
    }

    /**
     * Execute browser agent (placeholder)
     */
    private void executeBrowserAgent(SearchJob job) {
        updateJobProgress(job, 10, "브라우저 에이전트 시작...");
        // TODO: Implement browser agent execution
        markJobCompleted(job, Map.of("status", "not_implemented"));
    }

    /**
     * Update job progress
     */
    public void updateJobProgress(String jobId, int progress, String phase) {
        SearchJob job = activeJobs.get(jobId);
        if (job != null) {
            updateJobProgress(job, progress, phase);
        }
    }

    private void updateJobProgress(SearchJob job, int progress, String phase) {
        job.setProgress(progress);
        job.setCurrentPhase(phase);
        
        // Update SearchHistory
        searchHistoryRepository.findByExternalId(job.getJobId()).ifPresent(history -> {
            history.updateProgress(progress, phase);
            searchHistoryRepository.save(history);
        });
        
        notifyJobUpdate(job, "progress", phase);
    }

    /**
     * Mark job as completed
     */
    public void markJobCompleted(String jobId, Map<String, Object> result) {
        SearchJob job = activeJobs.get(jobId);
        if (job != null) {
            markJobCompleted(job, result);
        }
    }

    private void markJobCompleted(SearchJob job, Map<String, Object> result) {
        job.setStatus(JobStatus.COMPLETED);
        job.setProgress(100);
        job.setCompletedAt(LocalDateTime.now());
        job.setResult(result);
        
        // Update SearchHistory
        searchHistoryRepository.findByExternalId(job.getJobId()).ifPresent(history -> {
            history.markCompleted();
            searchHistoryRepository.save(history);
        });
        
        notifyJobUpdate(job, "completed", "검색이 완료되었습니다");
        
        // Keep in active jobs for a while for status queries
        // Will be cleaned up by scheduled task
        
        log.info("Job completed: jobId={}, duration={}ms", 
                job.getJobId(), 
                java.time.Duration.between(job.getStartedAt(), job.getCompletedAt()).toMillis());
    }

    /**
     * Mark job as failed
     */
    public void markJobFailed(String jobId, String errorMessage) {
        SearchJob job = activeJobs.get(jobId);
        if (job != null) {
            markJobFailed(job, errorMessage);
        }
    }

    private void markJobFailed(SearchJob job, String errorMessage) {
        job.setStatus(JobStatus.FAILED);
        job.setErrorMessage(errorMessage);
        job.setCompletedAt(LocalDateTime.now());
        
        // Update SearchHistory
        searchHistoryRepository.findByExternalId(job.getJobId()).ifPresent(history -> {
            history.markFailed(job.getCurrentPhase(), errorMessage, null);
            searchHistoryRepository.save(history);
        });
        
        notifyJobUpdate(job, "failed", errorMessage);
        
        log.error("Job failed: jobId={}, error={}", job.getJobId(), errorMessage);
    }

    /**
     * Cancel a job
     */
    public boolean cancelJob(String jobId) {
        SearchJob job = activeJobs.get(jobId);
        if (job == null || job.getStatus() != JobStatus.RUNNING) {
            return false;
        }
        
        job.setStatus(JobStatus.CANCELLED);
        job.setCompletedAt(LocalDateTime.now());
        
        // Update SearchHistory
        searchHistoryRepository.findByExternalId(jobId).ifPresent(history -> {
            history.setCompletionStatus(SearchHistory.CompletionStatus.CANCELLED);
            searchHistoryRepository.save(history);
        });
        
        notifyJobUpdate(job, "cancelled", "작업이 취소되었습니다");
        
        log.info("Job cancelled: jobId={}", jobId);
        return true;
    }

    /**
     * Get job status
     */
    public Optional<SearchJob> getJobStatus(String jobId) {
        return Optional.ofNullable(activeJobs.get(jobId));
    }

    /**
     * Get active jobs for user
     */
    public List<SearchJob> getActiveJobs(String userId) {
        return activeJobs.values().stream()
                .filter(job -> userId.equals(job.getUserId()))
                .filter(job -> job.getStatus() == JobStatus.PENDING || job.getStatus() == JobStatus.RUNNING)
                .toList();
    }

    /**
     * Get all jobs for user (including completed)
     */
    public List<SearchJob> getAllJobs(String userId, int limit) {
        return activeJobs.values().stream()
                .filter(job -> userId.equals(job.getUserId()))
                .sorted((a, b) -> b.getStartedAt().compareTo(a.getStartedAt()))
                .limit(limit)
                .toList();
    }

    /**
     * Register job listener for SSE
     */
    public void registerListener(String jobId, Consumer<SearchJobEvent> listener) {
        jobListeners.put(jobId, listener);
    }

    /**
     * Unregister job listener
     */
    public void unregisterListener(String jobId) {
        jobListeners.remove(jobId);
    }

    /**
     * Notify job update to listeners
     */
    private void notifyJobUpdate(SearchJob job, String eventType, String message) {
        Consumer<SearchJobEvent> listener = jobListeners.get(job.getJobId());
        if (listener != null) {
            SearchJobEvent event = SearchJobEvent.builder()
                    .jobId(job.getJobId())
                    .eventType(eventType)
                    .status(job.getStatus())
                    .progress(job.getProgress())
                    .currentPhase(job.getCurrentPhase())
                    .message(message)
                    .timestamp(System.currentTimeMillis())
                    .build();
            
            try {
                listener.accept(event);
            } catch (Exception e) {
                log.warn("Failed to notify job listener: jobId={}, error={}", job.getJobId(), e.getMessage());
            }
        }
    }

    /**
     * Cleanup completed jobs (called by scheduler)
     */
    public void cleanupCompletedJobs() {
        LocalDateTime cutoff = LocalDateTime.now().minusHours(1);
        
        activeJobs.entrySet().removeIf(entry -> {
            SearchJob job = entry.getValue();
            return (job.getStatus() == JobStatus.COMPLETED 
                    || job.getStatus() == JobStatus.FAILED 
                    || job.getStatus() == JobStatus.CANCELLED)
                    && job.getCompletedAt() != null 
                    && job.getCompletedAt().isBefore(cutoff);
        });
    }

    // ============ DTOs ============

    @Data
    @Builder
    public static class SearchJobRequest {
        private SearchType type;
        private String query;
        private String timeWindow;
        private String userId;
        private String sessionId;
        private Long projectId;
        private Map<String, Object> options;
    }

    @Data
    @Builder
    public static class SearchJob {
        private String jobId;
        private SearchType type;
        private String query;
        private String timeWindow;
        private String userId;
        private String sessionId;
        private Long projectId;
        private JobStatus status;
        private int progress;
        private String currentPhase;
        private String errorMessage;
        private LocalDateTime startedAt;
        private LocalDateTime completedAt;
        private Map<String, Object> result;
    }

    @Data
    @Builder
    public static class SearchJobEvent {
        private String jobId;
        private String eventType;
        private JobStatus status;
        private int progress;
        private String currentPhase;
        private String message;
        private long timestamp;
    }

    public enum JobStatus {
        PENDING,
        RUNNING,
        COMPLETED,
        FAILED,
        CANCELLED
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/SearchTemplateService.java

```java
package com.newsinsight.collector.service;

import com.newsinsight.collector.dto.SearchTemplateDto;
import com.newsinsight.collector.entity.search.SearchTemplate;
import com.newsinsight.collector.repository.SearchTemplateRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.PageRequest;
import org.springframework.data.domain.Pageable;
import org.springframework.data.domain.Sort;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.util.List;
import java.util.Map;
import java.util.Optional;

/**
 * Service for managing search templates.
 * Provides CRUD operations and query functionality for templates.
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class SearchTemplateService {

    private final SearchTemplateRepository searchTemplateRepository;

    /**
     * Create a new template
     */
    @Transactional
    public SearchTemplate create(SearchTemplateDto dto) {
        if (dto.getName() == null || dto.getName().isBlank()) {
            throw new IllegalArgumentException("Template name is required");
        }
        if (dto.getQuery() == null || dto.getQuery().isBlank()) {
            throw new IllegalArgumentException("Template query is required");
        }
        if (dto.getMode() == null || dto.getMode().isBlank()) {
            throw new IllegalArgumentException("Template mode is required");
        }

        // Check for duplicate name for same user
        if (dto.getUserId() != null && 
            searchTemplateRepository.existsByUserIdAndName(dto.getUserId(), dto.getName())) {
            throw new IllegalArgumentException("Template with this name already exists");
        }

        SearchTemplate template = dto.toEntity();
        SearchTemplate saved = searchTemplateRepository.save(template);
        log.info("Created template: id={}, name='{}', mode={}, userId={}", 
                saved.getId(), saved.getName(), saved.getMode(), saved.getUserId());
        return saved;
    }

    /**
     * Update an existing template
     */
    @Transactional
    public SearchTemplate update(Long id, SearchTemplateDto dto) {
        SearchTemplate template = searchTemplateRepository.findById(id)
                .orElseThrow(() -> new IllegalArgumentException("Template not found: " + id));

        if (dto.getName() != null && !dto.getName().isBlank()) {
            // Check for duplicate name if changing
            if (!dto.getName().equals(template.getName()) && 
                template.getUserId() != null &&
                searchTemplateRepository.existsByUserIdAndName(template.getUserId(), dto.getName())) {
                throw new IllegalArgumentException("Template with this name already exists");
            }
            template.setName(dto.getName());
        }
        if (dto.getQuery() != null) {
            template.setQuery(dto.getQuery());
        }
        if (dto.getMode() != null) {
            template.setMode(dto.getMode());
        }
        if (dto.getItems() != null) {
            template.setItems(dto.getItems());
        }
        if (dto.getDescription() != null) {
            template.setDescription(dto.getDescription());
        }
        if (dto.getTags() != null) {
            template.setTags(dto.getTags());
        }
        if (dto.getMetadata() != null) {
            template.setMetadata(dto.getMetadata());
        }

        SearchTemplate updated = searchTemplateRepository.save(template);
        log.info("Updated template: id={}, name='{}'", updated.getId(), updated.getName());
        return updated;
    }

    /**
     * Find by ID
     */
    public Optional<SearchTemplate> findById(Long id) {
        return searchTemplateRepository.findById(id);
    }

    /**
     * Get paginated templates
     */
    public Page<SearchTemplate> findAll(int page, int size, String sortBy, String direction) {
        Sort sort = direction.equalsIgnoreCase("ASC")
                ? Sort.by(sortBy).ascending()
                : Sort.by(sortBy).descending();
        Pageable pageable = PageRequest.of(page, size, sort);
        return searchTemplateRepository.findAll(pageable);
    }

    /**
     * Get templates by user
     */
    public Page<SearchTemplate> findByUser(String userId, int page, int size) {
        Pageable pageable = PageRequest.of(page, size, Sort.by("createdAt").descending());
        return searchTemplateRepository.findByUserId(userId, pageable);
    }

    /**
     * Get all templates for a user (list)
     */
    public List<SearchTemplate> findAllByUser(String userId) {
        return searchTemplateRepository.findByUserIdOrderByCreatedAtDesc(userId);
    }

    /**
     * Get templates by user and mode
     */
    public Page<SearchTemplate> findByUserAndMode(String userId, String mode, int page, int size) {
        Pageable pageable = PageRequest.of(page, size, Sort.by("createdAt").descending());
        return searchTemplateRepository.findByUserIdAndMode(userId, mode, pageable);
    }

    /**
     * Get favorite templates for a user
     */
    public List<SearchTemplate> findFavoritesByUser(String userId) {
        return searchTemplateRepository.findByUserIdAndFavoriteTrueOrderByLastUsedAtDesc(userId);
    }

    /**
     * Search templates by name
     */
    public Page<SearchTemplate> searchByName(String name, String userId, int page, int size) {
        Pageable pageable = PageRequest.of(page, size, Sort.by("createdAt").descending());
        if (userId != null) {
            return searchTemplateRepository.searchByNameAndUserId(name, userId, pageable);
        }
        return searchTemplateRepository.searchByName(name, pageable);
    }

    /**
     * Get most used templates for a user
     */
    public List<SearchTemplate> findMostUsed(String userId, int limit) {
        Pageable pageable = PageRequest.of(0, limit);
        return searchTemplateRepository.findMostUsedByUser(userId, pageable);
    }

    /**
     * Get recently used templates for a user
     */
    public List<SearchTemplate> findRecentlyUsed(String userId, int limit) {
        Pageable pageable = PageRequest.of(0, limit);
        return searchTemplateRepository.findRecentlyUsedByUser(userId, pageable);
    }

    /**
     * Toggle favorite status
     */
    @Transactional
    public SearchTemplate toggleFavorite(Long id) {
        SearchTemplate template = searchTemplateRepository.findById(id)
                .orElseThrow(() -> new IllegalArgumentException("Template not found: " + id));
        template.setFavorite(!template.getFavorite());
        return searchTemplateRepository.save(template);
    }

    /**
     * Record template usage
     */
    @Transactional
    public void recordUsage(Long id) {
        searchTemplateRepository.incrementUseCount(id);
        log.debug("Recorded usage for template: id={}", id);
    }

    /**
     * Delete template
     */
    @Transactional
    public void delete(Long id) {
        if (!searchTemplateRepository.existsById(id)) {
            throw new IllegalArgumentException("Template not found: " + id);
        }
        searchTemplateRepository.deleteById(id);
        log.info("Deleted template: id={}", id);
    }

    /**
     * Get template statistics
     */
    public Map<String, Object> getStatistics(String userId) {
        long totalCount = userId != null 
                ? searchTemplateRepository.countByUserId(userId)
                : searchTemplateRepository.count();
        
        long unifiedCount = searchTemplateRepository.countByMode("unified");
        long deepCount = searchTemplateRepository.countByMode("deep");
        long factcheckCount = searchTemplateRepository.countByMode("factcheck");

        return Map.of(
                "totalTemplates", totalCount,
                "byMode", Map.of(
                        "unified", unifiedCount,
                        "deep", deepCount,
                        "factcheck", factcheckCount
                ),
                "userId", userId != null ? userId : "all"
        );
    }

    /**
     * Duplicate a template
     */
    @Transactional
    public SearchTemplate duplicate(Long id, String newName, String userId) {
        SearchTemplate original = searchTemplateRepository.findById(id)
                .orElseThrow(() -> new IllegalArgumentException("Template not found: " + id));

        String name = newName != null ? newName : original.getName() + " (copy)";
        
        // Ensure unique name
        String finalName = name;
        int counter = 1;
        while (userId != null && searchTemplateRepository.existsByUserIdAndName(userId, finalName)) {
            finalName = name + " " + counter++;
        }

        SearchTemplate copy = SearchTemplate.builder()
                .name(finalName)
                .query(original.getQuery())
                .mode(original.getMode())
                .userId(userId != null ? userId : original.getUserId())
                .items(original.getItems())
                .description(original.getDescription())
                .tags(original.getTags())
                .metadata(original.getMetadata())
                .sourceSearchId(original.getSourceSearchId())
                .build();

        SearchTemplate saved = searchTemplateRepository.save(copy);
        log.info("Duplicated template: originalId={}, newId={}, newName='{}'", 
                id, saved.getId(), saved.getName());
        return saved;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/UnifiedSearchEventService.java

```java
package com.newsinsight.collector.service;

import lombok.extern.slf4j.Slf4j;
import org.springframework.http.codec.ServerSentEvent;
import org.springframework.stereotype.Service;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Sinks;

import java.time.Duration;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;

/**
 * Service for managing SSE streams for unified search jobs.
 * Each job has its own event sink that clients can subscribe to.
 * 
 * This follows the same pattern as DeepSearchEventService to enable:
 * - Job-based async search execution
 * - SSE stream reconnection with same jobId
 * - Page navigation without losing search results
 * - Collecting all search results for persistence
 */
@Service
@Slf4j
public class UnifiedSearchEventService {

    // Map of jobId -> Sinks for that job
    private final Map<String, Sinks.Many<ServerSentEvent<Object>>> jobSinks = new ConcurrentHashMap<>();
    
    // Map of jobId -> Job metadata (status, query, etc.)
    private final Map<String, JobMetadata> jobMetadataMap = new ConcurrentHashMap<>();

    // Map of jobId -> Collected results for persistence
    private final Map<String, List<Map<String, Object>>> jobResults = new ConcurrentHashMap<>();

    /**
     * Metadata for a search job.
     */
    public record JobMetadata(
            String jobId,
            String query,
            String window,
            String status, // PENDING, IN_PROGRESS, COMPLETED, FAILED
            long createdAt,
            Long completedAt
    ) {
        public JobMetadata withStatus(String newStatus) {
            return new JobMetadata(jobId, query, window, newStatus, createdAt, 
                    "COMPLETED".equals(newStatus) || "FAILED".equals(newStatus) 
                            ? Long.valueOf(System.currentTimeMillis()) : completedAt);
        }
    }

    /**
     * Create a new job and return its metadata.
     */
    public JobMetadata createJob(String jobId, String query, String window) {
        JobMetadata metadata = new JobMetadata(jobId, query, window, "PENDING", System.currentTimeMillis(), null);
        jobMetadataMap.put(jobId, metadata);
        jobResults.put(jobId, Collections.synchronizedList(new ArrayList<>()));
        getOrCreateSink(jobId); // Ensure sink is created
        log.info("Created unified search job: {} for query: '{}'", jobId, query);
        return metadata;
    }

    /**
     * Get job metadata.
     */
    public JobMetadata getJobMetadata(String jobId) {
        return jobMetadataMap.get(jobId);
    }

    /**
     * Update job status.
     */
    public void updateJobStatus(String jobId, String status) {
        JobMetadata existing = jobMetadataMap.get(jobId);
        if (existing != null) {
            jobMetadataMap.put(jobId, existing.withStatus(status));
        }
    }

    /**
     * Get or create a sink for a job.
     */
    private Sinks.Many<ServerSentEvent<Object>> getOrCreateSink(String jobId) {
        return jobSinks.computeIfAbsent(jobId, id -> {
            log.info("Creating new SSE sink for unified search job: {}", id);
            return Sinks.many().multicast().onBackpressureBuffer(200);
        });
    }

    /**
     * Get the event stream for a specific job.
     * Includes heartbeats every 15 seconds to keep connection alive.
     */
    public Flux<ServerSentEvent<Object>> getJobEventStream(String jobId) {
        Sinks.Many<ServerSentEvent<Object>> sink = getOrCreateSink(jobId);
        
        // Heartbeat stream
        Flux<ServerSentEvent<Object>> heartbeat = Flux.interval(Duration.ofSeconds(15))
                .map(tick -> ServerSentEvent.builder()
                        .event("heartbeat")
                        .data(Map.of("timestamp", System.currentTimeMillis(), "jobId", jobId))
                        .build());

        // Main event stream from sink
        Flux<ServerSentEvent<Object>> events = sink.asFlux();

        // Check if job is already completed - send initial status
        JobMetadata metadata = jobMetadataMap.get(jobId);
        Flux<ServerSentEvent<Object>> initialStatus = Flux.empty();
        if (metadata != null) {
            initialStatus = Flux.just(ServerSentEvent.builder()
                    .event("job_status")
                    .data(Map.of(
                            "jobId", jobId,
                            "query", metadata.query(),
                            "window", metadata.window(),
                            "status", metadata.status(),
                            "createdAt", metadata.createdAt()
                    ))
                    .build());
        }

        return Flux.concat(initialStatus, Flux.merge(heartbeat, events))
                .doOnSubscribe(sub -> log.info("New SSE subscriber for unified search job: {}", jobId))
                .doOnCancel(() -> log.info("SSE subscriber disconnected for unified search job: {}", jobId))
                .doOnError(e -> log.error("SSE stream error for unified search job: {}", jobId, e));
    }

    /**
     * Publish a status update event.
     */
    public void publishStatusUpdate(String jobId, String source, String message) {
        Sinks.Many<ServerSentEvent<Object>> sink = jobSinks.get(jobId);
        if (sink == null) {
            log.debug("No sink found for job: {}, creating new one", jobId);
            sink = getOrCreateSink(jobId);
        }

        ServerSentEvent<Object> event = ServerSentEvent.builder()
                .event("status")
                .data(Map.of(
                        "jobId", jobId,
                        "eventType", "status",
                        "source", source,
                        "message", message != null ? message : "",
                        "timestamp", System.currentTimeMillis()
                ))
                .build();

        sink.tryEmitNext(event);
        log.debug("Published status event for job: {}, source: {}", jobId, source);
    }

    /**
     * Publish a search result event and collect it for persistence.
     */
    public void publishResult(String jobId, String source, Object result) {
        Sinks.Many<ServerSentEvent<Object>> sink = jobSinks.get(jobId);
        if (sink == null) return;

        // Collect result for persistence (skip AI results as they're saved separately)
        if (!"ai".equals(source) && result != null) {
            collectResult(jobId, source, result);
        }

        ServerSentEvent<Object> event = ServerSentEvent.builder()
                .event("result")
                .data(Map.of(
                        "jobId", jobId,
                        "eventType", "result",
                        "source", source,
                        "result", result,
                        "timestamp", System.currentTimeMillis()
                ))
                .build();

        sink.tryEmitNext(event);
        log.debug("Published result event for job: {}, source: {}", jobId, source);
    }

    /**
     * Collect a search result for later persistence.
     */
    @SuppressWarnings("unchecked")
    private void collectResult(String jobId, String source, Object result) {
        List<Map<String, Object>> results = jobResults.get(jobId);
        if (results == null) {
            results = Collections.synchronizedList(new ArrayList<>());
            jobResults.put(jobId, results);
        }

        try {
            Map<String, Object> resultMap;
            if (result instanceof Map) {
                resultMap = new ConcurrentHashMap<>((Map<String, Object>) result);
            } else {
                // Convert SearchResult object to Map
                resultMap = convertToMap(result);
            }
            resultMap.put("_source", source);
            resultMap.put("_collectedAt", System.currentTimeMillis());
            results.add(resultMap);
            log.debug("Collected result for job: {}, source: {}, total collected: {}", jobId, source, results.size());
        } catch (Exception e) {
            log.warn("Failed to collect result for job: {}, source: {}, error: {}", jobId, source, e.getMessage());
        }
    }

    /**
     * Convert a SearchResult object to a Map for persistence.
     */
    private Map<String, Object> convertToMap(Object result) {
        Map<String, Object> map = new ConcurrentHashMap<>();
        try {
            // Use reflection to convert SearchResult to Map
            for (var field : result.getClass().getDeclaredFields()) {
                field.setAccessible(true);
                Object value = field.get(result);
                if (value != null) {
                    map.put(field.getName(), value);
                }
            }
        } catch (Exception e) {
            log.warn("Failed to convert result to map: {}", e.getMessage());
        }
        return map;
    }

    /**
     * Get all collected results for a job.
     */
    public List<Map<String, Object>> getCollectedResults(String jobId) {
        List<Map<String, Object>> results = jobResults.get(jobId);
        return results != null ? new ArrayList<>(results) : new ArrayList<>();
    }

    /**
     * Get the count of collected results for a job.
     */
    public int getCollectedResultCount(String jobId) {
        List<Map<String, Object>> results = jobResults.get(jobId);
        return results != null ? results.size() : 0;
    }

    /**
     * Publish an AI chunk event.
     */
    public void publishAiChunk(String jobId, String chunk) {
        Sinks.Many<ServerSentEvent<Object>> sink = jobSinks.get(jobId);
        if (sink == null) return;

        ServerSentEvent<Object> event = ServerSentEvent.builder()
                .event("ai_chunk")
                .data(Map.of(
                        "jobId", jobId,
                        "eventType", "ai_chunk",
                        "source", "ai",
                        "message", chunk,
                        "timestamp", System.currentTimeMillis()
                ))
                .build();

        sink.tryEmitNext(event);
    }

    /**
     * Publish a source complete event.
     */
    public void publishSourceComplete(String jobId, String source, String message, int totalCount) {
        Sinks.Many<ServerSentEvent<Object>> sink = jobSinks.get(jobId);
        if (sink == null) return;

        ServerSentEvent<Object> event = ServerSentEvent.builder()
                .event("source_complete")
                .data(Map.of(
                        "jobId", jobId,
                        "eventType", "complete",
                        "source", source,
                        "message", message != null ? message : "",
                        "totalCount", totalCount,
                        "timestamp", System.currentTimeMillis()
                ))
                .build();

        sink.tryEmitNext(event);
        log.debug("Published source complete event for job: {}, source: {}, count: {}", jobId, source, totalCount);
    }

    /**
     * Publish an error event for a source.
     */
    public void publishSourceError(String jobId, String source, String errorMessage) {
        Sinks.Many<ServerSentEvent<Object>> sink = jobSinks.get(jobId);
        if (sink == null) return;

        ServerSentEvent<Object> event = ServerSentEvent.builder()
                .event("source_error")
                .data(Map.of(
                        "jobId", jobId,
                        "eventType", "error",
                        "source", source,
                        "message", errorMessage,
                        "timestamp", System.currentTimeMillis()
                ))
                .build();

        sink.tryEmitNext(event);
        log.warn("Published source error event for job: {}, source: {}, error: {}", jobId, source, errorMessage);
    }

    /**
     * Publish a job completion event (all sources done).
     */
    public void publishJobComplete(String jobId, int totalResults) {
        updateJobStatus(jobId, "COMPLETED");
        
        Sinks.Many<ServerSentEvent<Object>> sink = jobSinks.get(jobId);
        if (sink == null) return;

        ServerSentEvent<Object> event = ServerSentEvent.builder()
                .event("done")
                .data(Map.of(
                        "jobId", jobId,
                        "totalResults", totalResults,
                        "message", "Search completed",
                        "timestamp", System.currentTimeMillis()
                ))
                .build();

        sink.tryEmitNext(event);
        log.info("Published job complete event for job: {}, total results: {}", jobId, totalResults);

        // Complete the sink and schedule cleanup
        sink.tryEmitComplete();
        scheduleCleanup(jobId);
    }

    /**
     * Publish a job error event.
     */
    public void publishJobError(String jobId, String errorMessage) {
        updateJobStatus(jobId, "FAILED");
        
        Sinks.Many<ServerSentEvent<Object>> sink = jobSinks.get(jobId);
        if (sink == null) return;

        ServerSentEvent<Object> event = ServerSentEvent.builder()
                .event("job_error")
                .data(Map.of(
                        "jobId", jobId,
                        "error", errorMessage,
                        "timestamp", System.currentTimeMillis()
                ))
                .build();

        sink.tryEmitNext(event);
        log.error("Published job error event for job: {}, error: {}", jobId, errorMessage);

        sink.tryEmitComplete();
        scheduleCleanup(jobId);
    }

    /**
     * Schedule cleanup of a job's sink after a delay.
     */
    private void scheduleCleanup(String jobId) {
        Thread.startVirtualThread(() -> {
            try {
                Thread.sleep(Duration.ofMinutes(10)); // Keep completed jobs for 10 minutes
                jobSinks.remove(jobId);
                jobMetadataMap.remove(jobId);
                jobResults.remove(jobId); // Also clean up collected results
                log.debug("Cleaned up sink, metadata, and results for job: {}", jobId);
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
            }
        });
    }

    /**
     * Remove a job's sink immediately.
     */
    public void removeSink(String jobId) {
        Sinks.Many<ServerSentEvent<Object>> sink = jobSinks.remove(jobId);
        if (sink != null) {
            sink.tryEmitComplete();
            log.debug("Removed sink for job: {}", jobId);
        }
        jobMetadataMap.remove(jobId);
        jobResults.remove(jobId);
    }

    /**
     * Check if a job exists.
     */
    public boolean hasJob(String jobId) {
        return jobMetadataMap.containsKey(jobId);
    }

    /**
     * Get the number of active jobs.
     */
    public int getActiveJobCount() {
        return jobSinks.size();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/UnifiedSearchService.java

```java
package com.newsinsight.collector.service;

import com.newsinsight.collector.dto.SearchHistoryMessage;
import com.newsinsight.collector.client.Crawl4aiClient;
import com.newsinsight.collector.client.PerplexityClient;
import com.newsinsight.collector.client.OpenAICompatibleClient;
import com.newsinsight.collector.client.AIDoveClient;
import com.newsinsight.collector.dto.ArticleDto;
import com.newsinsight.collector.dto.ArticleWithAnalysisDto;
import com.newsinsight.collector.entity.CollectedData;
import com.newsinsight.collector.entity.DataSource;
import com.newsinsight.collector.entity.analysis.ArticleAnalysis;
import com.newsinsight.collector.entity.analysis.ArticleDiscussion;
import com.newsinsight.collector.entity.search.SearchType;
import com.newsinsight.collector.repository.ArticleAnalysisRepository;
import com.newsinsight.collector.repository.ArticleDiscussionRepository;
import com.newsinsight.collector.repository.CollectedDataRepository;
import com.newsinsight.collector.repository.DataSourceRepository;
import com.newsinsight.collector.service.autocrawl.AutoCrawlIntegrationService;
import com.newsinsight.collector.service.search.AdvancedIntentAnalyzer;
import com.newsinsight.collector.service.search.AdvancedIntentAnalyzer.AnalyzedQuery;
import com.newsinsight.collector.service.search.AdvancedIntentAnalyzer.FallbackStrategy;
import com.newsinsight.collector.service.search.AdvancedIntentAnalyzer.RealtimeAnalysisResult;
import com.newsinsight.collector.service.search.HybridSearchService;
import com.newsinsight.collector.service.factcheck.RealtimeSearchSource;
import com.newsinsight.collector.service.FactVerificationService.SourceEvidence;
import com.newsinsight.collector.service.search.HybridRankingService.RankedResult;
import lombok.Builder;
import lombok.Data;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.jsoup.Jsoup;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.PageRequest;
import org.springframework.data.domain.Sort;
import org.springframework.scheduling.annotation.Async;
import org.springframework.stereotype.Service;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Mono;
import reactor.core.scheduler.Schedulers;

import java.net.URLEncoder;
import java.nio.charset.StandardCharsets;
import java.time.Duration;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.time.format.DateTimeParseException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.UUID;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.function.Function;
import java.util.stream.Collectors;

/**
 * Unified Search Service - 병렬 검색 통합 서비스
 * 
 * DB, 웹 크롤링, AI 검색을 병렬로 실행하고 결과가 나오는 대로 스트리밍합니다.
 * 특정 기술/API 이름을 노출하지 않고 통합된 검색 경험을 제공합니다.
 * 
 * AutoCrawl Integration: 검색 결과에서 발견된 URL을 자동 크롤링 큐에 추가합니다.
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class UnifiedSearchService {

    private final CollectedDataRepository collectedDataRepository;
    private final DataSourceRepository dataSourceRepository;
    private final ArticleAnalysisRepository articleAnalysisRepository;
    private final ArticleDiscussionRepository articleDiscussionRepository;
    private final PerplexityClient perplexityClient;
    private final OpenAICompatibleClient openAICompatibleClient;
    private final AIDoveClient aiDoveClient;
    private final Crawl4aiClient crawl4aiClient;
    private final CrawlSearchService crawlSearchService;
    private final UnifiedSearchEventService unifiedSearchEventService;
    private final HybridSearchService hybridSearchService;
    private final AutoCrawlIntegrationService autoCrawlIntegrationService;
    private final SearchHistoryService searchHistoryService;
    private final AdvancedIntentAnalyzer advancedIntentAnalyzer;
    private final SearchCacheService searchCacheService;
    private final RealtimeSearchSource realtimeSearchSource;

    @Value("${autocrawl.enabled:true}")
    private boolean autoCrawlEnabled;

    @Value("${search.fallback.max-attempts:3}")
    private int maxFallbackAttempts;

    private static final int SNIPPET_MAX_LENGTH = 200;
    private static final int MAX_DB_RESULTS = 20;
    
    // Deduplication settings
    private static final double TITLE_SIMILARITY_THRESHOLD = 0.85;
    private static final double CONTENT_SIMILARITY_THRESHOLD = 0.90;

    private static final String AI_SUMMARY_KEY_CONTENT = "content";
    private static final String AI_SUMMARY_KEY_SUMMARY = "summary";
    private static final String AI_SUMMARY_KEY_GENERATED_AT = "generatedAt";
    
    // Thread-safe deduplication tracker for streaming results
    private final java.util.concurrent.ConcurrentHashMap<String, SearchResult> seenResults = 
            new java.util.concurrent.ConcurrentHashMap<>();

    // ============================================
    // DTO Classes
    // ============================================

    @Data
    @Builder
    public static class SearchResult {
        private String id;
        private String source;          // "database", "web", "ai"
        private String sourceLabel;     // 사용자에게 보여줄 출처명
        private String title;
        private String snippet;         // UI 표시용 요약 (200자)
        private String content;         // 전체 본문 (export/저장용)
        private String url;
        private String publishedAt;
        private Double relevanceScore;
        private String category;        // 주제 분류
        
        // ========== 분석 결과 (optional) ==========
        private Boolean analyzed;           // 분석 완료 여부
        private String analysisStatus;      // pending, partial, complete
        
        // 신뢰도
        private Double reliabilityScore;    // 0-100
        private String reliabilityGrade;    // high, medium, low
        private String reliabilityColor;    // green, yellow, red
        
        // 감정 분석
        private String sentimentLabel;      // positive, negative, neutral
        private Double sentimentScore;      // -1 ~ 1
        
        // 편향도
        private String biasLabel;           // left, right, center
        private Double biasScore;           // -1 ~ 1
        
        // 팩트체크
        private String factcheckStatus;     // verified, suspicious, unverified
        private String misinfoRisk;         // low, mid, high
        
        // 위험 태그
        private List<String> riskTags;
        
        // 토픽
        private List<String> topics;
        
        // 여론 정보
        private Boolean hasDiscussion;
        private Integer totalCommentCount;
        private String discussionSentiment;
    }

    @Data
    @Builder
    public static class SearchEvent {
        private String eventType;       // "status", "result", "complete", "error"
        private String source;          // 어느 소스에서 온 이벤트인지
        private String message;         // 상태 메시지
        private SearchResult result;    // 검색 결과 (result 타입일 때)
        private Integer totalCount;     // 총 결과 수 (complete 타입일 때)
    }

    @Data
    @Builder
    public static class AISummary {
        private String summary;
        private List<String> keyPoints;
        private String sentiment;
    }

    // ============================================
    // Deduplication Methods
    // ============================================
    
    /**
     * URL 정규화 - 쿼리 파라미터 제거, 프로토콜 통일
     */
    private String normalizeUrl(String url) {
        if (url == null || url.isBlank()) {
            return "";
        }
        try {
            // Remove protocol, www, trailing slash, query params, and fragments
            String normalized = url.toLowerCase()
                    .replaceFirst("^https?://", "")
                    .replaceFirst("^www\\.", "")
                    .replaceAll("\\?.*$", "")
                    .replaceAll("#.*$", "")
                    .replaceAll("/$", "");
            return normalized;
        } catch (Exception e) {
            return url.toLowerCase();
        }
    }
    
    /**
     * 제목 유사도 계산 (Jaccard similarity)
     */
    private double calculateTitleSimilarity(String title1, String title2) {
        if (title1 == null || title2 == null) {
            return 0.0;
        }
        
        // 간단한 토큰화 및 정규화
        java.util.Set<String> tokens1 = java.util.Arrays.stream(
                title1.toLowerCase().replaceAll("[^가-힣a-z0-9\\s]", " ").split("\\s+"))
                .filter(s -> s.length() >= 2)
                .collect(Collectors.toSet());
        
        java.util.Set<String> tokens2 = java.util.Arrays.stream(
                title2.toLowerCase().replaceAll("[^가-힣a-z0-9\\s]", " ").split("\\s+"))
                .filter(s -> s.length() >= 2)
                .collect(Collectors.toSet());
        
        if (tokens1.isEmpty() || tokens2.isEmpty()) {
            return 0.0;
        }
        
        // Jaccard similarity
        java.util.Set<String> intersection = new java.util.HashSet<>(tokens1);
        intersection.retainAll(tokens2);
        
        java.util.Set<String> union = new java.util.HashSet<>(tokens1);
        union.addAll(tokens2);
        
        return (double) intersection.size() / union.size();
    }
    
    /**
     * 중복 검사 - URL 기반 + 제목 유사도 기반
     */
    private boolean isDuplicate(SearchResult newResult, Map<String, SearchResult> existingResults) {
        // 1. URL 기반 중복 체크 (정확한 매칭)
        String normalizedUrl = normalizeUrl(newResult.getUrl());
        if (!normalizedUrl.isEmpty()) {
            for (SearchResult existing : existingResults.values()) {
                if (normalizedUrl.equals(normalizeUrl(existing.getUrl()))) {
                    log.debug("Duplicate detected by URL: {}", normalizedUrl);
                    return true;
                }
            }
        }
        
        // 2. 제목 유사도 기반 중복 체크 (유사한 기사 필터링)
        String newTitle = newResult.getTitle();
        if (newTitle != null && !newTitle.isBlank()) {
            for (SearchResult existing : existingResults.values()) {
                double similarity = calculateTitleSimilarity(newTitle, existing.getTitle());
                if (similarity >= TITLE_SIMILARITY_THRESHOLD) {
                    log.debug("Duplicate detected by title similarity ({:.2f}): '{}' ~ '{}'", 
                            similarity, newTitle, existing.getTitle());
                    return true;
                }
            }
        }
        
        return false;
    }
    
    /**
     * 검색 세션용 중복 제거 트래커 초기화
     */
    private Map<String, SearchResult> createDeduplicationTracker() {
        return new java.util.concurrent.ConcurrentHashMap<>();
    }
    
    /**
     * 중복이 아닌 경우에만 결과 추가하고 true 반환
     */
    private boolean addIfNotDuplicate(SearchResult result, Map<String, SearchResult> tracker) {
        if (isDuplicate(result, tracker)) {
            return false;
        }
        
        // 결과 ID 또는 URL을 키로 사용
        String key = result.getId() != null ? result.getId() : 
                    (result.getUrl() != null ? normalizeUrl(result.getUrl()) : 
                    String.valueOf(System.nanoTime()));
        tracker.put(key, result);
        return true;
    }

    // ============================================
    // Main Search Method - Parallel Execution
    // ============================================

    /**
     * 병렬 통합 검색 - 모든 소스에서 동시에 검색하고 결과를 스트리밍
     *
     * @param query 검색 쿼리
     * @param window 시간 범위 (1d, 7d, 30d)
     * @return 검색 이벤트 스트림
     */
    public Flux<SearchEvent> searchParallel(String query, String window) {
        if (query == null || query.isBlank()) {
            return Flux.just(SearchEvent.builder()
                    .eventType("error")
                    .message("검색어를 입력해주세요.")
                    .build());
        }

        log.info("Starting parallel search for query: '{}', window: {}", query, window);

        // Advanced Intent Analysis
        AnalyzedQuery analyzedQuery = advancedIntentAnalyzer.analyzeQuery(query);
        log.info("Query analyzed: keywords={}, primary='{}', intent={}, confidence={}, strategies={}",
                analyzedQuery.getKeywords().size(),
                analyzedQuery.getPrimaryKeyword(),
                analyzedQuery.getIntentType(),
                analyzedQuery.getConfidence(),
                analyzedQuery.getFallbackStrategies().size());

        // Collect discovered URLs for AutoCrawl integration
        List<String> discoveredUrls = new ArrayList<>();
        
        // Deduplication tracker for this search session
        Map<String, SearchResult> deduplicationTracker = createDeduplicationTracker();
        java.util.concurrent.atomic.AtomicInteger duplicateCount = new java.util.concurrent.atomic.AtomicInteger(0);

        return Flux.merge(
                // 1. 데이터베이스 검색 (가장 빠름)
                searchDatabase(query, window)
                        .subscribeOn(Schedulers.boundedElastic()),

                // 2. 웹 크롤링 검색
                searchWeb(query, window)
                        .subscribeOn(Schedulers.boundedElastic()),

                // 3. AI 기반 실시간 분석
                searchAI(query, window)
                        .subscribeOn(Schedulers.boundedElastic())
        )
        // Filter duplicates before emitting results
        .filter(event -> {
            if (!"result".equals(event.getEventType()) || event.getResult() == null) {
                return true; // Pass through non-result events
            }
            
            SearchResult result = event.getResult();
            if (addIfNotDuplicate(result, deduplicationTracker)) {
                return true; // Unique result, pass through
            } else {
                duplicateCount.incrementAndGet();
                log.debug("Filtered duplicate result: '{}' from {}", 
                        result.getTitle(), result.getSource());
                return false; // Duplicate, filter out
            }
        })
        .doOnNext(event -> {
            // Collect URLs from search results for AutoCrawl
            if ("result".equals(event.getEventType()) && event.getResult() != null 
                    && event.getResult().getUrl() != null) {
                synchronized (discoveredUrls) {
                    discoveredUrls.add(event.getResult().getUrl());
                }
            }
        })
        .doOnComplete(() -> {
            log.info("Parallel search completed for query: '{}', discovered {} URLs, filtered {} duplicates", 
                    query, discoveredUrls.size(), duplicateCount.get());
            
            // Notify AutoCrawl of discovered URLs
            if (autoCrawlEnabled && !discoveredUrls.isEmpty()) {
                autoCrawlIntegrationService.onSearchCompleted(query, discoveredUrls);
            }
        })
        .doOnError(e -> log.error("Parallel search error for query '{}': {}", query, e.getMessage()));
    }

    /**
     * 결과 보장 검색 - 폴백 전략을 사용하여 최소 결과 보장
     * Intent analysis를 사용하여 더 높은 확률로 의도에 맞는 결과 반환
     *
     * @param query 검색 쿼리
     * @param window 시간 범위
     * @return 검색 이벤트 스트림 (결과 보장)
     */
    public Flux<SearchEvent> searchWithGuaranteedResults(String query, String window) {
        if (query == null || query.isBlank()) {
            return Flux.just(SearchEvent.builder()
                    .eventType("error")
                    .message("검색어를 입력해주세요.")
                    .build());
        }

        // Advanced Intent Analysis
        AnalyzedQuery analyzedQuery = advancedIntentAnalyzer.analyzeQuery(query);
        
        return searchWithFallback(analyzedQuery, window, 0, new ArrayList<>());
    }

    /**
     * 폴백 전략을 사용한 검색 (재귀적)
     */
    private Flux<SearchEvent> searchWithFallback(
            AnalyzedQuery analyzedQuery, 
            String window, 
            int attemptIndex,
            List<SearchResult> accumulatedResults) {

        String currentQuery = attemptIndex == 0 
                ? analyzedQuery.getOriginalQuery()
                : analyzedQuery.getFallbackStrategies().size() > attemptIndex - 1
                        ? analyzedQuery.getFallbackStrategies().get(attemptIndex - 1).getQuery()
                        : analyzedQuery.getPrimaryKeyword();

        String strategyDescription = attemptIndex == 0 
                ? "원본 쿼리"
                : attemptIndex <= analyzedQuery.getFallbackStrategies().size()
                        ? analyzedQuery.getFallbackStrategies().get(attemptIndex - 1).getDescription()
                        : "주요 키워드";

        log.info("Search attempt {}/{}: query='{}', strategy='{}'", 
                attemptIndex + 1, maxFallbackAttempts, currentQuery, strategyDescription);

        return Flux.create(sink -> {
            // 현재 시도에 대한 상태 이벤트
            sink.next(SearchEvent.builder()
                    .eventType("status")
                    .source("system")
                    .message("검색 전략 " + (attemptIndex + 1) + ": " + strategyDescription)
                    .build());

            // DB 검색 실행
            List<SearchResult> currentResults = new ArrayList<>();
            
            searchDatabaseSync(currentQuery, window).forEach(result -> {
                currentResults.add(result);
                sink.next(SearchEvent.builder()
                        .eventType("result")
                        .source("database")
                        .result(result)
                        .build());
            });

            // 결과 누적
            accumulatedResults.addAll(currentResults);

            // 충분한 결과가 있거나 최대 시도 횟수에 도달한 경우
            if (accumulatedResults.size() >= 5 || attemptIndex >= maxFallbackAttempts - 1) {
                // 검색 완료
                if (accumulatedResults.isEmpty()) {
                    // 결과가 없을 때 도움말 메시지 생성
                    String noResultMessage = advancedIntentAnalyzer.buildNoResultMessage(analyzedQuery);
                    sink.next(SearchEvent.builder()
                            .eventType("no_result_help")
                            .source("system")
                            .message(noResultMessage)
                            .build());
                }

                sink.next(SearchEvent.builder()
                        .eventType("complete")
                        .source("system")
                        .message("검색 완료 (시도: " + (attemptIndex + 1) + ", 결과: " + accumulatedResults.size() + ")")
                        .totalCount(accumulatedResults.size())
                        .build());

                sink.complete();
            } else if (currentResults.isEmpty() || currentResults.size() < 3) {
                // 결과가 부족하면 다음 폴백 전략 시도
                sink.next(SearchEvent.builder()
                        .eventType("status")
                        .source("system")
                        .message("결과가 부족합니다. 다음 전략을 시도합니다...")
                        .build());

                // 재귀적으로 다음 폴백 시도
                searchWithFallback(analyzedQuery, window, attemptIndex + 1, accumulatedResults)
                        .subscribe(
                                sink::next,
                                sink::error,
                                sink::complete
                        );
            } else {
                sink.next(SearchEvent.builder()
                        .eventType("complete")
                        .source("system")
                        .message("검색 완료")
                        .totalCount(accumulatedResults.size())
                        .build());
                sink.complete();
            }
        });
    }

    /**
     * 동기식 데이터베이스 검색 (폴백용) - with caching
     */
    private List<SearchResult> searchDatabaseSync(String query, String window) {
        // Check cache first
        var cachedResults = searchCacheService.getDbSearchResults(query, window, SearchResult.class);
        if (cachedResults.isPresent()) {
            log.debug("Returning cached DB search results for query: '{}'", query);
            return cachedResults.get();
        }
        
        List<SearchResult> results = new ArrayList<>();
        
        try {
            LocalDateTime since = calculateSinceDate(window);
            // Note: Native query already has ORDER BY clause, so use unsorted PageRequest
            // to avoid duplicate ORDER BY causing SQL syntax error
            PageRequest pageRequest = PageRequest.of(0, MAX_DB_RESULTS, Sort.unsorted());

            Page<CollectedData> page = collectedDataRepository.searchByQueryAndSince(query, since, pageRequest);

            List<Long> articleIds = page.getContent().stream()
                    .map(CollectedData::getId)
                    .filter(id -> id != null)
                    .toList();

            Map<Long, ArticleAnalysis> analysisMap = articleIds.isEmpty()
                    ? Map.of()
                    : articleAnalysisRepository.findByArticleIdIn(articleIds).stream()
                            .collect(Collectors.toMap(ArticleAnalysis::getArticleId, Function.identity()));

            Map<Long, ArticleDiscussion> discussionMap = articleIds.isEmpty()
                    ? Map.of()
                    : articleDiscussionRepository.findByArticleIdIn(articleIds).stream()
                            .collect(Collectors.toMap(ArticleDiscussion::getArticleId, Function.identity()));

            for (CollectedData data : page.getContent()) {
                ArticleAnalysis analysis = data.getId() != null ? analysisMap.get(data.getId()) : null;
                ArticleDiscussion discussion = data.getId() != null ? discussionMap.get(data.getId()) : null;
                results.add(convertToSearchResult(data, analysis, discussion));
            }
            
            // Cache the results
            if (!results.isEmpty()) {
                searchCacheService.cacheDbSearchResults(query, window, results);
            }
        } catch (Exception e) {
            log.error("Database sync search failed: {}", e.getMessage());
        }

        return results;
    }

    // ============================================
    // Database Search (with Hybrid Search integration)
    // ============================================

    private Flux<SearchEvent> searchDatabase(String query, String window) {
        // Use hybrid search if available, otherwise fall back to keyword-only search
        if (hybridSearchService.isEnabled() && hybridSearchService.isSemanticSearchAvailable()) {
            return searchDatabaseHybrid(query, window);
        }
        return searchDatabaseKeywordOnly(query, window);
    }

    /**
     * Hybrid search: combines keyword + semantic search with RRF ranking
     */
    private Flux<SearchEvent> searchDatabaseHybrid(String query, String window) {
        return Flux.create(sink -> {
            try {
                sink.next(SearchEvent.builder()
                        .eventType("status")
                        .source("database")
                        .message("하이브리드 검색 중 (키워드 + 시맨틱)...")
                        .build());

                hybridSearchService.search(query, window)
                        .subscribe(
                                hybridResult -> {
                                    log.info("Hybrid search completed: keyword={}, semantic={}, total={}",
                                            hybridResult.getKeywordResultCount(),
                                            hybridResult.getSemanticResultCount(),
                                            hybridResult.getTotalResultCount());

                                    // Batch load analysis data for hybrid results
                                    List<Long> articleIds = hybridResult.getResults().stream()
                                            .map(r -> {
                                                try {
                                                    return Long.parseLong(r.getId());
                                                } catch (NumberFormatException e) {
                                                    return null;
                                                }
                                            })
                                            .filter(id -> id != null)
                                            .toList();

                                    Map<Long, ArticleAnalysis> analysisMap = articleIds.isEmpty()
                                            ? Map.of()
                                            : articleAnalysisRepository.findByArticleIdIn(articleIds).stream()
                                                    .collect(Collectors.toMap(ArticleAnalysis::getArticleId, Function.identity()));

                                    Map<Long, ArticleDiscussion> discussionMap = articleIds.isEmpty()
                                            ? Map.of()
                                            : articleDiscussionRepository.findByArticleIdIn(articleIds).stream()
                                                    .collect(Collectors.toMap(ArticleDiscussion::getArticleId, Function.identity()));

                                    int count = 0;
                                    for (RankedResult rankedResult : hybridResult.getResults()) {
                                        Long articleId = null;
                                        try {
                                            articleId = Long.parseLong(rankedResult.getId());
                                        } catch (NumberFormatException ignored) {}

                                        ArticleAnalysis analysis = articleId != null ? analysisMap.get(articleId) : null;
                                        ArticleDiscussion discussion = articleId != null ? discussionMap.get(articleId) : null;

                                        SearchResult result = convertRankedResultToSearchResult(rankedResult, analysis, discussion);
                                        sink.next(SearchEvent.builder()
                                                .eventType("result")
                                                .source("database")
                                                .result(result)
                                                .build());
                                        count++;
                                    }

                                    // Include search metadata in complete message
                                    String message = String.format("하이브리드 검색 완료 (키워드: %d, 시맨틱: %d, RRF 융합: %d, %dms)",
                                            hybridResult.getKeywordResultCount(),
                                            hybridResult.getSemanticResultCount(),
                                            hybridResult.getTotalResultCount(),
                                            hybridResult.getSearchTimeMs());

                                    sink.next(SearchEvent.builder()
                                            .eventType("complete")
                                            .source("database")
                                            .message(message)
                                            .totalCount(count)
                                            .build());

                                    sink.complete();
                                },
                                error -> {
                                    log.error("Hybrid search failed, falling back to keyword search: {}", error.getMessage());
                                    // Fall back to keyword-only search on error
                                    searchDatabaseKeywordOnly(query, window)
                                            .subscribe(sink::next, sink::error, sink::complete);
                                }
                        );
            } catch (Exception e) {
                log.error("Hybrid search initialization failed: {}", e.getMessage());
                // Fall back to keyword-only search
                searchDatabaseKeywordOnly(query, window)
                        .subscribe(sink::next, sink::error, sink::complete);
            }
        });
    }

    /**
     * Keyword-only search (original implementation)
     */
    private Flux<SearchEvent> searchDatabaseKeywordOnly(String query, String window) {
        return Flux.create(sink -> {
            try {
                sink.next(SearchEvent.builder()
                        .eventType("status")
                        .source("database")
                        .message("저장된 뉴스에서 검색 중...")
                        .build());

                LocalDateTime since = calculateSinceDate(window);
                // Note: Native query already has ORDER BY clause, so use unsorted PageRequest
                PageRequest pageRequest = PageRequest.of(0, MAX_DB_RESULTS, Sort.unsorted());

                Page<CollectedData> page = collectedDataRepository.searchByQueryAndSince(
                        query, since, pageRequest);

                // 분석 결과 일괄 조회 (N+1 방지)
                List<Long> articleIds = page.getContent().stream()
                        .map(CollectedData::getId)
                        .filter(id -> id != null)
                        .toList();
                
                Map<Long, ArticleAnalysis> analysisMap = articleIds.isEmpty() 
                        ? Map.of()
                        : articleAnalysisRepository.findByArticleIdIn(articleIds).stream()
                                .collect(Collectors.toMap(ArticleAnalysis::getArticleId, Function.identity()));
                
                Map<Long, ArticleDiscussion> discussionMap = articleIds.isEmpty()
                        ? Map.of()
                        : articleDiscussionRepository.findByArticleIdIn(articleIds).stream()
                                .collect(Collectors.toMap(ArticleDiscussion::getArticleId, Function.identity()));

                int count = 0;
                for (CollectedData data : page.getContent()) {
                    ArticleAnalysis analysis = data.getId() != null ? analysisMap.get(data.getId()) : null;
                    ArticleDiscussion discussion = data.getId() != null ? discussionMap.get(data.getId()) : null;
                    
                    SearchResult result = convertToSearchResult(data, analysis, discussion);
                    sink.next(SearchEvent.builder()
                            .eventType("result")
                            .source("database")
                            .result(result)
                            .build());
                    count++;
                }

                sink.next(SearchEvent.builder()
                        .eventType("complete")
                        .source("database")
                        .message("저장된 뉴스 검색 완료")
                        .totalCount(count)
                        .build());

                sink.complete();
            } catch (Exception e) {
                log.error("Database search failed: {}", e.getMessage());
                sink.next(SearchEvent.builder()
                        .eventType("error")
                        .source("database")
                        .message("데이터베이스 검색 오류: " + e.getMessage())
                        .build());
                sink.complete();
            }
        });
    }

    private SearchResult convertToSearchResult(CollectedData data, ArticleAnalysis analysis, ArticleDiscussion discussion) {
        DataSource source = data.getSourceId() != null
                ? dataSourceRepository.findById(data.getSourceId()).orElse(null)
                : null;
        String sourceName = source != null ? source.getName() : "뉴스";

        String publishedAt = data.getPublishedDate() != null
                ? data.getPublishedDate().toString()
                : (data.getCollectedAt() != null ? data.getCollectedAt().toString() : null);

        // 원본 콘텐츠를 보존하면서 정제된 텍스트 생성
        String rawContent = data.getContent();
        String cleanedContent = cleanContent(rawContent);
        
        // snippet은 정제된 콘텐츠에서 생성하되, content는 정제된 전체 텍스트 사용
        SearchResult.SearchResultBuilder builder = SearchResult.builder()
                .id(data.getId() != null ? data.getId().toString() : UUID.randomUUID().toString())
                .source("database")
                .sourceLabel(sourceName)
                .title(data.getTitle())
                .snippet(buildSnippetFromCleanText(cleanedContent))
                .content(cleanedContent)  // HTML 제거된 전체 본문 (원본 텍스트 보존)
                .url(data.getUrl())
                .publishedAt(publishedAt)
                .relevanceScore(data.getQualityScore());
        
        // 분석 결과 추가
        if (analysis != null) {
            builder.analyzed(true)
                    .analysisStatus(analysis.getFullyAnalyzed() != null && analysis.getFullyAnalyzed() 
                            ? "complete" : "partial")
                    .reliabilityScore(analysis.getReliabilityScore())
                    .reliabilityGrade(analysis.getReliabilityGrade())
                    .reliabilityColor(analysis.getReliabilityColor())
                    .sentimentLabel(analysis.getSentimentLabel())
                    .sentimentScore(analysis.getSentimentScore())
                    .biasLabel(analysis.getBiasLabel())
                    .biasScore(analysis.getBiasScore())
                    .factcheckStatus(analysis.getFactcheckStatus())
                    .misinfoRisk(analysis.getMisinfoRisk())
                    .riskTags(analysis.getRiskTags())
                    .topics(analysis.getTopics());
        } else {
            builder.analyzed(false)
                    .analysisStatus("pending");
        }
        
        // 여론 분석 결과 추가
        if (discussion != null) {
            builder.hasDiscussion(true)
                    .totalCommentCount(discussion.getTotalCommentCount())
                    .discussionSentiment(discussion.getOverallSentiment());
        } else {
            builder.hasDiscussion(false);
        }
        
        return builder.build();
    }

    /**
     * Convert RankedResult from hybrid search to SearchResult
     */
    private SearchResult convertRankedResultToSearchResult(RankedResult rankedResult, ArticleAnalysis analysis, ArticleDiscussion discussion) {
        // Determine source label based on the sources that found this result
        String sourceLabel = "뉴스";
        if (rankedResult.getSources() != null && !rankedResult.getSources().isEmpty()) {
            if (rankedResult.getSources().contains("semantic") && rankedResult.getSources().contains("keyword")) {
                sourceLabel = "하이브리드 검색";
            } else if (rankedResult.getSources().contains("semantic")) {
                sourceLabel = "시맨틱 검색";
            } else if (rankedResult.getSources().contains("keyword")) {
                sourceLabel = "키워드 검색";
            }
        }

        SearchResult.SearchResultBuilder builder = SearchResult.builder()
                .id(rankedResult.getId())
                .source("database")
                .sourceLabel(sourceLabel)
                .title(rankedResult.getTitle())
                .snippet(rankedResult.getSnippet())
                .content(rankedResult.getContent())
                .url(rankedResult.getUrl())
                .publishedAt(rankedResult.getPublishedAt())
                .relevanceScore(rankedResult.getRrfScore());  // Use RRF score as relevance

        // 분석 결과 추가
        if (analysis != null) {
            builder.analyzed(true)
                    .analysisStatus(analysis.getFullyAnalyzed() != null && analysis.getFullyAnalyzed()
                            ? "complete" : "partial")
                    .reliabilityScore(analysis.getReliabilityScore())
                    .reliabilityGrade(analysis.getReliabilityGrade())
                    .reliabilityColor(analysis.getReliabilityColor())
                    .sentimentLabel(analysis.getSentimentLabel())
                    .sentimentScore(analysis.getSentimentScore())
                    .biasLabel(analysis.getBiasLabel())
                    .biasScore(analysis.getBiasScore())
                    .factcheckStatus(analysis.getFactcheckStatus())
                    .misinfoRisk(analysis.getMisinfoRisk())
                    .riskTags(analysis.getRiskTags())
                    .topics(analysis.getTopics());
        } else {
            builder.analyzed(false)
                    .analysisStatus("pending");
        }

        // 여론 분석 결과 추가
        if (discussion != null) {
            builder.hasDiscussion(true)
                    .totalCommentCount(discussion.getTotalCommentCount())
                    .discussionSentiment(discussion.getOverallSentiment());
        } else {
            builder.hasDiscussion(false);
        }

        return builder.build();
    }

    // ============================================
    // Web Crawling Search
    // ============================================

    private Flux<SearchEvent> searchWeb(String query, String window) {
        return Flux.create(sink -> {
            try {
                sink.next(SearchEvent.builder()
                        .eventType("status")
                        .source("web")
                        .message("웹에서 최신 정보 수집 중...")
                        .build());

                List<String> searchUrls = generateSearchUrls(query, window);
                int successCount = 0;

                for (String url : searchUrls) {
                    try {
                        Crawl4aiClient.CrawlResult crawlResult = crawl4aiClient.crawl(url);
                        if (crawlResult != null && crawlResult.getContent() != null) {
                            String rawContent = crawlResult.getContent();
                            String fullContent = cleanContent(rawContent);  // 전체 본문 정제
                            
                            SearchResult result = SearchResult.builder()
                                    .id(UUID.randomUUID().toString())
                                    .source("web")
                                    .sourceLabel("웹 검색")
                                    .title(crawlResult.getTitle() != null ? crawlResult.getTitle() : extractTitleFromUrl(url))
                                    .snippet(buildSnippet(rawContent))
                                    .content(fullContent)  // 전체 본문 보존
                                    .url(url)
                                    .build();

                            sink.next(SearchEvent.builder()
                                    .eventType("result")
                                    .source("web")
                                    .result(result)
                                    .build());
                            successCount++;
                        }
                    } catch (Exception e) {
                        log.debug("Failed to crawl URL {}: {}", url, e.getMessage());
                    }
                }

                sink.next(SearchEvent.builder()
                        .eventType("complete")
                        .source("web")
                        .message("웹 검색 완료")
                        .totalCount(successCount)
                        .build());

                sink.complete();
            } catch (Exception e) {
                log.error("Web search failed: {}", e.getMessage());
                sink.next(SearchEvent.builder()
                        .eventType("error")
                        .source("web")
                        .message("웹 검색 오류")
                        .build());
                sink.complete();
            }
        });
    }

    private List<String> generateSearchUrls(String query, String window) {
        List<String> urls = new ArrayList<>();
        String encodedQuery = URLEncoder.encode(query, StandardCharsets.UTF_8);

        // 1. 먼저 DB에서 활성화된 웹 검색 소스를 조회
        List<DataSource> webSearchSources = dataSourceRepository.findActiveWebSearchSources();
        
        if (!webSearchSources.isEmpty()) {
            log.info("Found {} active web search sources from database", webSearchSources.size());
            for (DataSource source : webSearchSources) {
                String searchUrl = source.buildSearchUrl(encodedQuery);
                if (searchUrl != null) {
                    urls.add(searchUrl);
                    log.debug("Added search URL from source '{}': {}", source.getName(), searchUrl);
                }
            }
        }
        
        // 2. DB에 등록된 소스가 없으면 기본 포털 사용 (폴백)
        if (urls.isEmpty()) {
            log.info("No web search sources in database, using default portals");
            
            // 네이버 뉴스
            urls.add("https://search.naver.com/search.naver?where=news&query=" + encodedQuery);

            // 다음 뉴스
            urls.add("https://search.daum.net/search?w=news&q=" + encodedQuery);

            // 구글 뉴스 (한국)
            urls.add("https://news.google.com/search?q=" + encodedQuery + "&hl=ko&gl=KR");
        }

        log.info("Generated {} search URLs for query: '{}'", urls.size(), query);
        return urls;
    }

    private String extractTitleFromUrl(String url) {
        if (url.contains("naver")) return "네이버 뉴스";
        if (url.contains("daum")) return "다음 뉴스";
        if (url.contains("google")) return "구글 뉴스";
        return "웹 검색 결과";
    }

    // ============================================
    // AI-Powered Search with Fallback Chain
    // ============================================

    private Flux<SearchEvent> searchAI(String query, String window) {
        // Check if any AI provider is available
        boolean hasAnyProvider = perplexityClient.isEnabled() 
                || openAICompatibleClient.isEnabled() 
                || aiDoveClient.isEnabled()
                || crawlSearchService.isAvailable();

        if (!hasAnyProvider) {
            return Flux.just(SearchEvent.builder()
                    .eventType("status")
                    .source("ai")
                    .message("AI 분석 기능이 비활성화되어 있습니다.")
                    .build());
        }

        return Flux.create(sink -> {
            sink.next(SearchEvent.builder()
                    .eventType("status")
                    .source("ai")
                    .message("AI가 관련 정보를 분석하고 있습니다...")
                    .build());

            String prompt = buildAISearchPrompt(query, window);

            // Build AI stream with fallback chain
            Flux<String> aiStream = getAiStreamWithFallbackForSearch(prompt, query, window);

            StringBuilder fullResponse = new StringBuilder();

            aiStream
                    .doOnNext(chunk -> {
                        fullResponse.append(chunk);
                        // AI 응답을 실시간으로 전송
                        sink.next(SearchEvent.builder()
                                .eventType("ai_chunk")
                                .source("ai")
                                .message(chunk)
                                .build());
                    })
                    .doOnComplete(() -> {
                        // AI 분석 완료 - 전체 텍스트 보존
                        String fullContent = fullResponse.toString();
                        SearchResult aiResult = SearchResult.builder()
                                .id(UUID.randomUUID().toString())
                                .source("ai")
                                .sourceLabel("AI 분석")
                                .title("'" + query + "' AI 분석 결과")
                                .snippet(fullContent.length() > SNIPPET_MAX_LENGTH
                                        ? fullContent.substring(0, SNIPPET_MAX_LENGTH) + "..."
                                        : fullContent)
                                .content(fullContent)  // 전체 AI 분석 결과 보존
                                .build();

                        sink.next(SearchEvent.builder()
                                .eventType("result")
                                .source("ai")
                                .result(aiResult)
                                .build());

                        sink.next(SearchEvent.builder()
                                .eventType("complete")
                                .source("ai")
                                .message("AI 분석 완료")
                                .totalCount(1)
                                .build());

                        sink.complete();
                    })
                    .doOnError(e -> {
                        log.error("AI search failed: {}", e.getMessage());
                        sink.next(SearchEvent.builder()
                                .eventType("error")
                                .source("ai")
                                .message("AI 분석 오류")
                                .build());
                        sink.complete();
                    })
                    .subscribe();
        });
    }

    /**
     * Get AI stream with fallback chain for search.
     * Tries providers in order until one succeeds.
     */
    private Flux<String> getAiStreamWithFallbackForSearch(String prompt, String query, String window) {
        List<AiSearchProviderAttempt> providers = buildAiSearchProviderChain(prompt, query, window);
        
        if (providers.isEmpty()) {
            log.warn("No AI providers available for search");
            return Flux.just("AI 분석을 수행할 수 없습니다.");
        }

        log.info("AI search using fallback chain: {}", 
                providers.stream().map(AiSearchProviderAttempt::name).toList());

        return tryAiSearchProvidersInSequence(providers, 0);
    }

    /**
     * Build AI provider chain for search
     */
    private List<AiSearchProviderAttempt> buildAiSearchProviderChain(String prompt, String query, String window) {
        List<AiSearchProviderAttempt> chain = new ArrayList<>();

        // 1. Perplexity - Best for search with online capabilities
        if (perplexityClient.isEnabled()) {
            chain.add(new AiSearchProviderAttempt("Perplexity", () -> perplexityClient.streamCompletion(prompt)));
        }

        // 2. OpenAI
        if (openAICompatibleClient.isOpenAIEnabled()) {
            chain.add(new AiSearchProviderAttempt("OpenAI", () -> openAICompatibleClient.streamFromOpenAI(prompt)));
        }

        // 3. OpenRouter
        if (openAICompatibleClient.isOpenRouterEnabled()) {
            chain.add(new AiSearchProviderAttempt("OpenRouter", () -> openAICompatibleClient.streamFromOpenRouter(prompt)));
        }

        // 4. Azure OpenAI
        if (openAICompatibleClient.isAzureEnabled()) {
            chain.add(new AiSearchProviderAttempt("Azure", () -> openAICompatibleClient.streamFromAzure(prompt)));
        }

        // 5. AI Dove
        if (aiDoveClient.isEnabled()) {
            chain.add(new AiSearchProviderAttempt("AI Dove", () -> aiDoveClient.chatStream(prompt, null)));
        }

        // 6. CrawlSearchService as fallback
        if (crawlSearchService.isAvailable()) {
            chain.add(new AiSearchProviderAttempt("Crawl Search", () -> crawlSearchService.searchAndAnalyze(query, window)));
        }

        // 7. Ollama - Local LLM
        chain.add(new AiSearchProviderAttempt("Ollama", () -> openAICompatibleClient.streamFromOllama(prompt)));

        // 8. Custom endpoint
        if (openAICompatibleClient.isCustomEnabled()) {
            chain.add(new AiSearchProviderAttempt("Custom", () -> openAICompatibleClient.streamFromCustom(prompt)));
        }

        return chain;
    }

    /**
     * Try AI search providers in sequence
     */
    private Flux<String> tryAiSearchProvidersInSequence(List<AiSearchProviderAttempt> providers, int index) {
        if (index >= providers.size()) {
            log.warn("All AI search providers exhausted");
            return Flux.just("AI 분석 서비스에 연결할 수 없습니다.");
        }

        AiSearchProviderAttempt current = providers.get(index);
        log.info("Trying AI search provider: {} ({}/{})", current.name(), index + 1, providers.size());

        return current.streamSupplier().get()
                .timeout(Duration.ofSeconds(90))
                .onErrorResume(e -> {
                    log.warn("AI search provider {} failed: {}. Trying next...", current.name(), e.getMessage());
                    return tryAiSearchProvidersInSequence(providers, index + 1);
                })
                .switchIfEmpty(Flux.defer(() -> {
                    log.warn("AI search provider {} returned empty. Trying next...", current.name());
                    return tryAiSearchProvidersInSequence(providers, index + 1);
                }));
    }

    /**
     * AI search provider attempt wrapper
     */
    private record AiSearchProviderAttempt(
            String name,
            java.util.function.Supplier<Flux<String>> streamSupplier
    ) {}

    private String buildAISearchPrompt(String query, String window) {
        String timeFrame = switch (window) {
            case "1d" -> "최근 24시간";
            case "30d" -> "최근 한 달";
            default -> "최근 일주일";
        };
        
        // 통화/단위 맥락 분석 힌트 생성
        String currencyContext = buildCurrencyContext(query);

        return """
                [중요: "알겠습니다", "네", "검색하겠습니다" 등의 서두 없이 바로 아래 형식으로 보고서를 작성하세요]
                
                '%s'에 대해 %s 동안의 정보를 철저히 조사하고 분석한 보고서입니다.
                
                ## 분석 원칙
                - **확실한 정보만 보고**: 불확실하거나 추측성 내용은 포함하지 마세요
                - **출처 명시**: 모든 주요 주장에는 반드시 출처를 표기하세요
                - **교차 검증**: 가능한 경우 여러 출처에서 확인된 정보만 포함하세요
                - **객관적 분석**: 특정 입장에 치우치지 않고 균형 있게 분석하세요
                %s
                
                ## 보고서 형식 (이 형식을 정확히 따라주세요)
                
                ### [요약] 핵심 요약
                현재 상황을 4-5문장으로 명확하게 요약해주세요. 핵심 사실만 포함하세요.
                
                ### [검증] 검증된 사실
                여러 출처에서 확인된 사실들을 나열하세요. 각 사실에 출처를 명시하세요.
                
                | 사실 | 출처 | 검증 수준 |
                |------|------|----------|
                | [사실 내용] | [출처명/기관] | 높음/중간/낮음 |
                
                ### [데이터] 주요 수치 및 데이터
                관련된 구체적인 수치, 통계, 날짜 등을 정리하세요.
                - 수치1: [내용] (출처: [출처명])
                - 수치2: [내용] (출처: [출처명])
                
                ### [관점] 다양한 관점
                이 주제에 대한 서로 다른 입장이나 시각을 균형있게 제시하세요.
                
                **입장 A**: [내용] - 출처: [기관/매체명]
                **입장 B**: [내용] - 출처: [기관/매체명]
                
                ### [주의] 주의사항 및 한계
                - 정보의 한계나 불확실한 부분
                - 추가 확인이 필요한 사항
                - 잠재적인 편향이나 이해관계
                
                ### [결론] 결론
                수집된 정보를 바탕으로 한 객관적인 종합 분석을 제공하세요.
                확실하지 않은 내용은 "추가 확인 필요"로 명시하세요.
                
                ---
                * 이 분석은 수집된 자료를 기반으로 작성되었으며, 모든 주장은 출처와 함께 제공됩니다.
                * 최종 판단은 독자의 몫입니다.
                
                한국어로 답변해주세요. 마크다운 형식을 사용하고, "### [요약]"부터 바로 시작하세요.
                """.formatted(query, timeFrame, currencyContext);
    }
    
    /**
     * 쿼리에서 통화/단위 맥락을 분석하여 AI에게 힌트 제공
     * 
     * 예: "비트코인 10억" → 한국어 맥락에서 원화(KRW)일 가능성 높음
     * 예: "Bitcoin $1B" → 달러(USD)로 명시됨
     */
    private String buildCurrencyContext(String query) {
        StringBuilder context = new StringBuilder();
        
        // 숫자 + 억/만/조 패턴 감지 (한국어 숫자 단위)
        boolean hasKoreanNumber = query.matches(".*\\d+\\s*(억|만|조|천).*");
        
        // 명시적 통화 기호 감지
        boolean hasExplicitUsd = query.matches(".*\\$|USD|달러|dollar.*");
        boolean hasExplicitKrw = query.matches(".*₩|KRW|원화|won.*");
        boolean hasExplicitBtc = query.matches(".*BTC|비트코인|bitcoin.*");
        
        // 가격/금액 관련 키워드 감지
        boolean hasPriceKeyword = query.matches(".*(가격|price|도달|목표|전망|예측|forecast).*");
        
        if (hasKoreanNumber && !hasExplicitUsd && hasPriceKeyword) {
            context.append("""
                
                ## 통화/단위 주의사항
                - **중요**: 이 쿼리에 한국어 숫자 단위(억, 만 등)가 포함되어 있습니다
                - 한국어 맥락에서 단위 없는 숫자는 **한국 원화(KRW)**일 가능성이 높습니다
                - 예: "10억" = 10억 원(KRW) ≈ $670,000 USD (환율에 따라 변동)
                - 분석 시 **원화와 달러 양쪽 해석**을 모두 고려하여 작성해주세요
                - 현재 환율 정보도 함께 제공하면 좋습니다
                """);
        } else if (hasExplicitBtc && hasPriceKeyword && !hasExplicitUsd && !hasExplicitKrw) {
            context.append("""
                
                ## 통화/단위 주의사항
                - 암호화폐 가격 분석 시 **USD와 KRW 양쪽 기준**을 모두 언급해주세요
                - 현재 시세와 비교하여 현실적인 분석을 제공해주세요
                - 명시되지 않은 금액은 맥락에 따라 해석하되, 양쪽 가능성을 모두 제시하세요
                """);
        }
        
        return context.toString();
    }

    // ============================================
    // Utility Methods
    // ============================================

    private LocalDateTime calculateSinceDate(String window) {
        return calculateSinceDate(window, null, null);
    }

    /**
     * Calculate the start date for search based on window or custom date range.
     * 
     * @param window Time window (1d, 3d, 7d, 14d, 30d, 90d, 180d, 365d, all)
     * @param startDate Custom start date (ISO 8601 format)
     * @param endDate Custom end date (ISO 8601 format) - currently unused for "since" calculation
     * @return LocalDateTime representing the start date for search
     */
    private LocalDateTime calculateSinceDate(String window, String startDate, String endDate) {
        // If custom startDate is provided, use it
        if (startDate != null && !startDate.isBlank()) {
            try {
                return LocalDateTime.parse(startDate, DateTimeFormatter.ISO_DATE_TIME);
            } catch (DateTimeParseException e) {
                log.warn("Invalid startDate format: '{}', falling back to window: {}", startDate, window);
            }
        }

        LocalDateTime now = LocalDateTime.now();
        return switch (window) {
            case "1h" -> now.minusHours(1);
            case "1d" -> now.minusDays(1);
            case "3d" -> now.minusDays(3);
            case "14d" -> now.minusDays(14);
            case "30d" -> now.minusDays(30);
            case "90d" -> now.minusDays(90);
            case "180d" -> now.minusDays(180);
            case "365d" -> now.minusDays(365);
            case "all" -> LocalDateTime.of(2000, 1, 1, 0, 0);  // Effectively no time limit
            default -> now.minusDays(7);  // Default to 7 days
        };
    }

    /**
     * Calculate the end date for search (for custom date range support).
     * 
     * @param endDate Custom end date (ISO 8601 format)
     * @return LocalDateTime representing the end date for search, or null for "now"
     */
    private LocalDateTime calculateEndDate(String endDate) {
        if (endDate != null && !endDate.isBlank()) {
            try {
                return LocalDateTime.parse(endDate, DateTimeFormatter.ISO_DATE_TIME);
            } catch (DateTimeParseException e) {
                log.warn("Invalid endDate format: '{}', using current time", endDate);
            }
        }
        return null;  // null means "now" (no upper limit)
    }

    /**
     * 이미 정제된 텍스트에서 snippet 생성 (HTML 파싱 불필요)
     */
    private String buildSnippetFromCleanText(String cleanText) {
        if (cleanText == null || cleanText.isBlank()) {
            return null;
        }

        if (cleanText.length() <= SNIPPET_MAX_LENGTH) {
            return cleanText;
        }

        // 단어 경계에서 자르기
        int cut = SNIPPET_MAX_LENGTH;
        for (int i = Math.min(SNIPPET_MAX_LENGTH - 1, cleanText.length() - 1); 
             i > SNIPPET_MAX_LENGTH * 0.6 && i >= 0; i--) {
            if (Character.isWhitespace(cleanText.charAt(i))) {
                cut = i;
                break;
            }
        }

        return cleanText.substring(0, cut).trim() + "...";
    }

    /**
     * 레거시 호환성을 위한 buildSnippet (HTML 파싱 포함)
     * 웹 크롤링 결과 등에서 사용
     */
    private String buildSnippet(String content) {
        if (content == null || content.isBlank()) {
            return null;
        }

        String text;
        try {
            text = Jsoup.parse(content).text();
        } catch (Exception e) {
            text = content;
        }

        text = text.replaceAll("\\s+", " ").trim();
        if (text.isEmpty()) {
            return null;
        }

        if (text.length() <= SNIPPET_MAX_LENGTH) {
            return text;
        }

        int cut = SNIPPET_MAX_LENGTH;
        for (int i = Math.min(SNIPPET_MAX_LENGTH - 1, text.length() - 1); 
             i > SNIPPET_MAX_LENGTH * 0.6 && i >= 0; i--) {
            if (Character.isWhitespace(text.charAt(i))) {
                cut = i;
                break;
            }
        }

        return text.substring(0, cut).trim() + "...";
    }

    /**
     * HTML 태그를 제거하고 정리된 전체 텍스트를 반환합니다.
     * snippet과 달리 길이 제한 없이 전체 내용을 반환합니다.
     * 
     * 중요: 이 메서드는 원본 텍스트 내용을 최대한 보존하며,
     * HTML 태그만 제거하고 실제 텍스트 데이터는 변경하지 않습니다.
     *
     * @param content 원본 콘텐츠 (HTML 포함 가능)
     * @return 정리된 전체 텍스트 (원본 데이터 보존)
     */
    private String cleanContent(String content) {
        if (content == null || content.isBlank()) {
            return null;
        }

        String text;
        try {
            // Jsoup을 사용하여 HTML 태그만 제거, 텍스트 내용은 보존
            text = Jsoup.parse(content).text();
        } catch (Exception e) {
            // HTML 파싱 실패 시 원본 그대로 사용
            text = content;
        }

        // 연속 공백만 정리 (실제 텍스트 내용은 변경하지 않음)
        text = text.replaceAll("\\s+", " ").trim();
        
        return text.isEmpty() ? null : text;
    }

    // ============================================
    // Async Job-based Search (for SSE reconnection support)
    // ============================================

    /**
     * Execute search asynchronously for a job.
     * Results are published to UnifiedSearchEventService.
     * This allows SSE reconnection with the same jobId.
     * 
     * Uses AdvancedIntentAnalyzer for better query understanding and fallback strategies.
     *
     * @param jobId The job ID
     * @param query Search query
     * @param window Time window (1d, 7d, 30d)
     * @param priorityUrls Optional list of URLs to prioritize for web crawling
     * @param startDate Custom start date (ISO 8601 format) - overrides window if provided
     * @param endDate Custom end date (ISO 8601 format)
     */
    @Async
    public void executeSearchAsync(String jobId, String query, String window, List<String> priorityUrls, 
                                   String startDate, String endDate) {
        log.info("Starting async search for job: {}, query: '{}', window: {}, priorityUrls: {}, startDate: {}, endDate: {}", 
                jobId, query, window, priorityUrls != null ? priorityUrls.size() : 0, startDate, endDate);
        
        // Advanced Intent Analysis
        AnalyzedQuery analyzedQuery = advancedIntentAnalyzer.analyzeQuery(query);
        log.info("Async search - Query analyzed: keywords={}, primary='{}', intent={}, strategies={}",
                analyzedQuery.getKeywords().size(),
                analyzedQuery.getPrimaryKeyword(),
                analyzedQuery.getIntentType(),
                analyzedQuery.getFallbackStrategies().size());
        
        unifiedSearchEventService.updateJobStatus(jobId, "IN_PROGRESS");
        
        AtomicInteger totalResults = new AtomicInteger(0);
        AtomicInteger completedSources = new AtomicInteger(0);
        
        // Collect discovered URLs for AutoCrawl integration
        List<String> discoveredUrls = new ArrayList<>();
        
        // Calculate effective date range
        LocalDateTime effectiveStartDate = calculateSinceDate(window, startDate, endDate);
        LocalDateTime effectiveEndDate = calculateEndDate(endDate);
        
        log.info("Effective date range for job {}: {} to {}", jobId, effectiveStartDate, 
                effectiveEndDate != null ? effectiveEndDate : "now");
        
        try {
            // Execute all three searches in parallel
            CompletableFuture<Void> dbFuture = CompletableFuture.runAsync(() -> 
                    executeDbSearchWithFallback(jobId, analyzedQuery, window, startDate, endDate, totalResults, discoveredUrls));
            
            CompletableFuture<Void> webFuture = CompletableFuture.runAsync(() -> 
                    executeWebSearch(jobId, query, window, totalResults, priorityUrls, discoveredUrls));
            
            CompletableFuture<Void> aiFuture = CompletableFuture.runAsync(() -> 
                    executeAiSearch(jobId, query, window, totalResults));
            
            // Wait for all to complete
            CompletableFuture.allOf(dbFuture, webFuture, aiFuture)
                    .thenRun(() -> {
                        log.info("All sources completed for job: {}, total results: {}, discovered URLs: {}", 
                                jobId, totalResults.get(), discoveredUrls.size());
                        
                        // If no results, provide helpful message
                        if (totalResults.get() == 0) {
                            String noResultMessage = advancedIntentAnalyzer.buildNoResultMessage(analyzedQuery);
                            unifiedSearchEventService.publishStatusUpdate(jobId, "system", noResultMessage);
                        }
                        
                        // Save all collected results to search history
                        persistAllResultsToSearchHistory(jobId, query, window, discoveredUrls);
                        
                        unifiedSearchEventService.publishJobComplete(jobId, totalResults.get());
                        
                        // Notify AutoCrawl of discovered URLs
                        if (autoCrawlEnabled && !discoveredUrls.isEmpty()) {
                            autoCrawlIntegrationService.onSearchCompleted(query, discoveredUrls);
                        }
                    })
                    .exceptionally(ex -> {
                        log.error("Error in async search for job: {}", jobId, ex);
                        unifiedSearchEventService.publishJobError(jobId, ex.getMessage());
                        return null;
                    });
                    
        } catch (Exception e) {
            log.error("Failed to start async search for job: {}", jobId, e);
            unifiedSearchEventService.publishJobError(jobId, e.getMessage());
        }
    }

    /**
     * Execute search asynchronously (backward compatible - without custom date range).
     */
    @Async
    public void executeSearchAsync(String jobId, String query, String window, List<String> priorityUrls) {
        executeSearchAsync(jobId, query, window, priorityUrls, null, null);
    }

    /**
     * DB 검색 with 폴백 전략
     */
    private void executeDbSearchWithFallback(
            String jobId, 
            AnalyzedQuery analyzedQuery, 
            String window,
            String startDate,
            String endDate,
            AtomicInteger totalResults,
            List<String> discoveredUrls) {
        
        int attempt = 0;
        int resultsFound = 0;
        
        // 원본 쿼리로 먼저 시도
        String currentQuery = analyzedQuery.getOriginalQuery();
        
        while (attempt < maxFallbackAttempts && resultsFound < 3) {
            String strategyDesc = attempt == 0 
                    ? "원본 쿼리" 
                    : (attempt <= analyzedQuery.getFallbackStrategies().size() 
                            ? analyzedQuery.getFallbackStrategies().get(attempt - 1).getDescription()
                            : "주요 키워드");
            
            unifiedSearchEventService.publishStatusUpdate(jobId, "database", 
                    "검색 전략 " + (attempt + 1) + "/" + maxFallbackAttempts + ": " + strategyDesc);
            
            int found = executeDbSearchForQuery(jobId, currentQuery, window, startDate, endDate, totalResults, discoveredUrls);
            resultsFound += found;
            
            if (resultsFound >= 3) {
                break;  // 충분한 결과 찾음
            }
            
            // 다음 폴백 전략으로
            attempt++;
            if (attempt <= analyzedQuery.getFallbackStrategies().size()) {
                currentQuery = analyzedQuery.getFallbackStrategies().get(attempt - 1).getQuery();
            } else {
                currentQuery = analyzedQuery.getPrimaryKeyword();
            }
        }
        
        String finalMessage = resultsFound > 0 
                ? "데이터베이스 검색 완료 (시도: " + (attempt + 1) + ", 결과: " + resultsFound + ")"
                : "데이터베이스에서 관련 결과를 찾지 못했습니다. 다른 소스를 확인해주세요.";
        
        unifiedSearchEventService.publishSourceComplete(jobId, "database", finalMessage, resultsFound);
    }

    /**
     * 단일 쿼리로 DB 검색 실행
     */
    private int executeDbSearchForQuery(
            String jobId, 
            String query, 
            String window,
            String startDate,
            String endDate,
            AtomicInteger totalResults,
            List<String> discoveredUrls) {
        
        try {
            // Use hybrid search if available
            if (hybridSearchService.isEnabled() && hybridSearchService.isSemanticSearchAvailable()) {
                return executeDbSearchHybridForQuery(jobId, query, window, startDate, endDate, totalResults, discoveredUrls);
            } else {
                return executeDbSearchKeywordForQuery(jobId, query, window, startDate, endDate, totalResults, discoveredUrls);
            }
        } catch (Exception e) {
            log.error("DB search failed for query '{}': {}", query, e.getMessage());
            return 0;
        }
    }

    private int executeDbSearchHybridForQuery(
            String jobId, 
            String query, 
            String window,
            String startDate,
            String endDate,
            AtomicInteger totalResults,
            List<String> discoveredUrls) {
        
        try {
            // Use custom date range if provided, otherwise use window
            String effectiveWindow = (startDate != null && !startDate.isBlank()) ? "custom" : window;
            HybridSearchService.HybridSearchResult hybridResult = hybridSearchService
                    .search(query, effectiveWindow, startDate, endDate).block();
            
            if (hybridResult == null || hybridResult.getResults().isEmpty()) {
                return 0;
            }

            List<Long> articleIds = hybridResult.getResults().stream()
                    .map(r -> {
                        try { return Long.parseLong(r.getId()); } 
                        catch (NumberFormatException e) { return null; }
                    })
                    .filter(id -> id != null)
                    .toList();

            Map<Long, ArticleAnalysis> analysisMap = articleIds.isEmpty()
                    ? Map.of()
                    : articleAnalysisRepository.findByArticleIdIn(articleIds).stream()
                            .collect(Collectors.toMap(ArticleAnalysis::getArticleId, Function.identity()));

            Map<Long, ArticleDiscussion> discussionMap = articleIds.isEmpty()
                    ? Map.of()
                    : articleDiscussionRepository.findByArticleIdIn(articleIds).stream()
                            .collect(Collectors.toMap(ArticleDiscussion::getArticleId, Function.identity()));

            int count = 0;
            for (RankedResult rankedResult : hybridResult.getResults()) {
                Long articleId = null;
                try { articleId = Long.parseLong(rankedResult.getId()); } 
                catch (NumberFormatException ignored) {}

                ArticleAnalysis analysis = articleId != null ? analysisMap.get(articleId) : null;
                ArticleDiscussion discussion = articleId != null ? discussionMap.get(articleId) : null;

                SearchResult result = convertRankedResultToSearchResult(rankedResult, analysis, discussion);
                unifiedSearchEventService.publishResult(jobId, "database", result);
                
                if (result.getUrl() != null && discoveredUrls != null) {
                    synchronized (discoveredUrls) {
                        discoveredUrls.add(result.getUrl());
                    }
                }
                
                count++;
                totalResults.incrementAndGet();
            }
            
            return count;
        } catch (Exception e) {
            log.error("Hybrid search failed: {}", e.getMessage());
            return 0;
        }
    }

    private int executeDbSearchKeywordForQuery(
            String jobId, 
            String query, 
            String window,
            String startDate,
            String endDate,
            AtomicInteger totalResults,
            List<String> discoveredUrls) {
        
        try {
            LocalDateTime since = calculateSinceDate(window, startDate, endDate);
            LocalDateTime until = calculateEndDate(endDate);
            
            // Note: Native query already has ORDER BY clause, so use unsorted PageRequest
            PageRequest pageRequest = PageRequest.of(0, MAX_DB_RESULTS, Sort.unsorted());

            // Use date range query if endDate is specified
            Page<CollectedData> page;
            if (until != null) {
                page = collectedDataRepository.searchByQueryAndDateRange(query, since, until, pageRequest);
            } else {
                page = collectedDataRepository.searchByQueryAndSince(query, since, pageRequest);
            }

            List<Long> articleIds = page.getContent().stream()
                    .map(CollectedData::getId)
                    .filter(id -> id != null)
                    .toList();

            Map<Long, ArticleAnalysis> analysisMap = articleIds.isEmpty()
                    ? Map.of()
                    : articleAnalysisRepository.findByArticleIdIn(articleIds).stream()
                            .collect(Collectors.toMap(ArticleAnalysis::getArticleId, Function.identity()));

            Map<Long, ArticleDiscussion> discussionMap = articleIds.isEmpty()
                    ? Map.of()
                    : articleDiscussionRepository.findByArticleIdIn(articleIds).stream()
                            .collect(Collectors.toMap(ArticleDiscussion::getArticleId, Function.identity()));

            int count = 0;
            for (CollectedData data : page.getContent()) {
                ArticleAnalysis analysis = data.getId() != null ? analysisMap.get(data.getId()) : null;
                ArticleDiscussion discussion = data.getId() != null ? discussionMap.get(data.getId()) : null;

                SearchResult result = convertToSearchResult(data, analysis, discussion);
                unifiedSearchEventService.publishResult(jobId, "database", result);

                if (result.getUrl() != null && discoveredUrls != null) {
                    synchronized (discoveredUrls) {
                        discoveredUrls.add(result.getUrl());
                    }
                }

                count++;
                totalResults.incrementAndGet();
            }

            return count;
        } catch (Exception e) {
            log.error("Keyword search failed: {}", e.getMessage());
            return 0;
        }
    }

    private void executeDbSearch(String jobId, String query, String window, AtomicInteger totalResults, List<String> discoveredUrls) {
        // Use hybrid search if available
        if (hybridSearchService.isEnabled() && hybridSearchService.isSemanticSearchAvailable()) {
            executeDbSearchHybrid(jobId, query, window, totalResults, discoveredUrls);
        } else {
            executeDbSearchKeywordOnly(jobId, query, window, totalResults, discoveredUrls);
        }
    }

    private void executeDbSearchHybrid(String jobId, String query, String window, AtomicInteger totalResults, List<String> discoveredUrls) {
        try {
            unifiedSearchEventService.publishStatusUpdate(jobId, "database", "하이브리드 검색 중 (키워드 + 시맨틱)...");

            HybridSearchService.HybridSearchResult hybridResult = hybridSearchService.search(query, window).block();
            
            if (hybridResult == null || hybridResult.getResults().isEmpty()) {
                log.info("Hybrid search returned no results for job: {}, falling back to keyword search", jobId);
                executeDbSearchKeywordOnly(jobId, query, window, totalResults, discoveredUrls);
                return;
            }

            log.info("Hybrid search completed for job {}: keyword={}, semantic={}, fused={}",
                    jobId, hybridResult.getKeywordResultCount(),
                    hybridResult.getSemanticResultCount(),
                    hybridResult.getTotalResultCount());

            // Batch load analysis data for hybrid results
            List<Long> articleIds = hybridResult.getResults().stream()
                    .map(r -> {
                        try {
                            return Long.parseLong(r.getId());
                        } catch (NumberFormatException e) {
                            return null;
                        }
                    })
                    .filter(id -> id != null)
                    .toList();

            Map<Long, ArticleAnalysis> analysisMap = articleIds.isEmpty()
                    ? Map.of()
                    : articleAnalysisRepository.findByArticleIdIn(articleIds).stream()
                            .collect(Collectors.toMap(ArticleAnalysis::getArticleId, Function.identity()));

            Map<Long, ArticleDiscussion> discussionMap = articleIds.isEmpty()
                    ? Map.of()
                    : articleDiscussionRepository.findByArticleIdIn(articleIds).stream()
                            .collect(Collectors.toMap(ArticleDiscussion::getArticleId, Function.identity()));

            int count = 0;
            for (RankedResult rankedResult : hybridResult.getResults()) {
                Long articleId = null;
                try {
                    articleId = Long.parseLong(rankedResult.getId());
                } catch (NumberFormatException ignored) {}

                ArticleAnalysis analysis = articleId != null ? analysisMap.get(articleId) : null;
                ArticleDiscussion discussion = articleId != null ? discussionMap.get(articleId) : null;

                SearchResult result = convertRankedResultToSearchResult(rankedResult, analysis, discussion);
                unifiedSearchEventService.publishResult(jobId, "database", result);
                
                // Collect URL for AutoCrawl
                if (result.getUrl() != null && discoveredUrls != null) {
                    synchronized (discoveredUrls) {
                        discoveredUrls.add(result.getUrl());
                    }
                }
                
                count++;
                totalResults.incrementAndGet();
            }

            String message = String.format("하이브리드 검색 완료 (키워드: %d, 시맨틱: %d, RRF 융합: %d, %dms)",
                    hybridResult.getKeywordResultCount(),
                    hybridResult.getSemanticResultCount(),
                    hybridResult.getTotalResultCount(),
                    hybridResult.getSearchTimeMs());

            unifiedSearchEventService.publishSourceComplete(jobId, "database", message, count);

        } catch (Exception e) {
            log.error("Hybrid search failed for job: {}, falling back to keyword search: {}", jobId, e.getMessage());
            executeDbSearchKeywordOnly(jobId, query, window, totalResults, discoveredUrls);
        }
    }

    private void executeDbSearchKeywordOnly(String jobId, String query, String window, AtomicInteger totalResults, List<String> discoveredUrls) {
        try {
            unifiedSearchEventService.publishStatusUpdate(jobId, "database", "저장된 뉴스에서 검색 중...");
            
            LocalDateTime since = calculateSinceDate(window);
            // Note: Native query already has ORDER BY clause, so use unsorted PageRequest
            PageRequest pageRequest = PageRequest.of(0, MAX_DB_RESULTS, Sort.unsorted());

            Page<CollectedData> page = collectedDataRepository.searchByQueryAndSince(
                    query, since, pageRequest);

            // Batch load analysis data
            List<Long> articleIds = page.getContent().stream()
                    .map(CollectedData::getId)
                    .filter(id -> id != null)
                    .toList();
            
            Map<Long, ArticleAnalysis> analysisMap = articleIds.isEmpty() 
                    ? Map.of()
                    : articleAnalysisRepository.findByArticleIdIn(articleIds).stream()
                            .collect(Collectors.toMap(ArticleAnalysis::getArticleId, Function.identity()));
            
            Map<Long, ArticleDiscussion> discussionMap = articleIds.isEmpty()
                    ? Map.of()
                    : articleDiscussionRepository.findByArticleIdIn(articleIds).stream()
                            .collect(Collectors.toMap(ArticleDiscussion::getArticleId, Function.identity()));

            int count = 0;
            for (CollectedData data : page.getContent()) {
                ArticleAnalysis analysis = data.getId() != null ? analysisMap.get(data.getId()) : null;
                ArticleDiscussion discussion = data.getId() != null ? discussionMap.get(data.getId()) : null;
                
                SearchResult result = convertToSearchResult(data, analysis, discussion);
                unifiedSearchEventService.publishResult(jobId, "database", result);
                
                // Collect URL for AutoCrawl
                if (result.getUrl() != null && discoveredUrls != null) {
                    synchronized (discoveredUrls) {
                        discoveredUrls.add(result.getUrl());
                    }
                }
                
                count++;
                totalResults.incrementAndGet();
            }

            unifiedSearchEventService.publishSourceComplete(jobId, "database", "저장된 뉴스 검색 완료", count);
            
        } catch (Exception e) {
            log.error("Database search failed for job: {}", jobId, e);
            unifiedSearchEventService.publishSourceError(jobId, "database", "데이터베이스 검색 오류: " + e.getMessage());
        }
    }

    private void executeWebSearch(String jobId, String query, String window, AtomicInteger totalResults, List<String> priorityUrls, List<String> discoveredUrls) {
        try {
            unifiedSearchEventService.publishStatusUpdate(jobId, "web", "웹에서 최신 정보 수집 중...");
            
            // Use priorityUrls if provided, otherwise fall back to generated URLs
            List<String> searchUrls;
            if (priorityUrls != null && !priorityUrls.isEmpty()) {
                searchUrls = priorityUrls;
                log.info("Using {} priority URLs for web search in job: {}", priorityUrls.size(), jobId);
            } else {
                searchUrls = generateSearchUrls(query, window);
                log.info("Using {} generated search URLs for web search in job: {}", searchUrls.size(), jobId);
            }
            
            int successCount = 0;

            for (String url : searchUrls) {
                try {
                    Crawl4aiClient.CrawlResult crawlResult = crawl4aiClient.crawl(url);
                    if (crawlResult != null && crawlResult.getContent() != null) {
                        String fullContent = cleanContent(crawlResult.getContent());
                        SearchResult result = SearchResult.builder()
                                .id(UUID.randomUUID().toString())
                                .source("web")
                                .sourceLabel("웹 검색")
                                .title(crawlResult.getTitle() != null ? crawlResult.getTitle() : extractTitleFromUrl(url))
                                .snippet(buildSnippet(crawlResult.getContent()))
                                .content(fullContent)  // 전체 본문 추가
                                .url(url)
                                .build();

                        unifiedSearchEventService.publishResult(jobId, "web", result);
                        
                        // Collect URL for AutoCrawl
                        if (discoveredUrls != null) {
                            synchronized (discoveredUrls) {
                                discoveredUrls.add(url);
                            }
                        }
                        
                        successCount++;
                        totalResults.incrementAndGet();
                    }
                } catch (Exception e) {
                    log.debug("Failed to crawl URL {} for job {}: {}", url, jobId, e.getMessage());
                }
            }

            unifiedSearchEventService.publishSourceComplete(jobId, "web", "웹 검색 완료", successCount);
            
        } catch (Exception e) {
            log.error("Web search failed for job: {}", jobId, e);
            unifiedSearchEventService.publishSourceError(jobId, "web", "웹 검색 오류");
        }
    }

    private void executeAiSearch(String jobId, String query, String window, AtomicInteger totalResults) {
        // Check if any AI provider is available
        boolean hasAnyProvider = perplexityClient.isEnabled() 
                || openAICompatibleClient.isEnabled() 
                || aiDoveClient.isEnabled()
                || crawlSearchService.isAvailable();

        if (!hasAnyProvider) {
            unifiedSearchEventService.publishStatusUpdate(jobId, "ai", "AI 분석 기능이 비활성화되어 있습니다.");
            unifiedSearchEventService.publishSourceComplete(jobId, "ai", "AI 분석 비활성화", 0);
            return;
        }

        try {
            unifiedSearchEventService.publishStatusUpdate(jobId, "ai", "AI가 관련 정보를 분석하고 있습니다...");
            
            // ===== 실시간 데이터 필요 여부 분석 =====
            String realtimeContext = collectRealtimeDataIfNeeded(jobId, query);
            
            // 프롬프트 생성 (실시간 데이터 포함)
            String prompt = buildAISearchPromptWithRealtimeData(query, window, realtimeContext);
            
            // Use fallback chain
            Flux<String> aiStream = getAiStreamWithFallbackForSearch(prompt, query, window);

            StringBuilder fullResponse = new StringBuilder();
            
            // Block and collect all AI response (since we're in async context)
            aiStream
                    .doOnNext(chunk -> {
                        fullResponse.append(chunk);
                        unifiedSearchEventService.publishAiChunk(jobId, chunk);
                    })
                    .blockLast(Duration.ofMinutes(2));

            // Publish final AI result - 전체 텍스트 보존
            String fullContent = fullResponse.toString();
            SearchResult aiResult = SearchResult.builder()
                    .id(UUID.randomUUID().toString())
                    .source("ai")
                    .sourceLabel("AI 분석")
                    .title("'" + query + "' AI 분석 결과")
                    .snippet(fullContent.length() > SNIPPET_MAX_LENGTH
                            ? fullContent.substring(0, SNIPPET_MAX_LENGTH) + "..."
                            : fullContent)
                    .content(fullContent)  // 전체 AI 분석 결과 보존
                    .build();

            unifiedSearchEventService.publishResult(jobId, "ai", aiResult);
            totalResults.incrementAndGet();
            unifiedSearchEventService.publishSourceComplete(jobId, "ai", "AI 분석 완료", 1);

            persistAiReportToSearchHistory(jobId, query, window, fullContent);
            
        } catch (Exception e) {
            log.error("AI search failed for job: {}", jobId, e);
            unifiedSearchEventService.publishSourceError(jobId, "ai", "AI 분석 오류");
        }
    }

    /**
     * 실시간 데이터 필요 여부를 분석하고, 필요시 Perplexity Online으로 데이터를 수집합니다.
     * 
     * @param jobId 검색 작업 ID
     * @param query 사용자 쿼리
     * @return 실시간 데이터 컨텍스트 문자열 (필요 없는 경우 빈 문자열)
     */
    private String collectRealtimeDataIfNeeded(String jobId, String query) {
        try {
            // 실시간 데이터 필요 여부 분석
            RealtimeAnalysisResult realtimeAnalysis = advancedIntentAnalyzer.analyzeRealtimeDataNeed(query);
            
            if (!realtimeAnalysis.isNeedsRealtimeData()) {
                log.debug("Query '{}' does not require realtime data (confidence: {})", 
                        query, realtimeAnalysis.getConfidence());
                return "";
            }
            
            if (!realtimeSearchSource.isAvailable()) {
                log.debug("Realtime search source is not available for query: '{}'", query);
                return "";
            }
            
            log.info("Query '{}' requires realtime data (type: {}, confidence: {}, reason: {})", 
                    query, realtimeAnalysis.getDataType(), realtimeAnalysis.getConfidence(), 
                    realtimeAnalysis.getReason());
            
            unifiedSearchEventService.publishStatusUpdate(jobId, "ai", "실시간 데이터 수집 중...");
            
            // Perplexity Online으로 실시간 데이터 수집
            List<SourceEvidence> realtimeEvidence = realtimeSearchSource
                    .fetchEvidence(query, "ko")
                    .collectList()
                    .block(Duration.ofSeconds(30));
            
            if (realtimeEvidence == null || realtimeEvidence.isEmpty()) {
                log.info("No realtime evidence collected for query: '{}'", query);
                return "";
            }
            
            // 실시간 데이터를 컨텍스트 문자열로 변환
            StringBuilder contextBuilder = new StringBuilder();
            contextBuilder.append("\n\n## 실시간 검색 결과 (반드시 이 데이터를 우선 참조하세요)\n\n");
            
            for (SourceEvidence evidence : realtimeEvidence) {
                contextBuilder.append("### 출처: ").append(evidence.getSourceName()).append("\n");
                contextBuilder.append(evidence.getExcerpt()).append("\n");
                if (evidence.getUrl() != null) {
                    contextBuilder.append("URL: ").append(evidence.getUrl()).append("\n");
                }
                contextBuilder.append("\n");
            }
            
            log.info("Collected {} realtime evidence items for query: '{}'", 
                    realtimeEvidence.size(), query);
            
            return contextBuilder.toString();
            
        } catch (Exception e) {
            log.warn("Failed to collect realtime data for query '{}': {}", query, e.getMessage());
            return "";
        }
    }

    /**
     * 실시간 데이터를 포함한 AI 검색 프롬프트를 생성합니다.
     * 
     * @param query 사용자 쿼리
     * @param window 시간 범위
     * @param realtimeContext 실시간 데이터 컨텍스트 (빈 문자열 가능)
     * @return AI 프롬프트
     */
    private String buildAISearchPromptWithRealtimeData(String query, String window, String realtimeContext) {
        String basePrompt = buildAISearchPrompt(query, window);
        
        if (realtimeContext == null || realtimeContext.isBlank()) {
            return basePrompt;
        }
        
        // 실시간 데이터를 프롬프트에 삽입
        // "한국어로 답변해주세요" 또는 프롬프트 끝에 추가
        String realtimeInstructions = realtimeContext + 
                "\n\n**중요**: 위의 실시간 검색 결과에 포함된 가격, 시세, 통계 데이터를 반드시 사용하세요. " +
                "추정하거나 과거 데이터를 사용하지 마세요. 실시간 데이터의 출처도 명시해주세요.\n\n";
        
        // 프롬프트에 "한국어로 답변해주세요"가 있으면 그 앞에 삽입
        if (basePrompt.contains("한국어로 답변해주세요")) {
            return basePrompt.replace(
                    "한국어로 답변해주세요.",
                    realtimeInstructions + "한국어로 답변해주세요."
            );
        }
        
        // 없으면 프롬프트 끝에 추가
        return basePrompt + realtimeInstructions;
    }

    private void persistAiReportToSearchHistory(String jobId, String query, String window, String fullMarkdown) {
        try {
            Map<String, Object> aiSummary = new HashMap<>();
            aiSummary.put(AI_SUMMARY_KEY_CONTENT, fullMarkdown);
            aiSummary.put(AI_SUMMARY_KEY_SUMMARY, extractSummarySection(fullMarkdown));
            aiSummary.put(AI_SUMMARY_KEY_GENERATED_AT, System.currentTimeMillis());

            SearchHistoryMessage message = SearchHistoryMessage.builder()
                    .externalId(jobId)
                    .searchType(SearchType.UNIFIED)
                    .query(query)
                    .timeWindow(window)
                    .resultCount(0)
                    .results(List.of())
                    .aiSummary(aiSummary)
                    .success(true)
                    .timestamp(System.currentTimeMillis())
                    .build();

            searchHistoryService.saveFromMessage(message);
            log.info("Saved unified AI report to search history: jobId={}", jobId);
        } catch (Exception e) {
            log.warn("Failed to save unified AI report to search history: jobId={}, error={}", jobId, e.getMessage());
        }
    }

    /**
     * Save all collected search results to search history.
     * This includes DB results, web crawl results, and discovered URLs.
     */
    private void persistAllResultsToSearchHistory(String jobId, String query, String window, List<String> discoveredUrls) {
        try {
            // Get all collected results from the event service
            List<Map<String, Object>> collectedResults = unifiedSearchEventService.getCollectedResults(jobId);
            
            if (collectedResults.isEmpty()) {
                log.debug("No results to persist for job: {}", jobId);
                return;
            }

            SearchHistoryMessage message = SearchHistoryMessage.builder()
                    .externalId(jobId + "-results")
                    .searchType(SearchType.UNIFIED)
                    .query(query)
                    .timeWindow(window)
                    .resultCount(collectedResults.size())
                    .results(collectedResults)
                    .discoveredUrls(discoveredUrls)
                    .success(true)
                    .timestamp(System.currentTimeMillis())
                    .build();

            searchHistoryService.saveFromMessage(message);
            log.info("Saved {} unified search results to search history: jobId={}", collectedResults.size(), jobId);
        } catch (Exception e) {
            log.warn("Failed to save unified search results to search history: jobId={}, error={}", jobId, e.getMessage());
        }
    }

    private String extractSummarySection(String markdown) {
        if (markdown == null || markdown.isBlank()) {
            return null;
        }

        int start = markdown.indexOf("### [요약]");
        if (start < 0) {
            start = markdown.indexOf("## [요약]");
        }
        if (start < 0) {
            return null;
        }

        int next = markdown.indexOf("\n### ", start + 1);
        if (next < 0) {
            next = markdown.length();
        }

        String section = markdown.substring(start, next).trim();
        return section.isEmpty() ? null : section;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/VectorEmbeddingService.java

```java
package com.newsinsight.collector.service;

import io.micrometer.core.instrument.Counter;
import io.micrometer.core.instrument.MeterRegistry;
import io.micrometer.core.instrument.Timer;
import jakarta.annotation.PostConstruct;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.boot.context.event.ApplicationReadyEvent;
import org.springframework.context.event.EventListener;
import org.springframework.http.MediaType;
import org.springframework.retry.annotation.Backoff;
import org.springframework.retry.annotation.Retryable;
import org.springframework.stereotype.Service;
import org.springframework.web.reactive.function.client.WebClient;
import org.springframework.web.reactive.function.client.WebClientResponseException;
import reactor.core.publisher.Mono;
import reactor.util.retry.Retry;

import java.time.Duration;
import java.util.*;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicLong;

/**
 * 벡터 임베딩 서비스
 * 
 * 채팅 메시지를 벡터 DB에 임베딩하여 저장합니다.
 * 유사 질문 검색, 컨텍스트 검색 등에 활용됩니다.
 * 
 * 개선사항:
 * - 자동 초기화 (ApplicationReadyEvent)
 * - 재시도 로직 (Retry)
 * - 배치 임베딩 지원
 * - 연결 상태 확인
 * - 메트릭 수집
 * - 로컬 임베딩 대체 지원
 */
@Service
@Slf4j
public class VectorEmbeddingService {

    private final WebClient webClient;
    private final MeterRegistry meterRegistry;

    @Value("${vector.db.enabled:false}")
    private boolean vectorDbEnabled;

    @Value("${vector.db.url:http://localhost:6333}")
    private String vectorDbUrl;

    @Value("${vector.db.collection:factcheck_chat}")
    private String collectionName;

    @Value("${vector.embedding.model:text-embedding-ada-002}")
    private String embeddingModel;

    @Value("${vector.embedding.api-key:}")
    private String apiKey;

    @Value("${vector.embedding.dimension:1536}")
    private int embeddingDimension;

    @Value("${vector.embedding.timeout-seconds:30}")
    private int timeoutSeconds;

    @Value("${vector.embedding.max-retry:3}")
    private int maxRetry;

    @Value("${vector.embedding.batch-size:10}")
    private int batchSize;

    // 로컬 임베딩 서비스 설정 (HuggingFace TEI 등)
    @Value("${vector.embedding.local.enabled:false}")
    private boolean localEmbeddingEnabled;

    @Value("${vector.embedding.local.url:http://localhost:8011}")
    private String localEmbeddingUrl;

    // 상태 플래그
    private final AtomicBoolean vectorDbHealthy = new AtomicBoolean(false);
    private final AtomicBoolean embeddingServiceHealthy = new AtomicBoolean(false);

    // 메트릭
    private Counter embeddingSuccessCounter;
    private Counter embeddingErrorCounter;
    private Counter searchSuccessCounter;
    private Counter searchErrorCounter;
    private Timer embeddingDurationTimer;
    private Timer searchDurationTimer;
    private final AtomicLong embeddingQueueSize = new AtomicLong(0);

    public VectorEmbeddingService(WebClient.Builder webClientBuilder, MeterRegistry meterRegistry) {
        this.webClient = webClientBuilder
                .codecs(configurer -> configurer.defaultCodecs().maxInMemorySize(16 * 1024 * 1024))
                .build();
        this.meterRegistry = meterRegistry;
    }

    @PostConstruct
    public void initMetrics() {
        embeddingSuccessCounter = Counter.builder("vector.embedding.success")
                .description("Number of successful embeddings")
                .register(meterRegistry);

        embeddingErrorCounter = Counter.builder("vector.embedding.error")
                .description("Number of failed embeddings")
                .register(meterRegistry);

        searchSuccessCounter = Counter.builder("vector.search.success")
                .description("Number of successful searches")
                .register(meterRegistry);

        searchErrorCounter = Counter.builder("vector.search.error")
                .description("Number of failed searches")
                .register(meterRegistry);

        embeddingDurationTimer = Timer.builder("vector.embedding.duration")
                .description("Time taken for embedding generation")
                .register(meterRegistry);

        searchDurationTimer = Timer.builder("vector.search.duration")
                .description("Time taken for vector search")
                .register(meterRegistry);

        meterRegistry.gauge("vector.embedding.queue.size", embeddingQueueSize);
        meterRegistry.gauge("vector.db.healthy", vectorDbHealthy, b -> b.get() ? 1.0 : 0.0);
        meterRegistry.gauge("vector.embedding.service.healthy", embeddingServiceHealthy, b -> b.get() ? 1.0 : 0.0);
    }

    /**
     * 애플리케이션 시작 시 벡터 DB 초기화
     */
    @EventListener(ApplicationReadyEvent.class)
    public void onApplicationReady() {
        if (vectorDbEnabled) {
            log.info("Initializing Vector DB on application startup...");
            initializeVectorDb();
            checkVectorDbHealth();
            checkEmbeddingServiceHealth();
        } else {
            log.info("Vector DB is disabled");
        }
    }

    /**
     * 채팅 메시지를 벡터 DB에 임베딩
     * 
     * @param sessionId 세션 ID
     * @param messageId 메시지 ID
     * @param content 메시지 내용
     * @param metadata 메타데이터
     * @return 임베딩 ID
     */
    @Retryable(
            retryFor = {WebClientResponseException.class},
            maxAttempts = 3,
            backoff = @Backoff(delay = 1000, multiplier = 2)
    )
    public String embedChatMessage(String sessionId, String messageId, String content, Object metadata) {
        if (!vectorDbEnabled) {
            log.debug("Vector DB is disabled, skipping embedding");
            return null;
        }

        if (!vectorDbHealthy.get()) {
            log.warn("Vector DB is not healthy, skipping embedding");
            return null;
        }

        Timer.Sample sample = Timer.start(meterRegistry);
        embeddingQueueSize.incrementAndGet();

        try {
            // 1. 텍스트 임베딩 생성
            List<Double> embedding = generateEmbeddingWithFallback(content);

            if (embedding == null || embedding.isEmpty()) {
                log.error("Failed to generate embedding for message {}", messageId);
                embeddingErrorCounter.increment();
                return null;
            }

            // 2. 벡터 DB에 저장
            String embeddingId = UUID.randomUUID().toString();
            storeEmbedding(embeddingId, sessionId, messageId, content, embedding, metadata);

            sample.stop(embeddingDurationTimer);
            embeddingSuccessCounter.increment();
            
            log.info("Embedded message {} to vector DB with ID: {}", messageId, embeddingId);
            return embeddingId;

        } catch (Exception e) {
            log.error("Failed to embed message {}: {}", messageId, e.getMessage(), e);
            sample.stop(embeddingDurationTimer);
            embeddingErrorCounter.increment();
            return null;
        } finally {
            embeddingQueueSize.decrementAndGet();
        }
    }

    /**
     * 배치로 여러 메시지 임베딩
     */
    public List<String> embedChatMessagesBatch(List<EmbeddingRequest> requests) {
        if (!vectorDbEnabled || !vectorDbHealthy.get()) {
            return Collections.emptyList();
        }

        List<String> embeddingIds = new ArrayList<>();
        
        // 배치 크기로 나누어 처리
        for (int i = 0; i < requests.size(); i += batchSize) {
            List<EmbeddingRequest> batch = requests.subList(i, Math.min(i + batchSize, requests.size()));
            
            for (EmbeddingRequest request : batch) {
                String embeddingId = embedChatMessage(
                        request.getSessionId(),
                        request.getMessageId(),
                        request.getContent(),
                        request.getMetadata()
                );
                if (embeddingId != null) {
                    embeddingIds.add(embeddingId);
                }
            }
        }
        
        return embeddingIds;
    }

    /**
     * 텍스트 임베딩 생성 (폴백 포함)
     */
    private List<Double> generateEmbeddingWithFallback(String text) {
        // 1. 로컬 임베딩 서비스 시도
        if (localEmbeddingEnabled && embeddingServiceHealthy.get()) {
            try {
                List<Double> embedding = generateLocalEmbedding(text);
                if (embedding != null && !embedding.isEmpty()) {
                    return embedding;
                }
            } catch (Exception e) {
                log.warn("Local embedding failed, falling back to OpenAI: {}", e.getMessage());
            }
        }

        // 2. OpenAI API 시도
        if (apiKey != null && !apiKey.isBlank()) {
            try {
                return generateOpenAIEmbedding(text);
            } catch (Exception e) {
                log.error("OpenAI embedding failed: {}", e.getMessage());
            }
        }

        // 3. 더미 임베딩 (최후의 수단)
        log.warn("All embedding methods failed, using dummy embedding");
        return generateDummyEmbedding();
    }

    /**
     * 로컬 임베딩 서비스로 임베딩 생성 (HuggingFace TEI)
     */
    private List<Double> generateLocalEmbedding(String text) {
        Map<String, Object> request = new HashMap<>();
        request.put("inputs", text);

        return webClient.post()
                .uri(localEmbeddingUrl + "/embed")
                .contentType(MediaType.APPLICATION_JSON)
                .bodyValue(request)
                .retrieve()
                .bodyToMono(List.class)
                .timeout(Duration.ofSeconds(timeoutSeconds))
                .retryWhen(Retry.backoff(maxRetry, Duration.ofSeconds(1)))
                .block();
    }

    /**
     * OpenAI API로 임베딩 생성
     */
    @SuppressWarnings("unchecked")
    private List<Double> generateOpenAIEmbedding(String text) {
        Map<String, Object> request = new HashMap<>();
        request.put("input", text);
        request.put("model", embeddingModel);

        Map<String, Object> response = webClient.post()
                .uri("https://api.openai.com/v1/embeddings")
                .header("Authorization", "Bearer " + apiKey)
                .contentType(MediaType.APPLICATION_JSON)
                .bodyValue(request)
                .retrieve()
                .bodyToMono(Map.class)
                .timeout(Duration.ofSeconds(timeoutSeconds))
                .retryWhen(Retry.backoff(maxRetry, Duration.ofSeconds(1)))
                .block();

        if (response != null && response.containsKey("data")) {
            List<Map<String, Object>> data = (List<Map<String, Object>>) response.get("data");
            if (!data.isEmpty()) {
                return (List<Double>) data.get(0).get("embedding");
            }
        }

        return null;
    }

    /**
     * 벡터 DB에 임베딩 저장 (Qdrant)
     */
    private void storeEmbedding(
            String embeddingId,
            String sessionId,
            String messageId,
            String content,
            List<Double> embedding,
            Object metadata
    ) {
        Map<String, Object> point = new HashMap<>();
        point.put("id", embeddingId);
        point.put("vector", embedding);

        Map<String, Object> payload = new HashMap<>();
        payload.put("session_id", sessionId);
        payload.put("message_id", messageId);
        payload.put("content", content);
        payload.put("metadata", metadata);
        payload.put("timestamp", System.currentTimeMillis());
        payload.put("created_at", java.time.Instant.now().toString());
        point.put("payload", payload);

        Map<String, Object> request = new HashMap<>();
        request.put("points", List.of(point));

        webClient.put()
                .uri(vectorDbUrl + "/collections/" + collectionName + "/points")
                .contentType(MediaType.APPLICATION_JSON)
                .bodyValue(request)
                .retrieve()
                .bodyToMono(Void.class)
                .timeout(Duration.ofSeconds(timeoutSeconds))
                .retryWhen(Retry.backoff(maxRetry, Duration.ofSeconds(1)))
                .block();

        log.debug("Stored embedding {} in vector DB", embeddingId);
    }

    /**
     * 유사 메시지 검색
     * 
     * @param queryText 검색 쿼리
     * @param limit 결과 개수
     * @return 유사 메시지 목록
     */
    @SuppressWarnings("unchecked")
    public List<Map<String, Object>> searchSimilarMessages(String queryText, int limit) {
        return searchSimilarMessages(queryText, limit, 0.5f);
    }

    /**
     * 유사 메시지 검색 (최소 점수 지정)
     */
    @SuppressWarnings("unchecked")
    public List<Map<String, Object>> searchSimilarMessages(String queryText, int limit, float minScore) {
        if (!vectorDbEnabled || !vectorDbHealthy.get()) {
            return List.of();
        }

        Timer.Sample sample = Timer.start(meterRegistry);

        try {
            List<Double> queryEmbedding = generateEmbeddingWithFallback(queryText);
            
            if (queryEmbedding == null || queryEmbedding.isEmpty()) {
                log.error("Failed to generate query embedding");
                searchErrorCounter.increment();
                return List.of();
            }

            Map<String, Object> request = new HashMap<>();
            request.put("vector", queryEmbedding);
            request.put("limit", limit);
            request.put("with_payload", true);
            request.put("score_threshold", minScore);

            Map<String, Object> response = webClient.post()
                    .uri(vectorDbUrl + "/collections/" + collectionName + "/points/search")
                    .contentType(MediaType.APPLICATION_JSON)
                    .bodyValue(request)
                    .retrieve()
                    .bodyToMono(Map.class)
                    .timeout(Duration.ofSeconds(timeoutSeconds))
                    .retryWhen(Retry.backoff(maxRetry, Duration.ofSeconds(1)))
                    .block();

            sample.stop(searchDurationTimer);
            searchSuccessCounter.increment();

            if (response != null && response.containsKey("result")) {
                return (List<Map<String, Object>>) response.get("result");
            }

            return List.of();

        } catch (Exception e) {
            log.error("Failed to search similar messages: {}", e.getMessage());
            sample.stop(searchDurationTimer);
            searchErrorCounter.increment();
            return List.of();
        }
    }

    /**
     * 세션 ID로 필터링된 유사 메시지 검색
     */
    @SuppressWarnings("unchecked")
    public List<Map<String, Object>> searchSimilarMessagesInSession(String queryText, String sessionId, int limit) {
        if (!vectorDbEnabled || !vectorDbHealthy.get()) {
            return List.of();
        }

        try {
            List<Double> queryEmbedding = generateEmbeddingWithFallback(queryText);
            
            if (queryEmbedding == null || queryEmbedding.isEmpty()) {
                return List.of();
            }

            Map<String, Object> filter = Map.of(
                    "must", List.of(
                            Map.of("key", "session_id",
                                   "match", Map.of("value", sessionId))
                    )
            );

            Map<String, Object> request = new HashMap<>();
            request.put("vector", queryEmbedding);
            request.put("limit", limit);
            request.put("with_payload", true);
            request.put("filter", filter);

            Map<String, Object> response = webClient.post()
                    .uri(vectorDbUrl + "/collections/" + collectionName + "/points/search")
                    .contentType(MediaType.APPLICATION_JSON)
                    .bodyValue(request)
                    .retrieve()
                    .bodyToMono(Map.class)
                    .timeout(Duration.ofSeconds(timeoutSeconds))
                    .block();

            if (response != null && response.containsKey("result")) {
                return (List<Map<String, Object>>) response.get("result");
            }

            return List.of();

        } catch (Exception e) {
            log.error("Failed to search similar messages in session: {}", e.getMessage());
            return List.of();
        }
    }

    /**
     * 더미 임베딩 생성 (테스트용)
     */
    private List<Double> generateDummyEmbedding() {
        List<Double> dummy = new ArrayList<>();
        Random random = new Random(System.currentTimeMillis());
        for (int i = 0; i < embeddingDimension; i++) {
            dummy.add(random.nextGaussian() * 0.1);
        }
        // 정규화
        double norm = Math.sqrt(dummy.stream().mapToDouble(d -> d * d).sum());
        return dummy.stream().map(d -> d / norm).toList();
    }

    /**
     * 벡터 DB 초기화 (컬렉션 생성)
     */
    public void initializeVectorDb() {
        if (!vectorDbEnabled) {
            return;
        }

        try {
            // 1. 컬렉션 존재 여부 확인
            Boolean exists = checkCollectionExists();
            
            if (Boolean.TRUE.equals(exists)) {
                log.info("Vector DB collection '{}' already exists", collectionName);
                vectorDbHealthy.set(true);
                return;
            }

            // 2. 컬렉션 생성
            Map<String, Object> config = new HashMap<>();
            config.put("vectors", Map.of(
                    "size", embeddingDimension,
                    "distance", "Cosine"
            ));

            // 최적화 설정
            config.put("optimizers_config", Map.of(
                    "indexing_threshold", 20000,
                    "memmap_threshold", 50000
            ));

            webClient.put()
                    .uri(vectorDbUrl + "/collections/" + collectionName)
                    .contentType(MediaType.APPLICATION_JSON)
                    .bodyValue(config)
                    .retrieve()
                    .bodyToMono(Void.class)
                    .timeout(Duration.ofSeconds(30))
                    .block();

            // 3. 인덱스 생성
            createPayloadIndex("session_id");
            createPayloadIndex("message_id");

            vectorDbHealthy.set(true);
            log.info("Initialized vector DB collection: {}", collectionName);

        } catch (Exception e) {
            log.error("Vector DB initialization failed: {}", e.getMessage());
            vectorDbHealthy.set(false);
        }
    }

    /**
     * 컬렉션 존재 여부 확인
     */
    @SuppressWarnings("unchecked")
    private Boolean checkCollectionExists() {
        try {
            Map<String, Object> response = webClient.get()
                    .uri(vectorDbUrl + "/collections/" + collectionName)
                    .retrieve()
                    .bodyToMono(Map.class)
                    .timeout(Duration.ofSeconds(10))
                    .block();
            return response != null && response.containsKey("result");
        } catch (WebClientResponseException.NotFound e) {
            return false;
        } catch (Exception e) {
            log.warn("Failed to check collection existence: {}", e.getMessage());
            return false;
        }
    }

    /**
     * 페이로드 인덱스 생성
     */
    private void createPayloadIndex(String fieldName) {
        try {
            Map<String, Object> request = Map.of(
                    "field_name", fieldName,
                    "field_schema", "keyword"
            );

            webClient.put()
                    .uri(vectorDbUrl + "/collections/" + collectionName + "/index")
                    .contentType(MediaType.APPLICATION_JSON)
                    .bodyValue(request)
                    .retrieve()
                    .bodyToMono(Void.class)
                    .timeout(Duration.ofSeconds(10))
                    .block();

            log.debug("Created payload index for field: {}", fieldName);
        } catch (Exception e) {
            log.warn("Failed to create payload index for {}: {}", fieldName, e.getMessage());
        }
    }

    /**
     * 벡터 DB 헬스 체크
     */
    public void checkVectorDbHealth() {
        if (!vectorDbEnabled) {
            vectorDbHealthy.set(false);
            return;
        }

        try {
            webClient.get()
                    .uri(vectorDbUrl + "/")
                    .retrieve()
                    .bodyToMono(Map.class)
                    .timeout(Duration.ofSeconds(5))
                    .block();
            
            vectorDbHealthy.set(true);
            log.debug("Vector DB health check passed");
        } catch (Exception e) {
            vectorDbHealthy.set(false);
            log.warn("Vector DB health check failed: {}", e.getMessage());
        }
    }

    /**
     * 임베딩 서비스 헬스 체크
     */
    public void checkEmbeddingServiceHealth() {
        if (!localEmbeddingEnabled) {
            embeddingServiceHealthy.set(apiKey != null && !apiKey.isBlank());
            return;
        }

        try {
            webClient.get()
                    .uri(localEmbeddingUrl + "/health")
                    .retrieve()
                    .bodyToMono(String.class)
                    .timeout(Duration.ofSeconds(5))
                    .block();
            
            embeddingServiceHealthy.set(true);
            log.debug("Embedding service health check passed");
        } catch (Exception e) {
            embeddingServiceHealthy.set(apiKey != null && !apiKey.isBlank());
            log.warn("Local embedding service health check failed, using OpenAI: {}", e.getMessage());
        }
    }

    /**
     * 임베딩 삭제
     */
    public boolean deleteEmbedding(String embeddingId) {
        if (!vectorDbEnabled || !vectorDbHealthy.get()) {
            return false;
        }

        try {
            Map<String, Object> request = Map.of(
                    "points", List.of(embeddingId)
            );

            webClient.post()
                    .uri(vectorDbUrl + "/collections/" + collectionName + "/points/delete")
                    .contentType(MediaType.APPLICATION_JSON)
                    .bodyValue(request)
                    .retrieve()
                    .bodyToMono(Void.class)
                    .timeout(Duration.ofSeconds(10))
                    .block();

            log.info("Deleted embedding: {}", embeddingId);
            return true;
        } catch (Exception e) {
            log.error("Failed to delete embedding {}: {}", embeddingId, e.getMessage());
            return false;
        }
    }

    /**
     * 서비스 상태 조회
     */
    public VectorServiceStatus getStatus() {
        return VectorServiceStatus.builder()
                .enabled(vectorDbEnabled)
                .vectorDbHealthy(vectorDbHealthy.get())
                .embeddingServiceHealthy(embeddingServiceHealthy.get())
                .queueSize(embeddingQueueSize.get())
                .vectorDbUrl(vectorDbUrl)
                .collectionName(collectionName)
                .embeddingModel(embeddingModel)
                .embeddingDimension(embeddingDimension)
                .localEmbeddingEnabled(localEmbeddingEnabled)
                .build();
    }

    /**
     * 임베딩 요청 DTO
     */
    @lombok.Data
    @lombok.Builder
    @lombok.NoArgsConstructor
    @lombok.AllArgsConstructor
    public static class EmbeddingRequest {
        private String sessionId;
        private String messageId;
        private String content;
        private Object metadata;
    }

    /**
     * 서비스 상태 DTO
     */
    @lombok.Data
    @lombok.Builder
    public static class VectorServiceStatus {
        private boolean enabled;
        private boolean vectorDbHealthy;
        private boolean embeddingServiceHealthy;
        private long queueSize;
        private String vectorDbUrl;
        private String collectionName;
        private String embeddingModel;
        private int embeddingDimension;
        private boolean localEmbeddingEnabled;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/WebScraperService.java

```java
package com.newsinsight.collector.service;

import com.fasterxml.jackson.databind.ObjectMapper;
import com.newsinsight.collector.client.Crawl4aiClient;
import com.newsinsight.collector.entity.CollectedData;
import com.newsinsight.collector.entity.DataSource;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Mono;

import java.time.Duration;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;

@Service
@RequiredArgsConstructor
@Slf4j
public class WebScraperService {

    private final WebClient webClient;
    private final CollectedDataService collectedDataService;
    private final ObjectMapper objectMapper;
    private final Crawl4aiClient crawl4aiClient;

    @Value("${collector.crawler.enabled:true}")
    private boolean crawlerEnabled;

    /**
     * 공백을 정리하여 텍스트를 정규화
     */
    private String normalizeText(String text) {
        if (text == null || text.isBlank()) {
            return "";
        }
        return text.replaceAll("\\s+", " ").trim();
    }

    /**
     * 웹 페이지를 가져와 스크랩
     */
    public List<CollectedData> scrapeWebPage(DataSource source) {
        List<CollectedData> results = new ArrayList<>();

        try {
            log.info("Scraping web page: {}", source.getUrl());

            String title = null;
            String normalizedContent = null;
            String method = "jsoup";

            // Try Crawl4AI first if enabled
            if (crawlerEnabled) {
                Crawl4aiClient.CrawlResult r = crawl4aiClient.crawl(source.getUrl());
                if (r != null && r.getContent() != null && r.getContent().length() >= 100) {
                    title = r.getTitle();
                    normalizedContent = r.getContent();
                    method = "crawl4ai";
                } else {
                    log.debug("Crawl4AI returned no/short content for {}. Falling back to Jsoup.", source.getUrl());
                }
            }

            if (normalizedContent == null) {
                // WebClient로 HTML 가져오기
                String html = webClient.get()
                        .uri(source.getUrl())
                        .retrieve()
                        .bodyToMono(String.class)
                        .timeout(Duration.ofSeconds(30))
                        .onErrorResume(e -> {
                            log.error("Error fetching web page {}: {}", source.getUrl(), e.getMessage());
                            return Mono.empty();
                        })
                        .block();

                if (html == null || html.isBlank()) {
                    log.warn("Empty response from: {}", source.getUrl());
                    return results;
                }

                // Jsoup으로 HTML 파싱
                Document doc = Jsoup.parse(html);

                // script/style/nav/footer/aside 제거
                doc.select("script, style, nav, footer, aside").remove();

                // 본문 텍스트 추출
                String textContent = doc.body().text();
                normalizedContent = normalizeText(textContent);

                // 제목 추출 (fallback path)
                title = doc.title();
            }

            // 제목 보정
            if (title == null || title.isBlank()) {
                title = source.getName();
            }

            // 내용이 너무 짧으면 건너뜀
            if (normalizedContent == null || normalizedContent.length() < 100) {
                log.debug("Skipping page with too short content: {}", source.getUrl());
                return results;
            }

            // 콘텐츠 해시 계산
            String contentHash = collectedDataService.computeContentHash(
                    source.getUrl(), title, normalizedContent);

            // 중복 확인
            if (collectedDataService.isDuplicate(contentHash)) {
                log.debug("Duplicate page detected: {}", source.getUrl());
                return results;
            }

            // 메타데이터 구성
            Map<String, Object> metadata = Map.of(
                    "adapter", "web",
                    "source_name", source.getName(),
                    "scrape_method", method
            );

            // 메타데이터를 JSON 문자열로 변환
            String metadataJson;
            try {
                metadataJson = objectMapper.writeValueAsString(metadata);
            } catch (Exception e) {
                log.warn("Failed to serialize metadata to JSON: {}", e.getMessage());
                metadataJson = "{}";
            }

            // CollectedData 엔티티 생성
            CollectedData data = CollectedData.builder()
                    .sourceId(source.getId())
                    .title(title)
                    .content(normalizedContent)
                    .url(source.getUrl())
                    .publishedDate(null) // 웹 페이지는 게시일 정보가 없음
                    .contentHash(contentHash)
                    .metadataJson(metadataJson)
                    .processed(false)
                    .hasContent(true)
                    .duplicate(false)
                    .normalized(true)
                    .build();

            results.add(data);
            log.info("Successfully scraped web page: {} ({} chars, method={})", source.getName(), normalizedContent.length(), method);

        } catch (Exception e) {
            log.error("Error scraping web page {}: {}", source.getUrl(), e.getMessage(), e);
        }

        return results;
    }

    /**
     * CSS 셀렉터로 특정 콘텐츠 추출 (메타데이터에 제공된 경우)
     */
    public String extractWithSelector(Document doc, String cssSelector) {
        if (cssSelector == null || cssSelector.isBlank()) {
            return doc.body().text();
        }

        try {
            return doc.select(cssSelector).text();
        } catch (Exception e) {
            log.warn("CSS 셀렉터 사용 오류 {}: {}", cssSelector, e.getMessage());
            return doc.body().text();
        }
    }
}


```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/WorkspaceFileService.java

```java
package com.newsinsight.collector.service;

import com.newsinsight.collector.entity.workspace.WorkspaceFile;
import com.newsinsight.collector.repository.WorkspaceFileRepository;
import lombok.Builder;
import lombok.Data;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.core.io.Resource;
import org.springframework.core.io.UrlResource;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.PageRequest;
import org.springframework.data.domain.Pageable;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;
import org.springframework.util.StringUtils;
import org.springframework.web.multipart.MultipartFile;

import jakarta.annotation.PostConstruct;
import java.io.IOException;
import java.io.InputStream;
import java.net.MalformedURLException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.nio.file.StandardCopyOption;
import java.security.MessageDigest;
import java.security.NoSuchAlgorithmException;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import java.util.UUID;

/**
 * Service for managing workspace files.
 * Handles file upload, download, deletion and metadata management.
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class WorkspaceFileService {

    private final WorkspaceFileRepository fileRepository;

    @Value("${workspace.storage.path:/data/workspace}")
    private String storagePath;

    @Value("${workspace.storage.max-file-size:104857600}") // 100MB default
    private long maxFileSize;

    @Value("${workspace.storage.max-files-per-session:100}")
    private int maxFilesPerSession;

    @Value("${workspace.storage.session-file-ttl-hours:24}")
    private int sessionFileTtlHours;

    private Path rootLocation;

    @PostConstruct
    public void init() {
        this.rootLocation = Paths.get(storagePath);
        try {
            Files.createDirectories(rootLocation);
            log.info("Workspace storage initialized at: {}", rootLocation.toAbsolutePath());
        } catch (IOException e) {
            log.warn("Could not initialize workspace storage location at {}: {}. Workspace file features will be disabled.", 
                    rootLocation.toAbsolutePath(), e.getMessage());
            // Don't throw exception - allow service to start without workspace storage
            this.rootLocation = null;
        }
    }

    // ============================================
    // File Upload
    // ============================================

    /**
     * Upload a file for session-based user.
     */
    @Transactional
    public WorkspaceFile uploadFile(MultipartFile file, String sessionId, UploadRequest request) {
        return uploadFileInternal(file, sessionId, null, request);
    }

    /**
     * Upload a file for authenticated user.
     */
    @Transactional
    public WorkspaceFile uploadFileForUser(MultipartFile file, String userId, UploadRequest request) {
        return uploadFileInternal(file, null, userId, request);
    }

    /**
     * Internal upload logic.
     */
    private WorkspaceFile uploadFileInternal(MultipartFile file, String sessionId, String userId, UploadRequest request) {
        // Check if storage is available
        if (rootLocation == null) {
            throw new RuntimeException("Workspace storage is not available. Please check server configuration.");
        }
        
        // Validate file
        validateFile(file, sessionId, userId);

        String originalFilename = StringUtils.cleanPath(file.getOriginalFilename());
        String extension = getFileExtension(originalFilename);
        String storedName = generateStoredName(extension);
        String relativePath = generateRelativePath(sessionId, userId, storedName);
        Path targetPath = rootLocation.resolve(relativePath);

        try {
            // Create directories if needed
            Files.createDirectories(targetPath.getParent());

            // Calculate checksum
            String checksum = calculateChecksum(file.getInputStream());

            // Check for duplicate (same file already uploaded)
            Optional<WorkspaceFile> existing = fileRepository.findByChecksumAndOwner(checksum, sessionId, userId);
            if (existing.isPresent()) {
                log.info("Duplicate file detected, returning existing: {}", existing.get().getFileUuid());
                return existing.get();
            }

            // Save file to disk
            Files.copy(file.getInputStream(), targetPath, StandardCopyOption.REPLACE_EXISTING);
            log.info("File saved to: {}", targetPath);

            // Create entity
            WorkspaceFile workspaceFile = WorkspaceFile.builder()
                    .fileUuid(UUID.randomUUID().toString())
                    .sessionId(sessionId)
                    .userId(userId)
                    .projectId(request != null ? request.getProjectId() : null)
                    .originalName(originalFilename)
                    .storedName(storedName)
                    .extension(extension)
                    .mimeType(file.getContentType())
                    .fileSize(file.getSize())
                    .fileType(WorkspaceFile.determineFileType(extension))
                    .storageType(WorkspaceFile.StorageType.LOCAL)
                    .storagePath(relativePath)
                    .status(WorkspaceFile.FileStatus.ACTIVE)
                    .description(request != null ? request.getDescription() : null)
                    .checksum(checksum)
                    .downloadCount(0)
                    .expiresAt(sessionId != null ? LocalDateTime.now().plusHours(sessionFileTtlHours) : null)
                    .metadata(request != null ? request.getMetadata() : null)
                    .build();

            WorkspaceFile saved = fileRepository.save(workspaceFile);
            log.info("Workspace file created: id={}, uuid={}, name='{}', size={}", 
                    saved.getId(), saved.getFileUuid(), saved.getOriginalName(), saved.getHumanReadableSize());

            return saved;

        } catch (IOException e) {
            log.error("Failed to store file: {}", originalFilename, e);
            throw new RuntimeException("Failed to store file: " + originalFilename, e);
        }
    }

    // ============================================
    // File Download
    // ============================================

    /**
     * Get file for download.
     */
    @Transactional
    public FileDownloadResponse getFileForDownload(String fileUuid, String sessionId, String userId) {
        WorkspaceFile file = fileRepository.findActiveByFileUuid(fileUuid)
                .orElseThrow(() -> new IllegalArgumentException("File not found: " + fileUuid));

        // Check access
        if (!file.isAccessibleBy(sessionId, userId)) {
            throw new IllegalStateException("Access denied to file: " + fileUuid);
        }

        // Check expiration
        if (file.isExpired()) {
            throw new IllegalStateException("File has expired: " + fileUuid);
        }

        // Load file resource
        try {
            Path filePath = rootLocation.resolve(file.getStoragePath());
            Resource resource = new UrlResource(filePath.toUri());

            if (!resource.exists() || !resource.isReadable()) {
                throw new RuntimeException("Could not read file: " + fileUuid);
            }

            // Update download count
            fileRepository.incrementDownloadCount(file.getId(), LocalDateTime.now());

            return FileDownloadResponse.builder()
                    .resource(resource)
                    .filename(file.getOriginalName())
                    .contentType(file.getMimeType())
                    .fileSize(file.getFileSize())
                    .build();

        } catch (MalformedURLException e) {
            throw new RuntimeException("Could not read file: " + fileUuid, e);
        }
    }

    /**
     * Get file metadata.
     */
    public Optional<WorkspaceFile> getFile(String fileUuid) {
        return fileRepository.findActiveByFileUuid(fileUuid);
    }

    /**
     * Get file metadata with access check.
     */
    public Optional<WorkspaceFile> getFileWithAccess(String fileUuid, String sessionId, String userId) {
        return fileRepository.findActiveByFileUuid(fileUuid)
                .filter(f -> f.isAccessibleBy(sessionId, userId));
    }

    // ============================================
    // File Listing
    // ============================================

    /**
     * List files for session.
     */
    public Page<WorkspaceFile> listFilesForSession(String sessionId, int page, int size) {
        Pageable pageable = PageRequest.of(page, size);
        return fileRepository.findBySessionIdAndStatusOrderByCreatedAtDesc(
                sessionId, WorkspaceFile.FileStatus.ACTIVE, pageable);
    }

    /**
     * List files for user.
     */
    public Page<WorkspaceFile> listFilesForUser(String userId, int page, int size) {
        Pageable pageable = PageRequest.of(page, size);
        return fileRepository.findByUserIdAndStatusOrderByCreatedAtDesc(
                userId, WorkspaceFile.FileStatus.ACTIVE, pageable);
    }

    /**
     * List files for project.
     */
    public Page<WorkspaceFile> listFilesForProject(Long projectId, int page, int size) {
        Pageable pageable = PageRequest.of(page, size);
        return fileRepository.findByProjectIdAndStatusOrderByCreatedAtDesc(
                projectId, WorkspaceFile.FileStatus.ACTIVE, pageable);
    }

    /**
     * List files by type for session.
     */
    public Page<WorkspaceFile> listFilesByTypeForSession(String sessionId, WorkspaceFile.FileType fileType, int page, int size) {
        Pageable pageable = PageRequest.of(page, size);
        return fileRepository.findBySessionIdAndFileTypeAndStatus(
                sessionId, fileType, WorkspaceFile.FileStatus.ACTIVE, pageable);
    }

    /**
     * Search files for session.
     */
    public Page<WorkspaceFile> searchFilesForSession(String sessionId, String query, int page, int size) {
        Pageable pageable = PageRequest.of(page, size);
        return fileRepository.searchByNameForSession(sessionId, query, pageable);
    }

    /**
     * Search files for user.
     */
    public Page<WorkspaceFile> searchFilesForUser(String userId, String query, int page, int size) {
        Pageable pageable = PageRequest.of(page, size);
        return fileRepository.searchByNameForUser(userId, query, pageable);
    }

    // ============================================
    // File Deletion
    // ============================================

    /**
     * Delete file (soft delete).
     */
    @Transactional
    public void deleteFile(String fileUuid, String sessionId, String userId) {
        WorkspaceFile file = fileRepository.findActiveByFileUuid(fileUuid)
                .orElseThrow(() -> new IllegalArgumentException("File not found: " + fileUuid));

        // Check access
        if (!file.isAccessibleBy(sessionId, userId)) {
            throw new IllegalStateException("Access denied to file: " + fileUuid);
        }

        // Soft delete
        fileRepository.updateStatus(file.getId(), WorkspaceFile.FileStatus.DELETED, LocalDateTime.now());
        log.info("File marked as deleted: uuid={}, name='{}'", fileUuid, file.getOriginalName());
    }

    /**
     * Permanently delete file (hard delete).
     */
    @Transactional
    public void permanentlyDeleteFile(Long fileId) {
        WorkspaceFile file = fileRepository.findById(fileId)
                .orElseThrow(() -> new IllegalArgumentException("File not found: " + fileId));

        // Delete physical file
        try {
            Path filePath = rootLocation.resolve(file.getStoragePath());
            Files.deleteIfExists(filePath);
            log.info("Physical file deleted: {}", filePath);
        } catch (IOException e) {
            log.error("Failed to delete physical file: {}", file.getStoragePath(), e);
        }

        // Delete database record
        fileRepository.delete(file);
        log.info("File permanently deleted: id={}, name='{}'", fileId, file.getOriginalName());
    }

    /**
     * Delete all files for session.
     */
    @Transactional
    public void deleteAllFilesForSession(String sessionId) {
        fileRepository.markDeletedBySessionId(sessionId, LocalDateTime.now());
        log.info("All files marked as deleted for session: {}", sessionId);
    }

    // ============================================
    // File Migration (Session to User)
    // ============================================

    /**
     * Transfer session files to user (when anonymous user logs in).
     */
    @Transactional
    public int transferSessionFilesToUser(String sessionId, String userId) {
        long count = fileRepository.countBySessionIdAndStatus(sessionId, WorkspaceFile.FileStatus.ACTIVE);
        
        if (count > 0) {
            fileRepository.transferSessionFilesToUser(sessionId, userId, LocalDateTime.now());
            log.info("Transferred {} files from session {} to user {}", count, sessionId, userId);
        }
        
        return (int) count;
    }

    // ============================================
    // Cleanup
    // ============================================

    /**
     * Cleanup expired files.
     */
    @Transactional
    public int cleanupExpiredFiles() {
        List<WorkspaceFile> expired = fileRepository.findExpiredFiles(LocalDateTime.now());
        
        for (WorkspaceFile file : expired) {
            fileRepository.updateStatus(file.getId(), WorkspaceFile.FileStatus.PENDING_DELETE, LocalDateTime.now());
        }
        
        log.info("Marked {} expired files for deletion", expired.size());
        return expired.size();
    }

    /**
     * Cleanup old session files (orphaned anonymous files).
     */
    @Transactional
    public int cleanupOldSessionFiles(int olderThanHours) {
        LocalDateTime threshold = LocalDateTime.now().minusHours(olderThanHours);
        List<WorkspaceFile> oldFiles = fileRepository.findOldSessionFiles(threshold);
        
        for (WorkspaceFile file : oldFiles) {
            fileRepository.updateStatus(file.getId(), WorkspaceFile.FileStatus.PENDING_DELETE, LocalDateTime.now());
        }
        
        log.info("Marked {} old session files for deletion", oldFiles.size());
        return oldFiles.size();
    }

    /**
     * Permanently delete files marked for deletion.
     */
    @Transactional
    public int purgeDeletedFiles() {
        List<WorkspaceFile> pendingDelete = fileRepository.findByStatus(WorkspaceFile.FileStatus.PENDING_DELETE);
        List<WorkspaceFile> deleted = fileRepository.findByStatus(WorkspaceFile.FileStatus.DELETED);
        
        int count = 0;
        for (WorkspaceFile file : pendingDelete) {
            permanentlyDeleteFile(file.getId());
            count++;
        }
        for (WorkspaceFile file : deleted) {
            permanentlyDeleteFile(file.getId());
            count++;
        }
        
        log.info("Purged {} files permanently", count);
        return count;
    }

    // ============================================
    // Statistics
    // ============================================

    /**
     * Get storage statistics for session.
     */
    public StorageStats getStorageStatsForSession(String sessionId) {
        long fileCount = fileRepository.countBySessionIdAndStatus(sessionId, WorkspaceFile.FileStatus.ACTIVE);
        long totalSize = fileRepository.sumFileSizeBySessionId(sessionId);
        
        return StorageStats.builder()
                .fileCount(fileCount)
                .totalSize(totalSize)
                .maxFiles(maxFilesPerSession)
                .maxFileSize(maxFileSize)
                .build();
    }

    /**
     * Get storage statistics for user.
     */
    public StorageStats getStorageStatsForUser(String userId) {
        long fileCount = fileRepository.countByUserIdAndStatus(userId, WorkspaceFile.FileStatus.ACTIVE);
        long totalSize = fileRepository.sumFileSizeByUserId(userId);
        
        return StorageStats.builder()
                .fileCount(fileCount)
                .totalSize(totalSize)
                .maxFiles(-1) // No limit for users
                .maxFileSize(maxFileSize)
                .build();
    }

    // ============================================
    // Helper Methods
    // ============================================

    private void validateFile(MultipartFile file, String sessionId, String userId) {
        if (file == null || file.isEmpty()) {
            throw new IllegalArgumentException("File is empty");
        }

        if (file.getSize() > maxFileSize) {
            throw new IllegalArgumentException(
                    String.format("File size exceeds maximum allowed (%d bytes)", maxFileSize));
        }

        // Check file count limit for session
        if (sessionId != null) {
            long currentCount = fileRepository.countBySessionIdAndStatus(sessionId, WorkspaceFile.FileStatus.ACTIVE);
            if (currentCount >= maxFilesPerSession) {
                throw new IllegalStateException(
                        String.format("Maximum file limit reached (%d files)", maxFilesPerSession));
            }
        }
    }

    private String getFileExtension(String filename) {
        if (filename == null) return "";
        int dotIndex = filename.lastIndexOf('.');
        return dotIndex > 0 ? filename.substring(dotIndex + 1).toLowerCase() : "";
    }

    private String generateStoredName(String extension) {
        String uuid = UUID.randomUUID().toString().replace("-", "");
        return extension.isEmpty() ? uuid : uuid + "." + extension;
    }

    private String generateRelativePath(String sessionId, String userId, String storedName) {
        String datePath = LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyy/MM/dd"));
        String ownerPath = userId != null ? "users/" + userId : "sessions/" + sessionId;
        return ownerPath + "/" + datePath + "/" + storedName;
    }

    private String calculateChecksum(InputStream inputStream) {
        try {
            MessageDigest digest = MessageDigest.getInstance("SHA-256");
            byte[] buffer = new byte[8192];
            int bytesRead;
            
            inputStream.mark(Integer.MAX_VALUE);
            while ((bytesRead = inputStream.read(buffer)) != -1) {
                digest.update(buffer, 0, bytesRead);
            }
            inputStream.reset();
            
            byte[] hashBytes = digest.digest();
            StringBuilder sb = new StringBuilder();
            for (byte b : hashBytes) {
                sb.append(String.format("%02x", b));
            }
            return sb.toString();
        } catch (NoSuchAlgorithmException | IOException e) {
            log.warn("Failed to calculate checksum", e);
            return null;
        }
    }

    // ============================================
    // DTOs
    // ============================================

    @Data
    @Builder
    public static class UploadRequest {
        private Long projectId;
        private String description;
        private Map<String, Object> metadata;
    }

    @Data
    @Builder
    public static class FileDownloadResponse {
        private Resource resource;
        private String filename;
        private String contentType;
        private Long fileSize;
    }

    @Data
    @Builder
    public static class StorageStats {
        private long fileCount;
        private long totalSize;
        private int maxFiles;
        private long maxFileSize;
        
        public String getHumanReadableTotalSize() {
            if (totalSize < 1024) return totalSize + " B";
            if (totalSize < 1024 * 1024) return String.format("%.1f KB", totalSize / 1024.0);
            if (totalSize < 1024 * 1024 * 1024) return String.format("%.1f MB", totalSize / (1024.0 * 1024));
            return String.format("%.1f GB", totalSize / (1024.0 * 1024 * 1024));
        }
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/autocrawl/AutoCrawlDiscoveryService.java

```java
package com.newsinsight.collector.service.autocrawl;

import com.newsinsight.collector.entity.CollectedData;
import com.newsinsight.collector.entity.autocrawl.ContentType;
import com.newsinsight.collector.entity.autocrawl.CrawlTarget;
import com.newsinsight.collector.entity.autocrawl.CrawlTargetStatus;
import com.newsinsight.collector.entity.autocrawl.DiscoverySource;
import com.newsinsight.collector.repository.CrawlTargetRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;
import org.jsoup.nodes.Element;
import org.jsoup.select.Elements;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.net.URI;
import java.nio.charset.StandardCharsets;
import java.security.MessageDigest;
import java.security.NoSuchAlgorithmException;
import java.time.LocalDateTime;
import java.util.*;
import java.util.regex.Pattern;
import java.util.stream.Collectors;

/**
 * 자동 크롤링 대상 URL 발견 서비스.
 * 
 * 검색 결과, 기사 내 링크, 트렌딩 토픽 등에서 크롤링 대상 URL을 자동으로 발견합니다.
 * 발견된 URL은 CrawlTarget 엔티티로 저장되어 크롤링 큐에 추가됩니다.
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class AutoCrawlDiscoveryService {

    private final CrawlTargetRepository crawlTargetRepository;

    // ========================================
    // URL 패턴 매칭
    // ========================================
    
    /**
     * 뉴스 URL 패턴 (한국 주요 언론사)
     */
    private static final Pattern NEWS_URL_PATTERN = Pattern.compile(
            ".*(?:news\\.naver\\.com|news\\.daum\\.net|news\\.kakao\\.com|" +
            "n\\.news\\.naver\\.com|v\\.media\\.daum\\.net|" +
            "chosun\\.com|donga\\.com|joongang\\.co\\.kr|hani\\.co\\.kr|" +
            "khan\\.co\\.kr|yna\\.co\\.kr|yonhapnews\\.co\\.kr|" +
            "hankookilbo\\.com|mk\\.co\\.kr|mt\\.co\\.kr|sedaily\\.com|" +
            "etnews\\.com|zdnet\\.co\\.kr|inews24\\.com|itworld\\.co\\.kr|" +
            "bloter\\.net|techholic\\.co\\.kr|" +
            "reuters\\.com|apnews\\.com|bbc\\.com\\/news|cnn\\.com).*"
    );

    /**
     * 제외할 URL 패턴 (광고, 로그인, 정적 리소스 등)
     */
    private static final Pattern EXCLUDED_URL_PATTERN = Pattern.compile(
            ".*(?:login|logout|signin|signup|register|auth|oauth|" +
            "advertisement|ad\\.|ads\\.|banner|popup|tracking|" +
            "\\.css|\\.js|\\.jpg|\\.jpeg|\\.png|\\.gif|\\.ico|\\.svg|\\.webp|" +
            "\\.pdf|\\.doc|\\.xls|\\.ppt|\\.zip|\\.rar|" +
            "facebook\\.com|twitter\\.com|instagram\\.com|youtube\\.com|" +
            "play\\.google\\.com|apps\\.apple\\.com|" +
            "javascript:|mailto:|tel:|#).*",
            Pattern.CASE_INSENSITIVE
    );

    /**
     * 도메인별 콘텐츠 타입 매핑
     */
    private static final Map<Pattern, ContentType> DOMAIN_CONTENT_TYPE_MAP = Map.of(
            Pattern.compile(".*(?:news\\.naver|news\\.daum|chosun|donga|joongang|hani|khan|yna).*"), ContentType.NEWS,
            Pattern.compile(".*(?:blog\\.naver|tistory|brunch|velog|medium).*"), ContentType.BLOG,
            Pattern.compile(".*(?:reddit|dcinside|ruliweb|clien|ppomppu).*"), ContentType.FORUM,
            Pattern.compile(".*(?:twitter|x\\.com|facebook|instagram).*"), ContentType.SOCIAL,
            Pattern.compile(".*(?:\\.go\\.kr|\\.or\\.kr|\\.gov\\.|assembly).*"), ContentType.OFFICIAL,
            Pattern.compile(".*(?:arxiv|scholar\\.google|dbpia|riss|sciencedirect).*"), ContentType.ACADEMIC
    );

    // ========================================
    // 검색 결과에서 URL 발견
    // ========================================

    /**
     * 검색 쿼리와 검색 결과 HTML에서 URL 발견
     * 
     * @param query 검색 쿼리
     * @param htmlContent 검색 결과 HTML
     * @param baseUrl 검색 페이지 URL (상대 경로 해결용)
     * @return 발견된 CrawlTarget 목록
     */
    @Transactional
    public List<CrawlTarget> discoverFromSearchResult(String query, String htmlContent, String baseUrl) {
        log.info("Discovering URLs from search result for query: '{}'", query);
        
        List<String> extractedUrls = extractUrlsFromHtml(htmlContent, baseUrl);
        List<CrawlTarget> targets = new ArrayList<>();
        
        for (String url : extractedUrls) {
            if (!isValidCrawlUrl(url)) {
                continue;
            }
            
            CrawlTarget target = createOrUpdateTarget(
                    url,
                    DiscoverySource.SEARCH,
                    "search_query:" + query,
                    calculateSearchPriority(url, query),
                    query
            );
            
            if (target != null) {
                targets.add(target);
            }
        }
        
        log.info("Discovered {} URLs from search result for query: '{}'", targets.size(), query);
        return targets;
    }

    /**
     * 검색 결과 URL 목록에서 직접 발견
     */
    @Transactional
    public List<CrawlTarget> discoverFromSearchUrls(String query, List<String> urls) {
        log.info("Discovering from {} search result URLs for query: '{}'", urls.size(), query);
        
        List<CrawlTarget> targets = new ArrayList<>();
        
        for (String url : urls) {
            if (!isValidCrawlUrl(url)) {
                continue;
            }
            
            CrawlTarget target = createOrUpdateTarget(
                    url,
                    DiscoverySource.SEARCH,
                    "search_query:" + query,
                    calculateSearchPriority(url, query),
                    query
            );
            
            if (target != null) {
                targets.add(target);
            }
        }
        
        return targets;
    }

    // ========================================
    // 기사 내 링크에서 URL 발견
    // ========================================

    /**
     * 수집된 기사 콘텐츠에서 외부 링크 발견
     * 
     * @param collectedData 수집된 기사 데이터
     * @return 발견된 CrawlTarget 목록
     */
    @Transactional
    public List<CrawlTarget> discoverFromArticle(CollectedData collectedData) {
        if (collectedData.getContent() == null || collectedData.getContent().isBlank()) {
            return Collections.emptyList();
        }
        
        log.debug("Discovering URLs from article: id={}, url={}", 
                collectedData.getId(), collectedData.getUrl());
        
        List<String> extractedUrls = extractUrlsFromHtml(collectedData.getContent(), collectedData.getUrl());
        List<CrawlTarget> targets = new ArrayList<>();
        
        String context = "article_id:" + collectedData.getId();
        String keywords = collectedData.getTitle(); // 기사 제목을 키워드로 사용
        
        for (String url : extractedUrls) {
            if (!isValidCrawlUrl(url)) {
                continue;
            }
            
            // 같은 도메인 링크는 낮은 우선순위
            int priority = isSameDomain(url, collectedData.getUrl()) ? 30 : 50;
            
            CrawlTarget target = createOrUpdateTarget(
                    url,
                    DiscoverySource.ARTICLE_LINK,
                    context,
                    priority,
                    keywords
            );
            
            if (target != null) {
                targets.add(target);
            }
        }
        
        log.debug("Discovered {} URLs from article: id={}", targets.size(), collectedData.getId());
        return targets;
    }

    // ========================================
    // 트렌딩 토픽에서 URL 발견
    // ========================================

    /**
     * 트렌딩 키워드에서 URL 발견 (포털 실시간 검색어 등)
     */
    @Transactional
    public List<CrawlTarget> discoverFromTrendingTopic(String topic, List<String> relatedUrls) {
        log.info("Discovering from trending topic: '{}' with {} URLs", topic, relatedUrls.size());
        
        List<CrawlTarget> targets = new ArrayList<>();
        
        for (String url : relatedUrls) {
            if (!isValidCrawlUrl(url)) {
                continue;
            }
            
            // 트렌딩 토픽은 높은 우선순위
            CrawlTarget target = createOrUpdateTarget(
                    url,
                    DiscoverySource.TRENDING,
                    "trending_topic:" + topic,
                    80,
                    topic
            );
            
            if (target != null) {
                targets.add(target);
            }
        }
        
        return targets;
    }

    // ========================================
    // Deep Search 결과에서 URL 발견
    // ========================================

    /**
     * Deep Search 결과에서 URL 발견
     */
    @Transactional
    public List<CrawlTarget> discoverFromDeepSearch(String searchId, String query, List<String> urls) {
        log.info("Discovering from deep search: id={}, query='{}', urls={}", searchId, query, urls.size());
        
        List<CrawlTarget> targets = new ArrayList<>();
        
        for (String url : urls) {
            if (!isValidCrawlUrl(url)) {
                continue;
            }
            
            // Deep Search는 높은 우선순위
            CrawlTarget target = createOrUpdateTarget(
                    url,
                    DiscoverySource.DEEP_SEARCH,
                    "deep_search:" + searchId,
                    75,
                    query
            );
            
            if (target != null) {
                targets.add(target);
            }
        }
        
        return targets;
    }

    // ========================================
    // AI 추천 URL 발견
    // ========================================

    /**
     * AI가 추천한 URL 발견
     */
    @Transactional
    public List<CrawlTarget> discoverFromAiRecommendation(String context, List<String> urls, String keywords) {
        log.info("Discovering from AI recommendation: {} URLs", urls.size());
        
        List<CrawlTarget> targets = new ArrayList<>();
        
        for (String url : urls) {
            if (!isValidCrawlUrl(url)) {
                continue;
            }
            
            CrawlTarget target = createOrUpdateTarget(
                    url,
                    DiscoverySource.AI_RECOMMENDATION,
                    "ai_context:" + context,
                    70,
                    keywords
            );
            
            if (target != null) {
                targets.add(target);
            }
        }
        
        return targets;
    }

    // ========================================
    // 수동 URL 추가
    // ========================================

    /**
     * 수동으로 URL 추가
     */
    @Transactional
    public CrawlTarget addManualTarget(String url, String keywords, int priority) {
        if (!isValidCrawlUrl(url)) {
            throw new IllegalArgumentException("Invalid URL: " + url);
        }
        
        return createOrUpdateTarget(
                url,
                DiscoverySource.MANUAL,
                "manual_add:" + LocalDateTime.now(),
                Math.min(100, Math.max(0, priority)),
                keywords
        );
    }

    /**
     * 수동으로 여러 URL 추가
     */
    @Transactional
    public List<CrawlTarget> addManualTargets(List<String> urls, String keywords, int priority) {
        return urls.stream()
                .filter(this::isValidCrawlUrl)
                .map(url -> createOrUpdateTarget(
                        url,
                        DiscoverySource.MANUAL,
                        "manual_add:" + LocalDateTime.now(),
                        Math.min(100, Math.max(0, priority)),
                        keywords))
                .filter(Objects::nonNull)
                .toList();
    }

    // ========================================
    // 내부 유틸리티 메서드
    // ========================================

    /**
     * HTML에서 URL 추출
     */
    private List<String> extractUrlsFromHtml(String htmlContent, String baseUrl) {
        if (htmlContent == null || htmlContent.isBlank()) {
            return Collections.emptyList();
        }
        
        Set<String> urls = new LinkedHashSet<>();
        
        try {
            Document doc = Jsoup.parse(htmlContent, baseUrl != null ? baseUrl : "");
            
            // <a> 태그에서 href 추출
            Elements links = doc.select("a[href]");
            for (Element link : links) {
                String href = link.absUrl("href");
                if (!href.isBlank()) {
                    urls.add(normalizeUrl(href));
                }
            }
            
            // 추가로 텍스트에서 URL 패턴 추출
            String text = doc.text();
            Pattern urlPattern = Pattern.compile("https?://[\\w\\-._~:/?#\\[\\]@!$&'()*+,;=%]+");
            var matcher = urlPattern.matcher(text);
            while (matcher.find()) {
                String url = matcher.group();
                urls.add(normalizeUrl(url));
            }
            
        } catch (Exception e) {
            log.warn("Failed to parse HTML for URL extraction: {}", e.getMessage());
        }
        
        return new ArrayList<>(urls);
    }

    /**
     * URL 정규화
     */
    private String normalizeUrl(String url) {
        if (url == null) return null;
        
        // Fragment 제거 (#...)
        int fragmentIndex = url.indexOf('#');
        if (fragmentIndex > 0) {
            url = url.substring(0, fragmentIndex);
        }
        
        // 후행 슬래시 정규화
        if (url.endsWith("/")) {
            url = url.substring(0, url.length() - 1);
        }
        
        return url.trim();
    }

    /**
     * URL 해시 생성
     */
    private String computeUrlHash(String url) {
        try {
            MessageDigest digest = MessageDigest.getInstance("SHA-256");
            byte[] hash = digest.digest(url.getBytes(StandardCharsets.UTF_8));
            StringBuilder hexString = new StringBuilder();
            for (byte b : hash) {
                String hex = Integer.toHexString(0xff & b);
                if (hex.length() == 1) hexString.append('0');
                hexString.append(hex);
            }
            return hexString.toString();
        } catch (NoSuchAlgorithmException e) {
            throw new RuntimeException("SHA-256 not available", e);
        }
    }

    /**
     * 도메인 추출
     */
    private String extractDomain(String url) {
        try {
            URI uri = URI.create(url);
            return uri.getHost();
        } catch (Exception e) {
            return null;
        }
    }

    /**
     * 같은 도메인인지 확인
     */
    private boolean isSameDomain(String url1, String url2) {
        String domain1 = extractDomain(url1);
        String domain2 = extractDomain(url2);
        return domain1 != null && domain1.equals(domain2);
    }

    /**
     * 유효한 크롤링 대상 URL인지 확인
     */
    private boolean isValidCrawlUrl(String url) {
        if (url == null || url.isBlank()) {
            return false;
        }
        
        // HTTP/HTTPS만 허용
        if (!url.startsWith("http://") && !url.startsWith("https://")) {
            return false;
        }
        
        // 제외 패턴 체크
        if (EXCLUDED_URL_PATTERN.matcher(url).matches()) {
            return false;
        }
        
        return true;
    }

    /**
     * 콘텐츠 타입 추정
     */
    private ContentType inferContentType(String url) {
        for (var entry : DOMAIN_CONTENT_TYPE_MAP.entrySet()) {
            if (entry.getKey().matcher(url).matches()) {
                return entry.getValue();
            }
        }
        return ContentType.UNKNOWN;
    }

    /**
     * 검색 우선순위 계산
     */
    private int calculateSearchPriority(String url, String query) {
        int priority = 50; // 기본값
        
        // 뉴스 URL은 우선순위 상승
        if (NEWS_URL_PATTERN.matcher(url).matches()) {
            priority += 20;
        }
        
        // URL에 검색어가 포함되면 우선순위 상승
        String lowerUrl = url.toLowerCase();
        if (query != null && !query.isBlank()) {
            for (String keyword : query.toLowerCase().split("\\s+")) {
                if (keyword.length() >= 2 && lowerUrl.contains(keyword)) {
                    priority += 5;
                }
            }
        }
        
        return Math.min(100, priority);
    }

    /**
     * CrawlTarget 생성 또는 업데이트
     */
    private CrawlTarget createOrUpdateTarget(
            String url,
            DiscoverySource source,
            String context,
            int priority,
            String keywords) {
        
        String urlHash = computeUrlHash(url);
        
        Optional<CrawlTarget> existingOpt = crawlTargetRepository.findByUrlHash(urlHash);
        
        if (existingOpt.isPresent()) {
            CrawlTarget existing = existingOpt.get();
            
            // 이미 완료된 대상은 업데이트하지 않음
            if (existing.getStatus() == CrawlTargetStatus.COMPLETED) {
                return null;
            }
            
            // 우선순위가 더 높으면 업데이트
            if (priority > existing.getPriority()) {
                existing.setPriority(priority);
                
                // 키워드 병합
                if (keywords != null && !keywords.isBlank()) {
                    String existingKeywords = existing.getRelatedKeywords();
                    if (existingKeywords == null || existingKeywords.isBlank()) {
                        existing.setRelatedKeywords(keywords);
                    } else if (!existingKeywords.contains(keywords)) {
                        existing.setRelatedKeywords(existingKeywords + ", " + keywords);
                    }
                }
                
                return crawlTargetRepository.save(existing);
            }
            
            return null; // 변경 없음
        }
        
        // 새 대상 생성
        CrawlTarget target = CrawlTarget.builder()
                .url(url)
                .urlHash(urlHash)
                .discoverySource(source)
                .discoveryContext(context)
                .priority(priority)
                .status(CrawlTargetStatus.PENDING)
                .domain(extractDomain(url))
                .expectedContentType(inferContentType(url))
                .relatedKeywords(keywords)
                .build();
        
        return crawlTargetRepository.save(target);
    }

    // ========================================
    // 통계/조회 메서드
    // ========================================

    /**
     * 대기 중인 대상 수 조회
     */
    public long countPending() {
        return crawlTargetRepository.countByStatus(CrawlTargetStatus.PENDING);
    }

    /**
     * 발견 출처별 통계 조회
     */
    public Map<DiscoverySource, Long> getDiscoveryStats() {
        List<Object[]> stats = crawlTargetRepository.getDiscoveryStatsSince(
                LocalDateTime.now().minusDays(7));
        return stats.stream()
                .collect(Collectors.toMap(
                        row -> (DiscoverySource) row[0],
                        row -> (Long) row[1]
                ));
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/autocrawl/AutoCrawlIntegrationService.java

```java
package com.newsinsight.collector.service.autocrawl;

import com.newsinsight.collector.entity.CollectedData;
import com.newsinsight.collector.entity.autocrawl.CrawlTarget;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.scheduling.annotation.Async;
import org.springframework.stereotype.Service;

import java.util.List;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
import java.util.stream.Collectors;

/**
 * 자동 크롤링 통합 서비스.
 * 
 * 기존 시스템의 이벤트를 수신하여 자동으로 URL을 발견합니다:
 * - 검색 이벤트: 검색 결과에서 URL 발견
 * - 기사 수집 이벤트: 기사 내 링크에서 URL 발견
 * - Deep Search 이벤트: 심층 검색 결과에서 URL 발견
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class AutoCrawlIntegrationService {

    private final AutoCrawlDiscoveryService discoveryService;
    private final CrawlQueueService queueService;

    @Value("${autocrawl.enabled:true}")
    private boolean autoCrawlEnabled;

    @Value("${autocrawl.discover-from-search:true}")
    private boolean discoverFromSearch;

    @Value("${autocrawl.discover-from-articles:true}")
    private boolean discoverFromArticles;

    @Value("${autocrawl.discover-from-deep-search:true}")
    private boolean discoverFromDeepSearch;

    @Value("${autocrawl.min-content-length:200}")
    private int minContentLength;

    // URL 추출 패턴
    private static final Pattern URL_PATTERN = Pattern.compile(
            "https?://[\\w\\-._~:/?#\\[\\]@!$&'()*+,;=%]+",
            Pattern.CASE_INSENSITIVE
    );

    // ========================================
    // 검색 결과에서 URL 발견
    // ========================================

    /**
     * 검색 완료 시 URL 발견
     * UnifiedSearchService의 검색 완료 이벤트에서 호출
     */
    @Async
    public void onSearchCompleted(String query, List<String> resultUrls) {
        if (!autoCrawlEnabled || !discoverFromSearch) {
            return;
        }

        if (resultUrls == null || resultUrls.isEmpty()) {
            return;
        }

        try {
            log.debug("Discovering URLs from search: query='{}', urlCount={}", query, resultUrls.size());
            List<CrawlTarget> targets = discoveryService.discoverFromSearchUrls(query, resultUrls);
            
            if (!targets.isEmpty()) {
                log.info("AutoCrawl: Discovered {} URLs from search query '{}'", targets.size(), query);
            }
        } catch (Exception e) {
            log.warn("Failed to discover URLs from search: query='{}', error={}", query, e.getMessage());
        }
    }

    /**
     * 검색 결과 HTML에서 URL 발견 (더 상세한 발견)
     */
    @Async
    public void onSearchHtmlReceived(String query, String htmlContent, String baseUrl) {
        if (!autoCrawlEnabled || !discoverFromSearch) {
            return;
        }

        if (htmlContent == null || htmlContent.length() < minContentLength) {
            return;
        }

        try {
            List<CrawlTarget> targets = discoveryService.discoverFromSearchResult(query, htmlContent, baseUrl);
            
            if (!targets.isEmpty()) {
                log.info("AutoCrawl: Discovered {} URLs from search HTML for query '{}'", targets.size(), query);
            }
        } catch (Exception e) {
            log.warn("Failed to discover URLs from search HTML: query='{}', error={}", query, e.getMessage());
        }
    }

    // ========================================
    // 수집된 기사에서 URL 발견
    // ========================================

    /**
     * 기사 수집 완료 시 내부 링크 발견
     * CrawlResultConsumerService에서 호출 가능
     */
    @Async
    public void onArticleCollected(CollectedData article) {
        if (!autoCrawlEnabled || !discoverFromArticles) {
            return;
        }

        if (article == null || article.getContent() == null || 
            article.getContent().length() < minContentLength) {
            return;
        }

        try {
            List<CrawlTarget> targets = discoveryService.discoverFromArticle(article);
            
            if (!targets.isEmpty()) {
                log.debug("AutoCrawl: Discovered {} URLs from article id={}", targets.size(), article.getId());
            }
        } catch (Exception e) {
            log.warn("Failed to discover URLs from article: id={}, error={}", 
                    article.getId(), e.getMessage());
        }
    }

    /**
     * 기사 일괄 수집 완료 시 URL 발견
     */
    @Async
    public void onArticlesBatchCollected(List<CollectedData> articles) {
        if (!autoCrawlEnabled || !discoverFromArticles) {
            return;
        }

        if (articles == null || articles.isEmpty()) {
            return;
        }

        int totalDiscovered = 0;
        for (CollectedData article : articles) {
            try {
                if (article.getContent() != null && article.getContent().length() >= minContentLength) {
                    List<CrawlTarget> targets = discoveryService.discoverFromArticle(article);
                    totalDiscovered += targets.size();
                }
            } catch (Exception e) {
                log.warn("Failed to discover URLs from article: id={}, error={}", 
                        article.getId(), e.getMessage());
            }
        }

        if (totalDiscovered > 0) {
            log.info("AutoCrawl: Discovered {} URLs from {} articles", totalDiscovered, articles.size());
        }
    }

    // ========================================
    // Deep Search에서 URL 발견
    // ========================================

    /**
     * Deep Search 완료 시 URL 발견
     */
    @Async
    public void onDeepSearchCompleted(String searchId, String query, List<String> urls) {
        if (!autoCrawlEnabled || !discoverFromDeepSearch) {
            return;
        }

        if (urls == null || urls.isEmpty()) {
            return;
        }

        try {
            List<CrawlTarget> targets = discoveryService.discoverFromDeepSearch(searchId, query, urls);
            
            if (!targets.isEmpty()) {
                log.info("AutoCrawl: Discovered {} URLs from deep search id={}", targets.size(), searchId);
            }
        } catch (Exception e) {
            log.warn("Failed to discover URLs from deep search: id={}, error={}", searchId, e.getMessage());
        }
    }

    /**
     * AI 분석 결과에서 URL 추출 및 발견
     */
    @Async
    public void onAiAnalysisCompleted(String context, String aiResponse, String keywords) {
        if (!autoCrawlEnabled) {
            return;
        }

        if (aiResponse == null || aiResponse.isBlank()) {
            return;
        }

        try {
            // AI 응답에서 URL 추출
            List<String> extractedUrls = extractUrlsFromText(aiResponse);
            
            if (!extractedUrls.isEmpty()) {
                List<CrawlTarget> targets = discoveryService.discoverFromAiRecommendation(
                        context, extractedUrls, keywords);
                
                if (!targets.isEmpty()) {
                    log.info("AutoCrawl: Discovered {} URLs from AI analysis", targets.size());
                }
            }
        } catch (Exception e) {
            log.warn("Failed to discover URLs from AI analysis: error={}", e.getMessage());
        }
    }

    // ========================================
    // 트렌딩 토픽에서 URL 발견
    // ========================================

    /**
     * 트렌딩 토픽 감지 시 URL 발견
     */
    @Async
    public void onTrendingTopicDetected(String topic, List<String> relatedUrls) {
        if (!autoCrawlEnabled) {
            return;
        }

        if (relatedUrls == null || relatedUrls.isEmpty()) {
            return;
        }

        try {
            List<CrawlTarget> targets = discoveryService.discoverFromTrendingTopic(topic, relatedUrls);
            
            if (!targets.isEmpty()) {
                log.info("AutoCrawl: Discovered {} URLs from trending topic '{}'", targets.size(), topic);
                
                // 트렌딩 토픽은 높은 우선순위로 즉시 처리 트리거
                queueService.prioritizeKeyword(topic, 30);
            }
        } catch (Exception e) {
            log.warn("Failed to discover URLs from trending topic: topic='{}', error={}", 
                    topic, e.getMessage());
        }
    }

    // ========================================
    // 크롤링 완료 콜백 처리
    // ========================================

    /**
     * 크롤링 완료 시 결과 처리
     */
    public void onCrawlCompleted(String url, Long collectedDataId) {
        if (!autoCrawlEnabled) {
            return;
        }

        try {
            queueService.handleCrawlCompleteByUrl(url, collectedDataId);
        } catch (Exception e) {
            log.warn("Failed to handle crawl completion: url={}, error={}", url, e.getMessage());
        }
    }

    /**
     * 크롤링 실패 시 결과 처리
     */
    public void onCrawlFailed(String url, String errorMessage) {
        if (!autoCrawlEnabled) {
            return;
        }

        try {
            queueService.handleCrawlFailedByUrl(url, errorMessage);
        } catch (Exception e) {
            log.warn("Failed to handle crawl failure: url={}, error={}", url, e.getMessage());
        }
    }

    // ========================================
    // 유틸리티
    // ========================================

    /**
     * 텍스트에서 URL 추출
     */
    private List<String> extractUrlsFromText(String text) {
        Matcher matcher = URL_PATTERN.matcher(text);
        return matcher.results()
                .map(m -> m.group())
                .distinct()
                .collect(Collectors.toList());
    }

    /**
     * AutoCrawl 활성화 여부 확인
     */
    public boolean isEnabled() {
        return autoCrawlEnabled;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/autocrawl/CrawlQueueService.java

```java
package com.newsinsight.collector.service.autocrawl;

import com.newsinsight.collector.dto.BrowserTaskMessage;
import com.newsinsight.collector.entity.BrowserAgentPolicy;
import com.newsinsight.collector.entity.autocrawl.CrawlTarget;
import com.newsinsight.collector.entity.autocrawl.CrawlTargetStatus;
import com.newsinsight.collector.entity.autocrawl.ContentType;
import com.newsinsight.collector.repository.CrawlTargetRepository;
import lombok.Builder;
import lombok.Data;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.time.LocalDateTime;
import java.util.*;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.stream.Collectors;

/**
 * 크롤링 큐 관리 서비스.
 * 
 * CrawlTarget을 우선순위에 따라 관리하고 autonomous-crawler-service로 작업을 분배합니다.
 * 도메인별 rate limiting, 동시성 제어, 실패 처리를 담당합니다.
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class CrawlQueueService {

    private final CrawlTargetRepository crawlTargetRepository;
    private final KafkaTemplate<String, BrowserTaskMessage> browserTaskKafkaTemplate;

    @Value("${collector.crawl.topic.browser-task:newsinsight.crawl.browser.tasks}")
    private String browserTaskTopic;

    @Value("${collector.browser-agent.callback-base-url:http://localhost:8081}")
    private String browserAgentCallbackBaseUrl;

    @Value("${collector.browser-agent.callback-token:}")
    private String browserAgentCallbackToken;

    @Value("${autocrawl.max-concurrent-per-domain:3}")
    private int maxConcurrentPerDomain;

    @Value("${autocrawl.batch-size:10}")
    private int defaultBatchSize;

    @Value("${autocrawl.stuck-timeout-minutes:30}")
    private int stuckTimeoutMinutes;

    /**
     * 도메인별 진행 중인 크롤링 수 추적
     */
    private final ConcurrentHashMap<String, AtomicInteger> domainConcurrencyMap = new ConcurrentHashMap<>();

    /**
     * 큐 통계용 카운터
     */
    private final AtomicInteger totalDispatched = new AtomicInteger(0);
    private final AtomicInteger totalCompleted = new AtomicInteger(0);
    private final AtomicInteger totalFailed = new AtomicInteger(0);

    // ========================================
    // 큐 처리 메서드
    // ========================================

    /**
     * 대기 중인 대상을 처리하고 크롤러로 분배
     * 
     * @param batchSize 한 번에 처리할 대상 수
     * @return 분배된 대상 수
     */
    @Transactional
    public int processQueue(int batchSize) {
        log.debug("Processing crawl queue with batch size: {}", batchSize);
        
        // 먼저 멈춘 작업 복구
        recoverStuckTargets();
        
        // 대기 중인 대상 조회 (우선순위 순)
        List<CrawlTarget> pendingTargets = crawlTargetRepository.findReadyToCrawl(batchSize);
        
        if (pendingTargets.isEmpty()) {
            log.debug("No pending targets in queue");
            return 0;
        }
        
        int dispatched = 0;
        
        for (CrawlTarget target : pendingTargets) {
            // 도메인별 동시성 체크
            if (!canDispatchForDomain(target.getDomain())) {
                log.debug("Skipping target due to domain concurrency limit: domain={}, url={}",
                        target.getDomain(), target.getUrl());
                continue;
            }
            
            try {
                dispatchTarget(target);
                dispatched++;
            } catch (Exception e) {
                log.error("Failed to dispatch target: id={}, url={}, error={}",
                        target.getId(), target.getUrl(), e.getMessage());
                target.markFailed("Dispatch error: " + e.getMessage());
                crawlTargetRepository.save(target);
            }
        }
        
        log.info("Processed queue: dispatched {} of {} pending targets", dispatched, pendingTargets.size());
        return dispatched;
    }

    /**
     * 기본 배치 사이즈로 큐 처리
     */
    public int processQueue() {
        return processQueue(defaultBatchSize);
    }

    /**
     * 단일 대상 즉시 분배
     */
    @Transactional
    public boolean dispatchSingle(Long targetId) {
        Optional<CrawlTarget> targetOpt = crawlTargetRepository.findById(targetId);
        if (targetOpt.isEmpty()) {
            log.warn("Target not found: id={}", targetId);
            return false;
        }
        
        CrawlTarget target = targetOpt.get();
        if (target.getStatus() != CrawlTargetStatus.PENDING) {
            log.warn("Target is not in PENDING status: id={}, status={}", targetId, target.getStatus());
            return false;
        }
        
        try {
            dispatchTarget(target);
            return true;
        } catch (Exception e) {
            log.error("Failed to dispatch target: id={}", targetId, e);
            return false;
        }
    }

    /**
     * 특정 키워드 관련 대상 우선 처리
     */
    @Transactional
    public int prioritizeKeyword(String keyword, int boostAmount) {
        List<CrawlTarget> targets = crawlTargetRepository.findByRelatedKeywordsContaining(keyword);
        int boosted = 0;
        
        for (CrawlTarget target : targets) {
            if (target.getStatus() == CrawlTargetStatus.PENDING) {
                target.boostPriority(boostAmount);
                crawlTargetRepository.save(target);
                boosted++;
            }
        }
        
        log.info("Boosted priority for {} targets with keyword: '{}'", boosted, keyword);
        return boosted;
    }

    // ========================================
    // 크롤링 결과 처리
    // ========================================

    /**
     * 크롤링 완료 처리
     * autonomous-crawler-service의 콜백에서 호출됨
     */
    @Transactional
    public void handleCrawlComplete(String urlHash, Long collectedDataId) {
        Optional<CrawlTarget> targetOpt = crawlTargetRepository.findByUrlHash(urlHash);
        if (targetOpt.isEmpty()) {
            log.debug("Target not found for completion: urlHash={}", urlHash);
            return;
        }
        
        CrawlTarget target = targetOpt.get();
        target.markCompleted(collectedDataId);
        crawlTargetRepository.save(target);
        
        // 도메인 동시성 카운터 감소
        decrementDomainConcurrency(target.getDomain());
        
        totalCompleted.incrementAndGet();
        log.info("Crawl completed: id={}, url={}, collectedDataId={}",
                target.getId(), target.getUrl(), collectedDataId);
    }

    /**
     * 크롤링 실패 처리
     */
    @Transactional
    public void handleCrawlFailed(String urlHash, String errorMessage) {
        Optional<CrawlTarget> targetOpt = crawlTargetRepository.findByUrlHash(urlHash);
        if (targetOpt.isEmpty()) {
            log.debug("Target not found for failure: urlHash={}", urlHash);
            return;
        }
        
        CrawlTarget target = targetOpt.get();
        target.markFailed(errorMessage);
        crawlTargetRepository.save(target);
        
        // 도메인 동시성 카운터 감소
        decrementDomainConcurrency(target.getDomain());
        
        totalFailed.incrementAndGet();
        log.info("Crawl failed: id={}, url={}, error={}, retryCount={}",
                target.getId(), target.getUrl(), errorMessage, target.getRetryCount());
    }

    /**
     * URL로 완료/실패 처리 (URL 해시 계산 필요)
     */
    @Transactional
    public void handleCrawlCompleteByUrl(String url, Long collectedDataId) {
        String urlHash = computeUrlHash(url);
        handleCrawlComplete(urlHash, collectedDataId);
    }

    @Transactional
    public void handleCrawlFailedByUrl(String url, String errorMessage) {
        String urlHash = computeUrlHash(url);
        handleCrawlFailed(urlHash, errorMessage);
    }

    // ========================================
    // 큐 관리 메서드
    // ========================================

    /**
     * 멈춘 작업 복구 (IN_PROGRESS 상태로 오래 방치된 경우)
     */
    @Transactional
    public int recoverStuckTargets() {
        LocalDateTime timeout = LocalDateTime.now().minusMinutes(stuckTimeoutMinutes);
        int recovered = crawlTargetRepository.recoverStuckTargets(timeout);
        
        if (recovered > 0) {
            log.warn("Recovered {} stuck targets (timeout: {} minutes)", recovered, stuckTimeoutMinutes);
        }
        
        return recovered;
    }

    /**
     * 오래된 완료/실패 대상 정리
     */
    @Transactional
    public int cleanupOldTargets(int daysOld) {
        LocalDateTime cutoff = LocalDateTime.now().minusDays(daysOld);
        int deleted = crawlTargetRepository.deleteOldTargets(
                List.of(CrawlTargetStatus.COMPLETED, CrawlTargetStatus.FAILED, CrawlTargetStatus.SKIPPED),
                cutoff);
        
        log.info("Cleaned up {} old targets (older than {} days)", deleted, daysOld);
        return deleted;
    }

    /**
     * 오래 대기 중인 대상 만료 처리
     */
    @Transactional
    public int expireOldPendingTargets(int daysOld) {
        LocalDateTime cutoff = LocalDateTime.now().minusDays(daysOld);
        int expired = crawlTargetRepository.expireOldPendingTargets(cutoff);
        
        if (expired > 0) {
            log.info("Expired {} old pending targets (older than {} days)", expired, daysOld);
        }
        
        return expired;
    }

    /**
     * 대상 상태 강제 변경 (관리용)
     */
    @Transactional
    public boolean updateTargetStatus(Long targetId, CrawlTargetStatus newStatus, String reason) {
        Optional<CrawlTarget> targetOpt = crawlTargetRepository.findById(targetId);
        if (targetOpt.isEmpty()) {
            return false;
        }
        
        CrawlTarget target = targetOpt.get();
        CrawlTargetStatus oldStatus = target.getStatus();
        
        target.setStatus(newStatus);
        if (reason != null) {
            target.setLastError(reason);
        }
        
        crawlTargetRepository.save(target);
        log.info("Updated target status: id={}, {} -> {}, reason={}",
                targetId, oldStatus, newStatus, reason);
        
        return true;
    }

    // ========================================
    // 통계 조회
    // ========================================

    /**
     * 큐 상태 통계 조회
     */
    public QueueStats getQueueStats() {
        return QueueStats.builder()
                .pendingCount(crawlTargetRepository.countByStatus(CrawlTargetStatus.PENDING))
                .inProgressCount(crawlTargetRepository.countByStatus(CrawlTargetStatus.IN_PROGRESS))
                .completedCount(crawlTargetRepository.countByStatus(CrawlTargetStatus.COMPLETED))
                .failedCount(crawlTargetRepository.countByStatus(CrawlTargetStatus.FAILED))
                .skippedCount(crawlTargetRepository.countByStatus(CrawlTargetStatus.SKIPPED))
                .totalDispatched(totalDispatched.get())
                .totalCompleted(totalCompleted.get())
                .totalFailed(totalFailed.get())
                .domainConcurrency(getDomainConcurrencySnapshot())
                .build();
    }

    /**
     * 도메인별 대기 중 대상 수 조회
     */
    public Map<String, Long> getPendingCountByDomain() {
        List<Object[]> results = crawlTargetRepository.countPendingByDomain();
        return results.stream()
                .limit(20) // 상위 20개만
                .collect(Collectors.toMap(
                        row -> (String) row[0],
                        row -> (Long) row[1],
                        (a, b) -> a,
                        LinkedHashMap::new
                ));
    }

    @Data
    @Builder
    public static class QueueStats {
        private long pendingCount;
        private long inProgressCount;
        private long completedCount;
        private long failedCount;
        private long skippedCount;
        private int totalDispatched;
        private int totalCompleted;
        private int totalFailed;
        private Map<String, Integer> domainConcurrency;
    }

    // ========================================
    // 내부 헬퍼 메서드
    // ========================================

    /**
     * 대상을 크롤러로 분배
     */
    private void dispatchTarget(CrawlTarget target) {
        // 상태 변경
        target.markInProgress();
        crawlTargetRepository.save(target);
        
        // 도메인 동시성 카운터 증가
        incrementDomainConcurrency(target.getDomain());
        
        // 콜백 URL 생성
        String callbackUrl = browserAgentCallbackBaseUrl.endsWith("/")
                ? browserAgentCallbackBaseUrl + "api/v1/autocrawl/callback"
                : browserAgentCallbackBaseUrl + "/api/v1/autocrawl/callback";
        
        // 정책 결정
        BrowserAgentPolicy policy = determineCrawlPolicy(target);
        
        // Kafka 메시지 생성
        BrowserTaskMessage task = BrowserTaskMessage.builder()
                .jobId(target.getId())
                .sourceId(-1L) // 동적 발견 대상은 sourceId가 없음
                .sourceName("AutoCrawl")
                .seedUrl(target.getUrl())
                .maxDepth(1) // 발견된 단일 페이지만 크롤링
                .maxPages(1)
                .budgetSeconds(60)
                .policy(policy.getValue())
                .focusKeywords(target.getRelatedKeywords())
                .captureScreenshots(false)
                .extractStructured(true)
                .callbackUrl(callbackUrl)
                .callbackToken(browserAgentCallbackToken)
                .metadata(Map.of(
                        "targetId", target.getId().toString(),
                        "urlHash", target.getUrlHash(),
                        "discoverySource", target.getDiscoverySource().name(),
                        "priority", target.getPriority().toString()
                ))
                .createdAt(LocalDateTime.now())
                .build();
        
        // Kafka로 발행
        browserTaskKafkaTemplate.send(browserTaskTopic, target.getId().toString(), task);
        
        totalDispatched.incrementAndGet();
        log.info("Dispatched crawl target: id={}, url={}, policy={}, priority={}",
                target.getId(), target.getUrl(), policy, target.getPriority());
    }

    /**
     * 콘텐츠 타입에 따른 크롤링 정책 결정
     */
    private BrowserAgentPolicy determineCrawlPolicy(CrawlTarget target) {
        ContentType contentType = target.getExpectedContentType();
        
        return switch (contentType) {
            case NEWS -> BrowserAgentPolicy.NEWS_ONLY;
            case BLOG, FORUM -> BrowserAgentPolicy.FOCUSED_TOPIC;
            case OFFICIAL, ACADEMIC -> BrowserAgentPolicy.SINGLE_PAGE;
            default -> BrowserAgentPolicy.SINGLE_PAGE;
        };
    }

    /**
     * 도메인별 동시성 체크
     */
    private boolean canDispatchForDomain(String domain) {
        if (domain == null) return true;
        
        AtomicInteger count = domainConcurrencyMap.get(domain);
        if (count == null) return true;
        
        return count.get() < maxConcurrentPerDomain;
    }

    private void incrementDomainConcurrency(String domain) {
        if (domain == null) return;
        domainConcurrencyMap.computeIfAbsent(domain, k -> new AtomicInteger(0))
                .incrementAndGet();
    }

    private void decrementDomainConcurrency(String domain) {
        if (domain == null) return;
        AtomicInteger count = domainConcurrencyMap.get(domain);
        if (count != null) {
            int newValue = count.decrementAndGet();
            if (newValue <= 0) {
                domainConcurrencyMap.remove(domain);
            }
        }
    }

    private Map<String, Integer> getDomainConcurrencySnapshot() {
        return domainConcurrencyMap.entrySet().stream()
                .filter(e -> e.getValue().get() > 0)
                .collect(Collectors.toMap(
                        Map.Entry::getKey,
                        e -> e.getValue().get()
                ));
    }

    /**
     * URL 해시 생성 (AutoCrawlDiscoveryService와 동일)
     */
    private String computeUrlHash(String url) {
        try {
            java.security.MessageDigest digest = java.security.MessageDigest.getInstance("SHA-256");
            byte[] hash = digest.digest(url.getBytes(java.nio.charset.StandardCharsets.UTF_8));
            StringBuilder hexString = new StringBuilder();
            for (byte b : hash) {
                String hex = Integer.toHexString(0xff & b);
                if (hex.length() == 1) hexString.append('0');
                hexString.append(hex);
            }
            return hexString.toString();
        } catch (Exception e) {
            throw new RuntimeException("SHA-256 not available", e);
        }
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/factcheck/CORESource.java

```java
package com.newsinsight.collector.service.factcheck;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.newsinsight.collector.config.TrustScoreConfig;
import com.newsinsight.collector.service.FactVerificationService.SourceEvidence;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.MediaType;
import org.springframework.stereotype.Component;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Flux;

import java.net.URLEncoder;
import java.nio.charset.StandardCharsets;
import java.time.Duration;
import java.util.ArrayList;
import java.util.List;

/**
 * CORE API를 통한 오픈 액세스 학술 논문 검색
 * 
 * CORE(COnnecting REpositories)는 세계 최대의 오픈 액세스 연구 논문 수집 서비스로,
 * 2억 개 이상의 학술 자료를 무료로 검색할 수 있습니다.
 * 
 * API 문서: https://core.ac.uk/documentation/api
 * 
 * 특징:
 * - 오픈 액세스 전문 (전문 텍스트 접근 가능)
 * - 무료 API 키 제공
 * - 다양한 기관/저장소의 논문 통합 검색
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class CORESource implements FactCheckSource {

    private final WebClient webClient;
    private final ObjectMapper objectMapper;
    private final TrustScoreConfig trustScoreConfig;

    @Value("${collector.fact-check.core.enabled:true}")
    private boolean enabled;

    @Value("${collector.fact-check.core.api-key:}")
    private String apiKey;

    @Value("${collector.fact-check.timeout-seconds:15}")
    private int timeoutSeconds;

    private static final String API_BASE = "https://api.core.ac.uk/v3/search/works";

    @Override
    public String getSourceId() {
        return "core";
    }

    @Override
    public String getSourceName() {
        return "CORE (오픈 액세스)";
    }

    @Override
    public double getTrustScore() {
        try {
            return trustScoreConfig.getFactCheck().getOpenalex();
        } catch (Exception e) {
            return 0.80; // 오픈 액세스 기본 신뢰도
        }
    }

    @Override
    public SourceType getSourceType() {
        return SourceType.ACADEMIC;
    }

    @Override
    public boolean isAvailable() {
        // API 키가 있어야 사용 가능
        return enabled && apiKey != null && !apiKey.isBlank();
    }

    @Override
    public Flux<SourceEvidence> fetchEvidence(String topic, String language) {
        if (!isAvailable()) {
            return Flux.empty();
        }

        return Flux.defer(() -> {
            try {
                String encodedQuery = URLEncoder.encode(topic, StandardCharsets.UTF_8);
                String url = String.format("%s?q=%s&limit=5", API_BASE, encodedQuery);

                log.debug("Fetching CORE evidence for topic: {}", topic);

                String response = webClient.get()
                        .uri(url)
                        .accept(MediaType.APPLICATION_JSON)
                        .header("Authorization", "Bearer " + apiKey)
                        .retrieve()
                        .bodyToMono(String.class)
                        .timeout(Duration.ofSeconds(timeoutSeconds))
                        .block();

                return Flux.fromIterable(parseResponse(response, topic));
            } catch (Exception e) {
                log.warn("CORE API call failed for topic '{}': {}", topic, e.getMessage());
                return Flux.empty();
            }
        });
    }

    @Override
    public Flux<SourceEvidence> verifyClaimAgainstSource(String claim, String language) {
        String[] words = claim.split("[\\s,\\.!?]+");
        String searchQuery = String.join(" ", 
                java.util.Arrays.stream(words)
                        .filter(w -> w.length() > 3)
                        .limit(6)
                        .toList());
        
        if (searchQuery.isBlank()) {
            searchQuery = claim.length() > 60 ? claim.substring(0, 60) : claim;
        }
        
        return fetchEvidence(searchQuery, language);
    }

    private List<SourceEvidence> parseResponse(String response, String query) {
        List<SourceEvidence> evidenceList = new ArrayList<>();
        
        if (response == null || response.isBlank()) {
            return evidenceList;
        }

        try {
            JsonNode root = objectMapper.readTree(response);
            JsonNode results = root.path("results");

            if (results.isArray()) {
                for (JsonNode work : results) {
                    try {
                        String title = work.path("title").asText("");
                        if (title.isBlank()) continue;

                        String abstractText = work.path("abstract").asText("");
                        int year = work.path("yearPublished").asInt(0);
                        String doi = work.path("doi").asText("");
                        String downloadUrl = work.path("downloadUrl").asText("");
                        
                        // 저자 추출
                        String authors = extractAuthors(work.path("authors"));
                        
                        // 출판사/저널
                        String publisher = work.path("publisher").asText("");

                        // 발췌문 구성
                        StringBuilder excerpt = new StringBuilder();
                        excerpt.append("📄 ").append(title);
                        if (year > 0) {
                            excerpt.append(" (").append(year).append(")");
                        }
                        excerpt.append("\n");
                        
                        if (!authors.isBlank()) {
                            excerpt.append("저자: ").append(authors).append("\n");
                        }
                        if (!publisher.isBlank()) {
                            excerpt.append("출판: ").append(publisher).append("\n");
                        }
                        
                        // 오픈 액세스 표시
                        if (!downloadUrl.isBlank()) {
                            excerpt.append("🔓 오픈 액세스 - 전문 열람 가능\n");
                        }
                        
                        if (!abstractText.isBlank()) {
                            String shortAbstract = abstractText.length() > 250 
                                    ? abstractText.substring(0, 250) + "..." 
                                    : abstractText;
                            excerpt.append("\n").append(shortAbstract);
                        }

                        // URL 결정 (DOI > downloadUrl > CORE URL)
                        String url;
                        if (!doi.isBlank()) {
                            url = doi.startsWith("http") ? doi : "https://doi.org/" + doi;
                        } else if (!downloadUrl.isBlank()) {
                            url = downloadUrl;
                        } else {
                            String coreId = work.path("id").asText("");
                            url = coreId.isBlank() ? "" : "https://core.ac.uk/works/" + coreId;
                        }

                        // 관련성 점수 계산
                        double relevance = calculateRelevance(query, title, abstractText);

                        evidenceList.add(SourceEvidence.builder()
                                .sourceType("academic")
                                .sourceName(getSourceName())
                                .url(url)
                                .excerpt(truncate(excerpt.toString(), 550))
                                .relevanceScore(relevance)
                                .stance("neutral")
                                .build());
                    } catch (Exception e) {
                        log.debug("Failed to parse CORE work: {}", e.getMessage());
                    }
                }
            }
        } catch (Exception e) {
            log.warn("Failed to parse CORE response: {}", e.getMessage());
        }

        return evidenceList;
    }

    private String extractAuthors(JsonNode authorsNode) {
        if (!authorsNode.isArray() || authorsNode.isEmpty()) {
            return "";
        }

        List<String> authorNames = new ArrayList<>();
        for (JsonNode author : authorsNode) {
            String name = author.path("name").asText("");
            if (!name.isBlank()) {
                authorNames.add(name);
                if (authorNames.size() >= 3) break;
            }
        }

        if (authorNames.isEmpty()) return "";
        if (authorNames.size() < 3) return String.join(", ", authorNames);
        return authorNames.get(0) + " 외 " + (authorsNode.size() - 1) + "명";
    }

    private double calculateRelevance(String query, String title, String abstractText) {
        double score = 0.5;

        String lowerQuery = query.toLowerCase();
        String lowerTitle = title.toLowerCase();
        String lowerAbstract = abstractText != null ? abstractText.toLowerCase() : "";

        String[] queryWords = lowerQuery.split("\\s+");
        int matches = 0;
        for (String word : queryWords) {
            if (word.length() > 2) {
                if (lowerTitle.contains(word)) matches += 2;
                if (lowerAbstract.contains(word)) matches++;
            }
        }
        score += Math.min(0.4, matches * 0.08);

        return Math.min(1.0, Math.max(0.3, score));
    }

    private String truncate(String text, int maxLength) {
        if (text == null) return "";
        if (text.length() <= maxLength) return text;
        return text.substring(0, maxLength) + "...";
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/factcheck/CrossRefSource.java

```java
package com.newsinsight.collector.service.factcheck;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.newsinsight.collector.config.TrustScoreConfig;
import com.newsinsight.collector.service.FactVerificationService.SourceEvidence;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.MediaType;
import org.springframework.stereotype.Component;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Flux;

import java.net.URLEncoder;
import java.nio.charset.StandardCharsets;
import java.time.Duration;
import java.util.ArrayList;
import java.util.List;

/**
 * CrossRef API를 통한 학술 논문 검색 소스
 * 
 * CrossRef는 학술 논문의 메타데이터를 제공하는 무료 API입니다.
 * DOI를 기반으로 논문 정보를 검색할 수 있습니다.
 * 
 * API 문서: https://api.crossref.org/swagger-ui/index.html
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class CrossRefSource implements FactCheckSource {

    private final WebClient webClient;
    private final ObjectMapper objectMapper;
    private final TrustScoreConfig trustScoreConfig;

    @Value("${collector.fact-check.crossref.enabled:true}")
    private boolean enabled;

    @Value("${collector.fact-check.crossref.mailto:newsinsight@example.com}")
    private String mailto;

    @Value("${collector.fact-check.timeout-seconds:15}")
    private int timeoutSeconds;

    private static final String CROSSREF_API_BASE = "https://api.crossref.org/works";

    @Override
    public String getSourceId() {
        return "crossref";
    }

    @Override
    public String getSourceName() {
        return "CrossRef (학술 논문)";
    }

    @Override
    public double getTrustScore() {
        return trustScoreConfig.getFactCheck().getCrossref();
    }

    @Override
    public SourceType getSourceType() {
        return SourceType.ACADEMIC;
    }

    @Override
    public boolean isAvailable() {
        return enabled;
    }

    @Override
    public Flux<SourceEvidence> fetchEvidence(String topic, String language) {
        if (!enabled) {
            return Flux.empty();
        }

        return Flux.defer(() -> {
            try {
                String encodedQuery = URLEncoder.encode(topic, StandardCharsets.UTF_8);
                String url = String.format(
                        "%s?query=%s&rows=5&sort=relevance&order=desc&mailto=%s",
                        CROSSREF_API_BASE, encodedQuery, mailto
                );

                log.debug("Fetching CrossRef evidence for topic: {}", topic);

                String response = webClient.get()
                        .uri(url)
                        .accept(MediaType.APPLICATION_JSON)
                        .retrieve()
                        .bodyToMono(String.class)
                        .timeout(Duration.ofSeconds(timeoutSeconds))
                        .block();

                return Flux.fromIterable(parseResponse(response, topic));
            } catch (Exception e) {
                log.warn("CrossRef API call failed for topic '{}': {}", topic, e.getMessage());
                return Flux.empty();
            }
        });
    }

    @Override
    public Flux<SourceEvidence> verifyClaimAgainstSource(String claim, String language) {
        // 학술 검색은 주장 전체보다 키워드 추출 후 검색이 효과적
        String[] keywords = claim.split("[\\s,\\.]+");
        String searchQuery = String.join(" ", 
                java.util.Arrays.stream(keywords)
                        .filter(w -> w.length() > 3)
                        .limit(5)
                        .toList());
        
        if (searchQuery.isBlank()) {
            searchQuery = claim.substring(0, Math.min(50, claim.length()));
        }
        
        return fetchEvidence(searchQuery, language);
    }

    private List<SourceEvidence> parseResponse(String response, String query) {
        List<SourceEvidence> evidenceList = new ArrayList<>();
        
        if (response == null || response.isBlank()) {
            return evidenceList;
        }

        try {
            JsonNode root = objectMapper.readTree(response);
            JsonNode items = root.path("message").path("items");

            if (items.isArray()) {
                for (JsonNode item : items) {
                    try {
                        String title = extractTitle(item);
                        String doi = item.path("DOI").asText("");
                        String abstractText = item.path("abstract").asText("");
                        String publisher = item.path("publisher").asText("Unknown Publisher");
                        int citationCount = item.path("is-referenced-by-count").asInt(0);

                        // 초록이 없으면 제목 + 출판사 정보 사용
                        String excerpt;
                        if (!abstractText.isBlank()) {
                            // HTML 태그 제거
                            excerpt = abstractText.replaceAll("<[^>]+>", "").trim();
                            if (excerpt.length() > 400) {
                                excerpt = excerpt.substring(0, 400) + "...";
                            }
                        } else {
                            excerpt = String.format("제목: %s (출판: %s, 인용: %d회)", 
                                    title, publisher, citationCount);
                        }

                        String url = doi.isEmpty() ? "" : "https://doi.org/" + doi;

                        // 제목과 쿼리의 관련성 점수 계산
                        double relevance = calculateRelevance(query, title, abstractText);

                        evidenceList.add(SourceEvidence.builder()
                                .sourceType("academic")
                                .sourceName(getSourceName())
                                .url(url)
                                .excerpt(excerpt)
                                .relevanceScore(relevance)
                                .stance("neutral") // 학술 자료는 기본적으로 중립
                                .build());
                    } catch (Exception e) {
                        log.debug("Failed to parse CrossRef item: {}", e.getMessage());
                    }
                }
            }
        } catch (Exception e) {
            log.warn("Failed to parse CrossRef response: {}", e.getMessage());
        }

        return evidenceList;
    }

    private String extractTitle(JsonNode item) {
        JsonNode titleNode = item.path("title");
        if (titleNode.isArray() && !titleNode.isEmpty()) {
            return titleNode.get(0).asText("");
        }
        return titleNode.asText("Unknown Title");
    }

    private double calculateRelevance(String query, String title, String abstractText) {
        if (query == null || query.isBlank()) return 0.0;
        
        String lowerQuery = query.toLowerCase();
        String lowerTitle = title.toLowerCase();
        String lowerAbstract = abstractText.toLowerCase();
        
        // 한국어 키워드 추출 (조사 제거)
        String[] queryWords = lowerQuery
                .replaceAll("[은는이가을를의에에서로으로와과도만]", " ")
                .split("\\s+");
        
        int significantWords = 0;
        int titleMatches = 0;
        int abstractMatches = 0;
        
        for (String word : queryWords) {
            // 의미있는 단어만 카운트 (한글 2자 이상, 영어 3자 이상)
            boolean isKorean = word.matches(".*[가-힣].*");
            int minLength = isKorean ? 2 : 3;
            
            if (word.length() >= minLength) {
                significantWords++;
                if (lowerTitle.contains(word)) titleMatches++;
                if (lowerAbstract.contains(word)) abstractMatches++;
            }
        }
        
        if (significantWords == 0) return 0.0;
        
        // 제목 매칭은 높은 가중치, 초록은 낮은 가중치
        double titleScore = (double) titleMatches / significantWords;
        double abstractScore = (double) abstractMatches / significantWords;
        
        // 최종 점수: 제목 60% + 초록 40%
        double score = titleScore * 0.6 + abstractScore * 0.4;
        
        // 전혀 매칭이 없으면 0 반환 (관련 없는 결과 필터링)
        if (titleMatches == 0 && abstractMatches == 0) {
            return 0.0;
        }
        
        return score;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/factcheck/FactCheckSource.java

```java
package com.newsinsight.collector.service.factcheck;

import com.newsinsight.collector.service.FactVerificationService.SourceEvidence;
import reactor.core.publisher.Flux;

import java.util.List;

/**
 * 팩트체크를 위한 데이터 소스 인터페이스
 * 
 * 각 구현체는 특정 데이터 소스(Wikipedia, CrossRef, Google Fact Check 등)에서
 * 주제 또는 주장에 대한 근거를 수집합니다.
 */
public interface FactCheckSource {
    
    /**
     * 소스 식별자
     */
    String getSourceId();
    
    /**
     * 소스 표시 이름
     */
    String getSourceName();
    
    /**
     * 신뢰도 점수 (0.0 ~ 1.0)
     * 학술 자료 > 공식 통계 > 백과사전 > 뉴스 팩트체크 순
     */
    double getTrustScore();
    
    /**
     * 주어진 주제/키워드에 대한 근거 수집
     * 
     * @param topic 검색할 주제 또는 키워드
     * @param language 언어 코드 (ko, en 등)
     * @return 수집된 근거 목록
     */
    Flux<SourceEvidence> fetchEvidence(String topic, String language);
    
    /**
     * 특정 주장에 대한 팩트체크 결과 조회
     * 
     * @param claim 검증할 주장
     * @param language 언어 코드
     * @return 팩트체크 근거 목록
     */
    Flux<SourceEvidence> verifyClaimAgainstSource(String claim, String language);
    
    /**
     * 이 소스가 사용 가능한지 확인 (API 키 설정 등)
     */
    boolean isAvailable();
    
    /**
     * 소스 유형 (참고용)
     */
    default SourceType getSourceType() {
        return SourceType.REFERENCE;
    }
    
    enum SourceType {
        ENCYCLOPEDIA,     // 백과사전 (Wikipedia, Britannica)
        ACADEMIC,         // 학술 자료 (CrossRef, OpenAlex)
        FACT_CHECK,       // 팩트체크 사이트 (Google Fact Check, Snopes)
        OFFICIAL_STATS,   // 공식 통계 (KOSIS, World Bank)
        REFERENCE         // 기타 참고 자료
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/factcheck/GoogleFactCheckSource.java

```java
package com.newsinsight.collector.service.factcheck;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.newsinsight.collector.config.TrustScoreConfig;
import com.newsinsight.collector.service.FactVerificationService.SourceEvidence;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.MediaType;
import org.springframework.stereotype.Component;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Flux;

import java.net.URLEncoder;
import java.nio.charset.StandardCharsets;
import java.time.Duration;
import java.util.ArrayList;
import java.util.List;

/**
 * Google Fact Check Tools API를 통한 팩트체크 결과 조회
 * 
 * Google Fact Check Tools API는 전 세계 팩트체커들이 검증한
 * 주장들의 데이터베이스를 제공합니다.
 * 
 * API 키 필요: https://developers.google.com/fact-check/tools/api/reference/rest
 * 
 * 무료 할당량: 10,000 요청/일
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class GoogleFactCheckSource implements FactCheckSource {

    private final WebClient webClient;
    private final ObjectMapper objectMapper;
    private final TrustScoreConfig trustScoreConfig;

    @Value("${collector.fact-check.google.api-key:}")
    private String apiKey;

    @Value("${collector.fact-check.google.enabled:true}")
    private boolean enabled;

    @Value("${collector.fact-check.timeout-seconds:15}")
    private int timeoutSeconds;

    private static final String FACT_CHECK_API_BASE = "https://factchecktools.googleapis.com/v1alpha1/claims:search";

    @Override
    public String getSourceId() {
        return "google_factcheck";
    }

    @Override
    public String getSourceName() {
        return "Google Fact Check";
    }

    @Override
    public double getTrustScore() {
        return trustScoreConfig.getFactCheck().getGoogleFactCheck();
    }

    @Override
    public SourceType getSourceType() {
        return SourceType.FACT_CHECK;
    }

    @Override
    public boolean isAvailable() {
        return enabled && apiKey != null && !apiKey.isBlank();
    }

    @Override
    public Flux<SourceEvidence> fetchEvidence(String topic, String language) {
        if (!isAvailable()) {
            log.debug("Google Fact Check API is not available (enabled={}, hasKey={})", 
                    enabled, apiKey != null && !apiKey.isBlank());
            return Flux.empty();
        }

        return Flux.defer(() -> {
            try {
                String encodedQuery = URLEncoder.encode(topic, StandardCharsets.UTF_8);
                String languageCode = mapLanguageCode(language);
                
                String url = String.format(
                        "%s?query=%s&languageCode=%s&pageSize=10&key=%s",
                        FACT_CHECK_API_BASE, encodedQuery, languageCode, apiKey
                );

                log.debug("Fetching Google Fact Check evidence for topic: {}", topic);

                String response = webClient.get()
                        .uri(url)
                        .accept(MediaType.APPLICATION_JSON)
                        .retrieve()
                        .bodyToMono(String.class)
                        .timeout(Duration.ofSeconds(timeoutSeconds))
                        .block();

                return Flux.fromIterable(parseResponse(response));
            } catch (Exception e) {
                log.warn("Google Fact Check API call failed for topic '{}': {}", topic, e.getMessage());
                return Flux.empty();
            }
        });
    }

    @Override
    public Flux<SourceEvidence> verifyClaimAgainstSource(String claim, String language) {
        return fetchEvidence(claim, language);
    }

    private String mapLanguageCode(String language) {
        if (language == null) return "ko";
        return switch (language.toLowerCase()) {
            case "ko", "kor", "korean" -> "ko";
            case "en", "eng", "english" -> "en";
            case "ja", "jpn", "japanese" -> "ja";
            case "zh", "chi", "chinese" -> "zh";
            default -> language;
        };
    }

    private List<SourceEvidence> parseResponse(String response) {
        List<SourceEvidence> evidenceList = new ArrayList<>();
        
        if (response == null || response.isBlank()) {
            return evidenceList;
        }

        try {
            JsonNode root = objectMapper.readTree(response);
            JsonNode claims = root.path("claims");

            if (claims.isArray()) {
                for (JsonNode claimNode : claims) {
                    try {
                        String claimText = claimNode.path("text").asText("");
                        String claimant = claimNode.path("claimant").asText("");
                        
                        // 팩트체크 리뷰 정보 추출
                        JsonNode reviews = claimNode.path("claimReview");
                        if (reviews.isArray() && !reviews.isEmpty()) {
                            JsonNode review = reviews.get(0);
                            
                            String publisher = review.path("publisher").path("name").asText("Unknown");
                            String reviewUrl = review.path("url").asText("");
                            String rating = review.path("textualRating").asText("");
                            String title = review.path("title").asText("");
                            String languageCode = review.path("languageCode").asText("");

                            // stance 결정 (rating 기반)
                            String stance = determineStance(rating);
                            
                            // 발췌문 구성
                            StringBuilder excerpt = new StringBuilder();
                            if (!claimText.isBlank()) {
                                excerpt.append("주장: ").append(claimText);
                                if (!claimant.isBlank()) {
                                    excerpt.append(" (").append(claimant).append(")");
                                }
                                excerpt.append("\n");
                            }
                            excerpt.append("판정: ").append(rating);
                            if (!title.isBlank()) {
                                excerpt.append("\n").append(title);
                            }

                            evidenceList.add(SourceEvidence.builder()
                                    .sourceType("factcheck")
                                    .sourceName(publisher + " (Fact Check)")
                                    .url(reviewUrl)
                                    .excerpt(truncate(excerpt.toString(), 500))
                                    .relevanceScore(0.85)
                                    .stance(stance)
                                    .build());
                        }
                    } catch (Exception e) {
                        log.debug("Failed to parse Google Fact Check claim: {}", e.getMessage());
                    }
                }
            }
        } catch (Exception e) {
            log.warn("Failed to parse Google Fact Check response: {}", e.getMessage());
        }

        return evidenceList;
    }

    /**
     * 팩트체크 판정을 stance로 변환
     */
    private String determineStance(String rating) {
        if (rating == null) return "neutral";
        
        String lower = rating.toLowerCase();
        
        // 거짓 판정 패턴
        if (lower.contains("false") || lower.contains("거짓") || lower.contains("허위") ||
            lower.contains("wrong") || lower.contains("incorrect") || lower.contains("틀") ||
            lower.contains("fake") || lower.contains("misleading") || lower.contains("오해")) {
            return "contradict";
        }
        
        // 진실 판정 패턴
        if (lower.contains("true") || lower.contains("사실") || lower.contains("correct") ||
            lower.contains("accurate") || lower.contains("정확") || lower.contains("맞")) {
            return "support";
        }
        
        // 부분적/혼합 판정
        if (lower.contains("partly") || lower.contains("partially") || lower.contains("mixed") ||
            lower.contains("부분") || lower.contains("일부") || lower.contains("반")) {
            return "neutral";
        }
        
        return "neutral";
    }

    private String truncate(String text, int maxLength) {
        if (text == null) return "";
        if (text.length() <= maxLength) return text;
        return text.substring(0, maxLength) + "...";
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/factcheck/NaverNewsSource.java

```java
package com.newsinsight.collector.service.factcheck;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.newsinsight.collector.config.TrustScoreConfig;
import com.newsinsight.collector.service.FactVerificationService.SourceEvidence;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Component;
import org.springframework.web.reactive.function.client.WebClient;
import org.springframework.web.util.UriComponentsBuilder;
import reactor.core.publisher.Flux;

import java.net.URLEncoder;
import java.nio.charset.StandardCharsets;
import java.time.Duration;
import java.util.ArrayList;
import java.util.List;

/**
 * 네이버 뉴스 검색 API를 통한 팩트체크 소스
 * 
 * 한국어 뉴스 기사 검색에 최적화되어 있습니다.
 * 반도체, 경제, 정치 등 시사 관련 팩트체크에 유용합니다.
 * 
 * API 키 발급: https://developers.naver.com/
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class NaverNewsSource implements FactCheckSource {

    private final WebClient webClient;
    private final ObjectMapper objectMapper;
    private final TrustScoreConfig trustScoreConfig;

    @Value("${NAVER_CLIENT_ID:}")
    private String clientId;

    @Value("${NAVER_CLIENT_SECRET:}")
    private String clientSecret;

    @Value("${collector.naver-news.enabled:true}")
    private boolean enabled;

    @Value("${collector.naver-news.display:10}")
    private int displayCount;

    private static final String NAVER_NEWS_API_URL = "https://openapi.naver.com/v1/search/news.json";
    private static final double NEWS_TRUST_SCORE = 0.75; // 뉴스 소스 기본 신뢰도

    @Override
    public String getSourceId() {
        return "naver_news";
    }

    @Override
    public String getSourceName() {
        return "네이버 뉴스";
    }

    @Override
    public double getTrustScore() {
        return NEWS_TRUST_SCORE;
    }

    @Override
    public boolean isAvailable() {
        return enabled && clientId != null && !clientId.isEmpty() 
               && clientSecret != null && !clientSecret.isEmpty();
    }

    @Override
    public Flux<SourceEvidence> verifyClaimAgainstSource(String claim, String language) {
        return fetchEvidence(claim, language);
    }

    @Override
    public SourceType getSourceType() {
        return SourceType.REFERENCE;
    }

    @Override
    public Flux<SourceEvidence> fetchEvidence(String topic, String language) {
        if (!isAvailable()) {
            log.debug("Naver News source is disabled or API keys not configured");
            return Flux.empty();
        }

        // 한국어 검색에 최적화
        if (!"ko".equalsIgnoreCase(language) && !"kr".equalsIgnoreCase(language)) {
            log.debug("Naver News is optimized for Korean language, skipping for: {}", language);
            return Flux.empty();
        }

        return Flux.defer(() -> {
            try {
                String encodedQuery = URLEncoder.encode(topic, StandardCharsets.UTF_8);
                String url = UriComponentsBuilder.fromUriString(NAVER_NEWS_API_URL)
                        .queryParam("query", encodedQuery)
                        .queryParam("display", displayCount)
                        .queryParam("sort", "date")
                        .build()
                        .toUriString();

                return webClient.get()
                        .uri(url)
                        .header("X-Naver-Client-Id", clientId)
                        .header("X-Naver-Client-Secret", clientSecret)
                        .retrieve()
                        .bodyToMono(String.class)
                        .timeout(Duration.ofSeconds(10))
                        .flatMapMany(response -> {
                            List<SourceEvidence> evidenceList = parseNaverNewsResponse(response, topic);
                            return Flux.fromIterable(evidenceList);
                        })
                        .onErrorResume(e -> {
                            log.warn("Failed to fetch from Naver News: {}", e.getMessage());
                            return Flux.empty();
                        });
            } catch (Exception e) {
                log.error("Error preparing Naver News request: {}", e.getMessage());
                return Flux.empty();
            }
        });
    }

    private List<SourceEvidence> parseNaverNewsResponse(String response, String topic) {
        List<SourceEvidence> evidenceList = new ArrayList<>();

        try {
            JsonNode root = objectMapper.readTree(response);
            JsonNode items = root.path("items");

            if (items.isArray()) {
                for (JsonNode item : items) {
                    String title = cleanHtml(item.path("title").asText(""));
                    String description = cleanHtml(item.path("description").asText(""));
                    String link = item.path("originallink").asText(item.path("link").asText(""));

                    if (title.isEmpty() || link.isEmpty()) {
                        continue;
                    }

                    // 관련성 점수 계산
                    double relevance = calculateRelevance(topic, title, description);
                    
                    // excerpt 생성: 제목 + 설명
                    String excerpt = String.format("%s - %s", title, 
                            description.length() > 200 ? description.substring(0, 200) + "..." : description);

                    SourceEvidence evidence = SourceEvidence.builder()
                            .sourceType("news")
                            .sourceName(getSourceName())
                            .url(link)
                            .excerpt(excerpt)
                            .relevanceScore(relevance)
                            .stance("neutral")
                            .build();

                    evidenceList.add(evidence);
                    
                    if (evidenceList.size() >= 5) {
                        break;
                    }
                }
            }

            log.info("Fetched {} news articles from Naver for topic: {}", evidenceList.size(), topic);

        } catch (Exception e) {
            log.error("Failed to parse Naver News response: {}", e.getMessage());
        }

        return evidenceList;
    }

    private String cleanHtml(String text) {
        if (text == null) return "";
        return text.replaceAll("<[^>]*>", "")
                   .replaceAll("&quot;", "\"")
                   .replaceAll("&amp;", "&")
                   .replaceAll("&lt;", "<")
                   .replaceAll("&gt;", ">")
                   .replaceAll("&nbsp;", " ")
                   .trim();
    }

    private double calculateRelevance(String topic, String title, String description) {
        String topicLower = topic.toLowerCase();
        String titleLower = title.toLowerCase();
        String descLower = description.toLowerCase();
        
        String[] keywords = topicLower.split("\\s+");
        int matchCount = 0;
        
        for (String keyword : keywords) {
            if (keyword.length() < 2) continue;
            if (titleLower.contains(keyword)) matchCount += 2;
            if (descLower.contains(keyword)) matchCount += 1;
        }
        
        double score = 0.3 + (Math.min(matchCount, 10) / 10.0) * 0.6;
        return Math.round(score * 100) / 100.0;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/factcheck/OpenAlexSource.java

```java
package com.newsinsight.collector.service.factcheck;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.newsinsight.collector.config.TrustScoreConfig;
import com.newsinsight.collector.service.FactVerificationService.SourceEvidence;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.MediaType;
import org.springframework.stereotype.Component;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Flux;

import java.net.URLEncoder;
import java.nio.charset.StandardCharsets;
import java.time.Duration;
import java.util.ArrayList;
import java.util.List;

/**
 * OpenAlex API를 통한 학술 연구 검색
 * 
 * OpenAlex는 무료로 사용할 수 있는 오픈 학술 데이터베이스로,
 * 2억 개 이상의 학술 저작물을 검색할 수 있습니다.
 * 
 * API 문서: https://docs.openalex.org/
 * 
 * 특징:
 * - API 키 불필요 (무료)
 * - 빠른 응답 속도
 * - 풍부한 메타데이터
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class OpenAlexSource implements FactCheckSource {

    private final WebClient webClient;
    private final ObjectMapper objectMapper;
    private final TrustScoreConfig trustScoreConfig;

    @Value("${collector.fact-check.openalex.enabled:true}")
    private boolean enabled;

    @Value("${collector.fact-check.openalex.mailto:newsinsight@example.com}")
    private String mailto;

    @Value("${collector.fact-check.timeout-seconds:15}")
    private int timeoutSeconds;

    private static final String OPENALEX_API_BASE = "https://api.openalex.org/works";

    @Override
    public String getSourceId() {
        return "openalex";
    }

    @Override
    public String getSourceName() {
        return "OpenAlex (학술 DB)";
    }

    @Override
    public double getTrustScore() {
        return trustScoreConfig.getFactCheck().getOpenalex();
    }

    @Override
    public SourceType getSourceType() {
        return SourceType.ACADEMIC;
    }

    @Override
    public boolean isAvailable() {
        return enabled;
    }

    @Override
    public Flux<SourceEvidence> fetchEvidence(String topic, String language) {
        if (!enabled) {
            return Flux.empty();
        }

        return Flux.defer(() -> {
            try {
                String encodedQuery = URLEncoder.encode(topic, StandardCharsets.UTF_8);
                
                // OpenAlex는 polite pool을 위해 mailto 파라미터 권장
                String url = String.format(
                        "%s?search=%s&per_page=5&sort=relevance_score:desc&mailto=%s",
                        OPENALEX_API_BASE, encodedQuery, mailto
                );

                log.debug("Fetching OpenAlex evidence for topic: {}", topic);

                String response = webClient.get()
                        .uri(url)
                        .accept(MediaType.APPLICATION_JSON)
                        .retrieve()
                        .bodyToMono(String.class)
                        .timeout(Duration.ofSeconds(timeoutSeconds))
                        .block();

                return Flux.fromIterable(parseResponse(response, topic));
            } catch (Exception e) {
                log.warn("OpenAlex API call failed for topic '{}': {}", topic, e.getMessage());
                return Flux.empty();
            }
        });
    }

    @Override
    public Flux<SourceEvidence> verifyClaimAgainstSource(String claim, String language) {
        // 학술 검색은 주장에서 핵심 키워드만 추출
        String[] words = claim.split("[\\s,\\.!?]+");
        String searchQuery = String.join(" ", 
                java.util.Arrays.stream(words)
                        .filter(w -> w.length() > 3)
                        .limit(6)
                        .toList());
        
        if (searchQuery.isBlank()) {
            searchQuery = claim.length() > 60 ? claim.substring(0, 60) : claim;
        }
        
        return fetchEvidence(searchQuery, language);
    }

    private List<SourceEvidence> parseResponse(String response, String query) {
        List<SourceEvidence> evidenceList = new ArrayList<>();
        
        if (response == null || response.isBlank()) {
            return evidenceList;
        }

        try {
            JsonNode root = objectMapper.readTree(response);
            JsonNode results = root.path("results");

            if (results.isArray()) {
                for (JsonNode work : results) {
                    try {
                        String title = work.path("title").asText("");
                        if (title.isBlank()) continue;

                        String doi = work.path("doi").asText("");
                        int citedByCount = work.path("cited_by_count").asInt(0);
                        int publicationYear = work.path("publication_year").asInt(0);
                        double relevanceScore = work.path("relevance_score").asDouble(0.5);
                        
                        // 초록 추출 (inverted index에서 복원 또는 없으면 생략)
                        String abstractText = extractAbstract(work);
                        
                        // 저자 정보
                        String authors = extractAuthors(work);
                        
                        // 발췌문 구성
                        StringBuilder excerpt = new StringBuilder();
                        excerpt.append("📄 ").append(title);
                        if (publicationYear > 0) {
                            excerpt.append(" (").append(publicationYear).append(")");
                        }
                        excerpt.append("\n");
                        if (!authors.isBlank()) {
                            excerpt.append("저자: ").append(authors).append("\n");
                        }
                        excerpt.append("인용: ").append(citedByCount).append("회");
                        if (!abstractText.isBlank()) {
                            excerpt.append("\n\n").append(abstractText);
                        }

                        String url = doi.isBlank() ? work.path("id").asText("") : doi;

                        // 관련성 점수 계산: API 점수 + 키워드 매칭
                        double apiScore = Math.min(1.0, relevanceScore / 100.0);
                        double keywordScore = calculateKeywordRelevance(query, title, abstractText);
                        
                        // 키워드 매칭이 전혀 없으면 0으로 설정 (필터링됨)
                        double normalizedRelevance = keywordScore > 0 
                                ? (apiScore * 0.4 + keywordScore * 0.6) 
                                : 0.0;

                        evidenceList.add(SourceEvidence.builder()
                                .sourceType("academic")
                                .sourceName(getSourceName())
                                .url(url)
                                .excerpt(truncate(excerpt.toString(), 500))
                                .relevanceScore(normalizedRelevance)
                                .stance("neutral")
                                .build());
                    } catch (Exception e) {
                        log.debug("Failed to parse OpenAlex work: {}", e.getMessage());
                    }
                }
            }
        } catch (Exception e) {
            log.warn("Failed to parse OpenAlex response: {}", e.getMessage());
        }

        return evidenceList;
    }

    private String extractAbstract(JsonNode work) {
        // OpenAlex는 abstract를 inverted index 형태로 저장
        JsonNode abstractIndex = work.path("abstract_inverted_index");
        if (abstractIndex.isMissingNode() || abstractIndex.isNull()) {
            return "";
        }

        try {
            // inverted index를 원문으로 복원
            java.util.TreeMap<Integer, String> positionToWord = new java.util.TreeMap<>();
            
            abstractIndex.fields().forEachRemaining(entry -> {
                String word = entry.getKey();
                JsonNode positions = entry.getValue();
                if (positions.isArray()) {
                    for (JsonNode pos : positions) {
                        positionToWord.put(pos.asInt(), word);
                    }
                }
            });
            
            StringBuilder sb = new StringBuilder();
            for (String word : positionToWord.values()) {
                if (!sb.isEmpty()) sb.append(" ");
                sb.append(word);
            }
            
            String result = sb.toString();
            return result.length() > 300 ? result.substring(0, 300) + "..." : result;
        } catch (Exception e) {
            log.debug("Failed to extract abstract: {}", e.getMessage());
            return "";
        }
    }

    private String extractAuthors(JsonNode work) {
        JsonNode authorships = work.path("authorships");
        if (!authorships.isArray() || authorships.isEmpty()) {
            return "";
        }

        List<String> authorNames = new ArrayList<>();
        for (JsonNode authorship : authorships) {
            String name = authorship.path("author").path("display_name").asText("");
            if (!name.isBlank()) {
                authorNames.add(name);
                if (authorNames.size() >= 3) break; // 최대 3명까지만
            }
        }

        if (authorNames.isEmpty()) return "";
        if (authorNames.size() < 3) return String.join(", ", authorNames);
        return authorNames.get(0) + " 외 " + (authorships.size() - 1) + "명";
    }

    private String truncate(String text, int maxLength) {
        if (text == null) return "";
        if (text.length() <= maxLength) return text;
        return text.substring(0, maxLength) + "...";
    }

    /**
     * 쿼리 키워드와 제목/초록의 관련성 점수 계산
     */
    private double calculateKeywordRelevance(String query, String title, String abstractText) {
        if (query == null || query.isBlank()) return 0.0;
        
        String lowerQuery = query.toLowerCase();
        String lowerTitle = title.toLowerCase();
        String lowerAbstract = abstractText.toLowerCase();
        
        // 한국어 조사 제거 후 키워드 추출
        String[] queryWords = lowerQuery
                .replaceAll("[은는이가을를의에에서로으로와과도만]", " ")
                .split("\\s+");
        
        int significantWords = 0;
        int titleMatches = 0;
        int abstractMatches = 0;
        
        for (String word : queryWords) {
            boolean isKorean = word.matches(".*[가-힣].*");
            int minLength = isKorean ? 2 : 3;
            
            if (word.length() >= minLength) {
                significantWords++;
                if (lowerTitle.contains(word)) titleMatches++;
                if (lowerAbstract.contains(word)) abstractMatches++;
            }
        }
        
        if (significantWords == 0) return 0.0;
        if (titleMatches == 0 && abstractMatches == 0) return 0.0;
        
        double titleScore = (double) titleMatches / significantWords;
        double abstractScore = (double) abstractMatches / significantWords;
        
        return titleScore * 0.6 + abstractScore * 0.4;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/factcheck/PubMedSource.java

```java
package com.newsinsight.collector.service.factcheck;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.dataformat.xml.XmlMapper;
import com.newsinsight.collector.config.TrustScoreConfig;
import com.newsinsight.collector.service.FactVerificationService.SourceEvidence;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.MediaType;
import org.springframework.stereotype.Component;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Flux;

import java.net.URLEncoder;
import java.nio.charset.StandardCharsets;
import java.time.Duration;
import java.util.ArrayList;
import java.util.List;

/**
 * PubMed/NCBI API를 통한 의학/생명과학 학술 논문 검색
 * 
 * PubMed는 미국 국립의학도서관(NLM)에서 제공하는 의생명과학 문헌 데이터베이스로,
 * 3,500만 건 이상의 논문을 무료로 검색할 수 있습니다.
 * 
 * API 문서: https://www.ncbi.nlm.nih.gov/books/NBK25500/
 * 
 * 특징:
 * - 의학/건강 관련 주장 검증에 최적
 * - 피어리뷰된 고품질 논문
 * - API 키 없이 초당 3회 요청 가능
 * - API 키 있으면 초당 10회 가능
 */
@Component
@Slf4j
public class PubMedSource implements FactCheckSource {

    private final WebClient webClient;
    private final ObjectMapper objectMapper;
    private final XmlMapper xmlMapper;
    private final TrustScoreConfig trustScoreConfig;

    @Value("${collector.fact-check.pubmed.enabled:true}")
    private boolean enabled;

    @Value("${collector.fact-check.pubmed.api-key:}")
    private String apiKey;

    @Value("${collector.fact-check.timeout-seconds:15}")
    private int timeoutSeconds;

    private static final String ESEARCH_URL = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi";
    private static final String EFETCH_URL = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi";
    private static final String ESUMMARY_URL = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi";

    public PubMedSource(WebClient webClient, ObjectMapper objectMapper, TrustScoreConfig trustScoreConfig) {
        this.webClient = webClient;
        this.objectMapper = objectMapper;
        this.xmlMapper = new XmlMapper();
        this.trustScoreConfig = trustScoreConfig;
    }

    @Override
    public String getSourceId() {
        return "pubmed";
    }

    @Override
    public String getSourceName() {
        return "PubMed (의학 논문)";
    }

    @Override
    public double getTrustScore() {
        try {
            return trustScoreConfig.getFactCheck().getCrossref(); // CrossRef과 동일한 수준
        } catch (Exception e) {
            return 0.90; // 의학 논문은 높은 신뢰도
        }
    }

    @Override
    public SourceType getSourceType() {
        return SourceType.ACADEMIC;
    }

    @Override
    public boolean isAvailable() {
        return enabled;
    }

    @Override
    public Flux<SourceEvidence> fetchEvidence(String topic, String language) {
        if (!enabled) {
            return Flux.empty();
        }

        return Flux.defer(() -> {
            try {
                // 1. 먼저 검색하여 PubMed ID 목록 가져오기
                List<String> pmids = searchPubMed(topic);
                if (pmids.isEmpty()) {
                    log.debug("No PubMed results found for topic: {}", topic);
                    return Flux.empty();
                }

                // 2. ID로 상세 정보 가져오기
                return Flux.fromIterable(fetchSummaries(pmids, topic));
            } catch (Exception e) {
                log.warn("PubMed API call failed for topic '{}': {}", topic, e.getMessage());
                return Flux.empty();
            }
        });
    }

    @Override
    public Flux<SourceEvidence> verifyClaimAgainstSource(String claim, String language) {
        // 의학 관련 키워드 추출 및 검색어 최적화
        String[] words = claim.split("[\\s,\\.!?]+");
        String searchQuery = String.join(" ", 
                java.util.Arrays.stream(words)
                        .filter(w -> w.length() > 3)
                        .filter(w -> !isCommonWord(w))
                        .limit(6)
                        .toList());
        
        if (searchQuery.isBlank()) {
            searchQuery = claim.length() > 60 ? claim.substring(0, 60) : claim;
        }
        
        return fetchEvidence(searchQuery, language);
    }

    private boolean isCommonWord(String word) {
        return List.of("that", "this", "with", "from", "have", "been", "were", "will",
                "about", "which", "their", "there", "would", "could", "should",
                "이것", "저것", "그것", "있는", "없는", "하는", "되는", "대한").contains(word.toLowerCase());
    }

    private List<String> searchPubMed(String query) {
        try {
            String encodedQuery = URLEncoder.encode(query, StandardCharsets.UTF_8);
            StringBuilder urlBuilder = new StringBuilder();
            urlBuilder.append(ESEARCH_URL)
                    .append("?db=pubmed")
                    .append("&term=").append(encodedQuery)
                    .append("&retmax=5")
                    .append("&retmode=json")
                    .append("&sort=relevance");
            
            if (apiKey != null && !apiKey.isBlank()) {
                urlBuilder.append("&api_key=").append(apiKey);
            }

            String response = webClient.get()
                    .uri(urlBuilder.toString())
                    .accept(MediaType.APPLICATION_JSON)
                    .retrieve()
                    .bodyToMono(String.class)
                    .timeout(Duration.ofSeconds(timeoutSeconds))
                    .block();

            if (response == null) return List.of();

            JsonNode root = objectMapper.readTree(response);
            JsonNode idList = root.path("esearchresult").path("idlist");

            List<String> pmids = new ArrayList<>();
            if (idList.isArray()) {
                for (JsonNode id : idList) {
                    pmids.add(id.asText());
                }
            }
            return pmids;
        } catch (Exception e) {
            log.warn("PubMed search failed: {}", e.getMessage());
            return List.of();
        }
    }

    private List<SourceEvidence> fetchSummaries(List<String> pmids, String query) {
        List<SourceEvidence> evidenceList = new ArrayList<>();
        
        if (pmids.isEmpty()) return evidenceList;

        try {
            String ids = String.join(",", pmids);
            StringBuilder urlBuilder = new StringBuilder();
            urlBuilder.append(ESUMMARY_URL)
                    .append("?db=pubmed")
                    .append("&id=").append(ids)
                    .append("&retmode=json");
            
            if (apiKey != null && !apiKey.isBlank()) {
                urlBuilder.append("&api_key=").append(apiKey);
            }

            String response = webClient.get()
                    .uri(urlBuilder.toString())
                    .accept(MediaType.APPLICATION_JSON)
                    .retrieve()
                    .bodyToMono(String.class)
                    .timeout(Duration.ofSeconds(timeoutSeconds))
                    .block();

            if (response == null) return evidenceList;

            JsonNode root = objectMapper.readTree(response);
            JsonNode result = root.path("result");

            for (String pmid : pmids) {
                try {
                    JsonNode article = result.path(pmid);
                    if (article.isMissingNode()) continue;

                    String title = article.path("title").asText("");
                    if (title.isBlank()) continue;

                    String source = article.path("source").asText(""); // 저널명
                    String pubDate = article.path("pubdate").asText("");
                    
                    // 저자 추출
                    String authors = extractAuthors(article.path("authors"));

                    // 발췌문 구성
                    StringBuilder excerpt = new StringBuilder();
                    excerpt.append("📄 ").append(title).append("\n");
                    
                    if (!source.isBlank()) {
                        excerpt.append("📚 저널: ").append(source);
                        if (!pubDate.isBlank()) {
                            excerpt.append(" (").append(pubDate).append(")");
                        }
                        excerpt.append("\n");
                    }
                    
                    if (!authors.isBlank()) {
                        excerpt.append("저자: ").append(authors).append("\n");
                    }

                    // PubMed URL
                    String url = "https://pubmed.ncbi.nlm.nih.gov/" + pmid + "/";

                    // 관련성 점수 계산
                    double relevance = calculateRelevance(query, title, source);

                    evidenceList.add(SourceEvidence.builder()
                            .sourceType("academic")
                            .sourceName(getSourceName())
                            .url(url)
                            .excerpt(truncate(excerpt.toString(), 500))
                            .relevanceScore(relevance)
                            .stance("neutral")
                            .build());
                } catch (Exception e) {
                    log.debug("Failed to parse PubMed article {}: {}", pmid, e.getMessage());
                }
            }
        } catch (Exception e) {
            log.warn("Failed to fetch PubMed summaries: {}", e.getMessage());
        }

        return evidenceList;
    }

    private String extractAuthors(JsonNode authorsNode) {
        if (!authorsNode.isArray() || authorsNode.isEmpty()) {
            return "";
        }

        List<String> authorNames = new ArrayList<>();
        for (JsonNode author : authorsNode) {
            String name = author.path("name").asText("");
            if (!name.isBlank()) {
                authorNames.add(name);
                if (authorNames.size() >= 3) break;
            }
        }

        if (authorNames.isEmpty()) return "";
        if (authorNames.size() < 3) return String.join(", ", authorNames);
        return authorNames.get(0) + " 외 " + (authorsNode.size() - 1) + "명";
    }

    private double calculateRelevance(String query, String title, String source) {
        double score = 0.6; // PubMed 기본 점수 (피어리뷰 저널)

        String lowerQuery = query.toLowerCase();
        String lowerTitle = title.toLowerCase();

        String[] queryWords = lowerQuery.split("\\s+");
        int matches = 0;
        for (String word : queryWords) {
            if (word.length() > 2 && lowerTitle.contains(word)) {
                matches++;
            }
        }
        score += Math.min(0.3, matches * 0.1);

        // 유명 저널 보너스
        String lowerSource = source.toLowerCase();
        if (lowerSource.contains("nature") || lowerSource.contains("science") ||
            lowerSource.contains("lancet") || lowerSource.contains("nejm") ||
            lowerSource.contains("jama") || lowerSource.contains("bmj")) {
            score += 0.1;
        }

        return Math.min(1.0, Math.max(0.5, score));
    }

    private String truncate(String text, int maxLength) {
        if (text == null) return "";
        if (text.length() <= maxLength) return text;
        return text.substring(0, maxLength) + "...";
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/factcheck/RRFEvidenceFusionService.java

```java
package com.newsinsight.collector.service.factcheck;

import com.newsinsight.collector.service.FactVerificationService.SourceEvidence;
import com.newsinsight.collector.service.search.AdvancedIntentAnalyzer;
import com.newsinsight.collector.service.search.AdvancedIntentAnalyzer.AnalyzedQuery;
import com.newsinsight.collector.service.search.AdvancedIntentAnalyzer.FallbackStrategy;
import com.newsinsight.collector.service.validation.EvidenceValidator;
import lombok.Builder;
import lombok.Data;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Mono;
import reactor.core.scheduler.Schedulers;

import java.time.Duration;
import java.util.*;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.stream.Collectors;

/**
 * RRF(Reciprocal Rank Fusion) 기반 증거 검색 및 융합 서비스
 * 
 * 의도 분석을 통해 생성된 여러 검색 쿼리를 병렬로 실행하고,
 * RRF 알고리즘을 사용하여 결과를 융합합니다.
 * 
 * 주요 기능:
 * 1. 다중 쿼리 병렬 실행
 * 2. 다중 소스 병렬 검색
 * 3. RRF 기반 결과 융합 및 랭킹
 * 4. 중복 제거 및 품질 필터링
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class RRFEvidenceFusionService {

    private final List<FactCheckSource> factCheckSources;
    private final AdvancedIntentAnalyzer intentAnalyzer;
    private final StatisticalWeightCalculator weightCalculator;
    private final EvidenceValidator evidenceValidator;

    @Value("${collector.fact-check.timeout-seconds:15}")
    private int timeoutSeconds;

    @Value("${collector.fact-check.rrf.k:60}")
    private int rrfK; // RRF 상수 k (기본값 60)

    @Value("${collector.fact-check.rrf.max-queries:5}")
    private int maxQueries; // 최대 병렬 쿼리 수

    @Value("${collector.fact-check.rrf.max-results:50}")
    private int maxResults; // 최대 결과 수

    @Value("${collector.fact-check.rrf.min-relevance:0.1}")
    private double minRelevance; // 최소 관련성 점수 (이하 결과는 제외)

    @Value("${collector.fact-check.rrf.url-validation-enabled:true}")
    private boolean urlValidationEnabled; // URL 실존 여부 검증 활성화

    /**
     * 주어진 주제에 대해 다중 쿼리 병렬 검색 및 RRF 융합 수행
     *
     * @param topic 검색 주제
     * @param language 언어 코드 (ko, en)
     * @return 융합된 증거 목록
     */
    public Mono<FusionResult> searchAndFuse(String topic, String language) {
        log.info("Starting RRF-based multi-query search for topic: {}", topic);
        
        // 1. 의도 분석 및 쿼리 확장
        AnalyzedQuery analyzedQuery = intentAnalyzer.analyzeQuery(topic);
        
        // 2. 검색 쿼리 목록 생성
        List<SearchQuery> searchQueries = buildSearchQueries(analyzedQuery, topic);
        
        log.info("Generated {} search queries for parallel execution", searchQueries.size());
        
        // 3. 활성화된 소스 목록
        List<FactCheckSource> activeSources = factCheckSources.stream()
                .filter(FactCheckSource::isAvailable)
                .toList();
        
        if (activeSources.isEmpty()) {
            log.warn("No active fact-check sources available");
            return Mono.just(FusionResult.builder()
                    .topic(topic)
                    .evidences(List.of())
                    .queryCount(0)
                    .sourceCount(0)
                    .fusionMethod("RRF")
                    .build());
        }

        // 4. 병렬 검색 실행
        return executeParallelSearch(searchQueries, activeSources, language)
                .collectList()
                .flatMap(allResults -> {
                    // 5. RRF 융합
                    List<SourceEvidence> fused = fuseWithRRF(allResults, searchQueries.size());
                    
                    log.info("RRF fusion completed: {} queries × {} sources → {} unique evidences",
                            searchQueries.size(), activeSources.size(), fused.size());
                    
                    // 6. URL 실존 여부 검증 및 필터링
                    if (urlValidationEnabled && evidenceValidator != null) {
                        return evidenceValidator.filterValidEvidences(fused)
                                .map(validatedEvidences -> {
                                    int filtered = fused.size() - validatedEvidences.size();
                                    if (filtered > 0) {
                                        log.info("URL validation filtered out {} invalid evidences (hallucinations, dead links, etc.)",
                                                filtered);
                                    }
                                    return FusionResult.builder()
                                            .topic(topic)
                                            .analyzedQuery(analyzedQuery)
                                            .evidences(validatedEvidences)
                                            .queryCount(searchQueries.size())
                                            .sourceCount(activeSources.size())
                                            .fusionMethod("RRF (k=" + rrfK + ") + URL Validation")
                                            .build();
                                });
                    }
                    
                    return Mono.just(FusionResult.builder()
                            .topic(topic)
                            .analyzedQuery(analyzedQuery)
                            .evidences(fused)
                            .queryCount(searchQueries.size())
                            .sourceCount(activeSources.size())
                            .fusionMethod("RRF (k=" + rrfK + ")")
                            .build());
                });
    }

    /**
     * 검색 쿼리 목록 생성
     */
    private List<SearchQuery> buildSearchQueries(AnalyzedQuery analyzed, String originalTopic) {
        List<SearchQuery> queries = new ArrayList<>();
        Set<String> seenQueries = new HashSet<>();
        
        // 1. 원본 쿼리 (최고 우선순위)
        queries.add(SearchQuery.builder()
                .query(originalTopic)
                .weight(1.0)
                .strategyType("ORIGINAL")
                .build());
        seenQueries.add(originalTopic.toLowerCase().trim());
        
        // 2. 한국어 → 영어 학술 쿼리 변환 (학술 DB 검색용)
        String academicQuery = convertToAcademicQuery(originalTopic, analyzed.getKeywords());
        if (academicQuery != null && !seenQueries.contains(academicQuery.toLowerCase().trim())) {
            queries.add(SearchQuery.builder()
                    .query(academicQuery)
                    .weight(0.95)
                    .strategyType("ACADEMIC_ENGLISH")
                    .build());
            seenQueries.add(academicQuery.toLowerCase().trim());
            log.info("Generated English academic query from Korean: '{}' -> '{}'", originalTopic, academicQuery);
        }
        
        // 3. 폴백 전략에서 쿼리 추출
        if (analyzed.getFallbackStrategies() != null) {
            for (FallbackStrategy strategy : analyzed.getFallbackStrategies()) {
                String query = strategy.getQuery();
                String normalizedQuery = query.toLowerCase().trim();
                
                if (!seenQueries.contains(normalizedQuery) && !query.isBlank()) {
                    // 우선순위에 따른 가중치 계산
                    double weight = Math.max(0.5, 1.0 - (strategy.getPriority() - 1) * 0.1);
                    
                    queries.add(SearchQuery.builder()
                            .query(query)
                            .weight(weight)
                            .strategyType(strategy.getStrategyType())
                            .build());
                    seenQueries.add(normalizedQuery);
                }
                
                if (queries.size() >= maxQueries) break;
            }
        }
        
        // 4. 확장 쿼리 추가
        if (queries.size() < maxQueries && analyzed.getExpandedQueries() != null) {
            for (String expanded : analyzed.getExpandedQueries()) {
                String normalizedQuery = expanded.toLowerCase().trim();
                
                if (!seenQueries.contains(normalizedQuery) && !expanded.isBlank()) {
                    queries.add(SearchQuery.builder()
                            .query(expanded)
                            .weight(0.6)
                            .strategyType("EXPANDED")
                            .build());
                    seenQueries.add(normalizedQuery);
                }
                
                if (queries.size() >= maxQueries) break;
            }
        }
        
        // 5. 주요 키워드 단독 검색 추가 (없는 경우)
        if (analyzed.getPrimaryKeyword() != null && 
            !seenQueries.contains(analyzed.getPrimaryKeyword().toLowerCase().trim())) {
            queries.add(SearchQuery.builder()
                    .query(analyzed.getPrimaryKeyword())
                    .weight(0.7)
                    .strategyType("PRIMARY_KEYWORD")
                    .build());
        }

        return queries;
    }

    /**
     * 한국어 질문을 학술 DB 검색에 적합한 영어 쿼리로 변환
     */
    private String convertToAcademicQuery(String koreanQuery, List<String> keywords) {
        // 한국어 → 영어 키워드 매핑
        Map<String, String> korToEngMap = new LinkedHashMap<>();
        
        // 기술/자동차 (우선순위 높음 - 더 긴 복합어부터 매칭)
        korToEngMap.put("전기차", "electric vehicle");
        korToEngMap.put("전기자동차", "electric vehicle");
        korToEngMap.put("배터리", "battery");
        korToEngMap.put("리튬이온", "lithium-ion");
        korToEngMap.put("수명", "lifespan");
        korToEngMap.put("내구성", "durability");
        korToEngMap.put("충전", "charging");
        korToEngMap.put("자율주행", "autonomous driving");
        korToEngMap.put("인공지능", "artificial intelligence");
        korToEngMap.put("머신러닝", "machine learning");
        korToEngMap.put("딥러닝", "deep learning");
        korToEngMap.put("블록체인", "blockchain");
        korToEngMap.put("암호화폐", "cryptocurrency");
        korToEngMap.put("비트코인", "bitcoin");
        korToEngMap.put("반도체", "semiconductor");
        korToEngMap.put("칩", "chip");
        korToEngMap.put("프로세서", "processor");
        korToEngMap.put("양자컴퓨터", "quantum computer");
        korToEngMap.put("로봇", "robot");
        korToEngMap.put("드론", "drone");
        
        // 동물
        korToEngMap.put("코끼리", "elephant");
        korToEngMap.put("두더지", "mole");
        korToEngMap.put("쥐", "mouse");
        korToEngMap.put("생쥐", "mouse");
        korToEngMap.put("뱀", "snake");
        korToEngMap.put("개", "dog");
        korToEngMap.put("고양이", "cat");
        korToEngMap.put("사자", "lion");
        korToEngMap.put("호랑이", "tiger");
        korToEngMap.put("곰", "bear");
        korToEngMap.put("원숭이", "monkey");
        korToEngMap.put("새", "bird");
        korToEngMap.put("물고기", "fish");
        korToEngMap.put("상어", "shark");
        korToEngMap.put("고래", "whale");
        korToEngMap.put("돌고래", "dolphin");
        korToEngMap.put("박쥐", "bat");
        korToEngMap.put("거미", "spider");
        korToEngMap.put("벌", "bee");
        korToEngMap.put("개미", "ant");
        korToEngMap.put("나비", "butterfly");
        
        // 행동/감정
        korToEngMap.put("무서워하", "fear");
        korToEngMap.put("무서워한다", "fear");
        korToEngMap.put("두려워하", "fear");
        korToEngMap.put("두려워한다", "fear");
        korToEngMap.put("좋아하", "like");
        korToEngMap.put("싫어하", "dislike");
        korToEngMap.put("공격하", "attack");
        korToEngMap.put("피하", "avoid");
        korToEngMap.put("도망가", "flee");
        korToEngMap.put("도망친다", "flee");
        
        // 과학 용어
        korToEngMap.put("행동", "behavior");
        korToEngMap.put("습성", "behavior");
        korToEngMap.put("본능", "instinct");
        korToEngMap.put("진화", "evolution");
        korToEngMap.put("유전자", "gene");
        korToEngMap.put("뇌", "brain");
        korToEngMap.put("신경", "nerve");
        korToEngMap.put("연구", "study");
        korToEngMap.put("실험", "experiment");
        
        // 건강/의학
        korToEngMap.put("암", "cancer");
        korToEngMap.put("당뇨", "diabetes");
        korToEngMap.put("고혈압", "hypertension");
        korToEngMap.put("비만", "obesity");
        korToEngMap.put("바이러스", "virus");
        korToEngMap.put("세균", "bacteria");
        korToEngMap.put("면역", "immunity");
        korToEngMap.put("백신", "vaccine");
        korToEngMap.put("치료", "treatment");
        korToEngMap.put("약", "drug");
        korToEngMap.put("부작용", "side effect");
        
        // 환경/지구과학
        korToEngMap.put("지구온난화", "global warming");
        korToEngMap.put("기후변화", "climate change");
        korToEngMap.put("오존층", "ozone layer");
        korToEngMap.put("미세먼지", "fine dust PM2.5");
        korToEngMap.put("환경오염", "environmental pollution");
        korToEngMap.put("방사능", "radiation");
        korToEngMap.put("핵", "nuclear");
        korToEngMap.put("지진", "earthquake");
        korToEngMap.put("화산", "volcano");
        korToEngMap.put("태풍", "typhoon");
        korToEngMap.put("홍수", "flood");
        
        // 음식/영양 (주의: "차"는 제거 - "전기차"와 충돌 방지)
        korToEngMap.put("커피", "coffee");
        korToEngMap.put("녹차", "green tea");
        korToEngMap.put("홍차", "black tea");
        korToEngMap.put("술", "alcohol");
        korToEngMap.put("설탕", "sugar");
        korToEngMap.put("소금", "salt");
        korToEngMap.put("비타민", "vitamin");
        korToEngMap.put("단백질", "protein");
        korToEngMap.put("지방", "fat");
        korToEngMap.put("탄수화물", "carbohydrate");
        
        // 변환 실행
        List<String> englishTerms = new ArrayList<>();
        String lowerQuery = koreanQuery.toLowerCase();
        
        for (Map.Entry<String, String> entry : korToEngMap.entrySet()) {
            if (lowerQuery.contains(entry.getKey())) {
                if (!englishTerms.contains(entry.getValue())) {
                    englishTerms.add(entry.getValue());
                }
            }
        }
        
        // 키워드에서도 변환 시도
        for (String keyword : keywords) {
            String lowerKeyword = keyword.toLowerCase();
            for (Map.Entry<String, String> entry : korToEngMap.entrySet()) {
                if (lowerKeyword.contains(entry.getKey())) {
                    if (!englishTerms.contains(entry.getValue())) {
                        englishTerms.add(entry.getValue());
                    }
                }
            }
        }
        
        if (englishTerms.isEmpty()) {
            return null;
        }
        
        // 학술 검색용 쿼리 생성
        return String.join(" ", englishTerms);
    }

    /**
     * 병렬 검색 실행
     */
    private Flux<RankedResultSet> executeParallelSearch(
            List<SearchQuery> queries,
            List<FactCheckSource> sources,
            String language) {
        
        // 각 (쿼리, 소스) 조합에 대해 병렬 검색
        List<Mono<RankedResultSet>> searchTasks = new ArrayList<>();
        AtomicInteger queryIndex = new AtomicInteger(0);
        
        for (SearchQuery searchQuery : queries) {
            int qIdx = queryIndex.getAndIncrement();
            
            for (FactCheckSource source : sources) {
                Mono<RankedResultSet> task = source.fetchEvidence(searchQuery.getQuery(), language)
                        .filter(evidence -> evidence.getRelevanceScore() >= minRelevance) // 관련성 필터
                        .collectList()
                        .timeout(Duration.ofSeconds(timeoutSeconds))
                        .map(evidences -> {
                            // 각 증거에 순위 부여
                            List<RankedEvidence> ranked = new ArrayList<>();
                            for (int i = 0; i < evidences.size(); i++) {
                                ranked.add(RankedEvidence.builder()
                                        .evidence(evidences.get(i))
                                        .rank(i + 1)
                                        .queryIndex(qIdx)
                                        .queryWeight(searchQuery.getWeight())
                                        .sourceId(source.getSourceId())
                                        .build());
                            }
                            return RankedResultSet.builder()
                                    .queryIndex(qIdx)
                                    .sourceId(source.getSourceId())
                                    .results(ranked)
                                    .build();
                        })
                        .onErrorResume(e -> {
                            log.debug("Search failed for query '{}' on source '{}': {}",
                                    searchQuery.getQuery(), source.getSourceId(), e.getMessage());
                            return Mono.just(RankedResultSet.builder()
                                    .queryIndex(qIdx)
                                    .sourceId(source.getSourceId())
                                    .results(List.of())
                                    .build());
                        })
                        .subscribeOn(Schedulers.boundedElastic());
                
                searchTasks.add(task);
            }
        }
        
        log.debug("Executing {} parallel search tasks", searchTasks.size());
        
        // 모든 검색 동시 실행
        return Flux.merge(searchTasks);
    }

    /**
     * RRF(Reciprocal Rank Fusion) 알고리즘으로 결과 융합
     * 
     * 통계적 가중치 기반 RRF Score = Σ (query_weight × source_weight / (k + rank))
     */
    private List<SourceEvidence> fuseWithRRF(List<RankedResultSet> allResults, int queryCount) {
        // 1. 소스별 증거 그룹화 (통계적 가중치 계산용)
        Map<String, List<SourceEvidence>> evidencesBySource = new HashMap<>();
        for (RankedResultSet resultSet : allResults) {
            String sourceId = resultSet.getSourceId();
            List<SourceEvidence> sourceEvidences = resultSet.getResults().stream()
                    .map(RankedEvidence::getEvidence)
                    .toList();
            
            evidencesBySource.computeIfAbsent(sourceId, k -> new ArrayList<>())
                    .addAll(sourceEvidences);
        }
        
        // 2. 통계적 가중치 계산 (동적 가중치)
        Map<String, Double> sourceWeights = weightCalculator.calculateSourceWeights(evidencesBySource);
        
        log.info("Applied statistical weights for {} sources", sourceWeights.size());
        
        // 3. URL 또는 제목 기반으로 증거 그룹화 및 RRF 점수 계산
        Map<String, EvidenceScore> evidenceScores = new ConcurrentHashMap<>();
        
        for (RankedResultSet resultSet : allResults) {
            String sourceId = resultSet.getSourceId();
            double sourceWeight = sourceWeights.getOrDefault(sourceId, 1.0);
            
            for (RankedEvidence ranked : resultSet.getResults()) {
                SourceEvidence evidence = ranked.getEvidence();
                
                // 고유 키 생성 (URL 우선, 없으면 제목 해시)
                String key = generateEvidenceKey(evidence);
                
                // 통계적 가중치 기반 RRF 점수 계산: query_weight × source_weight / (k + rank)
                double rrfScore = ranked.getQueryWeight() * sourceWeight / (rrfK + ranked.getRank());
                
                evidenceScores.compute(key, (k, existing) -> {
                    if (existing == null) {
                        return EvidenceScore.builder()
                                .evidence(evidence)
                                .rrfScore(rrfScore)
                                .appearanceCount(1)
                                .sources(new HashSet<>(Set.of(ranked.getSourceId())))
                                .queries(new HashSet<>(Set.of(ranked.getQueryIndex())))
                                .build();
                    } else {
                        // 점수 누적 및 메타데이터 업데이트
                        existing.setRrfScore(existing.getRrfScore() + rrfScore);
                        existing.setAppearanceCount(existing.getAppearanceCount() + 1);
                        existing.getSources().add(ranked.getSourceId());
                        existing.getQueries().add(ranked.getQueryIndex());
                        
                        // 더 상세한 정보가 있으면 업데이트
                        if (evidence.getExcerpt() != null && 
                            evidence.getExcerpt().length() > existing.getEvidence().getExcerpt().length()) {
                            existing.setEvidence(evidence);
                        }
                        return existing;
                    }
                });
            }
        }
        
        // RRF 점수로 정렬 및 상위 결과 추출
        return evidenceScores.values().stream()
                .sorted((a, b) -> Double.compare(b.getRrfScore(), a.getRrfScore()))
                .limit(maxResults)
                .map(scored -> {
                    SourceEvidence evidence = scored.getEvidence();
                    
                    // 융합 점수를 relevanceScore에 반영 (0-1 정규화)
                    double normalizedScore = Math.min(1.0, scored.getRrfScore() * 10);
                    evidence.setRelevanceScore(normalizedScore);
                    
                    // 다중 소스에서 발견된 경우 신뢰도 보너스
                    if (scored.getSources().size() > 1) {
                        double bonus = Math.min(0.2, scored.getSources().size() * 0.05);
                        evidence.setRelevanceScore(Math.min(1.0, normalizedScore + bonus));
                    }
                    
                    return evidence;
                })
                .toList();
    }

    /**
     * 증거의 고유 키 생성
     */
    private String generateEvidenceKey(SourceEvidence evidence) {
        // URL이 있으면 URL 사용
        if (evidence.getUrl() != null && !evidence.getUrl().isBlank()) {
            return evidence.getUrl().toLowerCase()
                    .replaceAll("https?://", "")
                    .replaceAll("www\\.", "")
                    .replaceAll("/$", "");
        }
        
        // 없으면 소스 + 발췌문 해시
        String content = evidence.getSourceName() + ":" + 
                (evidence.getExcerpt() != null ? evidence.getExcerpt().substring(0, Math.min(100, evidence.getExcerpt().length())) : "");
        return String.valueOf(content.hashCode());
    }

    // ============================================
    // DTO Classes
    // ============================================

    @Data
    @Builder
    public static class SearchQuery {
        private String query;
        private double weight;
        private String strategyType;
    }

    @Data
    @Builder
    public static class RankedEvidence {
        private SourceEvidence evidence;
        private int rank;
        private int queryIndex;
        private double queryWeight;
        private String sourceId;
    }

    @Data
    @Builder
    public static class RankedResultSet {
        private int queryIndex;
        private String sourceId;
        private List<RankedEvidence> results;
    }

    @Data
    @Builder
    public static class EvidenceScore {
        private SourceEvidence evidence;
        private double rrfScore;
        private int appearanceCount;
        private Set<String> sources;
        private Set<Integer> queries;
    }

    @Data
    @Builder
    public static class FusionResult {
        private String topic;
        private AnalyzedQuery analyzedQuery;
        private List<SourceEvidence> evidences;
        private int queryCount;
        private int sourceCount;
        private String fusionMethod;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/factcheck/RealtimeSearchSource.java

```java
package com.newsinsight.collector.service.factcheck;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.newsinsight.collector.config.TrustScoreConfig;
import com.newsinsight.collector.service.FactVerificationService.SourceEvidence;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.HttpHeaders;
import org.springframework.http.MediaType;
import org.springframework.stereotype.Component;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Flux;

import java.time.Duration;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

/**
 * 실시간 웹 검색을 통한 최신 데이터 수집 소스
 * 
 * Perplexity Sonar Online 모델을 사용하여 실시간 웹 검색 결과를 가져옵니다.
 * 암호화폐 시세, 주식 가격, 최신 뉴스 등 실시간 데이터 검증에 사용됩니다.
 * 
 * 특징:
 * - 실시간 웹 검색 (Perplexity Online Search)
 * - citations 추출을 통한 출처 제공
 * - 숫자/가격 데이터 추출 및 검증
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class RealtimeSearchSource implements FactCheckSource {

    private final WebClient webClient;
    private final ObjectMapper objectMapper;
    private final TrustScoreConfig trustScoreConfig;
    private final com.newsinsight.collector.service.LlmProviderSettingsService llmProviderSettingsService;

    @Value("${PERPLEXITY_API_KEY:}")
    private String envApiKey;

    @Value("${PERPLEXITY_BASE_URL:https://api.perplexity.ai}")
    private String envBaseUrl;

    @Value("${collector.realtime-search.enabled:true}")
    private boolean enabled;

    @Value("${collector.realtime-search.timeout-seconds:30}")
    private int timeoutSeconds;
    
    /**
     * Get API key from LLM Provider Settings or fall back to environment variable
     */
    private String getApiKey() {
        try {
            var apiKey = llmProviderSettingsService.getGlobalApiKey(
                com.newsinsight.collector.entity.settings.LlmProviderType.PERPLEXITY);
            if (apiKey.isPresent() && !apiKey.get().isBlank()) {
                return apiKey.get();
            }
        } catch (Exception e) {
            log.debug("Failed to get Perplexity settings from database: {}", e.getMessage());
        }
        return envApiKey;
    }
    
    /**
     * Get base URL from LLM Provider Settings or fall back to environment variable
     */
    private String getBaseUrl() {
        try {
            var baseUrl = llmProviderSettingsService.getGlobalBaseUrl(
                com.newsinsight.collector.entity.settings.LlmProviderType.PERPLEXITY);
            if (baseUrl.isPresent() && !baseUrl.get().isBlank()) {
                return baseUrl.get();
            }
        } catch (Exception e) {
            log.debug("Failed to get Perplexity base URL from database: {}", e.getMessage());
        }
        return envBaseUrl;
    }

    // 실시간 검색이 필요한 키워드 패턴
    private static final List<String> REALTIME_KEYWORDS = List.of(
            "현재", "오늘", "지금", "최신", "실시간", "시세", "가격",
            "current", "today", "now", "latest", "price", "rate",
            "비트코인", "이더리움", "암호화폐", "주가", "환율", "코스피", "코스닥",
            "bitcoin", "ethereum", "crypto", "stock", "exchange rate"
    );

    // 가격/숫자 추출 패턴
    private static final Pattern PRICE_PATTERN = Pattern.compile(
            "\\$?([0-9]{1,3}(?:,?[0-9]{3})*(?:\\.[0-9]+)?)"
    );
    
    private static final Pattern KRW_PATTERN = Pattern.compile(
            "([0-9]{1,3}(?:,?[0-9]{3})*(?:\\.[0-9]+)?)[\\s]*(?:원|KRW|₩)"
    );

    @Override
    public String getSourceId() {
        return "realtime_search";
    }

    @Override
    public String getSourceName() {
        return "Realtime Web Search (Perplexity)";
    }

    @Override
    public double getTrustScore() {
        // 실시간 검색은 신뢰도를 중간 정도로 설정 (출처에 따라 달라질 수 있음)
        return 0.80;
    }

    @Override
    public SourceType getSourceType() {
        return SourceType.REFERENCE;
    }

    @Override
    public boolean isAvailable() {
        String apiKey = getApiKey();
        return enabled && apiKey != null && !apiKey.isBlank();
    }

    /**
     * 주어진 주제가 실시간 검색이 필요한지 판단
     */
    public boolean needsRealtimeSearch(String topic) {
        if (topic == null) return false;
        String lower = topic.toLowerCase();
        return REALTIME_KEYWORDS.stream().anyMatch(lower::contains);
    }

    @Override
    public Flux<SourceEvidence> fetchEvidence(String topic, String language) {
        String apiKey = getApiKey();
        String baseUrl = getBaseUrl();
        
        if (!isAvailable()) {
            log.debug("Realtime Search is not available (enabled={}, hasKey={})", 
                    enabled, apiKey != null && !apiKey.isBlank());
            return Flux.empty();
        }

        return Flux.defer(() -> {
            try {
                String url = baseUrl.endsWith("/") ? baseUrl + "chat/completions" : baseUrl + "/chat/completions";
                
                // 현재 날짜/시간 포함한 프롬프트 생성
                String currentDateTime = LocalDateTime.now()
                        .format(DateTimeFormatter.ofPattern("yyyy년 MM월 dd일 HH:mm"));
                
                String systemPrompt = """
                        당신은 실시간 정보 검색 전문가입니다. 
                        사용자의 질문에 대해 최신 정보를 검색하고 정확한 데이터를 제공합니다.
                        
                        규칙:
                        1. 가격, 시세 등 숫자 데이터는 반드시 출처와 함께 제공
                        2. 검색된 정보의 날짜/시간을 명시
                        3. 여러 출처에서 교차 검증
                        4. 확인되지 않은 정보는 명시적으로 표시
                        
                        현재 시각: %s
                        """.formatted(currentDateTime);
                
                String userPrompt = """
                        다음 주제에 대한 최신 실시간 정보를 검색해주세요:
                        
                        주제: %s
                        
                        다음 형식으로 응답해주세요:
                        
                        ## 검색 결과
                        - 핵심 정보: [최신 데이터]
                        - 출처: [URL 또는 출처명]
                        - 확인 시점: [날짜/시간]
                        
                        ## 상세 정보
                        [관련 세부 정보]
                        
                        ## 추가 출처
                        [다른 출처에서 확인된 정보]
                        """.formatted(topic);

                log.debug("Fetching realtime evidence for topic: {}", topic);

                Map<String, Object> body = Map.of(
                        "model", "llama-3.1-sonar-large-128k-online",  // 실시간 검색 지원 모델
                        "stream", false,
                        "messages", List.of(
                                Map.of("role", "system", "content", systemPrompt),
                                Map.of("role", "user", "content", userPrompt)
                        ),
                        "return_citations", true  // 출처 URL 반환 요청
                );

                String response = webClient.post()
                        .uri(url)
                        .header(HttpHeaders.AUTHORIZATION, "Bearer " + apiKey)
                        .contentType(MediaType.APPLICATION_JSON)
                        .accept(MediaType.APPLICATION_JSON)
                        .bodyValue(body)
                        .retrieve()
                        .bodyToMono(String.class)
                        .timeout(Duration.ofSeconds(timeoutSeconds))
                        .block();

                return Flux.fromIterable(parseResponse(response, topic));
            } catch (Exception e) {
                log.warn("Realtime search failed for topic '{}': {}", topic, e.getMessage());
                return Flux.empty();
            }
        });
    }

    @Override
    public Flux<SourceEvidence> verifyClaimAgainstSource(String claim, String language) {
        // 주장에서 핵심 키워드 추출 후 검색
        return fetchEvidence(claim, language);
    }

    private List<SourceEvidence> parseResponse(String response, String originalTopic) {
        List<SourceEvidence> evidenceList = new ArrayList<>();
        
        if (response == null || response.isBlank()) {
            return evidenceList;
        }

        try {
            JsonNode root = objectMapper.readTree(response);
            
            // 응답 내용 추출
            String content = "";
            JsonNode choices = root.path("choices");
            if (choices.isArray() && !choices.isEmpty()) {
                content = choices.get(0).path("message").path("content").asText("");
            }
            
            // Citations (출처) 추출
            List<String> citations = new ArrayList<>();
            JsonNode citationsNode = root.path("citations");
            if (citationsNode.isArray()) {
                for (JsonNode citation : citationsNode) {
                    citations.add(citation.asText());
                }
            }
            
            // 메인 증거 생성
            if (!content.isBlank()) {
                String currentTime = LocalDateTime.now()
                        .format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"));
                
                // 가격 정보 추출
                String extractedPrices = extractPrices(content);
                
                StringBuilder excerpt = new StringBuilder();
                excerpt.append("[실시간 검색 결과 - ").append(currentTime).append("]\n\n");
                excerpt.append(truncate(content, 800));
                
                if (!extractedPrices.isEmpty()) {
                    excerpt.append("\n\n[추출된 가격 정보]\n").append(extractedPrices);
                }

                // 메인 증거
                SourceEvidence mainEvidence = SourceEvidence.builder()
                        .sourceType("realtime_search")
                        .sourceName("Perplexity Realtime Search")
                        .url(citations.isEmpty() ? null : citations.get(0))
                        .excerpt(excerpt.toString())
                        .relevanceScore(0.85)
                        .stance("neutral")  // 실시간 검색은 중립적 정보 제공
                        .build();
                evidenceList.add(mainEvidence);
                
                // 추가 출처들을 개별 증거로 추가
                for (int i = 1; i < Math.min(citations.size(), 5); i++) {
                    String citationUrl = citations.get(i);
                    SourceEvidence citationEvidence = SourceEvidence.builder()
                            .sourceType("realtime_search_citation")
                            .sourceName(extractDomain(citationUrl))
                            .url(citationUrl)
                            .excerpt("[출처 " + (i + 1) + "] " + citationUrl)
                            .relevanceScore(0.75)
                            .stance("neutral")
                            .build();
                    evidenceList.add(citationEvidence);
                }
            }
            
        } catch (Exception e) {
            log.warn("Failed to parse realtime search response: {}", e.getMessage());
        }

        return evidenceList;
    }

    /**
     * 텍스트에서 가격 정보 추출
     */
    private String extractPrices(String text) {
        StringBuilder prices = new StringBuilder();
        
        // USD 가격 추출
        Matcher usdMatcher = PRICE_PATTERN.matcher(text);
        int usdCount = 0;
        while (usdMatcher.find() && usdCount < 5) {
            String price = usdMatcher.group(1);
            // 의미있는 가격만 추출 (1000 이상)
            try {
                double value = Double.parseDouble(price.replace(",", ""));
                if (value >= 100) {
                    if (prices.length() > 0) prices.append("\n");
                    prices.append("- $").append(price);
                    usdCount++;
                }
            } catch (NumberFormatException ignored) {}
        }
        
        // KRW 가격 추출
        Matcher krwMatcher = KRW_PATTERN.matcher(text);
        int krwCount = 0;
        while (krwMatcher.find() && krwCount < 5) {
            String price = krwMatcher.group(1);
            try {
                double value = Double.parseDouble(price.replace(",", ""));
                if (value >= 1000) {
                    if (prices.length() > 0) prices.append("\n");
                    prices.append("- ₩").append(price);
                    krwCount++;
                }
            } catch (NumberFormatException ignored) {}
        }
        
        return prices.toString();
    }

    /**
     * URL에서 도메인 추출
     */
    private String extractDomain(String url) {
        if (url == null || url.isBlank()) return "Unknown Source";
        try {
            String domain = url.replaceFirst("https?://", "")
                    .replaceFirst("www\\.", "")
                    .split("/")[0];
            return domain;
        } catch (Exception e) {
            return "Unknown Source";
        }
    }

    private String truncate(String text, int maxLength) {
        if (text == null) return "";
        if (text.length() <= maxLength) return text;
        return text.substring(0, maxLength) + "...";
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/factcheck/SemanticScholarSource.java

```java
package com.newsinsight.collector.service.factcheck;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.newsinsight.collector.config.TrustScoreConfig;
import com.newsinsight.collector.service.FactVerificationService.SourceEvidence;
import com.newsinsight.collector.service.RateLimitRetryService;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.MediaType;
import org.springframework.stereotype.Component;
import org.springframework.web.reactive.function.client.WebClient;
import org.springframework.web.reactive.function.client.WebClientResponseException;
import reactor.core.publisher.Flux;

import java.net.URLEncoder;
import java.nio.charset.StandardCharsets;
import java.time.Duration;
import java.util.ArrayList;
import java.util.List;

/**
 * Semantic Scholar API를 통한 학술 논문 검색
 * 
 * Semantic Scholar는 AI 기반의 학술 검색 엔진으로,
 * 논문 간의 인용 관계와 영향력을 분석하여 더 관련성 높은 결과를 제공합니다.
 * 
 * API 문서: https://api.semanticscholar.org/api-docs/
 * 
 * 특징:
 * - API 키 없이 분당 100회 요청 가능
 * - 인용 관계 분석
 * - 영향력 있는 인용(influential citations) 제공
 * - 초록 및 TLDR 요약 제공
 * - 429 Too Many Requests 시 IP rotation을 통해 재시도
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class SemanticScholarSource implements FactCheckSource {

    private final WebClient webClient;
    private final ObjectMapper objectMapper;
    private final TrustScoreConfig trustScoreConfig;
    private final RateLimitRetryService rateLimitRetryService;

    @Value("${collector.fact-check.semantic-scholar.enabled:true}")
    private boolean enabled;

    @Value("${collector.fact-check.semantic-scholar.api-key:}")
    private String apiKey;

    @Value("${collector.fact-check.timeout-seconds:15}")
    private int timeoutSeconds;

    private static final String API_BASE = "https://api.semanticscholar.org/graph/v1/paper/search";
    private static final String FIELDS = "title,abstract,year,citationCount,influentialCitationCount,authors,url,tldr";

    @Override
    public String getSourceId() {
        return "semantic_scholar";
    }

    @Override
    public String getSourceName() {
        return "Semantic Scholar (학술 논문)";
    }

    @Override
    public double getTrustScore() {
        // TrustScoreConfig에 semantic scholar 설정이 없으면 기본값 사용
        try {
            return trustScoreConfig.getFactCheck().getOpenalex(); // OpenAlex와 동일한 수준
        } catch (Exception e) {
            return 0.85; // 기본 학술 신뢰도
        }
    }

    @Override
    public SourceType getSourceType() {
        return SourceType.ACADEMIC;
    }

    @Override
    public boolean isAvailable() {
        return enabled;
    }

    @Override
    public Flux<SourceEvidence> fetchEvidence(String topic, String language) {
        if (!enabled) {
            return Flux.empty();
        }

        return Flux.defer(() -> {
            try {
                String encodedQuery = URLEncoder.encode(topic, StandardCharsets.UTF_8);
                String url = String.format(
                        "%s?query=%s&limit=5&fields=%s",
                        API_BASE, encodedQuery, FIELDS
                );

                log.debug("Fetching Semantic Scholar evidence for topic: {}", topic);

                // 먼저 일반 요청 시도, 429 에러 시 프록시를 통해 재시도
                String response = executeRequestWithRetry(url);

                if (response == null || response.isBlank()) {
                    log.debug("No response from Semantic Scholar for topic: {}", topic);
                    return Flux.empty();
                }

                return Flux.fromIterable(parseResponse(response, topic));
            } catch (Exception e) {
                log.warn("Semantic Scholar API call failed for topic '{}': {}", topic, e.getMessage());
                return Flux.empty();
            }
        });
    }

    /**
     * 요청 실행 - 429 에러 시 IP rotation을 통해 재시도
     */
    private String executeRequestWithRetry(String url) {
        try {
            // 1차 시도: 일반 요청
            WebClient.RequestHeadersSpec<?> request = webClient.get()
                    .uri(url)
                    .accept(MediaType.APPLICATION_JSON);

            // API 키가 있으면 헤더에 추가 (더 높은 rate limit)
            if (apiKey != null && !apiKey.isBlank()) {
                request = webClient.get()
                        .uri(url)
                        .accept(MediaType.APPLICATION_JSON)
                        .header("x-api-key", apiKey);
            }

            return request
                    .retrieve()
                    .bodyToMono(String.class)
                    .timeout(Duration.ofSeconds(timeoutSeconds))
                    .block();
                    
        } catch (Exception e) {
            // 429 또는 403 에러인 경우 프록시를 통해 재시도
            if (isRateLimitError(e)) {
                log.info("Rate limit hit for Semantic Scholar, attempting proxy retry for: {}", url);
                return retryWithProxy(url);
            }
            throw e;
        }
    }

    /**
     * 프록시를 통한 재시도
     */
    private String retryWithProxy(String url) {
        try {
            String[] headers = apiKey != null && !apiKey.isBlank() 
                    ? new String[]{"x-api-key", apiKey, "Accept", "application/json"}
                    : new String[]{"Accept", "application/json"};
            
            String response = rateLimitRetryService.executeWithRetryBlocking(url, headers);
            
            if (response != null) {
                log.info("Semantic Scholar proxy retry succeeded for: {}", url);
            }
            return response;
        } catch (Exception e) {
            log.warn("Semantic Scholar proxy retry failed: {}", e.getMessage());
            return null;
        }
    }

    /**
     * Rate limit 에러인지 확인
     */
    private boolean isRateLimitError(Throwable e) {
        if (e instanceof WebClientResponseException wce) {
            int statusCode = wce.getStatusCode().value();
            return statusCode == 429 || statusCode == 403;
        }
        String message = e.getMessage();
        if (message != null) {
            message = message.toLowerCase();
            return message.contains("429") || 
                   message.contains("403") || 
                   message.contains("too many requests") ||
                   message.contains("rate limit");
        }
        return false;
    }

    @Override
    public Flux<SourceEvidence> verifyClaimAgainstSource(String claim, String language) {
        // 학술 검색에 적합하도록 키워드 추출
        String[] words = claim.split("[\\s,\\.!?]+");
        String searchQuery = String.join(" ", 
                java.util.Arrays.stream(words)
                        .filter(w -> w.length() > 3)
                        .filter(w -> !isCommonWord(w))
                        .limit(6)
                        .toList());
        
        if (searchQuery.isBlank()) {
            searchQuery = claim.length() > 60 ? claim.substring(0, 60) : claim;
        }
        
        return fetchEvidence(searchQuery, language);
    }

    private boolean isCommonWord(String word) {
        return List.of("that", "this", "with", "from", "have", "been", "were", "will",
                "이것", "저것", "그것", "있는", "없는", "하는", "되는").contains(word.toLowerCase());
    }

    private List<SourceEvidence> parseResponse(String response, String query) {
        List<SourceEvidence> evidenceList = new ArrayList<>();
        
        if (response == null || response.isBlank()) {
            return evidenceList;
        }

        try {
            JsonNode root = objectMapper.readTree(response);
            JsonNode data = root.path("data");

            if (data.isArray()) {
                for (JsonNode paper : data) {
                    try {
                        String title = paper.path("title").asText("");
                        if (title.isBlank()) continue;

                        String paperAbstract = paper.path("abstract").asText("");
                        String tldr = paper.path("tldr").path("text").asText("");
                        int year = paper.path("year").asInt(0);
                        int citationCount = paper.path("citationCount").asInt(0);
                        int influentialCount = paper.path("influentialCitationCount").asInt(0);
                        String paperUrl = paper.path("url").asText("");

                        // 저자 추출
                        String authors = extractAuthors(paper.path("authors"));

                        // 발췌문 구성 - TLDR이 있으면 우선 사용
                        StringBuilder excerpt = new StringBuilder();
                        excerpt.append("📄 ").append(title);
                        if (year > 0) {
                            excerpt.append(" (").append(year).append(")");
                        }
                        excerpt.append("\n");
                        
                        if (!authors.isBlank()) {
                            excerpt.append("저자: ").append(authors).append("\n");
                        }
                        
                        excerpt.append("인용: ").append(citationCount).append("회");
                        if (influentialCount > 0) {
                            excerpt.append(" (영향력 있는 인용: ").append(influentialCount).append("회)");
                        }
                        
                        // TLDR이 있으면 추가 (간결한 요약)
                        if (!tldr.isBlank()) {
                            excerpt.append("\n\n📝 요약: ").append(tldr);
                        } else if (!paperAbstract.isBlank()) {
                            // 초록 추가 (최대 300자)
                            String shortAbstract = paperAbstract.length() > 300 
                                    ? paperAbstract.substring(0, 300) + "..." 
                                    : paperAbstract;
                            excerpt.append("\n\n").append(shortAbstract);
                        }

                        // 관련성 점수 계산 (인용 수 및 영향력 기반)
                        double relevance = calculateRelevance(query, title, paperAbstract, citationCount, influentialCount);

                        evidenceList.add(SourceEvidence.builder()
                                .sourceType("academic")
                                .sourceName(getSourceName())
                                .url(paperUrl)
                                .excerpt(truncate(excerpt.toString(), 600))
                                .relevanceScore(relevance)
                                .stance("neutral")
                                .build());
                    } catch (Exception e) {
                        log.debug("Failed to parse Semantic Scholar paper: {}", e.getMessage());
                    }
                }
            }
        } catch (Exception e) {
            log.warn("Failed to parse Semantic Scholar response: {}", e.getMessage());
        }

        return evidenceList;
    }

    private String extractAuthors(JsonNode authorsNode) {
        if (!authorsNode.isArray() || authorsNode.isEmpty()) {
            return "";
        }

        List<String> authorNames = new ArrayList<>();
        for (JsonNode author : authorsNode) {
            String name = author.path("name").asText("");
            if (!name.isBlank()) {
                authorNames.add(name);
                if (authorNames.size() >= 3) break;
            }
        }

        if (authorNames.isEmpty()) return "";
        if (authorNames.size() < 3) return String.join(", ", authorNames);
        return authorNames.get(0) + " 외 " + (authorsNode.size() - 1) + "명";
    }

    private double calculateRelevance(String query, String title, String abstractText, 
                                      int citationCount, int influentialCount) {
        double score = 0.5; // 기본 점수

        // 제목/초록과 쿼리 매칭
        String lowerQuery = query.toLowerCase();
        String lowerTitle = title.toLowerCase();
        String lowerAbstract = abstractText != null ? abstractText.toLowerCase() : "";

        String[] queryWords = lowerQuery.split("\\s+");
        int matches = 0;
        for (String word : queryWords) {
            if (word.length() > 2) {
                if (lowerTitle.contains(word)) matches += 2;
                if (lowerAbstract.contains(word)) matches++;
            }
        }
        score += Math.min(0.3, matches * 0.05);

        // 인용 수 기반 보너스 (많이 인용된 논문은 더 신뢰할 수 있음)
        if (citationCount > 100) score += 0.1;
        else if (citationCount > 50) score += 0.07;
        else if (citationCount > 10) score += 0.05;

        // 영향력 있는 인용 보너스
        if (influentialCount > 10) score += 0.1;
        else if (influentialCount > 5) score += 0.05;

        return Math.min(1.0, Math.max(0.3, score));
    }

    private String truncate(String text, int maxLength) {
        if (text == null) return "";
        if (text.length() <= maxLength) return text;
        return text.substring(0, maxLength) + "...";
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/factcheck/StatisticalWeightCalculator.java

```java
package com.newsinsight.collector.service.factcheck;

import com.newsinsight.collector.service.FactVerificationService.SourceEvidence;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Component;

import java.time.Duration;
import java.time.LocalDateTime;
import java.time.ZoneId;
import java.time.format.DateTimeFormatter;
import java.time.format.DateTimeParseException;
import java.util.*;
import java.util.stream.Collectors;

/**
 * 통계적 가중치 계산기
 * 
 * 수집된 데이터의 특성(시의성, 품질, 다양성 등)을 분석하여
 * 각 소스의 동적 가중치를 계산합니다.
 * 
 * 주요 기능:
 * 1. 시의성(Recency) 분석 - 최신 데이터일수록 높은 점수
 * 2. 품질(Quality) 분석 - 관련성 점수, 내용 길이 등
 * 3. 다양성(Diversity) 분석 - 여러 소스에서 교차 검증
 * 4. 동적 가중치 조정 - 데이터 특성에 따라 실시간 조정
 */
@Component
@Slf4j
public class StatisticalWeightCalculator {

    // 기본 소스 가중치 (베이스라인)
    private static final Map<String, Double> BASE_WEIGHTS = Map.ofEntries(
            Map.entry("realtime_search", 1.2),      // 실시간 검색 (Perplexity)
            Map.entry("naver_news", 1.1),           // 뉴스 (시의성 중요)
            Map.entry("wikipedia", 1.0),            // 백과사전 (기본)
            Map.entry("academic", 1.3),             // 학술 자료 (신뢰도 높음)
            Map.entry("semantic_scholar", 1.3),     // 학술 검색
            Map.entry("crossref", 1.25),            // 학술 DB
            Map.entry("pubmed", 1.3),               // 의학 논문
            Map.entry("openalex", 1.25),            // 학술 DB
            Map.entry("core", 1.2),                 // 학술 자료
            Map.entry("google_factcheck", 1.15),    // 팩트체크
            Map.entry("news", 1.1)                  // 일반 뉴스
    );

    // 시의성 가중치 (시간에 따른 감쇠)
    private static final double RECENCY_WEIGHT = 0.3;
    // 품질 가중치
    private static final double QUALITY_WEIGHT = 0.4;
    // 다양성 가중치
    private static final double DIVERSITY_WEIGHT = 0.3;

    /**
     * 소스별 증거 리스트를 분석하여 동적 가중치 맵 생성
     * 
     * @param evidencesBySource 소스별로 그룹화된 증거 목록
     * @return 소스별 동적 가중치 맵
     */
    public Map<String, Double> calculateSourceWeights(Map<String, List<SourceEvidence>> evidencesBySource) {
        if (evidencesBySource == null || evidencesBySource.isEmpty()) {
            return new HashMap<>(BASE_WEIGHTS);
        }

        Map<String, Double> weights = new HashMap<>();
        
        // 1. 각 소스의 시의성, 품질, 다양성 점수 계산
        Map<String, SourceMetrics> metricsMap = new HashMap<>();
        
        for (Map.Entry<String, List<SourceEvidence>> entry : evidencesBySource.entrySet()) {
            String sourceType = entry.getKey();
            List<SourceEvidence> evidences = entry.getValue();
            
            if (evidences == null || evidences.isEmpty()) {
                continue;
            }
            
            SourceMetrics metrics = calculateSourceMetrics(evidences);
            metricsMap.put(sourceType, metrics);
        }
        
        // 2. 전체 평균 계산 (정규화용)
        double avgRecency = metricsMap.values().stream()
                .mapToDouble(SourceMetrics::getRecencyScore)
                .average()
                .orElse(0.5);
        
        double avgQuality = metricsMap.values().stream()
                .mapToDouble(SourceMetrics::getQualityScore)
                .average()
                .orElse(0.5);
        
        // 3. 각 소스의 최종 가중치 계산
        for (Map.Entry<String, SourceMetrics> entry : metricsMap.entrySet()) {
            String sourceType = entry.getKey();
            SourceMetrics metrics = entry.getValue();
            
            // 기본 가중치
            double baseWeight = BASE_WEIGHTS.getOrDefault(sourceType, 1.0);
            
            // 시의성 보너스/페널티 (평균 대비)
            double recencyFactor = 1.0;
            if (avgRecency > 0) {
                recencyFactor = 1.0 + (metrics.getRecencyScore() - avgRecency) * RECENCY_WEIGHT;
            }
            
            // 품질 보너스/페널티
            double qualityFactor = 1.0;
            if (avgQuality > 0) {
                qualityFactor = 1.0 + (metrics.getQualityScore() - avgQuality) * QUALITY_WEIGHT;
            }
            
            // 다양성 보너스 (여러 소스에서 교차 검증된 경우)
            double diversityFactor = 1.0 + metrics.getDiversityScore() * DIVERSITY_WEIGHT;
            
            // 최종 가중치 = 기본 가중치 × 시의성 × 품질 × 다양성
            double finalWeight = baseWeight * recencyFactor * qualityFactor * diversityFactor;
            
            // 가중치 범위 제한 (0.5 ~ 2.0)
            finalWeight = Math.max(0.5, Math.min(2.0, finalWeight));
            
            weights.put(sourceType, finalWeight);
            
            log.debug("Source '{}': base={}, recency={}, quality={}, diversity={} → final={}",
                    sourceType, baseWeight, recencyFactor, qualityFactor, diversityFactor, finalWeight);
        }
        
        // 4. 가중치가 없는 소스는 기본값 사용
        for (String sourceType : evidencesBySource.keySet()) {
            if (!weights.containsKey(sourceType)) {
                weights.put(sourceType, BASE_WEIGHTS.getOrDefault(sourceType, 1.0));
            }
        }
        
        log.info("Calculated dynamic weights for {} sources: {}", weights.size(), weights);
        
        return weights;
    }

    /**
     * 소스별 메트릭 계산
     */
    private SourceMetrics calculateSourceMetrics(List<SourceEvidence> evidences) {
        double recencyScore = calculateRecencyScore(evidences);
        double qualityScore = calculateQualityScore(evidences);
        double diversityScore = calculateDiversityScore(evidences);
        
        return new SourceMetrics(recencyScore, qualityScore, diversityScore);
    }

    /**
     * 시의성 점수 계산
     * 
     * 최근 데이터일수록 높은 점수 (시간 감쇠 함수 사용)
     */
    private double calculateRecencyScore(List<SourceEvidence> evidences) {
        if (evidences == null || evidences.isEmpty()) {
            return 0.0;
        }

        LocalDateTime now = LocalDateTime.now();
        List<Double> scores = new ArrayList<>();
        
        for (SourceEvidence evidence : evidences) {
            // 증거에서 날짜 정보 추출 시도
            LocalDateTime publishedDate = extractPublishedDate(evidence);
            
            if (publishedDate != null) {
                // 경과 시간 계산 (시간 단위)
                long hoursAgo = Duration.between(publishedDate, now).toHours();
                
                // 시간 감쇠 함수: 1 / (1 + hours/24)
                // 24시간 이내: 0.5~1.0, 1주일: ~0.2, 1개월: ~0.1
                double score = 1.0 / (1.0 + hoursAgo / 24.0);
                scores.add(score);
            }
        }
        
        // 평균 시의성 점수
        return scores.isEmpty() ? 0.5 : scores.stream()
                .mapToDouble(Double::doubleValue)
                .average()
                .orElse(0.5);
    }

    /**
     * 품질 점수 계산
     * 
     * 관련성 점수, 내용 길이, 출처 신뢰도 등을 종합
     */
    private double calculateQualityScore(List<SourceEvidence> evidences) {
        if (evidences == null || evidences.isEmpty()) {
            return 0.0;
        }

        return evidences.stream()
                .mapToDouble(evidence -> {
                    double score = 0.0;
                    
                    // 1. 관련성 점수 (가장 중요)
                    if (evidence.getRelevanceScore() != null) {
                        score += evidence.getRelevanceScore() * 0.6;
                    }
                    
                    // 2. 내용 길이 (적절한 길이일수록 높은 점수)
                    if (evidence.getExcerpt() != null) {
                        int length = evidence.getExcerpt().length();
                        // 100~1000자 사이가 이상적
                        if (length >= 100 && length <= 1000) {
                            score += 0.2;
                        } else if (length > 50 && length < 2000) {
                            score += 0.1;
                        }
                    }
                    
                    // 3. URL 존재 여부 (출처 확인 가능)
                    if (evidence.getUrl() != null && !evidence.getUrl().isBlank()) {
                        score += 0.1;
                    }
                    
                    // 4. 소스 이름 존재 여부
                    if (evidence.getSourceName() != null && !evidence.getSourceName().isBlank()) {
                        score += 0.1;
                    }
                    
                    return Math.min(1.0, score);
                })
                .average()
                .orElse(0.5);
    }

    /**
     * 다양성 점수 계산
     * 
     * 여러 소스에서 유사한 정보가 교차 검증되는 경우 높은 점수
     */
    private double calculateDiversityScore(List<SourceEvidence> evidences) {
        if (evidences == null || evidences.size() <= 1) {
            return 0.0;
        }

        // URL 중복도 체크
        Set<String> uniqueUrls = evidences.stream()
                .map(SourceEvidence::getUrl)
                .filter(url -> url != null && !url.isBlank())
                .collect(Collectors.toSet());
        
        // 고유 URL 비율 (중복이 적을수록 다양성 높음)
        double urlDiversity = uniqueUrls.isEmpty() ? 0.0 : 
                (double) uniqueUrls.size() / evidences.size();
        
        // 다양성 점수: 0.0 ~ 1.0
        return Math.min(1.0, urlDiversity);
    }

    /**
     * 증거에서 발행 날짜 추출
     * 
     * excerpt나 sourceName에서 날짜 정보를 파싱 시도
     */
    private LocalDateTime extractPublishedDate(SourceEvidence evidence) {
        if (evidence == null) {
            return null;
        }

        // 1. excerpt에서 날짜 패턴 찾기
        String excerpt = evidence.getExcerpt();
        if (excerpt != null) {
            // "2024-12-22", "2024년 12월 22일" 등의 패턴
            LocalDateTime date = tryParseDateFromText(excerpt);
            if (date != null) {
                return date;
            }
        }

        // 2. 실시간 검색 결과는 현재 시간으로 간주
        if ("realtime_search".equals(evidence.getSourceType()) || 
            "realtime_search_citation".equals(evidence.getSourceType())) {
            return LocalDateTime.now();
        }

        // 3. 뉴스는 최근 데이터로 간주 (1일 전)
        if ("news".equals(evidence.getSourceType()) || 
            evidence.getSourceName() != null && evidence.getSourceName().contains("뉴스")) {
            return LocalDateTime.now().minusDays(1);
        }

        // 4. 학술 자료는 오래된 데이터로 간주 (1년 전)
        if ("academic".equals(evidence.getSourceType())) {
            return LocalDateTime.now().minusYears(1);
        }

        // 5. 기본값: 1주일 전
        return LocalDateTime.now().minusWeeks(1);
    }

    /**
     * 텍스트에서 날짜 파싱 시도
     */
    private LocalDateTime tryParseDateFromText(String text) {
        if (text == null || text.isBlank()) {
            return null;
        }

        // ISO 날짜 형식: 2024-12-22
        try {
            if (text.matches(".*\\d{4}-\\d{2}-\\d{2}.*")) {
                String dateStr = text.replaceAll(".*(\\d{4}-\\d{2}-\\d{2}).*", "$1");
                return LocalDateTime.parse(dateStr + "T00:00:00");
            }
        } catch (DateTimeParseException ignored) {}

        // 한국어 날짜 형식: 2024년 12월 22일
        try {
            if (text.matches(".*\\d{4}년\\s*\\d{1,2}월\\s*\\d{1,2}일.*")) {
                String dateStr = text.replaceAll(".*(\\d{4})년\\s*(\\d{1,2})월\\s*(\\d{1,2})일.*", "$1-$2-$3");
                return LocalDateTime.parse(dateStr + "T00:00:00");
            }
        } catch (DateTimeParseException ignored) {}

        return null;
    }

    /**
     * 소스 메트릭 내부 클래스
     */
    private static class SourceMetrics {
        private final double recencyScore;
        private final double qualityScore;
        private final double diversityScore;

        public SourceMetrics(double recencyScore, double qualityScore, double diversityScore) {
            this.recencyScore = recencyScore;
            this.qualityScore = qualityScore;
            this.diversityScore = diversityScore;
        }

        public double getRecencyScore() {
            return recencyScore;
        }

        public double getQualityScore() {
            return qualityScore;
        }

        public double getDiversityScore() {
            return diversityScore;
        }
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/factcheck/WikipediaSource.java

```java
package com.newsinsight.collector.service.factcheck;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.newsinsight.collector.config.TrustScoreConfig;
import com.newsinsight.collector.service.FactVerificationService.SourceEvidence;
import com.newsinsight.collector.service.RateLimitRetryService;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.MediaType;
import org.springframework.stereotype.Component;
import org.springframework.web.reactive.function.client.WebClient;
import org.springframework.web.reactive.function.client.WebClientResponseException;
import reactor.core.publisher.Flux;

import java.net.URLEncoder;
import java.nio.charset.StandardCharsets;
import java.time.Duration;
import java.util.ArrayList;
import java.util.List;

/**
 * Wikipedia 기반 팩트체크 소스.
 *
 * 간단한 제목/주제 검색으로 ko/en 위키백과 요약을 가져와
 * SourceEvidence 형태로 반환합니다.
 * 
 * 403/429 에러 발생 시 IP rotation을 통해 재시도합니다.
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class WikipediaSource implements FactCheckSource {

    private final WebClient webClient;
    private final ObjectMapper objectMapper;
    private final TrustScoreConfig trustScoreConfig;
    private final RateLimitRetryService rateLimitRetryService;

    @Value("${collector.fact-check.wikipedia.enabled:true}")
    private boolean enabled;

    @Value("${collector.fact-check.timeout-seconds:15}")
    private int timeoutSeconds;

    @Override
    public String getSourceId() {
        return "wikipedia_api";
    }

    @Override
    public String getSourceName() {
        return "Wikipedia";
    }

    @Override
    public double getTrustScore() {
        return trustScoreConfig.getFactCheck().getWikipedia();
    }

    @Override
    public SourceType getSourceType() {
        return SourceType.ENCYCLOPEDIA;
    }

    @Override
    public boolean isAvailable() {
        return enabled;
    }

    @Override
    public Flux<SourceEvidence> fetchEvidence(String topic, String language) {
        if (!enabled || topic == null || topic.isBlank()) {
            return Flux.empty();
        }

        return Flux.defer(() -> {
            List<SourceEvidence> evidenceList = new ArrayList<>();

            // 언어 코드에 따라 우선순위 결정 (ko 우선, 그 다음 en)
            String primaryLang = mapLanguage(language);

            try {
                String summary = fetchWikipediaSummary(topic, primaryLang);
                if (summary != null && !summary.isBlank()) {
                    evidenceList.add(SourceEvidence.builder()
                            .sourceType("wikipedia")
                            .sourceName(primaryLang.equals("ko") ? "위키백과" : "Wikipedia")
                            .url(String.format("https://%s.wikipedia.org/wiki/%s", primaryLang,
                                    URLEncoder.encode(topic.replace(" ", "_"), StandardCharsets.UTF_8)))
                            .excerpt(truncate(summary, 500))
                            .relevanceScore(0.9)
                            .stance("neutral")
                            .build());
                }
            } catch (Exception e) {
                log.debug("Failed to fetch Wikipedia summary for topic '{}' ({}): {}", topic, primaryLang, e.getMessage());
            }

            // 보조 언어(en)도 시도 (primary가 ko인 경우)
            if ("ko".equals(primaryLang)) {
                try {
                    String enSummary = fetchWikipediaSummary(topic, "en");
                    if (enSummary != null && !enSummary.isBlank()) {
                        evidenceList.add(SourceEvidence.builder()
                                .sourceType("wikipedia")
                                .sourceName("Wikipedia (EN)")
                                .url(String.format("https://en.wikipedia.org/wiki/%s",
                                        URLEncoder.encode(topic.replace(" ", "_"), StandardCharsets.UTF_8)))
                                .excerpt(truncate(enSummary, 500))
                                .relevanceScore(0.9)
                                .stance("neutral")
                                .build());
                    }
                } catch (Exception e) {
                    log.debug("Failed to fetch English Wikipedia summary for topic '{}': {}", topic, e.getMessage());
                }
            }

            return Flux.fromIterable(evidenceList);
        });
    }

    @Override
    public Flux<SourceEvidence> verifyClaimAgainstSource(String claim, String language) {
        // 간단히 claim 전체를 주제로 보고 fetchEvidence 재사용
        return fetchEvidence(claim, language);
    }

    private String mapLanguage(String language) {
        if (language == null || language.isBlank()) {
            return "ko";
        }
        String lower = language.toLowerCase();
        if (lower.startsWith("en")) return "en";
        if (lower.startsWith("ko")) return "ko";
        return "ko";
    }

    private String fetchWikipediaSummary(String topic, String lang) {
        String apiUrl = String.format(
                "https://%s.wikipedia.org/api/rest_v1/page/summary/%s",
                lang,
                URLEncoder.encode(topic.replace(" ", "_"), StandardCharsets.UTF_8)
        );

        try {
            // 1차 시도: 일반 요청
            String response = webClient.get()
                    .uri(apiUrl)
                    .accept(MediaType.APPLICATION_JSON)
                    .retrieve()
                    .bodyToMono(String.class)
                    .timeout(Duration.ofSeconds(timeoutSeconds))
                    .block();

            return extractSummary(response);
        } catch (Exception e) {
            // 403 또는 429 에러인 경우 프록시를 통해 재시도
            if (isRateLimitError(e)) {
                log.info("Rate limit/forbidden hit for Wikipedia ({}), attempting proxy retry", lang);
                return retryWithProxy(apiUrl, topic, lang);
            }
            log.debug("Wikipedia API call failed for topic '{}' ({}): {}", topic, lang, e.getMessage());
            return null;
        }
    }

    /**
     * 프록시를 통한 재시도
     */
    private String retryWithProxy(String apiUrl, String topic, String lang) {
        try {
            String response = rateLimitRetryService.executeWithRetryBlocking(
                    apiUrl, 
                    "Accept", "application/json",
                    "User-Agent", "NewsInsight/1.0 (https://newsinsight.com; contact@newsinsight.com)"
            );
            
            if (response != null) {
                log.info("Wikipedia proxy retry succeeded for topic '{}' ({})", topic, lang);
                return extractSummary(response);
            }
        } catch (Exception e) {
            log.warn("Wikipedia proxy retry failed for topic '{}' ({}): {}", topic, lang, e.getMessage());
        }
        return null;
    }

    /**
     * 응답에서 요약 추출
     */
    private String extractSummary(String response) {
        if (response == null || response.isBlank()) {
            return null;
        }
        try {
            JsonNode node = objectMapper.readTree(response);
            if (node.has("extract")) {
                return node.get("extract").asText();
            }
        } catch (Exception e) {
            log.debug("Failed to parse Wikipedia response: {}", e.getMessage());
        }
        return null;
    }

    /**
     * Rate limit 또는 forbidden 에러인지 확인
     */
    private boolean isRateLimitError(Throwable e) {
        if (e instanceof WebClientResponseException wce) {
            int statusCode = wce.getStatusCode().value();
            return statusCode == 429 || statusCode == 403;
        }
        String message = e.getMessage();
        if (message != null) {
            message = message.toLowerCase();
            return message.contains("429") || 
                   message.contains("403") || 
                   message.contains("too many requests") ||
                   message.contains("forbidden") ||
                   message.contains("rate limit");
        }
        return false;
    }

    private String truncate(String text, int maxLength) {
        if (text == null) return "";
        if (text.length() <= maxLength) return text;
        return text.substring(0, maxLength) + "...";
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/report/ChartGenerationService.java

```java
package com.newsinsight.collector.service.report;

import com.newsinsight.collector.dto.report.ChartData;
import lombok.extern.slf4j.Slf4j;
import org.jfree.chart.ChartFactory;
import org.jfree.chart.ChartUtils;
import org.jfree.chart.JFreeChart;
import org.jfree.chart.plot.CategoryPlot;
import org.jfree.chart.plot.PiePlot;
import org.jfree.chart.plot.PlotOrientation;
import org.jfree.chart.plot.XYPlot;
import org.jfree.chart.renderer.category.BarRenderer;
import org.jfree.chart.renderer.xy.XYLineAndShapeRenderer;
import org.jfree.chart.title.TextTitle;
import org.jfree.data.category.DefaultCategoryDataset;
import org.jfree.data.general.DefaultPieDataset;
import org.jfree.data.xy.XYSeries;
import org.jfree.data.xy.XYSeriesCollection;
import org.springframework.stereotype.Service;

import java.awt.*;
import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.util.List;

/**
 * 서버 사이드 차트 생성 서비스
 * 
 * JFreeChart를 사용하여 PDF에 삽입할 차트 이미지를 생성합니다.
 */
@Service
@Slf4j
public class ChartGenerationService {

    // 색상 팔레트
    private static final Color[] DEFAULT_COLORS = {
            new Color(59, 130, 246),   // Blue
            new Color(16, 185, 129),   // Green
            new Color(245, 158, 11),   // Yellow/Orange
            new Color(239, 68, 68),    // Red
            new Color(139, 92, 246),   // Purple
            new Color(236, 72, 153),   // Pink
            new Color(20, 184, 166),   // Teal
            new Color(249, 115, 22),   // Orange
    };

    private static final Color BACKGROUND_COLOR = Color.WHITE;
    private static final Color TEXT_COLOR = new Color(30, 41, 59);
    private static final Color GRID_COLOR = new Color(226, 232, 240);

    /**
     * 파이 차트 생성
     */
    public byte[] generatePieChart(ChartData chartData) throws IOException {
        DefaultPieDataset<String> dataset = new DefaultPieDataset<>();
        
        List<String> labels = chartData.getLabels();
        List<Number> values = chartData.getValues();
        
        for (int i = 0; i < labels.size(); i++) {
            dataset.setValue(labels.get(i), values.get(i));
        }
        
        JFreeChart chart = ChartFactory.createPieChart(
                chartData.getTitle(),
                dataset,
                true,   // legend
                true,   // tooltips
                false   // urls
        );
        
        // 스타일링
        chart.setBackgroundPaint(BACKGROUND_COLOR);
        chart.getTitle().setPaint(TEXT_COLOR);
        chart.getTitle().setFont(new Font("SansSerif", Font.BOLD, 16));
        
        PiePlot plot = (PiePlot) chart.getPlot();
        plot.setBackgroundPaint(BACKGROUND_COLOR);
        plot.setOutlineVisible(false);
        plot.setShadowPaint(null);
        plot.setLabelFont(new Font("SansSerif", Font.PLAIN, 12));
        plot.setLabelPaint(TEXT_COLOR);
        
        // 색상 적용
        List<String> colors = chartData.getColors();
        for (int i = 0; i < labels.size(); i++) {
            Color color = colors != null && i < colors.size() 
                    ? Color.decode(colors.get(i)) 
                    : DEFAULT_COLORS[i % DEFAULT_COLORS.length];
            plot.setSectionPaint(labels.get(i), color);
        }
        
        return chartToBytes(chart, chartData.getWidth(), chartData.getHeight());
    }

    /**
     * 도넛 차트 생성
     */
    public byte[] generateDoughnutChart(ChartData chartData) throws IOException {
        // JFreeChart에서 도넛 차트는 RingPlot 사용 (파이 차트와 유사하게 처리)
        return generatePieChart(chartData);  // 간단히 파이 차트로 대체
    }

    /**
     * 바 차트 생성
     */
    public byte[] generateBarChart(ChartData chartData) throws IOException {
        DefaultCategoryDataset dataset = new DefaultCategoryDataset();
        
        List<String> labels = chartData.getLabels();
        List<Number> values = chartData.getValues();
        
        for (int i = 0; i < labels.size(); i++) {
            dataset.addValue(values.get(i), "데이터", labels.get(i));
        }
        
        JFreeChart chart = ChartFactory.createBarChart(
                chartData.getTitle(),
                chartData.getXAxisLabel(),
                chartData.getYAxisLabel(),
                dataset,
                PlotOrientation.VERTICAL,
                false,  // legend
                true,   // tooltips
                false   // urls
        );
        
        // 스타일링
        chart.setBackgroundPaint(BACKGROUND_COLOR);
        chart.getTitle().setPaint(TEXT_COLOR);
        chart.getTitle().setFont(new Font("SansSerif", Font.BOLD, 16));
        
        CategoryPlot plot = chart.getCategoryPlot();
        plot.setBackgroundPaint(BACKGROUND_COLOR);
        plot.setRangeGridlinePaint(GRID_COLOR);
        plot.setOutlineVisible(false);
        
        BarRenderer renderer = (BarRenderer) plot.getRenderer();
        renderer.setSeriesPaint(0, DEFAULT_COLORS[0]);
        renderer.setDrawBarOutline(false);
        renderer.setShadowVisible(false);
        
        return chartToBytes(chart, chartData.getWidth(), chartData.getHeight());
    }

    /**
     * 수평 바 차트 생성
     */
    public byte[] generateHorizontalBarChart(ChartData chartData) throws IOException {
        DefaultCategoryDataset dataset = new DefaultCategoryDataset();
        
        List<String> labels = chartData.getLabels();
        List<Number> values = chartData.getValues();
        
        for (int i = 0; i < labels.size(); i++) {
            dataset.addValue(values.get(i), "데이터", labels.get(i));
        }
        
        JFreeChart chart = ChartFactory.createBarChart(
                chartData.getTitle(),
                chartData.getXAxisLabel(),
                chartData.getYAxisLabel(),
                dataset,
                PlotOrientation.HORIZONTAL,
                false,
                true,
                false
        );
        
        // 스타일링
        chart.setBackgroundPaint(BACKGROUND_COLOR);
        chart.getTitle().setPaint(TEXT_COLOR);
        
        CategoryPlot plot = chart.getCategoryPlot();
        plot.setBackgroundPaint(BACKGROUND_COLOR);
        plot.setRangeGridlinePaint(GRID_COLOR);
        
        BarRenderer renderer = (BarRenderer) plot.getRenderer();
        renderer.setSeriesPaint(0, DEFAULT_COLORS[0]);
        renderer.setDrawBarOutline(false);
        
        return chartToBytes(chart, chartData.getWidth(), chartData.getHeight());
    }

    /**
     * 라인 차트 생성
     */
    public byte[] generateLineChart(ChartData chartData) throws IOException {
        XYSeriesCollection dataset = new XYSeriesCollection();
        
        List<ChartData.DataSeries> seriesList = chartData.getSeries();
        if (seriesList != null) {
            for (ChartData.DataSeries seriesData : seriesList) {
                XYSeries series = new XYSeries(seriesData.getName());
                List<Number> data = seriesData.getData();
                for (int i = 0; i < data.size(); i++) {
                    series.add(i, data.get(i));
                }
                dataset.addSeries(series);
            }
        } else if (chartData.getValues() != null) {
            XYSeries series = new XYSeries("데이터");
            List<Number> values = chartData.getValues();
            for (int i = 0; i < values.size(); i++) {
                series.add(i, values.get(i));
            }
            dataset.addSeries(series);
        }
        
        JFreeChart chart = ChartFactory.createXYLineChart(
                chartData.getTitle(),
                chartData.getXAxisLabel(),
                chartData.getYAxisLabel(),
                dataset,
                PlotOrientation.VERTICAL,
                true,
                true,
                false
        );
        
        // 스타일링
        chart.setBackgroundPaint(BACKGROUND_COLOR);
        chart.getTitle().setPaint(TEXT_COLOR);
        chart.getTitle().setFont(new Font("SansSerif", Font.BOLD, 16));
        
        XYPlot plot = chart.getXYPlot();
        plot.setBackgroundPaint(BACKGROUND_COLOR);
        plot.setRangeGridlinePaint(GRID_COLOR);
        plot.setDomainGridlinePaint(GRID_COLOR);
        plot.setOutlineVisible(false);
        
        XYLineAndShapeRenderer renderer = new XYLineAndShapeRenderer();
        for (int i = 0; i < dataset.getSeriesCount(); i++) {
            renderer.setSeriesPaint(i, DEFAULT_COLORS[i % DEFAULT_COLORS.length]);
            renderer.setSeriesStroke(i, new BasicStroke(2.0f));
            renderer.setSeriesShapesVisible(i, true);
        }
        plot.setRenderer(renderer);
        
        return chartToBytes(chart, chartData.getWidth(), chartData.getHeight());
    }

    /**
     * 영역 차트 생성
     */
    public byte[] generateAreaChart(ChartData chartData) throws IOException {
        XYSeriesCollection dataset = new XYSeriesCollection();
        
        if (chartData.getValues() != null) {
            XYSeries series = new XYSeries("데이터");
            List<Number> values = chartData.getValues();
            for (int i = 0; i < values.size(); i++) {
                series.add(i, values.get(i));
            }
            dataset.addSeries(series);
        }
        
        JFreeChart chart = ChartFactory.createXYAreaChart(
                chartData.getTitle(),
                chartData.getXAxisLabel(),
                chartData.getYAxisLabel(),
                dataset,
                PlotOrientation.VERTICAL,
                false,
                true,
                false
        );
        
        // 스타일링
        chart.setBackgroundPaint(BACKGROUND_COLOR);
        chart.getTitle().setPaint(TEXT_COLOR);
        
        XYPlot plot = chart.getXYPlot();
        plot.setBackgroundPaint(BACKGROUND_COLOR);
        plot.setForegroundAlpha(0.65f);
        plot.getRenderer().setSeriesPaint(0, DEFAULT_COLORS[0]);
        
        return chartToBytes(chart, chartData.getWidth(), chartData.getHeight());
    }

    /**
     * 게이지 차트 생성 (간단한 미터 형태)
     */
    public byte[] generateGaugeChart(ChartData chartData) throws IOException {
        // JFreeChart에는 기본 게이지 차트가 없으므로 파이 차트로 시뮬레이션
        List<Number> values = chartData.getValues();
        double value = values.get(0).doubleValue();
        double min = values.size() > 1 ? values.get(1).doubleValue() : 0;
        double max = values.size() > 2 ? values.get(2).doubleValue() : 100;
        
        double percentage = ((value - min) / (max - min)) * 100;
        double remaining = 100 - percentage;
        
        DefaultPieDataset<String> dataset = new DefaultPieDataset<>();
        dataset.setValue("값", percentage);
        dataset.setValue("남은", remaining);
        
        JFreeChart chart = ChartFactory.createPieChart(
                chartData.getTitle(),
                dataset,
                false,
                true,
                false
        );
        
        // 스타일링
        chart.setBackgroundPaint(BACKGROUND_COLOR);
        chart.getTitle().setPaint(TEXT_COLOR);
        
        // 값 표시 추가
        TextTitle valueTitle = new TextTitle(String.format("%.0f", value));
        valueTitle.setFont(new Font("SansSerif", Font.BOLD, 24));
        valueTitle.setPaint(DEFAULT_COLORS[0]);
        chart.addSubtitle(valueTitle);
        
        PiePlot plot = (PiePlot) chart.getPlot();
        plot.setBackgroundPaint(BACKGROUND_COLOR);
        plot.setOutlineVisible(false);
        plot.setShadowPaint(null);
        plot.setLabelGenerator(null);  // 라벨 숨김
        
        // 색상: 값은 파란색, 남은 부분은 연한 회색
        Color gaugeColor = percentage >= 70 ? new Color(16, 185, 129) :
                           percentage >= 40 ? new Color(245, 158, 11) :
                           new Color(239, 68, 68);
        plot.setSectionPaint("값", gaugeColor);
        plot.setSectionPaint("남은", new Color(226, 232, 240));
        
        return chartToBytes(chart, chartData.getWidth(), chartData.getHeight());
    }

    /**
     * ChartData 타입에 따라 적절한 차트 생성
     */
    public byte[] generateChart(ChartData chartData) throws IOException {
        return switch (chartData.getChartType()) {
            case PIE -> generatePieChart(chartData);
            case DOUGHNUT -> generateDoughnutChart(chartData);
            case BAR -> generateBarChart(chartData);
            case HORIZONTAL_BAR -> generateHorizontalBarChart(chartData);
            case LINE -> generateLineChart(chartData);
            case AREA -> generateAreaChart(chartData);
            case GAUGE -> generateGaugeChart(chartData);
            case STACKED_BAR -> generateBarChart(chartData);  // 간단히 바 차트로 대체
            case HISTOGRAM -> generateBarChart(chartData);    // 간단히 바 차트로 대체
            case RADAR -> generatePieChart(chartData);        // 레이더는 파이로 대체
        };
    }

    /**
     * JFreeChart를 PNG 바이트 배열로 변환
     */
    private byte[] chartToBytes(JFreeChart chart, int width, int height) throws IOException {
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        ChartUtils.writeChartAsPNG(baos, chart, width, height);
        return baos.toByteArray();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/report/PdfExportService.java

```java
package com.newsinsight.collector.service.report;

import com.itextpdf.io.font.PdfEncodings;
import com.itextpdf.io.image.ImageDataFactory;
import com.itextpdf.kernel.colors.ColorConstants;
import com.itextpdf.kernel.colors.DeviceRgb;
import com.itextpdf.kernel.font.PdfFont;
import com.itextpdf.kernel.font.PdfFontFactory;
import com.itextpdf.kernel.geom.PageSize;
import com.itextpdf.kernel.geom.Rectangle;
import com.itextpdf.kernel.pdf.PdfDocument;
import com.itextpdf.kernel.pdf.PdfPage;
import com.itextpdf.kernel.pdf.PdfWriter;
import com.itextpdf.layout.Document;
import com.itextpdf.layout.borders.Border;
import com.itextpdf.layout.borders.SolidBorder;
import com.itextpdf.layout.element.*;
import com.itextpdf.layout.properties.HorizontalAlignment;
import com.itextpdf.layout.properties.TextAlignment;
import com.itextpdf.layout.properties.UnitValue;
import com.itextpdf.layout.properties.VerticalAlignment;
import com.newsinsight.collector.dto.report.ChartData;
import com.newsinsight.collector.dto.report.ReportRequest;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;

import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.Base64;
import java.util.List;
import java.util.Map;

/**
 * PDF 생성 엔진 서비스
 * 
 * iText 7을 사용하여 PDF 문서를 생성합니다.
 * 한글 폰트 지원 및 차트 이미지 삽입 기능을 제공합니다.
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class PdfExportService {

    private final ChartGenerationService chartGenerationService;

    // 색상 상수
    private static final DeviceRgb PRIMARY_COLOR = new DeviceRgb(59, 130, 246);    // Blue
    private static final DeviceRgb SUCCESS_COLOR = new DeviceRgb(16, 185, 129);    // Green
    private static final DeviceRgb WARNING_COLOR = new DeviceRgb(245, 158, 11);    // Yellow
    private static final DeviceRgb DANGER_COLOR = new DeviceRgb(239, 68, 68);      // Red
    private static final DeviceRgb NEUTRAL_COLOR = new DeviceRgb(107, 114, 128);   // Gray
    private static final DeviceRgb LIGHT_BG = new DeviceRgb(248, 250, 252);        // Light Gray BG

    // 폰트 경로 (클래스패스 또는 시스템)
    private static final String FONT_REGULAR = "fonts/NotoSansKR-Regular.ttf";
    private static final String FONT_BOLD = "fonts/NotoSansKR-Bold.ttf";

    /**
     * 통합 검색 보고서 PDF 생성
     */
    public byte[] generateUnifiedSearchReport(
            String title,
            String query,
            String timeWindow,
            Map<String, Object> summaryData,
            List<Map<String, Object>> results,
            Map<String, String> chartImages,
            List<ReportRequest.ReportSection> sections
    ) throws IOException {
        
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        PdfWriter writer = new PdfWriter(baos);
        PdfDocument pdf = new PdfDocument(writer);
        Document document = new Document(pdf, PageSize.A4);
        
        try {
            // 폰트 설정
            PdfFont regularFont = loadFont(FONT_REGULAR);
            PdfFont boldFont = loadFont(FONT_BOLD);
            document.setFont(regularFont);
            
            // 여백 설정
            document.setMargins(50, 50, 50, 50);
            
            // 1. 표지
            if (sections.contains(ReportRequest.ReportSection.COVER)) {
                addCoverPage(document, boldFont, title, query, timeWindow);
            }
            
            // 2. 요약
            if (sections.contains(ReportRequest.ReportSection.EXECUTIVE_SUMMARY)) {
                addExecutiveSummary(document, boldFont, regularFont, summaryData);
            }
            
            // 3. 데이터 소스 분석
            if (sections.contains(ReportRequest.ReportSection.DATA_SOURCE)) {
                addDataSourceAnalysis(document, boldFont, regularFont, results, chartImages);
            }
            
            // 4. 키워드 분석
            if (sections.contains(ReportRequest.ReportSection.KEYWORD_ANALYSIS)) {
                addKeywordAnalysis(document, boldFont, regularFont, summaryData, chartImages);
            }
            
            // 5. 감정 분석
            if (sections.contains(ReportRequest.ReportSection.SENTIMENT_ANALYSIS)) {
                addSentimentAnalysis(document, boldFont, regularFont, summaryData, chartImages);
            }
            
            // 6. 신뢰도 분석
            if (sections.contains(ReportRequest.ReportSection.RELIABILITY)) {
                addReliabilityAnalysis(document, boldFont, regularFont, summaryData, chartImages);
            }
            
            // 7. 상세 결과
            if (sections.contains(ReportRequest.ReportSection.DETAILED_RESULTS)) {
                addDetailedResults(document, boldFont, regularFont, results);
            }
            
            // 페이지 번호 추가
            addPageNumbers(pdf, regularFont);
            
        } finally {
            document.close();
        }
        
        return baos.toByteArray();
    }

    /**
     * 표지 페이지 추가
     */
    private void addCoverPage(Document document, PdfFont boldFont, String title, String query, String timeWindow) {
        // 상단 여백
        document.add(new Paragraph("\n\n\n\n\n\n"));
        
        // 로고 영역 (텍스트로 대체)
        Paragraph logo = new Paragraph("NewsInsight")
                .setFont(boldFont)
                .setFontSize(36)
                .setFontColor(PRIMARY_COLOR)
                .setTextAlignment(TextAlignment.CENTER);
        document.add(logo);
        
        // 부제목
        Paragraph subtitle = new Paragraph("뉴스 분석 플랫폼")
                .setFontSize(14)
                .setFontColor(NEUTRAL_COLOR)
                .setTextAlignment(TextAlignment.CENTER)
                .setMarginBottom(60);
        document.add(subtitle);
        
        // 구분선
        document.add(createDivider());
        
        // 보고서 제목
        Paragraph reportTitle = new Paragraph(title)
                .setFont(boldFont)
                .setFontSize(24)
                .setTextAlignment(TextAlignment.CENTER)
                .setMarginTop(40)
                .setMarginBottom(20);
        document.add(reportTitle);
        
        // 검색 쿼리
        Paragraph queryPara = new Paragraph("검색어: " + query)
                .setFontSize(16)
                .setFontColor(NEUTRAL_COLOR)
                .setTextAlignment(TextAlignment.CENTER)
                .setMarginBottom(10);
        document.add(queryPara);
        
        // 기간
        Paragraph periodPara = new Paragraph("분석 기간: " + formatTimeWindow(timeWindow))
                .setFontSize(14)
                .setFontColor(NEUTRAL_COLOR)
                .setTextAlignment(TextAlignment.CENTER)
                .setMarginBottom(60);
        document.add(periodPara);
        
        // 구분선
        document.add(createDivider());
        
        // 생성 일시
        String dateStr = LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyy년 MM월 dd일 HH:mm"));
        Paragraph datePara = new Paragraph("생성 일시: " + dateStr)
                .setFontSize(12)
                .setFontColor(NEUTRAL_COLOR)
                .setTextAlignment(TextAlignment.CENTER)
                .setMarginTop(40);
        document.add(datePara);
        
        // 페이지 나누기
        document.add(new AreaBreak());
    }

    /**
     * 요약 섹션 추가
     */
    @SuppressWarnings("unchecked")
    private void addExecutiveSummary(Document document, PdfFont boldFont, PdfFont regularFont, 
                                     Map<String, Object> summaryData) {
        // 섹션 제목
        document.add(createSectionTitle("요약 (Executive Summary)", boldFont));
        
        // 주요 통계 테이블
        Table statsTable = new Table(UnitValue.createPercentArray(new float[]{1, 1, 1, 1}))
                .useAllAvailableWidth()
                .setMarginBottom(20);
        
        int totalResults = getIntValue(summaryData, "totalResults", 0);
        int dbResults = getIntValue(summaryData, "dbResults", 0);
        int webResults = getIntValue(summaryData, "webResults", 0);
        int aiResults = getIntValue(summaryData, "aiResults", 0);
        
        statsTable.addCell(createStatCell("총 결과", String.valueOf(totalResults), boldFont, regularFont, PRIMARY_COLOR));
        statsTable.addCell(createStatCell("DB 검색", String.valueOf(dbResults), boldFont, regularFont, SUCCESS_COLOR));
        statsTable.addCell(createStatCell("웹 크롤링", String.valueOf(webResults), boldFont, regularFont, WARNING_COLOR));
        statsTable.addCell(createStatCell("AI 분석", String.valueOf(aiResults), boldFont, regularFont, DANGER_COLOR));
        
        document.add(statsTable);
        
        // AI 요약
        String aiSummary = (String) summaryData.get("aiSummary");
        if (aiSummary != null && !aiSummary.isBlank()) {
            document.add(new Paragraph("AI 분석 요약")
                    .setFont(boldFont)
                    .setFontSize(14)
                    .setMarginTop(20)
                    .setMarginBottom(10));
            
            // 요약 박스
            Div summaryBox = new Div()
                    .setBackgroundColor(LIGHT_BG)
                    .setPadding(15)
                    .setBorder(new SolidBorder(new DeviceRgb(226, 232, 240), 1))
                    .setMarginBottom(20);
            
            summaryBox.add(new Paragraph(truncateText(aiSummary, 5000))
                    .setFontSize(11)
                    .setFontColor(new DeviceRgb(51, 65, 85)));
            
            document.add(summaryBox);
        }
        
        // 주요 발견 사항
        List<String> keyFindings = (List<String>) summaryData.get("keyFindings");
        if (keyFindings != null && !keyFindings.isEmpty()) {
            document.add(new Paragraph("주요 발견 사항")
                    .setFont(boldFont)
                    .setFontSize(14)
                    .setMarginTop(10)
                    .setMarginBottom(10));
            
            com.itextpdf.layout.element.List findingsList = new com.itextpdf.layout.element.List()
                    .setSymbolIndent(12)
                    .setListSymbol("•");
            
            for (String finding : keyFindings) {
                ListItem item = new ListItem(finding);
                item.setFontSize(11);
                findingsList.add(item);
            }
            
            document.add(findingsList);
        }
        
        document.add(new AreaBreak());
    }

    /**
     * 데이터 소스 분석 섹션 추가
     */
    private void addDataSourceAnalysis(Document document, PdfFont boldFont, PdfFont regularFont,
                                       List<Map<String, Object>> results, Map<String, String> chartImages) {
        document.add(createSectionTitle("데이터 소스 분석", boldFont));
        
        // 소스별 분포 계산
        Map<String, Long> sourceDistribution = calculateSourceDistribution(results);
        
        // 통계 테이블
        Table table = new Table(UnitValue.createPercentArray(new float[]{2, 1, 1}))
                .useAllAvailableWidth()
                .setMarginBottom(20);
        
        table.addHeaderCell(createHeaderCell("소스", boldFont));
        table.addHeaderCell(createHeaderCell("결과 수", boldFont));
        table.addHeaderCell(createHeaderCell("비율", boldFont));
        
        long total = sourceDistribution.values().stream().mapToLong(Long::longValue).sum();
        
        sourceDistribution.forEach((source, count) -> {
            double percentage = total > 0 ? (count * 100.0 / total) : 0;
            table.addCell(createDataCell(formatSourceName(source), regularFont));
            table.addCell(createDataCell(String.valueOf(count), regularFont));
            table.addCell(createDataCell(String.format("%.1f%%", percentage), regularFont));
        });
        
        document.add(table);
        
        // 차트 이미지 삽입
        if (chartImages != null && chartImages.containsKey("sourceDistribution")) {
            addChartImage(document, chartImages.get("sourceDistribution"), "소스별 결과 분포");
        } else {
            // 서버 사이드 차트 생성
            try {
                byte[] chartBytes = chartGenerationService.generatePieChart(
                        ChartData.pie("소스별 결과 분포",
                                sourceDistribution.keySet().stream().map(this::formatSourceName).toList(),
                                sourceDistribution.values().stream().map(v -> (Number) v).toList(),
                                List.of("#3b82f6", "#10b981", "#f59e0b", "#ef4444"))
                );
                addChartImage(document, chartBytes, null);
            } catch (Exception e) {
                log.warn("Failed to generate source distribution chart: {}", e.getMessage());
            }
        }
    }

    /**
     * 키워드 분석 섹션 추가
     */
    @SuppressWarnings("unchecked")
    private void addKeywordAnalysis(Document document, PdfFont boldFont, PdfFont regularFont,
                                    Map<String, Object> summaryData, Map<String, String> chartImages) {
        document.add(createSectionTitle("키워드 분석", boldFont));
        
        List<Map<String, Object>> keywords = (List<Map<String, Object>>) summaryData.get("keywords");
        
        if (keywords != null && !keywords.isEmpty()) {
            // 키워드 테이블
            Table table = new Table(UnitValue.createPercentArray(new float[]{1, 3, 1}))
                    .useAllAvailableWidth()
                    .setMarginBottom(20);
            
            table.addHeaderCell(createHeaderCell("순위", boldFont));
            table.addHeaderCell(createHeaderCell("키워드", boldFont));
            table.addHeaderCell(createHeaderCell("빈도", boldFont));
            
            int rank = 1;
            for (Map<String, Object> kw : keywords.subList(0, Math.min(15, keywords.size()))) {
                table.addCell(createDataCell(String.valueOf(rank++), regularFont));
                table.addCell(createDataCell((String) kw.get("word"), regularFont));
                table.addCell(createDataCell(String.valueOf(kw.get("count")), regularFont));
            }
            
            document.add(table);
        }
        
        // 차트 이미지 삽입
        if (chartImages != null && chartImages.containsKey("keywords")) {
            addChartImage(document, chartImages.get("keywords"), "상위 키워드 분포");
        }
        
        document.add(new AreaBreak());
    }

    /**
     * 감정 분석 섹션 추가
     */
    @SuppressWarnings("unchecked")
    private void addSentimentAnalysis(Document document, PdfFont boldFont, PdfFont regularFont,
                                      Map<String, Object> summaryData, Map<String, String> chartImages) {
        document.add(createSectionTitle("감정 분석", boldFont));
        
        Map<String, Object> sentiment = (Map<String, Object>) summaryData.get("sentiment");
        
        if (sentiment != null) {
            double positive = getDoubleValue(sentiment, "positive", 0);
            double neutral = getDoubleValue(sentiment, "neutral", 0);
            double negative = getDoubleValue(sentiment, "negative", 0);
            
            // 감정 통계 테이블
            Table statsTable = new Table(UnitValue.createPercentArray(new float[]{1, 1, 1}))
                    .useAllAvailableWidth()
                    .setMarginBottom(20);
            
            statsTable.addCell(createStatCell("긍정", String.format("%.1f%%", positive * 100), 
                    boldFont, regularFont, SUCCESS_COLOR));
            statsTable.addCell(createStatCell("중립", String.format("%.1f%%", neutral * 100), 
                    boldFont, regularFont, NEUTRAL_COLOR));
            statsTable.addCell(createStatCell("부정", String.format("%.1f%%", negative * 100), 
                    boldFont, regularFont, DANGER_COLOR));
            
            document.add(statsTable);
            
            // 분석 설명
            String dominantSentiment = positive > negative ? 
                    (positive > neutral ? "긍정적" : "중립적") : 
                    (negative > neutral ? "부정적" : "중립적");
            
            document.add(new Paragraph("분석 결과, 전체적으로 " + dominantSentiment + "인 톤이 우세합니다.")
                    .setFontSize(11)
                    .setFontColor(NEUTRAL_COLOR)
                    .setMarginBottom(20));
        }
        
        // 차트 이미지 삽입
        if (chartImages != null && chartImages.containsKey("sentiment")) {
            addChartImage(document, chartImages.get("sentiment"), "감정 분석 결과");
        }
    }

    /**
     * 신뢰도 분석 섹션 추가
     */
    @SuppressWarnings("unchecked")
    private void addReliabilityAnalysis(Document document, PdfFont boldFont, PdfFont regularFont,
                                        Map<String, Object> summaryData, Map<String, String> chartImages) {
        document.add(createSectionTitle("신뢰도 분석", boldFont));
        
        Map<String, Object> reliability = (Map<String, Object>) summaryData.get("reliability");
        
        if (reliability != null) {
            double avgScore = getDoubleValue(reliability, "averageScore", 0);
            String grade = (String) reliability.getOrDefault("grade", "N/A");
            
            // 신뢰도 점수 표시
            Div scoreBox = new Div()
                    .setBackgroundColor(getGradeColor(grade))
                    .setPadding(20)
                    .setMarginBottom(20)
                    .setTextAlignment(TextAlignment.CENTER);
            
            scoreBox.add(new Paragraph("평균 신뢰도 점수")
                    .setFont(boldFont)
                    .setFontSize(14)
                    .setFontColor(ColorConstants.WHITE));
            
            scoreBox.add(new Paragraph(String.format("%.0f / 100", avgScore))
                    .setFont(boldFont)
                    .setFontSize(36)
                    .setFontColor(ColorConstants.WHITE));
            
            scoreBox.add(new Paragraph("등급: " + grade)
                    .setFontSize(14)
                    .setFontColor(ColorConstants.WHITE));
            
            document.add(scoreBox);
        }
        
        // 차트 이미지 삽입
        if (chartImages != null && chartImages.containsKey("reliability")) {
            addChartImage(document, chartImages.get("reliability"), "신뢰도 분포");
        }
        
        document.add(new AreaBreak());
    }

    /**
     * 상세 결과 섹션 추가
     */
    private void addDetailedResults(Document document, PdfFont boldFont, PdfFont regularFont,
                                    List<Map<String, Object>> results) {
        document.add(createSectionTitle("상세 검색 결과", boldFont));
        
        if (results == null || results.isEmpty()) {
            document.add(new Paragraph("검색 결과가 없습니다.")
                    .setFontSize(11)
                    .setFontColor(NEUTRAL_COLOR));
            return;
        }
        
        int count = 0;
        for (Map<String, Object> result : results) {
            if (count >= 30) {  // 최대 30개 결과만 표시
                document.add(new Paragraph("... 외 " + (results.size() - 30) + "개 결과")
                        .setFontSize(11)
                        .setFontColor(NEUTRAL_COLOR)
                        .setMarginTop(10));
                break;
            }
            
            // 결과 박스
            Div resultBox = new Div()
                    .setBackgroundColor(LIGHT_BG)
                    .setPadding(12)
                    .setBorder(new SolidBorder(new DeviceRgb(226, 232, 240), 1))
                    .setMarginBottom(10);
            
            // 제목
            String title = (String) result.getOrDefault("title", "제목 없음");
            resultBox.add(new Paragraph(truncateText(title, 100))
                    .setFont(boldFont)
                    .setFontSize(12)
                    .setFontColor(PRIMARY_COLOR)
                    .setMarginBottom(5));
            
            // 출처 및 날짜
            String source = (String) result.getOrDefault("source", "");
            String publishedAt = (String) result.getOrDefault("publishedAt", "");
            resultBox.add(new Paragraph(source + " | " + publishedAt)
                    .setFontSize(10)
                    .setFontColor(NEUTRAL_COLOR)
                    .setMarginBottom(5));
            
            // 본문 내용 (content가 있으면 전체 사용, 없으면 snippet 사용)
            String content = (String) result.get("content");
            String snippet = (String) result.get("snippet");
            String displayContent = (content != null && !content.isBlank()) ? content : snippet;
            if (displayContent != null && !displayContent.isBlank()) {
                // PDF에서는 너무 긴 내용은 적절히 잘라서 표시 (최대 10000자)
                String truncatedContent = displayContent.length() > 10000 
                    ? displayContent.substring(0, 10000) + "..." 
                    : displayContent;
                resultBox.add(new Paragraph(truncatedContent)
                        .setFontSize(10)
                        .setFontColor(new DeviceRgb(71, 85, 105)));
            }
            
            // URL (출처 링크)
            String url = (String) result.get("url");
            if (url != null && !url.isBlank()) {
                resultBox.add(new Paragraph("출처: " + url)
                        .setFontSize(9)
                        .setFontColor(PRIMARY_COLOR)
                        .setMarginTop(5));
            }
            
            document.add(resultBox);
            count++;
        }
    }

    // ===== 헬퍼 메서드 =====

    private PdfFont loadFont(String fontPath) throws IOException {
        try {
            // 클래스패스에서 리소스 로드
            var resource = getClass().getClassLoader().getResourceAsStream(fontPath);
            if (resource == null) {
                log.warn("Font resource not found: {}, using default font", fontPath);
                return PdfFontFactory.createFont();
            }
            
            byte[] fontBytes = resource.readAllBytes();
            return PdfFontFactory.createFont(fontBytes, PdfEncodings.IDENTITY_H);
        } catch (Exception e) {
            log.error("Failed to load font from {}: {}", fontPath, e.getMessage(), e);
            // 기본 폰트 사용
            return PdfFontFactory.createFont();
        }
    }

    private Paragraph createSectionTitle(String title, PdfFont boldFont) {
        return new Paragraph(title)
                .setFont(boldFont)
                .setFontSize(18)
                .setFontColor(new DeviceRgb(30, 41, 59))
                .setMarginTop(20)
                .setMarginBottom(15)
                .setBorderBottom(new SolidBorder(PRIMARY_COLOR, 2))
                .setPaddingBottom(10);
    }

    private Div createDivider() {
        return new Div()
                .setHeight(1)
                .setBackgroundColor(new DeviceRgb(226, 232, 240))
                .setMarginTop(10)
                .setMarginBottom(10);
    }

    private Cell createStatCell(String label, String value, PdfFont boldFont, PdfFont regularFont, DeviceRgb color) {
        Cell cell = new Cell()
                .setBorder(Border.NO_BORDER)
                .setBackgroundColor(LIGHT_BG)
                .setPadding(15)
                .setTextAlignment(TextAlignment.CENTER);
        
        cell.add(new Paragraph(value)
                .setFont(boldFont)
                .setFontSize(24)
                .setFontColor(color));
        
        cell.add(new Paragraph(label)
                .setFont(regularFont)
                .setFontSize(11)
                .setFontColor(NEUTRAL_COLOR));
        
        return cell;
    }

    private Cell createHeaderCell(String text, PdfFont boldFont) {
        return new Cell()
                .add(new Paragraph(text).setFont(boldFont).setFontSize(11))
                .setBackgroundColor(new DeviceRgb(241, 245, 249))
                .setPadding(8)
                .setTextAlignment(TextAlignment.CENTER);
    }

    private Cell createDataCell(String text, PdfFont regularFont) {
        return new Cell()
                .add(new Paragraph(text != null ? text : "-").setFont(regularFont).setFontSize(10))
                .setPadding(8)
                .setTextAlignment(TextAlignment.CENTER);
    }

    private void addChartImage(Document document, String base64Image, String caption) {
        try {
            byte[] imageBytes = Base64.getDecoder().decode(
                    base64Image.contains(",") ? base64Image.split(",")[1] : base64Image
            );
            addChartImage(document, imageBytes, caption);
        } catch (Exception e) {
            log.warn("Failed to add chart image: {}", e.getMessage());
        }
    }

    private void addChartImage(Document document, byte[] imageBytes, String caption) {
        try {
            Image image = new Image(ImageDataFactory.create(imageBytes))
                    .setMaxWidth(450)
                    .setHorizontalAlignment(HorizontalAlignment.CENTER)
                    .setMarginTop(10)
                    .setMarginBottom(10);
            
            document.add(image);
            
            if (caption != null && !caption.isBlank()) {
                document.add(new Paragraph(caption)
                        .setFontSize(10)
                        .setFontColor(NEUTRAL_COLOR)
                        .setTextAlignment(TextAlignment.CENTER)
                        .setMarginBottom(15));
            }
        } catch (Exception e) {
            log.warn("Failed to add chart image: {}", e.getMessage());
        }
    }

    private void addPageNumbers(PdfDocument pdf, PdfFont font) {
        int numberOfPages = pdf.getNumberOfPages();
        if (numberOfPages <= 0) {
            log.warn("No pages in PDF document, skipping page numbers");
            return;
        }
        
        for (int i = 1; i <= numberOfPages; i++) {
            // 페이지 번호는 표지를 제외하고 시작
            if (i == 1) continue;
            
            try {
                PdfPage page = pdf.getPage(i);
                if (page == null) {
                    log.warn("Page {} is null, skipping page number", i);
                    continue;
                }
                
                // Get page size with null safety
                Rectangle pageSize = page.getPageSize();
                if (pageSize == null) {
                    log.warn("Page {} has no size defined, skipping page number", i);
                    continue;
                }
                
                Document doc = new Document(pdf);
                Paragraph pageNumber = new Paragraph(String.format("%d / %d", i, numberOfPages))
                        .setFont(font)
                        .setFontSize(10)
                        .setFontColor(NEUTRAL_COLOR);
                
                // 하단 중앙에 추가
                doc.showTextAligned(pageNumber,
                        pageSize.getWidth() / 2,
                        30,
                        i,
                        TextAlignment.CENTER,
                        VerticalAlignment.BOTTOM,
                        0);
            } catch (Exception e) {
                log.warn("Failed to add page number to page {}: {}", i, e.getMessage());
            }
        }
    }

    private Map<String, Long> calculateSourceDistribution(List<Map<String, Object>> results) {
        if (results == null) return Map.of();
        
        return results.stream()
                .map(r -> (String) r.getOrDefault("_source", r.getOrDefault("source", "unknown")))
                .collect(java.util.stream.Collectors.groupingBy(
                        s -> s,
                        java.util.stream.Collectors.counting()
                ));
    }

    private String formatSourceName(String source) {
        return switch (source.toLowerCase()) {
            case "database" -> "데이터베이스";
            case "web" -> "웹 크롤링";
            case "ai" -> "AI 분석";
            default -> source;
        };
    }

    private String formatTimeWindow(String window) {
        return switch (window) {
            case "1d" -> "최근 1일";
            case "7d" -> "최근 7일";
            case "30d" -> "최근 30일";
            case "90d" -> "최근 90일";
            case "1y" -> "최근 1년";
            default -> window;
        };
    }

    private DeviceRgb getGradeColor(String grade) {
        return switch (grade.toUpperCase()) {
            case "A", "HIGH" -> SUCCESS_COLOR;
            case "B", "MEDIUM" -> new DeviceRgb(34, 197, 94);
            case "C" -> WARNING_COLOR;
            case "D", "LOW" -> new DeviceRgb(249, 115, 22);
            case "F" -> DANGER_COLOR;
            default -> NEUTRAL_COLOR;
        };
    }

    private String truncateText(String text, int maxLength) {
        if (text == null) return "";
        return text.length() > maxLength ? text.substring(0, maxLength) + "..." : text;
    }

    private int getIntValue(Map<String, Object> map, String key, int defaultValue) {
        Object value = map.get(key);
        if (value instanceof Number) {
            return ((Number) value).intValue();
        }
        return defaultValue;
    }

    private double getDoubleValue(Map<String, Object> map, String key, double defaultValue) {
        Object value = map.get(key);
        if (value instanceof Number) {
            return ((Number) value).doubleValue();
        }
        return defaultValue;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/report/ReportGenerationService.java

```java
package com.newsinsight.collector.service.report;

import com.newsinsight.collector.dto.report.ReportMetadata;
import com.newsinsight.collector.dto.report.ReportRequest;
import com.newsinsight.collector.entity.CrawlEvidence;
import com.newsinsight.collector.entity.CrawlJob;
import com.newsinsight.collector.entity.search.SearchHistory;
import com.newsinsight.collector.repository.CrawlEvidenceRepository;
import com.newsinsight.collector.repository.CrawlJobRepository;
import com.newsinsight.collector.repository.SearchHistoryRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.scheduling.annotation.Async;
import org.springframework.stereotype.Service;

import java.io.IOException;
import java.time.LocalDateTime;
import java.util.*;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ConcurrentHashMap;

/**
 * 보고서 생성 오케스트레이터 서비스
 * 
 * 보고서 생성 요청을 관리하고, PDF 생성을 조정합니다.
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class ReportGenerationService {

    private final PdfExportService pdfExportService;
    private final SearchHistoryRepository searchHistoryRepository;
    private final CrawlJobRepository crawlJobRepository;
    private final CrawlEvidenceRepository crawlEvidenceRepository;
    
    // 생성된 보고서 캐시 (실제 운영에서는 Redis나 파일 시스템 사용)
    private final Map<String, byte[]> reportCache = new ConcurrentHashMap<>();
    private final Map<String, ReportMetadata> metadataCache = new ConcurrentHashMap<>();

    /**
     * 통합 검색 보고서 생성 요청
     */
    public ReportMetadata requestUnifiedSearchReport(String jobId, ReportRequest request) {
        String reportId = UUID.randomUUID().toString();
        
        ReportMetadata metadata = ReportMetadata.builder()
                .reportId(reportId)
                .reportType(ReportRequest.ReportType.UNIFIED_SEARCH)
                .targetId(jobId)
                .query(request.getQuery())
                .status(ReportMetadata.ReportStatus.PENDING)
                .createdAt(LocalDateTime.now())
                .expiresAt(LocalDateTime.now().plusDays(7))
                .build();
        
        metadataCache.put(reportId, metadata);
        
        // 비동기로 보고서 생성 시작
        generateReportAsync(reportId, jobId, request);
        
        return metadata;
    }

    /**
     * 비동기 보고서 생성
     */
    @Async
    public CompletableFuture<ReportMetadata> generateReportAsync(String reportId, String jobId, ReportRequest request) {
        long startTime = System.currentTimeMillis();
        
        try {
            // 상태 업데이트: 생성 중
            updateMetadataStatus(reportId, ReportMetadata.ReportStatus.GENERATING);
            
            // 검색 이력 조회
            List<SearchHistory> histories = searchHistoryRepository.findByExternalIdContaining(jobId);
            
            if (histories.isEmpty()) {
                throw new IllegalArgumentException("Search history not found for job: " + jobId);
            }
            
            // 데이터 집계
            Map<String, Object> summaryData = aggregateSummaryData(histories);
            List<Map<String, Object>> results = aggregateResults(histories);
            
            // 보고서 제목 생성
            String title = request.getCustomTitle() != null 
                    ? request.getCustomTitle()
                    : "'" + request.getQuery() + "' 통합 검색 분석 보고서";
            
            // PDF 생성
            byte[] pdfBytes = pdfExportService.generateUnifiedSearchReport(
                    title,
                    request.getQuery(),
                    request.getTimeWindow(),
                    summaryData,
                    results,
                    request.getChartImages(),
                    request.getIncludeSections()
            );
            
            // 캐시에 저장
            reportCache.put(reportId, pdfBytes);
            
            // 메타데이터 업데이트
            long duration = System.currentTimeMillis() - startTime;
            ReportMetadata metadata = metadataCache.get(reportId);
            ReportMetadata updatedMetadata = ReportMetadata.builder()
                    .reportId(reportId)
                    .title(title)
                    .reportType(ReportRequest.ReportType.UNIFIED_SEARCH)
                    .targetId(jobId)
                    .query(request.getQuery())
                    .status(ReportMetadata.ReportStatus.COMPLETED)
                    .fileSize((long) pdfBytes.length)
                    .generationTimeMs(duration)
                    .createdAt(metadata.getCreatedAt())
                    .expiresAt(metadata.getExpiresAt())
                    .downloadUrl("/api/v1/reports/" + reportId + "/download")
                    .build();
            
            metadataCache.put(reportId, updatedMetadata);
            
            log.info("Report generated successfully: reportId={}, size={}KB, duration={}ms",
                    reportId, pdfBytes.length / 1024, duration);
            
            return CompletableFuture.completedFuture(updatedMetadata);
            
        } catch (Exception e) {
            log.error("Failed to generate report: reportId={}, error={}", reportId, e.getMessage(), e);
            
            ReportMetadata metadata = metadataCache.get(reportId);
            ReportMetadata failedMetadata = ReportMetadata.builder()
                    .reportId(reportId)
                    .reportType(ReportRequest.ReportType.UNIFIED_SEARCH)
                    .targetId(jobId)
                    .query(request.getQuery())
                    .status(ReportMetadata.ReportStatus.FAILED)
                    .createdAt(metadata != null ? metadata.getCreatedAt() : LocalDateTime.now())
                    .errorMessage(e.getMessage())
                    .build();
            
            metadataCache.put(reportId, failedMetadata);
            
            return CompletableFuture.completedFuture(failedMetadata);
        }
    }

    /**
     * 동기 보고서 생성 (즉시 다운로드용)
     */
    public byte[] generateReportSync(String jobId, ReportRequest request) throws IOException {
        // Deep Search 보고서인 경우 별도 처리
        if (request.getReportType() == ReportRequest.ReportType.DEEP_SEARCH) {
            return generateDeepSearchReportSync(jobId, request);
        }
        
        // 검색 이력 조회 (통합 검색)
        List<SearchHistory> histories = searchHistoryRepository.findByExternalIdContaining(jobId);
        
        if (histories.isEmpty()) {
            throw new IllegalArgumentException("Search history not found for job: " + jobId);
        }
        
        // 데이터 집계
        Map<String, Object> summaryData = aggregateSummaryData(histories);
        List<Map<String, Object>> results = aggregateResults(histories);
        
        // 보고서 제목 생성
        String title = request.getCustomTitle() != null 
                ? request.getCustomTitle()
                : "'" + request.getQuery() + "' 통합 검색 분석 보고서";
        
        // PDF 생성 및 반환
        return pdfExportService.generateUnifiedSearchReport(
                title,
                request.getQuery(),
                request.getTimeWindow(),
                summaryData,
                results,
                request.getChartImages(),
                request.getIncludeSections()
        );
    }

    /**
     * Deep Search 보고서 동기 생성
     */
    private byte[] generateDeepSearchReportSync(String jobId, ReportRequest request) throws IOException {
        // CrawlJob 조회
        CrawlJob job = crawlJobRepository.findById(jobId)
                .orElseThrow(() -> new IllegalArgumentException("Deep Search job not found: " + jobId));
        
        // CrawlEvidence 조회
        List<CrawlEvidence> evidenceList = crawlEvidenceRepository.findByJobId(jobId);
        
        // 증거를 결과 형식으로 변환
        List<Map<String, Object>> results = new ArrayList<>();
        for (CrawlEvidence evidence : evidenceList) {
            Map<String, Object> result = new HashMap<>();
            result.put("id", evidence.getId());
            result.put("title", evidence.getTitle());
            result.put("url", evidence.getUrl());
            result.put("snippet", evidence.getSnippet());
            result.put("content", evidence.getSnippet());
            result.put("source", evidence.getSource());
            result.put("stance", evidence.getStance() != null ? evidence.getStance().name().toLowerCase() : "neutral");
            results.add(result);
        }
        
        // 입장별 분포 계산
        long proCount = evidenceList.stream().filter(e -> e.getStance() != null && "PRO".equals(e.getStance().name())).count();
        long conCount = evidenceList.stream().filter(e -> e.getStance() != null && "CON".equals(e.getStance().name())).count();
        long neutralCount = evidenceList.stream().filter(e -> e.getStance() == null || "NEUTRAL".equals(e.getStance().name())).count();
        long total = Math.max(evidenceList.size(), 1);
        
        // 요약 데이터 생성
        Map<String, Object> summaryData = new HashMap<>();
        summaryData.put("totalResults", evidenceList.size());
        summaryData.put("dbResults", 0);
        summaryData.put("webResults", evidenceList.size());
        summaryData.put("aiResults", 0);
        summaryData.put("topic", job.getTopic());
        
        // 입장 분포
        Map<String, Object> stanceDistribution = new HashMap<>();
        stanceDistribution.put("pro", proCount);
        stanceDistribution.put("con", conCount);
        stanceDistribution.put("neutral", neutralCount);
        stanceDistribution.put("proRatio", proCount / (double) total);
        stanceDistribution.put("conRatio", conCount / (double) total);
        stanceDistribution.put("neutralRatio", neutralCount / (double) total);
        summaryData.put("stanceDistribution", stanceDistribution);
        
        // AI 요약 생성
        String aiSummary = String.format(
                "Deep Search 분석 결과: '%s' 주제에 대해 %d개의 증거를 수집했습니다. " +
                "찬성 %d개 (%.0f%%), 반대 %d개 (%.0f%%), 중립 %d개 (%.0f%%)의 입장 분포를 보입니다.",
                job.getTopic(), evidenceList.size(),
                proCount, (proCount * 100.0 / total),
                conCount, (conCount * 100.0 / total),
                neutralCount, (neutralCount * 100.0 / total)
        );
        summaryData.put("aiSummary", aiSummary);
        
        // 보고서 제목 생성
        String title = request.getCustomTitle() != null 
                ? request.getCustomTitle()
                : "'" + job.getTopic() + "' Deep Search 분석 보고서";
        
        String query = request.getQuery() != null ? request.getQuery() : job.getTopic();
        
        // PDF 생성 및 반환
        return pdfExportService.generateUnifiedSearchReport(
                title,
                query,
                request.getTimeWindow(),
                summaryData,
                results,
                request.getChartImages(),
                request.getIncludeSections()
        );
    }

    /**
     * 보고서 다운로드
     */
    public byte[] downloadReport(String reportId) {
        byte[] pdfBytes = reportCache.get(reportId);
        if (pdfBytes == null) {
            throw new IllegalArgumentException("Report not found or expired: " + reportId);
        }
        return pdfBytes;
    }

    /**
     * 보고서 메타데이터 조회
     */
    public ReportMetadata getReportMetadata(String reportId) {
        return metadataCache.get(reportId);
    }

    /**
     * 보고서 존재 여부 확인
     */
    public boolean reportExists(String reportId) {
        return reportCache.containsKey(reportId);
    }

    // ===== 헬퍼 메서드 =====

    private void updateMetadataStatus(String reportId, ReportMetadata.ReportStatus status) {
        ReportMetadata existing = metadataCache.get(reportId);
        if (existing != null) {
            ReportMetadata updated = ReportMetadata.builder()
                    .reportId(existing.getReportId())
                    .title(existing.getTitle())
                    .reportType(existing.getReportType())
                    .targetId(existing.getTargetId())
                    .query(existing.getQuery())
                    .status(status)
                    .fileSize(existing.getFileSize())
                    .pageCount(existing.getPageCount())
                    .generationTimeMs(existing.getGenerationTimeMs())
                    .createdAt(existing.getCreatedAt())
                    .expiresAt(existing.getExpiresAt())
                    .downloadUrl(existing.getDownloadUrl())
                    .errorMessage(existing.getErrorMessage())
                    .build();
            metadataCache.put(reportId, updated);
        }
    }

    @SuppressWarnings("unchecked")
    private Map<String, Object> aggregateSummaryData(List<SearchHistory> histories) {
        Map<String, Object> summary = new HashMap<>();
        
        int totalResults = 0;
        int dbResults = 0;
        int webResults = 0;
        int aiResults = 0;
        String aiSummary = null;
        Map<String, Object> sentiment = new HashMap<>();
        Map<String, Object> reliability = new HashMap<>();
        List<Map<String, Object>> keywords = new ArrayList<>();
        
        for (SearchHistory history : histories) {
            // 결과 수 집계
            if (history.getResults() != null) {
                List<Map<String, Object>> results = history.getResults();
                totalResults += results.size();
                
                for (Map<String, Object> result : results) {
                    String source = (String) result.getOrDefault("_source", 
                            result.getOrDefault("source", "unknown"));
                    switch (source.toLowerCase()) {
                        case "database" -> dbResults++;
                        case "web" -> webResults++;
                        case "ai" -> aiResults++;
                    }
                }
            }
            
            // AI 요약 추출
            if (history.getAiSummary() != null && aiSummary == null) {
                Map<String, Object> aiSummaryMap = history.getAiSummary();
                aiSummary = (String) aiSummaryMap.get("summary");
                if (aiSummary == null) {
                    aiSummary = (String) aiSummaryMap.get("content");
                }
            }
        }
        
        summary.put("totalResults", totalResults);
        summary.put("dbResults", dbResults);
        summary.put("webResults", webResults);
        summary.put("aiResults", aiResults);
        summary.put("aiSummary", aiSummary);
        summary.put("sentiment", sentiment);
        summary.put("reliability", reliability);
        summary.put("keywords", keywords);
        
        return summary;
    }

    private List<Map<String, Object>> aggregateResults(List<SearchHistory> histories) {
        List<Map<String, Object>> allResults = new ArrayList<>();
        
        for (SearchHistory history : histories) {
            if (history.getResults() != null) {
                allResults.addAll(history.getResults());
            }
        }
        
        // 중복 제거 (URL 기준)
        Set<String> seenUrls = new HashSet<>();
        List<Map<String, Object>> uniqueResults = new ArrayList<>();
        
        for (Map<String, Object> result : allResults) {
            String url = (String) result.get("url");
            if (url == null || !seenUrls.contains(url)) {
                uniqueResults.add(result);
                if (url != null) {
                    seenUrls.add(url);
                }
            }
        }
        
        return uniqueResults;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/search/AdvancedIntentAnalyzer.java

```java
package com.newsinsight.collector.service.search;

import com.newsinsight.collector.client.AIDoveClient;
import com.newsinsight.collector.service.search.HybridRankingService.QueryIntent;
import com.newsinsight.collector.service.search.HybridRankingService.QueryIntent.IntentType;
import lombok.Builder;
import lombok.Data;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;

import java.util.*;
import java.util.concurrent.ConcurrentHashMap;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
import java.util.stream.Collectors;

/**
 * 고급 의도 분석 서비스
 * 
 * 사용자 쿼리에서 키워드 추출, 문맥 분석, 쿼리 확장, 폴백 전략 생성을 수행하여
 * 검색 결과의 품질과 적중률을 보장합니다.
 * 
 * 주요 기능:
 * 1. 한국어/영어 키워드 추출
 * 2. 의도 유형 분석 (팩트체크, 최신뉴스, 심층분석 등)
 * 3. 쿼리 확장 및 변형 생성
 * 4. 폴백 검색 전략 생성
 * 5. 결과 보장 로직
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class AdvancedIntentAnalyzer {

    private final AIDoveClient aiDoveClient;
    
    // LLM 분석 결과 캐시 (메모리 절약을 위해 최대 1000개, 5분 TTL 개념으로 관리)
    private final Map<String, RealtimeAnalysisResult> realtimeAnalysisCache = new ConcurrentHashMap<>();
    private static final int MAX_CACHE_SIZE = 1000;

    // ============================================
    // 상수 및 패턴 정의
    // ============================================

    // 한국어 불용어
    private static final Set<String> KOREAN_STOPWORDS = Set.of(
            "은", "는", "이", "가", "을", "를", "의", "에", "에서", "로", "으로",
            "와", "과", "도", "만", "부터", "까지", "에게", "한테", "께",
            "이다", "하다", "있다", "없다", "되다", "않다",
            "그", "저", "이것", "그것", "저것", "여기", "거기", "저기",
            "뭐", "어디", "언제", "어떻게", "왜", "누구",
            "아주", "매우", "정말", "너무", "조금", "약간",
            "그리고", "그러나", "하지만", "그래서", "때문에",
            "것", "수", "등", "들", "및", "더", "덜",
            "대해", "대한", "관련", "관한"
    );

    // 영어 불용어
    private static final Set<String> ENGLISH_STOPWORDS = Set.of(
            "the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for",
            "of", "with", "by", "from", "is", "are", "was", "were", "be", "been",
            "being", "have", "has", "had", "do", "does", "did", "will", "would",
            "could", "should", "may", "might", "must", "shall", "can",
            "this", "that", "these", "those", "it", "its",
            "i", "you", "he", "she", "we", "they", "me", "him", "her", "us", "them",
            "what", "which", "who", "whom", "where", "when", "why", "how",
            "all", "each", "every", "both", "few", "more", "most", "other", "some",
            "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too",
            "very", "just", "also", "now", "here", "there", "then", "about"
    );

    // 의도별 키워드 패턴 (확장)
    private static final Map<IntentType, List<String>> INTENT_PATTERNS = Map.of(
            IntentType.FACT_CHECK, List.of(
                    "사실", "진짜", "가짜", "팩트체크", "팩트 체크", "검증",
                    "진위", "확인", "루머", "허위", "오보", "실제로",
                    "정말", "맞는", "틀린", "fact", "check", "verify",
                    "true", "false", "fake", "hoax", "myth", "믿을 수",
                    "신뢰", "거짓", "조작", "왜곡"
            ),
            IntentType.LATEST_NEWS, List.of(
                    "오늘", "최근", "속보", "긴급", "방금", "지금",
                    "현재", "실시간", "최신", "breaking", "today",
                    "어제", "이번주", "금일", "latest", "recent",
                    "새로운", "발표", "업데이트"
            ),
            IntentType.DEEP_ANALYSIS, List.of(
                    "분석", "원인", "배경", "이유", "왜", "어떻게",
                    "영향", "전망", "예측", "해설", "설명", "의미",
                    "history", "analysis", "impact", "결과", "심층",
                    "상세", "깊이", "인사이트", "근본", "핵심"
            ),
            IntentType.OPINION_SEARCH, List.of(
                    "여론", "반응", "논란", "비판", "지지", "반대",
                    "찬성", "의견", "댓글", "네티즌", "SNS", "트위터",
                    "opinion", "reaction", "controversy", "debate",
                    "토론", "갑론을박", "시각"
            )
    );

    // 질문 패턴
    private static final Pattern QUESTION_PATTERN = Pattern.compile(
            "(\\?|인가요|인가|입니까|일까|일까요|나요|습니까|은가요|는가요|맞나요|아닌가요|뭔가요|무엇|어떤)$"
    );

    // 비교 패턴
    private static final Pattern COMPARISON_PATTERN = Pattern.compile(
            "(vs|versus|비교|차이|다른|어느|어떤 것이|보다)"
    );

    // 시간 표현 패턴
    private static final Pattern TIME_PATTERN = Pattern.compile(
            "(오늘|어제|이번주|지난주|최근|\\d+일|\\d+시간|\\d+분|\\d{4}년)"
    );

    // 한글 패턴
    private static final Pattern KOREAN_PATTERN = Pattern.compile("[가-힣]");

    // ============================================
    // DTO 클래스
    // ============================================

    @Data
    @Builder
    public static class AnalyzedQuery {
        private String originalQuery;
        private List<String> keywords;
        private String primaryKeyword;
        private IntentType intentType;
        private double confidence;
        private String language;
        private List<String> expandedQueries;
        private List<FallbackStrategy> fallbackStrategies;
        private String timeRange;
        private Map<String, Object> metadata;
    }

    @Data
    @Builder
    public static class FallbackStrategy {
        private String strategyType;
        private String query;
        private int priority;
        private String description;
    }

    public enum StrategyType {
        FULL_QUERY,
        KEYWORDS_AND,
        KEYWORDS_OR,
        PRIMARY_KEYWORD,
        SEMANTIC_VARIANT,
        RELATED_TOPIC,
        PARTIAL_MATCH,
        SYNONYM_SEARCH
    }

    /**
     * 실시간 데이터 필요성 분석 결과
     */
    @Data
    @Builder
    public static class RealtimeAnalysisResult {
        private boolean needsRealtimeData;
        private String dataType;          // price, statistics, news, event, weather 등
        private String reason;            // 판단 이유
        private double confidence;        // 0.0 ~ 1.0
        private List<String> entities;    // 관련 엔티티 (비트코인, 삼성전자 등)
        private long timestamp;           // 분석 시점
    }

    // ============================================
    // 메인 분석 메서드
    // ============================================

    /**
     * 쿼리를 종합적으로 분석합니다.
     *
     * @param query 사용자 쿼리
     * @return 분석된 쿼리 정보
     */
    public AnalyzedQuery analyzeQuery(String query) {
        if (query == null || query.isBlank()) {
            return buildEmptyResult();
        }

        String normalizedQuery = query.trim();
        
        // 1. 언어 감지
        String language = detectLanguage(normalizedQuery);
        
        // 2. 키워드 추출
        List<String> keywords = extractKeywords(normalizedQuery, language);
        
        // 3. 주요 키워드 식별
        String primaryKeyword = identifyPrimaryKeyword(keywords, normalizedQuery);
        
        // 4. 의도 분석
        IntentType intentType = detectIntentType(normalizedQuery);
        double confidence = calculateConfidence(normalizedQuery, intentType);
        
        // 5. 시간 범위 추출
        String timeRange = extractTimeRange(normalizedQuery);
        
        // 6. 쿼리 확장
        List<String> expandedQueries = generateExpandedQueries(keywords, primaryKeyword, normalizedQuery, language);
        
        // 7. 폴백 전략 생성
        List<FallbackStrategy> fallbackStrategies = generateFallbackStrategies(
                normalizedQuery, keywords, primaryKeyword, expandedQueries, language
        );

        AnalyzedQuery result = AnalyzedQuery.builder()
                .originalQuery(normalizedQuery)
                .keywords(keywords)
                .primaryKeyword(primaryKeyword)
                .intentType(intentType)
                .confidence(confidence)
                .language(language)
                .expandedQueries(expandedQueries)
                .fallbackStrategies(fallbackStrategies)
                .timeRange(timeRange)
                .metadata(Map.of(
                        "keywordCount", keywords.size(),
                        "strategyCount", fallbackStrategies.size(),
                        "isQuestion", QUESTION_PATTERN.matcher(normalizedQuery).find(),
                        "isComparison", COMPARISON_PATTERN.matcher(normalizedQuery.toLowerCase()).find()
                ))
                .build();

        log.info("Query analyzed: keywords={}, primary='{}', intent={}, confidence={}, strategies={}",
                keywords.size(), primaryKeyword, intentType, String.format("%.2f", confidence), fallbackStrategies.size());

        return result;
    }

    /**
     * 기존 QueryIntent 형태로 변환합니다 (호환성 유지).
     */
    public QueryIntent toQueryIntent(AnalyzedQuery analyzed) {
        return QueryIntent.builder()
                .type(analyzed.getIntentType())
                .keywords(analyzed.getKeywords())
                .timeRange(analyzed.getTimeRange())
                .confidence(analyzed.getConfidence())
                .build();
    }

    // ============================================
    // 언어 감지
    // ============================================

    private String detectLanguage(String text) {
        if (text == null || text.isEmpty()) return "ko";
        
        int koreanCount = 0;
        int englishCount = 0;
        
        for (char c : text.toCharArray()) {
            if (c >= '가' && c <= '힣') {
                koreanCount++;
            } else if ((c >= 'A' && c <= 'Z') || (c >= 'a' && c <= 'z')) {
                englishCount++;
            }
        }
        
        int total = koreanCount + englishCount;
        if (total == 0) return "ko";
        
        return (double) koreanCount / total > 0.3 ? "ko" : "en";
    }

    // ============================================
    // 키워드 추출
    // ============================================

    private List<String> extractKeywords(String text, String language) {
        Set<String> stopwords = "ko".equals(language) ? KOREAN_STOPWORDS : ENGLISH_STOPWORDS;
        
        // 토큰화
        String[] tokens = text.toLowerCase()
                .replaceAll("[^가-힣a-zA-Z0-9\\s]", " ")
                .split("\\s+");
        
        List<String> keywords = new ArrayList<>();
        
        for (String token : tokens) {
            String trimmed = token.trim();
            
            // 불용어 제외
            if (stopwords.contains(trimmed)) continue;
            
            // 너무 짧은 토큰 제외 (한글은 1자도 의미있을 수 있음)
            if ("ko".equals(language) && trimmed.length() < 1) continue;
            if ("en".equals(language) && trimmed.length() < 2) continue;
            
            // 순수 숫자 제외
            if (trimmed.matches("\\d+")) continue;
            
            keywords.add(trimmed);
        }
        
        // 인용구 내 구문 추출
        Pattern quotedPattern = Pattern.compile("[\"']([^\"']+)[\"']");
        Matcher matcher = quotedPattern.matcher(text);
        while (matcher.find()) {
            String phrase = matcher.group(1).trim();
            if (!phrase.isEmpty() && !keywords.contains(phrase.toLowerCase())) {
                keywords.add(phrase.toLowerCase());
            }
        }
        
        // 복합명사 추출 (한국어)
        if ("ko".equals(language)) {
            Pattern compoundPattern = Pattern.compile("[가-힣]+(?:기업|회사|뉴스|정보|서비스|시스템|데이터|분석|결과|사건|사고|정책|발표)");
            Matcher compoundMatcher = compoundPattern.matcher(text);
            while (compoundMatcher.find()) {
                String compound = compoundMatcher.group().toLowerCase();
                if (!keywords.contains(compound)) {
                    keywords.add(compound);
                }
            }
        }
        
        // 중복 제거 및 최대 10개 제한
        return keywords.stream()
                .distinct()
                .limit(10)
                .collect(Collectors.toList());
    }

    // ============================================
    // 주요 키워드 식별
    // ============================================

    private String identifyPrimaryKeyword(List<String> keywords, String originalQuery) {
        if (keywords.isEmpty()) {
            String[] words = originalQuery.split("\\s+");
            return words.length > 0 ? words[0] : originalQuery;
        }
        
        // 점수 기반 주요 키워드 선정
        Map<String, Double> scores = new HashMap<>();
        
        for (String keyword : keywords) {
            double score = 0.0;
            
            // 길이 가중치 (더 긴 키워드가 더 구체적)
            score += Math.min(keyword.length() / 10.0, 1.0) * 0.3;
            
            // 위치 가중치 (앞에 있을수록 중요)
            int pos = originalQuery.toLowerCase().indexOf(keyword.toLowerCase());
            if (pos >= 0) {
                score += (1.0 - (double) pos / originalQuery.length()) * 0.3;
            }
            
            // 대문자 시작 (고유명사 가능성)
            if (Character.isUpperCase(keyword.charAt(0))) {
                score += 0.2;
            }
            
            // 숫자 포함 (구체적 식별자 가능성)
            if (keyword.matches(".*\\d+.*")) {
                score += 0.1;
            }
            
            // 복합어 (한국어)
            if (keyword.matches(".*[가-힣]+(기업|회사|사건|정책)$")) {
                score += 0.3;
            }
            
            scores.put(keyword, score);
        }
        
        return scores.entrySet().stream()
                .max(Map.Entry.comparingByValue())
                .map(Map.Entry::getKey)
                .orElse(keywords.get(0));
    }

    // ============================================
    // 의도 분석
    // ============================================

    private IntentType detectIntentType(String query) {
        String lowerQuery = query.toLowerCase();
        
        Map<IntentType, Double> scores = new EnumMap<>(IntentType.class);
        for (IntentType type : IntentType.values()) {
            scores.put(type, 0.0);
        }
        
        // 패턴 매칭
        for (Map.Entry<IntentType, List<String>> entry : INTENT_PATTERNS.entrySet()) {
            for (String pattern : entry.getValue()) {
                if (lowerQuery.contains(pattern.toLowerCase())) {
                    scores.merge(entry.getKey(), 1.0, Double::sum);
                }
            }
        }
        
        // 질문 형태 → 팩트체크 가능성
        if (QUESTION_PATTERN.matcher(lowerQuery).find()) {
            scores.merge(IntentType.FACT_CHECK, 0.5, Double::sum);
        }
        
        // 시간 표현 → 최신 뉴스 가능성
        if (TIME_PATTERN.matcher(lowerQuery).find()) {
            scores.merge(IntentType.LATEST_NEWS, 0.5, Double::sum);
        }
        
        // 비교 표현 → 심층 분석 가능성
        if (COMPARISON_PATTERN.matcher(lowerQuery).find()) {
            scores.merge(IntentType.DEEP_ANALYSIS, 0.3, Double::sum);
        }
        
        // 최고 점수 의도 선택
        IntentType bestIntent = IntentType.GENERAL;
        double maxScore = 0.0;
        
        for (Map.Entry<IntentType, Double> entry : scores.entrySet()) {
            if (entry.getValue() > maxScore) {
                maxScore = entry.getValue();
                bestIntent = entry.getKey();
            }
        }
        
        // 최소 임계값
        if (maxScore < 0.5) {
            bestIntent = IntentType.GENERAL;
        }
        
        return bestIntent;
    }

    private double calculateConfidence(String query, IntentType intentType) {
        if (intentType == IntentType.GENERAL) {
            return 0.7;
        }
        
        String lowerQuery = query.toLowerCase();
        List<String> patterns = INTENT_PATTERNS.getOrDefault(intentType, List.of());
        
        long matchCount = patterns.stream()
                .filter(p -> lowerQuery.contains(p.toLowerCase()))
                .count();
        
        double baseConfidence = Math.min(0.5 + matchCount * 0.15, 0.95);
        
        // 질문 형태 보너스
        if (QUESTION_PATTERN.matcher(lowerQuery).find()) {
            baseConfidence = Math.min(baseConfidence + 0.1, 0.95);
        }
        
        return baseConfidence;
    }

    // ============================================
    // 시간 범위 추출
    // ============================================

    private String extractTimeRange(String query) {
        String lowerQuery = query.toLowerCase();
        
        if (lowerQuery.contains("오늘") || lowerQuery.contains("금일") || lowerQuery.contains("today")) {
            return "1d";
        } else if (lowerQuery.contains("어제") || lowerQuery.contains("yesterday")) {
            return "2d";
        } else if (lowerQuery.contains("이번주") || lowerQuery.contains("this week")) {
            return "7d";
        } else if (lowerQuery.contains("지난주") || lowerQuery.contains("last week")) {
            return "14d";
        } else if (lowerQuery.contains("최근") || lowerQuery.contains("recent")) {
            return "7d";
        } else if (lowerQuery.contains("이번달") || lowerQuery.contains("한달") || lowerQuery.contains("this month")) {
            return "30d";
        }
        
        return null;
    }

    // ============================================
    // 쿼리 확장
    // ============================================

    private List<String> generateExpandedQueries(
            List<String> keywords,
            String primaryKeyword,
            String originalQuery,
            String language) {
        
        List<String> variants = new ArrayList<>();
        
        // 1. 원본 쿼리
        variants.add(originalQuery);
        
        // 2. 키워드 조합
        if (keywords.size() > 1) {
            variants.add(String.join(" ", keywords));
        }
        
        // 3. 주요 키워드만
        variants.add(primaryKeyword);
        
        // 4. 상위 2-3개 키워드
        if (keywords.size() >= 2) {
            variants.add(keywords.get(0) + " " + keywords.get(1));
        }
        if (keywords.size() >= 3) {
            variants.add(keywords.get(0) + " " + keywords.get(1) + " " + keywords.get(2));
        }
        
        // 5. 산업/시장 관련 동의어 확장
        String lowerQuery = originalQuery.toLowerCase();
        if (lowerQuery.contains("반도체") || lowerQuery.contains("메모리") || lowerQuery.contains("semiconductor")) {
            variants.add("DRAM 가격");
            variants.add("낸드플래시 시장");
            variants.add("반도체 업황");
            variants.add("메모리칩 시세");
            if (lowerQuery.contains("가격") || lowerQuery.contains("상승") || lowerQuery.contains("하락")) {
                variants.add("SK하이닉스 실적");
                variants.add("삼성전자 반도체");
                variants.add("메모리 반도체 시황");
            }
        }
        
        // 6. 언어별 검색 접미사 추가
        if ("ko".equals(language)) {
            for (String keyword : keywords.subList(0, Math.min(3, keywords.size()))) {
                variants.add(keyword + " 뉴스");
                variants.add(keyword + " 정보");
                variants.add(keyword + " 최신");
                // 시장/가격 관련 키워드면 추가 변형
                if (keyword.contains("가격") || keyword.contains("시세") || keyword.contains("시장")) {
                    variants.add(keyword + " 동향");
                    variants.add(keyword + " 전망");
                }
            }
        } else {
            for (String keyword : keywords.subList(0, Math.min(3, keywords.size()))) {
                variants.add(keyword + " news");
                variants.add(keyword + " information");
                variants.add("about " + keyword);
                if (keyword.contains("price") || keyword.contains("market")) {
                    variants.add(keyword + " trend");
                    variants.add(keyword + " outlook");
                }
            }
        }
        
        // 중복 제거
        return variants.stream()
                .distinct()
                .filter(v -> !v.isBlank())
                .collect(Collectors.toList());
    }

    // ============================================
    // 폴백 전략 생성
    // ============================================

    private List<FallbackStrategy> generateFallbackStrategies(
            String originalQuery,
            List<String> keywords,
            String primaryKeyword,
            List<String> expandedQueries,
            String language) {
        
        List<FallbackStrategy> strategies = new ArrayList<>();
        
        // 전략 1: 전체 쿼리
        strategies.add(FallbackStrategy.builder()
                .strategyType(StrategyType.FULL_QUERY.name())
                .query(originalQuery)
                .priority(1)
                .description("원본 쿼리로 검색")
                .build());
        
        // 전략 2: 키워드 AND
        if (keywords.size() > 1) {
            strategies.add(FallbackStrategy.builder()
                    .strategyType(StrategyType.KEYWORDS_AND.name())
                    .query(String.join(" ", keywords))
                    .priority(2)
                    .description("모든 키워드로 검색")
                    .build());
        }
        
        // 전략 3: 주요 키워드
        strategies.add(FallbackStrategy.builder()
                .strategyType(StrategyType.PRIMARY_KEYWORD.name())
                .query(primaryKeyword)
                .priority(3)
                .description("주요 키워드만으로 검색")
                .build());
        
        // 전략 4: 확장 쿼리들
        int priority = 4;
        for (String expanded : expandedQueries.subList(0, Math.min(3, expandedQueries.size()))) {
            if (!expanded.equals(originalQuery) && !expanded.equals(primaryKeyword)) {
                strategies.add(FallbackStrategy.builder()
                        .strategyType(StrategyType.SEMANTIC_VARIANT.name())
                        .query(expanded)
                        .priority(priority++)
                        .description("변형 쿼리: " + expanded)
                        .build());
            }
        }
        
        // 전략 5: 키워드 OR (넓은 검색)
        if (keywords.size() > 1) {
            strategies.add(FallbackStrategy.builder()
                    .strategyType(StrategyType.KEYWORDS_OR.name())
                    .query(String.join(" OR ", keywords.subList(0, Math.min(5, keywords.size()))))
                    .priority(priority++)
                    .description("키워드 OR 검색 (넓은 검색)")
                    .build());
        }
        
        // 전략 6: 부분 매칭
        if (keywords.size() >= 2) {
            strategies.add(FallbackStrategy.builder()
                    .strategyType(StrategyType.PARTIAL_MATCH.name())
                    .query(keywords.get(0) + " " + keywords.get(1))
                    .priority(priority++)
                    .description("상위 키워드 부분 매칭")
                    .build());
        }
        
        // 정렬
        strategies.sort(Comparator.comparingInt(FallbackStrategy::getPriority));
        
        return strategies;
    }

    // ============================================
    // 결과 보장 메서드
    // ============================================

    /**
     * 검색 결과가 없을 때 사용할 대체 메시지를 생성합니다.
     */
    public String buildNoResultMessage(AnalyzedQuery analyzed) {
        StringBuilder message = new StringBuilder();
        
        if ("ko".equals(analyzed.getLanguage())) {
            message.append("검색 결과를 찾기 어려웠습니다. 다음을 시도해 보세요:\n\n");
            message.append("시도한 검색어:\n");
            message.append("- ").append(analyzed.getOriginalQuery()).append("\n");
            message.append("- ").append(analyzed.getPrimaryKeyword()).append("\n");
            
            message.append("\n추천 검색 방법:\n");
            message.append("1. 검색어를 더 구체적으로 변경해 보세요\n");
            message.append("2. 다른 키워드를 사용해 보세요: ")
                    .append(String.join(", ", analyzed.getKeywords().subList(0, Math.min(3, analyzed.getKeywords().size()))))
                    .append("\n");
            message.append("3. 시간 범위를 조정해 보세요\n");
            
            message.append("\n분석된 의도: ").append(getIntentDescription(analyzed.getIntentType()));
        } else {
            message.append("Search results were difficult to find. Try the following:\n\n");
            message.append("Queries attempted:\n");
            message.append("- ").append(analyzed.getOriginalQuery()).append("\n");
            message.append("- ").append(analyzed.getPrimaryKeyword()).append("\n");
            
            message.append("\nRecommended approaches:\n");
            message.append("1. Try more specific keywords\n");
            message.append("2. Use alternative keywords: ")
                    .append(String.join(", ", analyzed.getKeywords().subList(0, Math.min(3, analyzed.getKeywords().size()))))
                    .append("\n");
            message.append("3. Adjust the time range\n");
            
            message.append("\nDetected intent: ").append(getIntentDescription(analyzed.getIntentType()));
        }
        
        return message.toString();
    }

    /**
     * 향상된 검색 태스크를 생성합니다.
     */
    public String buildEnhancedSearchTask(AnalyzedQuery analyzed, String originalTask) {
        StringBuilder task = new StringBuilder(originalTask);
        
        task.append("\n\n");
        
        if ("ko".equals(analyzed.getLanguage())) {
            task.append("검색 전략 (결과가 없으면 순서대로 시도하세요):\n");
            int num = 1;
            for (FallbackStrategy strategy : analyzed.getFallbackStrategies().subList(0, Math.min(5, analyzed.getFallbackStrategies().size()))) {
                task.append(num++).append(". ").append(strategy.getDescription())
                        .append(": \"").append(strategy.getQuery()).append("\"\n");
            }
            task.append("\n중요: 검색 결과가 없다고 말하지 마세요. 위의 전략을 모두 시도하고,\n");
            task.append("관련된 정보라도 찾아서 제공하세요. 완전히 일치하지 않더라도\n");
            task.append("가장 관련성 있는 정보를 제공하는 것이 중요합니다.");
        } else {
            task.append("Search strategies (try in order if no results):\n");
            int num = 1;
            for (FallbackStrategy strategy : analyzed.getFallbackStrategies().subList(0, Math.min(5, analyzed.getFallbackStrategies().size()))) {
                task.append(num++).append(". ").append(strategy.getDescription())
                        .append(": \"").append(strategy.getQuery()).append("\"\n");
            }
            task.append("\nIMPORTANT: Never say \"not found\" or \"no results\". Try ALL strategies above,\n");
            task.append("and provide whatever relevant information you can find. Even if not an exact match,\n");
            task.append("providing the most relevant information is important.");
        }
        
        return task.toString();
    }

    private String getIntentDescription(IntentType type) {
        return switch (type) {
            case FACT_CHECK -> "팩트체크/Fact Check - 정보의 진위 여부 검증";
            case LATEST_NEWS -> "최신 뉴스/Latest News - 최근 소식 우선";
            case DEEP_ANALYSIS -> "심층 분석/Deep Analysis - 배경과 맥락 포함";
            case OPINION_SEARCH -> "여론 검색/Opinion Search - 다양한 의견 수집";
            case GENERAL -> "일반 검색/General Search - 관련성 높은 정보";
        };
    }

    private AnalyzedQuery buildEmptyResult() {
        return AnalyzedQuery.builder()
                .originalQuery("")
                .keywords(List.of())
                .primaryKeyword("")
                .intentType(IntentType.GENERAL)
                .confidence(1.0)
                .language("ko")
                .expandedQueries(List.of())
                .fallbackStrategies(List.of())
                .metadata(Map.of())
                .build();
    }

    // ============================================
    // 실시간 데이터 필요성 분석 (LLM + 휴리스틱 하이브리드)
    // ============================================

    /**
     * 쿼리가 실시간 데이터를 필요로 하는지 분석합니다.
     * 
     * 키워드 매칭의 한계를 극복하기 위해:
     * 1. 빠른 휴리스틱 체크 (키워드 기반)
     * 2. 의미 기반 패턴 매칭
     * 3. 필요시 LLM 분석 (새로운 개념, 알려지지 않은 엔티티)
     * 
     * @param query 분석할 쿼리
     * @return 실시간 데이터 필요성 분석 결과
     */
    public RealtimeAnalysisResult analyzeRealtimeDataNeed(String query) {
        if (query == null || query.isBlank()) {
            return buildDefaultRealtimeResult(false, "empty_query");
        }

        String cacheKey = query.toLowerCase().trim();
        
        // 1. 캐시 확인
        RealtimeAnalysisResult cached = realtimeAnalysisCache.get(cacheKey);
        if (cached != null && System.currentTimeMillis() - cached.getTimestamp() < 300_000) { // 5분 TTL
            log.debug("Realtime analysis cache hit for: {}", query);
            return cached;
        }

        // 2. 빠른 휴리스틱 체크
        RealtimeAnalysisResult heuristicResult = analyzeWithHeuristics(query);
        if (heuristicResult.getConfidence() >= 0.8) {
            cacheResult(cacheKey, heuristicResult);
            return heuristicResult;
        }

        // 3. 의미 기반 패턴 분석
        RealtimeAnalysisResult semanticResult = analyzeWithSemanticPatterns(query);
        if (semanticResult.getConfidence() >= 0.7) {
            cacheResult(cacheKey, semanticResult);
            return semanticResult;
        }

        // 4. LLM 분석 (알 수 없는 엔티티나 새로운 개념의 경우)
        if (aiDoveClient != null && aiDoveClient.isEnabled() && heuristicResult.getConfidence() < 0.5) {
            try {
                RealtimeAnalysisResult llmResult = analyzeWithLLM(query);
                if (llmResult != null) {
                    cacheResult(cacheKey, llmResult);
                    return llmResult;
                }
            } catch (Exception e) {
                log.warn("LLM analysis failed for realtime check: {}", e.getMessage());
            }
        }

        // 5. 휴리스틱 + 의미 분석 결과 결합
        RealtimeAnalysisResult combined = combineResults(heuristicResult, semanticResult);
        cacheResult(cacheKey, combined);
        return combined;
    }

    /**
     * 휴리스틱 기반 빠른 분석
     */
    private RealtimeAnalysisResult analyzeWithHeuristics(String query) {
        String lower = query.toLowerCase();
        double confidence = 0.0;
        String dataType = "unknown";
        List<String> entities = new ArrayList<>();
        StringBuilder reason = new StringBuilder();

        // 시간 민감 키워드
        Map<String, Double> timeKeywords = Map.ofEntries(
                Map.entry("현재", 0.9), Map.entry("지금", 0.9), Map.entry("오늘", 0.85),
                Map.entry("실시간", 0.95), Map.entry("최신", 0.8), Map.entry("방금", 0.9),
                Map.entry("current", 0.9), Map.entry("now", 0.85), Map.entry("today", 0.8),
                Map.entry("latest", 0.8), Map.entry("live", 0.9), Map.entry("real-time", 0.95)
        );

        // 가격/시세 키워드
        Map<String, Double> priceKeywords = Map.ofEntries(
                Map.entry("가격", 0.85), Map.entry("시세", 0.9), Map.entry("시가", 0.85),
                Map.entry("종가", 0.85), Map.entry("환율", 0.9), Map.entry("얼마", 0.7),
                Map.entry("price", 0.85), Map.entry("rate", 0.8), Map.entry("cost", 0.7),
                Map.entry("worth", 0.75), Map.entry("value", 0.7)
        );

        // 금융 자산 키워드
        Map<String, String> assetKeywords = Map.ofEntries(
                // 암호화폐
                Map.entry("비트코인", "crypto"), Map.entry("이더리움", "crypto"),
                Map.entry("리플", "crypto"), Map.entry("도지코인", "crypto"),
                Map.entry("bitcoin", "crypto"), Map.entry("btc", "crypto"),
                Map.entry("ethereum", "crypto"), Map.entry("eth", "crypto"),
                Map.entry("암호화폐", "crypto"), Map.entry("코인", "crypto"),
                Map.entry("crypto", "crypto"), Map.entry("cryptocurrency", "crypto"),
                // 주식
                Map.entry("주가", "stock"), Map.entry("주식", "stock"),
                Map.entry("코스피", "stock"), Map.entry("코스닥", "stock"),
                Map.entry("나스닥", "stock"), Map.entry("다우", "stock"),
                Map.entry("s&p", "stock"), Map.entry("stock", "stock"),
                Map.entry("nasdaq", "stock"), Map.entry("dow", "stock"),
                // 환율
                Map.entry("달러", "forex"), Map.entry("엔화", "forex"),
                Map.entry("유로", "forex"), Map.entry("원화", "forex"),
                Map.entry("dollar", "forex"), Map.entry("yen", "forex"),
                Map.entry("euro", "forex"), Map.entry("usd", "forex"),
                Map.entry("krw", "forex"), Map.entry("jpy", "forex"),
                // 반도체 및 IT 산업
                Map.entry("반도체", "semiconductor"), Map.entry("메모리", "semiconductor"),
                Map.entry("dram", "semiconductor"), Map.entry("낸드", "semiconductor"),
                Map.entry("nand", "semiconductor"), Map.entry("ssd", "semiconductor"),
                Map.entry("파운드리", "semiconductor"), Map.entry("웨이퍼", "semiconductor"),
                Map.entry("칩", "semiconductor"), Map.entry("chip", "semiconductor"),
                Map.entry("semiconductor", "semiconductor"),
                // 기업
                Map.entry("삼성전자", "stock"), Map.entry("sk하이닉스", "stock"),
                Map.entry("마이크론", "stock"), Map.entry("인텔", "stock"),
                Map.entry("tsmc", "stock"), Map.entry("nvidia", "stock"),
                Map.entry("엔비디아", "stock"), Map.entry("amd", "stock")
        );

        // 통계/지표 키워드
        Map<String, Double> statsKeywords = Map.ofEntries(
                Map.entry("통계", 0.7), Map.entry("지표", 0.75), Map.entry("수치", 0.7),
                Map.entry("데이터", 0.6), Map.entry("지수", 0.75), Map.entry("률", 0.7),
                Map.entry("statistics", 0.7), Map.entry("index", 0.75), Map.entry("rate", 0.7)
        );

        // 시간 키워드 체크
        for (Map.Entry<String, Double> entry : timeKeywords.entrySet()) {
            if (lower.contains(entry.getKey())) {
                confidence = Math.max(confidence, entry.getValue());
                reason.append("time_keyword:").append(entry.getKey()).append(" ");
            }
        }

        // 가격 키워드 체크
        for (Map.Entry<String, Double> entry : priceKeywords.entrySet()) {
            if (lower.contains(entry.getKey())) {
                confidence = Math.max(confidence, entry.getValue());
                dataType = "price";
                reason.append("price_keyword:").append(entry.getKey()).append(" ");
            }
        }

        // 자산 키워드 체크
        for (Map.Entry<String, String> entry : assetKeywords.entrySet()) {
            if (lower.contains(entry.getKey())) {
                confidence = Math.max(confidence, 0.8);
                dataType = entry.getValue();
                entities.add(entry.getKey());
                reason.append("asset:").append(entry.getKey()).append(" ");
            }
        }

        // 통계 키워드 체크
        for (Map.Entry<String, Double> entry : statsKeywords.entrySet()) {
            if (lower.contains(entry.getKey())) {
                confidence = Math.max(confidence, entry.getValue());
                if ("unknown".equals(dataType)) dataType = "statistics";
                reason.append("stats:").append(entry.getKey()).append(" ");
            }
        }

        return RealtimeAnalysisResult.builder()
                .needsRealtimeData(confidence >= 0.6)
                .dataType(dataType)
                .reason(reason.toString().trim())
                .confidence(confidence)
                .entities(entities)
                .timestamp(System.currentTimeMillis())
                .build();
    }

    /**
     * 의미 기반 패턴 분석
     * 키워드가 없어도 문맥에서 실시간 데이터 필요성 감지
     */
    private RealtimeAnalysisResult analyzeWithSemanticPatterns(String query) {
        String lower = query.toLowerCase();
        double confidence = 0.0;
        String dataType = "unknown";
        List<String> entities = new ArrayList<>();
        StringBuilder reason = new StringBuilder();

        // 패턴 1: "X가 얼마야?", "X 몇이야?" 등의 가격 질문 패턴
        Pattern priceQuestionPattern = Pattern.compile(
                "(.+?)(?:가|이|은|는)?\\s*(?:얼마|몇|어느정도|어느 정도|how much|what.*price)"
        );
        Matcher priceMatch = priceQuestionPattern.matcher(lower);
        if (priceMatch.find()) {
            confidence = Math.max(confidence, 0.75);
            dataType = "price";
            entities.add(priceMatch.group(1).trim());
            reason.append("price_question_pattern ");
        }

        // 패턴 2: "X 전망", "X 예측" 등 미래 관련 패턴 (현재 데이터 필요)
        Pattern forecastPattern = Pattern.compile(
                "(.+?)\\s*(?:전망|예측|예상|향후|미래|forecast|prediction|outlook)"
        );
        Matcher forecastMatch = forecastPattern.matcher(lower);
        if (forecastMatch.find()) {
            confidence = Math.max(confidence, 0.7);
            if ("unknown".equals(dataType)) dataType = "forecast";
            entities.add(forecastMatch.group(1).trim());
            reason.append("forecast_pattern ");
        }

        // 패턴 3: 숫자 + 단위 질문 (현재 값 확인)
        Pattern numericQuestionPattern = Pattern.compile(
                "(.+?)\\s*(?:달러|원|엔|유로|\\$|₩|¥|€|%|퍼센트).*(?:인가요?|일까요?|입니까|야\\??|인지)"
        );
        Matcher numericMatch = numericQuestionPattern.matcher(lower);
        if (numericMatch.find()) {
            confidence = Math.max(confidence, 0.8);
            dataType = "price";
            reason.append("numeric_question_pattern ");
        }

        // 패턴 4: 비교 질문 (두 시점 비교 = 현재 데이터 필요)
        Pattern comparisonPattern = Pattern.compile(
                "(?:어제|지난주|지난달|작년|전|ago).*(?:비교|대비|vs|versus|compared)"
        );
        if (comparisonPattern.matcher(lower).find()) {
            confidence = Math.max(confidence, 0.7);
            if ("unknown".equals(dataType)) dataType = "comparison";
            reason.append("comparison_pattern ");
        }

        // 패턴 5: 급등/급락, 상승/하락 등 시장 동향
        Pattern marketTrendPattern = Pattern.compile(
                "(?:급등|급락|폭등|폭락|상승|하락|오르|내리|올라|내려|증가|감소|surge|crash|rise|fall|up|down|increase|decrease)"
        );
        if (marketTrendPattern.matcher(lower).find()) {
            confidence = Math.max(confidence, 0.75);
            if ("unknown".equals(dataType)) dataType = "market_trend";
            reason.append("market_trend_pattern ");
        }
        
        // 패턴 6: 산업/시장 관련 뉴스 패턴 (반도체, 자동차, 에너지 등)
        Pattern industryNewsPattern = Pattern.compile(
                "(?:반도체|메모리|dram|낸드|nand|자동차|배터리|에너지|석유|철강|화학|제약|바이오).*(?:가격|시장|산업|업계|전망|동향|뉴스)"
        );
        if (industryNewsPattern.matcher(lower).find()) {
            confidence = Math.max(confidence, 0.8);
            if ("unknown".equals(dataType)) dataType = "industry_news";
            reason.append("industry_news_pattern ");
        }

        return RealtimeAnalysisResult.builder()
                .needsRealtimeData(confidence >= 0.6)
                .dataType(dataType)
                .reason(reason.toString().trim())
                .confidence(confidence)
                .entities(entities)
                .timestamp(System.currentTimeMillis())
                .build();
    }

    /**
     * LLM 기반 의미 분석 (새로운 개념이나 알려지지 않은 엔티티용)
     */
    private RealtimeAnalysisResult analyzeWithLLM(String query) {
        String prompt = """
                다음 질문이 실시간/최신 데이터를 필요로 하는지 분석해주세요.
                
                질문: "%s"
                
                다음 JSON 형식으로만 응답하세요 (다른 텍스트 없이):
                {
                    "needs_realtime": true/false,
                    "data_type": "price|statistics|news|event|weather|sports|unknown",
                    "reason": "판단 이유 (한 문장)",
                    "confidence": 0.0~1.0,
                    "entities": ["관련 엔티티1", "엔티티2"]
                }
                
                실시간 데이터가 필요한 경우:
                - 현재 가격, 시세, 환율 등 시간에 따라 변하는 수치
                - 오늘/현재/지금의 상태를 묻는 질문
                - 실시간 뉴스, 속보, 이벤트
                - 날씨, 스포츠 경기 결과 등 시시각각 변하는 정보
                - 새로운 암호화폐, 주식, 자산의 현재 가치
                """.formatted(query);

        try {
            String response = aiDoveClient.chat(prompt, null)
                    .map(r -> r.reply())
                    .block(java.time.Duration.ofSeconds(10));

            if (response != null && response.contains("{")) {
                // JSON 파싱
                int start = response.indexOf("{");
                int end = response.lastIndexOf("}") + 1;
                String json = response.substring(start, end);
                
                // 간단한 파싱 (ObjectMapper 없이)
                boolean needsRealtime = json.contains("\"needs_realtime\": true") || 
                                        json.contains("\"needs_realtime\":true");
                
                String dataType = extractJsonValue(json, "data_type");
                String reason = extractJsonValue(json, "reason");
                double confidence = parseConfidence(extractJsonValue(json, "confidence"));
                List<String> entities = parseEntities(json);

                return RealtimeAnalysisResult.builder()
                        .needsRealtimeData(needsRealtime)
                        .dataType(dataType != null ? dataType : "unknown")
                        .reason("LLM: " + (reason != null ? reason : "analyzed"))
                        .confidence(confidence)
                        .entities(entities)
                        .timestamp(System.currentTimeMillis())
                        .build();
            }
        } catch (Exception e) {
            log.debug("LLM realtime analysis failed: {}", e.getMessage());
        }

        return null;
    }

    private String extractJsonValue(String json, String key) {
        Pattern pattern = Pattern.compile("\"" + key + "\"\\s*:\\s*\"([^\"]+)\"");
        Matcher matcher = pattern.matcher(json);
        if (matcher.find()) {
            return matcher.group(1);
        }
        return null;
    }

    private double parseConfidence(String value) {
        if (value == null) return 0.5;
        try {
            return Double.parseDouble(value);
        } catch (NumberFormatException e) {
            return 0.5;
        }
    }

    private List<String> parseEntities(String json) {
        List<String> entities = new ArrayList<>();
        Pattern pattern = Pattern.compile("\"entities\"\\s*:\\s*\\[([^\\]]+)\\]");
        Matcher matcher = pattern.matcher(json);
        if (matcher.find()) {
            String entitiesStr = matcher.group(1);
            Pattern entityPattern = Pattern.compile("\"([^\"]+)\"");
            Matcher entityMatcher = entityPattern.matcher(entitiesStr);
            while (entityMatcher.find()) {
                entities.add(entityMatcher.group(1));
            }
        }
        return entities;
    }

    private RealtimeAnalysisResult combineResults(
            RealtimeAnalysisResult heuristic, 
            RealtimeAnalysisResult semantic) {
        
        double combinedConfidence = Math.max(heuristic.getConfidence(), semantic.getConfidence());
        boolean needsRealtime = combinedConfidence >= 0.6;
        
        String dataType = !"unknown".equals(heuristic.getDataType()) 
                ? heuristic.getDataType() 
                : semantic.getDataType();
        
        List<String> allEntities = new ArrayList<>(heuristic.getEntities());
        for (String entity : semantic.getEntities()) {
            if (!allEntities.contains(entity)) {
                allEntities.add(entity);
            }
        }
        
        String reason = (heuristic.getReason() + " " + semantic.getReason()).trim();

        return RealtimeAnalysisResult.builder()
                .needsRealtimeData(needsRealtime)
                .dataType(dataType)
                .reason(reason)
                .confidence(combinedConfidence)
                .entities(allEntities)
                .timestamp(System.currentTimeMillis())
                .build();
    }

    private RealtimeAnalysisResult buildDefaultRealtimeResult(boolean needsRealtime, String reason) {
        return RealtimeAnalysisResult.builder()
                .needsRealtimeData(needsRealtime)
                .dataType("unknown")
                .reason(reason)
                .confidence(needsRealtime ? 0.5 : 0.0)
                .entities(List.of())
                .timestamp(System.currentTimeMillis())
                .build();
    }

    private void cacheResult(String key, RealtimeAnalysisResult result) {
        // 캐시 크기 제한
        if (realtimeAnalysisCache.size() >= MAX_CACHE_SIZE) {
            // 가장 오래된 항목 제거 (간단한 구현)
            realtimeAnalysisCache.entrySet().stream()
                    .min(Comparator.comparingLong(e -> e.getValue().getTimestamp()))
                    .ifPresent(oldest -> realtimeAnalysisCache.remove(oldest.getKey()));
        }
        realtimeAnalysisCache.put(key, result);
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/search/EmbeddingService.java

```java
package com.newsinsight.collector.service.search;

import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.MediaType;
import org.springframework.stereotype.Service;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Mono;

import jakarta.annotation.PostConstruct;
import java.time.Duration;
import java.util.Collections;
import java.util.List;
import java.util.Map;

/**
 * 텍스트 임베딩 서비스.
 * HuggingFace Text Embeddings Inference (TEI) 서버와 연동하여
 * 텍스트를 벡터로 변환합니다.
 * 
 * 지원 모델:
 * - intfloat/multilingual-e5-large (다국어, 1024차원)
 * - sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 (다국어, 384차원)
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class EmbeddingService {

    @Value("${collector.embedding.enabled:true}")
    private boolean enabled;

    @Value("${collector.embedding.base-url:http://localhost:8011}")
    private String baseUrl;

    @Value("${collector.embedding.model:intfloat/multilingual-e5-large}")
    private String modelName;

    @Value("${collector.embedding.timeout-seconds:30}")
    private int timeoutSeconds;

    @Value("${collector.embedding.dimension:1024}")
    private int embeddingDimension;

    private WebClient webClient;

    @PostConstruct
    public void init() {
        this.webClient = WebClient.builder()
                .baseUrl(baseUrl)
                .build();
        log.info("EmbeddingService initialized: enabled={}, baseUrl={}, model={}, dimension={}", 
                enabled, baseUrl, modelName, embeddingDimension);
    }

    /**
     * 텍스트를 벡터로 변환합니다.
     *
     * @param text 변환할 텍스트
     * @return 임베딩 벡터 (float 배열)
     */
    public Mono<float[]> embed(String text) {
        if (!enabled) {
            return Mono.empty();
        }

        if (text == null || text.isBlank()) {
            return Mono.just(new float[embeddingDimension]);
        }

        // E5 모델은 검색 쿼리에 "query: " 접두사를 붙이면 성능이 향상됨
        String processedText = text.length() > 8000 ? text.substring(0, 8000) : text;

        return webClient.post()
                .uri("/embed")
                .contentType(MediaType.APPLICATION_JSON)
                .bodyValue(Map.of("inputs", processedText))
                .retrieve()
                .bodyToMono(float[][].class)
                .timeout(Duration.ofSeconds(timeoutSeconds))
                .map(result -> result != null && result.length > 0 ? result[0] : new float[embeddingDimension])
                .doOnError(e -> log.error("Embedding failed for text (length={}): {}", 
                        text.length(), e.getMessage()))
                .onErrorReturn(new float[embeddingDimension]);
    }

    /**
     * 검색 쿼리를 벡터로 변환합니다.
     * E5 모델의 경우 "query: " 접두사를 추가합니다.
     *
     * @param query 검색 쿼리
     * @return 임베딩 벡터
     */
    public Mono<float[]> embedQuery(String query) {
        if (!enabled) {
            return Mono.empty();
        }

        // E5 모델용 쿼리 접두사
        String prefixedQuery = modelName.contains("e5") 
                ? "query: " + query 
                : query;
        
        return embed(prefixedQuery);
    }

    /**
     * 문서를 벡터로 변환합니다.
     * E5 모델의 경우 "passage: " 접두사를 추가합니다.
     *
     * @param document 문서 텍스트
     * @return 임베딩 벡터
     */
    public Mono<float[]> embedDocument(String document) {
        if (!enabled) {
            return Mono.empty();
        }

        // E5 모델용 문서 접두사
        String prefixedDoc = modelName.contains("e5") 
                ? "passage: " + document 
                : document;
        
        return embed(prefixedDoc);
    }

    /**
     * 여러 텍스트를 일괄 벡터 변환합니다.
     *
     * @param texts 변환할 텍스트 목록
     * @return 임베딩 벡터 목록
     */
    public Mono<List<float[]>> embedBatch(List<String> texts) {
        if (!enabled || texts == null || texts.isEmpty()) {
            return Mono.just(List.of());
        }

        // 텍스트 전처리
        List<String> processedTexts = texts.stream()
                .map(t -> t != null && t.length() > 8000 ? t.substring(0, 8000) : t)
                .map(t -> t != null ? t : "")
                .toList();

        return webClient.post()
                .uri("/embed")
                .contentType(MediaType.APPLICATION_JSON)
                .bodyValue(Map.of("inputs", processedTexts))
                .retrieve()
                .bodyToMono(float[][].class)
                .timeout(Duration.ofSeconds(timeoutSeconds * 2))
                .map(result -> result != null ? List.of(result) : List.<float[]>of())
                .doOnError(e -> log.error("Batch embedding failed for {} texts: {}", 
                        texts.size(), e.getMessage()))
                .onErrorResume(e -> Mono.just(List.<float[]>of()));
    }

    /**
     * 두 벡터 간의 코사인 유사도를 계산합니다.
     *
     * @param vec1 첫 번째 벡터
     * @param vec2 두 번째 벡터
     * @return 코사인 유사도 (-1 ~ 1)
     */
    public double cosineSimilarity(float[] vec1, float[] vec2) {
        if (vec1 == null || vec2 == null || vec1.length != vec2.length) {
            return 0.0;
        }

        double dotProduct = 0.0;
        double norm1 = 0.0;
        double norm2 = 0.0;

        for (int i = 0; i < vec1.length; i++) {
            dotProduct += vec1[i] * vec2[i];
            norm1 += vec1[i] * vec1[i];
            norm2 += vec2[i] * vec2[i];
        }

        if (norm1 == 0 || norm2 == 0) {
            return 0.0;
        }

        return dotProduct / (Math.sqrt(norm1) * Math.sqrt(norm2));
    }

    /**
     * 서비스 활성화 여부를 반환합니다.
     */
    public boolean isEnabled() {
        return enabled;
    }

    /**
     * 임베딩 차원을 반환합니다.
     */
    public int getDimension() {
        return embeddingDimension;
    }

    /**
     * 임베딩 서버 상태를 확인합니다.
     */
    public Mono<Boolean> healthCheck() {
        if (!enabled) {
            return Mono.just(false);
        }

        return webClient.get()
                .uri("/health")
                .retrieve()
                .bodyToMono(String.class)
                .timeout(Duration.ofSeconds(5))
                .map(response -> true)
                .onErrorReturn(false);
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/search/HybridRankingService.java

```java
package com.newsinsight.collector.service.search;

import com.newsinsight.collector.service.search.AdvancedIntentAnalyzer.AnalyzedQuery;
import com.newsinsight.collector.service.search.AdvancedIntentAnalyzer.FallbackStrategy;
import lombok.Builder;
import lombok.Data;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;

import java.util.*;
import java.util.stream.Collectors;

/**
 * Reciprocal Rank Fusion (RRF) 기반 하이브리드 랭킹 서비스.
 * 
 * 여러 검색 소스의 결과를 RRF 알고리즘으로 융합하여
 * 사용자 의도에 맞는 최적의 순서로 정렬합니다.
 * 
 * RRF 공식: score(d) = Σ 1/(k + rank_i(d))
 * - k: 상수 (기본값 60, 낮을수록 상위 랭크에 가중치)
 * - rank_i(d): i번째 검색 소스에서 문서 d의 순위
 * 
 * 참고: https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class HybridRankingService {

    private final AdvancedIntentAnalyzer advancedIntentAnalyzer;

    // RRF 상수 k - 낮을수록 상위 결과에 더 높은 가중치
    private static final double RRF_K = 60.0;

    // 검색 소스별 기본 가중치
    private static final Map<String, Double> DEFAULT_SOURCE_WEIGHTS = Map.of(
            "keyword", 1.0,      // 키워드 검색
            "semantic", 1.0,    // 시맨틱(벡터) 검색
            "database", 0.9,    // DB 검색 (기존 수집 데이터)
            "web", 0.8,         // 웹 크롤링
            "ai", 0.7           // AI 분석 결과
    );

    /**
     * 여러 소스의 검색 결과를 RRF로 융합합니다.
     *
     * @param rankedLists 소스별 검색 결과 (소스명 → 순위별 결과 리스트)
     * @return RRF 점수로 정렬된 통합 결과
     */
    public List<RankedResult> fuseResults(Map<String, List<SearchCandidate>> rankedLists) {
        return fuseResults(rankedLists, DEFAULT_SOURCE_WEIGHTS, null);
    }

    /**
     * 쿼리 의도에 따라 가중치를 조정하여 결과를 융합합니다.
     *
     * @param rankedLists 소스별 검색 결과
     * @param intent 쿼리 의도 (null이면 기본 가중치 사용)
     * @return RRF 점수로 정렬된 통합 결과
     */
    public List<RankedResult> fuseResults(
            Map<String, List<SearchCandidate>> rankedLists,
            QueryIntent intent) {
        
        Map<String, Double> adjustedWeights = adjustWeightsForIntent(intent);
        return fuseResults(rankedLists, adjustedWeights, intent);
    }

    /**
     * AdvancedIntentAnalyzer의 AnalyzedQuery를 사용하여 결과를 융합합니다.
     * 더 정교한 의도 분석과 키워드 부스팅을 적용합니다.
     *
     * @param rankedLists 소스별 검색 결과
     * @param analyzedQuery 분석된 쿼리 정보
     * @return RRF 점수로 정렬된 통합 결과
     */
    public List<RankedResult> fuseResultsWithAnalyzedQuery(
            Map<String, List<SearchCandidate>> rankedLists,
            AnalyzedQuery analyzedQuery) {

        if (rankedLists == null || rankedLists.isEmpty()) {
            return List.of();
        }

        // AnalyzedQuery에서 QueryIntent로 변환
        QueryIntent intent = advancedIntentAnalyzer.toQueryIntent(analyzedQuery);
        Map<String, Double> adjustedWeights = adjustWeightsForAnalyzedQuery(analyzedQuery);

        // 기본 RRF 융합 수행
        List<RankedResult> results = fuseResults(rankedLists, adjustedWeights, intent);

        // AnalyzedQuery 기반 향상된 부스팅 적용
        results = applyAdvancedBoost(results, analyzedQuery);

        log.debug("RRF fusion with AnalyzedQuery: {} sources → {} results, intent={}, confidence={}",
                rankedLists.size(), results.size(), analyzedQuery.getIntentType(), analyzedQuery.getConfidence());

        return results;
    }

    /**
     * 결과가 없을 때 폴백 전략을 사용하여 검색 쿼리를 제안합니다.
     *
     * @param analyzedQuery 분석된 쿼리 정보
     * @return 다음 시도할 검색 쿼리 (폴백 전략에 따라)
     */
    public Optional<String> getNextFallbackQuery(AnalyzedQuery analyzedQuery, int attemptIndex) {
        List<FallbackStrategy> strategies = analyzedQuery.getFallbackStrategies();
        if (strategies == null || attemptIndex >= strategies.size()) {
            return Optional.empty();
        }
        return Optional.of(strategies.get(attemptIndex).getQuery());
    }

    /**
     * 쿼리를 분석하고 결과를 융합합니다.
     * AdvancedIntentAnalyzer를 내부적으로 사용합니다.
     *
     * @param query 검색 쿼리
     * @param rankedLists 소스별 검색 결과
     * @return RRF 점수로 정렬된 통합 결과
     */
    public List<RankedResult> analyzeAndFuse(String query, Map<String, List<SearchCandidate>> rankedLists) {
        AnalyzedQuery analyzedQuery = advancedIntentAnalyzer.analyzeQuery(query);
        return fuseResultsWithAnalyzedQuery(rankedLists, analyzedQuery);
    }

    /**
     * AnalyzedQuery 기반 소스 가중치 조정.
     */
    private Map<String, Double> adjustWeightsForAnalyzedQuery(AnalyzedQuery analyzed) {
        Map<String, Double> adjusted = new HashMap<>(DEFAULT_SOURCE_WEIGHTS);

        // 기본 의도 기반 조정
        switch (analyzed.getIntentType()) {
            case FACT_CHECK:
                adjusted.put("database", 1.3);  // 검증된 DB 데이터 우선
                adjusted.put("ai", 1.2);        // AI 분석
                adjusted.put("semantic", 1.1);
                adjusted.put("web", 0.7);       // 웹 결과는 낮게
                break;

            case LATEST_NEWS:
                adjusted.put("web", 1.3);       // 최신 웹 정보 우선
                adjusted.put("keyword", 1.2);
                adjusted.put("database", 0.8);  // DB는 최신 아닐 수 있음
                break;

            case DEEP_ANALYSIS:
                adjusted.put("semantic", 1.3);  // 의미적 유사도 중요
                adjusted.put("ai", 1.2);
                adjusted.put("database", 1.1);
                adjusted.put("keyword", 0.9);
                break;

            case OPINION_SEARCH:
                adjusted.put("semantic", 1.2);
                adjusted.put("web", 1.1);
                adjusted.put("database", 1.0);
                break;

            case GENERAL:
            default:
                break;
        }

        // 신뢰도 기반 미세 조정
        double confidence = analyzed.getConfidence();
        if (confidence > 0.8) {
            // 높은 신뢰도: 의도에 맞는 가중치 강화
            for (String key : adjusted.keySet()) {
                double current = adjusted.get(key);
                if (current > 1.0) {
                    adjusted.put(key, current * 1.1);  // 추가 10% 부스트
                }
            }
        }

        return adjusted;
    }

    /**
     * AnalyzedQuery 기반 향상된 결과 부스팅.
     */
    private List<RankedResult> applyAdvancedBoost(List<RankedResult> results, AnalyzedQuery analyzed) {
        if (results.isEmpty()) {
            return results;
        }

        List<String> keywords = analyzed.getKeywords();
        String primaryKeyword = analyzed.getPrimaryKeyword();

        for (RankedResult result : results) {
            double boost = 0.0;

            // 1. 키워드 매칭 부스트
            String text = buildSearchableText(result);

            // 주요 키워드 매칭 (가장 높은 부스트)
            if (primaryKeyword != null && !primaryKeyword.isBlank() && 
                    text.contains(primaryKeyword.toLowerCase())) {
                boost += 0.15;
            }

            // 기타 키워드 매칭
            if (keywords != null && !keywords.isEmpty()) {
                int matchCount = 0;
                for (String keyword : keywords) {
                    if (text.contains(keyword.toLowerCase())) {
                        matchCount++;
                    }
                }
                boost += (double) matchCount / keywords.size() * 0.1;
            }

            // 2. 의도별 추가 부스트
            switch (analyzed.getIntentType()) {
                case LATEST_NEWS:
                    // 최신성 부스트
                    boost += calculateRecencyBoostAdvanced(result.getPublishedAt());
                    break;

                case FACT_CHECK:
                    // 신뢰할 수 있는 출처 부스트
                    if (result.getSources() != null && result.getSources().contains("database")) {
                        boost += 0.1;
                    }
                    break;

                case DEEP_ANALYSIS:
                    // 긴 콘텐츠 부스트 (더 상세한 정보)
                    if (result.getContent() != null && result.getContent().length() > 500) {
                        boost += 0.05;
                    }
                    break;

                default:
                    break;
            }

            // 3. 다중 소스 부스트
            if (result.getSources() != null && result.getSources().size() > 1) {
                boost += 0.1 * (result.getSources().size() - 1);
            }

            // 부스트 적용
            result.setRrfScore(result.getRrfScore() * (1 + boost));
        }

        // 재정렬
        results.sort(Comparator.comparingDouble(RankedResult::getRrfScore).reversed());
        return results;
    }

    private String buildSearchableText(RankedResult result) {
        StringBuilder text = new StringBuilder();
        if (result.getTitle() != null) {
            text.append(result.getTitle()).append(" ");
        }
        if (result.getSnippet() != null) {
            text.append(result.getSnippet()).append(" ");
        }
        if (result.getContent() != null) {
            text.append(result.getContent().substring(0, Math.min(200, result.getContent().length())));
        }
        return text.toString().toLowerCase();
    }

    private double calculateRecencyBoostAdvanced(String publishedAt) {
        if (publishedAt == null || publishedAt.isBlank()) {
            return 0;
        }

        try {
            // ISO 날짜 파싱 시도
            java.time.LocalDateTime published;
            if (publishedAt.length() > 10) {
                published = java.time.LocalDateTime.parse(publishedAt.replace(" ", "T").substring(0, 19));
            } else {
                published = java.time.LocalDate.parse(publishedAt).atStartOfDay();
            }

            java.time.LocalDateTime now = java.time.LocalDateTime.now();
            long hoursDiff = java.time.Duration.between(published, now).toHours();

            // 최신일수록 높은 부스트
            if (hoursDiff < 24) return 0.2;       // 24시간 내
            if (hoursDiff < 72) return 0.15;      // 3일 내
            if (hoursDiff < 168) return 0.1;      // 1주일 내
            if (hoursDiff < 720) return 0.05;     // 30일 내
            return 0;
        } catch (Exception e) {
            return 0.05; // 파싱 실패시 기본 부스트
        }
    }

    /**
     * 커스텀 가중치로 결과를 융합합니다.
     *
     * @param rankedLists 소스별 검색 결과
     * @param sourceWeights 소스별 가중치
     * @param intent 쿼리 의도 (후처리용)
     * @return RRF 점수로 정렬된 통합 결과
     */
    public List<RankedResult> fuseResults(
            Map<String, List<SearchCandidate>> rankedLists,
            Map<String, Double> sourceWeights,
            QueryIntent intent) {

        if (rankedLists == null || rankedLists.isEmpty()) {
            return List.of();
        }

        // 문서 ID → RRF 점수 누적
        Map<String, Double> rrfScores = new HashMap<>();
        // 문서 ID → 원본 정보
        Map<String, SearchCandidate> candidateMap = new HashMap<>();
        // 문서 ID → 출처 소스들
        Map<String, Set<String>> documentSources = new HashMap<>();

        for (Map.Entry<String, List<SearchCandidate>> entry : rankedLists.entrySet()) {
            String source = entry.getKey();
            List<SearchCandidate> candidates = entry.getValue();
            double weight = sourceWeights.getOrDefault(source, 1.0);

            for (int rank = 0; rank < candidates.size(); rank++) {
                SearchCandidate candidate = candidates.get(rank);
                String docId = candidate.getId();

                // RRF 점수 계산: weight * 1/(k + rank)
                double rrfScore = weight * (1.0 / (RRF_K + rank + 1));
                rrfScores.merge(docId, rrfScore, Double::sum);

                // 원본 정보 저장 (처음 등장한 정보 유지)
                candidateMap.putIfAbsent(docId, candidate);

                // 출처 추적
                documentSources.computeIfAbsent(docId, k -> new HashSet<>()).add(source);
            }
        }

        // RRF 점수순 정렬
        List<RankedResult> results = rrfScores.entrySet().stream()
                .map(entry -> {
                    String docId = entry.getKey();
                    SearchCandidate candidate = candidateMap.get(docId);
                    return RankedResult.builder()
                            .id(docId)
                            .title(candidate.getTitle())
                            .snippet(candidate.getSnippet())
                            .content(candidate.getContent())
                            .url(candidate.getUrl())
                            .publishedAt(candidate.getPublishedAt())
                            .rrfScore(entry.getValue())
                            .originalScore(candidate.getOriginalScore())
                            .sources(documentSources.get(docId))
                            .metadata(candidate.getMetadata())
                            .build();
                })
                .sorted(Comparator.comparingDouble(RankedResult::getRrfScore).reversed())
                .collect(Collectors.toList());

        // 의도 기반 후처리 (재정렬)
        if (intent != null) {
            results = applyIntentBoost(results, intent);
        }

        log.debug("RRF fusion completed: {} sources → {} unique results", 
                rankedLists.size(), results.size());

        return results;
    }

    /**
     * 쿼리 의도에 따라 소스 가중치를 조정합니다.
     */
    private Map<String, Double> adjustWeightsForIntent(QueryIntent intent) {
        if (intent == null) {
            return DEFAULT_SOURCE_WEIGHTS;
        }

        Map<String, Double> adjusted = new HashMap<>(DEFAULT_SOURCE_WEIGHTS);

        switch (intent.getType()) {
            case FACT_CHECK:
                // 팩트체크: DB(검증된 데이터)와 AI 분석 우선
                adjusted.put("database", 1.2);
                adjusted.put("ai", 1.1);
                adjusted.put("semantic", 1.0);
                adjusted.put("web", 0.7);
                break;

            case LATEST_NEWS:
                // 최신 뉴스: 웹 크롤링과 키워드 검색 우선
                adjusted.put("web", 1.2);
                adjusted.put("keyword", 1.1);
                adjusted.put("database", 0.9);
                adjusted.put("semantic", 0.8);
                break;

            case DEEP_ANALYSIS:
                // 심층 분석: 시맨틱 검색과 AI 분석 우선
                adjusted.put("semantic", 1.2);
                adjusted.put("ai", 1.1);
                adjusted.put("database", 1.0);
                adjusted.put("keyword", 0.8);
                break;

            case OPINION_SEARCH:
                // 여론/의견 검색: 다양한 소스 균형
                adjusted.put("semantic", 1.1);
                adjusted.put("web", 1.0);
                adjusted.put("database", 1.0);
                adjusted.put("ai", 0.9);
                break;

            case GENERAL:
            default:
                // 일반 검색: 기본 가중치 유지
                break;
        }

        return adjusted;
    }

    /**
     * 의도 기반 부스팅을 적용합니다.
     */
    private List<RankedResult> applyIntentBoost(List<RankedResult> results, QueryIntent intent) {
        // 키워드가 포함된 결과 부스팅
        if (intent.getKeywords() != null && !intent.getKeywords().isEmpty()) {
            for (RankedResult result : results) {
                double keywordBoost = calculateKeywordBoost(result, intent.getKeywords());
                result.setRrfScore(result.getRrfScore() * (1 + keywordBoost));
            }
        }

        // 시간 기반 부스팅 (최신 뉴스 의도일 경우)
        if (intent.getType() == QueryIntent.IntentType.LATEST_NEWS) {
            for (RankedResult result : results) {
                double recencyBoost = calculateRecencyBoost(result.getPublishedAt());
                result.setRrfScore(result.getRrfScore() * (1 + recencyBoost));
            }
        }

        // 다중 소스 부스팅 (여러 소스에서 발견된 문서 신뢰도 향상)
        for (RankedResult result : results) {
            int sourceCount = result.getSources() != null ? result.getSources().size() : 1;
            if (sourceCount > 1) {
                double multiSourceBoost = 0.1 * (sourceCount - 1);
                result.setRrfScore(result.getRrfScore() * (1 + multiSourceBoost));
            }
        }

        // 재정렬
        results.sort(Comparator.comparingDouble(RankedResult::getRrfScore).reversed());
        return results;
    }

    /**
     * 키워드 부스트 점수를 계산합니다.
     */
    private double calculateKeywordBoost(RankedResult result, List<String> keywords) {
        String text = (result.getTitle() + " " + 
                      (result.getSnippet() != null ? result.getSnippet() : "")).toLowerCase();
        
        int matchCount = 0;
        for (String keyword : keywords) {
            if (text.contains(keyword.toLowerCase())) {
                matchCount++;
            }
        }
        
        return keywords.isEmpty() ? 0 : (double) matchCount / keywords.size() * 0.2;
    }

    /**
     * 최신성 부스트 점수를 계산합니다.
     */
    private double calculateRecencyBoost(String publishedAt) {
        if (publishedAt == null) return 0;
        
        try {
            // 간단한 최신성 계산 (더 정교한 로직으로 대체 가능)
            // publishedAt이 최근일수록 높은 부스트
            return 0.1; // 기본 부스트
        } catch (Exception e) {
            return 0;
        }
    }

    // ==================== Inner Classes ====================

    /**
     * 검색 후보 문서
     */
    @Data
    @Builder
    public static class SearchCandidate {
        private String id;
        private String title;
        private String snippet;
        private String content;
        private String url;
        private String publishedAt;
        private Double originalScore;  // 원본 검색 점수 (유사도, relevance 등)
        private Map<String, Object> metadata;
    }

    /**
     * RRF 점수가 포함된 최종 결과
     */
    @Data
    @Builder
    public static class RankedResult {
        private String id;
        private String title;
        private String snippet;
        private String content;
        private String url;
        private String publishedAt;
        private double rrfScore;
        private Double originalScore;
        private Set<String> sources;  // 이 문서가 발견된 소스들
        private Map<String, Object> metadata;
    }

    /**
     * 쿼리 의도
     */
    @Data
    @Builder
    public static class QueryIntent {
        private IntentType type;
        private List<String> keywords;
        private String timeRange;
        private Double confidence;

        public enum IntentType {
            GENERAL,        // 일반 검색
            FACT_CHECK,     // 팩트체크/검증
            LATEST_NEWS,    // 최신 뉴스
            DEEP_ANALYSIS,  // 심층 분석
            OPINION_SEARCH  // 여론/의견 검색
        }
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/search/HybridSearchService.java

```java
package com.newsinsight.collector.service.search;

import com.newsinsight.collector.entity.CollectedData;
import com.newsinsight.collector.repository.CollectedDataRepository;
import com.newsinsight.collector.service.search.HybridRankingService.QueryIntent;
import com.newsinsight.collector.service.search.HybridRankingService.RankedResult;
import com.newsinsight.collector.service.search.HybridRankingService.SearchCandidate;
import com.newsinsight.collector.service.search.VectorSearchService.ScoredDocument;
import lombok.Builder;
import lombok.Data;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.PageRequest;
import org.springframework.data.domain.Sort;
import org.springframework.stereotype.Service;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Mono;
import reactor.core.scheduler.Schedulers;

import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.time.format.DateTimeParseException;
import java.util.*;
import java.util.concurrent.ConcurrentHashMap;
import java.util.stream.Collectors;

/**
 * 하이브리드 검색 서비스 - 키워드 + 시맨틱 + RRF 통합 검색
 * 
 * 검색 흐름:
 * 1. QueryIntentAnalyzer로 사용자 의도 분석
 * 2. 병렬로 키워드 검색(DB) + 시맨틱 검색(pgvector) 실행
 * 3. HybridRankingService의 RRF 알고리즘으로 결과 융합
 * 4. 의도 기반 부스팅 적용 후 최종 결과 반환
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class HybridSearchService {

    private final CollectedDataRepository collectedDataRepository;
    private final VectorSearchService vectorSearchService;
    private final EmbeddingService embeddingService;
    private final HybridRankingService hybridRankingService;
    private final QueryIntentAnalyzer queryIntentAnalyzer;

    @Value("${collector.hybrid-search.enabled:true}")
    private boolean enabled;

    @Value("${collector.hybrid-search.keyword-top-k:30}")
    private int keywordTopK;

    @Value("${collector.hybrid-search.semantic-top-k:20}")
    private int semanticTopK;

    @Value("${collector.hybrid-search.final-top-k:20}")
    private int finalTopK;

    @Value("${collector.hybrid-search.semantic-weight:1.0}")
    private double semanticWeight;

    @Value("${collector.hybrid-search.keyword-weight:1.0}")
    private double keywordWeight;

    /**
     * 하이브리드 검색을 실행합니다.
     * 키워드 검색과 시맨틱 검색을 병렬로 실행하고 RRF로 융합합니다.
     *
     * @param query 검색 쿼리
     * @param window 시간 범위 (1d, 7d, 30d)
     * @return RRF 점수순 정렬된 검색 결과
     */
    public Mono<HybridSearchResult> search(String query, String window) {
        return search(query, window, null, null);
    }

    /**
     * 하이브리드 검색을 실행합니다 (커스텀 날짜 범위 지원).
     * 키워드 검색과 시맨틱 검색을 병렬로 실행하고 RRF로 융합합니다.
     *
     * @param query 검색 쿼리
     * @param window 시간 범위 (1d, 7d, 30d) - startDate가 지정되면 무시됨
     * @param startDate 커스텀 시작 날짜 (ISO 8601 형식)
     * @param endDate 커스텀 종료 날짜 (ISO 8601 형식)
     * @return RRF 점수순 정렬된 검색 결과
     */
    public Mono<HybridSearchResult> search(String query, String window, String startDate, String endDate) {
        if (query == null || query.isBlank()) {
            return Mono.just(HybridSearchResult.empty());
        }

        log.info("Starting hybrid search: query='{}', window={}, startDate={}, endDate={}", 
                query, window, startDate, endDate);
        long startTime = System.currentTimeMillis();

        // 1. 의도 분석
        QueryIntent intent = queryIntentAnalyzer.analyzeIntent(query);
        log.debug("Query intent: type={}, confidence={}, keywords={}", 
                intent.getType(), intent.getConfidence(), intent.getKeywords());

        // Calculate date range
        LocalDateTime since = calculateSinceDate(window, intent, startDate);
        LocalDateTime until = calculateEndDate(endDate);
        
        log.debug("Effective date range: {} to {}", since, until != null ? until : "now");

        // 2. 병렬 검색 실행
        Mono<List<SearchCandidate>> keywordResults = searchKeyword(query, since, until)
                .subscribeOn(Schedulers.boundedElastic());
        
        Mono<List<SearchCandidate>> semanticResults = searchSemantic(query, since, until)
                .subscribeOn(Schedulers.boundedElastic());

        // 3. 결과 융합
        return Mono.zip(keywordResults, semanticResults)
                .map(tuple -> {
                    Map<String, List<SearchCandidate>> rankedLists = new HashMap<>();
                    
                    if (!tuple.getT1().isEmpty()) {
                        rankedLists.put("keyword", tuple.getT1());
                    }
                    if (!tuple.getT2().isEmpty()) {
                        rankedLists.put("semantic", tuple.getT2());
                    }

                    if (rankedLists.isEmpty()) {
                        log.info("Hybrid search found no results for query: '{}'", query);
                        return HybridSearchResult.empty();
                    }

                    // RRF 융합
                    List<RankedResult> fusedResults = hybridRankingService.fuseResults(rankedLists, intent);
                    
                    // 상위 N개만 반환
                    List<RankedResult> topResults = fusedResults.stream()
                            .limit(finalTopK)
                            .toList();

                    long elapsed = System.currentTimeMillis() - startTime;
                    log.info("Hybrid search completed: query='{}', keyword={}, semantic={}, fused={}, time={}ms",
                            query, tuple.getT1().size(), tuple.getT2().size(), topResults.size(), elapsed);

                    return HybridSearchResult.builder()
                            .query(query)
                            .intent(intent)
                            .results(topResults)
                            .keywordResultCount(tuple.getT1().size())
                            .semanticResultCount(tuple.getT2().size())
                            .totalResultCount(topResults.size())
                            .searchTimeMs(elapsed)
                            .build();
                })
                .onErrorResume(e -> {
                    log.error("Hybrid search failed for query '{}': {}", query, e.getMessage());
                    return Mono.just(HybridSearchResult.empty());
                });
    }

    /**
     * 키워드 기반 검색 (PostgreSQL LIKE/Full-text)
     */
    private Mono<List<SearchCandidate>> searchKeyword(String query, LocalDateTime since, LocalDateTime until) {
        return Mono.fromCallable(() -> {
            try {
                // Note: Native query already has ORDER BY clause, so use unsorted PageRequest
                PageRequest pageRequest = PageRequest.of(0, keywordTopK, Sort.unsorted());

                Page<CollectedData> page;
                if (until != null) {
                    // Use date range query
                    page = collectedDataRepository.searchByQueryAndDateRange(
                            query, since, until, pageRequest);
                } else {
                    // Use since-only query
                    page = collectedDataRepository.searchByQueryAndSince(
                            query, since, pageRequest);
                }

                return page.getContent().stream()
                        .map(data -> SearchCandidate.builder()
                                .id(data.getId() != null ? data.getId().toString() : UUID.randomUUID().toString())
                                .title(data.getTitle())
                                .snippet(buildSnippet(data.getContent(), 200))
                                .content(data.getContent())
                                .url(data.getUrl())
                                .publishedAt(data.getPublishedDate() != null 
                                        ? data.getPublishedDate().toString() 
                                        : null)
                                .originalScore(data.getQualityScore())
                                .metadata(Map.of(
                                        "sourceId", data.getSourceId() != null ? data.getSourceId() : 0L,
                                        "collectedAt", data.getCollectedAt() != null ? data.getCollectedAt().toString() : ""
                                ))
                                .build())
                        .collect(Collectors.toList());
            } catch (Exception e) {
                log.error("Keyword search failed: {}", e.getMessage());
                return List.of();
            }
        });
    }

    /**
     * 시맨틱(벡터) 검색 (pgvector)
     */
    private Mono<List<SearchCandidate>> searchSemantic(String query, LocalDateTime since, LocalDateTime until) {
        if (!vectorSearchService.isAvailable()) {
            log.debug("Vector search not available, skipping semantic search");
            return Mono.just(List.of());
        }

        return vectorSearchService.searchSimilar(query, semanticTopK)
                .map(scoredDocs -> {
                    if (scoredDocs == null || scoredDocs.isEmpty()) {
                        return List.<SearchCandidate>of();
                    }

                    // ID로 전체 데이터 조회
                    List<Long> docIds = scoredDocs.stream()
                            .map(ScoredDocument::id)
                            .filter(Objects::nonNull)
                            .toList();

                    if (docIds.isEmpty()) {
                        return List.<SearchCandidate>of();
                    }

                    Map<Long, CollectedData> dataMap = collectedDataRepository.findAllById(docIds)
                            .stream()
                            .collect(Collectors.toMap(CollectedData::getId, d -> d));

                    // 유사도 점수 매핑
                    Map<Long, Double> similarityMap = scoredDocs.stream()
                            .collect(Collectors.toMap(ScoredDocument::id, ScoredDocument::similarity));

                    return scoredDocs.stream()
                            .filter(doc -> dataMap.containsKey(doc.id()))
                            .filter(doc -> {
                                // 시간 필터링 적용 (since ~ until)
                                CollectedData data = dataMap.get(doc.id());
                                LocalDateTime publishedDate = data.getPublishedDate();
                                if (publishedDate == null) {
                                    publishedDate = data.getCollectedAt();
                                }
                                if (publishedDate == null) {
                                    return true;  // No date info, include by default
                                }
                                // Check since
                                if (publishedDate.isBefore(since)) {
                                    return false;
                                }
                                // Check until (if specified)
                                if (until != null && publishedDate.isAfter(until)) {
                                    return false;
                                }
                                return true;
                            })
                            .map(doc -> {
                                CollectedData data = dataMap.get(doc.id());
                                return SearchCandidate.builder()
                                        .id(data.getId().toString())
                                        .title(data.getTitle())
                                        .snippet(buildSnippet(data.getContent(), 200))
                                        .content(data.getContent())
                                        .url(data.getUrl())
                                        .publishedAt(data.getPublishedDate() != null 
                                                ? data.getPublishedDate().toString() 
                                                : null)
                                        .originalScore(similarityMap.get(doc.id()))
                                        .metadata(Map.of(
                                                "similarity", similarityMap.get(doc.id()),
                                                "sourceId", data.getSourceId() != null ? data.getSourceId() : 0L
                                        ))
                                        .build();
                            })
                            .collect(Collectors.toList());
                })
                .onErrorResume(e -> {
                    log.error("Semantic search failed: {}", e.getMessage());
                    return Mono.just(List.of());
                });
    }

    /**
     * 실시간 스트리밍 하이브리드 검색
     * UnifiedSearchService와 통합하여 실시간 결과 스트리밍에 사용됩니다.
     *
     * @param query 검색 쿼리
     * @param window 시간 범위
     * @return 검색 결과 스트림
     */
    public Flux<RankedResult> searchStream(String query, String window) {
        return search(query, window)
                .flatMapMany(result -> Flux.fromIterable(result.getResults()));
    }

    /**
     * 하이브리드 검색이 활성화되어 있는지 확인합니다.
     */
    public boolean isEnabled() {
        return enabled;
    }

    /**
     * 시맨틱 검색이 가능한지 확인합니다.
     */
    public boolean isSemanticSearchAvailable() {
        return vectorSearchService.isAvailable() && embeddingService.isEnabled();
    }

    /**
     * 의도를 고려한 시간 범위 계산 (커스텀 날짜 지원)
     */
    private LocalDateTime calculateSinceDate(String window, QueryIntent intent, String startDate) {
        // Custom startDate takes priority
        if (startDate != null && !startDate.isBlank()) {
            try {
                return LocalDateTime.parse(startDate, DateTimeFormatter.ISO_DATE_TIME);
            } catch (DateTimeParseException e) {
                log.warn("Invalid startDate format: '{}', falling back to window", startDate);
            }
        }
        
        return calculateSinceDate(window, intent);
    }

    /**
     * 의도를 고려한 시간 범위 계산
     */
    private LocalDateTime calculateSinceDate(String window, QueryIntent intent) {
        LocalDateTime now = LocalDateTime.now();
        
        // 의도에서 추출된 시간 범위가 있으면 우선 사용
        if (intent != null && intent.getTimeRange() != null) {
            return switch (intent.getTimeRange()) {
                case "1d" -> now.minusDays(1);
                case "2d" -> now.minusDays(2);
                case "7d" -> now.minusDays(7);
                case "14d" -> now.minusDays(14);
                case "30d" -> now.minusDays(30);
                default -> calculateFromWindow(window, now);
            };
        }
        
        return calculateFromWindow(window, now);
    }

    /**
     * Calculate end date from custom endDate string
     */
    private LocalDateTime calculateEndDate(String endDate) {
        if (endDate != null && !endDate.isBlank()) {
            try {
                return LocalDateTime.parse(endDate, DateTimeFormatter.ISO_DATE_TIME);
            } catch (DateTimeParseException e) {
                log.warn("Invalid endDate format: '{}', using current time", endDate);
            }
        }
        return null;  // null means no upper limit (i.e., "now")
    }

    private LocalDateTime calculateFromWindow(String window, LocalDateTime now) {
        return switch (window != null ? window : "7d") {
            case "1h" -> now.minusHours(1);
            case "1d" -> now.minusDays(1);
            case "3d" -> now.minusDays(3);
            case "14d" -> now.minusDays(14);
            case "30d" -> now.minusDays(30);
            case "90d" -> now.minusDays(90);
            case "180d" -> now.minusDays(180);
            case "365d" -> now.minusDays(365);
            case "all" -> LocalDateTime.of(2000, 1, 1, 0, 0);
            default -> now.minusDays(7);
        };
    }

    /**
     * 텍스트에서 스니펫 생성
     */
    private String buildSnippet(String content, int maxLength) {
        if (content == null || content.isBlank()) {
            return null;
        }

        // HTML 태그 제거
        String text = content.replaceAll("<[^>]*>", " ")
                .replaceAll("\\s+", " ")
                .trim();

        if (text.length() <= maxLength) {
            return text;
        }

        // 단어 경계에서 자르기
        int cut = maxLength;
        for (int i = maxLength - 1; i > maxLength * 0.6; i--) {
            if (Character.isWhitespace(text.charAt(i))) {
                cut = i;
                break;
            }
        }

        return text.substring(0, cut).trim() + "...";
    }

    // ==================== Inner Classes ====================

    /**
     * 하이브리드 검색 결과
     */
    @Data
    @Builder
    public static class HybridSearchResult {
        private String query;
        private QueryIntent intent;
        private List<RankedResult> results;
        private int keywordResultCount;
        private int semanticResultCount;
        private int totalResultCount;
        private long searchTimeMs;

        public static HybridSearchResult empty() {
            return HybridSearchResult.builder()
                    .results(List.of())
                    .keywordResultCount(0)
                    .semanticResultCount(0)
                    .totalResultCount(0)
                    .searchTimeMs(0)
                    .build();
        }
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/search/QueryIntentAnalyzer.java

```java
package com.newsinsight.collector.service.search;

import com.newsinsight.collector.service.search.HybridRankingService.QueryIntent;
import com.newsinsight.collector.service.search.HybridRankingService.QueryIntent.IntentType;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;

import java.util.*;
import java.util.regex.Pattern;

/**
 * 쿼리 의도 분석 서비스.
 * 
 * 사용자 쿼리를 분석하여 검색 의도를 파악하고,
 * 하이브리드 검색의 가중치 조정에 활용합니다.
 * 
 * 지원 의도:
 * - FACT_CHECK: 팩트체크/검증 ("사실인가", "진짜", "팩트체크")
 * - LATEST_NEWS: 최신 뉴스 ("오늘", "최근", "속보")
 * - DEEP_ANALYSIS: 심층 분석 ("분석", "원인", "배경")
 * - OPINION_SEARCH: 여론 검색 ("여론", "반응", "논란")
 * - GENERAL: 일반 검색
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class QueryIntentAnalyzer {

    // 의도별 키워드 패턴
    private static final Map<IntentType, List<String>> INTENT_KEYWORDS = Map.of(
            IntentType.FACT_CHECK, List.of(
                    "사실", "진짜", "가짜", "팩트체크", "팩트 체크", "검증",
                    "진위", "확인", "루머", "허위", "오보", "실제로",
                    "정말", "맞는", "틀린", "fact", "check", "verify"
            ),
            IntentType.LATEST_NEWS, List.of(
                    "오늘", "최근", "속보", "긴급", "방금", "지금",
                    "현재", "실시간", "최신", "breaking", "today",
                    "어제", "이번주", "금일"
            ),
            IntentType.DEEP_ANALYSIS, List.of(
                    "분석", "원인", "배경", "이유", "왜", "어떻게",
                    "영향", "전망", "예측", "해설", "설명", "의미",
                    "history", "analysis", "impact", "결과"
            ),
            IntentType.OPINION_SEARCH, List.of(
                    "여론", "반응", "논란", "비판", "지지", "반대",
                    "찬성", "의견", "댓글", "네티즌", "SNS", "트위터",
                    "opinion", "reaction", "controversy"
            )
    );

    // 질문 패턴
    private static final Pattern QUESTION_PATTERN = Pattern.compile(
            "(\\?|인가요|인가|입니까|일까|일까요|나요|습니까|은가요|는가요|맞나요|아닌가요)$"
    );

    // 시간 표현 패턴
    private static final Pattern TIME_PATTERN = Pattern.compile(
            "(오늘|어제|이번주|지난주|최근|\\d+일|\\d+시간|\\d+분)"
    );

    /**
     * 쿼리를 분석하여 의도를 파악합니다.
     *
     * @param query 사용자 쿼리
     * @return 분석된 쿼리 의도
     */
    public QueryIntent analyzeIntent(String query) {
        if (query == null || query.isBlank()) {
            return QueryIntent.builder()
                    .type(IntentType.GENERAL)
                    .confidence(1.0)
                    .keywords(List.of())
                    .build();
        }

        String normalizedQuery = query.toLowerCase().trim();
        
        // 각 의도 유형별 점수 계산
        Map<IntentType, Double> scores = new EnumMap<>(IntentType.class);
        for (IntentType type : IntentType.values()) {
            scores.put(type, 0.0);
        }

        // 키워드 매칭 점수
        List<String> matchedKeywords = new ArrayList<>();
        for (Map.Entry<IntentType, List<String>> entry : INTENT_KEYWORDS.entrySet()) {
            IntentType type = entry.getKey();
            for (String keyword : entry.getValue()) {
                if (normalizedQuery.contains(keyword.toLowerCase())) {
                    scores.merge(type, 1.0, Double::sum);
                    matchedKeywords.add(keyword);
                }
            }
        }

        // 질문 형태 분석 - 팩트체크 의도 가능성 증가
        if (QUESTION_PATTERN.matcher(normalizedQuery).find()) {
            scores.merge(IntentType.FACT_CHECK, 0.5, Double::sum);
        }

        // 시간 표현 분석 - 최신 뉴스 의도 가능성 증가
        if (TIME_PATTERN.matcher(normalizedQuery).find()) {
            scores.merge(IntentType.LATEST_NEWS, 0.5, Double::sum);
        }

        // 최고 점수 의도 선택
        IntentType bestIntent = IntentType.GENERAL;
        double maxScore = 0.0;
        for (Map.Entry<IntentType, Double> entry : scores.entrySet()) {
            if (entry.getValue() > maxScore) {
                maxScore = entry.getValue();
                bestIntent = entry.getKey();
            }
        }

        // 신뢰도 계산 (0~1)
        double confidence = calculateConfidence(maxScore, scores);

        // 최소 점수 미달 시 일반 검색으로
        if (maxScore < 0.5) {
            bestIntent = IntentType.GENERAL;
            confidence = 1.0 - (maxScore * 0.5); // 낮은 매칭일수록 일반 검색 신뢰도 높음
        }

        // 시간 범위 추출
        String timeRange = extractTimeRange(normalizedQuery);

        // 핵심 키워드 추출
        List<String> keywords = extractKeywords(query);

        QueryIntent intent = QueryIntent.builder()
                .type(bestIntent)
                .confidence(confidence)
                .keywords(keywords)
                .timeRange(timeRange)
                .build();

        log.debug("Query intent analyzed: query='{}', type={}, confidence={}, keywords={}", 
                query, bestIntent, String.format("%.2f", confidence), keywords);

        return intent;
    }

    /**
     * 신뢰도를 계산합니다.
     */
    private double calculateConfidence(double maxScore, Map<IntentType, Double> scores) {
        if (maxScore == 0) return 0.5;

        // 최고 점수와 다른 점수들의 차이로 신뢰도 계산
        double totalScore = scores.values().stream().mapToDouble(Double::doubleValue).sum();
        if (totalScore == 0) return 0.5;

        double confidence = maxScore / totalScore;
        return Math.min(1.0, Math.max(0.5, confidence));
    }

    /**
     * 시간 범위를 추출합니다.
     */
    private String extractTimeRange(String query) {
        if (query.contains("오늘") || query.contains("금일")) {
            return "1d";
        } else if (query.contains("어제")) {
            return "2d";
        } else if (query.contains("이번주") || query.contains("최근")) {
            return "7d";
        } else if (query.contains("지난주")) {
            return "14d";
        } else if (query.contains("이번달") || query.contains("한달")) {
            return "30d";
        }
        return null; // 기본값 사용
    }

    /**
     * 쿼리에서 핵심 키워드를 추출합니다.
     */
    private List<String> extractKeywords(String query) {
        // 불용어 목록
        Set<String> stopwords = Set.of(
                "은", "는", "이", "가", "을", "를", "의", "에", "와", "과",
                "로", "으로", "에서", "부터", "까지", "도", "만", "뿐",
                "대해", "대한", "관련", "관한", "에게", "한테",
                "그", "저", "이것", "그것", "저것",
                "어떤", "무슨", "어느", "무엇", "뭐",
                "the", "a", "an", "is", "are", "was", "were", "be",
                "to", "of", "in", "for", "on", "with", "at", "by"
        );

        // 의도 키워드 제외
        Set<String> intentKeywords = new HashSet<>();
        INTENT_KEYWORDS.values().forEach(intentKeywords::addAll);

        // 키워드 추출
        String[] tokens = query.toLowerCase()
                .replaceAll("[^가-힣a-z0-9\\s]", " ")
                .split("\\s+");

        List<String> keywords = new ArrayList<>();
        for (String token : tokens) {
            if (token.length() >= 2 
                    && !stopwords.contains(token)
                    && !intentKeywords.contains(token)) {
                keywords.add(token);
            }
        }

        // 최대 5개 키워드
        return keywords.size() > 5 ? keywords.subList(0, 5) : keywords;
    }

    /**
     * 의도에 대한 설명을 반환합니다.
     */
    public String getIntentDescription(IntentType type) {
        return switch (type) {
            case FACT_CHECK -> "팩트체크 - 정보의 진위 여부를 검증합니다";
            case LATEST_NEWS -> "최신 뉴스 - 가장 최근 소식을 우선합니다";
            case DEEP_ANALYSIS -> "심층 분석 - 배경과 맥락을 포함한 분석을 제공합니다";
            case OPINION_SEARCH -> "여론 검색 - 다양한 의견과 반응을 수집합니다";
            case GENERAL -> "일반 검색 - 관련성 높은 정보를 제공합니다";
        };
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/search/VectorSearchService.java

```java
package com.newsinsight.collector.service.search;

import com.newsinsight.collector.entity.CollectedData;
import com.newsinsight.collector.repository.CollectedDataRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.jdbc.core.JdbcTemplate;
import org.springframework.stereotype.Service;
import reactor.core.publisher.Mono;

import jakarta.annotation.PostConstruct;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import java.util.Optional;

/**
 * PostgreSQL pgvector 기반 벡터 검색 서비스.
 * 
 * 시맨틱 검색을 위해 문서 임베딩을 저장하고 유사도 검색을 수행합니다.
 * pgvector 확장이 설치되어 있어야 합니다.
 * 
 * 사용 전 필요한 SQL:
 * CREATE EXTENSION IF NOT EXISTS vector;
 * ALTER TABLE collected_data ADD COLUMN IF NOT EXISTS embedding vector(1024);
 * CREATE INDEX IF NOT EXISTS collected_data_embedding_idx ON collected_data 
 *   USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class VectorSearchService {

    private final JdbcTemplate jdbcTemplate;
    private final EmbeddingService embeddingService;
    private final CollectedDataRepository collectedDataRepository;

    @Value("${collector.vector-search.enabled:true}")
    private boolean enabled;

    @Value("${collector.vector-search.top-k:20}")
    private int defaultTopK;

    @Value("${collector.vector-search.min-similarity:0.5}")
    private double minSimilarity;

    @Value("${collector.embedding.dimension:1024}")
    private int embeddingDimension;

    private boolean pgvectorAvailable = false;

    @PostConstruct
    public void init() {
        if (!enabled) {
            log.info("VectorSearchService is disabled");
            return;
        }

        // pgvector 확장 및 컬럼 존재 여부 확인
        try {
            jdbcTemplate.queryForObject(
                    "SELECT 1 FROM pg_extension WHERE extname = 'vector'",
                    Integer.class);
            
            // embedding 컬럼 존재 확인
            Integer columnExists = jdbcTemplate.queryForObject(
                    "SELECT COUNT(*) FROM information_schema.columns " +
                    "WHERE table_name = 'collected_data' AND column_name = 'embedding'",
                    Integer.class);
            
            pgvectorAvailable = columnExists != null && columnExists > 0;
            
            if (pgvectorAvailable) {
                log.info("VectorSearchService initialized: pgvector available, dimension={}", embeddingDimension);
            } else {
                log.warn("VectorSearchService: embedding column not found in collected_data table. " +
                         "Run migration to add vector column.");
            }
        } catch (Exception e) {
            log.warn("VectorSearchService: pgvector extension not available. " +
                     "Install with: CREATE EXTENSION vector;");
            pgvectorAvailable = false;
        }
    }

    /**
     * 쿼리와 유사한 문서를 벡터 검색합니다.
     *
     * @param query 검색 쿼리
     * @param topK 반환할 결과 수
     * @return 유사도순 정렬된 문서 목록
     */
    public Mono<List<ScoredDocument>> searchSimilar(String query, int topK) {
        if (!isAvailable()) {
            return Mono.just(List.of());
        }

        return embeddingService.embedQuery(query)
                .flatMap(queryEmbedding -> {
                    if (queryEmbedding == null || queryEmbedding.length == 0) {
                        return Mono.just(List.<ScoredDocument>of());
                    }
                    return Mono.fromCallable(() -> searchByVector(queryEmbedding, topK));
                })
                .onErrorResume(e -> {
                    log.error("Vector search failed: {}", e.getMessage());
                    return Mono.just(List.<ScoredDocument>of());
                });
    }

    /**
     * 벡터로 직접 유사 문서를 검색합니다.
     *
     * @param queryEmbedding 쿼리 임베딩 벡터
     * @param topK 반환할 결과 수
     * @return 유사도순 정렬된 문서 목록
     */
    public List<ScoredDocument> searchByVector(float[] queryEmbedding, int topK) {
        if (!isAvailable() || queryEmbedding == null) {
            return List.of();
        }

        String vectorStr = vectorToString(queryEmbedding);
        
        // pgvector 코사인 유사도 검색 (1 - cosine_distance)
        String sql = """
                SELECT id, title, url, 
                       1 - (embedding <=> ?::vector) as similarity
                FROM collected_data
                WHERE embedding IS NOT NULL
                  AND 1 - (embedding <=> ?::vector) >= ?
                ORDER BY embedding <=> ?::vector
                LIMIT ?
                """;

        try {
            return jdbcTemplate.query(sql, 
                    (rs, rowNum) -> new ScoredDocument(
                            rs.getLong("id"),
                            rs.getString("title"),
                            rs.getString("url"),
                            rs.getDouble("similarity")
                    ),
                    vectorStr, vectorStr, minSimilarity, vectorStr, topK);
        } catch (Exception e) {
            log.error("Vector search failed: {}", e.getMessage());
            return List.of();
        }
    }

    /**
     * 문서에 임베딩을 저장합니다.
     *
     * @param documentId 문서 ID
     * @param content 문서 내용
     * @return 저장 성공 여부
     */
    public Mono<Boolean> saveEmbedding(Long documentId, String content) {
        if (!isAvailable() || content == null || content.isBlank()) {
            return Mono.just(false);
        }

        return embeddingService.embedDocument(content)
                .flatMap(embedding -> {
                    if (embedding == null || embedding.length == 0) {
                        return Mono.just(false);
                    }
                    return Mono.fromCallable(() -> {
                        String vectorStr = vectorToString(embedding);
                        int updated = jdbcTemplate.update(
                                "UPDATE collected_data SET embedding = ?::vector WHERE id = ?",
                                vectorStr, documentId);
                        return updated > 0;
                    });
                })
                .onErrorReturn(false);
    }

    /**
     * 여러 문서의 임베딩을 일괄 저장합니다.
     *
     * @param documents 문서 목록 (ID, 내용)
     * @return 저장된 문서 수
     */
    public Mono<Integer> saveEmbeddingsBatch(List<DocumentContent> documents) {
        if (!isAvailable() || documents == null || documents.isEmpty()) {
            return Mono.just(0);
        }

        List<String> contents = documents.stream()
                .map(DocumentContent::content)
                .toList();

        return embeddingService.embedBatch(contents)
                .map(embeddings -> {
                    int savedCount = 0;
                    for (int i = 0; i < Math.min(documents.size(), embeddings.size()); i++) {
                        try {
                            String vectorStr = vectorToString(embeddings.get(i));
                            int updated = jdbcTemplate.update(
                                    "UPDATE collected_data SET embedding = ?::vector WHERE id = ?",
                                    vectorStr, documents.get(i).id());
                            if (updated > 0) savedCount++;
                        } catch (Exception e) {
                            log.debug("Failed to save embedding for document {}: {}", 
                                    documents.get(i).id(), e.getMessage());
                        }
                    }
                    return savedCount;
                })
                .onErrorReturn(0);
    }

    /**
     * 임베딩이 없는 문서에 대해 임베딩을 생성합니다.
     *
     * @param batchSize 한 번에 처리할 문서 수
     * @return 처리된 문서 수
     */
    public Mono<Integer> indexMissingEmbeddings(int batchSize) {
        if (!isAvailable()) {
            return Mono.just(0);
        }

        try {
            // 임베딩이 없는 문서 조회
            List<DocumentContent> documents = jdbcTemplate.query(
                    "SELECT id, COALESCE(title, '') || ' ' || COALESCE(content, '') as content " +
                    "FROM collected_data WHERE embedding IS NULL LIMIT ?",
                    (rs, rowNum) -> new DocumentContent(
                            rs.getLong("id"),
                            rs.getString("content")
                    ),
                    batchSize);

            if (documents.isEmpty()) {
                return Mono.just(0);
            }

            log.info("Indexing {} documents without embeddings", documents.size());
            return saveEmbeddingsBatch(documents);
        } catch (Exception e) {
            log.error("Failed to index missing embeddings: {}", e.getMessage());
            return Mono.just(0);
        }
    }

    /**
     * 서비스 사용 가능 여부를 반환합니다.
     */
    public boolean isAvailable() {
        return enabled && pgvectorAvailable && embeddingService.isEnabled();
    }

    /**
     * float 배열을 pgvector 형식 문자열로 변환합니다.
     */
    private String vectorToString(float[] vector) {
        StringBuilder sb = new StringBuilder("[");
        for (int i = 0; i < vector.length; i++) {
            if (i > 0) sb.append(",");
            sb.append(vector[i]);
        }
        sb.append("]");
        return sb.toString();
    }

    // ==================== Inner Classes ====================

    /**
     * 유사도 점수가 포함된 문서
     */
    public record ScoredDocument(
            Long id,
            String title,
            String url,
            double similarity
    ) {}

    /**
     * 문서 ID와 내용
     */
    public record DocumentContent(
            Long id,
            String content
    ) {}
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/validation/EvidenceValidator.java

```java
package com.newsinsight.collector.service.validation;

import com.newsinsight.collector.service.FactVerificationService.SourceEvidence;
import com.newsinsight.collector.service.validation.UrlLivenessValidator.ValidationResult;
import com.newsinsight.collector.service.validation.UrlLivenessValidator.ContentValidationResult;
import lombok.Builder;
import lombok.Data;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Mono;

import java.util.*;
import java.util.stream.Collectors;

/**
 * 증거(Evidence) 유효성 검증 서비스
 * 
 * RRF 알고리즘과 연동하여 수집된 증거의 품질을 검증합니다.
 * 
 * 주요 기능:
 * 1. URL 실존 여부 검증 (UrlLivenessValidator 활용)
 * 2. 콘텐츠 유효성 검증 (삭제 페이지, 에러 페이지 필터링)
 * 3. LLM 환각(Hallucination) 필터링
 * 4. 검증된 증거만 RRF 파이프라인에 전달
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class EvidenceValidator {

    private final UrlLivenessValidator urlLivenessValidator;

    @Value("${collector.evidence-validation.enabled:true}")
    private boolean validationEnabled;

    @Value("${collector.evidence-validation.strict-mode:false}")
    private boolean strictMode;

    @Value("${collector.evidence-validation.min-content-length:50}")
    private int minContentLength;

    /**
     * 단일 증거 검증
     */
    public Mono<EvidenceValidationResult> validateEvidence(SourceEvidence evidence) {
        if (!validationEnabled) {
            return Mono.just(EvidenceValidationResult.builder()
                    .evidence(evidence)
                    .isValid(true)
                    .build());
        }

        if (evidence == null) {
            return Mono.just(EvidenceValidationResult.builder()
                    .isValid(false)
                    .failureReason("Evidence is null")
                    .build());
        }

        String url = evidence.getUrl();
        String content = evidence.getExcerpt();

        // 1. URL 검증
        Mono<ValidationResult> urlValidation;
        if (url != null && !url.isBlank()) {
            urlValidation = urlLivenessValidator.validateUrl(url);
        } else {
            // URL이 없는 경우 - strict mode에서는 실패
            urlValidation = Mono.just(ValidationResult.builder()
                    .url(url)
                    .isValid(!strictMode) // strict mode가 아니면 허용
                    .isAccessible(false)
                    .failureReason("No URL provided")
                    .build());
        }

        return urlValidation.map(urlResult -> {
            // 2. 콘텐츠 검증
            ContentValidationResult contentResult = urlLivenessValidator.validateContent(url, content);

            // 3. 종합 판정
            boolean isUrlValid = urlResult.isValid() || urlResult.isTrustedDomain();
            boolean isContentValid = contentResult.isValid();
            boolean isHallucination = urlResult.isHallucination();

            // 최종 유효성: URL과 콘텐츠 모두 유효해야 함
            boolean isValid;
            String failureReason = null;

            if (isHallucination) {
                isValid = false;
                failureReason = "URL is likely LLM hallucination";
            } else if (strictMode) {
                isValid = isUrlValid && isContentValid;
                if (!isUrlValid) {
                    failureReason = urlResult.getFailureReason();
                } else if (!isContentValid) {
                    failureReason = contentResult.getFailureReason();
                }
            } else {
                // 완화 모드: URL 또는 콘텐츠 중 하나만 유효해도 통과
                isValid = isUrlValid || isContentValid;
                if (!isValid) {
                    failureReason = "Both URL and content validation failed";
                }
            }

            // 신뢰도 점수 조정
            double adjustedRelevance = calculateAdjustedRelevance(
                    evidence.getRelevanceScore(),
                    urlResult,
                    contentResult
            );

            return EvidenceValidationResult.builder()
                    .evidence(evidence)
                    .isValid(isValid)
                    .isUrlValid(isUrlValid)
                    .isContentValid(isContentValid)
                    .isHallucination(isHallucination)
                    .isTrustedSource(urlResult.isTrustedDomain())
                    .urlValidation(urlResult)
                    .contentValidation(contentResult)
                    .adjustedRelevanceScore(adjustedRelevance)
                    .failureReason(failureReason)
                    .build();
        });
    }

    /**
     * 다중 증거 병렬 검증
     */
    public Flux<EvidenceValidationResult> validateEvidences(List<SourceEvidence> evidences) {
        if (evidences == null || evidences.isEmpty()) {
            return Flux.empty();
        }

        return Flux.fromIterable(evidences)
                .flatMap(this::validateEvidence, 10); // 동시성 10개 제한
    }

    /**
     * 증거 목록에서 유효한 증거만 필터링 (RRF 파이프라인용)
     */
    public Mono<List<SourceEvidence>> filterValidEvidences(List<SourceEvidence> evidences) {
        if (!validationEnabled) {
            return Mono.just(new ArrayList<>(evidences));
        }

        return validateEvidences(evidences)
                .filter(EvidenceValidationResult::isValid)
                .map(result -> {
                    // 조정된 관련성 점수 적용
                    SourceEvidence evidence = result.getEvidence();
                    if (result.getAdjustedRelevanceScore() != null) {
                        evidence.setRelevanceScore(result.getAdjustedRelevanceScore());
                    }
                    return evidence;
                })
                .collectList()
                .doOnNext(validEvidences -> {
                    int filtered = evidences.size() - validEvidences.size();
                    if (filtered > 0) {
                        log.info("Evidence validation: {} valid, {} filtered out of {} total",
                                validEvidences.size(), filtered, evidences.size());
                    }
                });
    }

    /**
     * 증거 목록 검증 및 상세 보고서 생성
     */
    public Mono<ValidationReport> generateValidationReport(List<SourceEvidence> evidences) {
        return validateEvidences(evidences)
                .collectList()
                .map(results -> {
                    int total = results.size();
                    int valid = (int) results.stream().filter(EvidenceValidationResult::isValid).count();
                    int urlInvalid = (int) results.stream().filter(r -> !r.isUrlValid()).count();
                    int contentInvalid = (int) results.stream().filter(r -> !r.isContentValid()).count();
                    int hallucinations = (int) results.stream().filter(EvidenceValidationResult::isHallucination).count();
                    int trustedSources = (int) results.stream().filter(EvidenceValidationResult::isTrustedSource).count();

                    // 실패 이유별 그룹화
                    Map<String, Long> failureReasons = results.stream()
                            .filter(r -> !r.isValid())
                            .filter(r -> r.getFailureReason() != null)
                            .collect(Collectors.groupingBy(
                                    EvidenceValidationResult::getFailureReason,
                                    Collectors.counting()
                            ));

                    return ValidationReport.builder()
                            .totalEvidences(total)
                            .validEvidences(valid)
                            .invalidEvidences(total - valid)
                            .urlValidationFailures(urlInvalid)
                            .contentValidationFailures(contentInvalid)
                            .hallucinationDetections(hallucinations)
                            .trustedSourceCount(trustedSources)
                            .validationRate(total > 0 ? (double) valid / total : 0.0)
                            .failureReasonCounts(failureReasons)
                            .validEvidenceList(results.stream()
                                    .filter(EvidenceValidationResult::isValid)
                                    .map(EvidenceValidationResult::getEvidence)
                                    .toList())
                            .build();
                });
    }

    /**
     * 검증 결과에 따른 관련성 점수 조정
     */
    private double calculateAdjustedRelevance(
            Double originalScore,
            ValidationResult urlResult,
            ContentValidationResult contentResult) {
        
        double base = originalScore != null ? originalScore : 0.5;

        // 신뢰할 수 있는 도메인: 보너스
        if (urlResult.isTrustedDomain()) {
            base = Math.min(1.0, base + 0.1);
        }

        // URL 접근 불가: 페널티
        if (!urlResult.isAccessible() && !urlResult.isTrustedDomain()) {
            base = base * 0.7;
        }

        // 콘텐츠 유효성 검증 실패: 페널티
        if (!contentResult.isValid()) {
            base = base * 0.5;
        }

        // 환각 의심: 대폭 감점
        if (urlResult.isHallucination()) {
            base = base * 0.1;
        }

        // 응답 시간이 너무 긴 경우: 약간 감점
        if (urlResult.getResponseTimeMs() > 3000) {
            base = base * 0.95;
        }

        return Math.max(0.0, Math.min(1.0, base));
    }

    /**
     * 증거 검증 결과
     */
    @Data
    @Builder
    public static class EvidenceValidationResult {
        private SourceEvidence evidence;
        private boolean isValid;
        private boolean isUrlValid;
        private boolean isContentValid;
        private boolean isHallucination;
        private boolean isTrustedSource;
        private ValidationResult urlValidation;
        private ContentValidationResult contentValidation;
        private Double adjustedRelevanceScore;
        private String failureReason;
    }

    /**
     * 검증 보고서
     */
    @Data
    @Builder
    public static class ValidationReport {
        private int totalEvidences;
        private int validEvidences;
        private int invalidEvidences;
        private int urlValidationFailures;
        private int contentValidationFailures;
        private int hallucinationDetections;
        private int trustedSourceCount;
        private double validationRate;
        private Map<String, Long> failureReasonCounts;
        private List<SourceEvidence> validEvidenceList;
    }

    /**
     * 설정 정보 조회
     */
    public Map<String, Object> getConfiguration() {
        return Map.of(
                "validationEnabled", validationEnabled,
                "strictMode", strictMode,
                "minContentLength", minContentLength,
                "urlValidatorCacheStats", urlLivenessValidator.getCacheStats()
        );
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/service/validation/UrlLivenessValidator.java

```java
package com.newsinsight.collector.service.validation;

import lombok.Builder;
import lombok.Data;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.HttpMethod;
import org.springframework.http.HttpStatusCode;
import org.springframework.stereotype.Service;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Mono;
import reactor.core.scheduler.Schedulers;

import java.net.URI;
import java.net.URISyntaxException;
import java.time.Duration;
import java.util.*;
import java.util.concurrent.ConcurrentHashMap;
import java.util.regex.Pattern;

/**
 * URL 실존 여부 검증 서비스 (Liveness Check)
 * 
 * HTTP HEAD/GET 요청을 통해 URL이 실제로 접근 가능한지 확인하고,
 * 삭제된 페이지, 에러 페이지, LLM 환각(Hallucination) URL을 필터링합니다.
 * 
 * 주요 기능:
 * 1. HTTP HEAD 요청으로 URL 접근 가능 여부 확인
 * 2. 삭제된 페이지/에러 페이지 콘텐츠 패턴 감지
 * 3. LLM이 생성한 가짜 URL 필터링
 * 4. 결과 캐싱으로 중복 요청 방지
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class UrlLivenessValidator {

    private final WebClient webClient;

    @Value("${collector.url-validation.timeout-seconds:5}")
    private int timeoutSeconds;

    @Value("${collector.url-validation.cache-ttl-minutes:30}")
    private int cacheTtlMinutes;

    @Value("${collector.url-validation.enabled:true}")
    private boolean validationEnabled;

    // URL 검증 결과 캐시 (TTL 적용)
    private final Map<String, CachedValidation> validationCache = new ConcurrentHashMap<>();

    // 삭제된 페이지를 나타내는 키워드 패턴 (한국어/영어)
    private static final List<Pattern> DELETED_PAGE_PATTERNS = List.of(
            // 한국어 패턴
            Pattern.compile("삭제된\\s*게시[글물]", Pattern.CASE_INSENSITIVE),
            Pattern.compile("삭제되었습니다", Pattern.CASE_INSENSITIVE),
            Pattern.compile("존재하지\\s*않는\\s*페이지", Pattern.CASE_INSENSITIVE),
            Pattern.compile("페이지를\\s*찾을\\s*수\\s*없", Pattern.CASE_INSENSITIVE),
            Pattern.compile("접근\\s*권한이\\s*없", Pattern.CASE_INSENSITIVE),
            Pattern.compile("비공개\\s*게시", Pattern.CASE_INSENSITIVE),
            Pattern.compile("회원만\\s*열람", Pattern.CASE_INSENSITIVE),
            Pattern.compile("로그인이\\s*필요", Pattern.CASE_INSENSITIVE),
            Pattern.compile("게시글이\\s*없습니다", Pattern.CASE_INSENSITIVE),
            Pattern.compile("이\\s*글은\\s*삭제", Pattern.CASE_INSENSITIVE),
            
            // 영어 패턴
            Pattern.compile("page\\s*not\\s*found", Pattern.CASE_INSENSITIVE),
            Pattern.compile("404\\s*error", Pattern.CASE_INSENSITIVE),
            Pattern.compile("not\\s*found", Pattern.CASE_INSENSITIVE),
            Pattern.compile("this\\s*page\\s*does\\s*not\\s*exist", Pattern.CASE_INSENSITIVE),
            Pattern.compile("content\\s*has\\s*been\\s*removed", Pattern.CASE_INSENSITIVE),
            Pattern.compile("content\\s*is\\s*no\\s*longer\\s*available", Pattern.CASE_INSENSITIVE),
            Pattern.compile("access\\s*denied", Pattern.CASE_INSENSITIVE),
            Pattern.compile("permission\\s*denied", Pattern.CASE_INSENSITIVE),
            Pattern.compile("article\\s*not\\s*found", Pattern.CASE_INSENSITIVE),
            Pattern.compile("post\\s*has\\s*been\\s*deleted", Pattern.CASE_INSENSITIVE),
            Pattern.compile("this\\s*content\\s*is\\s*unavailable", Pattern.CASE_INSENSITIVE),
            Pattern.compile("sorry.*couldn't find", Pattern.CASE_INSENSITIVE),
            Pattern.compile("the\\s*requested\\s*url\\s*was\\s*not\\s*found", Pattern.CASE_INSENSITIVE)
    );

    // LLM이 자주 생성하는 가짜 URL 패턴
    private static final List<Pattern> HALLUCINATION_URL_PATTERNS = List.of(
            // 존재하지 않는 도메인 패턴
            Pattern.compile("example\\.com", Pattern.CASE_INSENSITIVE),
            Pattern.compile("sample\\.com", Pattern.CASE_INSENSITIVE),
            Pattern.compile("test\\.com", Pattern.CASE_INSENSITIVE),
            Pattern.compile("fake\\.(com|org|net)", Pattern.CASE_INSENSITIVE),
            Pattern.compile("placeholder\\.(com|org|net)", Pattern.CASE_INSENSITIVE),
            
            // 명백히 가짜인 경로 패턴
            Pattern.compile("/article/\\d{10,}", Pattern.CASE_INSENSITIVE), // 비정상적으로 긴 ID
            Pattern.compile("/news/fake-", Pattern.CASE_INSENSITIVE),
            Pattern.compile("/example-", Pattern.CASE_INSENSITIVE),
            
            // 흔한 환각 패턴
            Pattern.compile("www\\d+\\.", Pattern.CASE_INSENSITIVE), // www1., www2. 등
            Pattern.compile("\\.fake\\.", Pattern.CASE_INSENSITIVE)
    );

    // 신뢰할 수 있는 도메인 (검증 생략 가능)
    private static final Set<String> TRUSTED_DOMAINS = Set.of(
            "wikipedia.org",
            "en.wikipedia.org",
            "ko.wikipedia.org",
            "scholar.google.com",
            "pubmed.ncbi.nlm.nih.gov",
            "doi.org",
            "arxiv.org",
            "nature.com",
            "science.org",
            "sciencedirect.com",
            "springer.com",
            "wiley.com",
            "ncbi.nlm.nih.gov",
            "britannica.com",
            "namu.wiki",
            "kosis.kr",
            "stats.oecd.org",
            "data.worldbank.org"
    );

    /**
     * URL 유효성 검증 결과
     */
    @Data
    @Builder
    public static class ValidationResult {
        private String url;
        private boolean isValid;
        private boolean isAccessible;      // HTTP 접근 가능 여부
        private boolean isContentValid;    // 콘텐츠가 유효한지 (삭제 페이지 아님)
        private boolean isTrustedDomain;   // 신뢰할 수 있는 도메인인지
        private boolean isHallucination;   // LLM 환각으로 의심되는지
        private int httpStatusCode;
        private String failureReason;
        private long responseTimeMs;
    }

    /**
     * 캐시된 검증 결과
     */
    @Data
    @Builder
    private static class CachedValidation {
        private ValidationResult result;
        private long cachedAt;
    }

    /**
     * 단일 URL 검증
     */
    public Mono<ValidationResult> validateUrl(String url) {
        if (!validationEnabled) {
            return Mono.just(ValidationResult.builder()
                    .url(url)
                    .isValid(true)
                    .isAccessible(true)
                    .isContentValid(true)
                    .isTrustedDomain(false)
                    .isHallucination(false)
                    .build());
        }

        // 캐시 확인
        CachedValidation cached = validationCache.get(url);
        if (cached != null && !isCacheExpired(cached)) {
            return Mono.just(cached.getResult());
        }

        // 1. URL 형식 검증
        if (!isValidUrlFormat(url)) {
            ValidationResult result = ValidationResult.builder()
                    .url(url)
                    .isValid(false)
                    .isAccessible(false)
                    .isContentValid(false)
                    .isTrustedDomain(false)
                    .isHallucination(true)
                    .failureReason("Invalid URL format")
                    .build();
            cacheResult(url, result);
            return Mono.just(result);
        }

        // 2. 환각 URL 패턴 검사
        if (isLikelyHallucination(url)) {
            ValidationResult result = ValidationResult.builder()
                    .url(url)
                    .isValid(false)
                    .isAccessible(false)
                    .isContentValid(false)
                    .isTrustedDomain(false)
                    .isHallucination(true)
                    .failureReason("URL matches hallucination pattern")
                    .build();
            cacheResult(url, result);
            return Mono.just(result);
        }

        // 3. 신뢰할 수 있는 도메인 확인
        if (isTrustedDomain(url)) {
            ValidationResult result = ValidationResult.builder()
                    .url(url)
                    .isValid(true)
                    .isAccessible(true)
                    .isContentValid(true)
                    .isTrustedDomain(true)
                    .isHallucination(false)
                    .build();
            cacheResult(url, result);
            return Mono.just(result);
        }

        // 4. HTTP HEAD 요청으로 실제 접근 가능 여부 확인
        long startTime = System.currentTimeMillis();

        return performHttpValidation(url)
                .map(statusCode -> {
                    long responseTime = System.currentTimeMillis() - startTime;
                    boolean isAccessible = statusCode >= 200 && statusCode < 400;
                    
                    ValidationResult result = ValidationResult.builder()
                            .url(url)
                            .isValid(isAccessible)
                            .isAccessible(isAccessible)
                            .isContentValid(isAccessible) // HEAD만으로는 콘텐츠 검증 불가
                            .isTrustedDomain(false)
                            .isHallucination(false)
                            .httpStatusCode(statusCode)
                            .responseTimeMs(responseTime)
                            .failureReason(isAccessible ? null : "HTTP " + statusCode)
                            .build();
                    
                    cacheResult(url, result);
                    return result;
                })
                .onErrorResume(e -> {
                    long responseTime = System.currentTimeMillis() - startTime;
                    String reason = e.getMessage() != null ? e.getMessage() : "Connection failed";
                    
                    ValidationResult result = ValidationResult.builder()
                            .url(url)
                            .isValid(false)
                            .isAccessible(false)
                            .isContentValid(false)
                            .isTrustedDomain(false)
                            .isHallucination(false)
                            .responseTimeMs(responseTime)
                            .failureReason(reason)
                            .build();
                    
                    cacheResult(url, result);
                    return Mono.just(result);
                });
    }

    /**
     * 다중 URL 병렬 검증
     */
    public Flux<ValidationResult> validateUrls(List<String> urls) {
        if (urls == null || urls.isEmpty()) {
            return Flux.empty();
        }

        return Flux.fromIterable(urls)
                .flatMap(this::validateUrl, 10) // 동시성 10개 제한
                .subscribeOn(Schedulers.boundedElastic());
    }

    /**
     * URL 목록에서 유효한 URL만 필터링
     */
    public Mono<List<String>> filterValidUrls(List<String> urls) {
        return validateUrls(urls)
                .filter(ValidationResult::isValid)
                .map(ValidationResult::getUrl)
                .collectList();
    }

    /**
     * 콘텐츠가 삭제된 페이지인지 검사
     */
    public boolean isDeletedPageContent(String content) {
        if (content == null || content.isBlank()) {
            return true; // 빈 콘텐츠는 삭제된 것으로 간주
        }

        // 콘텐츠가 너무 짧으면 에러 페이지일 가능성 높음
        if (content.length() < 100) {
            for (Pattern pattern : DELETED_PAGE_PATTERNS) {
                if (pattern.matcher(content).find()) {
                    return true;
                }
            }
        }

        // 일반 콘텐츠에서도 삭제 패턴 검사
        for (Pattern pattern : DELETED_PAGE_PATTERNS) {
            if (pattern.matcher(content).find()) {
                // 패턴이 발견되었지만, 충분히 긴 콘텐츠면 실제 뉴스일 수 있음
                if (content.length() > 500) {
                    continue; // 더 많은 패턴 확인
                }
                return true;
            }
        }

        return false;
    }

    /**
     * 콘텐츠 유효성 검증 (삭제 페이지, 에러 페이지 필터링)
     */
    public ContentValidationResult validateContent(String url, String content) {
        ContentValidationResult.ContentValidationResultBuilder builder = ContentValidationResult.builder()
                .url(url)
                .originalContent(content);

        // 1. 빈 콘텐츠 검사
        if (content == null || content.isBlank()) {
            return builder
                    .isValid(false)
                    .failureReason("Empty content")
                    .contentType(ContentType.EMPTY)
                    .build();
        }

        // 2. 너무 짧은 콘텐츠 검사
        if (content.length() < 50) {
            return builder
                    .isValid(false)
                    .failureReason("Content too short: " + content.length() + " chars")
                    .contentType(ContentType.TOO_SHORT)
                    .build();
        }

        // 3. 삭제된 페이지 패턴 검사
        if (isDeletedPageContent(content)) {
            return builder
                    .isValid(false)
                    .failureReason("Content matches deleted page pattern")
                    .contentType(ContentType.DELETED_PAGE)
                    .build();
        }

        // 4. 유효한 콘텐츠
        return builder
                .isValid(true)
                .contentType(ContentType.VALID)
                .contentLength(content.length())
                .build();
    }

    /**
     * 콘텐츠 검증 결과
     */
    @Data
    @Builder
    public static class ContentValidationResult {
        private String url;
        private String originalContent;
        private boolean isValid;
        private String failureReason;
        private ContentType contentType;
        private int contentLength;
    }

    public enum ContentType {
        VALID,
        EMPTY,
        TOO_SHORT,
        DELETED_PAGE,
        ERROR_PAGE,
        ACCESS_DENIED
    }

    // ============================================
    // Private Helper Methods
    // ============================================

    private boolean isValidUrlFormat(String url) {
        if (url == null || url.isBlank()) {
            return false;
        }

        try {
            URI uri = new URI(url);
            String scheme = uri.getScheme();
            String host = uri.getHost();
            
            return (scheme != null && (scheme.equals("http") || scheme.equals("https")))
                    && host != null && !host.isBlank();
        } catch (URISyntaxException e) {
            return false;
        }
    }

    private boolean isLikelyHallucination(String url) {
        for (Pattern pattern : HALLUCINATION_URL_PATTERNS) {
            if (pattern.matcher(url).find()) {
                log.debug("URL matches hallucination pattern: {}", url);
                return true;
            }
        }
        return false;
    }

    private boolean isTrustedDomain(String url) {
        try {
            URI uri = new URI(url);
            String host = uri.getHost();
            if (host == null) {
                return false;
            }

            host = host.toLowerCase();
            for (String trusted : TRUSTED_DOMAINS) {
                if (host.equals(trusted) || host.endsWith("." + trusted)) {
                    return true;
                }
            }
        } catch (URISyntaxException e) {
            return false;
        }
        return false;
    }

    private Mono<Integer> performHttpValidation(String url) {
        return webClient.method(HttpMethod.HEAD)
                .uri(url)
                .exchangeToMono(response -> {
                    HttpStatusCode status = response.statusCode();
                    return Mono.just(status.value());
                })
                .timeout(Duration.ofSeconds(timeoutSeconds))
                .onErrorResume(e -> {
                    // HEAD 실패 시 GET으로 재시도
                    return webClient.method(HttpMethod.GET)
                            .uri(url)
                            .exchangeToMono(response -> {
                                HttpStatusCode status = response.statusCode();
                                return Mono.just(status.value());
                            })
                            .timeout(Duration.ofSeconds(timeoutSeconds));
                });
    }

    private void cacheResult(String url, ValidationResult result) {
        validationCache.put(url, CachedValidation.builder()
                .result(result)
                .cachedAt(System.currentTimeMillis())
                .build());
        
        // 캐시 크기 제한 (1000개 초과 시 오래된 항목 정리)
        if (validationCache.size() > 1000) {
            cleanupExpiredCache();
        }
    }

    private boolean isCacheExpired(CachedValidation cached) {
        long expiryTime = cached.getCachedAt() + (cacheTtlMinutes * 60 * 1000L);
        return System.currentTimeMillis() > expiryTime;
    }

    private void cleanupExpiredCache() {
        long now = System.currentTimeMillis();
        long expiryThreshold = now - (cacheTtlMinutes * 60 * 1000L);
        
        validationCache.entrySet().removeIf(entry -> 
                entry.getValue().getCachedAt() < expiryThreshold);
        
        log.info("Cache cleanup completed. Remaining entries: {}", validationCache.size());
    }

    /**
     * 캐시 통계 조회
     */
    public Map<String, Object> getCacheStats() {
        long validCount = validationCache.values().stream()
                .filter(c -> c.getResult().isValid())
                .count();
        
        return Map.of(
                "totalEntries", validationCache.size(),
                "validUrls", validCount,
                "invalidUrls", validationCache.size() - validCount,
                "cacheTtlMinutes", cacheTtlMinutes
        );
    }

    /**
     * 캐시 초기화
     */
    public void clearCache() {
        validationCache.clear();
        log.info("URL validation cache cleared");
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/util/ApiKeyEncryptor.java

```java
package com.newsinsight.collector.util;

import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Component;

import javax.crypto.Cipher;
import javax.crypto.SecretKey;
import javax.crypto.SecretKeyFactory;
import javax.crypto.spec.GCMParameterSpec;
import javax.crypto.spec.PBEKeySpec;
import javax.crypto.spec.SecretKeySpec;
import java.nio.charset.StandardCharsets;
import java.security.SecureRandom;
import java.security.spec.KeySpec;
import java.util.Base64;

/**
 * API 키 암호화/복호화 유틸리티.
 * 
 * AES-256-GCM 암호화를 사용하여 API 키를 안전하게 저장합니다.
 * 
 * 저장 형식: Base64(iv + encryptedData + authTag)
 */
@Component
@Slf4j
public class ApiKeyEncryptor {

    private static final String ALGORITHM = "AES/GCM/NoPadding";
    private static final int GCM_IV_LENGTH = 12;
    private static final int GCM_TAG_LENGTH = 128;
    private static final int ITERATION_COUNT = 65536;
    private static final int KEY_LENGTH = 256;
    
    private final SecretKey secretKey;
    private final SecureRandom secureRandom;

    public ApiKeyEncryptor(
            @Value("${newsinsight.encryption.secret:defaultSecretKeyForDevelopmentOnly32}") String secretKeyString,
            @Value("${newsinsight.encryption.salt:defaultSaltValue16ch}") String saltString
    ) {
        this.secretKey = deriveKey(secretKeyString, saltString);
        this.secureRandom = new SecureRandom();
        log.info("ApiKeyEncryptor initialized with AES-256-GCM encryption");
    }

    /**
     * 비밀번호와 salt로부터 AES 키 유도
     */
    private SecretKey deriveKey(String password, String salt) {
        try {
            SecretKeyFactory factory = SecretKeyFactory.getInstance("PBKDF2WithHmacSHA256");
            KeySpec spec = new PBEKeySpec(
                    password.toCharArray(),
                    salt.getBytes(StandardCharsets.UTF_8),
                    ITERATION_COUNT,
                    KEY_LENGTH
            );
            SecretKey tmp = factory.generateSecret(spec);
            return new SecretKeySpec(tmp.getEncoded(), "AES");
        } catch (Exception e) {
            log.error("Failed to derive encryption key", e);
            throw new RuntimeException("Failed to initialize encryption", e);
        }
    }

    /**
     * API 키 암호화
     * 
     * @param plaintext 평문 API 키
     * @return 암호화된 Base64 문자열 (prefix: "ENC:")
     */
    public String encrypt(String plaintext) {
        if (plaintext == null || plaintext.isBlank()) {
            return plaintext;
        }
        
        // 이미 암호화된 경우 그대로 반환
        if (isEncrypted(plaintext)) {
            return plaintext;
        }

        try {
            // 랜덤 IV 생성
            byte[] iv = new byte[GCM_IV_LENGTH];
            secureRandom.nextBytes(iv);

            // 암호화
            Cipher cipher = Cipher.getInstance(ALGORITHM);
            GCMParameterSpec parameterSpec = new GCMParameterSpec(GCM_TAG_LENGTH, iv);
            cipher.init(Cipher.ENCRYPT_MODE, secretKey, parameterSpec);
            byte[] ciphertext = cipher.doFinal(plaintext.getBytes(StandardCharsets.UTF_8));

            // IV + 암호문 결합
            byte[] combined = new byte[iv.length + ciphertext.length];
            System.arraycopy(iv, 0, combined, 0, iv.length);
            System.arraycopy(ciphertext, 0, combined, iv.length, ciphertext.length);

            // Base64 인코딩 + prefix 추가
            return "ENC:" + Base64.getEncoder().encodeToString(combined);

        } catch (Exception e) {
            log.error("Failed to encrypt API key", e);
            throw new RuntimeException("Encryption failed", e);
        }
    }

    /**
     * API 키 복호화
     * 
     * @param encrypted 암호화된 문자열 (prefix: "ENC:")
     * @return 평문 API 키
     */
    public String decrypt(String encrypted) {
        if (encrypted == null || encrypted.isBlank()) {
            return encrypted;
        }
        
        // 암호화되지 않은 평문인 경우 그대로 반환 (하위 호환성)
        if (!isEncrypted(encrypted)) {
            log.debug("Returning plain text API key (not encrypted)");
            return encrypted;
        }

        try {
            // prefix 제거 후 Base64 디코딩
            String base64Data = encrypted.substring(4); // "ENC:" 제거
            byte[] combined = Base64.getDecoder().decode(base64Data);

            // IV 추출
            byte[] iv = new byte[GCM_IV_LENGTH];
            System.arraycopy(combined, 0, iv, 0, iv.length);

            // 암호문 추출
            byte[] ciphertext = new byte[combined.length - iv.length];
            System.arraycopy(combined, iv.length, ciphertext, 0, ciphertext.length);

            // 복호화
            Cipher cipher = Cipher.getInstance(ALGORITHM);
            GCMParameterSpec parameterSpec = new GCMParameterSpec(GCM_TAG_LENGTH, iv);
            cipher.init(Cipher.DECRYPT_MODE, secretKey, parameterSpec);
            byte[] plaintext = cipher.doFinal(ciphertext);

            return new String(plaintext, StandardCharsets.UTF_8);

        } catch (Exception e) {
            log.error("Failed to decrypt API key", e);
            throw new RuntimeException("Decryption failed", e);
        }
    }

    /**
     * 문자열이 암호화된 상태인지 확인
     */
    public boolean isEncrypted(String value) {
        return value != null && value.startsWith("ENC:");
    }

    /**
     * API 키 마스킹 (표시용)
     * 복호화 후 마스킹 적용
     */
    public String getMaskedKey(String encryptedOrPlain) {
        if (encryptedOrPlain == null || encryptedOrPlain.isBlank()) {
            return "****";
        }

        String plainKey;
        try {
            plainKey = decrypt(encryptedOrPlain);
        } catch (Exception e) {
            // 복호화 실패 시 원본으로 마스킹
            plainKey = encryptedOrPlain;
        }

        if (plainKey.length() < 8) {
            return "****";
        }
        
        return plainKey.substring(0, 4) + "****" + plainKey.substring(plainKey.length() - 4);
    }
}

```

---

## backend/data-collection-service/src/main/resources/application-local-consul.yml

```yml
# ============================================
# Local Consul Profile Configuration
# ============================================
# 
# 로컬 개발 및 Consul 연동 환경에서 사용하는 설정.
# 모든 기능이 기본적으로 활성화됩니다.
#
# 사용법:
# - SPRING_PROFILES_ACTIVE=local-consul
# - docker-compose.consul.yml 사용 시 자동 적용
#

# ============================================
# Scheduling (Collection Jobs)
# ============================================
collector:
  scheduling:
    enabled: true
    # 30분마다 RSS 수집
    cron: "0 0/30 * * * ?"
    skip-if-running: true
  
  # RSS Feed
  rss:
    enabled: true
  
  # Web Scraper
  web-scraper:
    enabled: true
  
  # Browser Agent (autonomous-crawler-service)
  browser-agent:
    enabled: true
  
  # Deep Search (내부 통합 크롤러 사용)
  deep-search:
    timeout-minutes: 30
    cleanup-days: 7
  
  # Integrated Crawler
  integrated-crawler:
    max-depth: 2
    max-pages: 20
    timeout-seconds: 30
    concurrent-crawls: 5
  
  # Hybrid Search (Keyword + Semantic)
  hybrid-search:
    enabled: true
    keyword-top-k: 30
    semantic-top-k: 20
    final-top-k: 20
  
  # Embedding Service
  embedding:
    enabled: true
  
  # Vector Search
  vector-search:
    enabled: true

# ============================================
# Auto-Crawl (실시간 확장형 크롤링)
# ============================================
autocrawl:
  # 자동 크롤링 활성화
  enabled: true
  
  # 배치 크기
  batch-size: 10
  
  # 도메인당 최대 동시 크롤링
  max-concurrent-per-domain: 3
  
  # 타임아웃 설정
  stuck-timeout-minutes: 30
  
  # 정리 주기
  cleanup-days: 7
  expire-pending-days: 7
  
  # 스케줄러 간격 (개발환경에서는 짧게)
  queue-interval-ms: 15000        # 15초마다 큐 처리
  recovery-interval-ms: 120000    # 2분마다 복구 체크
  stats-interval-ms: 300000       # 5분마다 통계 로깅
  
  # 모든 발견 소스 활성화
  discover-from-search: true
  discover-from-articles: true
  discover-from-deep-search: true
  
  # 최소 콘텐츠 길이
  min-content-length: 200

# ============================================
# Logging (개발환경에서 상세 로깅)
# ============================================
logging:
  level:
    root: INFO
    com.newsinsight: DEBUG
    com.newsinsight.collector.service.autocrawl: DEBUG
    com.newsinsight.collector.scheduler: DEBUG

```

---

## backend/data-collection-service/src/main/resources/application.yml

```yml
spring:
  application:
    name: collector-service
  config:
    # Consul config import (optional - Consul 없어도 시작 가능)
    import: ${SPRING_CONFIG_IMPORT:optional:consul:}
  kafka:
    bootstrap-servers: ${KAFKA_BOOTSTRAP_SERVERS:localhost:9092}
    # Producer reliability settings
    producer:
      acks: ${KAFKA_PRODUCER_ACKS:all}
      retries: ${KAFKA_PRODUCER_RETRIES:3}
      retry-backoff-ms: ${KAFKA_PRODUCER_RETRY_BACKOFF_MS:1000}
      delivery-timeout-ms: ${KAFKA_PRODUCER_DELIVERY_TIMEOUT_MS:30000}
      enable-idempotence: ${KAFKA_PRODUCER_ENABLE_IDEMPOTENCE:true}
      properties:
        # Fast-fail settings to avoid long blocking on Kafka unavailability
        max.block.ms: ${KAFKA_PRODUCER_MAX_BLOCK_MS:5000}
        request.timeout.ms: ${KAFKA_PRODUCER_REQUEST_TIMEOUT_MS:10000}
        metadata.max.age.ms: ${KAFKA_PRODUCER_METADATA_MAX_AGE_MS:60000}
    # Consumer reliability settings
    consumer:
      max-retry-attempts: ${KAFKA_CONSUMER_MAX_RETRY_ATTEMPTS:3}
      retry-backoff-ms: ${KAFKA_CONSUMER_RETRY_BACKOFF_MS:1000}
      retry-max-backoff-ms: ${KAFKA_CONSUMER_RETRY_MAX_BACKOFF_MS:30000}
      concurrency: ${KAFKA_CONSUMER_CONCURRENCY:1}
  data:
    mongodb:
      uri: ${MONGODB_URI:mongodb://localhost:27017/newsinsight}
    redis:
      host: ${REDIS_HOST:localhost}
      port: ${REDIS_PORT:6379}
      password: ${REDIS_PASSWORD:}
      timeout: 3000ms
      lettuce:
        pool:
          max-active: 8
          max-idle: 8
          min-idle: 0
          max-wait: -1ms
  
  datasource:
    url: jdbc:postgresql://${DB_HOST:localhost}:${DB_PORT:5432}/${DB_NAME:newsinsight}
    username: ${DB_USER:postgres}
    password: ${DB_PASSWORD:postgres}
    driver-class-name: org.postgresql.Driver
    hikari:
      maximum-pool-size: 10
      minimum-idle: 5
      connection-timeout: 30000
      idle-timeout: 600000
      max-lifetime: 1800000
  
  jpa:
    hibernate:
      ddl-auto: update
    show-sql: ${JPA_SHOW_SQL:false}
    properties:
      hibernate:
        dialect: org.hibernate.dialect.PostgreSQLDialect
        format_sql: true
        jdbc:
          batch_size: 20
        order_inserts: true
        order_updates: true
    open-in-view: false
  
  cloud:
    consul:
      enabled: ${CONSUL_ENABLED:true}
      host: ${CONSUL_HOST:localhost}
      port: ${CONSUL_PORT:8500}
      discovery:
        enabled: ${CONSUL_DISCOVERY_ENABLED:true}
        instance-id: ${spring.application.name}:${random.value}
        service-name: ${spring.application.name}
        prefer-ip-address: false
        hostname: ${CONSUL_DISCOVERY_HOSTNAME:collector-service}
        health-check-path: /actuator/health
        health-check-interval: 10s
        # Consul 연결 실패해도 시작 가능
        fail-fast: false
      config:
        enabled: ${CONSUL_CONFIG_ENABLED:true}
        prefix: config
        default-context: ${spring.application.name}
        profile-separator: '::'
        format: PROPERTIES
        # Consul config 없어도 시작 가능
        fail-fast: false
  
  task:
    execution:
      pool:
        core-size: 5
        max-size: 20
        queue-capacity: 100
      thread-name-prefix: async-task-

  flyway:
    enabled: ${SPRING_FLYWAY_ENABLED:false}

server:
  port: ${SERVER_PORT:8081}
  servlet:
    context-path: /
  compression:
    enabled: true
  error:
    include-message: always
    include-binding-errors: always

management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics,prometheus
  endpoint:
    health:
      show-details: always
  health:
    consul:
      enabled: ${CONSUL_ENABLED:true}
  # Kafka metrics via Micrometer
  metrics:
    tags:
      application: ${spring.application.name}
    distribution:
      percentiles-histogram:
        kafka: true
      slo:
        kafka.producer.request.latency: 10ms,50ms,100ms,500ms
        kafka.consumer.fetch.latency: 10ms,50ms,100ms,500ms

logging:
  level:
    root: INFO
    com.newsinsight: ${LOG_LEVEL:DEBUG}
    org.springframework.web: INFO
    org.hibernate.SQL: ${JPA_SHOW_SQL:false}
    org.hibernate.type.descriptor.sql.BasicBinder: ${JPA_SHOW_SQL:false}
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss} - %msg%n"

# Application specific configuration
collector:
  scheduling:
    enabled: ${SCHEDULING_ENABLED:true}
    # Cron expression: run every hour
    cron: ${COLLECTION_CRON:0 0 * * * ?}
    # Skip scheduled collection if there are already running jobs
    skip-if-running: ${SCHEDULING_SKIP_IF_RUNNING:true}
  
  http:
    user-agent: ${HTTP_USER_AGENT:NewsInsight-Collector/1.0}
    timeout:
      connect: ${HTTP_CONNECT_TIMEOUT:10000}
      read: ${HTTP_READ_TIMEOUT:30000}
    max-redirects: 5
  
  rss:
    enabled: true
    fetch-timeout: 30000
  
  web-scraper:
    enabled: true
    max-content-length: 1048576  # 1MB
    respect-robots-txt: true
  
  collection:
    max-concurrent: ${MAX_CONCURRENT_COLLECTIONS:5}
    retry:
      max-attempts: 3
      backoff-delay: 1000
    cleanup:
      old-jobs-days: 30
  
  quality-assurance:
    min-content-length: ${QA_MIN_CONTENT_LENGTH:50}
    enable-network-checks: ${QA_ENABLE_NETWORK_CHECKS:false}
    domain-whitelist: ${QA_DOMAIN_WHITELIST:example.com,news.example.com}
    expected-keywords: ${QA_EXPECTED_KEYWORDS:news,article,report}

  ai:
    topic:
      request: ${COLLECTOR_AI_REQUEST_TOPIC:newsinsight.ai.requests}
      response: ${COLLECTOR_AI_RESPONSE_TOPIC:newsinsight.ai.responses}
    default-provider-id: ${COLLECTOR_AI_PROVIDER_ID:openai}
    default-model-id: ${COLLECTOR_AI_MODEL_ID:gpt-4.1}
    
    # AI Orchestration Configuration (Multi-provider Job Management)
    orchestration:
      # Kafka topic for AI task requests
      topic: ${COLLECTOR_AI_ORCHESTRATION_TOPIC:ai.tasks.requests}
      # Base URL for callback (this service's public URL)
      callback-base-url: ${COLLECTOR_AI_ORCHESTRATION_CALLBACK_BASE_URL:${collector.deep-search.callback-base-url}}
      # Token for validating callbacks from internal workers
      callback-token: ${COLLECTOR_AI_ORCHESTRATION_CALLBACK_TOKEN:${collector.deep-search.callback-token}}
      # Minutes before marking a job as timed out
      timeout-minutes: ${COLLECTOR_AI_ORCHESTRATION_TIMEOUT_MINUTES:30}
      # Interval for checking timed out jobs (milliseconds)
      timeout-check-interval: ${COLLECTOR_AI_ORCHESTRATION_TIMEOUT_CHECK_INTERVAL:300000}
      # Days to keep completed jobs before cleanup
      cleanup-days: ${COLLECTOR_AI_ORCHESTRATION_CLEANUP_DAYS:7}
      # Cron expression for cleanup job (3 AM daily)
      cleanup-cron: ${COLLECTOR_AI_ORCHESTRATION_CLEANUP_CRON:0 0 3 * * ?}

  crawl:
    topic:
      command: ${COLLECTOR_CRAWL_COMMAND_TOPIC:newsinsight.crawl.commands}
      result: ${COLLECTOR_CRAWL_RESULT_TOPIC:newsinsight.crawl.results}
      browser-task: ${COLLECTOR_CRAWL_BROWSER_TASK_TOPIC:newsinsight.crawl.browser.tasks}

  crawler:
    enabled: ${COLLECTOR_SERVICE_CRAWLER_ENABLED:true}
    base-url: ${COLLECTOR_SERVICE_CRAWLER_BASE_URL:http://web-crawler:11235}

  # Browser Agent Configuration (autonomous-crawler-service)
  browser-agent:
    enabled: ${COLLECTOR_BROWSER_AGENT_ENABLED:true}
    # Callback URL for session completion
    callback-base-url: ${COLLECTOR_CALLBACK_BASE_URL:${COLLECTOR_BROWSER_AGENT_CALLBACK_BASE_URL:http://collector-service:8081}}
    callback-token: ${COLLECTOR_BROWSER_AGENT_CALLBACK_TOKEN:}

  # Deep AI Search Configuration
  # DeepSearch runs entirely on internal services (IntegratedCrawlerService + AIDove).
  # Legacy n8n webhook integration has been removed.
  deep-search:
    # Base URL for internal callbacks
    callback-base-url: ${COLLECTOR_CALLBACK_BASE_URL:${DEEP_SEARCH_CALLBACK_BASE_URL:http://collector-service:8081}}
    # Token for validating internal callbacks
    callback-token: ${DEEP_SEARCH_CALLBACK_TOKEN:}
    # Minutes before marking a job as timed out
    timeout-minutes: ${DEEP_SEARCH_TIMEOUT_MINUTES:30}
    # Interval for checking timed out jobs (milliseconds)
    timeout-check-interval: ${DEEP_SEARCH_TIMEOUT_CHECK_INTERVAL:300000}
    # Days to keep completed jobs before cleanup
    cleanup-days: ${DEEP_SEARCH_CLEANUP_DAYS:7}
    # Cron expression for cleanup job (3 AM daily)
    cleanup-cron: ${DEEP_SEARCH_CLEANUP_CRON:0 0 3 * * ?}

  # Integrated Crawler Configuration (multi-strategy crawling)
  integrated-crawler:
    # Maximum crawl depth (how many links deep to follow)
    max-depth: ${COLLECTOR_INTEGRATED_CRAWLER_MAX_DEPTH:3}
    # Maximum pages to crawl per job
    max-pages: ${COLLECTOR_INTEGRATED_CRAWLER_MAX_PAGES:50}
    # Timeout per page crawl in seconds
    timeout-seconds: ${COLLECTOR_INTEGRATED_CRAWLER_TIMEOUT_SECONDS:30}
    # Number of concurrent crawls
    concurrent-crawls: ${COLLECTOR_INTEGRATED_CRAWLER_CONCURRENT:10}

  # Browser-Use API Configuration (for JS-rendered content)
  browser-use:
    base-url: ${COLLECTOR_BROWSER_USE_BASE_URL:${BROWSER_USE_API_URL:http://browser-use-api:8500}}

  # AI Dove Configuration (for evidence analysis)
  ai-dove:
    enabled: ${COLLECTOR_AI_DOVE_ENABLED:${AI_DOVE_ENABLED:true}}
    base-url: ${COLLECTOR_AI_DOVE_BASE_URL:${AI_DOVE_BASE_URL:https://workflow.nodove.com/webhook/aidove}}
    # Timeout for AI Dove API calls in seconds (evidence analysis can take time)
    timeout-seconds: ${COLLECTOR_AI_DOVE_TIMEOUT_SECONDS:${AI_DOVE_TIMEOUT_SECONDS:180}}

  # Fact Check Sources Configuration
  fact-check:
    # Timeout for all fact-check API calls
    timeout-seconds: ${COLLECTOR_FACT_CHECK_TIMEOUT_SECONDS:15}
    
    # CrossRef - Academic paper search (free, no API key required)
    crossref:
      enabled: ${COLLECTOR_CROSSREF_ENABLED:true}
      # Email for polite pool (faster rate limits)
      mailto: ${COLLECTOR_CROSSREF_MAILTO:newsinsight@example.com}
    
    # OpenAlex - Open academic database (free, no API key required)
    openalex:
      enabled: ${COLLECTOR_OPENALEX_ENABLED:true}
      # Email for polite pool
      mailto: ${COLLECTOR_OPENALEX_MAILTO:newsinsight@example.com}
    
    # Google Fact Check Tools API (requires API key)
    # Get API key: https://developers.google.com/fact-check/tools/api
    google:
      enabled: ${COLLECTOR_GOOGLE_FACTCHECK_ENABLED:true}
      api-key: ${COLLECTOR_GOOGLE_FACTCHECK_API_KEY:}
    
    # Semantic Scholar - AI-powered academic search (free, optional API key)
    # API docs: https://api.semanticscholar.org/api-docs/
    semantic-scholar:
      enabled: ${COLLECTOR_SEMANTIC_SCHOLAR_ENABLED:true}
      api-key: ${COLLECTOR_SEMANTIC_SCHOLAR_API_KEY:}
    
    # PubMed/NCBI - Medical and life sciences (free, optional API key)
    # API docs: https://www.ncbi.nlm.nih.gov/books/NBK25500/
    pubmed:
      enabled: ${COLLECTOR_PUBMED_ENABLED:true}
      api-key: ${COLLECTOR_PUBMED_API_KEY:}
    
    # CORE - Open access research papers (requires free API key)
    # Get API key: https://core.ac.uk/services/api
    core:
      enabled: ${COLLECTOR_CORE_ENABLED:true}
      api-key: ${COLLECTOR_CORE_API_KEY:}
    
    # RRF (Reciprocal Rank Fusion) Configuration
    # 다중 쿼리 병렬 검색 및 결과 융합 설정
    rrf:
      # RRF 기반 검색 활성화 (비활성화 시 기존 순차 검색 사용)
      enabled: ${COLLECTOR_RRF_ENABLED:true}
      # RRF 상수 k (낮을수록 상위 랭크에 가중치, 기본 60)
      k: ${COLLECTOR_RRF_K:60}
      # 최대 병렬 쿼리 수 (의도 분석 + 확장 쿼리)
      max-queries: ${COLLECTOR_RRF_MAX_QUERIES:5}
      # 최대 융합 결과 수
      max-results: ${COLLECTOR_RRF_MAX_RESULTS:50}
      # 최소 관련성 점수 (이 점수 이하의 결과는 필터링)
      min-relevance: ${COLLECTOR_RRF_MIN_RELEVANCE:0.1}
      # URL 실존 여부 검증 활성화 (LLM 환각, 죽은 링크 필터링)
      url-validation-enabled: ${COLLECTOR_RRF_URL_VALIDATION_ENABLED:true}
  
  # ============================================
  # URL Validation Configuration (실존 여부 검증)
  # ============================================
  # URL이 실제로 접근 가능한지, LLM 환각인지, 삭제된 페이지인지 검증합니다.
  url-validation:
    # URL 검증 활성화
    enabled: ${COLLECTOR_URL_VALIDATION_ENABLED:true}
    # HTTP HEAD/GET 요청 타임아웃 (초)
    timeout-seconds: ${COLLECTOR_URL_VALIDATION_TIMEOUT_SECONDS:5}
    # 검증 결과 캐시 TTL (분)
    cache-ttl-minutes: ${COLLECTOR_URL_VALIDATION_CACHE_TTL_MINUTES:30}
  
  # ============================================
  # Evidence Validation Configuration (증거 품질 검증)
  # ============================================
  # 수집된 증거의 품질을 검증하여 불량 데이터를 필터링합니다.
  evidence-validation:
    # 증거 검증 활성화
    enabled: ${COLLECTOR_EVIDENCE_VALIDATION_ENABLED:true}
    # Strict 모드 (URL과 콘텐츠 모두 유효해야 함)
    strict-mode: ${COLLECTOR_EVIDENCE_VALIDATION_STRICT_MODE:false}
    # 최소 콘텐츠 길이 (이하인 경우 삭제된 페이지로 간주)
    min-content-length: ${COLLECTOR_EVIDENCE_VALIDATION_MIN_CONTENT_LENGTH:50}

  # ============================================
  # IP Rotation Configuration (for rate limit bypass)
  # ============================================
  ip-rotation:
    # IP rotation 서비스 활성화
    enabled: ${COLLECTOR_IP_ROTATION_ENABLED:true}
    # IP rotation 서비스 URL
    base-url: ${COLLECTOR_IP_ROTATION_URL:http://ip-rotation:8050}
    # 프록시 요청 타임아웃
    timeout-seconds: ${COLLECTOR_IP_ROTATION_TIMEOUT:15}
    # 최대 재시도 횟수
    max-retries: ${COLLECTOR_IP_ROTATION_MAX_RETRIES:3}
    # 재시도 간 대기 시간 (밀리초)
    retry-delay-ms: ${COLLECTOR_IP_ROTATION_RETRY_DELAY:1000}

  # ============================================
  # Hybrid Search Configuration (Keyword + Semantic + RRF)
  # ============================================
  
  # Embedding Service (HuggingFace Text Embeddings Inference)
  embedding:
    enabled: ${EMBEDDING_ENABLED:true}
    base-url: ${EMBEDDING_BASE_URL:http://localhost:8011}
    model: ${EMBEDDING_MODEL:intfloat/multilingual-e5-large}
    dimension: ${EMBEDDING_DIMENSION:1024}
    timeout-seconds: ${EMBEDDING_TIMEOUT_SECONDS:30}
  
  # Vector Search (PostgreSQL pgvector)
  vector-search:
    enabled: ${VECTOR_SEARCH_ENABLED:true}
    top-k: ${VECTOR_SEARCH_TOP_K:20}
    min-similarity: ${VECTOR_SEARCH_MIN_SIMILARITY:0.5}
  
  # Hybrid Search Orchestration
  hybrid-search:
    enabled: ${HYBRID_SEARCH_ENABLED:true}
    # Number of results from keyword search
    keyword-top-k: ${HYBRID_SEARCH_KEYWORD_TOP_K:30}
    # Number of results from semantic search
    semantic-top-k: ${HYBRID_SEARCH_SEMANTIC_TOP_K:20}
    # Final number of results after RRF fusion
    final-top-k: ${HYBRID_SEARCH_FINAL_TOP_K:20}
    # Source weights for RRF (adjustable)
    keyword-weight: ${HYBRID_SEARCH_KEYWORD_WEIGHT:1.0}
    semantic-weight: ${HYBRID_SEARCH_SEMANTIC_WEIGHT:1.0}

  # Trust Score Configuration
  # Externalized trust scores for fact-check sources and data quality assessment.
  # Hierarchy: Academic (0.95) > Official Stats (0.95) > Encyclopedia (0.90) > Fact Check (0.85)
  trust-scores:
    # Fact-check sources (used by FactCheckSource implementations)
    fact-check:
      crossref: ${TRUST_SCORE_CROSSREF:0.95}            # Academic papers (highest)
      openalex: ${TRUST_SCORE_OPENALEX:0.92}            # Academic database
      wikipedia: ${TRUST_SCORE_WIKIPEDIA:0.90}          # Encyclopedia
      google-fact-check: ${TRUST_SCORE_GOOGLE_FC:0.85}  # Verified fact-check
    
    # Trusted reference sources (used by FactVerificationService)
    trusted:
      wikipedia-ko: ${TRUST_SCORE_WIKI_KO:0.90}         # Korean Wikipedia
      wikipedia-en: ${TRUST_SCORE_WIKI_EN:0.90}         # English Wikipedia
      britannica: ${TRUST_SCORE_BRITANNICA:0.95}        # Britannica (very high)
      namu-wiki: ${TRUST_SCORE_NAMU:0.60}               # Namu Wiki (community)
      kosis: ${TRUST_SCORE_KOSIS:0.95}                  # Korean Statistics (official)
      google-scholar: ${TRUST_SCORE_SCHOLAR:0.85}       # Academic search
    
    # Data quality assessment (used by CollectedDataService)
    data-quality:
      base-score: ${TRUST_SCORE_BASE:0.50}              # Unknown sources
      whitelist-score: ${TRUST_SCORE_WHITELIST:0.90}    # Whitelisted domains
      http-ok-bonus: ${TRUST_SCORE_HTTP_BONUS:0.10}     # Successful HTTP connection
    
    # Custom source scores (add your own)
    # custom:
    #   my-trusted-source: 0.88

# ============================================
# Auto-Crawl Configuration (실시간 확장형 크롤링)
# ============================================
# 
# 검색, 기사, 트렌딩 등에서 자동으로 URL을 발견하고 크롤링합니다.
# autonomous-crawler-service와 연동하여 동작합니다.
#
autocrawl:
  # 자동 크롤링 활성화 여부
  enabled: ${AUTOCRAWL_ENABLED:true}
  
  # 한 번에 처리할 대상 수
  batch-size: ${AUTOCRAWL_BATCH_SIZE:10}
  
  # 도메인당 최대 동시 크롤링 수 (rate limiting)
  max-concurrent-per-domain: ${AUTOCRAWL_MAX_CONCURRENT_PER_DOMAIN:3}
  
  # IN_PROGRESS 상태 타임아웃 (분) - 이 시간 초과 시 PENDING으로 복구
  stuck-timeout-minutes: ${AUTOCRAWL_STUCK_TIMEOUT_MINUTES:30}
  
  # 오래된 완료/실패 대상 정리 기준 (일)
  cleanup-days: ${AUTOCRAWL_CLEANUP_DAYS:7}
  
  # 오래된 PENDING 대상 만료 기준 (일)
  expire-pending-days: ${AUTOCRAWL_EXPIRE_PENDING_DAYS:7}
  
  # 스케줄러 간격 (밀리초)
  queue-interval-ms: ${AUTOCRAWL_QUEUE_INTERVAL_MS:30000}        # 30초마다 큐 처리
  recovery-interval-ms: ${AUTOCRAWL_RECOVERY_INTERVAL_MS:300000}  # 5분마다 복구 체크
  stats-interval-ms: ${AUTOCRAWL_STATS_INTERVAL_MS:600000}        # 10분마다 통계 로깅
  
  # 정리 작업 cron (새벽 3시)
  cleanup-cron: ${AUTOCRAWL_CLEANUP_CRON:0 0 3 * * *}
  
  # URL 발견 소스별 활성화
  discover-from-search: ${AUTOCRAWL_DISCOVER_FROM_SEARCH:true}
  discover-from-articles: ${AUTOCRAWL_DISCOVER_FROM_ARTICLES:true}
  discover-from-deep-search: ${AUTOCRAWL_DISCOVER_FROM_DEEP_SEARCH:true}
  
  # 최소 콘텐츠 길이 (이 이상인 경우에만 링크 추출)
  min-content-length: ${AUTOCRAWL_MIN_CONTENT_LENGTH:200}
  
  # ============================================
  # Seed URL 자동 초기화 설정
  # ============================================
  # docker-compose 시작 시 자동으로 seed URL을 큐에 추가합니다.
  seed:
    # seed 자동 초기화 활성화 (true로 설정 시 시작 시 seed URL 추가)
    enabled: ${AUTOCRAWL_SEED_ENABLED:false}
    
    # 커스텀 seed URL (콤마로 구분, 비어있으면 기본 뉴스 포털 URL 사용)
    urls: ${AUTOCRAWL_SEED_URLS:}
    
    # seed URL에 연결할 키워드
    keywords: ${AUTOCRAWL_SEED_KEYWORDS:뉴스,정치,경제,사회,IT,기술}
    
    # seed URL 우선순위 (0-100, 높을수록 먼저 크롤링)
    priority: ${AUTOCRAWL_SEED_PRIORITY:70}

# ============================================
# Vector Database 설정 (Qdrant)
# ============================================
vector:
  db:
    enabled: ${VECTOR_DB_ENABLED:false}
    url: ${VECTOR_DB_URL:http://localhost:6333}
    collection: ${VECTOR_DB_COLLECTION:factcheck_chat}
  embedding:
    model: ${EMBEDDING_MODEL:text-embedding-ada-002}
    api-key: ${OPENAI_API_KEY:}
    dimension: ${EMBEDDING_DIMENSION:1536}
    timeout-seconds: ${EMBEDDING_TIMEOUT_SECONDS:30}
    max-retry: ${EMBEDDING_MAX_RETRY:3}
    batch-size: ${EMBEDDING_BATCH_SIZE:10}
    local:
      enabled: ${LOCAL_EMBEDDING_ENABLED:false}
      url: ${LOCAL_EMBEDDING_URL:http://localhost:8011}

# ============================================
# 채팅 서비스 설정
# ============================================
chat:
  sync:
    # 동기화 트리거 조건
    min-messages: ${CHAT_SYNC_MIN_MESSAGES:10}
    max-idle-minutes: ${CHAT_SYNC_MAX_IDLE_MINUTES:5}
    # 배치 처리
    batch-size: ${CHAT_SYNC_BATCH_SIZE:50}
    # 재시도
    max-retry: ${CHAT_SYNC_MAX_RETRY:3}
    # 세션 만료
    session-expire-hours: ${CHAT_SESSION_EXPIRE_HOURS:24}
    # 스케줄러 간격
    scheduler:
      interval: ${CHAT_SYNC_SCHEDULER_INTERVAL:300000}
    expire:
      interval: ${CHAT_SYNC_EXPIRE_INTERVAL:3600000}

# ============================================
# 캐시 설정
# ============================================
cache:
  chat-sessions:
    ttl-hours: ${CACHE_CHAT_SESSIONS_TTL_HOURS:2}
  chat-messages:
    ttl-minutes: ${CACHE_CHAT_MESSAGES_TTL_MINUTES:30}
  default:
    ttl-hours: ${CACHE_DEFAULT_TTL_HOURS:24}
  local:
    max-size: ${CACHE_LOCAL_MAX_SIZE:1000}
    ttl-minutes: ${CACHE_LOCAL_TTL_MINUTES:10}

# ============================================
# 비동기 실행자 설정
# ============================================
async:
  executor:
    core-pool-size: ${ASYNC_CORE_POOL_SIZE:5}
    max-pool-size: ${ASYNC_MAX_POOL_SIZE:20}
    queue-capacity: ${ASYNC_QUEUE_CAPACITY:100}
  chat-sync:
    core-pool-size: ${CHAT_SYNC_CORE_POOL_SIZE:3}
    max-pool-size: ${CHAT_SYNC_MAX_POOL_SIZE:10}
    queue-capacity: ${CHAT_SYNC_QUEUE_CAPACITY:50}

# ============================================
# Workspace File Storage Configuration
# ============================================
workspace:
  storage:
    # Local storage path (Docker volume mount point)
    path: ${WORKSPACE_STORAGE_PATH:/data/workspace}
    # Maximum file size in bytes (default: 100MB)
    max-file-size: ${WORKSPACE_MAX_FILE_SIZE:104857600}
    # Maximum files per anonymous session
    max-files-per-session: ${WORKSPACE_MAX_FILES_PER_SESSION:100}
    # Session file TTL in hours (files auto-expire after this)
    session-file-ttl-hours: ${WORKSPACE_SESSION_FILE_TTL_HOURS:24}
  cleanup:
    # Cron for expired file cleanup (3:30 AM daily)
    cron: ${WORKSPACE_CLEANUP_CRON:0 30 3 * * *}
    # Hours before orphaned session files are marked for deletion
    orphan-hours: ${WORKSPACE_ORPHAN_HOURS:48}

# ============================================
# API Key Encryption Configuration
# ============================================
# AES-256-GCM encryption for secure API key storage.
# IMPORTANT: Change these values in production!
newsinsight:
  encryption:
    # Secret key for encryption (must be at least 32 characters for AES-256)
    # In production, use a strong random string and store securely
    secret: ${NEWSINSIGHT_ENCRYPTION_SECRET:defaultSecretKeyForDevelopmentOnly32}
    # Salt for key derivation (must be at least 16 characters)
    salt: ${NEWSINSIGHT_ENCRYPTION_SALT:defaultSaltValue16ch}
    # Enable migration on startup to encrypt existing plain text API keys
    # Set to true once to migrate, then set back to false
    migrate-on-startup: ${NEWSINSIGHT_ENCRYPTION_MIGRATE:false}

```

---

## backend/data-collection-service/src/test/java/com/newsinsight/collector/CollectorApplicationTests.java

```java
package com.newsinsight.collector;

import org.junit.jupiter.api.Test;
import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.test.context.ActiveProfiles;
import org.springframework.test.context.TestPropertySource;

/**
 * 스프링 컨텍스트 로드 테스트
 * 애플리케이션 설정이 올바르게 구성되었는지 확인합니다.
 */
@SpringBootTest
@ActiveProfiles("test")
@TestPropertySource(properties = {
    // Consul 비활성화
    "spring.cloud.consul.enabled=false",
    "spring.cloud.consul.config.enabled=false",
    "spring.cloud.consul.discovery.enabled=false",
    // Kafka 비활성화
    "spring.kafka.enabled=false",
    // 임베딩 비활성화
    "embedding.enabled=false",
    "hybrid-search.enabled=false",
    "vector-search.enabled=false"
})
class CollectorApplicationTests {

    @Test
    void contextLoads() {
        // 스프링 컨텍스트가 정상적으로 로드되면 테스트 통과
    }
}

```

---

## backend/data-collection-service/src/test/java/com/newsinsight/collector/controller/DashboardEventControllerTest.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.service.DashboardEventService;
import org.junit.jupiter.api.DisplayName;
import org.junit.jupiter.api.Test;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.test.autoconfigure.web.reactive.WebFluxTest;
import org.springframework.boot.test.mock.mockito.MockBean;
import org.springframework.test.context.ActiveProfiles;
import org.springframework.test.web.reactive.server.WebTestClient;

/**
 * DashboardEventController 단위 테스트
 */
@WebFluxTest(DashboardEventController.class)
@ActiveProfiles("test")
class DashboardEventControllerTest {

    @Autowired
    private WebTestClient webTestClient;

    @MockBean
    private DashboardEventService dashboardEventService;

    @Test
    @DisplayName("GET /api/v1/events/stream - SSE 스트림 연결")
    void testEventStream() {
        // SSE 스트림은 테스트가 복잡하므로 연결 자체만 테스트
        // 실제 테스트에서는 StepVerifier 사용 필요
        webTestClient.get()
            .uri("/api/v1/events/stream")
            .exchange()
            .expectStatus().isOk()
            .expectHeader().valueEquals("Content-Type", "text/event-stream");
    }
}

```

---

## backend/data-collection-service/src/test/java/com/newsinsight/collector/repository/DataSourceRepositoryIT.java

```java
package com.newsinsight.collector.repository;

import com.newsinsight.collector.entity.DataSource;
import com.newsinsight.collector.entity.SourceType;
import org.junit.jupiter.api.DisplayName;
import org.junit.jupiter.api.Test;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.test.autoconfigure.orm.jpa.DataJpaTest;
import org.springframework.test.context.ActiveProfiles;
import org.springframework.test.context.DynamicPropertyRegistry;
import org.springframework.test.context.DynamicPropertySource;
import org.testcontainers.containers.PostgreSQLContainer;
import org.testcontainers.junit.jupiter.Container;
import org.testcontainers.junit.jupiter.Testcontainers;

import java.util.List;

import static org.assertj.core.api.Assertions.assertThat;

/**
 * DataSourceRepository 통합 테스트 (Testcontainers 사용)
 * 실제 PostgreSQL 컨테이너에서 테스트를 실행합니다.
 */
@DataJpaTest
@Testcontainers
@ActiveProfiles("test")
class DataSourceRepositoryIT {

    @Container
    static PostgreSQLContainer<?> postgres = new PostgreSQLContainer<>("pgvector/pgvector:pg15")
            .withDatabaseName("testdb")
            .withUsername("test")
            .withPassword("test");

    @DynamicPropertySource
    static void configureProperties(DynamicPropertyRegistry registry) {
        registry.add("spring.datasource.url", postgres::getJdbcUrl);
        registry.add("spring.datasource.username", postgres::getUsername);
        registry.add("spring.datasource.password", postgres::getPassword);
        registry.add("spring.jpa.hibernate.ddl-auto", () -> "create-drop");
    }

    @Autowired
    private DataSourceRepository dataSourceRepository;

    @Test
    @DisplayName("데이터 소스 저장 및 조회")
    void saveAndFindDataSource() {
        // given
        DataSource source = new DataSource();
        source.setName("테스트 뉴스 소스");
        source.setSourceType(SourceType.RSS);
        source.setUrl("https://news.example.com");
        source.setIsActive(true);

        // when
        DataSource saved = dataSourceRepository.save(source);

        // then
        assertThat(saved.getId()).isNotNull();
        assertThat(saved.getName()).isEqualTo("테스트 뉴스 소스");
    }

    @Test
    @DisplayName("활성화된 소스만 조회")
    void findByActiveTrue() {
        // given
        DataSource activeSource = new DataSource();
        activeSource.setName("활성 소스");
        activeSource.setSourceType(SourceType.RSS);
        activeSource.setUrl("https://active.example.com");
        activeSource.setIsActive(true);

        DataSource inactiveSource = new DataSource();
        inactiveSource.setName("비활성 소스");
        inactiveSource.setSourceType(SourceType.RSS);
        inactiveSource.setUrl("https://inactive.example.com");
        inactiveSource.setIsActive(false);

        dataSourceRepository.save(activeSource);
        dataSourceRepository.save(inactiveSource);

        // when
        List<DataSource> activeSources = dataSourceRepository.findByIsActiveTrue();

        // then
        assertThat(activeSources).hasSize(1);
        assertThat(activeSources.get(0).getName()).isEqualTo("활성 소스");
    }

    @Test
    @DisplayName("타입별 소스 조회")
    void findByType() {
        // given
        DataSource newsSource = new DataSource();
        newsSource.setName("뉴스 소스");
        newsSource.setSourceType(SourceType.RSS);
        newsSource.setUrl("https://news.example.com");
        newsSource.setIsActive(true);

        DataSource socialSource = new DataSource();
        socialSource.setName("소셜 소스");
        socialSource.setSourceType(SourceType.WEB);
        socialSource.setUrl("https://social.example.com");
        socialSource.setIsActive(true);

        dataSourceRepository.save(newsSource);
        dataSourceRepository.save(socialSource);

        // when
        List<DataSource> newsSources = dataSourceRepository.findBySourceType(SourceType.RSS);

        // then
        assertThat(newsSources).hasSize(1);
        assertThat(newsSources.get(0).getSourceType()).isEqualTo(SourceType.RSS);
    }
}

```

---

## backend/data-collection-service/src/test/java/com/newsinsight/collector/service/DataSourceServiceTest.java

```java
package com.newsinsight.collector.service;

import com.newsinsight.collector.dto.DataSourceDTO;
import com.newsinsight.collector.entity.DataSource;
import com.newsinsight.collector.entity.SourceType;
import com.newsinsight.collector.mapper.EntityMapper;
import com.newsinsight.collector.repository.DataSourceRepository;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.DisplayName;
import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.extension.ExtendWith;
import org.mockito.InjectMocks;
import org.mockito.Mock;
import org.mockito.junit.jupiter.MockitoExtension;

import java.util.List;
import java.util.Optional;

import static org.assertj.core.api.Assertions.assertThat;
import static org.mockito.ArgumentMatchers.any;
import static org.mockito.Mockito.*;

/**
 * DataSourceService 단위 테스트
 */
@ExtendWith(MockitoExtension.class)
class DataSourceServiceTest {

    @Mock
    private DataSourceRepository dataSourceRepository;

    @Mock
    private EntityMapper entityMapper;

    @InjectMocks
    private DataSourceService dataSourceService;

    private DataSource testSource;

    @BeforeEach
    void setUp() {
        testSource = new DataSource();
        testSource.setId(1L);
        testSource.setName("테스트 소스");
        testSource.setSourceType(SourceType.RSS);
        testSource.setUrl("https://example.com");
        testSource.setIsActive(true);
    }

    @Test
    @DisplayName("활성화된 소스 목록 조회")
    void findActiveSources() {
        // given
        DataSourceDTO dto = new DataSourceDTO(
            1L,
            "테스트 소스",
            "https://example.com",
            SourceType.RSS,
            true,
            null,
            3600,
            null,
            null,
            null,
            null
        );
        
        when(dataSourceRepository.findByIsActiveTrue()).thenReturn(List.of(testSource));
        when(entityMapper.toDTO(testSource)).thenReturn(dto);

        // when
        List<DataSourceDTO> result = dataSourceService.getActiveSources();

        // then
        assertThat(result).hasSize(1);
        assertThat(result.get(0).name()).isEqualTo("테스트 소스");
        verify(dataSourceRepository, times(1)).findByIsActiveTrue();
    }

    @Test
    @DisplayName("ID로 소스 조회")
    void findById() {
        // given
        when(dataSourceRepository.findById(1L)).thenReturn(Optional.of(testSource));

        // when
        Optional<DataSource> result = dataSourceService.findById(1L);

        // then
        assertThat(result).isPresent();
        assertThat(result.get().getName()).isEqualTo("테스트 소스");
    }

    @Test
    @DisplayName("소스 저장")
    void saveSource() {
        // given
        when(dataSourceRepository.save(any(DataSource.class))).thenReturn(testSource);

        // when
        DataSource result = dataSourceService.save(testSource);

        // then
        assertThat(result).isNotNull();
        assertThat(result.getName()).isEqualTo("테스트 소스");
        verify(dataSourceRepository, times(1)).save(testSource);
    }
}

```

---

## backend/data-collection-service/src/test/java/com/newsinsight/collector/util/ApiKeyEncryptorTest.java

```java
package com.newsinsight.collector.util;

import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.DisplayName;
import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.Nested;
import org.junit.jupiter.params.ParameterizedTest;
import org.junit.jupiter.params.provider.ValueSource;
import org.junit.jupiter.params.provider.NullAndEmptySource;

import static org.assertj.core.api.Assertions.assertThat;
import static org.assertj.core.api.Assertions.assertThatThrownBy;

/**
 * ApiKeyEncryptor 단위 테스트
 * 
 * AES-256-GCM 암호화/복호화 및 마스킹 기능을 검증합니다.
 */
class ApiKeyEncryptorTest {

    private ApiKeyEncryptor encryptor;
    
    private static final String TEST_SECRET = "testSecretKeyForUnitTests123456";
    private static final String TEST_SALT = "testSaltValue16c";

    @BeforeEach
    void setUp() {
        encryptor = new ApiKeyEncryptor(TEST_SECRET, TEST_SALT);
    }

    @Nested
    @DisplayName("암호화 테스트")
    class EncryptTests {

        @Test
        @DisplayName("평문 API 키를 암호화하면 ENC: 접두사가 붙는다")
        void encryptAddsPrefix() {
            // given
            String plainApiKey = "sk-test-api-key-1234567890";

            // when
            String encrypted = encryptor.encrypt(plainApiKey);

            // then
            assertThat(encrypted).startsWith("ENC:");
            assertThat(encrypted).isNotEqualTo(plainApiKey);
        }

        @Test
        @DisplayName("동일한 평문을 암호화해도 매번 다른 결과가 나온다 (랜덤 IV)")
        void encryptProducesDifferentResultsForSameInput() {
            // given
            String plainApiKey = "sk-test-api-key-1234567890";

            // when
            String encrypted1 = encryptor.encrypt(plainApiKey);
            String encrypted2 = encryptor.encrypt(plainApiKey);

            // then
            assertThat(encrypted1).isNotEqualTo(encrypted2);
        }

        @Test
        @DisplayName("이미 암호화된 값은 다시 암호화하지 않는다")
        void encryptDoesNotDoubleEncrypt() {
            // given
            String plainApiKey = "sk-test-api-key-1234567890";
            String encrypted = encryptor.encrypt(plainApiKey);

            // when
            String doubleEncrypted = encryptor.encrypt(encrypted);

            // then
            assertThat(doubleEncrypted).isEqualTo(encrypted);
        }

        @ParameterizedTest
        @NullAndEmptySource
        @ValueSource(strings = {"   ", "\t", "\n"})
        @DisplayName("null, 빈 문자열, 공백은 그대로 반환한다")
        void encryptReturnsNullOrEmptyAsIs(String input) {
            // when
            String result = encryptor.encrypt(input);

            // then
            assertThat(result).isEqualTo(input);
        }

        @ParameterizedTest
        @ValueSource(strings = {
            "sk-1234567890abcdef",
            "sk-ant-api03-very-long-anthropic-key-here",
            "AIzaSyA-google-api-key-example",
            "pplx-perplexity-key",
            "한글이포함된API키테스트",
            "special!@#$%^&*()characters"
        })
        @DisplayName("다양한 형식의 API 키를 암호화할 수 있다")
        void encryptVariousApiKeyFormats(String apiKey) {
            // when
            String encrypted = encryptor.encrypt(apiKey);

            // then
            assertThat(encrypted).startsWith("ENC:");
        }
    }

    @Nested
    @DisplayName("복호화 테스트")
    class DecryptTests {

        @Test
        @DisplayName("암호화된 값을 복호화하면 원본을 복원한다")
        void decryptRestoresOriginal() {
            // given
            String original = "sk-test-api-key-1234567890";
            String encrypted = encryptor.encrypt(original);

            // when
            String decrypted = encryptor.decrypt(encrypted);

            // then
            assertThat(decrypted).isEqualTo(original);
        }

        @Test
        @DisplayName("암호화되지 않은 평문은 그대로 반환한다 (하위 호환성)")
        void decryptReturnsPlainTextAsIs() {
            // given
            String plainApiKey = "sk-plain-text-api-key";

            // when
            String result = encryptor.decrypt(plainApiKey);

            // then
            assertThat(result).isEqualTo(plainApiKey);
        }

        @ParameterizedTest
        @NullAndEmptySource
        @ValueSource(strings = {"   ", "\t", "\n"})
        @DisplayName("null, 빈 문자열, 공백은 그대로 반환한다")
        void decryptReturnsNullOrEmptyAsIs(String input) {
            // when
            String result = encryptor.decrypt(input);

            // then
            assertThat(result).isEqualTo(input);
        }

        @Test
        @DisplayName("잘못된 암호문은 예외를 발생시킨다")
        void decryptThrowsOnInvalidCiphertext() {
            // given
            String invalidEncrypted = "ENC:invalid-base64-data!!!";

            // when/then
            assertThatThrownBy(() -> encryptor.decrypt(invalidEncrypted))
                .isInstanceOf(RuntimeException.class);
        }

        @Test
        @DisplayName("손상된 암호문은 예외를 발생시킨다")
        void decryptThrowsOnCorruptedCiphertext() {
            // given
            String original = "sk-test-api-key";
            String encrypted = encryptor.encrypt(original);
            // 암호문의 일부를 손상
            String corrupted = encrypted.substring(0, encrypted.length() - 5) + "XXXXX";

            // when/then
            assertThatThrownBy(() -> encryptor.decrypt(corrupted))
                .isInstanceOf(RuntimeException.class);
        }

        @ParameterizedTest
        @ValueSource(strings = {
            "sk-1234567890abcdef",
            "한글이포함된API키테스트",
            "special!@#$%^&*()characters",
            "very-long-api-key-that-exceeds-typical-length-" + 
                "abcdefghijklmnopqrstuvwxyz0123456789"
        })
        @DisplayName("다양한 형식의 API 키를 암/복호화 라운드트립할 수 있다")
        void encryptDecryptRoundTrip(String original) {
            // when
            String encrypted = encryptor.encrypt(original);
            String decrypted = encryptor.decrypt(encrypted);

            // then
            assertThat(decrypted).isEqualTo(original);
        }
    }

    @Nested
    @DisplayName("isEncrypted 테스트")
    class IsEncryptedTests {

        @Test
        @DisplayName("ENC: 접두사가 있으면 암호화된 것으로 판단")
        void isEncryptedReturnsTrueForEncryptedValue() {
            // given
            String encrypted = encryptor.encrypt("test-key");

            // when/then
            assertThat(encryptor.isEncrypted(encrypted)).isTrue();
        }

        @Test
        @DisplayName("ENC: 접두사가 없으면 암호화되지 않은 것으로 판단")
        void isEncryptedReturnsFalseForPlainValue() {
            // given
            String plain = "sk-plain-api-key";

            // when/then
            assertThat(encryptor.isEncrypted(plain)).isFalse();
        }

        @Test
        @DisplayName("null은 암호화되지 않은 것으로 판단")
        void isEncryptedReturnsFalseForNull() {
            // when/then
            assertThat(encryptor.isEncrypted(null)).isFalse();
        }

        @Test
        @DisplayName("ENC:로 시작하지만 실제 암호문이 아닌 경우도 true 반환 (형식만 체크)")
        void isEncryptedChecksOnlyPrefix() {
            // given
            String fakeEncrypted = "ENC:this-is-not-real-ciphertext";

            // when/then
            assertThat(encryptor.isEncrypted(fakeEncrypted)).isTrue();
        }
    }

    @Nested
    @DisplayName("getMaskedKey 테스트")
    class GetMaskedKeyTests {

        @Test
        @DisplayName("평문 API 키를 마스킹한다")
        void maskPlainApiKey() {
            // given
            String plainApiKey = "sk-test-api-key-1234567890";

            // when
            String masked = encryptor.getMaskedKey(plainApiKey);

            // then
            assertThat(masked).isEqualTo("sk-t****7890");
        }

        @Test
        @DisplayName("암호화된 API 키를 복호화 후 마스킹한다")
        void maskEncryptedApiKey() {
            // given
            String original = "sk-test-api-key-1234567890";
            String encrypted = encryptor.encrypt(original);

            // when
            String masked = encryptor.getMaskedKey(encrypted);

            // then
            assertThat(masked).isEqualTo("sk-t****7890");
        }

        @Test
        @DisplayName("8자 미만의 짧은 키는 ****로 표시")
        void maskShortKey() {
            // given
            String shortKey = "sk-123";

            // when
            String masked = encryptor.getMaskedKey(shortKey);

            // then
            assertThat(masked).isEqualTo("****");
        }

        @Test
        @DisplayName("정확히 8자인 키도 마스킹된다")
        void maskEightCharKey() {
            // given
            String eightCharKey = "12345678";

            // when
            String masked = encryptor.getMaskedKey(eightCharKey);

            // then
            assertThat(masked).isEqualTo("1234****5678");
        }

        @ParameterizedTest
        @NullAndEmptySource
        @ValueSource(strings = {"   "})
        @DisplayName("null, 빈 문자열, 공백은 ****로 표시")
        void maskNullOrEmptyReturnsStars(String input) {
            // when
            String masked = encryptor.getMaskedKey(input);

            // then
            assertThat(masked).isEqualTo("****");
        }
    }

    @Nested
    @DisplayName("다른 키로 생성된 암호문 테스트")
    class DifferentKeyTests {

        @Test
        @DisplayName("다른 secret으로 생성된 Encryptor는 복호화에 실패한다")
        void differentSecretFailsDecryption() {
            // given
            String original = "sk-test-api-key";
            String encrypted = encryptor.encrypt(original);
            
            ApiKeyEncryptor differentEncryptor = new ApiKeyEncryptor(
                "differentSecretKey123456789012", 
                TEST_SALT
            );

            // when/then
            assertThatThrownBy(() -> differentEncryptor.decrypt(encrypted))
                .isInstanceOf(RuntimeException.class);
        }

        @Test
        @DisplayName("다른 salt로 생성된 Encryptor는 복호화에 실패한다")
        void differentSaltFailsDecryption() {
            // given
            String original = "sk-test-api-key";
            String encrypted = encryptor.encrypt(original);
            
            ApiKeyEncryptor differentEncryptor = new ApiKeyEncryptor(
                TEST_SECRET, 
                "differentSalt123"
            );

            // when/then
            assertThatThrownBy(() -> differentEncryptor.decrypt(encrypted))
                .isInstanceOf(RuntimeException.class);
        }

        @Test
        @DisplayName("동일한 키로 생성된 새 인스턴스는 복호화에 성공한다")
        void sameKeySucceedsDecryption() {
            // given
            String original = "sk-test-api-key";
            String encrypted = encryptor.encrypt(original);
            
            ApiKeyEncryptor sameKeyEncryptor = new ApiKeyEncryptor(TEST_SECRET, TEST_SALT);

            // when
            String decrypted = sameKeyEncryptor.decrypt(encrypted);

            // then
            assertThat(decrypted).isEqualTo(original);
        }
    }
}

```

---

## backend/data-collection-service/src/test/resources/application-test.yml

```yml
# 테스트 환경 설정
spring:
  application:
    name: collector-service-test
  
  # Consul 비활성화
  cloud:
    consul:
      enabled: false
      config:
        enabled: false
      discovery:
        enabled: false
  
  # 테스트용 H2 인메모리 DB
  datasource:
    url: jdbc:h2:mem:testdb;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE
    driver-class-name: org.h2.Driver
    username: sa
    password:
  
  jpa:
    database-platform: org.hibernate.dialect.H2Dialect
    hibernate:
      ddl-auto: create-drop
    show-sql: false
  
  # MongoDB 비활성화
  data:
    mongodb:
      uri: mongodb://localhost:27017/test
      auto-index-creation: false
    redis:
      host: localhost
      port: 6379
  
  # Kafka 비활성화
  kafka:
    bootstrap-servers: localhost:9092
    consumer:
      auto-startup: false
    producer:
      retries: 0

# 임베딩 서비스 비활성화
embedding:
  enabled: false
  base-url: http://localhost:8011

# 하이브리드/벡터 검색 비활성화
hybrid-search:
  enabled: false

vector-search:
  enabled: false

# 외부 서비스 Mock URL
collector:
  browser-use:
    base-url: http://localhost:8500
  service:
    crawler:
      base-url: http://localhost:11235
  callback:
    base-url: http://localhost:8081

# 로깅 설정
logging:
  level:
    root: WARN
    com.newsinsight: DEBUG
    org.springframework.web: DEBUG
    org.hibernate.SQL: DEBUG

```

---

## backend/ml-addons/bias-addon/addon_server.py

```py
"""
ML Add-on Server: Bias Analysis with ML Models

NewsInsight ML Add-on 시스템의 편향도 분석 구현.
KoELECTRA/KoBERT 기반 ML 모델을 사용하여 뉴스 기사의
정치적/이념적 편향성을 정확하게 분석합니다.

Features:
- KoELECTRA 기반 정치 성향 분류
- 언론사 기반 편향 분석
- 키워드/프레이밍 기반 폴백 모드 지원
- 객관성/감정적 어조 분석
- 배치 처리 지원
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import Response
from pydantic import BaseModel, Field
from typing import Optional, Dict, List, Any
import time
import os
import re
import logging
import asyncio
from contextlib import asynccontextmanager
from functools import lru_cache

# Prometheus metrics
try:
    from prometheus_client import (
        Counter,
        Histogram,
        Gauge,
        CONTENT_TYPE_LATEST,
        generate_latest,
    )

    PROMETHEUS_AVAILABLE = True
except ImportError:
    PROMETHEUS_AVAILABLE = False

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ML Model cache (lazy loading)
_ml_models = {}
_model_loading = False


def get_ml_models():
    """Lazy load ML models on first use"""
    global _ml_models, _model_loading

    if _ml_models.get("loaded") or _model_loading:
        return _ml_models

    _model_loading = True

    try:
        import torch
        from transformers import (
            AutoTokenizer,
            AutoModelForSequenceClassification,
            pipeline,
        )

        logger.info("Loading bias ML models...")

        # Device selection
        device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"Using device: {device}")

        # 1. Primary Bias/Stance Model
        # Use a Korean text classification model that can be fine-tuned for stance detection
        bias_model_name = os.getenv(
            "BIAS_MODEL",
            "monologg/koelectra-base-v3-discriminator",  # Base Korean ELECTRA
        )

        try:
            # Try to load a sentiment/stance model first
            stance_model_name = os.getenv(
                "STANCE_MODEL",
                "snunlp/KR-FinBert-SC",  # Korean sentiment classifier as stance proxy
            )
            _ml_models["stance_pipeline"] = pipeline(
                "text-classification",
                model=stance_model_name,
                tokenizer=stance_model_name,
                device=0 if device == "cuda" else -1,
                max_length=512,
                truncation=True,
            )
            logger.info(f"Loaded stance model: {stance_model_name}")
        except Exception as e:
            logger.warning(f"Failed to load stance model: {e}")

        # 2. Load base tokenizer and model for custom analysis
        try:
            _ml_models["base_tokenizer"] = AutoTokenizer.from_pretrained(
                bias_model_name
            )
            _ml_models["base_model"] = (
                AutoModelForSequenceClassification.from_pretrained(
                    bias_model_name,
                    num_labels=3,  # left, center, right
                    ignore_mismatched_sizes=True,
                ).to(device)
            )
            _ml_models["base_model"].eval()
            logger.info(f"Loaded base model: {bias_model_name}")
        except Exception as e:
            logger.warning(f"Failed to load base model: {e}")

        # 3. Zero-shot classification for flexible bias detection
        try:
            zero_shot_model = os.getenv(
                "ZERO_SHOT_MODEL",
                "MoritzLaworski/korean-text-classification-zero-shot",
            )
            _ml_models["zero_shot_pipeline"] = pipeline(
                "zero-shot-classification",
                model=zero_shot_model,
                device=0 if device == "cuda" else -1,
            )
            logger.info(f"Loaded zero-shot model: {zero_shot_model}")
        except Exception as e:
            logger.warning(f"Failed to load zero-shot model (optional): {e}")

        _ml_models["device"] = device
        _ml_models["loaded"] = True
        logger.info("All bias ML models loaded successfully")

    except ImportError as e:
        logger.warning(f"ML libraries not available, using heuristic mode: {e}")
        _ml_models["loaded"] = False
    except Exception as e:
        logger.error(f"Error loading ML models: {e}")
        _ml_models["loaded"] = False

    _model_loading = False
    return _ml_models


# Model loading status for health checks
_model_warmup_complete = False
_model_warmup_error = None


async def _warmup_models():
    """Warm up models and track completion status"""
    global _model_warmup_complete, _model_warmup_error
    try:
        logger.info("Starting bias model warm-up...")
        start_time = time.time()

        # Load models synchronously in thread
        models = await asyncio.to_thread(get_ml_models)

        if models.get("loaded"):
            # Run a dummy inference to fully warm up the model
            if "stance_pipeline" in models:
                try:
                    dummy_text = "테스트 문장입니다."
                    _ = models["stance_pipeline"](dummy_text)
                    logger.info("Stance pipeline warm-up inference complete")
                except Exception as e:
                    logger.warning(f"Stance pipeline warm-up failed: {e}")

            elapsed = time.time() - start_time
            logger.info(f"Bias model warm-up completed in {elapsed:.2f}s")
            _model_warmup_complete = True
        else:
            logger.warning("Models not loaded, running in heuristic mode")
            _model_warmup_complete = True

    except Exception as e:
        logger.error(f"Bias model warm-up failed: {e}")
        _model_warmup_error = str(e)
        _model_warmup_complete = True


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan handler"""
    # Startup
    logger.info("Bias addon starting...")

    # Preload models in background if ML is enabled
    if os.getenv("ENABLE_ML_MODELS", "true").lower() == "true":
        asyncio.create_task(_warmup_models())

    yield

    # Shutdown
    logger.info("Bias addon shutting down...")


app = FastAPI(
    title="Bias Analysis Add-on (ML Enhanced)",
    description="Korean news article political/ideological bias analysis with ML for NewsInsight",
    version="2.0.0",
    lifespan=lifespan,
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ========== Prometheus Metrics ==========

if PROMETHEUS_AVAILABLE:
    # Request metrics
    REQUEST_COUNT = Counter(
        "bias_requests_total",
        "Total number of bias analysis requests",
        ["endpoint", "status"],
    )
    REQUEST_LATENCY = Histogram(
        "bias_request_latency_seconds",
        "Request latency in seconds",
        ["endpoint"],
        buckets=(0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0),
    )

    # Analysis metrics
    ANALYSIS_COUNT = Counter(
        "bias_analysis_total",
        "Total number of bias analyses performed",
        ["direction", "mode"],
    )

    # Model metrics
    MODEL_LOADED = Gauge(
        "bias_model_loaded", "Whether ML models are loaded (1=yes, 0=no)"
    )
    MODEL_WARMUP_COMPLETE = Gauge(
        "bias_model_warmup_complete", "Whether model warm-up is complete (1=yes, 0=no)"
    )

    # Error metrics
    ERROR_COUNT = Counter("bias_errors_total", "Total number of errors", ["error_type"])

    @app.get("/metrics")
    async def metrics():
        """Prometheus metrics endpoint"""
        models = get_ml_models() if not _model_loading else {}
        MODEL_LOADED.set(1 if models.get("loaded") else 0)
        MODEL_WARMUP_COMPLETE.set(1 if _model_warmup_complete else 0)

        return Response(content=generate_latest(), media_type=CONTENT_TYPE_LATEST)


# ========== Request/Response Models ==========


class ArticleInput(BaseModel):
    id: Optional[int] = None
    title: Optional[str] = None
    content: Optional[str] = None
    url: Optional[str] = None
    source: Optional[str] = None
    published_at: Optional[str] = None


class AnalysisContext(BaseModel):
    language: Optional[str] = "ko"
    country: Optional[str] = "KR"
    previous_results: Optional[Dict[str, Any]] = None


class ExecutionOptions(BaseModel):
    importance: Optional[str] = "batch"
    debug: Optional[bool] = False
    timeout_ms: Optional[int] = None
    use_ml: Optional[bool] = True
    include_source_bias: Optional[bool] = True
    include_framing: Optional[bool] = True


class AddonRequest(BaseModel):
    request_id: str
    addon_id: str
    task: str = "article_analysis"
    input_schema_version: str = "1.0"
    article: Optional[ArticleInput] = None
    context: Optional[AnalysisContext] = None
    options: Optional[ExecutionOptions] = None


class BiasIndicator(BaseModel):
    phrase: str
    bias_type: str  # political, ideological, framing, selection
    direction: str  # left, right, neutral
    weight: float
    confidence: float = 0.5


class ToneAnalysis(BaseModel):
    objectivity_score: float = Field(
        ..., ge=0.0, le=1.0, description="1 = very objective"
    )
    emotional_language: float = Field(
        ..., ge=0.0, le=1.0, description="1 = very emotional"
    )
    loaded_words_count: int
    examples: Optional[List[str]] = None


class SourceBias(BaseModel):
    source_name: Optional[str] = None
    known_lean: Optional[str] = None  # left, center-left, center, center-right, right
    ownership_info: Optional[str] = None
    reliability_score: Optional[float] = None  # 0-1


class BiasResult(BaseModel):
    overall_bias_score: float = Field(
        ..., ge=-1.0, le=1.0, description="-1 (progressive) to 1 (conservative)"
    )
    bias_label: (
        str  # far_left, left, center_left, center, center_right, right, far_right
    )
    confidence: float = Field(..., ge=0.0, le=1.0)
    political_lean: str  # progressive, moderate, conservative
    distribution: Dict[str, float] = Field(
        default_factory=dict, description="Score distribution"
    )
    indicators: Optional[List[BiasIndicator]] = None
    tone_analysis: Optional[ToneAnalysis] = None
    source_bias: Optional[SourceBias] = None
    framing_notes: Optional[List[str]] = None
    explanations: Optional[List[str]] = None
    analysis_method: str = "ml"  # ml, heuristic, hybrid


class AnalysisResults(BaseModel):
    bias: Optional[BiasResult] = None
    raw: Optional[Dict[str, Any]] = None


class ResponseMeta(BaseModel):
    model_version: str
    model_name: str
    latency_ms: int
    processed_at: str
    device: str = "cpu"


class ErrorInfo(BaseModel):
    code: str
    message: str
    details: Optional[str] = None


class AddonResponse(BaseModel):
    request_id: str
    addon_id: str
    status: str  # success, error, partial
    output_schema_version: str = "1.0"
    results: Optional[AnalysisResults] = None
    error: Optional[ErrorInfo] = None
    meta: Optional[ResponseMeta] = None


# ========== Source Bias Database ==========

# Korean media source political leanings (general perception)
SOURCE_BIAS_MAP = {
    # Progressive-leaning
    "한겨레": {"lean": "left", "score": -0.6, "reliability": 0.7},
    "경향신문": {"lean": "center-left", "score": -0.4, "reliability": 0.7},
    "오마이뉴스": {"lean": "left", "score": -0.7, "reliability": 0.6},
    "프레시안": {"lean": "left", "score": -0.7, "reliability": 0.6},
    "뉴스타파": {"lean": "left", "score": -0.5, "reliability": 0.8},
    # Center
    "연합뉴스": {"lean": "center", "score": 0.0, "reliability": 0.85},
    "KBS": {"lean": "center", "score": 0.0, "reliability": 0.8},
    "MBC": {"lean": "center-left", "score": -0.2, "reliability": 0.75},
    "SBS": {"lean": "center", "score": 0.0, "reliability": 0.75},
    "JTBC": {"lean": "center-left", "score": -0.2, "reliability": 0.75},
    "YTN": {"lean": "center", "score": 0.0, "reliability": 0.75},
    "뉴시스": {"lean": "center", "score": 0.0, "reliability": 0.7},
    "뉴스1": {"lean": "center", "score": 0.05, "reliability": 0.7},
    # Conservative-leaning
    "조선일보": {"lean": "right", "score": 0.6, "reliability": 0.7},
    "동아일보": {"lean": "center-right", "score": 0.4, "reliability": 0.7},
    "중앙일보": {"lean": "center-right", "score": 0.3, "reliability": 0.75},
    "매일경제": {"lean": "center-right", "score": 0.3, "reliability": 0.7},
    "한국경제": {"lean": "right", "score": 0.5, "reliability": 0.7},
    "TV조선": {"lean": "right", "score": 0.7, "reliability": 0.55},
    "채널A": {"lean": "right", "score": 0.6, "reliability": 0.55},
    "MBN": {"lean": "center-right", "score": 0.4, "reliability": 0.6},
    "문화일보": {"lean": "right", "score": 0.5, "reliability": 0.65},
    "세계일보": {"lean": "center-right", "score": 0.3, "reliability": 0.65},
}

# Progressive keywords/expressions
PROGRESSIVE_KEYWORDS = [
    ("복지", 0.3, 0.6),
    ("노동자 권리", 0.5, 0.7),
    ("환경", 0.2, 0.5),
    ("평등", 0.4, 0.7),
    ("인권", 0.3, 0.6),
    ("진보", 0.6, 0.8),
    ("민주화", 0.4, 0.7),
    ("시민단체", 0.3, 0.6),
    ("재벌 개혁", 0.5, 0.7),
    ("사회 정의", 0.4, 0.7),
    ("최저임금", 0.3, 0.6),
    ("공공성", 0.3, 0.6),
    ("노동조합", 0.4, 0.7),
    ("비정규직", 0.3, 0.6),
    ("부유세", 0.5, 0.7),
    ("공정경제", 0.3, 0.6),
    ("대북 화해", 0.4, 0.7),
    ("성소수자", 0.4, 0.7),
    ("젠더", 0.3, 0.6),
    ("다양성", 0.3, 0.6),
    ("기후위기", 0.3, 0.6),
    ("탈원전", 0.4, 0.7),
]

# Conservative keywords/expressions
CONSERVATIVE_KEYWORDS = [
    ("안보", 0.3, 0.6),
    ("자유시장", 0.4, 0.7),
    ("규제 완화", 0.4, 0.7),
    ("전통", 0.3, 0.6),
    ("보수", 0.6, 0.8),
    ("국가 안보", 0.4, 0.7),
    ("한미동맹", 0.3, 0.6),
    ("기업 친화", 0.4, 0.7),
    ("성장", 0.2, 0.5),
    ("법질서", 0.3, 0.6),
    ("애국", 0.4, 0.7),
    ("반공", 0.6, 0.8),
    ("자유민주주의", 0.3, 0.6),
    ("대북 강경", 0.4, 0.7),
    ("북핵", 0.3, 0.6),
    ("기업규제 완화", 0.4, 0.7),
    ("시장경제", 0.3, 0.6),
    ("원전", 0.3, 0.6),
    ("국방", 0.3, 0.6),
    ("자유대한민국", 0.5, 0.7),
    ("종북", 0.6, 0.8),
]

# Framing patterns
FRAMING_PATTERNS = {
    "left": [
        (r"민중", "진보적 프레이밍", 0.4),
        (r"사회적\s*약자", "진보적 관점", 0.3),
        (r"불평등\s*심화", "불평등 강조", 0.4),
        (r"재벌\s*특혜", "대기업 비판적", 0.4),
        (r"노동\s*착취", "노동자 권익 강조", 0.5),
        (r"검찰\s*독재", "검찰 비판적", 0.5),
        (r"적폐", "적폐 청산 프레임", 0.5),
    ],
    "right": [
        (r"종북", "보수적 프레이밍", 0.6),
        (r"안보\s*위협", "안보 강조", 0.4),
        (r"경제\s*성장", "성장 중심", 0.3),
        (r"시장\s*원리", "시장주의적 관점", 0.3),
        (r"규제\s*폐해", "규제 비판적", 0.4),
        (r"좌파\s*정권", "정권 비판적", 0.5),
        (r"포퓰리즘", "포퓰리즘 비판", 0.4),
    ],
}

# Emotional/loaded words
LOADED_WORDS = {
    "left": ["착취", "불의", "특권층", "기득권", "차별", "탄압", "독재", "적폐"],
    "right": ["종북", "좌파", "선동", "매국", "폭력", "과격", "빨갱이", "친중"],
    "emotional": [
        "충격적",
        "경악",
        "황당",
        "기막힌",
        "어처구니",
        "분노",
        "통탄",
        "개탄",
    ],
}


# ========== Heuristic Analysis Functions ==========


def get_source_bias(source: Optional[str]) -> SourceBias:
    """Get bias info from media source name"""
    if not source:
        return SourceBias()

    for name, info in SOURCE_BIAS_MAP.items():
        if name in source:
            return SourceBias(
                source_name=name,
                known_lean=info["lean"],
                reliability_score=info.get("reliability"),
            )

    return SourceBias(source_name=source, known_lean="unknown")


def analyze_keyword_bias_heuristic(text: str) -> tuple[float, List[BiasIndicator]]:
    """Keyword-based bias analysis (heuristic)"""
    if not text:
        return 0.0, []

    indicators = []
    progressive_score = 0.0
    conservative_score = 0.0

    text_lower = text.lower()

    # Progressive keywords
    for keyword, weight, conf in PROGRESSIVE_KEYWORDS:
        if keyword in text_lower:
            progressive_score += weight
            indicators.append(
                BiasIndicator(
                    phrase=keyword,
                    bias_type="political",
                    direction="left",
                    weight=weight,
                    confidence=conf,
                )
            )

    # Conservative keywords
    for keyword, weight, conf in CONSERVATIVE_KEYWORDS:
        if keyword in text_lower:
            conservative_score += weight
            indicators.append(
                BiasIndicator(
                    phrase=keyword,
                    bias_type="political",
                    direction="right",
                    weight=weight,
                    confidence=conf,
                )
            )

    # Normalize score (-1 ~ 1)
    total = progressive_score + conservative_score
    if total == 0:
        return 0.0, indicators

    bias_score = (conservative_score - progressive_score) / max(total, 1)
    return bias_score, indicators


def analyze_framing_heuristic(text: str) -> tuple[float, List[str]]:
    """Framing analysis (heuristic)"""
    if not text:
        return 0.0, []

    notes = []
    left_score = 0.0
    right_score = 0.0

    for pattern, note, weight in FRAMING_PATTERNS["left"]:
        if re.search(pattern, text):
            left_score += weight
            notes.append(f"[진보] {note}")

    for pattern, note, weight in FRAMING_PATTERNS["right"]:
        if re.search(pattern, text):
            right_score += weight
            notes.append(f"[보수] {note}")

    total = left_score + right_score
    if total == 0:
        return 0.0, notes

    framing_bias = (right_score - left_score) / total
    return framing_bias, notes


def analyze_tone_heuristic(text: str) -> ToneAnalysis:
    """Tone/objectivity analysis (heuristic)"""
    if not text:
        return ToneAnalysis(
            objectivity_score=0.5, emotional_language=0.0, loaded_words_count=0
        )

    loaded_count = 0
    examples = []

    # Count loaded words
    for direction, words in LOADED_WORDS.items():
        for word in words:
            count = text.count(word)
            if count > 0:
                loaded_count += count
                examples.append(word)

    # Emotional expression ratio estimation
    emotional_score = min(loaded_count / 10, 1.0)
    objectivity_score = 1.0 - emotional_score

    return ToneAnalysis(
        objectivity_score=round(objectivity_score, 3),
        emotional_language=round(emotional_score, 3),
        loaded_words_count=loaded_count,
        examples=examples[:5] if examples else None,
    )


def score_to_label(score: float) -> tuple[str, str]:
    """Convert bias score to label"""
    if score <= -0.6:
        return "far_left", "progressive"
    elif score <= -0.3:
        return "left", "progressive"
    elif score <= -0.1:
        return "center_left", "moderate"
    elif score <= 0.1:
        return "center", "moderate"
    elif score <= 0.3:
        return "center_right", "moderate"
    elif score <= 0.6:
        return "right", "conservative"
    else:
        return "far_right", "conservative"


def analyze_bias_heuristic(
    text: str,
    source: Optional[str] = None,
    include_source_bias: bool = True,
    include_framing: bool = True,
) -> BiasResult:
    """Full heuristic bias analysis"""
    if not text or not text.strip():
        return BiasResult(
            overall_bias_score=0.0,
            bias_label="center",
            confidence=0.0,
            political_lean="unknown",
            distribution={"left": 0.33, "center": 0.34, "right": 0.33},
            explanations=["분석할 텍스트 없음"],
            analysis_method="heuristic",
        )

    explanations = []

    # 1. Source bias
    source_bias = get_source_bias(source) if include_source_bias else SourceBias()
    source_score = 0.0
    if source_bias.known_lean and source and include_source_bias:
        for name, info in SOURCE_BIAS_MAP.items():
            if name in source:
                source_score = info["score"]
                explanations.append(f"언론사 성향: {name} ({source_bias.known_lean})")
                break

    # 2. Keyword-based bias
    keyword_score, indicators = analyze_keyword_bias_heuristic(text)
    if indicators:
        explanations.append(f"편향 키워드 {len(indicators)}개 발견")

    # 3. Framing analysis
    framing_score, framing_notes = (0.0, [])
    if include_framing:
        framing_score, framing_notes = analyze_framing_heuristic(text)
        if framing_notes:
            explanations.append(f"프레이밍 패턴 {len(framing_notes)}개 발견")

    # 4. Tone analysis
    tone_analysis = analyze_tone_heuristic(text)
    if tone_analysis.loaded_words_count > 0:
        explanations.append(f"감정적 표현 {tone_analysis.loaded_words_count}개 발견")

    # 5. Combined score (weighted average)
    # Source 30%, Keywords 40%, Framing 30%
    weights = {"source": 0.3, "keyword": 0.4, "framing": 0.3}
    overall_score = (
        source_score * weights["source"]
        + keyword_score * weights["keyword"]
        + framing_score * weights["framing"]
    )
    overall_score = max(-1.0, min(1.0, overall_score))

    # 6. Determine label
    bias_label, political_lean = score_to_label(overall_score)

    # 7. Calculate confidence
    evidence_count = len(indicators) + len(framing_notes)
    base_confidence = 0.3 if source_score != 0 else 0.2
    confidence = min(base_confidence + evidence_count * 0.08, 0.9)

    # 8. Calculate distribution
    if overall_score < 0:
        left_ratio = abs(overall_score) * 0.5 + 0.25
        right_ratio = 0.25 - abs(overall_score) * 0.15
    elif overall_score > 0:
        right_ratio = overall_score * 0.5 + 0.25
        left_ratio = 0.25 - overall_score * 0.15
    else:
        left_ratio = 0.25
        right_ratio = 0.25
    center_ratio = 1.0 - left_ratio - right_ratio

    distribution = {
        "left": round(max(0, left_ratio), 3),
        "center": round(max(0, center_ratio), 3),
        "right": round(max(0, right_ratio), 3),
    }

    explanations.append(f"종합 편향 점수: {overall_score:.2f} (-1=진보, 1=보수)")

    return BiasResult(
        overall_bias_score=round(overall_score, 4),
        bias_label=bias_label,
        confidence=round(confidence, 3),
        political_lean=political_lean,
        distribution=distribution,
        indicators=indicators if indicators else None,
        tone_analysis=tone_analysis,
        source_bias=source_bias if include_source_bias else None,
        framing_notes=framing_notes if framing_notes else None,
        explanations=explanations,
        analysis_method="heuristic",
    )


# ========== ML-based Bias Analysis ==========


def analyze_bias_ml(
    text: str,
    source: Optional[str] = None,
    include_source_bias: bool = True,
    include_framing: bool = True,
) -> BiasResult:
    """ML-based bias analysis"""
    models = get_ml_models()

    if not models.get("loaded"):
        logger.warning("ML models not loaded, falling back to heuristic")
        return analyze_bias_heuristic(
            text, source, include_source_bias, include_framing
        )

    try:
        import torch

        device = models.get("device", "cpu")
        explanations = []
        ml_score = 0.0
        ml_confidence = 0.5

        # 1. Try zero-shot classification for political stance
        if "zero_shot_pipeline" in models:
            try:
                candidate_labels = ["진보적", "중도", "보수적"]
                result = models["zero_shot_pipeline"](
                    text[:512],
                    candidate_labels,
                    hypothesis_template="이 기사는 {} 관점으로 작성되었습니다.",
                )

                label_scores = dict(zip(result["labels"], result["scores"]))
                progressive_prob = label_scores.get("진보적", 0.33)
                center_prob = label_scores.get("중도", 0.34)
                conservative_prob = label_scores.get("보수적", 0.33)

                # Calculate bias score from zero-shot
                ml_score = conservative_prob - progressive_prob
                ml_confidence = max(result["scores"])

                explanations.append(
                    f"Zero-shot 분류: 진보 {progressive_prob:.1%}, 중도 {center_prob:.1%}, 보수 {conservative_prob:.1%}"
                )

            except Exception as e:
                logger.warning(f"Zero-shot classification failed: {e}")

        # 2. Use stance pipeline as fallback/supplement
        elif "stance_pipeline" in models:
            try:
                pipeline_result = models["stance_pipeline"](text[:512])

                if isinstance(pipeline_result, list):
                    pipeline_result = pipeline_result[0]

                # Map sentiment to stance (proxy)
                label = pipeline_result.get("label", "").upper()
                score = pipeline_result.get("score", 0.5)

                # Positive sentiment often correlates with pro-status-quo (center-right)
                # Negative sentiment often correlates with criticism (can be either side)
                if "POSITIVE" in label or "긍정" in label:
                    ml_score = 0.1 * score  # Slight center-right
                elif "NEGATIVE" in label or "부정" in label:
                    ml_score = 0.0  # Neutral for negative (needs more context)
                else:
                    ml_score = 0.0

                ml_confidence = score * 0.5  # Lower confidence since it's a proxy
                explanations.append(f"감정 기반 추정 (신뢰도: {ml_confidence:.1%})")

            except Exception as e:
                logger.warning(f"Stance pipeline failed: {e}")

        # 3. Combine ML with heuristic for better results
        heuristic_result = analyze_bias_heuristic(
            text, source, include_source_bias, include_framing
        )

        # Weighted combination: ML 40%, Heuristic 60% (heuristic is more reliable for political bias)
        combined_score = ml_score * 0.4 + heuristic_result.overall_bias_score * 0.6
        combined_score = max(-1.0, min(1.0, combined_score))

        combined_confidence = ml_confidence * 0.4 + heuristic_result.confidence * 0.6

        # Determine label
        bias_label, political_lean = score_to_label(combined_score)

        # Calculate distribution
        if combined_score < 0:
            left_ratio = abs(combined_score) * 0.5 + 0.25
            right_ratio = 0.25 - abs(combined_score) * 0.15
        elif combined_score > 0:
            right_ratio = combined_score * 0.5 + 0.25
            left_ratio = 0.25 - combined_score * 0.15
        else:
            left_ratio = 0.25
            right_ratio = 0.25
        center_ratio = 1.0 - left_ratio - right_ratio

        distribution = {
            "left": round(max(0, left_ratio), 3),
            "center": round(max(0, center_ratio), 3),
            "right": round(max(0, right_ratio), 3),
        }

        # Merge explanations
        all_explanations = explanations + (heuristic_result.explanations or [])
        all_explanations.append(
            f"종합 편향 점수: {combined_score:.2f} (-1=진보, 1=보수)"
        )

        return BiasResult(
            overall_bias_score=round(combined_score, 4),
            bias_label=bias_label,
            confidence=round(combined_confidence, 3),
            political_lean=political_lean,
            distribution=distribution,
            indicators=heuristic_result.indicators,
            tone_analysis=heuristic_result.tone_analysis,
            source_bias=heuristic_result.source_bias,
            framing_notes=heuristic_result.framing_notes,
            explanations=all_explanations,
            analysis_method="hybrid",  # ML + heuristic
        )

    except Exception as e:
        logger.error(f"ML bias analysis failed: {e}", exc_info=True)
        result = analyze_bias_heuristic(
            text, source, include_source_bias, include_framing
        )
        result.explanations = (result.explanations or []) + [
            f"ML 분석 실패, 휴리스틱 폴백: {str(e)[:50]}"
        ]
        return result


def analyze_bias(
    text: str,
    source: Optional[str] = None,
    use_ml: bool = True,
    include_source_bias: bool = True,
    include_framing: bool = True,
) -> BiasResult:
    """Main bias analysis function"""
    if not text or not text.strip():
        return BiasResult(
            overall_bias_score=0.0,
            bias_label="center",
            confidence=0.0,
            political_lean="unknown",
            distribution={"left": 0.33, "center": 0.34, "right": 0.33},
            explanations=["분석할 텍스트 없음"],
            analysis_method="none",
        )

    if use_ml:
        return analyze_bias_ml(text, source, include_source_bias, include_framing)
    else:
        return analyze_bias_heuristic(
            text, source, include_source_bias, include_framing
        )


# ========== API Endpoints ==========


@app.get("/health")
async def health_check():
    """Health check endpoint"""
    models = get_ml_models()
    ml_status = "loaded" if models.get("loaded") else "heuristic_mode"

    return {
        "status": "healthy",
        "service": "bias-addon",
        "version": "2.0.0",
        "ml_status": ml_status,
        "warmup_complete": _model_warmup_complete,
        "warmup_error": _model_warmup_error,
        "device": models.get("device", "cpu"),
        "models": {
            "zero_shot_pipeline": "zero_shot_pipeline" in models,
            "stance_pipeline": "stance_pipeline" in models,
            "base_model": "base_model" in models,
        },
    }


@app.get("/ready")
async def readiness_check():
    """Readiness check - only returns healthy when models are warmed up"""
    if not _model_warmup_complete:
        return {"status": "warming_up", "ready": False}

    models = get_ml_models()
    return {
        "status": "ready",
        "ready": True,
        "ml_enabled": models.get("loaded", False),
        "warmup_error": _model_warmup_error,
    }


@app.get("/models")
async def get_model_info():
    """Get information about loaded models"""
    models = get_ml_models()

    return {
        "loaded": models.get("loaded", False),
        "device": models.get("device", "cpu"),
        "available_models": [k for k in models.keys() if k not in ["loaded", "device"]],
        "bias_model": os.getenv(
            "BIAS_MODEL", "monologg/koelectra-base-v3-discriminator"
        ),
        "zero_shot_model": os.getenv(
            "ZERO_SHOT_MODEL", "MoritzLaworski/korean-text-classification-zero-shot"
        ),
    }


@app.get("/sources")
async def get_source_database():
    """Get the source bias database"""
    return {
        "sources": [
            {
                "name": name,
                "lean": info["lean"],
                "score": info["score"],
                "reliability": info.get("reliability", 0.5),
            }
            for name, info in SOURCE_BIAS_MAP.items()
        ],
        "total": len(SOURCE_BIAS_MAP),
    }


@app.post("/analyze", response_model=AddonResponse)
async def analyze(request: AddonRequest):
    """
    Article bias analysis endpoint.
    Called by NewsInsight Orchestrator.
    """
    start_time = time.time()
    models = get_ml_models()

    try:
        # Validate input
        if not request.article:
            raise ValueError("article is required")

        # Prepare text for analysis
        text = ""
        if request.article.title:
            text += request.article.title + " "
        if request.article.content:
            text += request.article.content

        # Get options
        options = request.options or ExecutionOptions()
        use_ml = options.use_ml if options.use_ml is not None else True
        include_source_bias = (
            options.include_source_bias
            if options.include_source_bias is not None
            else True
        )
        include_framing = (
            options.include_framing if options.include_framing is not None else True
        )

        # Run bias analysis
        bias_result = analyze_bias(
            text,
            source=request.article.source,
            use_ml=use_ml,
            include_source_bias=include_source_bias,
            include_framing=include_framing,
        )

        # Build response
        latency_ms = int((time.time() - start_time) * 1000)

        model_name = (
            "koelectra-bias-hybrid-v2" if models.get("loaded") else "bias-heuristic-v1"
        )

        # Track Prometheus metrics
        if PROMETHEUS_AVAILABLE:
            REQUEST_COUNT.labels(endpoint="analyze", status="success").inc()
            REQUEST_LATENCY.labels(endpoint="analyze").observe(time.time() - start_time)
            ANALYSIS_COUNT.labels(
                direction=bias_result.political_lean,
                mode="ml" if models.get("loaded") and use_ml else "heuristic",
            ).inc()

        return AddonResponse(
            request_id=request.request_id,
            addon_id=request.addon_id,
            status="success",
            results=AnalysisResults(
                bias=bias_result,
                raw={
                    "text_length": len(text),
                    "source": request.article.source,
                    "ml_available": models.get("loaded", False),
                    "analysis_method": bias_result.analysis_method,
                },
            ),
            meta=ResponseMeta(
                model_version="2.0.0",
                model_name=model_name,
                latency_ms=latency_ms,
                processed_at=time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
                device=models.get("device", "cpu"),
            ),
        )

    except ValueError as e:
        latency_ms = int((time.time() - start_time) * 1000)

        # Track error metrics
        if PROMETHEUS_AVAILABLE:
            REQUEST_COUNT.labels(endpoint="analyze", status="error").inc()
            REQUEST_LATENCY.labels(endpoint="analyze").observe(time.time() - start_time)
            ERROR_COUNT.labels(error_type="ValidationError").inc()

        return AddonResponse(
            request_id=request.request_id,
            addon_id=request.addon_id,
            status="error",
            error=ErrorInfo(code="VALIDATION_ERROR", message=str(e)),
            meta=ResponseMeta(
                model_version="2.0.0",
                model_name="bias-addon",
                latency_ms=latency_ms,
                processed_at=time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
                device=models.get("device", "cpu"),
            ),
        )
    except Exception as e:
        logger.error(f"Analysis error: {e}", exc_info=True)
        latency_ms = int((time.time() - start_time) * 1000)

        # Track error metrics
        if PROMETHEUS_AVAILABLE:
            REQUEST_COUNT.labels(endpoint="analyze", status="error").inc()
            REQUEST_LATENCY.labels(endpoint="analyze").observe(time.time() - start_time)
            ERROR_COUNT.labels(error_type=type(e).__name__).inc()

        return AddonResponse(
            request_id=request.request_id,
            addon_id=request.addon_id,
            status="error",
            error=ErrorInfo(
                code="ANALYSIS_ERROR",
                message=str(e),
                details="Error occurred during bias analysis",
            ),
            meta=ResponseMeta(
                model_version="2.0.0",
                model_name="bias-addon",
                latency_ms=latency_ms,
                processed_at=time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
                device=models.get("device", "cpu"),
            ),
        )


@app.post("/batch", response_model=List[AddonResponse])
async def analyze_batch(requests: List[AddonRequest]):
    """Batch analyze multiple articles"""
    results = []
    for req in requests:
        result = await analyze(req)
        results.append(result)
    return results


@app.post("/analyze/simple")
async def analyze_simple(text: str, source: Optional[str] = None):
    """
    Simple text bias analysis endpoint.
    For quick testing without full request structure.
    """
    start_time = time.time()

    result = analyze_bias(text, source=source)
    latency_ms = int((time.time() - start_time) * 1000)

    return {
        "text": text[:100] + "..." if len(text) > 100 else text,
        "source": source,
        "bias": result.model_dump(),
        "latency_ms": latency_ms,
    }


class SourceAnalysisRequest(BaseModel):
    """Request model for source-only analysis"""

    source: str


@app.post("/analyze/source")
async def analyze_source_only(request: SourceAnalysisRequest):
    """
    Analyze bias based on source name only.
    Quick lookup without content analysis.

    Request body:
        {"source": "조선일보"}
    """
    source = request.source
    source_bias = get_source_bias(source)

    if source_bias.known_lean == "unknown":
        return {
            "source": source,
            "found": False,
            "message": "Source not found in database",
        }

    # Get score from database
    score = 0.0
    for name, info in SOURCE_BIAS_MAP.items():
        if name in source:
            score = info["score"]
            break

    bias_label, political_lean = score_to_label(score)

    return {
        "source": source,
        "found": True,
        "bias": {
            "score": score,
            "label": bias_label,
            "political_lean": political_lean,
            "known_lean": source_bias.known_lean,
            "reliability": source_bias.reliability_score,
        },
    }


# ========== Entry Point ==========

if __name__ == "__main__":
    import uvicorn

    port = int(os.getenv("PORT", "8102"))
    host = os.getenv("HOST", "0.0.0.0")

    uvicorn.run(app, host=host, port=port)

```

---

## backend/ml-addons/docker-compose.yml

```yml
# ML Add-ons Docker Compose
# 모든 ML Add-on 서비스를 함께 실행합니다.

version: '3.8'

services:
  # 감정 분석 Add-on
  sentiment-addon:
    build:
      context: ./sentiment-addon
      dockerfile: Dockerfile
    container_name: newsinsight-sentiment-addon
    ports:
      - "8100:8100"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8100/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - ml-addons-network
    labels:
      - "addon.type=sentiment"
      - "addon.version=1.0.0"

  # 팩트체크 Add-on (ML Enhanced)
  factcheck-addon:
    build:
      context: ./factcheck-addon
      dockerfile: Dockerfile
      args:
        ENABLE_GPU: ${ENABLE_GPU:-false}
        PRELOAD_MODELS: ${PRELOAD_MODELS:-false}
    container_name: newsinsight-factcheck-addon
    ports:
      - "8101:8101"
    restart: unless-stopped
    environment:
      # ML Model Configuration
      - ENABLE_ML_MODELS=${ENABLE_ML_MODELS:-true}
      - CLAIM_MODEL=${CLAIM_MODEL:-monologg/koelectra-base-v3-discriminator}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2}
      - NER_MODEL=${NER_MODEL:-klue/bert-base}
      - SENTIMENT_MODEL=${SENTIMENT_MODEL:-klue/roberta-base}
      # External API Keys (optional)
      - GOOGLE_FACTCHECK_API_KEY=${GOOGLE_FACTCHECK_API_KEY:-}
      - SNU_FACTCHECK_API_URL=${SNU_FACTCHECK_API_URL:-}
      # Cache Configuration
      - HF_HOME=/app/.cache/huggingface
      - TRANSFORMERS_CACHE=/app/.cache/huggingface/transformers
      - TORCH_HOME=/app/.cache/torch
    volumes:
      # Persist model cache to speed up restarts
      - factcheck-model-cache:/app/.cache
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8101/health"]
      interval: 30s
      timeout: 30s
      retries: 3
      start_period: 120s  # Longer start period for model loading
    networks:
      - ml-addons-network
    labels:
      - "addon.type=factcheck"
      - "addon.version=2.0.0"
      - "addon.ml_enabled=true"
    deploy:
      resources:
        limits:
          memory: 4G  # ML models require more memory
        reservations:
          memory: 2G

  # 편향도 분석 Add-on
  bias-addon:
    build:
      context: ./bias-addon
      dockerfile: Dockerfile
    container_name: newsinsight-bias-addon
    ports:
      - "8102:8102"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8102/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - ml-addons-network
    labels:
      - "addon.type=bias"
      - "addon.version=1.0.0"

volumes:
  factcheck-model-cache:
    name: newsinsight-factcheck-models

networks:
  ml-addons-network:
    driver: bridge
    name: newsinsight-ml-addons

```

---

## backend/ml-addons/factcheck-addon/addon_server.py

```py
"""
ML Add-on Server: Fact-Check Analysis with ML Models

NewsInsight ML Add-on 시스템의 팩트체크 구현.
KoBERT/KoELECTRA 기반 ML 모델과 외부 팩트체크 API를 통합하여
뉴스 기사의 사실 검증 및 신뢰도 분석을 수행합니다.

Features:
- ML 기반 주장 추출 및 분류
- 의미론적 유사도 기반 교차 검증
- 외부 팩트체크 API 연동 (SNU, Google Fact Check)
- 신뢰도 점수 산출 세부 분석 제공
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import Response
from pydantic import BaseModel, Field
from typing import Optional, Dict, List, Any, Tuple
from enum import Enum
import time
import re
import hashlib
import os
import logging
import asyncio
from functools import lru_cache
from contextlib import asynccontextmanager

# Prometheus metrics
try:
    from prometheus_client import (
        Counter,
        Histogram,
        Gauge,
        CONTENT_TYPE_LATEST,
        generate_latest,
    )

    PROMETHEUS_AVAILABLE = True
except ImportError:
    PROMETHEUS_AVAILABLE = False

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ML Model imports (lazy loading for faster startup)
_ml_models = {}
_model_loading = False


def get_ml_models():
    """Lazy load ML models on first use"""
    global _ml_models, _model_loading

    if _ml_models or _model_loading:
        return _ml_models

    _model_loading = True

    try:
        import torch
        from transformers import (
            AutoTokenizer,
            AutoModelForSequenceClassification,
            AutoModelForTokenClassification,
            pipeline,
        )
        from sentence_transformers import SentenceTransformer

        logger.info("Loading ML models...")

        # Device selection
        device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"Using device: {device}")

        # 1. Claim Classification Model (KoELECTRA)
        claim_model_name = os.getenv(
            "CLAIM_MODEL", "monologg/koelectra-base-v3-discriminator"
        )
        try:
            _ml_models["claim_tokenizer"] = AutoTokenizer.from_pretrained(
                claim_model_name
            )
            _ml_models["claim_model"] = (
                AutoModelForSequenceClassification.from_pretrained(
                    claim_model_name,
                    num_labels=3,  # claim, non-claim, uncertain
                ).to(device)
            )
            _ml_models["claim_model"].eval()
            logger.info(f"Loaded claim model: {claim_model_name}")
        except Exception as e:
            logger.warning(f"Failed to load claim model: {e}")

        # 2. Sentence Transformer for Semantic Similarity
        embedding_model_name = os.getenv(
            "EMBEDDING_MODEL",
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        )
        try:
            _ml_models["sentence_transformer"] = SentenceTransformer(
                embedding_model_name
            )
            logger.info(f"Loaded embedding model: {embedding_model_name}")
        except Exception as e:
            logger.warning(f"Failed to load embedding model: {e}")

        # 3. NER for Entity Extraction (Korean)
        ner_model_name = os.getenv("NER_MODEL", "klue/bert-base")
        try:
            _ml_models["ner_pipeline"] = pipeline(
                "ner",
                model=ner_model_name,
                tokenizer=ner_model_name,
                aggregation_strategy="simple",
                device=0 if device == "cuda" else -1,
            )
            logger.info(f"Loaded NER model: {ner_model_name}")
        except Exception as e:
            logger.warning(f"Failed to load NER model: {e}")

        # 4. Sentiment for tone analysis
        sentiment_model_name = os.getenv("SENTIMENT_MODEL", "klue/roberta-base")
        try:
            _ml_models["sentiment_pipeline"] = pipeline(
                "sentiment-analysis",
                model=sentiment_model_name,
                tokenizer=sentiment_model_name,
                device=0 if device == "cuda" else -1,
            )
            logger.info(f"Loaded sentiment model: {sentiment_model_name}")
        except Exception as e:
            logger.warning(f"Failed to load sentiment model: {e}")

        _ml_models["device"] = device
        _ml_models["loaded"] = True
        logger.info("All ML models loaded successfully")

    except ImportError as e:
        logger.warning(f"ML libraries not available, using heuristic mode: {e}")
        _ml_models["loaded"] = False
    except Exception as e:
        logger.error(f"Error loading ML models: {e}")
        _ml_models["loaded"] = False

    _model_loading = False
    return _ml_models


# External API clients (lazy initialization)
_api_clients = {}


async def get_http_client():
    """Get async HTTP client"""
    global _api_clients

    if "http_client" not in _api_clients:
        try:
            import httpx

            _api_clients["http_client"] = httpx.AsyncClient(timeout=30.0)
        except ImportError:
            _api_clients["http_client"] = None

    return _api_clients.get("http_client")


# Model loading status for health checks
_model_warmup_complete = False
_model_warmup_error = None


async def _warmup_models():
    """Warm up models and track completion status"""
    global _model_warmup_complete, _model_warmup_error
    try:
        logger.info("Starting model warm-up...")
        start_time = time.time()

        # Load models synchronously in thread
        models = await asyncio.to_thread(get_ml_models)

        if models.get("loaded"):
            # Run a dummy inference to fully warm up the model
            if "claim_model" in models and "claim_tokenizer" in models:
                try:
                    dummy_text = "테스트 문장입니다."
                    tokenizer = models["claim_tokenizer"]
                    model = models["claim_model"]
                    inputs = tokenizer(
                        dummy_text, return_tensors="pt", truncation=True, max_length=128
                    )
                    if models.get("device") == "cuda":
                        inputs = {k: v.cuda() for k, v in inputs.items()}
                    with __import__("torch").no_grad():
                        _ = model(**inputs)
                    logger.info("Claim model warm-up inference complete")
                except Exception as e:
                    logger.warning(f"Claim model warm-up inference failed: {e}")

            elapsed = time.time() - start_time
            logger.info(f"Model warm-up completed in {elapsed:.2f}s")
            _model_warmup_complete = True
        else:
            logger.warning("Models not loaded, running in heuristic mode")
            _model_warmup_complete = True  # Still mark as complete for heuristic mode

    except Exception as e:
        logger.error(f"Model warm-up failed: {e}")
        _model_warmup_error = str(e)
        _model_warmup_complete = True  # Mark complete even on error to avoid blocking


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan handler"""
    # Startup
    logger.info("Factcheck addon starting...")

    # Preload models in background if ML is enabled
    if os.getenv("ENABLE_ML_MODELS", "true").lower() == "true":
        # Start warm-up task
        asyncio.create_task(_warmup_models())

    yield

    # Shutdown
    logger.info("Factcheck addon shutting down...")
    if "http_client" in _api_clients and _api_clients["http_client"]:
        await _api_clients["http_client"].aclose()


app = FastAPI(
    title="Fact-Check Analysis Add-on (ML Enhanced)",
    description="Korean news article fact-checking with ML models and external API integration",
    version="2.0.0",
    lifespan=lifespan,
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ========== Prometheus Metrics ==========

if PROMETHEUS_AVAILABLE:
    # Request metrics
    REQUEST_COUNT = Counter(
        "factcheck_requests_total",
        "Total number of factcheck requests",
        ["endpoint", "status"],
    )
    REQUEST_LATENCY = Histogram(
        "factcheck_request_latency_seconds",
        "Request latency in seconds",
        ["endpoint"],
        buckets=(0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0, 30.0, 60.0),
    )

    # Analysis metrics
    ANALYSIS_COUNT = Counter(
        "factcheck_analysis_total",
        "Total number of analyses performed",
        ["mode", "verdict"],
    )
    CLAIMS_EXTRACTED = Counter(
        "factcheck_claims_extracted_total", "Total number of claims extracted"
    )

    # Model metrics
    MODEL_LOADED = Gauge(
        "factcheck_model_loaded", "Whether ML models are loaded (1=yes, 0=no)"
    )
    MODEL_WARMUP_COMPLETE = Gauge(
        "factcheck_model_warmup_complete",
        "Whether model warm-up is complete (1=yes, 0=no)",
    )

    # Error metrics
    ERROR_COUNT = Counter(
        "factcheck_errors_total", "Total number of errors", ["error_type"]
    )

    @app.get("/metrics")
    async def metrics():
        """Prometheus metrics endpoint"""
        # Update model status gauges
        models = get_ml_models() if not _model_loading else {}
        MODEL_LOADED.set(1 if models.get("loaded") else 0)
        MODEL_WARMUP_COMPLETE.set(1 if _model_warmup_complete else 0)

        return Response(content=generate_latest(), media_type=CONTENT_TYPE_LATEST)

# ========== Enums and Models ==========


class AnalysisMode(str, Enum):
    HEURISTIC = "heuristic"
    ML_BASIC = "ml_basic"
    ML_FULL = "ml_full"
    EXTERNAL_API = "external_api"
    HYBRID = "hybrid"


class ClaimVerdict(str, Enum):
    VERIFIED = "verified"
    FALSE = "false"
    UNVERIFIED = "unverified"
    MISLEADING = "misleading"
    PARTIALLY_TRUE = "partially_true"


class CredibilityGrade(str, Enum):
    A = "A"
    B = "B"
    C = "C"
    D = "D"
    F = "F"


class ArticleInput(BaseModel):
    id: Optional[int] = None
    title: Optional[str] = None
    content: Optional[str] = None
    url: Optional[str] = None
    source: Optional[str] = None
    published_at: Optional[str] = None


class AnalysisContext(BaseModel):
    language: Optional[str] = "ko"
    country: Optional[str] = "KR"
    previous_results: Optional[Dict[str, Any]] = None


class ExecutionOptions(BaseModel):
    importance: Optional[str] = "batch"
    debug: Optional[bool] = False
    timeout_ms: Optional[int] = None
    analysis_mode: Optional[AnalysisMode] = AnalysisMode.HYBRID
    include_detailed_analytics: Optional[bool] = True


class AddonRequest(BaseModel):
    request_id: str
    addon_id: str
    task: str = "article_analysis"
    input_schema_version: str = "1.0"
    article: Optional[ArticleInput] = None
    context: Optional[AnalysisContext] = None
    options: Optional[ExecutionOptions] = None


# ========== Detailed Analytics Models ==========


class SourceAnalytics(BaseModel):
    source_name: Optional[str] = None
    is_trusted: bool = False
    trust_score: float = 0.0
    trust_level: str = "unknown"  # trusted, unknown, untrusted
    matched_trusted_source: Optional[str] = None
    reason: str = ""


class ClickbaitAnalytics(BaseModel):
    is_clickbait: bool = False
    score: float = 0.0
    detected_patterns: List[Dict[str, Any]] = []
    total_patterns_checked: int = 0


class MisinfoAnalytics(BaseModel):
    risk_score: float = 0.0
    risk_level: str = "low"  # low, medium, high
    detected_patterns: List[Dict[str, Any]] = []
    unverifiable_claim_count: int = 0


class ClaimAnalytics(BaseModel):
    claim_id: str
    claim_text: str
    verdict: str
    confidence: float
    ml_confidence: Optional[float] = None
    claim_indicator: Optional[str] = None
    analysis_method: str = "heuristic"
    entities: Optional[List[Dict[str, str]]] = None
    semantic_similarity_scores: Optional[List[Dict[str, float]]] = None
    supporting_factors: List[str] = []
    contradicting_factors: List[str] = []
    external_verification: Optional[Dict[str, Any]] = None


class ScoreBreakdown(BaseModel):
    source_weight: int = 30
    clickbait_weight: int = 20
    misinfo_weight: int = 20
    verification_weight: int = 30

    source_contribution: float = 0.0
    clickbait_contribution: float = 0.0
    misinfo_contribution: float = 0.0
    verification_contribution: float = 0.0

    total_score: float = 0.0
    grade: str = "C"


class DetailedAnalytics(BaseModel):
    """Detailed analytics for transparency"""

    source_analysis: SourceAnalytics
    clickbait_analysis: ClickbaitAnalytics
    misinfo_analysis: MisinfoAnalytics
    claim_analyses: List[ClaimAnalytics] = []
    score_breakdown: ScoreBreakdown

    analysis_mode: str = "heuristic"
    ml_models_used: List[str] = []
    external_apis_used: List[str] = []
    processing_time_ms: int = 0
    analyzed_at: str = ""


class ClaimResult(BaseModel):
    claim: str
    verdict: str
    confidence: float
    evidence: Optional[str] = None
    source_url: Optional[str] = None
    ml_analysis: Optional[Dict[str, Any]] = None


class FactCheckResult(BaseModel):
    overall_credibility: float
    credibility_grade: str
    verdict: str
    claims_analyzed: int
    verified_claims: int
    false_claims: int
    unverified_claims: int
    claims: Optional[List[ClaimResult]] = None
    risk_flags: Optional[List[str]] = None
    explanations: Optional[List[str]] = None
    detailed_analytics: Optional[DetailedAnalytics] = None


class AnalysisResults(BaseModel):
    factcheck: Optional[FactCheckResult] = None
    raw: Optional[Dict[str, Any]] = None


class ResponseMeta(BaseModel):
    model_version: str
    latency_ms: int
    processed_at: str
    ml_enabled: bool = False
    models_loaded: List[str] = []


class ErrorInfo(BaseModel):
    code: str
    message: str
    details: Optional[str] = None


class AddonResponse(BaseModel):
    request_id: str
    addon_id: str
    status: str
    output_schema_version: str = "1.0"
    results: Optional[AnalysisResults] = None
    error: Optional[ErrorInfo] = None
    meta: Optional[ResponseMeta] = None


# ========== Constants ==========

TRUSTED_SOURCES = {
    # Tier 1: Wire services, public broadcasting (95%)
    "연합뉴스": 0.95,
    "KBS": 0.90,
    "MBC": 0.85,
    "SBS": 0.85,
    "YTN": 0.85,
    "EBS": 0.85,
    # Tier 2: Major newspapers (80%)
    "조선일보": 0.80,
    "중앙일보": 0.80,
    "동아일보": 0.80,
    "한겨레": 0.80,
    "경향신문": 0.80,
    "한국일보": 0.80,
    # Tier 3: Business newspapers (80%)
    "매일경제": 0.80,
    "한국경제": 0.80,
    "서울경제": 0.75,
    "머니투데이": 0.75,
    "이데일리": 0.75,
    # Tier 4: Cable news (75-85%)
    "JTBC": 0.85,
    "TV조선": 0.75,
    "채널A": 0.75,
    "MBN": 0.75,
    # Tier 5: Online news (70-75%)
    "뉴시스": 0.75,
    "뉴스1": 0.75,
    "오마이뉴스": 0.70,
    "프레시안": 0.70,
}

CLICKBAIT_PATTERNS = [
    {"pattern": r"충격[!]*", "severity": "high", "label": "충격"},
    {"pattern": r"경악[!]*", "severity": "high", "label": "경악"},
    {"pattern": r"대박[!]*", "severity": "medium", "label": "대박"},
    {"pattern": r"헉[!]*", "severity": "low", "label": "헉"},
    {"pattern": r"알고\s*보니", "severity": "medium", "label": "알고보니"},
    {"pattern": r"결국[.]*$", "severity": "low", "label": "결국"},
    {"pattern": r"드디어[!]*", "severity": "low", "label": "드디어"},
    {"pattern": r"\.\.\.$", "severity": "low", "label": "..."},
    {"pattern": r"\?\?\?+", "severity": "medium", "label": "???"},
    {"pattern": r"!!!+", "severity": "medium", "label": "!!!"},
    {"pattern": r"속보[!:]*", "severity": "low", "label": "속보"},
    {"pattern": r"단독[!:]*", "severity": "low", "label": "단독"},
    {"pattern": r"긴급[!:]*", "severity": "medium", "label": "긴급"},
]

MISINFORMATION_PATTERNS = [
    {"pattern": r"정부가\s*숨기", "type": "conspiracy", "severity": "high"},
    {"pattern": r"언론이\s*보도하지\s*않는", "type": "conspiracy", "severity": "high"},
    {"pattern": r"비밀리에", "type": "conspiracy", "severity": "medium"},
    {"pattern": r"충격\s*진실", "type": "sensational", "severity": "high"},
    {"pattern": r"알려지지\s*않은\s*진실", "type": "conspiracy", "severity": "high"},
]

UNVERIFIABLE_PATTERNS = [
    {"pattern": r"최초", "type": "absolute", "severity": "low"},
    {"pattern": r"유일", "type": "absolute", "severity": "low"},
    {"pattern": r"최고", "type": "absolute", "severity": "low"},
    {"pattern": r"최대", "type": "absolute", "severity": "low"},
    {"pattern": r"100%", "type": "absolute", "severity": "medium"},
    {"pattern": r"모든\s*사람", "type": "universal", "severity": "medium"},
    {"pattern": r"아무도", "type": "universal", "severity": "medium"},
    {"pattern": r"절대", "type": "absolute", "severity": "medium"},
    {"pattern": r"반드시", "type": "absolute", "severity": "low"},
]

CLAIM_INDICATORS = [
    "라고 밝혔다",
    "라고 주장했다",
    "라고 전했다",
    "에 따르면",
    "것으로 알려졌다",
    "것으로 확인됐다",
    "것으로 보인다",
    "할 전망이다",
    "할 예정이다",
    "관계자는",
    "전문가는",
    "소식통에 따르면",
]

# Korean stopwords for keyword extraction
KOREAN_STOPWORDS = {
    "은",
    "는",
    "이",
    "가",
    "을",
    "를",
    "의",
    "에",
    "에서",
    "로",
    "으로",
    "와",
    "과",
    "도",
    "만",
    "부터",
    "까지",
    "에게",
    "한테",
    "께",
    "이다",
    "하다",
    "있다",
    "없다",
    "되다",
    "않다",
    "그",
    "저",
    "이것",
    "그것",
    "저것",
    "여기",
    "거기",
    "저기",
    "뭐",
    "어디",
    "언제",
    "어떻게",
    "왜",
    "누구",
    "아주",
    "매우",
    "정말",
    "너무",
    "조금",
    "약간",
    "그리고",
    "그러나",
    "하지만",
    "그래서",
    "때문에",
    "것",
    "수",
    "등",
    "들",
    "및",
    "더",
    "덜",
    "대해",
    "대한",
    "관련",
    "관한",
}

# English stopwords
ENGLISH_STOPWORDS = {
    "the",
    "a",
    "an",
    "and",
    "or",
    "but",
    "in",
    "on",
    "at",
    "to",
    "for",
    "of",
    "with",
    "by",
    "from",
    "is",
    "are",
    "was",
    "were",
    "be",
    "been",
    "being",
    "have",
    "has",
    "had",
    "do",
    "does",
    "did",
    "will",
    "would",
    "this",
    "that",
    "these",
    "those",
    "it",
    "its",
    "what",
    "which",
    "who",
}

# Intent patterns for factcheck
FACTCHECK_INTENT_PATTERNS = {
    "fact_claim": [
        "사실",
        "진짜",
        "실제로",
        "정말",
        "맞는",
        "틀린",
        "fact",
        "true",
        "false",
    ],
    "opinion": ["생각", "의견", "판단", "보인다", "것 같다", "추측"],
    "quote": ["라고", "밝혔다", "전했다", "말했다", "주장했다"],
    "data": ["수치", "통계", "퍼센트", "%", "건", "명", "원"],
}


class IntentAnalyzer:
    """
    Enhanced intent analyzer for factcheck.
    Extracts keywords, identifies primary claims, and generates search strategies.
    """

    def __init__(self):
        self.stopwords = KOREAN_STOPWORDS | ENGLISH_STOPWORDS

    def detect_language(self, text: str) -> str:
        """Detect language based on character composition."""
        if not text:
            return "ko"

        korean_count = len(re.findall(r"[가-힣]", text))
        english_count = len(re.findall(r"[a-zA-Z]", text))

        total = korean_count + english_count
        if total == 0:
            return "ko"

        return "ko" if korean_count / total > 0.3 else "en"

    def extract_keywords(self, text: str) -> List[str]:
        """Extract meaningful keywords from text."""
        if not text:
            return []

        # Tokenize
        words = re.findall(r"[\w가-힣]+", text.lower())

        # Filter stopwords and short words
        keywords = []
        for word in words:
            if word in self.stopwords:
                continue
            if len(word) < 2:
                continue
            if word.isdigit():
                continue
            keywords.append(word)

        # Deduplicate while preserving order
        seen = set()
        unique_keywords = []
        for kw in keywords:
            if kw not in seen:
                seen.add(kw)
                unique_keywords.append(kw)

        return unique_keywords[:10]

    def identify_primary_keyword(self, keywords: List[str], original_text: str) -> str:
        """Identify the most important keyword."""
        if not keywords:
            words = original_text.split()
            return words[0] if words else ""

        # Score-based selection
        scores = {}
        for keyword in keywords:
            score = 0.0

            # Length weight
            score += min(len(keyword) / 10.0, 1.0) * 0.3

            # Position weight
            pos = original_text.lower().find(keyword.lower())
            if pos >= 0:
                score += (1.0 - pos / len(original_text)) * 0.3

            # Entity patterns (Korean compound nouns)
            if re.match(r".*[가-힣]+(기업|회사|정책|사건|발표|결과)$", keyword):
                score += 0.3

            scores[keyword] = score

        return max(scores.items(), key=lambda x: x[1])[0] if scores else keywords[0]

    def detect_claim_intent(self, text: str) -> Dict[str, Any]:
        """Detect the intent type of a potential claim."""
        text_lower = text.lower()

        intent_scores = {intent: 0 for intent in FACTCHECK_INTENT_PATTERNS}

        for intent, patterns in FACTCHECK_INTENT_PATTERNS.items():
            for pattern in patterns:
                if pattern.lower() in text_lower:
                    intent_scores[intent] += 1

        # Determine primary intent
        max_intent = max(intent_scores.items(), key=lambda x: x[1])

        return {
            "primary_intent": max_intent[0] if max_intent[1] > 0 else "general",
            "intent_scores": intent_scores,
            "is_verifiable": intent_scores["fact_claim"] > 0
            or intent_scores["data"] > 0,
        }

    def generate_search_queries(self, claim: str) -> List[Dict[str, Any]]:
        """Generate multiple search queries for claim verification."""
        keywords = self.extract_keywords(claim)
        primary_keyword = self.identify_primary_keyword(keywords, claim)

        queries = []

        # Strategy 1: Full claim
        queries.append(
            {
                "query": claim[:100],
                "strategy": "full_query",
                "weight": 1.0,
                "description": "원본 주장으로 검색",
            }
        )

        # Strategy 2: All keywords
        if len(keywords) > 1:
            queries.append(
                {
                    "query": " ".join(keywords),
                    "strategy": "keywords_and",
                    "weight": 0.9,
                    "description": "모든 키워드로 검색",
                }
            )

        # Strategy 3: Primary keyword + fact check terms
        queries.append(
            {
                "query": f"{primary_keyword} 팩트체크",
                "strategy": "primary_factcheck",
                "weight": 0.85,
                "description": "주요 키워드 + 팩트체크",
            }
        )

        # Strategy 4: Primary keyword + verification
        queries.append(
            {
                "query": f"{primary_keyword} 사실 확인",
                "strategy": "primary_verify",
                "weight": 0.8,
                "description": "주요 키워드 + 사실 확인",
            }
        )

        # Strategy 5: Keywords OR (broader)
        if len(keywords) > 1:
            or_query = " OR ".join(keywords[:5])
            queries.append(
                {
                    "query": or_query,
                    "strategy": "keywords_or",
                    "weight": 0.7,
                    "description": "키워드 OR 검색 (넓은 검색)",
                }
            )

        return queries

    def analyze(self, text: str) -> Dict[str, Any]:
        """Full intent analysis of text."""
        language = self.detect_language(text)
        keywords = self.extract_keywords(text)
        primary_keyword = self.identify_primary_keyword(keywords, text)
        claim_intent = self.detect_claim_intent(text)
        search_queries = self.generate_search_queries(text)

        return {
            "original_text": text,
            "language": language,
            "keywords": keywords,
            "primary_keyword": primary_keyword,
            "intent": claim_intent,
            "search_queries": search_queries,
            "fallback_strategies": [q["query"] for q in search_queries],
        }


# Global intent analyzer instance
_intent_analyzer = IntentAnalyzer()


# ========== Analysis Functions ==========


def analyze_source(source: Optional[str]) -> SourceAnalytics:
    """Analyze source credibility"""
    if not source:
        return SourceAnalytics(
            source_name=None,
            is_trusted=False,
            trust_score=0.3,
            trust_level="untrusted",
            reason="출처 정보가 제공되지 않았습니다.",
        )

    # Check against trusted sources
    for trusted_name, score in TRUSTED_SOURCES.items():
        if trusted_name in source:
            return SourceAnalytics(
                source_name=source,
                is_trusted=True,
                trust_score=score,
                trust_level="trusted",
                matched_trusted_source=trusted_name,
                reason=f"{trusted_name}은(는) 신뢰할 수 있는 언론사입니다.",
            )

    return SourceAnalytics(
        source_name=source,
        is_trusted=False,
        trust_score=0.5,
        trust_level="unknown",
        reason="신뢰 매체 목록에 없는 출처입니다. 추가 확인이 필요합니다.",
    )


def analyze_clickbait(title: Optional[str]) -> ClickbaitAnalytics:
    """Detect clickbait patterns in title"""
    if not title:
        return ClickbaitAnalytics(
            is_clickbait=False,
            score=0.0,
            detected_patterns=[],
            total_patterns_checked=len(CLICKBAIT_PATTERNS),
        )

    detected = []
    for pattern_info in CLICKBAIT_PATTERNS:
        matches = re.findall(pattern_info["pattern"], title, re.IGNORECASE)
        if matches:
            detected.append(
                {
                    "pattern": pattern_info["label"],
                    "matched_text": matches[0],
                    "severity": pattern_info["severity"],
                }
            )

    # Calculate score based on severity
    severity_weights = {"low": 0.1, "medium": 0.2, "high": 0.3}
    score = sum(severity_weights.get(p["severity"], 0.1) for p in detected)
    score = min(score, 1.0)

    is_clickbait = score > 0.3 or any(p["severity"] == "high" for p in detected)

    return ClickbaitAnalytics(
        is_clickbait=is_clickbait,
        score=score,
        detected_patterns=detected,
        total_patterns_checked=len(CLICKBAIT_PATTERNS),
    )


def analyze_misinformation(text: str) -> MisinfoAnalytics:
    """Detect misinformation risk patterns"""
    if not text:
        return MisinfoAnalytics()

    detected = []

    # Check misinformation patterns
    for pattern_info in MISINFORMATION_PATTERNS:
        matches = re.findall(pattern_info["pattern"], text, re.IGNORECASE)
        if matches:
            detected.append(
                {
                    "pattern": pattern_info["pattern"],
                    "matched_text": matches[0],
                    "type": "misinformation",
                    "category": pattern_info["type"],
                    "severity": pattern_info["severity"],
                }
            )

    # Check unverifiable patterns
    unverifiable_count = 0
    for pattern_info in UNVERIFIABLE_PATTERNS:
        matches = re.findall(pattern_info["pattern"], text, re.IGNORECASE)
        if matches:
            unverifiable_count += len(matches)
            detected.append(
                {
                    "pattern": pattern_info["pattern"],
                    "matched_text": matches[0],
                    "type": "unverifiable",
                    "category": pattern_info["type"],
                    "severity": pattern_info["severity"],
                }
            )

    # Calculate risk score
    severity_weights = {"low": 0.1, "medium": 0.2, "high": 0.35}
    misinfo_score = sum(
        severity_weights.get(p["severity"], 0.1)
        for p in detected
        if p["type"] == "misinformation"
    )
    unverifiable_score = sum(
        severity_weights.get(p["severity"], 0.1) * 0.5
        for p in detected
        if p["type"] == "unverifiable"
    )

    total_score = min(misinfo_score + unverifiable_score, 1.0)

    risk_level = (
        "high" if total_score > 0.5 else "medium" if total_score > 0.2 else "low"
    )

    return MisinfoAnalytics(
        risk_score=total_score,
        risk_level=risk_level,
        detected_patterns=detected,
        unverifiable_claim_count=unverifiable_count,
    )


def extract_claims_heuristic(text: str) -> List[Tuple[str, str, Dict[str, Any]]]:
    """
    Extract claims using heuristic patterns with intent analysis.

    Returns:
        List of tuples: (claim_text, indicator, intent_analysis)
    """
    claims = []
    sentences = re.split(r"[.!?]\s+", text)

    for sentence in sentences:
        sentence = sentence.strip()
        if len(sentence) < 10:
            continue

        for indicator in CLAIM_INDICATORS:
            pattern = indicator.replace("~", ".*")
            if re.search(pattern, sentence):
                # Perform intent analysis on the claim
                intent_analysis = _intent_analyzer.analyze(sentence)
                claims.append((sentence, indicator, intent_analysis))
                break

    # Sort claims by verifiability score (prioritize verifiable claims)
    claims_with_score = []
    for claim_text, indicator, intent_analysis in claims:
        score = 0.0
        # Boost verifiable claims
        if intent_analysis["intent"]["is_verifiable"]:
            score += 0.5
        # Boost claims with data
        if intent_analysis["intent"]["intent_scores"].get("data", 0) > 0:
            score += 0.3
        # Boost claims with fact indicators
        if intent_analysis["intent"]["intent_scores"].get("fact_claim", 0) > 0:
            score += 0.2
        claims_with_score.append((claim_text, indicator, intent_analysis, score))

    # Sort by score descending
    claims_with_score.sort(key=lambda x: x[3], reverse=True)

    return [(c[0], c[1], c[2]) for c in claims_with_score[:10]]  # Max 10 claims


async def extract_claims_ml(text: str) -> List[Tuple[str, str, float, Dict[str, Any]]]:
    """
    Extract and classify claims using ML model with intent analysis.

    Returns:
        List of tuples: (claim_text, indicator, ml_confidence, intent_analysis)
    """
    models = get_ml_models()

    if not models.get("loaded") or "claim_model" not in models:
        # Fallback to heuristic
        heuristic_claims = extract_claims_heuristic(text)
        return [(c[0], c[1], 0.7, c[2]) for c in heuristic_claims]

    try:
        import torch

        tokenizer = models["claim_tokenizer"]
        model = models["claim_model"]
        device = models["device"]

        sentences = re.split(r"[.!?]\s+", text)
        claims = []

        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) < 10:
                continue

            # Tokenize
            inputs = tokenizer(
                sentence,
                return_tensors="pt",
                truncation=True,
                max_length=256,
                padding=True,
            ).to(device)

            # Predict
            with torch.no_grad():
                outputs = model(**inputs)
                probs = torch.softmax(outputs.logits, dim=-1)
                predicted_class = torch.argmax(probs, dim=-1).item()
                confidence = probs[0][predicted_class].item()

            # Class 0 = non-claim, 1 = claim, 2 = uncertain
            if predicted_class == 1 and confidence > 0.5:
                # Find matching indicator
                indicator = "ML 분류"
                for ind in CLAIM_INDICATORS:
                    pattern = ind.replace("~", ".*")
                    if re.search(pattern, sentence):
                        indicator = ind
                        break

                # Perform intent analysis
                intent_analysis = _intent_analyzer.analyze(sentence)
                claims.append((sentence, indicator, confidence, intent_analysis))

        # Sort by combined score (ML confidence + verifiability)
        def combined_score(claim_tuple):
            _, _, conf, intent = claim_tuple
            score = conf * 0.6  # ML confidence weight
            if intent["intent"]["is_verifiable"]:
                score += 0.25
            if intent["intent"]["intent_scores"].get("data", 0) > 0:
                score += 0.15
            return score

        claims.sort(key=combined_score, reverse=True)
        return claims[:10]

    except Exception as e:
        logger.error(f"ML claim extraction failed: {e}")
        heuristic_claims = extract_claims_heuristic(text)
        return [(c[0], c[1], 0.7, c[2]) for c in heuristic_claims]


def extract_entities_ml(text: str) -> List[Dict[str, str]]:
    """Extract named entities using NER model"""
    models = get_ml_models()

    if not models.get("loaded") or "ner_pipeline" not in models:
        return []

    try:
        ner = models["ner_pipeline"]
        entities = ner(text[:1024])  # Limit input length

        return [
            {
                "entity": e.get("entity_group", e.get("entity", "UNKNOWN")),
                "word": e.get("word", ""),
                "score": round(e.get("score", 0), 3),
            }
            for e in entities
            if e.get("score", 0) > 0.7
        ]
    except Exception as e:
        logger.error(f"NER extraction failed: {e}")
        return []


async def compute_semantic_similarity(
    claim: str, reference_texts: List[str]
) -> List[Dict[str, float]]:
    """Compute semantic similarity between claim and references"""
    models = get_ml_models()

    if not models.get("loaded") or "sentence_transformer" not in models:
        return []

    try:
        st_model = models["sentence_transformer"]

        # Encode claim
        claim_embedding = st_model.encode([claim])[0]

        # Encode references
        ref_embeddings = st_model.encode(reference_texts)

        # Compute cosine similarities
        from numpy import dot
        from numpy.linalg import norm

        similarities = []
        for i, ref_emb in enumerate(ref_embeddings):
            sim = dot(claim_embedding, ref_emb) / (
                norm(claim_embedding) * norm(ref_emb)
            )
            similarities.append(
                {"reference_index": i, "similarity": round(float(sim), 3)}
            )

        return sorted(similarities, key=lambda x: x["similarity"], reverse=True)

    except Exception as e:
        logger.error(f"Semantic similarity computation failed: {e}")
        return []


async def verify_with_external_api(claim: str) -> Optional[Dict[str, Any]]:
    """Verify claim using external fact-check APIs"""
    client = await get_http_client()

    if not client:
        return None

    results = {}

    # Google Fact Check API
    google_api_key = os.getenv("GOOGLE_FACTCHECK_API_KEY")
    if google_api_key:
        try:
            response = await client.get(
                "https://factchecktools.googleapis.com/v1alpha1/claims:search",
                params={
                    "query": claim[:200],
                    "key": google_api_key,
                    "languageCode": "ko",
                },
            )
            if response.status_code == 200:
                data = response.json()
                if data.get("claims"):
                    results["google_factcheck"] = {
                        "found": True,
                        "claims": data["claims"][:3],
                    }
        except Exception as e:
            logger.warning(f"Google Fact Check API error: {e}")

    # SNU Factcheck (if available)
    snu_api_url = os.getenv("SNU_FACTCHECK_API_URL")
    if snu_api_url:
        try:
            response = await client.post(snu_api_url, json={"query": claim[:200]})
            if response.status_code == 200:
                results["snu_factcheck"] = response.json()
        except Exception as e:
            logger.warning(f"SNU Factcheck API error: {e}")

    return results if results else None


def compute_keyword_similarity(claim_keywords: List[str], reference_text: str) -> float:
    """
    Compute keyword-based similarity between claim keywords and reference text.
    Uses Jaccard similarity on extracted keywords.
    """
    if not claim_keywords or not reference_text:
        return 0.0

    # Extract keywords from reference
    ref_keywords = set(_intent_analyzer.extract_keywords(reference_text))
    claim_kw_set = set(claim_keywords)

    if not ref_keywords or not claim_kw_set:
        return 0.0

    # Jaccard similarity
    intersection = len(claim_kw_set & ref_keywords)
    union = len(claim_kw_set | ref_keywords)

    return intersection / union if union > 0 else 0.0


def analyze_claim(
    claim_text: str,
    claim_indicator: str,
    ml_confidence: Optional[float] = None,
    entities: Optional[List[Dict[str, str]]] = None,
    external_verification: Optional[Dict[str, Any]] = None,
    intent_analysis: Optional[Dict[str, Any]] = None,
) -> ClaimAnalytics:
    """
    Analyze a single claim with enhanced intent analysis.

    Args:
        claim_text: The claim text to analyze
        claim_indicator: The indicator that identified this as a claim
        ml_confidence: ML model confidence if available
        entities: Named entities extracted from context
        external_verification: External API verification results
        intent_analysis: Intent analysis from IntentAnalyzer
    """

    # Generate claim ID
    claim_id = hashlib.md5(claim_text.encode()).hexdigest()[:8]

    # Determine verdict based on available information
    verdict = ClaimVerdict.UNVERIFIED.value
    confidence = 0.5
    supporting = []
    contradicting = []
    analysis_method = "heuristic"

    # Use intent analysis for enhanced verification
    if intent_analysis:
        keywords = intent_analysis.get("keywords", [])
        primary_keyword = intent_analysis.get("primary_keyword", "")
        intent_info = intent_analysis.get("intent", {})

        # Boost confidence for verifiable claims
        if intent_info.get("is_verifiable", False):
            confidence += 0.1
            supporting.append("검증 가능한 팩트성 주장")

        # Boost for data-driven claims
        if intent_info.get("intent_scores", {}).get("data", 0) > 0:
            confidence += 0.1
            supporting.append("데이터/수치 포함")

        # Note opinion-based claims
        if intent_info.get("intent_scores", {}).get("opinion", 0) > 0:
            confidence -= 0.1
            contradicting.append("의견성 표현 포함")

        # Store search strategies for potential follow-up verification
        search_queries = intent_analysis.get("search_queries", [])
        if search_queries:
            analysis_method = "heuristic_with_intent"

    # If ML confidence available
    if ml_confidence is not None:
        analysis_method = (
            "ml_classification" if not intent_analysis else "ml_with_intent"
        )
        confidence = (
            (confidence + ml_confidence) / 2 if intent_analysis else ml_confidence
        )

    # If external verification available
    if external_verification:
        analysis_method = (
            "external_api" if not intent_analysis else "external_with_intent"
        )

        # Check Google Fact Check results
        if "google_factcheck" in external_verification:
            gfc = external_verification["google_factcheck"]
            if gfc.get("found"):
                claims_data = gfc.get("claims", [])
                for c in claims_data:
                    review = c.get("claimReview", [{}])[0]
                    rating = review.get("textualRating", "").lower()

                    if any(x in rating for x in ["true", "correct", "accurate"]):
                        verdict = ClaimVerdict.VERIFIED.value
                        supporting.append(f"Google Fact Check: {rating}")
                    elif any(x in rating for x in ["false", "incorrect", "wrong"]):
                        verdict = ClaimVerdict.FALSE.value
                        contradicting.append(f"Google Fact Check: {rating}")
                    elif any(x in rating for x in ["misleading", "partial"]):
                        verdict = ClaimVerdict.MISLEADING.value
                        contradicting.append(f"Google Fact Check: {rating}")

                    # Enhance with keyword similarity check
                    if intent_analysis:
                        claim_text_from_api = c.get("text", "")
                        keywords = intent_analysis.get("keywords", [])
                        similarity = compute_keyword_similarity(
                            keywords, claim_text_from_api
                        )
                        if similarity > 0.5:
                            supporting.append(f"키워드 유사도: {similarity:.0%}")

    # If no external verification available, mark as UNVERIFIED with appropriate context
    # IMPORTANT: We do NOT use pseudo-random verdicts as they mislead users
    if verdict == ClaimVerdict.UNVERIFIED.value:
        # Use intent analysis to provide better context
        if intent_analysis and intent_analysis.get("intent", {}).get("is_verifiable"):
            # This is a verifiable claim but we couldn't verify it externally
            # Keep it as UNVERIFIED with low confidence
            verdict = ClaimVerdict.UNVERIFIED.value
            confidence = 0.35  # Low confidence since no external verification
            analysis_method = "needs_external_verification"
            supporting.append("검증 가능한 주장이나 외부 검증 소스를 찾지 못했습니다")
        else:
            # Non-verifiable claims (opinions, subjective statements, etc.)
            verdict = ClaimVerdict.UNVERIFIED.value
            confidence = 0.25  # Very low confidence for non-verifiable claims
            analysis_method = "opinion_or_subjective"
            supporting.append("의견 또는 주관적 주장으로 사실 검증이 어렵습니다")

        # If ML model provided some insight, use that to adjust confidence slightly
        if ml_confidence and ml_confidence > 0.5:
            confidence = min(
                0.5, ml_confidence * 0.6
            )  # Cap at 0.5 for ML-only analysis
            analysis_method = "ml_analysis_only"

    # Clamp confidence
    confidence = max(0.0, min(1.0, confidence))

    return ClaimAnalytics(
        claim_id=claim_id,
        claim_text=claim_text[:200] + "..." if len(claim_text) > 200 else claim_text,
        verdict=verdict,
        confidence=round(confidence, 2),
        ml_confidence=ml_confidence,
        claim_indicator=claim_indicator,
        analysis_method=analysis_method,
        entities=entities,
        supporting_factors=supporting,
        contradicting_factors=contradicting,
        external_verification=external_verification,
    )


def calculate_score_breakdown(
    source_analysis: SourceAnalytics,
    clickbait_analysis: ClickbaitAnalytics,
    misinfo_analysis: MisinfoAnalytics,
    claim_analyses: List[ClaimAnalytics],
) -> ScoreBreakdown:
    """Calculate detailed score breakdown"""

    # Weights
    source_weight = 30
    clickbait_weight = 20
    misinfo_weight = 20
    verification_weight = 30

    # Calculate contributions
    source_contribution = source_analysis.trust_score * source_weight

    clickbait_score = 0.7 if clickbait_analysis.is_clickbait else 1.0
    clickbait_contribution = clickbait_score * clickbait_weight

    misinfo_score = 1 - misinfo_analysis.risk_score
    misinfo_contribution = misinfo_score * misinfo_weight

    verified_count = sum(
        1 for c in claim_analyses if c.verdict == ClaimVerdict.VERIFIED.value
    )
    verification_ratio = verified_count / len(claim_analyses) if claim_analyses else 0.5
    verification_contribution = verification_ratio * verification_weight

    total_score = (
        source_contribution
        + clickbait_contribution
        + misinfo_contribution
        + verification_contribution
    )

    # Grade
    if total_score >= 80:
        grade = CredibilityGrade.A.value
    elif total_score >= 60:
        grade = CredibilityGrade.B.value
    elif total_score >= 40:
        grade = CredibilityGrade.C.value
    elif total_score >= 20:
        grade = CredibilityGrade.D.value
    else:
        grade = CredibilityGrade.F.value

    return ScoreBreakdown(
        source_weight=source_weight,
        clickbait_weight=clickbait_weight,
        misinfo_weight=misinfo_weight,
        verification_weight=verification_weight,
        source_contribution=round(source_contribution, 1),
        clickbait_contribution=round(clickbait_contribution, 1),
        misinfo_contribution=round(misinfo_contribution, 1),
        verification_contribution=round(verification_contribution, 1),
        total_score=round(total_score, 1),
        grade=grade,
    )


async def perform_factcheck(
    article: ArticleInput, options: Optional[ExecutionOptions] = None
) -> FactCheckResult:
    """Perform comprehensive fact-checking analysis"""

    start_time = time.time()

    # Get options
    opts = options or ExecutionOptions()
    analysis_mode = opts.analysis_mode or AnalysisMode.HYBRID
    include_analytics = opts.include_detailed_analytics

    # Combine text
    text = ""
    if article.title:
        text += article.title + " "
    if article.content:
        text += article.content

    if not text.strip():
        return FactCheckResult(
            overall_credibility=0.0,
            credibility_grade=CredibilityGrade.F.value,
            verdict="unverified",
            claims_analyzed=0,
            verified_claims=0,
            false_claims=0,
            unverified_claims=0,
            explanations=["분석할 콘텐츠가 없습니다."],
        )

    models_used = []
    apis_used = []

    # 1. Source Analysis
    source_analysis = analyze_source(article.source)

    # 2. Clickbait Detection
    clickbait_analysis = analyze_clickbait(article.title)

    # 3. Misinformation Risk
    misinfo_analysis = analyze_misinformation(text)

    # 4. Claim Extraction & Analysis
    claim_analyses = []

    if analysis_mode in [
        AnalysisMode.ML_BASIC,
        AnalysisMode.ML_FULL,
        AnalysisMode.HYBRID,
    ]:
        # Use ML for claim extraction
        claims_with_conf = await extract_claims_ml(text)
        models_used.append("koelectra-claim-classifier")

        # Extract entities if ML_FULL
        entities = None
        if analysis_mode == AnalysisMode.ML_FULL:
            entities = extract_entities_ml(text)
            if entities:
                models_used.append("klue-ner")

        for claim_text, indicator, confidence, intent_analysis in claims_with_conf:
            external_verification = None

            # External API verification for ML_FULL or HYBRID
            if analysis_mode in [AnalysisMode.ML_FULL, AnalysisMode.HYBRID]:
                external_verification = await verify_with_external_api(claim_text)
                if external_verification:
                    apis_used.extend(external_verification.keys())

            claim_analytics = analyze_claim(
                claim_text=claim_text,
                claim_indicator=indicator,
                ml_confidence=confidence,
                entities=entities,
                external_verification=external_verification,
                intent_analysis=intent_analysis,
            )
            claim_analyses.append(claim_analytics)
    else:
        # Heuristic only
        claims = extract_claims_heuristic(text)
        for claim_text, indicator, intent_analysis in claims:
            claim_analytics = analyze_claim(
                claim_text=claim_text,
                claim_indicator=indicator,
                intent_analysis=intent_analysis,
            )
            claim_analyses.append(claim_analytics)

    # 5. Calculate Score Breakdown
    score_breakdown = calculate_score_breakdown(
        source_analysis, clickbait_analysis, misinfo_analysis, claim_analyses
    )

    # 6. Build results
    verified_count = sum(
        1 for c in claim_analyses if c.verdict == ClaimVerdict.VERIFIED.value
    )
    false_count = sum(
        1
        for c in claim_analyses
        if c.verdict in [ClaimVerdict.FALSE.value, ClaimVerdict.MISLEADING.value]
    )
    unverified_count = sum(
        1 for c in claim_analyses if c.verdict == ClaimVerdict.UNVERIFIED.value
    )

    # Final verdict
    if score_breakdown.total_score >= 70:
        verdict = "verified"
    elif score_breakdown.total_score >= 40:
        verdict = "suspicious"
    else:
        verdict = "unverified"

    # Risk flags
    risk_flags = []
    if clickbait_analysis.is_clickbait:
        risk_flags.append("낚시성 제목 의심")
    if source_analysis.trust_score < 0.5:
        risk_flags.append("출처 신뢰도 낮음")
    if misinfo_analysis.risk_level in ["medium", "high"]:
        risk_flags.append(f"허위정보 위험도: {misinfo_analysis.risk_level}")

    # Explanations
    explanations = [
        f"출처 신뢰도: {source_analysis.trust_score * 100:.0f}%",
        f"분석된 주장: {len(claim_analyses)}개",
    ]
    if verified_count > 0:
        explanations.append(f"검증된 주장: {verified_count}개")
    if false_count > 0:
        explanations.append(f"의심스러운 주장: {false_count}개")
    if clickbait_analysis.is_clickbait:
        patterns = [p["pattern"] for p in clickbait_analysis.detected_patterns]
        explanations.append(f"낚시성 패턴: {', '.join(patterns)}")

    # Build detailed analytics if requested
    detailed_analytics = None
    if include_analytics:
        processing_time = int((time.time() - start_time) * 1000)
        detailed_analytics = DetailedAnalytics(
            source_analysis=source_analysis,
            clickbait_analysis=clickbait_analysis,
            misinfo_analysis=misinfo_analysis,
            claim_analyses=claim_analyses,
            score_breakdown=score_breakdown,
            analysis_mode=analysis_mode.value,
            ml_models_used=list(set(models_used)),
            external_apis_used=list(set(apis_used)),
            processing_time_ms=processing_time,
            analyzed_at=time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        )

    # Build claim results
    claim_results = [
        ClaimResult(
            claim=c.claim_text,
            verdict=c.verdict,
            confidence=c.confidence,
            evidence=c.analysis_method,
            ml_analysis={
                "ml_confidence": c.ml_confidence,
                "entities": c.entities,
                "supporting": c.supporting_factors,
                "contradicting": c.contradicting_factors,
            }
            if c.ml_confidence
            else None,
        )
        for c in claim_analyses
    ]

    return FactCheckResult(
        overall_credibility=score_breakdown.total_score,
        credibility_grade=score_breakdown.grade,
        verdict=verdict,
        claims_analyzed=len(claim_analyses),
        verified_claims=verified_count,
        false_claims=false_count,
        unverified_claims=unverified_count,
        claims=claim_results if claim_results else None,
        risk_flags=risk_flags if risk_flags else None,
        explanations=explanations,
        detailed_analytics=detailed_analytics,
    )


# ========== API Endpoints ==========


@app.get("/health")
async def health_check():
    """Health check endpoint with ML status"""
    models = get_ml_models()

    return {
        "status": "healthy",
        "service": "factcheck-addon",
        "version": "2.0.0",
        "ml_enabled": models.get("loaded", False),
        "warmup_complete": _model_warmup_complete,
        "warmup_error": _model_warmup_error,
        "models_loaded": [
            k
            for k in models.keys()
            if k not in ["loaded", "device"] and models.get(k) is not None
        ],
        "device": models.get("device", "cpu"),
    }


@app.get("/ready")
async def readiness_check():
    """Readiness check - only returns healthy when models are warmed up"""
    if not _model_warmup_complete:
        return {"status": "warming_up", "ready": False}

    models = get_ml_models()
    return {
        "status": "ready",
        "ready": True,
        "ml_enabled": models.get("loaded", False),
        "warmup_error": _model_warmup_error,
    }


@app.post("/analyze", response_model=AddonResponse)
async def analyze(request: AddonRequest):
    """
    Main analysis endpoint with ML-enhanced fact-checking.
    """
    start_time = time.time()

    try:
        if not request.article:
            raise ValueError("article is required")

        # Perform fact-check
        factcheck_result = await perform_factcheck(request.article, request.options)

        # Response metadata
        latency_ms = int((time.time() - start_time) * 1000)
        models = get_ml_models()

        # Track Prometheus metrics
        if PROMETHEUS_AVAILABLE:
            REQUEST_COUNT.labels(endpoint="analyze", status="success").inc()
            REQUEST_LATENCY.labels(endpoint="analyze").observe(time.time() - start_time)
            ANALYSIS_COUNT.labels(
                mode=request.options.analysis_mode.value
                if request.options and request.options.analysis_mode
                else "hybrid",
                verdict=factcheck_result.overall_verdict
                if factcheck_result
                else "unknown",
            ).inc()
            if factcheck_result and factcheck_result.claims:
                CLAIMS_EXTRACTED.inc(len(factcheck_result.claims))

        return AddonResponse(
            request_id=request.request_id,
            addon_id=request.addon_id,
            status="success",
            results=AnalysisResults(factcheck=factcheck_result),
            meta=ResponseMeta(
                model_version="factcheck-ko-ml-v2.0",
                latency_ms=latency_ms,
                processed_at=time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
                ml_enabled=models.get("loaded", False),
                models_loaded=[
                    k
                    for k in models.keys()
                    if k not in ["loaded", "device"] and models.get(k) is not None
                ],
            ),
        )

    except Exception as e:
        logger.error(f"Analysis error: {e}")
        latency_ms = int((time.time() - start_time) * 1000)

        # Track error metrics
        if PROMETHEUS_AVAILABLE:
            REQUEST_COUNT.labels(endpoint="analyze", status="error").inc()
            REQUEST_LATENCY.labels(endpoint="analyze").observe(time.time() - start_time)
            ERROR_COUNT.labels(error_type=type(e).__name__).inc()

        return AddonResponse(
            request_id=request.request_id,
            addon_id=request.addon_id,
            status="error",
            error=ErrorInfo(code="FACTCHECK_ERROR", message=str(e)),
            meta=ResponseMeta(
                model_version="factcheck-ko-ml-v2.0",
                latency_ms=latency_ms,
                processed_at=time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
            ),
        )


@app.post("/batch")
async def analyze_batch(requests: List[AddonRequest]):
    """Batch analysis endpoint"""
    results = []
    for req in requests:
        result = await analyze(req)
        results.append(result)
    return results


@app.get("/models")
async def list_models():
    """List available ML models and their status"""
    models = get_ml_models()

    return {
        "loaded": models.get("loaded", False),
        "device": models.get("device", "cpu"),
        "models": {
            "claim_classifier": {
                "loaded": "claim_model" in models,
                "name": os.getenv(
                    "CLAIM_MODEL", "monologg/koelectra-base-v3-discriminator"
                ),
            },
            "sentence_transformer": {
                "loaded": "sentence_transformer" in models,
                "name": os.getenv(
                    "EMBEDDING_MODEL",
                    "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                ),
            },
            "ner": {
                "loaded": "ner_pipeline" in models,
                "name": os.getenv("NER_MODEL", "klue/bert-base"),
            },
            "sentiment": {
                "loaded": "sentiment_pipeline" in models,
                "name": os.getenv("SENTIMENT_MODEL", "klue/roberta-base"),
            },
        },
    }


@app.post("/reload-models")
async def reload_models(background_tasks: BackgroundTasks):
    """Reload ML models"""
    global _ml_models
    _ml_models = {}

    background_tasks.add_task(get_ml_models)

    return {"message": "Model reload initiated"}


# ========== Entry Point ==========

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=int(os.getenv("PORT", 8101)))

```

---

## backend/ml-addons/ml-trainer/main.py

```py
"""
ML Training Service - Production Version

FastAPI-based ML training service that supports multiple model types:
- Sentiment Analysis (BERT-based)
- ABSA (Aspect-Based Sentiment Analysis)
- NER (Named Entity Recognition)
- Text Classification
- Embedding Models

This service performs REAL training using HuggingFace Transformers.
"""

import os
import uuid
import json
import asyncio
import hashlib
import threading
import secrets
import shutil
import zipfile
from datetime import datetime
from pathlib import Path
from typing import Optional, AsyncGenerator
from enum import Enum
from dataclasses import dataclass, field, asdict
from contextlib import asynccontextmanager
from collections import deque

import structlog
from fastapi import FastAPI, HTTPException, BackgroundTasks, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field

# Configure structlog
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer(),
    ],
    wrapper_class=structlog.stdlib.BoundLogger,
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    cache_logger_on_first_use=True,
)

log = structlog.get_logger()

# Environment configuration
MODEL_DIR = Path(os.getenv("MODEL_DIR", "/app/models"))
DATASET_DIR = Path(os.getenv("DATASET_DIR", "/app/datasets"))
LOG_DIR = Path(os.getenv("LOG_DIR", "/app/logs"))
USE_GPU = os.getenv("USE_GPU", "true").lower() == "true"
MAX_CONCURRENT_JOBS = int(os.getenv("MAX_CONCURRENT_JOBS", "2"))

# Ensure directories exist
MODEL_DIR.mkdir(parents=True, exist_ok=True)
DATASET_DIR.mkdir(parents=True, exist_ok=True)
LOG_DIR.mkdir(parents=True, exist_ok=True)


# =============================================================================
# Enums and Data Classes
# =============================================================================


class JobState(str, Enum):
    PENDING = "PENDING"
    INITIALIZING = "INITIALIZING"
    RUNNING = "RUNNING"
    COMPLETED = "COMPLETED"
    FAILED = "FAILED"
    CANCELLED = "CANCELLED"


class ModelType(str, Enum):
    SENTIMENT = "sentiment"
    ABSA = "absa"
    NER = "ner"
    CLASSIFICATION = "classification"
    EMBEDDING = "embedding"
    TRANSFORMER = "transformer"


@dataclass
class TrainingMetrics:
    epoch: int = 0
    total_epochs: int = 0
    step: int = 0
    total_steps: int = 0
    loss: float = 0.0
    accuracy: float = 0.0
    validation_loss: float = 0.0
    validation_accuracy: float = 0.0
    learning_rate: float = 0.0
    samples_processed: int = 0
    total_samples: int = 0
    f1_score: float = 0.0
    precision: float = 0.0
    recall: float = 0.0


@dataclass
class TrainingJob:
    job_id: str
    model_name: str
    model_type: ModelType
    dataset_path: str
    dataset_format: str
    base_model: Optional[str]
    max_epochs: int
    validation_split: float
    hyperparameters: dict = field(default_factory=dict)
    callbacks: dict = field(default_factory=dict)
    metadata: dict = field(default_factory=dict)
    state: JobState = JobState.PENDING
    progress: float = 0.0
    metrics: TrainingMetrics = field(default_factory=TrainingMetrics)
    error_message: Optional[str] = None
    model_path: Optional[str] = None
    created_at: str = field(default_factory=lambda: datetime.utcnow().isoformat())
    started_at: Optional[str] = None
    completed_at: Optional[str] = None
    # For SSE streaming
    event_queue: deque = field(default_factory=lambda: deque(maxlen=100))
    # External training support (Colab/Jupyter)
    is_external: bool = False
    upload_token: Optional[str] = None
    external_worker_info: dict = field(default_factory=dict)


# In-memory job storage (backed by Redis via state_store)
# This dict is a cache, actual persistence is handled by StateStore
jobs: dict[str, TrainingJob] = {}
# Lock for thread-safe operations
jobs_lock = threading.Lock()

# State store instance (initialized in lifespan)
state_store = None


# =============================================================================
# Pydantic Models for API
# =============================================================================


class TrainingRequest(BaseModel):
    model_name: str = Field(..., description="Name of the model to train")
    model_type: str = Field(
        ..., description="Type of model (sentiment, absa, ner, etc.)"
    )
    dataset_path: str = Field(..., description="Path to training dataset")
    dataset_format: str = Field(
        default="csv", description="Format of dataset (csv, jsonl, parquet)"
    )
    base_model: Optional[str] = Field(
        default=None, description="Base model for fine-tuning"
    )
    max_epochs: int = Field(
        default=3, ge=1, le=100, description="Maximum training epochs"
    )
    validation_split: float = Field(
        default=0.1, ge=0.0, le=0.5, description="Validation data split ratio"
    )
    hyperparameters: dict = Field(
        default_factory=dict, description="Training hyperparameters"
    )
    callbacks: dict = Field(default_factory=dict, description="Callback configurations")
    metadata: dict = Field(default_factory=dict, description="Additional metadata")


class TrainingResponse(BaseModel):
    job_id: str
    model_name: str
    model_type: str
    state: str
    progress: float
    created_at: str
    message: str


class JobStatusResponse(BaseModel):
    job_id: str
    model_name: str
    model_type: str
    state: str
    progress: float
    metrics: dict
    error_message: Optional[str]
    model_path: Optional[str]
    created_at: str
    started_at: Optional[str]
    completed_at: Optional[str]
    current_epoch: int = 0
    total_epochs: int = 0


class ModelArtifactResponse(BaseModel):
    model_path: str
    model_name: str
    model_type: str
    framework: str
    version: str
    size_bytes: int
    checksum: str
    metrics: dict
    model_filename: str


class HealthResponse(BaseModel):
    status: str
    version: str
    gpu_available: bool
    active_jobs: int
    supported_model_types: list[str]
    max_concurrent_jobs: int
    redis_connected: bool = False
    persisted_jobs: int = 0


# =============================================================================
# External Training (Colab/Jupyter) Request/Response Models
# =============================================================================


class ExternalTrainingStartRequest(BaseModel):
    """Request to start an external training session from Colab/Jupyter."""

    model_name: str = Field(..., description="Name for the trained model")
    model_type: str = Field(
        default="sentiment", description="Type of model being trained"
    )
    base_model: Optional[str] = Field(
        default=None, description="Base model being fine-tuned (optional)"
    )
    max_epochs: int = Field(default=10, ge=1, le=1000, description="Expected epochs")
    metadata: dict = Field(
        default_factory=dict,
        description="Additional metadata (e.g., Colab notebook URL)",
    )


class ExternalTrainingStartResponse(BaseModel):
    """Response with job_id and upload_token for external training."""

    job_id: str
    upload_token: str
    model_name: str
    model_type: str
    state: str
    created_at: str
    message: str
    api_endpoints: dict = Field(
        default_factory=dict, description="API endpoints for progress/upload/complete"
    )


class ExternalProgressRequest(BaseModel):
    """Request to report training progress from external worker."""

    upload_token: str = Field(
        ..., description="Token received from /train/external/start"
    )
    progress: float = Field(
        default=0.0, ge=0.0, le=100.0, description="Progress percentage (0-100)"
    )
    epoch: int = Field(default=0, ge=0, description="Current epoch")
    total_epochs: int = Field(default=0, ge=0, description="Total epochs")
    step: int = Field(default=0, ge=0, description="Current step")
    total_steps: int = Field(default=0, ge=0, description="Total steps")
    loss: float = Field(default=0.0, description="Current loss value")
    accuracy: float = Field(default=0.0, ge=0.0, le=1.0, description="Current accuracy")
    validation_loss: float = Field(default=0.0, description="Validation loss")
    validation_accuracy: float = Field(
        default=0.0, ge=0.0, le=1.0, description="Validation accuracy"
    )
    f1_score: float = Field(default=0.0, ge=0.0, le=1.0, description="F1 score")
    learning_rate: float = Field(default=0.0, description="Current learning rate")
    message: Optional[str] = Field(default=None, description="Optional status message")


class ExternalCompleteRequest(BaseModel):
    """Request to mark external training as complete."""

    upload_token: str = Field(
        ..., description="Token received from /train/external/start"
    )
    final_metrics: dict = Field(
        default_factory=dict, description="Final training metrics"
    )
    model_path: Optional[str] = Field(
        default=None, description="Path to model if already uploaded"
    )


# =============================================================================
# Training Progress Callback (Real HuggingFace Integration)
# =============================================================================


class TrainingProgressCallback:
    """Custom callback to track training progress for HuggingFace Trainer."""

    def __init__(self, job: TrainingJob):
        self.job = job

    def on_train_begin(self, args, state, control, **kwargs):
        self.job.metrics.total_steps = state.max_steps if state.max_steps else 0
        self.job.metrics.total_epochs = args.num_train_epochs
        self._emit_event(
            "training_started", {"total_steps": self.job.metrics.total_steps}
        )

    def on_step_end(self, args, state, control, **kwargs):
        self.job.metrics.step = state.global_step
        if state.max_steps:
            self.job.progress = (state.global_step / state.max_steps) * 100

        if state.log_history:
            latest = state.log_history[-1]
            self.job.metrics.loss = latest.get("loss", self.job.metrics.loss)
            self.job.metrics.learning_rate = latest.get(
                "learning_rate", self.job.metrics.learning_rate
            )

        self._emit_event(
            "step_completed",
            {
                "step": state.global_step,
                "progress": self.job.progress,
                "loss": self.job.metrics.loss,
            },
        )

    def on_epoch_end(self, args, state, control, **kwargs):
        self.job.metrics.epoch = int(state.epoch) if state.epoch else 0
        self._emit_event(
            "epoch_completed",
            {"epoch": self.job.metrics.epoch, "progress": self.job.progress},
        )

    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if metrics:
            self.job.metrics.validation_loss = metrics.get("eval_loss", 0.0)
            self.job.metrics.validation_accuracy = metrics.get("eval_accuracy", 0.0)
            self.job.metrics.f1_score = metrics.get("eval_f1", 0.0)
            self._emit_event(
                "evaluation_completed",
                {
                    "eval_loss": self.job.metrics.validation_loss,
                    "eval_accuracy": self.job.metrics.validation_accuracy,
                },
            )

    def on_train_end(self, args, state, control, **kwargs):
        self._emit_event("training_ended", {"final_step": state.global_step})

    def _emit_event(self, event_type: str, data: dict):
        """Emit event to job's event queue for SSE streaming."""
        event = {
            "type": event_type,
            "timestamp": datetime.utcnow().isoformat(),
            "job_id": self.job.job_id,
            "progress": self.job.progress,
            "state": self.job.state.value,
            "metrics": asdict(self.job.metrics),
            **data,
        }
        self.job.event_queue.append(event)


# =============================================================================
# Real Training Logic
# =============================================================================


def get_default_base_model(model_type: ModelType) -> str:
    """Returns the default base model for each model type."""
    defaults = {
        ModelType.SENTIMENT: "klue/bert-base",
        ModelType.ABSA: "monologg/koelectra-base-v3-discriminator",
        ModelType.NER: "klue/bert-base",
        ModelType.CLASSIFICATION: "klue/roberta-base",
        ModelType.EMBEDDING: "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        ModelType.TRANSFORMER: "klue/bert-base",
    }
    return defaults.get(model_type, "klue/bert-base")


def load_dataset_from_path(
    dataset_path: str, dataset_format: str, validation_split: float
):
    """Load dataset from file path with proper format handling."""
    from datasets import load_dataset, Dataset, DatasetDict
    import pandas as pd

    path = Path(dataset_path)
    if not path.exists():
        # Check in DATASET_DIR
        path = DATASET_DIR / dataset_path
        if not path.exists():
            raise FileNotFoundError(f"Dataset not found: {dataset_path}")

    log.info("Loading dataset", path=str(path), format=dataset_format)

    if dataset_format == "csv":
        df = pd.read_csv(path)
    elif dataset_format == "jsonl":
        df = pd.read_json(path, lines=True)
    elif dataset_format == "json":
        df = pd.read_json(path)
    elif dataset_format == "parquet":
        df = pd.read_parquet(path)
    else:
        raise ValueError(f"Unsupported dataset format: {dataset_format}")

    # Convert to HuggingFace dataset
    dataset = Dataset.from_pandas(df)

    # Split into train/validation
    if validation_split > 0:
        split = dataset.train_test_split(test_size=validation_split, seed=42)
        return DatasetDict({"train": split["train"], "validation": split["test"]})
    else:
        return DatasetDict({"train": dataset})


def compute_metrics(eval_pred):
    """Compute evaluation metrics."""
    import numpy as np
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support

    predictions, labels = eval_pred
    if isinstance(predictions, tuple):
        predictions = predictions[0]
    predictions = np.argmax(predictions, axis=-1)

    accuracy = accuracy_score(labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, predictions, average="weighted", zero_division=0
    )

    return {"accuracy": accuracy, "precision": precision, "recall": recall, "f1": f1}


async def run_training(job: TrainingJob):
    """
    Runs actual training using HuggingFace Transformers.
    This is the production implementation.
    """
    try:
        # Import ML libraries
        import torch
        from transformers import (
            AutoTokenizer,
            AutoModelForSequenceClassification,
            Trainer,
            TrainingArguments,
            TrainerCallback,
            EarlyStoppingCallback,
        )

        job.state = JobState.INITIALIZING
        job.started_at = datetime.utcnow().isoformat()
        log.info(
            "Initializing training", job_id=job.job_id, model_type=job.model_type.value
        )

        # Determine device
        device = "cuda" if torch.cuda.is_available() and USE_GPU else "cpu"
        log.info(
            "Using device", device=device, cuda_available=torch.cuda.is_available()
        )

        # Determine base model
        base_model = job.base_model or get_default_base_model(job.model_type)
        log.info("Loading base model", base_model=base_model)

        # Load tokenizer and model
        tokenizer = AutoTokenizer.from_pretrained(base_model)

        # Load dataset
        dataset = load_dataset_from_path(
            job.dataset_path, job.dataset_format, job.validation_split
        )

        # Determine number of labels from dataset
        train_dataset = dataset["train"]
        if "label" in train_dataset.column_names:
            num_labels = len(set(train_dataset["label"]))
        elif "labels" in train_dataset.column_names:
            num_labels = len(set(train_dataset["labels"]))
        else:
            num_labels = 2  # Default binary classification

        log.info(
            "Dataset loaded", train_samples=len(train_dataset), num_labels=num_labels
        )

        job.metrics.total_samples = len(train_dataset)

        # Load model
        model = AutoModelForSequenceClassification.from_pretrained(
            base_model, num_labels=num_labels
        )
        model.to(device)

        # Tokenize dataset
        def tokenize_function(examples):
            # Try different text column names
            text_column = None
            for col in ["text", "content", "sentence", "review", "document"]:
                if col in examples:
                    text_column = col
                    break

            if text_column is None:
                raise ValueError(
                    f"No text column found. Available: {list(examples.keys())}"
                )

            return tokenizer(
                examples[text_column],
                padding="max_length",
                truncation=True,
                max_length=job.hyperparameters.get("max_length", 512),
            )

        tokenized_dataset = dataset.map(tokenize_function, batched=True)

        # Prepare output directory
        output_dir = MODEL_DIR / job.job_id
        output_dir.mkdir(parents=True, exist_ok=True)

        # Extract hyperparameters
        batch_size = job.hyperparameters.get("batch_size", 16)
        learning_rate = job.hyperparameters.get("learning_rate", 2e-5)
        warmup_steps = job.hyperparameters.get("warmup_steps", 500)
        weight_decay = job.hyperparameters.get("weight_decay", 0.01)

        # Training arguments
        training_args = TrainingArguments(
            output_dir=str(output_dir),
            num_train_epochs=job.max_epochs,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            warmup_steps=warmup_steps,
            weight_decay=weight_decay,
            learning_rate=learning_rate,
            logging_dir=str(LOG_DIR / job.job_id),
            logging_steps=10,
            eval_strategy="epoch" if "validation" in tokenized_dataset else "no",
            save_strategy="epoch",
            load_best_model_at_end=True if "validation" in tokenized_dataset else False,
            save_total_limit=2,
            report_to=[],  # Disable wandb, etc.
            disable_tqdm=True,  # We use our own progress tracking
            fp16=torch.cuda.is_available()
            and USE_GPU,  # Use mixed precision if available
        )

        # Create custom callback for progress tracking
        class ProgressCallback(TrainerCallback):
            def __init__(self, training_job: TrainingJob):
                self.job = training_job

            def on_train_begin(self, args, state, control, **kwargs):
                self.job.state = JobState.RUNNING
                self.job.metrics.total_steps = state.max_steps or 0
                self.job.metrics.total_epochs = int(args.num_train_epochs)
                self._emit("training_started")

            def on_step_end(self, args, state, control, **kwargs):
                self.job.metrics.step = state.global_step
                if state.max_steps:
                    self.job.progress = (state.global_step / state.max_steps) * 100

                if state.log_history:
                    latest = state.log_history[-1]
                    self.job.metrics.loss = latest.get("loss", self.job.metrics.loss)
                    self.job.metrics.learning_rate = latest.get("learning_rate", 0.0)

                # Emit every 10 steps
                if state.global_step % 10 == 0:
                    self._emit("step_update")

            def on_epoch_end(self, args, state, control, **kwargs):
                self.job.metrics.epoch = int(state.epoch) if state.epoch else 0
                self._emit("epoch_completed")

            def on_evaluate(self, args, state, control, metrics=None, **kwargs):
                if metrics:
                    self.job.metrics.validation_loss = metrics.get("eval_loss", 0.0)
                    self.job.metrics.validation_accuracy = metrics.get(
                        "eval_accuracy", 0.0
                    )
                    self.job.metrics.f1_score = metrics.get("eval_f1", 0.0)
                    self.job.metrics.precision = metrics.get("eval_precision", 0.0)
                    self.job.metrics.recall = metrics.get("eval_recall", 0.0)
                self._emit("evaluation_completed")

            def _emit(self, event_type: str):
                event = {
                    "type": event_type,
                    "timestamp": datetime.utcnow().isoformat(),
                    "job_id": self.job.job_id,
                    "progress": round(self.job.progress, 2),
                    "state": self.job.state.value,
                    "metrics": {
                        "epoch": self.job.metrics.epoch,
                        "step": self.job.metrics.step,
                        "loss": round(self.job.metrics.loss, 4)
                        if self.job.metrics.loss
                        else 0,
                        "accuracy": round(self.job.metrics.accuracy, 4)
                        if self.job.metrics.accuracy
                        else 0,
                        "validation_loss": round(self.job.metrics.validation_loss, 4)
                        if self.job.metrics.validation_loss
                        else 0,
                        "validation_accuracy": round(
                            self.job.metrics.validation_accuracy, 4
                        )
                        if self.job.metrics.validation_accuracy
                        else 0,
                        "f1_score": round(self.job.metrics.f1_score, 4)
                        if self.job.metrics.f1_score
                        else 0,
                        "learning_rate": self.job.metrics.learning_rate,
                    },
                }
                self.job.event_queue.append(event)

        # Create Trainer
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset["train"],
            eval_dataset=tokenized_dataset.get("validation"),
            tokenizer=tokenizer,
            compute_metrics=compute_metrics
            if "validation" in tokenized_dataset
            else None,
            callbacks=[
                ProgressCallback(job),
                EarlyStoppingCallback(early_stopping_patience=3)
                if "validation" in tokenized_dataset
                else None,
            ],
        )

        # Filter None callbacks
        trainer.callback_handler.callbacks = [
            cb for cb in trainer.callback_handler.callbacks if cb is not None
        ]

        log.info("Starting training", job_id=job.job_id)

        # Train
        trainer.train()

        # Save final model
        final_model_path = output_dir / "final"
        trainer.save_model(str(final_model_path))
        tokenizer.save_pretrained(str(final_model_path))

        # Save training info
        training_info = {
            "job_id": job.job_id,
            "model_name": job.model_name,
            "model_type": job.model_type.value,
            "base_model": base_model,
            "final_metrics": asdict(job.metrics),
            "hyperparameters": job.hyperparameters,
            "completed_at": datetime.utcnow().isoformat(),
        }

        with open(final_model_path / "training_info.json", "w") as f:
            json.dump(training_info, f, indent=2)

        job.model_path = str(final_model_path)
        job.state = JobState.COMPLETED
        job.completed_at = datetime.utcnow().isoformat()
        job.progress = 100.0

        # Final metrics from evaluation
        if "validation" in tokenized_dataset:
            final_metrics = trainer.evaluate()
            job.metrics.accuracy = final_metrics.get("eval_accuracy", 0.0)
            job.metrics.f1_score = final_metrics.get("eval_f1", 0.0)

        log.info(
            "Training completed",
            job_id=job.job_id,
            model_path=job.model_path,
            final_accuracy=job.metrics.accuracy,
        )

        # Emit completion event
        job.event_queue.append(
            {
                "type": "training_completed",
                "timestamp": datetime.utcnow().isoformat(),
                "job_id": job.job_id,
                "progress": 100.0,
                "state": JobState.COMPLETED.value,
                "model_path": job.model_path,
                "metrics": asdict(job.metrics),
            }
        )

    except Exception as e:
        job.state = JobState.FAILED
        job.error_message = str(e)
        job.completed_at = datetime.utcnow().isoformat()
        log.error("Training failed", job_id=job.job_id, error=str(e), exc_info=True)

        # Emit failure event
        job.event_queue.append(
            {
                "type": "training_failed",
                "timestamp": datetime.utcnow().isoformat(),
                "job_id": job.job_id,
                "state": JobState.FAILED.value,
                "error": str(e),
            }
        )


async def run_training_with_persistence(job: TrainingJob):
    """
    Wrapper around run_training that persists state changes to Redis.
    """
    try:
        # Save initial state
        if state_store:
            await state_store.save_job(job.job_id, job)

        # Run the actual training
        await run_training(job)

    finally:
        # Always save final state (success or failure)
        if state_store:
            await state_store.save_job(job.job_id, job)
            log.info("Job state persisted", job_id=job.job_id, state=job.state.value)


# =============================================================================
# FastAPI Application
# =============================================================================


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan manager."""
    global state_store

    log.info(
        "ML Training Service starting",
        model_dir=str(MODEL_DIR),
        dataset_dir=str(DATASET_DIR),
        max_concurrent_jobs=MAX_CONCURRENT_JOBS,
    )

    # Initialize state store
    from state_store import get_state_store

    state_store = await get_state_store()

    # Load existing jobs from Redis into memory
    if state_store.is_redis_connected:
        log.info("Redis connected, loading persisted jobs")
        memory_store = state_store.get_memory_store()
        with jobs_lock:
            for job_id, job_data in memory_store.items():
                # Reconstruct TrainingJob from stored data
                try:
                    job = TrainingJob(
                        job_id=job_data.get("job_id", job_id),
                        model_name=job_data.get("model_name", "unknown"),
                        model_type=ModelType(job_data.get("model_type", "sentiment")),
                        dataset_path=job_data.get("dataset_path", ""),
                        dataset_format=job_data.get("dataset_format", "csv"),
                        base_model=job_data.get("base_model"),
                        max_epochs=job_data.get("max_epochs", 3),
                        validation_split=job_data.get("validation_split", 0.1),
                        hyperparameters=job_data.get("hyperparameters", {}),
                        callbacks=job_data.get("callbacks", {}),
                        metadata=job_data.get("metadata", {}),
                        state=JobState(job_data.get("state", "PENDING")),
                        progress=job_data.get("progress", 0.0),
                        error_message=job_data.get("error_message"),
                        model_path=job_data.get("model_path"),
                        created_at=job_data.get(
                            "created_at", datetime.utcnow().isoformat()
                        ),
                        started_at=job_data.get("started_at"),
                        completed_at=job_data.get("completed_at"),
                        # External training fields
                        is_external=job_data.get("is_external", False),
                        upload_token=job_data.get("upload_token"),
                        external_worker_info=job_data.get("external_worker_info", {}),
                    )
                    # Restore metrics
                    if "metrics" in job_data:
                        m = job_data["metrics"]
                        job.metrics = TrainingMetrics(
                            epoch=m.get("epoch", 0),
                            total_epochs=m.get("total_epochs", 0),
                            step=m.get("step", 0),
                            total_steps=m.get("total_steps", 0),
                            loss=m.get("loss", 0.0),
                            accuracy=m.get("accuracy", 0.0),
                            validation_loss=m.get("validation_loss", 0.0),
                            validation_accuracy=m.get("validation_accuracy", 0.0),
                            learning_rate=m.get("learning_rate", 0.0),
                            samples_processed=m.get("samples_processed", 0),
                            total_samples=m.get("total_samples", 0),
                            f1_score=m.get("f1_score", 0.0),
                            precision=m.get("precision", 0.0),
                            recall=m.get("recall", 0.0),
                        )
                    jobs[job_id] = job
                    log.info(
                        "Loaded persisted job", job_id=job_id, state=job.state.value
                    )
                except Exception as e:
                    log.warning("Failed to restore job", job_id=job_id, error=str(e))
    else:
        log.warning(
            "Redis not available, using in-memory storage only (data will be lost on restart)"
        )

    yield

    # Cleanup: save all jobs before shutdown
    if state_store:
        log.info("Saving jobs before shutdown")
        with jobs_lock:
            for job_id, job in jobs.items():
                await state_store.save_job(job_id, job)
        await state_store.disconnect()

    log.info("ML Training Service shutting down")


app = FastAPI(
    title="ML Training Service",
    description="Production ML model training service for sentiment analysis, ABSA, NER, and more",
    version="2.0.0",
    lifespan=lifespan,
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint for service discovery."""
    try:
        import torch

        gpu_available = torch.cuda.is_available()
    except ImportError:
        gpu_available = False

    with jobs_lock:
        active_jobs = sum(
            1
            for j in jobs.values()
            if j.state in [JobState.RUNNING, JobState.INITIALIZING]
        )

    return HealthResponse(
        status="healthy",
        version="2.0.0",
        gpu_available=gpu_available,
        active_jobs=active_jobs,
        supported_model_types=[t.value for t in ModelType],
        max_concurrent_jobs=MAX_CONCURRENT_JOBS,
        redis_connected=state_store.is_redis_connected if state_store else False,
        persisted_jobs=len(state_store.get_memory_store()) if state_store else 0,
    )


@app.post("/train", response_model=TrainingResponse)
async def submit_training_job(
    request: TrainingRequest, background_tasks: BackgroundTasks
):
    """Submit a new training job."""
    # Check concurrent job limit
    with jobs_lock:
        active_jobs = sum(
            1
            for j in jobs.values()
            if j.state in [JobState.RUNNING, JobState.INITIALIZING, JobState.PENDING]
        )
        if active_jobs >= MAX_CONCURRENT_JOBS:
            raise HTTPException(
                status_code=429,
                detail=f"Maximum concurrent jobs ({MAX_CONCURRENT_JOBS}) reached. Please wait.",
            )

    job_id = str(uuid.uuid4())

    try:
        model_type = ModelType(request.model_type.lower())
    except ValueError:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid model type: {request.model_type}. "
            f"Supported types: {[t.value for t in ModelType]}",
        )

    job = TrainingJob(
        job_id=job_id,
        model_name=request.model_name,
        model_type=model_type,
        dataset_path=request.dataset_path,
        dataset_format=request.dataset_format,
        base_model=request.base_model,
        max_epochs=request.max_epochs,
        validation_split=request.validation_split,
        hyperparameters=request.hyperparameters,
        callbacks=request.callbacks,
        metadata=request.metadata,
    )

    with jobs_lock:
        jobs[job_id] = job

    # Persist job to Redis
    if state_store:
        await state_store.save_job(job_id, job)

    # Start training in background - ALWAYS use real training
    background_tasks.add_task(run_training_with_persistence, job)

    log.info("Training job submitted", job_id=job_id, model_name=request.model_name)

    return TrainingResponse(
        job_id=job_id,
        model_name=job.model_name,
        model_type=job.model_type.value,
        state=job.state.value,
        progress=job.progress,
        created_at=job.created_at,
        message="Training job submitted successfully",
    )


@app.get("/jobs/{job_id}/status", response_model=JobStatusResponse)
async def get_job_status(job_id: str):
    """Get the status of a training job."""
    with jobs_lock:
        if job_id not in jobs:
            raise HTTPException(status_code=404, detail=f"Job not found: {job_id}")
        job = jobs[job_id]

    return JobStatusResponse(
        job_id=job.job_id,
        model_name=job.model_name,
        model_type=job.model_type.value,
        state=job.state.value,
        progress=round(job.progress, 2),
        metrics=asdict(job.metrics),
        error_message=job.error_message,
        model_path=job.model_path,
        created_at=job.created_at,
        started_at=job.started_at,
        completed_at=job.completed_at,
        current_epoch=job.metrics.epoch,
        total_epochs=job.metrics.total_epochs,
    )


@app.get("/jobs/{job_id}/stream")
async def stream_job_status(job_id: str):
    """
    SSE endpoint for real-time training progress updates.
    """
    with jobs_lock:
        if job_id not in jobs:
            raise HTTPException(status_code=404, detail=f"Job not found: {job_id}")

    async def event_generator() -> AsyncGenerator[str, None]:
        last_sent_index = 0

        while True:
            with jobs_lock:
                if job_id not in jobs:
                    break
                job = jobs[job_id]

                # Send any new events
                events = list(job.event_queue)
                new_events = events[last_sent_index:]
                last_sent_index = len(events)

            for event in new_events:
                yield f"data: {json.dumps(event)}\n\n"

            # Check if job is done
            if job.state in [JobState.COMPLETED, JobState.FAILED, JobState.CANCELLED]:
                # Send final status
                final_event = {
                    "type": "job_finished",
                    "job_id": job_id,
                    "state": job.state.value,
                    "progress": job.progress,
                    "model_path": job.model_path,
                    "error_message": job.error_message,
                }
                yield f"data: {json.dumps(final_event)}\n\n"
                break

            await asyncio.sleep(1)  # Poll interval

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )


@app.post("/jobs/{job_id}/cancel")
async def cancel_job(job_id: str):
    """Cancel a training job."""
    with jobs_lock:
        if job_id not in jobs:
            raise HTTPException(status_code=404, detail=f"Job not found: {job_id}")

        job = jobs[job_id]

        if job.state not in [JobState.PENDING, JobState.RUNNING, JobState.INITIALIZING]:
            raise HTTPException(
                status_code=400, detail=f"Cannot cancel job in state: {job.state.value}"
            )

        job.state = JobState.CANCELLED
        job.completed_at = datetime.utcnow().isoformat()

    log.info("Training job cancelled", job_id=job_id)

    return {"success": True, "message": "Job cancelled successfully"}


@app.get("/jobs/{job_id}/artifact", response_model=ModelArtifactResponse)
async def get_model_artifact(job_id: str):
    """Get the trained model artifact information."""
    with jobs_lock:
        if job_id not in jobs:
            raise HTTPException(status_code=404, detail=f"Job not found: {job_id}")
        job = jobs[job_id]

    if job.state != JobState.COMPLETED:
        raise HTTPException(
            status_code=400, detail=f"Model not available. Job state: {job.state.value}"
        )

    if not job.model_path or not Path(str(job.model_path)).exists():
        raise HTTPException(status_code=404, detail="Model artifact not found")

    # Calculate file size and checksum
    model_path = Path(job.model_path)

    # Get total size of all files in directory
    size_bytes = sum(f.stat().st_size for f in model_path.rglob("*") if f.is_file())

    # Checksum of the config file
    config_file = model_path / "config.json"
    if config_file.exists():
        with open(config_file, "rb") as f:
            checksum = hashlib.sha256(f.read()).hexdigest()
    else:
        checksum = "directory"

    return ModelArtifactResponse(
        model_path=str(job.model_path),
        model_name=job.model_name,
        model_type=job.model_type.value,
        framework="pytorch",
        version="1.0.0",
        size_bytes=size_bytes,
        checksum=checksum,
        metrics={
            "accuracy": job.metrics.accuracy,
            "f1_score": job.metrics.f1_score,
            "precision": job.metrics.precision,
            "recall": job.metrics.recall,
            "final_loss": job.metrics.loss,
        },
        model_filename=f"{job.model_name}_{job.job_id[:8]}",
    )


@app.get("/jobs/{job_id}/model/meta")
async def get_model_metadata(job_id: str):
    """Get model metadata for download."""
    with jobs_lock:
        if job_id not in jobs:
            raise HTTPException(status_code=404, detail=f"Job not found: {job_id}")
        job = jobs[job_id]

    if job.state != JobState.COMPLETED:
        raise HTTPException(status_code=400, detail="Model not ready")

    model_path = Path(job.model_path)
    size_bytes = sum(f.stat().st_size for f in model_path.rglob("*") if f.is_file())

    return {
        "model_name": job.model_name,
        "model_type": job.model_type.value,
        "model_filename": f"{job.model_name}_{job.job_id[:8]}",
        "format": "huggingface",
        "size_bytes": size_bytes,
        "metrics": asdict(job.metrics),
        "checksum": "directory",
        "version": "1.0.0",
    }


@app.get("/jobs")
async def list_jobs(
    state: Optional[str] = None, model_type: Optional[str] = None, limit: int = 50
):
    """List training jobs with optional filtering."""
    result = []

    with jobs_lock:
        for job in list(jobs.values())[-limit:]:
            if state and job.state.value.lower() != state.lower():
                continue
            if model_type and job.model_type.value.lower() != model_type.lower():
                continue

            result.append(
                {
                    "job_id": job.job_id,
                    "model_name": job.model_name,
                    "model_type": job.model_type.value,
                    "state": job.state.value,
                    "progress": round(job.progress, 2),
                    "created_at": job.created_at,
                    "completed_at": job.completed_at,
                }
            )

    return {"jobs": result, "total": len(result)}


@app.get("/models")
async def list_models():
    """List available trained models."""
    models = []

    with jobs_lock:
        for job in jobs.values():
            if job.state == JobState.COMPLETED and job.model_path:
                models.append(
                    {
                        "job_id": job.job_id,
                        "model_name": job.model_name,
                        "model_type": job.model_type.value,
                        "model_path": job.model_path,
                        "accuracy": job.metrics.accuracy,
                        "f1_score": job.metrics.f1_score,
                        "completed_at": job.completed_at,
                    }
                )

    return {"models": models, "total": len(models)}


@app.get("/supported-types")
async def get_supported_types():
    """Get list of supported model types with their default configurations."""
    return {
        "types": [
            {
                "type": t.value,
                "default_base_model": get_default_base_model(t),
                "description": get_model_type_description(t),
            }
            for t in ModelType
        ]
    }


def get_model_type_description(model_type: ModelType) -> str:
    """Returns description for each model type."""
    descriptions = {
        ModelType.SENTIMENT: "Binary or multi-class sentiment classification",
        ModelType.ABSA: "Aspect-Based Sentiment Analysis for fine-grained opinion mining",
        ModelType.NER: "Named Entity Recognition for extracting entities from text",
        ModelType.CLASSIFICATION: "General text classification for custom categories",
        ModelType.EMBEDDING: "Text embedding models for semantic similarity",
        ModelType.TRANSFORMER: "Custom transformer models for various NLP tasks",
    }
    return descriptions.get(model_type, "")


# =============================================================================
# External Training API (Colab/Jupyter Integration)
# =============================================================================


def _verify_upload_token(job_id: str, token: str) -> TrainingJob:
    """Verify upload token and return the job if valid."""
    with jobs_lock:
        if job_id not in jobs:
            raise HTTPException(status_code=404, detail=f"Job not found: {job_id}")
        job = jobs[job_id]
        if not job.is_external:
            raise HTTPException(
                status_code=400, detail="This job is not an external training job"
            )
        if job.upload_token != token:
            raise HTTPException(status_code=403, detail="Invalid upload token")
        return job


@app.post("/train/external/start", response_model=ExternalTrainingStartResponse)
async def start_external_training(request: ExternalTrainingStartRequest):
    """
    Start an external training session (Colab/Jupyter).

    Returns a job_id and upload_token that the external worker uses to:
    - Report training progress via POST /jobs/{job_id}/progress
    - Upload the trained model via POST /jobs/{job_id}/upload
    - Mark training as complete via POST /jobs/{job_id}/complete
    """
    job_id = str(uuid.uuid4())
    upload_token = secrets.token_urlsafe(32)

    try:
        model_type = ModelType(request.model_type.lower())
    except ValueError:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid model type: {request.model_type}. "
            f"Supported types: {[t.value for t in ModelType]}",
        )

    job = TrainingJob(
        job_id=job_id,
        model_name=request.model_name,
        model_type=model_type,
        dataset_path="external",  # Not applicable for external training
        dataset_format="external",
        base_model=request.base_model,
        max_epochs=request.max_epochs,
        validation_split=0.0,
        metadata=request.metadata,
        state=JobState.PENDING,
        is_external=True,
        upload_token=upload_token,
        external_worker_info={
            "registered_at": datetime.utcnow().isoformat(),
            "source": request.metadata.get("source", "unknown"),
        },
    )

    with jobs_lock:
        jobs[job_id] = job

    # Persist job to Redis
    if state_store:
        await state_store.save_job(job_id, job)

    log.info(
        "External training session started",
        job_id=job_id,
        model_name=request.model_name,
        model_type=model_type.value,
    )

    return ExternalTrainingStartResponse(
        job_id=job_id,
        upload_token=upload_token,
        model_name=job.model_name,
        model_type=job.model_type.value,
        state=job.state.value,
        created_at=job.created_at,
        message="External training session created. Use the upload_token for progress updates and model upload.",
        api_endpoints={
            "progress": f"/jobs/{job_id}/progress",
            "upload": f"/jobs/{job_id}/upload",
            "complete": f"/jobs/{job_id}/complete",
            "status": f"/jobs/{job_id}/status",
            "stream": f"/jobs/{job_id}/stream",
        },
    )


@app.post("/jobs/{job_id}/progress")
async def report_external_progress(job_id: str, request: ExternalProgressRequest):
    """
    Report training progress from external worker (Colab/Jupyter).

    The external worker should call this endpoint periodically to update
    the dashboard with current training metrics.
    """
    job = _verify_upload_token(job_id, request.upload_token)

    # Update job state if still pending
    if job.state == JobState.PENDING:
        job.state = JobState.RUNNING
        job.started_at = datetime.utcnow().isoformat()

    # Update progress and metrics
    job.progress = request.progress
    job.metrics.epoch = request.epoch
    job.metrics.total_epochs = request.total_epochs
    job.metrics.step = request.step
    job.metrics.total_steps = request.total_steps
    job.metrics.loss = request.loss
    job.metrics.accuracy = request.accuracy
    job.metrics.validation_loss = request.validation_loss
    job.metrics.validation_accuracy = request.validation_accuracy
    job.metrics.f1_score = request.f1_score
    job.metrics.learning_rate = request.learning_rate

    # Emit SSE event
    event = {
        "type": "external_progress",
        "timestamp": datetime.utcnow().isoformat(),
        "job_id": job_id,
        "progress": round(job.progress, 2),
        "state": job.state.value,
        "metrics": {
            "epoch": job.metrics.epoch,
            "total_epochs": job.metrics.total_epochs,
            "step": job.metrics.step,
            "loss": round(job.metrics.loss, 4) if job.metrics.loss else 0,
            "accuracy": round(job.metrics.accuracy, 4) if job.metrics.accuracy else 0,
            "validation_loss": round(job.metrics.validation_loss, 4)
            if job.metrics.validation_loss
            else 0,
            "validation_accuracy": round(job.metrics.validation_accuracy, 4)
            if job.metrics.validation_accuracy
            else 0,
            "f1_score": round(job.metrics.f1_score, 4) if job.metrics.f1_score else 0,
        },
        "message": request.message,
    }
    job.event_queue.append(event)

    # Persist to Redis
    if state_store:
        await state_store.save_job(job_id, job)

    log.debug(
        "External progress reported",
        job_id=job_id,
        progress=job.progress,
        epoch=job.metrics.epoch,
    )

    return {
        "success": True,
        "job_id": job_id,
        "progress": job.progress,
        "state": job.state.value,
    }


@app.post("/jobs/{job_id}/upload")
async def upload_external_model(
    job_id: str,
    upload_token: str = Form(...),
    model_file: UploadFile = File(...),
):
    """
    Upload trained model artifact from external worker (Colab/Jupyter).

    Accepts .pt, .pth, .bin, .safetensors, .h5, .pkl, .joblib, .onnx files,
    or a .zip archive containing the model directory.
    """
    job = _verify_upload_token(job_id, upload_token)

    # Validate file extension
    allowed_extensions = {
        ".pt",
        ".pth",
        ".bin",
        ".safetensors",
        ".h5",
        ".pkl",
        ".joblib",
        ".onnx",
        ".zip",
    }
    file_ext = Path(model_file.filename or "").suffix.lower()
    if file_ext not in allowed_extensions:
        raise HTTPException(
            status_code=400,
            detail=f"Unsupported file type: {file_ext}. Allowed: {allowed_extensions}",
        )

    # Create model directory
    model_dir = MODEL_DIR / job_id / "external"
    model_dir.mkdir(parents=True, exist_ok=True)

    try:
        if file_ext == ".zip":
            # Save and extract zip file
            zip_path = model_dir / "model.zip"
            with open(zip_path, "wb") as f:
                content = await model_file.read()
                f.write(content)

            # Extract zip contents
            with zipfile.ZipFile(zip_path, "r") as zip_ref:
                zip_ref.extractall(model_dir)

            # Remove the zip file after extraction
            zip_path.unlink()
            log.info("Model zip extracted", job_id=job_id, path=str(model_dir))
        else:
            # Save single file
            file_path = model_dir / (model_file.filename or f"model{file_ext}")
            with open(file_path, "wb") as f:
                content = await model_file.read()
                f.write(content)
            log.info("Model file saved", job_id=job_id, path=str(file_path))

        # Update job with model path
        job.model_path = str(model_dir)
        job.external_worker_info["model_uploaded_at"] = datetime.utcnow().isoformat()
        job.external_worker_info["original_filename"] = model_file.filename

        # Persist to Redis
        if state_store:
            await state_store.save_job(job_id, job)

        # Emit SSE event
        event = {
            "type": "model_uploaded",
            "timestamp": datetime.utcnow().isoformat(),
            "job_id": job_id,
            "model_path": job.model_path,
        }
        job.event_queue.append(event)

        return {
            "success": True,
            "job_id": job_id,
            "model_path": job.model_path,
            "message": "Model uploaded successfully",
        }

    except Exception as e:
        log.error("Model upload failed", job_id=job_id, error=str(e))
        raise HTTPException(status_code=500, detail=f"Upload failed: {str(e)}")


@app.post("/jobs/{job_id}/complete")
async def complete_external_training(job_id: str, request: ExternalCompleteRequest):
    """
    Mark external training job as complete.

    Should be called after the model has been uploaded (or if using external storage).
    """
    job = _verify_upload_token(job_id, request.upload_token)

    if job.state == JobState.COMPLETED:
        return {
            "success": True,
            "job_id": job_id,
            "message": "Job already completed",
            "state": job.state.value,
        }

    # Update final metrics if provided
    if request.final_metrics:
        job.metrics.accuracy = request.final_metrics.get(
            "accuracy", job.metrics.accuracy
        )
        job.metrics.f1_score = request.final_metrics.get(
            "f1_score", job.metrics.f1_score
        )
        job.metrics.precision = request.final_metrics.get(
            "precision", job.metrics.precision
        )
        job.metrics.recall = request.final_metrics.get("recall", job.metrics.recall)
        job.metrics.loss = request.final_metrics.get("loss", job.metrics.loss)
        job.metrics.validation_loss = request.final_metrics.get(
            "validation_loss", job.metrics.validation_loss
        )
        job.metrics.validation_accuracy = request.final_metrics.get(
            "validation_accuracy", job.metrics.validation_accuracy
        )

    # Update model path if provided
    if request.model_path:
        job.model_path = request.model_path

    # Mark as complete
    job.state = JobState.COMPLETED
    job.progress = 100.0
    job.completed_at = datetime.utcnow().isoformat()

    # Create training info file
    if job.model_path:
        training_info_path = Path(job.model_path) / "training_info.json"
        if training_info_path.parent.exists():
            training_info = {
                "job_id": job.job_id,
                "model_name": job.model_name,
                "model_type": job.model_type.value,
                "base_model": job.base_model,
                "is_external": True,
                "source": job.external_worker_info.get("source", "external"),
                "final_metrics": asdict(job.metrics),
                "completed_at": job.completed_at,
            }
            try:
                with open(training_info_path, "w") as f:
                    json.dump(training_info, f, indent=2)
            except Exception as e:
                log.warning("Failed to save training info", error=str(e))

    # Persist to Redis
    if state_store:
        await state_store.save_job(job_id, job)

    # Emit SSE event
    event = {
        "type": "training_completed",
        "timestamp": datetime.utcnow().isoformat(),
        "job_id": job_id,
        "progress": 100.0,
        "state": JobState.COMPLETED.value,
        "model_path": job.model_path,
        "metrics": asdict(job.metrics),
    }
    job.event_queue.append(event)

    log.info(
        "External training completed",
        job_id=job_id,
        model_name=job.model_name,
        model_path=job.model_path,
        accuracy=job.metrics.accuracy,
    )

    return {
        "success": True,
        "job_id": job_id,
        "state": job.state.value,
        "model_path": job.model_path,
        "metrics": asdict(job.metrics),
        "message": "External training completed successfully",
    }


# =============================================================================
# Inference API
# =============================================================================


# Global model cache for inference
_model_cache: dict[str, tuple] = {}  # job_id -> (tokenizer, model)
_model_cache_lock = threading.Lock()


class InferenceRequest(BaseModel):
    """Request for model inference."""

    text: str = Field(
        ..., min_length=1, max_length=10000, description="Text to analyze"
    )
    texts: Optional[list[str]] = Field(
        None, description="Batch of texts (alternative to single text)"
    )
    aspects: Optional[list[str]] = Field(
        None, description="Aspects to analyze (for ABSA)"
    )
    max_length: int = Field(512, ge=32, le=2048, description="Maximum token length")
    return_probabilities: bool = Field(True, description="Return class probabilities")


class AspectSentiment(BaseModel):
    """Aspect sentiment result."""

    sentimentScore: float = Field(
        ..., ge=-1.0, le=1.0, description="Sentiment score -1 to 1"
    )
    sentimentLabel: str = Field(..., description="positive/negative/neutral")
    confidence: float = Field(..., ge=0.0, le=1.0, description="Confidence score")


class OverallSentiment(BaseModel):
    """Overall sentiment result."""

    score: float = Field(..., ge=-1.0, le=1.0)
    label: str


class InferenceResponse(BaseModel):
    """Response from model inference."""

    analysisId: str
    contentId: Optional[str] = None
    textPreview: str
    aspectsAnalyzed: list[str]
    aspectSentiments: dict[str, AspectSentiment]
    overallSentiment: OverallSentiment
    confidence: float
    analyzedAt: str
    modelUsed: str
    modelType: str
    responseTimeMs: int


def load_model_for_inference(job_id: str) -> tuple:
    """
    Load a trained model for inference with caching.
    Returns (tokenizer, model) tuple.
    """
    with _model_cache_lock:
        if job_id in _model_cache:
            return _model_cache[job_id]

    # Get job info
    with jobs_lock:
        if job_id not in jobs:
            raise HTTPException(status_code=404, detail=f"Job not found: {job_id}")
        job = jobs[job_id]

    if job.state != JobState.COMPLETED:
        raise HTTPException(
            status_code=400, detail=f"Model not available. Job state: {job.state.value}"
        )

    if not job.model_path or not Path(job.model_path).exists():
        raise HTTPException(status_code=404, detail="Model artifact not found")

    try:
        from transformers import AutoTokenizer, AutoModelForSequenceClassification
        import torch

        model_path = job.model_path
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForSequenceClassification.from_pretrained(model_path)

        # Move to GPU if available
        device = "cuda" if USE_GPU and torch.cuda.is_available() else "cpu"
        model = model.to(device)
        model.eval()

        with _model_cache_lock:
            _model_cache[job_id] = (tokenizer, model, device)

        log.info("Model loaded for inference", job_id=job_id, device=device)
        return tokenizer, model, device

    except Exception as e:
        log.error("Failed to load model", job_id=job_id, error=str(e))
        raise HTTPException(status_code=500, detail=f"Failed to load model: {str(e)}")


def compute_sentiment_label(score: float) -> str:
    """Convert sentiment score to label."""
    if score > 0.1:
        return "positive"
    elif score < -0.1:
        return "negative"
    return "neutral"


@app.post("/inference/{job_id}", response_model=InferenceResponse)
async def run_inference(job_id: str, request: InferenceRequest):
    """
    Run inference using a trained model.

    For ABSA models, analyzes sentiment for each aspect.
    For sentiment/classification models, returns overall sentiment.
    """
    import time
    import torch

    start_time = time.time()

    # Load model
    tokenizer, model, device = load_model_for_inference(job_id)

    # Get job info for model type
    with jobs_lock:
        job = jobs[job_id]

    texts = request.texts or [request.text]
    aspects = request.aspects or []

    # For ABSA: if no aspects provided, analyze full text
    if not aspects:
        aspects = ["전체"]  # Default aspect

    aspect_sentiments: dict[str, AspectSentiment] = {}
    all_scores = []

    try:
        for aspect in aspects:
            # For ABSA, combine aspect with text
            if job.model_type == ModelType.ABSA and aspect != "전체":
                input_text = f"[{aspect}] {texts[0]}"
            else:
                input_text = texts[0]

            # Tokenize
            inputs = tokenizer(
                input_text,
                return_tensors="pt",
                truncation=True,
                max_length=request.max_length,
                padding=True,
            )
            inputs = {k: v.to(device) for k, v in inputs.items()}

            # Inference
            with torch.no_grad():
                outputs = model(**inputs)

            logits = outputs.logits
            probabilities = torch.softmax(logits, dim=-1).cpu().numpy()[0]

            # Determine sentiment (assuming binary or 3-class classification)
            num_classes = len(probabilities)

            if num_classes == 2:
                # Binary: [negative, positive]
                sentiment_score = float(probabilities[1] - probabilities[0])
                confidence = float(max(probabilities))
            elif num_classes == 3:
                # 3-class: [negative, neutral, positive]
                sentiment_score = float(probabilities[2] - probabilities[0])
                confidence = float(max(probabilities))
            else:
                # Multi-class: use argmax
                predicted_class = int(probabilities.argmax())
                sentiment_score = (
                    float(2 * (predicted_class / (num_classes - 1)) - 1)
                    if num_classes > 1
                    else 0.0
                )
                confidence = float(probabilities[predicted_class])

            all_scores.append(sentiment_score)

            aspect_sentiments[aspect] = AspectSentiment(
                sentimentScore=round(sentiment_score, 4),
                sentimentLabel=compute_sentiment_label(sentiment_score),
                confidence=round(confidence, 4),
            )

        # Compute overall sentiment (average of all aspects)
        overall_score = sum(all_scores) / len(all_scores) if all_scores else 0.0
        overall_confidence = (
            sum(a.confidence for a in aspect_sentiments.values())
            / len(aspect_sentiments)
            if aspect_sentiments
            else 0.0
        )

        response_time_ms = int((time.time() - start_time) * 1000)

        return InferenceResponse(
            analysisId=str(uuid.uuid4()),
            contentId=None,
            textPreview=texts[0][:200] + "..." if len(texts[0]) > 200 else texts[0],
            aspectsAnalyzed=list(aspect_sentiments.keys()),
            aspectSentiments=aspect_sentiments,
            overallSentiment=OverallSentiment(
                score=round(overall_score, 4),
                label=compute_sentiment_label(overall_score),
            ),
            confidence=round(overall_confidence, 4),
            analyzedAt=datetime.utcnow().isoformat(),
            modelUsed=job.model_name,
            modelType=job.model_type.value,
            responseTimeMs=response_time_ms,
        )

    except Exception as e:
        log.error("Inference failed", job_id=job_id, error=str(e), exc_info=True)
        raise HTTPException(status_code=500, detail=f"Inference failed: {str(e)}")


@app.post("/inference/by-name/{model_name}", response_model=InferenceResponse)
async def run_inference_by_name(model_name: str, request: InferenceRequest):
    """
    Run inference using a model by name (uses latest completed model with that name).
    """
    # Find the latest completed job with this model name
    job_id = None
    latest_completed_at = None

    with jobs_lock:
        for job in jobs.values():
            if (
                job.model_name == model_name
                and job.state == JobState.COMPLETED
                and job.model_path
            ):
                if latest_completed_at is None or (
                    job.completed_at and job.completed_at > latest_completed_at
                ):
                    job_id = job.job_id
                    latest_completed_at = job.completed_at

    if not job_id:
        raise HTTPException(
            status_code=404, detail=f"No completed model found with name: {model_name}"
        )

    return await run_inference(job_id, request)


@app.delete("/inference/cache/{job_id}")
async def clear_model_cache(job_id: str):
    """Clear a model from the inference cache to free memory."""
    with _model_cache_lock:
        if job_id in _model_cache:
            del _model_cache[job_id]
            return {"success": True, "message": f"Model {job_id} removed from cache"}
    return {"success": False, "message": "Model not in cache"}


@app.get("/inference/cache/status")
async def get_cache_status():
    """Get inference cache status."""
    with _model_cache_lock:
        cached_models = list(_model_cache.keys())
    return {
        "cached_models": cached_models,
        "total_cached": len(cached_models),
    }


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8090)

```

---

## backend/ml-addons/ml-trainer/notebooks/main_ref.py

```py
"""
ML Training Service - Production Version

FastAPI-based ML training service that supports multiple model types:
- Sentiment Analysis (BERT-based)
- ABSA (Aspect-Based Sentiment Analysis)
- NER (Named Entity Recognition)
- Text Classification
- Embedding Models

This service performs REAL training using HuggingFace Transformers.
"""

import os
import uuid
import json
import asyncio
import hashlib
import threading
from datetime import datetime
from pathlib import Path
from typing import Optional, AsyncGenerator
from enum import Enum
from dataclasses import dataclass, field, asdict
from contextlib import asynccontextmanager
from collections import deque

import structlog
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field

# Configure structlog
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer(),
    ],
    wrapper_class=structlog.stdlib.BoundLogger,
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    cache_logger_on_first_use=True,
)

log = structlog.get_logger()

# Environment configuration
MODEL_DIR = Path(os.getenv("MODEL_DIR", "/app/models"))
DATASET_DIR = Path(os.getenv("DATASET_DIR", "/app/datasets"))
LOG_DIR = Path(os.getenv("LOG_DIR", "/app/logs"))
USE_GPU = os.getenv("USE_GPU", "true").lower() == "true"
MAX_CONCURRENT_JOBS = int(os.getenv("MAX_CONCURRENT_JOBS", "2"))

# Ensure directories exist
MODEL_DIR.mkdir(parents=True, exist_ok=True)
DATASET_DIR.mkdir(parents=True, exist_ok=True)
LOG_DIR.mkdir(parents=True, exist_ok=True)


# =============================================================================
# Enums and Data Classes
# =============================================================================


class JobState(str, Enum):
    PENDING = "PENDING"
    INITIALIZING = "INITIALIZING"
    RUNNING = "RUNNING"
    COMPLETED = "COMPLETED"
    FAILED = "FAILED"
    CANCELLED = "CANCELLED"


class ModelType(str, Enum):
    SENTIMENT = "sentiment"
    ABSA = "absa"
    NER = "ner"
    CLASSIFICATION = "classification"
    EMBEDDING = "embedding"
    TRANSFORMER = "transformer"


@dataclass
class TrainingMetrics:
    epoch: int = 0
    total_epochs: int = 0
    step: int = 0
    total_steps: int = 0
    loss: float = 0.0
    accuracy: float = 0.0
    validation_loss: float = 0.0
    validation_accuracy: float = 0.0
    learning_rate: float = 0.0
    samples_processed: int = 0
    total_samples: int = 0
    f1_score: float = 0.0
    precision: float = 0.0
    recall: float = 0.0


@dataclass
class TrainingJob:
    job_id: str
    model_name: str
    model_type: ModelType
    dataset_path: str
    dataset_format: str
    base_model: Optional[str]
    max_epochs: int
    validation_split: float
    hyperparameters: dict = field(default_factory=dict)
    callbacks: dict = field(default_factory=dict)
    metadata: dict = field(default_factory=dict)
    state: JobState = JobState.PENDING
    progress: float = 0.0
    metrics: TrainingMetrics = field(default_factory=TrainingMetrics)
    error_message: Optional[str] = None
    model_path: Optional[str] = None
    created_at: str = field(default_factory=lambda: datetime.utcnow().isoformat())
    started_at: Optional[str] = None
    completed_at: Optional[str] = None
    # For SSE streaming
    event_queue: deque = field(default_factory=lambda: deque(maxlen=100))


# In-memory job storage (backed by Redis via state_store)
# This dict is a cache, actual persistence is handled by StateStore
jobs: dict[str, TrainingJob] = {}
# Lock for thread-safe operations
jobs_lock = threading.Lock()

# State store instance (initialized in lifespan)
state_store = None


# =============================================================================
# Pydantic Models for API
# =============================================================================


class TrainingRequest(BaseModel):
    model_name: str = Field(..., description="Name of the model to train")
    model_type: str = Field(
        ..., description="Type of model (sentiment, absa, ner, etc.)"
    )
    dataset_path: str = Field(..., description="Path to training dataset")
    dataset_format: str = Field(
        default="csv", description="Format of dataset (csv, jsonl, parquet)"
    )
    base_model: Optional[str] = Field(
        default=None, description="Base model for fine-tuning"
    )
    max_epochs: int = Field(
        default=3, ge=1, le=100, description="Maximum training epochs"
    )
    validation_split: float = Field(
        default=0.1, ge=0.0, le=0.5, description="Validation data split ratio"
    )
    hyperparameters: dict = Field(
        default_factory=dict, description="Training hyperparameters"
    )
    callbacks: dict = Field(default_factory=dict, description="Callback configurations")
    metadata: dict = Field(default_factory=dict, description="Additional metadata")


class TrainingResponse(BaseModel):
    job_id: str
    model_name: str
    model_type: str
    state: str
    progress: float
    created_at: str
    message: str


class JobStatusResponse(BaseModel):
    job_id: str
    model_name: str
    model_type: str
    state: str
    progress: float
    metrics: dict
    error_message: Optional[str]
    model_path: Optional[str]
    created_at: str
    started_at: Optional[str]
    completed_at: Optional[str]
    current_epoch: int = 0
    total_epochs: int = 0


class ModelArtifactResponse(BaseModel):
    model_path: str
    model_name: str
    model_type: str
    framework: str
    version: str
    size_bytes: int
    checksum: str
    metrics: dict
    model_filename: str


class HealthResponse(BaseModel):
    status: str
    version: str
    gpu_available: bool
    active_jobs: int
    supported_model_types: list[str]
    max_concurrent_jobs: int
    redis_connected: bool = False
    persisted_jobs: int = 0


# =============================================================================
# Training Progress Callback (Real HuggingFace Integration)
# =============================================================================


class TrainingProgressCallback:
    """Custom callback to track training progress for HuggingFace Trainer."""

    def __init__(self, job: TrainingJob):
        self.job = job

    def on_train_begin(self, args, state, control, **kwargs):
        self.job.metrics.total_steps = state.max_steps if state.max_steps else 0
        self.job.metrics.total_epochs = args.num_train_epochs
        self._emit_event(
            "training_started", {"total_steps": self.job.metrics.total_steps}
        )

    def on_step_end(self, args, state, control, **kwargs):
        self.job.metrics.step = state.global_step
        if state.max_steps:
            self.job.progress = (state.global_step / state.max_steps) * 100

        if state.log_history:
            latest = state.log_history[-1]
            self.job.metrics.loss = latest.get("loss", self.job.metrics.loss)
            self.job.metrics.learning_rate = latest.get(
                "learning_rate", self.job.metrics.learning_rate
            )

        self._emit_event(
            "step_completed",
            {
                "step": state.global_step,
                "progress": self.job.progress,
                "loss": self.job.metrics.loss,
            },
        )

    def on_epoch_end(self, args, state, control, **kwargs):
        self.job.metrics.epoch = int(state.epoch) if state.epoch else 0
        self._emit_event(
            "epoch_completed",
            {"epoch": self.job.metrics.epoch, "progress": self.job.progress},
        )

    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if metrics:
            self.job.metrics.validation_loss = metrics.get("eval_loss", 0.0)
            self.job.metrics.validation_accuracy = metrics.get("eval_accuracy", 0.0)
            self.job.metrics.f1_score = metrics.get("eval_f1", 0.0)
            self._emit_event(
                "evaluation_completed",
                {
                    "eval_loss": self.job.metrics.validation_loss,
                    "eval_accuracy": self.job.metrics.validation_accuracy,
                },
            )

    def on_train_end(self, args, state, control, **kwargs):
        self._emit_event("training_ended", {"final_step": state.global_step})

    def _emit_event(self, event_type: str, data: dict):
        """Emit event to job's event queue for SSE streaming."""
        event = {
            "type": event_type,
            "timestamp": datetime.utcnow().isoformat(),
            "job_id": self.job.job_id,
            "progress": self.job.progress,
            "state": self.job.state.value,
            "metrics": asdict(self.job.metrics),
            **data,
        }
        self.job.event_queue.append(event)


# =============================================================================
# Real Training Logic
# =============================================================================


def get_default_base_model(model_type: ModelType) -> str:
    """Returns the default base model for each model type."""
    defaults = {
        ModelType.SENTIMENT: "klue/bert-base",
        ModelType.ABSA: "monologg/koelectra-base-v3-discriminator",
        ModelType.NER: "klue/bert-base",
        ModelType.CLASSIFICATION: "klue/roberta-base",
        ModelType.EMBEDDING: "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        ModelType.TRANSFORMER: "klue/bert-base",
    }
    return defaults.get(model_type, "klue/bert-base")


def load_dataset_from_path(
    dataset_path: str, dataset_format: str, validation_split: float
):
    """Load dataset from file path with proper format handling."""
    from datasets import load_dataset, Dataset, DatasetDict
    import pandas as pd

    path = Path(dataset_path)
    if not path.exists():
        # Check in DATASET_DIR
        path = DATASET_DIR / dataset_path
        if not path.exists():
            raise FileNotFoundError(f"Dataset not found: {dataset_path}")

    log.info("Loading dataset", path=str(path), format=dataset_format)

    if dataset_format == "csv":
        df = pd.read_csv(path)
    elif dataset_format == "jsonl":
        df = pd.read_json(path, lines=True)
    elif dataset_format == "json":
        df = pd.read_json(path)
    elif dataset_format == "parquet":
        df = pd.read_parquet(path)
    else:
        raise ValueError(f"Unsupported dataset format: {dataset_format}")

    # Convert to HuggingFace dataset
    dataset = Dataset.from_pandas(df)

    # Split into train/validation
    if validation_split > 0:
        split = dataset.train_test_split(test_size=validation_split, seed=42)
        return DatasetDict({"train": split["train"], "validation": split["test"]})
    else:
        return DatasetDict({"train": dataset})


def compute_metrics(eval_pred):
    """Compute evaluation metrics."""
    import numpy as np
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support

    predictions, labels = eval_pred
    if isinstance(predictions, tuple):
        predictions = predictions[0]
    predictions = np.argmax(predictions, axis=-1)

    accuracy = accuracy_score(labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, predictions, average="weighted", zero_division=0
    )

    return {"accuracy": accuracy, "precision": precision, "recall": recall, "f1": f1}


async def run_training(job: TrainingJob):
    """
    Runs actual training using HuggingFace Transformers.
    This is the production implementation.
    """
    try:
        # Import ML libraries
        import torch
        from transformers import (
            AutoTokenizer,
            AutoModelForSequenceClassification,
            Trainer,
            TrainingArguments,
            TrainerCallback,
            EarlyStoppingCallback,
        )

        job.state = JobState.INITIALIZING
        job.started_at = datetime.utcnow().isoformat()
        log.info(
            "Initializing training", job_id=job.job_id, model_type=job.model_type.value
        )

        # Determine device
        device = "cuda" if torch.cuda.is_available() and USE_GPU else "cpu"
        log.info(
            "Using device", device=device, cuda_available=torch.cuda.is_available()
        )

        # Determine base model
        base_model = job.base_model or get_default_base_model(job.model_type)
        log.info("Loading base model", base_model=base_model)

        # Load tokenizer and model
        tokenizer = AutoTokenizer.from_pretrained(base_model)

        # Load dataset
        dataset = load_dataset_from_path(
            job.dataset_path, job.dataset_format, job.validation_split
        )

        # Determine number of labels from dataset
        train_dataset = dataset["train"]
        if "label" in train_dataset.column_names:
            num_labels = len(set(train_dataset["label"]))
        elif "labels" in train_dataset.column_names:
            num_labels = len(set(train_dataset["labels"]))
        else:
            num_labels = 2  # Default binary classification

        log.info(
            "Dataset loaded", train_samples=len(train_dataset), num_labels=num_labels
        )

        job.metrics.total_samples = len(train_dataset)

        # Load model
        model = AutoModelForSequenceClassification.from_pretrained(
            base_model, num_labels=num_labels
        )
        model.to(device)

        # Tokenize dataset
        def tokenize_function(examples):
            # Try different text column names
            text_column = None
            for col in ["text", "content", "sentence", "review", "document"]:
                if col in examples:
                    text_column = col
                    break

            if text_column is None:
                raise ValueError(
                    f"No text column found. Available: {list(examples.keys())}"
                )

            return tokenizer(
                examples[text_column],
                padding="max_length",
                truncation=True,
                max_length=job.hyperparameters.get("max_length", 512),
            )

        tokenized_dataset = dataset.map(tokenize_function, batched=True)

        # Prepare output directory
        output_dir = MODEL_DIR / job.job_id
        output_dir.mkdir(parents=True, exist_ok=True)

        # Extract hyperparameters
        batch_size = job.hyperparameters.get("batch_size", 16)
        learning_rate = job.hyperparameters.get("learning_rate", 2e-5)
        warmup_steps = job.hyperparameters.get("warmup_steps", 500)
        weight_decay = job.hyperparameters.get("weight_decay", 0.01)

        # Training arguments
        training_args = TrainingArguments(
            output_dir=str(output_dir),
            num_train_epochs=job.max_epochs,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            warmup_steps=warmup_steps,
            weight_decay=weight_decay,
            learning_rate=learning_rate,
            logging_dir=str(LOG_DIR / job.job_id),
            logging_steps=10,
            eval_strategy="epoch" if "validation" in tokenized_dataset else "no",
            save_strategy="epoch",
            load_best_model_at_end=True if "validation" in tokenized_dataset else False,
            save_total_limit=2,
            report_to=[],  # Disable wandb, etc.
            disable_tqdm=True,  # We use our own progress tracking
            fp16=torch.cuda.is_available()
            and USE_GPU,  # Use mixed precision if available
        )

        # Create custom callback for progress tracking
        class ProgressCallback(TrainerCallback):
            def __init__(self, training_job: TrainingJob):
                self.job = training_job

            def on_train_begin(self, args, state, control, **kwargs):
                self.job.state = JobState.RUNNING
                self.job.metrics.total_steps = state.max_steps or 0
                self.job.metrics.total_epochs = int(args.num_train_epochs)
                self._emit("training_started")

            def on_step_end(self, args, state, control, **kwargs):
                self.job.metrics.step = state.global_step
                if state.max_steps:
                    self.job.progress = (state.global_step / state.max_steps) * 100

                if state.log_history:
                    latest = state.log_history[-1]
                    self.job.metrics.loss = latest.get("loss", self.job.metrics.loss)
                    self.job.metrics.learning_rate = latest.get("learning_rate", 0.0)

                # Emit every 10 steps
                if state.global_step % 10 == 0:
                    self._emit("step_update")

            def on_epoch_end(self, args, state, control, **kwargs):
                self.job.metrics.epoch = int(state.epoch) if state.epoch else 0
                self._emit("epoch_completed")

            def on_evaluate(self, args, state, control, metrics=None, **kwargs):
                if metrics:
                    self.job.metrics.validation_loss = metrics.get("eval_loss", 0.0)
                    self.job.metrics.validation_accuracy = metrics.get(
                        "eval_accuracy", 0.0
                    )
                    self.job.metrics.f1_score = metrics.get("eval_f1", 0.0)
                    self.job.metrics.precision = metrics.get("eval_precision", 0.0)
                    self.job.metrics.recall = metrics.get("eval_recall", 0.0)
                self._emit("evaluation_completed")

            def _emit(self, event_type: str):
                event = {
                    "type": event_type,
                    "timestamp": datetime.utcnow().isoformat(),
                    "job_id": self.job.job_id,
                    "progress": round(self.job.progress, 2),
                    "state": self.job.state.value,
                    "metrics": {
                        "epoch": self.job.metrics.epoch,
                        "step": self.job.metrics.step,
                        "loss": round(self.job.metrics.loss, 4)
                        if self.job.metrics.loss
                        else 0,
                        "accuracy": round(self.job.metrics.accuracy, 4)
                        if self.job.metrics.accuracy
                        else 0,
                        "validation_loss": round(self.job.metrics.validation_loss, 4)
                        if self.job.metrics.validation_loss
                        else 0,
                        "validation_accuracy": round(
                            self.job.metrics.validation_accuracy, 4
                        )
                        if self.job.metrics.validation_accuracy
                        else 0,
                        "f1_score": round(self.job.metrics.f1_score, 4)
                        if self.job.metrics.f1_score
                        else 0,
                        "learning_rate": self.job.metrics.learning_rate,
                    },
                }
                self.job.event_queue.append(event)

        # Create Trainer
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset["train"],
            eval_dataset=tokenized_dataset.get("validation"),
            tokenizer=tokenizer,
            compute_metrics=compute_metrics
            if "validation" in tokenized_dataset
            else None,
            callbacks=[
                ProgressCallback(job),
                EarlyStoppingCallback(early_stopping_patience=3)
                if "validation" in tokenized_dataset
                else None,
            ],
        )

        # Filter None callbacks
        trainer.callback_handler.callbacks = [
            cb for cb in trainer.callback_handler.callbacks if cb is not None
        ]

        log.info("Starting training", job_id=job.job_id)

        # Train
        trainer.train()

        # Save final model
        final_model_path = output_dir / "final"
        trainer.save_model(str(final_model_path))
        tokenizer.save_pretrained(str(final_model_path))

        # Save training info
        training_info = {
            "job_id": job.job_id,
            "model_name": job.model_name,
            "model_type": job.model_type.value,
            "base_model": base_model,
            "final_metrics": asdict(job.metrics),
            "hyperparameters": job.hyperparameters,
            "completed_at": datetime.utcnow().isoformat(),
        }

        with open(final_model_path / "training_info.json", "w") as f:
            json.dump(training_info, f, indent=2)

        job.model_path = str(final_model_path)
        job.state = JobState.COMPLETED
        job.completed_at = datetime.utcnow().isoformat()
        job.progress = 100.0

        # Final metrics from evaluation
        if "validation" in tokenized_dataset:
            final_metrics = trainer.evaluate()
            job.metrics.accuracy = final_metrics.get("eval_accuracy", 0.0)
            job.metrics.f1_score = final_metrics.get("eval_f1", 0.0)

        log.info(
            "Training completed",
            job_id=job.job_id,
            model_path=job.model_path,
            final_accuracy=job.metrics.accuracy,
        )

        # Emit completion event
        job.event_queue.append(
            {
                "type": "training_completed",
                "timestamp": datetime.utcnow().isoformat(),
                "job_id": job.job_id,
                "progress": 100.0,
                "state": JobState.COMPLETED.value,
                "model_path": job.model_path,
                "metrics": asdict(job.metrics),
            }
        )

    except Exception as e:
        job.state = JobState.FAILED
        job.error_message = str(e)
        job.completed_at = datetime.utcnow().isoformat()
        log.error("Training failed", job_id=job.job_id, error=str(e), exc_info=True)

        # Emit failure event
        job.event_queue.append(
            {
                "type": "training_failed",
                "timestamp": datetime.utcnow().isoformat(),
                "job_id": job.job_id,
                "state": JobState.FAILED.value,
                "error": str(e),
            }
        )


async def run_training_with_persistence(job: TrainingJob):
    """
    Wrapper around run_training that persists state changes to Redis.
    """
    try:
        # Save initial state
        if state_store:
            await state_store.save_job(job.job_id, job)

        # Run the actual training
        await run_training(job)

    finally:
        # Always save final state (success or failure)
        if state_store:
            await state_store.save_job(job.job_id, job)
            log.info("Job state persisted", job_id=job.job_id, state=job.state.value)


# =============================================================================
# FastAPI Application
# =============================================================================


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan manager."""
    global state_store

    log.info(
        "ML Training Service starting",
        model_dir=str(MODEL_DIR),
        dataset_dir=str(DATASET_DIR),
        max_concurrent_jobs=MAX_CONCURRENT_JOBS,
    )

    # Initialize state store
    from state_store import get_state_store

    state_store = await get_state_store()

    # Load existing jobs from Redis into memory
    if state_store.is_redis_connected:
        log.info("Redis connected, loading persisted jobs")
        memory_store = state_store.get_memory_store()
        with jobs_lock:
            for job_id, job_data in memory_store.items():
                # Reconstruct TrainingJob from stored data
                try:
                    job = TrainingJob(
                        job_id=job_data.get("job_id", job_id),
                        model_name=job_data.get("model_name", "unknown"),
                        model_type=ModelType(job_data.get("model_type", "sentiment")),
                        dataset_path=job_data.get("dataset_path", ""),
                        dataset_format=job_data.get("dataset_format", "csv"),
                        base_model=job_data.get("base_model"),
                        max_epochs=job_data.get("max_epochs", 3),
                        validation_split=job_data.get("validation_split", 0.1),
                        hyperparameters=job_data.get("hyperparameters", {}),
                        callbacks=job_data.get("callbacks", {}),
                        metadata=job_data.get("metadata", {}),
                        state=JobState(job_data.get("state", "PENDING")),
                        progress=job_data.get("progress", 0.0),
                        error_message=job_data.get("error_message"),
                        model_path=job_data.get("model_path"),
                        created_at=job_data.get(
                            "created_at", datetime.utcnow().isoformat()
                        ),
                        started_at=job_data.get("started_at"),
                        completed_at=job_data.get("completed_at"),
                    )
                    # Restore metrics
                    if "metrics" in job_data:
                        m = job_data["metrics"]
                        job.metrics = TrainingMetrics(
                            epoch=m.get("epoch", 0),
                            total_epochs=m.get("total_epochs", 0),
                            step=m.get("step", 0),
                            total_steps=m.get("total_steps", 0),
                            loss=m.get("loss", 0.0),
                            accuracy=m.get("accuracy", 0.0),
                            validation_loss=m.get("validation_loss", 0.0),
                            validation_accuracy=m.get("validation_accuracy", 0.0),
                            learning_rate=m.get("learning_rate", 0.0),
                            samples_processed=m.get("samples_processed", 0),
                            total_samples=m.get("total_samples", 0),
                            f1_score=m.get("f1_score", 0.0),
                            precision=m.get("precision", 0.0),
                            recall=m.get("recall", 0.0),
                        )
                    jobs[job_id] = job
                    log.info(
                        "Loaded persisted job", job_id=job_id, state=job.state.value
                    )
                except Exception as e:
                    log.warning("Failed to restore job", job_id=job_id, error=str(e))
    else:
        log.warning(
            "Redis not available, using in-memory storage only (data will be lost on restart)"
        )

    yield

    # Cleanup: save all jobs before shutdown
    if state_store:
        log.info("Saving jobs before shutdown")
        with jobs_lock:
            for job_id, job in jobs.items():
                await state_store.save_job(job_id, job)
        await state_store.disconnect()

    log.info("ML Training Service shutting down")


app = FastAPI(
    title="ML Training Service",
    description="Production ML model training service for sentiment analysis, ABSA, NER, and more",
    version="2.0.0",
    lifespan=lifespan,
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint for service discovery."""
    try:
        import torch

        gpu_available = torch.cuda.is_available()
    except ImportError:
        gpu_available = False

    with jobs_lock:
        active_jobs = sum(
            1
            for j in jobs.values()
            if j.state in [JobState.RUNNING, JobState.INITIALIZING]
        )

    return HealthResponse(
        status="healthy",
        version="2.0.0",
        gpu_available=gpu_available,
        active_jobs=active_jobs,
        supported_model_types=[t.value for t in ModelType],
        max_concurrent_jobs=MAX_CONCURRENT_JOBS,
        redis_connected=state_store.is_redis_connected if state_store else False,
        persisted_jobs=len(state_store.get_memory_store()) if state_store else 0,
    )


@app.post("/train", response_model=TrainingResponse)
async def submit_training_job(
    request: TrainingRequest, background_tasks: BackgroundTasks
):
    """Submit a new training job."""
    # Check concurrent job limit
    with jobs_lock:
        active_jobs = sum(
            1
            for j in jobs.values()
            if j.state in [JobState.RUNNING, JobState.INITIALIZING, JobState.PENDING]
        )
        if active_jobs >= MAX_CONCURRENT_JOBS:
            raise HTTPException(
                status_code=429,
                detail=f"Maximum concurrent jobs ({MAX_CONCURRENT_JOBS}) reached. Please wait.",
            )

    job_id = str(uuid.uuid4())

    try:
        model_type = ModelType(request.model_type.lower())
    except ValueError:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid model type: {request.model_type}. "
            f"Supported types: {[t.value for t in ModelType]}",
        )

    job = TrainingJob(
        job_id=job_id,
        model_name=request.model_name,
        model_type=model_type,
        dataset_path=request.dataset_path,
        dataset_format=request.dataset_format,
        base_model=request.base_model,
        max_epochs=request.max_epochs,
        validation_split=request.validation_split,
        hyperparameters=request.hyperparameters,
        callbacks=request.callbacks,
        metadata=request.metadata,
    )

    with jobs_lock:
        jobs[job_id] = job

    # Persist job to Redis
    if state_store:
        await state_store.save_job(job_id, job)

    # Start training in background - ALWAYS use real training
    background_tasks.add_task(run_training_with_persistence, job)

    log.info("Training job submitted", job_id=job_id, model_name=request.model_name)

    return TrainingResponse(
        job_id=job_id,
        model_name=job.model_name,
        model_type=job.model_type.value,
        state=job.state.value,
        progress=job.progress,
        created_at=job.created_at,
        message="Training job submitted successfully",
    )


@app.get("/jobs/{job_id}/status", response_model=JobStatusResponse)
async def get_job_status(job_id: str):
    """Get the status of a training job."""
    with jobs_lock:
        if job_id not in jobs:
            raise HTTPException(status_code=404, detail=f"Job not found: {job_id}")
        job = jobs[job_id]

    return JobStatusResponse(
        job_id=job.job_id,
        model_name=job.model_name,
        model_type=job.model_type.value,
        state=job.state.value,
        progress=round(job.progress, 2),
        metrics=asdict(job.metrics),
        error_message=job.error_message,
        model_path=job.model_path,
        created_at=job.created_at,
        started_at=job.started_at,
        completed_at=job.completed_at,
        current_epoch=job.metrics.epoch,
        total_epochs=job.metrics.total_epochs,
    )


@app.get("/jobs/{job_id}/stream")
async def stream_job_status(job_id: str):
    """
    SSE endpoint for real-time training progress updates.
    """
    with jobs_lock:
        if job_id not in jobs:
            raise HTTPException(status_code=404, detail=f"Job not found: {job_id}")

    async def event_generator() -> AsyncGenerator[str, None]:
        last_sent_index = 0

        while True:
            with jobs_lock:
                if job_id not in jobs:
                    break
                job = jobs[job_id]

                # Send any new events
                events = list(job.event_queue)
                new_events = events[last_sent_index:]
                last_sent_index = len(events)

            for event in new_events:
                yield f"data: {json.dumps(event)}\n\n"

            # Check if job is done
            if job.state in [JobState.COMPLETED, JobState.FAILED, JobState.CANCELLED]:
                # Send final status
                final_event = {
                    "type": "job_finished",
                    "job_id": job_id,
                    "state": job.state.value,
                    "progress": job.progress,
                    "model_path": job.model_path,
                    "error_message": job.error_message,
                }
                yield f"data: {json.dumps(final_event)}\n\n"
                break

            await asyncio.sleep(1)  # Poll interval

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )


@app.post("/jobs/{job_id}/cancel")
async def cancel_job(job_id: str):
    """Cancel a training job."""
    with jobs_lock:
        if job_id not in jobs:
            raise HTTPException(status_code=404, detail=f"Job not found: {job_id}")

        job = jobs[job_id]

        if job.state not in [JobState.PENDING, JobState.RUNNING, JobState.INITIALIZING]:
            raise HTTPException(
                status_code=400, detail=f"Cannot cancel job in state: {job.state.value}"
            )

        job.state = JobState.CANCELLED
        job.completed_at = datetime.utcnow().isoformat()

    log.info("Training job cancelled", job_id=job_id)

    return {"success": True, "message": "Job cancelled successfully"}


@app.get("/jobs/{job_id}/artifact", response_model=ModelArtifactResponse)
async def get_model_artifact(job_id: str):
    """Get the trained model artifact information."""
    with jobs_lock:
        if job_id not in jobs:
            raise HTTPException(status_code=404, detail=f"Job not found: {job_id}")
        job = jobs[job_id]

    if job.state != JobState.COMPLETED:
        raise HTTPException(
            status_code=400, detail=f"Model not available. Job state: {job.state.value}"
        )

    if not job.model_path or not Path(str(job.model_path)).exists():
        raise HTTPException(status_code=404, detail="Model artifact not found")

    # Calculate file size and checksum
    model_path = Path(job.model_path)

    # Get total size of all files in directory
    size_bytes = sum(f.stat().st_size for f in model_path.rglob("*") if f.is_file())

    # Checksum of the config file
    config_file = model_path / "config.json"
    if config_file.exists():
        with open(config_file, "rb") as f:
            checksum = hashlib.sha256(f.read()).hexdigest()
    else:
        checksum = "directory"

    return ModelArtifactResponse(
        model_path=str(job.model_path),
        model_name=job.model_name,
        model_type=job.model_type.value,
        framework="pytorch",
        version="1.0.0",
        size_bytes=size_bytes,
        checksum=checksum,
        metrics={
            "accuracy": job.metrics.accuracy,
            "f1_score": job.metrics.f1_score,
            "precision": job.metrics.precision,
            "recall": job.metrics.recall,
            "final_loss": job.metrics.loss,
        },
        model_filename=f"{job.model_name}_{job.job_id[:8]}",
    )


@app.get("/jobs/{job_id}/model/meta")
async def get_model_metadata(job_id: str):
    """Get model metadata for download."""
    with jobs_lock:
        if job_id not in jobs:
            raise HTTPException(status_code=404, detail=f"Job not found: {job_id}")
        job = jobs[job_id]

    if job.state != JobState.COMPLETED:
        raise HTTPException(status_code=400, detail="Model not ready")

    model_path = Path(job.model_path)
    size_bytes = sum(f.stat().st_size for f in model_path.rglob("*") if f.is_file())

    return {
        "model_name": job.model_name,
        "model_type": job.model_type.value,
        "model_filename": f"{job.model_name}_{job.job_id[:8]}",
        "format": "huggingface",
        "size_bytes": size_bytes,
        "metrics": asdict(job.metrics),
        "checksum": "directory",
        "version": "1.0.0",
    }


@app.get("/jobs")
async def list_jobs(
    state: Optional[str] = None, model_type: Optional[str] = None, limit: int = 50
):
    """List training jobs with optional filtering."""
    result = []

    with jobs_lock:
        for job in list(jobs.values())[-limit:]:
            if state and job.state.value.lower() != state.lower():
                continue
            if model_type and job.model_type.value.lower() != model_type.lower():
                continue

            result.append(
                {
                    "job_id": job.job_id,
                    "model_name": job.model_name,
                    "model_type": job.model_type.value,
                    "state": job.state.value,
                    "progress": round(job.progress, 2),
                    "created_at": job.created_at,
                    "completed_at": job.completed_at,
                }
            )

    return {"jobs": result, "total": len(result)}


@app.get("/models")
async def list_models():
    """List available trained models."""
    models = []

    with jobs_lock:
        for job in jobs.values():
            if job.state == JobState.COMPLETED and job.model_path:
                models.append(
                    {
                        "job_id": job.job_id,
                        "model_name": job.model_name,
                        "model_type": job.model_type.value,
                        "model_path": job.model_path,
                        "accuracy": job.metrics.accuracy,
                        "f1_score": job.metrics.f1_score,
                        "completed_at": job.completed_at,
                    }
                )

    return {"models": models, "total": len(models)}


@app.get("/supported-types")
async def get_supported_types():
    """Get list of supported model types with their default configurations."""
    return {
        "types": [
            {
                "type": t.value,
                "default_base_model": get_default_base_model(t),
                "description": get_model_type_description(t),
            }
            for t in ModelType
        ]
    }


def get_model_type_description(model_type: ModelType) -> str:
    """Returns description for each model type."""
    descriptions = {
        ModelType.SENTIMENT: "Binary or multi-class sentiment classification",
        ModelType.ABSA: "Aspect-Based Sentiment Analysis for fine-grained opinion mining",
        ModelType.NER: "Named Entity Recognition for extracting entities from text",
        ModelType.CLASSIFICATION: "General text classification for custom categories",
        ModelType.EMBEDDING: "Text embedding models for semantic similarity",
        ModelType.TRANSFORMER: "Custom transformer models for various NLP tasks",
    }
    return descriptions.get(model_type, "")


# =============================================================================
# Inference API
# =============================================================================


# Global model cache for inference
_model_cache: dict[str, tuple] = {}  # job_id -> (tokenizer, model)
_model_cache_lock = threading.Lock()


class InferenceRequest(BaseModel):
    """Request for model inference."""

    text: str = Field(
        ..., min_length=1, max_length=10000, description="Text to analyze"
    )
    texts: Optional[list[str]] = Field(
        None, description="Batch of texts (alternative to single text)"
    )
    aspects: Optional[list[str]] = Field(
        None, description="Aspects to analyze (for ABSA)"
    )
    max_length: int = Field(512, ge=32, le=2048, description="Maximum token length")
    return_probabilities: bool = Field(True, description="Return class probabilities")


class AspectSentiment(BaseModel):
    """Aspect sentiment result."""

    sentimentScore: float = Field(
        ..., ge=-1.0, le=1.0, description="Sentiment score -1 to 1"
    )
    sentimentLabel: str = Field(..., description="positive/negative/neutral")
    confidence: float = Field(..., ge=0.0, le=1.0, description="Confidence score")


class OverallSentiment(BaseModel):
    """Overall sentiment result."""

    score: float = Field(..., ge=-1.0, le=1.0)
    label: str


class InferenceResponse(BaseModel):
    """Response from model inference."""

    analysisId: str
    contentId: Optional[str] = None
    textPreview: str
    aspectsAnalyzed: list[str]
    aspectSentiments: dict[str, AspectSentiment]
    overallSentiment: OverallSentiment
    confidence: float
    analyzedAt: str
    modelUsed: str
    modelType: str
    responseTimeMs: int


def load_model_for_inference(job_id: str) -> tuple:
    """
    Load a trained model for inference with caching.
    Returns (tokenizer, model) tuple.
    """
    with _model_cache_lock:
        if job_id in _model_cache:
            return _model_cache[job_id]

    # Get job info
    with jobs_lock:
        if job_id not in jobs:
            raise HTTPException(status_code=404, detail=f"Job not found: {job_id}")
        job = jobs[job_id]

    if job.state != JobState.COMPLETED:
        raise HTTPException(
            status_code=400, detail=f"Model not available. Job state: {job.state.value}"
        )

    if not job.model_path or not Path(job.model_path).exists():
        raise HTTPException(status_code=404, detail="Model artifact not found")

    try:
        from transformers import AutoTokenizer, AutoModelForSequenceClassification
        import torch

        model_path = job.model_path
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForSequenceClassification.from_pretrained(model_path)

        # Move to GPU if available
        device = "cuda" if USE_GPU and torch.cuda.is_available() else "cpu"
        model = model.to(device)
        model.eval()

        with _model_cache_lock:
            _model_cache[job_id] = (tokenizer, model, device)

        log.info("Model loaded for inference", job_id=job_id, device=device)
        return tokenizer, model, device

    except Exception as e:
        log.error("Failed to load model", job_id=job_id, error=str(e))
        raise HTTPException(status_code=500, detail=f"Failed to load model: {str(e)}")


def compute_sentiment_label(score: float) -> str:
    """Convert sentiment score to label."""
    if score > 0.1:
        return "positive"
    elif score < -0.1:
        return "negative"
    return "neutral"


@app.post("/inference/{job_id}", response_model=InferenceResponse)
async def run_inference(job_id: str, request: InferenceRequest):
    """
    Run inference using a trained model.

    For ABSA models, analyzes sentiment for each aspect.
    For sentiment/classification models, returns overall sentiment.
    """
    import time
    import torch

    start_time = time.time()

    # Load model
    tokenizer, model, device = load_model_for_inference(job_id)

    # Get job info for model type
    with jobs_lock:
        job = jobs[job_id]

    texts = request.texts or [request.text]
    aspects = request.aspects or []

    # For ABSA: if no aspects provided, analyze full text
    if not aspects:
        aspects = ["전체"]  # Default aspect

    aspect_sentiments: dict[str, AspectSentiment] = {}
    all_scores = []

    try:
        for aspect in aspects:
            # For ABSA, combine aspect with text
            if job.model_type == ModelType.ABSA and aspect != "전체":
                input_text = f"[{aspect}] {texts[0]}"
            else:
                input_text = texts[0]

            # Tokenize
            inputs = tokenizer(
                input_text,
                return_tensors="pt",
                truncation=True,
                max_length=request.max_length,
                padding=True,
            )
            inputs = {k: v.to(device) for k, v in inputs.items()}

            # Inference
            with torch.no_grad():
                outputs = model(**inputs)

            logits = outputs.logits
            probabilities = torch.softmax(logits, dim=-1).cpu().numpy()[0]

            # Determine sentiment (assuming binary or 3-class classification)
            num_classes = len(probabilities)

            if num_classes == 2:
                # Binary: [negative, positive]
                sentiment_score = float(probabilities[1] - probabilities[0])
                confidence = float(max(probabilities))
            elif num_classes == 3:
                # 3-class: [negative, neutral, positive]
                sentiment_score = float(probabilities[2] - probabilities[0])
                confidence = float(max(probabilities))
            else:
                # Multi-class: use argmax
                predicted_class = int(probabilities.argmax())
                sentiment_score = (
                    float(2 * (predicted_class / (num_classes - 1)) - 1)
                    if num_classes > 1
                    else 0.0
                )
                confidence = float(probabilities[predicted_class])

            all_scores.append(sentiment_score)

            aspect_sentiments[aspect] = AspectSentiment(
                sentimentScore=round(sentiment_score, 4),
                sentimentLabel=compute_sentiment_label(sentiment_score),
                confidence=round(confidence, 4),
            )

        # Compute overall sentiment (average of all aspects)
        overall_score = sum(all_scores) / len(all_scores) if all_scores else 0.0
        overall_confidence = (
            sum(a.confidence for a in aspect_sentiments.values())
            / len(aspect_sentiments)
            if aspect_sentiments
            else 0.0
        )

        response_time_ms = int((time.time() - start_time) * 1000)

        return InferenceResponse(
            analysisId=str(uuid.uuid4()),
            contentId=None,
            textPreview=texts[0][:200] + "..." if len(texts[0]) > 200 else texts[0],
            aspectsAnalyzed=list(aspect_sentiments.keys()),
            aspectSentiments=aspect_sentiments,
            overallSentiment=OverallSentiment(
                score=round(overall_score, 4),
                label=compute_sentiment_label(overall_score),
            ),
            confidence=round(overall_confidence, 4),
            analyzedAt=datetime.utcnow().isoformat(),
            modelUsed=job.model_name,
            modelType=job.model_type.value,
            responseTimeMs=response_time_ms,
        )

    except Exception as e:
        log.error("Inference failed", job_id=job_id, error=str(e), exc_info=True)
        raise HTTPException(status_code=500, detail=f"Inference failed: {str(e)}")


@app.post("/inference/by-name/{model_name}", response_model=InferenceResponse)
async def run_inference_by_name(model_name: str, request: InferenceRequest):
    """
    Run inference using a model by name (uses latest completed model with that name).
    """
    # Find the latest completed job with this model name
    job_id = None
    latest_completed_at = None

    with jobs_lock:
        for job in jobs.values():
            if (
                job.model_name == model_name
                and job.state == JobState.COMPLETED
                and job.model_path
            ):
                if latest_completed_at is None or (
                    job.completed_at and job.completed_at > latest_completed_at
                ):
                    job_id = job.job_id
                    latest_completed_at = job.completed_at

    if not job_id:
        raise HTTPException(
            status_code=404, detail=f"No completed model found with name: {model_name}"
        )

    return await run_inference(job_id, request)


@app.delete("/inference/cache/{job_id}")
async def clear_model_cache(job_id: str):
    """Clear a model from the inference cache to free memory."""
    with _model_cache_lock:
        if job_id in _model_cache:
            del _model_cache[job_id]
            return {"success": True, "message": f"Model {job_id} removed from cache"}
    return {"success": False, "message": "Model not in cache"}


@app.get("/inference/cache/status")
async def get_cache_status():
    """Get inference cache status."""
    with _model_cache_lock:
        cached_models = list(_model_cache.keys())
    return {
        "cached_models": cached_models,
        "total_cached": len(cached_models),
    }


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8090)

```

---

## backend/ml-addons/ml-trainer/state_store.py

```py
"""
State Storage Module for ML Trainer Service

Provides persistent storage for training jobs using Redis.
Falls back to in-memory storage if Redis is unavailable.

Usage:
    store = StateStore()
    await store.connect()
    await store.save_job(job_id, job_data)
    job = await store.load_job(job_id)
"""

import os
import json
import asyncio
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, List
from dataclasses import asdict
from enum import Enum

import structlog

log = structlog.get_logger()

# Redis configuration from environment
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")
REDIS_PREFIX = os.getenv("REDIS_PREFIX", "ml_trainer")
JOB_TTL_DAYS = int(os.getenv("JOB_TTL_DAYS", "30"))  # Jobs expire after 30 days


class EnhancedJSONEncoder(json.JSONEncoder):
    """JSON encoder that handles dataclasses, enums, and datetime."""
    
    def default(self, o: Any) -> Any:
        if hasattr(o, '__dataclass_fields__'):
            return asdict(o)
        if isinstance(o, Enum):
            return o.value
        if isinstance(o, datetime):
            return o.isoformat()
        if hasattr(o, 'maxlen'):  # deque
            return list(o)
        return super().default(o)


class StateStore:
    """
    Persistent state storage with Redis backend.
    Falls back to in-memory storage if Redis is unavailable.
    """
    
    def __init__(self):
        self._redis = None
        self._memory_store: Dict[str, Any] = {}
        self._using_redis = False
        self._lock = asyncio.Lock()
    
    async def connect(self) -> bool:
        """
        Connect to Redis. Returns True if connected, False if falling back to memory.
        """
        try:
            import redis.asyncio as redis
            
            self._redis = redis.from_url(
                REDIS_URL,
                encoding="utf-8",
                decode_responses=True,
                socket_connect_timeout=5,
                socket_timeout=5,
            )
            
            # Test connection
            await self._redis.ping()
            self._using_redis = True
            log.info("Connected to Redis", url=REDIS_URL, prefix=REDIS_PREFIX)
            
            # Load existing jobs from Redis into memory cache
            await self._load_existing_jobs()
            
            return True
            
        except ImportError:
            log.warning("redis package not installed, using in-memory storage")
            self._using_redis = False
            return False
            
        except Exception as e:
            log.warning("Failed to connect to Redis, using in-memory storage", error=str(e))
            self._using_redis = False
            return False
    
    async def disconnect(self):
        """Close Redis connection."""
        if self._redis:
            await self._redis.close()
            self._redis = None
            self._using_redis = False
            log.info("Disconnected from Redis")
    
    async def _load_existing_jobs(self):
        """Load existing jobs from Redis into memory on startup."""
        if not self._using_redis:
            return
        
        try:
            pattern = f"{REDIS_PREFIX}:job:*"
            cursor = 0
            count = 0
            
            while True:
                if self._redis is None:
                    break
                cursor, keys = await self._redis.scan(cursor, match=pattern, count=100)
                for key in keys:
                    job_id = key.split(":")[-1]
                    job_data = await self._redis.get(key)
                    if job_data:
                        self._memory_store[job_id] = json.loads(job_data)
                        count += 1
                
                if cursor == 0:
                    break
            
            log.info("Loaded existing jobs from Redis", count=count)
            
        except Exception as e:
            log.error("Failed to load existing jobs from Redis", error=str(e))
    
    def _key(self, job_id: str) -> str:
        """Generate Redis key for a job."""
        return f"{REDIS_PREFIX}:job:{job_id}"
    
    async def save_job(self, job_id: str, job: Any) -> bool:
        """
        Save a training job to storage.
        
        Args:
            job_id: Unique job identifier
            job: TrainingJob dataclass or dict
            
        Returns:
            True if saved successfully
        """
        async with self._lock:
            try:
                # Convert to dict if needed
                if hasattr(job, '__dataclass_fields__'):
                    job_data = asdict(job)
                    # Handle non-serializable fields
                    job_data.pop('event_queue', None)  # Remove deque
                elif isinstance(job, dict):
                    job_data = job.copy()
                    job_data.pop('event_queue', None)
                else:
                    job_data = job
                
                job_json = json.dumps(job_data, cls=EnhancedJSONEncoder)
                
                # Save to memory (always)
                self._memory_store[job_id] = json.loads(job_json)
                
                # Save to Redis if available
                if self._using_redis and self._redis:
                    ttl_seconds = JOB_TTL_DAYS * 24 * 60 * 60
                    await self._redis.set(self._key(job_id), job_json, ex=ttl_seconds)
                
                return True
                
            except Exception as e:
                log.error("Failed to save job", job_id=job_id, error=str(e))
                return False
    
    async def load_job(self, job_id: str) -> Optional[Dict[str, Any]]:
        """
        Load a training job from storage.
        
        Args:
            job_id: Unique job identifier
            
        Returns:
            Job data as dict, or None if not found
        """
        # Check memory first
        if job_id in self._memory_store:
            return self._memory_store[job_id]
        
        # Try Redis
        if self._using_redis and self._redis:
            try:
                job_json = await self._redis.get(self._key(job_id))
                if job_json:
                    job_data = json.loads(job_json)
                    self._memory_store[job_id] = job_data
                    return job_data
            except Exception as e:
                log.error("Failed to load job from Redis", job_id=job_id, error=str(e))
        
        return None
    
    async def delete_job(self, job_id: str) -> bool:
        """
        Delete a training job from storage.
        
        Args:
            job_id: Unique job identifier
            
        Returns:
            True if deleted successfully
        """
        async with self._lock:
            try:
                # Remove from memory
                self._memory_store.pop(job_id, None)
                
                # Remove from Redis
                if self._using_redis and self._redis:
                    await self._redis.delete(self._key(job_id))
                
                return True
                
            except Exception as e:
                log.error("Failed to delete job", job_id=job_id, error=str(e))
                return False
    
    async def list_jobs(
        self,
        state: Optional[str] = None,
        model_type: Optional[str] = None,
        limit: int = 50
    ) -> List[Dict[str, Any]]:
        """
        List jobs with optional filtering.
        
        Args:
            state: Filter by job state
            model_type: Filter by model type
            limit: Maximum number of jobs to return
            
        Returns:
            List of job data dicts
        """
        jobs = []
        
        for job_id, job_data in list(self._memory_store.items())[-limit:]:
            if state and job_data.get('state', '').lower() != state.lower():
                continue
            if model_type and job_data.get('model_type', '').lower() != model_type.lower():
                continue
            jobs.append(job_data)
        
        return jobs
    
    async def update_job_status(
        self,
        job_id: str,
        state: str,
        progress: Optional[float] = None,
        error_message: Optional[str] = None,
        completed_at: Optional[str] = None,
        **kwargs: Any
    ) -> bool:
        """
        Update specific fields of a job.
        
        Args:
            job_id: Unique job identifier
            state: New job state
            progress: Current progress percentage
            error_message: Error message if failed
            completed_at: Completion timestamp
            **kwargs: Additional fields to update
            
        Returns:
            True if updated successfully
        """
        job_data = await self.load_job(job_id)
        if not job_data:
            return False
        
        job_data['state'] = state
        if progress is not None:
            job_data['progress'] = progress
        if error_message is not None:
            job_data['error_message'] = error_message
        if completed_at is not None:
            job_data['completed_at'] = completed_at
        
        # Update additional fields
        for key, value in kwargs.items():
            if value is not None:
                job_data[key] = value
        
        return await self.save_job(job_id, job_data)
    
    async def get_active_job_count(self) -> int:
        """Get count of active (pending, running, initializing) jobs."""
        count = 0
        active_states = {'PENDING', 'RUNNING', 'INITIALIZING'}
        
        for job_data in self._memory_store.values():
            if job_data.get('state', '').upper() in active_states:
                count += 1
        
        return count
    
    async def get_completed_models(self) -> List[Dict[str, Any]]:
        """Get list of completed jobs with model paths."""
        models = []
        
        for job_data in self._memory_store.values():
            if job_data.get('state') == 'COMPLETED' and job_data.get('model_path'):
                models.append({
                    'job_id': job_data.get('job_id'),
                    'model_name': job_data.get('model_name'),
                    'model_type': job_data.get('model_type'),
                    'model_path': job_data.get('model_path'),
                    'metrics': job_data.get('metrics', {}),
                    'completed_at': job_data.get('completed_at'),
                })
        
        return models
    
    @property
    def is_redis_connected(self) -> bool:
        """Check if Redis is connected."""
        return self._using_redis
    
    def get_memory_store(self) -> Dict[str, Any]:
        """Get reference to memory store for direct access (e.g., for event_queue)."""
        return self._memory_store


# Singleton instance
_store: Optional[StateStore] = None


async def get_state_store() -> StateStore:
    """Get or create the singleton StateStore instance."""
    global _store
    if _store is None:
        _store = StateStore()
        await _store.connect()
    return _store

```

---

## backend/ml-addons/sentiment-addon/addon_server.py

```py
"""
ML Add-on Server: Korean Sentiment Analysis with ML Models

NewsInsight ML Add-on 시스템의 감정 분석 구현.
KoELECTRA/KoBERT 기반 ML 모델을 사용하여 한국어 뉴스 기사의
감정(긍정/부정/중립)을 정확하게 분석합니다.

Features:
- KoELECTRA 기반 3-class 감정 분류
- 세부 감정(기쁨, 분노, 슬픔, 두려움 등) 분석
- 키워드 기반 폴백 모드 지원
- 배치 처리 지원
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import Response
from pydantic import BaseModel, Field
from typing import Optional, Dict, List, Any
import time
import os
import logging
import asyncio
from contextlib import asynccontextmanager
from functools import lru_cache

# Prometheus metrics
try:
    from prometheus_client import (
        Counter,
        Histogram,
        Gauge,
        CONTENT_TYPE_LATEST,
        generate_latest,
    )

    PROMETHEUS_AVAILABLE = True
except ImportError:
    PROMETHEUS_AVAILABLE = False

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ML Model cache (lazy loading)
_ml_models = {}
_model_loading = False


def get_ml_models():
    """Lazy load ML models on first use"""
    global _ml_models, _model_loading

    if _ml_models.get("loaded") or _model_loading:
        return _ml_models

    _model_loading = True

    try:
        import torch
        from transformers import (
            AutoTokenizer,
            AutoModelForSequenceClassification,
            pipeline,
        )

        logger.info("Loading sentiment ML models...")

        # Device selection
        device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"Using device: {device}")

        # 1. Primary Sentiment Model (KoELECTRA fine-tuned for sentiment)
        # Use Korean sentiment models
        sentiment_model_name = os.getenv(
            "SENTIMENT_MODEL",
            "snunlp/KR-FinBert-SC",  # Korean sentiment classifier
        )

        try:
            _ml_models["sentiment_pipeline"] = pipeline(
                "sentiment-analysis",
                model=sentiment_model_name,
                tokenizer=sentiment_model_name,
                device=0 if device == "cuda" else -1,
                max_length=512,
                truncation=True,
            )
            logger.info(f"Loaded primary sentiment model: {sentiment_model_name}")
        except Exception as e:
            logger.warning(f"Failed to load primary sentiment model: {e}")
            # Fallback to base KoELECTRA
            try:
                fallback_model = "monologg/koelectra-base-v3-discriminator"
                _ml_models["sentiment_tokenizer"] = AutoTokenizer.from_pretrained(
                    fallback_model
                )
                _ml_models["sentiment_model"] = (
                    AutoModelForSequenceClassification.from_pretrained(
                        fallback_model,
                        num_labels=3,  # positive, negative, neutral
                    ).to(device)
                )
                _ml_models["sentiment_model"].eval()
                logger.info(f"Loaded fallback sentiment model: {fallback_model}")
            except Exception as e2:
                logger.warning(f"Failed to load fallback model: {e2}")

        # 2. Emotion Classification Model (optional, for detailed emotions)
        emotion_model_name = os.getenv(
            "EMOTION_MODEL",
            "j-hartmann/emotion-english-distilroberta-base",  # Will work for general emotions
        )
        try:
            _ml_models["emotion_pipeline"] = pipeline(
                "text-classification",
                model=emotion_model_name,
                tokenizer=emotion_model_name,
                device=0 if device == "cuda" else -1,
                top_k=None,  # Return all emotion scores
                max_length=512,
                truncation=True,
            )
            logger.info(f"Loaded emotion model: {emotion_model_name}")
        except Exception as e:
            logger.warning(f"Failed to load emotion model (optional): {e}")

        _ml_models["device"] = device
        _ml_models["loaded"] = True
        logger.info("All sentiment ML models loaded successfully")

    except ImportError as e:
        logger.warning(f"ML libraries not available, using heuristic mode: {e}")
        _ml_models["loaded"] = False
    except Exception as e:
        logger.error(f"Error loading ML models: {e}")
        _ml_models["loaded"] = False

    _model_loading = False
    return _ml_models


# Model loading status for health checks
_model_warmup_complete = False
_model_warmup_error = None


async def _warmup_models():
    """Warm up models and track completion status"""
    global _model_warmup_complete, _model_warmup_error
    try:
        logger.info("Starting sentiment model warm-up...")
        start_time = time.time()

        # Load models synchronously in thread
        models = await asyncio.to_thread(get_ml_models)

        if models.get("loaded"):
            # Run a dummy inference to fully warm up the model
            if "sentiment_pipeline" in models:
                try:
                    dummy_text = "테스트 문장입니다."
                    _ = models["sentiment_pipeline"](dummy_text)
                    logger.info("Sentiment pipeline warm-up inference complete")
                except Exception as e:
                    logger.warning(f"Sentiment pipeline warm-up failed: {e}")

            elapsed = time.time() - start_time
            logger.info(f"Sentiment model warm-up completed in {elapsed:.2f}s")
            _model_warmup_complete = True
        else:
            logger.warning("Models not loaded, running in heuristic mode")
            _model_warmup_complete = True

    except Exception as e:
        logger.error(f"Sentiment model warm-up failed: {e}")
        _model_warmup_error = str(e)
        _model_warmup_complete = True


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan handler"""
    # Startup
    logger.info("Sentiment addon starting...")

    # Preload models in background if ML is enabled
    if os.getenv("ENABLE_ML_MODELS", "true").lower() == "true":
        asyncio.create_task(_warmup_models())

    yield

    # Shutdown
    logger.info("Sentiment addon shutting down...")


app = FastAPI(
    title="Sentiment Analysis Add-on (ML Enhanced)",
    description="Korean news article sentiment analysis with ML models for NewsInsight",
    version="2.0.0",
    lifespan=lifespan,
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ========== Prometheus Metrics ==========

if PROMETHEUS_AVAILABLE:
    # Request metrics
    REQUEST_COUNT = Counter(
        "sentiment_requests_total",
        "Total number of sentiment analysis requests",
        ["endpoint", "status"],
    )
    REQUEST_LATENCY = Histogram(
        "sentiment_request_latency_seconds",
        "Request latency in seconds",
        ["endpoint"],
        buckets=(0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0),
    )

    # Analysis metrics
    ANALYSIS_COUNT = Counter(
        "sentiment_analysis_total",
        "Total number of sentiment analyses performed",
        ["label", "mode"],
    )

    # Model metrics
    MODEL_LOADED = Gauge(
        "sentiment_model_loaded", "Whether ML models are loaded (1=yes, 0=no)"
    )
    MODEL_WARMUP_COMPLETE = Gauge(
        "sentiment_model_warmup_complete",
        "Whether model warm-up is complete (1=yes, 0=no)",
    )

    # Error metrics
    ERROR_COUNT = Counter(
        "sentiment_errors_total", "Total number of errors", ["error_type"]
    )

    @app.get("/metrics")
    async def metrics():
        """Prometheus metrics endpoint"""
        models = get_ml_models() if not _model_loading else {}
        MODEL_LOADED.set(1 if models.get("loaded") else 0)
        MODEL_WARMUP_COMPLETE.set(1 if _model_warmup_complete else 0)

        return Response(content=generate_latest(), media_type=CONTENT_TYPE_LATEST)


# ========== Request/Response Models ==========


class ArticleInput(BaseModel):
    id: Optional[int] = None
    title: Optional[str] = None
    content: Optional[str] = None
    url: Optional[str] = None
    source: Optional[str] = None
    published_at: Optional[str] = None


class AnalysisContext(BaseModel):
    language: Optional[str] = "ko"
    country: Optional[str] = "KR"
    previous_results: Optional[Dict[str, Any]] = None


class ExecutionOptions(BaseModel):
    importance: Optional[str] = "batch"
    debug: Optional[bool] = False
    timeout_ms: Optional[int] = None
    use_ml: Optional[bool] = True
    include_emotions: Optional[bool] = True


class AddonRequest(BaseModel):
    request_id: str
    addon_id: str
    task: str = "article_analysis"
    input_schema_version: str = "1.0"
    article: Optional[ArticleInput] = None
    context: Optional[AnalysisContext] = None
    options: Optional[ExecutionOptions] = None


class EmotionScore(BaseModel):
    joy: float = 0.0
    anger: float = 0.0
    sadness: float = 0.0
    fear: float = 0.0
    surprise: float = 0.0
    disgust: float = 0.0


class SentimentResult(BaseModel):
    score: float = Field(
        ...,
        ge=-1.0,
        le=1.0,
        description="Sentiment score from -1 (negative) to 1 (positive)",
    )
    label: str = Field(
        ..., description="Sentiment label: positive, negative, or neutral"
    )
    confidence: float = Field(..., ge=0.0, le=1.0, description="Model confidence score")
    distribution: Dict[str, float] = Field(
        ..., description="Score distribution across sentiment classes"
    )
    emotions: Optional[EmotionScore] = None
    explanations: List[str] = []
    analysis_method: str = "ml"  # ml, heuristic, hybrid


class AnalysisResults(BaseModel):
    sentiment: Optional[SentimentResult] = None
    raw: Optional[Dict[str, Any]] = None


class ResponseMeta(BaseModel):
    model_version: str
    model_name: str
    latency_ms: int
    processed_at: str
    device: str = "cpu"


class ErrorInfo(BaseModel):
    code: str
    message: str
    details: Optional[str] = None


class AddonResponse(BaseModel):
    request_id: str
    addon_id: str
    status: str  # success, error, partial
    output_schema_version: str = "1.0"
    results: Optional[AnalysisResults] = None
    error: Optional[ErrorInfo] = None
    meta: Optional[ResponseMeta] = None


# ========== Keyword-based Fallback (Heuristic) ==========

# Korean positive keywords
POSITIVE_KEYWORDS = [
    "성공",
    "발전",
    "향상",
    "긍정",
    "좋은",
    "훌륭",
    "최고",
    "행복",
    "성장",
    "협력",
    "지원",
    "개선",
    "희망",
    "기대",
    "축하",
    "승리",
    "호황",
    "상승",
    "증가",
    "활성화",
    "혁신",
    "돌파",
    "기록",
    "최대",
    "회복",
    "안정",
    "확대",
    "강화",
    "달성",
    "우수",
    "선도",
    "획기적",
    "감사",
    "축복",
    "영광",
    "존경",
    "사랑",
    "평화",
    "화합",
    "번영",
]

# Korean negative keywords
NEGATIVE_KEYWORDS = [
    "실패",
    "문제",
    "위기",
    "부정",
    "나쁜",
    "최악",
    "우려",
    "불안",
    "감소",
    "하락",
    "갈등",
    "비판",
    "논란",
    "피해",
    "사고",
    "범죄",
    "폭락",
    "붕괴",
    "파산",
    "침체",
    "악화",
    "충격",
    "위험",
    "경고",
    "분쟁",
    "반발",
    "혼란",
    "지연",
    "중단",
    "취소",
    "실망",
    "좌절",
    "공포",
    "분노",
    "슬픔",
    "죽음",
    "재난",
    "폭력",
    "테러",
    "전쟁",
]


def analyze_sentiment_heuristic(text: str) -> SentimentResult:
    """
    Fallback keyword-based sentiment analysis.
    Used when ML models are not available.
    """
    if not text:
        return SentimentResult(
            score=0.0,
            label="neutral",
            confidence=0.5,
            distribution={"positive": 0.33, "negative": 0.33, "neutral": 0.34},
            explanations=["텍스트 없음"],
            analysis_method="heuristic",
        )

    text_lower = text.lower()

    # Count keywords
    positive_count = sum(1 for kw in POSITIVE_KEYWORDS if kw in text_lower)
    negative_count = sum(1 for kw in NEGATIVE_KEYWORDS if kw in text_lower)
    total_keywords = positive_count + negative_count + 1

    # Calculate score (-1 ~ 1)
    raw_score = (positive_count - negative_count) / total_keywords
    score = max(-1.0, min(1.0, raw_score * 2))  # Scale up for more sensitivity

    # Calculate distribution
    positive_ratio = positive_count / total_keywords
    negative_ratio = negative_count / total_keywords
    neutral_ratio = max(0.0, 1.0 - positive_ratio - negative_ratio)

    # Normalize distribution
    total_ratio = positive_ratio + negative_ratio + neutral_ratio
    positive_ratio /= total_ratio
    negative_ratio /= total_ratio
    neutral_ratio /= total_ratio

    # Determine label
    if score > 0.15:
        label = "positive"
    elif score < -0.15:
        label = "negative"
    else:
        label = "neutral"

    # Calculate confidence based on keyword density
    confidence = min(0.9, 0.5 + (positive_count + negative_count) * 0.05)

    # Generate explanations
    explanations = []
    if positive_count > 0:
        explanations.append(f"긍정 키워드 {positive_count}개 발견")
    if negative_count > 0:
        explanations.append(f"부정 키워드 {negative_count}개 발견")
    if not explanations:
        explanations.append("특별한 감정 신호 없음 (중립)")

    # Simple emotion estimation from keywords
    emotions = EmotionScore(
        joy=positive_ratio * 0.8,
        anger=negative_ratio * 0.4,
        sadness=negative_ratio * 0.3,
        fear=negative_ratio * 0.2,
        surprise=0.1,
        disgust=negative_ratio * 0.1,
    )

    return SentimentResult(
        score=round(score, 4),
        label=label,
        confidence=round(confidence, 4),
        distribution={
            "positive": round(positive_ratio, 4),
            "negative": round(negative_ratio, 4),
            "neutral": round(neutral_ratio, 4),
        },
        emotions=emotions,
        explanations=explanations,
        analysis_method="heuristic",
    )


# ========== ML-based Sentiment Analysis ==========


def analyze_sentiment_ml(text: str, include_emotions: bool = True) -> SentimentResult:
    """
    ML-based sentiment analysis using KoELECTRA/KoBERT models.
    """
    models = get_ml_models()

    if not models.get("loaded"):
        logger.warning("ML models not loaded, falling back to heuristic")
        return analyze_sentiment_heuristic(text)

    try:
        import torch

        device = models.get("device", "cpu")
        explanations = []

        # Primary sentiment analysis
        sentiment_result = None

        if "sentiment_pipeline" in models:
            # Use pipeline-based inference
            pipeline_result = models["sentiment_pipeline"](text[:512])

            if isinstance(pipeline_result, list):
                pipeline_result = pipeline_result[0]

            label_map = {
                "POSITIVE": "positive",
                "NEGATIVE": "negative",
                "NEUTRAL": "neutral",
                "LABEL_0": "negative",  # Common mapping for 3-class
                "LABEL_1": "neutral",
                "LABEL_2": "positive",
                "긍정": "positive",
                "부정": "negative",
                "중립": "neutral",
            }

            raw_label = pipeline_result.get("label", "NEUTRAL").upper()
            label = label_map.get(raw_label, "neutral")
            confidence = pipeline_result.get("score", 0.5)

            # Convert label to score
            score_map = {"positive": 1.0, "negative": -1.0, "neutral": 0.0}
            score = score_map.get(label, 0.0) * confidence

            # Estimate distribution
            if label == "positive":
                distribution = {
                    "positive": confidence,
                    "negative": (1 - confidence) * 0.3,
                    "neutral": (1 - confidence) * 0.7,
                }
            elif label == "negative":
                distribution = {
                    "positive": (1 - confidence) * 0.3,
                    "negative": confidence,
                    "neutral": (1 - confidence) * 0.7,
                }
            else:
                distribution = {
                    "positive": (1 - confidence) * 0.5,
                    "negative": (1 - confidence) * 0.5,
                    "neutral": confidence,
                }

            explanations.append(f"KoELECTRA 모델 분석 (신뢰도: {confidence:.1%})")

        elif "sentiment_model" in models and "sentiment_tokenizer" in models:
            # Use manual inference with tokenizer + model
            tokenizer = models["sentiment_tokenizer"]
            model = models["sentiment_model"]

            inputs = tokenizer(
                text[:512],
                return_tensors="pt",
                truncation=True,
                max_length=512,
                padding=True,
            )
            inputs = {k: v.to(device) for k, v in inputs.items()}

            with torch.no_grad():
                outputs = model(**inputs)
                probs = torch.softmax(outputs.logits, dim=-1)[0]

            # Map to sentiment (assuming 3-class: negative, neutral, positive)
            neg_prob = probs[0].item()
            neu_prob = probs[1].item() if len(probs) > 2 else 0.33
            pos_prob = probs[2].item() if len(probs) > 2 else probs[1].item()

            distribution = {
                "negative": neg_prob,
                "neutral": neu_prob,
                "positive": pos_prob,
            }

            # Get dominant label
            max_idx = torch.argmax(probs).item()
            labels = ["negative", "neutral", "positive"]
            label = labels[min(max_idx, 2)]
            confidence = probs[max_idx].item()

            # Calculate score
            score = pos_prob - neg_prob

            explanations.append(
                f"KoELECTRA 베이스 모델 분석 (신뢰도: {confidence:.1%})"
            )

        else:
            logger.warning("No sentiment model available, using heuristic")
            return analyze_sentiment_heuristic(text)

        # Emotion analysis (optional)
        emotions = None
        if include_emotions and "emotion_pipeline" in models:
            try:
                emotion_result = models["emotion_pipeline"](text[:512])

                if isinstance(emotion_result, list) and len(emotion_result) > 0:
                    if isinstance(emotion_result[0], list):
                        emotion_result = emotion_result[0]

                    emotion_scores = {
                        e["label"].lower(): e["score"] for e in emotion_result
                    }

                    emotions = EmotionScore(
                        joy=emotion_scores.get("joy", 0.0),
                        anger=emotion_scores.get("anger", 0.0),
                        sadness=emotion_scores.get("sadness", 0.0),
                        fear=emotion_scores.get("fear", 0.0),
                        surprise=emotion_scores.get("surprise", 0.0),
                        disgust=emotion_scores.get("disgust", 0.0),
                    )
                    explanations.append("감정 세부 분석 완료")
            except Exception as e:
                logger.warning(f"Emotion analysis failed: {e}")
                # Estimate emotions from sentiment
                emotions = EmotionScore(
                    joy=distribution["positive"] * 0.8,
                    anger=distribution["negative"] * 0.4,
                    sadness=distribution["negative"] * 0.3,
                    fear=distribution["negative"] * 0.2,
                    surprise=0.1,
                    disgust=distribution["negative"] * 0.1,
                )
        else:
            # Estimate emotions from sentiment distribution
            emotions = EmotionScore(
                joy=distribution["positive"] * 0.8,
                anger=distribution["negative"] * 0.4,
                sadness=distribution["negative"] * 0.3,
                fear=distribution["negative"] * 0.2,
                surprise=0.1,
                disgust=distribution["negative"] * 0.1,
            )

        return SentimentResult(
            score=round(score, 4),
            label=label,
            confidence=round(confidence, 4),
            distribution={k: round(v, 4) for k, v in distribution.items()},
            emotions=emotions,
            explanations=explanations,
            analysis_method="ml",
        )

    except Exception as e:
        logger.error(f"ML sentiment analysis failed: {e}", exc_info=True)
        result = analyze_sentiment_heuristic(text)
        result.explanations.append(f"ML 분석 실패, 휴리스틱 폴백: {str(e)[:50]}")
        return result


def analyze_sentiment(
    text: str, use_ml: bool = True, include_emotions: bool = True
) -> SentimentResult:
    """
    Main sentiment analysis function.
    Automatically selects ML or heuristic based on availability.
    """
    if not text or not text.strip():
        return SentimentResult(
            score=0.0,
            label="neutral",
            confidence=0.5,
            distribution={"positive": 0.33, "negative": 0.33, "neutral": 0.34},
            explanations=["분석할 텍스트 없음"],
            analysis_method="none",
        )

    if use_ml:
        return analyze_sentiment_ml(text, include_emotions)
    else:
        return analyze_sentiment_heuristic(text)


# ========== API Endpoints ==========


@app.get("/health")
async def health_check():
    """Health check endpoint"""
    models = get_ml_models()
    ml_status = "loaded" if models.get("loaded") else "heuristic_mode"

    return {
        "status": "healthy",
        "service": "sentiment-addon",
        "version": "2.0.0",
        "ml_status": ml_status,
        "warmup_complete": _model_warmup_complete,
        "warmup_error": _model_warmup_error,
        "device": models.get("device", "cpu"),
        "models": {
            "sentiment_pipeline": "sentiment_pipeline" in models,
            "sentiment_model": "sentiment_model" in models,
            "emotion_pipeline": "emotion_pipeline" in models,
        },
    }


@app.get("/ready")
async def readiness_check():
    """Readiness check - only returns healthy when models are warmed up"""
    if not _model_warmup_complete:
        return {"status": "warming_up", "ready": False}

    models = get_ml_models()
    return {
        "status": "ready",
        "ready": True,
        "ml_enabled": models.get("loaded", False),
        "warmup_error": _model_warmup_error,
    }


@app.get("/models")
async def get_model_info():
    """Get information about loaded models"""
    models = get_ml_models()

    return {
        "loaded": models.get("loaded", False),
        "device": models.get("device", "cpu"),
        "available_models": [k for k in models.keys() if k not in ["loaded", "device"]],
        "sentiment_model": os.getenv("SENTIMENT_MODEL", "snunlp/KR-FinBert-SC"),
        "emotion_model": os.getenv(
            "EMOTION_MODEL", "j-hartmann/emotion-english-distilroberta-base"
        ),
    }


@app.post("/analyze", response_model=AddonResponse)
async def analyze(request: AddonRequest):
    """
    Article sentiment analysis endpoint.
    Called by NewsInsight Orchestrator.
    """
    start_time = time.time()
    models = get_ml_models()

    try:
        # Validate input
        if not request.article:
            raise ValueError("article is required")

        # Prepare text for analysis
        text = ""
        if request.article.title:
            text += request.article.title + " "
        if request.article.content:
            text += request.article.content

        # Get options
        options = request.options or ExecutionOptions()
        use_ml = options.use_ml if options.use_ml is not None else True
        include_emotions = (
            options.include_emotions if options.include_emotions is not None else True
        )

        # Run sentiment analysis
        sentiment_result = analyze_sentiment(
            text, use_ml=use_ml, include_emotions=include_emotions
        )

        # Build response
        latency_ms = int((time.time() - start_time) * 1000)

        model_name = (
            "koelectra-sentiment-v2" if models.get("loaded") else "keyword-heuristic-v1"
        )

        # Track Prometheus metrics
        if PROMETHEUS_AVAILABLE:
            REQUEST_COUNT.labels(endpoint="analyze", status="success").inc()
            REQUEST_LATENCY.labels(endpoint="analyze").observe(time.time() - start_time)
            ANALYSIS_COUNT.labels(
                label=sentiment_result.label,
                mode="ml" if models.get("loaded") and use_ml else "heuristic",
            ).inc()

        return AddonResponse(
            request_id=request.request_id,
            addon_id=request.addon_id,
            status="success",
            results=AnalysisResults(
                sentiment=sentiment_result,
                raw={
                    "text_length": len(text),
                    "ml_available": models.get("loaded", False),
                    "analysis_method": sentiment_result.analysis_method,
                },
            ),
            meta=ResponseMeta(
                model_version="2.0.0",
                model_name=model_name,
                latency_ms=latency_ms,
                processed_at=time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
                device=models.get("device", "cpu"),
            ),
        )

    except ValueError as e:
        latency_ms = int((time.time() - start_time) * 1000)

        # Track error metrics
        if PROMETHEUS_AVAILABLE:
            REQUEST_COUNT.labels(endpoint="analyze", status="error").inc()
            REQUEST_LATENCY.labels(endpoint="analyze").observe(time.time() - start_time)
            ERROR_COUNT.labels(error_type="ValidationError").inc()

        return AddonResponse(
            request_id=request.request_id,
            addon_id=request.addon_id,
            status="error",
            error=ErrorInfo(code="VALIDATION_ERROR", message=str(e)),
            meta=ResponseMeta(
                model_version="2.0.0",
                model_name="sentiment-addon",
                latency_ms=latency_ms,
                processed_at=time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
                device=models.get("device", "cpu"),
            ),
        )
    except Exception as e:
        logger.error(f"Analysis error: {e}", exc_info=True)
        latency_ms = int((time.time() - start_time) * 1000)

        # Track error metrics
        if PROMETHEUS_AVAILABLE:
            REQUEST_COUNT.labels(endpoint="analyze", status="error").inc()
            REQUEST_LATENCY.labels(endpoint="analyze").observe(time.time() - start_time)
            ERROR_COUNT.labels(error_type=type(e).__name__).inc()

        return AddonResponse(
            request_id=request.request_id,
            addon_id=request.addon_id,
            status="error",
            error=ErrorInfo(
                code="ANALYSIS_ERROR",
                message=str(e),
                details=f"Error occurred during sentiment analysis",
            ),
            meta=ResponseMeta(
                model_version="2.0.0",
                model_name="sentiment-addon",
                latency_ms=latency_ms,
                processed_at=time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
                device=models.get("device", "cpu"),
            ),
        )


@app.post("/batch", response_model=List[AddonResponse])
async def analyze_batch(requests: List[AddonRequest]):
    """Batch analyze multiple articles"""
    results = []
    for req in requests:
        result = await analyze(req)
        results.append(result)
    return results


@app.post("/analyze/simple")
async def analyze_simple(text: str):
    """
    Simple text sentiment analysis endpoint.
    For quick testing without full request structure.
    """
    start_time = time.time()

    result = analyze_sentiment(text)
    latency_ms = int((time.time() - start_time) * 1000)

    return {
        "text": text[:100] + "..." if len(text) > 100 else text,
        "sentiment": result.model_dump(),
        "latency_ms": latency_ms,
    }


# ========== Topic Classification (Bonus Feature) ==========

TOPIC_KEYWORDS = {
    "정치": [
        "국회",
        "대통령",
        "정당",
        "선거",
        "투표",
        "정부",
        "장관",
        "의원",
        "청와대",
        "여당",
        "야당",
    ],
    "경제": [
        "주식",
        "코스피",
        "환율",
        "금리",
        "투자",
        "부동산",
        "GDP",
        "물가",
        "인플레이션",
        "기업",
    ],
    "사회": [
        "교육",
        "학교",
        "범죄",
        "사건",
        "복지",
        "의료",
        "병원",
        "사고",
        "재난",
        "환경",
    ],
    "문화": [
        "영화",
        "드라마",
        "음악",
        "공연",
        "전시",
        "예술",
        "문학",
        "K팝",
        "연예",
        "방송",
    ],
    "IT/과학": [
        "AI",
        "인공지능",
        "반도체",
        "스마트폰",
        "로봇",
        "우주",
        "연구",
        "개발",
        "기술",
        "디지털",
    ],
    "스포츠": [
        "축구",
        "야구",
        "농구",
        "올림픽",
        "월드컵",
        "선수",
        "경기",
        "대회",
        "승리",
        "우승",
    ],
    "국제": [
        "미국",
        "중국",
        "일본",
        "북한",
        "유럽",
        "UN",
        "외교",
        "무역",
        "전쟁",
        "분쟁",
    ],
}


@app.post("/analyze/topic")
async def analyze_topic(request: AddonRequest):
    """
    Article topic classification endpoint.
    Simple keyword-based topic detection.
    """
    start_time = time.time()

    if not request.article:
        raise HTTPException(status_code=400, detail="article is required")

    text = ""
    if request.article.title:
        text += request.article.title + " "
    if request.article.content:
        text += request.article.content

    text_lower = text.lower()

    # Count topic keywords
    topic_scores = {}
    for topic, keywords in TOPIC_KEYWORDS.items():
        count = sum(1 for kw in keywords if kw in text_lower)
        if count > 0:
            topic_scores[topic] = count

    # Normalize scores
    total = sum(topic_scores.values()) or 1
    topic_distribution = {k: v / total for k, v in topic_scores.items()}

    # Get primary topic
    primary_topic = (
        max(topic_scores.keys(), key=lambda k: topic_scores[k])
        if topic_scores
        else "기타"
    )

    latency_ms = int((time.time() - start_time) * 1000)

    return {
        "request_id": request.request_id,
        "status": "success",
        "topic": {
            "primary": primary_topic,
            "confidence": topic_distribution.get(primary_topic, 0.0),
            "distribution": topic_distribution,
            "all_topics": list(topic_scores.keys()),
        },
        "meta": {
            "model_version": "topic-keyword-v1",
            "latency_ms": latency_ms,
            "processed_at": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        },
    }


# ========== Entry Point ==========

if __name__ == "__main__":
    import uvicorn

    port = int(os.getenv("PORT", "8100"))
    host = os.getenv("HOST", "0.0.0.0")

    uvicorn.run(app, host=host, port=port)

```

---

## backend/news-scraper/main.py

```py
"""
Newspaper4k News Article Scraper Service

A FastAPI microservice that extracts clean article content from news URLs
using the newspaper4k library. Designed to be called from Java services
with fallback support.
"""

import asyncio
import hashlib
import logging
import os
import uuid
from datetime import datetime
from typing import Optional, List, Dict, Any
from contextlib import asynccontextmanager
from enum import Enum

from fastapi import FastAPI, HTTPException, Request, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import newspaper
from newspaper import Article, Config

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


# --- Configuration ---
class AppConfig:
    """Application configuration from environment variables."""

    DEFAULT_LANGUAGE = os.getenv("NEWS_SCRAPER_DEFAULT_LANGUAGE", "ko")
    DEFAULT_TIMEOUT = int(os.getenv("NEWS_SCRAPER_DEFAULT_TIMEOUT", "10"))
    MAX_TIMEOUT = int(os.getenv("NEWS_SCRAPER_MAX_TIMEOUT", "30"))
    USER_AGENT = os.getenv(
        "NEWS_SCRAPER_USER_AGENT",
        "Mozilla/5.0 (compatible; NewsInsight-Scraper/1.0; +https://newsinsight.ai)",
    )
    ALLOWED_SCHEMES = {"http", "https"}
    # Batch processing settings
    BATCH_MAX_URLS = int(os.getenv("NEWS_SCRAPER_BATCH_MAX_URLS", "100"))
    BATCH_CONCURRENCY = int(os.getenv("NEWS_SCRAPER_BATCH_CONCURRENCY", "5"))
    BATCH_JOB_TTL_HOURS = int(os.getenv("NEWS_SCRAPER_BATCH_JOB_TTL_HOURS", "24"))


config = AppConfig()


# --- Request/Response Models ---
class ScrapeRequest(BaseModel):
    """Request model for article scraping."""

    url: str = Field(..., description="The URL of the news article to scrape")
    language: Optional[str] = Field(
        default=None,
        description="Language code (e.g., 'ko', 'en'). Auto-detected if not specified.",
    )
    timeout_sec: Optional[int] = Field(
        default=None, ge=1, le=30, description="Request timeout in seconds (1-30)"
    )
    extract_html: Optional[bool] = Field(
        default=False, description="Whether to include raw HTML in response"
    )


class ScrapeResponse(BaseModel):
    """Response model for successful article extraction."""

    status: str = Field(default="ok", description="Response status")
    url: str = Field(..., description="Original URL")
    title: Optional[str] = Field(default=None, description="Article title")
    text: str = Field(..., description="Extracted article text content")
    html: Optional[str] = Field(default=None, description="Raw article HTML")
    top_image: Optional[str] = Field(default=None, description="Main article image URL")
    authors: List[str] = Field(default_factory=list, description="List of authors")
    publish_date: Optional[str] = Field(
        default=None, description="Publication date in ISO8601 format"
    )
    keywords: List[str] = Field(default_factory=list, description="Extracted keywords")
    summary: Optional[str] = Field(
        default=None, description="Article summary/description"
    )
    content_hash: Optional[str] = Field(
        default=None, description="SHA-256 hash of extracted text for deduplication"
    )
    extraction_time_ms: Optional[int] = Field(
        default=None, description="Time taken for extraction in milliseconds"
    )


class ErrorResponse(BaseModel):
    """Response model for errors."""

    status: str = Field(default="error")
    url: str
    error_code: str
    error_message: str


class HealthResponse(BaseModel):
    """Health check response."""

    status: str
    version: str
    library_version: str


# --- Batch Processing Enums and Models ---
class JobStatus(str, Enum):
    """Batch job status enum."""

    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


class UrlStatus(str, Enum):
    """Individual URL processing status."""

    PENDING = "pending"
    PROCESSING = "processing"
    SUCCESS = "success"
    FAILED = "failed"


class BatchUrlItem(BaseModel):
    """Individual URL item in batch request."""

    url: str = Field(..., description="URL to scrape")
    language: Optional[str] = Field(
        default=None, description="Language override for this URL"
    )


class BatchScrapeRequest(BaseModel):
    """Request model for batch scraping."""

    urls: List[str] = Field(
        ...,
        min_length=1,
        max_length=100,
        description="List of URLs to scrape (max 100)",
    )
    language: Optional[str] = Field(
        default=None, description="Default language for all URLs"
    )
    timeout_sec: Optional[int] = Field(
        default=10, ge=1, le=30, description="Timeout per URL in seconds"
    )
    concurrency: Optional[int] = Field(
        default=5, ge=1, le=10, description="Number of concurrent scraping tasks"
    )
    callback_url: Optional[str] = Field(
        default=None, description="Webhook URL to notify when job completes"
    )


class UrlResult(BaseModel):
    """Result for a single URL in batch processing."""

    url: str
    status: UrlStatus
    title: Optional[str] = None
    text: Optional[str] = None
    top_image: Optional[str] = None
    authors: List[str] = Field(default_factory=list)
    publish_date: Optional[str] = None
    keywords: List[str] = Field(default_factory=list)
    summary: Optional[str] = None
    content_hash: Optional[str] = None
    extraction_time_ms: Optional[int] = None
    error_code: Optional[str] = None
    error_message: Optional[str] = None


class BatchJobResponse(BaseModel):
    """Response for batch job submission."""

    job_id: str = Field(..., description="Unique job identifier")
    status: JobStatus
    total_urls: int
    message: str


class BatchJobStatusResponse(BaseModel):
    """Response for batch job status query."""

    job_id: str
    status: JobStatus
    total_urls: int
    completed: int
    failed: int
    pending: int
    progress_percent: float
    created_at: str
    updated_at: str
    completed_at: Optional[str] = None
    results: Optional[List[UrlResult]] = None


class BatchJob:
    """Internal batch job tracking class."""

    def __init__(
        self,
        job_id: str,
        urls: List[str],
        language: Optional[str],
        timeout_sec: int,
        concurrency: int,
        callback_url: Optional[str] = None,
    ):
        self.job_id = job_id
        self.urls = urls
        self.language = language
        self.timeout_sec = timeout_sec
        self.concurrency = concurrency
        self.callback_url = callback_url
        self.status = JobStatus.PENDING
        self.results: Dict[str, UrlResult] = {}
        self.created_at = datetime.utcnow()
        self.updated_at = datetime.utcnow()
        self.completed_at: Optional[datetime] = None

        # Initialize all URLs as pending
        for url in urls:
            self.results[url] = UrlResult(url=url, status=UrlStatus.PENDING)

    @property
    def total_urls(self) -> int:
        return len(self.urls)

    @property
    def completed_count(self) -> int:
        return sum(1 for r in self.results.values() if r.status == UrlStatus.SUCCESS)

    @property
    def failed_count(self) -> int:
        return sum(1 for r in self.results.values() if r.status == UrlStatus.FAILED)

    @property
    def pending_count(self) -> int:
        return sum(
            1
            for r in self.results.values()
            if r.status in (UrlStatus.PENDING, UrlStatus.PROCESSING)
        )

    @property
    def progress_percent(self) -> float:
        processed = self.completed_count + self.failed_count
        return (
            round((processed / self.total_urls) * 100, 2)
            if self.total_urls > 0
            else 0.0
        )

    def to_status_response(
        self, include_results: bool = True
    ) -> BatchJobStatusResponse:
        return BatchJobStatusResponse(
            job_id=self.job_id,
            status=self.status,
            total_urls=self.total_urls,
            completed=self.completed_count,
            failed=self.failed_count,
            pending=self.pending_count,
            progress_percent=self.progress_percent,
            created_at=self.created_at.isoformat(),
            updated_at=self.updated_at.isoformat(),
            completed_at=self.completed_at.isoformat() if self.completed_at else None,
            results=list(self.results.values()) if include_results else None,
        )


# --- In-Memory Job Store ---
class JobStore:
    """Simple in-memory job store with TTL-based cleanup."""

    def __init__(self, ttl_hours: int = 24):
        self._jobs: Dict[str, BatchJob] = {}
        self._ttl_hours = ttl_hours
        self._lock = asyncio.Lock()

    async def create_job(
        self,
        urls: List[str],
        language: Optional[str],
        timeout_sec: int,
        concurrency: int,
        callback_url: Optional[str] = None,
    ) -> BatchJob:
        """Create a new batch job."""
        job_id = str(uuid.uuid4())
        job = BatchJob(
            job_id=job_id,
            urls=urls,
            language=language,
            timeout_sec=timeout_sec,
            concurrency=concurrency,
            callback_url=callback_url,
        )
        async with self._lock:
            self._jobs[job_id] = job
        return job

    async def get_job(self, job_id: str) -> Optional[BatchJob]:
        """Get job by ID."""
        async with self._lock:
            return self._jobs.get(job_id)

    async def update_job(self, job: BatchJob) -> None:
        """Update job in store."""
        job.updated_at = datetime.utcnow()
        async with self._lock:
            self._jobs[job.job_id] = job

    async def cleanup_old_jobs(self) -> int:
        """Remove jobs older than TTL."""
        cutoff = datetime.utcnow()
        removed = 0
        async with self._lock:
            to_remove = []
            for job_id, job in self._jobs.items():
                age_hours = (cutoff - job.created_at).total_seconds() / 3600
                if age_hours > self._ttl_hours:
                    to_remove.append(job_id)
            for job_id in to_remove:
                del self._jobs[job_id]
                removed += 1
        return removed

    async def list_jobs(self, limit: int = 50) -> List[BatchJob]:
        """List recent jobs."""
        async with self._lock:
            jobs = sorted(self._jobs.values(), key=lambda j: j.created_at, reverse=True)
            return jobs[:limit]


# Global job store
job_store = JobStore(ttl_hours=config.BATCH_JOB_TTL_HOURS)


# --- Lifespan Management ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan handler for startup/shutdown."""
    logger.info("News Scraper service starting up...")
    logger.info(f"Default language: {config.DEFAULT_LANGUAGE}")
    logger.info(f"Default timeout: {config.DEFAULT_TIMEOUT}s")
    yield
    logger.info("News Scraper service shutting down...")


# --- FastAPI App ---
app = FastAPI(
    title="Newspaper4k News Scraper",
    description="Extracts clean article content from news URLs using newspaper4k",
    version="1.0.0",
    lifespan=lifespan,
)

# CORS middleware for development
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# --- Helper Functions ---
def validate_url(url: str) -> None:
    """Validate URL scheme and format."""
    from urllib.parse import urlparse

    try:
        parsed = urlparse(url)
        if parsed.scheme not in config.ALLOWED_SCHEMES:
            raise ValueError(f"Unsupported URL scheme: {parsed.scheme}")
        if not parsed.netloc:
            raise ValueError("Invalid URL: missing domain")
    except Exception as e:
        raise ValueError(f"Invalid URL format: {str(e)}")


def compute_content_hash(text: str) -> str:
    """Compute SHA-256 hash of content for deduplication."""
    return hashlib.sha256(text.encode("utf-8")).hexdigest()


def create_newspaper_config(
    language: Optional[str] = None, timeout: int = 10
) -> Config:
    """Create newspaper4k configuration."""
    cfg = Config()
    cfg.browser_user_agent = config.USER_AGENT
    cfg.request_timeout = timeout
    cfg.fetch_images = True
    cfg.memoize_articles = False
    cfg.language = language or config.DEFAULT_LANGUAGE
    cfg.keep_article_html = True
    return cfg


async def extract_article(
    url: str,
    language: Optional[str] = None,
    timeout: int = 10,
    extract_html: bool = False,
) -> dict:
    """
    Extract article content from URL using newspaper4k.

    Runs in thread pool to avoid blocking the event loop.
    """

    def _extract():
        start_time = datetime.now()

        cfg = create_newspaper_config(language, timeout)
        article = Article(url, config=cfg)

        # Download and parse
        article.download()
        article.parse()

        # NLP processing for keywords/summary (optional, may fail)
        try:
            article.nlp()
        except Exception as nlp_error:
            logger.warning(f"NLP processing failed for {url}: {nlp_error}")

        # Calculate extraction time
        extraction_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)

        # Format publish date
        publish_date = None
        if article.publish_date:
            try:
                publish_date = article.publish_date.isoformat()
            except Exception:
                publish_date = str(article.publish_date)

        # Get text content
        text = article.text or ""

        return {
            "title": article.title,
            "text": text,
            "html": article.article_html if extract_html else None,
            "top_image": article.top_image,
            "authors": list(article.authors) if article.authors else [],
            "publish_date": publish_date,
            "keywords": list(article.keywords) if article.keywords else [],
            "summary": article.summary if hasattr(article, "summary") else None,
            "content_hash": compute_content_hash(text) if text else None,
            "extraction_time_ms": extraction_time_ms,
        }

    # Run in thread pool to avoid blocking
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(None, _extract)


# --- Batch Processing Functions ---
async def process_single_url(
    job: BatchJob, url: str, semaphore: asyncio.Semaphore
) -> UrlResult:
    """Process a single URL within a batch job with semaphore-controlled concurrency."""
    async with semaphore:
        # Mark as processing
        job.results[url].status = UrlStatus.PROCESSING

        try:
            # Validate URL first
            validate_url(url)

            # Extract article
            result = await asyncio.wait_for(
                extract_article(
                    url=url,
                    language=job.language,
                    timeout=job.timeout_sec,
                    extract_html=False,
                ),
                timeout=job.timeout_sec + 2,
            )

            # Check content quality
            if not result["text"] or len(result["text"].strip()) < 50:
                job.results[url] = UrlResult(
                    url=url,
                    status=UrlStatus.FAILED,
                    error_code="CONTENT_TOO_SHORT",
                    error_message="Extracted content is too short or empty",
                )
            else:
                job.results[url] = UrlResult(
                    url=url,
                    status=UrlStatus.SUCCESS,
                    title=result.get("title"),
                    text=result.get("text"),
                    top_image=result.get("top_image"),
                    authors=result.get("authors", []),
                    publish_date=result.get("publish_date"),
                    keywords=result.get("keywords", []),
                    summary=result.get("summary"),
                    content_hash=result.get("content_hash"),
                    extraction_time_ms=result.get("extraction_time_ms"),
                )
                logger.info(f"[Batch:{job.job_id}] Successfully scraped: {url}")

        except asyncio.TimeoutError:
            job.results[url] = UrlResult(
                url=url,
                status=UrlStatus.FAILED,
                error_code="TIMEOUT",
                error_message=f"Extraction timed out after {job.timeout_sec}s",
            )
            logger.warning(f"[Batch:{job.job_id}] Timeout: {url}")
        except ValueError as e:
            job.results[url] = UrlResult(
                url=url,
                status=UrlStatus.FAILED,
                error_code="INVALID_URL",
                error_message=str(e),
            )
            logger.warning(f"[Batch:{job.job_id}] Invalid URL: {url} - {e}")
        except Exception as e:
            job.results[url] = UrlResult(
                url=url,
                status=UrlStatus.FAILED,
                error_code="EXTRACTION_FAILED",
                error_message=str(e),
            )
            logger.error(f"[Batch:{job.job_id}] Error scraping {url}: {e}")

        return job.results[url]


async def process_batch_job(job: BatchJob) -> None:
    """Process all URLs in a batch job with controlled concurrency."""
    logger.info(
        f"[Batch:{job.job_id}] Starting batch processing of {len(job.urls)} URLs "
        f"with concurrency={job.concurrency}"
    )

    job.status = JobStatus.PROCESSING
    await job_store.update_job(job)

    # Create semaphore for concurrency control
    semaphore = asyncio.Semaphore(job.concurrency)

    # Process all URLs concurrently (limited by semaphore)
    tasks = [process_single_url(job, url, semaphore) for url in job.urls]

    try:
        await asyncio.gather(*tasks, return_exceptions=True)

        # Determine final job status
        if job.failed_count == job.total_urls:
            job.status = JobStatus.FAILED
        else:
            job.status = JobStatus.COMPLETED

    except Exception as e:
        logger.exception(f"[Batch:{job.job_id}] Batch processing error: {e}")
        job.status = JobStatus.FAILED

    job.completed_at = datetime.utcnow()
    await job_store.update_job(job)

    logger.info(
        f"[Batch:{job.job_id}] Batch completed: "
        f"{job.completed_count}/{job.total_urls} success, "
        f"{job.failed_count} failed"
    )

    # Optional: Send webhook callback
    if job.callback_url:
        await send_webhook_callback(job)


async def send_webhook_callback(job: BatchJob) -> None:
    """Send webhook notification when job completes."""
    try:
        import aiohttp

        async with aiohttp.ClientSession() as session:
            payload = {
                "job_id": job.job_id,
                "status": job.status.value,
                "total_urls": job.total_urls,
                "completed": job.completed_count,
                "failed": job.failed_count,
            }
            async with session.post(
                job.callback_url, json=payload, timeout=aiohttp.ClientTimeout(total=10)
            ) as response:
                logger.info(
                    f"[Batch:{job.job_id}] Webhook callback sent to {job.callback_url}: "
                    f"status={response.status}"
                )
    except Exception as e:
        logger.warning(f"[Batch:{job.job_id}] Webhook callback failed: {e}")


# --- API Endpoints ---
@app.get("/health", response_model=HealthResponse)
@app.head("/health")
async def health_check():
    """Health check endpoint."""
    return HealthResponse(
        status="ok",
        version="1.0.0",
        library_version=newspaper.__version__
        if hasattr(newspaper, "__version__")
        else "unknown",
    )


@app.post(
    "/v1/scrape/article",
    response_model=ScrapeResponse,
    responses={
        400: {"model": ErrorResponse, "description": "Invalid request"},
        500: {"model": ErrorResponse, "description": "Extraction failed"},
    },
)
async def scrape_article(request: ScrapeRequest, req: Request):
    """
    Extract article content from a news URL.

    Uses newspaper4k to extract:
    - Title
    - Main text content (cleaned)
    - Authors
    - Publication date
    - Top image
    - Keywords (via NLP)
    - Summary

    Returns content hash for deduplication support.
    """
    # Get trace ID from header if available
    trace_id = req.headers.get("X-Trace-Id", req.headers.get("X-Request-Id", "unknown"))

    logger.info(f"[{trace_id}] Scraping article: {request.url}")

    # Validate URL
    try:
        validate_url(request.url)
    except ValueError as e:
        logger.warning(f"[{trace_id}] Invalid URL: {request.url} - {e}")
        raise HTTPException(
            status_code=400,
            detail=ErrorResponse(
                url=request.url, error_code="INVALID_URL", error_message=str(e)
            ).model_dump(),
        )

    # Determine timeout
    timeout = request.timeout_sec or config.DEFAULT_TIMEOUT
    timeout = min(timeout, config.MAX_TIMEOUT)

    try:
        # Extract article with asyncio timeout
        result = await asyncio.wait_for(
            extract_article(
                url=request.url,
                language=request.language,
                timeout=timeout,
                extract_html=request.extract_html or False,
            ),
            timeout=timeout + 2,  # Add buffer for processing
        )

        # Check if we got meaningful content
        if not result["text"] or len(result["text"].strip()) < 50:
            logger.warning(
                f"[{trace_id}] Extracted content too short for {request.url}: "
                f"{len(result['text'] or '')} chars"
            )
            raise HTTPException(
                status_code=400,
                detail=ErrorResponse(
                    url=request.url,
                    error_code="CONTENT_TOO_SHORT",
                    error_message="Extracted article content is too short or empty",
                ).model_dump(),
            )

        logger.info(
            f"[{trace_id}] Successfully extracted {len(result['text'])} chars "
            f"from {request.url} in {result['extraction_time_ms']}ms"
        )

        return ScrapeResponse(status="ok", url=request.url, **result)

    except asyncio.TimeoutError:
        logger.error(f"[{trace_id}] Timeout extracting article: {request.url}")
        raise HTTPException(
            status_code=500,
            detail=ErrorResponse(
                url=request.url,
                error_code="TIMEOUT",
                error_message=f"Article extraction timed out after {timeout}s",
            ).model_dump(),
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.exception(f"[{trace_id}] Error extracting article: {request.url}")
        raise HTTPException(
            status_code=500,
            detail=ErrorResponse(
                url=request.url, error_code="EXTRACTION_FAILED", error_message=str(e)
            ).model_dump(),
        )


# --- Batch Processing Endpoints ---
@app.post(
    "/v1/scrape/batch",
    response_model=BatchJobResponse,
    responses={
        400: {"description": "Invalid request"},
        503: {"description": "Service overloaded"},
    },
)
async def submit_batch_job(
    request: BatchScrapeRequest, background_tasks: BackgroundTasks, req: Request
):
    """
    Submit a batch scraping job for multiple URLs.

    The job runs asynchronously in the background. Use the returned job_id
    to poll for status and results via GET /v1/scrape/batch/{job_id}.

    Features:
    - Processes up to 100 URLs per batch
    - Configurable concurrency (1-10 parallel requests)
    - Per-URL timeout with graceful error handling
    - Optional webhook callback on completion
    """
    trace_id = req.headers.get("X-Trace-Id", req.headers.get("X-Request-Id", "unknown"))

    # Validate URL count
    if len(request.urls) > config.BATCH_MAX_URLS:
        raise HTTPException(
            status_code=400,
            detail=f"Maximum {config.BATCH_MAX_URLS} URLs allowed per batch",
        )

    # Remove duplicates while preserving order
    unique_urls = list(dict.fromkeys(request.urls))

    logger.info(
        f"[{trace_id}] Batch job submitted: {len(unique_urls)} URLs "
        f"(concurrency={request.concurrency})"
    )

    # Create job
    job = await job_store.create_job(
        urls=unique_urls,
        language=request.language,
        timeout_sec=request.timeout_sec or config.DEFAULT_TIMEOUT,
        concurrency=min(request.concurrency or config.BATCH_CONCURRENCY, 10),
        callback_url=request.callback_url,
    )

    # Start background processing
    background_tasks.add_task(process_batch_job, job)

    return BatchJobResponse(
        job_id=job.job_id,
        status=job.status,
        total_urls=job.total_urls,
        message=f"Batch job created. Poll GET /v1/scrape/batch/{job.job_id} for status.",
    )


@app.get(
    "/v1/scrape/batch/{job_id}",
    response_model=BatchJobStatusResponse,
    responses={404: {"description": "Job not found"}},
)
async def get_batch_job_status(job_id: str, include_results: bool = True):
    """
    Get the status and results of a batch scraping job.

    Parameters:
    - job_id: The unique job identifier from the POST response
    - include_results: Whether to include individual URL results (default: true)

    Returns progress information and, when complete, the results for each URL.
    """
    job = await job_store.get_job(job_id)

    if not job:
        raise HTTPException(
            status_code=404, detail=f"Job {job_id} not found or has expired"
        )

    return job.to_status_response(include_results=include_results)


@app.delete(
    "/v1/scrape/batch/{job_id}",
    responses={
        404: {"description": "Job not found"},
        409: {"description": "Job cannot be cancelled"},
    },
)
async def cancel_batch_job(job_id: str):
    """
    Cancel a pending or processing batch job.

    Note: URLs that are already being processed may still complete.
    """
    job = await job_store.get_job(job_id)

    if not job:
        raise HTTPException(status_code=404, detail=f"Job {job_id} not found")

    if job.status in (JobStatus.COMPLETED, JobStatus.FAILED, JobStatus.CANCELLED):
        raise HTTPException(
            status_code=409,
            detail=f"Job {job_id} is already {job.status.value} and cannot be cancelled",
        )

    job.status = JobStatus.CANCELLED
    job.completed_at = datetime.utcnow()
    await job_store.update_job(job)

    logger.info(f"[Batch:{job_id}] Job cancelled by user")

    return {"status": "cancelled", "job_id": job_id}


@app.get("/v1/scrape/batch")
async def list_batch_jobs(limit: int = 20):
    """
    List recent batch jobs.

    Returns summary information for up to `limit` most recent jobs.
    """
    jobs = await job_store.list_jobs(limit=min(limit, 50))

    return {
        "jobs": [
            {
                "job_id": job.job_id,
                "status": job.status.value,
                "total_urls": job.total_urls,
                "completed": job.completed_count,
                "failed": job.failed_count,
                "progress_percent": job.progress_percent,
                "created_at": job.created_at.isoformat(),
            }
            for job in jobs
        ],
        "count": len(jobs),
    }


@app.get("/v1/scrape/batch/status")
async def batch_service_status():
    """Get batch processing service status and configuration."""
    jobs = await job_store.list_jobs(limit=100)
    active_jobs = [
        j for j in jobs if j.status in (JobStatus.PENDING, JobStatus.PROCESSING)
    ]

    return {
        "status": "ok",
        "config": {
            "max_urls_per_batch": config.BATCH_MAX_URLS,
            "default_concurrency": config.BATCH_CONCURRENCY,
            "job_ttl_hours": config.BATCH_JOB_TTL_HOURS,
        },
        "stats": {"active_jobs": len(active_jobs), "total_jobs_in_memory": len(jobs)},
    }


# --- Main Entry Point ---
if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=int(os.getenv("PORT", "8000")),
        reload=os.getenv("ENV", "production") == "development",
    )

```

---

## backend/services/bot-detector/main.py

```py
"""
Bot Detection Service - AI/봇 텍스트 탐지 및 사용자 포렌식 서비스

기능:
1. AI 생성 텍스트 탐지 (GPT, Claude 등)
2. 봇 행동 패턴 분석 (시간, 반복, 활동량)
3. 사용자 프로필 업데이트
"""

import os
import sys
import re
import math
import hashlib
import time
from typing import Optional, List, Dict, Any
from datetime import datetime, timezone
from collections import Counter

import structlog
from pydantic import BaseModel, Field
from pydantic_settings import BaseSettings
from fastapi import FastAPI, HTTPException
from cachetools import TTLCache

# Add shared module to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))
try:
    from shared.prometheus_metrics import (
        setup_metrics,
        track_request_time,
        track_operation,
        track_error,
        track_item_processed,
        ServiceMetrics,
    )

    METRICS_AVAILABLE = True
except ImportError:
    METRICS_AVAILABLE = False

# Lazy imports for ML models
log = structlog.get_logger()

# ============================================
# Configuration
# ============================================


class Settings(BaseSettings):
    """Application settings"""

    database_url: str = Field(default="postgresql://osint:osint@postgres:5432/osint")
    model_name: str = Field(default="roberta-base-openai-detector")
    cache_ttl: int = Field(default=3600)
    min_text_length: int = Field(default=50)
    bot_threshold: float = Field(default=0.7)

    # Pattern detection thresholds
    max_posts_per_minute: int = Field(default=3)
    max_posts_per_hour: int = Field(default=30)
    repetition_threshold: float = Field(default=0.8)

    class Config:
        env_prefix = "BOT_DETECTOR_"


settings = Settings()

# ============================================
# Request/Response Models
# ============================================


class BotDetectionRequest(BaseModel):
    """봇 탐지 요청"""

    content_id: Optional[str] = None
    text: str
    author: Optional[str] = None
    source_id: Optional[str] = None
    timestamp: Optional[datetime] = None
    user_hash: Optional[str] = None


class BotDetectionResponse(BaseModel):
    """봇 탐지 결과"""

    is_bot: bool
    confidence: float
    detection_model: str
    detection_reasons: List[str]
    pattern_flags: Dict[str, Any]
    perplexity: Optional[float] = None
    burstiness: Optional[float] = None
    repetition_rate: Optional[float] = None


class BatchDetectionRequest(BaseModel):
    """배치 봇 탐지 요청"""

    items: List[BotDetectionRequest]


class BatchDetectionResponse(BaseModel):
    """배치 봇 탐지 결과"""

    results: List[BotDetectionResponse]
    total: int
    bot_count: int


class UserProfileUpdateRequest(BaseModel):
    """사용자 프로필 업데이트 요청"""

    user_hash: str
    source_id: Optional[str] = None
    display_name: Optional[str] = None
    activity_timestamps: List[datetime] = []
    contents: List[str] = []


class UserProfileResponse(BaseModel):
    """사용자 프로필 응답"""

    user_hash: str
    bot_probability: float
    troll_score: float
    credibility_score: float
    activity_pattern: Dict[str, Any]
    writing_style: Dict[str, Any]


class AddonArticleInput(BaseModel):
    id: Optional[int] = None
    title: Optional[str] = None
    content: Optional[str] = None
    url: Optional[str] = None
    source: Optional[str] = None
    published_at: Optional[str] = None


class AddonCommentItem(BaseModel):
    id: Optional[str] = None
    content: Optional[str] = None
    created_at: Optional[str] = None
    likes: Optional[int] = None
    replies: Optional[int] = None
    author_id: Optional[str] = None


class AddonCommentsInput(BaseModel):
    article_id: Optional[int] = None
    items: Optional[List[AddonCommentItem]] = None
    platform: Optional[str] = None


class AddonRequest(BaseModel):
    request_id: str
    addon_id: str
    task: str = "article_analysis"
    input_schema_version: str = "1.0"
    article: Optional[AddonArticleInput] = None
    comments: Optional[AddonCommentsInput] = None
    context: Optional[Dict[str, Any]] = None
    options: Optional[Dict[str, Any]] = None


class AddonDiscussionResult(BaseModel):
    overall_sentiment: Optional[str] = None
    sentiment_distribution: Optional[Dict[str, float]] = None
    stance_distribution: Optional[Dict[str, float]] = None
    toxicity_score: Optional[float] = None
    top_keywords: Optional[List[Dict[str, Any]]] = None
    time_series: Optional[List[Dict[str, Any]]] = None
    bot_likelihood: Optional[float] = None


class AddonAnalysisResults(BaseModel):
    discussion: Optional[AddonDiscussionResult] = None
    raw: Optional[Dict[str, Any]] = None


class AddonResponse(BaseModel):
    request_id: str
    addon_id: str
    status: str
    output_schema_version: str = "1.0"
    results: Optional[AddonAnalysisResults] = None
    error: Optional[Dict[str, Any]] = None
    meta: Optional[Dict[str, Any]] = None


# ============================================
# Bot Detection Service
# ============================================


class BotDetectionService:
    """AI 봇 탐지 서비스"""

    def __init__(self):
        self._model = None
        self._tokenizer = None
        self._model_loaded = False
        self._cache = TTLCache(maxsize=10000, ttl=settings.cache_ttl)

    def _load_model(self):
        """모델 lazy loading"""
        if self._model_loaded:
            return

        try:
            from transformers import AutoModelForSequenceClassification, AutoTokenizer
            import torch

            log.info("Loading bot detection model", model=settings.model_name)

            self._tokenizer = AutoTokenizer.from_pretrained(settings.model_name)
            self._model = AutoModelForSequenceClassification.from_pretrained(
                settings.model_name
            )

            # GPU 사용 가능하면 사용
            if torch.cuda.is_available():
                self._model = self._model.cuda()

            self._model.eval()
            self._model_loaded = True
            log.info("Bot detection model loaded successfully")

        except Exception as e:
            log.error("Failed to load bot detection model", error=str(e))
            self._model_loaded = False

    def _compute_text_hash(self, text: str) -> str:
        """텍스트 해시 생성"""
        return hashlib.sha256(text.encode()).hexdigest()[:16]

    def detect_bot(self, request: BotDetectionRequest) -> BotDetectionResponse:
        """봇 탐지 메인 함수"""
        text = request.text.strip()
        reasons = []
        pattern_flags = {}

        # 캐시 확인
        text_hash = self._compute_text_hash(text)
        if text_hash in self._cache:
            return self._cache[text_hash]

        # 1. 텍스트 길이 확인
        if len(text) < settings.min_text_length:
            return BotDetectionResponse(
                is_bot=False,
                confidence=0.0,
                detection_model="rule_based",
                detection_reasons=["text_too_short"],
                pattern_flags={"text_length": len(text)},
            )

        # 2. 패턴 기반 탐지
        pattern_score, pattern_reasons, pattern_details = self._pattern_based_detection(
            text
        )
        reasons.extend(pattern_reasons)
        pattern_flags.update(pattern_details)

        # 3. ML 모델 기반 탐지
        ml_score = 0.0
        perplexity = None
        burstiness = None

        try:
            self._load_model()
            if self._model_loaded:
                ml_score = self._ml_based_detection(text)
                perplexity = self._calculate_perplexity(text)
                burstiness = self._calculate_burstiness(text)

                if ml_score > 0.7:
                    reasons.append("ml_model_high_confidence")
                if perplexity and perplexity < 20:
                    reasons.append("low_perplexity_suspicious")
                if burstiness and burstiness < 0.3:
                    reasons.append("low_burstiness_suspicious")
        except Exception as e:
            log.warning("ML detection failed", error=str(e))

        # 4. 최종 점수 계산 (가중 평균)
        if self._model_loaded:
            final_score = (pattern_score * 0.4) + (ml_score * 0.6)
        else:
            final_score = pattern_score

        # 5. 반복률 계산
        repetition_rate = self._calculate_repetition_rate(text)
        if repetition_rate > settings.repetition_threshold:
            reasons.append("high_repetition_rate")
            pattern_flags["repetition_rate"] = repetition_rate
            final_score = max(final_score, 0.6)

        result = BotDetectionResponse(
            is_bot=final_score >= settings.bot_threshold,
            confidence=round(final_score, 4),
            detection_model="hybrid" if self._model_loaded else "rule_based",
            detection_reasons=reasons,
            pattern_flags=pattern_flags,
            perplexity=perplexity,
            burstiness=burstiness,
            repetition_rate=repetition_rate,
        )

        # 캐시 저장
        self._cache[text_hash] = result
        return result

    def _pattern_based_detection(self, text: str) -> tuple:
        """패턴 기반 봇 탐지"""
        score = 0.0
        reasons = []
        details = {}

        # 1. 이모지 과다 사용
        emoji_pattern = re.compile(
            r"[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F700-\U0001F77F\U0001F780-\U0001F7FF\U0001F800-\U0001F8FF\U0001F900-\U0001F9FF\U0001FA00-\U0001FA6F\U0001FA70-\U0001FAFF\U00002702-\U000027B0\U000024C2-\U0001F251]"
        )
        emoji_count = len(emoji_pattern.findall(text))
        emoji_ratio = emoji_count / max(len(text.split()), 1)
        if emoji_ratio > 0.3:
            score += 0.2
            reasons.append("excessive_emoji_usage")
            details["emoji_ratio"] = emoji_ratio

        # 2. URL 스팸
        url_pattern = re.compile(r"https?://\S+")
        url_count = len(url_pattern.findall(text))
        if url_count > 3:
            score += 0.3
            reasons.append("multiple_urls")
            details["url_count"] = url_count

        # 3. 반복적인 구문
        words = text.lower().split()
        if len(words) > 5:
            word_freq = Counter(words)
            max_repeat = max(word_freq.values())
            if max_repeat / len(words) > 0.3:
                score += 0.2
                reasons.append("repetitive_words")
                details["max_word_repeat_ratio"] = max_repeat / len(words)

        # 4. 너무 완벽한 문장 (AI 특성)
        # 마침표, 쉼표 등이 규칙적으로 배치된 경우
        sentences = re.split(r"[.!?]", text)
        if len(sentences) > 3:
            sentence_lengths = [len(s.split()) for s in sentences if s.strip()]
            if sentence_lengths:
                avg_len = sum(sentence_lengths) / len(sentence_lengths)
                variance = sum((l - avg_len) ** 2 for l in sentence_lengths) / len(
                    sentence_lengths
                )
                std_dev = math.sqrt(variance) if variance > 0 else 0
                # 너무 일관된 문장 길이는 AI 의심
                if std_dev < 2 and len(sentence_lengths) > 3:
                    score += 0.15
                    reasons.append("uniform_sentence_length")
                    details["sentence_length_std"] = std_dev

        # 5. 특정 봇 키워드 패턴
        bot_keywords = [
            r"ai\s+assistant",
            r"as\s+an\s+ai",
            r"i\'m\s+here\s+to\s+help",
            r"대화형\s+인공지능",
            r"AI\s+언어\s+모델",
        ]
        for pattern in bot_keywords:
            if re.search(pattern, text, re.IGNORECASE):
                score += 0.4
                reasons.append("bot_keyword_detected")
                details["bot_keyword_pattern"] = pattern
                break

        return min(score, 1.0), reasons, details

    def _ml_based_detection(self, text: str) -> float:
        """ML 모델 기반 AI 텍스트 탐지"""
        if not self._model_loaded:
            return 0.0

        try:
            import torch

            # 토큰화
            inputs = self._tokenizer(
                text, return_tensors="pt", truncation=True, max_length=512
            )

            # GPU로 이동
            if torch.cuda.is_available():
                inputs = {k: v.cuda() for k, v in inputs.items()}

            # 추론
            with torch.no_grad():
                outputs = self._model(**inputs)
                probs = torch.softmax(outputs.logits, dim=-1)

            # AI 생성 확률 (모델에 따라 인덱스가 다를 수 있음)
            # roberta-base-openai-detector: label 1 = AI generated
            ai_prob = probs[0][1].item()
            return ai_prob

        except Exception as e:
            log.error("ML detection error", error=str(e))
            return 0.0

    def _calculate_perplexity(self, text: str) -> Optional[float]:
        """텍스트 Perplexity 계산 (간단한 근사)"""
        try:
            # 단어 빈도 기반 간단한 perplexity 근사
            words = text.lower().split()
            if len(words) < 5:
                return None

            word_freq = Counter(words)
            total = len(words)

            # 엔트로피 계산
            entropy = 0
            for count in word_freq.values():
                prob = count / total
                if prob > 0:
                    entropy -= prob * math.log2(prob)

            # Perplexity = 2^entropy
            perplexity = 2**entropy
            return round(perplexity, 2)

        except Exception:
            return None

    def _calculate_burstiness(self, text: str) -> Optional[float]:
        """텍스트 Burstiness 계산"""
        try:
            # 문장 길이의 변동성으로 burstiness 근사
            sentences = re.split(r"[.!?]", text)
            sentence_lengths = [len(s.split()) for s in sentences if s.strip()]

            if len(sentence_lengths) < 3:
                return None

            mean_len = sum(sentence_lengths) / len(sentence_lengths)
            if mean_len == 0:
                return None

            variance = sum((l - mean_len) ** 2 for l in sentence_lengths) / len(
                sentence_lengths
            )
            std_dev = math.sqrt(variance)

            # 정규화된 burstiness (0~1, 높을수록 변동성 높음)
            burstiness = (
                std_dev / (std_dev + mean_len) if (std_dev + mean_len) > 0 else 0
            )
            return round(burstiness, 4)

        except Exception:
            return None

    def _calculate_repetition_rate(self, text: str) -> float:
        """텍스트 내 반복률 계산"""
        try:
            words = text.lower().split()
            if len(words) < 5:
                return 0.0

            unique_words = set(words)
            repetition_rate = 1 - (len(unique_words) / len(words))
            return round(repetition_rate, 4)

        except Exception:
            return 0.0


# ============================================
# User Forensics Service
# ============================================


class UserForensicsService:
    """사용자 포렌식 서비스"""

    def __init__(self, bot_detector: BotDetectionService):
        self.bot_detector = bot_detector

    def analyze_user_activity(
        self, request: UserProfileUpdateRequest
    ) -> UserProfileResponse:
        """사용자 활동 분석 및 프로필 업데이트"""

        # 1. 활동 패턴 분석
        activity_pattern = self._analyze_activity_pattern(request.activity_timestamps)

        # 2. 작문 스타일 분석
        writing_style = self._analyze_writing_style(request.contents)

        # 3. 봇 확률 계산
        bot_probability = self._calculate_bot_probability(
            activity_pattern, writing_style, request.contents
        )

        # 4. 트롤 점수 계산
        troll_score = self._calculate_troll_score(writing_style)

        # 5. 신뢰도 점수 계산
        credibility_score = 1.0 - (bot_probability * 0.6 + troll_score * 0.4)

        return UserProfileResponse(
            user_hash=request.user_hash,
            bot_probability=round(bot_probability, 4),
            troll_score=round(troll_score, 4),
            credibility_score=round(max(0, credibility_score), 4),
            activity_pattern=activity_pattern,
            writing_style=writing_style,
        )

    def _analyze_activity_pattern(self, timestamps: List[datetime]) -> Dict[str, Any]:
        """활동 패턴 분석"""
        if not timestamps:
            return {}

        pattern = {
            "total_activities": len(timestamps),
            "hour_distribution": {},
            "is_24h_active": False,
            "avg_interval_seconds": 0,
            "suspicious_burst": False,
        }

        # 시간대별 분포
        hours = [ts.hour for ts in timestamps]
        hour_dist = Counter(hours)
        pattern["hour_distribution"] = dict(hour_dist)

        # 24시간 활동 여부 (봇 의심)
        active_hours = len(hour_dist)
        pattern["is_24h_active"] = active_hours >= 20

        # 활동 간격 분석
        if len(timestamps) > 1:
            sorted_ts = sorted(timestamps)
            intervals = [
                (sorted_ts[i + 1] - sorted_ts[i]).total_seconds()
                for i in range(len(sorted_ts) - 1)
            ]
            avg_interval = sum(intervals) / len(intervals)
            pattern["avg_interval_seconds"] = round(avg_interval, 2)

            # 1분 이내 연속 활동이 많으면 봇 의심
            burst_count = sum(1 for i in intervals if i < 60)
            pattern["suspicious_burst"] = burst_count > len(intervals) * 0.3

        return pattern

    def _analyze_writing_style(self, contents: List[str]) -> Dict[str, Any]:
        """작문 스타일 분석"""
        if not contents:
            return {}

        style = {
            "avg_length": 0,
            "vocabulary_diversity": 0,
            "sentiment_variance": 0,
            "formality_score": 0,
            "aggression_score": 0,
        }

        # 평균 길이
        lengths = [len(c) for c in contents]
        style["avg_length"] = round(sum(lengths) / len(lengths), 2)

        # 어휘 다양성
        all_words = []
        for content in contents:
            all_words.extend(content.lower().split())
        if all_words:
            unique_ratio = len(set(all_words)) / len(all_words)
            style["vocabulary_diversity"] = round(unique_ratio, 4)

        # 공격성 점수 (간단한 키워드 기반)
        aggressive_keywords = [
            "바보",
            "멍청",
            "꺼져",
            "죽어",
            "쓰레기",
            "놈",
            "년",
            "병신",
            "새끼",
        ]
        all_text = " ".join(contents).lower()
        aggression_count = sum(1 for kw in aggressive_keywords if kw in all_text)
        style["aggression_score"] = min(aggression_count / max(len(contents), 1), 1.0)

        return style

    def _calculate_bot_probability(
        self,
        activity_pattern: Dict[str, Any],
        writing_style: Dict[str, Any],
        contents: List[str],
    ) -> float:
        """봇 확률 종합 계산"""
        score = 0.0

        # 활동 패턴 기반
        if activity_pattern.get("is_24h_active"):
            score += 0.3
        if activity_pattern.get("suspicious_burst"):
            score += 0.25
        avg_interval = activity_pattern.get("avg_interval_seconds", 1000)
        if avg_interval < 30:  # 30초 미만 평균 간격
            score += 0.2

        # 작문 스타일 기반
        vocab_div = writing_style.get("vocabulary_diversity", 0.5)
        if vocab_div < 0.3:  # 낮은 어휘 다양성
            score += 0.15

        # 콘텐츠 ML 분석
        if contents:
            sample_contents = contents[:10]  # 최대 10개만 분석
            ml_scores = []
            for content in sample_contents:
                if len(content) >= settings.min_text_length:
                    result = self.bot_detector.detect_bot(
                        BotDetectionRequest(text=content)
                    )
                    ml_scores.append(result.confidence)
            if ml_scores:
                avg_ml_score = sum(ml_scores) / len(ml_scores)
                score += avg_ml_score * 0.3

        return min(score, 1.0)

    def _calculate_troll_score(self, writing_style: Dict[str, Any]) -> float:
        """트롤 점수 계산"""
        score = 0.0

        # 공격성 기반
        aggression = writing_style.get("aggression_score", 0)
        score += aggression * 0.7

        # 짧은 글 위주 (트롤 특성)
        avg_length = writing_style.get("avg_length", 100)
        if avg_length < 50:
            score += 0.2

        return min(score, 1.0)


# ============================================
# FastAPI Application
# ============================================

app = FastAPI(
    title="Bot Detection Service",
    description="AI 봇 탐지 및 사용자 포렌식 서비스",
    version="1.0.0",
)

# Setup Prometheus metrics
SERVICE_NAME = "bot-detector"
if METRICS_AVAILABLE:
    setup_metrics(app, SERVICE_NAME, version="1.0.0")
    service_metrics = ServiceMetrics(SERVICE_NAME)
    # Create service-specific metrics
    detections_total = service_metrics.create_counter(
        "detections_total",
        "Total bot detection requests",
        ["result", "detection_model"],
    )
    detection_confidence = service_metrics.create_histogram(
        "detection_confidence",
        "Bot detection confidence distribution",
        ["result"],
        buckets=(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0),
    )
    user_analyses = service_metrics.create_counter(
        "user_analyses_total", "Total user profile analyses", ["status"]
    )
    log.info("Prometheus metrics enabled for bot-detector service")
else:
    service_metrics = None
    log.warning("Prometheus metrics not available - shared module not found")

# 서비스 인스턴스
bot_detector = BotDetectionService()
user_forensics = UserForensicsService(bot_detector)


@app.get("/health")
async def health_check():
    """헬스 체크"""
    return {
        "status": "healthy",
        "model_loaded": bot_detector._model_loaded,
        "timestamp": datetime.now(timezone.utc).isoformat(),
    }


@app.post("/detect", response_model=BotDetectionResponse)
async def detect_bot(request: BotDetectionRequest):
    """단일 텍스트 봇 탐지"""
    try:
        return bot_detector.detect_bot(request)
    except Exception as e:
        log.error("Bot detection failed", error=str(e))
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/analyze", response_model=AddonResponse)
async def analyze_addon(request: AddonRequest):
    start_time = time.time()
    try:
        texts: List[str] = []
        if request.comments and request.comments.items:
            for item in request.comments.items:
                if item.content:
                    texts.append(item.content)

        if not texts and request.article:
            parts: List[str] = []
            if request.article.title:
                parts.append(request.article.title)
            if request.article.content:
                parts.append(request.article.content)
            merged = "\n".join(parts).strip()
            if merged:
                texts = [merged]

        results: List[BotDetectionResponse] = []
        confidences: List[float] = []
        reasons_set: set[str] = set()
        merged_flags: Dict[str, Any] = {}

        for text in texts:
            r = bot_detector.detect_bot(BotDetectionRequest(text=text))
            results.append(r)
            confidences.append(float(r.confidence))
            for reason in r.detection_reasons:
                reasons_set.add(reason)
            for k, v in r.pattern_flags.items():
                if k not in merged_flags:
                    merged_flags[k] = v

        avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0
        latency_ms = int((time.time() - start_time) * 1000)

        return AddonResponse(
            request_id=request.request_id,
            addon_id=request.addon_id,
            status="success",
            results=AddonAnalysisResults(
                discussion=AddonDiscussionResult(
                    bot_likelihood=round(avg_confidence, 4),
                ),
                raw={
                    "total": len(results),
                    "avg_confidence": round(avg_confidence, 4),
                    "detection_reasons": sorted(reasons_set),
                    "pattern_flags": merged_flags,
                },
            ),
            meta={
                "model_version": settings.model_name,
                "latency_ms": latency_ms,
                "processed_at": datetime.now(timezone.utc).isoformat(),
            },
        )
    except Exception as e:
        log.error("Addon analyze failed", error=str(e))
        return AddonResponse(
            request_id=request.request_id,
            addon_id=request.addon_id,
            status="error",
            error={"code": "BOT_DETECTOR_ERROR", "message": str(e)},
            meta={
                "model_version": settings.model_name,
                "latency_ms": int((time.time() - start_time) * 1000),
                "processed_at": datetime.now(timezone.utc).isoformat(),
            },
        )


@app.post("/detect/batch", response_model=BatchDetectionResponse)
async def detect_bot_batch(request: BatchDetectionRequest):
    """배치 봇 탐지"""
    try:
        results = []
        bot_count = 0

        for item in request.items:
            result = bot_detector.detect_bot(item)
            results.append(result)
            if result.is_bot:
                bot_count += 1

        return BatchDetectionResponse(
            results=results, total=len(results), bot_count=bot_count
        )
    except Exception as e:
        log.error("Batch detection failed", error=str(e))
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/user/analyze", response_model=UserProfileResponse)
async def analyze_user(request: UserProfileUpdateRequest):
    """사용자 활동 분석"""
    try:
        return user_forensics.analyze_user_activity(request)
    except Exception as e:
        log.error("User analysis failed", error=str(e))
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/preload-model")
async def preload_model():
    """ML 모델 사전 로딩"""
    try:
        bot_detector._load_model()
        return {"status": "success", "model_loaded": bot_detector._model_loaded}
    except Exception as e:
        log.error("Model preload failed", error=str(e))
        raise HTTPException(status_code=500, detail=str(e))


# ============================================
# Main
# ============================================

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8040)

```

---

## backend/services/browser-use-api/aidove_chat.py

```py
"""
AIDove Chat Model for Browser-Use

Implements the BaseChatModel protocol to integrate AIDove with browser-use Agent.
"""

import asyncio
import json
import logging
import os
from typing import Any, TypeVar, overload

import httpx
from pydantic import BaseModel

logger = logging.getLogger(__name__)

T = TypeVar('T', bound=BaseModel)

# AiDove Webhook URL
AIDOVE_WEBHOOK_URL = os.environ.get(
    "AIDOVE_WEBHOOK_URL", "https://workflow.nodove.com/webhook/aidove"
)


class ChatInvokeUsage(BaseModel):
    """Usage information for a chat model invocation."""
    prompt_tokens: int = 0
    prompt_cached_tokens: int | None = None
    prompt_cache_creation_tokens: int | None = None
    prompt_image_tokens: int | None = None
    completion_tokens: int = 0
    total_tokens: int = 0


class ChatInvokeCompletion(BaseModel):
    """Response from a chat model invocation."""
    completion: Any
    thinking: str | None = None
    redacted_thinking: str | None = None
    usage: ChatInvokeUsage | None = None
    stop_reason: str | None = None


class ChatAIDove:
    """
    AIDove LLM wrapper compatible with browser-use's BaseChatModel protocol.
    
    This integrates the AIDove webhook API with browser-use's agent system.
    """
    
    _verified_api_keys: bool = True
    model: str = "aidove"
    
    def __init__(
        self,
        session_id: str | None = None,
        timeout: float = 120.0,
        max_retries: int = 3,
        webhook_url: str | None = None,
    ):
        self.session_id = session_id
        self.timeout = timeout
        self.max_retries = max_retries
        self.webhook_url = webhook_url or AIDOVE_WEBHOOK_URL
        self._client: httpx.AsyncClient | None = None
    
    @property
    def provider(self) -> str:
        return "aidove"
    
    @property
    def name(self) -> str:
        return "aidove"
    
    @property
    def model_name(self) -> str:
        return self.model
    
    async def _get_client(self) -> httpx.AsyncClient:
        if self._client is None:
            self._client = httpx.AsyncClient(timeout=self.timeout)
        return self._client
    
    async def close(self) -> None:
        if self._client is not None:
            await self._client.aclose()
            self._client = None
    
    def _format_messages(self, messages: list[Any]) -> str:
        """Convert browser-use messages to a single prompt string for AIDove."""
        parts = []
        for msg in messages:
            if hasattr(msg, 'role') and hasattr(msg, 'content'):
                role = msg.role
                content = msg.content
                
                # Handle content that might be a list of parts
                if isinstance(content, list):
                    text_parts = []
                    for part in content:
                        if hasattr(part, 'text'):
                            text_parts.append(part.text)
                        elif isinstance(part, dict) and 'text' in part:
                            text_parts.append(part['text'])
                        elif isinstance(part, str):
                            text_parts.append(part)
                    content = '\n'.join(text_parts)
                
                if role == 'system':
                    parts.append(f"[System]\n{content}")
                elif role == 'user':
                    parts.append(f"[User]\n{content}")
                elif role == 'assistant':
                    parts.append(f"[Assistant]\n{content}")
                else:
                    parts.append(content)
            elif isinstance(msg, str):
                parts.append(msg)
        
        return '\n\n'.join(parts)
    
    def _parse_structured_response(self, response_text: str, output_format: type[T] | None) -> T | str:
        """Parse response into structured format if requested."""
        if output_format is None:
            return response_text
        
        try:
            # Try to extract JSON from the response
            # Look for JSON block markers
            if '``\`json' in response_text:
                start = response_text.find('``\`json') + 7
                end = response_text.find('``\`', start)
                if end > start:
                    json_str = response_text[start:end].strip()
                    data = json.loads(json_str)
                    return output_format.model_validate(data)
            
            # Try to parse the whole response as JSON
            if response_text.strip().startswith('{'):
                data = json.loads(response_text)
                return output_format.model_validate(data)
            
            # If all else fails, return as string
            return response_text
        except (json.JSONDecodeError, Exception) as e:
            logger.warning(f"Failed to parse structured response: {e}")
            return response_text
    
    @overload
    async def ainvoke(
        self, messages: list[Any], output_format: None = None, **kwargs: Any
    ) -> ChatInvokeCompletion: ...
    
    @overload
    async def ainvoke(
        self, messages: list[Any], output_format: type[T], **kwargs: Any
    ) -> ChatInvokeCompletion: ...
    
    async def ainvoke(
        self, messages: list[Any], output_format: type[T] | None = None, **kwargs: Any
    ) -> ChatInvokeCompletion:
        """
        Invoke the AIDove model with the given messages.
        
        Args:
            messages: List of messages in browser-use format
            output_format: Optional Pydantic model for structured output
            **kwargs: Additional arguments (ignored)
            
        Returns:
            ChatInvokeCompletion with the response
        """
        prompt = self._format_messages(messages)
        
        # Add structured output instructions if format is requested
        if output_format is not None:
            schema = output_format.model_json_schema()
            prompt += f"\n\n[Important: Respond with valid JSON matching this schema]\n{json.dumps(schema, indent=2)}"
        
        payload = {"chatInput": prompt}
        if self.session_id:
            payload["sessionId"] = self.session_id
        
        client = await self._get_client()
        last_error = None
        
        for attempt in range(self.max_retries):
            try:
                resp = await client.post(self.webhook_url, json=payload)
                resp.raise_for_status()
                data = resp.json()
                
                response_text = data.get("reply", data.get("output", ""))
                
                if not response_text:
                    logger.warning(f"Empty response from AIDove (attempt {attempt + 1})")
                    if attempt < self.max_retries - 1:
                        await asyncio.sleep(1.0 * (attempt + 1))
                        continue
                
                # Parse response
                completion = self._parse_structured_response(response_text, output_format)
                
                # Estimate token usage (rough approximation)
                prompt_tokens = len(prompt) // 4
                completion_tokens = len(response_text) // 4 if response_text else 0
                
                return ChatInvokeCompletion(
                    completion=completion,
                    thinking=None,
                    usage=ChatInvokeUsage(
                        prompt_tokens=prompt_tokens,
                        completion_tokens=completion_tokens,
                        total_tokens=prompt_tokens + completion_tokens,
                    ),
                    stop_reason="end_turn",
                )
                
            except httpx.TimeoutException as e:
                last_error = e
                logger.warning(f"AIDove timeout (attempt {attempt + 1}/{self.max_retries})")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(2.0 * (attempt + 1))
                    
            except httpx.HTTPError as e:
                last_error = e
                logger.warning(f"AIDove HTTP error (attempt {attempt + 1}/{self.max_retries}): {e}")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(1.0 * (attempt + 1))
        
        # All retries failed
        error_msg = f"AIDove invocation failed after {self.max_retries} attempts: {last_error}"
        logger.error(error_msg)
        
        return ChatInvokeCompletion(
            completion=error_msg if output_format is None else output_format.model_validate({}),
            thinking=None,
            usage=ChatInvokeUsage(prompt_tokens=0, completion_tokens=0, total_tokens=0),
            stop_reason="error",
        )

```

---

## backend/services/browser-use-api/api_key_provisioner.py

```py
"""
API Key Auto-Provisioner for Browser-Use API

This module provides automated API key provisioning via browser automation.
Users can request API keys from various providers (OpenAI, Anthropic, etc.)
through a chat interface, and the system will navigate to the provider's
website, handle authentication, and extract the generated API key.

Workflow:
1. User requests API key via chat (e.g., "OpenAI API 키 발급해줘")
2. IntentAnalyzer detects API_KEY_PROVISION intent and identifies provider
3. ApiKeyProvisioner launches stealth browser to provider's API key page
4. System handles login with human intervention for 2FA if needed
5. Extracts generated API key from the page
6. Saves to LlmProviderSettings via REST API
"""

import asyncio
import base64
import logging
import os
import re
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Optional

import httpx
from pydantic import BaseModel, Field

from intent_analyzer import APIProvider, PROVIDER_URLS, IntentAnalyzer

logger = logging.getLogger(__name__)


class ProvisioningStatus(str, Enum):
	"""Status of API key provisioning task."""

	PENDING = 'pending'
	NAVIGATING = 'navigating'
	WAITING_LOGIN = 'waiting_login'
	WAITING_2FA = 'waiting_2fa'
	GENERATING_KEY = 'generating_key'
	EXTRACTING_KEY = 'extracting_key'
	SAVING_KEY = 'saving_key'
	COMPLETED = 'completed'
	FAILED = 'failed'
	CANCELLED = 'cancelled'


class ProvisioningStep(str, Enum):
	"""Steps in the provisioning workflow."""

	NAVIGATE_TO_PROVIDER = 'navigate_to_provider'
	CHECK_LOGIN_STATUS = 'check_login_status'
	REQUEST_LOGIN = 'request_login'
	HANDLE_2FA = 'handle_2fa'
	NAVIGATE_TO_API_KEYS = 'navigate_to_api_keys'
	CREATE_NEW_KEY = 'create_new_key'
	EXTRACT_KEY = 'extract_key'
	SAVE_TO_SETTINGS = 'save_to_settings'


@dataclass
class ProvisioningContext:
	"""Context for API key provisioning task."""

	provider: APIProvider
	provider_url: str
	key_name: str = 'NewsInsight-AutoGenerated'
	extracted_key: Optional[str] = None
	status: ProvisioningStatus = ProvisioningStatus.PENDING
	current_step: Optional[ProvisioningStep] = None
	error: Optional[str] = None
	started_at: Optional[datetime] = None
	completed_at: Optional[datetime] = None
	screenshots: list[str] = field(default_factory=list)


class ProvisioningRequest(BaseModel):
	"""Request to provision an API key."""

	provider: str = Field(..., description='Provider name: openai, anthropic, google, etc.')
	key_name: Optional[str] = Field(None, description='Name for the generated key')
	auto_save: bool = Field(True, description='Automatically save to system settings')
	timeout_seconds: int = Field(300, description='Timeout for entire provisioning process')


class ProvisioningResponse(BaseModel):
	"""Response from API key provisioning."""

	job_id: str
	status: str
	provider: str
	message: str
	api_key_masked: Optional[str] = None  # e.g., "sk-...abc"
	saved_to_settings: bool = False
	error: Optional[str] = None


# Provider-specific selectors and patterns
PROVIDER_CONFIGS = {
	APIProvider.OPENAI: {
		'login_url': 'https://platform.openai.com/login',
		'api_keys_url': 'https://platform.openai.com/api-keys',
		'dashboard_url': 'https://platform.openai.com',
		'login_indicators': [
			'input[name="email"]',
			'input[name="username"]',
			'button[type="submit"]',
		],
		'logged_in_indicators': [
			'[data-testid="account-menu"]',
			'.avatar',
			'#user-menu',
		],
		'create_key_selectors': [
			'button:has-text("Create new secret key")',
			'[data-testid="create-api-key"]',
			'button:has-text("Create")',
		],
		'key_pattern': r'sk-[a-zA-Z0-9]{20,}',
		'key_display_selectors': [
			'.api-key-value',
			'[data-testid="api-key"]',
			'input[readonly][value^="sk-"]',
			'code:has-text("sk-")',
		],
	},
	APIProvider.ANTHROPIC: {
		'login_url': 'https://console.anthropic.com/login',
		'api_keys_url': 'https://console.anthropic.com/settings/keys',
		'dashboard_url': 'https://console.anthropic.com',
		'login_indicators': [
			'input[name="email"]',
			'input[type="email"]',
		],
		'logged_in_indicators': [
			'.user-avatar',
			'[data-testid="user-menu"]',
		],
		'create_key_selectors': [
			'button:has-text("Create Key")',
			'button:has-text("Generate")',
		],
		'key_pattern': r'sk-ant-[a-zA-Z0-9\-_]{40,}',
		'key_display_selectors': [
			'.api-key',
			'[data-testid="api-key-value"]',
			'input[readonly]',
		],
	},
	APIProvider.GOOGLE: {
		'login_url': 'https://accounts.google.com',
		'api_keys_url': 'https://aistudio.google.com/app/apikey',
		'dashboard_url': 'https://aistudio.google.com',
		'login_indicators': [
			'input[type="email"]',
			'#identifierId',
		],
		'logged_in_indicators': [
			'a[aria-label*="Google Account"]',
			'.gb_d',
		],
		'create_key_selectors': [
			'button:has-text("Create API key")',
			'button:has-text("Get API key")',
		],
		'key_pattern': r'AIza[a-zA-Z0-9\-_]{35}',
		'key_display_selectors': [
			'.api-key-display',
			'[data-testid="api-key"]',
		],
	},
	APIProvider.OPENROUTER: {
		'login_url': 'https://openrouter.ai/auth/login',
		'api_keys_url': 'https://openrouter.ai/keys',
		'dashboard_url': 'https://openrouter.ai',
		'login_indicators': [
			'input[name="email"]',
			'button:has-text("Sign in")',
		],
		'logged_in_indicators': [
			'.user-menu',
			'[data-testid="user-avatar"]',
		],
		'create_key_selectors': [
			'button:has-text("Create Key")',
			'button:has-text("New Key")',
		],
		'key_pattern': r'sk-or-[a-zA-Z0-9]{40,}',
		'key_display_selectors': [
			'.api-key',
			'input[readonly]',
		],
	},
}


class ApiKeyProvisioner:
	"""
	Handles automated API key provisioning via browser automation.

	This class orchestrates the entire provisioning workflow:
	1. Navigate to provider's website
	2. Handle authentication (with human intervention if needed)
	3. Generate new API key
	4. Extract and save the key
	"""

	def __init__(
		self,
		browser_session: Any,
		llm: Any,
		websocket_broadcast: Any = None,
		data_collection_api_url: Optional[str] = None,
	):
		"""
		Initialize the provisioner.

		Args:
			browser_session: BrowserSession instance from browser-use
			llm: LLM instance for AI-guided navigation
			websocket_broadcast: Function to broadcast updates to WebSocket clients
			data_collection_api_url: URL of data-collection-service for saving keys
		"""
		self.browser_session = browser_session
		self.llm = llm
		self.websocket_broadcast = websocket_broadcast
		self.data_collection_api_url = data_collection_api_url or os.environ.get(
			'DATA_COLLECTION_API_URL', 'http://data-collection-service:8081'
		)
		self.context: Optional[ProvisioningContext] = None
		self._http_client: Optional[httpx.AsyncClient] = None

	async def _get_http_client(self) -> httpx.AsyncClient:
		"""Get or create HTTP client."""
		if self._http_client is None:
			self._http_client = httpx.AsyncClient(timeout=30.0)
		return self._http_client

	async def close(self):
		"""Clean up resources."""
		if self._http_client:
			await self._http_client.aclose()
			self._http_client = None

	async def _broadcast(self, message: dict):
		"""Broadcast message to WebSocket clients."""
		if self.websocket_broadcast:
			try:
				await self.websocket_broadcast(message)
			except Exception as e:
				logger.warning(f'Failed to broadcast: {e}')

	async def _update_status(self, status: ProvisioningStatus, step: Optional[ProvisioningStep] = None, message: str = ''):
		"""Update provisioning status and broadcast to clients."""
		if self.context:
			self.context.status = status
			self.context.current_step = step

		await self._broadcast(
			{
				'type': 'provisioning_update',
				'status': status.value,
				'step': step.value if step else None,
				'message': message,
			}
		)
		logger.info(f'Provisioning status: {status.value} - {message}')

	async def _capture_screenshot(self) -> Optional[str]:
		"""Capture current browser screenshot."""
		try:
			if self.browser_session:
				screenshot_bytes = await self.browser_session.take_screenshot()
				return base64.b64encode(screenshot_bytes).decode('utf-8')
		except Exception as e:
			logger.warning(f'Failed to capture screenshot: {e}')
		return None

	async def _get_page(self):
		"""Get the current page from browser session."""
		try:
			context = getattr(self.browser_session, '_context', None)
			if context:
				pages = context.pages
				if pages:
					return pages[0]
		except Exception as e:
			logger.warning(f'Failed to get page: {e}')
		return None

	async def _check_login_status(self, provider: APIProvider) -> bool:
		"""Check if user is logged in to the provider."""
		page = await self._get_page()
		if not page:
			return False

		config = PROVIDER_CONFIGS.get(provider, {})
		logged_in_indicators = config.get('logged_in_indicators', [])

		for selector in logged_in_indicators:
			try:
				element = await page.query_selector(selector)
				if element:
					logger.info(f'User is logged in (found: {selector})')
					return True
			except Exception:
				continue

		return False

	async def _request_login_intervention(self, provider: APIProvider) -> bool:
		"""Request human intervention for login."""
		await self._update_status(
			ProvisioningStatus.WAITING_LOGIN,
			ProvisioningStep.REQUEST_LOGIN,
			f'{provider.value} 로그인이 필요합니다. 브라우저에서 로그인을 진행해주세요.',
		)

		screenshot = await self._capture_screenshot()

		await self._broadcast(
			{
				'type': 'intervention_requested',
				'intervention_type': 'login',
				'provider': provider.value,
				'reason': f'Please log in to {provider.value} to continue API key provisioning.',
				'screenshot': screenshot,
				'instructions': [
					f'1. {provider.value} 계정으로 로그인하세요',
					'2. 2차 인증(2FA)이 필요한 경우 완료해주세요',
					'3. 로그인이 완료되면 "완료" 버튼을 클릭하세요',
				],
			}
		)

		# Wait for human to complete login (this will be handled by WebSocket)
		return True

	async def _request_2fa_intervention(self, provider: APIProvider) -> Optional[str]:
		"""Request human intervention for 2FA code."""
		await self._update_status(ProvisioningStatus.WAITING_2FA, ProvisioningStep.HANDLE_2FA, '2차 인증 코드가 필요합니다.')

		screenshot = await self._capture_screenshot()

		await self._broadcast(
			{
				'type': 'intervention_requested',
				'intervention_type': '2fa',
				'provider': provider.value,
				'reason': 'Two-factor authentication code required.',
				'screenshot': screenshot,
				'instructions': [
					'휴대폰으로 전송된 인증 코드를 입력해주세요.',
					'또는 인증 앱의 코드를 입력해주세요.',
				],
			}
		)

		# This will return the 2FA code from human input
		return None

	async def _navigate_to_api_keys_page(self, provider: APIProvider) -> bool:
		"""Navigate to the API keys management page."""
		page = await self._get_page()
		if not page:
			return False

		api_keys_url = PROVIDER_URLS.get(provider)
		if not api_keys_url:
			config = PROVIDER_CONFIGS.get(provider, {})
			api_keys_url = config.get('api_keys_url')

		if not api_keys_url:
			logger.error(f'No API keys URL configured for {provider.value}')
			return False

		await self._update_status(
			ProvisioningStatus.NAVIGATING, ProvisioningStep.NAVIGATE_TO_API_KEYS, f'API 키 페이지로 이동 중: {api_keys_url}'
		)

		try:
			await page.goto(api_keys_url, wait_until='networkidle')
			await asyncio.sleep(2)  # Wait for page to stabilize
			return True
		except Exception as e:
			logger.error(f'Failed to navigate to API keys page: {e}')
			return False

	async def _create_new_key(self, provider: APIProvider) -> bool:
		"""Click the create new key button."""
		page = await self._get_page()
		if not page:
			return False

		config = PROVIDER_CONFIGS.get(provider, {})
		create_selectors = config.get('create_key_selectors', [])

		await self._update_status(ProvisioningStatus.GENERATING_KEY, ProvisioningStep.CREATE_NEW_KEY, '새 API 키 생성 중...')

		for selector in create_selectors:
			try:
				# Try different approaches for finding the button
				element = await page.query_selector(selector)
				if element:
					await element.click()
					await asyncio.sleep(2)
					logger.info(f'Clicked create key button: {selector}')
					return True
			except Exception as e:
				logger.debug(f'Selector {selector} failed: {e}')
				continue

		# Try using AI to find and click the button
		logger.info('Using AI to find create key button...')
		return False

	async def _extract_api_key(self, provider: APIProvider) -> Optional[str]:
		"""Extract the generated API key from the page."""
		page = await self._get_page()
		if not page:
			return None

		config = PROVIDER_CONFIGS.get(provider, {})
		key_pattern = config.get('key_pattern', r'[a-zA-Z0-9\-_]{20,}')
		key_selectors = config.get('key_display_selectors', [])

		await self._update_status(ProvisioningStatus.EXTRACTING_KEY, ProvisioningStep.EXTRACT_KEY, 'API 키 추출 중...')

		# Method 1: Try specific selectors
		for selector in key_selectors:
			try:
				element = await page.query_selector(selector)
				if element:
					# Try different ways to get the key value
					key = await element.get_attribute('value')
					if not key:
						key = await element.inner_text()

					if key and re.match(key_pattern, key.strip()):
						logger.info(f'Extracted API key from selector: {selector}')
						return key.strip()
			except Exception as e:
				logger.debug(f'Selector {selector} failed: {e}')
				continue

		# Method 2: Search entire page content for key pattern
		try:
			page_content = await page.content()
			matches = re.findall(key_pattern, page_content)
			if matches:
				# Return the longest match (likely the full key)
				key = max(matches, key=len)
				logger.info(f'Extracted API key from page content (pattern match)')
				return key
		except Exception as e:
			logger.warning(f'Failed to extract key from page content: {e}')

		# Method 3: Try to copy from clipboard after clicking copy button
		try:
			copy_button = await page.query_selector('button:has-text("Copy")')
			if copy_button:
				await copy_button.click()
				await asyncio.sleep(0.5)
				# Note: Getting clipboard content requires additional permissions
		except Exception:
			pass

		return None

	async def _save_to_settings(self, provider: APIProvider, api_key: str) -> bool:
		"""Save the API key to LlmProviderSettings."""
		await self._update_status(ProvisioningStatus.SAVING_KEY, ProvisioningStep.SAVE_TO_SETTINGS, '시스템 설정에 저장 중...')

		# Map APIProvider to LlmProviderType
		provider_type_map = {
			APIProvider.OPENAI: 'OPENAI',
			APIProvider.ANTHROPIC: 'ANTHROPIC',
			APIProvider.GOOGLE: 'GOOGLE',
			APIProvider.OPENROUTER: 'OPENROUTER',
			APIProvider.TOGETHER_AI: 'TOGETHER_AI',
			APIProvider.PERPLEXITY: 'PERPLEXITY',
			APIProvider.BRAVE_SEARCH: 'BRAVE_SEARCH',
			APIProvider.TAVILY: 'TAVILY',
		}

		provider_type = provider_type_map.get(provider, provider.value.upper())

		# Default models for each provider
		default_models = {
			'OPENAI': 'gpt-4o',
			'ANTHROPIC': 'claude-3-5-sonnet-20241022',
			'GOOGLE': 'gemini-1.5-pro',
			'OPENROUTER': 'openai/gpt-4o',
			'TOGETHER_AI': 'meta-llama/Llama-3-70b-chat-hf',
			'PERPLEXITY': 'llama-3.1-sonar-large-128k-online',
		}

		payload = {
			'providerType': provider_type,
			'apiKey': api_key,
			'defaultModel': default_models.get(provider_type, ''),
			'enabled': True,
			'priority': 100,
		}

		try:
			client = await self._get_http_client()
			response = await client.put(
				f'{self.data_collection_api_url}/api/v1/admin/llm-providers',
				json=payload,
			)

			if response.status_code in [200, 201]:
				logger.info(f'Successfully saved API key for {provider_type}')
				return True
			else:
				logger.error(f'Failed to save API key: {response.status_code} - {response.text}')
				return False

		except Exception as e:
			logger.error(f'Failed to save API key to settings: {e}')
			return False

	def _mask_api_key(self, api_key: str) -> str:
		"""Mask API key for display (show first 6 and last 4 chars)."""
		if len(api_key) <= 10:
			return '*' * len(api_key)
		return f'{api_key[:6]}...{api_key[-4:]}'

	async def provision(
		self,
		provider: APIProvider,
		key_name: str = 'NewsInsight-AutoGenerated',
		auto_save: bool = True,
		timeout_seconds: int = 300,
	) -> dict:
		"""
		Execute the full API key provisioning workflow.

		Args:
			provider: The API provider to get a key from
			key_name: Name to give the generated key
			auto_save: Whether to automatically save to system settings
			timeout_seconds: Timeout for the entire process

		Returns:
			Dictionary with provisioning result
		"""
		self.context = ProvisioningContext(
			provider=provider,
			provider_url=PROVIDER_URLS.get(provider, ''),
			key_name=key_name,
			started_at=datetime.now(),
		)

		try:
			# Step 1: Navigate to provider
			await self._update_status(
				ProvisioningStatus.NAVIGATING, ProvisioningStep.NAVIGATE_TO_PROVIDER, f'{provider.value} 웹사이트로 이동 중...'
			)

			page = await self._get_page()
			if not page:
				raise Exception('Browser session not available')

			# Navigate to the provider's dashboard first
			config = PROVIDER_CONFIGS.get(provider, {})
			dashboard_url = config.get('dashboard_url', PROVIDER_URLS.get(provider))

			await page.goto(dashboard_url, wait_until='networkidle')
			await asyncio.sleep(2)

			# Step 2: Check login status
			is_logged_in = await self._check_login_status(provider)

			if not is_logged_in:
				# Step 3: Request human intervention for login
				await self._request_login_intervention(provider)

				# Wait for login completion (handled via WebSocket)
				# This is a placeholder - actual implementation requires
				# WebSocket message handling to signal login completion
				return {
					'status': 'waiting_login',
					'provider': provider.value,
					'message': 'Please complete login in the browser window.',
					'requires_intervention': True,
				}

			# Step 4: Navigate to API keys page
			success = await self._navigate_to_api_keys_page(provider)
			if not success:
				raise Exception('Failed to navigate to API keys page')

			# Step 5: Create new key
			success = await self._create_new_key(provider)
			if not success:
				# Request human intervention to create key
				await self._broadcast(
					{
						'type': 'intervention_requested',
						'intervention_type': 'create_key',
						'provider': provider.value,
						'reason': 'Please click the button to create a new API key.',
						'screenshot': await self._capture_screenshot(),
					}
				)

				return {
					'status': 'waiting_create_key',
					'provider': provider.value,
					'message': 'Please click to create a new API key.',
					'requires_intervention': True,
				}

			# Step 6: Extract the generated key
			api_key = await self._extract_api_key(provider)

			if not api_key:
				raise Exception('Failed to extract API key from page')

			self.context.extracted_key = api_key

			# Step 7: Save to settings if requested
			saved = False
			if auto_save:
				saved = await self._save_to_settings(provider, api_key)

			# Success!
			self.context.status = ProvisioningStatus.COMPLETED
			self.context.completed_at = datetime.now()

			await self._update_status(ProvisioningStatus.COMPLETED, None, f'API 키가 성공적으로 발급되었습니다!')

			return {
				'status': 'completed',
				'provider': provider.value,
				'api_key_masked': self._mask_api_key(api_key),
				'saved_to_settings': saved,
				'message': f'{provider.value} API 키가 성공적으로 발급되었습니다.',
			}

		except asyncio.TimeoutError:
			self.context.status = ProvisioningStatus.FAILED
			self.context.error = 'Provisioning timed out'

			await self._update_status(ProvisioningStatus.FAILED, None, '시간 초과로 인해 키 발급에 실패했습니다.')

			return {
				'status': 'failed',
				'provider': provider.value,
				'error': 'Provisioning timed out',
				'message': '시간 초과로 인해 키 발급에 실패했습니다.',
			}

		except Exception as e:
			self.context.status = ProvisioningStatus.FAILED
			self.context.error = str(e)

			await self._update_status(ProvisioningStatus.FAILED, None, f'키 발급 실패: {str(e)}')

			return {
				'status': 'failed',
				'provider': provider.value,
				'error': str(e),
				'message': f'API 키 발급에 실패했습니다: {str(e)}',
			}

		finally:
			await self.close()


async def create_provisioning_task(
	provider_name: str, browser_session: Any, llm: Any, websocket_broadcast: Any = None, **kwargs
) -> dict:
	"""
	Convenience function to create and run a provisioning task.

	Args:
		provider_name: Name of the provider (e.g., 'openai', 'anthropic')
		browser_session: BrowserSession instance
		llm: LLM instance
		websocket_broadcast: Function to broadcast updates
		**kwargs: Additional arguments for provisioning

	Returns:
		Provisioning result dictionary
	"""
	# Parse provider name to APIProvider enum
	try:
		provider = APIProvider(provider_name.lower())
	except ValueError:
		return {
			'status': 'failed',
			'error': f'Unknown provider: {provider_name}',
			'supported_providers': [p.value for p in APIProvider if p != APIProvider.UNKNOWN],
		}

	if provider == APIProvider.UNKNOWN:
		return {
			'status': 'failed',
			'error': 'Provider not specified',
			'supported_providers': [p.value for p in APIProvider if p != APIProvider.UNKNOWN],
		}

	provisioner = ApiKeyProvisioner(
		browser_session=browser_session,
		llm=llm,
		websocket_broadcast=websocket_broadcast,
	)

	return await provisioner.provision(
		provider=provider,
		key_name=kwargs.get('key_name', 'NewsInsight-AutoGenerated'),
		auto_save=kwargs.get('auto_save', True),
		timeout_seconds=kwargs.get('timeout_seconds', 300),
	)

```

---

## backend/services/browser-use-api/intent_analyzer.py

```py
"""
Intent Analyzer for Browser-Use API

Provides intent analysis, keyword extraction, and search guarantees
for browser automation tasks.
"""

import asyncio
import logging
import re
from dataclasses import dataclass, field
from typing import Any, Optional
from enum import Enum

logger = logging.getLogger(__name__)


class IntentType(str, Enum):
	"""Types of user intents for browser tasks."""

	SEARCH = 'search'
	NAVIGATE = 'navigate'
	EXTRACT = 'extract'
	INTERACT = 'interact'
	MONITOR = 'monitor'
	API_KEY_PROVISION = 'api_key_provision'  # API 키 자동 발급
	UNKNOWN = 'unknown'


class APIProvider(str, Enum):
	"""Supported API providers for auto-provisioning."""

	OPENAI = 'openai'
	ANTHROPIC = 'anthropic'
	GOOGLE = 'google'
	OPENROUTER = 'openrouter'
	TOGETHER_AI = 'together_ai'
	PERPLEXITY = 'perplexity'
	BRAVE_SEARCH = 'brave_search'
	TAVILY = 'tavily'
	UNKNOWN = 'unknown'


# Provider detection keywords mapping
PROVIDER_KEYWORDS = {
	APIProvider.OPENAI: ['openai', 'gpt', 'chatgpt', 'gpt-4', 'gpt-3', 'dall-e', 'whisper'],
	APIProvider.ANTHROPIC: ['anthropic', 'claude', 'sonnet', 'opus', 'haiku'],
	APIProvider.GOOGLE: ['google', 'gemini', 'palm', 'bard', 'vertex'],
	APIProvider.OPENROUTER: ['openrouter', 'open router'],
	APIProvider.TOGETHER_AI: ['together', 'together ai', 'togetherai'],
	APIProvider.PERPLEXITY: ['perplexity', 'pplx'],
	APIProvider.BRAVE_SEARCH: ['brave', 'brave search'],
	APIProvider.TAVILY: ['tavily'],
}

# Provider URLs for API key pages
PROVIDER_URLS = {
	APIProvider.OPENAI: 'https://platform.openai.com/api-keys',
	APIProvider.ANTHROPIC: 'https://console.anthropic.com/settings/keys',
	APIProvider.GOOGLE: 'https://aistudio.google.com/app/apikey',
	APIProvider.OPENROUTER: 'https://openrouter.ai/keys',
	APIProvider.TOGETHER_AI: 'https://api.together.xyz/settings/api-keys',
	APIProvider.PERPLEXITY: 'https://www.perplexity.ai/settings/api',
	APIProvider.BRAVE_SEARCH: 'https://api.search.brave.com/app/keys',
	APIProvider.TAVILY: 'https://app.tavily.com/home',
}


@dataclass
class FallbackStrategy:
	"""A fallback search strategy."""

	description: str
	keywords: list[str]
	search_engine: str = 'google'
	priority: int = 0


@dataclass
class AnalyzedIntent:
	"""Result of intent analysis."""

	original_task: str
	intent_type: IntentType
	keywords: list[str]
	primary_keyword: str
	fallback_strategies: list[FallbackStrategy] = field(default_factory=list)
	confidence: float = 0.0
	entities: dict[str, list[str]] = field(default_factory=dict)


class IntentAnalyzer:
	"""
	Analyzes browser automation tasks to extract intent, keywords,
	and generate fallback strategies.
	"""

	def __init__(self, llm: Optional[Any] = None):
		self.llm = llm
		self._keyword_patterns = [
			r'"([^"]+)"',  # Quoted strings
			r"'([^']+)'",  # Single-quoted strings
			r'\b(?:search|find|look for|get|fetch)\s+(?:for\s+)?(.+?)(?:\s+on|\s+from|\s+in|$)',
		]

	def _extract_keywords_basic(self, text: str) -> list[str]:
		"""Extract keywords using basic pattern matching."""
		keywords = []

		# Extract quoted strings first
		for pattern in self._keyword_patterns[:2]:
			matches = re.findall(pattern, text, re.IGNORECASE)
			keywords.extend(matches)

		# Extract search terms
		for pattern in self._keyword_patterns[2:]:
			matches = re.findall(pattern, text, re.IGNORECASE)
			keywords.extend(matches)

		# Clean and dedupe
		cleaned = []
		seen = set()
		for kw in keywords:
			kw = kw.strip().lower()
			if kw and kw not in seen and len(kw) > 2:
				seen.add(kw)
				cleaned.append(kw)

		return cleaned

	def _detect_intent_type(self, text: str) -> IntentType:
		"""Detect the type of intent from the task description."""
		text_lower = text.lower()

		# API Key provisioning detection (highest priority)
		api_key_keywords = [
			'api key',
			'api 키',
			'apikey',
			'api-key',
			'키 발급',
			'키발급',
			'key 발급',
			'발급해',
			'발급 해',
			'register api',
			'create api',
			'generate key',
			'get key',
			'등록해',
			'설정해',
			'setup api',
			'configure api',
		]
		if any(w in text_lower for w in api_key_keywords):
			return IntentType.API_KEY_PROVISION

		if any(w in text_lower for w in ['search', 'find', 'look for', 'query']):
			return IntentType.SEARCH
		elif any(w in text_lower for w in ['navigate', 'go to', 'open', 'visit']):
			return IntentType.NAVIGATE
		elif any(w in text_lower for w in ['extract', 'scrape', 'get data', 'collect']):
			return IntentType.EXTRACT
		elif any(w in text_lower for w in ['click', 'fill', 'submit', 'login', 'interact']):
			return IntentType.INTERACT
		elif any(w in text_lower for w in ['monitor', 'watch', 'track', 'wait for']):
			return IntentType.MONITOR

		return IntentType.UNKNOWN

	def detect_api_provider(self, text: str) -> APIProvider:
		"""Detect which API provider the user is referring to."""
		text_lower = text.lower()

		for provider, keywords in PROVIDER_KEYWORDS.items():
			if any(kw in text_lower for kw in keywords):
				return provider

		return APIProvider.UNKNOWN

	def get_provider_url(self, provider: APIProvider) -> str:
		"""Get the API key management URL for a provider."""
		return PROVIDER_URLS.get(provider, '')

	def _generate_fallback_strategies(self, keywords: list[str], intent_type: IntentType) -> list[FallbackStrategy]:
		"""Generate fallback search strategies."""
		strategies = []

		if not keywords:
			return strategies

		primary = keywords[0] if keywords else ''

		# Strategy 1: Direct search
		strategies.append(
			FallbackStrategy(
				description=f"Direct search for '{primary}'",
				keywords=[primary],
				search_engine='google',
				priority=1,
			)
		)

		# Strategy 2: All keywords combined
		if len(keywords) > 1:
			strategies.append(
				FallbackStrategy(
					description=f'Combined search: {" ".join(keywords[:3])}',
					keywords=keywords[:3],
					search_engine='google',
					priority=2,
				)
			)

		# Strategy 3: News-specific search
		if intent_type == IntentType.SEARCH:
			strategies.append(
				FallbackStrategy(
					description=f"News search for '{primary}'",
					keywords=[f'{primary} news', f'{primary} latest'],
					search_engine='google_news',
					priority=3,
				)
			)

		return strategies

	async def analyze(self, task: str, use_llm: bool = True) -> AnalyzedIntent:
		"""
		Analyze a task to extract intent and keywords.

		Args:
		    task: The browser automation task description
		    use_llm: Whether to use LLM for enhanced analysis

		Returns:
		    AnalyzedIntent with extracted information
		"""
		# Basic extraction
		keywords = self._extract_keywords_basic(task)
		intent_type = self._detect_intent_type(task)

		# If we have LLM and use_llm is True, try enhanced analysis
		if use_llm and self.llm is not None:
			try:
				enhanced = await self._analyze_with_llm(task)
				if enhanced:
					# Merge LLM keywords with basic extraction
					all_keywords = list(set(keywords + enhanced.get('keywords', [])))
					keywords = all_keywords[:10]  # Limit to top 10

					if enhanced.get('intent_type'):
						try:
							intent_type = IntentType(enhanced['intent_type'])
						except ValueError:
							pass
			except Exception as e:
				logger.warning(f'LLM analysis failed, using basic extraction: {e}')

		# Determine primary keyword
		primary_keyword = keywords[0] if keywords else task.split()[0] if task.split() else ''

		# Generate fallback strategies
		strategies = self._generate_fallback_strategies(keywords, intent_type)

		return AnalyzedIntent(
			original_task=task,
			intent_type=intent_type,
			keywords=keywords,
			primary_keyword=primary_keyword,
			fallback_strategies=strategies,
			confidence=0.8 if keywords else 0.5,
		)

	async def _analyze_with_llm(self, task: str) -> dict[str, Any]:
		"""Use LLM for enhanced intent analysis."""
		if self.llm is None:
			return {}

		prompt = f"""Analyze this browser automation task and extract:
1. Main keywords (for search)
2. Intent type (search, navigate, extract, interact, monitor)
3. Target entities (websites, products, etc.)

Task: {task}

Respond with JSON:
{{"keywords": ["keyword1", "keyword2"], "intent_type": "search", "entities": {{"websites": [], "products": []}}}}"""

		try:
			from aidove_chat import ChatInvokeCompletion

			# Create a simple message structure
			class SimpleMessage:
				def __init__(self, role: str, content: str):
					self.role = role
					self.content = content

			messages = [SimpleMessage('user', prompt)]
			result = await self.llm.ainvoke(messages)

			if hasattr(result, 'completion') and isinstance(result.completion, str):
				import json

				# Try to extract JSON from response
				text = result.completion
				if '{' in text:
					start = text.find('{')
					end = text.rfind('}') + 1
					if end > start:
						return json.loads(text[start:end])

		except Exception as e:
			logger.debug(f'LLM analysis parsing failed: {e}')

		return {}


class SearchGuarantee:
	"""
	Ensures search tasks always return results by implementing
	fallback strategies and result validation.
	"""

	def __init__(self, intent_analyzer: IntentAnalyzer):
		self.intent_analyzer = intent_analyzer

	def build_enhanced_task(self, analyzed_intent: AnalyzedIntent, original_task: str) -> str:
		"""
		Build an enhanced task with fallback instructions.

		Args:
		    analyzed_intent: The analyzed intent from IntentAnalyzer
		    original_task: The original task description

		Returns:
		    Enhanced task string with fallback strategies
		"""
		if not analyzed_intent.fallback_strategies:
			return original_task

		# Build fallback instructions
		fallback_text = '\n\nFALLBACK STRATEGIES if primary search fails:\n'
		for i, strategy in enumerate(analyzed_intent.fallback_strategies[:3], 1):
			fallback_text += f'{i}. {strategy.description} - Keywords: {", ".join(strategy.keywords)}\n'

		fallback_text += """
If the main search returns no results:
1. Try alternative keywords from the fallback strategies above
2. Try a different search engine (Google, Bing, DuckDuckGo)
3. Look for related content that might contain the information
4. Report what was found even if not exact match"""

		return original_task + fallback_text

	async def validate_results(self, results: list[Any], analyzed_intent: AnalyzedIntent) -> bool:
		"""
		Validate if search results match the intent.

		Args:
		    results: List of search results
		    analyzed_intent: The analyzed intent

		Returns:
		    True if results are valid
		"""
		if not results:
			return False

		# Check if any result contains keywords
		keywords = set(k.lower() for k in analyzed_intent.keywords)
		for result in results:
			result_text = str(result).lower()
			if any(kw in result_text for kw in keywords):
				return True

		return len(results) > 0


class ResultFusion:
	"""
	Combines results from multiple search strategies
	to provide comprehensive search results.
	"""

	def __init__(self):
		self.results: list[dict[str, Any]] = []

	def add_results(self, results: list[Any], strategy: FallbackStrategy, score: float = 1.0):
		"""Add results from a strategy."""
		for result in results:
			self.results.append(
				{
					'result': result,
					'strategy': strategy.description,
					'score': score,
				}
			)

	def get_fused_results(self, top_k: int = 10) -> list[Any]:
		"""Get top-k fused results."""
		# Sort by score
		sorted_results = sorted(self.results, key=lambda x: x['score'], reverse=True)

		# Deduplicate
		seen = set()
		unique_results = []
		for item in sorted_results:
			result_str = str(item['result'])
			if result_str not in seen:
				seen.add(result_str)
				unique_results.append(item['result'])
				if len(unique_results) >= top_k:
					break

		return unique_results

	def clear(self):
		"""Clear all results."""
		self.results = []

```

---

## backend/services/browser-use-api/server.py

```py
"""
Browser-Use API Server with Human-in-the-Loop Support
FastAPI server that exposes browser-use functionality via HTTP API with real-time
human intervention capabilities.
"""

import asyncio
import base64
import os
import time
import uuid
import logging
from contextlib import asynccontextmanager
from dataclasses import dataclass, field
from typing import Optional, Any, Callable
from datetime import datetime
from enum import Enum

import httpx
from collections import defaultdict
from fastapi import FastAPI, HTTPException, BackgroundTasks, WebSocket, WebSocketDisconnect, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

# Browser-use imports
from browser_use import Agent, Controller
from browser_use.browser import BrowserSession, BrowserProfile, ProxySettings

# Local modules for AIDove integration and intent analysis
from aidove_chat import ChatAIDove
from intent_analyzer import IntentAnalyzer, SearchGuarantee, ResultFusion, IntentType, APIProvider, PROVIDER_URLS
from api_key_provisioner import ApiKeyProvisioner, ProvisioningRequest, ProvisioningResponse, create_provisioning_task

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


# ============================================
# Rate Limiting for API Key Provisioning
# ============================================


class RateLimiter:
	"""
	Simple in-memory rate limiter for API key provisioning.

	Uses a sliding window approach to limit requests per IP address.
	"""

	def __init__(self, requests_per_minute: int = 5, window_seconds: int = 60):
		self.requests_per_minute = requests_per_minute
		self.window_seconds = window_seconds
		self._requests: dict[str, list[float]] = defaultdict(list)
		self._lock = asyncio.Lock()

	async def is_allowed(self, client_ip: str) -> tuple[bool, dict]:
		"""
		Check if a request from client_ip is allowed.

		Returns:
			tuple: (is_allowed, rate_limit_info)
		"""
		async with self._lock:
			now = time.time()
			window_start = now - self.window_seconds

			# Clean old requests outside the window
			self._requests[client_ip] = [ts for ts in self._requests[client_ip] if ts > window_start]

			current_count = len(self._requests[client_ip])
			remaining = max(0, self.requests_per_minute - current_count)

			# Calculate retry-after if rate limited
			if current_count >= self.requests_per_minute:
				oldest_request = min(self._requests[client_ip])
				retry_after = int(oldest_request + self.window_seconds - now) + 1
				return False, {
					'limit': self.requests_per_minute,
					'remaining': 0,
					'reset': int(oldest_request + self.window_seconds),
					'retry_after': retry_after,
				}

			# Record this request
			self._requests[client_ip].append(now)

			return True, {
				'limit': self.requests_per_minute,
				'remaining': remaining - 1,  # After this request
				'reset': int(now + self.window_seconds),
			}

	async def get_stats(self) -> dict:
		"""Get current rate limiter statistics."""
		async with self._lock:
			now = time.time()
			window_start = now - self.window_seconds

			# Clean and count
			active_ips = 0
			total_requests = 0

			for ip, timestamps in list(self._requests.items()):
				valid = [ts for ts in timestamps if ts > window_start]
				if valid:
					self._requests[ip] = valid
					active_ips += 1
					total_requests += len(valid)
				else:
					del self._requests[ip]

			return {
				'active_ips': active_ips,
				'total_requests_in_window': total_requests,
				'window_seconds': self.window_seconds,
				'max_requests_per_ip': self.requests_per_minute,
			}


# Rate limiter configuration from environment
API_KEY_PROVISION_RATE_LIMIT = int(os.environ.get('API_KEY_PROVISION_RATE_LIMIT', '5'))
api_key_rate_limiter = RateLimiter(requests_per_minute=API_KEY_PROVISION_RATE_LIMIT)


def get_client_ip(request: Request) -> str:
	"""Extract client IP from request, considering proxy headers."""
	# Check for forwarded headers (behind reverse proxy)
	forwarded = request.headers.get('X-Forwarded-For')
	if forwarded:
		# Take the first IP in the chain (original client)
		return forwarded.split(',')[0].strip()

	real_ip = request.headers.get('X-Real-IP')
	if real_ip:
		return real_ip.strip()

	# Fall back to direct connection IP
	if request.client:
		return request.client.host

	return 'unknown'


# ============================================
# Audit Logging for API Key Provisioning
# ============================================


class ProvisioningAuditLogger:
	"""
	Audit logger for API key provisioning events.

	Logs to:
	1. Local structured log file (JSONL format)
	2. Optionally sends to admin-dashboard audit service via HTTP
	"""

	def __init__(
		self,
		log_file: str = '/var/log/newsinsight/api-key-provisioning.jsonl',
		admin_dashboard_url: Optional[str] = None,
	):
		self.log_file = log_file
		self.admin_dashboard_url = admin_dashboard_url
		self._ensure_log_file()
		self._client: Optional[httpx.AsyncClient] = None

	def _ensure_log_file(self) -> None:
		"""Ensure log directory and file exist."""
		try:
			import os

			log_dir = os.path.dirname(self.log_file)
			if log_dir:
				os.makedirs(log_dir, exist_ok=True)
			if not os.path.exists(self.log_file):
				with open(self.log_file, 'w') as f:
					pass  # Create empty file
		except Exception as e:
			logger.warning(f'Could not create audit log file {self.log_file}: {e}')

	async def log_event(
		self,
		event_type: str,
		provider: str,
		client_ip: str,
		job_id: str,
		success: bool,
		user_agent: Optional[str] = None,
		details: Optional[dict] = None,
		error_message: Optional[str] = None,
	) -> None:
		"""
		Log an API key provisioning event.

		Event types:
		- provision_started: Provisioning job started
		- provision_completed: API key successfully generated
		- provision_failed: Provisioning failed
		- rate_limited: Request was rate limited
		- login_required: Human login intervention requested
		- login_completed: Human completed login
		- twofa_required: 2FA intervention requested
		- twofa_completed: 2FA completed
		"""
		import json

		log_entry = {
			'id': f'audit-{uuid.uuid4().hex[:12]}',
			'timestamp': datetime.utcnow().isoformat() + 'Z',
			'event_type': event_type,
			'resource_type': 'api_key_provisioning',
			'provider': provider,
			'job_id': job_id,
			'client_ip': client_ip,
			'user_agent': user_agent,
			'success': success,
			'error_message': error_message,
			'details': details or {},
		}

		# Log locally
		try:
			with open(self.log_file, 'a') as f:
				f.write(json.dumps(log_entry) + '\n')
		except Exception as e:
			logger.warning(f'Failed to write audit log: {e}')

		# Log to console in structured format
		log_msg = f'AUDIT: {event_type} | provider={provider} | job_id={job_id} | ip={client_ip} | success={success}'
		if error_message:
			log_msg += f' | error={error_message}'

		if success:
			logger.info(log_msg)
		else:
			logger.warning(log_msg)

		# Optionally send to admin dashboard
		if self.admin_dashboard_url:
			try:
				if self._client is None:
					self._client = httpx.AsyncClient(timeout=5.0)

				await self._client.post(
					f'{self.admin_dashboard_url}/api/audit',
					json=log_entry,
				)
			except Exception as e:
				logger.debug(f'Failed to send audit log to admin dashboard: {e}')

	async def close(self) -> None:
		"""Clean up HTTP client."""
		if self._client:
			await self._client.aclose()
			self._client = None


# Audit logger configuration
PROVISIONING_AUDIT_LOG_FILE = os.environ.get('PROVISIONING_AUDIT_LOG_FILE', '/var/log/newsinsight/api-key-provisioning.jsonl')
ADMIN_DASHBOARD_URL = os.environ.get('ADMIN_DASHBOARD_URL')  # Optional

provisioning_audit_logger = ProvisioningAuditLogger(
	log_file=PROVISIONING_AUDIT_LOG_FILE,
	admin_dashboard_url=ADMIN_DASHBOARD_URL,
)


@dataclass
class ProxyInfo:
	id: str
	address: str
	protocol: str = 'http'
	username: Optional[str] = None
	password: Optional[str] = None
	country: Optional[str] = None
	health_status: Optional[str] = None


class ProxyRotationClient:
	def __init__(
		self,
		base_url: str,
		timeout: float = 5.0,
		enabled: bool = True,
	) -> None:
		self._base_url = base_url.rstrip('/')
		self._timeout = timeout
		self._enabled = enabled
		self._client: Optional[httpx.AsyncClient] = None
		self._lock = asyncio.Lock()

	async def _get_client(self) -> httpx.AsyncClient:
		async with self._lock:
			if self._client is None:
				self._client = httpx.AsyncClient(timeout=self._timeout)
			return self._client

	async def close(self) -> None:
		async with self._lock:
			if self._client is not None:
				await self._client.aclose()
				self._client = None

	async def health_check(self) -> bool:
		if not self._enabled:
			return False
		client = await self._get_client()
		try:
			resp = await client.get(f'{self._base_url}/health')
			return resp.status_code == 200
		except Exception:
			return False

	async def get_next_proxy(self) -> Optional[ProxyInfo]:
		if not self._enabled:
			return None
		client = await self._get_client()
		try:
			resp = await client.get(f'{self._base_url}/proxy/next')
			if resp.status_code != 200:
				return None

			data: Any = resp.json()
			if not isinstance(data, dict):
				return None

			proxy_id = data.get('proxyId') or data.get('proxy_id') or data.get('id')
			address = data.get('address')
			if not proxy_id or not address:
				return None

			return ProxyInfo(
				id=str(proxy_id),
				address=str(address),
				protocol=str(data.get('protocol') or 'http'),
				username=data.get('username'),
				password=data.get('password'),
				country=data.get('country'),
				health_status=data.get('healthStatus') or data.get('health_status'),
			)
		except Exception:
			return None

	async def record_success(self, proxy_id: str, latency_ms: int = 0) -> bool:
		return await self._record(proxy_id=proxy_id, success=True, latency_ms=latency_ms)

	async def record_failure(self, proxy_id: str, reason: str = '') -> bool:
		return await self._record(proxy_id=proxy_id, success=False, reason=reason)

	async def record_captcha(self, proxy_id: str, captcha_type: str = '') -> bool:
		if not self._enabled:
			return False
		client = await self._get_client()
		payload = {
			'proxyId': proxy_id,
			'type': captcha_type,
		}
		try:
			resp = await client.post(f'{self._base_url}/proxy/captcha', json=payload)
			return resp.status_code == 200
		except Exception:
			return False

	async def _record(
		self,
		proxy_id: str,
		success: bool,
		latency_ms: int = 0,
		reason: str = '',
	) -> bool:
		if not self._enabled:
			return False
		client = await self._get_client()
		payload = {
			'proxyId': proxy_id,
			'success': bool(success),
			'latencyMs': int(latency_ms),
			'reason': reason,
		}
		try:
			resp = await client.post(f'{self._base_url}/proxy/record', json=payload)
			return resp.status_code == 200
		except Exception:
			return False


def _env_bool(raw: str) -> bool:
	return raw.strip().lower() in {'1', 'true', 'yes', 'y', 'on'}


PROXY_ROTATION_ENABLED = _env_bool(os.environ.get('USE_PROXY_ROTATION', 'false'))
PROXY_ROTATION_URL = os.environ.get('PROXY_ROTATION_URL', 'http://ip-rotation:8050').rstrip('/')
PROXY_ROTATION_TIMEOUT_SECONDS = float(os.environ.get('PROXY_ROTATION_TIMEOUT_SECONDS', '5.0'))

proxy_rotation_client: Optional[ProxyRotationClient] = None


# ============================================
# Enums and Constants
# ============================================


class JobStatus(str, Enum):
	PENDING = 'pending'
	RUNNING = 'running'
	WAITING_HUMAN = 'waiting_human'  # Waiting for human intervention
	COMPLETED = 'completed'
	FAILED = 'failed'
	CANCELLED = 'cancelled'


class InterventionType(str, Enum):
	CAPTCHA = 'captcha'
	LOGIN = 'login'
	NAVIGATION = 'navigation'
	EXTRACTION = 'extraction'
	CONFIRMATION = 'confirmation'
	CUSTOM = 'custom'


# ============================================
# Request/Response Models
# ============================================


class BrowseRequest(BaseModel):
	"""Request to perform a browser automation task."""

	task: str = Field(..., description='The task for the AI agent to perform')
	url: Optional[str] = Field(None, description='Optional starting URL')
	session_id: Optional[str] = Field(None, description='Session ID for context continuity')
	max_steps: int = Field(25, description='Maximum number of steps', ge=1, le=100)
	timeout_seconds: int = Field(300, description='Timeout in seconds', ge=30, le=600)
	headless: bool = Field(False, description='Run browser in headless mode (False for human intervention)')
	enable_human_intervention: bool = Field(True, description='Allow human intervention when needed')
	auto_request_intervention: bool = Field(True, description='Automatically request intervention on issues')
	use_proxy_rotation: bool = Field(PROXY_ROTATION_ENABLED, description='Use proxy rotation via ip-rotation service')


class BrowseResponse(BaseModel):
	"""Response from a browser automation task."""

	job_id: str
	status: str
	message: str
	result: Optional[str] = None
	steps_taken: int = 0
	urls_visited: list[str] = []
	screenshots: list[str] = []
	error: Optional[str] = None
	started_at: Optional[str] = None
	completed_at: Optional[str] = None
	intervention_requested: bool = False
	intervention_type: Optional[str] = None


class InterventionRequest(BaseModel):
	"""Human intervention request details."""

	job_id: str
	intervention_type: InterventionType
	reason: str
	screenshot: Optional[str] = None  # Base64 encoded
	current_url: Optional[str] = None
	suggested_actions: list[str] = []
	timeout_seconds: int = Field(300, description='Timeout for human response')


class HumanAction(BaseModel):
	"""Human's response to an intervention request."""

	action_type: str = Field(..., description='Type of action: click, type, navigate, scroll, custom, skip, abort')
	selector: Optional[str] = Field(None, description='CSS selector for element interaction')
	value: Optional[str] = Field(None, description='Value for input/navigation')
	x: Optional[int] = Field(None, description='X coordinate for click')
	y: Optional[int] = Field(None, description='Y coordinate for click')
	custom_script: Optional[str] = Field(None, description='Custom JavaScript to execute')
	message: Optional[str] = Field(None, description='Message/feedback for the AI')


class JobStatusResponse(BaseModel):
	"""Status of a running job."""

	job_id: str
	status: str
	progress: float = 0.0
	current_step: int = 0
	max_steps: int = 25
	result: Optional[str] = None
	error: Optional[str] = None
	urls_visited: list[str] = []
	started_at: Optional[str] = None
	completed_at: Optional[str] = None
	# Human intervention fields
	intervention_requested: bool = False
	intervention_type: Optional[str] = None
	intervention_reason: Optional[str] = None
	intervention_screenshot: Optional[str] = None
	current_url: Optional[str] = None


class HealthResponse(BaseModel):
	"""Health check response."""

	status: str
	version: str
	uptime_seconds: float
	active_jobs: int
	waiting_intervention: int


# ============================================
# Job Management
# ============================================


@dataclass
class InterventionState:
	"""State for human intervention."""

	requested: bool = False
	type: Optional[InterventionType] = None
	reason: Optional[str] = None
	screenshot: Optional[str] = None
	current_url: Optional[str] = None
	suggested_actions: list[str] = field(default_factory=list)
	response: Optional[HumanAction] = None
	response_event: asyncio.Event = field(default_factory=asyncio.Event)
	timeout_seconds: int = 300


@dataclass
class Job:
	"""Represents a browser automation job."""

	id: str
	task: str
	url: Optional[str]
	session_id: Optional[str]
	max_steps: int
	timeout_seconds: int
	headless: bool
	enable_human_intervention: bool
	auto_request_intervention: bool
	use_proxy_rotation: bool
	proxy_id: Optional[str] = None
	proxy_address: Optional[str] = None
	status: JobStatus = JobStatus.PENDING
	progress: float = 0.0
	current_step: int = 0
	result: Optional[str] = None
	error: Optional[str] = None
	urls_visited: list = None
	screenshots: list = None
	started_at: Optional[datetime] = None
	completed_at: Optional[datetime] = None
	# Human intervention state
	intervention: InterventionState = None
	# Browser session reference
	browser_session: Optional[BrowserSession] = None
	# WebSocket connections for this job
	websocket_clients: list = None

	def __post_init__(self):
		if self.urls_visited is None:
			self.urls_visited = []
		if self.screenshots is None:
			self.screenshots = []
		if self.intervention is None:
			self.intervention = InterventionState()
		if self.websocket_clients is None:
			self.websocket_clients = []


# In-memory job storage (use Redis in production)
jobs: dict[str, Job] = {}
start_time = datetime.now()


# WebSocket connection manager
class ConnectionManager:
	def __init__(self):
		self.active_connections: dict[str, list[WebSocket]] = {}

	async def connect(self, websocket: WebSocket, job_id: str):
		await websocket.accept()
		if job_id not in self.active_connections:
			self.active_connections[job_id] = []
		self.active_connections[job_id].append(websocket)
		logger.info(f'WebSocket connected for job {job_id}')

	def disconnect(self, websocket: WebSocket, job_id: str):
		if job_id in self.active_connections:
			if websocket in self.active_connections[job_id]:
				self.active_connections[job_id].remove(websocket)
			if not self.active_connections[job_id]:
				del self.active_connections[job_id]
		logger.info(f'WebSocket disconnected for job {job_id}')

	async def broadcast(self, job_id: str, message: dict):
		if job_id in self.active_connections:
			disconnected = []
			for connection in self.active_connections[job_id]:
				try:
					await connection.send_json(message)
				except Exception:
					disconnected.append(connection)
			for conn in disconnected:
				self.disconnect(conn, job_id)


manager = ConnectionManager()


# ============================================
# Browser Agent Runner with Human Intervention
# ============================================


async def capture_screenshot(browser_session: Optional[BrowserSession]) -> Optional[str]:
	"""Capture a screenshot from the browser and return as base64."""
	try:
		if browser_session:
			# Use BrowserSession's take_screenshot method which returns bytes
			screenshot_bytes = await browser_session.take_screenshot()
			return base64.b64encode(screenshot_bytes).decode('utf-8')
	except Exception as e:
		logger.warning(f'Failed to capture screenshot: {e}')
	return None


async def get_current_url(browser_session: Optional[BrowserSession]) -> Optional[str]:
	"""Get the current page URL."""
	try:
		if browser_session:
			url = await browser_session.get_current_page_url()
			return url if url else None
	except Exception as e:
		logger.warning(f'Failed to get current URL: {e}')
	return None


async def _detect_captcha_on_page(browser_session: Optional[BrowserSession]) -> Optional[str]:
	"""
	Detect CAPTCHA on the current page by checking for common CAPTCHA indicators.

	Returns the type of CAPTCHA detected, or None if no CAPTCHA found.
	"""
	if not browser_session:
		return None

	try:
		context = getattr(browser_session, '_context', None)
		if not context:
			return None

		pages = context.pages
		if not pages:
			return None

		page = pages[0]

		# JavaScript to detect various CAPTCHA types
		detection_script = """
        () => {
            const indicators = [];
            
            // Check for reCAPTCHA
            if (document.querySelector('.g-recaptcha') || 
                document.querySelector('[data-sitekey]') ||
                document.querySelector('iframe[src*="recaptcha"]') ||
                document.querySelector('#recaptcha') ||
                document.querySelector('.recaptcha-checkbox')) {
                indicators.push('reCAPTCHA');
            }
            
            // Check for hCaptcha
            if (document.querySelector('.h-captcha') ||
                document.querySelector('[data-hcaptcha-widget-id]') ||
                document.querySelector('iframe[src*="hcaptcha"]')) {
                indicators.push('hCaptcha');
            }
            
            // Check for Cloudflare Turnstile
            if (document.querySelector('.cf-turnstile') ||
                document.querySelector('[data-turnstile-widget-id]') ||
                document.querySelector('iframe[src*="turnstile"]') ||
                document.querySelector('iframe[src*="challenges.cloudflare"]')) {
                indicators.push('Cloudflare Turnstile');
            }
            
            // Check for Cloudflare challenge page
            if (document.querySelector('#challenge-running') ||
                document.querySelector('#challenge-form') ||
                document.querySelector('.cf-browser-verification') ||
                document.title.includes('Just a moment') ||
                document.body.textContent.includes('Checking your browser') ||
                document.body.textContent.includes('Please wait while we verify')) {
                indicators.push('Cloudflare Challenge');
            }
            
            // Check for generic CAPTCHA indicators
            const bodyText = document.body.textContent.toLowerCase();
            if (bodyText.includes('verify you are human') ||
                bodyText.includes('prove you are human') ||
                bodyText.includes('robot verification') ||
                bodyText.includes('are you a robot') ||
                bodyText.includes('security check')) {
                indicators.push('Generic CAPTCHA');
            }
            
            // Check for FunCaptcha / Arkose Labs
            if (document.querySelector('#fc-iframe-wrap') ||
                document.querySelector('[data-fc-payload]') ||
                document.querySelector('iframe[src*="funcaptcha"]') ||
                document.querySelector('iframe[src*="arkoselabs"]')) {
                indicators.push('FunCaptcha');
            }
            
            // Check for access denied / blocked pages
            if (document.body.textContent.includes('Access Denied') ||
                document.body.textContent.includes('403 Forbidden') ||
                document.body.textContent.includes('Your access to this site has been limited')) {
                indicators.push('Access Blocked');
            }
            
            return indicators.length > 0 ? indicators.join(', ') : null;
        }
        """

		result = await page.evaluate(detection_script)
		return result

	except Exception as e:
		logger.warning(f'CAPTCHA detection failed: {e}')
		return None


async def request_human_intervention(
	job: Job, intervention_type: InterventionType, reason: str, suggested_actions: Optional[list[str]] = None
) -> Optional[HumanAction]:
	"""
	Request human intervention and wait for response.

	Returns the human's action or None if timeout/skip.
	"""
	if not job.enable_human_intervention:
		return None

	job.status = JobStatus.WAITING_HUMAN
	job.intervention.requested = True
	job.intervention.type = intervention_type
	job.intervention.reason = reason
	job.intervention.suggested_actions = suggested_actions or []
	job.intervention.response_event.clear()

	# Capture screenshot
	if job.browser_session:
		job.intervention.screenshot = await capture_screenshot(job.browser_session)
		job.intervention.current_url = await get_current_url(job.browser_session)

	# Notify via WebSocket
	await manager.broadcast(
		job.id,
		{
			'type': 'intervention_requested',
			'job_id': job.id,
			'intervention_type': intervention_type.value,
			'reason': reason,
			'screenshot': job.intervention.screenshot,
			'current_url': job.intervention.current_url,
			'suggested_actions': job.intervention.suggested_actions,
		},
	)

	logger.info(f'Job {job.id}: Requesting human intervention - {intervention_type.value}: {reason}')

	# Wait for human response with timeout
	try:
		await asyncio.wait_for(job.intervention.response_event.wait(), timeout=job.intervention.timeout_seconds)
		return job.intervention.response
	except asyncio.TimeoutError:
		logger.warning(f'Job {job.id}: Human intervention timed out')
		job.intervention.requested = False
		job.status = JobStatus.RUNNING
		return None


async def execute_human_action(job: Job, action: HumanAction) -> bool:
	"""Execute a human-specified action on the browser."""
	try:
		if not job.browser_session:
			return False

		context = getattr(job.browser_session, '_context', None)
		if not context:
			return False

		pages = context.pages
		if not pages:
			return False

		page = pages[0]

		if action.action_type == 'click':
			if action.selector:
				await page.click(action.selector)
			elif action.x is not None and action.y is not None:
				await page.mouse.click(action.x, action.y)
			else:
				return False

		elif action.action_type == 'type':
			if action.selector and action.value:
				await page.fill(action.selector, action.value)
			else:
				return False

		elif action.action_type == 'navigate':
			if action.value:
				await page.goto(action.value)
			else:
				return False

		elif action.action_type == 'scroll':
			if action.y:
				await page.evaluate(f'window.scrollBy(0, {action.y})')
			else:
				await page.evaluate('window.scrollBy(0, 500)')

		elif action.action_type == 'custom':
			if action.custom_script:
				await page.evaluate(action.custom_script)
			else:
				return False

		elif action.action_type == 'skip':
			# Just continue without action
			pass

		elif action.action_type == 'abort':
			return False  # Signal to abort

		else:
			logger.warning(f'Unknown action type: {action.action_type}')
			return False

		logger.info(f'Job {job.id}: Executed human action - {action.action_type}')
		return True

	except Exception as e:
		logger.error(f'Job {job.id}: Failed to execute human action: {e}')
		return False


def _extract_agent_result_text(result: Any) -> str:
	if result is None:
		return ''
	if isinstance(result, str):
		return result

	if hasattr(result, 'final_result'):
		final_result = getattr(result, 'final_result')
		try:
			text = final_result() if callable(final_result) else final_result
		except Exception:
			text = None
		if isinstance(text, str) and text.strip():
			return text

	if hasattr(result, 'extracted_content'):
		extracted_content = getattr(result, 'extracted_content')
		try:
			content = extracted_content() if callable(extracted_content) else extracted_content
		except Exception:
			content = None
		if isinstance(content, str) and content.strip():
			return content
		if isinstance(content, list):
			parts = [p.strip() for p in content if isinstance(p, str) and p.strip()]
			if parts:
				seen: set[str] = set()
				unique_parts: list[str] = []
				for p in parts:
					if p in seen:
						continue
					seen.add(p)
					unique_parts.append(p)
				return '\n\n'.join(unique_parts)

	history = getattr(result, 'history', None)
	if isinstance(history, list):
		for item in reversed(history):
			item_results = getattr(item, 'result', None)
			if isinstance(item_results, list):
				for r in reversed(item_results):
					extracted_content = getattr(r, 'extracted_content', None)
					if isinstance(extracted_content, str) and extracted_content.strip():
						return extracted_content

	return ''


async def run_browser_task(job: Job):
	"""Execute a browser automation task using browser-use with AI Dove and human intervention support."""
	job.status = JobStatus.RUNNING
	job.started_at = datetime.now()
	job_start_perf = time.perf_counter()
	proxy_info: Optional[ProxyInfo] = None
	screenshot_task_running = False
	broadcaster_task: Optional[asyncio.Task] = None

	try:
		# Initialize AI Dove LLM
		llm = ChatAIDove(session_id=job.session_id or f'browser-{job.id}', timeout=120.0, max_retries=3)

		# Configure browser profile - force headless in Docker environment
		# For human intervention with visible browser, need to run locally or use VNC
		import os

		is_docker = os.path.exists('/.dockerenv') or os.environ.get('DOCKER_CONTAINER', False)

		proxy_settings: Optional[ProxySettings] = None
		if job.use_proxy_rotation and proxy_rotation_client is not None:
			for attempt in range(3):
				proxy_info = await proxy_rotation_client.get_next_proxy()
				if proxy_info is not None:
					break
				await asyncio.sleep(0.2 * (attempt + 1))

			if proxy_info is not None:
				job.proxy_id = proxy_info.id
				job.proxy_address = proxy_info.address
				proxy_settings = ProxySettings(
					server=proxy_info.address,
					username=proxy_info.username,
					password=proxy_info.password,
				)
				logger.info(f'Job {job.id}: Using rotating proxy id={proxy_info.id} addr={proxy_info.address}')
			else:
				logger.warning(f'Job {job.id}: Proxy rotation enabled but no proxy available')

		profile_kwargs: dict[str, Any] = {
			'headless': True if is_docker else job.headless,
			'disable_security': True,
		}
		if proxy_settings is not None:
			profile_kwargs['proxy'] = proxy_settings

		profile = BrowserProfile(**profile_kwargs)

		# Create browser session
		job.browser_session = BrowserSession(browser_profile=profile)

		# Create controller
		controller = Controller()

		# ============================================
		# Intent Analysis for Guaranteed Search Results
		# ============================================
		# Analyze the task to extract keywords, generate fallback strategies,
		# and ensure search results are always returned
		intent_analyzer = IntentAnalyzer(llm=llm)
		search_guarantee = SearchGuarantee(intent_analyzer)

		# Analyze the task
		analyzed_intent = None
		try:
			analyzed_intent = await intent_analyzer.analyze(job.task, use_llm=True)
			logger.info(
				f'Job {job.id}: Intent analysis complete - '
				f'keywords={analyzed_intent.keywords}, '
				f"primary='{analyzed_intent.primary_keyword}', "
				f"intent_type='{analyzed_intent.intent_type}', "
				f'strategies={len(analyzed_intent.fallback_strategies)}'
			)

			# Build enhanced task with fallback strategies
			full_task = search_guarantee.build_enhanced_task(analyzed_intent, job.task)
		except Exception as e:
			logger.warning(f'Job {job.id}: Intent analysis failed, using original task: {e}')
			full_task = job.task

		# Add URL prefix if provided
		if job.url:
			full_task = f'First navigate to {job.url}, then: {full_task}'

		# Enhanced task prompt for human intervention awareness
		if job.enable_human_intervention:
			full_task += """

IMPORTANT: If you encounter any of these situations, clearly state the issue:
- CAPTCHA or verification challenges (reCAPTCHA, hCaptcha, Cloudflare Turnstile)
- Login required
- Page not loading correctly  
- Cannot find expected content
- Need user confirmation before proceeding
- Any blocking issue

State the problem clearly so a human operator can assist."""

		# CAPTCHA detection patterns for automatic intervention
		captcha_patterns = [
			'recaptcha',
			'hcaptcha',
			'captcha',
			'turnstile',
			'challenge',
			'verification',
			'verify you are human',
			'cloudflare',
			'checking your browser',
			'access denied',
		]

		# Create agent
		agent = Agent(
			task=full_task,
			llm=llm,
			controller=controller,
			browser_session=job.browser_session,
			max_actions_per_step=10,
		)

		# Background task to periodically capture and broadcast screenshots
		screenshot_task_running = True

		async def screenshot_broadcaster():
			"""Periodically capture and broadcast screenshots while job is running."""
			step_count = 0
			captcha_check_count = 0
			while screenshot_task_running and job.status == JobStatus.RUNNING:
				try:
					# Capture current state
					current_url = await get_current_url(job.browser_session)
					if current_url and current_url not in job.urls_visited:
						job.urls_visited.append(current_url)

					screenshot = await capture_screenshot(job.browser_session)
					if screenshot:
						step_count += 1
						job.current_step = step_count
						job.progress = min(step_count / job.max_steps, 0.95)  # Cap at 95% until done

						await manager.broadcast(
							job.id,
							{
								'type': 'step_update',
								'job_id': job.id,
								'step': step_count,
								'progress': job.progress,
								'current_url': current_url,
								'screenshot': screenshot,
							},
						)
						logger.info(f'Job {job.id}: Broadcast screenshot (step {step_count})')

					# Auto-detect CAPTCHA and request human intervention
					if job.enable_human_intervention and job.auto_request_intervention:
						captcha_detected = await _detect_captcha_on_page(job.browser_session)
						if captcha_detected:
							captcha_check_count += 1
							# Only request intervention if CAPTCHA persists (avoid false positives)
							if captcha_check_count >= 2:
								if proxy_info is not None and proxy_rotation_client is not None:
									try:
										await proxy_rotation_client.record_captcha(proxy_info.id, captcha_detected)
									except Exception:
										pass
								logger.info(f'Job {job.id}: CAPTCHA detected, requesting human intervention')
								await request_human_intervention(
									job,
									InterventionType.CAPTCHA,
									f'CAPTCHA detected: {captcha_detected}. Please solve the verification challenge.',
									['Click on CAPTCHA', 'Solve verification', 'Skip if possible'],
								)
								captcha_check_count = 0  # Reset after requesting
						else:
							captcha_check_count = 0  # Reset if no CAPTCHA

				except Exception as e:
					logger.warning(f'Screenshot broadcast error: {e}')

				await asyncio.sleep(2)  # Capture every 2 seconds

		# Start screenshot broadcaster as background task
		broadcaster_task = asyncio.create_task(screenshot_broadcaster())

		# Run with timeout
		try:
			result = await asyncio.wait_for(agent.run(max_steps=job.max_steps), timeout=job.timeout_seconds)

			# Stop screenshot broadcaster
			screenshot_task_running = False
			broadcaster_task.cancel()
			try:
				await broadcaster_task
			except asyncio.CancelledError:
				pass

			# ============================================
			# Result Guarantee Logic - Never return "not found"
			# ============================================
			result_str = _extract_agent_result_text(result)

			# Check for "not found" patterns in multiple languages
			not_found_patterns = [
				'not found',
				'no results',
				'cannot find',
				'could not find',
				'nothing found',
				'no matches',
				'no information',
				'찾을 수 없',
				'결과가 없',
				'검색 결과 없',
				'찾지 못',
				'정보를 찾을 수 없',
				'결과를 찾을 수 없',
			]

			result_lower = result_str.lower()
			is_not_found = (not result_str.strip()) or any(pattern in result_lower for pattern in not_found_patterns)

			if is_not_found and analyzed_intent:
				logger.info(f"Job {job.id}: Initial result indicates 'not found', attempting fallback strategies...")

				# Try fallback strategies
				fallback_attempted = False
				for strategy_info in analyzed_intent.fallback_strategies[1:4]:  # Skip first (already tried), try next 3
					fallback_query = strategy_info['query']
					strategy_name = strategy_info['strategy'].value

					logger.info(f"Job {job.id}: Trying fallback strategy '{strategy_name}' with query: '{fallback_query}'")

					# Build a new task with the fallback query
					fallback_task = f"""
이전 검색에서 결과를 찾지 못했습니다. 다른 방법으로 검색해주세요.

새로운 검색어: "{fallback_query}"

중요 지침:
1. 위의 새로운 검색어로 검색하세요
2. 완전히 일치하지 않더라도 관련된 정보를 찾아서 제공하세요
3. 절대로 "찾을 수 없다" 또는 "결과가 없다"고 말하지 마세요
4. 대신 찾은 가장 관련성 있는 정보를 제공하세요

Previous search found no results. Please try a different approach.

New search query: "{fallback_query}"

Important instructions:
1. Search with the new query above
2. Even if not an exact match, find and provide related information
3. NEVER say "not found" or "no results"
4. Instead, provide the most relevant information you can find
"""

					try:
						# Create a new agent for fallback
						fallback_agent = Agent(
							task=fallback_task,
							llm=llm,
							controller=controller,
							browser_session=job.browser_session,
							max_actions_per_step=10,
						)

						# Run with reduced steps for fallback
						fallback_result = await asyncio.wait_for(
							fallback_agent.run(max_steps=min(10, job.max_steps // 2)), timeout=job.timeout_seconds // 3
						)

						fallback_str = _extract_agent_result_text(fallback_result)

						# Check if fallback found something
						fallback_is_not_found = (not fallback_str.strip()) or any(
							pattern in fallback_str.lower() for pattern in not_found_patterns
						)

						if not fallback_is_not_found and fallback_str:
							logger.info(f"Job {job.id}: Fallback strategy '{strategy_name}' found results!")
							result_str = f"""검색 결과 (대체 검색어: "{fallback_query}" 사용):
Search Results (using alternative query: "{fallback_query}"):

{fallback_str}

참고: 원래 검색어로는 결과를 찾지 못해 관련 검색어로 검색한 결과입니다.
Note: Original query returned no results, so alternative search was performed."""
							fallback_attempted = True
							break

					except asyncio.TimeoutError:
						logger.warning(f"Job {job.id}: Fallback strategy '{strategy_name}' timed out")
					except Exception as e:
						logger.warning(f"Job {job.id}: Fallback strategy '{strategy_name}' failed: {e}")

				# If all fallbacks failed, provide a helpful message instead of "not found"
				if not fallback_attempted:
					logger.info(f'Job {job.id}: All fallback strategies exhausted, providing helpful response')
					result_str = f"""검색 결과를 찾기 어려웠습니다. 다음을 시도해 보세요:
Search was challenging. Here are some suggestions:

시도한 검색어 / Queries attempted:
- {analyzed_intent.original_query}
- {analyzed_intent.primary_keyword}
{chr(10).join('- ' + s['query'] for s in analyzed_intent.fallback_strategies[1:4])}

추천 검색 방법 / Recommended approaches:
1. 검색어를 더 구체적으로 변경해 보세요 / Try more specific keywords
2. 다른 검색 엔진이나 사이트를 이용해 보세요 / Try different search engines or sites
3. 관련 키워드: {', '.join(analyzed_intent.keywords[:5])} / Related keywords

분석된 의도 / Analyzed intent:
- 주요 키워드 / Primary keyword: {analyzed_intent.primary_keyword}
- 검색 유형 / Search type: {analyzed_intent.intent_type}
- 언어 / Language: {analyzed_intent.language}"""

			job.result = result_str if result_str else 'Task completed successfully'
			job.status = JobStatus.COMPLETED
			job.progress = 1.0

			# Capture final screenshot
			final_screenshot = await capture_screenshot(job.browser_session)

			# Broadcast completion
			await manager.broadcast(
				job.id,
				{
					'type': 'completed',
					'job_id': job.id,
					'result': job.result,
					'urls_visited': job.urls_visited,
					'screenshot': final_screenshot,
				},
			)

		except asyncio.TimeoutError:
			if job.status != JobStatus.CANCELLED:
				screenshot_task_running = False
				if broadcaster_task is not None and not broadcaster_task.done():
					broadcaster_task.cancel()
					try:
						await broadcaster_task
					except asyncio.CancelledError:
						pass

				job.error = f'Task timed out after {job.timeout_seconds} seconds'
				job.status = JobStatus.FAILED

				await manager.broadcast(
					job.id,
					{
						'type': 'failed',
						'job_id': job.id,
						'error': job.error,
					},
				)

	except Exception as e:
		if job.status != JobStatus.CANCELLED:
			logger.exception(f'Job {job.id} failed: {e}')
			job.error = str(e)
			job.status = JobStatus.FAILED

			await manager.broadcast(
				job.id,
				{
					'type': 'failed',
					'job_id': job.id,
					'error': job.error,
				},
			)
		else:
			logger.info(f'Job {job.id} cancelled')

	finally:
		screenshot_task_running = False
		if broadcaster_task is not None and not broadcaster_task.done():
			broadcaster_task.cancel()
			try:
				await broadcaster_task
			except asyncio.CancelledError:
				pass

		if proxy_info is not None and proxy_rotation_client is not None and job.status in [JobStatus.COMPLETED, JobStatus.FAILED]:
			elapsed_ms = int((time.perf_counter() - job_start_perf) * 1000)
			if job.status == JobStatus.COMPLETED:
				await proxy_rotation_client.record_success(proxy_info.id, latency_ms=elapsed_ms)
			else:
				await proxy_rotation_client.record_failure(proxy_info.id, reason=(job.error or '')[:500])

		job.completed_at = datetime.now()

		# Clean up browser session
		if job.browser_session:
			try:
				await job.browser_session.stop()
			except Exception as e:
				logger.warning(f'Error closing browser session: {e}')
			job.browser_session = None


# ============================================
# FastAPI Application
# ============================================


@asynccontextmanager
async def lifespan(app: FastAPI):
	"""Application lifespan handler."""
	logger.info('Browser-Use API Server with Human-in-the-Loop starting up...')
	global proxy_rotation_client
	if PROXY_ROTATION_ENABLED:
		proxy_rotation_client = ProxyRotationClient(
			base_url=PROXY_ROTATION_URL,
			timeout=PROXY_ROTATION_TIMEOUT_SECONDS,
			enabled=True,
		)
		healthy = await proxy_rotation_client.health_check()
		logger.info(f'Proxy rotation enabled: url={PROXY_ROTATION_URL} healthy={healthy}')

	yield
	logger.info('Browser-Use API Server shutting down...')

	if proxy_rotation_client is not None:
		await proxy_rotation_client.close()
		proxy_rotation_client = None

	# Cancel all running jobs
	for job in jobs.values():
		if job.status in [JobStatus.RUNNING, JobStatus.WAITING_HUMAN]:
			job.status = JobStatus.CANCELLED
			job.error = 'Server shutdown'

			if job.browser_session:
				try:
					await job.browser_session.stop()
				except:
					pass


app = FastAPI(
	title='Browser-Use API with Human-in-the-Loop',
	description='AI-powered browser automation API with real-time human intervention support',
	version='2.0.0',
	lifespan=lifespan,
)

# CORS middleware
app.add_middleware(
	CORSMiddleware,
	allow_origins=['*'],
	allow_credentials=True,
	allow_methods=['*'],
	allow_headers=['*'],
)


# ============================================
# WebSocket Endpoints
# ============================================


@app.websocket('/ws/{job_id}')
async def websocket_endpoint(websocket: WebSocket, job_id: str):
	"""
	WebSocket endpoint for real-time job updates and human intervention.

	Message types received from client:
	- {"type": "intervention_response", "action": HumanAction}
	- {"type": "request_screenshot"}
	- {"type": "manual_intervention", "intervention_type": str, "reason": str}

	Message types sent to client:
	- {"type": "step_update", ...}
	- {"type": "intervention_requested", ...}
	- {"type": "completed", ...}
	- {"type": "failed", ...}
	- {"type": "screenshot", "data": base64_string}
	"""
	await manager.connect(websocket, job_id)

	try:
		while True:
			data = await websocket.receive_json()

			if job_id not in jobs:
				await websocket.send_json({'type': 'error', 'message': 'Job not found'})
				continue

			job = jobs[job_id]

			if data.get('type') == 'intervention_response':
				# Human provided a response to intervention request
				action_data = data.get('action', {})
				job.intervention.response = HumanAction(**action_data)
				job.intervention.response_event.set()
				job.intervention.requested = False
				job.status = JobStatus.RUNNING

				await websocket.send_json({'type': 'intervention_accepted', 'message': 'Action received and will be executed'})

			elif data.get('type') == 'request_screenshot':
				# Client requesting current screenshot
				if job.browser_session:
					screenshot = await capture_screenshot(job.browser_session)
					current_url = await get_current_url(job.browser_session)
					await websocket.send_json({'type': 'screenshot', 'data': screenshot, 'current_url': current_url})

			elif data.get('type') == 'manual_intervention':
				# Client manually requesting intervention mode
				intervention_type = InterventionType(data.get('intervention_type', 'custom'))
				reason = data.get('reason', 'Manual intervention requested')

				# Pause the agent and wait for human action
				action = await request_human_intervention(job, intervention_type, reason, data.get('suggested_actions', []))

				if action:
					success = await execute_human_action(job, action)
					await websocket.send_json({'type': 'intervention_result', 'success': success})

	except WebSocketDisconnect:
		manager.disconnect(websocket, job_id)
	except Exception as e:
		logger.error(f'WebSocket error for job {job_id}: {e}')
		manager.disconnect(websocket, job_id)


# ============================================
# REST API Endpoints
# ============================================


@app.get('/health', response_model=HealthResponse)
async def health_check():
	"""Health check endpoint."""
	uptime = (datetime.now() - start_time).total_seconds()
	active = sum(1 for j in jobs.values() if j.status in [JobStatus.RUNNING, JobStatus.PENDING])
	waiting = sum(1 for j in jobs.values() if j.status == JobStatus.WAITING_HUMAN)

	return HealthResponse(
		status='healthy', version='2.0.0', uptime_seconds=uptime, active_jobs=active, waiting_intervention=waiting
	)


@app.post('/browse', response_model=BrowseResponse)
async def browse(request: BrowseRequest, background_tasks: BackgroundTasks):
	"""
	Start a browser automation task with optional human intervention support.

	Connect to WebSocket at /ws/{job_id} to receive real-time updates and
	provide human intervention when requested.
	"""
	job_id = str(uuid.uuid4())[:8]

	job = Job(
		id=job_id,
		task=request.task,
		url=request.url,
		session_id=request.session_id,
		max_steps=request.max_steps,
		timeout_seconds=request.timeout_seconds,
		headless=request.headless,
		enable_human_intervention=request.enable_human_intervention,
		auto_request_intervention=request.auto_request_intervention,
		use_proxy_rotation=request.use_proxy_rotation,
	)

	jobs[job_id] = job

	# Start task in background
	background_tasks.add_task(run_browser_task, job)

	logger.info(f'Created job {job_id}: {request.task[:50]}... (intervention: {request.enable_human_intervention})')

	return BrowseResponse(
		job_id=job_id,
		status='pending',
		message='Task started. Connect to /ws/{job_id} for real-time updates and human intervention.',
		started_at=datetime.now().isoformat(),
	)


@app.get('/jobs/{job_id}', response_model=JobStatusResponse)
async def get_job_status(job_id: str):
	"""Get the status of a browser automation job including intervention state."""
	if job_id not in jobs:
		raise HTTPException(status_code=404, detail=f'Job {job_id} not found')

	job = jobs[job_id]

	return JobStatusResponse(
		job_id=job.id,
		status=job.status.value,
		progress=job.progress,
		current_step=job.current_step,
		max_steps=job.max_steps,
		result=job.result,
		error=job.error,
		urls_visited=job.urls_visited,
		started_at=job.started_at.isoformat() if job.started_at else None,
		completed_at=job.completed_at.isoformat() if job.completed_at else None,
		intervention_requested=job.intervention.requested,
		intervention_type=job.intervention.type.value if job.intervention.type else None,
		intervention_reason=job.intervention.reason,
		intervention_screenshot=job.intervention.screenshot,
		current_url=job.intervention.current_url,
	)


@app.post('/jobs/{job_id}/intervene')
async def submit_intervention(job_id: str, action: HumanAction):
	"""Submit a human intervention action for a job waiting for intervention."""
	if job_id not in jobs:
		raise HTTPException(status_code=404, detail=f'Job {job_id} not found')

	job = jobs[job_id]

	if job.status != JobStatus.WAITING_HUMAN:
		raise HTTPException(status_code=400, detail=f'Job is not waiting for intervention. Current status: {job.status.value}')

	job.intervention.response = action
	job.intervention.response_event.set()
	job.intervention.requested = False

	return {'message': 'Intervention action submitted', 'action_type': action.action_type}


@app.post('/jobs/{job_id}/request-intervention')
async def manual_intervention_request(
	job_id: str, intervention_type: InterventionType = InterventionType.CUSTOM, reason: str = 'Manual intervention requested'
):
	"""Manually request human intervention for a running job."""
	if job_id not in jobs:
		raise HTTPException(status_code=404, detail=f'Job {job_id} not found')

	job = jobs[job_id]

	if job.status != JobStatus.RUNNING:
		raise HTTPException(
			status_code=400, detail=f'Can only request intervention for running jobs. Current status: {job.status.value}'
		)

	# Set intervention state
	job.status = JobStatus.WAITING_HUMAN
	job.intervention.requested = True
	job.intervention.type = intervention_type
	job.intervention.reason = reason
	job.intervention.response_event.clear()

	# Capture current state
	if job.browser_session:
		job.intervention.screenshot = await capture_screenshot(job.browser_session)
		job.intervention.current_url = await get_current_url(job.browser_session)

	# Notify via WebSocket
	await manager.broadcast(
		job.id,
		{
			'type': 'intervention_requested',
			'job_id': job.id,
			'intervention_type': intervention_type.value,
			'reason': reason,
			'screenshot': job.intervention.screenshot,
			'current_url': job.intervention.current_url,
		},
	)

	return {
		'message': 'Intervention requested',
		'job_id': job_id,
		'intervention_type': intervention_type.value,
		'screenshot': job.intervention.screenshot,
		'current_url': job.intervention.current_url,
	}


@app.get('/jobs/{job_id}/screenshot')
async def get_screenshot(job_id: str):
	"""Get current screenshot from the browser session."""
	if job_id not in jobs:
		raise HTTPException(status_code=404, detail=f'Job {job_id} not found')

	job = jobs[job_id]

	if not job.browser_session:
		raise HTTPException(status_code=400, detail='No active browser session')

	screenshot = await capture_screenshot(job.browser_session)
	current_url = await get_current_url(job.browser_session)

	return {'screenshot': screenshot, 'current_url': current_url}


@app.delete('/jobs/{job_id}')
async def cancel_job(job_id: str):
	"""Cancel a running job."""
	if job_id not in jobs:
		raise HTTPException(status_code=404, detail=f'Job {job_id} not found')

	job = jobs[job_id]

	if job.status in [JobStatus.RUNNING, JobStatus.WAITING_HUMAN, JobStatus.PENDING]:
		job.status = JobStatus.CANCELLED
		job.error = 'Cancelled by user'
		job.completed_at = datetime.now()

		# Close browser session
		if job.browser_session:
			try:
				await job.browser_session.stop()
			except:
				pass
			job.browser_session = None

		# Notify via WebSocket
		await manager.broadcast(
			job.id,
			{
				'type': 'cancelled',
				'job_id': job.id,
			},
		)

		return {'message': f'Job {job_id} cancelled'}
	else:
		return {'message': f'Job {job_id} already finished with status: {job.status.value}'}


@app.get('/jobs')
async def list_jobs(status: Optional[str] = None, limit: int = 20):
	"""List all jobs, optionally filtered by status."""
	filtered_jobs = list(jobs.values())

	if status:
		filtered_jobs = [j for j in filtered_jobs if j.status.value == status]

	# Sort by started_at (most recent first)
	filtered_jobs.sort(key=lambda j: j.started_at or datetime.min, reverse=True)

	return [
		{
			'job_id': j.id,
			'task': j.task[:100],
			'status': j.status.value,
			'progress': j.progress,
			'intervention_requested': j.intervention.requested,
			'intervention_type': j.intervention.type.value if j.intervention.type else None,
			'started_at': j.started_at.isoformat() if j.started_at else None,
			'completed_at': j.completed_at.isoformat() if j.completed_at else None,
		}
		for j in filtered_jobs[:limit]
	]


@app.post('/browse/sync', response_model=BrowseResponse)
async def browse_sync(request: BrowseRequest):
	"""
	Execute a browser automation task synchronously.

	Note: Human intervention is not available in sync mode. For human intervention,
	use the async /browse endpoint with WebSocket connection.
	"""
	job_id = str(uuid.uuid4())[:8]

	# Disable human intervention for sync mode
	job = Job(
		id=job_id,
		task=request.task,
		url=request.url,
		session_id=request.session_id,
		max_steps=request.max_steps,
		timeout_seconds=request.timeout_seconds,
		headless=True,  # Force headless for sync mode
		enable_human_intervention=False,
		auto_request_intervention=False,
		use_proxy_rotation=request.use_proxy_rotation,
	)

	jobs[job_id] = job

	# Run synchronously
	await run_browser_task(job)

	return BrowseResponse(
		job_id=job_id,
		status=job.status.value,
		message=job.result or job.error or 'Task completed',
		result=job.result,
		steps_taken=job.current_step,
		urls_visited=job.urls_visited,
		screenshots=job.screenshots,
		error=job.error,
		started_at=job.started_at.isoformat() if job.started_at else None,
		completed_at=job.completed_at.isoformat() if job.completed_at else None,
	)


# ============================================
# API Key Provisioning Endpoints
# ============================================


class APIKeyProvisionRequest(BaseModel):
	"""Request to provision an API key from a provider."""

	provider: str = Field(
		..., description='Provider name: openai, anthropic, google, openrouter, together_ai, perplexity, brave_search, tavily'
	)
	key_name: Optional[str] = Field('NewsInsight-AutoGenerated', description='Name for the generated API key')
	auto_save: bool = Field(True, description='Automatically save the key to system settings')
	timeout_seconds: int = Field(300, description='Timeout for provisioning process', ge=60, le=600)
	headless: bool = Field(False, description='Run browser in headless mode (False recommended for login)')


class APIKeyProvisionResponse(BaseModel):
	"""Response from API key provisioning."""

	job_id: str
	status: str
	provider: str
	message: str
	api_key_masked: Optional[str] = None
	saved_to_settings: bool = False
	error: Optional[str] = None
	requires_intervention: bool = False
	intervention_type: Optional[str] = None


@app.get('/api-key/providers')
async def list_supported_providers():
	"""
	List all supported API providers for auto-provisioning.

	Returns provider names and their API key page URLs.
	"""
	providers = []
	for provider in APIProvider:
		if provider != APIProvider.UNKNOWN:
			providers.append(
				{
					'name': provider.value,
					'display_name': provider.value.replace('_', ' ').title(),
					'api_keys_url': PROVIDER_URLS.get(provider, ''),
				}
			)
	return {'providers': providers}


@app.get('/api-key/rate-limit')
async def get_api_key_rate_limit_info(http_request: Request):
	"""
	Get rate limit information for API key provisioning.

	Returns current rate limit settings and remaining quota for the client.
	"""
	client_ip = get_client_ip(http_request)

	# Peek at current status without consuming a request
	stats = await api_key_rate_limiter.get_stats()

	# Get remaining for this IP
	now = time.time()
	window_start = now - api_key_rate_limiter.window_seconds

	async with api_key_rate_limiter._lock:
		ip_requests = [ts for ts in api_key_rate_limiter._requests.get(client_ip, []) if ts > window_start]
		remaining = max(0, api_key_rate_limiter.requests_per_minute - len(ip_requests))

	return {
		'limit': api_key_rate_limiter.requests_per_minute,
		'remaining': remaining,
		'window_seconds': api_key_rate_limiter.window_seconds,
		'client_ip': client_ip,
		'stats': stats,
	}


@app.post('/api-key/provision', response_model=APIKeyProvisionResponse)
async def provision_api_key(
	request: APIKeyProvisionRequest,
	background_tasks: BackgroundTasks,
	http_request: Request,
):
	"""
	Start an API key provisioning task.

	This will:
	1. Launch a browser to the provider's API key page
	2. Handle login with human intervention if needed
	3. Generate a new API key
	4. Extract and optionally save the key to system settings

	Connect to WebSocket at /ws/{job_id} for real-time updates and
	to provide human intervention (login, 2FA) when requested.

	Rate Limited: {API_KEY_PROVISION_RATE_LIMIT} requests per minute per IP.
	"""
	# Check rate limit
	client_ip = get_client_ip(http_request)
	is_allowed, rate_info = await api_key_rate_limiter.is_allowed(client_ip)

	if not is_allowed:
		logger.warning(f'Rate limit exceeded for IP {client_ip} on API key provisioning')

		# Audit log rate limit event
		await provisioning_audit_logger.log_event(
			event_type='rate_limited',
			provider=request.provider,
			client_ip=client_ip,
			job_id='N/A',
			success=False,
			user_agent=http_request.headers.get('User-Agent'),
			error_message=f'Rate limit exceeded: {rate_info["limit"]} requests per minute',
		)

		raise HTTPException(
			status_code=429,
			detail={
				'error': 'Too Many Requests',
				'message': f'Rate limit exceeded. Maximum {rate_info["limit"]} requests per minute.',
				'retry_after': rate_info.get('retry_after', 60),
			},
			headers={
				'Retry-After': str(rate_info.get('retry_after', 60)),
				'X-RateLimit-Limit': str(rate_info['limit']),
				'X-RateLimit-Remaining': '0',
				'X-RateLimit-Reset': str(rate_info.get('reset', 0)),
			},
		)

	logger.info(f'API key provisioning request from {client_ip} (remaining: {rate_info["remaining"]})')

	# Validate provider
	try:
		provider = APIProvider(request.provider.lower())
		if provider == APIProvider.UNKNOWN:
			raise ValueError('Unknown provider')
	except ValueError:
		supported = [p.value for p in APIProvider if p != APIProvider.UNKNOWN]
		raise HTTPException(
			status_code=400,
			detail=f'Unknown provider: {request.provider}. Supported providers: {supported}',
		)

	job_id = str(uuid.uuid4())[:8]

	# Create job for tracking
	job = Job(
		id=job_id,
		task=f'API Key Provisioning for {provider.value}',
		url=PROVIDER_URLS.get(provider, ''),
		session_id=f'provision-{job_id}',
		max_steps=50,
		timeout_seconds=request.timeout_seconds,
		headless=request.headless,
		enable_human_intervention=True,
		auto_request_intervention=True,
		use_proxy_rotation=False,  # Don't use proxy for provider sites
	)

	jobs[job_id] = job

	# Get user agent for audit logging
	user_agent = http_request.headers.get('User-Agent')

	# Start provisioning in background
	async def run_provisioning():
		job.status = JobStatus.RUNNING
		job.started_at = datetime.now()

		# Audit log: provisioning started
		await provisioning_audit_logger.log_event(
			event_type='provision_started',
			provider=provider.value,
			client_ip=client_ip,
			job_id=job_id,
			success=True,
			user_agent=user_agent,
			details={
				'key_name': request.key_name or 'NewsInsight-AutoGenerated',
				'auto_save': request.auto_save,
				'headless': request.headless,
			},
		)

		try:
			# Initialize browser session
			import os

			is_docker = os.path.exists('/.dockerenv') or os.environ.get('DOCKER_CONTAINER', False)

			profile = BrowserProfile(
				headless=True if is_docker else request.headless,
				disable_security=True,
			)
			job.browser_session = BrowserSession(browser_profile=profile)

			# Initialize LLM
			llm = ChatAIDove(session_id=f'provision-{job_id}', timeout=120.0, max_retries=3)

			# Create provisioner with WebSocket broadcast
			async def broadcast_to_job(message: dict):
				message['job_id'] = job_id
				await manager.broadcast(job_id, message)

			provisioner = ApiKeyProvisioner(
				browser_session=job.browser_session,
				llm=llm,
				websocket_broadcast=broadcast_to_job,
			)

			# Run provisioning
			result = await provisioner.provision(
				provider=provider,
				key_name=request.key_name or 'NewsInsight-AutoGenerated',
				auto_save=request.auto_save,
				timeout_seconds=request.timeout_seconds,
			)

			# Update job based on result
			if result.get('status') == 'completed':
				job.status = JobStatus.COMPLETED
				job.result = result.get('message', 'API key provisioned successfully')

				# Audit log: provisioning completed
				await provisioning_audit_logger.log_event(
					event_type='provision_completed',
					provider=provider.value,
					client_ip=client_ip,
					job_id=job_id,
					success=True,
					user_agent=user_agent,
					details={
						'saved_to_settings': result.get('saved_to_settings', False),
						'key_masked': result.get('api_key_masked'),
					},
				)

			elif result.get('requires_intervention'):
				job.status = JobStatus.WAITING_HUMAN
				job.intervention.requested = True
				job.intervention.type = InterventionType.LOGIN
				job.intervention.reason = result.get('message', 'Human intervention required')

				# Audit log: login required
				await provisioning_audit_logger.log_event(
					event_type='login_required',
					provider=provider.value,
					client_ip=client_ip,
					job_id=job_id,
					success=True,
					user_agent=user_agent,
					details={'reason': result.get('message')},
				)

			else:
				job.status = JobStatus.FAILED
				job.error = result.get('error', 'Unknown error')

				# Audit log: provisioning failed
				await provisioning_audit_logger.log_event(
					event_type='provision_failed',
					provider=provider.value,
					client_ip=client_ip,
					job_id=job_id,
					success=False,
					user_agent=user_agent,
					error_message=result.get('error', 'Unknown error'),
				)

			# Broadcast final status
			await manager.broadcast(
				job_id,
				{
					'type': 'provisioning_complete' if result.get('status') == 'completed' else 'provisioning_update',
					'job_id': job_id,
					'status': result.get('status'),
					'provider': provider.value,
					'api_key_masked': result.get('api_key_masked'),
					'saved_to_settings': result.get('saved_to_settings', False),
					'error': result.get('error'),
					'message': result.get('message'),
				},
			)

		except Exception as e:
			logger.exception(f'Provisioning job {job_id} failed: {e}')
			job.status = JobStatus.FAILED
			job.error = str(e)

			# Audit log: exception during provisioning
			await provisioning_audit_logger.log_event(
				event_type='provision_failed',
				provider=provider.value,
				client_ip=client_ip,
				job_id=job_id,
				success=False,
				user_agent=user_agent,
				error_message=str(e),
				details={'exception_type': type(e).__name__},
			)

			await manager.broadcast(
				job_id,
				{
					'type': 'provisioning_failed',
					'job_id': job_id,
					'error': str(e),
				},
			)

		finally:
			job.completed_at = datetime.now()
			if job.browser_session:
				try:
					await job.browser_session.stop()
				except Exception as e:
					logger.warning(f'Error closing browser session: {e}')
				job.browser_session = None

	background_tasks.add_task(run_provisioning)

	logger.info(f'Started API key provisioning job {job_id} for {provider.value}')

	return APIKeyProvisionResponse(
		job_id=job_id,
		status='pending',
		provider=provider.value,
		message=f'API key provisioning started for {provider.value}. Connect to /ws/{job_id} for real-time updates.',
		requires_intervention=False,
	)


@app.post('/api-key/provision/{job_id}/login-complete')
async def notify_login_complete(job_id: str, http_request: Request):
	"""
	Notify that login has been completed for a provisioning job.

	Call this after completing login in the browser to continue
	the API key generation process.
	"""
	if job_id not in jobs:
		raise HTTPException(status_code=404, detail=f'Job {job_id} not found')

	job = jobs[job_id]

	if job.status != JobStatus.WAITING_HUMAN:
		raise HTTPException(status_code=400, detail=f'Job is not waiting for login. Current status: {job.status.value}')

	# Signal that login is complete
	job.intervention.response = HumanAction(
		action_type='login_complete',
		message='Login completed by user',
	)
	job.intervention.response_event.set()
	job.intervention.requested = False
	job.status = JobStatus.RUNNING

	# Audit log: login completed
	client_ip = get_client_ip(http_request)
	provider = job.task.replace('API Key Provisioning for ', '') if job.task else 'unknown'
	await provisioning_audit_logger.log_event(
		event_type='login_completed',
		provider=provider,
		client_ip=client_ip,
		job_id=job_id,
		success=True,
		user_agent=http_request.headers.get('User-Agent'),
	)

	await manager.broadcast(
		job_id,
		{
			'type': 'login_complete',
			'job_id': job_id,
			'message': 'Login completed, continuing with API key generation...',
		},
	)

	return {'message': 'Login completion acknowledged', 'job_id': job_id}


@app.post('/api-key/provision/{job_id}/submit-2fa')
async def submit_2fa_code(job_id: str, code: str, http_request: Request):
	"""
	Submit a 2FA code for a provisioning job.

	Provide the verification code received via SMS or authenticator app.
	"""
	if job_id not in jobs:
		raise HTTPException(status_code=404, detail=f'Job {job_id} not found')

	job = jobs[job_id]

	if job.status != JobStatus.WAITING_HUMAN:
		raise HTTPException(status_code=400, detail=f'Job is not waiting for 2FA. Current status: {job.status.value}')

	# Provide 2FA code
	job.intervention.response = HumanAction(
		action_type='type',
		value=code,
		message='2FA code provided by user',
	)
	job.intervention.response_event.set()
	job.intervention.requested = False
	job.status = JobStatus.RUNNING

	# Audit log: 2FA completed
	client_ip = get_client_ip(http_request)
	provider = job.task.replace('API Key Provisioning for ', '') if job.task else 'unknown'
	await provisioning_audit_logger.log_event(
		event_type='twofa_completed',
		provider=provider,
		client_ip=client_ip,
		job_id=job_id,
		success=True,
		user_agent=http_request.headers.get('User-Agent'),
	)

	await manager.broadcast(
		job_id,
		{
			'type': '2fa_submitted',
			'job_id': job_id,
			'message': '2FA code submitted, continuing...',
		},
	)

	return {'message': '2FA code submitted', 'job_id': job_id}


if __name__ == '__main__':
	import uvicorn

	uvicorn.run(app, host='0.0.0.0', port=8500)

```

---

## backend/services/ip-rotation/ip_rotation.go

```go
package main

import (
	"crypto/rand"
	"encoding/json"
	"errors"
	"fmt"
	"log"
	"math/big"
	"net"
	"net/url"
	"os"
	"path/filepath"
	"strings"
	"sync"
	"time"
)

// ProxyIP는 단일 프록시 설정과 통계 정보를 나타냅니다.
type ProxyIP struct {
	ID              string    `json:"id"`
	Address         string    `json:"address"`  // e.g., "http://proxy.example.com:8080" or "socks5://10.0.0.1:1080"
	Protocol        string    `json:"protocol"` // http, https, socks4, socks5
	Username        string    `json:"username,omitempty"`
	Password        string    `json:"password,omitempty"`
	Country         string    `json:"country,omitempty"`
	City            string    `json:"city,omitempty"`
	Enabled         bool      `json:"enabled"`
	UsageCount      int64     `json:"usageCount"`
	LastUsed        time.Time `json:"lastUsed,omitempty"`
	SuccessCount    int64     `json:"successCount"`
	FailCount       int64     `json:"failCount"`
	CaptchaCount    int64     `json:"captchaCount"`
	AvgLatencyMs    int64     `json:"avgLatencyMs"`
	CreatedAt       time.Time `json:"createdAt"`
	DisabledAt      time.Time `json:"disabledAt,omitempty"` // When proxy was auto-disabled
	LastHealthCheck time.Time `json:"lastHealthCheck,omitempty"`
	HealthStatus    string    `json:"healthStatus,omitempty"` // healthy, unhealthy, unknown
}

// RotationStrategy는 프록시 선택(로테이션) 전략을 정의합니다.
type RotationStrategy string

const (
	StrategyRoundRobin RotationStrategy = "round_robin"
	StrategyRandom     RotationStrategy = "random"
	StrategyLeastUsed  RotationStrategy = "least_used"
	StrategyWeighted   RotationStrategy = "weighted"   // based on success rate
	StrategyGeographic RotationStrategy = "geographic" // based on country/region
)

// validStrategies는 RotationStrategy 값 검증에 사용되는 허용 목록입니다.
var validStrategies = map[RotationStrategy]bool{
	StrategyRoundRobin: true,
	StrategyRandom:     true,
	StrategyLeastUsed:  true,
	StrategyWeighted:   true,
	StrategyGeographic: true,
}

// IPPoolConfig는 IP 풀의 동작(전략/쿨다운/헬스체크/영속화) 설정을 담습니다.
type IPPoolConfig struct {
	Strategy            RotationStrategy `json:"strategy"`
	MaxFailures         int              `json:"maxFailures"`     // auto-disable after N failures
	CooldownMinutes     int              `json:"cooldownMinutes"` // re-enable after cooldown
	PreferredCountry    string           `json:"preferredCountry,omitempty"`
	HealthCheckInterval int              `json:"healthCheckInterval"`       // seconds between health checks
	HealthCheckTimeout  int              `json:"healthCheckTimeout"`        // seconds for health check timeout
	PersistencePath     string           `json:"persistencePath,omitempty"` // path to save/load pool state
}

// Validate는 IPPoolConfig 값이 유효한지 검사하고, 잘못된 설정이면 오류를 반환합니다.
func (c *IPPoolConfig) Validate() error {
	if c.Strategy != "" && !validStrategies[c.Strategy] {
		return fmt.Errorf("invalid strategy: %s, must be one of: round_robin, random, least_used, weighted, geographic", c.Strategy)
	}
	if c.MaxFailures < 0 {
		return errors.New("maxFailures must be non-negative")
	}
	if c.CooldownMinutes < 0 {
		return errors.New("cooldownMinutes must be non-negative")
	}
	if c.HealthCheckInterval < 0 {
		return errors.New("healthCheckInterval must be non-negative")
	}
	if c.HealthCheckTimeout < 0 {
		return errors.New("healthCheckTimeout must be non-negative")
	}
	return nil
}

// IPPoolState는 IP 풀의 상태를 파일에 저장/복원하기 위한 직렬화 구조체입니다.
type IPPoolState struct {
	Proxies map[string]*ProxyIP `json:"proxies"`
	Order   []string            `json:"order"`
	Index   int                 `json:"index"`
	Config  IPPoolConfig        `json:"config"`
	SavedAt time.Time           `json:"savedAt"`
}

// IPPool은 프록시 풀을 관리하고 로테이션/통계/헬스체크/영속화를 제공합니다.
type IPPool struct {
	mu                 sync.RWMutex
	proxies            map[string]*ProxyIP
	order              []string // for round-robin
	index              int      // current index for round-robin
	config             IPPoolConfig
	cooldownTicker     *time.Ticker
	healthCheckTicker  *time.Ticker
	stopCooldown       chan struct{}
	stopHealthCheck    chan struct{}
	cooldownRunning    bool
	healthCheckRunning bool
}

var (
	globalIPPool *IPPool
	muIPPool     sync.RWMutex
)

// initIPPool은 환경 변수 기반 설정을 읽어 전역 IP 풀을 초기화합니다.
func initIPPool() {
	// Get config from environment
	strategy := RotationStrategy(os.Getenv("STRATEGY"))
	if strategy == "" {
		strategy = StrategyRoundRobin
	}

	maxFailures := 5
	if v := os.Getenv("MAX_FAILURES"); v != "" {
		fmt.Sscanf(v, "%d", &maxFailures)
	}

	cooldownMinutes := 30
	if v := os.Getenv("COOLDOWN_MINUTES"); v != "" {
		fmt.Sscanf(v, "%d", &cooldownMinutes)
	}

	healthCheckInterval := 300
	if v := os.Getenv("HEALTH_CHECK_INTERVAL"); v != "" {
		fmt.Sscanf(v, "%d", &healthCheckInterval)
	}

	persistencePath := os.Getenv("PERSISTENCE_PATH")

	globalIPPool = NewIPPool(IPPoolConfig{
		Strategy:            strategy,
		MaxFailures:         maxFailures,
		CooldownMinutes:     cooldownMinutes,
		HealthCheckInterval: healthCheckInterval,
		HealthCheckTimeout:  10,
		PersistencePath:     persistencePath,
	})

	// Load existing state if persistence path is set
	if persistencePath != "" {
		if err := globalIPPool.LoadFromFile(persistencePath); err != nil {
			log.Printf("[IP-ROTATION] Failed to load state: %v", err)
		}
	}
}

// NewIPPool은 주어진 설정으로 IPPool을 생성하고, 필요 시 쿨다운/헬스체크 루틴을 시작합니다.
func NewIPPool(config IPPoolConfig) *IPPool {
	pool := &IPPool{
		proxies:         make(map[string]*ProxyIP),
		order:           make([]string, 0),
		index:           0,
		config:          config,
		stopCooldown:    make(chan struct{}),
		stopHealthCheck: make(chan struct{}),
	}

	// Start cooldown checker if cooldown is configured
	if config.CooldownMinutes > 0 {
		pool.StartCooldownChecker()
	}

	// Start health checker if configured
	if config.HealthCheckInterval > 0 {
		pool.StartHealthChecker()
	}

	return pool
}

// StartCooldownChecker는 쿨다운 이후 프록시를 자동 재활성화하는 백그라운드 루틴을 시작합니다.
func (p *IPPool) StartCooldownChecker() {
	p.mu.Lock()
	if p.cooldownRunning {
		p.mu.Unlock()
		return
	}
	p.cooldownRunning = true
	// Check every minute for cooldown expiry
	p.cooldownTicker = time.NewTicker(1 * time.Minute)
	p.mu.Unlock()

	go func() {
		log.Printf("[IP-ROTATION] Cooldown checker started (cooldown=%d minutes)", p.config.CooldownMinutes)
		for {
			select {
			case <-p.cooldownTicker.C:
				p.checkAndReenableProxies()
			case <-p.stopCooldown:
				p.cooldownTicker.Stop()
				log.Printf("[IP-ROTATION] Cooldown checker stopped")
				return
			}
		}
	}()
}

// StopCooldownChecker는 쿨다운 체크 백그라운드 루틴을 중지합니다.
func (p *IPPool) StopCooldownChecker() {
	p.mu.Lock()
	defer p.mu.Unlock()
	if p.cooldownRunning {
		close(p.stopCooldown)
		p.cooldownRunning = false
		p.stopCooldown = make(chan struct{})
	}
}

// checkAndReenableProxies는 비활성화된 프록시의 쿨다운 만료 여부를 확인하고 재활성화합니다.
func (p *IPPool) checkAndReenableProxies() {
	p.mu.Lock()
	defer p.mu.Unlock()

	if p.config.CooldownMinutes <= 0 {
		return
	}

	cooldownDuration := time.Duration(p.config.CooldownMinutes) * time.Minute
	now := time.Now()

	for id, proxy := range p.proxies {
		if !proxy.Enabled && !proxy.DisabledAt.IsZero() {
			if now.Sub(proxy.DisabledAt) >= cooldownDuration {
				proxy.Enabled = true
				proxy.FailCount = 0 // Reset fail count on re-enable
				proxy.DisabledAt = time.Time{}
				log.Printf("[IP-ROTATION] Proxy re-enabled after cooldown: id=%s addr=%s", id, proxy.Address)
			}
		}
	}
}

// StartHealthChecker는 주기적으로 프록시 가용성을 점검하는 헬스체크 루틴을 시작합니다.
func (p *IPPool) StartHealthChecker() {
	p.mu.Lock()
	if p.healthCheckRunning {
		p.mu.Unlock()
		return
	}
	p.healthCheckRunning = true
	interval := p.config.HealthCheckInterval
	if interval <= 0 {
		interval = 300 // default 5 minutes
	}
	p.healthCheckTicker = time.NewTicker(time.Duration(interval) * time.Second)
	p.mu.Unlock()

	go func() {
		log.Printf("[IP-ROTATION] Health checker started (interval=%d seconds)", interval)
		for {
			select {
			case <-p.healthCheckTicker.C:
				p.runHealthChecks()
			case <-p.stopHealthCheck:
				p.healthCheckTicker.Stop()
				log.Printf("[IP-ROTATION] Health checker stopped")
				return
			}
		}
	}()
}

// StopHealthChecker는 헬스체크 백그라운드 루틴을 중지합니다.
func (p *IPPool) StopHealthChecker() {
	p.mu.Lock()
	defer p.mu.Unlock()
	if p.healthCheckRunning {
		close(p.stopHealthCheck)
		p.healthCheckRunning = false
		p.stopHealthCheck = make(chan struct{})
	}
}

// runHealthChecks는 활성화된 프록시들에 대해 병렬 헬스체크를 수행하고 상태를 업데이트합니다.
func (p *IPPool) runHealthChecks() {
	p.mu.RLock()
	proxiesToCheck := make([]*ProxyIP, 0)
	for _, proxy := range p.proxies {
		if proxy.Enabled {
			proxiesToCheck = append(proxiesToCheck, proxy)
		}
	}
	timeout := p.config.HealthCheckTimeout
	if timeout <= 0 {
		timeout = 10
	}
	p.mu.RUnlock()

	var wg sync.WaitGroup
	for _, proxy := range proxiesToCheck {
		wg.Add(1)
		go func(px *ProxyIP) {
			defer wg.Done()
			healthy := p.checkProxyHealth(px, time.Duration(timeout)*time.Second)
			p.mu.Lock()
			px.LastHealthCheck = time.Now()
			if healthy {
				px.HealthStatus = "healthy"
			} else {
				px.HealthStatus = "unhealthy"
			}
			p.mu.Unlock()
		}(proxy)
	}
	wg.Wait()
	log.Printf("[IP-ROTATION] Health check completed for %d proxies", len(proxiesToCheck))
}

// checkProxyHealth는 프록시 호스트에 TCP 연결을 시도하여 도달 가능 여부를 반환합니다.
func (p *IPPool) checkProxyHealth(proxy *ProxyIP, timeout time.Duration) bool {
	proxyURL, err := proxy.GetProxyURL()
	if err != nil {
		return false
	}

	// Extract host:port from proxy URL
	host := proxyURL.Host
	if host == "" {
		return false
	}

	conn, err := net.DialTimeout("tcp", host, timeout)
	if err != nil {
		log.Printf("[IP-ROTATION] Health check failed for %s: %v", proxy.ID, err)
		return false
	}
	conn.Close()
	return true
}

// RunHealthCheckNow는 즉시 헬스체크를 비동기로 트리거합니다.
func (p *IPPool) RunHealthCheckNow() {
	go p.runHealthChecks()
}

// GetNextProxy는 설정된 로테이션 전략에 따라 다음 프록시를 선택하고 사용 통계를 갱신합니다.
func (p *IPPool) GetNextProxy() (*ProxyIP, error) {
	p.mu.Lock()
	defer p.mu.Unlock()

	enabledProxies := p.getEnabledProxies()
	if len(enabledProxies) == 0 {
		return nil, errors.New("no enabled proxies available")
	}

	var selected *ProxyIP

	switch p.config.Strategy {
	case StrategyRoundRobin:
		selected = p.selectRoundRobin(enabledProxies)
	case StrategyRandom:
		selected = p.selectRandom(enabledProxies)
	case StrategyLeastUsed:
		selected = p.selectLeastUsed(enabledProxies)
	case StrategyWeighted:
		selected = p.selectWeighted(enabledProxies)
	case StrategyGeographic:
		selected = p.selectGeographic(enabledProxies)
	default:
		selected = p.selectRoundRobin(enabledProxies)
	}

	if selected != nil {
		selected.UsageCount++
		selected.LastUsed = time.Now()
		log.Printf("[IP-ROTATION] Selected proxy: id=%s addr=%s strategy=%s usage_count=%d",
			selected.ID, selected.Address, p.config.Strategy, selected.UsageCount)
	}

	return selected, nil
}

// getEnabledProxies는 Enabled=true인 프록시 목록을 반환합니다.
func (p *IPPool) getEnabledProxies() []*ProxyIP {
	var enabled []*ProxyIP
	for _, proxy := range p.proxies {
		if proxy.Enabled {
			enabled = append(enabled, proxy)
		}
	}
	return enabled
}

// selectRoundRobin은 라운드로빈 순서(order)를 기준으로 다음 사용 가능한 프록시를 선택합니다.
func (p *IPPool) selectRoundRobin(proxies []*ProxyIP) *ProxyIP {
	if len(proxies) == 0 {
		return nil
	}
	// Find valid index
	if p.index >= len(p.order) {
		p.index = 0
	}

	// Try to find next enabled proxy
	attempts := 0
	for attempts < len(p.order) {
		if p.index >= len(p.order) {
			p.index = 0
		}
		id := p.order[p.index]
		p.index++
		if proxy, ok := p.proxies[id]; ok && proxy.Enabled {
			return proxy
		}
		attempts++
	}

	// Fallback to first enabled
	if len(proxies) > 0 {
		return proxies[0]
	}
	return nil
}

// secureRandomInt는 crypto/rand를 사용해 [0, max) 범위의 난수를 생성합니다.
func secureRandomInt(max int) int {
	if max <= 0 {
		return 0
	}
	n, err := rand.Int(rand.Reader, big.NewInt(int64(max)))
	if err != nil {
		// Fallback to time-based (should not happen)
		return int(time.Now().UnixNano()) % max
	}
	return int(n.Int64())
}

// selectRandom은 사용 가능한 프록시 중 하나를 무작위로 선택합니다.
func (p *IPPool) selectRandom(proxies []*ProxyIP) *ProxyIP {
	if len(proxies) == 0 {
		return nil
	}
	idx := secureRandomInt(len(proxies))
	return proxies[idx]
}

// selectLeastUsed는 UsageCount가 가장 낮은 프록시를 선택합니다.
func (p *IPPool) selectLeastUsed(proxies []*ProxyIP) *ProxyIP {
	if len(proxies) == 0 {
		return nil
	}
	min := proxies[0]
	for _, proxy := range proxies[1:] {
		if proxy.UsageCount < min.UsageCount {
			min = proxy
		}
	}
	return min
}

// selectWeighted는 성공률과 CAPTCHA 패널티 기반 가중치 랜덤 선택으로 프록시를 선택합니다.
func (p *IPPool) selectWeighted(proxies []*ProxyIP) *ProxyIP {
	if len(proxies) == 0 {
		return nil
	}

	// Calculate weights based on success rate
	// Use a minimum weight to give all proxies some chance
	const minWeight = 10.0
	weights := make([]float64, len(proxies))
	totalWeight := 0.0

	for i, proxy := range proxies {
		total := proxy.SuccessCount + proxy.FailCount
		var baseWeight float64
		if total == 0 {
			// New proxy gets a neutral weight (50% success assumed + exploration bonus)
			baseWeight = 50.0 + minWeight
		} else {
			rate := float64(proxy.SuccessCount) / float64(total) * 100
			baseWeight = rate + minWeight
		}

		captchaRate := float64(proxy.CaptchaCount) / float64(proxy.UsageCount+1)
		captchaPenalty := 1.0 - (captchaRate * 0.7)
		if captchaPenalty < 0.1 {
			captchaPenalty = 0.1
		}

		weight := baseWeight * captchaPenalty
		if weight < minWeight {
			weight = minWeight
		}
		weights[i] = weight
		totalWeight += weight
	}

	if totalWeight <= 0 {
		return proxies[secureRandomInt(len(proxies))]
	}

	// Generate random value in [0, totalWeight)
	randN, err := rand.Int(rand.Reader, big.NewInt(int64(totalWeight*1000)))
	if err != nil {
		// Fallback
		return proxies[secureRandomInt(len(proxies))]
	}
	randVal := float64(randN.Int64()) / 1000.0

	// Select based on cumulative weight
	cumulative := 0.0
	for i, weight := range weights {
		cumulative += weight
		if randVal < cumulative {
			return proxies[i]
		}
	}

	// Fallback to last proxy
	return proxies[len(proxies)-1]
}

// selectGeographic은 선호 국가 설정이 있으면 해당 국가 프록시를 우선 선택하고, 없으면 라운드로빈으로 폴백합니다.
func (p *IPPool) selectGeographic(proxies []*ProxyIP) *ProxyIP {
	if len(proxies) == 0 {
		return nil
	}
	// Prefer proxies matching configured country
	if p.config.PreferredCountry != "" {
		var matchingProxies []*ProxyIP
		for _, proxy := range proxies {
			if strings.EqualFold(proxy.Country, p.config.PreferredCountry) {
				matchingProxies = append(matchingProxies, proxy)
			}
		}
		if len(matchingProxies) > 0 {
			// Use round-robin among matching proxies
			return matchingProxies[secureRandomInt(len(matchingProxies))]
		}
	}
	// Fallback to round-robin
	return p.selectRoundRobin(proxies)
}

// RecordSuccess는 특정 프록시의 성공 결과와 평균 지연시간을 기록합니다.
func (p *IPPool) RecordSuccess(proxyID string, latencyMs int64) {
	p.mu.Lock()
	defer p.mu.Unlock()

	if proxy, ok := p.proxies[proxyID]; ok {
		proxy.SuccessCount++
		// Update average latency
		total := proxy.SuccessCount + proxy.FailCount
		if total > 0 {
			proxy.AvgLatencyMs = (proxy.AvgLatencyMs*(total-1) + latencyMs) / total
		}
		log.Printf("[IP-ROTATION] Success recorded: id=%s success=%d fail=%d latency=%dms",
			proxyID, proxy.SuccessCount, proxy.FailCount, latencyMs)
	}
}

// RecordCaptcha는 특정 프록시에 CAPTCHA 발생을 기록하여 선택 가중치에 반영될 수 있도록 합니다.
func (p *IPPool) RecordCaptcha(proxyID string, captchaType string) {
	p.mu.Lock()
	defer p.mu.Unlock()

	if proxy, ok := p.proxies[proxyID]; ok {
		proxy.CaptchaCount++
		log.Printf("[IP-ROTATION] CAPTCHA recorded: id=%s count=%d type=%s",
			proxyID, proxy.CaptchaCount, captchaType)
	}
}

// RecordFailure는 특정 프록시의 실패를 기록하고, 임계치 초과 시 자동으로 비활성화합니다.
func (p *IPPool) RecordFailure(proxyID string, reason string) {
	p.mu.Lock()
	defer p.mu.Unlock()

	if proxy, ok := p.proxies[proxyID]; ok {
		proxy.FailCount++
		log.Printf("[IP-ROTATION] Failure recorded: id=%s success=%d fail=%d reason=%s",
			proxyID, proxy.SuccessCount, proxy.FailCount, reason)

		// Auto-disable if too many failures
		if p.config.MaxFailures > 0 && proxy.FailCount >= int64(p.config.MaxFailures) {
			proxy.Enabled = false
			proxy.DisabledAt = time.Now()
			log.Printf("[IP-ROTATION] Proxy auto-disabled due to failures: id=%s (will re-enable after %d minutes)",
				proxyID, p.config.CooldownMinutes)
		}
	}
}

// AddProxy는 프록시를 풀에 추가하고 형식/프로토콜을 검증한 뒤 기본값을 설정합니다.
func (p *IPPool) AddProxy(proxy *ProxyIP) error {
	p.mu.Lock()
	defer p.mu.Unlock()

	if proxy.ID == "" {
		proxy.ID = "proxy_" + randomID()
	}
	if proxy.Address == "" {
		return errors.New("proxy address is required")
	}

	// Validate proxy address format
	if _, err := url.Parse(proxy.Address); err != nil {
		return fmt.Errorf("invalid proxy address format: %w", err)
	}

	if proxy.Protocol == "" {
		proxy.Protocol = "http"
	}

	// Validate protocol
	validProtocols := map[string]bool{"http": true, "https": true, "socks4": true, "socks5": true}
	if !validProtocols[strings.ToLower(proxy.Protocol)] {
		return fmt.Errorf("invalid protocol: %s, must be one of: http, https, socks4, socks5", proxy.Protocol)
	}
	proxy.Protocol = strings.ToLower(proxy.Protocol)

	proxy.CreatedAt = time.Now()
	proxy.Enabled = true
	proxy.HealthStatus = "unknown"

	p.proxies[proxy.ID] = proxy
	p.order = append(p.order, proxy.ID)

	log.Printf("[IP-ROTATION] Proxy added: id=%s addr=%s protocol=%s country=%s",
		proxy.ID, proxy.Address, proxy.Protocol, proxy.Country)

	// Auto-save if persistence is configured
	p.autoSave()

	return nil
}

// RemoveProxy는 풀에서 프록시를 제거하고 라운드로빈 순서도 갱신합니다.
func (p *IPPool) RemoveProxy(id string) error {
	p.mu.Lock()
	defer p.mu.Unlock()

	if _, ok := p.proxies[id]; !ok {
		return errors.New("proxy not found")
	}

	delete(p.proxies, id)

	// Remove from order
	for i, oid := range p.order {
		if oid == id {
			p.order = append(p.order[:i], p.order[i+1:]...)
			break
		}
	}

	log.Printf("[IP-ROTATION] Proxy removed: id=%s", id)

	// Auto-save if persistence is configured
	p.autoSave()

	return nil
}

// GetAllProxies는 풀에 등록된 모든 프록시 목록을 반환합니다.
func (p *IPPool) GetAllProxies() []*ProxyIP {
	p.mu.RLock()
	defer p.mu.RUnlock()

	proxies := make([]*ProxyIP, 0, len(p.proxies))
	for _, proxy := range p.proxies {
		proxies = append(proxies, proxy)
	}
	return proxies
}

// GetPoolStats는 풀 전체의 통계를 집계하여 반환합니다.
func (p *IPPool) GetPoolStats() map[string]any {
	p.mu.RLock()
	defer p.mu.RUnlock()

	var totalUsage, totalSuccess, totalFail, totalCaptcha int64
	enabledCount := 0
	disabledCount := 0
	healthyCount := 0
	unhealthyCount := 0

	for _, proxy := range p.proxies {
		totalUsage += proxy.UsageCount
		totalSuccess += proxy.SuccessCount
		totalFail += proxy.FailCount
		totalCaptcha += proxy.CaptchaCount
		if proxy.Enabled {
			enabledCount++
		} else {
			disabledCount++
		}
		switch proxy.HealthStatus {
		case "healthy":
			healthyCount++
		case "unhealthy":
			unhealthyCount++
		}
	}

	successRate := float64(0)
	if totalSuccess+totalFail > 0 {
		successRate = float64(totalSuccess) / float64(totalSuccess+totalFail) * 100
	}

	captchaRate := float64(0)
	if totalUsage > 0 {
		captchaRate = float64(totalCaptcha) / float64(totalUsage) * 100
	}

	return map[string]any{
		"totalProxies":     len(p.proxies),
		"enabledProxies":   enabledCount,
		"disabledProxies":  disabledCount,
		"healthyProxies":   healthyCount,
		"unhealthyProxies": unhealthyCount,
		"totalUsage":       totalUsage,
		"totalSuccess":     totalSuccess,
		"totalFail":        totalFail,
		"totalCaptcha":     totalCaptcha,
		"successRate":      fmt.Sprintf("%.2f%%", successRate),
		"captchaRate":      fmt.Sprintf("%.2f%%", captchaRate),
		"strategy":         p.config.Strategy,
		"currentIndex":     p.index,
		"cooldownMinutes":  p.config.CooldownMinutes,
		"maxFailures":      p.config.MaxFailures,
	}
}

// UpdateConfig는 설정을 검증 후 적용하고, 변경 사항에 따라 백그라운드 루틴을 재시작합니다.
func (p *IPPool) UpdateConfig(cfg IPPoolConfig) error {
	if err := cfg.Validate(); err != nil {
		return err
	}

	p.mu.Lock()
	oldCooldown := p.config.CooldownMinutes
	oldHealthInterval := p.config.HealthCheckInterval
	p.config = cfg
	p.mu.Unlock()

	log.Printf("[IP-ROTATION] Config updated: strategy=%s maxFailures=%d cooldown=%dm healthInterval=%ds",
		cfg.Strategy, cfg.MaxFailures, cfg.CooldownMinutes, cfg.HealthCheckInterval)

	// Restart cooldown checker if cooldown setting changed
	if cfg.CooldownMinutes != oldCooldown {
		p.StopCooldownChecker()
		if cfg.CooldownMinutes > 0 {
			p.StartCooldownChecker()
		}
	}

	// Restart health checker if interval changed
	if cfg.HealthCheckInterval != oldHealthInterval {
		p.StopHealthChecker()
		if cfg.HealthCheckInterval > 0 {
			p.StartHealthChecker()
		}
	}

	// Auto-save if persistence is configured
	p.autoSave()

	return nil
}

// GetProxyURL은 프록시 주소(Address)에 인증 정보가 있으면 포함하여 url.URL을 반환합니다.
func (p *ProxyIP) GetProxyURL() (*url.URL, error) {
	proxyAddr := p.Address
	if p.Username != "" && p.Password != "" {
		// Parse and add auth
		u, err := url.Parse(proxyAddr)
		if err != nil {
			return nil, err
		}
		u.User = url.UserPassword(p.Username, p.Password)
		return u, nil
	}
	return url.Parse(proxyAddr)
}

// ========== Persistence Functions ==========

// SaveToFile은 현재 풀 상태를 JSON 파일로 저장합니다.
func (p *IPPool) SaveToFile(path string) error {
	p.mu.RLock()
	state := IPPoolState{
		Proxies: p.proxies,
		Order:   p.order,
		Index:   p.index,
		Config:  p.config,
		SavedAt: time.Now(),
	}
	p.mu.RUnlock()

	data, err := json.MarshalIndent(state, "", "  ")
	if err != nil {
		return fmt.Errorf("failed to marshal pool state: %w", err)
	}

	// Ensure directory exists
	dir := filepath.Dir(path)
	if err := os.MkdirAll(dir, 0755); err != nil {
		return fmt.Errorf("failed to create directory: %w", err)
	}

	if err := os.WriteFile(path, data, 0644); err != nil {
		return fmt.Errorf("failed to write file: %w", err)
	}

	log.Printf("[IP-ROTATION] Pool state saved to: %s", path)
	return nil
}

// LoadFromFile은 JSON 파일에서 풀 상태를 로드하여 적용합니다.
func (p *IPPool) LoadFromFile(path string) error {
	data, err := os.ReadFile(path)
	if err != nil {
		if os.IsNotExist(err) {
			log.Printf("[IP-ROTATION] No existing pool state file found: %s", path)
			return nil // Not an error if file doesn't exist
		}
		return fmt.Errorf("failed to read file: %w", err)
	}

	var state IPPoolState
	if err := json.Unmarshal(data, &state); err != nil {
		return fmt.Errorf("failed to unmarshal pool state: %w", err)
	}

	p.mu.Lock()
	p.proxies = state.Proxies
	p.order = state.Order
	p.index = state.Index
	if state.Config.Strategy != "" {
		p.config = state.Config
	}
	p.mu.Unlock()

	log.Printf("[IP-ROTATION] Pool state loaded from: %s (saved at: %s, proxies: %d)",
		path, state.SavedAt.Format(time.RFC3339), len(state.Proxies))

	return nil
}

// autoSave는 PersistencePath가 설정된 경우 풀 상태를 비동기로 저장합니다.
func (p *IPPool) autoSave() {
	if p.config.PersistencePath != "" {
		go func() {
			// Release lock before saving
			if err := p.SaveToFile(p.config.PersistencePath); err != nil {
				log.Printf("[IP-ROTATION] Auto-save failed: %v", err)
			}
		}()
	}
}

// ResetStats는 모든 프록시의 통계 값을 초기화합니다.
func (p *IPPool) ResetStats() {
	p.mu.Lock()
	defer p.mu.Unlock()

	for _, proxy := range p.proxies {
		proxy.UsageCount = 0
		proxy.SuccessCount = 0
		proxy.FailCount = 0
		proxy.CaptchaCount = 0
		proxy.AvgLatencyMs = 0
	}

	log.Printf("[IP-ROTATION] Statistics reset for all proxies")
}

// ResetProxyStats는 특정 프록시의 통계를 초기화하고 비활성화 상태였다면 재활성화합니다.
func (p *IPPool) ResetProxyStats(proxyID string) error {
	p.mu.Lock()
	defer p.mu.Unlock()

	proxy, ok := p.proxies[proxyID]
	if !ok {
		return errors.New("proxy not found")
	}

	proxy.UsageCount = 0
	proxy.SuccessCount = 0
	proxy.FailCount = 0
	proxy.CaptchaCount = 0
	proxy.AvgLatencyMs = 0
	// Re-enable if disabled
	if !proxy.Enabled {
		proxy.Enabled = true
		proxy.DisabledAt = time.Time{}
	}

	log.Printf("[IP-ROTATION] Statistics reset for proxy: %s", proxyID)
	return nil
}

// randomID는 프록시 ID 생성을 위한 짧은 랜덤 문자열을 반환합니다.
func randomID() string {
	const chars = "abcdefghijklmnopqrstuvwxyz0123456789"
	result := make([]byte, 8)
	for i := range result {
		n, _ := rand.Int(rand.Reader, big.NewInt(int64(len(chars))))
		result[i] = chars[n.Int64()]
	}
	return string(result)
}

// calculateSuccessRate는 성공/실패 카운트를 기반으로 성공률(%)을 계산합니다.
func calculateSuccessRate(p *ProxyIP) float64 {
	total := p.SuccessCount + p.FailCount
	if total == 0 {
		return 100.0
	}
	return float64(p.SuccessCount) / float64(total) * 100
}

```

---

## backend/services/ip-rotation/server.go

```go
package main

import (
	"encoding/json"
	"errors"
	"fmt"
	"log"
	"net/http"
	"os"
	"strings"
	"time"
)

// ========== IP Rotation HTTP 핸들러 ==========

// writeJSON은 주어진 데이터를 JSON으로 인코딩하여 응답으로 반환합니다.
func writeJSON(w http.ResponseWriter, status int, data any) {
	w.Header().Set("Content-Type", "application/json")
	w.WriteHeader(status)
	json.NewEncoder(w).Encode(data)
}

// writeErr는 에러를 JSON 형태로 응답합니다.
func writeErr(w http.ResponseWriter, status int, err error) {
	writeJSON(w, status, map[string]string{"error": err.Error()})
}

// handleHealth는 서비스 헬스체크 및 현재 프록시 풀 통계를 반환합니다.
func handleHealth(w http.ResponseWriter, r *http.Request) {
	stats := globalIPPool.GetPoolStats()
	writeJSON(w, http.StatusOK, map[string]any{
		"status":  "ok",
		"service": "ip-rotation",
		"stats":   stats,
	})
}

// handleProxyPool은 프록시 풀 전체 조회/추가(관리자용)를 처리합니다.
func handleProxyPool(w http.ResponseWriter, r *http.Request) {
	switch r.Method {
	case http.MethodGet:
		proxies := globalIPPool.GetAllProxies()
		stats := globalIPPool.GetPoolStats()
		writeJSON(w, http.StatusOK, map[string]any{
			"proxies": proxies,
			"stats":   stats,
		})
	case http.MethodPost:
		var proxy ProxyIP
		if err := json.NewDecoder(r.Body).Decode(&proxy); err != nil {
			writeErr(w, http.StatusBadRequest, err)
			return
		}
		if err := globalIPPool.AddProxy(&proxy); err != nil {
			writeErr(w, http.StatusBadRequest, err)
			return
		}
		writeJSON(w, http.StatusCreated, proxy)
	default:
		writeErr(w, http.StatusMethodNotAllowed, errors.New("method not allowed"))
	}
}

// handleProxyPoolByID는 특정 프록시 조회/삭제/부분 수정(관리자용)을 처리합니다.
func handleProxyPoolByID(w http.ResponseWriter, r *http.Request) {
	id := strings.TrimPrefix(r.URL.Path, "/admin/proxy-pool/")
	if id == "" {
		writeErr(w, http.StatusBadRequest, errors.New("missing proxy id"))
		return
	}

	switch r.Method {
	case http.MethodGet:
		globalIPPool.mu.RLock()
		proxy, ok := globalIPPool.proxies[id]
		globalIPPool.mu.RUnlock()
		if !ok {
			writeErr(w, http.StatusNotFound, errors.New("proxy not found"))
			return
		}
		writeJSON(w, http.StatusOK, proxy)
	case http.MethodDelete:
		if err := globalIPPool.RemoveProxy(id); err != nil {
			writeErr(w, http.StatusNotFound, err)
			return
		}
		writeJSON(w, http.StatusOK, map[string]string{"deleted": id})
	case http.MethodPatch:
		globalIPPool.mu.Lock()
		proxy, ok := globalIPPool.proxies[id]
		if !ok {
			globalIPPool.mu.Unlock()
			writeErr(w, http.StatusNotFound, errors.New("proxy not found"))
			return
		}
		var patch map[string]any
		if err := json.NewDecoder(r.Body).Decode(&patch); err != nil {
			globalIPPool.mu.Unlock()
			writeErr(w, http.StatusBadRequest, err)
			return
		}
		if v, ok := patch["enabled"].(bool); ok {
			proxy.Enabled = v
			if v {
				proxy.DisabledAt = time.Time{}
			} else {
				proxy.DisabledAt = time.Now()
			}
		}
		if v, ok := patch["address"].(string); ok && v != "" {
			proxy.Address = v
		}
		if v, ok := patch["country"].(string); ok {
			proxy.Country = v
		}
		if v, ok := patch["city"].(string); ok {
			proxy.City = v
		}
		if v, ok := patch["protocol"].(string); ok && v != "" {
			proxy.Protocol = v
		}
		if v, ok := patch["username"].(string); ok {
			proxy.Username = v
		}
		if v, ok := patch["password"].(string); ok {
			proxy.Password = v
		}
		// Handle success/failure recording
		if success, ok := patch["success"].(bool); ok && success {
			latency := int64(0)
			if v, ok := patch["latency_ms"].(float64); ok {
				latency = int64(v)
			}
			proxy.SuccessCount++
			total := proxy.SuccessCount + proxy.FailCount
			if total > 0 {
				proxy.AvgLatencyMs = (proxy.AvgLatencyMs*(total-1) + latency) / total
			}
		}
		if failure, ok := patch["failure"].(bool); ok && failure {
			proxy.FailCount++
			if globalIPPool.config.MaxFailures > 0 && proxy.FailCount >= int64(globalIPPool.config.MaxFailures) {
				proxy.Enabled = false
				proxy.DisabledAt = time.Now()
			}
		}
		globalIPPool.mu.Unlock()
		log.Printf("[IP-ROTATION] Proxy updated: id=%s enabled=%v", id, proxy.Enabled)

		// Auto-save
		globalIPPool.autoSave()

		writeJSON(w, http.StatusOK, proxy)
	default:
		writeErr(w, http.StatusMethodNotAllowed, errors.New("method not allowed"))
	}
}

// handleProxyPoolConfig는 풀 설정 조회/수정(관리자용)을 처리합니다.
func handleProxyPoolConfig(w http.ResponseWriter, r *http.Request) {
	switch r.Method {
	case http.MethodGet:
		globalIPPool.mu.RLock()
		cfg := globalIPPool.config
		globalIPPool.mu.RUnlock()
		writeJSON(w, http.StatusOK, cfg)
	case http.MethodPatch:
		var cfg IPPoolConfig
		if err := json.NewDecoder(r.Body).Decode(&cfg); err != nil {
			writeErr(w, http.StatusBadRequest, err)
			return
		}
		if err := globalIPPool.UpdateConfig(cfg); err != nil {
			writeErr(w, http.StatusBadRequest, err)
			return
		}
		writeJSON(w, http.StatusOK, cfg)
	default:
		writeErr(w, http.StatusMethodNotAllowed, errors.New("method not allowed"))
	}
}

// handleProxyRotateTest는 N회 로테이션을 수행해 선택 결과를 점검할 수 있는 테스트 API입니다.
func handleProxyRotateTest(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		writeErr(w, http.StatusMethodNotAllowed, errors.New("use POST"))
		return
	}

	var req struct {
		Count int `json:"count"`
	}
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		req.Count = 5 // default
	}
	if req.Count <= 0 {
		req.Count = 5
	}
	if req.Count > 100 {
		req.Count = 100
	}

	results := make([]map[string]any, 0, req.Count)

	for i := 0; i < req.Count; i++ {
		proxy, err := globalIPPool.GetNextProxy()
		if err != nil {
			results = append(results, map[string]any{
				"iteration": i + 1,
				"error":     err.Error(),
			})
			continue
		}
		results = append(results, map[string]any{
			"iteration":    i + 1,
			"proxyId":      proxy.ID,
			"address":      proxy.Address,
			"protocol":     proxy.Protocol,
			"country":      proxy.Country,
			"usageCount":   proxy.UsageCount,
			"successRate":  fmt.Sprintf("%.2f%%", calculateSuccessRate(proxy)),
			"healthStatus": proxy.HealthStatus,
		})
	}

	stats := globalIPPool.GetPoolStats()

	log.Printf("[IP-ROTATION] Rotation test completed: count=%d", req.Count)

	writeJSON(w, http.StatusOK, map[string]any{
		"rotations": results,
		"stats":     stats,
	})
}

// handleProxyHealthCheck는 즉시 헬스체크를 수행하도록 트리거합니다.
func handleProxyHealthCheck(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		writeErr(w, http.StatusMethodNotAllowed, errors.New("use POST"))
		return
	}

	globalIPPool.RunHealthCheckNow()
	writeJSON(w, http.StatusOK, map[string]string{
		"status":  "started",
		"message": "Health check started in background",
	})
}

// handleProxyResetStats는 전체 또는 특정 프록시의 통계를 초기화합니다.
func handleProxyResetStats(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		writeErr(w, http.StatusMethodNotAllowed, errors.New("use POST"))
		return
	}

	var req struct {
		ProxyID string `json:"proxyId"`
	}
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil || req.ProxyID == "" {
		// Reset all
		globalIPPool.ResetStats()
		writeJSON(w, http.StatusOK, map[string]string{
			"status":  "success",
			"message": "All proxy statistics reset",
		})
		return
	}

	if err := globalIPPool.ResetProxyStats(req.ProxyID); err != nil {
		writeErr(w, http.StatusNotFound, err)
		return
	}
	writeJSON(w, http.StatusOK, map[string]string{
		"status":  "success",
		"message": fmt.Sprintf("Statistics reset for proxy: %s", req.ProxyID),
	})
}

// handleProxySave는 현재 풀 상태를 파일로 저장합니다.
func handleProxySave(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		writeErr(w, http.StatusMethodNotAllowed, errors.New("use POST"))
		return
	}

	var req struct {
		Path string `json:"path"`
	}
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		writeErr(w, http.StatusBadRequest, err)
		return
	}

	path := req.Path
	if path == "" {
		globalIPPool.mu.RLock()
		path = globalIPPool.config.PersistencePath
		globalIPPool.mu.RUnlock()
	}
	if path == "" {
		path = "ip_pool_state.json"
	}

	if err := globalIPPool.SaveToFile(path); err != nil {
		writeErr(w, http.StatusInternalServerError, err)
		return
	}

	writeJSON(w, http.StatusOK, map[string]string{
		"status":  "success",
		"message": fmt.Sprintf("Pool state saved to: %s", path),
	})
}

// handleProxyLoad는 파일에서 풀 상태를 로드합니다.
func handleProxyLoad(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		writeErr(w, http.StatusMethodNotAllowed, errors.New("use POST"))
		return
	}

	var req struct {
		Path string `json:"path"`
	}
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		writeErr(w, http.StatusBadRequest, err)
		return
	}

	path := req.Path
	if path == "" {
		globalIPPool.mu.RLock()
		path = globalIPPool.config.PersistencePath
		globalIPPool.mu.RUnlock()
	}
	if path == "" {
		path = "ip_pool_state.json"
	}

	if err := globalIPPool.LoadFromFile(path); err != nil {
		writeErr(w, http.StatusInternalServerError, err)
		return
	}

	writeJSON(w, http.StatusOK, map[string]string{
		"status":  "success",
		"message": fmt.Sprintf("Pool state loaded from: %s", path),
	})
}

// handleGetNextProxy는 다음 프록시를 반환합니다(클라이언트/크롤러용).
func handleGetNextProxy(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodGet && r.Method != http.MethodPost {
		writeErr(w, http.StatusMethodNotAllowed, errors.New("use GET or POST"))
		return
	}

	proxy, err := globalIPPool.GetNextProxy()
	if err != nil {
		writeErr(w, http.StatusServiceUnavailable, err)
		return
	}

	writeJSON(w, http.StatusOK, map[string]any{
		"proxyId":      proxy.ID,
		"address":      proxy.Address,
		"protocol":     proxy.Protocol,
		"username":     proxy.Username,
		"password":     proxy.Password,
		"country":      proxy.Country,
		"healthStatus": proxy.HealthStatus,
	})
}

// handleRecordResult는 프록시의 성공/실패 결과를 기록합니다(클라이언트/크롤러용).
func handleRecordResult(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		writeErr(w, http.StatusMethodNotAllowed, errors.New("use POST"))
		return
	}

	var req struct {
		ProxyID   string `json:"proxyId"`
		Success   bool   `json:"success"`
		LatencyMs int64  `json:"latencyMs"`
		Reason    string `json:"reason"`
	}
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		writeErr(w, http.StatusBadRequest, err)
		return
	}

	if req.ProxyID == "" {
		writeErr(w, http.StatusBadRequest, errors.New("proxyId is required"))
		return
	}

	if req.Success {
		globalIPPool.RecordSuccess(req.ProxyID, req.LatencyMs)
	} else {
		globalIPPool.RecordFailure(req.ProxyID, req.Reason)
	}

	writeJSON(w, http.StatusOK, map[string]string{
		"status": "recorded",
	})
}

// handleRecordCaptcha는 프록시의 CAPTCHA 발생을 기록합니다(클라이언트/크롤러용).
func handleRecordCaptcha(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		writeErr(w, http.StatusMethodNotAllowed, errors.New("use POST"))
		return
	}

	var req struct {
		ProxyID string `json:"proxyId"`
		Type    string `json:"type"`
	}
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		writeErr(w, http.StatusBadRequest, err)
		return
	}

	if req.ProxyID == "" {
		writeErr(w, http.StatusBadRequest, errors.New("proxyId is required"))
		return
	}

	globalIPPool.RecordCaptcha(req.ProxyID, req.Type)

	writeJSON(w, http.StatusOK, map[string]string{
		"status": "recorded",
	})
}

// corsMiddleware는 CORS 헤더를 추가하고 OPTIONS 프리플라이트 요청을 처리합니다.
func corsMiddleware(next http.HandlerFunc) http.HandlerFunc {
	return func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Access-Control-Allow-Origin", "*")
		w.Header().Set("Access-Control-Allow-Methods", "GET, POST, PATCH, DELETE, OPTIONS")
		w.Header().Set("Access-Control-Allow-Headers", "Content-Type, Authorization")

		if r.Method == http.MethodOptions {
			w.WriteHeader(http.StatusOK)
			return
		}

		next(w, r)
	}
}

// main은 환경 변수 기반으로 IP 풀을 초기화하고 HTTP 서버를 시작합니다.
func main() {
	// Initialize the IP pool
	initIPPool()

	// Get port from environment
	port := os.Getenv("PORT")
	if port == "" {
		port = "8050"
	}

	// Register routes
	http.HandleFunc("/health", corsMiddleware(handleHealth))

	// Admin endpoints
	http.HandleFunc("/admin/proxy-pool", corsMiddleware(handleProxyPool))
	http.HandleFunc("/admin/proxy-pool/", corsMiddleware(handleProxyPoolByID))
	http.HandleFunc("/admin/proxy-pool-config", corsMiddleware(handleProxyPoolConfig))
	http.HandleFunc("/admin/proxy-rotate-test", corsMiddleware(handleProxyRotateTest))
	http.HandleFunc("/admin/proxy-health-check", corsMiddleware(handleProxyHealthCheck))
	http.HandleFunc("/admin/proxy-reset-stats", corsMiddleware(handleProxyResetStats))
	http.HandleFunc("/admin/proxy-save", corsMiddleware(handleProxySave))
	http.HandleFunc("/admin/proxy-load", corsMiddleware(handleProxyLoad))

	// Client endpoints (for crawlers to use)
	http.HandleFunc("/proxy/next", corsMiddleware(handleGetNextProxy))
	http.HandleFunc("/proxy/record", corsMiddleware(handleRecordResult))
	http.HandleFunc("/proxy/captcha", corsMiddleware(handleRecordCaptcha))

	log.Printf("[IP-ROTATION] Server starting on port %s", port)
	log.Printf("[IP-ROTATION] Config: strategy=%s maxFailures=%d cooldown=%dm",
		globalIPPool.config.Strategy, globalIPPool.config.MaxFailures, globalIPPool.config.CooldownMinutes)

	if err := http.ListenAndServe(":"+port, nil); err != nil {
		log.Fatalf("[IP-ROTATION] Server failed: %v", err)
	}
}

```

---

## backend/shared/__init__.py

```py
__all__ = []

```

---

## backend/shared/prometheus_metrics.py

```py
from __future__ import annotations

import time
from contextlib import contextmanager
from typing import Any, Callable, Iterable, Optional

from fastapi import FastAPI
from fastapi.responses import Response
from prometheus_client import CONTENT_TYPE_LATEST, Counter, Gauge, Histogram, generate_latest


def setup_metrics(app: FastAPI, service_name: str, version: str = "") -> None:
    def metrics_endpoint() -> Response:
        payload = generate_latest()
        return Response(content=payload, media_type=CONTENT_TYPE_LATEST)

    app.add_api_route("/metrics", metrics_endpoint, methods=["GET"])


class ServiceMetrics:
    def __init__(self, service_name: str) -> None:
        self._service_name = service_name.replace("-", "_")

    def _full_name(self, metric_name: str) -> str:
        metric = metric_name.strip()
        if metric.startswith(f"{self._service_name}_"):
            return metric
        return f"{self._service_name}_{metric}"

    def create_counter(self, name: str, documentation: str, labelnames: Optional[Iterable[str]] = None) -> Counter:
        return Counter(self._full_name(name), documentation, labelnames=list(labelnames or []))

    def create_histogram(
        self,
        name: str,
        documentation: str,
        labelnames: Optional[Iterable[str]] = None,
        buckets: Optional[tuple[float, ...]] = None,
    ) -> Histogram:
        return Histogram(
            self._full_name(name),
            documentation,
            labelnames=list(labelnames or []),
            buckets=buckets,
        )

    def create_gauge(self, name: str, documentation: str, labelnames: Optional[Iterable[str]] = None) -> Gauge:
        return Gauge(self._full_name(name), documentation, labelnames=list(labelnames or []))


@contextmanager
def track_request_time(metric: Any, *label_values: Any, **label_kwargs: Any):
    start = time.time()
    try:
        yield
    finally:
        duration = time.time() - start
        try:
            if hasattr(metric, "labels"):
                metric.labels(*label_values, **label_kwargs).observe(duration)
            else:
                metric.observe(duration)
        except Exception:
            pass


def track_operation(counter: Any, *label_values: Any, **label_kwargs: Any) -> None:
    try:
        if hasattr(counter, "labels"):
            counter.labels(*label_values, **label_kwargs).inc()
        else:
            counter.inc()
    except Exception:
        pass


def track_error(counter: Any, *label_values: Any, **label_kwargs: Any) -> None:
    track_operation(counter, *label_values, **label_kwargs)


def track_item_processed(counter: Any, count: int = 1, *label_values: Any, **label_kwargs: Any) -> None:
    try:
        if hasattr(counter, "labels"):
            counter.labels(*label_values, **label_kwargs).inc(count)
        else:
            counter.inc(count)
    except Exception:
        pass

```

---

## backend/shared/proxy_client.py

```py
from __future__ import annotations

import asyncio
from dataclasses import dataclass
from typing import Any, Optional

import httpx


@dataclass
class ProxyInfo:
    id: str
    address: str
    protocol: str = "http"
    username: Optional[str] = None
    password: Optional[str] = None
    country: Optional[str] = None
    health_status: Optional[str] = None

    def get_proxy_url(self) -> str:
        if self.username and self.password and "://" in self.address:
            scheme, rest = self.address.split("://", 1)
            return f"{scheme}://{self.username}:{self.password}@{rest}"
        return self.address


class ProxyRotationClient:
    def __init__(
        self,
        base_url: str,
        timeout: float = 5.0,
        enabled: bool = True,
    ) -> None:
        self._base_url = base_url.rstrip("/")
        self._timeout = timeout
        self._enabled = enabled
        self._client: Optional[httpx.AsyncClient] = None
        self._lock = asyncio.Lock()

    async def _get_client(self) -> httpx.AsyncClient:
        async with self._lock:
            if self._client is None:
                self._client = httpx.AsyncClient(timeout=self._timeout)
            return self._client

    async def close(self) -> None:
        async with self._lock:
            if self._client is not None:
                await self._client.aclose()
                self._client = None

    async def health_check(self) -> bool:
        if not self._enabled:
            return False
        client = await self._get_client()
        try:
            resp = await client.get(f"{self._base_url}/health")
            return resp.status_code == 200
        except Exception:
            return False

    async def get_next_proxy(self) -> Optional[ProxyInfo]:
        if not self._enabled:
            return None
        client = await self._get_client()
        try:
            resp = await client.get(f"{self._base_url}/proxy/next")
            if resp.status_code != 200:
                return None
            data: Any = resp.json()
            if not isinstance(data, dict):
                return None

            proxy_id = data.get("proxyId") or data.get("proxy_id") or data.get("id")
            address = data.get("address")
            if not proxy_id or not address:
                return None

            return ProxyInfo(
                id=str(proxy_id),
                address=str(address),
                protocol=str(data.get("protocol") or "http"),
                username=data.get("username"),
                password=data.get("password"),
                country=data.get("country"),
                health_status=data.get("healthStatus") or data.get("health_status"),
            )
        except Exception:
            return None

    async def record_success(self, proxy_id: str, latency_ms: int = 0) -> bool:
        return await self._record(proxy_id=proxy_id, success=True, latency_ms=latency_ms)

    async def record_failure(self, proxy_id: str, reason: str = "") -> bool:
        return await self._record(proxy_id=proxy_id, success=False, reason=reason)

    async def record_captcha(self, proxy_id: str, captcha_type: str = "") -> bool:
        if not self._enabled:
            return False
        client = await self._get_client()
        payload = {
            "proxyId": proxy_id,
            "type": captcha_type,
        }
        try:
            resp = await client.post(f"{self._base_url}/proxy/captcha", json=payload)
            return resp.status_code == 200
        except Exception:
            return False

    async def _record(
        self,
        proxy_id: str,
        success: bool,
        latency_ms: int = 0,
        reason: str = "",
    ) -> bool:
        if not self._enabled:
            return False
        client = await self._get_client()
        payload = {
            "proxyId": proxy_id,
            "success": bool(success),
            "latencyMs": int(latency_ms),
            "reason": reason,
        }
        try:
            resp = await client.post(f"{self._base_url}/proxy/record", json=payload)
            return resp.status_code == 200
        except Exception:
            return False

    async def get_pool_stats(self) -> Optional[dict[str, Any]]:
        if not self._enabled:
            return None
        client = await self._get_client()
        try:
            resp = await client.get(f"{self._base_url}/health")
            if resp.status_code != 200:
                return None
            data: Any = resp.json()
            if isinstance(data, dict):
                stats = data.get("stats")
                return stats if isinstance(stats, dict) else data
            return None
        except Exception:
            return None

```

---

## backend/shared-libs/build.gradle.kts

```kts
// 공통 라이브러리 모듈 빌드 설정 (선택 사항)

plugins {
    `java-library`
}

// 공통 라이브러리는 실행 가능한 JAR가 아니므로 일반 JAR로 패키징
tasks.named<Jar>("jar") {
    enabled = true
}

dependencies {
    // 공통으로 사용할 유틸리티 클래스, 예외, DTO 등
    
    // Consul Config (공통 설정 로더)
    api("org.springframework.cloud:spring-cloud-starter-consul-config")
    
    // Validation
    api("org.springframework.boot:spring-boot-starter-validation")
    
    // JSON Processing
    api("com.fasterxml.jackson.core:jackson-databind")
    api("com.fasterxml.jackson.datatype:jackson-datatype-jsr310")
}

```

---

## backend/shared-libs/src/main/java/com/newsinsight/shared/package-info.java

```java
/**
 * NewsInsight 공통 라이브러리 패키지.
 * 
 * 이 모듈은 모든 서비스에서 공유하는 공통 유틸리티, DTO, 예외 클래스 등을 포함합니다.
 */
package com.newsinsight.shared;

```

---

## build.gradle.kts

```kts
// NewsInsight 루트 빌드 설정

plugins {
    java
    id("org.springframework.boot") version "3.2.1" apply false
    id("io.spring.dependency-management") version "1.1.4" apply false
    kotlin("jvm") version "1.9.21" apply false
    kotlin("plugin.spring") version "1.9.21" apply false
}

allprojects {
    group = "com.newsinsight"
    version = "1.0.0"
    
    repositories {
        mavenCentral()
    }
}

subprojects {
    // backend 디렉터리 자체는 모듈이 아니므로 스킵
    if (path == ":backend") {
        return@subprojects
    }
    
    apply(plugin = "java")
    
    java {
        sourceCompatibility = JavaVersion.VERSION_21
        targetCompatibility = JavaVersion.VERSION_21
    }
    
    // 실행 가능한 모듈에만 Spring Boot 플러그인 적용
    if (name !in listOf("shared-libs")) {
        apply(plugin = "org.springframework.boot")
        apply(plugin = "io.spring.dependency-management")
        
        dependencies {
            // 모든 서비스 공통 의존성
            implementation("org.springframework.boot:spring-boot-starter-actuator")
            implementation("org.springframework.cloud:spring-cloud-starter-consul-config")
            implementation("org.springframework.cloud:spring-cloud-starter-consul-discovery")
            
            // Lombok (선택 사항)
            compileOnly("org.projectlombok:lombok")
            annotationProcessor("org.projectlombok:lombok")
            
            // Logging
            implementation("net.logstash.logback:logstash-logback-encoder:7.4")
            
            // Validation
            implementation("org.springframework.boot:spring-boot-starter-validation")
            
            // Test
            testImplementation("org.springframework.boot:spring-boot-starter-test")
            testRuntimeOnly("org.junit.platform:junit-platform-launcher")
        }
    } else {
        // shared-libs needs dependency management for Spring Boot dependencies
        apply(plugin = "io.spring.dependency-management")
    }
    
    // Spring Cloud 버전 관리
    extra["springCloudVersion"] = "2023.0.0"
    extra["springBootVersion"] = "3.2.1"
    
    configure<io.spring.gradle.dependencymanagement.dsl.DependencyManagementExtension> {
        imports {
            mavenBom("org.springframework.boot:spring-boot-dependencies:${project.extra["springBootVersion"]}")
            mavenBom("org.springframework.cloud:spring-cloud-dependencies:${project.extra["springCloudVersion"]}")
        }
    }
    
    tasks.withType<Test> {
        useJUnitPlatform()
    }
    
    tasks.withType<JavaCompile> {
        options.encoding = "UTF-8"
        options.compilerArgs.add("-parameters")
    }
}

```

---

## collect-code-to-md.js

```js
const fs = require('fs').promises;
const path = require('path');
const { spawn } = require('child_process');

const config = {
  rootDir: process.cwd(),
  // 기본 출력 파일 이름(여러 개로 나눌 경우 접미사가 붙습니다)
  outputFile: 'project-code-collection.md',
  // 포함할 확장자
  includeExtensions: [
    '.js',
    '.jsx',
    '.ts',
    '.tsx',
    '.java',
    '.kt',
    '.kts',
    '.py',
    '.go',
    '.rs',
    '.c',
    '.cpp',
    '.h',
    '.hpp',
    '.html',
    '.css',
    '.scss',
    '.json',
    '.yml',
    '.yaml'
  ],
  // 완전히 제외할 디렉터리명
  excludeDirs: [
    '.git',
    'node_modules',
    '.next',
    'dist',
    'build',
    'out',
    'coverage',
    'target',
    '.idea',
    '.vscode',
    '.gradle',
    '.turbo',
    '.github',
    'docs',
    'builds'
  ],
  // 제외할 파일 확장자 (includeExtensions에 있어도 우선 제외)
  excludeExtensions: [
    '.sh',
    '.jar',
    '.class',
    '.jpg'
  ],
  // 한 파일 최대 크기 (바이너리/초대형 파일 방지)
  maxFileSizeBytes: 2 * 1024 * 1024,
  // 출력할 Markdown 파일 개수 (1이면 단일 파일)
  maxOutputFiles: 5,
  // 첫 번째 출력 파일을 Kwrite로 열지 여부
  openInKwrite: false,
  // 출력 파일들이 있는 디렉터리를 Dolphin으로 열지 여부
  openInDolphin: true,
};

function shouldExcludeDir(name) {
  if (config.excludeDirs.includes(name)) return true;
  // 숨김 디렉터리는 기본 제외(.github 등 예외는 위에서 명시)
  if (name.startsWith('.') && !config.excludeDirs.includes(name)) return true;
  return false;
}

function shouldIncludeFile(name, fullPath, relPath) {
  if (relPath === config.outputFile || name === config.outputFile) return false;
  const ext = path.extname(name).toLowerCase();
  if (config.excludeExtensions.includes(ext)) return false;
  if (!config.includeExtensions.includes(ext)) return false;
  return true;
}

async function walk(dir, result) {
  const entries = await fs.readdir(dir, { withFileTypes: true });
  for (const entry of entries) {
    const fullPath = path.join(dir, entry.name);
    const relPath = path.relative(config.rootDir, fullPath);
    if (entry.isDirectory()) {
      if (shouldExcludeDir(entry.name)) continue;
      await walk(fullPath, result);
    } else if (entry.isFile()) {
      if (!shouldIncludeFile(entry.name, fullPath, relPath)) continue;
      result.push({ fullPath, relPath });
    }
  }
}

async function buildMarkdown(files) {
  const parts = [];
  parts.push('# Project Code Snapshot\n\n');
  parts.push(`Generated at ${new Date().toISOString()}\n`);
  for (const file of files) {
    const stat = await fs.stat(file.fullPath);
    if (stat.size > config.maxFileSizeBytes) continue;
    const content = await fs.readFile(file.fullPath, 'utf8');
    const ext = path.extname(file.fullPath).slice(1);
    const lang = ext || '';
    parts.push('\n---\n\n');
    parts.push(`## ${file.relPath}\n\n`);
    const safeContent = content.replace(/``\`/g, '``\\`');
    parts.push('``\`' + lang + '\n' + safeContent + '\n``\`\n');
  }
  return parts.join('');
}

async function openInKwrite(outPath) {
  if (!config.openInKwrite) return;
  try {
    const kwrite = spawn('kwrite', [outPath], {
      detached: true,
      stdio: 'ignore',
    });
    kwrite.on('error', (err) => {
      console.error('Failed to open Kwrite:', err && err.message ? err.message : err);
    });
    kwrite.unref();
  } catch (err) {
    console.error('Failed to spawn Kwrite:', err && err.message ? err.message : err);
  }
}

async function openInDolphin(dirPath) {
  if (!config.openInDolphin) return;
  try {
    const dolphin = spawn('dolphin', [dirPath], {
      detached: true,
      stdio: 'ignore',
    });
    dolphin.on('error', (err) => {
      console.error('Failed to open Dolphin:', err && err.message ? err.message : err);
    });
    dolphin.unref();
  } catch (err) {
    console.error('Failed to spawn Dolphin:', err && err.message ? err.message : err);
  }
}

function getOutputFileName(baseName, index, total) {
  if (total === 1) return baseName;
  const ext = path.extname(baseName); // .md
  const name = path.basename(baseName, ext); // project-code-collection
  return `${name}-${index + 1}${ext}`;
}

async function writeOutputMarkdowns(allFiles) {
  const totalFiles = allFiles.length;
  if (totalFiles === 0) {
    console.log('No files matched filters. Nothing to write.');
    return [];
  }

  const outputCount = Math.max(1, Math.min(config.maxOutputFiles || 1, totalFiles));
  const filesPerOutput = Math.ceil(totalFiles / outputCount);

  const outputPaths = [];

  for (let i = 0; i < outputCount; i++) {
    const start = i * filesPerOutput;
    if (start >= totalFiles) break;
    const end = Math.min(start + filesPerOutput, totalFiles);
    const slice = allFiles.slice(start, end);
    const markdown = await buildMarkdown(slice);
    const fileName = getOutputFileName(config.outputFile, i, outputCount);
    const outPath = path.join(config.rootDir, fileName);
    await fs.writeFile(outPath, markdown, 'utf8');
    outputPaths.push(outPath);
  }

  return outputPaths;
}

async function main() {
  try {
    const files = [];
    await walk(config.rootDir, files);

    const outputPaths = await writeOutputMarkdowns(files);
    if (outputPaths.length === 0) {
      return;
    }

    console.log(`Collected ${files.length} source files into ${outputPaths.length} markdown file(s):`);
    for (const p of outputPaths) {
      console.log('  - ' + path.relative(config.rootDir, p));
    }

    // 첫 번째 출력 파일을 Kwrite로 열기
    await openInKwrite(outputPaths[0]);
    // 출력 파일들이 있는 디렉터리를 Dolphin으로 열기 (모든 출력 파일이 같은 디렉터리에 있으므로 한 번만 호출)
    await openInDolphin(path.dirname(outputPaths[0]));
  } catch (err) {
    console.error('Failed to collect code to markdown:', err);
    process.exitCode = 1;
  }
}

main();

```

---

## etc/configs/services.json

```json
{
  "$schema": "./services.schema.json",
  "version": "1.1.0",
  "description": "NewsInsight service configuration profiles with Consul service discovery",
  
  "services": {
    "api-gateway": {
      "name": "API Gateway",
      "description": "Spring Boot API Gateway service",
      "port": 8000,
      "healthcheck": "/actuator/health",
      "hostname": "api-gateway",
      "dependencies": ["postgres", "mongo", "redis", "consul", "redpanda"],
      "consul": {
        "register": true,
        "service_name": "api-gateway",
        "tags": ["gateway", "api", "spring-boot"]
      },
      "profiles": {
        "development": {
          "replicas": 1,
          "resources": {
            "memory": "512Mi",
            "cpu": "250m"
          },
          "env": {
            "SPRING_PROFILES_ACTIVE": "development",
            "LOG_LEVEL": "DEBUG",
            "JAVA_OPTS": "-Xms128m -Xmx256m"
          }
        },
        "staging": {
          "replicas": 1,
          "resources": {
            "memory": "1Gi",
            "cpu": "500m"
          },
          "env": {
            "SPRING_PROFILES_ACTIVE": "staging",
            "LOG_LEVEL": "DEBUG",
            "JAVA_OPTS": "-Xms256m -Xmx512m"
          }
        },
        "production": {
          "replicas": 3,
          "resources": {
            "memory": "2Gi",
            "cpu": "1000m"
          },
          "env": {
            "SPRING_PROFILES_ACTIVE": "production",
            "LOG_LEVEL": "INFO",
            "JAVA_OPTS": "-Xms256m -Xmx512m"
          }
        }
      }
    },
    
    "collector-service": {
      "name": "Data Collector Service",
      "description": "News data collection and processing service",
      "port": 8081,
      "healthcheck": "/actuator/health",
      "hostname": "collector-service",
      "dependencies": ["postgres", "mongo", "redis", "consul", "redpanda"],
      "consul": {
        "register": true,
        "service_name": "collector-service",
        "tags": ["collector", "data", "spring-boot"]
      },
      "profiles": {
        "development": {
          "replicas": 1,
          "resources": {
            "memory": "512Mi",
            "cpu": "250m"
          },
          "env": {
            "SPRING_PROFILES_ACTIVE": "development",
            "LOG_LEVEL": "DEBUG",
            "JAVA_OPTS": "-Xms128m -Xmx256m"
          }
        },
        "staging": {
          "replicas": 1,
          "resources": {
            "memory": "1Gi",
            "cpu": "500m"
          },
          "env": {
            "SPRING_PROFILES_ACTIVE": "staging",
            "LOG_LEVEL": "DEBUG",
            "JAVA_OPTS": "-Xms256m -Xmx512m"
          }
        },
        "production": {
          "replicas": 2,
          "resources": {
            "memory": "2Gi",
            "cpu": "1000m"
          },
          "env": {
            "SPRING_PROFILES_ACTIVE": "production",
            "LOG_LEVEL": "INFO",
            "JAVA_OPTS": "-Xms256m -Xmx512m"
          }
        }
      }
    },
    
    "browser-use-api": {
      "name": "Browser-Use API",
      "description": "AI-powered browser automation service",
      "port": 8500,
      "healthcheck": "/health",
      "hostname": "browser-use-api",
      "dependencies": [],
      "consul": {
        "register": true,
        "service_name": "browser-use-api",
        "tags": ["browser", "automation", "python"]
      },
      "profiles": {
        "development": {
          "replicas": 1,
          "resources": {
            "memory": "1Gi",
            "cpu": "500m"
          },
          "env": {
            "PYTHONUNBUFFERED": "1",
            "LOG_LEVEL": "DEBUG"
          }
        },
        "staging": {
          "replicas": 1,
          "resources": {
            "memory": "2Gi",
            "cpu": "1000m"
          },
          "env": {
            "PYTHONUNBUFFERED": "1",
            "LOG_LEVEL": "DEBUG"
          }
        },
        "production": {
          "replicas": 2,
          "resources": {
            "memory": "4Gi",
            "cpu": "2000m"
          },
          "env": {
            "PYTHONUNBUFFERED": "1",
            "LOG_LEVEL": "INFO"
          }
        }
      }
    },
    
    "autonomous-crawler": {
      "name": "Autonomous Crawler",
      "description": "Autonomous web crawling and data extraction service",
      "port": 9090,
      "api_port": 8030,
      "healthcheck": "/health",
      "hostname": "autonomous-crawler",
      "dependencies": ["postgres", "mongo", "redis", "browser-use-api", "web-crawler"],
      "consul": {
        "register": true,
        "service_name": "autonomous-crawler",
        "tags": ["crawler", "autonomous", "python"]
      },
      "profiles": {
        "development": {
          "replicas": 1,
          "resources": {
            "memory": "512Mi",
            "cpu": "250m"
          },
          "env": {
            "LOG_LEVEL": "DEBUG",
            "LLM_PROVIDER": "custom",
            "LLM_OPENAI_MODEL": "gpt-4o",
            "LLM_ANTHROPIC_MODEL": "claude-3-5-sonnet-20241022",
            "LLM_OPENROUTER_MODEL": "anthropic/claude-3.5-sonnet",
            "LLM_OPENROUTER_BASE_URL": "https://openrouter.ai/api/v1",
            "LLM_OLLAMA_MODEL": "llama3.1",
            "LLM_OLLAMA_BASE_URL": "http://localhost:11434",
            "LLM_AZURE_API_VERSION": "2024-02-15-preview",
            "LLM_CUSTOM_BASE_URL": "https://workflow.nodove.com/webhook/aidove",
            "LLM_CUSTOM_MODEL": "aidove",
            "LLM_CUSTOM_REQUEST_FORMAT": "{\"chatInput\": \"{prompt}\"}",
            "LLM_CUSTOM_RESPONSE_PATH": "reply"
          }
        },
        "staging": {
          "replicas": 1,
          "resources": {
            "memory": "1Gi",
            "cpu": "500m"
          },
          "env": {
            "LOG_LEVEL": "DEBUG",
            "LLM_PROVIDER": "custom",
            "LLM_OPENAI_MODEL": "gpt-4o",
            "LLM_ANTHROPIC_MODEL": "claude-3-5-sonnet-20241022",
            "LLM_OPENROUTER_MODEL": "anthropic/claude-3.5-sonnet",
            "LLM_OPENROUTER_BASE_URL": "https://openrouter.ai/api/v1",
            "LLM_OLLAMA_MODEL": "llama3.1",
            "LLM_OLLAMA_BASE_URL": "http://localhost:11434",
            "LLM_AZURE_API_VERSION": "2024-02-15-preview",
            "LLM_CUSTOM_BASE_URL": "https://workflow.nodove.com/webhook/aidove",
            "LLM_CUSTOM_MODEL": "aidove",
            "LLM_CUSTOM_REQUEST_FORMAT": "{\"chatInput\": \"{prompt}\"}",
            "LLM_CUSTOM_RESPONSE_PATH": "reply"
          }
        },
        "production": {
          "replicas": 2,
          "resources": {
            "memory": "2Gi",
            "cpu": "1000m"
          },
          "env": {
            "LOG_LEVEL": "INFO",
            "LLM_PROVIDER": "custom",
            "LLM_OPENAI_MODEL": "gpt-4o",
            "LLM_ANTHROPIC_MODEL": "claude-3-5-sonnet-20241022",
            "LLM_OPENROUTER_MODEL": "anthropic/claude-3.5-sonnet",
            "LLM_OPENROUTER_BASE_URL": "https://openrouter.ai/api/v1",
            "LLM_OLLAMA_MODEL": "llama3.1",
            "LLM_OLLAMA_BASE_URL": "http://localhost:11434",
            "LLM_AZURE_API_VERSION": "2024-02-15-preview",
            "LLM_CUSTOM_BASE_URL": "https://workflow.nodove.com/webhook/aidove",
            "LLM_CUSTOM_MODEL": "aidove",
            "LLM_CUSTOM_REQUEST_FORMAT": "{\"chatInput\": \"{prompt}\"}",
            "LLM_CUSTOM_RESPONSE_PATH": "reply"
          }
        }
      }
    },
    
    "web-crawler": {
      "name": "Web Crawler",
      "description": "Crawl4AI web crawling service",
      "port": 11235,
      "healthcheck": "/health",
      "hostname": "web-crawler",
      "dependencies": [],
      "consul": {
        "register": true,
        "service_name": "web-crawler",
        "tags": ["crawler", "crawl4ai"]
      },
      "profiles": {
        "development": {
          "replicas": 1,
          "resources": {
            "memory": "1Gi",
            "cpu": "500m"
          },
          "env": {}
        },
        "staging": {
          "replicas": 1,
          "resources": {
            "memory": "2Gi",
            "cpu": "1000m"
          },
          "env": {}
        },
        "production": {
          "replicas": 1,
          "resources": {
            "memory": "2Gi",
            "cpu": "1000m"
          },
          "env": {}
        }
      }
    },
    
    "admin-dashboard": {
      "name": "Admin Dashboard",
      "description": "Administration dashboard API",
      "port": 8888,
      "healthcheck": "/health",
      "hostname": "admin-dashboard",
      "dependencies": ["postgres", "mongo"],
      "consul": {
        "register": true,
        "service_name": "admin-dashboard",
        "tags": ["admin", "dashboard", "python"]
      },
      "profiles": {
        "development": {
          "replicas": 1,
          "resources": {
            "memory": "256Mi",
            "cpu": "100m"
          },
          "env": {}
        },
        "staging": {
          "replicas": 1,
          "resources": {
            "memory": "512Mi",
            "cpu": "250m"
          },
          "env": {}
        },
        "production": {
          "replicas": 1,
          "resources": {
            "memory": "512Mi",
            "cpu": "250m"
          },
          "env": {}
        }
      }
    },
    
    "frontend": {
      "name": "Frontend",
      "description": "React/Vite frontend application",
      "port": 8080,
      "healthcheck": "/health",
      "hostname": "frontend",
      "dependencies": ["api-gateway"],
      "consul": {
        "register": false,
        "service_name": "frontend",
        "tags": ["frontend", "react", "nginx"]
      },
      "profiles": {
        "development": {
          "replicas": 1,
          "resources": {
            "memory": "128Mi",
            "cpu": "100m"
          },
          "env": {}
        },
        "staging": {
          "replicas": 1,
          "resources": {
            "memory": "256Mi",
            "cpu": "200m"
          },
          "env": {}
        },
        "production": {
          "replicas": 3,
          "resources": {
            "memory": "512Mi",
            "cpu": "500m"
          },
          "env": {}
        }
      }
    }
  },
  
  "service_urls": {
    "_comment": "Direct service URLs for API Gateway routing (used when Consul service discovery is unavailable)",
    "collector-service": "http://collector-service:8081",
    "browser-use-api": "http://browser-use-api:8500",
    "autonomous-crawler": "http://autonomous-crawler:8030",
    "web-crawler": "http://web-crawler:11235",
    "admin-dashboard": "http://admin-dashboard:8888",
    "sentiment-addon": "http://sentiment-addon:8100",
    "factcheck-addon": "http://factcheck-addon:8101",
    "bias-addon": "http://bias-addon:8102",
    "bot-detector": "http://bot-detector:8041",
    "ml-trainer": "http://ml-trainer:8090",
    "newsinsight-mcp": "http://newsinsight-mcp:5000",
    "bias-mcp": "http://bias-mcp:5001",
    "factcheck-mcp": "http://factcheck-mcp:5002",
    "topic-mcp": "http://topic-mcp:5003",
    "aiagent-mcp": "http://aiagent-mcp:5004",
    "huggingface-mcp": "http://huggingface-mcp:5011",
    "kaggle-mcp": "http://kaggle-mcp:5012",
    "mltraining-mcp": "http://mltraining-mcp:5013",
    "roboflow-mcp": "http://roboflow-mcp:5014",
    "ip-rotation": "http://ip-rotation:8050",
    "crawl-worker": "http://crawl-worker:8040",
    "maigret-worker": "http://maigret-worker:8020"
  },
  
  "ml-addons": {
    "sentiment-addon": {
      "name": "Sentiment Analysis Addon",
      "port": 8100,
      "healthcheck": "/health",
      "enabled": {
        "development": false,
        "staging": false,
        "production": true
      }
    },
    "factcheck-addon": {
      "name": "Fact Check Addon",
      "port": 8101,
      "healthcheck": "/health",
      "enabled": {
        "development": false,
        "staging": false,
        "production": true
      }
    },
    "bias-addon": {
      "name": "Bias Detection Addon",
      "port": 8102,
      "healthcheck": "/health",
      "enabled": {
        "development": false,
        "staging": false,
        "production": true
      }
    }
  },
  
  "infrastructure": {
    "postgres": {
      "image": "postgres:15-alpine",
      "port": 5432,
      "healthcheck": "pg_isready"
    },
    "mongo": {
      "image": "mongo:7",
      "port": 27017,
      "healthcheck": "mongosh --eval db.adminCommand('ping')"
    },
    "redis": {
      "image": "redis:7-alpine",
      "port": 6379,
      "healthcheck": "redis-cli ping"
    },
    "consul": {
      "image": "hashicorp/consul:1.18",
      "port": 8500,
      "healthcheck": "consul members"
    },
    "redpanda": {
      "image": "docker.redpanda.com/redpandadata/redpanda:latest",
      "port": 9092,
      "healthcheck": "rpk cluster health"
    }
  }
}

```

---

## etc/configs/services.schema.json

```json
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://newsinsight.nodove.com/schemas/services.json",
  "title": "NewsInsight Services Configuration",
  "description": "Schema for validating NewsInsight service configuration profiles",
  "type": "object",
  "required": ["version", "services"],
  "properties": {
    "$schema": {
      "type": "string",
      "description": "Reference to the JSON schema file"
    },
    "version": {
      "type": "string",
      "pattern": "^\\d+\\.\\d+\\.\\d+$",
      "description": "Semantic version of the configuration"
    },
    "description": {
      "type": "string",
      "description": "Human-readable description of this configuration"
    },
    "services": {
      "type": "object",
      "description": "Service definitions",
      "additionalProperties": {
        "$ref": "#/$defs/serviceDefinition"
      }
    },
    "ml-addons": {
      "type": "object",
      "description": "ML addon service definitions",
      "additionalProperties": {
        "$ref": "#/$defs/mlAddonDefinition"
      }
    },
    "infrastructure": {
      "type": "object",
      "description": "Infrastructure service definitions",
      "additionalProperties": {
        "$ref": "#/$defs/infrastructureDefinition"
      }
    }
  },
  "$defs": {
    "port": {
      "type": "integer",
      "minimum": 1,
      "maximum": 65535,
      "description": "Network port number"
    },
    "resourceValue": {
      "type": "string",
      "pattern": "^\\d+(Mi|Gi|m)?$",
      "description": "Kubernetes resource value (e.g., 256Mi, 1Gi, 500m)"
    },
    "environmentName": {
      "type": "string",
      "enum": ["development", "staging", "production"],
      "description": "Environment name"
    },
    "resources": {
      "type": "object",
      "properties": {
        "memory": {
          "$ref": "#/$defs/resourceValue",
          "description": "Memory allocation (e.g., 512Mi, 2Gi)"
        },
        "cpu": {
          "$ref": "#/$defs/resourceValue",
          "description": "CPU allocation (e.g., 250m, 1000m)"
        }
      },
      "additionalProperties": false
    },
    "environmentProfile": {
      "type": "object",
      "required": ["replicas", "resources"],
      "properties": {
        "replicas": {
          "type": "integer",
          "minimum": 1,
          "maximum": 100,
          "description": "Number of replicas for this environment"
        },
        "resources": {
          "$ref": "#/$defs/resources"
        },
        "env": {
          "type": "object",
          "description": "Environment-specific variables",
          "additionalProperties": {
            "type": "string"
          }
        }
      },
      "additionalProperties": false
    },
    "serviceDefinition": {
      "type": "object",
      "required": ["name", "port", "healthcheck", "profiles"],
      "properties": {
        "name": {
          "type": "string",
          "description": "Human-readable service name"
        },
        "description": {
          "type": "string",
          "description": "Service description"
        },
        "port": {
          "$ref": "#/$defs/port"
        },
        "healthcheck": {
          "type": "string",
          "description": "Health check endpoint path"
        },
        "dependencies": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "List of service dependencies"
        },
        "profiles": {
          "type": "object",
          "required": ["development", "staging", "production"],
          "properties": {
            "development": {
              "$ref": "#/$defs/environmentProfile"
            },
            "staging": {
              "$ref": "#/$defs/environmentProfile"
            },
            "production": {
              "$ref": "#/$defs/environmentProfile"
            }
          },
          "additionalProperties": false
        }
      },
      "additionalProperties": false
    },
    "mlAddonDefinition": {
      "type": "object",
      "required": ["name", "port", "healthcheck", "enabled"],
      "properties": {
        "name": {
          "type": "string",
          "description": "Human-readable addon name"
        },
        "port": {
          "$ref": "#/$defs/port"
        },
        "healthcheck": {
          "type": "string",
          "description": "Health check endpoint path"
        },
        "enabled": {
          "type": "object",
          "required": ["development", "staging", "production"],
          "properties": {
            "development": {
              "type": "boolean"
            },
            "staging": {
              "type": "boolean"
            },
            "production": {
              "type": "boolean"
            }
          },
          "additionalProperties": false
        }
      },
      "additionalProperties": false
    },
    "infrastructureDefinition": {
      "type": "object",
      "required": ["image", "port"],
      "properties": {
        "image": {
          "type": "string",
          "description": "Docker image reference"
        },
        "port": {
          "$ref": "#/$defs/port"
        },
        "healthcheck": {
          "type": "string",
          "description": "Health check command"
        }
      },
      "additionalProperties": false
    }
  }
}

```

---

## etc/docker/cloudflared-config.yml

```yml
tunnel: ed317942-3e87-4b0e-a3c2-3df106d4c0f4
ingress:
  - hostname: news.nodove.com
    path: "/api/.*"
    service: http://172.18.0.10:8000
    originRequest:
      connectTimeout: 30s
      noTLSVerify: true
  - hostname: news.nodove.com
    service: http://172.18.0.12:8080
    originRequest:
      connectTimeout: 30s
      noTLSVerify: true
  - service: http_status:404

```
