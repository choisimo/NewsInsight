# Project Code Snapshot

Generated at 2025-12-21T10:30:23.544Z

---

## backend/browser-use/examples/use-cases/phone_comparison.py

```py
import asyncio

from pydantic import BaseModel, Field

from browser_use import Agent, Browser, ChatBrowserUse


class ProductListing(BaseModel):
	"""A single product listing"""

	title: str = Field(..., description='Product title')
	url: str = Field(..., description='Full URL to listing')
	price: float = Field(..., description='Price as number')
	condition: str | None = Field(None, description='Condition: Used, New, Refurbished, etc')
	source: str = Field(..., description='Source website: Amazon, eBay, or Swappa')


class PriceComparison(BaseModel):
	"""Price comparison results"""

	search_query: str = Field(..., description='The search query used')
	listings: list[ProductListing] = Field(default_factory=list, description='All product listings')


async def find(item: str = 'Used iPhone 12'):
	"""
	Search for an item across multiple marketplaces and compare prices.

	Args:
	    item: The item to search for (e.g., "Used iPhone 12")

	Returns:
	    PriceComparison object with structured results
	"""
	browser = Browser(cdp_url='http://localhost:9222')

	llm = ChatBrowserUse()

	# Task prompt
	task = f"""
    Search for "{item}" on eBay, Amazon, and Swappa. Get any 2-3 listings from each site.

    For each site:
    1. Search for "{item}"
    2. Extract ANY 2-3 listings you find (sponsored, renewed, used - all are fine)
    3. Get: title, price (number only, if range use lower number), source, full URL, condition
    4. Move to next site

    Sites:
    - eBay: https://www.ebay.com/
    - Amazon: https://www.amazon.com/
    - Swappa: https://swappa.com/
    """

	# Create agent with structured output
	agent = Agent(
		browser=browser,
		llm=llm,
		task=task,
		output_model_schema=PriceComparison,
	)

	# Run the agent
	result = await agent.run()
	return result


if __name__ == '__main__':
	# Get user input
	query = input('What item would you like to compare prices for? ').strip()
	if not query:
		query = 'Used iPhone 12'
		print(f'Using default query: {query}')

	result = asyncio.run(find(query))

	# Access structured output
	if result and result.structured_output:
		comparison = result.structured_output

		print(f'\n{"=" * 60}')
		print(f'Price Comparison Results: {comparison.search_query}')
		print(f'{"=" * 60}\n')

		for listing in comparison.listings:
			print(f'Title: {listing.title}')
			print(f'Price: ${listing.price}')
			print(f'Source: {listing.source}')
			print(f'URL: {listing.url}')
			print(f'Condition: {listing.condition or "N/A"}')
			print(f'{"-" * 60}')

```

---

## backend/browser-use/examples/use-cases/shopping.py

```py
import asyncio
import os
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from dotenv import load_dotenv

load_dotenv()

from browser_use import Agent, ChatOpenAI

task = """
   ### Prompt for Shopping Agent – Migros Online Grocery Order

**Objective:**
Visit [Migros Online](https://www.migros.ch/en), search for the required grocery items, add them to the cart, select an appropriate delivery window, and complete the checkout process using TWINT.

**Important:**
- Make sure that you don't buy more than it's needed for each article.
- After your search, if you click  the "+" button, it adds the item to the basket.
- if you open the basket sidewindow menu, you can close it by clicking the X button on the top right. This will help you navigate easier.
---

### Step 1: Navigate to the Website
- Open [Migros Online](https://www.migros.ch/en).
- You should be logged in as Nikolaos Kaliorakis

---

### Step 2: Add Items to the Basket

#### Shopping List:

**Meat & Dairy:**
- Beef Minced meat (1 kg)
- Gruyère cheese (grated preferably)
- 2 liters full-fat milk
- Butter (cheapest available)

**Vegetables:**
- Carrots (1kg pack)
- Celery
- Leeks (1 piece)
- 1 kg potatoes

At this stage, check the basket on the top right (indicates the price) and check if you bought the right items.

**Fruits:**
- 2 lemons
- Oranges (for snacking)

**Pantry Items:**
- Lasagna sheets
- Tahini
- Tomato paste (below CHF2)
- Black pepper refill (not with the mill)
- 2x 1L Oatly Barista(oat milk)
- 1 pack of eggs (10 egg package)

#### Ingredients I already have (DO NOT purchase):
- Olive oil, garlic, canned tomatoes, dried oregano, bay leaves, salt, chili flakes, flour, nutmeg, cumin.

---

### Step 3: Handling Unavailable Items
- If an item is **out of stock**, find the best alternative.
- Use the following recipe contexts to choose substitutions:
  - **Pasta Bolognese & Lasagna:** Minced meat, tomato paste, lasagna sheets, milk (for béchamel), Gruyère cheese.
  - **Hummus:** Tahini, chickpeas, lemon juice, olive oil.
  - **Chickpea Curry Soup:** Chickpeas, leeks, curry, lemons.
  - **Crispy Slow-Cooked Pork Belly with Vegetables:** Potatoes, butter.
- Example substitutions:
  - If Gruyère cheese is unavailable, select another semi-hard cheese.
  - If Tahini is unavailable, a sesame-based alternative may work.

---

### Step 4: Adjusting for Minimum Order Requirement
- If the total order **is below CHF 99**, add **a liquid soap refill** to reach the minimum. If it;s still you can buy some bread, dark chockolate.
- At this step, check if you have bought MORE items than needed. If the price is more then CHF200, you MUST remove items.
- If an item is not available, choose an alternative.
- if an age verification is needed, remove alcoholic products, we haven't verified yet.

---

### Step 5: Select Delivery Window
- Choose a **delivery window within the current week**. It's ok to pay up to CHF2 for the window selection.
- Preferably select a slot within the workweek.

---

### Step 6: Checkout
- Proceed to checkout.
- Select **TWINT** as the payment method.
- Check out.
- 
- if it's needed the username is: nikoskalio.dev@gmail.com 
- and the password is : TheCircuit.Migros.dev!
---

### Step 7: Confirm Order & Output Summary
- Once the order is placed, output a summary including:
  - **Final list of items purchased** (including any substitutions).
  - **Total cost**.
  - **Chosen delivery time**.

**Important:** Ensure efficiency and accuracy throughout the process."""


agent = Agent(task=task, llm=ChatOpenAI(model='gpt-4.1-mini'))


async def main():
	await agent.run()
	input('Press Enter to close the browser...')


if __name__ == '__main__':
	asyncio.run(main())

```

---

## backend/browser-use/intent_analyzer.py

```py
"""
Intent Analyzer for Browser AI Agent
Implements keyword extraction, context analysis, query expansion, and RRF
to guarantee search results even for complex mixed-keyword queries.
"""

import asyncio
import logging
import re
from dataclasses import dataclass, field
from typing import Optional, Any
from enum import Enum

logger = logging.getLogger(__name__)


class SearchStrategy(str, Enum):
	"""Different search strategies for fallback"""

	FULL_QUERY = 'full_query'  # Original full query
	KEYWORDS_AND = 'keywords_and'  # All keywords with AND logic
	KEYWORDS_OR = 'keywords_or'  # Keywords with OR logic
	PRIMARY_KEYWORD = 'primary_keyword'  # Most important keyword only
	SEMANTIC_VARIANT = 'semantic_variant'  # Semantically similar query
	RELATED_TOPIC = 'related_topic'  # Related topic search
	PARTIAL_MATCH = 'partial_match'  # Partial keyword matching


@dataclass
class AnalyzedIntent:
	"""Result of intent analysis"""

	original_query: str
	keywords: list[str]
	primary_keyword: str
	context: str
	intent_type: str  # search, navigate, extract, compare, etc.
	expanded_queries: list[str]
	fallback_strategies: list[dict[str, Any]]
	semantic_variants: list[str]
	related_topics: list[str]
	language: str  # ko, en, etc.
	confidence: float


@dataclass
class SearchResult:
	"""Individual search result"""

	content: str
	source: str
	relevance_score: float
	strategy_used: SearchStrategy
	rank: int


@dataclass
class FusedResults:
	"""Results after RRF fusion"""

	results: list[SearchResult]
	total_found: int
	strategies_used: list[SearchStrategy]
	best_strategy: SearchStrategy
	has_results: bool


class IntentAnalyzer:
	"""
	Analyzes user intent from mixed keyword sentences and generates
	multiple search strategies to guarantee results.
	"""

	# Task instruction patterns to filter out (not search topics)
	TASK_INSTRUCTION_PATTERNS_KO = [
		r'검색\s*(해|하|해주|하세요|해봐|해라)',
		r'찾아\s*(줘|주세요|봐|라)',
		r'알려\s*(줘|주세요)',
		r'보여\s*(줘|주세요)',
		r'확인\s*(해|하|해주|해봐)',
		r'분석\s*(해|하|해주|해봐)',
		r'수집\s*(해|하|해주|해봐)',
		r'가져\s*(와|다줘|와라)',
		r'(먼저|그리고|다음에|그런\s*다음)',
		r'(에\s*대해|에\s*관해|에\s*관한|에\s*대한)',
		r'(정보를?|데이터를?|내용을?|결과를?)',
		r'(최신|최근|오늘|어제|이번\s*주)',
		r'(관련된?|연관된?)',
		r'(요약|정리|목록)',
	]

	TASK_INSTRUCTION_PATTERNS_EN = [
		r'(search|find|look\s*for|get|fetch|retrieve)\s+(for|about|the)?',
		r'(show|tell|give)\s+me',
		r'(please|kindly|could\s+you)',
		r'(first|then|next|after\s+that|finally)',
		r'(about|regarding|related\s+to|concerning)',
		r'(information|data|content|results?)\s+(on|about|for)',
		r'(latest|recent|current|today|this\s+week)',
		r'(analyze|summarize|collect|gather)',
	]

	# Common Korean stopwords
	KOREAN_STOPWORDS = {
		'은',
		'는',
		'이',
		'가',
		'을',
		'를',
		'의',
		'에',
		'에서',
		'로',
		'으로',
		'와',
		'과',
		'도',
		'만',
		'부터',
		'까지',
		'에게',
		'한테',
		'께',
		'이다',
		'하다',
		'있다',
		'없다',
		'되다',
		'않다',
		'그',
		'저',
		'이것',
		'그것',
		'저것',
		'여기',
		'거기',
		'저기',
		'뭐',
		'어디',
		'언제',
		'어떻게',
		'왜',
		'누구',
		'아주',
		'매우',
		'정말',
		'너무',
		'조금',
		'약간',
		'그리고',
		'그러나',
		'하지만',
		'그래서',
		'때문에',
		'것',
		'수',
		'등',
		'들',
		'및',
		'더',
		'덜',
	}

	# Common English stopwords
	ENGLISH_STOPWORDS = {
		'the',
		'a',
		'an',
		'and',
		'or',
		'but',
		'in',
		'on',
		'at',
		'to',
		'for',
		'of',
		'with',
		'by',
		'from',
		'is',
		'are',
		'was',
		'were',
		'be',
		'been',
		'being',
		'have',
		'has',
		'had',
		'do',
		'does',
		'did',
		'will',
		'would',
		'could',
		'should',
		'may',
		'might',
		'must',
		'shall',
		'can',
		'this',
		'that',
		'these',
		'those',
		'it',
		'its',
		'i',
		'you',
		'he',
		'she',
		'we',
		'they',
		'me',
		'him',
		'her',
		'us',
		'them',
		'what',
		'which',
		'who',
		'whom',
		'where',
		'when',
		'why',
		'how',
		'all',
		'each',
		'every',
		'both',
		'few',
		'more',
		'most',
		'other',
		'some',
		'such',
		'no',
		'nor',
		'not',
		'only',
		'own',
		'same',
		'so',
		'than',
		'too',
		'very',
		'just',
		'also',
		'now',
		'here',
		'there',
		'then',
	}

	# Intent type patterns
	INTENT_PATTERNS = {
		'search': ['찾', '검색', '알려', '뭐', '어디', 'find', 'search', 'look for', 'what', 'where'],
		'compare': ['비교', '차이', '어떤 것이', 'compare', 'difference', 'versus', 'vs'],
		'extract': ['추출', '가져', '수집', 'extract', 'get', 'collect', 'scrape'],
		'navigate': ['이동', '가', '열', 'go to', 'navigate', 'open', 'visit'],
		'analyze': ['분석', '평가', '리뷰', 'analyze', 'review', 'evaluate'],
		'summarize': ['요약', '정리', 'summarize', 'summary', 'overview'],
	}

	def __init__(self, llm: Optional[Any] = None):
		"""
		Initialize the Intent Analyzer.

		Args:
		    llm: Optional LLM instance for semantic analysis
		"""
		self.llm = llm
		self._cache: dict[str, AnalyzedIntent] = {}

	def detect_language(self, text: str) -> str:
		"""Detect the primary language of the text."""
		# Count Korean characters
		korean_chars = len(re.findall(r'[가-힣]', text))
		# Count ASCII letters
		english_chars = len(re.findall(r'[a-zA-Z]', text))

		total = korean_chars + english_chars
		if total == 0:
			return 'unknown'

		if korean_chars / total > 0.3:
			return 'ko'
		return 'en'

	def extract_search_topic(self, task_description: str) -> str:
		"""
		Extract the meaningful search topic from a task description.
		Filters out task instructions, commands, and filler words to get
		only the core search topic.

		Args:
		    task_description: Full task description that may contain instructions

		Returns:
		    Extracted search topic suitable for search query analysis
		"""
		if not task_description or not task_description.strip():
			return task_description

		language = self.detect_language(task_description)
		cleaned = task_description.strip()

		# Apply instruction pattern filters based on language
		if language == 'ko':
			patterns = self.TASK_INSTRUCTION_PATTERNS_KO
		else:
			patterns = self.TASK_INSTRUCTION_PATTERNS_EN

		# Remove instruction patterns
		for pattern in patterns:
			cleaned = re.sub(pattern, ' ', cleaned, flags=re.IGNORECASE)

		# Remove extra whitespace
		cleaned = re.sub(r'\s+', ' ', cleaned).strip()

		# Extract quoted content as high-priority topics
		quoted_matches = re.findall(r'"([^"]+)"|\'([^\']+)\'|「([^」]+)」|『([^』]+)』', task_description)
		quoted_topics = []
		for match in quoted_matches:
			topic = next((m for m in match if m), None)
			if topic and topic.strip():
				quoted_topics.append(topic.strip())

		# If we have quoted topics, prioritize them
		if quoted_topics:
			# Combine quoted topics with remaining cleaned content
			remaining_keywords = self._extract_meaningful_tokens(cleaned, language)
			if remaining_keywords:
				# Check if quoted topics overlap with remaining keywords
				combined = quoted_topics.copy()
				for kw in remaining_keywords[:3]:
					if not any(kw.lower() in qt.lower() for qt in quoted_topics):
						combined.append(kw)
				return ' '.join(combined[:5])  # Limit to 5 tokens
			return ' '.join(quoted_topics)

		# Extract meaningful tokens from cleaned text
		meaningful_tokens = self._extract_meaningful_tokens(cleaned, language)

		if meaningful_tokens:
			return ' '.join(meaningful_tokens[:5])  # Limit to 5 most important tokens

		# Fallback: return original if nothing extracted
		return task_description.strip()

	def _extract_meaningful_tokens(self, text: str, language: str) -> list[str]:
		"""
		Extract meaningful tokens from text, filtering stopwords and short tokens.

		Args:
		    text: Input text
		    language: Language code ('ko' or 'en')

		Returns:
		    List of meaningful tokens
		"""
		if not text:
			return []

		# Split by common delimiters
		tokens = re.split(r'[\s,;.!?()[\]{}"\']', text)
		tokens = [t.strip() for t in tokens if t.strip()]

		stopwords = self.KOREAN_STOPWORDS if language == 'ko' else self.ENGLISH_STOPWORDS

		meaningful = []
		for token in tokens:
			# Skip stopwords
			if token.lower() in stopwords:
				continue

			# Skip very short tokens (unless Korean which can have meaningful 2-char words)
			if language != 'ko' and len(token) < 3:
				continue
			if language == 'ko' and len(token) < 2:
				continue

			# Skip pure numbers
			if token.isdigit():
				continue

			# Skip common task-related words
			task_words_ko = {'검색', '찾기', '확인', '분석', '수집', '가져오기', '보기', '알아보기'}
			task_words_en = {'search', 'find', 'check', 'analyze', 'collect', 'get', 'show', 'look'}

			if language == 'ko' and token in task_words_ko:
				continue
			if language == 'en' and token.lower() in task_words_en:
				continue

			meaningful.append(token)

		# Score and sort by importance
		scored_tokens = []
		for token in meaningful:
			score = 0.0

			# Longer tokens are often more specific/meaningful
			score += min(len(token) / 10, 1.0) * 0.3

			# Capitalized words (proper nouns) are important
			if token[0].isupper():
				score += 0.3

			# Korean compound nouns are valuable
			if language == 'ko' and re.search(r'[가-힣]+(기업|회사|뉴스|서비스|시스템|산업|기술|정책|사건|인물)', token):
				score += 0.4

			# English compound words or technical terms
			if language == 'en' and (token.isupper() or '-' in token or '_' in token):
				score += 0.3

			# Words with numbers might be specific identifiers
			if any(c.isdigit() for c in token):
				score += 0.2

			scored_tokens.append((token, score))

		# Sort by score descending and return tokens only
		scored_tokens.sort(key=lambda x: x[1], reverse=True)
		return [t[0] for t in scored_tokens]

	def extract_keywords(self, text: str, language: str = 'auto') -> list[str]:
		"""
		Extract meaningful keywords from the text.

		Args:
		    text: Input text to extract keywords from
		    language: Language code ('ko', 'en', 'auto')

		Returns:
		    List of extracted keywords
		"""
		if language == 'auto':
			language = self.detect_language(text)

		keywords = []

		# Split by common delimiters
		tokens = re.split(r'[\s,;.!?()[\]{}"\']', text)
		tokens = [t.strip() for t in tokens if t.strip()]

		stopwords = self.KOREAN_STOPWORDS if language == 'ko' else self.ENGLISH_STOPWORDS

		for token in tokens:
			# Skip stopwords
			if token.lower() in stopwords:
				continue

			# Skip very short tokens (unless Korean)
			if language != 'ko' and len(token) < 2:
				continue

			# Skip pure numbers
			if token.isdigit():
				continue

			keywords.append(token)

		# Also extract quoted phrases as keywords
		quoted_phrases = re.findall(r'"([^"]+)"|\'([^\']+)\'', text)
		for match in quoted_phrases:
			phrase = match[0] or match[1]
			if phrase and phrase not in keywords:
				keywords.append(phrase)

		# Extract potential compound terms (noun phrases)
		if language == 'ko':
			# Korean compound nouns often end with specific suffixes
			compounds = re.findall(r'[가-힣]+(?:기업|회사|뉴스|정보|서비스|시스템|데이터|분석|결과)', text)
			for compound in compounds:
				if compound not in keywords:
					keywords.append(compound)

		return keywords

	def identify_primary_keyword(self, keywords: list[str], original_query: str) -> str:
		"""Identify the most important keyword from the list."""
		if not keywords:
			return original_query.split()[0] if original_query.split() else original_query

		# Score keywords by various factors
		scores: dict[str, float] = {}

		for keyword in keywords:
			score = 0.0

			# Longer keywords tend to be more specific
			score += min(len(keyword) / 10, 1.0) * 0.3

			# Keywords at the beginning of the query are often more important
			pos = original_query.find(keyword)
			if pos >= 0:
				score += (1.0 - pos / len(original_query)) * 0.3

			# Capitalized words (in English) or proper nouns
			if keyword[0].isupper():
				score += 0.2

			# Keywords with numbers might be specific identifiers
			if any(c.isdigit() for c in keyword):
				score += 0.1

			# Korean noun endings
			if re.search(r'[가-힣]+(기업|회사|뉴스|서비스|시스템)$', keyword):
				score += 0.3

			scores[keyword] = score

		# Return the keyword with the highest score
		return max(scores.keys(), key=lambda k: scores[k])

	def detect_intent_type(self, text: str) -> str:
		"""Detect the type of intent from the query."""
		text_lower = text.lower()

		for intent_type, patterns in self.INTENT_PATTERNS.items():
			for pattern in patterns:
				if pattern in text_lower:
					return intent_type

		return 'search'  # Default to search

	def generate_query_variants(self, keywords: list[str], primary_keyword: str, original_query: str, language: str) -> list[str]:
		"""Generate multiple query variants for better search coverage."""
		variants = []

		# 1. Original query
		variants.append(original_query)

		# 2. Keywords joined with spaces
		if len(keywords) > 1:
			variants.append(' '.join(keywords))

		# 3. Primary keyword only
		variants.append(primary_keyword)

		# 4. Top 2-3 keywords
		if len(keywords) >= 2:
			variants.append(' '.join(keywords[:2]))
		if len(keywords) >= 3:
			variants.append(' '.join(keywords[:3]))

		# 5. Keyword combinations (for AND-like searches)
		if len(keywords) >= 2:
			# Try different combinations
			for i in range(len(keywords)):
				for j in range(i + 1, min(i + 3, len(keywords))):
					combo = f'{keywords[i]} {keywords[j]}'
					if combo not in variants:
						variants.append(combo)

		# 6. Add language-specific variants
		if language == 'ko':
			# Add common Korean search suffixes
			for keyword in keywords[:3]:
				variants.append(f'{keyword} 정보')
				variants.append(f'{keyword} 뉴스')
				variants.append(f'{keyword} 관련')
		else:
			# Add common English search patterns
			for keyword in keywords[:3]:
				variants.append(f'{keyword} information')
				variants.append(f'{keyword} news')
				variants.append(f'about {keyword}')

		# Remove duplicates while preserving order
		seen = set()
		unique_variants = []
		for v in variants:
			v_lower = v.lower().strip()
			if v_lower and v_lower not in seen:
				seen.add(v_lower)
				unique_variants.append(v)

		return unique_variants

	def generate_fallback_strategies(self, analyzed: 'AnalyzedIntent') -> list[dict[str, Any]]:
		"""Generate a prioritized list of fallback search strategies."""
		strategies = []

		# Strategy 1: Full original query
		strategies.append(
			{
				'strategy': SearchStrategy.FULL_QUERY,
				'query': analyzed.original_query,
				'priority': 1,
				'description': 'Search with the complete original query',
			}
		)

		# Strategy 2: All keywords (AND logic)
		if len(analyzed.keywords) > 1:
			strategies.append(
				{
					'strategy': SearchStrategy.KEYWORDS_AND,
					'query': ' '.join(analyzed.keywords),
					'priority': 2,
					'description': 'Search with all extracted keywords',
				}
			)

		# Strategy 3: Primary keyword only
		strategies.append(
			{
				'strategy': SearchStrategy.PRIMARY_KEYWORD,
				'query': analyzed.primary_keyword,
				'priority': 3,
				'description': 'Search with the most important keyword only',
			}
		)

		# Strategy 4: Semantic variants
		for i, variant in enumerate(analyzed.semantic_variants[:3]):
			strategies.append(
				{
					'strategy': SearchStrategy.SEMANTIC_VARIANT,
					'query': variant,
					'priority': 4 + i,
					'description': f'Search with semantic variant: {variant}',
				}
			)

		# Strategy 5: Related topics
		for i, topic in enumerate(analyzed.related_topics[:2]):
			strategies.append(
				{
					'strategy': SearchStrategy.RELATED_TOPIC,
					'query': topic,
					'priority': 7 + i,
					'description': f'Search for related topic: {topic}',
				}
			)

		# Strategy 6: Keywords OR logic (broader search)
		if len(analyzed.keywords) > 1:
			strategies.append(
				{
					'strategy': SearchStrategy.KEYWORDS_OR,
					'query': ' OR '.join(analyzed.keywords[:5]),
					'priority': 9,
					'description': 'Broader search with keywords in OR logic',
				}
			)

		# Strategy 7: Partial matching (first 2 keywords)
		if len(analyzed.keywords) >= 2:
			strategies.append(
				{
					'strategy': SearchStrategy.PARTIAL_MATCH,
					'query': ' '.join(analyzed.keywords[:2]),
					'priority': 10,
					'description': 'Partial match with top keywords',
				}
			)

		return sorted(strategies, key=lambda x: x['priority'])

	async def analyze_with_llm(self, query: str, language: str) -> dict[str, Any]:
		"""Use LLM to perform deeper semantic analysis."""
		if not self.llm:
			return {'semantic_variants': [], 'related_topics': [], 'context': '', 'confidence': 0.5}

		try:
			# Build prompt for LLM analysis
			if language == 'ko':
				prompt = f"""다음 검색 쿼리를 분석하세요:
쿼리: "{query}"

다음을 JSON 형식으로 제공하세요:
1. semantic_variants: 의미가 유사한 다른 검색어 3개 (리스트)
2. related_topics: 관련 주제 2개 (리스트)
3. context: 사용자가 찾고자 하는 것에 대한 간단한 설명
4. confidence: 분석 신뢰도 (0.0-1.0)

JSON만 출력하세요."""
			else:
				prompt = f"""Analyze this search query:
Query: "{query}"

Provide the following in JSON format:
1. semantic_variants: 3 similar search queries (list)
2. related_topics: 2 related topics (list)  
3. context: Brief description of what the user is looking for
4. confidence: Analysis confidence (0.0-1.0)

Output only JSON."""

			# Call LLM (assuming it has an async method)
			# This is a simplified version - actual implementation depends on your LLM interface
			response = await self._call_llm(prompt)

			# Parse JSON response
			import json

			try:
				result = json.loads(response)
				return result
			except json.JSONDecodeError:
				# Try to extract JSON from response
				json_match = re.search(r'\{[^{}]*\}', response, re.DOTALL)
				if json_match:
					return json.loads(json_match.group())
				return {
					'semantic_variants': [],
					'related_topics': [],
					'context': response[:200] if response else '',
					'confidence': 0.3,
				}

		except Exception as e:
			logger.warning(f'LLM analysis failed: {e}')
			return {'semantic_variants': [], 'related_topics': [], 'context': '', 'confidence': 0.3}

	async def _call_llm(self, prompt: str) -> str:
		"""Call the LLM with the given prompt."""
		if not self.llm:
			return ''

		try:
			# Try different LLM interfaces
			if hasattr(self.llm, 'ainvoke'):
				from browser_use.llm.messages import UserMessage

				response = await self.llm.ainvoke([UserMessage(content=prompt)])
				if hasattr(response, 'completion'):
					return str(response.completion)
				return str(response)
			elif hasattr(self.llm, 'generate'):
				response = await self.llm.generate(prompt)
				return str(response)
			elif hasattr(self.llm, '__call__'):
				response = await self.llm(prompt)
				return str(response)
		except Exception as e:
			logger.warning(f'LLM call failed: {e}')

		return ''

	async def analyze(self, query: str, use_llm: bool = True, extract_topic: bool = False) -> AnalyzedIntent:
		"""
		Perform full intent analysis on the query.

		Args:
		    query: The user's search query or task description
		    use_llm: Whether to use LLM for deeper analysis
		    extract_topic: If True, extract meaningful search topic from task description first

		Returns:
		    AnalyzedIntent with all analysis results
		"""
		# Extract search topic from task description if requested
		original_query = query
		if extract_topic:
			search_topic = self.extract_search_topic(query)
			logger.info(f"Extracted search topic: '{search_topic}' from task: '{query[:100]}...'")
			query = search_topic

		# Check cache
		cache_key = f'{query}_{use_llm}'
		if cache_key in self._cache:
			return self._cache[cache_key]

		# Detect language
		language = self.detect_language(query)

		# Extract keywords
		keywords = self.extract_keywords(query, language)

		# Identify primary keyword
		primary_keyword = self.identify_primary_keyword(keywords, query)

		# Detect intent type
		intent_type = self.detect_intent_type(original_query)  # Use original for intent detection

		# Generate basic query variants
		expanded_queries = self.generate_query_variants(keywords, primary_keyword, query, language)

		# LLM-based semantic analysis
		llm_analysis = {'semantic_variants': [], 'related_topics': [], 'context': '', 'confidence': 0.5}
		if use_llm and self.llm:
			llm_analysis = await self.analyze_with_llm(query, language)

		# Create the analyzed intent
		analyzed = AnalyzedIntent(
			original_query=query,  # Use extracted topic as the query for search
			keywords=keywords,
			primary_keyword=primary_keyword,
			context=llm_analysis.get('context', ''),
			intent_type=intent_type,
			expanded_queries=expanded_queries,
			fallback_strategies=[],  # Will be filled below
			semantic_variants=llm_analysis.get('semantic_variants', []),
			related_topics=llm_analysis.get('related_topics', []),
			language=language,
			confidence=llm_analysis.get('confidence', 0.5),
		)

		# Generate fallback strategies
		analyzed.fallback_strategies = self.generate_fallback_strategies(analyzed)

		# Cache the result
		self._cache[cache_key] = analyzed

		logger.info(
			f'Intent analysis complete: {len(keywords)} keywords, '
			f'{len(expanded_queries)} variants, '
			f'{len(analyzed.fallback_strategies)} strategies'
		)

		return analyzed


class ResultFusion:
	"""
	Implements Reciprocal Rank Fusion (RRF) to combine results
	from multiple search strategies.
	"""

	def __init__(self, k: int = 60):
		"""
		Initialize RRF with constant k.

		Args:
		    k: RRF constant (typically 60)
		"""
		self.k = k

	def calculate_rrf_score(self, ranks: list[int]) -> float:
		"""
		Calculate RRF score for an item given its ranks across different searches.

		RRF_score = sum(1 / (k + rank_i))

		Args:
		    ranks: List of ranks from different search strategies

		Returns:
		    Combined RRF score
		"""
		return sum(1.0 / (self.k + rank) for rank in ranks if rank > 0)

	def fuse_results(self, results_by_strategy: dict[SearchStrategy, list[SearchResult]]) -> FusedResults:
		"""
		Fuse results from multiple search strategies using RRF.

		Args:
		    results_by_strategy: Dict mapping strategy to list of results

		Returns:
		    FusedResults with combined and ranked results
		"""
		# Track all unique results by content
		content_scores: dict[str, dict[str, Any]] = {}

		for strategy, results in results_by_strategy.items():
			for result in results:
				content_key = result.content.lower().strip()[:200]  # Use first 200 chars as key

				if content_key not in content_scores:
					content_scores[content_key] = {'result': result, 'ranks': [], 'strategies': []}

				content_scores[content_key]['ranks'].append(result.rank)
				content_scores[content_key]['strategies'].append(strategy)

		# Calculate RRF scores
		scored_results = []
		for content_key, data in content_scores.items():
			rrf_score = self.calculate_rrf_score(data['ranks'])
			result = data['result']
			result.relevance_score = rrf_score
			scored_results.append({'result': result, 'score': rrf_score, 'strategies': data['strategies']})

		# Sort by RRF score (descending)
		scored_results.sort(key=lambda x: x['score'], reverse=True)

		# Determine best strategy (the one that contributed most high-ranked results)
		strategy_contributions: dict[SearchStrategy, float] = {}
		for item in scored_results[:10]:  # Top 10 results
			for strategy in item['strategies']:
				strategy_contributions[strategy] = strategy_contributions.get(strategy, 0) + item['score']

		best_strategy = (
			max(strategy_contributions.keys(), key=lambda s: strategy_contributions[s])
			if strategy_contributions
			else SearchStrategy.FULL_QUERY
		)

		# Build final results
		final_results = [item['result'] for item in scored_results]
		strategies_used = list(results_by_strategy.keys())

		return FusedResults(
			results=final_results,
			total_found=len(final_results),
			strategies_used=strategies_used,
			best_strategy=best_strategy,
			has_results=len(final_results) > 0,
		)


class SearchGuarantee:
	"""
	Ensures that search always returns results by trying multiple strategies.
	"""

	def __init__(self, intent_analyzer: IntentAnalyzer, result_fusion: Optional[ResultFusion] = None):
		"""
		Initialize the search guarantee system.

		Args:
		    intent_analyzer: IntentAnalyzer instance
		    result_fusion: Optional ResultFusion instance
		"""
		self.analyzer = intent_analyzer
		self.fusion = result_fusion or ResultFusion()
		self._search_executor: Optional[Any] = None

	def set_search_executor(self, executor: Any):
		"""Set the function/method that actually executes searches."""
		self._search_executor = executor

	async def guaranteed_search(self, query: str, search_func: Optional[Any] = None, max_strategies: int = 5) -> FusedResults:
		"""
		Perform a guaranteed search that will return results.

		Args:
		    query: User's search query
		    search_func: Async function that performs the actual search
		    max_strategies: Maximum number of strategies to try

		Returns:
		    FusedResults with guaranteed results
		"""
		executor = search_func or self._search_executor
		if not executor:
			raise ValueError('No search executor provided')

		# Analyze the query
		analyzed = await self.analyzer.analyze(query)

		results_by_strategy: dict[SearchStrategy, list[SearchResult]] = {}
		strategies_tried = 0

		# Try strategies in order until we get results or exhaust strategies
		for strategy_info in analyzed.fallback_strategies:
			if strategies_tried >= max_strategies:
				break

			strategy = strategy_info['strategy']
			search_query = strategy_info['query']

			try:
				logger.info(f"Trying strategy {strategy.value}: '{search_query}'")

				# Execute the search
				results = await executor(search_query)

				if results:
					# Convert to SearchResult objects if needed
					search_results = []
					for i, result in enumerate(results):
						if isinstance(result, SearchResult):
							result.strategy_used = strategy
							result.rank = i + 1
							search_results.append(result)
						else:
							search_results.append(
								SearchResult(
									content=str(result), source='search', relevance_score=0.0, strategy_used=strategy, rank=i + 1
								)
							)

					results_by_strategy[strategy] = search_results
					logger.info(f'Strategy {strategy.value} found {len(search_results)} results')
				else:
					logger.info(f'Strategy {strategy.value} found no results')

			except Exception as e:
				logger.warning(f'Strategy {strategy.value} failed: {e}')

			strategies_tried += 1

			# If we have good results from multiple strategies, we can stop early
			total_results = sum(len(r) for r in results_by_strategy.values())
			if total_results >= 10 and len(results_by_strategy) >= 2:
				break

		# Fuse results
		if results_by_strategy:
			return self.fusion.fuse_results(results_by_strategy)

		# If still no results, return empty but valid response
		return FusedResults(
			results=[],
			total_found=0,
			strategies_used=[s['strategy'] for s in analyzed.fallback_strategies[:strategies_tried]],
			best_strategy=SearchStrategy.FULL_QUERY,
			has_results=False,
		)

	def build_enhanced_task(self, analyzed: AnalyzedIntent, original_task: str) -> str:
		"""
		Build an enhanced task description with fallback instructions.

		Args:
		    analyzed: The analyzed intent
		    original_task: The original task description

		Returns:
		    Enhanced task with fallback instructions
		"""
		# Build fallback query list
		fallback_queries = [s['query'] for s in analyzed.fallback_strategies[:5]]

		if analyzed.language == 'ko':
			enhanced = f"""{original_task}

검색 전략 (결과가 없으면 순서대로 시도하세요):
1. 먼저 원래 검색어로 검색: "{analyzed.original_query}"
2. 키워드로 검색: {', '.join(analyzed.keywords[:3])}
3. 주요 키워드만 검색: "{analyzed.primary_keyword}"
"""
			if analyzed.semantic_variants:
				enhanced += f'4. 유사한 검색어 시도: {", ".join(analyzed.semantic_variants[:2])}\n'

			enhanced += """
중요: 검색 결과가 없다고 말하지 마세요. 위의 전략을 모두 시도하고, 
관련된 정보라도 찾아서 제공하세요. 완전히 일치하지 않더라도 
가장 관련성 있는 정보를 제공하는 것이 중요합니다."""
		else:
			enhanced = f"""{original_task}

Search strategies (try in order if no results):
1. First search with original query: "{analyzed.original_query}"
2. Search with keywords: {', '.join(analyzed.keywords[:3])}
3. Search with primary keyword only: "{analyzed.primary_keyword}"
"""
			if analyzed.semantic_variants:
				enhanced += f'4. Try similar searches: {", ".join(analyzed.semantic_variants[:2])}\n'

			enhanced += """
IMPORTANT: Never say "not found" or "no results". Try ALL strategies above,
and provide whatever relevant information you can find. Even if not an exact match,
providing the most relevant information is important."""

		return enhanced

```

---

## backend/browser-use/ip-rotation/ip_rotation.go

```go
package main

import (
	"crypto/rand"
	"encoding/json"
	"errors"
	"fmt"
	"log"
	"math/big"
	"net"
	"net/url"
	"os"
	"path/filepath"
	"strings"
	"sync"
	"time"
)

// ProxyIP는 단일 프록시 설정과 통계 정보를 나타냅니다.
type ProxyIP struct {
	ID              string    `json:"id"`
	Address         string    `json:"address"`  // e.g., "http://proxy.example.com:8080" or "socks5://10.0.0.1:1080"
	Protocol        string    `json:"protocol"` // http, https, socks4, socks5
	Username        string    `json:"username,omitempty"`
	Password        string    `json:"password,omitempty"`
	Country         string    `json:"country,omitempty"`
	City            string    `json:"city,omitempty"`
	Enabled         bool      `json:"enabled"`
	UsageCount      int64     `json:"usageCount"`
	LastUsed        time.Time `json:"lastUsed,omitempty"`
	SuccessCount    int64     `json:"successCount"`
	FailCount       int64     `json:"failCount"`
	CaptchaCount    int64     `json:"captchaCount"`
	AvgLatencyMs    int64     `json:"avgLatencyMs"`
	CreatedAt       time.Time `json:"createdAt"`
	DisabledAt      time.Time `json:"disabledAt,omitempty"` // When proxy was auto-disabled
	LastHealthCheck time.Time `json:"lastHealthCheck,omitempty"`
	HealthStatus    string    `json:"healthStatus,omitempty"` // healthy, unhealthy, unknown
}

// RotationStrategy는 프록시 선택(로테이션) 전략을 정의합니다.
type RotationStrategy string

const (
	StrategyRoundRobin RotationStrategy = "round_robin"
	StrategyRandom     RotationStrategy = "random"
	StrategyLeastUsed  RotationStrategy = "least_used"
	StrategyWeighted   RotationStrategy = "weighted"   // based on success rate
	StrategyGeographic RotationStrategy = "geographic" // based on country/region
)

// validStrategies는 RotationStrategy 값 검증에 사용되는 허용 목록입니다.
var validStrategies = map[RotationStrategy]bool{
	StrategyRoundRobin: true,
	StrategyRandom:     true,
	StrategyLeastUsed:  true,
	StrategyWeighted:   true,
	StrategyGeographic: true,
}

// IPPoolConfig는 IP 풀의 동작(전략/쿨다운/헬스체크/영속화) 설정을 담습니다.
type IPPoolConfig struct {
	Strategy            RotationStrategy `json:"strategy"`
	MaxFailures         int              `json:"maxFailures"`     // auto-disable after N failures
	CooldownMinutes     int              `json:"cooldownMinutes"` // re-enable after cooldown
	PreferredCountry    string           `json:"preferredCountry,omitempty"`
	HealthCheckInterval int              `json:"healthCheckInterval"`       // seconds between health checks
	HealthCheckTimeout  int              `json:"healthCheckTimeout"`        // seconds for health check timeout
	PersistencePath     string           `json:"persistencePath,omitempty"` // path to save/load pool state
}

// Validate는 IPPoolConfig 값이 유효한지 검사하고, 잘못된 설정이면 오류를 반환합니다.
func (c *IPPoolConfig) Validate() error {
	if c.Strategy != "" && !validStrategies[c.Strategy] {
		return fmt.Errorf("invalid strategy: %s, must be one of: round_robin, random, least_used, weighted, geographic", c.Strategy)
	}
	if c.MaxFailures < 0 {
		return errors.New("maxFailures must be non-negative")
	}
	if c.CooldownMinutes < 0 {
		return errors.New("cooldownMinutes must be non-negative")
	}
	if c.HealthCheckInterval < 0 {
		return errors.New("healthCheckInterval must be non-negative")
	}
	if c.HealthCheckTimeout < 0 {
		return errors.New("healthCheckTimeout must be non-negative")
	}
	return nil
}

// IPPoolState는 IP 풀의 상태를 파일에 저장/복원하기 위한 직렬화 구조체입니다.
type IPPoolState struct {
	Proxies map[string]*ProxyIP `json:"proxies"`
	Order   []string            `json:"order"`
	Index   int                 `json:"index"`
	Config  IPPoolConfig        `json:"config"`
	SavedAt time.Time           `json:"savedAt"`
}

// IPPool은 프록시 풀을 관리하고 로테이션/통계/헬스체크/영속화를 제공합니다.
type IPPool struct {
	mu                 sync.RWMutex
	proxies            map[string]*ProxyIP
	order              []string // for round-robin
	index              int      // current index for round-robin
	config             IPPoolConfig
	cooldownTicker     *time.Ticker
	healthCheckTicker  *time.Ticker
	stopCooldown       chan struct{}
	stopHealthCheck    chan struct{}
	cooldownRunning    bool
	healthCheckRunning bool
}

var (
	globalIPPool *IPPool
	muIPPool     sync.RWMutex
)

// initIPPool은 환경 변수 기반 설정을 읽어 전역 IP 풀을 초기화합니다.
func initIPPool() {
	// Get config from environment
	strategy := RotationStrategy(os.Getenv("STRATEGY"))
	if strategy == "" {
		strategy = StrategyRoundRobin
	}

	maxFailures := 5
	if v := os.Getenv("MAX_FAILURES"); v != "" {
		fmt.Sscanf(v, "%d", &maxFailures)
	}

	cooldownMinutes := 30
	if v := os.Getenv("COOLDOWN_MINUTES"); v != "" {
		fmt.Sscanf(v, "%d", &cooldownMinutes)
	}

	healthCheckInterval := 300
	if v := os.Getenv("HEALTH_CHECK_INTERVAL"); v != "" {
		fmt.Sscanf(v, "%d", &healthCheckInterval)
	}

	persistencePath := os.Getenv("PERSISTENCE_PATH")

	globalIPPool = NewIPPool(IPPoolConfig{
		Strategy:            strategy,
		MaxFailures:         maxFailures,
		CooldownMinutes:     cooldownMinutes,
		HealthCheckInterval: healthCheckInterval,
		HealthCheckTimeout:  10,
		PersistencePath:     persistencePath,
	})

	// Load existing state if persistence path is set
	if persistencePath != "" {
		if err := globalIPPool.LoadFromFile(persistencePath); err != nil {
			log.Printf("[IP-ROTATION] Failed to load state: %v", err)
		}
	}
}

// NewIPPool은 주어진 설정으로 IPPool을 생성하고, 필요 시 쿨다운/헬스체크 루틴을 시작합니다.
func NewIPPool(config IPPoolConfig) *IPPool {
	pool := &IPPool{
		proxies:         make(map[string]*ProxyIP),
		order:           make([]string, 0),
		index:           0,
		config:          config,
		stopCooldown:    make(chan struct{}),
		stopHealthCheck: make(chan struct{}),
	}

	// Start cooldown checker if cooldown is configured
	if config.CooldownMinutes > 0 {
		pool.StartCooldownChecker()
	}

	// Start health checker if configured
	if config.HealthCheckInterval > 0 {
		pool.StartHealthChecker()
	}

	return pool
}

// StartCooldownChecker는 쿨다운 이후 프록시를 자동 재활성화하는 백그라운드 루틴을 시작합니다.
func (p *IPPool) StartCooldownChecker() {
	p.mu.Lock()
	if p.cooldownRunning {
		p.mu.Unlock()
		return
	}
	p.cooldownRunning = true
	// Check every minute for cooldown expiry
	p.cooldownTicker = time.NewTicker(1 * time.Minute)
	p.mu.Unlock()

	go func() {
		log.Printf("[IP-ROTATION] Cooldown checker started (cooldown=%d minutes)", p.config.CooldownMinutes)
		for {
			select {
			case <-p.cooldownTicker.C:
				p.checkAndReenableProxies()
			case <-p.stopCooldown:
				p.cooldownTicker.Stop()
				log.Printf("[IP-ROTATION] Cooldown checker stopped")
				return
			}
		}
	}()
}

// StopCooldownChecker는 쿨다운 체크 백그라운드 루틴을 중지합니다.
func (p *IPPool) StopCooldownChecker() {
	p.mu.Lock()
	defer p.mu.Unlock()
	if p.cooldownRunning {
		close(p.stopCooldown)
		p.cooldownRunning = false
		p.stopCooldown = make(chan struct{})
	}
}

// checkAndReenableProxies는 비활성화된 프록시의 쿨다운 만료 여부를 확인하고 재활성화합니다.
func (p *IPPool) checkAndReenableProxies() {
	p.mu.Lock()
	defer p.mu.Unlock()

	if p.config.CooldownMinutes <= 0 {
		return
	}

	cooldownDuration := time.Duration(p.config.CooldownMinutes) * time.Minute
	now := time.Now()

	for id, proxy := range p.proxies {
		if !proxy.Enabled && !proxy.DisabledAt.IsZero() {
			if now.Sub(proxy.DisabledAt) >= cooldownDuration {
				proxy.Enabled = true
				proxy.FailCount = 0 // Reset fail count on re-enable
				proxy.DisabledAt = time.Time{}
				log.Printf("[IP-ROTATION] Proxy re-enabled after cooldown: id=%s addr=%s", id, proxy.Address)
			}
		}
	}
}

// StartHealthChecker는 주기적으로 프록시 가용성을 점검하는 헬스체크 루틴을 시작합니다.
func (p *IPPool) StartHealthChecker() {
	p.mu.Lock()
	if p.healthCheckRunning {
		p.mu.Unlock()
		return
	}
	p.healthCheckRunning = true
	interval := p.config.HealthCheckInterval
	if interval <= 0 {
		interval = 300 // default 5 minutes
	}
	p.healthCheckTicker = time.NewTicker(time.Duration(interval) * time.Second)
	p.mu.Unlock()

	go func() {
		log.Printf("[IP-ROTATION] Health checker started (interval=%d seconds)", interval)
		for {
			select {
			case <-p.healthCheckTicker.C:
				p.runHealthChecks()
			case <-p.stopHealthCheck:
				p.healthCheckTicker.Stop()
				log.Printf("[IP-ROTATION] Health checker stopped")
				return
			}
		}
	}()
}

// StopHealthChecker는 헬스체크 백그라운드 루틴을 중지합니다.
func (p *IPPool) StopHealthChecker() {
	p.mu.Lock()
	defer p.mu.Unlock()
	if p.healthCheckRunning {
		close(p.stopHealthCheck)
		p.healthCheckRunning = false
		p.stopHealthCheck = make(chan struct{})
	}
}

// runHealthChecks는 활성화된 프록시들에 대해 병렬 헬스체크를 수행하고 상태를 업데이트합니다.
func (p *IPPool) runHealthChecks() {
	p.mu.RLock()
	proxiesToCheck := make([]*ProxyIP, 0)
	for _, proxy := range p.proxies {
		if proxy.Enabled {
			proxiesToCheck = append(proxiesToCheck, proxy)
		}
	}
	timeout := p.config.HealthCheckTimeout
	if timeout <= 0 {
		timeout = 10
	}
	p.mu.RUnlock()

	var wg sync.WaitGroup
	for _, proxy := range proxiesToCheck {
		wg.Add(1)
		go func(px *ProxyIP) {
			defer wg.Done()
			healthy := p.checkProxyHealth(px, time.Duration(timeout)*time.Second)
			p.mu.Lock()
			px.LastHealthCheck = time.Now()
			if healthy {
				px.HealthStatus = "healthy"
			} else {
				px.HealthStatus = "unhealthy"
			}
			p.mu.Unlock()
		}(proxy)
	}
	wg.Wait()
	log.Printf("[IP-ROTATION] Health check completed for %d proxies", len(proxiesToCheck))
}

// checkProxyHealth는 프록시 호스트에 TCP 연결을 시도하여 도달 가능 여부를 반환합니다.
func (p *IPPool) checkProxyHealth(proxy *ProxyIP, timeout time.Duration) bool {
	proxyURL, err := proxy.GetProxyURL()
	if err != nil {
		return false
	}

	// Extract host:port from proxy URL
	host := proxyURL.Host
	if host == "" {
		return false
	}

	conn, err := net.DialTimeout("tcp", host, timeout)
	if err != nil {
		log.Printf("[IP-ROTATION] Health check failed for %s: %v", proxy.ID, err)
		return false
	}
	conn.Close()
	return true
}

// RunHealthCheckNow는 즉시 헬스체크를 비동기로 트리거합니다.
func (p *IPPool) RunHealthCheckNow() {
	go p.runHealthChecks()
}

// GetNextProxy는 설정된 로테이션 전략에 따라 다음 프록시를 선택하고 사용 통계를 갱신합니다.
func (p *IPPool) GetNextProxy() (*ProxyIP, error) {
	p.mu.Lock()
	defer p.mu.Unlock()

	enabledProxies := p.getEnabledProxies()
	if len(enabledProxies) == 0 {
		return nil, errors.New("no enabled proxies available")
	}

	var selected *ProxyIP

	switch p.config.Strategy {
	case StrategyRoundRobin:
		selected = p.selectRoundRobin(enabledProxies)
	case StrategyRandom:
		selected = p.selectRandom(enabledProxies)
	case StrategyLeastUsed:
		selected = p.selectLeastUsed(enabledProxies)
	case StrategyWeighted:
		selected = p.selectWeighted(enabledProxies)
	case StrategyGeographic:
		selected = p.selectGeographic(enabledProxies)
	default:
		selected = p.selectRoundRobin(enabledProxies)
	}

	if selected != nil {
		selected.UsageCount++
		selected.LastUsed = time.Now()
		log.Printf("[IP-ROTATION] Selected proxy: id=%s addr=%s strategy=%s usage_count=%d",
			selected.ID, selected.Address, p.config.Strategy, selected.UsageCount)
	}

	return selected, nil
}

// getEnabledProxies는 Enabled=true인 프록시 목록을 반환합니다.
func (p *IPPool) getEnabledProxies() []*ProxyIP {
	var enabled []*ProxyIP
	for _, proxy := range p.proxies {
		if proxy.Enabled {
			enabled = append(enabled, proxy)
		}
	}
	return enabled
}

// selectRoundRobin은 라운드로빈 순서(order)를 기준으로 다음 사용 가능한 프록시를 선택합니다.
func (p *IPPool) selectRoundRobin(proxies []*ProxyIP) *ProxyIP {
	if len(proxies) == 0 {
		return nil
	}
	// Find valid index
	if p.index >= len(p.order) {
		p.index = 0
	}

	// Try to find next enabled proxy
	attempts := 0
	for attempts < len(p.order) {
		if p.index >= len(p.order) {
			p.index = 0
		}
		id := p.order[p.index]
		p.index++
		if proxy, ok := p.proxies[id]; ok && proxy.Enabled {
			return proxy
		}
		attempts++
	}

	// Fallback to first enabled
	if len(proxies) > 0 {
		return proxies[0]
	}
	return nil
}

// secureRandomInt는 crypto/rand를 사용해 [0, max) 범위의 난수를 생성합니다.
func secureRandomInt(max int) int {
	if max <= 0 {
		return 0
	}
	n, err := rand.Int(rand.Reader, big.NewInt(int64(max)))
	if err != nil {
		// Fallback to time-based (should not happen)
		return int(time.Now().UnixNano()) % max
	}
	return int(n.Int64())
}

// selectRandom은 사용 가능한 프록시 중 하나를 무작위로 선택합니다.
func (p *IPPool) selectRandom(proxies []*ProxyIP) *ProxyIP {
	if len(proxies) == 0 {
		return nil
	}
	idx := secureRandomInt(len(proxies))
	return proxies[idx]
}

// selectLeastUsed는 UsageCount가 가장 낮은 프록시를 선택합니다.
func (p *IPPool) selectLeastUsed(proxies []*ProxyIP) *ProxyIP {
	if len(proxies) == 0 {
		return nil
	}
	min := proxies[0]
	for _, proxy := range proxies[1:] {
		if proxy.UsageCount < min.UsageCount {
			min = proxy
		}
	}
	return min
}

// selectWeighted는 성공률과 CAPTCHA 패널티 기반 가중치 랜덤 선택으로 프록시를 선택합니다.
func (p *IPPool) selectWeighted(proxies []*ProxyIP) *ProxyIP {
	if len(proxies) == 0 {
		return nil
	}

	// Calculate weights based on success rate
	// Use a minimum weight to give all proxies some chance
	const minWeight = 10.0
	weights := make([]float64, len(proxies))
	totalWeight := 0.0

	for i, proxy := range proxies {
		total := proxy.SuccessCount + proxy.FailCount
		var baseWeight float64
		if total == 0 {
			// New proxy gets a neutral weight (50% success assumed + exploration bonus)
			baseWeight = 50.0 + minWeight
		} else {
			rate := float64(proxy.SuccessCount) / float64(total) * 100
			baseWeight = rate + minWeight
		}

		captchaRate := float64(proxy.CaptchaCount) / float64(proxy.UsageCount+1)
		captchaPenalty := 1.0 - (captchaRate * 0.7)
		if captchaPenalty < 0.1 {
			captchaPenalty = 0.1
		}

		weight := baseWeight * captchaPenalty
		if weight < minWeight {
			weight = minWeight
		}
		weights[i] = weight
		totalWeight += weight
	}

	if totalWeight <= 0 {
		return proxies[secureRandomInt(len(proxies))]
	}

	// Generate random value in [0, totalWeight)
	randN, err := rand.Int(rand.Reader, big.NewInt(int64(totalWeight*1000)))
	if err != nil {
		// Fallback
		return proxies[secureRandomInt(len(proxies))]
	}
	randVal := float64(randN.Int64()) / 1000.0

	// Select based on cumulative weight
	cumulative := 0.0
	for i, weight := range weights {
		cumulative += weight
		if randVal < cumulative {
			return proxies[i]
		}
	}

	// Fallback to last proxy
	return proxies[len(proxies)-1]
}

// selectGeographic은 선호 국가 설정이 있으면 해당 국가 프록시를 우선 선택하고, 없으면 라운드로빈으로 폴백합니다.
func (p *IPPool) selectGeographic(proxies []*ProxyIP) *ProxyIP {
	if len(proxies) == 0 {
		return nil
	}
	// Prefer proxies matching configured country
	if p.config.PreferredCountry != "" {
		var matchingProxies []*ProxyIP
		for _, proxy := range proxies {
			if strings.EqualFold(proxy.Country, p.config.PreferredCountry) {
				matchingProxies = append(matchingProxies, proxy)
			}
		}
		if len(matchingProxies) > 0 {
			// Use round-robin among matching proxies
			return matchingProxies[secureRandomInt(len(matchingProxies))]
		}
	}
	// Fallback to round-robin
	return p.selectRoundRobin(proxies)
}

// RecordSuccess는 특정 프록시의 성공 결과와 평균 지연시간을 기록합니다.
func (p *IPPool) RecordSuccess(proxyID string, latencyMs int64) {
	p.mu.Lock()
	defer p.mu.Unlock()

	if proxy, ok := p.proxies[proxyID]; ok {
		proxy.SuccessCount++
		// Update average latency
		total := proxy.SuccessCount + proxy.FailCount
		if total > 0 {
			proxy.AvgLatencyMs = (proxy.AvgLatencyMs*(total-1) + latencyMs) / total
		}
		log.Printf("[IP-ROTATION] Success recorded: id=%s success=%d fail=%d latency=%dms",
			proxyID, proxy.SuccessCount, proxy.FailCount, latencyMs)
	}
}

// RecordCaptcha는 특정 프록시에 CAPTCHA 발생을 기록하여 선택 가중치에 반영될 수 있도록 합니다.
func (p *IPPool) RecordCaptcha(proxyID string, captchaType string) {
	p.mu.Lock()
	defer p.mu.Unlock()

	if proxy, ok := p.proxies[proxyID]; ok {
		proxy.CaptchaCount++
		log.Printf("[IP-ROTATION] CAPTCHA recorded: id=%s count=%d type=%s",
			proxyID, proxy.CaptchaCount, captchaType)
	}
}

// RecordFailure는 특정 프록시의 실패를 기록하고, 임계치 초과 시 자동으로 비활성화합니다.
func (p *IPPool) RecordFailure(proxyID string, reason string) {
	p.mu.Lock()
	defer p.mu.Unlock()

	if proxy, ok := p.proxies[proxyID]; ok {
		proxy.FailCount++
		log.Printf("[IP-ROTATION] Failure recorded: id=%s success=%d fail=%d reason=%s",
			proxyID, proxy.SuccessCount, proxy.FailCount, reason)

		// Auto-disable if too many failures
		if p.config.MaxFailures > 0 && proxy.FailCount >= int64(p.config.MaxFailures) {
			proxy.Enabled = false
			proxy.DisabledAt = time.Now()
			log.Printf("[IP-ROTATION] Proxy auto-disabled due to failures: id=%s (will re-enable after %d minutes)",
				proxyID, p.config.CooldownMinutes)
		}
	}
}

// AddProxy는 프록시를 풀에 추가하고 형식/프로토콜을 검증한 뒤 기본값을 설정합니다.
func (p *IPPool) AddProxy(proxy *ProxyIP) error {
	p.mu.Lock()
	defer p.mu.Unlock()

	if proxy.ID == "" {
		proxy.ID = "proxy_" + randomID()
	}
	if proxy.Address == "" {
		return errors.New("proxy address is required")
	}

	// Validate proxy address format
	if _, err := url.Parse(proxy.Address); err != nil {
		return fmt.Errorf("invalid proxy address format: %w", err)
	}

	if proxy.Protocol == "" {
		proxy.Protocol = "http"
	}

	// Validate protocol
	validProtocols := map[string]bool{"http": true, "https": true, "socks4": true, "socks5": true}
	if !validProtocols[strings.ToLower(proxy.Protocol)] {
		return fmt.Errorf("invalid protocol: %s, must be one of: http, https, socks4, socks5", proxy.Protocol)
	}
	proxy.Protocol = strings.ToLower(proxy.Protocol)

	proxy.CreatedAt = time.Now()
	proxy.Enabled = true
	proxy.HealthStatus = "unknown"

	p.proxies[proxy.ID] = proxy
	p.order = append(p.order, proxy.ID)

	log.Printf("[IP-ROTATION] Proxy added: id=%s addr=%s protocol=%s country=%s",
		proxy.ID, proxy.Address, proxy.Protocol, proxy.Country)

	// Auto-save if persistence is configured
	p.autoSave()

	return nil
}

// RemoveProxy는 풀에서 프록시를 제거하고 라운드로빈 순서도 갱신합니다.
func (p *IPPool) RemoveProxy(id string) error {
	p.mu.Lock()
	defer p.mu.Unlock()

	if _, ok := p.proxies[id]; !ok {
		return errors.New("proxy not found")
	}

	delete(p.proxies, id)

	// Remove from order
	for i, oid := range p.order {
		if oid == id {
			p.order = append(p.order[:i], p.order[i+1:]...)
			break
		}
	}

	log.Printf("[IP-ROTATION] Proxy removed: id=%s", id)

	// Auto-save if persistence is configured
	p.autoSave()

	return nil
}

// GetAllProxies는 풀에 등록된 모든 프록시 목록을 반환합니다.
func (p *IPPool) GetAllProxies() []*ProxyIP {
	p.mu.RLock()
	defer p.mu.RUnlock()

	proxies := make([]*ProxyIP, 0, len(p.proxies))
	for _, proxy := range p.proxies {
		proxies = append(proxies, proxy)
	}
	return proxies
}

// GetPoolStats는 풀 전체의 통계를 집계하여 반환합니다.
func (p *IPPool) GetPoolStats() map[string]any {
	p.mu.RLock()
	defer p.mu.RUnlock()

	var totalUsage, totalSuccess, totalFail, totalCaptcha int64
	enabledCount := 0
	disabledCount := 0
	healthyCount := 0
	unhealthyCount := 0

	for _, proxy := range p.proxies {
		totalUsage += proxy.UsageCount
		totalSuccess += proxy.SuccessCount
		totalFail += proxy.FailCount
		totalCaptcha += proxy.CaptchaCount
		if proxy.Enabled {
			enabledCount++
		} else {
			disabledCount++
		}
		switch proxy.HealthStatus {
		case "healthy":
			healthyCount++
		case "unhealthy":
			unhealthyCount++
		}
	}

	successRate := float64(0)
	if totalSuccess+totalFail > 0 {
		successRate = float64(totalSuccess) / float64(totalSuccess+totalFail) * 100
	}

	captchaRate := float64(0)
	if totalUsage > 0 {
		captchaRate = float64(totalCaptcha) / float64(totalUsage) * 100
	}

	return map[string]any{
		"totalProxies":     len(p.proxies),
		"enabledProxies":   enabledCount,
		"disabledProxies":  disabledCount,
		"healthyProxies":   healthyCount,
		"unhealthyProxies": unhealthyCount,
		"totalUsage":       totalUsage,
		"totalSuccess":     totalSuccess,
		"totalFail":        totalFail,
		"totalCaptcha":     totalCaptcha,
		"successRate":      fmt.Sprintf("%.2f%%", successRate),
		"captchaRate":      fmt.Sprintf("%.2f%%", captchaRate),
		"strategy":         p.config.Strategy,
		"currentIndex":     p.index,
		"cooldownMinutes":  p.config.CooldownMinutes,
		"maxFailures":      p.config.MaxFailures,
	}
}

// UpdateConfig는 설정을 검증 후 적용하고, 변경 사항에 따라 백그라운드 루틴을 재시작합니다.
func (p *IPPool) UpdateConfig(cfg IPPoolConfig) error {
	if err := cfg.Validate(); err != nil {
		return err
	}

	p.mu.Lock()
	oldCooldown := p.config.CooldownMinutes
	oldHealthInterval := p.config.HealthCheckInterval
	p.config = cfg
	p.mu.Unlock()

	log.Printf("[IP-ROTATION] Config updated: strategy=%s maxFailures=%d cooldown=%dm healthInterval=%ds",
		cfg.Strategy, cfg.MaxFailures, cfg.CooldownMinutes, cfg.HealthCheckInterval)

	// Restart cooldown checker if cooldown setting changed
	if cfg.CooldownMinutes != oldCooldown {
		p.StopCooldownChecker()
		if cfg.CooldownMinutes > 0 {
			p.StartCooldownChecker()
		}
	}

	// Restart health checker if interval changed
	if cfg.HealthCheckInterval != oldHealthInterval {
		p.StopHealthChecker()
		if cfg.HealthCheckInterval > 0 {
			p.StartHealthChecker()
		}
	}

	// Auto-save if persistence is configured
	p.autoSave()

	return nil
}

// GetProxyURL은 프록시 주소(Address)에 인증 정보가 있으면 포함하여 url.URL을 반환합니다.
func (p *ProxyIP) GetProxyURL() (*url.URL, error) {
	proxyAddr := p.Address
	if p.Username != "" && p.Password != "" {
		// Parse and add auth
		u, err := url.Parse(proxyAddr)
		if err != nil {
			return nil, err
		}
		u.User = url.UserPassword(p.Username, p.Password)
		return u, nil
	}
	return url.Parse(proxyAddr)
}

// ========== Persistence Functions ==========

// SaveToFile은 현재 풀 상태를 JSON 파일로 저장합니다.
func (p *IPPool) SaveToFile(path string) error {
	p.mu.RLock()
	state := IPPoolState{
		Proxies: p.proxies,
		Order:   p.order,
		Index:   p.index,
		Config:  p.config,
		SavedAt: time.Now(),
	}
	p.mu.RUnlock()

	data, err := json.MarshalIndent(state, "", "  ")
	if err != nil {
		return fmt.Errorf("failed to marshal pool state: %w", err)
	}

	// Ensure directory exists
	dir := filepath.Dir(path)
	if err := os.MkdirAll(dir, 0755); err != nil {
		return fmt.Errorf("failed to create directory: %w", err)
	}

	if err := os.WriteFile(path, data, 0644); err != nil {
		return fmt.Errorf("failed to write file: %w", err)
	}

	log.Printf("[IP-ROTATION] Pool state saved to: %s", path)
	return nil
}

// LoadFromFile은 JSON 파일에서 풀 상태를 로드하여 적용합니다.
func (p *IPPool) LoadFromFile(path string) error {
	data, err := os.ReadFile(path)
	if err != nil {
		if os.IsNotExist(err) {
			log.Printf("[IP-ROTATION] No existing pool state file found: %s", path)
			return nil // Not an error if file doesn't exist
		}
		return fmt.Errorf("failed to read file: %w", err)
	}

	var state IPPoolState
	if err := json.Unmarshal(data, &state); err != nil {
		return fmt.Errorf("failed to unmarshal pool state: %w", err)
	}

	p.mu.Lock()
	p.proxies = state.Proxies
	p.order = state.Order
	p.index = state.Index
	if state.Config.Strategy != "" {
		p.config = state.Config
	}
	p.mu.Unlock()

	log.Printf("[IP-ROTATION] Pool state loaded from: %s (saved at: %s, proxies: %d)",
		path, state.SavedAt.Format(time.RFC3339), len(state.Proxies))

	return nil
}

// autoSave는 PersistencePath가 설정된 경우 풀 상태를 비동기로 저장합니다.
func (p *IPPool) autoSave() {
	if p.config.PersistencePath != "" {
		go func() {
			// Release lock before saving
			if err := p.SaveToFile(p.config.PersistencePath); err != nil {
				log.Printf("[IP-ROTATION] Auto-save failed: %v", err)
			}
		}()
	}
}

// ResetStats는 모든 프록시의 통계 값을 초기화합니다.
func (p *IPPool) ResetStats() {
	p.mu.Lock()
	defer p.mu.Unlock()

	for _, proxy := range p.proxies {
		proxy.UsageCount = 0
		proxy.SuccessCount = 0
		proxy.FailCount = 0
		proxy.CaptchaCount = 0
		proxy.AvgLatencyMs = 0
	}

	log.Printf("[IP-ROTATION] Statistics reset for all proxies")
}

// ResetProxyStats는 특정 프록시의 통계를 초기화하고 비활성화 상태였다면 재활성화합니다.
func (p *IPPool) ResetProxyStats(proxyID string) error {
	p.mu.Lock()
	defer p.mu.Unlock()

	proxy, ok := p.proxies[proxyID]
	if !ok {
		return errors.New("proxy not found")
	}

	proxy.UsageCount = 0
	proxy.SuccessCount = 0
	proxy.FailCount = 0
	proxy.CaptchaCount = 0
	proxy.AvgLatencyMs = 0
	// Re-enable if disabled
	if !proxy.Enabled {
		proxy.Enabled = true
		proxy.DisabledAt = time.Time{}
	}

	log.Printf("[IP-ROTATION] Statistics reset for proxy: %s", proxyID)
	return nil
}

// randomID는 프록시 ID 생성을 위한 짧은 랜덤 문자열을 반환합니다.
func randomID() string {
	const chars = "abcdefghijklmnopqrstuvwxyz0123456789"
	result := make([]byte, 8)
	for i := range result {
		n, _ := rand.Int(rand.Reader, big.NewInt(int64(len(chars))))
		result[i] = chars[n.Int64()]
	}
	return string(result)
}

// calculateSuccessRate는 성공/실패 카운트를 기반으로 성공률(%)을 계산합니다.
func calculateSuccessRate(p *ProxyIP) float64 {
	total := p.SuccessCount + p.FailCount
	if total == 0 {
		return 100.0
	}
	return float64(p.SuccessCount) / float64(total) * 100
}

```

---

## backend/browser-use/ip-rotation/server.go

```go
package main

import (
	"encoding/json"
	"errors"
	"fmt"
	"log"
	"net/http"
	"os"
	"strings"
	"time"
)

// ========== IP Rotation HTTP 핸들러 ==========

// writeJSON은 주어진 데이터를 JSON으로 인코딩하여 응답으로 반환합니다.
func writeJSON(w http.ResponseWriter, status int, data any) {
	w.Header().Set("Content-Type", "application/json")
	w.WriteHeader(status)
	json.NewEncoder(w).Encode(data)
}

// writeErr는 에러를 JSON 형태로 응답합니다.
func writeErr(w http.ResponseWriter, status int, err error) {
	writeJSON(w, status, map[string]string{"error": err.Error()})
}

// handleHealth는 서비스 헬스체크 및 현재 프록시 풀 통계를 반환합니다.
func handleHealth(w http.ResponseWriter, r *http.Request) {
	stats := globalIPPool.GetPoolStats()
	writeJSON(w, http.StatusOK, map[string]any{
		"status":  "ok",
		"service": "ip-rotation",
		"stats":   stats,
	})
}

// handleProxyPool은 프록시 풀 전체 조회/추가(관리자용)를 처리합니다.
func handleProxyPool(w http.ResponseWriter, r *http.Request) {
	switch r.Method {
	case http.MethodGet:
		proxies := globalIPPool.GetAllProxies()
		stats := globalIPPool.GetPoolStats()
		writeJSON(w, http.StatusOK, map[string]any{
			"proxies": proxies,
			"stats":   stats,
		})
	case http.MethodPost:
		var proxy ProxyIP
		if err := json.NewDecoder(r.Body).Decode(&proxy); err != nil {
			writeErr(w, http.StatusBadRequest, err)
			return
		}
		if err := globalIPPool.AddProxy(&proxy); err != nil {
			writeErr(w, http.StatusBadRequest, err)
			return
		}
		writeJSON(w, http.StatusCreated, proxy)
	default:
		writeErr(w, http.StatusMethodNotAllowed, errors.New("method not allowed"))
	}
}

// handleProxyPoolByID는 특정 프록시 조회/삭제/부분 수정(관리자용)을 처리합니다.
func handleProxyPoolByID(w http.ResponseWriter, r *http.Request) {
	id := strings.TrimPrefix(r.URL.Path, "/admin/proxy-pool/")
	if id == "" {
		writeErr(w, http.StatusBadRequest, errors.New("missing proxy id"))
		return
	}

	switch r.Method {
	case http.MethodGet:
		globalIPPool.mu.RLock()
		proxy, ok := globalIPPool.proxies[id]
		globalIPPool.mu.RUnlock()
		if !ok {
			writeErr(w, http.StatusNotFound, errors.New("proxy not found"))
			return
		}
		writeJSON(w, http.StatusOK, proxy)
	case http.MethodDelete:
		if err := globalIPPool.RemoveProxy(id); err != nil {
			writeErr(w, http.StatusNotFound, err)
			return
		}
		writeJSON(w, http.StatusOK, map[string]string{"deleted": id})
	case http.MethodPatch:
		globalIPPool.mu.Lock()
		proxy, ok := globalIPPool.proxies[id]
		if !ok {
			globalIPPool.mu.Unlock()
			writeErr(w, http.StatusNotFound, errors.New("proxy not found"))
			return
		}
		var patch map[string]any
		if err := json.NewDecoder(r.Body).Decode(&patch); err != nil {
			globalIPPool.mu.Unlock()
			writeErr(w, http.StatusBadRequest, err)
			return
		}
		if v, ok := patch["enabled"].(bool); ok {
			proxy.Enabled = v
			if v {
				proxy.DisabledAt = time.Time{}
			} else {
				proxy.DisabledAt = time.Now()
			}
		}
		if v, ok := patch["address"].(string); ok && v != "" {
			proxy.Address = v
		}
		if v, ok := patch["country"].(string); ok {
			proxy.Country = v
		}
		if v, ok := patch["city"].(string); ok {
			proxy.City = v
		}
		if v, ok := patch["protocol"].(string); ok && v != "" {
			proxy.Protocol = v
		}
		if v, ok := patch["username"].(string); ok {
			proxy.Username = v
		}
		if v, ok := patch["password"].(string); ok {
			proxy.Password = v
		}
		// Handle success/failure recording
		if success, ok := patch["success"].(bool); ok && success {
			latency := int64(0)
			if v, ok := patch["latency_ms"].(float64); ok {
				latency = int64(v)
			}
			proxy.SuccessCount++
			total := proxy.SuccessCount + proxy.FailCount
			if total > 0 {
				proxy.AvgLatencyMs = (proxy.AvgLatencyMs*(total-1) + latency) / total
			}
		}
		if failure, ok := patch["failure"].(bool); ok && failure {
			proxy.FailCount++
			if globalIPPool.config.MaxFailures > 0 && proxy.FailCount >= int64(globalIPPool.config.MaxFailures) {
				proxy.Enabled = false
				proxy.DisabledAt = time.Now()
			}
		}
		globalIPPool.mu.Unlock()
		log.Printf("[IP-ROTATION] Proxy updated: id=%s enabled=%v", id, proxy.Enabled)

		// Auto-save
		globalIPPool.autoSave()

		writeJSON(w, http.StatusOK, proxy)
	default:
		writeErr(w, http.StatusMethodNotAllowed, errors.New("method not allowed"))
	}
}

// handleProxyPoolConfig는 풀 설정 조회/수정(관리자용)을 처리합니다.
func handleProxyPoolConfig(w http.ResponseWriter, r *http.Request) {
	switch r.Method {
	case http.MethodGet:
		globalIPPool.mu.RLock()
		cfg := globalIPPool.config
		globalIPPool.mu.RUnlock()
		writeJSON(w, http.StatusOK, cfg)
	case http.MethodPatch:
		var cfg IPPoolConfig
		if err := json.NewDecoder(r.Body).Decode(&cfg); err != nil {
			writeErr(w, http.StatusBadRequest, err)
			return
		}
		if err := globalIPPool.UpdateConfig(cfg); err != nil {
			writeErr(w, http.StatusBadRequest, err)
			return
		}
		writeJSON(w, http.StatusOK, cfg)
	default:
		writeErr(w, http.StatusMethodNotAllowed, errors.New("method not allowed"))
	}
}

// handleProxyRotateTest는 N회 로테이션을 수행해 선택 결과를 점검할 수 있는 테스트 API입니다.
func handleProxyRotateTest(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		writeErr(w, http.StatusMethodNotAllowed, errors.New("use POST"))
		return
	}

	var req struct {
		Count int `json:"count"`
	}
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		req.Count = 5 // default
	}
	if req.Count <= 0 {
		req.Count = 5
	}
	if req.Count > 100 {
		req.Count = 100
	}

	results := make([]map[string]any, 0, req.Count)

	for i := 0; i < req.Count; i++ {
		proxy, err := globalIPPool.GetNextProxy()
		if err != nil {
			results = append(results, map[string]any{
				"iteration": i + 1,
				"error":     err.Error(),
			})
			continue
		}
		results = append(results, map[string]any{
			"iteration":    i + 1,
			"proxyId":      proxy.ID,
			"address":      proxy.Address,
			"protocol":     proxy.Protocol,
			"country":      proxy.Country,
			"usageCount":   proxy.UsageCount,
			"successRate":  fmt.Sprintf("%.2f%%", calculateSuccessRate(proxy)),
			"healthStatus": proxy.HealthStatus,
		})
	}

	stats := globalIPPool.GetPoolStats()

	log.Printf("[IP-ROTATION] Rotation test completed: count=%d", req.Count)

	writeJSON(w, http.StatusOK, map[string]any{
		"rotations": results,
		"stats":     stats,
	})
}

// handleProxyHealthCheck는 즉시 헬스체크를 수행하도록 트리거합니다.
func handleProxyHealthCheck(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		writeErr(w, http.StatusMethodNotAllowed, errors.New("use POST"))
		return
	}

	globalIPPool.RunHealthCheckNow()
	writeJSON(w, http.StatusOK, map[string]string{
		"status":  "started",
		"message": "Health check started in background",
	})
}

// handleProxyResetStats는 전체 또는 특정 프록시의 통계를 초기화합니다.
func handleProxyResetStats(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		writeErr(w, http.StatusMethodNotAllowed, errors.New("use POST"))
		return
	}

	var req struct {
		ProxyID string `json:"proxyId"`
	}
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil || req.ProxyID == "" {
		// Reset all
		globalIPPool.ResetStats()
		writeJSON(w, http.StatusOK, map[string]string{
			"status":  "success",
			"message": "All proxy statistics reset",
		})
		return
	}

	if err := globalIPPool.ResetProxyStats(req.ProxyID); err != nil {
		writeErr(w, http.StatusNotFound, err)
		return
	}
	writeJSON(w, http.StatusOK, map[string]string{
		"status":  "success",
		"message": fmt.Sprintf("Statistics reset for proxy: %s", req.ProxyID),
	})
}

// handleProxySave는 현재 풀 상태를 파일로 저장합니다.
func handleProxySave(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		writeErr(w, http.StatusMethodNotAllowed, errors.New("use POST"))
		return
	}

	var req struct {
		Path string `json:"path"`
	}
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		writeErr(w, http.StatusBadRequest, err)
		return
	}

	path := req.Path
	if path == "" {
		globalIPPool.mu.RLock()
		path = globalIPPool.config.PersistencePath
		globalIPPool.mu.RUnlock()
	}
	if path == "" {
		path = "ip_pool_state.json"
	}

	if err := globalIPPool.SaveToFile(path); err != nil {
		writeErr(w, http.StatusInternalServerError, err)
		return
	}

	writeJSON(w, http.StatusOK, map[string]string{
		"status":  "success",
		"message": fmt.Sprintf("Pool state saved to: %s", path),
	})
}

// handleProxyLoad는 파일에서 풀 상태를 로드합니다.
func handleProxyLoad(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		writeErr(w, http.StatusMethodNotAllowed, errors.New("use POST"))
		return
	}

	var req struct {
		Path string `json:"path"`
	}
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		writeErr(w, http.StatusBadRequest, err)
		return
	}

	path := req.Path
	if path == "" {
		globalIPPool.mu.RLock()
		path = globalIPPool.config.PersistencePath
		globalIPPool.mu.RUnlock()
	}
	if path == "" {
		path = "ip_pool_state.json"
	}

	if err := globalIPPool.LoadFromFile(path); err != nil {
		writeErr(w, http.StatusInternalServerError, err)
		return
	}

	writeJSON(w, http.StatusOK, map[string]string{
		"status":  "success",
		"message": fmt.Sprintf("Pool state loaded from: %s", path),
	})
}

// handleGetNextProxy는 다음 프록시를 반환합니다(클라이언트/크롤러용).
func handleGetNextProxy(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodGet && r.Method != http.MethodPost {
		writeErr(w, http.StatusMethodNotAllowed, errors.New("use GET or POST"))
		return
	}

	proxy, err := globalIPPool.GetNextProxy()
	if err != nil {
		writeErr(w, http.StatusServiceUnavailable, err)
		return
	}

	writeJSON(w, http.StatusOK, map[string]any{
		"proxyId":      proxy.ID,
		"address":      proxy.Address,
		"protocol":     proxy.Protocol,
		"username":     proxy.Username,
		"password":     proxy.Password,
		"country":      proxy.Country,
		"healthStatus": proxy.HealthStatus,
	})
}

// handleRecordResult는 프록시의 성공/실패 결과를 기록합니다(클라이언트/크롤러용).
func handleRecordResult(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		writeErr(w, http.StatusMethodNotAllowed, errors.New("use POST"))
		return
	}

	var req struct {
		ProxyID   string `json:"proxyId"`
		Success   bool   `json:"success"`
		LatencyMs int64  `json:"latencyMs"`
		Reason    string `json:"reason"`
	}
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		writeErr(w, http.StatusBadRequest, err)
		return
	}

	if req.ProxyID == "" {
		writeErr(w, http.StatusBadRequest, errors.New("proxyId is required"))
		return
	}

	if req.Success {
		globalIPPool.RecordSuccess(req.ProxyID, req.LatencyMs)
	} else {
		globalIPPool.RecordFailure(req.ProxyID, req.Reason)
	}

	writeJSON(w, http.StatusOK, map[string]string{
		"status": "recorded",
	})
}

// handleRecordCaptcha는 프록시의 CAPTCHA 발생을 기록합니다(클라이언트/크롤러용).
func handleRecordCaptcha(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		writeErr(w, http.StatusMethodNotAllowed, errors.New("use POST"))
		return
	}

	var req struct {
		ProxyID string `json:"proxyId"`
		Type    string `json:"type"`
	}
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		writeErr(w, http.StatusBadRequest, err)
		return
	}

	if req.ProxyID == "" {
		writeErr(w, http.StatusBadRequest, errors.New("proxyId is required"))
		return
	}

	globalIPPool.RecordCaptcha(req.ProxyID, req.Type)

	writeJSON(w, http.StatusOK, map[string]string{
		"status": "recorded",
	})
}

// corsMiddleware는 CORS 헤더를 추가하고 OPTIONS 프리플라이트 요청을 처리합니다.
func corsMiddleware(next http.HandlerFunc) http.HandlerFunc {
	return func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Access-Control-Allow-Origin", "*")
		w.Header().Set("Access-Control-Allow-Methods", "GET, POST, PATCH, DELETE, OPTIONS")
		w.Header().Set("Access-Control-Allow-Headers", "Content-Type, Authorization")

		if r.Method == http.MethodOptions {
			w.WriteHeader(http.StatusOK)
			return
		}

		next(w, r)
	}
}

// main은 환경 변수 기반으로 IP 풀을 초기화하고 HTTP 서버를 시작합니다.
func main() {
	// Initialize the IP pool
	initIPPool()

	// Get port from environment
	port := os.Getenv("PORT")
	if port == "" {
		port = "8050"
	}

	// Register routes
	http.HandleFunc("/health", corsMiddleware(handleHealth))

	// Admin endpoints
	http.HandleFunc("/admin/proxy-pool", corsMiddleware(handleProxyPool))
	http.HandleFunc("/admin/proxy-pool/", corsMiddleware(handleProxyPoolByID))
	http.HandleFunc("/admin/proxy-pool-config", corsMiddleware(handleProxyPoolConfig))
	http.HandleFunc("/admin/proxy-rotate-test", corsMiddleware(handleProxyRotateTest))
	http.HandleFunc("/admin/proxy-health-check", corsMiddleware(handleProxyHealthCheck))
	http.HandleFunc("/admin/proxy-reset-stats", corsMiddleware(handleProxyResetStats))
	http.HandleFunc("/admin/proxy-save", corsMiddleware(handleProxySave))
	http.HandleFunc("/admin/proxy-load", corsMiddleware(handleProxyLoad))

	// Client endpoints (for crawlers to use)
	http.HandleFunc("/proxy/next", corsMiddleware(handleGetNextProxy))
	http.HandleFunc("/proxy/record", corsMiddleware(handleRecordResult))
	http.HandleFunc("/proxy/captcha", corsMiddleware(handleRecordCaptcha))

	log.Printf("[IP-ROTATION] Server starting on port %s", port)
	log.Printf("[IP-ROTATION] Config: strategy=%s maxFailures=%d cooldown=%dm",
		globalIPPool.config.Strategy, globalIPPool.config.MaxFailures, globalIPPool.config.CooldownMinutes)

	if err := http.ListenAndServe(":"+port, nil); err != nil {
		log.Fatalf("[IP-ROTATION] Server failed: %v", err)
	}
}

```

---

## backend/browser-use/server.py

```py
"""
Browser-Use API Server with Human-in-the-Loop Support
FastAPI server that exposes browser-use functionality via HTTP API with real-time
human intervention capabilities.
"""

import asyncio
import base64
import os
import time
import uuid
import logging
from contextlib import asynccontextmanager
from dataclasses import dataclass, field
from typing import Optional, Any, Callable
from datetime import datetime
from enum import Enum

import httpx
from fastapi import FastAPI, HTTPException, BackgroundTasks, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

# Browser-use imports
from browser_use import Agent, Controller
from browser_use.browser import BrowserSession, BrowserProfile, ProxySettings
from browser_use.llm.aidove.chat import ChatAIDove

# Intent analyzer for guaranteed search results
from intent_analyzer import IntentAnalyzer, SearchGuarantee, ResultFusion

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


@dataclass
class ProxyInfo:
	id: str
	address: str
	protocol: str = 'http'
	username: Optional[str] = None
	password: Optional[str] = None
	country: Optional[str] = None
	health_status: Optional[str] = None


class ProxyRotationClient:
	def __init__(
		self,
		base_url: str,
		timeout: float = 5.0,
		enabled: bool = True,
	) -> None:
		self._base_url = base_url.rstrip('/')
		self._timeout = timeout
		self._enabled = enabled
		self._client: Optional[httpx.AsyncClient] = None
		self._lock = asyncio.Lock()

	async def _get_client(self) -> httpx.AsyncClient:
		async with self._lock:
			if self._client is None:
				self._client = httpx.AsyncClient(timeout=self._timeout)
			return self._client

	async def close(self) -> None:
		async with self._lock:
			if self._client is not None:
				await self._client.aclose()
				self._client = None

	async def health_check(self) -> bool:
		if not self._enabled:
			return False
		client = await self._get_client()
		try:
			resp = await client.get(f'{self._base_url}/health')
			return resp.status_code == 200
		except Exception:
			return False

	async def get_next_proxy(self) -> Optional[ProxyInfo]:
		if not self._enabled:
			return None
		client = await self._get_client()
		try:
			resp = await client.get(f'{self._base_url}/proxy/next')
			if resp.status_code != 200:
				return None

			data: Any = resp.json()
			if not isinstance(data, dict):
				return None

			proxy_id = data.get('proxyId') or data.get('proxy_id') or data.get('id')
			address = data.get('address')
			if not proxy_id or not address:
				return None

			return ProxyInfo(
				id=str(proxy_id),
				address=str(address),
				protocol=str(data.get('protocol') or 'http'),
				username=data.get('username'),
				password=data.get('password'),
				country=data.get('country'),
				health_status=data.get('healthStatus') or data.get('health_status'),
			)
		except Exception:
			return None

	async def record_success(self, proxy_id: str, latency_ms: int = 0) -> bool:
		return await self._record(proxy_id=proxy_id, success=True, latency_ms=latency_ms)

	async def record_failure(self, proxy_id: str, reason: str = '') -> bool:
		return await self._record(proxy_id=proxy_id, success=False, reason=reason)

	async def record_captcha(self, proxy_id: str, captcha_type: str = '') -> bool:
		if not self._enabled:
			return False
		client = await self._get_client()
		payload = {
			'proxyId': proxy_id,
			'type': captcha_type,
		}
		try:
			resp = await client.post(f'{self._base_url}/proxy/captcha', json=payload)
			return resp.status_code == 200
		except Exception:
			return False

	async def _record(
		self,
		proxy_id: str,
		success: bool,
		latency_ms: int = 0,
		reason: str = '',
	) -> bool:
		if not self._enabled:
			return False
		client = await self._get_client()
		payload = {
			'proxyId': proxy_id,
			'success': bool(success),
			'latencyMs': int(latency_ms),
			'reason': reason,
		}
		try:
			resp = await client.post(f'{self._base_url}/proxy/record', json=payload)
			return resp.status_code == 200
		except Exception:
			return False


def _env_bool(raw: str) -> bool:
	return raw.strip().lower() in {'1', 'true', 'yes', 'y', 'on'}


PROXY_ROTATION_ENABLED = _env_bool(os.environ.get('USE_PROXY_ROTATION', 'false'))
PROXY_ROTATION_URL = os.environ.get('PROXY_ROTATION_URL', 'http://ip-rotation:8050').rstrip('/')
PROXY_ROTATION_TIMEOUT_SECONDS = float(os.environ.get('PROXY_ROTATION_TIMEOUT_SECONDS', '5.0'))

proxy_rotation_client: Optional[ProxyRotationClient] = None


# ============================================
# Enums and Constants
# ============================================


class JobStatus(str, Enum):
	PENDING = 'pending'
	RUNNING = 'running'
	WAITING_HUMAN = 'waiting_human'  # Waiting for human intervention
	COMPLETED = 'completed'
	FAILED = 'failed'
	CANCELLED = 'cancelled'


class InterventionType(str, Enum):
	CAPTCHA = 'captcha'
	LOGIN = 'login'
	NAVIGATION = 'navigation'
	EXTRACTION = 'extraction'
	CONFIRMATION = 'confirmation'
	CUSTOM = 'custom'


# ============================================
# Request/Response Models
# ============================================


class BrowseRequest(BaseModel):
	"""Request to perform a browser automation task."""

	task: str = Field(..., description='The task for the AI agent to perform')
	url: Optional[str] = Field(None, description='Optional starting URL')
	session_id: Optional[str] = Field(None, description='Session ID for context continuity')
	max_steps: int = Field(25, description='Maximum number of steps', ge=1, le=100)
	timeout_seconds: int = Field(300, description='Timeout in seconds', ge=30, le=600)
	headless: bool = Field(False, description='Run browser in headless mode (False for human intervention)')
	enable_human_intervention: bool = Field(True, description='Allow human intervention when needed')
	auto_request_intervention: bool = Field(True, description='Automatically request intervention on issues')
	use_proxy_rotation: bool = Field(PROXY_ROTATION_ENABLED, description='Use proxy rotation via ip-rotation service')


class BrowseResponse(BaseModel):
	"""Response from a browser automation task."""

	job_id: str
	status: str
	message: str
	result: Optional[str] = None
	steps_taken: int = 0
	urls_visited: list[str] = []
	screenshots: list[str] = []
	error: Optional[str] = None
	started_at: Optional[str] = None
	completed_at: Optional[str] = None
	intervention_requested: bool = False
	intervention_type: Optional[str] = None


class InterventionRequest(BaseModel):
	"""Human intervention request details."""

	job_id: str
	intervention_type: InterventionType
	reason: str
	screenshot: Optional[str] = None  # Base64 encoded
	current_url: Optional[str] = None
	suggested_actions: list[str] = []
	timeout_seconds: int = Field(300, description='Timeout for human response')


class HumanAction(BaseModel):
	"""Human's response to an intervention request."""

	action_type: str = Field(..., description='Type of action: click, type, navigate, scroll, custom, skip, abort')
	selector: Optional[str] = Field(None, description='CSS selector for element interaction')
	value: Optional[str] = Field(None, description='Value for input/navigation')
	x: Optional[int] = Field(None, description='X coordinate for click')
	y: Optional[int] = Field(None, description='Y coordinate for click')
	custom_script: Optional[str] = Field(None, description='Custom JavaScript to execute')
	message: Optional[str] = Field(None, description='Message/feedback for the AI')


class JobStatusResponse(BaseModel):
	"""Status of a running job."""

	job_id: str
	status: str
	progress: float = 0.0
	current_step: int = 0
	max_steps: int = 25
	result: Optional[str] = None
	error: Optional[str] = None
	urls_visited: list[str] = []
	started_at: Optional[str] = None
	completed_at: Optional[str] = None
	# Human intervention fields
	intervention_requested: bool = False
	intervention_type: Optional[str] = None
	intervention_reason: Optional[str] = None
	intervention_screenshot: Optional[str] = None
	current_url: Optional[str] = None


class HealthResponse(BaseModel):
	"""Health check response."""

	status: str
	version: str
	uptime_seconds: float
	active_jobs: int
	waiting_intervention: int


# ============================================
# Job Management
# ============================================


@dataclass
class InterventionState:
	"""State for human intervention."""

	requested: bool = False
	type: Optional[InterventionType] = None
	reason: Optional[str] = None
	screenshot: Optional[str] = None
	current_url: Optional[str] = None
	suggested_actions: list[str] = field(default_factory=list)
	response: Optional[HumanAction] = None
	response_event: asyncio.Event = field(default_factory=asyncio.Event)
	timeout_seconds: int = 300


@dataclass
class Job:
	"""Represents a browser automation job."""

	id: str
	task: str
	url: Optional[str]
	session_id: Optional[str]
	max_steps: int
	timeout_seconds: int
	headless: bool
	enable_human_intervention: bool
	auto_request_intervention: bool
	use_proxy_rotation: bool
	proxy_id: Optional[str] = None
	proxy_address: Optional[str] = None
	status: JobStatus = JobStatus.PENDING
	progress: float = 0.0
	current_step: int = 0
	result: Optional[str] = None
	error: Optional[str] = None
	urls_visited: list = None
	screenshots: list = None
	started_at: Optional[datetime] = None
	completed_at: Optional[datetime] = None
	# Human intervention state
	intervention: InterventionState = None
	# Browser session reference
	browser_session: Optional[BrowserSession] = None
	# WebSocket connections for this job
	websocket_clients: list = None

	def __post_init__(self):
		if self.urls_visited is None:
			self.urls_visited = []
		if self.screenshots is None:
			self.screenshots = []
		if self.intervention is None:
			self.intervention = InterventionState()
		if self.websocket_clients is None:
			self.websocket_clients = []


# In-memory job storage (use Redis in production)
jobs: dict[str, Job] = {}
start_time = datetime.now()


# WebSocket connection manager
class ConnectionManager:
	def __init__(self):
		self.active_connections: dict[str, list[WebSocket]] = {}

	async def connect(self, websocket: WebSocket, job_id: str):
		await websocket.accept()
		if job_id not in self.active_connections:
			self.active_connections[job_id] = []
		self.active_connections[job_id].append(websocket)
		logger.info(f'WebSocket connected for job {job_id}')

	def disconnect(self, websocket: WebSocket, job_id: str):
		if job_id in self.active_connections:
			if websocket in self.active_connections[job_id]:
				self.active_connections[job_id].remove(websocket)
			if not self.active_connections[job_id]:
				del self.active_connections[job_id]
		logger.info(f'WebSocket disconnected for job {job_id}')

	async def broadcast(self, job_id: str, message: dict):
		if job_id in self.active_connections:
			disconnected = []
			for connection in self.active_connections[job_id]:
				try:
					await connection.send_json(message)
				except Exception:
					disconnected.append(connection)
			for conn in disconnected:
				self.disconnect(conn, job_id)


manager = ConnectionManager()


# ============================================
# Browser Agent Runner with Human Intervention
# ============================================


async def capture_screenshot(browser_session: Optional[BrowserSession]) -> Optional[str]:
	"""Capture a screenshot from the browser and return as base64."""
	try:
		if browser_session:
			# Use BrowserSession's take_screenshot method which returns bytes
			screenshot_bytes = await browser_session.take_screenshot()
			return base64.b64encode(screenshot_bytes).decode('utf-8')
	except Exception as e:
		logger.warning(f'Failed to capture screenshot: {e}')
	return None


async def get_current_url(browser_session: Optional[BrowserSession]) -> Optional[str]:
	"""Get the current page URL."""
	try:
		if browser_session:
			url = await browser_session.get_current_page_url()
			return url if url else None
	except Exception as e:
		logger.warning(f'Failed to get current URL: {e}')
	return None


async def _detect_captcha_on_page(browser_session: Optional[BrowserSession]) -> Optional[str]:
	"""
	Detect CAPTCHA on the current page by checking for common CAPTCHA indicators.

	Returns the type of CAPTCHA detected, or None if no CAPTCHA found.
	"""
	if not browser_session:
		return None

	try:
		context = getattr(browser_session, '_context', None)
		if not context:
			return None

		pages = context.pages
		if not pages:
			return None

		page = pages[0]

		# JavaScript to detect various CAPTCHA types
		detection_script = """
        () => {
            const indicators = [];
            
            // Check for reCAPTCHA
            if (document.querySelector('.g-recaptcha') || 
                document.querySelector('[data-sitekey]') ||
                document.querySelector('iframe[src*="recaptcha"]') ||
                document.querySelector('#recaptcha') ||
                document.querySelector('.recaptcha-checkbox')) {
                indicators.push('reCAPTCHA');
            }
            
            // Check for hCaptcha
            if (document.querySelector('.h-captcha') ||
                document.querySelector('[data-hcaptcha-widget-id]') ||
                document.querySelector('iframe[src*="hcaptcha"]')) {
                indicators.push('hCaptcha');
            }
            
            // Check for Cloudflare Turnstile
            if (document.querySelector('.cf-turnstile') ||
                document.querySelector('[data-turnstile-widget-id]') ||
                document.querySelector('iframe[src*="turnstile"]') ||
                document.querySelector('iframe[src*="challenges.cloudflare"]')) {
                indicators.push('Cloudflare Turnstile');
            }
            
            // Check for Cloudflare challenge page
            if (document.querySelector('#challenge-running') ||
                document.querySelector('#challenge-form') ||
                document.querySelector('.cf-browser-verification') ||
                document.title.includes('Just a moment') ||
                document.body.textContent.includes('Checking your browser') ||
                document.body.textContent.includes('Please wait while we verify')) {
                indicators.push('Cloudflare Challenge');
            }
            
            // Check for generic CAPTCHA indicators
            const bodyText = document.body.textContent.toLowerCase();
            if (bodyText.includes('verify you are human') ||
                bodyText.includes('prove you are human') ||
                bodyText.includes('robot verification') ||
                bodyText.includes('are you a robot') ||
                bodyText.includes('security check')) {
                indicators.push('Generic CAPTCHA');
            }
            
            // Check for FunCaptcha / Arkose Labs
            if (document.querySelector('#fc-iframe-wrap') ||
                document.querySelector('[data-fc-payload]') ||
                document.querySelector('iframe[src*="funcaptcha"]') ||
                document.querySelector('iframe[src*="arkoselabs"]')) {
                indicators.push('FunCaptcha');
            }
            
            // Check for access denied / blocked pages
            if (document.body.textContent.includes('Access Denied') ||
                document.body.textContent.includes('403 Forbidden') ||
                document.body.textContent.includes('Your access to this site has been limited')) {
                indicators.push('Access Blocked');
            }
            
            return indicators.length > 0 ? indicators.join(', ') : null;
        }
        """

		result = await page.evaluate(detection_script)
		return result

	except Exception as e:
		logger.warning(f'CAPTCHA detection failed: {e}')
		return None


async def request_human_intervention(
	job: Job, intervention_type: InterventionType, reason: str, suggested_actions: Optional[list[str]] = None
) -> Optional[HumanAction]:
	"""
	Request human intervention and wait for response.

	Returns the human's action or None if timeout/skip.
	"""
	if not job.enable_human_intervention:
		return None

	job.status = JobStatus.WAITING_HUMAN
	job.intervention.requested = True
	job.intervention.type = intervention_type
	job.intervention.reason = reason
	job.intervention.suggested_actions = suggested_actions or []
	job.intervention.response_event.clear()

	# Capture screenshot
	if job.browser_session:
		job.intervention.screenshot = await capture_screenshot(job.browser_session)
		job.intervention.current_url = await get_current_url(job.browser_session)

	# Notify via WebSocket
	await manager.broadcast(
		job.id,
		{
			'type': 'intervention_requested',
			'job_id': job.id,
			'intervention_type': intervention_type.value,
			'reason': reason,
			'screenshot': job.intervention.screenshot,
			'current_url': job.intervention.current_url,
			'suggested_actions': job.intervention.suggested_actions,
		},
	)

	logger.info(f'Job {job.id}: Requesting human intervention - {intervention_type.value}: {reason}')

	# Wait for human response with timeout
	try:
		await asyncio.wait_for(job.intervention.response_event.wait(), timeout=job.intervention.timeout_seconds)
		return job.intervention.response
	except asyncio.TimeoutError:
		logger.warning(f'Job {job.id}: Human intervention timed out')
		job.intervention.requested = False
		job.status = JobStatus.RUNNING
		return None


async def execute_human_action(job: Job, action: HumanAction) -> bool:
	"""Execute a human-specified action on the browser."""
	try:
		if not job.browser_session:
			return False

		context = getattr(job.browser_session, '_context', None)
		if not context:
			return False

		pages = context.pages
		if not pages:
			return False

		page = pages[0]

		if action.action_type == 'click':
			if action.selector:
				await page.click(action.selector)
			elif action.x is not None and action.y is not None:
				await page.mouse.click(action.x, action.y)
			else:
				return False

		elif action.action_type == 'type':
			if action.selector and action.value:
				await page.fill(action.selector, action.value)
			else:
				return False

		elif action.action_type == 'navigate':
			if action.value:
				await page.goto(action.value)
			else:
				return False

		elif action.action_type == 'scroll':
			if action.y:
				await page.evaluate(f'window.scrollBy(0, {action.y})')
			else:
				await page.evaluate('window.scrollBy(0, 500)')

		elif action.action_type == 'custom':
			if action.custom_script:
				await page.evaluate(action.custom_script)
			else:
				return False

		elif action.action_type == 'skip':
			# Just continue without action
			pass

		elif action.action_type == 'abort':
			return False  # Signal to abort

		else:
			logger.warning(f'Unknown action type: {action.action_type}')
			return False

		logger.info(f'Job {job.id}: Executed human action - {action.action_type}')
		return True

	except Exception as e:
		logger.error(f'Job {job.id}: Failed to execute human action: {e}')
		return False


def _extract_agent_result_text(result: Any) -> str:
	if result is None:
		return ''
	if isinstance(result, str):
		return result

	if hasattr(result, 'final_result'):
		final_result = getattr(result, 'final_result')
		try:
			text = final_result() if callable(final_result) else final_result
		except Exception:
			text = None
		if isinstance(text, str) and text.strip():
			return text

	if hasattr(result, 'extracted_content'):
		extracted_content = getattr(result, 'extracted_content')
		try:
			content = extracted_content() if callable(extracted_content) else extracted_content
		except Exception:
			content = None
		if isinstance(content, str) and content.strip():
			return content
		if isinstance(content, list):
			parts = [p.strip() for p in content if isinstance(p, str) and p.strip()]
			if parts:
				seen: set[str] = set()
				unique_parts: list[str] = []
				for p in parts:
					if p in seen:
						continue
					seen.add(p)
					unique_parts.append(p)
				return '\n\n'.join(unique_parts)

	history = getattr(result, 'history', None)
	if isinstance(history, list):
		for item in reversed(history):
			item_results = getattr(item, 'result', None)
			if isinstance(item_results, list):
				for r in reversed(item_results):
					extracted_content = getattr(r, 'extracted_content', None)
					if isinstance(extracted_content, str) and extracted_content.strip():
						return extracted_content

	return ''


async def run_browser_task(job: Job):
	"""Execute a browser automation task using browser-use with AI Dove and human intervention support."""
	job.status = JobStatus.RUNNING
	job.started_at = datetime.now()
	job_start_perf = time.perf_counter()
	proxy_info: Optional[ProxyInfo] = None
	screenshot_task_running = False
	broadcaster_task: Optional[asyncio.Task] = None

	try:
		# Initialize AI Dove LLM
		llm = ChatAIDove(session_id=job.session_id or f'browser-{job.id}', timeout=120.0, max_retries=3)

		# Configure browser profile - force headless in Docker environment
		# For human intervention with visible browser, need to run locally or use VNC
		import os

		is_docker = os.path.exists('/.dockerenv') or os.environ.get('DOCKER_CONTAINER', False)

		proxy_settings: Optional[ProxySettings] = None
		if job.use_proxy_rotation and proxy_rotation_client is not None:
			for attempt in range(3):
				proxy_info = await proxy_rotation_client.get_next_proxy()
				if proxy_info is not None:
					break
				await asyncio.sleep(0.2 * (attempt + 1))

			if proxy_info is not None:
				job.proxy_id = proxy_info.id
				job.proxy_address = proxy_info.address
				proxy_settings = ProxySettings(
					server=proxy_info.address,
					username=proxy_info.username,
					password=proxy_info.password,
				)
				logger.info(f'Job {job.id}: Using rotating proxy id={proxy_info.id} addr={proxy_info.address}')
			else:
				logger.warning(f'Job {job.id}: Proxy rotation enabled but no proxy available')

		profile_kwargs: dict[str, Any] = {
			'headless': True if is_docker else job.headless,
			'disable_security': True,
		}
		if proxy_settings is not None:
			profile_kwargs['proxy'] = proxy_settings

		profile = BrowserProfile(**profile_kwargs)

		# Create browser session
		job.browser_session = BrowserSession(browser_profile=profile)

		# Create controller
		controller = Controller()

		# ============================================
		# Intent Analysis for Guaranteed Search Results
		# ============================================
		# Analyze the task to extract keywords, generate fallback strategies,
		# and ensure search results are always returned
		intent_analyzer = IntentAnalyzer(llm=llm)
		search_guarantee = SearchGuarantee(intent_analyzer)

		# Analyze the task
		analyzed_intent = None
		try:
			# Use extract_topic=True to extract meaningful search topic from task description
			analyzed_intent = await intent_analyzer.analyze(job.task, use_llm=True, extract_topic=True)
			logger.info(
				f'Job {job.id}: Intent analysis complete - '
				f'keywords={analyzed_intent.keywords}, '
				f"primary='{analyzed_intent.primary_keyword}', "
				f"intent_type='{analyzed_intent.intent_type}', "
				f"search_topic='{analyzed_intent.original_query}', "
				f'strategies={len(analyzed_intent.fallback_strategies)}'
			)

			# Build enhanced task with fallback strategies
			full_task = search_guarantee.build_enhanced_task(analyzed_intent, job.task)
		except Exception as e:
			logger.warning(f'Job {job.id}: Intent analysis failed, using original task: {e}')
			full_task = job.task

		# Add URL prefix if provided
		if job.url:
			full_task = f'First navigate to {job.url}, then: {full_task}'

		# Enhanced task prompt for human intervention awareness
		if job.enable_human_intervention:
			full_task += """

IMPORTANT: If you encounter any of these situations, clearly state the issue:
- CAPTCHA or verification challenges (reCAPTCHA, hCaptcha, Cloudflare Turnstile)
- Login required
- Page not loading correctly  
- Cannot find expected content
- Need user confirmation before proceeding
- Any blocking issue

State the problem clearly so a human operator can assist."""

		# CAPTCHA detection patterns for automatic intervention
		captcha_patterns = [
			'recaptcha',
			'hcaptcha',
			'captcha',
			'turnstile',
			'challenge',
			'verification',
			'verify you are human',
			'cloudflare',
			'checking your browser',
			'access denied',
		]

		# Create agent
		agent = Agent(
			task=full_task,
			llm=llm,
			controller=controller,
			browser_session=job.browser_session,
			max_actions_per_step=10,
		)

		# Background task to periodically capture and broadcast screenshots
		screenshot_task_running = True

		async def screenshot_broadcaster():
			"""Periodically capture and broadcast screenshots while job is running."""
			step_count = 0
			captcha_check_count = 0
			while screenshot_task_running and job.status == JobStatus.RUNNING:
				try:
					# Capture current state
					current_url = await get_current_url(job.browser_session)
					if current_url and current_url not in job.urls_visited:
						job.urls_visited.append(current_url)

					screenshot = await capture_screenshot(job.browser_session)
					if screenshot:
						step_count += 1
						job.current_step = step_count
						job.progress = min(step_count / job.max_steps, 0.95)  # Cap at 95% until done

						await manager.broadcast(
							job.id,
							{
								'type': 'step_update',
								'job_id': job.id,
								'step': step_count,
								'progress': job.progress,
								'current_url': current_url,
								'screenshot': screenshot,
							},
						)
						logger.info(f'Job {job.id}: Broadcast screenshot (step {step_count})')

					# Auto-detect CAPTCHA and request human intervention
					if job.enable_human_intervention and job.auto_request_intervention:
						captcha_detected = await _detect_captcha_on_page(job.browser_session)
						if captcha_detected:
							captcha_check_count += 1
							# Only request intervention if CAPTCHA persists (avoid false positives)
							if captcha_check_count >= 2:
								if proxy_info is not None and proxy_rotation_client is not None:
									try:
										await proxy_rotation_client.record_captcha(proxy_info.id, captcha_detected)
									except Exception:
										pass
								logger.info(f'Job {job.id}: CAPTCHA detected, requesting human intervention')
								await request_human_intervention(
									job,
									InterventionType.CAPTCHA,
									f'CAPTCHA detected: {captcha_detected}. Please solve the verification challenge.',
									['Click on CAPTCHA', 'Solve verification', 'Skip if possible'],
								)
								captcha_check_count = 0  # Reset after requesting
						else:
							captcha_check_count = 0  # Reset if no CAPTCHA

				except Exception as e:
					logger.warning(f'Screenshot broadcast error: {e}')

				await asyncio.sleep(2)  # Capture every 2 seconds

		# Start screenshot broadcaster as background task
		broadcaster_task = asyncio.create_task(screenshot_broadcaster())

		# Run with timeout
		try:
			result = await asyncio.wait_for(agent.run(max_steps=job.max_steps), timeout=job.timeout_seconds)

			# Stop screenshot broadcaster
			screenshot_task_running = False
			broadcaster_task.cancel()
			try:
				await broadcaster_task
			except asyncio.CancelledError:
				pass

			# ============================================
			# Result Guarantee Logic - Never return "not found"
			# ============================================
			result_str = _extract_agent_result_text(result)

			# Check for "not found" patterns in multiple languages
			not_found_patterns = [
				'not found',
				'no results',
				'cannot find',
				'could not find',
				'nothing found',
				'no matches',
				'no information',
				'찾을 수 없',
				'결과가 없',
				'검색 결과 없',
				'찾지 못',
				'정보를 찾을 수 없',
				'결과를 찾을 수 없',
			]

			result_lower = result_str.lower()
			is_not_found = (not result_str.strip()) or any(pattern in result_lower for pattern in not_found_patterns)

			if is_not_found and analyzed_intent:
				logger.info(f"Job {job.id}: Initial result indicates 'not found', attempting fallback strategies...")

				# Try fallback strategies
				fallback_attempted = False
				for strategy_info in analyzed_intent.fallback_strategies[1:4]:  # Skip first (already tried), try next 3
					fallback_query = strategy_info['query']
					strategy_name = strategy_info['strategy'].value

					logger.info(f"Job {job.id}: Trying fallback strategy '{strategy_name}' with query: '{fallback_query}'")

					# Build a new task with the fallback query
					fallback_task = f"""
이전 검색에서 결과를 찾지 못했습니다. 다른 방법으로 검색해주세요.

새로운 검색어: "{fallback_query}"

중요 지침:
1. 위의 새로운 검색어로 검색하세요
2. 완전히 일치하지 않더라도 관련된 정보를 찾아서 제공하세요
3. 절대로 "찾을 수 없다" 또는 "결과가 없다"고 말하지 마세요
4. 대신 찾은 가장 관련성 있는 정보를 제공하세요

Previous search found no results. Please try a different approach.

New search query: "{fallback_query}"

Important instructions:
1. Search with the new query above
2. Even if not an exact match, find and provide related information
3. NEVER say "not found" or "no results"
4. Instead, provide the most relevant information you can find
"""

					try:
						# Create a new agent for fallback
						fallback_agent = Agent(
							task=fallback_task,
							llm=llm,
							controller=controller,
							browser_session=job.browser_session,
							max_actions_per_step=10,
						)

						# Run with reduced steps for fallback
						fallback_result = await asyncio.wait_for(
							fallback_agent.run(max_steps=min(10, job.max_steps // 2)), timeout=job.timeout_seconds // 3
						)

						fallback_str = _extract_agent_result_text(fallback_result)

						# Check if fallback found something
						fallback_is_not_found = (not fallback_str.strip()) or any(
							pattern in fallback_str.lower() for pattern in not_found_patterns
						)

						if not fallback_is_not_found and fallback_str:
							logger.info(f"Job {job.id}: Fallback strategy '{strategy_name}' found results!")
							result_str = f"""검색 결과 (대체 검색어: "{fallback_query}" 사용):
Search Results (using alternative query: "{fallback_query}"):

{fallback_str}

참고: 원래 검색어로는 결과를 찾지 못해 관련 검색어로 검색한 결과입니다.
Note: Original query returned no results, so alternative search was performed."""
							fallback_attempted = True
							break

					except asyncio.TimeoutError:
						logger.warning(f"Job {job.id}: Fallback strategy '{strategy_name}' timed out")
					except Exception as e:
						logger.warning(f"Job {job.id}: Fallback strategy '{strategy_name}' failed: {e}")

				# If all fallbacks failed, provide a helpful message instead of "not found"
				if not fallback_attempted:
					logger.info(f'Job {job.id}: All fallback strategies exhausted, providing helpful response')
					result_str = f"""검색 결과를 찾기 어려웠습니다. 다음을 시도해 보세요:
Search was challenging. Here are some suggestions:

시도한 검색어 / Queries attempted:
- {analyzed_intent.original_query}
- {analyzed_intent.primary_keyword}
{chr(10).join('- ' + s['query'] for s in analyzed_intent.fallback_strategies[1:4])}

추천 검색 방법 / Recommended approaches:
1. 검색어를 더 구체적으로 변경해 보세요 / Try more specific keywords
2. 다른 검색 엔진이나 사이트를 이용해 보세요 / Try different search engines or sites
3. 관련 키워드: {', '.join(analyzed_intent.keywords[:5])} / Related keywords

분석된 의도 / Analyzed intent:
- 주요 키워드 / Primary keyword: {analyzed_intent.primary_keyword}
- 검색 유형 / Search type: {analyzed_intent.intent_type}
- 언어 / Language: {analyzed_intent.language}"""

			job.result = result_str if result_str else 'Task completed successfully'
			job.status = JobStatus.COMPLETED
			job.progress = 1.0

			# Capture final screenshot
			final_screenshot = await capture_screenshot(job.browser_session)

			# Broadcast completion
			await manager.broadcast(
				job.id,
				{
					'type': 'completed',
					'job_id': job.id,
					'result': job.result,
					'urls_visited': job.urls_visited,
					'screenshot': final_screenshot,
				},
			)

		except asyncio.TimeoutError:
			if job.status != JobStatus.CANCELLED:
				screenshot_task_running = False
				if broadcaster_task is not None and not broadcaster_task.done():
					broadcaster_task.cancel()
					try:
						await broadcaster_task
					except asyncio.CancelledError:
						pass

				job.error = f'Task timed out after {job.timeout_seconds} seconds'
				job.status = JobStatus.FAILED

				await manager.broadcast(
					job.id,
					{
						'type': 'failed',
						'job_id': job.id,
						'error': job.error,
					},
				)

	except Exception as e:
		if job.status != JobStatus.CANCELLED:
			logger.exception(f'Job {job.id} failed: {e}')
			job.error = str(e)
			job.status = JobStatus.FAILED

			await manager.broadcast(
				job.id,
				{
					'type': 'failed',
					'job_id': job.id,
					'error': job.error,
				},
			)
		else:
			logger.info(f'Job {job.id} cancelled')

	finally:
		screenshot_task_running = False
		if broadcaster_task is not None and not broadcaster_task.done():
			broadcaster_task.cancel()
			try:
				await broadcaster_task
			except asyncio.CancelledError:
				pass

		if proxy_info is not None and proxy_rotation_client is not None and job.status in [JobStatus.COMPLETED, JobStatus.FAILED]:
			elapsed_ms = int((time.perf_counter() - job_start_perf) * 1000)
			if job.status == JobStatus.COMPLETED:
				await proxy_rotation_client.record_success(proxy_info.id, latency_ms=elapsed_ms)
			else:
				await proxy_rotation_client.record_failure(proxy_info.id, reason=(job.error or '')[:500])

		job.completed_at = datetime.now()

		# Clean up browser session
		if job.browser_session:
			try:
				await job.browser_session.stop()
			except Exception as e:
				logger.warning(f'Error closing browser session: {e}')
			job.browser_session = None


# ============================================
# FastAPI Application
# ============================================


@asynccontextmanager
async def lifespan(app: FastAPI):
	"""Application lifespan handler."""
	logger.info('Browser-Use API Server with Human-in-the-Loop starting up...')
	global proxy_rotation_client
	if PROXY_ROTATION_ENABLED:
		proxy_rotation_client = ProxyRotationClient(
			base_url=PROXY_ROTATION_URL,
			timeout=PROXY_ROTATION_TIMEOUT_SECONDS,
			enabled=True,
		)
		healthy = await proxy_rotation_client.health_check()
		logger.info(f'Proxy rotation enabled: url={PROXY_ROTATION_URL} healthy={healthy}')

	yield
	logger.info('Browser-Use API Server shutting down...')

	if proxy_rotation_client is not None:
		await proxy_rotation_client.close()
		proxy_rotation_client = None

	# Cancel all running jobs
	for job in jobs.values():
		if job.status in [JobStatus.RUNNING, JobStatus.WAITING_HUMAN]:
			job.status = JobStatus.CANCELLED
			job.error = 'Server shutdown'

			if job.browser_session:
				try:
					await job.browser_session.stop()
				except:
					pass


app = FastAPI(
	title='Browser-Use API with Human-in-the-Loop',
	description='AI-powered browser automation API with real-time human intervention support',
	version='2.0.0',
	lifespan=lifespan,
)

# CORS middleware
app.add_middleware(
	CORSMiddleware,
	allow_origins=['*'],
	allow_credentials=True,
	allow_methods=['*'],
	allow_headers=['*'],
)


# ============================================
# WebSocket Endpoints
# ============================================


@app.websocket('/ws/{job_id}')
async def websocket_endpoint(websocket: WebSocket, job_id: str):
	"""
	WebSocket endpoint for real-time job updates and human intervention.

	Message types received from client:
	- {"type": "intervention_response", "action": HumanAction}
	- {"type": "request_screenshot"}
	- {"type": "manual_intervention", "intervention_type": str, "reason": str}

	Message types sent to client:
	- {"type": "step_update", ...}
	- {"type": "intervention_requested", ...}
	- {"type": "completed", ...}
	- {"type": "failed", ...}
	- {"type": "screenshot", "data": base64_string}
	"""
	await manager.connect(websocket, job_id)

	try:
		while True:
			data = await websocket.receive_json()

			if job_id not in jobs:
				await websocket.send_json({'type': 'error', 'message': 'Job not found'})
				continue

			job = jobs[job_id]

			if data.get('type') == 'intervention_response':
				# Human provided a response to intervention request
				action_data = data.get('action', {})
				job.intervention.response = HumanAction(**action_data)
				job.intervention.response_event.set()
				job.intervention.requested = False
				job.status = JobStatus.RUNNING

				await websocket.send_json({'type': 'intervention_accepted', 'message': 'Action received and will be executed'})

			elif data.get('type') == 'request_screenshot':
				# Client requesting current screenshot
				if job.browser_session:
					screenshot = await capture_screenshot(job.browser_session)
					current_url = await get_current_url(job.browser_session)
					await websocket.send_json({'type': 'screenshot', 'data': screenshot, 'current_url': current_url})

			elif data.get('type') == 'manual_intervention':
				# Client manually requesting intervention mode
				intervention_type = InterventionType(data.get('intervention_type', 'custom'))
				reason = data.get('reason', 'Manual intervention requested')

				# Pause the agent and wait for human action
				action = await request_human_intervention(job, intervention_type, reason, data.get('suggested_actions', []))

				if action:
					success = await execute_human_action(job, action)
					await websocket.send_json({'type': 'intervention_result', 'success': success})

	except WebSocketDisconnect:
		manager.disconnect(websocket, job_id)
	except Exception as e:
		logger.error(f'WebSocket error for job {job_id}: {e}')
		manager.disconnect(websocket, job_id)


# ============================================
# REST API Endpoints
# ============================================


@app.get('/health', response_model=HealthResponse)
async def health_check():
	"""Health check endpoint."""
	uptime = (datetime.now() - start_time).total_seconds()
	active = sum(1 for j in jobs.values() if j.status in [JobStatus.RUNNING, JobStatus.PENDING])
	waiting = sum(1 for j in jobs.values() if j.status == JobStatus.WAITING_HUMAN)

	return HealthResponse(
		status='healthy', version='2.0.0', uptime_seconds=uptime, active_jobs=active, waiting_intervention=waiting
	)


@app.post('/browse', response_model=BrowseResponse)
async def browse(request: BrowseRequest, background_tasks: BackgroundTasks):
	"""
	Start a browser automation task with optional human intervention support.

	Connect to WebSocket at /ws/{job_id} to receive real-time updates and
	provide human intervention when requested.
	"""
	job_id = str(uuid.uuid4())[:8]

	job = Job(
		id=job_id,
		task=request.task,
		url=request.url,
		session_id=request.session_id,
		max_steps=request.max_steps,
		timeout_seconds=request.timeout_seconds,
		headless=request.headless,
		enable_human_intervention=request.enable_human_intervention,
		auto_request_intervention=request.auto_request_intervention,
		use_proxy_rotation=request.use_proxy_rotation,
	)

	jobs[job_id] = job

	# Start task in background
	background_tasks.add_task(run_browser_task, job)

	logger.info(f'Created job {job_id}: {request.task[:50]}... (intervention: {request.enable_human_intervention})')

	return BrowseResponse(
		job_id=job_id,
		status='pending',
		message='Task started. Connect to /ws/{job_id} for real-time updates and human intervention.',
		started_at=datetime.now().isoformat(),
	)


@app.get('/jobs/{job_id}', response_model=JobStatusResponse)
async def get_job_status(job_id: str):
	"""Get the status of a browser automation job including intervention state."""
	if job_id not in jobs:
		raise HTTPException(status_code=404, detail=f'Job {job_id} not found')

	job = jobs[job_id]

	return JobStatusResponse(
		job_id=job.id,
		status=job.status.value,
		progress=job.progress,
		current_step=job.current_step,
		max_steps=job.max_steps,
		result=job.result,
		error=job.error,
		urls_visited=job.urls_visited,
		started_at=job.started_at.isoformat() if job.started_at else None,
		completed_at=job.completed_at.isoformat() if job.completed_at else None,
		intervention_requested=job.intervention.requested,
		intervention_type=job.intervention.type.value if job.intervention.type else None,
		intervention_reason=job.intervention.reason,
		intervention_screenshot=job.intervention.screenshot,
		current_url=job.intervention.current_url,
	)


@app.post('/jobs/{job_id}/intervene')
async def submit_intervention(job_id: str, action: HumanAction):
	"""Submit a human intervention action for a job waiting for intervention."""
	if job_id not in jobs:
		raise HTTPException(status_code=404, detail=f'Job {job_id} not found')

	job = jobs[job_id]

	if job.status != JobStatus.WAITING_HUMAN:
		raise HTTPException(status_code=400, detail=f'Job is not waiting for intervention. Current status: {job.status.value}')

	job.intervention.response = action
	job.intervention.response_event.set()
	job.intervention.requested = False

	return {'message': 'Intervention action submitted', 'action_type': action.action_type}


@app.post('/jobs/{job_id}/request-intervention')
async def manual_intervention_request(
	job_id: str, intervention_type: InterventionType = InterventionType.CUSTOM, reason: str = 'Manual intervention requested'
):
	"""Manually request human intervention for a running job."""
	if job_id not in jobs:
		raise HTTPException(status_code=404, detail=f'Job {job_id} not found')

	job = jobs[job_id]

	if job.status != JobStatus.RUNNING:
		raise HTTPException(
			status_code=400, detail=f'Can only request intervention for running jobs. Current status: {job.status.value}'
		)

	# Set intervention state
	job.status = JobStatus.WAITING_HUMAN
	job.intervention.requested = True
	job.intervention.type = intervention_type
	job.intervention.reason = reason
	job.intervention.response_event.clear()

	# Capture current state
	if job.browser_session:
		job.intervention.screenshot = await capture_screenshot(job.browser_session)
		job.intervention.current_url = await get_current_url(job.browser_session)

	# Notify via WebSocket
	await manager.broadcast(
		job.id,
		{
			'type': 'intervention_requested',
			'job_id': job.id,
			'intervention_type': intervention_type.value,
			'reason': reason,
			'screenshot': job.intervention.screenshot,
			'current_url': job.intervention.current_url,
		},
	)

	return {
		'message': 'Intervention requested',
		'job_id': job_id,
		'intervention_type': intervention_type.value,
		'screenshot': job.intervention.screenshot,
		'current_url': job.intervention.current_url,
	}


@app.get('/jobs/{job_id}/screenshot')
async def get_screenshot(job_id: str):
	"""Get current screenshot from the browser session."""
	if job_id not in jobs:
		raise HTTPException(status_code=404, detail=f'Job {job_id} not found')

	job = jobs[job_id]

	if not job.browser_session:
		raise HTTPException(status_code=400, detail='No active browser session')

	screenshot = await capture_screenshot(job.browser_session)
	current_url = await get_current_url(job.browser_session)

	return {'screenshot': screenshot, 'current_url': current_url}


@app.delete('/jobs/{job_id}')
async def cancel_job(job_id: str):
	"""Cancel a running job."""
	if job_id not in jobs:
		raise HTTPException(status_code=404, detail=f'Job {job_id} not found')

	job = jobs[job_id]

	if job.status in [JobStatus.RUNNING, JobStatus.WAITING_HUMAN, JobStatus.PENDING]:
		job.status = JobStatus.CANCELLED
		job.error = 'Cancelled by user'
		job.completed_at = datetime.now()

		# Close browser session
		if job.browser_session:
			try:
				await job.browser_session.stop()
			except:
				pass
			job.browser_session = None

		# Notify via WebSocket
		await manager.broadcast(
			job.id,
			{
				'type': 'cancelled',
				'job_id': job.id,
			},
		)

		return {'message': f'Job {job_id} cancelled'}
	else:
		return {'message': f'Job {job_id} already finished with status: {job.status.value}'}


@app.get('/jobs')
async def list_jobs(status: Optional[str] = None, limit: int = 20):
	"""List all jobs, optionally filtered by status."""
	filtered_jobs = list(jobs.values())

	if status:
		filtered_jobs = [j for j in filtered_jobs if j.status.value == status]

	# Sort by started_at (most recent first)
	filtered_jobs.sort(key=lambda j: j.started_at or datetime.min, reverse=True)

	return [
		{
			'job_id': j.id,
			'task': j.task[:100],
			'status': j.status.value,
			'progress': j.progress,
			'intervention_requested': j.intervention.requested,
			'intervention_type': j.intervention.type.value if j.intervention.type else None,
			'started_at': j.started_at.isoformat() if j.started_at else None,
			'completed_at': j.completed_at.isoformat() if j.completed_at else None,
		}
		for j in filtered_jobs[:limit]
	]


@app.post('/browse/sync', response_model=BrowseResponse)
async def browse_sync(request: BrowseRequest):
	"""
	Execute a browser automation task synchronously.

	Note: Human intervention is not available in sync mode. For human intervention,
	use the async /browse endpoint with WebSocket connection.
	"""
	job_id = str(uuid.uuid4())[:8]

	# Disable human intervention for sync mode
	job = Job(
		id=job_id,
		task=request.task,
		url=request.url,
		session_id=request.session_id,
		max_steps=request.max_steps,
		timeout_seconds=request.timeout_seconds,
		headless=True,  # Force headless for sync mode
		enable_human_intervention=False,
		auto_request_intervention=False,
		use_proxy_rotation=request.use_proxy_rotation,
	)

	jobs[job_id] = job

	# Run synchronously
	await run_browser_task(job)

	return BrowseResponse(
		job_id=job_id,
		status=job.status.value,
		message=job.result or job.error or 'Task completed',
		result=job.result,
		steps_taken=job.current_step,
		urls_visited=job.urls_visited,
		screenshots=job.screenshots,
		error=job.error,
		started_at=job.started_at.isoformat() if job.started_at else None,
		completed_at=job.completed_at.isoformat() if job.completed_at else None,
	)


if __name__ == '__main__':
	import uvicorn

	uvicorn.run(app, host='0.0.0.0', port=8500)

```

---

## backend/browser-use/tests/agent_tasks/amazon_laptop.yaml

```yaml
name: Amazon Laptop Search
task: Go to amazon.com, search for 'laptop', and return the first result
judge_context:
  - The agent must navigate to amazon.com
  - The agent must search for 'laptop'
  - The agent must return name of the first laptop 
max_steps: 10

```

---

## backend/browser-use/tests/agent_tasks/browser_use_pip.yaml

```yaml
name: Find pip install command for browser-use
task: Find the pip installation command for the browser-use repo
judge_context:
  - The output must include the command ('pip install browser-use') 
max_steps: 10

```

---

## backend/browser-use/tests/ci/browser/iframe_template.html

```html
<!DOCTYPE html>
<html>
<head>
	<title>Same-Origin Iframe</title>
</head>
<body style="padding: 10px; background: #fff;">
	<h3>Same-Origin Iframe Content</h3>
	<button id="iframe-btn">Iframe Button</button>
	<input type="text" id="iframe-input" placeholder="Iframe input" />

	<script>
		// When button is clicked, increment counter in parent page using addEventListener
		document.getElementById('iframe-btn').addEventListener('click', function() {
			if (window.parent && window.parent !== window) {
				// Call parent's incrementCounter function
				window.parent.incrementCounter('Same-Origin Iframe');
			}
		});
	</script>
</body>
</html>

```

---

## backend/browser-use/tests/ci/browser/test_cdp_headers.py

```py
"""
Test that headers are properly passed to CDPClient for authenticated remote browser connections.

This tests the fix for: When using browser-use with remote browser services that require
authentication headers, these headers need to be included in the WebSocket handshake.
"""

from unittest.mock import AsyncMock, MagicMock, patch

import pytest

from browser_use.browser.profile import BrowserProfile
from browser_use.browser.session import BrowserSession


def test_browser_profile_headers_attribute():
	"""Test that BrowserProfile correctly stores headers attribute."""
	test_headers = {'Authorization': 'Bearer token123', 'X-API-Key': 'key456'}

	profile = BrowserProfile(headers=test_headers)

	# Verify headers are stored correctly
	assert profile.headers == test_headers

	# Test with profile without headers
	profile_no_headers = BrowserProfile()
	assert profile_no_headers.headers is None


def test_browser_profile_headers_inherited():
	"""Test that BrowserSession can access headers from its profile."""
	test_headers = {'Authorization': 'Bearer test-token'}

	session = BrowserSession(cdp_url='wss://example.com/cdp', headers=test_headers)

	assert session.browser_profile.headers == test_headers


@pytest.mark.asyncio
async def test_cdp_client_headers_passed_on_connect():
	"""Test that headers from BrowserProfile are passed to CDPClient on connect()."""
	test_headers = {
		'Authorization': 'AWS4-HMAC-SHA256 Credential=test...',
		'X-Amz-Date': '20250914T163733Z',
		'X-Amz-Security-Token': 'test-token',
		'Host': 'remote-browser.example.com',
	}

	session = BrowserSession(cdp_url='wss://remote-browser.example.com/cdp', headers=test_headers)

	with patch('browser_use.browser.session.CDPClient') as mock_cdp_client_class:
		# Setup mock CDPClient instance
		mock_cdp_client = AsyncMock()
		mock_cdp_client_class.return_value = mock_cdp_client
		mock_cdp_client.start = AsyncMock()
		mock_cdp_client.stop = AsyncMock()

		# Mock CDP methods
		mock_cdp_client.send = MagicMock()
		mock_cdp_client.send.Target = MagicMock()
		mock_cdp_client.send.Target.setAutoAttach = AsyncMock()
		mock_cdp_client.send.Target.getTargets = AsyncMock(return_value={'targetInfos': []})
		mock_cdp_client.send.Target.createTarget = AsyncMock(return_value={'targetId': 'test-target-id'})

		# Mock SessionManager (imported inside connect() from browser_use.browser.session_manager)
		with patch('browser_use.browser.session_manager.SessionManager') as mock_session_manager_class:
			mock_session_manager = MagicMock()
			mock_session_manager_class.return_value = mock_session_manager
			mock_session_manager.start_monitoring = AsyncMock()
			mock_session_manager.get_all_page_targets = MagicMock(return_value=[])

			try:
				await session.connect()
			except Exception:
				# May fail due to incomplete mocking, but we can still verify the key assertion
				pass

			# Verify CDPClient was instantiated with the headers
			mock_cdp_client_class.assert_called_once()
			call_kwargs = mock_cdp_client_class.call_args

			# Check positional args and keyword args
			assert call_kwargs[0][0] == 'wss://remote-browser.example.com/cdp', 'CDP URL should be first arg'
			assert call_kwargs[1].get('additional_headers') == test_headers, 'Headers should be passed as additional_headers'
			assert call_kwargs[1].get('max_ws_frame_size') == 200 * 1024 * 1024, 'max_ws_frame_size should be set'


@pytest.mark.asyncio
async def test_cdp_client_no_headers_when_none():
	"""Test that CDPClient is created with None headers when profile has no headers."""
	session = BrowserSession(cdp_url='wss://example.com/cdp')

	assert session.browser_profile.headers is None

	with patch('browser_use.browser.session.CDPClient') as mock_cdp_client_class:
		mock_cdp_client = AsyncMock()
		mock_cdp_client_class.return_value = mock_cdp_client
		mock_cdp_client.start = AsyncMock()
		mock_cdp_client.stop = AsyncMock()
		mock_cdp_client.send = MagicMock()
		mock_cdp_client.send.Target = MagicMock()
		mock_cdp_client.send.Target.setAutoAttach = AsyncMock()
		mock_cdp_client.send.Target.getTargets = AsyncMock(return_value={'targetInfos': []})
		mock_cdp_client.send.Target.createTarget = AsyncMock(return_value={'targetId': 'test-target-id'})

		with patch('browser_use.browser.session_manager.SessionManager') as mock_session_manager_class:
			mock_session_manager = MagicMock()
			mock_session_manager_class.return_value = mock_session_manager
			mock_session_manager.start_monitoring = AsyncMock()
			mock_session_manager.get_all_page_targets = MagicMock(return_value=[])

			try:
				await session.connect()
			except Exception:
				pass

			# Verify CDPClient was called with None for additional_headers
			call_kwargs = mock_cdp_client_class.call_args
			assert call_kwargs[1].get('additional_headers') is None


@pytest.mark.asyncio
async def test_headers_used_for_json_version_endpoint():
	"""Test that headers are also used when fetching WebSocket URL from /json/version."""
	test_headers = {'Authorization': 'Bearer test-token'}

	# Use HTTP URL (not ws://) to trigger /json/version fetch
	session = BrowserSession(cdp_url='http://remote-browser.example.com:9222', headers=test_headers)

	with patch('browser_use.browser.session.httpx.AsyncClient') as mock_client_class:
		mock_client = AsyncMock()
		mock_client_class.return_value.__aenter__ = AsyncMock(return_value=mock_client)
		mock_client_class.return_value.__aexit__ = AsyncMock()

		# Mock the /json/version response
		mock_response = MagicMock()
		mock_response.json.return_value = {'webSocketDebuggerUrl': 'ws://remote-browser.example.com:9222/devtools/browser/abc'}
		mock_client.get = AsyncMock(return_value=mock_response)

		with patch('browser_use.browser.session.CDPClient') as mock_cdp_client_class:
			mock_cdp_client = AsyncMock()
			mock_cdp_client_class.return_value = mock_cdp_client
			mock_cdp_client.start = AsyncMock()
			mock_cdp_client.send = MagicMock()
			mock_cdp_client.send.Target = MagicMock()
			mock_cdp_client.send.Target.setAutoAttach = AsyncMock()

			with patch('browser_use.browser.session_manager.SessionManager') as mock_sm_class:
				mock_sm = MagicMock()
				mock_sm_class.return_value = mock_sm
				mock_sm.start_monitoring = AsyncMock()
				mock_sm.get_all_page_targets = MagicMock(return_value=[])

				try:
					await session.connect()
				except Exception:
					pass

				# Verify headers were passed to the HTTP GET request
				mock_client.get.assert_called_once()
				call_kwargs = mock_client.get.call_args
				assert call_kwargs[1].get('headers') == test_headers

```

---

## backend/browser-use/tests/ci/browser/test_cloud_browser.py

```py
"""Tests for cloud browser functionality."""

import tempfile
from pathlib import Path
from unittest.mock import AsyncMock, patch

import pytest

from browser_use.browser.cloud.cloud import (
	CloudBrowserAuthError,
	CloudBrowserClient,
	CloudBrowserError,
)
from browser_use.browser.cloud.views import CreateBrowserRequest
from browser_use.browser.profile import BrowserProfile
from browser_use.browser.session import BrowserSession
from browser_use.sync.auth import CloudAuthConfig


@pytest.fixture
def temp_config_dir(monkeypatch):
	"""Create temporary config directory."""
	with tempfile.TemporaryDirectory() as tmpdir:
		temp_dir = Path(tmpdir) / '.config' / 'browseruse'
		temp_dir.mkdir(parents=True, exist_ok=True)

		# Use monkeypatch to set the environment variable
		monkeypatch.setenv('BROWSER_USE_CONFIG_DIR', str(temp_dir))

		yield temp_dir


@pytest.fixture
def mock_auth_config(temp_config_dir):
	"""Create a mock auth config with valid token."""
	auth_config = CloudAuthConfig(api_token='test-token', user_id='test-user-id', authorized_at=None)
	auth_config.save_to_file()
	return auth_config


class TestCloudBrowserClient:
	"""Test CloudBrowserClient class."""

	async def test_create_browser_success(self, mock_auth_config, monkeypatch):
		"""Test successful cloud browser creation."""

		# Clear environment variable so test uses mock_auth_config
		monkeypatch.delenv('BROWSER_USE_API_KEY', raising=False)

		# Mock response data matching the API
		mock_response_data = {
			'id': 'test-browser-id',
			'status': 'active',
			'liveUrl': 'https://live.browser-use.com?wss=test',
			'cdpUrl': 'wss://test.proxy.daytona.works',
			'timeoutAt': '2025-09-17T04:35:36.049892',
			'startedAt': '2025-09-17T03:35:36.049974',
			'finishedAt': None,
		}

		# Mock the httpx client
		with patch('httpx.AsyncClient') as mock_client_class:
			mock_response = AsyncMock()
			mock_response.status_code = 201
			mock_response.is_success = True
			mock_response.json = lambda: mock_response_data

			mock_client = AsyncMock()
			mock_client.post.return_value = mock_response
			mock_client_class.return_value = mock_client

			client = CloudBrowserClient()
			client.client = mock_client

			result = await client.create_browser(CreateBrowserRequest())

			assert result.id == 'test-browser-id'
			assert result.status == 'active'
			assert result.cdpUrl == 'wss://test.proxy.daytona.works'

			# Verify auth headers were included
			mock_client.post.assert_called_once()
			call_args = mock_client.post.call_args
			assert 'X-Browser-Use-API-Key' in call_args.kwargs['headers']
			assert call_args.kwargs['headers']['X-Browser-Use-API-Key'] == 'test-token'

	async def test_create_browser_auth_error(self, temp_config_dir, monkeypatch):
		"""Test cloud browser creation with auth error."""

		# Clear environment variable and don't create auth config - should trigger auth error
		monkeypatch.delenv('BROWSER_USE_API_KEY', raising=False)

		client = CloudBrowserClient()

		with pytest.raises(CloudBrowserAuthError) as exc_info:
			await client.create_browser(CreateBrowserRequest())

		assert 'BROWSER_USE_API_KEY environment variable' in str(exc_info.value)

	async def test_create_browser_http_401(self, mock_auth_config, monkeypatch):
		"""Test cloud browser creation with HTTP 401 response."""

		# Clear environment variable so test uses mock_auth_config
		monkeypatch.delenv('BROWSER_USE_API_KEY', raising=False)

		with patch('httpx.AsyncClient') as mock_client_class:
			mock_response = AsyncMock()
			mock_response.status_code = 401
			mock_response.is_success = False

			mock_client = AsyncMock()
			mock_client.post.return_value = mock_response
			mock_client_class.return_value = mock_client

			client = CloudBrowserClient()
			client.client = mock_client

			with pytest.raises(CloudBrowserAuthError) as exc_info:
				await client.create_browser(CreateBrowserRequest())

			assert 'Authentication failed' in str(exc_info.value)

	async def test_create_browser_with_env_var(self, temp_config_dir, monkeypatch):
		"""Test cloud browser creation using BROWSER_USE_API_KEY environment variable."""

		# Set environment variable
		monkeypatch.setenv('BROWSER_USE_API_KEY', 'env-test-token')

		# Mock response data matching the API
		mock_response_data = {
			'id': 'test-browser-id',
			'status': 'active',
			'liveUrl': 'https://live.browser-use.com?wss=test',
			'cdpUrl': 'wss://test.proxy.daytona.works',
			'timeoutAt': '2025-09-17T04:35:36.049892',
			'startedAt': '2025-09-17T03:35:36.049974',
			'finishedAt': None,
		}

		with patch('httpx.AsyncClient') as mock_client_class:
			mock_response = AsyncMock()
			mock_response.status_code = 201
			mock_response.is_success = True
			mock_response.json = lambda: mock_response_data

			mock_client = AsyncMock()
			mock_client.post.return_value = mock_response
			mock_client_class.return_value = mock_client

			client = CloudBrowserClient()
			client.client = mock_client

			result = await client.create_browser(CreateBrowserRequest())

			assert result.id == 'test-browser-id'
			assert result.status == 'active'
			assert result.cdpUrl == 'wss://test.proxy.daytona.works'

			# Verify environment variable was used
			mock_client.post.assert_called_once()
			call_args = mock_client.post.call_args
			assert 'X-Browser-Use-API-Key' in call_args.kwargs['headers']
			assert call_args.kwargs['headers']['X-Browser-Use-API-Key'] == 'env-test-token'

	async def test_stop_browser_success(self, mock_auth_config, monkeypatch):
		"""Test successful cloud browser session stop."""

		# Clear environment variable so test uses mock_auth_config
		monkeypatch.delenv('BROWSER_USE_API_KEY', raising=False)

		# Mock response data for stop
		mock_response_data = {
			'id': 'test-browser-id',
			'status': 'stopped',
			'liveUrl': 'https://live.browser-use.com?wss=test',
			'cdpUrl': 'wss://test.proxy.daytona.works',
			'timeoutAt': '2025-09-17T04:35:36.049892',
			'startedAt': '2025-09-17T03:35:36.049974',
			'finishedAt': '2025-09-17T04:35:36.049892',
		}

		with patch('httpx.AsyncClient') as mock_client_class:
			mock_response = AsyncMock()
			mock_response.status_code = 200
			mock_response.is_success = True
			mock_response.json = lambda: mock_response_data

			mock_client = AsyncMock()
			mock_client.patch.return_value = mock_response
			mock_client_class.return_value = mock_client

			client = CloudBrowserClient()
			client.client = mock_client
			client.current_session_id = 'test-browser-id'

			result = await client.stop_browser()

			assert result.id == 'test-browser-id'
			assert result.status == 'stopped'
			assert result.finishedAt is not None

			# Verify correct API call
			mock_client.patch.assert_called_once()
			call_args = mock_client.patch.call_args
			assert 'test-browser-id' in call_args.args[0]  # URL contains session ID
			assert call_args.kwargs['json'] == {'action': 'stop'}
			assert 'X-Browser-Use-API-Key' in call_args.kwargs['headers']

	async def test_stop_browser_session_not_found(self, mock_auth_config, monkeypatch):
		"""Test stopping a browser session that doesn't exist."""

		# Clear environment variable so test uses mock_auth_config
		monkeypatch.delenv('BROWSER_USE_API_KEY', raising=False)

		with patch('httpx.AsyncClient') as mock_client_class:
			mock_response = AsyncMock()
			mock_response.status_code = 404
			mock_response.is_success = False

			mock_client = AsyncMock()
			mock_client.patch.return_value = mock_response
			mock_client_class.return_value = mock_client

			client = CloudBrowserClient()
			client.client = mock_client

			with pytest.raises(CloudBrowserError) as exc_info:
				await client.stop_browser('nonexistent-session')

			assert 'not found' in str(exc_info.value)


class TestBrowserSessionCloudIntegration:
	"""Test BrowserSession integration with cloud browsers."""

	async def test_cloud_browser_profile_property(self):
		"""Test that cloud_browser property works correctly."""

		# Just test the profile and session properties without connecting
		profile = BrowserProfile(use_cloud=True)
		session = BrowserSession(browser_profile=profile, cdp_url='ws://mock-url')  # Provide CDP URL to avoid connection

		assert session.cloud_browser is True
		assert session.browser_profile.use_cloud is True

	async def test_browser_session_cloud_browser_logic(self, mock_auth_config, monkeypatch):
		"""Test that cloud browser profile settings work correctly."""

		# Clear environment variable so test uses mock_auth_config
		monkeypatch.delenv('BROWSER_USE_API_KEY', raising=False)

		# Test cloud browser profile creation
		profile = BrowserProfile(use_cloud=True)
		assert profile.use_cloud is True

		# Test that BrowserSession respects cloud_browser setting
		# Provide CDP URL to avoid actual connection attempts
		session = BrowserSession(browser_profile=profile, cdp_url='ws://mock-url')
		assert session.cloud_browser is True

```

---

## backend/browser-use/tests/ci/browser/test_cross_origin_click.py

```py
"""Test clicking elements inside cross-origin iframes."""

import asyncio

import pytest

from browser_use.browser.profile import BrowserProfile, ViewportSize
from browser_use.browser.session import BrowserSession
from browser_use.tools.service import Tools


@pytest.fixture
async def browser_session():
	"""Create browser session with cross-origin iframe support."""
	session = BrowserSession(
		browser_profile=BrowserProfile(
			headless=True,
			user_data_dir=None,
			keep_alive=True,
			window_size=ViewportSize(width=1920, height=1400),
			cross_origin_iframes=True,  # Enable cross-origin iframe extraction
		)
	)
	await session.start()
	yield session
	await session.kill()


class TestCrossOriginIframeClick:
	"""Test clicking elements inside cross-origin iframes."""

	async def test_click_element_in_cross_origin_iframe(self, httpserver, browser_session: BrowserSession):
		"""Verify that elements inside iframes in different CDP targets can be clicked."""

		# Create iframe content with clickable elements
		iframe_html = """
		<!DOCTYPE html>
		<html>
		<head><title>Iframe Page</title></head>
		<body>
			<h1>Iframe Content</h1>
			<a href="https://test-domain.example/page" id="iframe-link">Test Link</a>
			<button id="iframe-button">Iframe Button</button>
		</body>
		</html>
		"""

		# Create main page with iframe pointing to our test server
		main_html = """
		<!DOCTYPE html>
		<html>
		<head><title>Multi-Target Test</title></head>
		<body>
			<h1>Main Page</h1>
			<button id="main-button">Main Button</button>
			<iframe id="test-iframe" src="/iframe-content" style="width: 800px; height: 600px;"></iframe>
		</body>
		</html>
		"""

		# Serve both pages
		httpserver.expect_request('/multi-target-test').respond_with_data(main_html, content_type='text/html')
		httpserver.expect_request('/iframe-content').respond_with_data(iframe_html, content_type='text/html')
		url = httpserver.url_for('/multi-target-test')

		# Navigate to the page
		await browser_session.navigate_to(url)

		# Wait for iframe to load
		await asyncio.sleep(2)

		# Get DOM state with cross-origin iframe extraction enabled
		# Use browser_session.get_browser_state_summary() instead of directly creating DomService
		# This goes through the proper event bus and watchdog system
		browser_state = await browser_session.get_browser_state_summary(
			include_screenshot=False,
			include_recent_events=False,
		)
		assert browser_state.dom_state is not None
		state = browser_state.dom_state

		print(f'\n📊 Found {len(state.selector_map)} total elements')

		# Find elements from different targets
		targets_found = set()
		main_page_elements = []
		iframe_elements = []

		for idx, element in state.selector_map.items():
			target_id = element.target_id
			targets_found.add(target_id)

			# Check if element is from iframe (identified by id attributes we set)
			# Iframe elements will have a different target_id when cross_origin_iframes=True
			if element.attributes:
				element_id = element.attributes.get('id', '')
				if element_id in ('iframe-link', 'iframe-button'):
					iframe_elements.append((idx, element))
					print(f'   ✅ Found iframe element: [{idx}] {element.tag_name} id={element_id}')
				elif element_id == 'main-button':
					main_page_elements.append((idx, element))

		# Verify we found elements from at least 2 different targets
		print(f'\n🎯 Found elements from {len(targets_found)} different CDP targets')

		# Check if iframe elements were found
		if len(iframe_elements) == 0:
			pytest.fail('Expected to find at least one element from iframe, but found none')

		# Verify we found at least one element from the iframe
		assert len(iframe_elements) > 0, 'Expected to find at least one element from iframe'

		# Try clicking the iframe element
		print('\n🖱️  Testing Click on Iframe Element:')
		tools = Tools()

		link_idx, link_element = iframe_elements[0]
		print(f'   Attempting to click element [{link_idx}] from iframe...')

		try:
			result = await tools.click(index=link_idx, browser_session=browser_session)

			# Check for errors
			if result.error:
				pytest.fail(f'Click on iframe element [{link_idx}] failed with error: {result.error}')

			if result.extracted_content and (
				'not available' in result.extracted_content.lower() or 'failed' in result.extracted_content.lower()
			):
				pytest.fail(f'Click on iframe element [{link_idx}] failed: {result.extracted_content}')

			print(f'   ✅ Click succeeded on iframe element [{link_idx}]!')
			print('   🎉 Iframe element clicking works!')

		except Exception as e:
			pytest.fail(f'Exception while clicking iframe element [{link_idx}]: {e}')

		print('\n✅ Test passed: Iframe elements can be clicked')

```

---

## backend/browser-use/tests/ci/browser/test_dom_serializer.py

```py
"""
Test DOM serializer with complex scenarios: shadow DOM, same-origin and cross-origin iframes.

This test verifies that the DOM serializer correctly:
1. Extracts interactive elements from shadow DOM
2. Processes same-origin iframes
3. Handles cross-origin iframes (should be blocked)
4. Generates correct selector_map with expected element counts

Usage:
	uv run pytest tests/ci/browser/test_dom_serializer.py -v -s
"""

import pytest
from pytest_httpserver import HTTPServer

from browser_use.agent.service import Agent
from browser_use.browser import BrowserSession
from browser_use.browser.profile import BrowserProfile, ViewportSize
from tests.ci.conftest import create_mock_llm


@pytest.fixture(scope='session')
def http_server():
	"""Create and provide a test HTTP server for DOM serializer tests."""
	from pathlib import Path

	server = HTTPServer()
	server.start()

	# Load HTML templates from files
	test_dir = Path(__file__).parent
	main_page_html = (test_dir / 'test_page_template.html').read_text()
	iframe_html = (test_dir / 'iframe_template.html').read_text()
	stacked_page_html = (test_dir / 'test_page_stacked_template.html').read_text()

	# Route 1: Main page with shadow DOM and iframes
	server.expect_request('/dom-test-main').respond_with_data(main_page_html, content_type='text/html')

	# Route 2: Same-origin iframe content
	server.expect_request('/iframe-same-origin').respond_with_data(iframe_html, content_type='text/html')

	# Route 3: Stacked complex scenarios test page
	server.expect_request('/stacked-test').respond_with_data(stacked_page_html, content_type='text/html')

	yield server
	server.stop()


@pytest.fixture(scope='session')
def base_url(http_server):
	"""Return the base URL for the test HTTP server."""
	return f'http://{http_server.host}:{http_server.port}'


@pytest.fixture(scope='function')
async def browser_session():
	"""Create a browser session for DOM serializer tests."""
	session = BrowserSession(
		browser_profile=BrowserProfile(
			headless=True,
			user_data_dir=None,
			keep_alive=True,
			window_size=ViewportSize(width=1920, height=1400),  # Taller window to fit all stacked elements
			cross_origin_iframes=True,  # Enable cross-origin iframe extraction via CDP target switching
		)
	)
	await session.start()
	yield session
	await session.kill()


class TestDOMSerializer:
	"""Test DOM serializer with complex scenarios."""

	async def test_dom_serializer_with_shadow_dom_and_iframes(self, browser_session, base_url):
		"""Test DOM serializer extracts elements from shadow DOM, same-origin iframes, and cross-origin iframes.

		This test verifies:
		1. Elements are in the serializer (selector_map)
		2. We can click elements using click(index)

		Expected interactive elements:
		- Regular DOM: 3 elements (button, input, link on main page)
		- Shadow DOM: 3 elements (2 buttons, 1 input inside shadow root)
		- Same-origin iframe: 2 elements (button, input inside iframe)
		- Cross-origin iframe placeholder: about:blank (no interactive elements)
		- Iframe tags: 2 elements (the iframe elements themselves)
		Total: ~10 interactive elements
		"""
		from browser_use.tools.service import Tools

		tools = Tools()

		# Create mock LLM actions that will click elements from each category
		# We'll generate actions dynamically after we know the indices
		actions = [
			f"""
			{{
				"thinking": "I'll navigate to the DOM test page",
				"evaluation_previous_goal": "Starting task",
				"memory": "Navigating to test page",
				"next_goal": "Navigate to test page",
				"action": [
					{{
						"navigate": {{
							"url": "{base_url}/dom-test-main",
							"new_tab": false
						}}
					}}
				]
			}}
			"""
		]
		await tools.navigate(url=f'{base_url}/dom-test-main', new_tab=False, browser_session=browser_session)

		import asyncio

		await asyncio.sleep(1)

		# Get the browser state to access selector_map
		browser_state_summary = await browser_session.get_browser_state_summary(
			include_screenshot=False,
			include_recent_events=False,
		)

		assert browser_state_summary is not None, 'Browser state summary should not be None'
		assert browser_state_summary.dom_state is not None, 'DOM state should not be None'

		selector_map = browser_state_summary.dom_state.selector_map
		print(f'   Selector map: {selector_map.keys()}')

		print('\n📊 DOM Serializer Analysis:')
		print(f'   Total interactive elements found: {len(selector_map)}')
		serilized_text = browser_state_summary.dom_state.llm_representation()
		print(f'   Serialized text: {serilized_text}')
		# assume all selector map keys are as text in the serialized text
		# for idx, element in selector_map.items():
		# 	assert str(idx) in serilized_text, f'Element {idx} should be in serialized text'
		# 	print(f'   ✓ Element {idx} found in serialized text')

		# assume at least 10 interactive elements are in the selector map
		assert len(selector_map) >= 10, f'Should find at least 10 interactive elements, found {len(selector_map)}'

		# assert all interactive elements marked with [123] from serialized text are in selector map
		# find all [index] from serialized text with regex
		import re

		indices = re.findall(r'\[(\d+)\]', serilized_text)
		for idx in indices:
			assert int(idx) in selector_map.keys(), f'Element {idx} should be in selector map'
			print(f'   ✓ Element {idx} found in selector map')

		regular_elements = []
		shadow_elements = []
		iframe_content_elements = []
		iframe_tags = []

		# Categorize elements by their IDs (more stable than hardcoded indices)
		# Check element attributes to identify their location
		for idx, element in selector_map.items():
			# Check if this is an iframe tag (not content inside iframe)
			if element.tag_name == 'iframe':
				iframe_tags.append((idx, element))
			# Check if element has an ID attribute
			elif hasattr(element, 'attributes') and 'id' in element.attributes:
				elem_id = element.attributes['id'].lower()
				# Shadow DOM elements have IDs starting with "shadow-"
				if elem_id.startswith('shadow-'):
					shadow_elements.append((idx, element))
				# Iframe content elements have IDs starting with "iframe-"
				elif elem_id.startswith('iframe-'):
					iframe_content_elements.append((idx, element))
				# Everything else is regular DOM
				else:
					regular_elements.append((idx, element))
			# Elements without IDs are regular DOM
			else:
				regular_elements.append((idx, element))

		# Verify element counts based on our test page structure:
		# - Regular DOM: 3-4 elements (button, input, link on main page + possible cross-origin content)
		# - Shadow DOM: 3 elements (2 buttons, 1 input inside shadow root)
		# - Iframe content: 2 elements (button, input from same-origin iframe)
		# - Iframe tags: 2 elements (the iframe elements themselves)
		# Total: ~10-11 interactive elements depending on cross-origin iframe extraction

		print('\n✅ DOM Serializer Test Summary:')
		print(f'   • Regular DOM: {len(regular_elements)} elements {"✓" if len(regular_elements) >= 3 else "✗"}')
		print(f'   • Shadow DOM: {len(shadow_elements)} elements {"✓" if len(shadow_elements) >= 3 else "✗"}')
		print(
			f'   • Same-origin iframe content: {len(iframe_content_elements)} elements {"✓" if len(iframe_content_elements) >= 2 else "✗"}'
		)
		print(f'   • Iframe tags: {len(iframe_tags)} elements {"✓" if len(iframe_tags) >= 2 else "✗"}')
		print(f'   • Total elements: {len(selector_map)}')

		# Verify we found elements from all sources
		assert len(selector_map) >= 8, f'Should find at least 8 interactive elements, found {len(selector_map)}'
		assert len(regular_elements) >= 1, f'Should find at least 1 regular DOM element, found {len(regular_elements)}'
		assert len(shadow_elements) >= 1, f'Should find at least 1 shadow DOM element, found {len(shadow_elements)}'
		assert len(iframe_content_elements) >= 1, (
			f'Should find at least 1 iframe content element, found {len(iframe_content_elements)}'
		)

		# Now test clicking elements from each category using tools.click(index)
		print('\n🖱️  Testing Click Functionality:')

		# Helper to call tools.click(index) and verify it worked
		async def click(index: int, element_description: str, browser_session: BrowserSession):
			result = await tools.click(index=index, browser_session=browser_session)
			# Check both error field and extracted_content for failure messages
			if result.error:
				raise AssertionError(f'Click on {element_description} [{index}] failed: {result.error}')
			if result.extracted_content and (
				'not available' in result.extracted_content.lower() or 'failed' in result.extracted_content.lower()
			):
				raise AssertionError(f'Click on {element_description} [{index}] failed: {result.extracted_content}')
			print(f'   ✓ {element_description} [{index}] clicked successfully')
			return result

		# Test clicking a regular DOM element (button)
		if regular_elements:
			regular_button_idx = next((idx for idx, el in regular_elements if 'regular-btn' in el.attributes.get('id', '')), None)
			if regular_button_idx:
				await click(regular_button_idx, 'Regular DOM button', browser_session)

		# Test clicking a shadow DOM element (button)
		if shadow_elements:
			shadow_button_idx = next((idx for idx, el in shadow_elements if 'btn' in el.attributes.get('id', '')), None)
			if shadow_button_idx:
				await click(shadow_button_idx, 'Shadow DOM button', browser_session)

		# Test clicking a same-origin iframe element (button)
		if iframe_content_elements:
			iframe_button_idx = next((idx for idx, el in iframe_content_elements if 'btn' in el.attributes.get('id', '')), None)
			if iframe_button_idx:
				await click(iframe_button_idx, 'Same-origin iframe button', browser_session)

		# Validate click counter - verify all 3 clicks actually executed JavaScript
		print('\n✅ Validating click counter...')

		# Get the CDP session for the main page (use target from a regular DOM element)
		# Note: browser_session.agent_focus_target_id may point to a different target than the page
		if regular_elements and regular_elements[0][1].target_id:
			cdp_session = await browser_session.get_or_create_cdp_session(target_id=regular_elements[0][1].target_id)
		else:
			cdp_session = await browser_session.get_or_create_cdp_session()

		result = await cdp_session.cdp_client.send.Runtime.evaluate(
			params={
				'expression': 'window.getClickCount()',
				'returnByValue': True,
			},
			session_id=cdp_session.session_id,
		)

		click_count = result.get('result', {}).get('value', 0)
		print(f'   Click counter value: {click_count}')

		assert click_count == 3, (
			f'Expected 3 clicks (Regular DOM + Shadow DOM + Iframe), but counter shows {click_count}. '
			f'This means some clicks did not execute JavaScript properly.'
		)

		print('\n🎉 DOM Serializer test completed successfully!')

	async def test_dom_serializer_element_counts_detailed(self, browser_session, base_url):
		"""Detailed test to verify specific element types are captured correctly."""

		actions = [
			f"""
			{{
				"thinking": "Navigating to test page",
				"evaluation_previous_goal": "Starting",
				"memory": "Navigate",
				"next_goal": "Navigate",
				"action": [
					{{
						"navigate": {{
							"url": "{base_url}/dom-test-main",
							"new_tab": false
						}}
					}}
				]
			}}
			""",
			"""
			{
				"thinking": "Done",
				"evaluation_previous_goal": "Navigated",
				"memory": "Complete",
				"next_goal": "Done",
				"action": [
					{
						"done": {
							"text": "Done",
							"success": true
						}
					}
				]
			}
			""",
		]

		mock_llm = create_mock_llm(actions=actions)
		agent = Agent(
			task=f'Navigate to {base_url}/dom-test-main',
			llm=mock_llm,
			browser_session=browser_session,
		)

		history = await agent.run(max_steps=2)

		# Get current browser state to access selector_map
		browser_state_summary = await browser_session.get_browser_state_summary(
			include_screenshot=False,
			include_recent_events=False,
		)
		selector_map = browser_state_summary.dom_state.selector_map

		# Count different element types
		buttons = 0
		inputs = 0
		links = 0

		for idx, element in selector_map.items():
			element_str = str(element).lower()
			if 'button' in element_str or '<button' in element_str:
				buttons += 1
			elif 'input' in element_str or '<input' in element_str:
				inputs += 1
			elif 'link' in element_str or '<a' in element_str or 'href' in element_str:
				links += 1

		print('\n📊 Element Type Counts:')
		print(f'   Buttons: {buttons}')
		print(f'   Inputs: {inputs}')
		print(f'   Links: {links}')
		print(f'   Total: {len(selector_map)}')

		# We should have at least some of each type from the regular DOM
		assert buttons >= 1, f'Should find at least 1 button, found {buttons}'
		assert inputs >= 1, f'Should find at least 1 input, found {inputs}'

		print('\n✅ Element type verification passed!')

	async def test_stacked_complex_scenarios(self, browser_session, base_url):
		"""Test clicking through stacked complex scenarios and verify cross-origin iframe extraction.

		This test verifies:
		1. Open shadow DOM element interaction
		2. Closed shadow DOM element interaction (nested inside open shadow)
		3. Same-origin iframe element interaction (inside closed shadow)
		4. Cross-origin iframe placeholder with about:blank (no external dependencies)
		5. Truly nested structure: Open Shadow → Closed Shadow → Iframe
		"""
		from browser_use.tools.service import Tools

		tools = Tools()

		# Navigate to stacked test page
		await tools.navigate(url=f'{base_url}/stacked-test', new_tab=False, browser_session=browser_session)

		import asyncio

		await asyncio.sleep(1)

		# Get browser state
		browser_state_summary = await browser_session.get_browser_state_summary(
			include_screenshot=False,
			include_recent_events=False,
		)

		selector_map = browser_state_summary.dom_state.selector_map
		print(f'\n📊 Stacked Test - Found {len(selector_map)} elements')

		# Debug: Show all elements
		print('\n🔍 All elements found:')
		for idx, element in selector_map.items():
			elem_id = element.attributes.get('id', 'NO_ID') if hasattr(element, 'attributes') else 'NO_ATTR'
			print(f'   [{idx}] {element.tag_name} id={elem_id} target={element.target_id[-4:] if element.target_id else "None"}')

		# Categorize elements
		open_shadow_elements = []
		closed_shadow_elements = []
		iframe_elements = []
		final_button = None

		for idx, element in selector_map.items():
			if hasattr(element, 'attributes') and 'id' in element.attributes:
				elem_id = element.attributes['id'].lower()

				if 'open-shadow' in elem_id:
					open_shadow_elements.append((idx, element))
				elif 'closed-shadow' in elem_id:
					closed_shadow_elements.append((idx, element))
				elif 'iframe' in elem_id and element.tag_name != 'iframe':
					iframe_elements.append((idx, element))
				elif 'final-button' in elem_id:
					final_button = (idx, element)

		print('\n📋 Element Distribution:')
		print(f'   Open Shadow: {len(open_shadow_elements)} elements')
		print(f'   Closed Shadow: {len(closed_shadow_elements)} elements')
		print(f'   Iframe content: {len(iframe_elements)} elements')
		print(f'   Final button: {"Found" if final_button else "Not found"}')

		# Test clicking through each stacked layer
		print('\n🖱️  Testing Click Functionality Through Stacked Layers:')

		async def click(index: int, element_description: str, browser_session: BrowserSession):
			result = await tools.click(index=index, browser_session=browser_session)
			if result.error:
				raise AssertionError(f'Click on {element_description} [{index}] failed: {result.error}')
			if result.extracted_content and (
				'not available' in result.extracted_content.lower() or 'failed' in result.extracted_content.lower()
			):
				raise AssertionError(f'Click on {element_description} [{index}] failed: {result.extracted_content}')
			print(f'   ✓ {element_description} [{index}] clicked successfully')
			return result

		clicks_performed = 0

		# 1. Click open shadow button
		if open_shadow_elements:
			open_shadow_btn = next((idx for idx, el in open_shadow_elements if 'btn' in el.attributes.get('id', '')), None)
			if open_shadow_btn:
				await click(open_shadow_btn, 'Open Shadow DOM button', browser_session)
				clicks_performed += 1

		# 2. Click closed shadow button
		if closed_shadow_elements:
			closed_shadow_btn = next((idx for idx, el in closed_shadow_elements if 'btn' in el.attributes.get('id', '')), None)
			if closed_shadow_btn:
				await click(closed_shadow_btn, 'Closed Shadow DOM button', browser_session)
				clicks_performed += 1

		# 3. Click iframe button
		if iframe_elements:
			iframe_btn = next((idx for idx, el in iframe_elements if 'btn' in el.attributes.get('id', '')), None)
			if iframe_btn:
				await click(iframe_btn, 'Same-origin iframe button', browser_session)
				clicks_performed += 1

		# 4. Try clicking cross-origin iframe tag (can click the tag, but not elements inside)
		cross_origin_iframe_tag = None
		for idx, element in selector_map.items():
			if (
				element.tag_name == 'iframe'
				and hasattr(element, 'attributes')
				and 'cross-origin' in element.attributes.get('id', '').lower()
			):
				cross_origin_iframe_tag = (idx, element)
				break

		# Verify cross-origin iframe extraction is working
		# Check the full DOM tree (not just selector_map which only has interactive elements)
		def count_targets_in_tree(node, targets=None):
			if targets is None:
				targets = set()
			# SimplifiedNode has original_node which is an EnhancedDOMTreeNode
			if hasattr(node, 'original_node') and node.original_node and node.original_node.target_id:
				targets.add(node.original_node.target_id)
			# Recursively check children
			if hasattr(node, 'children') and node.children:
				for child in node.children:
					count_targets_in_tree(child, targets)
			return targets

		all_targets = count_targets_in_tree(browser_state_summary.dom_state._root)

		print('\n📊 Cross-Origin Iframe Extraction:')
		print(f'   Found elements from {len(all_targets)} different CDP targets in full DOM tree')

		if len(all_targets) >= 2:
			print('   ✅ Multi-target iframe extraction IS WORKING!')
			print('   ✓ Successfully extracted DOM from multiple CDP targets')
			print('   ✓ CDP target switching feature is enabled and functional')
		else:
			print('   ⚠️  Only found elements from 1 target (cross-origin extraction may not be working)')

		if cross_origin_iframe_tag:
			print(f'\n   📌 Found cross-origin iframe tag [{cross_origin_iframe_tag[0]}]')
			# Note: We don't increment clicks_performed since this doesn't trigger our counter
			# await click(cross_origin_iframe_tag[0], 'Cross-origin iframe tag (scroll)', browser_session)

		# 5. Click final button (after all stacked elements)
		if final_button:
			await click(final_button[0], 'Final button (after stack)', browser_session)
			clicks_performed += 1

		# Validate click counter
		print('\n✅ Validating click counter...')

		# Get CDP session from a non-iframe element (open shadow or final button)
		if open_shadow_elements:
			cdp_session = await browser_session.get_or_create_cdp_session(target_id=open_shadow_elements[0][1].target_id)
		elif final_button:
			cdp_session = await browser_session.get_or_create_cdp_session(target_id=final_button[1].target_id)
		else:
			cdp_session = await browser_session.get_or_create_cdp_session()

		result = await cdp_session.cdp_client.send.Runtime.evaluate(
			params={
				'expression': 'window.getClickCount()',
				'returnByValue': True,
			},
			session_id=cdp_session.session_id,
		)

		click_count = result.get('result', {}).get('value', 0)
		print(f'   Click counter value: {click_count}')
		print(f'   Expected clicks: {clicks_performed}')

		assert click_count == clicks_performed, (
			f'Expected {clicks_performed} clicks, but counter shows {click_count}. '
			f'Some clicks did not execute JavaScript properly.'
		)

		print('\n🎉 Stacked scenario test completed successfully!')
		print('   ✓ Open shadow DOM clicks work')
		print('   ✓ Closed shadow DOM clicks work')
		print('   ✓ Same-origin iframe clicks work (can access elements inside)')
		print('   ✓ Cross-origin iframe extraction works (CDP target switching enabled)')
		print('   ✓ Truly nested structure works: Open Shadow → Closed Shadow → Iframe')


if __name__ == '__main__':
	"""Run test in debug mode with manual fixture setup."""
	import asyncio
	import logging

	# Set up debug logging
	logging.basicConfig(
		level=logging.DEBUG,
		format='%(levelname)-8s [%(name)s] %(message)s',
	)

	async def main():
		# Set up HTTP server fixture
		from pathlib import Path

		from pytest_httpserver import HTTPServer

		server = HTTPServer()
		server.start()

		# Load HTML templates from files (same as http_server fixture)
		test_dir = Path(__file__).parent
		main_page_html = (test_dir / 'test_page_stacked_template.html').read_text()
		# Set up routes using templates
		server.expect_request('/stacked-test').respond_with_data(main_page_html, content_type='text/html')

		base_url = f'http://{server.host}:{server.port}'
		print(f'\n🌐 HTTP Server running at {base_url}')

		# Set up browser session
		from browser_use.browser import BrowserSession
		from browser_use.browser.profile import BrowserProfile

		session = BrowserSession(
			browser_profile=BrowserProfile(
				headless=False,  # Set to False to see browser in action
				user_data_dir=None,
				keep_alive=True,
			)
		)

		try:
			await session.start()
			print('🚀 Browser session started\n')

			# Run the test
			test = TestDOMSerializer()
			await test.test_stacked_complex_scenarios(session, base_url)

			print('\n✅ Test completed successfully!')

		finally:
			# Cleanup
			await session.kill()
			server.stop()
			print('\n🧹 Cleanup complete')

	asyncio.run(main())

```

---

## backend/browser-use/tests/ci/browser/test_navigation.py

```py
"""
Test navigation edge cases: broken pages, slow loading, non-existing pages.

Tests verify that:
1. Agent can handle navigation to broken/malformed HTML pages
2. Agent can handle slow-loading pages without hanging
3. Agent can handle non-existing pages (404, connection refused, etc.)
4. Agent can recover and continue making LLM calls after encountering these issues

All tests use:
- max_steps=3 to limit agent actions
- 120s timeout to fail if test takes too long
- Mock LLM to verify agent can still make decisions after navigation errors

Usage:
	uv run pytest tests/ci/browser/test_navigation.py -v -s
"""

import asyncio
import time

import pytest
from pytest_httpserver import HTTPServer
from werkzeug import Response

from browser_use.agent.service import Agent
from browser_use.browser import BrowserSession
from browser_use.browser.profile import BrowserProfile
from tests.ci.conftest import create_mock_llm


@pytest.fixture(scope='session')
def http_server():
	"""Create and provide a test HTTP server for navigation tests."""
	server = HTTPServer()
	server.start()

	# Route 1: Broken/malformed HTML page
	server.expect_request('/broken').respond_with_data(
		'<html><head><title>Broken Page</title></head><body><h1>Incomplete HTML',
		content_type='text/html',
	)

	# Route 2: Valid page for testing navigation after error recovery
	server.expect_request('/valid').respond_with_data(
		'<html><head><title>Valid Page</title></head><body><h1>Valid Page</h1><p>This page loaded successfully</p></body></html>',
		content_type='text/html',
	)

	# Route 3: Slow loading page - delays 10 seconds before responding
	def slow_handler(request):
		time.sleep(10)
		return Response(
			'<html><head><title>Slow Page</title></head><body><h1>Slow Loading Page</h1><p>This page took 10 seconds to load</p></body></html>',
			content_type='text/html',
		)

	server.expect_request('/slow').respond_with_handler(slow_handler)

	# Route 4: 404 page
	server.expect_request('/notfound').respond_with_data(
		'<html><head><title>404 Not Found</title></head><body><h1>404 - Page Not Found</h1></body></html>',
		status=404,
		content_type='text/html',
	)

	yield server
	server.stop()


@pytest.fixture(scope='session')
def base_url(http_server):
	"""Return the base URL for the test HTTP server."""
	return f'http://{http_server.host}:{http_server.port}'


@pytest.fixture(scope='function')
async def browser_session():
	"""Create a browser session for navigation tests."""
	session = BrowserSession(
		browser_profile=BrowserProfile(
			headless=True,
			user_data_dir=None,
			keep_alive=True,
		)
	)
	await session.start()
	yield session
	await session.kill()


class TestNavigationEdgeCases:
	"""Test navigation error handling and recovery."""

	async def test_broken_page_navigation(self, browser_session, base_url):
		"""Test that agent can handle broken/malformed HTML and still make LLM calls."""

		# Create actions for the agent:
		# 1. Navigate to broken page
		# 2. Check if page exists
		# 3. Done
		actions = [
			f"""
			{{
				"thinking": "I need to navigate to the broken page",
				"evaluation_previous_goal": "Starting task",
				"memory": "Navigating to broken page",
				"next_goal": "Navigate to broken page",
				"action": [
					{{
						"navigate": {{
							"url": "{base_url}/broken"
						}}
					}}
				]
			}}
			""",
			"""
			{
				"thinking": "I should check if the page loaded",
				"evaluation_previous_goal": "Navigated to page",
				"memory": "Checking page state",
				"next_goal": "Verify page exists",
				"action": [
					{
						"done": {
							"text": "Page exists despite broken HTML",
							"success": true
						}
					}
				]
			}
			""",
		]

		mock_llm = create_mock_llm(actions=actions)

		agent = Agent(
			task=f'Navigate to {base_url}/broken and check if page exists',
			llm=mock_llm,
			browser_session=browser_session,
		)

		# Run with timeout - should complete within 2 minutes
		try:
			history = await asyncio.wait_for(agent.run(max_steps=3), timeout=120)
			assert len(history) > 0, 'Agent should have completed at least one step'
			# If agent completes successfully, it means LLM was called and functioning
			final_result = history.final_result()
			assert final_result is not None, 'Agent should return a final result'
		except TimeoutError:
			pytest.fail('Test timed out after 2 minutes - agent hung on broken page')

	async def test_slow_loading_page(self, browser_session, base_url):
		"""Test that agent can handle slow-loading pages without hanging."""

		actions = [
			f"""
			{{
				"thinking": "I need to navigate to the slow page",
				"evaluation_previous_goal": "Starting task",
				"memory": "Navigating to slow page",
				"next_goal": "Navigate to slow page",
				"action": [
					{{
						"navigate": {{
							"url": "{base_url}/slow"
						}}
					}}
				]
			}}
			""",
			"""
			{
				"thinking": "The page loaded, even though it was slow",
				"evaluation_previous_goal": "Successfully navigated",
				"memory": "Page loaded after delay",
				"next_goal": "Complete task",
				"action": [
					{
						"done": {
							"text": "Slow page loaded successfully",
							"success": true
						}
					}
				]
			}
			""",
		]

		mock_llm = create_mock_llm(actions=actions)

		agent = Agent(
			task=f'Navigate to {base_url}/slow and wait for it to load',
			llm=mock_llm,
			browser_session=browser_session,
		)

		# Run with timeout - should complete within 2 minutes
		start_time = time.time()
		try:
			history = await asyncio.wait_for(agent.run(max_steps=3), timeout=120)
			elapsed = time.time() - start_time

			assert len(history) > 0, 'Agent should have completed at least one step'
			assert elapsed >= 10, f'Agent should have waited for slow page (10s delay), but only took {elapsed:.1f}s'
			final_result = history.final_result()
			assert final_result is not None, 'Agent should return a final result'
		except TimeoutError:
			pytest.fail('Test timed out after 2 minutes - agent hung on slow page')

	async def test_nonexisting_page_404(self, browser_session, base_url):
		"""Test that agent can handle 404 pages and still make LLM calls."""

		actions = [
			f"""
			{{
				"thinking": "I need to navigate to the non-existing page",
				"evaluation_previous_goal": "Starting task",
				"memory": "Navigating to 404 page",
				"next_goal": "Navigate to non-existing page",
				"action": [
					{{
						"navigate": {{
							"url": "{base_url}/notfound"
						}}
					}}
				]
			}}
			""",
			"""
			{
				"thinking": "I got a 404 error but the browser still works",
				"evaluation_previous_goal": "Navigated to 404 page",
				"memory": "Page not found",
				"next_goal": "Report that page does not exist",
				"action": [
					{
						"done": {
							"text": "Page does not exist (404 error)",
							"success": false
						}
					}
				]
			}
			""",
		]

		mock_llm = create_mock_llm(actions=actions)

		agent = Agent(
			task=f'Navigate to {base_url}/notfound and check if page exists',
			llm=mock_llm,
			browser_session=browser_session,
		)

		# Run with timeout - should complete within 2 minutes
		try:
			history = await asyncio.wait_for(agent.run(max_steps=3), timeout=120)
			assert len(history) > 0, 'Agent should have completed at least one step'
			final_result = history.final_result()
			assert final_result is not None, 'Agent should return a final result'
		except TimeoutError:
			pytest.fail('Test timed out after 2 minutes - agent hung on 404 page')

	async def test_nonexisting_domain(self, browser_session):
		"""Test that agent can handle completely non-existing domains (connection refused)."""

		# Use a localhost port that's not listening
		nonexisting_url = 'http://localhost:59999/page'

		actions = [
			f"""
			{{
				"thinking": "I need to navigate to a non-existing domain",
				"evaluation_previous_goal": "Starting task",
				"memory": "Attempting to navigate",
				"next_goal": "Navigate to non-existing domain",
				"action": [
					{{
						"navigate": {{
							"url": "{nonexisting_url}"
						}}
					}}
				]
			}}
			""",
			"""
			{
				"thinking": "The connection failed but I can still proceed",
				"evaluation_previous_goal": "Connection failed",
				"memory": "Domain does not exist",
				"next_goal": "Report failure",
				"action": [
					{
						"done": {
							"text": "Domain does not exist (connection refused)",
							"success": false
						}
					}
				]
			}
			""",
		]

		mock_llm = create_mock_llm(actions=actions)

		agent = Agent(
			task=f'Navigate to {nonexisting_url} and check if it exists',
			llm=mock_llm,
			browser_session=browser_session,
		)

		# Run with timeout - should complete within 2 minutes
		try:
			history = await asyncio.wait_for(agent.run(max_steps=3), timeout=120)
			assert len(history) > 0, 'Agent should have completed at least one step'
			final_result = history.final_result()
			assert final_result is not None, 'Agent should return a final result'
		except TimeoutError:
			pytest.fail('Test timed out after 2 minutes - agent hung on non-existing domain')

	async def test_recovery_after_navigation_error(self, browser_session, base_url):
		"""Test that agent can recover and navigate to valid page after encountering error."""

		actions = [
			f"""
			{{
				"thinking": "First, I'll try the broken page",
				"evaluation_previous_goal": "Starting task",
				"memory": "Navigating to broken page",
				"next_goal": "Navigate to broken page first",
				"action": [
					{{
						"navigate": {{
							"url": "{base_url}/broken"
						}}
					}}
				]
			}}
			""",
			f"""
			{{
				"thinking": "That page was broken, let me try a valid page now",
				"evaluation_previous_goal": "Broken page loaded",
				"memory": "Now navigating to valid page",
				"next_goal": "Navigate to valid page",
				"action": [
					{{
						"navigate": {{
							"url": "{base_url}/valid"
						}}
					}}
				]
			}}
			""",
			"""
			{
				"thinking": "The valid page loaded successfully after the broken one",
				"evaluation_previous_goal": "Valid page loaded",
				"memory": "Successfully recovered from error",
				"next_goal": "Complete task",
				"action": [
					{
						"done": {
							"text": "Successfully navigated to valid page after broken page",
							"success": true
						}
					}
				]
			}
			""",
		]

		mock_llm = create_mock_llm(actions=actions)

		agent = Agent(
			task=f'First navigate to {base_url}/broken, then navigate to {base_url}/valid',
			llm=mock_llm,
			browser_session=browser_session,
		)

		# Run with timeout - should complete within 2 minutes
		try:
			history = await asyncio.wait_for(agent.run(max_steps=3), timeout=120)
			assert len(history) >= 2, 'Agent should have completed at least 2 steps (broken -> valid)'

			# Verify final page is the valid one
			final_url = await browser_session.get_current_page_url()
			assert final_url.endswith('/valid'), f'Final URL should be /valid, got {final_url}'

			# Verify agent completed successfully
			final_result = history.final_result()
			assert final_result is not None, 'Agent should return a final result'
		except TimeoutError:
			pytest.fail('Test timed out after 2 minutes - agent could not recover from broken page')

```

---

## backend/browser-use/tests/ci/browser/test_output_paths.py

```py
"""Test all recording and save functionality for Agent and BrowserSession."""

from pathlib import Path

import pytest

from browser_use import Agent, AgentHistoryList
from browser_use.browser import BrowserProfile, BrowserSession
from tests.ci.conftest import create_mock_llm


@pytest.fixture
def test_dir(tmp_path):
	"""Create a test directory that gets cleaned up after each test."""
	test_path = tmp_path / 'test_recordings'
	test_path.mkdir(exist_ok=True)
	yield test_path


@pytest.fixture
async def httpserver_url(httpserver):
	"""Simple test page."""
	# Use expect_ordered_request with multiple handlers to handle repeated requests
	for _ in range(10):  # Allow up to 10 requests to the same URL
		httpserver.expect_ordered_request('/').respond_with_data(
			"""
			<!DOCTYPE html>
			<html>
			<head>
				<title>Test Page</title>
			</head>
			<body>
				<h1>Test Recording Page</h1>
				<input type="text" id="search" placeholder="Search here" />
				<button type="button" id="submit">Submit</button>
			</body>
			</html>
			""",
			content_type='text/html',
		)
	return httpserver.url_for('/')


@pytest.fixture
def llm():
	"""Create mocked LLM instance for tests."""
	return create_mock_llm()


@pytest.fixture
def interactive_llm(httpserver_url):
	"""Create mocked LLM that navigates to page and interacts with elements."""
	actions = [
		# First action: Navigate to the page
		f"""
		{{
			"thinking": "null",
			"evaluation_previous_goal": "Starting the task",
			"memory": "Need to navigate to the test page",
			"next_goal": "Navigate to the URL",
			"action": [
				{{
					"navigate": {{
						"url": "{httpserver_url}",
						"new_tab": false
					}}
				}}
			]
		}}
		""",
		# Second action: Click in the search box
		"""
		{
			"thinking": "null",
		"evaluation_previous_goal": "Successfully navigated to the page",
		"memory": "Page loaded, can see search box and submit button",
		"next_goal": "Click on the search box to focus it",
		"action": [
			{
				"click": {
					"index": 0
				}
			}
		]
		}
		""",
		# Third action: Type text in the search box
		"""
		{
			"thinking": "null",
			"evaluation_previous_goal": "Clicked on search box",
			"memory": "Search box is focused and ready for input",
			"next_goal": "Type 'test' in the search box",
			"action": [
				{
					"input_text": {
						"index": 0,
						"text": "test"
					}
				}
			]
		}
		""",
		# Fourth action: Click submit button
		"""
		{
			"thinking": "null",
		"evaluation_previous_goal": "Typed 'test' in search box",
		"memory": "Text 'test' has been entered successfully",
		"next_goal": "Click the submit button to complete the task",
		"action": [
			{
				"click": {
					"index": 1
				}
			}
		]
		}
		""",
		# Fifth action: Done - task completed
		"""
		{
			"thinking": "null",
			"evaluation_previous_goal": "Clicked the submit button",
			"memory": "Successfully navigated to the page, typed 'test' in the search box, and clicked submit",
			"next_goal": "Task completed",
			"action": [
				{
					"done": {
						"text": "Task completed - typed 'test' in search box and clicked submit",
						"success": true
					}
				}
			]
		}
		""",
	]
	return create_mock_llm(actions)


class TestAgentRecordings:
	"""Test Agent save_conversation_path and generate_gif parameters."""

	@pytest.mark.parametrize('path_type', ['with_slash', 'without_slash', 'deep_directory'])
	async def test_save_conversation_path(self, test_dir, httpserver_url, llm, path_type):
		"""Test saving conversation with different path types."""
		if path_type == 'with_slash':
			conversation_path = test_dir / 'logs' / 'conversation'
		elif path_type == 'without_slash':
			conversation_path = test_dir / 'logs'
		else:  # deep_directory
			conversation_path = test_dir / 'logs' / 'deep' / 'directory' / 'conversation'

		browser_session = BrowserSession(browser_profile=BrowserProfile(headless=True, disable_security=True, user_data_dir=None))
		await browser_session.start()
		try:
			agent = Agent(
				task=f'go to {httpserver_url} and type "test" in the search box',
				llm=llm,
				browser_session=browser_session,
				save_conversation_path=str(conversation_path),
			)
			history: AgentHistoryList = await agent.run(max_steps=2)

			result = history.final_result()
			assert result is not None

			# Check that the conversation directory and files were created
			assert conversation_path.exists(), f'{path_type}: conversation directory was not created'
			# Files are now always created as conversation_<agent_id>_<step>.txt inside the directory
			conversation_files = list(conversation_path.glob('conversation_*.txt'))
			assert len(conversation_files) > 0, f'{path_type}: conversation file was not created in {conversation_path}'
		finally:
			await browser_session.kill()

	@pytest.mark.skip(reason='TODO: fix')
	@pytest.mark.parametrize('generate_gif', [False, True, 'custom_path'])
	async def test_generate_gif(self, test_dir, httpserver_url, llm, generate_gif):
		"""Test GIF generation with different settings."""
		# Clean up any existing GIFs first
		for gif in Path.cwd().glob('agent_*.gif'):
			gif.unlink()

		gif_param = generate_gif
		expected_gif_path = None

		if generate_gif == 'custom_path':
			expected_gif_path = test_dir / 'custom_agent.gif'
			gif_param = str(expected_gif_path)

		browser_session = BrowserSession(browser_profile=BrowserProfile(headless=True, disable_security=True, user_data_dir=None))
		await browser_session.start()
		try:
			agent = Agent(
				task=f'go to {httpserver_url}',
				llm=llm,
				browser_session=browser_session,
				generate_gif=gif_param,
			)
			history: AgentHistoryList = await agent.run(max_steps=2)

			result = history.final_result()
			assert result is not None

			# Check GIF creation
			if generate_gif is False:
				gif_files = list(Path.cwd().glob('*.gif'))
				assert len(gif_files) == 0, 'GIF file was created when generate_gif=False'
			elif generate_gif is True:
				# With mock LLM that doesn't navigate, all screenshots will be about:blank placeholders
				# So no GIF will be created (this is expected behavior)
				gif_files = list(Path.cwd().glob('agent_history.gif'))
				assert len(gif_files) == 0, 'GIF should not be created when all screenshots are placeholders'
			else:  # custom_path
				assert expected_gif_path is not None, 'expected_gif_path should be set for custom_path'
				# With mock LLM that doesn't navigate, no GIF will be created
				assert not expected_gif_path.exists(), 'GIF should not be created when all screenshots are placeholders'
		finally:
			await browser_session.kill()

```

---

## backend/browser-use/tests/ci/browser/test_page_stacked_template.html

```html
<!DOCTYPE html>
<html>
<head>
	<title>Stacked DOM Elements Test</title>
	<style>
		body { font-family: Arial; padding: 20px; min-height: 3000px; }
		.section {
			margin: 20px 0;
			padding: 15px;
			border: 2px solid #333;
			background: #f9f9f9;
		}
		#click-counter {
			position: fixed;
			top: 20px;
			right: 20px;
			background: #4CAF50;
			color: white;
			padding: 30px 50px;
			border-radius: 15px;
			font-size: 48px;
			font-weight: bold;
			box-shadow: 0 4px 20px rgba(0,0,0,0.3);
			transition: all 0.2s ease;
			z-index: 9999;
		}
		#counter-value {
			font-size: 64px;
			display: inline-block;
			min-width: 60px;
			text-align: center;
		}
		@keyframes flash {
			0% { transform: scale(1); }
			50% { transform: scale(1.3); background: #FFC107; }
			100% { transform: scale(1); }
		}
		.flash {
			animation: flash 0.3s ease;
		}
		.final-button {
			margin-top: 50px;
			padding: 20px 40px;
			font-size: 24px;
			background: #2196F3;
			color: white;
			border: none;
			border-radius: 8px;
			cursor: pointer;
		}
	</style>
</head>
<body>
	<div id="click-counter">Clicks: <span id="counter-value">0</span></div>
	<h1>Nested DOM Test</h1>

	<!-- Root: Open Shadow DOM (contains everything else) -->
	<div class="section">
		<div id="open-shadow-host"></div>
	</div>

	<script>
		// Global click counter
		let clickCount = 0;

		function incrementCounter(source) {
			clickCount++;
			const counter = document.getElementById('click-counter');
			const counterValue = document.getElementById('counter-value');

			counterValue.textContent = clickCount;
			console.log(`Click #${clickCount} from: ${source}`);

			// Add flash animation
			counter.classList.remove('flash');
			void counter.offsetWidth; // Trigger reflow
			counter.classList.add('flash');
		}

		// Expose counter for testing
		window.getClickCount = function() {
			return clickCount;
		};

		// Build nested structure: Open Shadow → Closed Shadow → Iframe → Final Button

		// 1. Create Open Shadow DOM (contains everything else)
		const openShadowHost = document.getElementById('open-shadow-host');
		const openShadowRoot = openShadowHost.attachShadow({mode: 'open'});
		openShadowRoot.innerHTML = `
			<style>
				.shadow-content { padding: 15px; background: #e3f2fd; border: 2px solid #2196F3; margin: 10px 0; }
				button { padding: 10px 20px; font-size: 16px; margin: 10px 0; display: block; }
				.nested-info { font-weight: bold; color: #1976D2; }
			</style>
			<div class="shadow-content">
				<button id="open-shadow-btn">Open Shadow Button</button>
				<div id="closed-shadow-host"></div>
			</div>
		`;

		openShadowRoot.getElementById('open-shadow-btn').addEventListener('click', function() {
			incrementCounter('Open Shadow DOM');
		});

		// 2. Create Closed Shadow DOM INSIDE Open Shadow (nested!)
		const closedShadowHost = openShadowRoot.getElementById('closed-shadow-host');
		const closedShadowRoot = closedShadowHost.attachShadow({mode: 'closed'});
		closedShadowRoot.innerHTML = `
			<style>
				.shadow-content { padding: 15px; background: #fff3e0; border: 2px solid #FF9800; margin: 10px 0; }
				button { padding: 10px 20px; font-size: 16px; margin: 10px 0; display: block; }
				iframe { width: 100%; height: 250px; border: 2px solid #4CAF50; margin: 10px 0; }
				.nested-info { font-weight: bold; color: #F57C00; }
				.iframe-label { font-size: 14px; color: #666; margin-top: 10px; }
			</style>
			<div class="shadow-content">
				<button id="closed-shadow-btn">Closed Shadow Button</button>
				<iframe id="cross-origin-iframe" src="about:blank"></iframe>
				<iframe id="nested-iframe" src="/iframe-same-origin"></iframe>
			</div>
		`;

		closedShadowRoot.getElementById('closed-shadow-btn').addEventListener('click', function() {
			incrementCounter('Closed Shadow DOM');
		});
	</script>
</body>
</html>

```

---

## backend/browser-use/tests/ci/browser/test_page_template.html

```html
<!DOCTYPE html>
<html>
<head>
	<title>DOM Serializer Test - Main Page</title>
	<style>
		body { font-family: Arial; padding: 20px; }
		.section { margin: 20px 0; padding: 15px; border: 1px solid #ccc; }
		#click-counter {
			position: fixed;
			top: 20px;
			right: 20px;
			background: #4CAF50;
			color: white;
			padding: 30px 50px;
			border-radius: 15px;
			font-size: 48px;
			font-weight: bold;
			box-shadow: 0 4px 20px rgba(0,0,0,0.3);
			transition: all 0.2s ease;
			z-index: 9999;
		}
		#counter-value {
			font-size: 64px;
			display: inline-block;
			min-width: 60px;
			text-align: center;
		}
		@keyframes flash {
			0% { transform: scale(1); }
			50% { transform: scale(1.3); background: #FFC107; }
			100% { transform: scale(1); }
		}
		.flash {
			animation: flash 0.3s ease;
		}
	</style>
</head>
<body>
	<div id="click-counter">Clicks: <span id="counter-value">0</span></div>
	<h1>DOM Serializer Test Page</h1>

	<!-- Regular DOM elements (3 interactive elements) -->
	<div class="section">
		<h2>Regular DOM Elements</h2>
		<button id="regular-btn-1">Regular Button 1</button>
		<input type="text" id="regular-input" placeholder="Regular input" />
		<a href="#test" id="regular-link">Regular Link</a>
	</div>

	<!-- Shadow DOM elements (3 interactive elements inside shadow) -->
	<div class="section">
		<h2>Shadow DOM Elements</h2>
		<div id="shadow-host"></div>
	</div>

	<!-- Same-origin iframe (2 interactive elements inside) -->
	<div class="section">
		<h2>Same-Origin Iframe</h2>
		<iframe id="same-origin-iframe" src="/iframe-same-origin" style="width:100%; height:200px; border:1px solid #999;"></iframe>
	</div>

	<!-- Cross-origin iframe placeholder (external domain removed for test isolation) -->
	<div class="section">
		<h2>Cross-Origin Iframe (Placeholder)</h2>
		<iframe id="cross-origin-iframe" src="about:blank" style="width:100%; height:200px; border:1px solid #999;"></iframe>
	</div>

	<script>
		// Global click counter
		let clickCount = 0;

		function incrementCounter(source) {
			clickCount++;
			const counter = document.getElementById('click-counter');
			const counterValue = document.getElementById('counter-value');

			counterValue.textContent = clickCount;
			console.log(`Click #${clickCount} from: ${source}`);

			// Add flash animation
			counter.classList.remove('flash');
			void counter.offsetWidth; // Trigger reflow
			counter.classList.add('flash');
		}

		// Expose counter for testing
		window.getClickCount = function() {
			return clickCount;
		};

		// Add click handler to regular button using addEventListener
		document.getElementById('regular-btn-1').addEventListener('click', function() {
			incrementCounter('Regular DOM');
		});

		// Create shadow DOM with interactive elements
		const shadowHost = document.getElementById('shadow-host');
		const shadowRoot = shadowHost.attachShadow({mode: 'open'});

		shadowRoot.innerHTML = `
			<style>
				.shadow-content { padding: 10px; background: #f0f0f0; }
			</style>
			<div class="shadow-content">
				<p>Content inside Shadow DOM:</p>
				<button id="shadow-btn-1">Shadow Button 1</button>
				<input type="text" id="shadow-input" placeholder="Shadow input" />
				<button id="shadow-btn-2">Shadow Button 2</button>
			</div>
		`;

		// Add click handler to shadow DOM button using addEventListener
		shadowRoot.getElementById('shadow-btn-1').addEventListener('click', function() {
			incrementCounter('Shadow DOM');
		});
	</script>
</body>
</html>

```

---

## backend/browser-use/tests/ci/browser/test_proxy.py

```py
import asyncio
from typing import Any

import pytest

from browser_use.browser import BrowserProfile, BrowserSession
from browser_use.browser.profile import ProxySettings
from browser_use.config import CONFIG


def test_chromium_args_include_proxy_flags():
	profile = BrowserProfile(
		headless=True,
		user_data_dir=str(CONFIG.BROWSER_USE_PROFILES_DIR / 'proxy-smoke'),
		proxy=ProxySettings(
			server='http://proxy.local:8080',
			bypass='localhost,127.0.0.1',
		),
	)
	args = profile.get_args()
	assert any(a == '--proxy-server=http://proxy.local:8080' for a in args), args
	assert any(a == '--proxy-bypass-list=localhost,127.0.0.1' for a in args), args


@pytest.mark.asyncio
async def test_cdp_proxy_auth_handler_registers_and_responds():
	# Create profile with proxy auth credentials
	profile = BrowserProfile(
		headless=True,
		user_data_dir=str(CONFIG.BROWSER_USE_PROFILES_DIR / 'proxy-smoke'),
		proxy=ProxySettings(username='user', password='pass'),
	)
	session = BrowserSession(browser_profile=profile)

	# Stub CDP client with minimal Fetch support
	class StubCDP:
		def __init__(self) -> None:
			self.enabled = False
			self.last_auth: dict[str, Any] | None = None
			self.last_default: dict[str, Any] | None = None
			self.auth_callback = None
			self.request_paused_callback = None

			class _FetchSend:
				def __init__(self, outer: 'StubCDP') -> None:
					self._outer = outer

				async def enable(self, params: dict, session_id: str | None = None) -> None:
					self._outer.enabled = True

				async def continueWithAuth(self, params: dict, session_id: str | None = None) -> None:
					self._outer.last_auth = {'params': params, 'session_id': session_id}

				async def continueRequest(self, params: dict, session_id: str | None = None) -> None:
					# no-op; included to mirror CDP API surface used by impl
					pass

			class _Send:
				def __init__(self, outer: 'StubCDP') -> None:
					self.Fetch = _FetchSend(outer)

			class _FetchRegister:
				def __init__(self, outer: 'StubCDP') -> None:
					self._outer = outer

				def authRequired(self, callback) -> None:
					self._outer.auth_callback = callback

				def requestPaused(self, callback) -> None:
					self._outer.request_paused_callback = callback

			class _Register:
				def __init__(self, outer: 'StubCDP') -> None:
					self.Fetch = _FetchRegister(outer)

			self.send = _Send(self)
			self.register = _Register(self)

	root = StubCDP()

	# Attach stubs to session
	session._cdp_client_root = root  # type: ignore[attr-defined]
	# No need to attach a real CDPSession; _setup_proxy_auth works with root client

	# Should register Fetch handler and enable auth handling without raising
	await session._setup_proxy_auth()

	assert root.enabled is True
	assert callable(root.auth_callback)

	# Simulate proxy auth required event
	ev = {'requestId': 'r1', 'authChallenge': {'source': 'Proxy'}}
	root.auth_callback(ev, session_id='s1')  # type: ignore[misc]

	# Let scheduled task run
	await asyncio.sleep(0.05)

	assert root.last_auth is not None
	params = root.last_auth['params']
	assert params['authChallengeResponse']['response'] == 'ProvideCredentials'
	assert params['authChallengeResponse']['username'] == 'user'
	assert params['authChallengeResponse']['password'] == 'pass'
	assert root.last_auth['session_id'] == 's1'

	# Now simulate a non-proxy auth challenge and ensure default handling
	ev2 = {'requestId': 'r2', 'authChallenge': {'source': 'Server'}}
	root.auth_callback(ev2, session_id='s2')  # type: ignore[misc]
	await asyncio.sleep(0.05)
	# After non-proxy challenge, last_auth should reflect Default response
	assert root.last_auth is not None
	params2 = root.last_auth['params']
	assert params2['requestId'] == 'r2'
	assert params2['authChallengeResponse']['response'] == 'Default'

```

---

## backend/browser-use/tests/ci/browser/test_screenshot.py

```py
import pytest
from pytest_httpserver import HTTPServer

from browser_use.agent.service import Agent
from browser_use.browser.events import NavigateToUrlEvent
from browser_use.browser.profile import BrowserProfile
from browser_use.browser.session import BrowserSession
from tests.ci.conftest import create_mock_llm


@pytest.fixture(scope='session')
def http_server():
	"""Create and provide a test HTTP server for screenshot tests."""
	server = HTTPServer()
	server.start()

	# Route: Page with visible content for screenshot testing
	server.expect_request('/screenshot-page').respond_with_data(
		"""
		<!DOCTYPE html>
		<html>
		<head>
			<title>Screenshot Test Page</title>
			<style>
				body { font-family: Arial; padding: 20px; background: #f0f0f0; }
				h1 { color: #333; font-size: 32px; }
				.content { background: white; padding: 20px; border-radius: 8px; margin: 10px 0; }
			</style>
		</head>
		<body>
			<h1>Screenshot Test Page</h1>
			<div class="content">
				<p>This page is used to test screenshot capture with vision enabled.</p>
				<p>The agent should capture a screenshot when navigating to this page.</p>
			</div>
		</body>
		</html>
		""",
		content_type='text/html',
	)

	yield server
	server.stop()


@pytest.fixture(scope='session')
def base_url(http_server):
	"""Return the base URL for the test HTTP server."""
	return f'http://{http_server.host}:{http_server.port}'


@pytest.fixture(scope='function')
async def browser_session():
	session = BrowserSession(browser_profile=BrowserProfile(headless=True))
	await session.start()
	yield session
	await session.kill()


@pytest.mark.asyncio
async def test_basic_screenshots(browser_session: BrowserSession, httpserver):
	"""Navigate to a local page and ensure screenshot helpers return bytes."""

	html = """
    <html><body><h1 id='title'>Hello</h1><p>Screenshot demo.</p></body></html>
    """
	httpserver.expect_request('/demo').respond_with_data(html, content_type='text/html')
	url = httpserver.url_for('/demo')

	nav = browser_session.event_bus.dispatch(NavigateToUrlEvent(url=url, new_tab=False))
	await nav

	data = await browser_session.take_screenshot(full_page=False)
	assert data, 'Viewport screenshot returned no data'

	element = await browser_session.screenshot_element('h1')
	assert element, 'Element screenshot returned no data'


async def test_agent_screenshot_with_vision_enabled(browser_session, base_url):
	"""Test that agent captures screenshots when vision is enabled.

	This integration test verifies that:
	1. Agent with vision=True navigates to a page
	2. After prepare_context/update message manager, screenshot is captured
	3. Screenshot is included in the agent's history state
	"""

	# Create mock LLM actions
	actions = [
		f"""
		{{
			"thinking": "I'll navigate to the screenshot test page",
			"evaluation_previous_goal": "Starting task",
			"memory": "Navigating to page",
			"next_goal": "Navigate to test page",
			"action": [
				{{
					"navigate": {{
						"url": "{base_url}/screenshot-page",
						"new_tab": false
					}}
				}}
			]
		}}
		""",
		"""
		{
			"thinking": "Page loaded, completing task",
			"evaluation_previous_goal": "Page loaded",
			"memory": "Task completed",
			"next_goal": "Complete task",
			"action": [
				{
					"done": {
						"text": "Successfully navigated and captured screenshot",
						"success": true
					}
				}
			]
		}
		""",
	]

	mock_llm = create_mock_llm(actions=actions)

	# Create agent with vision enabled
	agent = Agent(
		task=f'Navigate to {base_url}/screenshot-page',
		llm=mock_llm,
		browser_session=browser_session,
		use_vision=True,  # Enable vision/screenshots
	)

	# Run agent
	history = await agent.run(max_steps=2)

	# Verify agent completed successfully
	assert len(history) >= 1, 'Agent should have completed at least 1 step'
	final_result = history.final_result()
	assert final_result is not None, 'Agent should return a final result'

	# Verify screenshots were captured in the history
	screenshot_found = False
	for i, step in enumerate(history.history):
		# Check if browser state has screenshot path
		if step.state and hasattr(step.state, 'screenshot_path') and step.state.screenshot_path:
			screenshot_found = True
			print(f'\n✅ Step {i + 1}: Screenshot captured at {step.state.screenshot_path}')

			# Verify screenshot file exists (it should be saved to disk)
			import os

			assert os.path.exists(step.state.screenshot_path), f'Screenshot file should exist at {step.state.screenshot_path}'

			# Verify screenshot file has content
			screenshot_size = os.path.getsize(step.state.screenshot_path)
			assert screenshot_size > 0, f'Screenshot file should have content, got {screenshot_size} bytes'
			print(f'   Screenshot size: {screenshot_size} bytes')

	assert screenshot_found, 'At least one screenshot should be captured when vision is enabled'

	print('\n🎉 Integration test passed: Screenshots are captured correctly with vision enabled')

```

---

## backend/browser-use/tests/ci/browser/test_session_start.py

```py
"""
Test script for BrowserSession.start() method to ensure proper initialization,
concurrency handling, and error handling.

Tests cover:
- Calling .start() on a session that's already started
- Simultaneously calling .start() from two parallel coroutines
- Calling .start() on a session that's started but has a closed browser connection
- Calling .close() on a session that hasn't been started yet
"""

import asyncio
import logging

import pytest

from browser_use.browser.profile import (
	BROWSERUSE_DEFAULT_CHANNEL,
	BrowserChannel,
	BrowserProfile,
)
from browser_use.browser.session import BrowserSession
from browser_use.config import CONFIG

# Set up test logging
logger = logging.getLogger('browser_session_start_tests')
# logger.setLevel(logging.DEBUG)


# run with pytest -k test_user_data_dir_not_allowed_to_corrupt_default_profile


class TestBrowserSessionStart:
	"""Tests for BrowserSession.start() method initialization and concurrency."""

	@pytest.fixture(scope='module')
	async def browser_profile(self):
		"""Create and provide a BrowserProfile with headless mode."""
		profile = BrowserProfile(headless=True, user_data_dir=None, keep_alive=False)
		yield profile

	@pytest.fixture(scope='function')
	async def browser_session(self, browser_profile):
		"""Create a BrowserSession instance without starting it."""
		session = BrowserSession(browser_profile=browser_profile)
		yield session
		await session.kill()

	async def test_start_already_started_session(self, browser_session):
		"""Test calling .start() on a session that's already started."""
		# logger.info('Testing start on already started session')

		# Start the session for the first time
		await browser_session.start()
		assert browser_session._cdp_client_root is not None

		# Start the session again - should return immediately without re-initialization
		await browser_session.start()
		assert browser_session._cdp_client_root is not None

	# @pytest.mark.skip(reason="Race condition - DOMWatchdog tries to inject scripts into tab that's being closed")
	# async def test_page_lifecycle_management(self, browser_session: BrowserSession):
	# 	"""Test session handles page lifecycle correctly."""
	# 	# logger.info('Testing page lifecycle management')

	# 	# Start the session and get initial state
	# 	await browser_session.start()
	# 	initial_tabs = await browser_session.get_tabs()
	# 	initial_count = len(initial_tabs)

	# 	# Get current tab info
	# 	current_url = await browser_session.get_current_page_url()
	# 	assert current_url is not None

	# 	# Get current tab ID
	# 	current_tab_id = browser_session.agent_focus.target_id if browser_session.agent_focus else None
	# 	assert current_tab_id is not None

	# 	# Close the current tab using the event system
	# 	from browser_use.browser.events import CloseTabEvent

	# 	close_event = browser_session.event_bus.dispatch(CloseTabEvent(target_id=current_tab_id))
	# 	await close_event

	# 	# Operations should still work - may create new page or use existing
	# 	tabs_after_close = await browser_session.get_tabs()
	# 	assert isinstance(tabs_after_close, list)

	# 	# Create a new tab explicitly
	# 	event = browser_session.event_bus.dispatch(NavigateToUrlEvent(url='about:blank', new_tab=True))
	# 	await event
	# 	await event.event_result(raise_if_any=True, raise_if_none=False)

	# 	# Should have at least one tab now
	# 	final_tabs = await browser_session.get_tabs()
	# 	assert len(final_tabs) >= 1

	async def test_user_data_dir_not_allowed_to_corrupt_default_profile(self):
		"""Test user_data_dir handling for different browser channels and version mismatches."""
		# Test 1: Chromium with default user_data_dir and default channel should work fine
		session = BrowserSession(
			browser_profile=BrowserProfile(
				headless=True,
				user_data_dir=CONFIG.BROWSER_USE_DEFAULT_USER_DATA_DIR,
				channel=BROWSERUSE_DEFAULT_CHANNEL,  # chromium
				keep_alive=False,
			),
		)

		try:
			await session.start()
			assert session._cdp_client_root is not None
			# Verify the user_data_dir wasn't changed
			assert session.browser_profile.user_data_dir == CONFIG.BROWSER_USE_DEFAULT_USER_DATA_DIR
		finally:
			await session.kill()

		# Test 2: Chrome with default user_data_dir should change dir AND copy to temp
		profile2 = BrowserProfile(
			headless=True,
			user_data_dir=CONFIG.BROWSER_USE_DEFAULT_USER_DATA_DIR,
			channel=BrowserChannel.CHROME,
			keep_alive=False,
		)

		# The validator should have changed the user_data_dir to avoid corruption
		# And then _copy_profile copies it to a temp directory (Chrome only)
		assert profile2.user_data_dir != CONFIG.BROWSER_USE_DEFAULT_USER_DATA_DIR
		assert 'browser-use-user-data-dir-' in str(profile2.user_data_dir)

		# Test 3: Edge with default user_data_dir should also change
		profile3 = BrowserProfile(
			headless=True,
			user_data_dir=CONFIG.BROWSER_USE_DEFAULT_USER_DATA_DIR,
			channel=BrowserChannel.MSEDGE,
			keep_alive=False,
		)

		assert profile3.user_data_dir != CONFIG.BROWSER_USE_DEFAULT_USER_DATA_DIR
		assert profile3.user_data_dir == CONFIG.BROWSER_USE_DEFAULT_USER_DATA_DIR.parent / 'default-msedge'
		assert 'browser-use-user-data-dir-' not in str(profile3.user_data_dir)


class TestBrowserSessionReusePatterns:
	"""Tests for all browser re-use patterns documented in docs/customize/real-browser.mdx"""

	async def test_sequential_agents_same_profile_different_browser(self, mock_llm):
		"""Test Sequential Agents, Same Profile, Different Browser pattern"""
		from browser_use import Agent
		from browser_use.browser.profile import BrowserProfile

		# Create a reusable profile
		reused_profile = BrowserProfile(
			user_data_dir=None,  # Use temp dir for testing
			headless=True,
		)

		# First agent
		agent1 = Agent(
			task='The first task...',
			llm=mock_llm,
			browser_profile=reused_profile,
		)
		await agent1.run()

		# Verify first agent's session is closed
		assert agent1.browser_session is not None
		assert not agent1.browser_session._cdp_client_root is not None

		# Second agent with same profile
		agent2 = Agent(
			task='The second task...',
			llm=mock_llm,
			browser_profile=reused_profile,
			# Disable memory for tests
		)
		await agent2.run()

		# Verify second agent created a new session
		assert agent2.browser_session is not None
		assert agent1.browser_session is not agent2.browser_session
		assert not agent2.browser_session._cdp_client_root is not None

	async def test_sequential_agents_same_profile_same_browser(self, mock_llm):
		"""Test Sequential Agents, Same Profile, Same Browser pattern"""
		from browser_use import Agent, BrowserSession

		# Create a reusable session with keep_alive
		reused_session = BrowserSession(
			browser_profile=BrowserProfile(
				user_data_dir=None,  # Use temp dir for testing
				headless=True,
				keep_alive=True,  # Don't close browser after agent.run()
			),
		)

		try:
			# Start the session manually (agents will reuse this initialized session)
			await reused_session.start()

			# First agent
			agent1 = Agent(
				task='The first task...',
				llm=mock_llm,
				browser_session=reused_session,
				# Disable memory for tests
			)
			await agent1.run()

			# Verify session is still alive
			assert reused_session._cdp_client_root is not None

			# Second agent reusing the same session
			agent2 = Agent(
				task='The second task...',
				llm=mock_llm,
				browser_session=reused_session,
				# Disable memory for tests
			)
			await agent2.run()

			# Verify same browser was used (using __eq__ to check browser_pid, cdp_url)
			assert agent1.browser_session == agent2.browser_session
			assert agent1.browser_session == reused_session
			assert reused_session._cdp_client_root is not None

		finally:
			await reused_session.kill()


class TestBrowserSessionEventSystem:
	"""Tests for the new event system integration in BrowserSession."""

	@pytest.fixture(scope='function')
	async def browser_session(self):
		"""Create a BrowserSession instance for event system testing."""
		profile = BrowserProfile(headless=True, user_data_dir=None, keep_alive=False)
		session = BrowserSession(browser_profile=profile)
		yield session
		await session.kill()

	async def test_event_bus_initialization(self, browser_session):
		"""Test that event bus is properly initialized with unique name."""
		# Event bus should be created during __init__
		assert browser_session.event_bus is not None
		assert browser_session.event_bus.name.startswith('EventBus_')
		# Event bus name format may vary, just check it exists

	async def test_event_handlers_registration(self, browser_session: BrowserSession):
		"""Test that event handlers are properly registered."""
		# Attach all watchdogs to register their handlers
		await browser_session.attach_all_watchdogs()

		# Check that handlers are registered in the event bus
		from browser_use.browser.events import (
			BrowserStartEvent,
			BrowserStateRequestEvent,
			BrowserStopEvent,
			ClickElementEvent,
			CloseTabEvent,
			ScreenshotEvent,
			ScrollEvent,
			TypeTextEvent,
		)

		# These event types should have handlers registered
		event_types_with_handlers = [
			BrowserStartEvent,
			BrowserStopEvent,
			ClickElementEvent,
			TypeTextEvent,
			ScrollEvent,
			CloseTabEvent,
			BrowserStateRequestEvent,
			ScreenshotEvent,
		]

		for event_type in event_types_with_handlers:
			handlers = browser_session.event_bus.handlers.get(event_type.__name__, [])
			assert len(handlers) > 0, f'No handlers registered for {event_type.__name__}'

	async def test_direct_event_dispatching(self, browser_session):
		"""Test direct event dispatching without using the public API."""
		from browser_use.browser.events import BrowserConnectedEvent, BrowserStartEvent

		# Dispatch BrowserStartEvent directly
		start_event = browser_session.event_bus.dispatch(BrowserStartEvent())

		# Wait for event to complete
		await start_event

		# Check if BrowserConnectedEvent was dispatched
		assert browser_session._cdp_client_root is not None

		# Check event history
		event_history = list(browser_session.event_bus.event_history.values())
		assert len(event_history) >= 2  # BrowserStartEvent + BrowserConnectedEvent + others

		# Find the BrowserConnectedEvent in history
		started_events = [e for e in event_history if isinstance(e, BrowserConnectedEvent)]
		assert len(started_events) >= 1
		assert started_events[0].cdp_url is not None

	async def test_event_system_error_handling(self, browser_session):
		"""Test error handling in event system."""
		from browser_use.browser.events import BrowserStartEvent

		# Create session with invalid CDP URL to trigger error
		error_session = BrowserSession(
			browser_profile=BrowserProfile(headless=True),
			cdp_url='http://localhost:99999',  # Invalid port
		)

		try:
			# Dispatch start event directly - should trigger error handling
			start_event = error_session.event_bus.dispatch(BrowserStartEvent())

			# The event bus catches and logs the error, but the event awaits successfully
			await start_event

			# The session should not be initialized due to the error
			assert error_session._cdp_client_root is None, 'Session should not be initialized after connection error'

			# Verify the error was logged in the event history (good enough for error handling test)
			assert len(error_session.event_bus.event_history) > 0, 'Event should be tracked even with errors'

		finally:
			await error_session.kill()

	async def test_concurrent_event_dispatching(self, browser_session: BrowserSession):
		"""Test that concurrent events are handled properly."""
		from browser_use.browser.events import ScreenshotEvent

		# Start browser first
		await browser_session.start()

		# Dispatch multiple events concurrently
		screenshot_event1 = browser_session.event_bus.dispatch(ScreenshotEvent())
		screenshot_event2 = browser_session.event_bus.dispatch(ScreenshotEvent())

		# Both should complete successfully
		results = await asyncio.gather(screenshot_event1, screenshot_event2, return_exceptions=True)

		# Check that no exceptions were raised
		for result in results:
			assert not isinstance(result, Exception), f'Event failed with: {result}'

	# async def test_many_parallel_browser_sessions(self):
	# 	"""Test spawning 12 parallel browser_sessions with different settings and ensure they all work"""
	# 	from browser_use import BrowserSession

	# 	browser_sessions = []

	# 	for i in range(3):
	# 		browser_sessions.append(
	# 			BrowserSession(
	# 				browser_profile=BrowserProfile(
	# 					user_data_dir=None,
	# 					headless=True,
	# 					keep_alive=True,
	# 				),
	# 			)
	# 		)
	# 	for i in range(3):
	# 		browser_sessions.append(
	# 			BrowserSession(
	# 				browser_profile=BrowserProfile(
	# 					user_data_dir=Path(tempfile.mkdtemp(prefix=f'browseruse-tmp-{i}')),
	# 					headless=True,
	# 					keep_alive=True,
	# 				),
	# 			)
	# 		)
	# 	for i in range(3):
	# 		browser_sessions.append(
	# 			BrowserSession(
	# 				browser_profile=BrowserProfile(
	# 					user_data_dir=None,
	# 					headless=True,
	# 					keep_alive=False,
	# 				),
	# 			)
	# 		)
	# 	for i in range(3):
	# 		browser_sessions.append(
	# 			BrowserSession(
	# 				browser_profile=BrowserProfile(
	# 					user_data_dir=Path(tempfile.mkdtemp(prefix=f'browseruse-tmp-{i}')),
	# 					headless=True,
	# 					keep_alive=False,
	# 				),
	# 			)
	# 		)

	# 	print('Starting many parallel browser sessions...')
	# 	await asyncio.gather(*[browser_session.start() for browser_session in browser_sessions])

	# 	print('Ensuring all parallel browser sessions are connected and usable...')
	# 	new_tab_tasks = []
	# 	for browser_session in browser_sessions:
	# 		assert browser_session._cdp_client_root is not None
	# 		assert browser_session._cdp_client_root is not None
	# 		new_tab_tasks.append(browser_session.create_new_tab('chrome://version'))
	# 	await asyncio.gather(*new_tab_tasks)

	# 	print('killing every 3rd browser_session to test parallel shutdown')
	# 	kill_tasks = []
	# 	for i in range(0, len(browser_sessions), 3):
	# 		kill_tasks.append(browser_sessions[i].kill())
	# 		browser_sessions[i] = None
	# 	results = await asyncio.gather(*kill_tasks, return_exceptions=True)
	# 	# Check that no exceptions were raised during cleanup
	# 	for i, result in enumerate(results):
	# 		if isinstance(result, Exception):
	# 			print(f'Warning: Browser session kill raised exception: {type(result).__name__}: {result}')

	# 	print('ensuring the remaining browser_sessions are still connected and usable')
	# 	new_tab_tasks = []
	# 	screenshot_tasks = []
	# 	for browser_session in filter(bool, browser_sessions):
	# 		assert browser_session._cdp_client_root is not None
	# 		assert browser_session._cdp_client_root is not None
	# 		new_tab_tasks.append(browser_session.create_new_tab('chrome://version'))
	# 		screenshot_tasks.append(browser_session.take_screenshot())
	# 	await asyncio.gather(*new_tab_tasks)
	# 	await asyncio.gather(*screenshot_tasks)

	# 	kill_tasks = []
	# 	print('killing the remaining browser_sessions')
	# 	for browser_session in filter(bool, browser_sessions):
	# 		kill_tasks.append(browser_session.kill())
	# 	results = await asyncio.gather(*kill_tasks, return_exceptions=True)
	# 	# Check that no exceptions were raised during cleanup
	# 	for i, result in enumerate(results):
	# 		if isinstance(result, Exception):
	# 			print(f'Warning: Browser session kill raised exception: {type(result).__name__}: {result}')

```

---

## backend/browser-use/tests/ci/browser/test_tabs.py

```py
"""
Test multi-tab operations: creation, switching, closing, and background tabs.

Tests verify that:
1. Agent can create multiple tabs (3) and switch between them
2. Agent can close tabs with vision=True
3. Agent can handle buttons that open new tabs in background
4. Agent can continue and call done() after each tab operation
5. Browser state doesn't timeout during background tab operations

All tests use:
- max_steps=5 to allow multiple tab operations
- 120s timeout to fail if test takes too long
- Mock LLM to verify agent can still make decisions after tab operations

Usage:
	uv run pytest tests/ci/browser/test_tabs.py -v -s
"""

import asyncio
import time

import pytest
from pytest_httpserver import HTTPServer

from browser_use.agent.service import Agent
from browser_use.browser import BrowserSession
from browser_use.browser.profile import BrowserProfile
from tests.ci.conftest import create_mock_llm


@pytest.fixture(scope='session')
def http_server():
	"""Create and provide a test HTTP server for tab tests."""
	server = HTTPServer()
	server.start()

	# Route 1: Home page
	server.expect_request('/home').respond_with_data(
		'<html><head><title>Home Page</title></head><body><h1>Home Page</h1><p>This is the home page</p></body></html>',
		content_type='text/html',
	)

	# Route 2: Page 1
	server.expect_request('/page1').respond_with_data(
		'<html><head><title>Page 1</title></head><body><h1>Page 1</h1><p>First test page</p></body></html>',
		content_type='text/html',
	)

	# Route 3: Page 2
	server.expect_request('/page2').respond_with_data(
		'<html><head><title>Page 2</title></head><body><h1>Page 2</h1><p>Second test page</p></body></html>',
		content_type='text/html',
	)

	# Route 4: Page 3
	server.expect_request('/page3').respond_with_data(
		'<html><head><title>Page 3</title></head><body><h1>Page 3</h1><p>Third test page</p></body></html>',
		content_type='text/html',
	)

	# Route 5: Background tab page - has a link that opens a new tab in the background
	server.expect_request('/background-tab-test').respond_with_data(
		"""
		<!DOCTYPE html>
		<html>
		<head><title>Background Tab Test</title></head>
		<body style="padding: 20px; font-family: Arial;">
			<h1>Background Tab Test</h1>
			<p>Click the link below to open a new tab in the background:</p>
			<a href="/page3" target="_blank" id="open-tab-link">Open New Tab (link)</a>
			<br><br>
			<button id="open-tab-btn" onclick="window.open('/page3', '_blank'); document.getElementById('status').textContent='Tab opened!'">
				Open New Tab (button)
			</button>
			<p id="status" style="margin-top: 20px; color: green;"></p>
		</body>
		</html>
		""",
		content_type='text/html',
	)

	yield server
	server.stop()


@pytest.fixture(scope='session')
def base_url(http_server):
	"""Return the base URL for the test HTTP server."""
	return f'http://{http_server.host}:{http_server.port}'


@pytest.fixture(scope='function')
async def browser_session():
	"""Create a browser session for tab tests."""
	session = BrowserSession(
		browser_profile=BrowserProfile(
			headless=True,
			user_data_dir=None,
			keep_alive=True,
		)
	)
	await session.start()
	yield session
	await session.kill()


class TestMultiTabOperations:
	"""Test multi-tab creation, switching, and closing."""

	async def test_create_and_switch_three_tabs(self, browser_session, base_url):
		"""Test that agent can create 3 tabs, switch between them, and call done().

		This test verifies that browser state is retrieved between each step.
		"""
		start_time = time.time()

		actions = [
			# Action 1: Navigate to home page
			f"""
			{{
				"thinking": "I'll start by navigating to the home page",
				"evaluation_previous_goal": "Starting task",
				"memory": "Navigating to home page",
				"next_goal": "Navigate to home page",
				"action": [
					{{
						"navigate": {{
							"url": "{base_url}/home",
							"new_tab": false
						}}
					}}
				]
			}}
			""",
			# Action 2: Open page1 in new tab
			f"""
			{{
				"thinking": "Now I'll open page 1 in a new tab",
				"evaluation_previous_goal": "Home page loaded",
				"memory": "Opening page 1 in new tab",
				"next_goal": "Open page 1 in new tab",
				"action": [
					{{
						"navigate": {{
							"url": "{base_url}/page1",
							"new_tab": true
						}}
					}}
				]
			}}
			""",
			# Action 3: Open page2 in new tab
			f"""
			{{
				"thinking": "Now I'll open page 2 in a new tab",
				"evaluation_previous_goal": "Page 1 opened in new tab",
				"memory": "Opening page 2 in new tab",
				"next_goal": "Open page 2 in new tab",
				"action": [
					{{
						"navigate": {{
							"url": "{base_url}/page2",
							"new_tab": true
						}}
					}}
				]
			}}
			""",
			# Action 4: Switch to first tab
			"""
			{
				"thinking": "Now I'll switch back to the first tab",
				"evaluation_previous_goal": "Page 2 opened in new tab",
				"memory": "Switching to first tab",
				"next_goal": "Switch to first tab",
				"action": [
					{
						"switch": {
							"tab_id": "0000"
						}
					}
				]
			}
			""",
			# Action 5: Done
			"""
			{
				"thinking": "I've successfully created 3 tabs and switched between them",
				"evaluation_previous_goal": "Switched to first tab",
				"memory": "All tabs created and switched",
				"next_goal": "Complete task",
				"action": [
					{
						"done": {
							"text": "Successfully created 3 tabs and switched between them",
							"success": true
						}
					}
				]
			}
			""",
		]

		mock_llm = create_mock_llm(actions=actions)

		agent = Agent(
			task=f'Navigate to {base_url}/home, then open {base_url}/page1 and {base_url}/page2 in new tabs, then switch back to the first tab',
			llm=mock_llm,
			browser_session=browser_session,
		)

		# Run with timeout - should complete within 2 minutes
		try:
			history = await asyncio.wait_for(agent.run(max_steps=5), timeout=120)
			elapsed = time.time() - start_time

			print(f'\n⏱️  Test completed in {elapsed:.2f} seconds')
			print(f'📊 Completed {len(history)} steps')

			# Verify each step has browser state
			for i, step in enumerate(history.history):
				assert step.state is not None, f'Step {i} should have browser state'
				assert step.state.url is not None, f'Step {i} should have URL in browser state'
				print(f'  Step {i + 1}: URL={step.state.url}, tabs={len(step.state.tabs) if step.state.tabs else 0}')

			assert len(history) >= 4, 'Agent should have completed at least 4 steps'

			# Verify we have 3 tabs open
			tabs = await browser_session.get_tabs()
			assert len(tabs) >= 3, f'Should have at least 3 tabs open, got {len(tabs)}'

			# Verify agent completed successfully
			final_result = history.final_result()
			assert final_result is not None, 'Agent should return a final result'
			assert 'Successfully' in final_result, 'Agent should report success'

			# Note: Test is fast (< 1s) because mock LLM returns instantly and pages are simple,
			# but browser state IS being retrieved correctly between steps as verified above
		except TimeoutError:
			pytest.fail('Test timed out after 2 minutes - agent hung during tab operations')

	async def test_close_tab_with_vision(self, browser_session, base_url):
		"""Test that agent can close a tab with vision=True and call done()."""

		actions = [
			# Action 1: Navigate to home page
			f"""
			{{
				"thinking": "I'll start by navigating to the home page",
				"evaluation_previous_goal": "Starting task",
				"memory": "Navigating to home page",
				"next_goal": "Navigate to home page",
				"action": [
					{{
						"navigate": {{
							"url": "{base_url}/home",
							"new_tab": false
						}}
					}}
				]
			}}
			""",
			# Action 2: Open page1 in new tab
			f"""
			{{
				"thinking": "Now I'll open page 1 in a new tab",
				"evaluation_previous_goal": "Home page loaded",
				"memory": "Opening page 1 in new tab",
				"next_goal": "Open page 1 in new tab",
				"action": [
					{{
						"navigate": {{
							"url": "{base_url}/page1",
							"new_tab": true
						}}
					}}
				]
			}}
			""",
			# Action 3: Close the current tab
			"""
			{
				"thinking": "Now I'll close the current tab (page1)",
				"evaluation_previous_goal": "Page 1 opened in new tab",
				"memory": "Closing current tab",
				"next_goal": "Close current tab",
				"action": [
					{
						"close": {
							"tab_id": "0001"
						}
					}
				]
			}
			""",
			# Action 4: Done
			"""
			{
				"thinking": "I've successfully closed the tab",
				"evaluation_previous_goal": "Tab closed",
				"memory": "Tab closed successfully",
				"next_goal": "Complete task",
				"action": [
					{
						"done": {
							"text": "Successfully closed the tab",
							"success": true
						}
					}
				]
			}
			""",
		]

		mock_llm = create_mock_llm(actions=actions)

		agent = Agent(
			task=f'Navigate to {base_url}/home, then open {base_url}/page1 in a new tab, then close the page1 tab',
			llm=mock_llm,
			browser_session=browser_session,
			use_vision=True,  # Enable vision for this test
		)

		# Run with timeout - should complete within 2 minutes
		try:
			history = await asyncio.wait_for(agent.run(max_steps=5), timeout=120)
			assert len(history) >= 3, 'Agent should have completed at least 3 steps'

			# Verify agent completed successfully
			final_result = history.final_result()
			assert final_result is not None, 'Agent should return a final result'
			assert 'Successfully' in final_result, 'Agent should report success'
		except TimeoutError:
			pytest.fail('Test timed out after 2 minutes - agent hung during tab closing with vision')

	async def test_background_tab_open_no_timeout(self, browser_session, base_url):
		"""Test that browser state doesn't timeout when a new tab opens in the background."""
		start_time = time.time()

		actions = [
			# Action 1: Navigate to home page
			f"""
			{{
				"thinking": "I'll navigate to the home page first",
				"evaluation_previous_goal": "Starting task",
				"memory": "Navigating to home page",
				"next_goal": "Navigate to home page",
				"action": [
					{{
						"navigate": {{
							"url": "{base_url}/home",
							"new_tab": false
						}}
					}}
				]
			}}
			""",
			# Action 2: Open page1 in new background tab (stay on home page)
			f"""
			{{
				"thinking": "I'll open page1 in a new background tab",
				"evaluation_previous_goal": "Home page loaded",
				"memory": "Opening background tab",
				"next_goal": "Open background tab without switching to it",
				"action": [
					{{
						"navigate": {{
							"url": "{base_url}/page1",
							"new_tab": true
						}}
					}}
				]
			}}
			""",
			# Action 3: Immediately check browser state after background tab opens
			"""
			{
				"thinking": "After opening background tab, browser state should still be accessible",
				"evaluation_previous_goal": "Background tab opened",
				"memory": "Verifying browser state works",
				"next_goal": "Complete task",
				"action": [
					{
						"done": {
							"text": "Successfully opened background tab, browser state remains accessible",
							"success": true
						}
					}
				]
			}
			""",
		]

		mock_llm = create_mock_llm(actions=actions)

		agent = Agent(
			task=f'Navigate to {base_url}/home and open {base_url}/page1 in a new tab',
			llm=mock_llm,
			browser_session=browser_session,
		)

		# Run with timeout - this tests if browser state times out when new tabs open
		try:
			history = await asyncio.wait_for(agent.run(max_steps=3), timeout=120)
			elapsed = time.time() - start_time

			print(f'\n⏱️  Test completed in {elapsed:.2f} seconds')
			print(f'📊 Completed {len(history)} steps')

			# Verify each step has browser state (the key test - no timeouts)
			for i, step in enumerate(history.history):
				assert step.state is not None, f'Step {i} should have browser state'
				assert step.state.url is not None, f'Step {i} should have URL in browser state'
				print(f'  Step {i + 1}: URL={step.state.url}, tabs={len(step.state.tabs) if step.state.tabs else 0}')

			assert len(history) >= 2, 'Agent should have completed at least 2 steps'

			# Verify agent completed successfully
			final_result = history.final_result()
			assert final_result is not None, 'Agent should return a final result'
			assert 'Successfully' in final_result, 'Agent should report success'

			# Verify we have at least 2 tabs
			tabs = await browser_session.get_tabs()
			print(f'  Final tab count: {len(tabs)}')
			assert len(tabs) >= 2, f'Should have at least 2 tabs after opening background tab, got {len(tabs)}'

		except TimeoutError:
			pytest.fail('Test timed out after 2 minutes - browser state timed out after opening background tab')

	async def test_rapid_tab_operations_no_timeout(self, browser_session, base_url):
		"""Test that browser state doesn't timeout during rapid tab operations."""

		actions = [
			# Action 1: Navigate to home page
			f"""
			{{
				"thinking": "I'll navigate to the home page",
				"evaluation_previous_goal": "Starting task",
				"memory": "Navigating to home page",
				"next_goal": "Navigate to home page",
				"action": [
					{{
						"navigate": {{
							"url": "{base_url}/home",
							"new_tab": false
						}}
					}}
				]
			}}
			""",
			# Action 2: Open page1 in new tab
			f"""
			{{
				"thinking": "Opening page1 in new tab",
				"evaluation_previous_goal": "Home page loaded",
				"memory": "Opening page1",
				"next_goal": "Open page1",
				"action": [
					{{
						"navigate": {{
							"url": "{base_url}/page1",
							"new_tab": true
						}}
					}}
				]
			}}
			""",
			# Action 3: Open page2 in new tab
			f"""
			{{
				"thinking": "Opening page2 in new tab",
				"evaluation_previous_goal": "Page1 opened",
				"memory": "Opening page2",
				"next_goal": "Open page2",
				"action": [
					{{
						"navigate": {{
							"url": "{base_url}/page2",
							"new_tab": true
						}}
					}}
				]
			}}
			""",
			# Action 4: Open page3 in new tab
			f"""
			{{
				"thinking": "Opening page3 in new tab",
				"evaluation_previous_goal": "Page2 opened",
				"memory": "Opening page3",
				"next_goal": "Open page3",
				"action": [
					{{
						"navigate": {{
							"url": "{base_url}/page3",
							"new_tab": true
						}}
					}}
				]
			}}
			""",
			# Action 5: Verify browser state is still accessible
			"""
			{
				"thinking": "All tabs opened rapidly, browser state should still be accessible",
				"evaluation_previous_goal": "Page3 opened",
				"memory": "All tabs opened",
				"next_goal": "Complete task",
				"action": [
					{
						"done": {
							"text": "Successfully opened 4 tabs rapidly without timeout",
							"success": true
						}
					}
				]
			}
			""",
		]

		mock_llm = create_mock_llm(actions=actions)

		agent = Agent(
			task='Open multiple tabs rapidly and verify browser state remains accessible',
			llm=mock_llm,
			browser_session=browser_session,
		)

		# Run with timeout - should complete within 2 minutes
		try:
			history = await asyncio.wait_for(agent.run(max_steps=5), timeout=120)
			assert len(history) >= 4, 'Agent should have completed at least 4 steps'

			# Verify we have 4 tabs open
			tabs = await browser_session.get_tabs()
			assert len(tabs) >= 4, f'Should have at least 4 tabs open, got {len(tabs)}'

			# Verify agent completed successfully
			final_result = history.final_result()
			assert final_result is not None, 'Agent should return a final result'
			assert 'Successfully' in final_result, 'Agent should report success'
		except TimeoutError:
			pytest.fail('Test timed out after 2 minutes - browser state timed out during rapid tab operations')

	async def test_multiple_tab_switches_and_close(self, browser_session, base_url):
		"""Test that agent can switch between multiple tabs and close one."""

		actions = [
			# Action 1: Navigate to home page
			f"""
			{{
				"thinking": "I'll start by navigating to the home page",
				"evaluation_previous_goal": "Starting task",
				"memory": "Navigating to home page",
				"next_goal": "Navigate to home page",
				"action": [
					{{
						"navigate": {{
							"url": "{base_url}/home",
							"new_tab": false
						}}
					}}
				]
			}}
			""",
			# Action 2: Open page1 in new tab
			f"""
			{{
				"thinking": "Opening page 1 in new tab",
				"evaluation_previous_goal": "Home page loaded",
				"memory": "Opening page 1",
				"next_goal": "Open page 1",
				"action": [
					{{
						"navigate": {{
							"url": "{base_url}/page1",
							"new_tab": true
						}}
					}}
				]
			}}
			""",
			# Action 3: Open page2 in new tab
			f"""
			{{
				"thinking": "Opening page 2 in new tab",
				"evaluation_previous_goal": "Page 1 opened",
				"memory": "Opening page 2",
				"next_goal": "Open page 2",
				"action": [
					{{
						"navigate": {{
							"url": "{base_url}/page2",
							"new_tab": true
						}}
					}}
				]
			}}
			""",
			# Action 4: Switch to tab 1
			"""
			{
				"thinking": "Switching to tab 1 (page1)",
				"evaluation_previous_goal": "Page 2 opened",
				"memory": "Switching to page 1",
				"next_goal": "Switch to page 1",
				"action": [
					{
						"switch": {
							"tab_id": "0001"
						}
					}
				]
			}
			""",
			# Action 5: Close current tab
			"""
			{
				"thinking": "Closing the current tab (page1)",
				"evaluation_previous_goal": "Switched to page 1",
				"memory": "Closing page 1",
				"next_goal": "Close page 1",
				"action": [
					{
						"close": {
							"tab_id": "0001"
						}
					}
				]
			}
			""",
			# Action 6: Done
			"""
			{
				"thinking": "Successfully completed all tab operations",
				"evaluation_previous_goal": "Tab closed",
				"memory": "All operations completed",
				"next_goal": "Complete task",
				"action": [
					{
						"done": {
							"text": "Successfully created, switched, and closed tabs",
							"success": true
						}
					}
				]
			}
			""",
		]

		mock_llm = create_mock_llm(actions=actions)

		agent = Agent(
			task='Create 3 tabs, switch to the second one, then close it',
			llm=mock_llm,
			browser_session=browser_session,
		)

		# Run with timeout - should complete within 2 minutes
		try:
			history = await asyncio.wait_for(agent.run(max_steps=6), timeout=120)
			assert len(history) >= 5, 'Agent should have completed at least 5 steps'

			# Verify agent completed successfully
			final_result = history.final_result()
			assert final_result is not None, 'Agent should return a final result'
			assert 'Successfully' in final_result, 'Agent should report success'
		except TimeoutError:
			pytest.fail('Test timed out after 2 minutes - agent hung during multiple tab operations')

```

---

## backend/browser-use/tests/ci/browser/test_true_cross_origin_click.py

```py
"""Test clicking elements inside TRUE cross-origin iframes (external domains)."""

import asyncio

import pytest

from browser_use.browser.profile import BrowserProfile, ViewportSize
from browser_use.browser.session import BrowserSession
from browser_use.tools.service import Tools


@pytest.fixture
async def browser_session():
	"""Create browser session with cross-origin iframe support."""
	session = BrowserSession(
		browser_profile=BrowserProfile(
			headless=True,
			user_data_dir=None,
			keep_alive=True,
			window_size=ViewportSize(width=1920, height=1400),
			cross_origin_iframes=True,  # Enable cross-origin iframe extraction
		)
	)
	await session.start()
	yield session
	await session.kill()


class TestTrueCrossOriginIframeClick:
	"""Test clicking elements inside true cross-origin iframes."""

	async def test_click_element_in_true_cross_origin_iframe(self, httpserver, browser_session: BrowserSession):
		"""Verify that elements inside TRUE cross-origin iframes (example.com) can be clicked.

		This test uses example.com which is a real external domain, testing actual cross-origin
		iframe extraction and clicking via CDP target switching.
		"""

		# Create main page with TRUE cross-origin iframe pointing to example.com
		main_html = """
		<!DOCTYPE html>
		<html>
		<head><title>True Cross-Origin Test</title></head>
		<body>
			<h1>Main Page</h1>
			<button id="main-button">Main Button</button>
			<iframe id="cross-origin" src="https://example.com" style="width: 800px; height: 600px;"></iframe>
		</body>
		</html>
		"""

		# Serve the main page
		httpserver.expect_request('/true-cross-origin-test').respond_with_data(main_html, content_type='text/html')
		url = httpserver.url_for('/true-cross-origin-test')

		# Navigate to the page
		await browser_session.navigate_to(url)

		# Wait for cross-origin iframe to load (network request)
		await asyncio.sleep(5)

		# Get DOM state with cross-origin iframe extraction enabled
		browser_state = await browser_session.get_browser_state_summary(
			include_screenshot=False,
			include_recent_events=False,
		)
		assert browser_state.dom_state is not None
		state = browser_state.dom_state

		print(f'\n📊 Found {len(state.selector_map)} total elements')

		# Find elements from different targets
		targets_found = set()
		main_page_elements = []
		cross_origin_elements = []

		for idx, element in state.selector_map.items():
			target_id = element.target_id
			targets_found.add(target_id)

			# Check if element is from cross-origin iframe (example.com)
			# Look for links - example.com has a link to iana.org/domains/reserved
			if element.attributes:
				href = element.attributes.get('href', '')
				element_id = element.attributes.get('id', '')

				# example.com has a link to iana.org/domains/reserved
				if 'iana.org' in href:
					cross_origin_elements.append((idx, element))
					print(f'   ✅ Found cross-origin element: [{idx}] {element.tag_name} href={href}')
				elif element_id == 'main-button':
					main_page_elements.append((idx, element))

		# Verify we found elements from at least 2 different targets
		print(f'\n🎯 Found elements from {len(targets_found)} different CDP targets')

		# Check if cross-origin iframe loaded
		if len(targets_found) < 2:
			print('⚠️  Warning: Cross-origin iframe did not create separate CDP target')
			print('   This may indicate cross_origin_iframes feature is not working as expected')
			pytest.skip('Cross-origin iframe did not create separate CDP target - skipping test')

		if len(cross_origin_elements) == 0:
			print('⚠️  Warning: No elements found from example.com iframe')
			print('   Network may be restricted in CI environment')
			pytest.skip('No elements extracted from example.com - skipping click test')

		# Verify we found at least one element from the cross-origin iframe
		assert len(cross_origin_elements) > 0, 'Expected to find at least one element from cross-origin iframe (example.com)'

		# Try clicking the cross-origin element
		print('\n🖱️  Testing Click on True Cross-Origin Iframe Element:')
		tools = Tools()

		link_idx, link_element = cross_origin_elements[0]
		print(f'   Attempting to click element [{link_idx}] from example.com iframe...')

		try:
			result = await tools.click(index=link_idx, browser_session=browser_session)

			# Check for errors
			if result.error:
				pytest.fail(f'Click on cross-origin element [{link_idx}] failed with error: {result.error}')

			if result.extracted_content and (
				'not available' in result.extracted_content.lower() or 'failed' in result.extracted_content.lower()
			):
				pytest.fail(f'Click on cross-origin element [{link_idx}] failed: {result.extracted_content}')

			print(f'   ✅ Click succeeded on cross-origin element [{link_idx}]!')
			print('   🎉 True cross-origin iframe element clicking works!')

		except Exception as e:
			pytest.fail(f'Exception while clicking cross-origin element [{link_idx}]: {e}')

		print('\n✅ Test passed: True cross-origin iframe elements can be clicked')

```

---

## backend/browser-use/tests/ci/conftest.py

```py
"""
Pytest configuration for browser-use CI tests.

Sets up environment variables to ensure tests never connect to production services.
"""

import os
import socketserver
import tempfile
from unittest.mock import AsyncMock

import pytest
from dotenv import load_dotenv
from pytest_httpserver import HTTPServer

# Fix for httpserver hanging on shutdown - prevent blocking on socket close
# This prevents tests from hanging when shutting down HTTP servers
socketserver.ThreadingMixIn.block_on_close = False
# Also set daemon threads to prevent hanging
socketserver.ThreadingMixIn.daemon_threads = True

from browser_use.agent.views import AgentOutput
from browser_use.llm import BaseChatModel
from browser_use.llm.views import ChatInvokeCompletion
from browser_use.tools.service import Tools

# Load environment variables before any imports
load_dotenv()


# Skip LLM API key verification for tests
os.environ['SKIP_LLM_API_KEY_VERIFICATION'] = 'true'

from bubus import BaseEvent

from browser_use import Agent
from browser_use.browser import BrowserProfile, BrowserSession
from browser_use.sync.service import CloudSync


@pytest.fixture(autouse=True)
def setup_test_environment():
	"""
	Automatically set up test environment for all tests.
	"""

	# Create a temporary directory for test config (but not for extensions)
	config_dir = tempfile.mkdtemp(prefix='browseruse_tests_')

	original_env = {}
	test_env_vars = {
		'SKIP_LLM_API_KEY_VERIFICATION': 'true',
		'ANONYMIZED_TELEMETRY': 'false',
		'BROWSER_USE_CLOUD_SYNC': 'true',
		'BROWSER_USE_CLOUD_API_URL': 'http://placeholder-will-be-replaced-by-specific-test-fixtures',
		'BROWSER_USE_CLOUD_UI_URL': 'http://placeholder-will-be-replaced-by-specific-test-fixtures',
		# Don't set BROWSER_USE_CONFIG_DIR anymore - let it use the default ~/.config/browseruse
		# This way extensions will be cached in ~/.config/browseruse/extensions
	}

	for key, value in test_env_vars.items():
		original_env[key] = os.environ.get(key)
		os.environ[key] = value

	yield

	# Restore original environment
	for key, value in original_env.items():
		if value is None:
			os.environ.pop(key, None)
		else:
			os.environ[key] = value


# not a fixture, mock_llm() provides this in a fixture below, this is a helper so that it can accept args
def create_mock_llm(actions: list[str] | None = None) -> BaseChatModel:
	"""Create a mock LLM that returns specified actions or a default done action.

	Args:
		actions: Optional list of JSON strings representing actions to return in sequence.
			If not provided, returns a single done action.
			After all actions are exhausted, returns a done action.

	Returns:
		Mock LLM that will return the actions in order, or just a done action if no actions provided.
	"""
	tools = Tools()
	ActionModel = tools.registry.create_action_model()
	AgentOutputWithActions = AgentOutput.type_with_custom_actions(ActionModel)

	llm = AsyncMock(spec=BaseChatModel)
	llm.model = 'mock-llm'
	llm._verified_api_keys = True

	# Add missing properties from BaseChatModel protocol
	llm.provider = 'mock'
	llm.name = 'mock-llm'
	llm.model_name = 'mock-llm'  # Ensure this returns a string, not a mock

	# Default done action
	default_done_action = """
	{
		"thinking": "null",
		"evaluation_previous_goal": "Successfully completed the task",
		"memory": "Task completed",
		"next_goal": "Task completed",
		"action": [
			{
				"done": {
					"text": "Task completed successfully",
					"success": true
				}
			}
		]
	}
	"""

	# Unified logic for both cases
	action_index = 0

	def get_next_action() -> str:
		nonlocal action_index
		if actions is not None and action_index < len(actions):
			action = actions[action_index]
			action_index += 1
			return action
		else:
			return default_done_action

	async def mock_ainvoke(*args, **kwargs):
		# Check if output_format is provided (2nd argument or in kwargs)
		output_format = None
		if len(args) >= 2:
			output_format = args[1]
		elif 'output_format' in kwargs:
			output_format = kwargs['output_format']

		action_json = get_next_action()

		if output_format is None:
			# Return string completion
			return ChatInvokeCompletion(completion=action_json, usage=None)
		else:
			# Parse with provided output_format (could be AgentOutputWithActions or another model)
			if output_format == AgentOutputWithActions:
				parsed = AgentOutputWithActions.model_validate_json(action_json)
			else:
				# For other output formats, try to parse the JSON with that model
				parsed = output_format.model_validate_json(action_json)
			return ChatInvokeCompletion(completion=parsed, usage=None)

	llm.ainvoke.side_effect = mock_ainvoke

	return llm


@pytest.fixture(scope='module')
async def browser_session():
	"""Create a real browser session for testing"""
	session = BrowserSession(
		browser_profile=BrowserProfile(
			headless=True,
			user_data_dir=None,  # Use temporary directory
			keep_alive=True,
			enable_default_extensions=True,  # Enable extensions during tests
		)
	)
	await session.start()
	yield session
	await session.kill()
	# Ensure event bus is properly stopped
	await session.event_bus.stop(clear=True, timeout=5)


@pytest.fixture(scope='function')
def cloud_sync(httpserver: HTTPServer):
	"""
	Create a CloudSync instance configured for testing.

	This fixture creates a real CloudSync instance and sets up the test environment
	to use the httpserver URLs.
	"""

	# Set up test environment
	test_http_server_url = httpserver.url_for('')
	os.environ['BROWSER_USE_CLOUD_API_URL'] = test_http_server_url
	os.environ['BROWSER_USE_CLOUD_UI_URL'] = test_http_server_url
	os.environ['BROWSER_USE_CLOUD_SYNC'] = 'true'

	# Create CloudSync with test server URL
	cloud_sync = CloudSync(
		base_url=test_http_server_url,
	)

	return cloud_sync


@pytest.fixture(scope='function')
def mock_llm():
	"""Create a mock LLM that just returns the done action if queried"""
	return create_mock_llm(actions=None)


@pytest.fixture(scope='function')
def agent_with_cloud(browser_session, mock_llm, cloud_sync):
	"""Create agent (cloud_sync parameter removed)."""
	agent = Agent(
		task='Test task',
		llm=mock_llm,
		browser_session=browser_session,
	)
	return agent


@pytest.fixture(scope='function')
def event_collector():
	"""Helper to collect all events emitted during tests"""
	events = []
	event_order = []

	class EventCollector:
		def __init__(self):
			self.events = events
			self.event_order = event_order

		async def collect_event(self, event: BaseEvent):
			self.events.append(event)
			self.event_order.append(event.event_type)
			return 'collected'

		def get_events_by_type(self, event_type: str) -> list[BaseEvent]:
			return [e for e in self.events if e.event_type == event_type]

		def clear(self):
			self.events.clear()
			self.event_order.clear()

	return EventCollector()

```

---

## backend/browser-use/tests/ci/evaluate_tasks.py

```py
"""
Runs all agent tasks in parallel (up to 10 at a time) using separate subprocesses.
Each task gets its own Python process, preventing browser session interference.
Fails with exit code 1 if 0% of tasks pass.
"""

import argparse
import asyncio
import glob
import json
import logging
import os
import sys
import warnings

import anyio
import yaml
from dotenv import load_dotenv
from pydantic import BaseModel

load_dotenv()
from browser_use import Agent, AgentHistoryList, BrowserProfile, BrowserSession, ChatBrowserUse
from browser_use.llm.google.chat import ChatGoogle
from browser_use.llm.messages import UserMessage

# --- CONFIG ---
MAX_PARALLEL = 10
TASK_DIR = (
	sys.argv[1]
	if len(sys.argv) > 1 and not sys.argv[1].startswith('--')
	else os.path.join(os.path.dirname(__file__), '../agent_tasks')
)
TASK_FILES = glob.glob(os.path.join(TASK_DIR, '*.yaml'))


class JudgeResponse(BaseModel):
	success: bool
	explanation: str


async def run_single_task(task_file):
	"""Run a single task in the current process (called by subprocess)"""
	try:
		print(f'[DEBUG] Starting task: {os.path.basename(task_file)}', file=sys.stderr)

		# Suppress all logging in subprocess to avoid interfering with JSON output
		logging.getLogger().setLevel(logging.CRITICAL)
		for logger_name in ['browser_use', 'telemetry', 'message_manager']:
			logging.getLogger(logger_name).setLevel(logging.CRITICAL)
		warnings.filterwarnings('ignore')

		print('[DEBUG] Loading task file...', file=sys.stderr)
		content = await anyio.Path(task_file).read_text()
		task_data = yaml.safe_load(content)
		task = task_data['task']
		judge_context = task_data.get('judge_context', ['The agent must solve the task'])
		max_steps = task_data.get('max_steps', 15)

		print(f'[DEBUG] Task: {task[:100]}...', file=sys.stderr)
		print(f'[DEBUG] Max steps: {max_steps}', file=sys.stderr)
		api_key = os.getenv('BROWSER_USE_API_KEY')
		if not api_key:
			print('[SKIP] BROWSER_USE_API_KEY is not set - skipping task evaluation', file=sys.stderr)
			return {
				'file': os.path.basename(task_file),
				'success': True,  # Mark as success so it doesn't fail CI
				'explanation': 'Skipped - API key not available (fork PR or missing secret)',
			}

		agent_llm = ChatBrowserUse(api_key=api_key)

		# Check if Google API key is available for judge LLM
		google_api_key = os.getenv('GOOGLE_API_KEY')
		if not google_api_key:
			print('[SKIP] GOOGLE_API_KEY is not set - skipping task evaluation', file=sys.stderr)
			return {
				'file': os.path.basename(task_file),
				'success': True,  # Mark as success so it doesn't fail CI
				'explanation': 'Skipped - Google API key not available (fork PR or missing secret)',
			}

		judge_llm = ChatGoogle(model='gemini-flash-lite-latest')
		print('[DEBUG] LLMs initialized', file=sys.stderr)

		# Each subprocess gets its own profile and session
		print('[DEBUG] Creating browser session...', file=sys.stderr)
		profile = BrowserProfile(
			headless=True,
			user_data_dir=None,
			chromium_sandbox=False,  # Disable sandbox for CI environment (GitHub Actions)
		)
		session = BrowserSession(browser_profile=profile)
		print('[DEBUG] Browser session created', file=sys.stderr)

		# Test if browser is working
		try:
			await session.start()
			from browser_use.browser.events import NavigateToUrlEvent

			event = session.event_bus.dispatch(NavigateToUrlEvent(url='https://httpbin.org/get', new_tab=True))
			await event
			print('[DEBUG] Browser test: navigation successful', file=sys.stderr)
			title = await session.get_current_page_title()
			print(f"[DEBUG] Browser test: got title '{title}'", file=sys.stderr)
		except Exception as browser_error:
			print(f'[DEBUG] Browser test failed: {str(browser_error)}', file=sys.stderr)
			print(
				f'[DEBUG] Browser error type: {type(browser_error).__name__}',
				file=sys.stderr,
			)

		print('[DEBUG] Starting agent execution...', file=sys.stderr)
		agent = Agent(task=task, llm=agent_llm, browser_session=session)

		try:
			history: AgentHistoryList = await agent.run(max_steps=max_steps)
			print('[DEBUG] Agent.run() returned successfully', file=sys.stderr)
		except Exception as agent_error:
			print(
				f'[DEBUG] Agent.run() failed with error: {str(agent_error)}',
				file=sys.stderr,
			)
			print(f'[DEBUG] Error type: {type(agent_error).__name__}', file=sys.stderr)
			# Re-raise to be caught by outer try-catch
			raise agent_error

		agent_output = history.final_result() or ''
		print('[DEBUG] Agent execution completed', file=sys.stderr)

		# Test if LLM is working by making a simple call
		try:
			response = await agent_llm.ainvoke([UserMessage(content="Say 'test'")])
			print(
				f'[DEBUG] LLM test call successful: {response.completion[:50]}',
				file=sys.stderr,
			)
		except Exception as llm_error:
			print(f'[DEBUG] LLM test call failed: {str(llm_error)}', file=sys.stderr)

		# Debug: capture more details about the agent execution
		total_steps = len(history.history) if hasattr(history, 'history') else 0
		last_action = history.history[-1] if hasattr(history, 'history') and history.history else None
		debug_info = f'Steps: {total_steps}, Final result length: {len(agent_output)}'
		if last_action:
			debug_info += f', Last action: {type(last_action).__name__}'

		# Log to stderr so it shows up in GitHub Actions (won't interfere with JSON output to stdout)
		print(f'[DEBUG] Task {os.path.basename(task_file)}: {debug_info}', file=sys.stderr)
		if agent_output:
			print(
				f'[DEBUG] Agent output preview: {agent_output[:200]}...',
				file=sys.stderr,
			)
		else:
			print('[DEBUG] Agent produced no output!', file=sys.stderr)

		criteria = '\n- '.join(judge_context)
		judge_prompt = f"""
You are a evaluator of a browser agent task inside a ci/cd pipeline. Here was the agent's task:
{task}

Here is the agent's output:
{agent_output if agent_output else '[No output provided]'}

Debug info: {debug_info}

Criteria for success:
- {criteria}

Reply in JSON with keys: success (true/false), explanation (string).
If the agent provided no output, explain what might have gone wrong.
"""
		response = await judge_llm.ainvoke([UserMessage(content=judge_prompt)], output_format=JudgeResponse)
		judge_response = response.completion

		result = {
			'file': os.path.basename(task_file),
			'success': judge_response.success,
			'explanation': judge_response.explanation,
		}

		# Clean up session before returning
		await session.kill()

		return result

	except Exception as e:
		# Ensure session cleanup even on error
		try:
			await session.kill()
		except Exception:
			pass

		return {
			'file': os.path.basename(task_file),
			'success': False,
			'explanation': f'Task failed with error: {str(e)}',
		}


async def run_task_subprocess(task_file, semaphore):
	"""Run a task in a separate subprocess"""
	async with semaphore:
		try:
			# Set environment to reduce noise in subprocess
			env = os.environ.copy()
			env['PYTHONPATH'] = os.pathsep.join(sys.path)

			proc = await asyncio.create_subprocess_exec(
				sys.executable,
				__file__,
				'--task',
				task_file,
				stdout=asyncio.subprocess.PIPE,
				stderr=asyncio.subprocess.PIPE,
				env=env,
			)
			stdout, stderr = await proc.communicate()

			if proc.returncode == 0:
				try:
					# Parse JSON result from subprocess
					stdout_text = stdout.decode().strip()
					stderr_text = stderr.decode().strip()

					# Display subprocess debug logs
					if stderr_text:
						print(f'[SUBPROCESS {os.path.basename(task_file)}] Debug output:')
						for line in stderr_text.split('\n'):
							if line.strip():
								print(f'  {line}')

					# Find the JSON line (should be the last line that starts with {)
					lines = stdout_text.split('\n')
					json_line = None
					for line in reversed(lines):
						line = line.strip()
						if line.startswith('{') and line.endswith('}'):
							json_line = line
							break

					if json_line:
						result = json.loads(json_line)
						print(f'[PARENT] Task {os.path.basename(task_file)} completed: {result["success"]}')
					else:
						raise ValueError(f'No JSON found in output: {stdout_text}')

				except (json.JSONDecodeError, ValueError) as e:
					result = {
						'file': os.path.basename(task_file),
						'success': False,
						'explanation': f'Failed to parse subprocess result: {str(e)[:100]}',
					}
					print(f'[PARENT] Task {os.path.basename(task_file)} failed to parse: {str(e)}')
					print(f'[PARENT] Full stdout was: {stdout.decode()[:500]}')
			else:
				stderr_text = stderr.decode().strip()
				result = {
					'file': os.path.basename(task_file),
					'success': False,
					'explanation': f'Subprocess failed (code {proc.returncode}): {stderr_text[:200]}',
				}
				print(f'[PARENT] Task {os.path.basename(task_file)} subprocess failed with code {proc.returncode}')
				if stderr_text:
					print(f'[PARENT] stderr: {stderr_text[:1000]}')
				stdout_text = stdout.decode().strip()
				if stdout_text:
					print(f'[PARENT] stdout: {stdout_text[:1000]}')
		except Exception as e:
			result = {
				'file': os.path.basename(task_file),
				'success': False,
				'explanation': f'Failed to start subprocess: {str(e)}',
			}
			print(f'[PARENT] Failed to start subprocess for {os.path.basename(task_file)}: {str(e)}')

		return result


async def main():
	"""Run all tasks in parallel using subprocesses"""
	semaphore = asyncio.Semaphore(MAX_PARALLEL)

	print(f'Found task files: {TASK_FILES}')

	if not TASK_FILES:
		print('No task files found!')
		return 0, 0

	# Run all tasks in parallel subprocesses
	tasks = [run_task_subprocess(task_file, semaphore) for task_file in TASK_FILES]
	results = await asyncio.gather(*tasks)

	passed = sum(1 for r in results if r['success'])
	total = len(results)

	print('\n' + '=' * 60)
	print(f'{"RESULTS":^60}\n')

	# Prepare table data
	headers = ['Task', 'Success', 'Reason']
	rows = []
	for r in results:
		status = '✅' if r['success'] else '❌'
		rows.append([r['file'], status, r['explanation']])

	# Calculate column widths
	col_widths = [max(len(str(row[i])) for row in ([headers] + rows)) for i in range(3)]

	# Print header
	header_row = ' | '.join(headers[i].ljust(col_widths[i]) for i in range(3))
	print(header_row)
	print('-+-'.join('-' * w for w in col_widths))

	# Print rows
	for row in rows:
		print(' | '.join(str(row[i]).ljust(col_widths[i]) for i in range(3)))

	print('\n' + '=' * 60)
	print(f'\n{"SCORE":^60}')
	print(f'\n{"=" * 60}\n')
	print(f'\n{"*" * 10}  {passed}/{total} PASSED  {"*" * 10}\n')
	print('=' * 60 + '\n')

	# Output results for GitHub Actions
	print(f'PASSED={passed}')
	print(f'TOTAL={total}')

	# Output detailed results as JSON for GitHub Actions
	detailed_results = []
	for r in results:
		detailed_results.append(
			{
				'task': r['file'].replace('.yaml', ''),
				'success': r['success'],
				'reason': r['explanation'],
			}
		)

	print('DETAILED_RESULTS=' + json.dumps(detailed_results))

	return passed, total


if __name__ == '__main__':
	parser = argparse.ArgumentParser()
	parser.add_argument('--task', type=str, help='Path to a single task YAML file (for subprocess mode)')
	args = parser.parse_args()

	if args.task:
		# Subprocess mode: run a single task and output ONLY JSON
		try:
			result = asyncio.run(run_single_task(args.task))
			# Output ONLY the JSON result, nothing else
			print(json.dumps(result))
		except Exception as e:
			# Even on critical failure, output valid JSON
			error_result = {
				'file': os.path.basename(args.task),
				'success': False,
				'explanation': f'Critical subprocess error: {str(e)}',
			}
			print(json.dumps(error_result))
	else:
		# Parent process mode: run all tasks in parallel subprocesses
		passed, total = asyncio.run(main())
		# Results already printed by main() function

		# Fail if 0% pass rate (all tasks failed)
		if total > 0 and passed == 0:
			print('\n❌ CRITICAL: 0% pass rate - all tasks failed!')
			sys.exit(1)

```

---

## backend/browser-use/tests/ci/infrastructure/test_config.py

```py
"""Tests for lazy loading configuration system."""

import os

from browser_use.config import CONFIG


class TestLazyConfig:
	"""Test lazy loading of environment variables through CONFIG object."""

	def test_config_reads_env_vars_lazily(self):
		"""Test that CONFIG reads environment variables each time they're accessed."""
		# Set an env var
		original_value = os.environ.get('BROWSER_USE_LOGGING_LEVEL', '')
		try:
			os.environ['BROWSER_USE_LOGGING_LEVEL'] = 'debug'
			assert CONFIG.BROWSER_USE_LOGGING_LEVEL == 'debug'

			# Change the env var
			os.environ['BROWSER_USE_LOGGING_LEVEL'] = 'info'
			assert CONFIG.BROWSER_USE_LOGGING_LEVEL == 'info'

			# Delete the env var to test default
			del os.environ['BROWSER_USE_LOGGING_LEVEL']
			assert CONFIG.BROWSER_USE_LOGGING_LEVEL == 'info'  # default value
		finally:
			# Restore original value
			if original_value:
				os.environ['BROWSER_USE_LOGGING_LEVEL'] = original_value
			else:
				os.environ.pop('BROWSER_USE_LOGGING_LEVEL', None)

	def test_boolean_env_vars(self):
		"""Test boolean environment variables are parsed correctly."""
		original_value = os.environ.get('ANONYMIZED_TELEMETRY', '')
		try:
			# Test true values
			for true_val in ['true', 'True', 'TRUE', 'yes', 'Yes', '1']:
				os.environ['ANONYMIZED_TELEMETRY'] = true_val
				assert CONFIG.ANONYMIZED_TELEMETRY is True, f'Failed for value: {true_val}'

			# Test false values
			for false_val in ['false', 'False', 'FALSE', 'no', 'No', '0']:
				os.environ['ANONYMIZED_TELEMETRY'] = false_val
				assert CONFIG.ANONYMIZED_TELEMETRY is False, f'Failed for value: {false_val}'
		finally:
			if original_value:
				os.environ['ANONYMIZED_TELEMETRY'] = original_value
			else:
				os.environ.pop('ANONYMIZED_TELEMETRY', None)

	def test_api_keys_lazy_loading(self):
		"""Test API keys are loaded lazily."""
		original_value = os.environ.get('OPENAI_API_KEY', '')
		try:
			# Test empty default
			os.environ.pop('OPENAI_API_KEY', None)
			assert CONFIG.OPENAI_API_KEY == ''

			# Set a value
			os.environ['OPENAI_API_KEY'] = 'test-key-123'
			assert CONFIG.OPENAI_API_KEY == 'test-key-123'

			# Change the value
			os.environ['OPENAI_API_KEY'] = 'new-key-456'
			assert CONFIG.OPENAI_API_KEY == 'new-key-456'
		finally:
			if original_value:
				os.environ['OPENAI_API_KEY'] = original_value
			else:
				os.environ.pop('OPENAI_API_KEY', None)

	def test_path_configuration(self):
		"""Test path configuration variables."""
		original_value = os.environ.get('XDG_CACHE_HOME', '')
		try:
			# Test custom path
			test_path = '/tmp/test-cache'
			os.environ['XDG_CACHE_HOME'] = test_path
			# Use Path().resolve() to handle symlinks (e.g., /tmp -> /private/tmp on macOS)
			from pathlib import Path

			assert CONFIG.XDG_CACHE_HOME == Path(test_path).resolve()

			# Test default path expansion
			os.environ.pop('XDG_CACHE_HOME', None)
			assert '/.cache' in str(CONFIG.XDG_CACHE_HOME)
		finally:
			if original_value:
				os.environ['XDG_CACHE_HOME'] = original_value
			else:
				os.environ.pop('XDG_CACHE_HOME', None)

	def test_cloud_sync_inherits_telemetry(self):
		"""Test BROWSER_USE_CLOUD_SYNC inherits from ANONYMIZED_TELEMETRY when not set."""
		telemetry_original = os.environ.get('ANONYMIZED_TELEMETRY', '')
		sync_original = os.environ.get('BROWSER_USE_CLOUD_SYNC', '')
		try:
			# When BROWSER_USE_CLOUD_SYNC is not set, it should inherit from ANONYMIZED_TELEMETRY
			os.environ['ANONYMIZED_TELEMETRY'] = 'true'
			os.environ.pop('BROWSER_USE_CLOUD_SYNC', None)
			assert CONFIG.BROWSER_USE_CLOUD_SYNC is True

			os.environ['ANONYMIZED_TELEMETRY'] = 'false'
			os.environ.pop('BROWSER_USE_CLOUD_SYNC', None)
			assert CONFIG.BROWSER_USE_CLOUD_SYNC is False

			# When explicitly set, it should use its own value
			os.environ['ANONYMIZED_TELEMETRY'] = 'false'
			os.environ['BROWSER_USE_CLOUD_SYNC'] = 'true'
			assert CONFIG.BROWSER_USE_CLOUD_SYNC is True
		finally:
			if telemetry_original:
				os.environ['ANONYMIZED_TELEMETRY'] = telemetry_original
			else:
				os.environ.pop('ANONYMIZED_TELEMETRY', None)
			if sync_original:
				os.environ['BROWSER_USE_CLOUD_SYNC'] = sync_original
			else:
				os.environ.pop('BROWSER_USE_CLOUD_SYNC', None)

```

---

## backend/browser-use/tests/ci/infrastructure/test_filesystem.py

```py
"""Tests for the FileSystem class and related file operations."""

import asyncio
import tempfile
from pathlib import Path

import pytest

from browser_use.filesystem.file_system import (
	DEFAULT_FILE_SYSTEM_PATH,
	INVALID_FILENAME_ERROR_MESSAGE,
	CsvFile,
	FileSystem,
	FileSystemState,
	JsonFile,
	JsonlFile,
	MarkdownFile,
	TxtFile,
)


class TestBaseFile:
	"""Test the BaseFile abstract base class and its implementations."""

	def test_markdown_file_creation(self):
		"""Test MarkdownFile creation and basic properties."""
		md_file = MarkdownFile(name='test', content='# Hello World')

		assert md_file.name == 'test'
		assert md_file.content == '# Hello World'
		assert md_file.extension == 'md'
		assert md_file.full_name == 'test.md'
		assert md_file.get_size == 13
		assert md_file.get_line_count == 1

	def test_txt_file_creation(self):
		"""Test TxtFile creation and basic properties."""
		txt_file = TxtFile(name='notes', content='Hello\nWorld')

		assert txt_file.name == 'notes'
		assert txt_file.content == 'Hello\nWorld'
		assert txt_file.extension == 'txt'
		assert txt_file.full_name == 'notes.txt'
		assert txt_file.get_size == 11
		assert txt_file.get_line_count == 2

	def test_json_file_creation(self):
		"""Test JsonFile creation and basic properties."""
		json_content = '{"name": "John", "age": 30, "city": "New York"}'
		json_file = JsonFile(name='data', content=json_content)

		assert json_file.name == 'data'
		assert json_file.content == json_content
		assert json_file.extension == 'json'
		assert json_file.full_name == 'data.json'
		assert json_file.get_size == len(json_content)
		assert json_file.get_line_count == 1

	def test_csv_file_creation(self):
		"""Test CsvFile creation and basic properties."""
		csv_content = 'name,age,city\nJohn,30,New York\nJane,25,London'
		csv_file = CsvFile(name='users', content=csv_content)

		assert csv_file.name == 'users'
		assert csv_file.content == csv_content
		assert csv_file.extension == 'csv'
		assert csv_file.full_name == 'users.csv'
		assert csv_file.get_size == len(csv_content)
		assert csv_file.get_line_count == 3

	def test_jsonl_file_creation(self):
		"""Test JsonlFile creation and basic properties."""
		jsonl_content = '{"id": 1, "name": "John"}\n{"id": 2, "name": "Jane"}'
		jsonl_file = JsonlFile(name='data', content=jsonl_content)

		assert jsonl_file.name == 'data'
		assert jsonl_file.content == jsonl_content
		assert jsonl_file.extension == 'jsonl'
		assert jsonl_file.full_name == 'data.jsonl'
		assert jsonl_file.get_size == len(jsonl_content)
		assert jsonl_file.get_line_count == 2

	def test_file_content_operations(self):
		"""Test content update and append operations."""
		file_obj = TxtFile(name='test')

		# Initial content
		assert file_obj.content == ''
		assert file_obj.get_size == 0

		# Write content
		file_obj.write_file_content('First line')
		assert file_obj.content == 'First line'
		assert file_obj.get_size == 10

		# Append content
		file_obj.append_file_content('\nSecond line')
		assert file_obj.content == 'First line\nSecond line'
		assert file_obj.get_line_count == 2

		# Update content
		file_obj.update_content('New content')
		assert file_obj.content == 'New content'

	async def test_file_disk_operations(self):
		"""Test file sync to disk operations."""
		with tempfile.TemporaryDirectory() as tmp_dir:
			tmp_path = Path(tmp_dir)
			file_obj = MarkdownFile(name='test', content='# Test Content')

			# Test sync to disk
			await file_obj.sync_to_disk(tmp_path)

			# Verify file was created on disk
			file_path = tmp_path / 'test.md'
			assert file_path.exists()
			assert file_path.read_text() == '# Test Content'

			# Test write operation
			await file_obj.write('# New Content', tmp_path)
			assert file_path.read_text() == '# New Content'
			assert file_obj.content == '# New Content'

			# Test append operation
			await file_obj.append('\n## Section 2', tmp_path)
			expected_content = '# New Content\n## Section 2'
			assert file_path.read_text() == expected_content
			assert file_obj.content == expected_content

	async def test_json_file_disk_operations(self):
		"""Test JSON file sync to disk operations."""
		with tempfile.TemporaryDirectory() as tmp_dir:
			tmp_path = Path(tmp_dir)
			json_content = '{"users": [{"name": "John", "age": 30}]}'
			json_file = JsonFile(name='data', content=json_content)

			# Test sync to disk
			await json_file.sync_to_disk(tmp_path)

			# Verify file was created on disk
			file_path = tmp_path / 'data.json'
			assert file_path.exists()
			assert file_path.read_text() == json_content

			# Test write operation
			new_content = '{"users": [{"name": "Jane", "age": 25}]}'
			await json_file.write(new_content, tmp_path)
			assert file_path.read_text() == new_content
			assert json_file.content == new_content

			# Test append operation
			await json_file.append(', {"name": "Bob", "age": 35}', tmp_path)
			expected_content = new_content + ', {"name": "Bob", "age": 35}'
			assert file_path.read_text() == expected_content
			assert json_file.content == expected_content

	async def test_csv_file_disk_operations(self):
		"""Test CSV file sync to disk operations."""
		with tempfile.TemporaryDirectory() as tmp_dir:
			tmp_path = Path(tmp_dir)
			csv_content = 'name,age,city\nJohn,30,New York'
			csv_file = CsvFile(name='users', content=csv_content)

			# Test sync to disk
			await csv_file.sync_to_disk(tmp_path)

			# Verify file was created on disk
			file_path = tmp_path / 'users.csv'
			assert file_path.exists()
			assert file_path.read_text() == csv_content

			# Test write operation
			new_content = 'name,age,city\nJane,25,London'
			await csv_file.write(new_content, tmp_path)
			assert file_path.read_text() == new_content
			assert csv_file.content == new_content

			# Test append operation
			await csv_file.append('\nBob,35,Paris', tmp_path)
			expected_content = new_content + '\nBob,35,Paris'
			assert file_path.read_text() == expected_content
			assert csv_file.content == expected_content

	def test_file_sync_to_disk_sync(self):
		"""Test synchronous disk sync operation."""
		with tempfile.TemporaryDirectory() as tmp_dir:
			tmp_path = Path(tmp_dir)
			file_obj = TxtFile(name='sync_test', content='Sync content')

			# Test synchronous sync
			file_obj.sync_to_disk_sync(tmp_path)

			# Verify file was created
			file_path = tmp_path / 'sync_test.txt'
			assert file_path.exists()
			assert file_path.read_text() == 'Sync content'


class TestFileSystem:
	"""Test the FileSystem class functionality."""

	@pytest.fixture
	def temp_filesystem(self):
		"""Create a temporary FileSystem for testing."""
		with tempfile.TemporaryDirectory() as tmp_dir:
			fs = FileSystem(base_dir=tmp_dir, create_default_files=True)
			yield fs
			try:
				fs.nuke()
			except Exception:
				pass  # Directory might already be cleaned up

	@pytest.fixture
	def empty_filesystem(self):
		"""Create a temporary FileSystem without default files."""
		with tempfile.TemporaryDirectory() as tmp_dir:
			fs = FileSystem(base_dir=tmp_dir, create_default_files=False)
			yield fs
			try:
				fs.nuke()
			except Exception:
				pass

	def test_filesystem_initialization(self, temp_filesystem):
		"""Test FileSystem initialization with default files."""
		fs = temp_filesystem

		# Check that base directory and data directory exist
		assert fs.base_dir.exists()
		assert fs.data_dir.exists()
		assert fs.data_dir.name == DEFAULT_FILE_SYSTEM_PATH

		# Check default files are created
		assert 'todo.md' in fs.files
		assert len(fs.files) == 1

		# Check files exist on disk
		todo_path = fs.data_dir / 'todo.md'
		assert todo_path.exists()

	def test_filesystem_without_default_files(self, empty_filesystem):
		"""Test FileSystem initialization without default files."""
		fs = empty_filesystem

		assert fs.base_dir.exists()
		assert fs.data_dir.exists()
		assert len(fs.files) == 0

	def test_get_allowed_extensions(self, temp_filesystem):
		"""Test getting allowed file extensions."""
		fs = temp_filesystem
		extensions = fs.get_allowed_extensions()

		assert 'md' in extensions
		assert 'txt' in extensions
		assert 'json' in extensions
		assert 'jsonl' in extensions
		assert 'csv' in extensions

	def test_filename_validation(self, temp_filesystem):
		"""Test filename validation."""
		fs = temp_filesystem

		# Valid filenames
		assert fs._is_valid_filename('test.md') is True
		assert fs._is_valid_filename('my_file.txt') is True
		assert fs._is_valid_filename('file-name.md') is True
		assert fs._is_valid_filename('file123.txt') is True
		assert fs._is_valid_filename('data.json') is True
		assert fs._is_valid_filename('data.jsonl') is True
		assert fs._is_valid_filename('users.csv') is True
		assert fs._is_valid_filename('WebVoyager_data.jsonl') is True  # with underscores

		# Invalid filenames
		assert fs._is_valid_filename('test.doc') is False  # wrong extension
		assert fs._is_valid_filename('test') is False  # no extension
		assert fs._is_valid_filename('test.md.txt') is False  # multiple extensions
		assert fs._is_valid_filename('test with spaces.md') is False  # spaces
		assert fs._is_valid_filename('test@file.md') is False  # special chars
		assert fs._is_valid_filename('.md') is False  # no name
		assert fs._is_valid_filename('.json') is False  # no name
		assert fs._is_valid_filename('.jsonl') is False  # no name
		assert fs._is_valid_filename('.csv') is False  # no name

	def test_filename_parsing(self, temp_filesystem):
		"""Test filename parsing into name and extension."""
		fs = temp_filesystem

		name, ext = fs._parse_filename('test.md')
		assert name == 'test'
		assert ext == 'md'

		name, ext = fs._parse_filename('my_file.TXT')
		assert name == 'my_file'
		assert ext == 'txt'  # Should be lowercased

		name, ext = fs._parse_filename('data.json')
		assert name == 'data'
		assert ext == 'json'

		name, ext = fs._parse_filename('users.CSV')
		assert name == 'users'
		assert ext == 'csv'  # Should be lowercased

	def test_get_file(self, temp_filesystem):
		"""Test getting files from the filesystem."""
		fs = temp_filesystem

		# Get non-existent file
		non_existent = fs.get_file('nonexistent.md')
		assert non_existent is None

		# Get file with invalid name
		invalid = fs.get_file('invalid@name.md')
		assert invalid is None

	def test_list_files(self, temp_filesystem):
		"""Test listing files in the filesystem."""
		fs = temp_filesystem
		files = fs.list_files()

		assert 'todo.md' in files
		assert len(files) == 1

	def test_display_file(self, temp_filesystem):
		"""Test displaying file content."""
		fs = temp_filesystem

		# Display existing file
		content = fs.display_file('todo.md')
		assert content == ''  # Default files are empty

		# Display non-existent file
		content = fs.display_file('nonexistent.md')
		assert content is None

		# Display file with invalid name
		content = fs.display_file('invalid@name.md')
		assert content is None

	async def test_read_file(self, temp_filesystem: FileSystem):
		"""Test reading file content with proper formatting."""
		fs: FileSystem = temp_filesystem

		# Read existing empty file
		result = await fs.read_file('todo.md')
		expected = 'Read from file todo.md.\n<content>\n\n</content>'
		assert result == expected

		# Read non-existent file
		result = await fs.read_file('nonexistent.md')
		assert result == "File 'nonexistent.md' not found."

		# Read file with invalid name
		result = await fs.read_file('invalid@name.md')
		assert result == INVALID_FILENAME_ERROR_MESSAGE

	async def test_write_file(self, temp_filesystem):
		"""Test writing content to files."""
		fs = temp_filesystem

		# Write to existing file
		result = await fs.write_file('results.md', '# Test Results\nThis is a test.')
		assert result == 'Data written to file results.md successfully.'

		# Verify content was written
		content = await fs.read_file('results.md')
		assert '# Test Results\nThis is a test.' in content

		# Write to new file
		result = await fs.write_file('new_file.txt', 'New file content')
		assert result == 'Data written to file new_file.txt successfully.'
		assert 'new_file.txt' in fs.files
		assert fs.get_file('new_file.txt').content == 'New file content'

		# Write with invalid filename
		result = await fs.write_file('invalid@name.md', 'content')
		assert result == INVALID_FILENAME_ERROR_MESSAGE

		# Write with invalid extension
		result = await fs.write_file('test.doc', 'content')
		assert result == INVALID_FILENAME_ERROR_MESSAGE

	async def test_write_json_file(self, temp_filesystem):
		"""Test writing JSON files."""
		fs = temp_filesystem

		# Write valid JSON content
		json_content = '{"users": [{"name": "John", "age": 30}, {"name": "Jane", "age": 25}]}'
		result = await fs.write_file('data.json', json_content)
		assert result == 'Data written to file data.json successfully.'

		# Verify content was written
		content = await fs.read_file('data.json')
		assert json_content in content

		# Verify file object was created
		assert 'data.json' in fs.files
		file_obj = fs.get_file('data.json')
		assert file_obj is not None
		assert isinstance(file_obj, JsonFile)
		assert file_obj.content == json_content

		# Write to new JSON file
		result = await fs.write_file('config.json', '{"debug": true, "port": 8080}')
		assert result == 'Data written to file config.json successfully.'
		assert 'config.json' in fs.files

	async def test_write_csv_file(self, temp_filesystem):
		"""Test writing CSV files."""
		fs = temp_filesystem

		# Write valid CSV content
		csv_content = 'name,age,city\nJohn,30,New York\nJane,25,London\nBob,35,Paris'
		result = await fs.write_file('users.csv', csv_content)
		assert result == 'Data written to file users.csv successfully.'

		# Verify content was written
		content = await fs.read_file('users.csv')
		assert csv_content in content

		# Verify file object was created
		assert 'users.csv' in fs.files
		file_obj = fs.get_file('users.csv')
		assert file_obj is not None
		assert isinstance(file_obj, CsvFile)
		assert file_obj.content == csv_content

		# Write to new CSV file
		result = await fs.write_file('products.csv', 'id,name,price\n1,Laptop,999.99\n2,Mouse,29.99')
		assert result == 'Data written to file products.csv successfully.'
		assert 'products.csv' in fs.files

	async def test_append_file(self, temp_filesystem):
		"""Test appending content to files."""
		fs = temp_filesystem

		# First write some content
		await fs.write_file('test.md', '# Title')

		# Append content
		result = await fs.append_file('test.md', '\n## Section 1')
		assert result == 'Data appended to file test.md successfully.'

		# Verify content was appended
		content = fs.get_file('test.md').content
		assert content == '# Title\n## Section 1'

		# Append to non-existent file
		result = await fs.append_file('nonexistent.md', 'content')
		assert result == "File 'nonexistent.md' not found."

		# Append with invalid filename
		result = await fs.append_file('invalid@name.md', 'content')
		assert result == INVALID_FILENAME_ERROR_MESSAGE

	async def test_append_json_file(self, temp_filesystem):
		"""Test appending content to JSON files."""
		fs = temp_filesystem

		# First write some JSON content
		await fs.write_file('data.json', '{"users": [{"name": "John", "age": 30}]}')

		# Append additional JSON content (note: this creates invalid JSON, but tests the append functionality)
		result = await fs.append_file('data.json', ', {"name": "Jane", "age": 25}')
		assert result == 'Data appended to file data.json successfully.'

		# Verify content was appended
		file_obj = fs.get_file('data.json')
		assert file_obj is not None
		expected_content = '{"users": [{"name": "John", "age": 30}]}, {"name": "Jane", "age": 25}'
		assert file_obj.content == expected_content

	async def test_append_csv_file(self, temp_filesystem):
		"""Test appending content to CSV files."""
		fs = temp_filesystem

		# First write some CSV content
		await fs.write_file('users.csv', 'name,age,city\nJohn,30,New York')

		# Append additional CSV row
		result = await fs.append_file('users.csv', '\nJane,25,London')
		assert result == 'Data appended to file users.csv successfully.'

		# Verify content was appended
		file_obj = fs.get_file('users.csv')
		assert file_obj is not None
		expected_content = 'name,age,city\nJohn,30,New York\nJane,25,London'
		assert file_obj.content == expected_content

		# Append another row
		await fs.append_file('users.csv', '\nBob,35,Paris')
		expected_content = 'name,age,city\nJohn,30,New York\nJane,25,London\nBob,35,Paris'
		assert file_obj.content == expected_content

	async def test_write_jsonl_file(self, temp_filesystem):
		"""Test writing JSONL (JSON Lines) files."""
		fs = temp_filesystem

		# Write valid JSONL content
		jsonl_content = '{"id": 1, "name": "John", "age": 30}\n{"id": 2, "name": "Jane", "age": 25}'
		result = await fs.write_file('data.jsonl', jsonl_content)
		assert result == 'Data written to file data.jsonl successfully.'

		# Verify content was written
		content = await fs.read_file('data.jsonl')
		assert jsonl_content in content

		# Verify file object was created
		assert 'data.jsonl' in fs.files
		file_obj = fs.get_file('data.jsonl')
		assert file_obj is not None
		assert isinstance(file_obj, JsonlFile)
		assert file_obj.content == jsonl_content

		# Write to new JSONL file
		result = await fs.write_file('WebVoyager_data.jsonl', '{"task": "test", "url": "https://example.com"}')
		assert result == 'Data written to file WebVoyager_data.jsonl successfully.'
		assert 'WebVoyager_data.jsonl' in fs.files

	async def test_append_jsonl_file(self, temp_filesystem):
		"""Test appending content to JSONL files."""
		fs = temp_filesystem

		# First write some JSONL content
		await fs.write_file('data.jsonl', '{"id": 1, "name": "John", "age": 30}')

		# Append additional JSONL record
		result = await fs.append_file('data.jsonl', '\n{"id": 2, "name": "Jane", "age": 25}')
		assert result == 'Data appended to file data.jsonl successfully.'

		# Verify content was appended
		file_obj = fs.get_file('data.jsonl')
		assert file_obj is not None
		expected_content = '{"id": 1, "name": "John", "age": 30}\n{"id": 2, "name": "Jane", "age": 25}'
		assert file_obj.content == expected_content

		# Append another record
		await fs.append_file('data.jsonl', '\n{"id": 3, "name": "Bob", "age": 35}')
		expected_content = (
			'{"id": 1, "name": "John", "age": 30}\n{"id": 2, "name": "Jane", "age": 25}\n{"id": 3, "name": "Bob", "age": 35}'
		)
		assert file_obj.content == expected_content

	async def test_save_extracted_content(self, temp_filesystem):
		"""Test saving extracted content with auto-numbering."""
		fs = temp_filesystem

		# Save first extracted content
		result = await fs.save_extracted_content('First extracted content')
		assert result == 'extracted_content_0.md'
		assert 'extracted_content_0.md' in fs.files
		assert fs.extracted_content_count == 1

		# Save second extracted content
		result = await fs.save_extracted_content('Second extracted content')
		assert result == 'extracted_content_1.md'
		assert 'extracted_content_1.md' in fs.files
		assert fs.extracted_content_count == 2

		# Verify content
		content1 = fs.get_file('extracted_content_0.md').content
		content2 = fs.get_file('extracted_content_1.md').content
		assert content1 == 'First extracted content'
		assert content2 == 'Second extracted content'

	async def test_describe_with_content(self, temp_filesystem):
		"""Test describing filesystem with files containing content."""
		fs = temp_filesystem

		# Add content to files
		await fs.write_file('results.md', '# Results\nTest results here.')
		await fs.write_file('notes.txt', 'These are my notes.')

		description = fs.describe()

		# Should contain file information
		assert 'results.md' in description
		assert 'notes.txt' in description
		assert '# Results' in description
		assert 'These are my notes.' in description
		assert 'lines' in description

	async def test_describe_large_files(self, temp_filesystem):
		"""Test describing filesystem with large files (truncated content)."""
		fs = temp_filesystem

		# Create a large file
		large_content = '\n'.join([f'Line {i}' for i in range(100)])
		await fs.write_file('large.md', large_content)

		description = fs.describe()

		# Should be truncated with "more lines" indicator
		assert 'large.md' in description
		assert 'more lines' in description
		assert 'Line 0' in description  # Start should be shown
		assert 'Line 99' in description  # End should be shown

	def test_get_todo_contents(self, temp_filesystem):
		"""Test getting todo file contents."""
		fs = temp_filesystem

		# Initially empty
		todo_content = fs.get_todo_contents()
		assert todo_content == ''

		# Add content to todo
		fs.get_file('todo.md').update_content('- [ ] Task 1\n- [ ] Task 2')
		todo_content = fs.get_todo_contents()
		assert '- [ ] Task 1' in todo_content

	def test_get_state(self, temp_filesystem):
		"""Test getting filesystem state."""
		fs = temp_filesystem

		state = fs.get_state()

		assert isinstance(state, FileSystemState)
		assert state.base_dir == str(fs.base_dir)
		assert state.extracted_content_count == 0
		assert 'todo.md' in state.files

	async def test_from_state(self, temp_filesystem):
		"""Test restoring filesystem from state."""
		fs = temp_filesystem

		# Add some content
		await fs.write_file('results.md', '# Original Results')
		await fs.write_file('custom.txt', 'Custom content')
		await fs.save_extracted_content('Extracted data')

		# Get state
		state = fs.get_state()

		# Create new filesystem from state
		fs2 = FileSystem.from_state(state)

		# Verify restoration
		assert fs2.base_dir == fs.base_dir
		assert fs2.extracted_content_count == fs.extracted_content_count
		assert len(fs2.files) == len(fs.files)

		# Verify file contents
		file_obj = fs2.get_file('results.md')
		assert file_obj is not None
		assert file_obj.content == '# Original Results'
		file_obj = fs2.get_file('custom.txt')
		assert file_obj is not None
		assert file_obj.content == 'Custom content'
		file_obj = fs2.get_file('extracted_content_0.md')
		assert file_obj is not None
		assert file_obj.content == 'Extracted data'

		# Verify files exist on disk
		assert (fs2.data_dir / 'results.md').exists()
		assert (fs2.data_dir / 'custom.txt').exists()
		assert (fs2.data_dir / 'extracted_content_0.md').exists()

		# Clean up second filesystem
		fs2.nuke()

	async def test_complete_workflow_with_json_csv(self):
		"""Test a complete filesystem workflow with JSON and CSV files."""
		with tempfile.TemporaryDirectory() as tmp_dir:
			# Create filesystem
			fs = FileSystem(base_dir=tmp_dir, create_default_files=True)

			# Write JSON configuration file
			config_json = '{"app": {"name": "TestApp", "version": "1.0"}, "database": {"host": "localhost", "port": 5432}}'
			await fs.write_file('config.json', config_json)

			# Write CSV data file
			users_csv = 'id,name,email,age\n1,John Doe,john@example.com,30\n2,Jane Smith,jane@example.com,25'
			await fs.write_file('users.csv', users_csv)

			# Append more data to CSV
			await fs.append_file('users.csv', '\n3,Bob Johnson,bob@example.com,35')

			# Update JSON configuration
			updated_config = '{"app": {"name": "TestApp", "version": "1.1"}, "database": {"host": "localhost", "port": 5432}, "features": {"logging": true}}'
			await fs.write_file('config.json', updated_config)

			# Create another JSON file for API responses
			api_response = '{"status": "success", "data": [{"id": 1, "name": "Item 1"}, {"id": 2, "name": "Item 2"}]}'
			await fs.write_file('api_response.json', api_response)

			# Create a products CSV file
			products_csv = (
				'sku,name,price,category\nLAP001,Gaming Laptop,1299.99,Electronics\nMOU001,Wireless Mouse,29.99,Accessories'
			)
			await fs.write_file('products.csv', products_csv)

			# Verify file listing
			files = fs.list_files()
			expected_files = ['todo.md', 'config.json', 'users.csv', 'api_response.json', 'products.csv']
			assert len(files) == len(expected_files)
			for expected_file in expected_files:
				assert expected_file in files

			# Verify JSON file contents
			config_file = fs.get_file('config.json')
			assert config_file is not None
			assert isinstance(config_file, JsonFile)
			assert config_file.content == updated_config

			api_file = fs.get_file('api_response.json')
			assert api_file is not None
			assert isinstance(api_file, JsonFile)
			assert api_file.content == api_response

			# Verify CSV file contents
			users_file = fs.get_file('users.csv')
			assert users_file is not None
			assert isinstance(users_file, CsvFile)
			expected_users_content = 'id,name,email,age\n1,John Doe,john@example.com,30\n2,Jane Smith,jane@example.com,25\n3,Bob Johnson,bob@example.com,35'
			assert users_file.content == expected_users_content

			products_file = fs.get_file('products.csv')
			assert products_file is not None
			assert isinstance(products_file, CsvFile)
			assert products_file.content == products_csv

			# Test state persistence with JSON and CSV files
			state = fs.get_state()
			fs.nuke()

			# Restore from state
			fs2 = FileSystem.from_state(state)

			# Verify restoration
			assert len(fs2.files) == len(expected_files)

			# Verify JSON files were restored correctly
			restored_config = fs2.get_file('config.json')
			assert restored_config is not None
			assert isinstance(restored_config, JsonFile)
			assert restored_config.content == updated_config

			restored_api = fs2.get_file('api_response.json')
			assert restored_api is not None
			assert isinstance(restored_api, JsonFile)
			assert restored_api.content == api_response

			# Verify CSV files were restored correctly
			restored_users = fs2.get_file('users.csv')
			assert restored_users is not None
			assert isinstance(restored_users, CsvFile)
			assert restored_users.content == expected_users_content

			restored_products = fs2.get_file('products.csv')
			assert restored_products is not None
			assert isinstance(restored_products, CsvFile)
			assert restored_products.content == products_csv

			# Verify files exist on disk
			for filename in expected_files:
				if filename != 'todo.md':  # Skip todo.md as it's already tested
					assert (fs2.data_dir / filename).exists()

			fs2.nuke()

	async def test_from_state_with_json_csv_files(self, temp_filesystem):
		"""Test restoring filesystem from state with JSON and CSV files."""
		fs = temp_filesystem

		# Add JSON and CSV content
		await fs.write_file('data.json', '{"version": "1.0", "users": [{"name": "John", "age": 30}]}')
		await fs.write_file('users.csv', 'name,age,city\nJohn,30,New York\nJane,25,London')
		await fs.write_file('config.json', '{"debug": true, "port": 8080}')
		await fs.write_file('products.csv', 'id,name,price\n1,Laptop,999.99\n2,Mouse,29.99')

		# Get state
		state = fs.get_state()

		# Create new filesystem from state
		fs2 = FileSystem.from_state(state)

		# Verify restoration
		assert fs2.base_dir == fs.base_dir
		assert len(fs2.files) == len(fs.files)

		# Verify JSON file contents
		json_file = fs2.get_file('data.json')
		assert json_file is not None
		assert isinstance(json_file, JsonFile)
		assert json_file.content == '{"version": "1.0", "users": [{"name": "John", "age": 30}]}'

		config_file = fs2.get_file('config.json')
		assert config_file is not None
		assert isinstance(config_file, JsonFile)
		assert config_file.content == '{"debug": true, "port": 8080}'

		# Verify CSV file contents
		csv_file = fs2.get_file('users.csv')
		assert csv_file is not None
		assert isinstance(csv_file, CsvFile)
		assert csv_file.content == 'name,age,city\nJohn,30,New York\nJane,25,London'

		products_file = fs2.get_file('products.csv')
		assert products_file is not None
		assert isinstance(products_file, CsvFile)
		assert products_file.content == 'id,name,price\n1,Laptop,999.99\n2,Mouse,29.99'

		# Verify files exist on disk
		assert (fs2.data_dir / 'data.json').exists()
		assert (fs2.data_dir / 'users.csv').exists()
		assert (fs2.data_dir / 'config.json').exists()
		assert (fs2.data_dir / 'products.csv').exists()

		# Verify disk contents match
		assert (fs2.data_dir / 'data.json').read_text() == '{"version": "1.0", "users": [{"name": "John", "age": 30}]}'
		assert (fs2.data_dir / 'users.csv').read_text() == 'name,age,city\nJohn,30,New York\nJane,25,London'

		# Clean up second filesystem
		fs2.nuke()

	def test_nuke(self, empty_filesystem):
		"""Test filesystem destruction."""
		fs = empty_filesystem

		# Create a file to ensure directory has content
		fs.data_dir.mkdir(exist_ok=True)
		test_file = fs.data_dir / 'test.txt'
		test_file.write_text('test')
		assert test_file.exists()

		# Nuke the filesystem
		fs.nuke()

		# Verify directory is removed
		assert not fs.data_dir.exists()

	def test_get_dir(self, temp_filesystem):
		"""Test getting the filesystem directory."""
		fs = temp_filesystem

		directory = fs.get_dir()
		assert directory == fs.data_dir
		assert directory.exists()
		assert directory.name == DEFAULT_FILE_SYSTEM_PATH


class TestFileSystemEdgeCases:
	"""Test edge cases and error handling."""

	def test_filesystem_with_string_path(self):
		"""Test FileSystem creation with string path."""
		with tempfile.TemporaryDirectory() as tmp_dir:
			fs = FileSystem(base_dir=tmp_dir, create_default_files=False)
			assert isinstance(fs.base_dir, Path)
			assert fs.base_dir.exists()
			fs.nuke()

	def test_filesystem_with_path_object(self):
		"""Test FileSystem creation with Path object."""
		with tempfile.TemporaryDirectory() as tmp_dir:
			path_obj = Path(tmp_dir)
			fs = FileSystem(base_dir=path_obj, create_default_files=False)
			assert isinstance(fs.base_dir, Path)
			assert fs.base_dir == path_obj
			fs.nuke()

	def test_filesystem_recreates_data_dir(self):
		"""Test that FileSystem recreates data directory if it exists."""
		with tempfile.TemporaryDirectory() as tmp_dir:
			# Create filesystem
			fs1 = FileSystem(base_dir=tmp_dir, create_default_files=True)
			data_dir = fs1.data_dir

			# Add a custom file
			custom_file = data_dir / 'custom.txt'
			custom_file.write_text('custom content')
			assert custom_file.exists()

			# Create another filesystem with same base_dir (should clean data_dir)
			fs2 = FileSystem(base_dir=tmp_dir, create_default_files=True)

			# Custom file should be gone, default files should exist
			assert not custom_file.exists()
			assert (fs2.data_dir / 'todo.md').exists()

			fs2.nuke()

	async def test_write_file_exception_handling(self):
		"""Test exception handling in write_file."""
		with tempfile.TemporaryDirectory() as tmp_dir:
			fs = FileSystem(base_dir=tmp_dir, create_default_files=False)

			# Test with invalid extension
			result = await fs.write_file('test.invalid', 'content')
			assert result == INVALID_FILENAME_ERROR_MESSAGE

			fs.nuke()

	def test_from_state_with_unknown_file_type(self):
		"""Test restoring state with unknown file types (should skip them)."""
		with tempfile.TemporaryDirectory() as tmp_dir:
			# Create a state with unknown file type
			state = FileSystemState(
				files={
					'test.md': {'type': 'MarkdownFile', 'data': {'name': 'test', 'content': 'test content'}},
					'unknown.txt': {'type': 'UnknownFileType', 'data': {'name': 'unknown', 'content': 'unknown content'}},
				},
				base_dir=tmp_dir,
				extracted_content_count=0,
			)

			# Restore from state
			fs = FileSystem.from_state(state)

			# Should only have the known file type
			assert 'test.md' in fs.files
			assert 'unknown.txt' not in fs.files
			assert len(fs.files) == 1

			fs.nuke()


class TestFileSystemIntegration:
	"""Integration tests for FileSystem with real file operations."""

	async def test_complete_workflow(self):
		"""Test a complete filesystem workflow."""
		with tempfile.TemporaryDirectory() as tmp_dir:
			# Create filesystem
			fs = FileSystem(base_dir=tmp_dir, create_default_files=True)

			# Write to results file
			await fs.write_file('results.md', '# Test Results\n## Section 1\nInitial results.')

			# Append more content
			await fs.append_file('results.md', '\n## Section 2\nAdditional findings.')

			# Create a notes file
			await fs.write_file('notes.txt', 'Important notes:\n- Note 1\n- Note 2')

			# Save extracted content
			await fs.save_extracted_content('Extracted data from web page')
			await fs.save_extracted_content('Second extraction')

			# Verify file listing
			files = fs.list_files()
			assert len(files) == 5  # results.md, todo.md, notes.txt, 2 extracted files

			# Verify content
			file_obj = fs.get_file('results.md')
			assert file_obj is not None
			results_content = file_obj.content
			assert '# Test Results' in results_content
			assert '## Section 1' in results_content
			assert '## Section 2' in results_content
			assert 'Additional findings.' in results_content

			# Test state persistence
			state = fs.get_state()
			fs.nuke()

			# Restore from state
			fs2 = FileSystem.from_state(state)

			# Verify restoration
			assert len(fs2.files) == 5
			file_obj = fs2.get_file('results.md')
			assert file_obj is not None
			assert file_obj.content == results_content
			file_obj = fs2.get_file('notes.txt')
			assert file_obj is not None
			assert file_obj.content == 'Important notes:\n- Note 1\n- Note 2'
			assert fs2.extracted_content_count == 2

			# Verify files exist on disk
			for filename in files:
				assert (fs2.data_dir / filename).exists()

			fs2.nuke()

	async def test_concurrent_operations(self):
		"""Test concurrent file operations."""
		with tempfile.TemporaryDirectory() as tmp_dir:
			fs = FileSystem(base_dir=tmp_dir, create_default_files=False)

			# Create multiple files concurrently
			tasks = []
			for i in range(5):
				tasks.append(fs.write_file(f'file_{i}.md', f'Content for file {i}'))

			# Wait for all operations to complete
			results = await asyncio.gather(*tasks)

			# Verify all operations succeeded
			for result in results:
				assert 'successfully' in result

			# Verify all files were created
			assert len(fs.files) == 5
			for i in range(5):
				assert f'file_{i}.md' in fs.files
				file_obj = fs.get_file(f'file_{i}.md')
				assert file_obj is not None
				assert file_obj.content == f'Content for file {i}'

			fs.nuke()

```

---

## backend/browser-use/tests/ci/infrastructure/test_registry_action_parameter_injection.py

```py
import asyncio
import base64
import socketserver

import pytest
from pytest_httpserver import HTTPServer

from browser_use.browser import BrowserProfile, BrowserSession

# Fix for httpserver hanging on shutdown - prevent blocking on socket close
socketserver.ThreadingMixIn.block_on_close = False
socketserver.ThreadingMixIn.daemon_threads = True


class TestBrowserContext:
	"""Tests for browser context functionality using real browser instances."""

	@pytest.fixture(scope='session')
	def http_server(self):
		"""Create and provide a test HTTP server that serves static content."""
		server = HTTPServer()
		server.start()

		# Add routes for test pages
		server.expect_request('/').respond_with_data(
			'<html><head><title>Test Home Page</title></head><body><h1>Test Home Page</h1><p>Welcome to the test site</p></body></html>',
			content_type='text/html',
		)

		server.expect_request('/scroll_test').respond_with_data(
			"""
            <html>
            <head>
                <title>Scroll Test</title>
                <style>
                    body { height: 3000px; }
                    .marker { position: absolute; }
                    #top { top: 0; }
                    #middle { top: 1000px; }
                    #bottom { top: 2000px; }
                </style>
            </head>
            <body>
                <div id="top" class="marker">Top of the page</div>
                <div id="middle" class="marker">Middle of the page</div>
                <div id="bottom" class="marker">Bottom of the page</div>
            </body>
            </html>
            """,
			content_type='text/html',
		)

		yield server
		server.stop()

	@pytest.fixture(scope='session')
	def base_url(self, http_server):
		"""Return the base URL for the test HTTP server."""
		return f'http://{http_server.host}:{http_server.port}'

	@pytest.fixture(scope='module')
	async def browser_session(self):
		"""Create and provide a BrowserSession instance with security disabled."""
		browser_session = BrowserSession(
			browser_profile=BrowserProfile(
				headless=True,
				user_data_dir=None,
				keep_alive=True,
			)
		)
		await browser_session.start()
		yield browser_session
		await browser_session.kill()
		# Ensure event bus is properly stopped
		await browser_session.event_bus.stop(clear=True, timeout=5)

	@pytest.mark.skip(reason='TODO: fix')
	def test_is_url_allowed(self):
		"""
		Test the _is_url_allowed method to verify that it correctly checks URLs against
		the allowed domains configuration.
		"""
		# Scenario 1: allowed_domains is None, any URL should be allowed.
		from bubus import EventBus

		from browser_use.browser.watchdogs.security_watchdog import SecurityWatchdog

		config1 = BrowserProfile(allowed_domains=None, headless=True, user_data_dir=None)
		context1 = BrowserSession(browser_profile=config1)
		event_bus1 = EventBus()
		watchdog1 = SecurityWatchdog(browser_session=context1, event_bus=event_bus1)
		assert watchdog1._is_url_allowed('http://anydomain.com') is True
		assert watchdog1._is_url_allowed('https://anotherdomain.org/path') is True

		# Scenario 2: allowed_domains is provided.
		# Note: match_url_with_domain_pattern defaults to https:// scheme when none is specified
		allowed = ['https://example.com', 'http://example.com', 'http://*.mysite.org', 'https://*.mysite.org']
		config2 = BrowserProfile(allowed_domains=allowed, headless=True, user_data_dir=None)
		context2 = BrowserSession(browser_profile=config2)
		event_bus2 = EventBus()
		watchdog2 = SecurityWatchdog(browser_session=context2, event_bus=event_bus2)

		# URL exactly matching
		assert watchdog2._is_url_allowed('http://example.com') is True
		# URL with subdomain (should not be allowed)
		assert watchdog2._is_url_allowed('http://sub.example.com/path') is False
		# URL with subdomain for wildcard pattern (should be allowed)
		assert watchdog2._is_url_allowed('http://sub.mysite.org') is True
		# URL that matches second allowed domain
		assert watchdog2._is_url_allowed('https://mysite.org/page') is True
		# URL with port number, still allowed (port is stripped)
		assert watchdog2._is_url_allowed('http://example.com:8080') is True
		assert watchdog2._is_url_allowed('https://example.com:443') is True

		# Scenario 3: Malformed URL or empty domain
		# urlparse will return an empty netloc for some malformed URLs.
		assert watchdog2._is_url_allowed('notaurl') is False

	# Method was removed from BrowserSession

	def test_enhanced_css_selector_for_element(self):
		"""
		Test removed: _enhanced_css_selector_for_element method no longer exists.
		"""
		pass  # Method was removed from BrowserSession

	@pytest.mark.asyncio
	@pytest.mark.skip(reason='TODO: fix')
	async def test_navigate_and_get_current_page(self, browser_session, base_url):
		"""Test that navigate method changes the URL and get_current_page returns the proper page."""
		# Navigate to the test page
		from browser_use.browser.events import NavigateToUrlEvent

		event = browser_session.event_bus.dispatch(NavigateToUrlEvent(url=f'{base_url}/'))
		await event

		# Get the current page
		url = await browser_session.get_current_page_url()

		# Verify the page URL matches what we navigated to
		assert f'{base_url}/' in url

		# Verify the page title
		title = await browser_session.get_current_page_title()
		assert title == 'Test Home Page'

	@pytest.mark.asyncio
	@pytest.mark.skip(reason='TODO: fix')
	async def test_refresh_page(self, browser_session, base_url):
		"""Test that refresh_page correctly reloads the current page."""
		# Navigate to the test page
		from browser_use.browser.events import NavigateToUrlEvent

		event = browser_session.event_bus.dispatch(NavigateToUrlEvent(url=f'{base_url}/'))
		await event

		# Get the current page info before refresh
		url_before = await browser_session.get_current_page_url()
		title_before = await browser_session.get_current_page_title()

		# Refresh the page
		await browser_session.refresh()

		# Get the current page info after refresh
		url_after = await browser_session.get_current_page_url()
		title_after = await browser_session.get_current_page_title()

		# Verify it's still on the same URL
		assert url_after == url_before

		# Verify the page title is still correct
		assert title_after == 'Test Home Page'

	@pytest.mark.asyncio
	@pytest.mark.skip(reason='TODO: fix')
	async def test_execute_javascript(self, browser_session, base_url):
		"""Test that execute_javascript correctly executes JavaScript in the current page."""
		# Navigate to a test page
		from browser_use.browser.events import NavigateToUrlEvent

		event = browser_session.event_bus.dispatch(NavigateToUrlEvent(url=f'{base_url}/'))
		await event

		# Execute a simple JavaScript snippet that returns a value
		result = await browser_session.execute_javascript('document.title')

		# Verify the result
		assert result == 'Test Home Page'

		# Execute JavaScript that modifies the page
		await browser_session.execute_javascript("document.body.style.backgroundColor = 'red'")

		# Verify the change by reading back the value
		bg_color = await browser_session.execute_javascript('document.body.style.backgroundColor')
		assert bg_color == 'red'

	@pytest.mark.asyncio
	@pytest.mark.skip(reason='TODO: fix')
	@pytest.mark.skip(reason='get_scroll_info API changed - depends on page object that no longer exists')
	async def test_get_scroll_info(self, browser_session, base_url):
		"""Test that get_scroll_info returns the correct scroll position information."""
		# Navigate to the scroll test page
		from browser_use.browser.events import NavigateToUrlEvent

		event = browser_session.event_bus.dispatch(NavigateToUrlEvent(url=f'{base_url}/scroll_test'))
		await event
		page = await browser_session.get_current_page()

		# Get initial scroll info
		pixels_above_initial, pixels_below_initial = await browser_session.get_scroll_info(page)

		# Verify initial scroll position
		assert pixels_above_initial == 0, 'Initial scroll position should be at the top'
		assert pixels_below_initial > 0, 'There should be content below the viewport'

		# Scroll down the page
		await browser_session.execute_javascript('window.scrollBy(0, 500)')
		await asyncio.sleep(0.2)  # Brief delay for scroll to complete

		# Get new scroll info
		pixels_above_after_scroll, pixels_below_after_scroll = await browser_session.get_scroll_info(page)

		# Verify new scroll position
		assert pixels_above_after_scroll > 0, 'Page should be scrolled down'
		assert pixels_above_after_scroll >= 400, 'Page should be scrolled down at least 400px'
		assert pixels_below_after_scroll < pixels_below_initial, 'Less content should be below viewport after scrolling'

	@pytest.mark.asyncio
	@pytest.mark.skip(reason='TODO: fix')
	async def test_take_screenshot(self, browser_session, base_url):
		"""Test that take_screenshot returns a valid base64 encoded image."""
		# Navigate to the test page
		from browser_use.browser.events import NavigateToUrlEvent

		event = browser_session.event_bus.dispatch(NavigateToUrlEvent(url=f'{base_url}/'))
		await event

		# Take a screenshot
		screenshot_base64 = await browser_session.take_screenshot()

		# Verify the screenshot is a valid base64 string
		assert isinstance(screenshot_base64, str)
		assert len(screenshot_base64) > 0

		# Verify it can be decoded as base64
		try:
			image_data = base64.b64decode(screenshot_base64)
			# Verify the data starts with a valid image signature (PNG file header)
			assert image_data[:8] == b'\x89PNG\r\n\x1a\n', 'Screenshot is not a valid PNG image'
		except Exception as e:
			pytest.fail(f'Failed to decode screenshot as base64: {e}')

	@pytest.mark.asyncio
	@pytest.mark.skip(reason='TODO: fix')
	async def test_switch_tab_operations(self, browser_session, base_url):
		"""Test tab creation, switching, and closing operations."""
		# Navigate to home page in first tab
		from browser_use.browser.events import NavigateToUrlEvent

		event = browser_session.event_bus.dispatch(NavigateToUrlEvent(url=f'{base_url}/'))
		await event

		# Create a new tab
		await browser_session.create_new_tab(f'{base_url}/scroll_test')

		# Verify we have two tabs now
		tabs_info = await browser_session.get_tabs()
		assert len(tabs_info) == 2, 'Should have two tabs open'

		# Verify current tab is the scroll test page
		current_url = await browser_session.get_current_page_url()
		assert f'{base_url}/scroll_test' in current_url

		# Switch back to the first tab
		await browser_session.switch_to_tab(0)

		# Verify we're back on the home page
		current_url = await browser_session.get_current_page_url()
		assert f'{base_url}/' in current_url

		# Close the second tab
		await browser_session.close_tab(1)

		# Verify we have the expected number of tabs
		# The first tab remains plus any about:blank tabs created by AboutBlankWatchdog
		tabs_info = await browser_session.get_tabs_info()
		# Filter out about:blank tabs created by the watchdog
		non_blank_tabs = [tab for tab in tabs_info if 'about:blank' not in tab.url]
		assert len(non_blank_tabs) == 1, (
			f'Should have one non-blank tab open after closing the second, but got {len(non_blank_tabs)}: {non_blank_tabs}'
		)
		assert base_url in non_blank_tabs[0].url, 'The remaining tab should be the home page'

	# TODO: highlighting doesn't exist anymore
	# @pytest.mark.asyncio
	# async def test_remove_highlights(self, browser_session, base_url):
	# 	"""Test that remove_highlights successfully removes highlight elements."""
	# 	# Navigate to a test page
	# 	from browser_use.browser.events import NavigateToUrlEvent; event = browser_session.event_bus.dispatch(NavigateToUrlEvent(url=f'{base_url}/')

	# 	# Add a highlight via JavaScript
	# 	await browser_session.execute_javascript("""
	#         const container = document.createElement('div');
	#         container.id = 'playwright-highlight-container';
	#         document.body.appendChild(container);

	#         const highlight = document.createElement('div');
	#         highlight.id = 'playwright-highlight-1';
	#         container.appendChild(highlight);

	#         const element = document.querySelector('h1');
	#         element.setAttribute('browser-user-highlight-id', 'playwright-highlight-1');
	#     """)

	# 	# Verify the highlight container exists
	# 	container_exists = await browser_session.execute_javascript(
	# 		"document.getElementById('playwright-highlight-container') !== null"
	# 	)
	# 	assert container_exists, 'Highlight container should exist before removal'

	# 	# Call remove_highlights
	# 	await browser_session.remove_highlights()

	# 	# Verify the highlight container was removed
	# 	container_exists_after = await browser_session.execute_javascript(
	# 		"document.getElementById('playwright-highlight-container') !== null"
	# 	)
	# 	assert not container_exists_after, 'Highlight container should be removed'

	# 	# Verify the highlight attribute was removed from the element
	# 	attribute_exists = await browser_session.execute_javascript(
	# 		"document.querySelector('h1').hasAttribute('browser-user-highlight-id')"
	# 	)
	# 	assert not attribute_exists, 'browser-user-highlight-id attribute should be removed'

	@pytest.mark.asyncio
	@pytest.mark.skip(reason='TODO: fix')
	async def test_custom_action_with_no_arguments(self, browser_session, base_url):
		"""Test that custom actions with no arguments are handled correctly"""
		from browser_use.agent.views import ActionResult
		from browser_use.tools.registry.service import Registry

		# Create a registry
		registry = Registry()

		# Register a custom action with no arguments
		@registry.action('Some custom action with no args')
		def simple_action():
			return ActionResult(extracted_content='return some result')

		# Navigate to a test page
		from browser_use.browser.events import NavigateToUrlEvent

		event = browser_session.event_bus.dispatch(NavigateToUrlEvent(url=f'{base_url}/'))
		await event

		# Execute the action
		result = await registry.execute_action('simple_action', {})

		# Verify the result
		assert isinstance(result, ActionResult)
		assert result.extracted_content == 'return some result'

		# Test that the action model is created correctly
		action_model = registry.create_action_model()

		# The action should be in the model fields
		assert 'simple_action' in action_model.model_fields

		# Create an instance with the simple_action
		action_instance = action_model(simple_action={})  # type: ignore[call-arg]

		# Test that model_dump works correctly
		dumped = action_instance.model_dump(exclude_unset=True)
		assert 'simple_action' in dumped
		assert dumped['simple_action'] == {}

		# Test async version as well
		@registry.action('Async custom action with no args')
		async def async_simple_action():
			return ActionResult(extracted_content='async result')

		result = await registry.execute_action('async_simple_action', {})
		assert result.extracted_content == 'async result'

		# Test with special parameters but no regular arguments
		@registry.action('Action with only special params')
		async def special_params_only(browser_session):
			current_url = await browser_session.get_current_page_url()
			return ActionResult(extracted_content=f'Page URL: {current_url}')

		result = await registry.execute_action('special_params_only', {}, browser_session=browser_session)
		assert 'Page URL:' in result.extracted_content
		assert base_url in result.extracted_content

```

---

## backend/browser-use/tests/ci/infrastructure/test_registry_core.py

```py
"""
Comprehensive tests for the action registry system - Core functionality.

Tests cover:
1. Existing parameter patterns (individual params, pydantic models)
2. Special parameter injection (browser_session, page_extraction_llm, etc.)
3. Action-to-action calling scenarios
4. Mixed parameter patterns
5. Registry execution edge cases
"""

import asyncio
import logging

import pytest
from pydantic import Field
from pytest_httpserver import HTTPServer
from pytest_httpserver.httpserver import HandlerType

from browser_use.agent.views import ActionResult
from browser_use.browser import BrowserSession
from browser_use.browser.profile import BrowserProfile
from browser_use.llm.messages import UserMessage
from browser_use.tools.registry.service import Registry
from browser_use.tools.registry.views import ActionModel as BaseActionModel
from browser_use.tools.views import (
	ClickElementAction,
	InputTextAction,
	NoParamsAction,
	SearchAction,
)
from tests.ci.conftest import create_mock_llm

# Configure logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)


class TestContext:
	"""Simple context for testing"""

	pass


# Test parameter models
class SimpleParams(BaseActionModel):
	"""Simple parameter model"""

	value: str = Field(description='Test value')


class ComplexParams(BaseActionModel):
	"""Complex parameter model with multiple fields"""

	text: str = Field(description='Text input')
	number: int = Field(description='Number input', default=42)
	optional_flag: bool = Field(description='Optional boolean', default=False)


# Test fixtures
@pytest.fixture(scope='session')
def http_server():
	"""Create and provide a test HTTP server that serves static content."""
	server = HTTPServer()
	server.start()

	# Add a simple test page that can handle multiple requests
	server.expect_request('/test', handler_type=HandlerType.PERMANENT).respond_with_data(
		'<html><head><title>Test Page</title></head><body><h1>Test Page</h1><p>Hello from test page</p></body></html>',
		content_type='text/html',
	)

	yield server

	server.stop()


@pytest.fixture(scope='session')
def base_url(http_server):
	"""Return the base URL for the test HTTP server."""
	return f'http://{http_server.host}:{http_server.port}'


@pytest.fixture(scope='module')
def mock_llm():
	"""Create a mock LLM"""
	return create_mock_llm()


@pytest.fixture(scope='function')
def registry():
	"""Create a fresh registry for each test"""
	return Registry[TestContext]()


@pytest.fixture(scope='function')
async def browser_session(base_url):
	"""Create a real BrowserSession for testing"""
	browser_session = BrowserSession(
		browser_profile=BrowserProfile(
			headless=True,
			user_data_dir=None,
			keep_alive=True,
		)
	)
	await browser_session.start()
	from browser_use.browser.events import NavigateToUrlEvent

	browser_session.event_bus.dispatch(NavigateToUrlEvent(url=f'{base_url}/test'))
	await asyncio.sleep(0.5)  # Wait for navigation
	yield browser_session
	await browser_session.kill()


class TestActionRegistryParameterPatterns:
	"""Test different parameter patterns that should all continue to work"""

	async def test_individual_parameters_no_browser(self, registry):
		"""Test action with individual parameters, no special injection"""

		@registry.action('Simple action with individual params')
		async def simple_action(text: str, number: int = 10):
			return ActionResult(extracted_content=f'Text: {text}, Number: {number}')

		# Test execution
		result = await registry.execute_action('simple_action', {'text': 'hello', 'number': 42})

		assert isinstance(result, ActionResult)
		assert result.extracted_content is not None
		assert 'Text: hello, Number: 42' in result.extracted_content

	async def test_individual_parameters_with_browser(self, registry, browser_session, base_url):
		"""Test action with individual parameters plus browser_session injection"""

		@registry.action('Action with individual params and browser')
		async def action_with_browser(text: str, browser_session: BrowserSession):
			url = await browser_session.get_current_page_url()
			return ActionResult(extracted_content=f'Text: {text}, URL: {url}')

		# Navigate to test page first
		from browser_use.browser.events import NavigateToUrlEvent

		event = browser_session.event_bus.dispatch(NavigateToUrlEvent(url=f'{base_url}/test', new_tab=True))
		await event

		# Test execution
		result = await registry.execute_action('action_with_browser', {'text': 'hello'}, browser_session=browser_session)

		assert isinstance(result, ActionResult)
		assert result.extracted_content is not None
		assert 'Text: hello, URL:' in result.extracted_content
		assert base_url in result.extracted_content

	async def test_pydantic_model_parameters(self, registry, browser_session, base_url):
		"""Test action that takes a pydantic model as first parameter"""

		@registry.action('Action with pydantic model', param_model=ComplexParams)
		async def pydantic_action(params: ComplexParams, browser_session: BrowserSession):
			url = await browser_session.get_current_page_url()
			return ActionResult(
				extracted_content=f'Text: {params.text}, Number: {params.number}, Flag: {params.optional_flag}, URL: {url}'
			)

		# Navigate to test page first
		from browser_use.browser.events import NavigateToUrlEvent

		event = browser_session.event_bus.dispatch(NavigateToUrlEvent(url=f'{base_url}/test', new_tab=True))
		await event

		# Test execution
		result = await registry.execute_action(
			'pydantic_action', {'text': 'test', 'number': 100, 'optional_flag': True}, browser_session=browser_session
		)

		assert isinstance(result, ActionResult)
		assert result.extracted_content is not None
		assert 'Text: test, Number: 100, Flag: True' in result.extracted_content
		assert base_url in result.extracted_content

	async def test_mixed_special_parameters(self, registry, browser_session, base_url, mock_llm):
		"""Test action with multiple special injected parameters"""

		from browser_use.llm.base import BaseChatModel

		@registry.action('Action with multiple special params')
		async def multi_special_action(
			text: str,
			browser_session: BrowserSession,
			page_extraction_llm: BaseChatModel,
			available_file_paths: list,
		):
			llm_response = await page_extraction_llm.ainvoke([UserMessage(content='test')])
			files = available_file_paths or []
			url = await browser_session.get_current_page_url()

			return ActionResult(
				extracted_content=f'Text: {text}, URL: {url}, LLM: {llm_response.completion}, Files: {len(files)}'
			)

		# Navigate to test page first
		from browser_use.browser.events import NavigateToUrlEvent

		event = browser_session.event_bus.dispatch(NavigateToUrlEvent(url=f'{base_url}/test', new_tab=True))
		await event

		# Test execution
		result = await registry.execute_action(
			'multi_special_action',
			{'text': 'hello'},
			browser_session=browser_session,
			page_extraction_llm=mock_llm,
			available_file_paths=['file1.txt', 'file2.txt'],
		)

		assert isinstance(result, ActionResult)
		assert result.extracted_content is not None
		assert 'Text: hello' in result.extracted_content
		assert base_url in result.extracted_content
		# The mock LLM returns a JSON response
		assert '"Task completed successfully"' in result.extracted_content
		assert 'Files: 2' in result.extracted_content

	async def test_no_params_action(self, registry, browser_session):
		"""Test action with NoParamsAction model"""

		@registry.action('No params action', param_model=NoParamsAction)
		async def no_params_action(params: NoParamsAction, browser_session: BrowserSession):
			url = await browser_session.get_current_page_url()
			return ActionResult(extracted_content=f'No params action executed on {url}')

		# Test execution with any parameters (should be ignored)
		result = await registry.execute_action(
			'no_params_action', {'random': 'data', 'should': 'be', 'ignored': True}, browser_session=browser_session
		)

		assert isinstance(result, ActionResult)
		assert result.extracted_content is not None
		assert 'No params action executed on' in result.extracted_content
		assert '/test' in result.extracted_content


class TestActionToActionCalling:
	"""Test scenarios where actions call other actions"""

	async def test_action_calling_action_with_kwargs(self, registry, browser_session):
		"""Test action calling another action using kwargs (current problematic pattern)"""

		# Helper function that actions can call
		async def helper_function(browser_session: BrowserSession, data: str):
			url = await browser_session.get_current_page_url()
			return f'Helper processed: {data} on {url}'

		@registry.action('First action')
		async def first_action(text: str, browser_session: BrowserSession):
			# This should work without parameter conflicts
			result = await helper_function(browser_session=browser_session, data=text)
			return ActionResult(extracted_content=f'First: {result}')

		@registry.action('Calling action')
		async def calling_action(message: str, browser_session: BrowserSession):
			# Call the first action through the registry (simulates action-to-action calling)
			intermediate_result = await registry.execute_action(
				'first_action', {'text': message}, browser_session=browser_session
			)
			return ActionResult(extracted_content=f'Called result: {intermediate_result.extracted_content}')

		# Test the calling chain
		result = await registry.execute_action('calling_action', {'message': 'test'}, browser_session=browser_session)

		assert isinstance(result, ActionResult)
		assert result.extracted_content is not None
		assert 'Called result: First: Helper processed: test on' in result.extracted_content
		assert '/test' in result.extracted_content

	async def test_google_sheets_style_calling_pattern(self, registry, browser_session):
		"""Test the specific pattern from Google Sheets actions that causes the error"""

		# Simulate the _select_cell_or_range helper function
		async def _select_cell_or_range(browser_session: BrowserSession, cell_or_range: str):
			url = await browser_session.get_current_page_url()
			return ActionResult(extracted_content=f'Selected cell {cell_or_range} on {url}')

		@registry.action('Select cell or range')
		async def select_cell_or_range(cell_or_range: str, browser_session: BrowserSession):
			# This pattern now works with kwargs
			return await _select_cell_or_range(browser_session=browser_session, cell_or_range=cell_or_range)

		@registry.action('Select cell or range (fixed)')
		async def select_cell_or_range_fixed(cell_or_range: str, browser_session: BrowserSession):
			# This pattern also works
			return await _select_cell_or_range(browser_session, cell_or_range)

		@registry.action('Update range contents')
		async def update_range_contents(range_name: str, new_contents: str, browser_session: BrowserSession):
			# This action calls select_cell_or_range, simulating the real Google Sheets pattern
			# Get the action's param model to call it properly
			action = registry.registry.actions['select_cell_or_range_fixed']
			params = action.param_model(cell_or_range=range_name)
			await select_cell_or_range_fixed(cell_or_range=range_name, browser_session=browser_session)
			return ActionResult(extracted_content=f'Updated range {range_name} with {new_contents}')

		# Test the fixed version (should work)
		result_fixed = await registry.execute_action(
			'select_cell_or_range_fixed', {'cell_or_range': 'A1:F100'}, browser_session=browser_session
		)
		assert result_fixed.extracted_content is not None
		assert 'Selected cell A1:F100 on' in result_fixed.extracted_content
		assert '/test' in result_fixed.extracted_content

		# Test the chained calling pattern
		result_chain = await registry.execute_action(
			'update_range_contents', {'range_name': 'B2:D4', 'new_contents': 'test data'}, browser_session=browser_session
		)
		assert result_chain.extracted_content is not None
		assert 'Updated range B2:D4 with test data' in result_chain.extracted_content

		# Test the problematic version (should work with enhanced registry)
		result_problematic = await registry.execute_action(
			'select_cell_or_range', {'cell_or_range': 'A1:F100'}, browser_session=browser_session
		)
		# With the enhanced registry, this should succeed
		assert result_problematic.extracted_content is not None
		assert 'Selected cell A1:F100 on' in result_problematic.extracted_content
		assert '/test' in result_problematic.extracted_content

	async def test_complex_action_chain(self, registry, browser_session):
		"""Test a complex chain of actions calling other actions"""

		@registry.action('Base action')
		async def base_action(value: str, browser_session: BrowserSession):
			url = await browser_session.get_current_page_url()
			return ActionResult(extracted_content=f'Base: {value} on {url}')

		@registry.action('Middle action')
		async def middle_action(input_val: str, browser_session: BrowserSession):
			# Call base action
			base_result = await registry.execute_action(
				'base_action', {'value': f'processed-{input_val}'}, browser_session=browser_session
			)
			return ActionResult(extracted_content=f'Middle: {base_result.extracted_content}')

		@registry.action('Top action')
		async def top_action(original: str, browser_session: BrowserSession):
			# Call middle action
			middle_result = await registry.execute_action(
				'middle_action', {'input_val': f'enhanced-{original}'}, browser_session=browser_session
			)
			return ActionResult(extracted_content=f'Top: {middle_result.extracted_content}')

		# Test the full chain
		result = await registry.execute_action('top_action', {'original': 'test'}, browser_session=browser_session)

		assert isinstance(result, ActionResult)
		assert result.extracted_content is not None
		assert 'Top: Middle: Base: processed-enhanced-test on' in result.extracted_content
		assert '/test' in result.extracted_content


class TestRegistryEdgeCases:
	"""Test edge cases and error conditions"""

	async def test_decorated_action_rejects_positional_args(self, registry, browser_session):
		"""Test that decorated actions reject positional arguments"""

		@registry.action('Action that should reject positional args')
		async def test_action(cell_or_range: str, browser_session: BrowserSession):
			url = await browser_session.get_current_page_url()
			return ActionResult(extracted_content=f'Selected cell {cell_or_range} on {url}')

		# Test that calling with positional arguments raises TypeError
		with pytest.raises(
			TypeError, match='test_action\\(\\) does not accept positional arguments, only keyword arguments are allowed'
		):
			await test_action('A1:B2', browser_session)

		# Test that calling with keyword arguments works
		result = await test_action(browser_session=browser_session, cell_or_range='A1:B2')
		assert isinstance(result, ActionResult)
		assert result.extracted_content is not None
		assert 'Selected cell A1:B2 on' in result.extracted_content

	async def test_missing_required_browser_session(self, registry):
		"""Test that actions requiring browser_session fail appropriately when not provided"""

		@registry.action('Requires browser')
		async def requires_browser(text: str, browser_session: BrowserSession):
			url = await browser_session.get_current_page_url()
			return ActionResult(extracted_content=f'Text: {text}, URL: {url}')

		# Should raise RuntimeError when browser_session is required but not provided
		with pytest.raises(RuntimeError, match='requires browser_session but none provided'):
			await registry.execute_action(
				'requires_browser',
				{'text': 'test'},
				# No browser_session provided
			)

	async def test_missing_required_llm(self, registry, browser_session):
		"""Test that actions requiring page_extraction_llm fail appropriately when not provided"""

		from browser_use.llm.base import BaseChatModel

		@registry.action('Requires LLM')
		async def requires_llm(text: str, browser_session: BrowserSession, page_extraction_llm: BaseChatModel):
			url = await browser_session.get_current_page_url()
			llm_response = await page_extraction_llm.ainvoke([UserMessage(content='test')])
			return ActionResult(extracted_content=f'Text: {text}, LLM: {llm_response.completion}')

		# Should raise RuntimeError when page_extraction_llm is required but not provided
		with pytest.raises(RuntimeError, match='requires page_extraction_llm but none provided'):
			await registry.execute_action(
				'requires_llm',
				{'text': 'test'},
				browser_session=browser_session,
				# No page_extraction_llm provided
			)

	async def test_invalid_parameters(self, registry, browser_session):
		"""Test handling of invalid parameters"""

		@registry.action('Typed action')
		async def typed_action(number: int, browser_session: BrowserSession):
			return ActionResult(extracted_content=f'Number: {number}')

		# Should raise RuntimeError when parameter validation fails
		with pytest.raises(RuntimeError, match='Invalid parameters'):
			await registry.execute_action(
				'typed_action',
				{'number': 'not a number'},  # Invalid type
				browser_session=browser_session,
			)

	async def test_nonexistent_action(self, registry, browser_session):
		"""Test calling a non-existent action"""

		with pytest.raises(ValueError, match='Action nonexistent_action not found'):
			await registry.execute_action('nonexistent_action', {'param': 'value'}, browser_session=browser_session)

	async def test_sync_action_wrapper(self, registry, browser_session):
		"""Test that sync functions are properly wrapped to be async"""

		@registry.action('Sync action')
		def sync_action(text: str, browser_session: BrowserSession):
			# This is a sync function that should be wrapped
			return ActionResult(extracted_content=f'Sync: {text}')

		# Should work even though the original function is sync
		result = await registry.execute_action('sync_action', {'text': 'test'}, browser_session=browser_session)

		assert isinstance(result, ActionResult)
		assert result.extracted_content is not None
		assert 'Sync: test' in result.extracted_content

	async def test_excluded_actions(self, browser_session):
		"""Test that excluded actions are not registered"""

		registry_with_exclusions = Registry[TestContext](exclude_actions=['excluded_action'])

		@registry_with_exclusions.action('Excluded action')
		async def excluded_action(text: str):
			return ActionResult(extracted_content=f'Should not execute: {text}')

		@registry_with_exclusions.action('Included action')
		async def included_action(text: str):
			return ActionResult(extracted_content=f'Should execute: {text}')

		# Excluded action should not be in registry
		assert 'excluded_action' not in registry_with_exclusions.registry.actions
		assert 'included_action' in registry_with_exclusions.registry.actions

		# Should raise error when trying to execute excluded action
		with pytest.raises(ValueError, match='Action excluded_action not found'):
			await registry_with_exclusions.execute_action('excluded_action', {'text': 'test'})

		# Included action should work
		result = await registry_with_exclusions.execute_action('included_action', {'text': 'test'})
		assert result.extracted_content is not None
		assert 'Should execute: test' in result.extracted_content


class TestExistingToolsActions:
	"""Test that existing tools actions continue to work"""

	async def test_existing_action_models(self, registry, browser_session):
		"""Test that existing action parameter models work correctly"""

		@registry.action('Test search', param_model=SearchAction)
		async def test_search(params: SearchAction, browser_session: BrowserSession):
			return ActionResult(extracted_content=f'Searched for: {params.query}')

		@registry.action('Test click', param_model=ClickElementAction)
		async def test_click(params: ClickElementAction, browser_session: BrowserSession):
			return ActionResult(extracted_content=f'Clicked element: {params.index}')

		@registry.action('Test input', param_model=InputTextAction)
		async def test_input(params: InputTextAction, browser_session: BrowserSession):
			return ActionResult(extracted_content=f'Input text: {params.text} at index: {params.index}')

		# Test SearchGoogleAction
		result1 = await registry.execute_action('test_search', {'query': 'python testing'}, browser_session=browser_session)
		assert result1.extracted_content is not None
		assert 'Searched for: python testing' in result1.extracted_content

		# Test ClickElementAction
		result2 = await registry.execute_action('test_click', {'index': 42}, browser_session=browser_session)
		assert result2.extracted_content is not None
		assert 'Clicked element: 42' in result2.extracted_content

		# Test InputTextAction
		result3 = await registry.execute_action('test_input', {'index': 5, 'text': 'test input'}, browser_session=browser_session)
		assert result3.extracted_content is not None
		assert 'Input text: test input at index: 5' in result3.extracted_content

	async def test_pydantic_vs_individual_params_consistency(self, registry, browser_session):
		"""Test that pydantic and individual parameter patterns produce consistent results"""

		# Action using individual parameters
		@registry.action('Individual params')
		async def individual_params_action(text: str, number: int, browser_session: BrowserSession):
			return ActionResult(extracted_content=f'Individual: {text}-{number}')

		# Action using pydantic model
		class TestParams(BaseActionModel):
			text: str
			number: int

		@registry.action('Pydantic params', param_model=TestParams)
		async def pydantic_params_action(params: TestParams, browser_session: BrowserSession):
			return ActionResult(extracted_content=f'Pydantic: {params.text}-{params.number}')

		# Both should produce similar results
		test_data = {'text': 'hello', 'number': 42}

		result1 = await registry.execute_action('individual_params_action', test_data, browser_session=browser_session)

		result2 = await registry.execute_action('pydantic_params_action', test_data, browser_session=browser_session)

		# Both should extract the same content (just different prefixes)
		assert result1.extracted_content is not None
		assert 'hello-42' in result1.extracted_content
		assert result2.extracted_content is not None
		assert 'hello-42' in result2.extracted_content
		assert 'Individual:' in result1.extracted_content
		assert 'Pydantic:' in result2.extracted_content

```

---

## backend/browser-use/tests/ci/infrastructure/test_registry_validation.py

```py
"""
Comprehensive tests for the action registry system - Validation and patterns.

Tests cover:
1. Type 1 and Type 2 patterns
2. Validation rules
3. Decorated function behavior
4. Parameter model generation
5. Parameter ordering
"""

import asyncio
import logging

import pytest
from pydantic import Field

from browser_use.agent.views import ActionResult
from browser_use.browser import BrowserSession
from browser_use.tools.registry.service import Registry
from browser_use.tools.registry.views import ActionModel as BaseActionModel
from tests.ci.conftest import create_mock_llm

# Configure logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)


class TestType1Pattern:
	"""Test Type 1 Pattern: Pydantic model first (from normalization tests)"""

	def test_type1_with_param_model(self):
		"""Type 1: action(params: Model, special_args...) should work"""
		registry = Registry()

		class ClickAction(BaseActionModel):
			index: int
			delay: float = 0.0

		@registry.action('Click element', param_model=ClickAction)
		async def click_element(params: ClickAction, browser_session: BrowserSession):
			return ActionResult(extracted_content=f'Clicked {params.index}')

		# Verify registration
		assert 'click_element' in registry.registry.actions
		action = registry.registry.actions['click_element']
		assert action.param_model == ClickAction

		# Verify decorated function signature (should be kwargs-only)
		import inspect

		sig = inspect.signature(click_element)
		params = list(sig.parameters.values())

		# Should have no positional-only or positional-or-keyword params
		for param in params:
			assert param.kind in (inspect.Parameter.KEYWORD_ONLY, inspect.Parameter.VAR_KEYWORD)

	def test_type1_with_multiple_special_params(self):
		"""Type 1 with multiple special params should work"""
		registry = Registry()

		class ExtractAction(BaseActionModel):
			goal: str
			include_links: bool = False

		from browser_use.llm.base import BaseChatModel

		@registry.action('Extract content', param_model=ExtractAction)
		async def extract_content(params: ExtractAction, browser_session: BrowserSession, page_extraction_llm: BaseChatModel):
			return ActionResult(extracted_content=params.goal)

		assert 'extract_content' in registry.registry.actions


class TestType2Pattern:
	"""Test Type 2 Pattern: loose parameters (from normalization tests)"""

	def test_type2_simple_action(self):
		"""Type 2: action(arg1, arg2, special_args...) should work"""
		registry = Registry()

		@registry.action('Fill field')
		async def fill_field(index: int, text: str, browser_session: BrowserSession):
			return ActionResult(extracted_content=f'Filled {index} with {text}')

		# Verify registration
		assert 'fill_field' in registry.registry.actions
		action = registry.registry.actions['fill_field']

		# Should auto-generate param model
		assert action.param_model is not None
		assert 'index' in action.param_model.model_fields
		assert 'text' in action.param_model.model_fields

	def test_type2_with_defaults(self):
		"""Type 2 with default values should preserve defaults"""
		registry = Registry()

		@registry.action('Scroll page')
		async def scroll_page(direction: str = 'down', amount: int = 100, browser_session: BrowserSession = None):  # type: ignore
			return ActionResult(extracted_content=f'Scrolled {direction} by {amount}')

		action = registry.registry.actions['scroll_page']
		# Check that defaults are preserved in generated model
		schema = action.param_model.model_json_schema()
		assert schema['properties']['direction']['default'] == 'down'
		assert schema['properties']['amount']['default'] == 100

	def test_type2_no_action_params(self):
		"""Type 2 with only special params should work"""
		registry = Registry()

		@registry.action('Save PDF')
		async def save_pdf(browser_session: BrowserSession):
			return ActionResult(extracted_content='Saved PDF')

		action = registry.registry.actions['save_pdf']
		# Should have empty or minimal param model
		fields = action.param_model.model_fields
		assert len(fields) == 0 or all(f in ['title'] for f in fields)

	def test_no_special_params_action(self):
		"""Test action with no special params (like wait action in Tools)"""
		registry = Registry()

		@registry.action('Wait for x seconds default 3')
		async def wait(seconds: int = 3):
			await asyncio.sleep(seconds)
			return ActionResult(extracted_content=f'Waited {seconds} seconds')

		# Should register successfully
		assert 'wait' in registry.registry.actions
		action = registry.registry.actions['wait']

		# Should have seconds in param model
		assert 'seconds' in action.param_model.model_fields

		# Should preserve default value
		schema = action.param_model.model_json_schema()
		assert schema['properties']['seconds']['default'] == 3


class TestValidationRules:
	"""Test validation rules for action registration (from normalization tests)"""

	def test_error_on_kwargs_in_original_function(self):
		"""Should error if original function has kwargs"""
		registry = Registry()

		with pytest.raises(ValueError, match='kwargs.*not allowed'):

			@registry.action('Bad action')
			async def bad_action(index: int, browser_session: BrowserSession, **kwargs):
				pass

	def test_error_on_special_param_name_with_wrong_type(self):
		"""Should error if special param name used with wrong type"""
		registry = Registry()

		# Using 'browser_session' with wrong type should error
		with pytest.raises(ValueError, match='conflicts with special argument.*browser_session: BrowserSession'):

			@registry.action('Bad session')
			async def bad_session(browser_session: str):
				pass

	def test_special_params_must_match_type(self):
		"""Special params with correct types should work"""
		registry = Registry()

		@registry.action('Good action')
		async def good_action(
			index: int,
			browser_session: BrowserSession,  # Correct type
		):
			return ActionResult()

		assert 'good_action' in registry.registry.actions


class TestDecoratedFunctionBehavior:
	"""Test behavior of decorated action functions (from normalization tests)"""

	async def test_decorated_function_only_accepts_kwargs(self):
		"""Decorated functions should only accept kwargs, no positional args"""
		registry = Registry()

		class MockBrowserSession:
			async def get_current_page(self):
				return None

		@registry.action('Click')
		async def click(index: int, browser_session: BrowserSession):
			return ActionResult()

		# Should raise error when called with positional args
		with pytest.raises(TypeError, match='positional arguments'):
			await click(5, MockBrowserSession())

	async def test_decorated_function_accepts_params_model(self):
		"""Decorated function should accept params as model"""
		registry = Registry()

		class MockBrowserSession:
			async def get_current_page(self):
				return None

		@registry.action('Input text')
		async def input_text(index: int, text: str, browser_session: BrowserSession):
			return ActionResult(extracted_content=f'{index}:{text}')

		# Get the generated param model class
		action = registry.registry.actions['input_text']
		ParamsModel = action.param_model

		# Should work with params model
		result = await input_text(params=ParamsModel(index=5, text='hello'), browser_session=MockBrowserSession())
		assert result.extracted_content == '5:hello'

	async def test_decorated_function_ignores_extra_kwargs(self):
		"""Decorated function should ignore extra kwargs for easy unpacking"""
		registry = Registry()

		@registry.action('Simple action')
		async def simple_action(value: int):
			return ActionResult(extracted_content=str(value))

		# Should work even with extra kwargs
		special_context = {
			'browser_session': None,
			'page_extraction_llm': create_mock_llm(),
			'context': {'extra': 'data'},
			'unknown_param': 'ignored',
		}

		action = registry.registry.actions['simple_action']
		ParamsModel = action.param_model

		result = await simple_action(params=ParamsModel(value=42), **special_context)
		assert result.extracted_content == '42'


class TestParamsModelGeneration:
	"""Test automatic parameter model generation (from normalization tests)"""

	def test_generates_model_from_non_special_args(self):
		"""Should generate param model from non-special positional args"""
		registry = Registry()

		@registry.action('Complex action')
		async def complex_action(
			query: str,
			max_results: int,
			include_images: bool = True,
			browser_session: BrowserSession = None,  # type: ignore
		):
			return ActionResult()

		action = registry.registry.actions['complex_action']
		model_fields = action.param_model.model_fields

		# Should include only non-special params
		assert 'query' in model_fields
		assert 'max_results' in model_fields
		assert 'include_images' in model_fields

		# Should NOT include special params
		assert 'browser_session' not in model_fields

	def test_preserves_type_annotations(self):
		"""Generated model should preserve type annotations"""
		registry = Registry()

		@registry.action('Typed action')
		async def typed_action(
			count: int,
			rate: float,
			enabled: bool,
			name: str | None = None,
			browser_session: BrowserSession = None,  # type: ignore
		):
			return ActionResult()

		action = registry.registry.actions['typed_action']
		schema = action.param_model.model_json_schema()

		# Check types are preserved
		assert schema['properties']['count']['type'] == 'integer'
		assert schema['properties']['rate']['type'] == 'number'
		assert schema['properties']['enabled']['type'] == 'boolean'
		# Optional should allow null
		assert 'null' in schema['properties']['name']['anyOf'][1]['type']


class TestParameterOrdering:
	"""Test mixed ordering of parameters (from normalization tests)"""

	def test_mixed_param_ordering(self):
		"""Should handle any ordering of action params and special params"""
		registry = Registry()
		from browser_use.llm.base import BaseChatModel

		# Special params mixed throughout
		@registry.action('Mixed params')
		async def mixed_action(
			first: str,
			browser_session: BrowserSession,
			second: int,
			third: bool = True,
			page_extraction_llm: BaseChatModel = None,  # type: ignore
		):
			return ActionResult()

		action = registry.registry.actions['mixed_action']
		model_fields = action.param_model.model_fields

		# Only action params in model
		assert set(model_fields.keys()) == {'first', 'second', 'third'}
		assert model_fields['third'].default is True

	def test_extract_content_pattern_registration(self):
		"""Test that the extract_content pattern with mixed params registers correctly"""
		registry = Registry()

		# This is the problematic pattern: positional arg, then special args, then kwargs with defaults
		@registry.action('Extract content from page')
		async def extract_content(
			goal: str,
			page_extraction_llm,
			include_links: bool = False,
		):
			return ActionResult(extracted_content=f'Goal: {goal}, include_links: {include_links}')

		# Verify registration
		assert 'extract_content' in registry.registry.actions
		action = registry.registry.actions['extract_content']

		# Check that the param model only includes user-facing params
		model_fields = action.param_model.model_fields
		assert 'goal' in model_fields
		assert 'include_links' in model_fields
		assert model_fields['include_links'].default is False

		# Special params should NOT be in the model
		assert 'page' not in model_fields
		assert 'page_extraction_llm' not in model_fields

		# Verify the action was properly registered
		assert action.name == 'extract_content'
		assert action.description == 'Extract content from page'


class TestParamsModelArgsAndKwargs:
	async def test_browser_session_double_kwarg(self):
		"""Run the test to diagnose browser_session parameter issue

		This test demonstrates the problem and our fix. The issue happens because:

		1. In tools/service.py, we have:
		``\`python
		@registry.action('Google Sheets: Select a specific cell or range of cells')
		async def select_cell_or_range(browser_session: BrowserSession, cell_or_range: str):
		    return await _select_cell_or_range(browser_session=browser_session, cell_or_range=cell_or_range)
		``\`

		2. When registry.execute_action calls this function, it adds browser_session to extra_args:
		``\`python
		# In registry/service.py
		if 'browser_session' in parameter_names:
		    extra_args['browser_session'] = browser_session
		``\`

		3. Then later, when calling action.function:
		``\`python
		return await action.function(**params_dict, **extra_args)
		``\`

		4. This effectively means browser_session is passed twice:
		- Once through extra_args['browser_session']
		- And again through params_dict['browser_session'] (from the original function)

		The fix is to pass browser_session positionally in select_cell_or_range:
		``\`python
		return await _select_cell_or_range(browser_session, cell_or_range)
		``\`

		This test confirms that this approach works.
		"""

		from browser_use.tools.registry.service import Registry
		from browser_use.tools.registry.views import ActionModel

		# Simple context for testing
		class TestContext:
			pass

		class MockBrowserSession:
			async def get_current_page(self):
				return None

		browser_session = MockBrowserSession()

		# Create registry
		registry = Registry[TestContext]()

		# Model that doesn't include browser_session (renamed to avoid pytest collecting it)
		class CellActionParams(ActionModel):
			value: str = Field(description='Test value')

		# Model that includes browser_session
		class ModelWithBrowser(ActionModel):
			value: str = Field(description='Test value')
			browser_session: BrowserSession = None  # type: ignore

		# Create a custom param model for select_cell_or_range
		class CellRangeParams(ActionModel):
			cell_or_range: str = Field(description='Cell or range to select')

		# Use the provided real browser session

		# Test with the real issue: select_cell_or_range
		# logger.info('\n\n=== Test: Simulating select_cell_or_range issue with correct model ===')

		# Define the function without using our registry - this will be a helper function
		async def _select_cell_or_range(browser_session, cell_or_range):
			"""Helper function for select_cell_or_range"""
			return f'Selected cell {cell_or_range}'

		# This simulates the actual issue we're seeing in the real code
		# The browser_session parameter is in both the function signature and passed as a named arg
		@registry.action('Google Sheets: Select a cell or range', param_model=CellRangeParams)
		async def select_cell_or_range(browser_session: BrowserSession, cell_or_range: str):
			# logger.info(f'select_cell_or_range called with browser_session={browser_session}, cell_or_range={cell_or_range}')

			# PROBLEMATIC LINE: browser_session is passed by name, matching the parameter name
			# This is what causes the "got multiple values" error in the real code
			return await _select_cell_or_range(browser_session=browser_session, cell_or_range=cell_or_range)

		# Fix attempt: Register a version that uses positional args instead
		@registry.action('Google Sheets: Select a cell or range (fixed)', param_model=CellRangeParams)
		async def select_cell_or_range_fixed(browser_session: BrowserSession, cell_or_range: str):
			# logger.info(f'select_cell_or_range_fixed called with browser_session={browser_session}, cell_or_range={cell_or_range}')

			# FIXED LINE: browser_session is passed positionally, avoiding the parameter name conflict
			return await _select_cell_or_range(browser_session, cell_or_range)

		# Another attempt: explicitly call using **kwargs to simulate what the registry does
		@registry.action('Google Sheets: Select with kwargs', param_model=CellRangeParams)
		async def select_with_kwargs(browser_session: BrowserSession, cell_or_range: str):
			# logger.info(f'select_with_kwargs called with browser_session={browser_session}, cell_or_range={cell_or_range}')

			# Get params and extra_args, like in Registry.execute_action
			params = {'cell_or_range': cell_or_range, 'browser_session': browser_session}
			extra_args = {'browser_session': browser_session}

			# Try to call _select_cell_or_range with both params and extra_args
			# This will fail with "got multiple values for keyword argument 'browser_session'"
			try:
				# logger.info('Attempting to call with both params and extra_args (should fail):')
				await _select_cell_or_range(**params, **extra_args)
			except TypeError as e:
				# logger.info(f'Expected error: {e}')

				# Remove browser_session from params to avoid the conflict
				params_fixed = dict(params)
				del params_fixed['browser_session']

				# logger.info(f'Fixed params: {params_fixed}')

				# This should work
				result = await _select_cell_or_range(**params_fixed, **extra_args)
				# logger.info(f'Success after fix: {result}')
				return result

		# Test the original problematic version
		# logger.info('\n--- Testing original problematic version ---')
		try:
			result1 = await registry.execute_action(
				'select_cell_or_range',
				{'cell_or_range': 'A1:F100'},
				browser_session=browser_session,  # type: ignore
			)
			# logger.info(f'Success! Result: {result1}')
		except Exception as e:
			logger.error(f'Error: {str(e)}')

		# Test the fixed version (using positional args)
		# logger.info('\n--- Testing fixed version (positional args) ---')
		try:
			result2 = await registry.execute_action(
				'select_cell_or_range_fixed',
				{'cell_or_range': 'A1:F100'},
				browser_session=browser_session,  # type: ignore
			)
			# logger.info(f'Success! Result: {result2}')
		except Exception as e:
			logger.error(f'Error: {str(e)}')

		# Test with kwargs version that simulates what Registry.execute_action does
		# logger.info('\n--- Testing kwargs simulation version ---')
		try:
			result3 = await registry.execute_action(
				'select_with_kwargs',
				{'cell_or_range': 'A1:F100'},
				browser_session=browser_session,  # type: ignore
			)
			# logger.info(f'Success! Result: {result3}')
		except Exception as e:
			logger.error(f'Error: {str(e)}')

		# Manual test of our theory: browser_session is passed twice
		# logger.info('\n--- Direct test of our theory ---')
		try:
			# Create the model instance
			params = CellRangeParams(cell_or_range='A1:F100')

			# First check if the extra_args approach works
			# logger.info('Checking if extra_args approach works:')
			extra_args = {'browser_session': browser_session}

			# If we were to modify Registry.execute_action:
			# 1. Check if the function parameter needs browser_session
			parameter_names = ['browser_session', 'cell_or_range']
			browser_keys = ['browser_session', 'browser', 'browser_context']

			# Create params dict
			param_dict = params.model_dump()
			# logger.info(f'params dict before: {param_dict}')

			# Apply our fix: remove browser_session from params dict
			for key in browser_keys:
				if key in param_dict and key in extra_args:
					# logger.info(f'Removing {key} from params dict')
					del param_dict[key]

			# logger.info(f'params dict after: {param_dict}')
			# logger.info(f'extra_args: {extra_args}')

			# This would be the fixed code:
			# return await action.function(**param_dict, **extra_args)

			# Call directly to test
			result3 = await select_cell_or_range(**param_dict, **extra_args)
			# logger.info(f'Success with our fix! Result: {result3}')
		except Exception as e:
			logger.error(f'Error with our manual test: {str(e)}')

```

---

## backend/browser-use/tests/ci/infrastructure/test_url_shortening.py

```py
"""
Simplified tests for URL shortening functionality in Agent service.

Three focused tests:
1. Input message processing with URL shortening
2. Output processing with custom actions and URL restoration
3. End-to-end pipeline test
"""

import json

import pytest

from browser_use.agent.service import Agent
from browser_use.agent.views import AgentOutput
from browser_use.llm.messages import AssistantMessage, BaseMessage, UserMessage

# Super long URL to reuse across tests - much longer than the 25 character limit
# Includes both query params (?...) and fragment params (#...)
SUPER_LONG_URL = 'https://documentation.example-company.com/api/v3/enterprise/user-management/endpoints/administration/create-new-user-account-with-permissions/advanced-settings?format=detailed-json&version=3.2.1&timestamp=1699123456789&session_id=abc123def456ghi789&authentication_token=very_long_authentication_token_string_here&include_metadata=true&expand_relationships=user_groups,permissions,roles&sort_by=created_at&order=desc&page_size=100&include_deprecated_fields=false&api_key=super_long_api_key_that_exceeds_normal_limits#section=user_management&tab=advanced&view=detailed&scroll_to=permissions_table&highlight=admin_settings&filter=active_users&expand_all=true&debug_mode=enabled'


@pytest.fixture
def agent():
	"""Create an agent instance for testing URL shortening functionality."""
	from tests.ci.conftest import create_mock_llm

	return Agent(task='Test URL shortening', llm=create_mock_llm(), url_shortening_limit=25)


class TestUrlShorteningInputProcessing:
	"""Test URL shortening for input messages."""

	def test_process_input_messages_with_url_shortening(self, agent: Agent):
		"""Test that long URLs in input messages are shortened and mappings stored."""
		original_content = f'Please visit {SUPER_LONG_URL} and extract information'

		messages: list[BaseMessage] = [UserMessage(content=original_content)]

		# Process messages (modifies messages in-place and returns URL mappings)
		url_mappings = agent._process_messsages_and_replace_long_urls_shorter_ones(messages)

		# Verify URL was shortened in the message (modified in-place)
		processed_content = messages[0].content or ''
		assert processed_content != original_content
		assert 'https://documentation.example-company.com' in processed_content
		assert len(processed_content) < len(original_content)

		# Verify URL mapping was returned
		assert len(url_mappings) == 1
		shortened_url = next(iter(url_mappings.keys()))
		assert url_mappings[shortened_url] == SUPER_LONG_URL

	def test_process_user_and_assistant_messages_with_url_shortening(self, agent: Agent):
		"""Test URL shortening in both UserMessage and AssistantMessage."""
		user_content = f'I need to access {SUPER_LONG_URL} for the API documentation'
		assistant_content = f'I will help you navigate to {SUPER_LONG_URL} to retrieve the documentation'

		messages: list[BaseMessage] = [UserMessage(content=user_content), AssistantMessage(content=assistant_content)]

		# Process messages (modifies messages in-place and returns URL mappings)
		url_mappings = agent._process_messsages_and_replace_long_urls_shorter_ones(messages)

		# Verify URL was shortened in both messages
		user_processed_content = messages[0].content or ''
		assistant_processed_content = messages[1].content or ''

		assert user_processed_content != user_content
		assert assistant_processed_content != assistant_content
		assert 'https://documentation.example-company.com' in user_processed_content
		assert 'https://documentation.example-company.com' in assistant_processed_content
		assert len(user_processed_content) < len(user_content)
		assert len(assistant_processed_content) < len(assistant_content)

		# Verify URL mapping was returned (should be same shortened URL for both occurrences)
		assert len(url_mappings) == 1
		shortened_url = next(iter(url_mappings.keys()))
		assert url_mappings[shortened_url] == SUPER_LONG_URL


class TestUrlShorteningOutputProcessing:
	"""Test URL restoration for output processing with custom actions."""

	def test_process_output_with_custom_actions_and_url_restoration(self, agent: Agent):
		"""Test that shortened URLs in AgentOutput with custom actions are restored."""
		# Set up URL mapping (simulating previous shortening)
		shortened_url: str = agent._replace_urls_in_text(SUPER_LONG_URL)[0]
		url_mappings = {shortened_url: SUPER_LONG_URL}

		# Create AgentOutput with shortened URLs using JSON parsing
		output_json = {
			'thinking': f'I need to navigate to {shortened_url} for documentation',
			'evaluation_previous_goal': 'Successfully processed the request',
			'memory': f'Found useful info at {shortened_url}',
			'next_goal': 'Complete the documentation review',
			'action': [{'navigate': {'url': shortened_url, 'new_tab': False}}],
		}

		# Create properly typed AgentOutput with custom actions
		tools = agent.tools
		ActionModel = tools.registry.create_action_model()
		AgentOutputWithActions = AgentOutput.type_with_custom_actions(ActionModel)
		agent_output = AgentOutputWithActions.model_validate_json(json.dumps(output_json))

		# Process the output to restore URLs (modifies agent_output in-place)
		agent._recursive_process_all_strings_inside_pydantic_model(agent_output, url_mappings)

		# Verify URLs were restored in all locations
		assert SUPER_LONG_URL in (agent_output.thinking or '')
		assert SUPER_LONG_URL in (agent_output.memory or '')
		action_data = agent_output.action[0].model_dump()
		assert action_data['navigate']['url'] == SUPER_LONG_URL


class TestUrlShorteningEndToEnd:
	"""Test complete URL shortening pipeline end-to-end."""

	def test_complete_url_shortening_pipeline(self, agent: Agent):
		"""Test the complete pipeline: input shortening -> processing -> output restoration."""

		# Step 1: Input processing with URL shortening
		original_content = f'Navigate to {SUPER_LONG_URL} and extract the API documentation'

		messages: list[BaseMessage] = [UserMessage(content=original_content)]

		url_mappings = agent._process_messsages_and_replace_long_urls_shorter_ones(messages)

		# Verify URL was shortened in input
		assert len(url_mappings) == 1
		shortened_url = next(iter(url_mappings.keys()))
		assert url_mappings[shortened_url] == SUPER_LONG_URL
		assert shortened_url in (messages[0].content or '')

		# Step 2: Simulate agent output with shortened URL
		output_json = {
			'thinking': f'I will navigate to {shortened_url} to get the documentation',
			'evaluation_previous_goal': 'Starting documentation extraction',
			'memory': f'Target URL: {shortened_url}',
			'next_goal': 'Extract API documentation',
			'action': [{'navigate': {'url': shortened_url, 'new_tab': True}}],
		}

		# Create AgentOutput with custom actions
		tools = agent.tools
		ActionModel = tools.registry.create_action_model()
		AgentOutputWithActions = AgentOutput.type_with_custom_actions(ActionModel)
		agent_output = AgentOutputWithActions.model_validate_json(json.dumps(output_json))

		# Step 3: Output processing with URL restoration (modifies agent_output in-place)
		agent._recursive_process_all_strings_inside_pydantic_model(agent_output, url_mappings)

		# Verify complete pipeline worked correctly
		assert SUPER_LONG_URL in (agent_output.thinking or '')
		assert SUPER_LONG_URL in (agent_output.memory or '')
		action_data = agent_output.action[0].model_dump()
		assert action_data['navigate']['url'] == SUPER_LONG_URL
		assert action_data['navigate']['new_tab'] is True

		# Verify original shortened content is no longer present
		assert shortened_url not in (agent_output.thinking or '')
		assert shortened_url not in (agent_output.memory or '')

```

---

## backend/browser-use/tests/ci/interactions/test_dropdown_aria_menus.py

```py
import pytest
from pytest_httpserver import HTTPServer

from browser_use.agent.views import ActionResult
from browser_use.browser import BrowserSession
from browser_use.browser.profile import BrowserProfile
from browser_use.tools.service import Tools


@pytest.fixture(scope='session')
def http_server():
	"""Create and provide a test HTTP server that serves static content."""
	server = HTTPServer()
	server.start()

	# Add route for ARIA menu test page
	server.expect_request('/aria-menu').respond_with_data(
		"""
		<!DOCTYPE html>
		<html>
		<head>
			<title>ARIA Menu Test</title>
			<style>
				.menu {
					list-style: none;
					padding: 0;
					margin: 0;
					border: 1px solid #ccc;
					background: white;
					width: 200px;
				}
				.menu-item {
					padding: 10px 20px;
					border-bottom: 1px solid #eee;
				}
				.menu-item:hover {
					background: #f0f0f0;
				}
				.menu-item-anchor {
					text-decoration: none;
					color: #333;
					display: block;
				}
				#result {
					margin-top: 20px;
					padding: 10px;
					border: 1px solid #ddd;
					min-height: 20px;
				}
			</style>
		</head>
		<body>
			<h1>ARIA Menu Test</h1>
			<p>This menu uses ARIA roles instead of native select elements</p>
			
			<!-- Exactly like the HTML provided in the issue -->
			<ul class="menu menu-format-standard menu-regular" role="menu" id="pyNavigation1752753375773" style="display: block;">
				<li class="menu-item menu-item-enabled" role="presentation">
					<a href="#" onclick="pd(event);" class="menu-item-anchor" tabindex="0" role="menuitem">
						<span class="menu-item-title-wrap"><span class="menu-item-title">Filter</span></span>
					</a>
				</li>
				<li class="menu-item menu-item-enabled" role="presentation" id="menu-item-$PpyNavigation1752753375773$ppyElements$l2">
					<a href="#" onclick="pd(event);" class="menu-item-anchor menu-item-expand" tabindex="0" role="menuitem" aria-haspopup="true">
						<span class="menu-item-title-wrap"><span class="menu-item-title">Sort</span></span>
					</a>
					<div class="menu-panel-wrapper">
						<ul class="menu menu-format-standard menu-regular" role="menu" id="$PpyNavigation1752753375773$ppyElements$l2">
							<li class="menu-item menu-item-enabled" role="presentation">
								<a href="#" onclick="pd(event);" class="menu-item-anchor" tabindex="0" role="menuitem">
									<span class="menu-item-title-wrap"><span class="menu-item-title">Lowest to highest</span></span>
								</a>
							</li>
							<li class="menu-item menu-item-enabled" role="presentation">
								<a href="#" onclick="pd(event);" class="menu-item-anchor" tabindex="0" role="menuitem">
									<span class="menu-item-title-wrap"><span class="menu-item-title">Highest to lowest</span></span>
								</a>
							</li>
						</ul>
					</div>
				</li>
				<li class="menu-item menu-item-enabled" role="presentation">
					<a href="#" onclick="pd(event);" class="menu-item-anchor" tabindex="0" role="menuitem">
						<span class="menu-item-title-wrap"><span class="menu-item-title">Appearance</span></span>
					</a>
				</li>
				<li class="menu-item menu-item-enabled" role="presentation">
					<a href="#" onclick="pd(event);" class="menu-item-anchor" tabindex="0" role="menuitem">
						<span class="menu-item-title-wrap"><span class="menu-item-title">Summarize</span></span>
					</a>
				</li>
				<li class="menu-item menu-item-enabled" role="presentation">
					<a href="#" onclick="pd(event);" class="menu-item-anchor" tabindex="0" role="menuitem">
						<span class="menu-item-title-wrap"><span class="menu-item-title">Delete</span></span>
					</a>
				</li>
			</ul>
			
			<div id="result">Click an option to see the result</div>
			
			<script>
				// Mock the pd function that prevents default
				function pd(event) {
					event.preventDefault();
					const text = event.target.closest('[role="menuitem"]').textContent.trim();
					document.getElementById('result').textContent = 'Clicked: ' + text;
				}
			</script>
		</body>
		</html>
		""",
		content_type='text/html',
	)

	yield server
	server.stop()


@pytest.fixture(scope='session')
def base_url(http_server):
	"""Return the base URL for the test HTTP server."""
	return f'http://{http_server.host}:{http_server.port}'


@pytest.fixture(scope='module')
async def browser_session():
	"""Create and provide a Browser instance with security disabled."""
	browser_session = BrowserSession(
		browser_profile=BrowserProfile(
			headless=True,
			user_data_dir=None,
			keep_alive=True,
			chromium_sandbox=False,  # Disable sandbox for CI environment
		)
	)
	await browser_session.start()
	yield browser_session
	await browser_session.kill()


@pytest.fixture(scope='function')
def tools():
	"""Create and provide a Tools instance."""
	return Tools()


class TestARIAMenuDropdown:
	"""Test ARIA menu support for get_dropdown_options and select_dropdown_option."""

	@pytest.mark.skip(reason='TODO: fix')
	async def test_get_dropdown_options_with_aria_menu(self, tools, browser_session: BrowserSession, base_url):
		"""Test that get_dropdown_options can retrieve options from ARIA menus."""
		# Navigate to the ARIA menu test page
		await tools.navigate(url=f'{base_url}/aria-menu', new_tab=False, browser_session=browser_session)

		# Wait for the page to load
		from browser_use.browser.events import NavigationCompleteEvent

		await browser_session.event_bus.expect(NavigationCompleteEvent, timeout=10.0)

		# Initialize the DOM state to populate the selector map
		await browser_session.get_browser_state_summary()

		# Find the ARIA menu element by ID
		menu_index = await browser_session.get_index_by_id('pyNavigation1752753375773')

		assert menu_index is not None, 'Could not find ARIA menu element'

		# Execute the action with the menu index
		result = await tools.dropdown_options(index=menu_index, browser_session=browser_session)

		# Verify the result structure
		assert isinstance(result, ActionResult)
		assert result.extracted_content is not None

		# Expected ARIA menu options
		expected_options = ['Filter', 'Sort', 'Appearance', 'Summarize', 'Delete']

		# Verify all options are returned
		for option in expected_options:
			assert option in result.extracted_content, f"Option '{option}' not found in result content"

		# Verify the instruction for using the text in select_dropdown is included
		assert 'Use the exact text string in select_dropdown' in result.extracted_content

	@pytest.mark.skip(reason='TODO: fix')
	async def test_select_dropdown_option_with_aria_menu(self, tools, browser_session: BrowserSession, base_url):
		"""Test that select_dropdown_option can select an option from ARIA menus."""
		# Navigate to the ARIA menu test page
		await tools.navigate(url=f'{base_url}/aria-menu', new_tab=False, browser_session=browser_session)

		# Wait for the page to load
		from browser_use.browser.events import NavigationCompleteEvent

		await browser_session.event_bus.expect(NavigationCompleteEvent, timeout=10.0)

		# Initialize the DOM state to populate the selector map
		await browser_session.get_browser_state_summary()

		# Find the ARIA menu element by ID
		menu_index = await browser_session.get_index_by_id('pyNavigation1752753375773')

		assert menu_index is not None, 'Could not find ARIA menu element'

		# Execute the action with the menu index to select "Filter"
		result = await tools.select_dropdown(index=menu_index, text='Filter', browser_session=browser_session)

		# Verify the result structure
		assert isinstance(result, ActionResult)

		# Core logic validation: Verify selection was successful
		assert result.extracted_content is not None
		assert 'selected option' in result.extracted_content.lower() or 'clicked' in result.extracted_content.lower()
		assert 'Filter' in result.extracted_content

		# Verify the click actually had an effect on the page using CDP
		cdp_session = await browser_session.get_or_create_cdp_session()
		result = await cdp_session.cdp_client.send.Runtime.evaluate(
			params={'expression': "document.getElementById('result').textContent", 'returnByValue': True},
			session_id=cdp_session.session_id,
		)
		result_text = result.get('result', {}).get('value', '')
		assert 'Filter' in result_text, f"Expected 'Filter' in result text, got '{result_text}'"

	@pytest.mark.skip(reason='TODO: fix')
	async def test_get_dropdown_options_with_nested_aria_menu(self, tools, browser_session: BrowserSession, base_url):
		"""Test that get_dropdown_options can handle nested ARIA menus (like Sort submenu)."""
		# Navigate to the ARIA menu test page
		await tools.navigate(url=f'{base_url}/aria-menu', new_tab=False, browser_session=browser_session)

		# Wait for the page to load
		from browser_use.browser.events import NavigationCompleteEvent

		await browser_session.event_bus.expect(NavigationCompleteEvent, timeout=10.0)

		# Initialize the DOM state to populate the selector map
		await browser_session.get_browser_state_summary()

		# Get the selector map
		selector_map = await browser_session.get_selector_map()

		# Find the nested ARIA menu element in the selector map
		nested_menu_index = None
		for idx, element in selector_map.items():
			# Look for the nested UL with id containing "$PpyNavigation"
			if (
				element.tag_name.lower() == 'ul'
				and '$PpyNavigation' in str(element.attributes.get('id', ''))
				and element.attributes.get('role') == 'menu'
			):
				nested_menu_index = idx
				break

		# The nested menu might not be in the selector map initially if it's hidden
		# In that case, we should test the main menu
		if nested_menu_index is None:
			# Find the main menu instead
			for idx, element in selector_map.items():
				if element.tag_name.lower() == 'ul' and element.attributes.get('id') == 'pyNavigation1752753375773':
					nested_menu_index = idx
					break

		assert nested_menu_index is not None, (
			f'Could not find any ARIA menu element in selector map. Available elements: {[f"{idx}: {element.tag_name}" for idx, element in selector_map.items()]}'
		)

		# Execute the action with the menu index
		result = await tools.dropdown_options(index=nested_menu_index, browser_session=browser_session)

		# Verify the result structure
		assert isinstance(result, ActionResult)
		assert result.extracted_content is not None

		# The action should return some menu options
		assert 'Use the exact text string in select_dropdown' in result.extracted_content

```

---

## backend/browser-use/tests/ci/interactions/test_dropdown_native.py

```py
"""Test GetDropdownOptionsEvent and SelectDropdownOptionEvent functionality.

This file consolidates all tests related to dropdown functionality including:
- Native <select> dropdowns
- ARIA role="menu" dropdowns
- Custom dropdown implementations
"""

import pytest
from pytest_httpserver import HTTPServer

from browser_use.agent.views import ActionResult
from browser_use.browser import BrowserSession
from browser_use.browser.events import GetDropdownOptionsEvent, NavigationCompleteEvent, SelectDropdownOptionEvent
from browser_use.browser.profile import BrowserProfile
from browser_use.tools.service import Tools


@pytest.fixture(scope='session')
def http_server():
	"""Create and provide a test HTTP server that serves static content."""
	server = HTTPServer()
	server.start()

	# Add route for native dropdown test page
	server.expect_request('/native-dropdown').respond_with_data(
		"""
		<!DOCTYPE html>
		<html>
		<head>
			<title>Native Dropdown Test</title>
		</head>
		<body>
			<h1>Native Dropdown Test</h1>
			<select id="test-dropdown" name="test-dropdown">
				<option value="">Please select</option>
				<option value="option1">First Option</option>
				<option value="option2">Second Option</option>
				<option value="option3">Third Option</option>
			</select>
			<div id="result">No selection made</div>
			<script>
				document.getElementById('test-dropdown').addEventListener('change', function(e) {
					document.getElementById('result').textContent = 'Selected: ' + e.target.options[e.target.selectedIndex].text;
				});
			</script>
		</body>
		</html>
		""",
		content_type='text/html',
	)

	# Add route for ARIA menu test page
	server.expect_request('/aria-menu').respond_with_data(
		"""
		<!DOCTYPE html>
		<html>
		<head>
			<title>ARIA Menu Test</title>
			<style>
				.menu {
					list-style: none;
					padding: 0;
					margin: 0;
					border: 1px solid #ccc;
					background: white;
					width: 200px;
				}
				.menu-item {
					padding: 10px 20px;
					border-bottom: 1px solid #eee;
				}
				.menu-item:hover {
					background: #f0f0f0;
				}
				.menu-item-anchor {
					text-decoration: none;
					color: #333;
					display: block;
				}
				#result {
					margin-top: 20px;
					padding: 10px;
					border: 1px solid #ddd;
					min-height: 20px;
				}
			</style>
		</head>
		<body>
			<h1>ARIA Menu Test</h1>
			<p>This menu uses ARIA roles instead of native select elements</p>
			
			<ul class="menu menu-format-standard menu-regular" role="menu" id="pyNavigation1752753375773" style="display: block;">
				<li class="menu-item menu-item-enabled" role="presentation">
					<a href="#" onclick="pd(event);" class="menu-item-anchor" tabindex="0" role="menuitem">
						<span class="menu-item-title-wrap"><span class="menu-item-title">Filter</span></span>
					</a>
				</li>
				<li class="menu-item menu-item-enabled" role="presentation" id="menu-item-$PpyNavigation1752753375773$ppyElements$l2">
					<a href="#" onclick="pd(event);" class="menu-item-anchor menu-item-expand" tabindex="0" role="menuitem" aria-haspopup="true">
						<span class="menu-item-title-wrap"><span class="menu-item-title">Sort</span></span>
					</a>
					<div class="menu-panel-wrapper">
						<ul class="menu menu-format-standard menu-regular" role="menu" id="$PpyNavigation1752753375773$ppyElements$l2">
							<li class="menu-item menu-item-enabled" role="presentation">
								<a href="#" onclick="pd(event);" class="menu-item-anchor" tabindex="0" role="menuitem">
									<span class="menu-item-title-wrap"><span class="menu-item-title">Lowest to highest</span></span>
								</a>
							</li>
							<li class="menu-item menu-item-enabled" role="presentation">
								<a href="#" onclick="pd(event);" class="menu-item-anchor" tabindex="0" role="menuitem">
									<span class="menu-item-title-wrap"><span class="menu-item-title">Highest to lowest</span></span>
								</a>
							</li>
						</ul>
					</div>
				</li>
				<li class="menu-item menu-item-enabled" role="presentation">
					<a href="#" onclick="pd(event);" class="menu-item-anchor" tabindex="0" role="menuitem">
						<span class="menu-item-title-wrap"><span class="menu-item-title">Appearance</span></span>
					</a>
				</li>
				<li class="menu-item menu-item-enabled" role="presentation">
					<a href="#" onclick="pd(event);" class="menu-item-anchor" tabindex="0" role="menuitem">
						<span class="menu-item-title-wrap"><span class="menu-item-title">Summarize</span></span>
					</a>
				</li>
				<li class="menu-item menu-item-enabled" role="presentation">
					<a href="#" onclick="pd(event);" class="menu-item-anchor" tabindex="0" role="menuitem">
						<span class="menu-item-title-wrap"><span class="menu-item-title">Delete</span></span>
					</a>
				</li>
			</ul>
			
			<div id="result">Click an option to see the result</div>
			
			<script>
				// Mock the pd function that prevents default
				function pd(event) {
					event.preventDefault();
					const text = event.target.closest('[role="menuitem"]').textContent.trim();
					document.getElementById('result').textContent = 'Clicked: ' + text;
				}
			</script>
		</body>
		</html>
		""",
		content_type='text/html',
	)

	# Add route for custom dropdown test page
	server.expect_request('/custom-dropdown').respond_with_data(
		"""
		<!DOCTYPE html>
		<html>
		<head>
			<title>Custom Dropdown Test</title>
			<style>
				.dropdown {
					position: relative;
					display: inline-block;
					width: 200px;
				}
				.dropdown-button {
					padding: 10px;
					border: 1px solid #ccc;
					background: white;
					cursor: pointer;
					width: 100%;
				}
				.dropdown-menu {
					position: absolute;
					top: 100%;
					left: 0;
					right: 0;
					border: 1px solid #ccc;
					background: white;
					display: block;
					z-index: 1000;
				}
				.dropdown-menu.hidden {
					display: none;
				}
				.dropdown .item {
					padding: 10px;
					cursor: pointer;
				}
				.dropdown .item:hover {
					background: #f0f0f0;
				}
				.dropdown .item.selected {
					background: #e0e0e0;
				}
				#result {
					margin-top: 20px;
					padding: 10px;
					border: 1px solid #ddd;
				}
			</style>
		</head>
		<body>
			<h1>Custom Dropdown Test</h1>
			<p>This is a custom dropdown implementation (like Semantic UI)</p>
			
			<div class="dropdown ui" id="custom-dropdown">
				<div class="dropdown-button" onclick="toggleDropdown()">
					<span id="selected-text">Choose an option</span>
				</div>
				<div class="dropdown-menu" id="dropdown-menu">
					<div class="item" data-value="red" onclick="selectOption('Red', 'red')">Red</div>
					<div class="item" data-value="green" onclick="selectOption('Green', 'green')">Green</div>
					<div class="item" data-value="blue" onclick="selectOption('Blue', 'blue')">Blue</div>
					<div class="item" data-value="yellow" onclick="selectOption('Yellow', 'yellow')">Yellow</div>
				</div>
			</div>
			
			<div id="result">No selection made</div>
			
			<script>
				function toggleDropdown() {
					const menu = document.getElementById('dropdown-menu');
					menu.classList.toggle('hidden');
				}
				
				function selectOption(text, value) {
					document.getElementById('selected-text').textContent = text;
					document.getElementById('result').textContent = 'Selected: ' + text + ' (value: ' + value + ')';
					// Mark as selected
					document.querySelectorAll('.item').forEach(item => item.classList.remove('selected'));
					event.target.classList.add('selected');
					// Close dropdown
					document.getElementById('dropdown-menu').classList.add('hidden');
				}
			</script>
		</body>
		</html>
		""",
		content_type='text/html',
	)

	yield server
	server.stop()


@pytest.fixture(scope='session')
def base_url(http_server):
	"""Return the base URL for the test HTTP server."""
	return f'http://{http_server.host}:{http_server.port}'


@pytest.fixture(scope='module')
async def browser_session():
	"""Create and provide a Browser instance with security disabled."""
	browser_session = BrowserSession(
		browser_profile=BrowserProfile(
			headless=True,
			user_data_dir=None,
			keep_alive=True,
			chromium_sandbox=False,  # Disable sandbox for CI environment
		)
	)
	await browser_session.start()
	yield browser_session
	await browser_session.kill()


@pytest.fixture(scope='function')
def tools():
	"""Create and provide a Tools instance."""
	return Tools()


class TestGetDropdownOptionsEvent:
	"""Test GetDropdownOptionsEvent functionality for various dropdown types."""

	@pytest.mark.skip(reason='Dropdown text assertion issue - test expects specific text format')
	async def test_native_select_dropdown(self, tools, browser_session: BrowserSession, base_url):
		"""Test get_dropdown_options with native HTML select element."""
		# Navigate to the native dropdown test page
		await tools.navigate(url=f'{base_url}/native-dropdown', new_tab=False, browser_session=browser_session)

		# Initialize the DOM state to populate the selector map
		await browser_session.get_browser_state_summary()

		# Find the select element by ID
		dropdown_index = await browser_session.get_index_by_id('test-dropdown')

		assert dropdown_index is not None, 'Could not find select element'

		# Test via tools action
		result = await tools.dropdown_options(index=dropdown_index, browser_session=browser_session)

		# Verify the result
		assert isinstance(result, ActionResult)
		assert result.extracted_content is not None

		# Verify all expected options are present
		expected_options = ['Please select', 'First Option', 'Second Option', 'Third Option']
		for option in expected_options:
			assert option in result.extracted_content, f"Option '{option}' not found in result content"

		# Verify instruction is included
		assert 'Use the exact text string' in result.extracted_content and 'select_dropdown' in result.extracted_content

		# Also test direct event dispatch
		node = await browser_session.get_element_by_index(dropdown_index)
		assert node is not None
		event = browser_session.event_bus.dispatch(GetDropdownOptionsEvent(node=node))
		dropdown_data = await event.event_result(timeout=3.0)

		assert dropdown_data is not None
		assert 'options' in dropdown_data
		assert 'type' in dropdown_data
		assert dropdown_data['type'] == 'select'

	@pytest.mark.skip(reason='ARIA menu detection issue - element not found in selector map')
	async def test_aria_menu_dropdown(self, tools, browser_session: BrowserSession, base_url):
		"""Test get_dropdown_options with ARIA role='menu' element."""
		# Navigate to the ARIA menu test page
		await tools.navigate(url=f'{base_url}/aria-menu', new_tab=False, browser_session=browser_session)

		# Initialize the DOM state
		await browser_session.get_browser_state_summary()

		# Find the ARIA menu by ID
		menu_index = await browser_session.get_index_by_id('pyNavigation1752753375773')

		assert menu_index is not None, 'Could not find ARIA menu element'

		# Test via tools action
		result = await tools.dropdown_options(index=menu_index, browser_session=browser_session)

		# Verify the result
		assert isinstance(result, ActionResult)
		assert result.extracted_content is not None

		# Verify expected ARIA menu options are present
		expected_options = ['Filter', 'Sort', 'Appearance', 'Summarize', 'Delete']
		for option in expected_options:
			assert option in result.extracted_content, f"Option '{option}' not found in result content"

		# Also test direct event dispatch
		node = await browser_session.get_element_by_index(menu_index)
		assert node is not None
		event = browser_session.event_bus.dispatch(GetDropdownOptionsEvent(node=node))
		dropdown_data = await event.event_result(timeout=3.0)

		assert dropdown_data is not None
		assert 'options' in dropdown_data
		assert 'type' in dropdown_data
		assert dropdown_data['type'] == 'aria'

	@pytest.mark.skip(reason='Custom dropdown detection issue - element not found in selector map')
	async def test_custom_dropdown(self, tools, browser_session: BrowserSession, base_url):
		"""Test get_dropdown_options with custom dropdown implementation."""
		# Navigate to the custom dropdown test page
		await tools.navigate(url=f'{base_url}/custom-dropdown', new_tab=False, browser_session=browser_session)

		# Initialize the DOM state
		await browser_session.get_browser_state_summary()

		# Find the custom dropdown by ID
		dropdown_index = await browser_session.get_index_by_id('custom-dropdown')

		assert dropdown_index is not None, 'Could not find custom dropdown element'

		# Test via tools action
		result = await tools.dropdown_options(index=dropdown_index, browser_session=browser_session)

		# Verify the result
		assert isinstance(result, ActionResult)
		assert result.extracted_content is not None

		# Verify expected custom dropdown options are present
		expected_options = ['Red', 'Green', 'Blue', 'Yellow']
		for option in expected_options:
			assert option in result.extracted_content, f"Option '{option}' not found in result content"

		# Also test direct event dispatch
		node = await browser_session.get_element_by_index(dropdown_index)
		assert node is not None
		event = browser_session.event_bus.dispatch(GetDropdownOptionsEvent(node=node))
		dropdown_data = await event.event_result(timeout=3.0)

		assert dropdown_data is not None
		assert 'options' in dropdown_data
		assert 'type' in dropdown_data
		assert dropdown_data['type'] == 'custom'


class TestSelectDropdownOptionEvent:
	"""Test SelectDropdownOptionEvent functionality for various dropdown types."""

	@pytest.mark.skip(reason='Timeout issue - test takes too long to complete')
	async def test_select_native_dropdown_option(self, tools, browser_session: BrowserSession, base_url):
		"""Test select_dropdown_option with native HTML select element."""
		# Navigate to the native dropdown test page
		await tools.navigate(url=f'{base_url}/native-dropdown', new_tab=False, browser_session=browser_session)
		await browser_session.event_bus.expect(NavigationCompleteEvent, timeout=10.0)

		# Initialize the DOM state
		await browser_session.get_browser_state_summary()

		# Find the select element by ID
		dropdown_index = await browser_session.get_index_by_id('test-dropdown')

		assert dropdown_index is not None

		# Test via tools action
		result = await tools.select_dropdown(index=dropdown_index, text='Second Option', browser_session=browser_session)

		# Verify the result
		assert isinstance(result, ActionResult)
		assert result.extracted_content is not None
		assert 'Second Option' in result.extracted_content

		# Verify the selection actually worked using CDP
		cdp_session = await browser_session.get_or_create_cdp_session()
		result = await cdp_session.cdp_client.send.Runtime.evaluate(
			params={'expression': "document.getElementById('test-dropdown').selectedIndex", 'returnByValue': True},
			session_id=cdp_session.session_id,
		)
		selected_index = result.get('result', {}).get('value', -1)
		assert selected_index == 2, f'Expected selected index 2, got {selected_index}'

	@pytest.mark.skip(reason='Timeout issue - test takes too long to complete')
	async def test_select_aria_menu_option(self, tools, browser_session: BrowserSession, base_url):
		"""Test select_dropdown_option with ARIA menu."""
		# Navigate to the ARIA menu test page
		await tools.navigate(url=f'{base_url}/aria-menu', new_tab=False, browser_session=browser_session)
		await browser_session.event_bus.expect(NavigationCompleteEvent, timeout=10.0)

		# Initialize the DOM state
		await browser_session.get_browser_state_summary()

		# Find the ARIA menu by ID
		menu_index = await browser_session.get_index_by_id('pyNavigation1752753375773')

		assert menu_index is not None

		# Test via tools action
		result = await tools.select_dropdown(index=menu_index, text='Filter', browser_session=browser_session)

		# Verify the result
		assert isinstance(result, ActionResult)
		assert result.extracted_content is not None
		assert 'Filter' in result.extracted_content

		# Verify the click had an effect using CDP
		cdp_session = await browser_session.get_or_create_cdp_session()
		result = await cdp_session.cdp_client.send.Runtime.evaluate(
			params={'expression': "document.getElementById('result').textContent", 'returnByValue': True},
			session_id=cdp_session.session_id,
		)
		result_text = result.get('result', {}).get('value', '')
		assert 'Filter' in result_text, f"Expected 'Filter' in result text, got '{result_text}'"

	@pytest.mark.skip(reason='Timeout issue - test takes too long to complete')
	async def test_select_custom_dropdown_option(self, tools, browser_session: BrowserSession, base_url):
		"""Test select_dropdown_option with custom dropdown."""
		# Navigate to the custom dropdown test page
		await tools.navigate(url=f'{base_url}/custom-dropdown', new_tab=False, browser_session=browser_session)
		await browser_session.event_bus.expect(NavigationCompleteEvent, timeout=10.0)

		# Initialize the DOM state
		await browser_session.get_browser_state_summary()

		# Find the custom dropdown by ID
		dropdown_index = await browser_session.get_index_by_id('custom-dropdown')

		assert dropdown_index is not None

		# Test via tools action
		result = await tools.select_dropdown(index=dropdown_index, text='Blue', browser_session=browser_session)

		# Verify the result
		assert isinstance(result, ActionResult)
		assert result.extracted_content is not None
		assert 'Blue' in result.extracted_content

		# Verify the selection worked using CDP
		cdp_session = await browser_session.get_or_create_cdp_session()
		result = await cdp_session.cdp_client.send.Runtime.evaluate(
			params={'expression': "document.getElementById('result').textContent", 'returnByValue': True},
			session_id=cdp_session.session_id,
		)
		result_text = result.get('result', {}).get('value', '')
		assert 'Blue' in result_text, f"Expected 'Blue' in result text, got '{result_text}'"

	@pytest.mark.skip(reason='Timeout issue - test takes too long to complete')
	async def test_select_invalid_option_error(self, tools, browser_session: BrowserSession, base_url):
		"""Test select_dropdown_option with non-existent option text."""
		# Navigate to the native dropdown test page
		await tools.navigate(url=f'{base_url}/native-dropdown', new_tab=False, browser_session=browser_session)
		await browser_session.event_bus.expect(NavigationCompleteEvent, timeout=10.0)

		# Initialize the DOM state
		await browser_session.get_browser_state_summary()

		# Find the select element by ID
		dropdown_index = await browser_session.get_index_by_id('test-dropdown')

		assert dropdown_index is not None

		# Try to select non-existent option via direct event
		node = await browser_session.get_element_by_index(dropdown_index)
		assert node is not None
		event = browser_session.event_bus.dispatch(SelectDropdownOptionEvent(node=node, text='Non-existent Option'))

		try:
			selection_data = await event.event_result(timeout=3.0)
			# Should have an error in the result
			assert selection_data is not None
			assert 'error' in selection_data or 'not found' in str(selection_data).lower()
		except Exception as e:
			# Or raise an exception
			assert 'not found' in str(e).lower() or 'no option' in str(e).lower()

```

---

## backend/browser-use/tests/ci/interactions/test_radio_buttons.html

```html
<!DOCTYPE html>
<html>
<head>
    <title>Radio Button Test</title>
</head>
<body>
    <h1>Radio Button Test Page</h1>
    
    <form>
        <fieldset>
            <legend>Select your favorite color:</legend>
            
            <label>
                <input type="radio" name="color" value="red" id="radio-red">
                Red
            </label>
            <br>
            
            <label>
                <input type="radio" name="color" value="blue" id="radio-blue">
                Blue
            </label>
            <br>
            
            <label>
                <input type="radio" name="color" value="green" id="radio-green">
                Green
            </label>
            <br>
        </fieldset>
        
        <fieldset>
            <legend>Select your favorite animal:</legend>
            
            <label>
                <input type="radio" name="animal" value="cat" id="radio-cat">
                Cat
            </label>
            <br>
            
            <label>
                <input type="radio" name="animal" value="dog" id="radio-dog">
                Dog
            </label>
            <br>
            
            <label>
                <input type="radio" name="animal" value="bird" id="radio-bird">
                Bird
            </label>
            <br>
        </fieldset>
        
        <div id="result-message" style="margin-top: 20px; padding: 10px; background-color: #f0f0f0; display: none;">
            <p id="secret-text"></p>
        </div>
    </form>
    
    <script>
        function checkSelection() {
            const colorRadios = document.querySelectorAll('input[name="color"]');
            const animalRadios = document.querySelectorAll('input[name="animal"]');
            
            let selectedColor = null;
            let selectedAnimal = null;
            
            // Get selected color
            for (const radio of colorRadios) {
                if (radio.checked) {
                    selectedColor = radio.value;
                    break;
                }
            }
            
            // Get selected animal
            for (const radio of animalRadios) {
                if (radio.checked) {
                    selectedAnimal = radio.value;
                    break;
                }
            }
            
            const resultDiv = document.getElementById('result-message');
            const secretText = document.getElementById('secret-text');
            
            // Show secret if both Blue and Dog are selected
            if (selectedColor === 'blue' && selectedAnimal === 'dog') {
                secretText.textContent = 'SECRET_SUCCESS_12345: Blue dog combination unlocked!';
                resultDiv.style.display = 'block';
                resultDiv.style.backgroundColor = '#d4edda';
            } else if (selectedColor && selectedAnimal) {
                secretText.textContent = `Selected: ${selectedColor} ${selectedAnimal}`;
                resultDiv.style.display = 'block';
                resultDiv.style.backgroundColor = '#f8d7da';
            } else {
                resultDiv.style.display = 'none';
            }
        }
        
        // Add event listeners to all radio buttons
        document.querySelectorAll('input[type="radio"]').forEach(radio => {
            radio.addEventListener('change', checkSelection);
        });
    </script>
</body>
</html>

```

---

## backend/browser-use/tests/ci/interactions/test_radio_buttons.py

```py
# @file purpose: Test radio button interactions and serialization in browser-use
"""
Test file for verifying radio button clicking functionality and DOM serialization.

This test creates a simple HTML page with radio buttons, sends an agent to click them,
and logs the final agent message to show how radio buttons are represented in the serializer.

The serialization shows radio buttons as:
[index]<input type=radio name=groupname value=optionvalue checked=true/false />

Usage:
    uv run pytest tests/ci/test_radio_buttons.py -v -s

Note: This test requires a real LLM API key and is skipped in CI environments.
"""

import os
from pathlib import Path

import pytest
from pytest_httpserver import HTTPServer

from browser_use.agent.service import Agent
from browser_use.browser import BrowserSession
from browser_use.browser.profile import BrowserProfile


@pytest.fixture(scope='session')
def http_server():
	"""Create and provide a test HTTP server that serves static content."""
	server = HTTPServer()
	server.start()

	# Read the HTML file content
	html_file = Path(__file__).parent / 'test_radio_buttons.html'
	with open(html_file) as f:
		html_content = f.read()

	# Add route for radio buttons test page
	server.expect_request('/radio-test').respond_with_data(
		html_content,
		content_type='text/html',
	)

	yield server
	server.stop()


@pytest.fixture(scope='session')
def base_url(http_server):
	"""Return the base URL for the test HTTP server."""
	return f'http://{http_server.host}:{http_server.port}'


@pytest.fixture(scope='module')
async def browser_session():
	"""Create and provide a Browser instance with security disabled."""
	browser_session = BrowserSession(
		browser_profile=BrowserProfile(
			headless=True,
			user_data_dir=None,
			keep_alive=True,
		)
	)
	await browser_session.start()
	yield browser_session
	await browser_session.kill()


@pytest.mark.skipif(
	os.getenv('CI') == 'true' or os.getenv('GITHUB_ACTIONS') == 'true',
	reason='Skipped in CI: requires real LLM API key which blocks other tests',
)
class TestRadioButtons:
	"""Test cases for radio button interactions."""

	async def test_radio_button_clicking(self, browser_session, base_url):
		"""Test that agent can click radio buttons by checking for secret message."""

		task = f"Go to {base_url}/radio-test and click on the 'Blue' radio button and the 'Dog' radio button. After clicking both buttons, look for any text message that appears on the page and report exactly what you see."

		agent = Agent(
			task=task,
			browser_session=browser_session,
			max_actions_per_step=5,
			flash_mode=True,
		)

		# Run the agent
		history = await agent.run(max_steps=8)

		# Check if the secret message appears in the final response
		secret_found = False
		final_response = history.final_result()

		if final_response and 'SECRET_SUCCESS_12345' in final_response:
			secret_found = True
			print('\n✅ SUCCESS: Secret message found! Radio buttons were clicked correctly.')

		assert secret_found, (
			"Secret message 'SECRET_SUCCESS_12345' should be present, indicating both Blue and Dog radio buttons were clicked. Actual response: "
			+ str(final_response)
		)

		print(f'\n🎉 Test completed successfully! Agent completed {len(history)} steps and found the secret message.')

```

---

## backend/browser-use/tests/ci/models/model_test_helper.py

```py
"""Shared test helper for LLM model tests."""

import os

import pytest

from browser_use.agent.service import Agent
from browser_use.browser.profile import BrowserProfile
from browser_use.browser.session import BrowserSession


async def run_model_button_click_test(
	model_class,
	model_name: str,
	api_key_env: str | None,
	extra_kwargs: dict,
	httpserver,
):
	"""Test that an LLM model can click a button.

	This test verifies:
	1. Model can be initialized with API key
	2. Agent can navigate and click a button
	3. Button click is verified by checking page state change
	4. Completes within max 2 steps
	"""
	# Handle API key validation - skip test if not available
	if api_key_env is not None:
		api_key = os.getenv(api_key_env)
		if not api_key:
			pytest.skip(f'{api_key_env} not set - skipping test')
	else:
		api_key = None

	# Handle Azure-specific endpoint validation
	from browser_use.llm.azure.chat import ChatAzureOpenAI

	if model_class is ChatAzureOpenAI:
		azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')
		if not azure_endpoint:
			pytest.skip('AZURE_OPENAI_ENDPOINT not set - skipping test')
		# Add the azure_endpoint to extra_kwargs
		extra_kwargs = {**extra_kwargs, 'azure_endpoint': azure_endpoint}

	# Create HTML page with a button that changes page content when clicked
	html = """
	<!DOCTYPE html>
	<html>
	<head><title>Button Test</title></head>
	<body>
		<h1>Button Click Test</h1>
		<button id="test-button" onclick="document.getElementById('result').innerText='SUCCESS'">
			Click Me
		</button>
		<div id="result">NOT_CLICKED</div>
	</body>
	</html>
	"""
	httpserver.expect_request('/').respond_with_data(html, content_type='text/html')

	# Create LLM instance with extra kwargs if provided
	llm_kwargs = {'model': model_name}
	if api_key is not None:
		llm_kwargs['api_key'] = api_key
	llm_kwargs.update(extra_kwargs)
	llm = model_class(**llm_kwargs)  # type: ignore[arg-type]

	# Create browser session
	browser = BrowserSession(
		browser_profile=BrowserProfile(
			headless=True,
			user_data_dir=None,  # Use temporary directory
		)
	)

	try:
		# Start browser
		await browser.start()

		# Create agent with button click task (URL in task triggers auto-navigation)
		test_url = httpserver.url_for('/')
		agent = Agent(
			task=f'{test_url} - Click the button',
			llm=llm,
			browser_session=browser,
			max_steps=2,  # Max 2 steps as per requirements
		)

		# Run the agent
		result = await agent.run()

		# Verify task completed
		assert result is not None
		assert len(result.history) > 0

		# Verify button was clicked by checking page state across any step
		button_clicked = False
		for step in result.history:
			# Check state_message which contains browser state with page text
			if step.state_message and 'SUCCESS' in step.state_message:
				button_clicked = True
				break

		# Check if SUCCESS appears in any step (indicating button was clicked)
		assert button_clicked, 'Button was not clicked - SUCCESS not found in any page state'

	finally:
		# Clean up browser session
		await browser.kill()

```

---

## backend/browser-use/tests/ci/models/test_llm_anthropic.py

```py
"""Test Anthropic model button click."""

from browser_use.llm.anthropic.chat import ChatAnthropic
from tests.ci.models.model_test_helper import run_model_button_click_test


async def test_anthropic_claude_sonnet_4_0(httpserver):
	"""Test Anthropic claude-sonnet-4-0 can click a button."""
	await run_model_button_click_test(
		model_class=ChatAnthropic,
		model_name='claude-sonnet-4-0',
		api_key_env='ANTHROPIC_API_KEY',
		extra_kwargs={},
		httpserver=httpserver,
	)

```

---

## backend/browser-use/tests/ci/models/test_llm_azure.py

```py
"""Test Azure OpenAI model button click."""

from browser_use.llm.azure.chat import ChatAzureOpenAI
from tests.ci.models.model_test_helper import run_model_button_click_test


async def test_azure_gpt_4_1_mini(httpserver):
	"""Test Azure OpenAI gpt-4.1-mini can click a button."""
	await run_model_button_click_test(
		model_class=ChatAzureOpenAI,
		model_name='gpt-4.1-mini',
		api_key_env='AZURE_OPENAI_KEY',
		extra_kwargs={},  # Azure endpoint will be added by helper
		httpserver=httpserver,
	)

```

---

## backend/browser-use/tests/ci/models/test_llm_browseruse.py

```py
"""Test Browser Use model button click."""

from browser_use.llm.browser_use.chat import ChatBrowserUse
from tests.ci.models.model_test_helper import run_model_button_click_test


async def test_browseruse_bu_latest(httpserver):
	"""Test Browser Use bu-latest can click a button."""
	await run_model_button_click_test(
		model_class=ChatBrowserUse,
		model_name='bu-latest',
		api_key_env='BROWSER_USE_API_KEY',
		extra_kwargs={},
		httpserver=httpserver,
	)

```

---

## backend/browser-use/tests/ci/models/test_llm_google.py

```py
"""Test Google model button click."""

from browser_use.llm.google.chat import ChatGoogle
from tests.ci.models.model_test_helper import run_model_button_click_test


async def test_google_gemini_flash_latest(httpserver):
	"""Test Google gemini-flash-latest can click a button."""
	await run_model_button_click_test(
		model_class=ChatGoogle,
		model_name='gemini-flash-latest',
		api_key_env='GOOGLE_API_KEY',
		extra_kwargs={},
		httpserver=httpserver,
	)

```

---

## backend/browser-use/tests/ci/models/test_llm_openai.py

```py
"""Test OpenAI model button click."""

from browser_use.llm.openai.chat import ChatOpenAI
from tests.ci.models.model_test_helper import run_model_button_click_test


async def test_openai_gpt_4_1_mini(httpserver):
	"""Test OpenAI gpt-4.1-mini can click a button."""
	await run_model_button_click_test(
		model_class=ChatOpenAI,
		model_name='gpt-4.1-mini',
		api_key_env='OPENAI_API_KEY',
		extra_kwargs={},
		httpserver=httpserver,
	)

```

---

## backend/browser-use/tests/ci/models/test_llm_schema_optimizer.py

```py
"""
Tests for the SchemaOptimizer to ensure it correctly processes and
optimizes the schemas for agent actions without losing information.
"""

from pydantic import BaseModel

from browser_use.agent.views import AgentOutput
from browser_use.llm.schema import SchemaOptimizer
from browser_use.tools.service import Tools


class ProductInfo(BaseModel):
	"""A sample structured output model with multiple fields."""

	price: str
	title: str
	rating: float | None = None


def test_optimizer_preserves_all_fields_in_structured_done_action():
	"""
	Ensures the SchemaOptimizer does not drop fields from a custom structured
	output model when creating the schema for the 'done' action.

	This test specifically checks for a bug where fields were being lost
	during the optimization process.
	"""
	# 1. Setup a tools with a custom output model, simulating an Agent
	#    being created with an `output_model_schema`.
	tools = Tools(output_model=ProductInfo)

	# 2. Get the dynamically created AgentOutput model, which includes all registered actions.
	ActionModel = tools.registry.create_action_model()
	agent_output_model = AgentOutput.type_with_custom_actions(ActionModel)

	# 3. Run the schema optimizer on the agent's output model.
	optimized_schema = SchemaOptimizer.create_optimized_json_schema(agent_output_model)

	# 4. Find the 'done' action schema within the optimized output.
	# The path is properties -> action -> items -> anyOf -> [schema with 'done'].
	done_action_schema = None
	actions_schemas = optimized_schema.get('properties', {}).get('action', {}).get('items', {}).get('anyOf', [])
	for action_schema in actions_schemas:
		if 'done' in action_schema.get('properties', {}):
			done_action_schema = action_schema
			break

	# 5. Assert that the 'done' action schema was successfully found.
	assert done_action_schema is not None, "Could not find 'done' action in the optimized schema."

	# 6. Navigate to the schema for our custom data model within the 'done' action.
	# The path is properties -> done -> properties -> data -> properties.
	done_params_schema = done_action_schema.get('properties', {}).get('done', {})
	structured_data_schema = done_params_schema.get('properties', {}).get('data', {})
	final_properties = structured_data_schema.get('properties', {})

	# 7. Assert that the set of fields in the optimized schema matches the original model's fields.
	original_fields = set(ProductInfo.model_fields.keys())
	optimized_fields = set(final_properties.keys())

	assert original_fields == optimized_fields, (
		f"Field mismatch between original and optimized structured 'done' action schema.\n"
		f'Missing from optimized: {original_fields - optimized_fields}\n'
		f'Unexpected in optimized: {optimized_fields - original_fields}'
	)


def test_gemini_schema_retains_required_fields():
	"""Gemini schema should keep explicit required arrays for mandatory fields."""
	schema = SchemaOptimizer.create_gemini_optimized_schema(ProductInfo)

	assert 'required' in schema, 'Gemini schema removed required fields.'

	required_fields = set(schema['required'])
	assert {'price', 'title'}.issubset(required_fields), 'Mandatory fields must stay required for Gemini.'

```

---

## backend/browser-use/tests/ci/security/test_domain_filtering.py

```py
from browser_use.browser import BrowserProfile, BrowserSession


class TestUrlAllowlistSecurity:
	"""Tests for URL allowlist security bypass prevention and URL allowlist glob pattern matching."""

	def test_authentication_bypass_prevention(self):
		"""Test that the URL allowlist cannot be bypassed using authentication credentials."""
		from bubus import EventBus

		from browser_use.browser.watchdogs.security_watchdog import SecurityWatchdog

		# Create a context config with a sample allowed domain
		browser_profile = BrowserProfile(allowed_domains=['example.com'], headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Security vulnerability test cases
		# These should all be detected as malicious despite containing "example.com"
		assert watchdog._is_url_allowed('https://example.com:password@malicious.com') is False
		assert watchdog._is_url_allowed('https://example.com@malicious.com') is False
		assert watchdog._is_url_allowed('https://example.com%20@malicious.com') is False
		assert watchdog._is_url_allowed('https://example.com%3A@malicious.com') is False

		# Make sure legitimate auth credentials still work
		assert watchdog._is_url_allowed('https://user:password@example.com') is True

	def test_glob_pattern_matching(self):
		"""Test that glob patterns in allowed_domains work correctly."""
		from bubus import EventBus

		from browser_use.browser.watchdogs.security_watchdog import SecurityWatchdog

		# Test *.example.com pattern (should match subdomains and main domain)
		browser_profile = BrowserProfile(allowed_domains=['*.example.com'], headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Should match subdomains
		assert watchdog._is_url_allowed('https://sub.example.com') is True
		assert watchdog._is_url_allowed('https://deep.sub.example.com') is True

		# Should also match main domain
		assert watchdog._is_url_allowed('https://example.com') is True

		# Should not match other domains
		assert watchdog._is_url_allowed('https://notexample.com') is False
		assert watchdog._is_url_allowed('https://example.org') is False

		# Test more complex glob patterns
		browser_profile = BrowserProfile(
			allowed_domains=[
				'*.google.com',
				'https://wiki.org',
				'https://good.com',
				'https://*.test.com',
				'chrome://version',
				'brave://*',
			],
			headless=True,
			user_data_dir=None,
		)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Should match domains ending with google.com
		assert watchdog._is_url_allowed('https://google.com') is True
		assert watchdog._is_url_allowed('https://www.google.com') is True
		assert (
			watchdog._is_url_allowed('https://evilgood.com') is False
		)  # make sure we dont allow *good.com patterns, only *.good.com

		# Should match domains starting with wiki
		assert watchdog._is_url_allowed('http://wiki.org') is False
		assert watchdog._is_url_allowed('https://wiki.org') is True

		# Should not match internal domains because scheme was not provided
		assert watchdog._is_url_allowed('chrome://google.com') is False
		assert watchdog._is_url_allowed('chrome://abc.google.com') is False

		# Test browser internal URLs
		assert watchdog._is_url_allowed('chrome://settings') is False
		assert watchdog._is_url_allowed('chrome://version') is True
		assert watchdog._is_url_allowed('chrome-extension://version/') is False
		assert watchdog._is_url_allowed('brave://anything/') is True
		assert watchdog._is_url_allowed('about:blank') is True
		assert watchdog._is_url_allowed('chrome://new-tab-page/') is True
		assert watchdog._is_url_allowed('chrome://new-tab-page') is True

		# Test security for glob patterns (authentication credentials bypass attempts)
		# These should all be detected as malicious despite containing allowed domain patterns
		assert watchdog._is_url_allowed('https://allowed.example.com:password@notallowed.com') is False
		assert watchdog._is_url_allowed('https://subdomain.example.com@evil.com') is False
		assert watchdog._is_url_allowed('https://sub.example.com%20@malicious.org') is False
		assert watchdog._is_url_allowed('https://anygoogle.com@evil.org') is False

		# Test pattern matching
		assert watchdog._is_url_allowed('https://www.test.com') is True
		assert watchdog._is_url_allowed('https://www.testx.com') is False

	def test_glob_pattern_edge_cases(self):
		"""Test edge cases for glob pattern matching to ensure proper behavior."""
		from bubus import EventBus

		from browser_use.browser.watchdogs.security_watchdog import SecurityWatchdog

		# Test with domains containing glob pattern in the middle
		browser_profile = BrowserProfile(allowed_domains=['*.google.com', 'https://wiki.org'], headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Verify that 'wiki*' pattern doesn't match domains that merely contain 'wiki' in the middle
		assert watchdog._is_url_allowed('https://notawiki.com') is False
		assert watchdog._is_url_allowed('https://havewikipages.org') is False
		assert watchdog._is_url_allowed('https://my-wiki-site.com') is False

		# Verify that '*google.com' doesn't match domains that have 'google' in the middle
		assert watchdog._is_url_allowed('https://mygoogle.company.com') is False

		# Create context with potentially risky glob pattern that demonstrates security concerns
		browser_profile = BrowserProfile(allowed_domains=['*.google.com', '*.google.co.uk'], headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Should match legitimate Google domains
		assert watchdog._is_url_allowed('https://www.google.com') is True
		assert watchdog._is_url_allowed('https://mail.google.co.uk') is True

		# Shouldn't match potentially malicious domains with a similar structure
		# This demonstrates why the previous pattern was risky and why it's now rejected
		assert watchdog._is_url_allowed('https://www.google.evil.com') is False

	def test_automatic_www_subdomain_addition(self):
		"""Test that root domains automatically allow www subdomain."""
		from bubus import EventBus

		from browser_use.browser.watchdogs.security_watchdog import SecurityWatchdog

		# Test with simple root domains
		browser_profile = BrowserProfile(allowed_domains=['example.com', 'test.org'], headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Root domain should allow itself
		assert watchdog._is_url_allowed('https://example.com') is True
		assert watchdog._is_url_allowed('https://test.org') is True

		# Root domain should automatically allow www subdomain
		assert watchdog._is_url_allowed('https://www.example.com') is True
		assert watchdog._is_url_allowed('https://www.test.org') is True

		# Should not allow other subdomains
		assert watchdog._is_url_allowed('https://mail.example.com') is False
		assert watchdog._is_url_allowed('https://sub.test.org') is False

		# Should not allow unrelated domains
		assert watchdog._is_url_allowed('https://notexample.com') is False
		assert watchdog._is_url_allowed('https://www.notexample.com') is False

	def test_www_subdomain_not_added_for_country_tlds(self):
		"""Test www subdomain is NOT automatically added for country-specific TLDs (2+ dots)."""
		from bubus import EventBus

		from browser_use.browser.watchdogs.security_watchdog import SecurityWatchdog

		# Test with country-specific TLDs - these should NOT get automatic www
		browser_profile = BrowserProfile(
			allowed_domains=['example.co.uk', 'test.com.au', 'site.co.jp'], headless=True, user_data_dir=None
		)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Root domains should work exactly as specified
		assert watchdog._is_url_allowed('https://example.co.uk') is True
		assert watchdog._is_url_allowed('https://test.com.au') is True
		assert watchdog._is_url_allowed('https://site.co.jp') is True

		# www subdomains should NOT work automatically (user must specify explicitly)
		assert watchdog._is_url_allowed('https://www.example.co.uk') is False
		assert watchdog._is_url_allowed('https://www.test.com.au') is False
		assert watchdog._is_url_allowed('https://www.site.co.jp') is False

		# Other subdomains should not work
		assert watchdog._is_url_allowed('https://mail.example.co.uk') is False
		assert watchdog._is_url_allowed('https://api.test.com.au') is False

	def test_www_subdomain_not_added_for_existing_subdomains(self):
		"""Test that www is not automatically added for domains that already have subdomains."""
		from bubus import EventBus

		from browser_use.browser.watchdogs.security_watchdog import SecurityWatchdog

		# Test with existing subdomains - should NOT get automatic www
		browser_profile = BrowserProfile(allowed_domains=['mail.example.com', 'api.test.org'], headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Exact subdomain should work
		assert watchdog._is_url_allowed('https://mail.example.com') is True
		assert watchdog._is_url_allowed('https://api.test.org') is True

		# www should NOT be automatically added to subdomains
		assert watchdog._is_url_allowed('https://www.mail.example.com') is False
		assert watchdog._is_url_allowed('https://www.api.test.org') is False

		# Root domains should not work either
		assert watchdog._is_url_allowed('https://example.com') is False
		assert watchdog._is_url_allowed('https://test.org') is False

	def test_www_subdomain_not_added_for_wildcard_patterns(self):
		"""Test that www is not automatically added for wildcard patterns."""
		from bubus import EventBus

		from browser_use.browser.watchdogs.security_watchdog import SecurityWatchdog

		# Test with wildcard patterns - should NOT get automatic www logic
		browser_profile = BrowserProfile(allowed_domains=['*.example.com'], headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Wildcard should match everything including root and www
		assert watchdog._is_url_allowed('https://example.com') is True
		assert watchdog._is_url_allowed('https://www.example.com') is True
		assert watchdog._is_url_allowed('https://mail.example.com') is True

	def test_www_subdomain_not_added_for_url_patterns(self):
		"""Test that www is not automatically added for full URL patterns."""
		from bubus import EventBus

		from browser_use.browser.watchdogs.security_watchdog import SecurityWatchdog

		# Test with full URL patterns - should NOT get automatic www logic
		browser_profile = BrowserProfile(
			allowed_domains=['https://example.com', 'http://test.org'], headless=True, user_data_dir=None
		)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Exact URL should work
		assert watchdog._is_url_allowed('https://example.com/path') is True
		assert watchdog._is_url_allowed('http://test.org/page') is True

		# www should NOT be automatically added for full URL patterns
		assert watchdog._is_url_allowed('https://www.example.com') is False
		assert watchdog._is_url_allowed('http://www.test.org') is False

	def test_is_root_domain_helper(self):
		"""Test the _is_root_domain helper method logic."""
		from bubus import EventBus

		from browser_use.browser.watchdogs.security_watchdog import SecurityWatchdog

		browser_profile = BrowserProfile(allowed_domains=['example.com'], headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Simple root domains (1 dot) - should return True
		assert watchdog._is_root_domain('example.com') is True
		assert watchdog._is_root_domain('test.org') is True
		assert watchdog._is_root_domain('site.net') is True

		# Subdomains (more than 1 dot) - should return False
		assert watchdog._is_root_domain('www.example.com') is False
		assert watchdog._is_root_domain('mail.example.com') is False
		assert watchdog._is_root_domain('example.co.uk') is False
		assert watchdog._is_root_domain('test.com.au') is False

		# Wildcards - should return False
		assert watchdog._is_root_domain('*.example.com') is False
		assert watchdog._is_root_domain('*example.com') is False

		# Full URLs - should return False
		assert watchdog._is_root_domain('https://example.com') is False
		assert watchdog._is_root_domain('http://test.org') is False

		# Invalid domains - should return False
		assert watchdog._is_root_domain('example') is False
		assert watchdog._is_root_domain('') is False


class TestUrlProhibitlistSecurity:
	"""Tests for URL prohibitlist (blocked domains) behavior and matching semantics."""

	def test_simple_prohibited_domains(self):
		"""Domain-only patterns block exact host and www, but not other subdomains."""
		from bubus import EventBus

		from browser_use.browser.watchdogs.security_watchdog import SecurityWatchdog

		browser_profile = BrowserProfile(prohibited_domains=['example.com', 'test.org'], headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Block exact and www
		assert watchdog._is_url_allowed('https://example.com') is False
		assert watchdog._is_url_allowed('https://www.example.com') is False
		assert watchdog._is_url_allowed('https://test.org') is False
		assert watchdog._is_url_allowed('https://www.test.org') is False

		# Allow other subdomains when only root is prohibited
		assert watchdog._is_url_allowed('https://mail.example.com') is True
		assert watchdog._is_url_allowed('https://api.test.org') is True

		# Allow unrelated domains
		assert watchdog._is_url_allowed('https://notexample.com') is True

	def test_glob_pattern_prohibited(self):
		"""Wildcard patterns block subdomains and main domain for http/https only."""
		from bubus import EventBus

		from browser_use.browser.watchdogs.security_watchdog import SecurityWatchdog

		browser_profile = BrowserProfile(prohibited_domains=['*.example.com'], headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Block subdomains and main domain
		assert watchdog._is_url_allowed('https://example.com') is False
		assert watchdog._is_url_allowed('https://www.example.com') is False
		assert watchdog._is_url_allowed('https://mail.example.com') is False

		# Allow other domains
		assert watchdog._is_url_allowed('https://notexample.com') is True

		# Wildcard with domain-only should not apply to non-http(s)
		assert watchdog._is_url_allowed('chrome://abc.example.com') is True

	def test_full_url_prohibited_patterns(self):
		"""Full URL patterns block only matching scheme/host/prefix."""
		from bubus import EventBus

		from browser_use.browser.watchdogs.security_watchdog import SecurityWatchdog

		browser_profile = BrowserProfile(prohibited_domains=['https://wiki.org', 'brave://*'], headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Scheme-specific blocking
		assert watchdog._is_url_allowed('http://wiki.org') is True
		assert watchdog._is_url_allowed('https://wiki.org') is False
		assert watchdog._is_url_allowed('https://wiki.org/path') is False

		# Internal URL prefix blocking
		assert watchdog._is_url_allowed('brave://anything/') is False
		assert watchdog._is_url_allowed('chrome://settings') is True

	def test_internal_urls_allowed_even_when_prohibited(self):
		"""Internal new-tab/blank URLs are always allowed regardless of prohibited list."""
		from bubus import EventBus

		from browser_use.browser.watchdogs.security_watchdog import SecurityWatchdog

		browser_profile = BrowserProfile(prohibited_domains=['*'], headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		assert watchdog._is_url_allowed('about:blank') is True
		assert watchdog._is_url_allowed('chrome://new-tab-page/') is True
		assert watchdog._is_url_allowed('chrome://new-tab-page') is True
		assert watchdog._is_url_allowed('chrome://newtab/') is True

	def test_prohibited_ignored_when_allowlist_present(self):
		"""When allowlist is set, prohibited list is ignored by design."""
		from bubus import EventBus

		from browser_use.browser.watchdogs.security_watchdog import SecurityWatchdog

		browser_profile = BrowserProfile(
			allowed_domains=['*.example.com'],
			prohibited_domains=['https://example.com'],
			headless=True,
			user_data_dir=None,
		)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Allowed by allowlist even though exact URL is in prohibited list
		assert watchdog._is_url_allowed('https://example.com') is True
		assert watchdog._is_url_allowed('https://www.example.com') is True

		# Not in allowlist => blocked (prohibited list is not consulted in this mode)
		assert watchdog._is_url_allowed('https://api.example.com') is True  # wildcard allowlist includes this
		# A domain outside the allowlist should be blocked
		assert watchdog._is_url_allowed('https://notexample.com') is False

	def test_auth_credentials_do_not_cause_false_block(self):
		"""Credentials injection with prohibited domain in username should not block unrelated hosts."""
		from bubus import EventBus

		from browser_use.browser.watchdogs.security_watchdog import SecurityWatchdog

		browser_profile = BrowserProfile(prohibited_domains=['example.com'], headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Host is malicious.com, should not be blocked just because username contains example.com
		assert watchdog._is_url_allowed('https://example.com:password@malicious.com') is True
		assert watchdog._is_url_allowed('https://example.com@malicious.com') is True
		assert watchdog._is_url_allowed('https://example.com%20@malicious.com') is True
		assert watchdog._is_url_allowed('https://example.com%3A@malicious.com') is True

		# Legitimate credentials to a prohibited host should be blocked
		assert watchdog._is_url_allowed('https://user:password@example.com') is False

	def test_case_insensitive_prohibited_domains(self):
		"""Prohibited domain matching should be case-insensitive."""
		from bubus import EventBus

		from browser_use.browser.watchdogs.security_watchdog import SecurityWatchdog

		browser_profile = BrowserProfile(prohibited_domains=['Example.COM'], headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		assert watchdog._is_url_allowed('https://example.com') is False
		assert watchdog._is_url_allowed('https://WWW.EXAMPLE.COM') is False
		assert watchdog._is_url_allowed('https://mail.example.com') is True


class TestDomainListOptimization:
	"""Tests for domain list optimization (set conversion for large lists)."""

	def test_small_list_keeps_pattern_support(self):
		"""Test that lists < 100 items keep pattern matching support."""
		from bubus import EventBus

		from browser_use.browser.watchdogs.security_watchdog import SecurityWatchdog

		browser_profile = BrowserProfile(
			prohibited_domains=['*.google.com', 'x.com', 'facebook.com'], headless=True, user_data_dir=None
		)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Should still be a list
		assert isinstance(browser_session.browser_profile.prohibited_domains, list)

		# Pattern matching should work
		assert watchdog._is_url_allowed('https://www.google.com') is False
		assert watchdog._is_url_allowed('https://mail.google.com') is False
		assert watchdog._is_url_allowed('https://google.com') is False

		# Exact matches should work
		assert watchdog._is_url_allowed('https://x.com') is False
		assert watchdog._is_url_allowed('https://facebook.com') is False

		# Other domains should be allowed
		assert watchdog._is_url_allowed('https://example.com') is True

	def test_large_list_converts_to_set(self):
		"""Test that lists >= 100 items are converted to sets."""
		from bubus import EventBus

		from browser_use.browser.watchdogs.security_watchdog import SecurityWatchdog

		# Create a list of 100 domains
		large_list = [f'blocked{i}.com' for i in range(100)]

		browser_profile = BrowserProfile(prohibited_domains=large_list, headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Should be converted to set
		assert isinstance(browser_session.browser_profile.prohibited_domains, set)
		assert len(browser_session.browser_profile.prohibited_domains) == 100

		# Exact matches should work
		assert watchdog._is_url_allowed('https://blocked0.com') is False
		assert watchdog._is_url_allowed('https://blocked50.com') is False
		assert watchdog._is_url_allowed('https://blocked99.com') is False

		# Other domains should be allowed
		assert watchdog._is_url_allowed('https://example.com') is True
		assert watchdog._is_url_allowed('https://blocked100.com') is True  # Not in list

	def test_www_variant_matching_with_sets(self):
		"""Test that www variants are checked in set-based lookups."""
		from bubus import EventBus

		from browser_use.browser.watchdogs.security_watchdog import SecurityWatchdog

		# Create a list with 100 domains (some with www, some without)
		large_list = [f'site{i}.com' for i in range(50)] + [f'www.domain{i}.org' for i in range(50)]

		browser_profile = BrowserProfile(prohibited_domains=large_list, headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Should be converted to set
		assert isinstance(browser_session.browser_profile.prohibited_domains, set)

		# Test www variant matching for domains without www prefix
		assert watchdog._is_url_allowed('https://site0.com') is False
		assert watchdog._is_url_allowed('https://www.site0.com') is False  # Should also be blocked

		# Test www variant matching for domains with www prefix
		assert watchdog._is_url_allowed('https://www.domain0.org') is False
		assert watchdog._is_url_allowed('https://domain0.org') is False  # Should also be blocked

		# Test that unrelated domains are allowed
		assert watchdog._is_url_allowed('https://example.com') is True
		assert watchdog._is_url_allowed('https://www.example.com') is True

	def test_allowed_domains_with_sets(self):
		"""Test that allowed_domains also works with set optimization."""
		from bubus import EventBus

		from browser_use.browser.watchdogs.security_watchdog import SecurityWatchdog

		# Create a large allowlist
		large_list = [f'allowed{i}.com' for i in range(100)]

		browser_profile = BrowserProfile(allowed_domains=large_list, headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Should be converted to set
		assert isinstance(browser_session.browser_profile.allowed_domains, set)

		# Allowed domains should work
		assert watchdog._is_url_allowed('https://allowed0.com') is True
		assert watchdog._is_url_allowed('https://www.allowed0.com') is True
		assert watchdog._is_url_allowed('https://allowed99.com') is True

		# Other domains should be blocked
		assert watchdog._is_url_allowed('https://example.com') is False
		assert watchdog._is_url_allowed('https://notallowed.com') is False

	def test_manual_set_input(self):
		"""Test that users can directly provide a set."""
		from bubus import EventBus

		from browser_use.browser.watchdogs.security_watchdog import SecurityWatchdog

		blocked_set = {f'blocked{i}.com' for i in range(50)}

		browser_profile = BrowserProfile(prohibited_domains=blocked_set, headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Should remain a set
		assert isinstance(browser_session.browser_profile.prohibited_domains, set)

		# Should work correctly
		assert watchdog._is_url_allowed('https://blocked0.com') is False
		assert watchdog._is_url_allowed('https://example.com') is True

```

---

## backend/browser-use/tests/ci/security/test_ip_blocking.py

```py
"""
Comprehensive tests for IP address blocking in SecurityWatchdog.

Tests cover IPv4, IPv6, localhost, private networks, edge cases, and interactions
with allowed_domains and prohibited_domains configurations.
"""

from bubus import EventBus

from browser_use.browser import BrowserProfile, BrowserSession
from browser_use.browser.watchdogs.security_watchdog import SecurityWatchdog


class TestIPv4Blocking:
	"""Test blocking of IPv4 addresses."""

	def test_block_public_ipv4_addresses(self):
		"""Test that public IPv4 addresses are blocked when block_ip_addresses=True."""
		browser_profile = BrowserProfile(block_ip_addresses=True, headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Public IPv4 addresses should be blocked
		assert watchdog._is_url_allowed('http://180.1.1.1/supersafe.txt') is False
		assert watchdog._is_url_allowed('https://8.8.8.8/') is False
		assert watchdog._is_url_allowed('http://1.1.1.1:8080/api') is False
		assert watchdog._is_url_allowed('https://142.250.185.46/search') is False
		assert watchdog._is_url_allowed('http://93.184.216.34/') is False

	def test_block_private_ipv4_networks(self):
		"""Test that private network IPv4 addresses are blocked."""
		browser_profile = BrowserProfile(block_ip_addresses=True, headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Private network ranges (RFC 1918)
		assert watchdog._is_url_allowed('http://192.168.1.1/') is False
		assert watchdog._is_url_allowed('http://192.168.0.100/admin') is False
		assert watchdog._is_url_allowed('http://10.0.0.1/') is False
		assert watchdog._is_url_allowed('http://10.255.255.255/') is False
		assert watchdog._is_url_allowed('http://172.16.0.1/') is False
		assert watchdog._is_url_allowed('http://172.31.255.254/') is False

	def test_block_localhost_ipv4(self):
		"""Test that localhost IPv4 addresses are blocked."""
		browser_profile = BrowserProfile(block_ip_addresses=True, headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Localhost/loopback addresses
		assert watchdog._is_url_allowed('http://127.0.0.1/') is False
		assert watchdog._is_url_allowed('http://127.0.0.1:8080/') is False
		assert watchdog._is_url_allowed('https://127.0.0.1:3000/api/test') is False
		assert watchdog._is_url_allowed('http://127.1.2.3/') is False  # Any 127.x.x.x

	def test_block_ipv4_with_ports_and_paths(self):
		"""Test that IPv4 addresses with ports and paths are blocked."""
		browser_profile = BrowserProfile(block_ip_addresses=True, headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# With various ports
		assert watchdog._is_url_allowed('http://8.8.8.8:80/') is False
		assert watchdog._is_url_allowed('https://8.8.8.8:443/') is False
		assert watchdog._is_url_allowed('http://192.168.1.1:8080/') is False
		assert watchdog._is_url_allowed('http://10.0.0.1:3000/api') is False

		# With paths and query strings
		assert watchdog._is_url_allowed('http://1.2.3.4/path/to/resource') is False
		assert watchdog._is_url_allowed('http://5.6.7.8/api?key=value') is False
		assert watchdog._is_url_allowed('https://9.10.11.12/path/to/file.html#anchor') is False

	def test_allow_ipv4_when_blocking_disabled(self):
		"""Test that IPv4 addresses are allowed when block_ip_addresses=False (default)."""
		browser_profile = BrowserProfile(block_ip_addresses=False, headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# All IP addresses should be allowed when blocking is disabled
		assert watchdog._is_url_allowed('http://180.1.1.1/supersafe.txt') is True
		assert watchdog._is_url_allowed('http://192.168.1.1/') is True
		assert watchdog._is_url_allowed('http://127.0.0.1:8080/') is True
		assert watchdog._is_url_allowed('http://8.8.8.8/') is True


class TestIPv6Blocking:
	"""Test blocking of IPv6 addresses."""

	def test_block_ipv6_addresses(self):
		"""Test that IPv6 addresses are blocked when block_ip_addresses=True."""
		browser_profile = BrowserProfile(block_ip_addresses=True, headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Public IPv6 addresses (with brackets as per URL standard)
		assert watchdog._is_url_allowed('http://[2001:db8::1]/') is False
		assert watchdog._is_url_allowed('https://[2001:4860:4860::8888]/') is False
		assert watchdog._is_url_allowed('http://[2606:4700:4700::1111]/path') is False
		assert watchdog._is_url_allowed('https://[2001:db8:85a3::8a2e:370:7334]/api') is False

	def test_block_ipv6_localhost(self):
		"""Test that IPv6 localhost addresses are blocked."""
		browser_profile = BrowserProfile(block_ip_addresses=True, headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# IPv6 loopback
		assert watchdog._is_url_allowed('http://[::1]/') is False
		assert watchdog._is_url_allowed('http://[::1]:8080/') is False
		assert watchdog._is_url_allowed('https://[::1]:3000/api') is False
		assert watchdog._is_url_allowed('http://[0:0:0:0:0:0:0:1]/') is False  # Expanded form

	def test_block_ipv6_with_ports_and_paths(self):
		"""Test that IPv6 addresses with ports and paths are blocked."""
		browser_profile = BrowserProfile(block_ip_addresses=True, headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# IPv6 with ports
		assert watchdog._is_url_allowed('http://[2001:db8::1]:80/') is False
		assert watchdog._is_url_allowed('https://[2001:db8::1]:443/') is False
		assert watchdog._is_url_allowed('http://[::1]:8080/api') is False

		# IPv6 with paths
		assert watchdog._is_url_allowed('http://[2001:db8::1]/path/to/resource') is False
		assert watchdog._is_url_allowed('https://[2001:db8::1]/api?key=value') is False

	def test_allow_ipv6_when_blocking_disabled(self):
		"""Test that IPv6 addresses are allowed when block_ip_addresses=False."""
		browser_profile = BrowserProfile(block_ip_addresses=False, headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# All IPv6 addresses should be allowed
		assert watchdog._is_url_allowed('http://[2001:db8::1]/') is True
		assert watchdog._is_url_allowed('http://[::1]:8080/') is True
		assert watchdog._is_url_allowed('https://[2001:4860:4860::8888]/') is True


class TestDomainNamesStillAllowed:
	"""Test that regular domain names are not affected by IP blocking."""

	def test_domain_names_allowed_with_ip_blocking(self):
		"""Test that domain names continue to work when IP blocking is enabled."""
		browser_profile = BrowserProfile(block_ip_addresses=True, headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Regular domain names should still be allowed
		assert watchdog._is_url_allowed('https://example.com') is True
		assert watchdog._is_url_allowed('https://www.google.com') is True
		assert watchdog._is_url_allowed('http://subdomain.example.org/path') is True
		assert watchdog._is_url_allowed('https://api.github.com/repos') is True
		assert watchdog._is_url_allowed('http://localhost/') is True  # "localhost" is a domain name, not IP
		assert watchdog._is_url_allowed('http://localhost:8080/api') is True

	def test_domains_with_numbers_allowed(self):
		"""Test that domain names containing numbers are still allowed."""
		browser_profile = BrowserProfile(block_ip_addresses=True, headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Domains with numbers (but not valid IP addresses)
		assert watchdog._is_url_allowed('https://example123.com') is True
		assert watchdog._is_url_allowed('https://123example.com') is True
		assert watchdog._is_url_allowed('https://server1.example.com') is True
		assert watchdog._is_url_allowed('http://web2.site.org') is True


class TestIPBlockingWithAllowedDomains:
	"""Test interaction between IP blocking and allowed_domains."""

	def test_ip_blocked_even_in_allowed_domains(self):
		"""Test that IPs are blocked even if they're in allowed_domains list."""
		# Note: It doesn't make sense to add IPs to allowed_domains, but if someone does,
		# IP blocking should take precedence
		browser_profile = BrowserProfile(
			block_ip_addresses=True,
			allowed_domains=['example.com', '192.168.1.1'],  # IP in allowlist
			headless=True,
			user_data_dir=None,
		)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# IP should be blocked despite being in allowed_domains
		assert watchdog._is_url_allowed('http://192.168.1.1/') is False

		# Regular domain should work as expected
		assert watchdog._is_url_allowed('https://example.com') is True

		# Other domains not in allowed_domains should be blocked
		assert watchdog._is_url_allowed('https://other.com') is False

	def test_allowed_domains_with_ip_blocking_enabled(self):
		"""Test that allowed_domains works normally with IP blocking enabled."""
		browser_profile = BrowserProfile(
			block_ip_addresses=True, allowed_domains=['example.com', '*.google.com'], headless=True, user_data_dir=None
		)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Allowed domains should work
		assert watchdog._is_url_allowed('https://example.com') is True
		assert watchdog._is_url_allowed('https://www.google.com') is True

		# Not allowed domains should be blocked
		assert watchdog._is_url_allowed('https://other.com') is False

		# IPs should be blocked regardless
		assert watchdog._is_url_allowed('http://8.8.8.8/') is False
		assert watchdog._is_url_allowed('http://192.168.1.1/') is False


class TestIPBlockingWithProhibitedDomains:
	"""Test interaction between IP blocking and prohibited_domains."""

	def test_ip_blocked_regardless_of_prohibited_domains(self):
		"""Test that IPs are blocked when IP blocking is on, independent of prohibited_domains."""
		browser_profile = BrowserProfile(
			block_ip_addresses=True, prohibited_domains=['example.com'], headless=True, user_data_dir=None
		)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# IPs should be blocked due to IP blocking
		assert watchdog._is_url_allowed('http://192.168.1.1/') is False
		assert watchdog._is_url_allowed('http://8.8.8.8/') is False

		# Prohibited domain should be blocked
		assert watchdog._is_url_allowed('https://example.com') is False

		# Other domains should be allowed
		assert watchdog._is_url_allowed('https://other.com') is True

	def test_prohibited_domains_without_ip_blocking(self):
		"""Test that prohibited_domains works normally when IP blocking is disabled."""
		browser_profile = BrowserProfile(
			block_ip_addresses=False, prohibited_domains=['example.com', '8.8.8.8'], headless=True, user_data_dir=None
		)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Prohibited domain should be blocked
		assert watchdog._is_url_allowed('https://example.com') is False

		# IP in prohibited list should be blocked (by prohibited_domains, not IP blocking)
		assert watchdog._is_url_allowed('http://8.8.8.8/') is False

		# Other IPs should be allowed (IP blocking is off)
		assert watchdog._is_url_allowed('http://192.168.1.1/') is True

		# Other domains should be allowed
		assert watchdog._is_url_allowed('https://other.com') is True


class TestEdgeCases:
	"""Test edge cases and invalid inputs."""

	def test_invalid_urls_handled_gracefully(self):
		"""Test that invalid URLs don't cause crashes."""
		browser_profile = BrowserProfile(block_ip_addresses=True, headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Invalid URLs should return False
		assert watchdog._is_url_allowed('not-a-url') is False
		assert watchdog._is_url_allowed('') is False
		assert watchdog._is_url_allowed('http://') is False
		assert watchdog._is_url_allowed('://example.com') is False

	def test_internal_browser_urls_allowed(self):
		"""Test that internal browser URLs are still allowed with IP blocking."""
		browser_profile = BrowserProfile(block_ip_addresses=True, headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Internal URLs should always be allowed
		assert watchdog._is_url_allowed('about:blank') is True
		assert watchdog._is_url_allowed('chrome://new-tab-page/') is True
		assert watchdog._is_url_allowed('chrome://new-tab-page') is True
		assert watchdog._is_url_allowed('chrome://newtab/') is True

	def test_ipv4_lookalike_domains_allowed(self):
		"""Test that domains that look like IPs but aren't are still allowed."""
		browser_profile = BrowserProfile(block_ip_addresses=True, headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# These look like IPs but have too many/few octets or invalid ranges
		# The IP parser should reject them, so they're treated as domain names
		assert watchdog._is_url_allowed('http://999.999.999.999/') is True  # Invalid IP range
		assert watchdog._is_url_allowed('http://1.2.3.4.5/') is True  # Too many octets
		assert watchdog._is_url_allowed('http://1.2.3/') is True  # Too few octets

	def test_different_schemes_with_ips(self):
		"""Test that IP blocking works across different URL schemes."""
		browser_profile = BrowserProfile(block_ip_addresses=True, headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# HTTP and HTTPS
		assert watchdog._is_url_allowed('http://192.168.1.1/') is False
		assert watchdog._is_url_allowed('https://192.168.1.1/') is False

		# FTP (if browser supports it)
		assert watchdog._is_url_allowed('ftp://192.168.1.1/') is False

		# WebSocket (parsed as regular URL)
		assert watchdog._is_url_allowed('ws://192.168.1.1:8080/') is False
		assert watchdog._is_url_allowed('wss://192.168.1.1:8080/') is False


class TestIsIPAddressHelper:
	"""Test the _is_ip_address helper method directly."""

	def test_valid_ipv4_detection(self):
		"""Test that valid IPv4 addresses are correctly detected."""
		browser_profile = BrowserProfile(block_ip_addresses=True, headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Valid IPv4 addresses
		assert watchdog._is_ip_address('127.0.0.1') is True
		assert watchdog._is_ip_address('192.168.1.1') is True
		assert watchdog._is_ip_address('8.8.8.8') is True
		assert watchdog._is_ip_address('255.255.255.255') is True
		assert watchdog._is_ip_address('0.0.0.0') is True

	def test_valid_ipv6_detection(self):
		"""Test that valid IPv6 addresses are correctly detected."""
		browser_profile = BrowserProfile(block_ip_addresses=True, headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Valid IPv6 addresses (without brackets - those are URL-specific)
		assert watchdog._is_ip_address('::1') is True
		assert watchdog._is_ip_address('2001:db8::1') is True
		assert watchdog._is_ip_address('2001:4860:4860::8888') is True
		assert watchdog._is_ip_address('fe80::1') is True
		assert watchdog._is_ip_address('2001:db8:85a3::8a2e:370:7334') is True

	def test_invalid_ip_detection(self):
		"""Test that non-IP strings are correctly identified as not IPs."""
		browser_profile = BrowserProfile(block_ip_addresses=True, headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Domain names (not IPs)
		assert watchdog._is_ip_address('example.com') is False
		assert watchdog._is_ip_address('www.google.com') is False
		assert watchdog._is_ip_address('localhost') is False

		# Invalid IPs
		assert watchdog._is_ip_address('999.999.999.999') is False
		assert watchdog._is_ip_address('1.2.3') is False
		assert watchdog._is_ip_address('1.2.3.4.5') is False
		assert watchdog._is_ip_address('not-an-ip') is False
		assert watchdog._is_ip_address('') is False

		# IPs with ports or paths (not valid for the helper - it only checks hostnames)
		assert watchdog._is_ip_address('192.168.1.1:8080') is False
		assert watchdog._is_ip_address('192.168.1.1/path') is False


class TestDefaultBehavior:
	"""Test that default behavior (no IP blocking) is maintained."""

	def test_default_block_ip_addresses_is_false(self):
		"""Test that block_ip_addresses defaults to False."""
		browser_profile = BrowserProfile(headless=True, user_data_dir=None)

		# Default should be False
		assert browser_profile.block_ip_addresses is False

	def test_no_blocking_by_default(self):
		"""Test that IPs are not blocked by default."""
		browser_profile = BrowserProfile(headless=True, user_data_dir=None)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# All IPs should be allowed by default
		assert watchdog._is_url_allowed('http://180.1.1.1/supersafe.txt') is True
		assert watchdog._is_url_allowed('http://192.168.1.1/') is True
		assert watchdog._is_url_allowed('http://127.0.0.1:8080/') is True
		assert watchdog._is_url_allowed('http://[::1]/') is True
		assert watchdog._is_url_allowed('https://8.8.8.8/') is True


class TestComplexScenarios:
	"""Test complex real-world scenarios."""

	def test_mixed_configuration_comprehensive(self):
		"""Test a complex configuration with multiple security settings."""
		browser_profile = BrowserProfile(
			block_ip_addresses=True,
			allowed_domains=['example.com', '*.google.com'],
			prohibited_domains=['bad.example.com'],  # Should be ignored when allowlist is set
			headless=True,
			user_data_dir=None,
		)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Allowed domains should work
		assert watchdog._is_url_allowed('https://example.com') is True
		assert watchdog._is_url_allowed('https://www.google.com') is True
		assert watchdog._is_url_allowed('https://mail.google.com') is True

		# IPs should be blocked
		assert watchdog._is_url_allowed('http://8.8.8.8/') is False
		assert watchdog._is_url_allowed('http://192.168.1.1/') is False

		# Domains not in allowlist should be blocked
		assert watchdog._is_url_allowed('https://other.com') is False

	def test_localhost_development_scenario(self):
		"""Test typical local development scenario."""
		# Developer wants to block external IPs but allow domain names
		browser_profile = BrowserProfile(
			block_ip_addresses=True,
			headless=True,
			user_data_dir=None,  # No domain restrictions
		)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Domain names should work (including localhost as a name)
		assert watchdog._is_url_allowed('http://localhost:3000/') is True
		assert watchdog._is_url_allowed('http://localhost:8080/api') is True

		# But localhost IP should be blocked
		assert watchdog._is_url_allowed('http://127.0.0.1:3000/') is False

		# External domains should work
		assert watchdog._is_url_allowed('https://api.example.com') is True

		# External IPs should be blocked
		assert watchdog._is_url_allowed('http://8.8.8.8/') is False

	def test_security_hardening_scenario(self):
		"""Test maximum security scenario with IP blocking and domain restrictions."""
		browser_profile = BrowserProfile(
			block_ip_addresses=True,
			allowed_domains=['example.com', 'api.example.com'],
			headless=True,
			user_data_dir=None,
		)
		browser_session = BrowserSession(browser_profile=browser_profile)
		event_bus = EventBus()
		watchdog = SecurityWatchdog(browser_session=browser_session, event_bus=event_bus)

		# Only specified domains allowed
		assert watchdog._is_url_allowed('https://example.com') is True
		assert watchdog._is_url_allowed('https://api.example.com') is True

		# IPs blocked
		assert watchdog._is_url_allowed('http://192.168.1.1/') is False

		# Other domains blocked
		assert watchdog._is_url_allowed('https://other.com') is False

		# Even localhost blocked
		assert watchdog._is_url_allowed('http://127.0.0.1/') is False

```

---

## backend/browser-use/tests/ci/security/test_security_flags.py

```py
"""Test that disable_security flag properly merges --disable-features flags without breaking extensions."""

import tempfile

from browser_use.browser.profile import BrowserProfile


class TestBrowserProfileDisableSecurity:
	"""Test disable_security flag behavior."""

	def test_disable_security_preserves_extension_features(self):
		"""Test that disable_security=True doesn't break extension features by properly merging --disable-features flags."""

		# Test with disable_security=False (baseline)
		profile_normal = BrowserProfile(disable_security=False, user_data_dir=tempfile.mkdtemp(prefix='test-normal-'))
		profile_normal.detect_display_configuration()
		args_normal = profile_normal.get_args()

		# Test with disable_security=True
		profile_security_disabled = BrowserProfile(disable_security=True, user_data_dir=tempfile.mkdtemp(prefix='test-security-'))
		profile_security_disabled.detect_display_configuration()
		args_security_disabled = profile_security_disabled.get_args()

		# Extract disable-features args
		def extract_disable_features(args):
			for arg in args:
				if arg.startswith('--disable-features='):
					return set(arg.split('=', 1)[1].split(','))
			return set()

		features_normal = extract_disable_features(args_normal)
		features_security_disabled = extract_disable_features(args_security_disabled)

		# Check that extension-related features are preserved
		extension_features = {
			'ExtensionManifestV2Disabled',
			'ExtensionDisableUnsupportedDeveloper',
			'ExtensionManifestV2Unsupported',
		}

		security_features = {'IsolateOrigins', 'site-per-process'}

		# Verify that security disabled has both extension and security features
		missing_extension_features = extension_features - features_security_disabled
		missing_security_features = security_features - features_security_disabled

		assert not missing_extension_features, (
			f'Missing extension features when disable_security=True: {missing_extension_features}'
		)
		assert not missing_security_features, f'Missing security features when disable_security=True: {missing_security_features}'

		# Verify that security disabled profile has more features than normal (due to added security features)
		assert len(features_security_disabled) > len(features_normal), (
			'Security disabled profile should have more features than normal profile'
		)

		# Verify all normal features are preserved in security disabled profile
		missing_normal_features = features_normal - features_security_disabled
		assert not missing_normal_features, f'Normal features missing from security disabled profile: {missing_normal_features}'

	def test_disable_features_flag_deduplication(self):
		"""Test that duplicate --disable-features values are properly deduplicated."""

		profile = BrowserProfile(
			disable_security=True,
			user_data_dir=tempfile.mkdtemp(prefix='test-dedup-'),
			# Add duplicate features to test deduplication
			args=['--disable-features=TestFeature1,TestFeature2', '--disable-features=TestFeature2,TestFeature3'],
		)
		profile.detect_display_configuration()
		args = profile.get_args()

		# Extract disable-features args
		disable_features_args = [arg for arg in args if arg.startswith('--disable-features=')]

		# Should only have one consolidated --disable-features flag
		assert len(disable_features_args) == 1, f'Expected 1 disable-features flag, got {len(disable_features_args)}'

		features = set(disable_features_args[0].split('=', 1)[1].split(','))

		# Should have all test features without duplicates
		expected_test_features = {'TestFeature1', 'TestFeature2', 'TestFeature3'}
		assert expected_test_features.issubset(features), f'Missing test features: {expected_test_features - features}'

```

---

## backend/browser-use/tests/ci/security/test_sensitive_data.py

```py
import pytest
from pydantic import BaseModel, Field

from browser_use.agent.message_manager.service import MessageManager
from browser_use.agent.views import MessageManagerState
from browser_use.filesystem.file_system import FileSystem
from browser_use.llm import SystemMessage, UserMessage
from browser_use.llm.messages import ContentPartTextParam
from browser_use.tools.registry.service import Registry
from browser_use.utils import is_new_tab_page, match_url_with_domain_pattern


class SensitiveParams(BaseModel):
	"""Test parameter model for sensitive data testing."""

	text: str = Field(description='Text with sensitive data placeholders')


@pytest.fixture
def registry():
	return Registry()


@pytest.fixture
def message_manager():
	import os
	import tempfile
	import uuid

	base_tmp = tempfile.gettempdir()  # e.g., /tmp on Unix
	file_system_path = os.path.join(base_tmp, str(uuid.uuid4()))
	return MessageManager(
		task='Test task',
		system_message=SystemMessage(content='System message'),
		state=MessageManagerState(),
		file_system=FileSystem(file_system_path),
	)


def test_replace_sensitive_data_with_missing_keys(registry, caplog):
	"""Test that _replace_sensitive_data handles missing keys gracefully"""
	# Create a simple Pydantic model with sensitive data placeholders
	params = SensitiveParams(text='Please enter <secret>username</secret> and <secret>password</secret>')

	# Case 1: All keys present - both placeholders should be replaced
	sensitive_data = {'username': 'user123', 'password': 'pass456'}
	result = registry._replace_sensitive_data(params, sensitive_data)
	assert result.text == 'Please enter user123 and pass456'
	assert '<secret>' not in result.text  # No secret tags should remain

	# Case 2: One key missing - only available key should be replaced
	sensitive_data = {'username': 'user123'}  # password is missing
	result = registry._replace_sensitive_data(params, sensitive_data)
	assert result.text == 'Please enter user123 and <secret>password</secret>'
	assert 'user123' in result.text
	assert '<secret>password</secret>' in result.text  # Missing key's tag remains

	# Case 3: Multiple keys missing - all tags should be preserved
	sensitive_data = {}  # both keys missing
	result = registry._replace_sensitive_data(params, sensitive_data)
	assert result.text == 'Please enter <secret>username</secret> and <secret>password</secret>'
	assert '<secret>username</secret>' in result.text
	assert '<secret>password</secret>' in result.text

	# Case 4: One key empty - empty values are treated as missing
	sensitive_data = {'username': 'user123', 'password': ''}
	result = registry._replace_sensitive_data(params, sensitive_data)
	assert result.text == 'Please enter user123 and <secret>password</secret>'
	assert 'user123' in result.text
	assert '<secret>password</secret>' in result.text  # Empty value's tag remains


def test_simple_domain_specific_sensitive_data(registry, caplog):
	"""Test the basic functionality of domain-specific sensitive data replacement"""
	# Create a simple Pydantic model with sensitive data placeholders
	params = SensitiveParams(text='Please enter <secret>username</secret> and <secret>password</secret>')

	# Simple test with directly instantiable values
	sensitive_data = {
		'example.com': {'username': 'example_user'},
		'other_data': 'non_secret_value',  # Old format mixed with new
	}

	# Without a URL, domain-specific secrets should NOT be exposed
	result = registry._replace_sensitive_data(params, sensitive_data)
	assert result.text == 'Please enter <secret>username</secret> and <secret>password</secret>'
	assert '<secret>username</secret>' in result.text  # Should NOT be replaced without URL
	assert '<secret>password</secret>' in result.text  # Password is missing in sensitive_data
	assert 'example_user' not in result.text  # Domain-specific value should not appear

	# Test with a matching URL - domain-specific secrets should be exposed
	result = registry._replace_sensitive_data(params, sensitive_data, 'https://example.com/login')
	assert result.text == 'Please enter example_user and <secret>password</secret>'
	assert 'example_user' in result.text  # Should be replaced with matching URL
	assert '<secret>password</secret>' in result.text  # Password is still missing
	assert '<secret>username</secret>' not in result.text  # Username tag should be replaced


def test_match_url_with_domain_pattern():
	"""Test that the domain pattern matching utility works correctly"""

	# Test exact domain matches
	assert match_url_with_domain_pattern('https://example.com', 'example.com') is True
	assert match_url_with_domain_pattern('http://example.com', 'example.com') is False  # Default scheme is now https
	assert match_url_with_domain_pattern('https://google.com', 'example.com') is False

	# Test subdomain pattern matches
	assert match_url_with_domain_pattern('https://sub.example.com', '*.example.com') is True
	assert match_url_with_domain_pattern('https://example.com', '*.example.com') is True  # Base domain should match too
	assert match_url_with_domain_pattern('https://sub.sub.example.com', '*.example.com') is True
	assert match_url_with_domain_pattern('https://example.org', '*.example.com') is False

	# Test protocol pattern matches
	assert match_url_with_domain_pattern('https://example.com', 'http*://example.com') is True
	assert match_url_with_domain_pattern('http://example.com', 'http*://example.com') is True
	assert match_url_with_domain_pattern('ftp://example.com', 'http*://example.com') is False

	# Test explicit http protocol
	assert match_url_with_domain_pattern('http://example.com', 'http://example.com') is True
	assert match_url_with_domain_pattern('https://example.com', 'http://example.com') is False

	# Test Chrome extension pattern
	assert match_url_with_domain_pattern('chrome-extension://abcdefghijkl', 'chrome-extension://*') is True
	assert match_url_with_domain_pattern('chrome-extension://mnopqrstuvwx', 'chrome-extension://abcdefghijkl') is False

	# Test new tab page handling
	assert match_url_with_domain_pattern('about:blank', 'example.com') is False
	assert match_url_with_domain_pattern('about:blank', '*://*') is False
	assert match_url_with_domain_pattern('chrome://new-tab-page/', 'example.com') is False
	assert match_url_with_domain_pattern('chrome://new-tab-page/', '*://*') is False
	assert match_url_with_domain_pattern('chrome://new-tab-page', 'example.com') is False
	assert match_url_with_domain_pattern('chrome://new-tab-page', '*://*') is False


def test_unsafe_domain_patterns():
	"""Test that unsafe domain patterns are rejected"""

	# These are unsafe patterns that could match too many domains
	assert match_url_with_domain_pattern('https://evil.com', '*google.com') is False
	assert match_url_with_domain_pattern('https://google.com.evil.com', '*.*.com') is False
	assert match_url_with_domain_pattern('https://google.com', '**google.com') is False
	assert match_url_with_domain_pattern('https://google.com', 'g*e.com') is False
	assert match_url_with_domain_pattern('https://google.com', '*com*') is False

	# Test with patterns that have multiple asterisks in different positions
	assert match_url_with_domain_pattern('https://subdomain.example.com', '*domain*example*') is False
	assert match_url_with_domain_pattern('https://sub.domain.example.com', '*.*.example.com') is False

	# Test patterns with wildcards in TLD part
	assert match_url_with_domain_pattern('https://example.com', 'example.*') is False
	assert match_url_with_domain_pattern('https://example.org', 'example.*') is False


def test_malformed_urls_and_patterns():
	"""Test handling of malformed URLs or patterns"""

	# Malformed URLs
	assert match_url_with_domain_pattern('not-a-url', 'example.com') is False
	assert match_url_with_domain_pattern('http://', 'example.com') is False
	assert match_url_with_domain_pattern('https://', 'example.com') is False
	assert match_url_with_domain_pattern('ftp:/example.com', 'example.com') is False  # Missing slash

	# Empty URLs or patterns
	assert match_url_with_domain_pattern('', 'example.com') is False
	assert match_url_with_domain_pattern('https://example.com', '') is False

	# URLs with no hostname
	assert match_url_with_domain_pattern('file:///path/to/file.txt', 'example.com') is False

	# Invalid pattern formats
	assert match_url_with_domain_pattern('https://example.com', '..example.com') is False
	assert match_url_with_domain_pattern('https://example.com', '.*.example.com') is False
	assert match_url_with_domain_pattern('https://example.com', '**') is False

	# Nested URL attacks in path, query or fragments
	assert match_url_with_domain_pattern('https://example.com/redirect?url=https://evil.com', 'example.com') is True
	assert match_url_with_domain_pattern('https://example.com/path/https://evil.com', 'example.com') is True
	assert match_url_with_domain_pattern('https://example.com#https://evil.com', 'example.com') is True
	# These should match example.com, not evil.com since urlparse extracts the hostname correctly

	# Complex URL obfuscation attempts
	assert match_url_with_domain_pattern('https://example.com/path?next=//evil.com/attack', 'example.com') is True
	assert match_url_with_domain_pattern('https://example.com@evil.com', 'example.com') is False
	assert match_url_with_domain_pattern('https://evil.com?example.com', 'example.com') is False
	assert match_url_with_domain_pattern('https://user:example.com@evil.com', 'example.com') is False
	# urlparse correctly identifies evil.com as the hostname in these cases


def test_url_components():
	"""Test handling of URL components like credentials, ports, fragments, etc."""

	# URLs with credentials (username:password@)
	assert match_url_with_domain_pattern('https://user:pass@example.com', 'example.com') is True
	assert match_url_with_domain_pattern('https://user:pass@example.com', '*.example.com') is True

	# URLs with ports
	assert match_url_with_domain_pattern('https://example.com:8080', 'example.com') is True
	assert match_url_with_domain_pattern('https://example.com:8080', 'example.com:8080') is True  # Port is stripped from pattern

	# URLs with paths
	assert match_url_with_domain_pattern('https://example.com/path/to/page', 'example.com') is True
	assert (
		match_url_with_domain_pattern('https://example.com/path/to/page', 'example.com/path') is False
	)  # Paths in patterns are not supported

	# URLs with query parameters
	assert match_url_with_domain_pattern('https://example.com?param=value', 'example.com') is True

	# URLs with fragments
	assert match_url_with_domain_pattern('https://example.com#section', 'example.com') is True

	# URLs with all components
	assert match_url_with_domain_pattern('https://user:pass@example.com:8080/path?query=val#fragment', 'example.com') is True


def test_filter_sensitive_data(message_manager):
	"""Test that _filter_sensitive_data handles all sensitive data scenarios correctly"""
	# Set up a message with sensitive information
	message = UserMessage(content='My username is admin and password is secret123')

	# Case 1: No sensitive data provided
	message_manager.sensitive_data = None
	result = message_manager._filter_sensitive_data(message)
	assert result.content == 'My username is admin and password is secret123'

	# Case 2: All sensitive data is properly replaced
	message_manager.sensitive_data = {'username': 'admin', 'password': 'secret123'}
	result = message_manager._filter_sensitive_data(message)
	assert '<secret>username</secret>' in result.content
	assert '<secret>password</secret>' in result.content

	# Case 3: Make sure it works with nested content
	nested_message = UserMessage(content=[ContentPartTextParam(text='My username is admin and password is secret123')])
	result = message_manager._filter_sensitive_data(nested_message)
	assert '<secret>username</secret>' in result.content[0].text
	assert '<secret>password</secret>' in result.content[0].text

	# Case 4: Test with empty values
	message_manager.sensitive_data = {'username': 'admin', 'password': ''}
	result = message_manager._filter_sensitive_data(message)
	assert '<secret>username</secret>' in result.content
	# Only username should be replaced since password is empty

	# Case 5: Test with domain-specific sensitive data format
	message_manager.sensitive_data = {
		'example.com': {'username': 'admin', 'password': 'secret123'},
		'google.com': {'email': 'user@example.com', 'password': 'google_pass'},
	}
	# Update the message to include the values we're going to test
	message = UserMessage(content='My username is admin, email is user@example.com and password is secret123 or google_pass')
	result = message_manager._filter_sensitive_data(message)
	# All sensitive values should be replaced regardless of domain
	assert '<secret>username</secret>' in result.content
	assert '<secret>password</secret>' in result.content
	assert '<secret>email</secret>' in result.content


def test_is_new_tab_page():
	"""Test is_new_tab_page function"""
	# Test about:blank
	assert is_new_tab_page('about:blank') is True

	# Test chrome://new-tab-page variations
	assert is_new_tab_page('chrome://new-tab-page/') is True
	assert is_new_tab_page('chrome://new-tab-page') is True

	# Test regular URLs
	assert is_new_tab_page('https://example.com') is False
	assert is_new_tab_page('http://google.com') is False
	assert is_new_tab_page('') is False
	assert is_new_tab_page('chrome://settings') is False

```

---

## backend/browser-use/tests/ci/test_file_system_docx.py

```py
"""Tests for DOCX file support in the FileSystem."""

from pathlib import Path

import pytest

from browser_use.filesystem.file_system import (
	DocxFile,
	FileSystem,
)


class TestDocxFile:
	"""Test DOCX file operations."""

	@pytest.mark.asyncio
	async def test_create_docx_file(self, tmp_path: Path):
		"""Test creating a DOCX file."""
		fs = FileSystem(tmp_path)
		content = """# Heading 1
## Heading 2
### Heading 3
Regular paragraph text.

Another paragraph."""

		result = await fs.write_file('test.docx', content)
		assert 'successfully' in result.lower()
		assert 'test.docx' in fs.list_files()

	@pytest.mark.asyncio
	async def test_read_docx_file_internal(self, tmp_path: Path):
		"""Test reading internal DOCX file."""
		fs = FileSystem(tmp_path)
		content = """# Title
Some content here."""

		await fs.write_file('test.docx', content)
		result = await fs.read_file('test.docx')

		assert 'test.docx' in result
		assert 'Title' in result or 'content' in result

	@pytest.mark.asyncio
	async def test_read_docx_file_external(self, tmp_path: Path):
		"""Test reading external DOCX file."""
		from docx import Document

		# Create an external DOCX file
		external_file = tmp_path / 'external.docx'
		doc = Document()
		doc.add_heading('Test Heading', level=1)
		doc.add_paragraph('Test paragraph content.')
		doc.save(str(external_file))

		fs = FileSystem(tmp_path / 'workspace')
		structured_result = await fs.read_file_structured(str(external_file), external_file=True)

		assert 'message' in structured_result
		assert 'Test Heading' in structured_result['message']
		assert 'Test paragraph content' in structured_result['message']

	def test_docx_file_extension(self):
		"""Test DOCX file extension property."""
		docx_file = DocxFile(name='test')
		assert docx_file.extension == 'docx'
		assert docx_file.full_name == 'test.docx'

	@pytest.mark.asyncio
	async def test_docx_with_unicode_characters(self, tmp_path: Path):
		"""Test DOCX with unicode and emoji content."""
		fs = FileSystem(tmp_path)
		content = """# Unicode Test 🚀
Chinese: 你好世界
Arabic: مرحبا بالعالم
Emoji: 😀 👍 🎉"""

		result = await fs.write_file('unicode.docx', content)
		assert 'successfully' in result.lower()

		read_result = await fs.read_file('unicode.docx')
		assert 'Unicode Test' in read_result
		# Note: Emoji may not be preserved in all systems

	@pytest.mark.asyncio
	async def test_empty_docx_file(self, tmp_path: Path):
		"""Test creating an empty DOCX file."""
		fs = FileSystem(tmp_path)
		result = await fs.write_file('empty.docx', '')
		assert 'successfully' in result.lower()

	@pytest.mark.asyncio
	async def test_large_docx_file(self, tmp_path: Path):
		"""Test creating a large DOCX file."""
		fs = FileSystem(tmp_path)
		# Create content with 1000 lines
		lines = [f'Line {i}: This is a test line with some content.' for i in range(1000)]
		content = '\n'.join(lines)

		result = await fs.write_file('large.docx', content)
		assert 'successfully' in result.lower()

		# Verify it can be read back
		read_result = await fs.read_file('large.docx')
		assert 'Line 0:' in read_result
		assert 'Line 999:' in read_result

	@pytest.mark.asyncio
	async def test_corrupted_docx_file(self, tmp_path: Path):
		"""Test reading a corrupted DOCX file."""
		# Create a corrupted DOCX file
		external_file = tmp_path / 'corrupted.docx'
		external_file.write_bytes(b'This is not a valid DOCX file')

		fs = FileSystem(tmp_path / 'workspace')
		structured_result = await fs.read_file_structured(str(external_file), external_file=True)

		assert 'message' in structured_result
		assert 'error' in structured_result['message'].lower() or 'could not' in structured_result['message'].lower()

	@pytest.mark.asyncio
	async def test_docx_with_multiple_paragraphs(self, tmp_path: Path):
		"""Test DOCX with various paragraph styles."""
		fs = FileSystem(tmp_path)
		content = """# Main Title
## Subtitle
This is a regular paragraph.

This is another paragraph with some text.

### Section 3
Final paragraph here."""

		await fs.write_file('multi.docx', content)
		result = await fs.read_file('multi.docx')

		# Should contain all the text (headings converted to paragraphs)
		assert 'Main Title' in result
		assert 'Subtitle' in result
		assert 'regular paragraph' in result
		assert 'Final paragraph' in result


class TestFileSystemDocxIntegration:
	"""Integration tests for DOCX file type."""

	@pytest.mark.asyncio
	async def test_multiple_file_types_with_docx(self, tmp_path: Path):
		"""Test working with DOCX alongside other file types."""
		fs = FileSystem(tmp_path)

		# Create different file types
		await fs.write_file('doc.docx', '# Document\nContent here')
		await fs.write_file('data.json', '{"key": "value"}')
		await fs.write_file('notes.txt', 'Some notes')

		# Verify all files exist
		files = fs.list_files()
		assert 'doc.docx' in files
		assert 'data.json' in files
		assert 'notes.txt' in files
		assert 'todo.md' in files  # Default file

	@pytest.mark.asyncio
	async def test_file_system_state_with_docx(self, tmp_path: Path):
		"""Test FileSystem state serialization with DOCX files."""
		fs = FileSystem(tmp_path)

		# Create files
		await fs.write_file('test.docx', '# Title\nContent')
		await fs.write_file('data.txt', 'Some text')

		# Get state
		state = fs.get_state()
		assert 'test.docx' in state.files
		assert 'data.txt' in state.files

		# Restore from state
		fs2 = FileSystem.from_state(state)
		assert 'test.docx' in fs2.list_files()
		assert 'data.txt' in fs2.list_files()

	def test_allowed_extensions_include_docx(self, tmp_path: Path):
		"""Test that DOCX is in allowed extensions."""
		fs = FileSystem(tmp_path)
		allowed = fs.get_allowed_extensions()

		assert 'docx' in allowed


if __name__ == '__main__':
	pytest.main([__file__, '-v'])

```

---

## backend/browser-use/tests/ci/test_file_system_images.py

```py
"""Tests for image file support in the FileSystem."""

import base64
import io
from pathlib import Path

import pytest
from PIL import Image

from browser_use.filesystem.file_system import FileSystem


class TestImageFiles:
	"""Test image file operations - only external reading supported."""

	def create_test_image(self, width: int = 100, height: int = 100, format: str = 'PNG') -> bytes:
		"""Create a test image and return bytes."""
		img = Image.new('RGB', (width, height), color='red')
		buffer = io.BytesIO()
		img.save(buffer, format=format)
		buffer.seek(0)
		return buffer.read()

	@pytest.mark.asyncio
	async def test_read_external_png_image(self, tmp_path: Path):
		"""Test reading external PNG image file."""
		# Create an external image file
		external_file = tmp_path / 'test.png'
		img_bytes = self.create_test_image(width=300, height=200, format='PNG')
		external_file.write_bytes(img_bytes)

		fs = FileSystem(tmp_path / 'workspace')
		structured_result = await fs.read_file_structured(str(external_file), external_file=True)

		assert 'message' in structured_result
		assert 'Read image file' in structured_result['message']
		assert 'images' in structured_result
		assert structured_result['images'] is not None
		assert len(structured_result['images']) == 1

		img_data = structured_result['images'][0]
		assert img_data['name'] == 'test.png'
		assert 'data' in img_data
		# Verify base64 is valid
		decoded = base64.b64decode(img_data['data'])
		assert decoded == img_bytes

	@pytest.mark.asyncio
	async def test_read_external_jpg_image(self, tmp_path: Path):
		"""Test reading external JPG image file."""
		# Create an external image file
		external_file = tmp_path / 'photo.jpg'
		img_bytes = self.create_test_image(width=150, height=100, format='JPEG')
		external_file.write_bytes(img_bytes)

		fs = FileSystem(tmp_path / 'workspace')
		structured_result = await fs.read_file_structured(str(external_file), external_file=True)

		assert 'message' in structured_result
		assert 'images' in structured_result
		assert structured_result['images'] is not None

		img_data = structured_result['images'][0]
		assert img_data['name'] == 'photo.jpg'
		decoded = base64.b64decode(img_data['data'])
		assert len(decoded) > 0

	@pytest.mark.asyncio
	async def test_read_jpeg_extension(self, tmp_path: Path):
		"""Test reading .jpeg extension (not just .jpg)."""
		external_file = tmp_path / 'test.jpeg'
		img_bytes = self.create_test_image(format='JPEG')
		external_file.write_bytes(img_bytes)

		fs = FileSystem(tmp_path / 'workspace')
		structured_result = await fs.read_file_structured(str(external_file), external_file=True)

		assert structured_result['images'] is not None
		assert structured_result['images'][0]['name'] == 'test.jpeg'

	@pytest.mark.asyncio
	async def test_read_nonexistent_image(self, tmp_path: Path):
		"""Test reading a nonexistent image file."""
		fs = FileSystem(tmp_path / 'workspace')
		structured_result = await fs.read_file_structured('/path/to/nonexistent.png', external_file=True)

		assert 'message' in structured_result
		assert 'not found' in structured_result['message'].lower()
		assert structured_result['images'] is None

	@pytest.mark.asyncio
	async def test_corrupted_image_file(self, tmp_path: Path):
		"""Test reading a corrupted image file."""
		external_file = tmp_path / 'corrupted.png'
		# Write invalid PNG data
		external_file.write_bytes(b'Not a valid PNG file')

		fs = FileSystem(tmp_path / 'workspace')
		structured_result = await fs.read_file_structured(str(external_file), external_file=True)

		# Should still return base64 data (we don't validate image format)
		assert 'message' in structured_result
		assert 'Read image file' in structured_result['message']
		# Base64 encoding will succeed even for invalid image data
		assert structured_result['images'] is not None

	@pytest.mark.asyncio
	async def test_large_image_file(self, tmp_path: Path):
		"""Test reading a large image file."""
		# Create a large image (2000x2000)
		external_file = tmp_path / 'large.png'
		img = Image.new('RGB', (2000, 2000), color='blue')
		img.save(str(external_file), format='PNG')

		fs = FileSystem(tmp_path / 'workspace')
		structured_result = await fs.read_file_structured(str(external_file), external_file=True)

		assert 'images' in structured_result
		assert structured_result['images'] is not None
		# Verify base64 data is present and substantial
		assert len(structured_result['images'][0]['data']) > 10000

	@pytest.mark.asyncio
	async def test_multiple_images_in_sequence(self, tmp_path: Path):
		"""Test reading multiple images in sequence."""
		fs = FileSystem(tmp_path / 'workspace')

		# Create three different images
		for i, color in enumerate(['red', 'green', 'blue']):
			img_file = tmp_path / f'image_{i}.png'
			img = Image.new('RGB', (100, 100), color=color)
			img.save(str(img_file), format='PNG')

		# Read them all
		results = []
		for i in range(3):
			img_file = tmp_path / f'image_{i}.png'
			result = await fs.read_file_structured(str(img_file), external_file=True)
			results.append(result)

		# Verify all were read successfully
		for i, result in enumerate(results):
			assert result['images'] is not None
			assert result['images'][0]['name'] == f'image_{i}.png'

	@pytest.mark.asyncio
	async def test_different_image_formats(self, tmp_path: Path):
		"""Test reading different image format variations."""
		fs = FileSystem(tmp_path / 'workspace')

		# Test .jpg
		jpg_file = tmp_path / 'test.jpg'
		img = Image.new('RGB', (50, 50), color='yellow')
		img.save(str(jpg_file), format='JPEG')
		result_jpg = await fs.read_file_structured(str(jpg_file), external_file=True)
		assert result_jpg['images'] is not None

		# Test .jpeg
		jpeg_file = tmp_path / 'test.jpeg'
		img.save(str(jpeg_file), format='JPEG')
		result_jpeg = await fs.read_file_structured(str(jpeg_file), external_file=True)
		assert result_jpeg['images'] is not None

		# Test .png
		png_file = tmp_path / 'test.png'
		img.save(str(png_file), format='PNG')
		result_png = await fs.read_file_structured(str(png_file), external_file=True)
		assert result_png['images'] is not None

	@pytest.mark.asyncio
	async def test_image_with_transparency(self, tmp_path: Path):
		"""Test reading PNG with transparency (RGBA)."""
		external_file = tmp_path / 'transparent.png'
		# Create RGBA image with transparency
		img = Image.new('RGBA', (100, 100), color=(255, 0, 0, 128))
		img.save(str(external_file), format='PNG')

		fs = FileSystem(tmp_path / 'workspace')
		structured_result = await fs.read_file_structured(str(external_file), external_file=True)

		assert structured_result['images'] is not None
		assert len(structured_result['images'][0]['data']) > 0


class TestActionResultImages:
	"""Test ActionResult with images field."""

	def test_action_result_with_images(self):
		"""Test creating ActionResult with images."""
		from browser_use.agent.views import ActionResult

		images = [{'name': 'test.png', 'data': 'base64_encoded_data_here'}]

		result = ActionResult(
			extracted_content='Read image file test.png',
			long_term_memory='Read image file test.png',
			images=images,
			include_extracted_content_only_once=True,
		)

		assert result.images is not None
		assert len(result.images) == 1
		assert result.images[0]['name'] == 'test.png'
		assert result.images[0]['data'] == 'base64_encoded_data_here'

	def test_action_result_without_images(self):
		"""Test ActionResult without images (default behavior)."""
		from browser_use.agent.views import ActionResult

		result = ActionResult(extracted_content='Some text', long_term_memory='Memory')

		assert result.images is None

	def test_action_result_with_multiple_images(self):
		"""Test ActionResult with multiple images."""
		from browser_use.agent.views import ActionResult

		images = [
			{'name': 'image1.png', 'data': 'base64_data_1'},
			{'name': 'image2.jpg', 'data': 'base64_data_2'},
		]

		result = ActionResult(
			extracted_content='Read multiple images',
			long_term_memory='Read image files',
			images=images,
			include_extracted_content_only_once=True,
		)

		assert result.images is not None
		assert len(result.images) == 2
		assert result.images[0]['name'] == 'image1.png'
		assert result.images[1]['name'] == 'image2.jpg'

	def test_action_result_with_empty_images_list(self):
		"""Test ActionResult with empty images list."""
		from browser_use.agent.views import ActionResult

		result = ActionResult(
			extracted_content='No images',
			images=[],
		)

		# Empty list is still valid
		assert result.images == []


if __name__ == '__main__':
	pytest.main([__file__, '-v'])

```

---

## backend/browser-use/tests/ci/test_file_system_llm_integration.py

```py
"""Integration tests for DOCX and image file support in LLM messages."""

import base64
import io
from pathlib import Path

import pytest
from PIL import Image

from browser_use.agent.message_manager.service import MessageManager
from browser_use.agent.prompts import AgentMessagePrompt
from browser_use.agent.views import ActionResult, AgentStepInfo
from browser_use.browser.views import BrowserStateSummary, TabInfo
from browser_use.dom.views import SerializedDOMState
from browser_use.filesystem.file_system import FileSystem
from browser_use.llm.messages import ContentPartImageParam, ContentPartTextParam, SystemMessage


class TestImageInLLMMessages:
	"""Test that images flow correctly through to LLM messages."""

	def create_test_image(self, width: int = 100, height: int = 100) -> bytes:
		"""Create a test image and return bytes."""
		img = Image.new('RGB', (width, height), color='red')
		buffer = io.BytesIO()
		img.save(buffer, format='PNG')
		buffer.seek(0)
		return buffer.read()

	@pytest.mark.asyncio
	async def test_image_stored_in_message_manager(self, tmp_path: Path):
		"""Test that images are stored in MessageManager state."""
		fs = FileSystem(tmp_path)
		system_message = SystemMessage(content='Test system message')
		mm = MessageManager(task='test', system_message=system_message, file_system=fs)

		# Create ActionResult with images
		images = [{'name': 'test.png', 'data': 'base64_test_data'}]
		action_results = [
			ActionResult(
				extracted_content='Read image file test.png',
				long_term_memory='Read image file test.png',
				images=images,
				include_extracted_content_only_once=True,
			)
		]

		# Update message manager with results
		step_info = AgentStepInfo(step_number=1, max_steps=10)
		mm._update_agent_history_description(model_output=None, result=action_results, step_info=step_info)

		# Verify images are stored
		assert mm.state.read_state_images is not None
		assert len(mm.state.read_state_images) == 1
		assert mm.state.read_state_images[0]['name'] == 'test.png'
		assert mm.state.read_state_images[0]['data'] == 'base64_test_data'

	@pytest.mark.asyncio
	async def test_images_cleared_after_step(self, tmp_path: Path):
		"""Test that images are cleared after each step."""
		fs = FileSystem(tmp_path)
		system_message = SystemMessage(content='Test system message')
		mm = MessageManager(task='test', system_message=system_message, file_system=fs)

		# First step with images
		images = [{'name': 'test.png', 'data': 'base64_data'}]
		action_results = [ActionResult(images=images, include_extracted_content_only_once=True)]
		step_info = AgentStepInfo(step_number=1, max_steps=10)
		mm._update_agent_history_description(model_output=None, result=action_results, step_info=step_info)

		assert len(mm.state.read_state_images) == 1

		# Second step without images - should clear
		action_results_2 = [ActionResult(extracted_content='No images')]
		step_info_2 = AgentStepInfo(step_number=2, max_steps=10)
		mm._update_agent_history_description(model_output=None, result=action_results_2, step_info=step_info_2)

		assert len(mm.state.read_state_images) == 0

	@pytest.mark.asyncio
	async def test_multiple_images_accumulated(self, tmp_path: Path):
		"""Test that multiple images in one step are accumulated."""
		fs = FileSystem(tmp_path)
		system_message = SystemMessage(content='Test system message')
		mm = MessageManager(task='test', system_message=system_message, file_system=fs)

		# Multiple action results with images
		action_results = [
			ActionResult(images=[{'name': 'img1.png', 'data': 'data1'}], include_extracted_content_only_once=True),
			ActionResult(images=[{'name': 'img2.jpg', 'data': 'data2'}], include_extracted_content_only_once=True),
		]
		step_info = AgentStepInfo(step_number=1, max_steps=10)
		mm._update_agent_history_description(model_output=None, result=action_results, step_info=step_info)

		assert len(mm.state.read_state_images) == 2
		assert mm.state.read_state_images[0]['name'] == 'img1.png'
		assert mm.state.read_state_images[1]['name'] == 'img2.jpg'

	def test_agent_message_prompt_includes_images(self, tmp_path: Path):
		"""Test that AgentMessagePrompt includes images in message content."""
		fs = FileSystem(tmp_path)

		# Create browser state
		browser_state = BrowserStateSummary(
			url='https://example.com',
			title='Test',
			tabs=[TabInfo(target_id='test-0', url='https://example.com', title='Test')],
			screenshot=None,
			dom_state=SerializedDOMState(_root=None, selector_map={}),
		)

		# Create images
		read_state_images = [{'name': 'test.png', 'data': 'base64_image_data_here'}]

		# Create message prompt
		prompt = AgentMessagePrompt(
			browser_state_summary=browser_state,
			file_system=fs,
			read_state_images=read_state_images,
		)

		# Get user message with vision enabled
		user_message = prompt.get_user_message(use_vision=True)

		# Verify message has content parts (not just string)
		assert isinstance(user_message.content, list)

		# Find image content parts
		image_parts = [part for part in user_message.content if isinstance(part, ContentPartImageParam)]
		text_parts = [part for part in user_message.content if isinstance(part, ContentPartTextParam)]

		# Should have at least one image
		assert len(image_parts) >= 1

		# Should have text label
		image_labels = [part.text for part in text_parts if 'test.png' in part.text]
		assert len(image_labels) >= 1

		# Verify image data URL format
		img_part = image_parts[0]
		assert 'data:image/' in img_part.image_url.url
		assert 'base64,base64_image_data_here' in img_part.image_url.url

	def test_agent_message_prompt_png_vs_jpg_media_type(self, tmp_path: Path):
		"""Test that AgentMessagePrompt correctly detects PNG vs JPG media types."""
		fs = FileSystem(tmp_path)

		browser_state = BrowserStateSummary(
			url='https://example.com',
			title='Test',
			tabs=[TabInfo(target_id='test-0', url='https://example.com', title='Test')],
			screenshot=None,
			dom_state=SerializedDOMState(_root=None, selector_map={}),
		)

		# Test PNG
		read_state_images_png = [{'name': 'test.png', 'data': 'data'}]
		prompt_png = AgentMessagePrompt(
			browser_state_summary=browser_state,
			file_system=fs,
			read_state_images=read_state_images_png,
		)
		message_png = prompt_png.get_user_message(use_vision=True)
		image_parts_png = [part for part in message_png.content if isinstance(part, ContentPartImageParam)]
		assert 'data:image/png;base64' in image_parts_png[0].image_url.url

		# Test JPG
		read_state_images_jpg = [{'name': 'photo.jpg', 'data': 'data'}]
		prompt_jpg = AgentMessagePrompt(
			browser_state_summary=browser_state,
			file_system=fs,
			read_state_images=read_state_images_jpg,
		)
		message_jpg = prompt_jpg.get_user_message(use_vision=True)
		image_parts_jpg = [part for part in message_jpg.content if isinstance(part, ContentPartImageParam)]
		assert 'data:image/jpeg;base64' in image_parts_jpg[0].image_url.url

	def test_agent_message_prompt_no_images(self, tmp_path: Path):
		"""Test that message works correctly when no images are present."""
		fs = FileSystem(tmp_path)

		browser_state = BrowserStateSummary(
			url='https://example.com',
			title='Test',
			tabs=[TabInfo(target_id='test-0', url='https://example.com', title='Test')],
			screenshot=None,
			dom_state=SerializedDOMState(_root=None, selector_map={}),
		)

		# No images
		prompt = AgentMessagePrompt(
			browser_state_summary=browser_state,
			file_system=fs,
			read_state_images=[],
		)

		# Get user message without vision
		user_message = prompt.get_user_message(use_vision=False)

		# Should be plain text, not content parts
		assert isinstance(user_message.content, str)

	def test_agent_message_prompt_empty_base64_skipped(self, tmp_path: Path):
		"""Test that images with empty base64 data are skipped."""
		fs = FileSystem(tmp_path)

		browser_state = BrowserStateSummary(
			url='https://example.com',
			title='Test',
			tabs=[TabInfo(target_id='test-0', url='https://example.com', title='Test')],
			screenshot=None,
			dom_state=SerializedDOMState(_root=None, selector_map={}),
		)

		# Image with empty data field
		read_state_images = [
			{'name': 'empty.png', 'data': ''},  # Empty - should be skipped
			{'name': 'valid.png', 'data': 'valid_data'},  # Valid
		]

		prompt = AgentMessagePrompt(
			browser_state_summary=browser_state,
			file_system=fs,
			read_state_images=read_state_images,
		)

		user_message = prompt.get_user_message(use_vision=True)
		image_parts = [part for part in user_message.content if isinstance(part, ContentPartImageParam)]

		# Should only have 1 image (the valid one)
		assert len(image_parts) == 1
		assert 'valid_data' in image_parts[0].image_url.url


class TestDocxInLLMMessages:
	"""Test that DOCX content flows correctly through to LLM messages."""

	@pytest.mark.asyncio
	async def test_docx_in_extracted_content(self, tmp_path: Path):
		"""Test that DOCX text appears in extracted_content."""
		fs = FileSystem(tmp_path)

		# Create DOCX file
		content = """# Title
Some important content here."""
		await fs.write_file('test.docx', content)

		# Read it
		result = await fs.read_file('test.docx')

		# Verify content is in the result
		assert 'Title' in result
		assert 'important content' in result

	@pytest.mark.asyncio
	async def test_docx_in_message_manager(self, tmp_path: Path):
		"""Test that DOCX content appears in message manager state."""
		fs = FileSystem(tmp_path)
		system_message = SystemMessage(content='Test system message')
		mm = MessageManager(task='test', system_message=system_message, file_system=fs)

		# Simulate read_file action result
		docx_content = """Read from file test.docx.
<content>
Title
Some content here.
</content>"""

		action_results = [
			ActionResult(
				extracted_content=docx_content,
				long_term_memory='Read file test.docx',
				include_extracted_content_only_once=True,
			)
		]

		step_info = AgentStepInfo(step_number=1, max_steps=10)
		mm._update_agent_history_description(model_output=None, result=action_results, step_info=step_info)

		# Verify it's in read_state_description
		assert 'Title' in mm.state.read_state_description
		assert 'Some content' in mm.state.read_state_description


class TestEndToEndIntegration:
	"""End-to-end tests for file reading and LLM message creation."""

	def create_test_image(self) -> bytes:
		"""Create a test image."""
		img = Image.new('RGB', (50, 50), color='blue')
		buffer = io.BytesIO()
		img.save(buffer, format='PNG')
		buffer.seek(0)
		return buffer.read()

	@pytest.mark.asyncio
	async def test_image_end_to_end(self, tmp_path: Path):
		"""Test complete flow: external image → FileSystem → ActionResult → MessageManager → Prompt."""
		# Step 1: Create external image
		external_file = tmp_path / 'photo.png'
		img_bytes = self.create_test_image()
		external_file.write_bytes(img_bytes)

		# Step 2: Read via FileSystem
		fs = FileSystem(tmp_path / 'workspace')
		structured_result = await fs.read_file_structured(str(external_file), external_file=True)

		assert structured_result['images'] is not None

		# Step 3: Create ActionResult (simulating tools/service.py)
		action_result = ActionResult(
			extracted_content=structured_result['message'],
			long_term_memory='Read image file photo.png',
			images=structured_result['images'],
			include_extracted_content_only_once=True,
		)

		# Step 4: Process in MessageManager
		system_message = SystemMessage(content='Test system message')
		mm = MessageManager(task='test', system_message=system_message, file_system=fs)
		step_info = AgentStepInfo(step_number=1, max_steps=10)
		mm._update_agent_history_description(model_output=None, result=[action_result], step_info=step_info)

		# Verify images stored
		assert len(mm.state.read_state_images) == 1
		assert mm.state.read_state_images[0]['name'] == 'photo.png'

		# Step 5: Create message with AgentMessagePrompt
		browser_state = BrowserStateSummary(
			url='https://example.com',
			title='Test',
			tabs=[TabInfo(target_id='test-0', url='https://example.com', title='Test')],
			screenshot=None,
			dom_state=SerializedDOMState(_root=None, selector_map={}),
		)

		prompt = AgentMessagePrompt(
			browser_state_summary=browser_state,
			file_system=fs,
			read_state_images=mm.state.read_state_images,
		)

		user_message = prompt.get_user_message(use_vision=True)

		# Verify image is in message
		assert isinstance(user_message.content, list)
		image_parts = [part for part in user_message.content if isinstance(part, ContentPartImageParam)]
		assert len(image_parts) >= 1

		# Verify image data is correct
		base64_str = base64.b64encode(img_bytes).decode('utf-8')
		assert base64_str in image_parts[0].image_url.url

	@pytest.mark.asyncio
	async def test_docx_end_to_end(self, tmp_path: Path):
		"""Test complete flow: DOCX file → FileSystem → ActionResult → MessageManager."""
		# Step 1: Create DOCX
		fs = FileSystem(tmp_path)
		docx_content = """# Important Document
This is critical information."""

		await fs.write_file('important.docx', docx_content)

		# Step 2: Read it
		read_result = await fs.read_file('important.docx')

		# Step 3: Create ActionResult (simulating tools/service.py)
		action_result = ActionResult(
			extracted_content=read_result,
			long_term_memory=read_result[:100] if len(read_result) > 100 else read_result,
			include_extracted_content_only_once=True,
		)

		# Step 4: Process in MessageManager
		system_message = SystemMessage(content='Test system message')
		mm = MessageManager(task='test', system_message=system_message, file_system=fs)
		step_info = AgentStepInfo(step_number=1, max_steps=10)
		mm._update_agent_history_description(model_output=None, result=[action_result], step_info=step_info)

		# Verify content is in read_state
		assert 'Important Document' in mm.state.read_state_description
		assert 'critical information' in mm.state.read_state_description


if __name__ == '__main__':
	pytest.main([__file__, '-v'])

```

---

## backend/browser-use/tests/ci/test_history_wait_time.py

```py
from browser_use.agent.views import StepMetadata


def test_step_metadata_has_step_interval_field():
	"""Test that StepMetadata includes step_interval field"""
	metadata = StepMetadata(step_number=1, step_start_time=10.0, step_end_time=12.5, step_interval=2.5)

	assert hasattr(metadata, 'step_interval')
	assert metadata.step_interval == 2.5


def test_step_metadata_step_interval_optional():
	"""Test that step_interval is optional (None for first step)"""
	# Explicitly None
	metadata_none = StepMetadata(step_number=0, step_start_time=0.0, step_end_time=1.0, step_interval=None)
	assert metadata_none.step_interval is None

	# Omitted (defaults to None)
	metadata_default = StepMetadata(step_number=0, step_start_time=0.0, step_end_time=1.0)
	assert metadata_default.step_interval is None


def test_step_interval_calculation():
	"""Test step_interval calculation logic (uses previous step's duration)"""
	# Previous step (Step 1): runs from 100.0 to 102.5 (duration: 2.5s)
	previous_start = 100.0
	previous_end = 102.5
	previous_duration = previous_end - previous_start

	# Current step (Step 2): should have step_interval = previous step's duration
	# This tells the rerun system "wait 2.5s before executing Step 2"
	expected_step_interval = previous_duration
	calculated_step_interval = max(0, previous_end - previous_start)

	assert abs(calculated_step_interval - expected_step_interval) < 0.001  # Float comparison
	assert calculated_step_interval == 2.5


def test_step_metadata_serialization_with_step_interval():
	"""Test that step_interval is included in metadata serialization"""
	# With step_interval
	metadata_with_wait = StepMetadata(step_number=1, step_start_time=10.0, step_end_time=12.5, step_interval=2.5)

	data = metadata_with_wait.model_dump()
	assert 'step_interval' in data
	assert data['step_interval'] == 2.5

	# Without step_interval (None)
	metadata_without_wait = StepMetadata(step_number=0, step_start_time=0.0, step_end_time=1.0, step_interval=None)

	data = metadata_without_wait.model_dump()
	assert 'step_interval' in data
	assert data['step_interval'] is None


def test_step_metadata_deserialization_with_step_interval():
	"""Test that step_interval can be loaded from dict"""
	# Load with step_interval
	data_with_wait = {'step_number': 1, 'step_start_time': 10.0, 'step_end_time': 12.5, 'step_interval': 2.5}

	metadata = StepMetadata.model_validate(data_with_wait)
	assert metadata.step_interval == 2.5

	# Load without step_interval (old format)
	data_without_wait = {
		'step_number': 0,
		'step_start_time': 0.0,
		'step_end_time': 1.0,
		# step_interval is missing
	}

	metadata = StepMetadata.model_validate(data_without_wait)
	assert metadata.step_interval is None  # Defaults to None


def test_step_interval_backwards_compatibility():
	"""Test that old metadata without step_interval still works"""
	# Simulate old format from JSON
	old_metadata_dict = {
		'step_number': 0,
		'step_start_time': 1000.0,
		'step_end_time': 1002.5,
		# step_interval field doesn't exist (old format)
	}

	# Should load successfully with step_interval defaulting to None
	metadata = StepMetadata.model_validate(old_metadata_dict)

	assert metadata.step_number == 0
	assert metadata.step_start_time == 1000.0
	assert metadata.step_end_time == 1002.5
	assert metadata.step_interval is None  # Default value


def test_duration_seconds_property_still_works():
	"""Test that existing duration_seconds property still works"""
	metadata = StepMetadata(step_number=1, step_start_time=10.0, step_end_time=13.5, step_interval=2.0)

	# duration_seconds should be 3.5 (13.5 - 10.0)
	assert metadata.duration_seconds == 3.5

	# step_interval is separate from duration
	assert metadata.step_interval == 2.0


def test_step_metadata_json_round_trip():
	"""Test that step_interval survives JSON serialization round-trip"""
	metadata = StepMetadata(step_number=1, step_start_time=100.0, step_end_time=102.5, step_interval=1.5)

	# Serialize to JSON
	json_str = metadata.model_dump_json()

	# Deserialize from JSON
	loaded = StepMetadata.model_validate_json(json_str)

	assert loaded.step_interval == 1.5
	assert loaded.step_number == 1
	assert loaded.step_start_time == 100.0
	assert loaded.step_end_time == 102.5

```

---

## backend/browser-use/tests/ci/test_llm_retries.py

```py
"""
Test retry logic with exponential backoff for LLM clients.
"""

import time
from unittest.mock import AsyncMock, MagicMock, patch

import httpx
import pytest


class TestChatBrowserUseRetries:
	"""Test retry logic for ChatBrowserUse."""

	@pytest.fixture
	def mock_env(self, monkeypatch):
		"""Set up environment for ChatBrowserUse."""
		monkeypatch.setenv('BROWSER_USE_API_KEY', 'test-api-key')

	@pytest.mark.asyncio
	async def test_retries_on_503_with_exponential_backoff(self, mock_env):
		"""Test that 503 errors trigger retries with exponential backoff."""
		from browser_use.llm.browser_use.chat import ChatBrowserUse
		from browser_use.llm.messages import UserMessage

		# Track timing of each attempt
		attempt_times: list[float] = []
		attempt_count = 0

		async def mock_post(*args, **kwargs):
			nonlocal attempt_count
			attempt_times.append(time.monotonic())
			attempt_count += 1

			if attempt_count < 3:
				# First 2 attempts fail with 503
				response = MagicMock()
				response.status_code = 503
				response.json.return_value = {'detail': 'Service temporarily unavailable'}
				raise httpx.HTTPStatusError('503', request=MagicMock(), response=response)
			else:
				# Third attempt succeeds
				response = MagicMock()
				response.json.return_value = {
					'completion': 'Success!',
					'usage': {
						'prompt_tokens': 10,
						'completion_tokens': 5,
						'total_tokens': 15,
						'prompt_cached_tokens': None,
						'prompt_cache_creation_tokens': None,
						'prompt_image_tokens': None,
					},
				}
				response.raise_for_status = MagicMock()
				return response

		with patch('httpx.AsyncClient') as mock_client_class:
			mock_client = AsyncMock()
			mock_client.post = mock_post
			mock_client.__aenter__ = AsyncMock(return_value=mock_client)
			mock_client.__aexit__ = AsyncMock(return_value=None)
			mock_client_class.return_value = mock_client

			# Use short delays for testing
			client = ChatBrowserUse(retry_base_delay=0.1, retry_max_delay=1.0)
			result = await client.ainvoke([UserMessage(content='test')])

		# Should have made 3 attempts
		assert attempt_count == 3
		assert result.completion == 'Success!'

		# Verify exponential backoff timing (with some tolerance for test execution)
		# First retry: ~0.1s, Second retry: ~0.2s
		delay_1 = attempt_times[1] - attempt_times[0]
		delay_2 = attempt_times[2] - attempt_times[1]

		# Allow 50% tolerance for timing
		assert 0.05 <= delay_1 <= 0.2, f'First delay {delay_1:.3f}s not in expected range'
		assert 0.1 <= delay_2 <= 0.4, f'Second delay {delay_2:.3f}s not in expected range'
		# Second delay should be roughly 2x the first (exponential)
		assert delay_2 > delay_1, 'Second delay should be longer than first (exponential backoff)'

	@pytest.mark.asyncio
	async def test_no_retry_on_401(self, mock_env):
		"""Test that 401 errors do NOT trigger retries."""
		from browser_use.llm.browser_use.chat import ChatBrowserUse
		from browser_use.llm.messages import UserMessage

		attempt_count = 0

		async def mock_post(*args, **kwargs):
			nonlocal attempt_count
			attempt_count += 1
			response = MagicMock()
			response.status_code = 401
			response.json.return_value = {'detail': 'Invalid API key'}
			raise httpx.HTTPStatusError('401', request=MagicMock(), response=response)

		with patch('httpx.AsyncClient') as mock_client_class:
			mock_client = AsyncMock()
			mock_client.post = mock_post
			mock_client.__aenter__ = AsyncMock(return_value=mock_client)
			mock_client.__aexit__ = AsyncMock(return_value=None)
			mock_client_class.return_value = mock_client

			client = ChatBrowserUse(retry_base_delay=0.01)

			with pytest.raises(ValueError, match='Invalid API key'):
				await client.ainvoke([UserMessage(content='test')])

		# Should only attempt once (no retries for 401)
		assert attempt_count == 1

	@pytest.mark.asyncio
	async def test_retries_on_timeout(self, mock_env):
		"""Test that timeouts trigger retries."""
		from browser_use.llm.browser_use.chat import ChatBrowserUse
		from browser_use.llm.messages import UserMessage

		attempt_count = 0

		async def mock_post(*args, **kwargs):
			nonlocal attempt_count
			attempt_count += 1
			if attempt_count < 2:
				raise httpx.TimeoutException('Request timed out')
			# Second attempt succeeds (with no usage data to test None handling)
			response = MagicMock()
			response.json.return_value = {'completion': 'Success after timeout!', 'usage': None}
			response.raise_for_status = MagicMock()
			return response

		with patch('httpx.AsyncClient') as mock_client_class:
			mock_client = AsyncMock()
			mock_client.post = mock_post
			mock_client.__aenter__ = AsyncMock(return_value=mock_client)
			mock_client.__aexit__ = AsyncMock(return_value=None)
			mock_client_class.return_value = mock_client

			client = ChatBrowserUse(retry_base_delay=0.01)
			result = await client.ainvoke([UserMessage(content='test')])

		assert attempt_count == 2
		assert result.completion == 'Success after timeout!'

	@pytest.mark.asyncio
	async def test_max_retries_exhausted(self, mock_env):
		"""Test that error is raised after max retries exhausted."""
		from browser_use.llm.browser_use.chat import ChatBrowserUse
		from browser_use.llm.messages import UserMessage

		attempt_count = 0

		async def mock_post(*args, **kwargs):
			nonlocal attempt_count
			attempt_count += 1
			response = MagicMock()
			response.status_code = 503
			response.json.return_value = {'detail': 'Service unavailable'}
			raise httpx.HTTPStatusError('503', request=MagicMock(), response=response)

		with patch('httpx.AsyncClient') as mock_client_class:
			mock_client = AsyncMock()
			mock_client.post = mock_post
			mock_client.__aenter__ = AsyncMock(return_value=mock_client)
			mock_client.__aexit__ = AsyncMock(return_value=None)
			mock_client_class.return_value = mock_client

			client = ChatBrowserUse(max_retries=3, retry_base_delay=0.01)

			with pytest.raises(ValueError, match='API request failed'):
				await client.ainvoke([UserMessage(content='test')])

		# Should have attempted max_retries times
		assert attempt_count == 3


class TestChatGoogleRetries:
	"""Test retry logic for ChatGoogle."""

	@pytest.fixture
	def mock_env(self, monkeypatch):
		"""Set up environment for ChatGoogle."""
		monkeypatch.setenv('GOOGLE_API_KEY', 'test-api-key')

	@pytest.mark.asyncio
	async def test_retries_on_503_with_exponential_backoff(self, mock_env):
		"""Test that 503 errors trigger retries with exponential backoff."""
		from browser_use.llm.exceptions import ModelProviderError
		from browser_use.llm.google.chat import ChatGoogle
		from browser_use.llm.messages import UserMessage

		attempt_times: list[float] = []
		attempt_count = 0

		# Mock the genai client
		with patch('browser_use.llm.google.chat.genai') as mock_genai:
			mock_client = MagicMock()
			mock_genai.Client.return_value = mock_client

			async def mock_generate(*args, **kwargs):
				nonlocal attempt_count
				attempt_times.append(time.monotonic())
				attempt_count += 1

				if attempt_count < 3:
					raise ModelProviderError(message='Service unavailable', status_code=503, model='gemini-2.0-flash')
				else:
					# Success on third attempt
					mock_response = MagicMock()
					mock_response.text = 'Success!'
					mock_response.usage_metadata = MagicMock(
						prompt_token_count=10, candidates_token_count=5, total_token_count=15, cached_content_token_count=0
					)
					mock_response.candidates = [MagicMock(content=MagicMock(parts=[MagicMock(text='Success!')]))]
					return mock_response

			# Mock the aio.models.generate_content path
			mock_client.aio.models.generate_content = mock_generate

			client = ChatGoogle(model='gemini-2.0-flash', api_key='test', retry_base_delay=0.1, retry_max_delay=1.0)
			result = await client.ainvoke([UserMessage(content='test')])

		assert attempt_count == 3
		assert result.completion == 'Success!'

		# Verify exponential backoff
		delay_1 = attempt_times[1] - attempt_times[0]
		delay_2 = attempt_times[2] - attempt_times[1]

		assert 0.05 <= delay_1 <= 0.2, f'First delay {delay_1:.3f}s not in expected range'
		assert 0.1 <= delay_2 <= 0.4, f'Second delay {delay_2:.3f}s not in expected range'
		assert delay_2 > delay_1, 'Second delay should be longer than first'

	@pytest.mark.asyncio
	async def test_no_retry_on_400(self, mock_env):
		"""Test that 400 errors do NOT trigger retries."""
		from browser_use.llm.exceptions import ModelProviderError
		from browser_use.llm.google.chat import ChatGoogle
		from browser_use.llm.messages import UserMessage

		attempt_count = 0

		with patch('browser_use.llm.google.chat.genai') as mock_genai:
			mock_client = MagicMock()
			mock_genai.Client.return_value = mock_client

			async def mock_generate(*args, **kwargs):
				nonlocal attempt_count
				attempt_count += 1
				raise ModelProviderError(message='Bad request', status_code=400, model='gemini-2.0-flash')

			mock_client.aio.models.generate_content = mock_generate

			client = ChatGoogle(model='gemini-2.0-flash', api_key='test', retry_base_delay=0.01)

			with pytest.raises(ModelProviderError):
				await client.ainvoke([UserMessage(content='test')])

		# Should only attempt once (400 is not retryable)
		assert attempt_count == 1

	@pytest.mark.asyncio
	async def test_retries_on_429_rate_limit(self, mock_env):
		"""Test that 429 rate limit errors trigger retries."""
		from browser_use.llm.exceptions import ModelProviderError
		from browser_use.llm.google.chat import ChatGoogle
		from browser_use.llm.messages import UserMessage

		attempt_count = 0

		with patch('browser_use.llm.google.chat.genai') as mock_genai:
			mock_client = MagicMock()
			mock_genai.Client.return_value = mock_client

			async def mock_generate(*args, **kwargs):
				nonlocal attempt_count
				attempt_count += 1

				if attempt_count < 2:
					raise ModelProviderError(message='Rate limit exceeded', status_code=429, model='gemini-2.0-flash')
				else:
					mock_response = MagicMock()
					mock_response.text = 'Success after rate limit!'
					mock_response.usage_metadata = MagicMock(
						prompt_token_count=10, candidates_token_count=5, total_token_count=15, cached_content_token_count=0
					)
					mock_response.candidates = [MagicMock(content=MagicMock(parts=[MagicMock(text='Success after rate limit!')]))]
					return mock_response

			mock_client.aio.models.generate_content = mock_generate

			client = ChatGoogle(model='gemini-2.0-flash', api_key='test', retry_base_delay=0.01)
			result = await client.ainvoke([UserMessage(content='test')])

		assert attempt_count == 2
		assert result.completion == 'Success after rate limit!'


if __name__ == '__main__':
	pytest.main([__file__, '-v'])

```

---

## backend/browser-use/tests/ci/test_rerun_ai_summary.py

```py
"""Tests for AI summary generation during rerun"""

from unittest.mock import AsyncMock

from browser_use.agent.service import Agent
from browser_use.agent.views import ActionResult, RerunSummaryAction
from tests.ci.conftest import create_mock_llm


async def test_generate_rerun_summary_success():
	"""Test that _generate_rerun_summary generates an AI summary for successful rerun"""
	# Create mock LLM that returns RerunSummaryAction
	summary_action = RerunSummaryAction(
		summary='Form filled successfully',
		success=True,
		completion_status='complete',
	)

	async def custom_ainvoke(*args, **kwargs):
		# Get output_format from second positional arg or kwargs
		output_format = args[1] if len(args) > 1 else kwargs.get('output_format')
		assert output_format is RerunSummaryAction
		from browser_use.llm.views import ChatInvokeCompletion

		return ChatInvokeCompletion(completion=summary_action, usage=None)

	# Mock ChatOpenAI class
	mock_openai = AsyncMock()
	mock_openai.ainvoke.side_effect = custom_ainvoke

	llm = create_mock_llm(actions=None)
	agent = Agent(task='Test task', llm=llm)
	await agent.browser_session.start()

	try:
		# Create some successful results
		results = [
			ActionResult(long_term_memory='Step 1 completed'),
			ActionResult(long_term_memory='Step 2 completed'),
		]

		# Pass the mock LLM directly as summary_llm
		summary = await agent._generate_rerun_summary('Test task', results, summary_llm=mock_openai)

		# Check that result is the AI summary
		assert summary.is_done is True
		assert summary.success is True
		assert summary.extracted_content == 'Form filled successfully'
		assert 'Rerun completed' in (summary.long_term_memory or '')

	finally:
		await agent.close()


async def test_generate_rerun_summary_with_errors():
	"""Test that AI summary correctly reflects errors in execution"""
	# Create mock LLM for summary
	summary_action = RerunSummaryAction(
		summary='Rerun had errors',
		success=False,
		completion_status='failed',
	)

	async def custom_ainvoke(*args, **kwargs):
		output_format = args[1] if len(args) > 1 else kwargs.get('output_format')
		assert output_format is RerunSummaryAction
		from browser_use.llm.views import ChatInvokeCompletion

		return ChatInvokeCompletion(completion=summary_action, usage=None)

	mock_openai = AsyncMock()
	mock_openai.ainvoke.side_effect = custom_ainvoke

	llm = create_mock_llm(actions=None)
	agent = Agent(task='Test task', llm=llm)
	await agent.browser_session.start()

	try:
		# Create results with errors
		results_with_errors = [
			ActionResult(error='Failed to find element'),
			ActionResult(error='Timeout'),
		]

		# Pass the mock LLM directly as summary_llm
		summary = await agent._generate_rerun_summary('Test task', results_with_errors, summary_llm=mock_openai)

		# Verify summary reflects errors
		assert summary.is_done is True
		assert summary.success is False
		assert summary.extracted_content == 'Rerun had errors'

	finally:
		await agent.close()


async def test_generate_rerun_summary_fallback_on_error():
	"""Test that a fallback summary is generated if LLM fails"""
	# Mock ChatOpenAI to throw an error
	mock_openai = AsyncMock()
	mock_openai.ainvoke.side_effect = Exception('LLM service unavailable')

	llm = create_mock_llm(actions=None)
	agent = Agent(task='Test task', llm=llm)
	await agent.browser_session.start()

	try:
		# Create some results
		results = [
			ActionResult(long_term_memory='Step 1 completed'),
			ActionResult(long_term_memory='Step 2 completed'),
		]

		# Pass the mock LLM directly as summary_llm
		summary = await agent._generate_rerun_summary('Test task', results, summary_llm=mock_openai)

		# Verify fallback summary
		assert summary.is_done is True
		assert summary.success is True  # No errors, so success=True
		assert 'Rerun completed' in (summary.extracted_content or '')
		assert '2/2' in (summary.extracted_content or '')  # Should show stats

	finally:
		await agent.close()


async def test_generate_rerun_summary_statistics():
	"""Test that summary includes execution statistics in the prompt"""
	# Create mock LLM
	summary_action = RerunSummaryAction(
		summary='3 of 5 steps succeeded',
		success=False,
		completion_status='partial',
	)

	async def custom_ainvoke(*args, **kwargs):
		output_format = args[1] if len(args) > 1 else kwargs.get('output_format')
		assert output_format is RerunSummaryAction
		from browser_use.llm.views import ChatInvokeCompletion

		return ChatInvokeCompletion(completion=summary_action, usage=None)

	mock_openai = AsyncMock()
	mock_openai.ainvoke.side_effect = custom_ainvoke

	llm = create_mock_llm(actions=None)
	agent = Agent(task='Test task', llm=llm)
	await agent.browser_session.start()

	try:
		# Create results with mix of success and errors
		results = [
			ActionResult(long_term_memory='Step 1 completed'),
			ActionResult(error='Step 2 failed'),
			ActionResult(long_term_memory='Step 3 completed'),
			ActionResult(error='Step 4 failed'),
			ActionResult(long_term_memory='Step 5 completed'),
		]

		# Pass the mock LLM directly as summary_llm
		summary = await agent._generate_rerun_summary('Test task', results, summary_llm=mock_openai)

		# Verify summary
		assert summary.is_done is True
		assert summary.success is False  # partial completion
		assert '3 of 5' in (summary.extracted_content or '')

	finally:
		await agent.close()

```

---

## backend/browser-use/tests/ci/test_screenshot_exclusion.py

```py
"""Test that screenshot action is excluded when use_vision != 'auto'."""

import pytest

from browser_use.agent.service import Agent
from browser_use.browser.profile import BrowserProfile
from browser_use.browser.session import BrowserSession
from browser_use.tools.service import Tools
from tests.ci.conftest import create_mock_llm


@pytest.fixture(scope='function')
async def browser_session():
	session = BrowserSession(browser_profile=BrowserProfile(headless=True))
	await session.start()
	yield session
	await session.kill()


def test_screenshot_excluded_with_use_vision_false():
	"""Test that screenshot action is excluded when use_vision=False."""
	mock_llm = create_mock_llm(actions=['{"action": [{"done": {"text": "test", "success": true}}]}'])

	agent = Agent(
		task='test',
		llm=mock_llm,
		use_vision=False,
	)

	# Verify screenshot is not in the registry
	assert 'screenshot' not in agent.tools.registry.registry.actions, 'Screenshot should be excluded when use_vision=False'


def test_screenshot_excluded_with_use_vision_true():
	"""Test that screenshot action is excluded when use_vision=True."""
	mock_llm = create_mock_llm(actions=['{"action": [{"done": {"text": "test", "success": true}}]}'])

	agent = Agent(
		task='test',
		llm=mock_llm,
		use_vision=True,
	)

	# Verify screenshot is not in the registry
	assert 'screenshot' not in agent.tools.registry.registry.actions, 'Screenshot should be excluded when use_vision=True'


def test_screenshot_included_with_use_vision_auto():
	"""Test that screenshot action is included when use_vision='auto'."""
	mock_llm = create_mock_llm(actions=['{"action": [{"done": {"text": "test", "success": true}}]}'])

	agent = Agent(
		task='test',
		llm=mock_llm,
		use_vision='auto',
	)

	# Verify screenshot IS in the registry
	assert 'screenshot' in agent.tools.registry.registry.actions, 'Screenshot should be included when use_vision="auto"'


def test_screenshot_excluded_with_custom_tools_and_use_vision_false():
	"""Test that screenshot action is excluded even when user passes custom tools and use_vision=False.

	This is the critical test case that verifies the fix:
	When users pass their own Tools instance with screenshot included,
	the Agent should still enforce the exclusion if use_vision != 'auto'.
	"""
	mock_llm = create_mock_llm(actions=['{"action": [{"done": {"text": "test", "success": true}}]}'])

	# Create custom tools that includes screenshot action
	custom_tools = Tools()
	assert 'screenshot' in custom_tools.registry.registry.actions, 'Custom tools should have screenshot by default'

	# Pass custom tools to agent with use_vision=False
	agent = Agent(
		task='test',
		llm=mock_llm,
		tools=custom_tools,
		use_vision=False,
	)

	# Verify screenshot is excluded even though user passed custom tools
	assert 'screenshot' not in agent.tools.registry.registry.actions, (
		'Screenshot should be excluded when use_vision=False, even with custom tools'
	)


def test_screenshot_excluded_with_custom_tools_and_use_vision_true():
	"""Test that screenshot action is excluded even when user passes custom tools and use_vision=True.

	This is another critical test case:
	When users pass their own Tools instance with screenshot included,
	the Agent should still enforce the exclusion if use_vision != 'auto'.
	"""
	mock_llm = create_mock_llm(actions=['{"action": [{"done": {"text": "test", "success": true}}]}'])

	# Create custom tools - by default Tools() includes screenshot
	# (unless exclude_actions is passed)
	custom_tools = Tools()
	# Note: We check if screenshot exists in the default set, but it might not
	# exist if use_vision defaults have changed. The key is that after passing
	# to Agent with use_vision=True, it should be excluded.
	has_screenshot_before = 'screenshot' in custom_tools.registry.registry.actions

	# Pass custom tools to agent with use_vision=True
	agent = Agent(
		task='test',
		llm=mock_llm,
		tools=custom_tools,
		use_vision=True,
	)

	# Verify screenshot is excluded even though user passed custom tools
	# The key test: screenshot should be excluded after Agent init
	assert 'screenshot' not in agent.tools.registry.registry.actions, (
		f'Screenshot should be excluded when use_vision=True, even with custom tools (had screenshot before: {has_screenshot_before})'
	)


def test_screenshot_included_with_custom_tools_and_use_vision_auto():
	"""Test that screenshot action is kept when user passes custom tools and use_vision='auto'."""
	mock_llm = create_mock_llm(actions=['{"action": [{"done": {"text": "test", "success": true}}]}'])

	# Create custom tools that includes screenshot action
	custom_tools = Tools()
	assert 'screenshot' in custom_tools.registry.registry.actions, 'Custom tools should have screenshot by default'

	# Pass custom tools to agent with use_vision='auto'
	agent = Agent(
		task='test',
		llm=mock_llm,
		tools=custom_tools,
		use_vision='auto',
	)

	# Verify screenshot is kept when use_vision='auto'
	assert 'screenshot' in agent.tools.registry.registry.actions, (
		'Screenshot should be included when use_vision="auto", even with custom tools'
	)


def test_tools_exclude_action_method():
	"""Test the Tools.exclude_action() method directly."""
	tools = Tools()

	# Verify screenshot is included initially
	assert 'screenshot' in tools.registry.registry.actions, 'Screenshot should be included by default'

	# Exclude screenshot
	tools.exclude_action('screenshot')

	# Verify screenshot is excluded
	assert 'screenshot' not in tools.registry.registry.actions, 'Screenshot should be excluded after calling exclude_action()'
	assert 'screenshot' in tools.registry.exclude_actions, 'Screenshot should be in exclude_actions list'


def test_exclude_action_prevents_re_registration():
	"""Test that excluded actions cannot be re-registered."""
	tools = Tools()

	# Exclude screenshot
	tools.exclude_action('screenshot')
	assert 'screenshot' not in tools.registry.registry.actions

	# Try to re-register screenshot (simulating what happens in __init__)
	# The decorator should skip registration since it's in exclude_actions
	@tools.registry.action('Test screenshot action')
	async def screenshot():
		return 'test'

	# Verify it was not re-registered
	assert 'screenshot' not in tools.registry.registry.actions, 'Excluded action should not be re-registered'

```

---

## backend/browser-use/tests/ci/test_tools.py

```py
import asyncio
import tempfile
import time

import pytest
from pydantic import BaseModel
from pytest_httpserver import HTTPServer

from browser_use.agent.views import ActionResult
from browser_use.browser import BrowserSession
from browser_use.browser.profile import BrowserProfile
from browser_use.filesystem.file_system import FileSystem
from browser_use.tools.service import Tools


@pytest.fixture(scope='session')
def http_server():
	"""Create and provide a test HTTP server that serves static content."""
	server = HTTPServer()
	server.start()

	# Add routes for common test pages
	server.expect_request('/').respond_with_data(
		'<html><head><title>Test Home Page</title></head><body><h1>Test Home Page</h1><p>Welcome to the test site</p></body></html>',
		content_type='text/html',
	)

	server.expect_request('/page1').respond_with_data(
		'<html><head><title>Test Page 1</title></head><body><h1>Test Page 1</h1><p>This is test page 1</p></body></html>',
		content_type='text/html',
	)

	server.expect_request('/page2').respond_with_data(
		'<html><head><title>Test Page 2</title></head><body><h1>Test Page 2</h1><p>This is test page 2</p></body></html>',
		content_type='text/html',
	)

	server.expect_request('/search').respond_with_data(
		"""
		<html>
		<head><title>Search Results</title></head>
		<body>
			<h1>Search Results</h1>
			<div class="results">
				<div class="result">Result 1</div>
				<div class="result">Result 2</div>
				<div class="result">Result 3</div>
			</div>
		</body>
		</html>
		""",
		content_type='text/html',
	)

	yield server
	server.stop()


@pytest.fixture(scope='session')
def base_url(http_server):
	"""Return the base URL for the test HTTP server."""
	return f'http://{http_server.host}:{http_server.port}'


@pytest.fixture(scope='module')
async def browser_session():
	"""Create and provide a Browser instance with security disabled."""
	browser_session = BrowserSession(
		browser_profile=BrowserProfile(
			headless=True,
			user_data_dir=None,
			keep_alive=True,
		)
	)
	await browser_session.start()
	yield browser_session
	await browser_session.kill()


@pytest.fixture(scope='function')
def tools():
	"""Create and provide a Tools instance."""
	return Tools()


class TestToolsIntegration:
	"""Integration tests for Tools using actual browser instances."""

	async def test_registry_actions(self, tools, browser_session):
		"""Test that the registry contains the expected default actions."""
		# Check that common actions are registered
		common_actions = [
			'navigate',
			'search',
			'click',
			'input',
			'scroll',
			'go_back',
			'switch',
			'close',
			'wait',
		]

		for action in common_actions:
			assert action in tools.registry.registry.actions
			assert tools.registry.registry.actions[action].function is not None
			assert tools.registry.registry.actions[action].description is not None

	async def test_custom_action_registration(self, tools, browser_session, base_url):
		"""Test registering a custom action and executing it."""

		# Define a custom action
		class CustomParams(BaseModel):
			text: str

		@tools.action('Test custom action', param_model=CustomParams)
		async def custom_action(params: CustomParams, browser_session):
			current_url = await browser_session.get_current_page_url()
			return ActionResult(extracted_content=f'Custom action executed with: {params.text} on {current_url}')

		# Navigate to a page first
		await tools.navigate(url=f'{base_url}/page1', new_tab=False, browser_session=browser_session)

		# Execute the custom action directly
		result = await tools.custom_action(text='test_value', browser_session=browser_session)

		# Verify the result
		assert isinstance(result, ActionResult)
		assert result.extracted_content is not None
		assert 'Custom action executed with: test_value on' in result.extracted_content
		assert f'{base_url}/page1' in result.extracted_content

	async def test_wait_action(self, tools, browser_session):
		"""Test that the wait action correctly waits for the specified duration."""

		# verify that it's in the default action set
		wait_action = None
		for action_name, action in tools.registry.registry.actions.items():
			if 'wait' in action_name.lower() and 'seconds' in str(action.param_model.model_fields):
				wait_action = action
				break
		assert wait_action is not None, 'Could not find wait action in tools'

		# Check that it has seconds parameter with default
		assert 'seconds' in wait_action.param_model.model_fields
		schema = wait_action.param_model.model_json_schema()
		assert schema['properties']['seconds']['default'] == 3

		# Record start time
		start_time = time.time()

		# Execute wait action
		result = await tools.wait(seconds=3, browser_session=browser_session)

		# Record end time
		end_time = time.time()

		# Verify the result
		assert isinstance(result, ActionResult)
		assert result.extracted_content is not None
		assert 'Waited for' in result.extracted_content or 'Waiting for' in result.extracted_content

		# Verify that approximately 1 second has passed (allowing some margin)
		assert end_time - start_time <= 2.5  # We wait 3-1 seconds for LLM call

		# longer wait
		# Record start time
		start_time = time.time()

		# Execute wait action
		result = await tools.wait(seconds=5, browser_session=browser_session)

		# Record end time
		end_time = time.time()

		# Verify the result
		assert isinstance(result, ActionResult)
		assert result.extracted_content is not None
		assert 'Waited for' in result.extracted_content or 'Waiting for' in result.extracted_content

		assert 3.5 <= end_time - start_time <= 4.5  # We wait 5-1 seconds for LLM call

	async def test_go_back_action(self, tools, browser_session, base_url):
		"""Test that go_back action navigates to the previous page."""
		# Navigate to first page
		await tools.navigate(url=f'{base_url}/page1', new_tab=False, browser_session=browser_session)

		# Store the first page URL
		first_url = await browser_session.get_current_page_url()
		print(f'First page URL: {first_url}')

		# Navigate to second page
		await tools.navigate(url=f'{base_url}/page2', new_tab=False, browser_session=browser_session)

		# Verify we're on the second page
		second_url = await browser_session.get_current_page_url()
		print(f'Second page URL: {second_url}')
		assert f'{base_url}/page2' in second_url

		# Execute go back action
		result = await tools.go_back(browser_session=browser_session)

		# Verify the result
		assert isinstance(result, ActionResult)
		assert result.extracted_content is not None
		assert 'Navigated back' in result.extracted_content

		# Add another delay to allow the navigation to complete
		await asyncio.sleep(1)

		# Verify we're back on a different page than before
		final_url = await browser_session.get_current_page_url()
		print(f'Final page URL after going back: {final_url}')

		# Try to verify we're back on the first page, but don't fail the test if not
		assert f'{base_url}/page1' in final_url, f'Expected to return to page1 but got {final_url}'

	async def test_navigation_chain(self, tools, browser_session, base_url):
		"""Test navigating through multiple pages and back through history."""
		# Set up a chain of navigation: Home -> Page1 -> Page2
		urls = [f'{base_url}/', f'{base_url}/page1', f'{base_url}/page2']

		# Navigate to each page in sequence
		for url in urls:
			await tools.navigate(url=url, new_tab=False, browser_session=browser_session)

			# Verify current page
			current_url = await browser_session.get_current_page_url()
			assert url in current_url

		# Go back twice and verify each step
		for expected_url in reversed(urls[:-1]):
			await tools.go_back(browser_session=browser_session)
			await asyncio.sleep(1)  # Wait for navigation to complete

			current_url = await browser_session.get_current_page_url()
			assert expected_url in current_url

	async def test_excluded_actions(self, browser_session):
		"""Test that excluded actions are not registered."""
		# Create tools with excluded actions
		excluded_tools = Tools(exclude_actions=['search', 'scroll'])

		# Verify excluded actions are not in the registry
		assert 'search' not in excluded_tools.registry.registry.actions
		assert 'scroll' not in excluded_tools.registry.registry.actions

		# But other actions are still there
		assert 'navigate' in excluded_tools.registry.registry.actions
		assert 'click' in excluded_tools.registry.registry.actions

	async def test_search_action(self, tools, browser_session, base_url):
		"""Test the search action."""

		await browser_session.get_current_page_url()

		# Execute search action - it will actually navigate to our search results page
		result = await tools.search(query='Python web automation', browser_session=browser_session)

		# Verify the result
		assert isinstance(result, ActionResult)
		assert result.extracted_content is not None
		assert 'Searched' in result.extracted_content and 'Python web automation' in result.extracted_content

		# For our test purposes, we just verify we're on some URL
		current_url = await browser_session.get_current_page_url()
		assert current_url is not None and 'Python' in current_url

	async def test_done_action(self, tools, browser_session, base_url):
		"""Test that DoneAction completes a task and reports success or failure."""
		# Create a temporary directory for the file system
		with tempfile.TemporaryDirectory() as temp_dir:
			file_system = FileSystem(temp_dir)

			# First navigate to a page
			await tools.navigate(url=f'{base_url}/page1', new_tab=False, browser_session=browser_session)

			success_done_message = 'Successfully completed task'

			# Execute done action with file_system
			result = await tools.done(
				text=success_done_message, success=True, browser_session=browser_session, file_system=file_system
			)

			# Verify the result
			assert isinstance(result, ActionResult)
			assert result.extracted_content is not None
			assert success_done_message in result.extracted_content
			assert result.success is True
			assert result.is_done is True
			assert result.error is None

			failed_done_message = 'Failed to complete task'

			# Execute failed done action with file_system
			result = await tools.done(
				text=failed_done_message, success=False, browser_session=browser_session, file_system=file_system
			)

			# Verify the result
			assert isinstance(result, ActionResult)
			assert result.extracted_content is not None
			assert failed_done_message in result.extracted_content
			assert result.success is False
			assert result.is_done is True
			assert result.error is None

	async def test_get_dropdown_options(self, tools, browser_session, base_url, http_server):
		"""Test that get_dropdown_options correctly retrieves options from a dropdown."""
		# Add route for dropdown test page
		http_server.expect_request('/dropdown1').respond_with_data(
			"""
			<!DOCTYPE html>
			<html>
			<head>
				<title>Dropdown Test</title>
			</head>
			<body>
				<h1>Dropdown Test</h1>
				<select id="test-dropdown" name="test-dropdown">
					<option value="">Please select</option>
					<option value="option1">First Option</option>
					<option value="option2">Second Option</option>
					<option value="option3">Third Option</option>
				</select>
			</body>
			</html>
			""",
			content_type='text/html',
		)

		# Navigate to the dropdown test page
		await tools.navigate(url=f'{base_url}/dropdown1', new_tab=False, browser_session=browser_session)

		# Wait for the page to load using CDP
		cdp_session = await browser_session.get_or_create_cdp_session()
		assert cdp_session is not None, 'CDP session not initialized'

		# Wait for page load by checking document ready state
		await asyncio.sleep(0.5)  # Brief wait for navigation to start
		ready_state = await cdp_session.cdp_client.send.Runtime.evaluate(
			params={'expression': 'document.readyState'}, session_id=cdp_session.session_id
		)
		# If not complete, wait a bit more
		if ready_state.get('result', {}).get('value') != 'complete':
			await asyncio.sleep(1.0)

		# Initialize the DOM state to populate the selector map
		await browser_session.get_browser_state_summary()

		# Get the selector map
		selector_map = await browser_session.get_selector_map()

		# Find the dropdown element in the selector map
		dropdown_index = None
		for idx, element in selector_map.items():
			if element.tag_name.lower() == 'select':
				dropdown_index = idx
				break

		assert dropdown_index is not None, (
			f'Could not find select element in selector map. Available elements: {[f"{idx}: {element.tag_name}" for idx, element in selector_map.items()]}'
		)

		# Execute the action with the dropdown index
		result = await tools.dropdown_options(index=dropdown_index, browser_session=browser_session)

		expected_options = [
			{'index': 0, 'text': 'Please select', 'value': ''},
			{'index': 1, 'text': 'First Option', 'value': 'option1'},
			{'index': 2, 'text': 'Second Option', 'value': 'option2'},
			{'index': 3, 'text': 'Third Option', 'value': 'option3'},
		]

		# Verify the result structure
		assert isinstance(result, ActionResult)

		# Core logic validation: Verify all options are returned
		assert result.extracted_content is not None
		for option in expected_options[1:]:  # Skip the placeholder option
			assert option['text'] in result.extracted_content, f"Option '{option['text']}' not found in result content"

		# Verify the instruction for using the text in select_dropdown is included
		assert 'Use the exact text or value string' in result.extracted_content and 'select_dropdown' in result.extracted_content

		# Verify the actual dropdown options in the DOM using CDP
		dropdown_options_result = await cdp_session.cdp_client.send.Runtime.evaluate(
			params={
				'expression': """
					JSON.stringify((() => {
						const select = document.getElementById('test-dropdown');
						return Array.from(select.options).map(opt => ({
							text: opt.text,
							value: opt.value
						}));
					})())
				""",
				'returnByValue': True,
			},
			session_id=cdp_session.session_id,
		)
		dropdown_options_json = dropdown_options_result.get('result', {}).get('value', '[]')
		import json

		dropdown_options = json.loads(dropdown_options_json) if isinstance(dropdown_options_json, str) else dropdown_options_json

		# Verify the dropdown has the expected options
		assert len(dropdown_options) == len(expected_options), (
			f'Expected {len(expected_options)} options, got {len(dropdown_options)}'
		)
		for i, expected in enumerate(expected_options):
			actual = dropdown_options[i]
			assert actual['text'] == expected['text'], (
				f"Option at index {i} has wrong text: expected '{expected['text']}', got '{actual['text']}'"
			)
			assert actual['value'] == expected['value'], (
				f"Option at index {i} has wrong value: expected '{expected['value']}', got '{actual['value']}'"
			)

	async def test_select_dropdown_option(self, tools, browser_session, base_url, http_server):
		"""Test that select_dropdown_option correctly selects an option from a dropdown."""
		# Add route for dropdown test page
		http_server.expect_request('/dropdown2').respond_with_data(
			"""
			<!DOCTYPE html>
			<html>
			<head>
				<title>Dropdown Test</title>
			</head>
			<body>
				<h1>Dropdown Test</h1>
				<select id="test-dropdown" name="test-dropdown">
					<option value="">Please select</option>
					<option value="option1">First Option</option>
					<option value="option2">Second Option</option>
					<option value="option3">Third Option</option>
				</select>
			</body>
			</html>
			""",
			content_type='text/html',
		)

		# Navigate to the dropdown test page
		await tools.navigate(url=f'{base_url}/dropdown2', new_tab=False, browser_session=browser_session)

		# Wait for the page to load using CDP
		cdp_session = await browser_session.get_or_create_cdp_session()
		assert cdp_session is not None, 'CDP session not initialized'

		# Wait for page load by checking document ready state
		await asyncio.sleep(0.5)  # Brief wait for navigation to start
		ready_state = await cdp_session.cdp_client.send.Runtime.evaluate(
			params={'expression': 'document.readyState'}, session_id=cdp_session.session_id
		)
		# If not complete, wait a bit more
		if ready_state.get('result', {}).get('value') != 'complete':
			await asyncio.sleep(1.0)

		# populate the selector map with highlight indices
		await browser_session.get_browser_state_summary()

		# Now get the selector map which should contain our dropdown
		selector_map = await browser_session.get_selector_map()

		# Find the dropdown element in the selector map
		dropdown_index = None
		for idx, element in selector_map.items():
			if element.tag_name.lower() == 'select':
				dropdown_index = idx
				break

		assert dropdown_index is not None, (
			f'Could not find select element in selector map. Available elements: {[f"{idx}: {element.tag_name}" for idx, element in selector_map.items()]}'
		)

		# Execute the action with the dropdown index
		result = await tools.select_dropdown(index=dropdown_index, text='Second Option', browser_session=browser_session)

		# Verify the result structure
		assert isinstance(result, ActionResult)

		# Core logic validation: Verify selection was successful
		assert result.extracted_content is not None
		assert 'selected option' in result.extracted_content.lower()
		assert 'Second Option' in result.extracted_content

		# Verify the actual dropdown selection was made by checking the DOM using CDP
		selected_value_result = await cdp_session.cdp_client.send.Runtime.evaluate(
			params={'expression': "document.getElementById('test-dropdown').value"}, session_id=cdp_session.session_id
		)
		selected_value = selected_value_result.get('result', {}).get('value')
		assert selected_value == 'option2'  # Second Option has value "option2"

```

---

## backend/browser-use/tests/ci/test_variable_detection.py

```py
"""Unit tests for variable detection in agent history"""

from browser_use.agent.variable_detector import (
	_detect_from_attributes,
	_detect_from_value_pattern,
	_detect_variable_type,
	_ensure_unique_name,
	detect_variables_in_history,
)
from browser_use.agent.views import DetectedVariable
from browser_use.dom.views import DOMInteractedElement, NodeType


def create_test_element(attributes: dict[str, str] | None = None) -> DOMInteractedElement:
	"""Helper to create a DOMInteractedElement for testing"""
	return DOMInteractedElement(
		node_id=1,
		backend_node_id=1,
		frame_id='frame1',
		node_type=NodeType.ELEMENT_NODE,
		node_value='',
		node_name='input',
		attributes=attributes or {},
		bounds=None,
		x_path='//*[@id="test"]',
		element_hash=12345,
	)


def create_mock_history(actions_with_elements: list[tuple[dict, DOMInteractedElement | None]]):
	"""Helper to create mock history for testing"""
	from types import SimpleNamespace

	history_items = []
	for action_dict, element in actions_with_elements:
		mock_action = SimpleNamespace(**action_dict)
		mock_output = SimpleNamespace(action=[mock_action])
		mock_state = SimpleNamespace(interacted_element=[element] if element else None)
		mock_history_item = SimpleNamespace(model_output=mock_output, state=mock_state)
		history_items.append(mock_history_item)

	return SimpleNamespace(history=history_items)


def test_detect_email_from_attributes():
	"""Test email detection via type='email' attribute"""
	attributes = {'type': 'email', 'id': 'email-input'}
	result = _detect_from_attributes(attributes)

	assert result is not None
	var_name, var_format = result
	assert var_name == 'email'
	assert var_format == 'email'


def test_detect_email_from_pattern():
	"""Test email detection via pattern matching"""
	result = _detect_from_value_pattern('test@example.com')

	assert result is not None
	var_name, var_format = result
	assert var_name == 'email'
	assert var_format == 'email'


def test_detect_phone_from_attributes():
	"""Test phone detection via type='tel' attribute"""
	attributes = {'type': 'tel', 'name': 'phone'}
	result = _detect_from_attributes(attributes)

	assert result is not None
	var_name, var_format = result
	assert var_name == 'phone'
	assert var_format == 'phone'


def test_detect_phone_from_pattern():
	"""Test phone detection via pattern matching"""
	result = _detect_from_value_pattern('+1 (555) 123-4567')

	assert result is not None
	var_name, var_format = result
	assert var_name == 'phone'
	assert var_format == 'phone'


def test_detect_date_from_attributes():
	"""Test date detection via type='date' attribute"""
	attributes = {'type': 'date', 'id': 'dob'}
	result = _detect_from_attributes(attributes)

	assert result is not None
	var_name, var_format = result
	assert var_name == 'date'
	assert var_format == 'date'


def test_detect_date_from_pattern():
	"""Test date detection via YYYY-MM-DD pattern"""
	result = _detect_from_value_pattern('1990-01-01')

	assert result is not None
	var_name, var_format = result
	assert var_name == 'date'
	assert var_format == 'date'


def test_detect_first_name_from_attributes():
	"""Test first name detection from element attributes"""
	attributes = {'name': 'first_name', 'placeholder': 'Enter your first name'}
	result = _detect_from_attributes(attributes)

	assert result is not None
	var_name, var_format = result
	assert var_name == 'first_name'
	assert var_format is None


def test_detect_first_name_from_pattern():
	"""Test first name detection from pattern (single capitalized word)"""
	result = _detect_from_value_pattern('John')

	assert result is not None
	var_name, var_format = result
	assert var_name == 'first_name'
	assert var_format is None


def test_detect_full_name_from_pattern():
	"""Test full name detection from pattern (two capitalized words)"""
	result = _detect_from_value_pattern('John Doe')

	assert result is not None
	var_name, var_format = result
	assert var_name == 'full_name'
	assert var_format is None


def test_detect_address_from_attributes():
	"""Test address detection from element attributes"""
	attributes = {'name': 'street_address', 'id': 'address-input'}
	result = _detect_from_attributes(attributes)

	assert result is not None
	var_name, var_format = result
	assert var_name == 'address'
	assert var_format is None


def test_detect_billing_address_from_attributes():
	"""Test billing address detection from element attributes"""
	attributes = {'name': 'billing_address', 'placeholder': 'Billing street address'}
	result = _detect_from_attributes(attributes)

	assert result is not None
	var_name, var_format = result
	assert var_name == 'billing_address'
	assert var_format is None


def test_detect_comment_from_attributes():
	"""Test comment detection from element attributes"""
	attributes = {'name': 'comment', 'placeholder': 'Enter your comment'}
	result = _detect_from_attributes(attributes)

	assert result is not None
	var_name, var_format = result
	assert var_name == 'comment'
	assert var_format is None


def test_detect_city_from_attributes():
	"""Test city detection from element attributes"""
	attributes = {'name': 'city', 'id': 'city-input'}
	result = _detect_from_attributes(attributes)

	assert result is not None
	var_name, var_format = result
	assert var_name == 'city'
	assert var_format is None


def test_detect_state_from_attributes():
	"""Test state detection from element attributes"""
	attributes = {'name': 'state', 'aria-label': 'State or Province'}
	result = _detect_from_attributes(attributes)

	assert result is not None
	var_name, var_format = result
	assert var_name == 'state'
	assert var_format is None


def test_detect_country_from_attributes():
	"""Test country detection from element attributes"""
	attributes = {'name': 'country', 'id': 'country-select'}
	result = _detect_from_attributes(attributes)

	assert result is not None
	var_name, var_format = result
	assert var_name == 'country'
	assert var_format is None


def test_detect_zip_code_from_attributes():
	"""Test zip code detection from element attributes"""
	attributes = {'name': 'zip_code', 'placeholder': 'Zip or postal code'}
	result = _detect_from_attributes(attributes)

	assert result is not None
	var_name, var_format = result
	assert var_name == 'zip_code'
	assert var_format == 'postal_code'


def test_detect_company_from_attributes():
	"""Test company detection from element attributes"""
	attributes = {'name': 'company', 'id': 'company-input'}
	result = _detect_from_attributes(attributes)

	assert result is not None
	var_name, var_format = result
	assert var_name == 'company'
	assert var_format is None


def test_detect_number_from_pattern():
	"""Test number detection from pattern (pure digits)"""
	result = _detect_from_value_pattern('12345')

	assert result is not None
	var_name, var_format = result
	assert var_name == 'number'
	assert var_format == 'number'


def test_no_detection_for_random_text():
	"""Test that random text is not detected as a variable"""
	result = _detect_from_value_pattern('some random text that is not a variable')
	assert result is None


def test_no_detection_for_short_text():
	"""Test that very short text is not detected"""
	result = _detect_from_value_pattern('a')
	assert result is None


def test_element_attributes_take_priority_over_pattern():
	"""Test that element attributes are checked before pattern matching"""
	# A value that could match pattern (capitalized name)
	value = 'Test'

	# Element with explicit type="email"
	element = create_test_element(attributes={'type': 'email', 'id': 'email-input'})

	result = _detect_variable_type(value, element)

	assert result is not None
	var_name, var_format = result
	# Should detect as email (from attributes), not first_name (from pattern)
	assert var_name == 'email'
	assert var_format == 'email'


def test_pattern_matching_used_when_no_element():
	"""Test that pattern matching is used when element context is missing"""
	value = 'test@example.com'

	result = _detect_variable_type(value, element=None)

	assert result is not None
	var_name, var_format = result
	assert var_name == 'email'
	assert var_format == 'email'


def test_ensure_unique_name_no_conflict():
	"""Test unique name generation with no conflicts"""
	existing = {}
	result = _ensure_unique_name('email', existing)
	assert result == 'email'


def test_ensure_unique_name_with_conflict():
	"""Test unique name generation with conflicts"""
	existing = {
		'email': DetectedVariable(name='email', original_value='test1@example.com'),
	}
	result = _ensure_unique_name('email', existing)
	assert result == 'email_2'


def test_ensure_unique_name_with_multiple_conflicts():
	"""Test unique name generation with multiple conflicts"""
	existing = {
		'email': DetectedVariable(name='email', original_value='test1@example.com'),
		'email_2': DetectedVariable(name='email_2', original_value='test2@example.com'),
	}
	result = _ensure_unique_name('email', existing)
	assert result == 'email_3'


def test_detect_variables_in_empty_history():
	"""Test variable detection in empty history"""
	from types import SimpleNamespace

	history = SimpleNamespace(history=[])

	result = detect_variables_in_history(history)  # type: ignore[arg-type]

	assert result == {}


def test_detect_variables_in_history_with_input_action():
	"""Test variable detection in history with input action"""
	# Use mock objects to avoid Pydantic validation issues
	from types import SimpleNamespace

	# Create mock history structure
	element = create_test_element(attributes={'type': 'email', 'id': 'email-input'})

	mock_action = SimpleNamespace(**{'input': {'index': 1, 'text': 'test@example.com'}})
	mock_output = SimpleNamespace(action=[mock_action])
	mock_state = SimpleNamespace(interacted_element=[element])
	mock_history_item = SimpleNamespace(model_output=mock_output, state=mock_state)
	mock_history = SimpleNamespace(history=[mock_history_item])

	result = detect_variables_in_history(mock_history)  # type: ignore[arg-type]

	assert len(result) == 1
	assert 'email' in result
	assert result['email'].original_value == 'test@example.com'
	assert result['email'].format == 'email'


def test_detect_variables_skips_duplicate_values():
	"""Test that duplicate values are only detected once"""
	# Create history with same value entered twice
	element = create_test_element(attributes={'type': 'email'})
	history = create_mock_history(
		[
			({'input': {'index': 1, 'text': 'test@example.com'}}, element),
			({'input': {'index': 2, 'text': 'test@example.com'}}, element),
		]
	)

	result = detect_variables_in_history(history)  # type: ignore[arg-type]

	# Should only detect one variable, not two
	assert len(result) == 1
	assert 'email' in result


def test_detect_variables_handles_missing_state():
	"""Test that detection works when state is missing"""
	from types import SimpleNamespace

	# Create history with None state
	mock_action = SimpleNamespace(**{'input': {'index': 1, 'text': 'test@example.com'}})
	mock_output = SimpleNamespace(action=[mock_action])
	mock_history_item = SimpleNamespace(model_output=mock_output, state=None)
	history = SimpleNamespace(history=[mock_history_item])

	result = detect_variables_in_history(history)  # type: ignore[arg-type]

	# Should still detect via pattern matching
	assert len(result) == 1
	assert 'email' in result
	assert result['email'].original_value == 'test@example.com'


def test_detect_variables_handles_missing_interacted_element():
	"""Test that detection works when interacted_element is missing"""
	# Use None as element to test when interacted_element is None
	history = create_mock_history(
		[
			({'input': {'index': 1, 'text': 'test@example.com'}}, None),
		]
	)

	result = detect_variables_in_history(history)  # type: ignore[arg-type]

	# Should still detect via pattern matching
	assert len(result) == 1
	assert 'email' in result


def test_detect_variables_multiple_types():
	"""Test detection of multiple variable types in one history"""
	history = create_mock_history(
		[
			({'input': {'index': 1, 'text': 'test@example.com'}}, create_test_element(attributes={'type': 'email'})),
			({'input': {'index': 2, 'text': 'John'}}, create_test_element(attributes={'name': 'first_name'})),
			({'input': {'index': 3, 'text': '1990-01-01'}}, create_test_element(attributes={'type': 'date'})),
		]
	)

	result = detect_variables_in_history(history)  # type: ignore[arg-type]

	assert len(result) == 3
	assert 'email' in result
	assert 'first_name' in result
	assert 'date' in result

	assert result['email'].original_value == 'test@example.com'
	assert result['first_name'].original_value == 'John'
	assert result['date'].original_value == '1990-01-01'

```

---

## backend/browser-use/tests/ci/test_variable_substitution.py

```py
"""Unit tests for variable substitution in agent history"""

from types import SimpleNamespace

from browser_use.agent.service import Agent
from browser_use.dom.views import DOMInteractedElement, NodeType


def create_test_element(attributes: dict[str, str] | None = None) -> DOMInteractedElement:
	"""Helper to create a DOMInteractedElement for testing"""
	return DOMInteractedElement(
		node_id=1,
		backend_node_id=1,
		frame_id='frame1',
		node_type=NodeType.ELEMENT_NODE,
		node_value='',
		node_name='input',
		attributes=attributes or {},
		bounds=None,
		x_path='//*[@id="test"]',
		element_hash=12345,
	)


def create_mock_history(actions_with_elements: list[tuple[dict, DOMInteractedElement | None]]):
	"""Helper to create mock history for testing"""
	history_items = []
	for action_dict, element in actions_with_elements:
		mock_action = SimpleNamespace(**action_dict)
		mock_output = SimpleNamespace(action=[mock_action])
		mock_state = SimpleNamespace(interacted_element=[element] if element else None)
		mock_history_item = SimpleNamespace(model_output=mock_output, state=mock_state)
		history_items.append(mock_history_item)

	return SimpleNamespace(history=history_items)


def test_substitute_single_variable(mock_llm):
	"""Test substitution of a single variable"""
	agent = Agent(task='test', llm=mock_llm)

	# Create mock history with email
	element = create_test_element(attributes={'type': 'email'})
	history = create_mock_history(
		[
			({'input': {'index': 1, 'text': 'old@example.com'}}, element),
		]
	)

	# Substitute the email
	modified_history = agent._substitute_variables_in_history(
		history,  # type: ignore[arg-type]
		{'email': 'new@example.com'},
	)

	# Check that the value was substituted
	action = modified_history.history[0].model_output.action[0]  # type: ignore[attr-defined]
	action_dict = vars(action)
	assert action_dict['input']['text'] == 'new@example.com'


def test_substitute_multiple_variables(mock_llm):
	"""Test substitution of multiple variables"""
	agent = Agent(task='test', llm=mock_llm)

	# Create mock history with email and name
	history = create_mock_history(
		[
			({'input': {'index': 1, 'text': 'old@example.com'}}, create_test_element(attributes={'type': 'email'})),
			({'input': {'index': 2, 'text': 'John'}}, create_test_element(attributes={'name': 'first_name'})),
			({'input': {'index': 3, 'text': '1990-01-01'}}, create_test_element(attributes={'type': 'date'})),
		]
	)

	# Substitute all variables
	modified_history = agent._substitute_variables_in_history(
		history,  # type: ignore[arg-type]
		{
			'email': 'new@example.com',
			'first_name': 'Jane',
			'date': '1995-05-15',
		},
	)

	# Check that all values were substituted
	action1 = modified_history.history[0].model_output.action[0]  # type: ignore[attr-defined]
	action2 = modified_history.history[1].model_output.action[0]  # type: ignore[attr-defined]
	action3 = modified_history.history[2].model_output.action[0]  # type: ignore[attr-defined]

	assert vars(action1)['input']['text'] == 'new@example.com'
	assert vars(action2)['input']['text'] == 'Jane'
	assert vars(action3)['input']['text'] == '1995-05-15'


def test_substitute_partial_variables(mock_llm):
	"""Test substitution of only some variables"""
	agent = Agent(task='test', llm=mock_llm)

	# Create mock history with email and name
	history = create_mock_history(
		[
			({'input': {'index': 1, 'text': 'old@example.com'}}, create_test_element(attributes={'type': 'email'})),
			({'input': {'index': 2, 'text': 'John'}}, create_test_element(attributes={'name': 'first_name'})),
		]
	)

	# Substitute only email
	modified_history = agent._substitute_variables_in_history(
		history,  # type: ignore[arg-type]
		{'email': 'new@example.com'},
	)

	# Check that only email was substituted
	action1 = modified_history.history[0].model_output.action[0]  # type: ignore[attr-defined]
	action2 = modified_history.history[1].model_output.action[0]  # type: ignore[attr-defined]

	assert vars(action1)['input']['text'] == 'new@example.com'
	assert vars(action2)['input']['text'] == 'John'  # Unchanged


def test_substitute_nonexistent_variable(mock_llm):
	"""Test that substituting a nonexistent variable doesn't break things"""
	agent = Agent(task='test', llm=mock_llm)

	# Create mock history with email
	element = create_test_element(attributes={'type': 'email'})
	history = create_mock_history(
		[
			({'input': {'index': 1, 'text': 'old@example.com'}}, element),
		]
	)

	# Try to substitute a variable that doesn't exist
	modified_history = agent._substitute_variables_in_history(
		history,  # type: ignore[arg-type]
		{'nonexistent_var': 'some_value'},
	)

	# Check that nothing changed
	action = modified_history.history[0].model_output.action[0]  # type: ignore[attr-defined]
	action_dict = vars(action)
	assert action_dict['input']['text'] == 'old@example.com'


def test_substitute_in_nested_dict(mock_llm):
	"""Test substitution in nested dictionary structures"""
	agent = Agent(task='test', llm=mock_llm)

	# Create a more complex action with nested structure
	complex_action = {
		'search_google': {
			'query': 'test@example.com',
			'metadata': {'user': 'test@example.com'},
		}
	}

	element = create_test_element(attributes={'type': 'email'})
	history = create_mock_history([(complex_action, element)])

	# Substitute the email
	modified_history = agent._substitute_variables_in_history(
		history,  # type: ignore[arg-type]
		{'email': 'new@example.com'},
	)

	# Check that values in nested structures were substituted
	action = modified_history.history[0].model_output.action[0]  # type: ignore[attr-defined]
	action_dict = vars(action)
	assert action_dict['search_google']['query'] == 'new@example.com'
	assert action_dict['search_google']['metadata']['user'] == 'new@example.com'


def test_substitute_in_list(mock_llm):
	"""Test substitution in list structures"""
	agent = Agent(task='test', llm=mock_llm)

	# Create history with an input action first (so email is detected)
	# Then an action with a list containing the same email
	history = create_mock_history(
		[
			({'input': {'index': 1, 'text': 'test@example.com'}}, create_test_element(attributes={'type': 'email'})),
			({'some_action': {'items': ['test@example.com', 'other_value', 'test@example.com']}}, None),
		]
	)

	# Substitute the email
	modified_history = agent._substitute_variables_in_history(
		history,  # type: ignore[arg-type]
		{'email': 'new@example.com'},
	)

	# Check that values in the first action were substituted
	action1 = modified_history.history[0].model_output.action[0]  # type: ignore[attr-defined]
	assert vars(action1)['input']['text'] == 'new@example.com'

	# Check that values in lists were also substituted
	action2 = modified_history.history[1].model_output.action[0]  # type: ignore[attr-defined]
	action_dict = vars(action2)
	assert action_dict['some_action']['items'] == ['new@example.com', 'other_value', 'new@example.com']


def test_substitute_preserves_original_history(mock_llm):
	"""Test that substitution doesn't modify the original history"""
	agent = Agent(task='test', llm=mock_llm)

	# Create mock history
	element = create_test_element(attributes={'type': 'email'})
	history = create_mock_history(
		[
			({'input': {'index': 1, 'text': 'old@example.com'}}, element),
		]
	)

	# Get original value
	original_action = history.history[0].model_output.action[0]
	original_value = vars(original_action)['input']['text']

	# Substitute
	agent._substitute_variables_in_history(history, {'email': 'new@example.com'})  # type: ignore[arg-type]

	# Check that original history is unchanged
	current_value = vars(original_action)['input']['text']
	assert current_value == original_value
	assert current_value == 'old@example.com'


def test_substitute_empty_variables(mock_llm):
	"""Test substitution with empty variables dict"""
	agent = Agent(task='test', llm=mock_llm)

	# Create mock history
	element = create_test_element(attributes={'type': 'email'})
	history = create_mock_history(
		[
			({'input': {'index': 1, 'text': 'old@example.com'}}, element),
		]
	)

	# Substitute with empty dict
	modified_history = agent._substitute_variables_in_history(history, {})  # type: ignore[arg-type]

	# Check that nothing changed
	action = modified_history.history[0].model_output.action[0]  # type: ignore[attr-defined]
	action_dict = vars(action)
	assert action_dict['input']['text'] == 'old@example.com'


def test_substitute_same_value_multiple_times(mock_llm):
	"""Test that the same value is substituted across multiple actions"""
	agent = Agent(task='test', llm=mock_llm)

	# Create history where same email appears twice
	element = create_test_element(attributes={'type': 'email'})
	history = create_mock_history(
		[
			({'input': {'index': 1, 'text': 'old@example.com'}}, element),
			({'input': {'index': 2, 'text': 'old@example.com'}}, element),
		]
	)

	# Substitute the email
	modified_history = agent._substitute_variables_in_history(
		history,  # type: ignore[arg-type]
		{'email': 'new@example.com'},
	)

	# Check that both occurrences were substituted
	action1 = modified_history.history[0].model_output.action[0]  # type: ignore[attr-defined]
	action2 = modified_history.history[1].model_output.action[0]  # type: ignore[attr-defined]

	assert vars(action1)['input']['text'] == 'new@example.com'
	assert vars(action2)['input']['text'] == 'new@example.com'

```

---

## backend/browser-use/tests/scripts/debug_iframe_scrolling.py

```py
"""
Debug test for iframe scrolling issue where DOM tree only shows top elements after scrolling.

This test verifies that after scrolling inside an iframe, the selector_map correctly
contains lower input elements like City, State, Zip Code, etc.
"""

import asyncio
import sys
from pathlib import Path

# Add parent directory to path to import browser_use modules
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from browser_use.agent.service import Agent
from browser_use.agent.views import ActionModel
from browser_use.browser import BrowserProfile, BrowserSession
from browser_use.browser.events import BrowserStateRequestEvent

# Import the mock LLM helper from conftest
from tests.ci.conftest import create_mock_llm


async def debug_iframe_scrolling():
	"""Debug iframe scrolling and DOM visibility issue."""

	print('Starting iframe scrolling debug test...')

	# Create the sequence of actions for the mock LLM
	# We need to format these as the LLM would return them
	actions = [
		# First action: Navigate to the test URL
		"""
		{
			"thinking": "Navigating to the iframe test page",
			"evaluation_previous_goal": null,
			"memory": "Starting test",
			"next_goal": "Navigate to the iframe test page",
			"action": [
				{
					"navigate": {
						"url": "https://browser-use.github.io/stress-tests/challenges/iframe-inception-level1.html",
						"new_tab": false
					}
				}
			]
		}
		""",
		# Second action: Input text in the first name field (to verify we can interact)
		"""
		{
			"thinking": "Inputting text in the first name field to test interaction",
			"evaluation_previous_goal": "Successfully navigated to the page",
			"memory": "Page loaded with nested iframes",
			"next_goal": "Type text in the first name field",
			"action": [
				{
					"input_text": {
						"index": 1,
						"text": "TestName"
					}
				}
			]
		}
		""",
		# Third action: Scroll the iframe (element_index=2 should be the iframe)
		"""
		{
			"thinking": "Scrolling inside the iframe to reveal lower form elements",
			"evaluation_previous_goal": "Successfully typed in first name field",
			"memory": "Typed TestName in first field",
			"next_goal": "Scroll inside the innermost iframe to see more form fields",
			"action": [
				{
					"scroll": {
						"down": true,
						"num_pages": 1.0,
						"index": 2
					}
				}
			]
		}
		""",
		# Fourth action: Done
		"""
		{
			"thinking": "Completed scrolling, ready to inspect DOM",
			"evaluation_previous_goal": "Successfully scrolled inside iframe",
			"memory": "Scrolled to reveal lower form fields",
			"next_goal": "Task completed",
			"action": [
				{
					"done": {
						"text": "Scrolling completed",
						"success": true
					}
				}
			]
		}
		""",
	]

	# Create mock LLM with our action sequence
	mock_llm = create_mock_llm(actions=actions)

	# Create browser session with headless=False so we can see what's happening
	browser_session = BrowserSession(
		browser_profile=BrowserProfile(
			headless=False,  # Set to False to see the browser
			user_data_dir=None,  # Use temporary directory
			keep_alive=True,
			enable_default_extensions=True,
			cross_origin_iframes=True,  # Enable cross-origin iframe support
		)
	)

	try:
		# Start the browser session
		await browser_session.start()
		print('Browser session started')

		# Create an agent with the mock LLM
		agent = Agent(
			task='Navigate to the iframe test page and scroll inside the iframe',
			llm=mock_llm,
			browser_session=browser_session,
		)

		# Helper function to capture and analyze DOM state
		async def capture_dom_state(label: str) -> dict:
			"""Capture DOM state and return analysis"""
			print(f'\n📸 Capturing DOM state: {label}')
			state_event = browser_session.event_bus.dispatch(
				BrowserStateRequestEvent(include_dom=True, include_screenshot=False, include_recent_events=False)
			)
			browser_state = await state_event.event_result()

			if browser_state and browser_state.dom_state and browser_state.dom_state.selector_map:
				selector_map = browser_state.dom_state.selector_map
				element_count = len(selector_map)

				# Check for specific elements
				found_elements = {}
				expected_checks = [
					('First Name', ['firstName', 'first name']),
					('Last Name', ['lastName', 'last name']),
					('Email', ['email']),
					('City', ['city']),
					('State', ['state']),
					('Zip', ['zip', 'zipCode']),
				]

				for name, keywords in expected_checks:
					for index, element in selector_map.items():
						element_str = str(element).lower()
						if any(kw.lower() in element_str for kw in keywords):
							found_elements[name] = True
							break

				return {
					'label': label,
					'total_elements': element_count,
					'found_elements': found_elements,
					'selector_map': selector_map,
				}
			return {'label': label, 'error': 'No DOM state available'}

		# Capture initial state before any actions
		print('\n' + '=' * 80)
		print('PHASE 1: INITIAL PAGE LOAD')
		print('=' * 80)

		# Navigate to the page first
		from browser_use.tools.service import Tools

		tools = Tools()

		# Create the action model for navigation
		goto_action = ActionModel.model_validate_json(actions[0])
		await tools.act(goto_action, browser_session)
		await asyncio.sleep(2)  # Wait for page to fully load

		initial_state = await capture_dom_state('INITIAL (after page load)')

		# Now run the rest of the actions via the agent
		print('\n' + '=' * 80)
		print('PHASE 2: EXECUTING ACTIONS')
		print('=' * 80)

		# Create new agent with remaining actions
		remaining_actions = actions[1:]  # Skip the navigation we already did
		mock_llm_remaining = create_mock_llm(actions=remaining_actions)
		agent = Agent(
			task='Input text and scroll inside the iframe',
			llm=mock_llm_remaining,
			browser_session=browser_session,
		)

		# Hook into agent actions to capture state after each one
		states = []
		original_act = tools.act

		async def wrapped_act(action, session):
			result = await original_act(action, session)
			# Capture state after each action
			action_type = 'unknown'
			if hasattr(action, 'input_text') and action.input_text:
				action_type = 'input_text'
				await asyncio.sleep(1)  # Give time for DOM to update
				state = await capture_dom_state('AFTER INPUT_TEXT')
				states.append(state)
			elif hasattr(action, 'scroll') and action.scroll:
				action_type = 'scroll'
				await asyncio.sleep(2)  # Give more time after scroll
				state = await capture_dom_state('AFTER SCROLL')
				states.append(state)
			return result

		tools.act = wrapped_act

		# Run the agent with remaining actions
		result = await agent.run()
		print(f'\nAgent completed with result: {result}')

		# Analyze all captured states
		print('\n' + '=' * 80)
		print('PHASE 3: ANALYSIS OF DOM STATES')
		print('=' * 80)

		all_states = [initial_state] + states

		for state in all_states:
			if 'error' in state:
				print(f'\n❌ {state["label"]}: {state["error"]}')
			else:
				print(f'\n📊 {state["label"]}:')
				print(f'  Total elements: {state["total_elements"]}')
				print('  Found elements:')
				for elem_name, found in state['found_elements'].items():
					status = '✓' if found else '✗'
					print(f'    {status} {elem_name}')

		# Compare states
		print('\n' + '=' * 80)
		print('COMPARISON SUMMARY')
		print('=' * 80)

		if len(all_states) >= 3:
			initial = all_states[0]
			after_input = all_states[1] if len(all_states) > 1 else None
			after_scroll = all_states[2] if len(all_states) > 2 else None

			print('\nElement count changes:')
			print(f'  Initial: {initial.get("total_elements", 0)} elements')
			if after_input:
				print(f'  After input_text: {after_input.get("total_elements", 0)} elements')
			if after_scroll:
				print(f'  After scroll: {after_scroll.get("total_elements", 0)} elements')

			# Check if lower form fields appear after scroll
			if after_scroll and 'found_elements' in after_scroll:
				lower_fields = ['City', 'State', 'Zip']
				missing_fields = [f for f in lower_fields if not after_scroll['found_elements'].get(f, False)]

				if missing_fields:
					print('\n⚠️  BUG CONFIRMED: Lower form fields missing after scroll:')
					for field in missing_fields:
						print(f'    ✗ {field}')
					print('\nThis confirms that scrolling inside iframes does not update the DOM tree properly.')
				else:
					print('\n✅ SUCCESS: All lower form fields are visible after scrolling!')

		# Show first few elements from final state for debugging
		if states and 'selector_map' in states[-1]:
			print('\n' + '=' * 80)
			print('DEBUG: First 5 elements in final selector_map')
			print('=' * 80)
			final_map = states[-1]['selector_map']
			for i, (index, element) in enumerate(list(final_map.items())[:5]):
				elem_preview = str(element)[:150]
				print(f'\n  [{index}]: {elem_preview}...')

		# Keep browser open for manual inspection if needed
		print('\n' + '=' * 80)
		print('Test complete. Browser will remain open for 10 seconds for inspection...')
		print('=' * 80)
		await asyncio.sleep(10)

	finally:
		# Clean up
		print('\nCleaning up...')
		await browser_session.kill()
		await browser_session.event_bus.stop(clear=True, timeout=5)
		print('Browser session closed')


if __name__ == '__main__':
	# Run the debug test
	asyncio.run(debug_iframe_scrolling())

```

---

## backend/browser-use/tests/scripts/test_frame_hierarchy.py

```py
#!/usr/bin/env python3
"""Test frame hierarchy for any URL passed as argument."""

import asyncio
import sys

from browser_use.browser import BrowserSession
from browser_use.browser.events import BrowserStartEvent
from browser_use.browser.profile import BrowserProfile


async def analyze_frame_hierarchy(url):
	"""Analyze and display complete frame hierarchy for a URL."""

	profile = BrowserProfile(headless=True, user_data_dir=None)
	session = BrowserSession(browser_profile=profile)

	try:
		print('🚀 Starting browser...')
		await session.on_BrowserStartEvent(BrowserStartEvent())

		print(f'📍 Navigating to: {url}')
		await session._cdp_navigate(url)
		await asyncio.sleep(3)

		print('\n' + '=' * 80)
		print('FRAME HIERARCHY ANALYSIS')
		print('=' * 80)

		# Get all targets from SessionManager
		all_targets = session.session_manager.get_all_targets()

		# Separate by type
		page_targets = [target for target in all_targets.values() if target.target_type == 'page']
		iframe_targets = [target for target in all_targets.values() if target.target_type == 'iframe']

		print('\n📊 Target Summary:')
		print(f'  Total targets: {len(all_targets)}')
		print(f'  Page targets: {len(page_targets)}')
		print(f'  Iframe targets (OOPIFs): {len(iframe_targets)}')

		# Show all targets
		print('\n📋 All Targets:')
		for i, (target_id, target) in enumerate(all_targets.items()):
			if target.target_type in ['page', 'iframe']:
				print(f'\n  [{i + 1}] Type: {target.target_type}')
				print(f'      URL: {target.url}')
				print(f'      Target ID: {target.target_id[:30]}...')

				# Check if target has active sessions using the public API
				try:
					cdp_session = await session.get_or_create_cdp_session(target.target_id, focus=False)
					has_session = cdp_session is not None
				except Exception:
					has_session = False
				print(f'      Has Session: {has_session}')

		# Get main page frame tree
		main_target = next((t for t in page_targets if url in t.url), page_targets[0] if page_targets else None)

		if main_target:
			print('\n📐 Main Page Frame Tree:')
			print(f'  Target: {main_target.url}')
			print(f'  Target ID: {main_target.target_id[:30]}...')

			s = await session.cdp_client.send.Target.attachToTarget(params={'targetId': main_target.target_id, 'flatten': True})
			sid = s['sessionId']

			try:
				await session.cdp_client.send.Page.enable(session_id=sid)
				tree = await session.cdp_client.send.Page.getFrameTree(session_id=sid)

				print('\n  Frame Tree Structure:')

				def print_tree(node, indent=0, parent_id=None):
					frame = node['frame']
					frame_id = frame.get('id', 'unknown')
					frame_url = frame.get('url', 'none')

					prefix = '  ' * indent + ('└─ ' if indent > 0 else '')
					print(f'{prefix}Frame: {frame_url}')
					print(f'{"  " * (indent + 1)}ID: {frame_id[:30]}...')

					if parent_id:
						print(f'{"  " * (indent + 1)}Parent: {parent_id[:30]}...')

					# Check cross-origin status
					cross_origin = frame.get('crossOriginIsolatedContextType', 'unknown')
					if cross_origin != 'NotIsolated':
						print(f'{"  " * (indent + 1)}⚠️  Cross-Origin: {cross_origin}')

					# Process children
					for child in node.get('childFrames', []):
						print_tree(child, indent + 1, frame_id)

				print_tree(tree['frameTree'])

			finally:
				await session.cdp_client.send.Target.detachFromTarget(params={'sessionId': sid})

		# Show iframe target trees
		if iframe_targets:
			print('\n🔸 OOPIF Target Frame Trees:')

			for iframe_target in iframe_targets:
				print(f'\n  OOPIF Target: {iframe_target.url}')
				print(f'  Target ID: {iframe_target.target_id[:30]}...')

				s = await session.cdp_client.send.Target.attachToTarget(
					params={'targetId': iframe_target.target_id, 'flatten': True}
				)
				sid = s['sessionId']

				try:
					await session.cdp_client.send.Page.enable(session_id=sid)
					tree = await session.cdp_client.send.Page.getFrameTree(session_id=sid)

					frame = tree['frameTree']['frame']
					print(f'  Frame ID: {frame.get("id", "unknown")[:30]}...')
					print(f'  Frame URL: {frame.get("url", "none")}')
					print('  ⚠️  This frame runs in a separate process (OOPIF)')

				except Exception as e:
					print(f'  Error: {e}')
				finally:
					await session.cdp_client.send.Target.detachFromTarget(params={'sessionId': sid})

		# Now show unified view from get_all_frames
		print('\n' + '=' * 80)
		print('UNIFIED FRAME HIERARCHY (get_all_frames method)')
		print('=' * 80)

		all_frames, target_sessions = await session.get_all_frames()

		# Clean up sessions
		for tid, sess_id in target_sessions.items():
			try:
				await session.cdp_client.send.Target.detachFromTarget(params={'sessionId': sess_id})
			except Exception:
				pass

		print('\n📊 Frame Statistics:')
		print(f'  Total frames discovered: {len(all_frames)}')

		# Separate root and child frames
		root_frames = []
		child_frames = []

		for frame_id, frame_info in all_frames.items():
			if not frame_info.get('parentFrameId'):
				root_frames.append((frame_id, frame_info))
			else:
				child_frames.append((frame_id, frame_info))

		print(f'  Root frames: {len(root_frames)}')
		print(f'  Child frames: {len(child_frames)}')

		# Display all frames with details
		print('\n📋 All Frames:')

		for i, (frame_id, frame_info) in enumerate(all_frames.items()):
			url = frame_info.get('url', 'none')
			parent = frame_info.get('parentFrameId')
			target_id = frame_info.get('frameTargetId', 'unknown')
			is_cross = frame_info.get('isCrossOrigin', False)

			print(f'\n  [{i + 1}] Frame URL: {url}')
			print(f'      Frame ID: {frame_id[:30]}...')
			print(f'      Parent Frame ID: {parent[:30] + "..." if parent else "None (ROOT)"}')
			print(f'      Target ID: {target_id[:30]}...')
			print(f'      Cross-Origin: {is_cross}')

			# Highlight problems
			if not parent and 'v0-simple-landing' in url:
				print('      ❌ PROBLEM: Cross-origin frame incorrectly marked as root!')
			elif not parent and url != 'about:blank' and url not in ['chrome://newtab/', 'about:blank']:
				# Check if this should be the main frame
				if any(url in t.url for t in page_targets):
					print('      ✅ Correctly identified as root frame')

			if is_cross:
				print('      🔸 This is a cross-origin frame (OOPIF)')

		# Show parent-child relationships
		print('\n🌳 Frame Relationships:')

		# Build a tree structure
		def print_frame_tree(frame_id, frame_info, indent=0, visited=None):
			if visited is None:
				visited = set()

			if frame_id in visited:
				return
			visited.add(frame_id)

			url = frame_info.get('url', 'none')
			prefix = '  ' * indent + ('└─ ' if indent > 0 else '')

			print(f'{prefix}{url[:60]}...')
			print(f'{"  " * (indent + 1)}[{frame_id[:20]}...]')

			# Find children
			for child_id, child_info in all_frames.items():
				if child_info.get('parentFrameId') == frame_id:
					print_frame_tree(child_id, child_info, indent + 1, visited)

		# Print trees starting from roots
		for frame_id, frame_info in root_frames:
			print('\n  Tree starting from root:')
			print_frame_tree(frame_id, frame_info)

		print('\n' + '=' * 80)
		print('✅ Analysis complete!')
		print('=' * 80)

	except Exception as e:
		print(f'❌ Error: {e}')
		import traceback

		traceback.print_exc()
	finally:
		# Stop the CDP client first before killing the browser
		print('\n🛑 Shutting down...')

		# Close CDP connection first while browser is still alive
		if session._cdp_client_root:
			try:
				await session._cdp_client_root.stop()
			except Exception:
				pass  # Ignore errors if already disconnected

		# Then stop the browser process
		from browser_use.browser.events import BrowserStopEvent

		stop_event = session.event_bus.dispatch(BrowserStopEvent())
		try:
			await asyncio.wait_for(stop_event, timeout=2.0)
		except TimeoutError:
			print('⚠️ Browser stop timed out')


def main():
	if len(sys.argv) != 2:
		print('Usage: python test_frame_hierarchy.py <URL>')
		print('\nExample URLs to test:')
		print('  https://v0-website-with-clickable-elements.vercel.app/nested-iframe')
		print('  https://v0-website-with-clickable-elements.vercel.app/cross-origin')
		print('  https://v0-website-with-clickable-elements.vercel.app/shadow-dom')
		sys.exit(1)

	url = sys.argv[1]
	asyncio.run(analyze_frame_hierarchy(url))

	# Ensure clean exit
	print('✅ Script completed')
	sys.exit(0)


if __name__ == '__main__':
	main()

```

---

## backend/data-collection-service/build.gradle.kts

```kts
// Collector Service 모듈 빌드 설정

plugins {
    java
    id("org.springframework.boot")
    id("io.spring.dependency-management")
}

dependencies {
    // Spring Boot Web
    implementation("org.springframework.boot:spring-boot-starter-web")
    
    // Spring Security
    implementation("org.springframework.boot:spring-boot-starter-security")
    
    // JWT Support
    implementation("io.jsonwebtoken:jjwt-api:0.12.3")
    runtimeOnly("io.jsonwebtoken:jjwt-impl:0.12.3")
    runtimeOnly("io.jsonwebtoken:jjwt-jackson:0.12.3")
    
    // OpenAPI / Swagger
    implementation("org.springdoc:springdoc-openapi-starter-webmvc-ui:2.3.0")
    
    // Spring Data JPA
    implementation("org.springframework.boot:spring-boot-starter-data-jpa")
    
    // Redis
    implementation("org.springframework.boot:spring-boot-starter-data-redis")
    
    // WebClient for HTTP calls (web-crawler 통합)
    implementation("org.springframework.boot:spring-boot-starter-webflux")
    
    // Database
    runtimeOnly("org.postgresql:postgresql")
    
    // Database Migration (선택)
    implementation("org.flywaydb:flyway-core:10.4.1")
    implementation("org.flywaydb:flyway-database-postgresql:10.4.1")
    
    // JSON Processing
    implementation("com.fasterxml.jackson.dataformat:jackson-dataformat-xml")
    implementation("com.fasterxml.jackson.datatype:jackson-datatype-jsr310")
    
    // HTML Parsing (웹 크롤링용)
    implementation("org.jsoup:jsoup:1.17.1")
    
    // MongoDB (AI 응답 저장용)
    implementation("org.springframework.boot:spring-boot-starter-data-mongodb")
    
    // RSS Feed Parsing
    implementation("com.rometools:rome:2.1.0")
    
    // Async Support
    implementation("org.springframework.boot:spring-boot-starter-aop")
    
    // Kafka (for AI integration and asynchronous messaging)
    implementation("org.springframework.kafka:spring-kafka")
    
    // PDF Generation - iText 7 (AGPL License)
    implementation("com.itextpdf:itext7-core:8.0.2")
    implementation("com.itextpdf:html2pdf:5.0.2")
    
    // Chart Generation - JFreeChart for server-side charts
    implementation("org.jfree:jfreechart:1.5.4")
    
    // Spring Boot Actuator (Health Check, Metrics)
    implementation("org.springframework.boot:spring-boot-starter-actuator")
    
    // Micrometer for Prometheus metrics
    implementation("io.micrometer:micrometer-registry-prometheus")
    
    // Caffeine Cache (Local cache fallback)
    implementation("com.github.ben-manes.caffeine:caffeine:3.1.8")
    
    // Spring Retry (재시도 로직)
    implementation("org.springframework.retry:spring-retry")
    
    // Test
    testImplementation("com.h2database:h2")
    testImplementation("org.testcontainers:testcontainers:1.19.3")
    testImplementation("org.testcontainers:postgresql:1.19.3")
    testImplementation("org.testcontainers:junit-jupiter:1.19.3")
    testImplementation("org.testcontainers:mongodb:1.19.3")
}

tasks.named<org.springframework.boot.gradle.tasks.bundling.BootJar>("bootJar") {
    archiveBaseName.set("collector-service")
    archiveVersion.set("1.0.0")
}

```

---

## backend/data-collection-service/maigret-worker/main.py

```py
import asyncio
import json
import logging
import os
import subprocess
import tempfile
import uuid
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from contextlib import asynccontextmanager
import sys

from fastapi import FastAPI, HTTPException, BackgroundTasks, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

# Add shared module to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

# Try to import proxy client
try:
    from shared.proxy_client import ProxyRotationClient, ProxyInfo

    PROXY_CLIENT_AVAILABLE = True
except ImportError:
    PROXY_CLIENT_AVAILABLE = False
    ProxyRotationClient = None
    ProxyInfo = None

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


# --- Configuration ---
class AppConfig:
    """Application configuration from environment variables."""

    PORT = int(os.getenv("PORT", "8020"))
    LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")

    # Maigret configuration
    MAX_CONCURRENT_SCANS = int(os.getenv("MAX_CONCURRENT_SCANS", "3"))
    SCAN_TIMEOUT_SEC = int(os.getenv("SCAN_TIMEOUT_SEC", "300"))  # 5 minutes default
    REQUEST_DELAY_MS = int(os.getenv("REQUEST_DELAY_MS", "100"))

    # Output directory for reports
    REPORTS_DIR = Path(os.getenv("REPORTS_DIR", "/app/reports"))

    # Proxy configuration (optional)
    PROXY_URL = os.getenv("PROXY_URL", None)
    USE_TOR = os.getenv("USE_TOR", "false").lower() == "true"

    # Site filtering
    TOP_SITES_ONLY = os.getenv("TOP_SITES_ONLY", "false").lower() == "true"
    MAX_SITES = int(os.getenv("MAX_SITES", "500"))


config = AppConfig()

# Ensure reports directory exists
config.REPORTS_DIR.mkdir(parents=True, exist_ok=True)

# Semaphore for concurrent scan control
scan_semaphore = asyncio.Semaphore(config.MAX_CONCURRENT_SCANS)

# Proxy rotation configuration
USE_PROXY_ROTATION = os.getenv("USE_PROXY_ROTATION", "true").lower() == "true"
PROXY_ROTATION_URL = os.getenv("PROXY_ROTATION_URL", "http://ip-rotation:8050")

# Initialize proxy client (if available)
proxy_client = None
if PROXY_CLIENT_AVAILABLE and USE_PROXY_ROTATION:
    proxy_client = ProxyRotationClient(
        base_url=PROXY_ROTATION_URL,
        timeout=5.0,
        enabled=True,
    )
    logger.info(f"Proxy rotation enabled, connecting to {PROXY_ROTATION_URL}")

# Proxy rotation configuration
USE_PROXY_ROTATION = os.getenv("USE_PROXY_ROTATION", "true").lower() == "true"
PROXY_ROTATION_URL = os.getenv("PROXY_ROTATION_URL", "http://ip-rotation:8050")

# Initialize proxy client (if available)
proxy_client: "ProxyRotationClient | None" = None
if PROXY_CLIENT_AVAILABLE and USE_PROXY_ROTATION:
    proxy_client = ProxyRotationClient(
        base_url=PROXY_ROTATION_URL,
        timeout=5.0,
        enabled=True,
    )
    logger.info(f"Proxy rotation enabled, connecting to {PROXY_ROTATION_URL}")


# --- Enums and Models ---
class ScanStatus(str, Enum):
    PENDING = "PENDING"
    RUNNING = "RUNNING"
    COMPLETED = "COMPLETED"
    FAILED = "FAILED"
    TIMEOUT = "TIMEOUT"


class ScanRequest(BaseModel):
    """Request model for username scan."""

    username: str = Field(
        ..., min_length=1, max_length=100, description="Username to scan"
    )
    options: Optional[Dict[str, Any]] = Field(
        default=None, description="Additional Maigret options"
    )
    timeout_sec: Optional[int] = Field(
        default=None, ge=30, le=600, description="Scan timeout in seconds (30-600)"
    )
    top_sites_only: Optional[bool] = Field(
        default=None, description="Only scan top/popular sites for faster results"
    )


class AccountInfo(BaseModel):
    """Information about a discovered account."""

    site_name: str
    url: str
    username: str
    status: str = "claimed"
    tags: List[str] = Field(default_factory=list)


class ScanSummary(BaseModel):
    """Summary of scan results."""

    total_sites_checked: int
    accounts_found: int
    accounts_claimed: int
    accounts_available: int
    accounts_error: int
    scan_duration_ms: int


class ScanResult(BaseModel):
    """Complete scan result."""

    scan_id: str
    username: str
    status: ScanStatus
    summary: Optional[ScanSummary] = None
    accounts: List[AccountInfo] = Field(default_factory=list)
    raw_json_path: Optional[str] = None
    error_message: Optional[str] = None
    started_at: Optional[str] = None
    completed_at: Optional[str] = None


class ScanResponse(BaseModel):
    """Response model for scan endpoint."""

    status: str = "ok"
    scan_id: str
    message: str
    result: Optional[ScanResult] = None


class HealthResponse(BaseModel):
    """Health check response."""

    status: str
    version: str
    maigret_available: bool
    active_scans: int
    max_concurrent_scans: int
    proxy_rotation_enabled: bool = False
    proxy_service_healthy: bool = False


# --- In-memory scan tracking ---
# In production, this should be replaced with Redis or a database
active_scans: Dict[str, ScanResult] = {}


# --- Helper Functions ---
def sanitize_username(username: str) -> str:
    """Sanitize username to prevent command injection."""
    # Remove any shell-dangerous characters
    import re

    # Only allow alphanumeric, underscore, dash, dot
    sanitized = re.sub(r"[^a-zA-Z0-9_\-.]", "", username)
    if not sanitized:
        raise ValueError("Invalid username after sanitization")
    return sanitized


def build_maigret_command(
    username: str,
    output_dir: Path,
    options: Optional[Dict] = None,
    proxy_url: Optional[str] = None,
) -> List[str]:
    """Build Maigret CLI command with options.

    Args:
        username: Username to scan
        output_dir: Directory for output files
        options: Additional scan options
        proxy_url: Optional proxy URL from rotation service (takes priority)
    """
    cmd = [
        "maigret",
        username,
        "--json",
        "simple",
        "--folderoutput",
        str(output_dir),
    ]

    # Add timeout per site
    cmd.extend(["--timeout", "10"])

    # Proxy configuration (rotation proxy takes priority)
    if proxy_url:
        # Determine proxy type from URL
        if proxy_url.startswith("socks"):
            cmd.extend(["--tor-proxy", proxy_url])
        else:
            cmd.extend(["--proxy", proxy_url])
    elif config.PROXY_URL:
        cmd.extend(["--proxy", config.PROXY_URL])
    elif config.USE_TOR:
        cmd.extend(["--tor-proxy", "socks5://127.0.0.1:9050"])

    # Top sites only for faster scanning
    top_sites = (
        options.get("top_sites_only", config.TOP_SITES_ONLY)
        if options
        else config.TOP_SITES_ONLY
    )
    if top_sites:
        cmd.extend(["--top-sites", "50"])

    # Limit number of sites
    max_sites = (
        options.get("max_sites", config.MAX_SITES) if options else config.MAX_SITES
    )
    if max_sites and max_sites < 500:
        cmd.extend(["--top-sites", str(max_sites)])

    # No recursive search for basic scans
    cmd.append("--no-recursion")

    # Suppress color output and progress bar for cleaner parsing
    cmd.append("--no-color")
    cmd.append("--no-progressbar")

    return cmd


async def get_proxy_for_scan() -> tuple:
    """
    Get a proxy from the rotation service for scanning.

    Returns:
        Tuple of (proxy_url, proxy_id) or (None, None) if unavailable
    """
    if not proxy_client:
        return None, None

    try:
        proxy_info = await proxy_client.get_next_proxy()
        if proxy_info:
            return proxy_info.get_proxy_url(), proxy_info.id
    except Exception as e:
        logger.warning(f"Failed to get proxy from rotation service: {e}")

    return None, None


async def record_proxy_result(
    proxy_id: Optional[str], success: bool, latency_ms: int = 0, error: str = ""
):
    """Record the result of a proxy-enabled scan."""
    if not proxy_id or not proxy_client:
        return

    try:
        if success:
            await proxy_client.record_success(proxy_id, latency_ms)
        else:
            await proxy_client.record_failure(proxy_id, error[:200])
    except Exception as e:
        logger.debug(f"Failed to record proxy result: {e}")


def parse_maigret_json(json_path: Path) -> Dict[str, Any]:
    """Parse Maigret JSON output file."""
    try:
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data
    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse Maigret JSON: {e}")
        return {}
    except FileNotFoundError:
        logger.error(f"Maigret output file not found: {json_path}")
        return {}


def extract_accounts_from_result(
    raw_result: Dict[str, Any], username: str
) -> List[AccountInfo]:
    """Extract account information from Maigret JSON result."""
    accounts = []

    # Maigret JSON structure: { "SiteName": { "status": { "status": "Claimed" }, "url_user": "..." } }
    if isinstance(raw_result, dict):
        for site_name, site_data in raw_result.items():
            if isinstance(site_data, dict):
                # Get status from nested structure
                status_obj = site_data.get("status", {})
                if isinstance(status_obj, dict):
                    status = status_obj.get("status", "Unknown")
                    tags = status_obj.get("tags", [])
                else:
                    status = str(status_obj)
                    tags = []

                url = site_data.get("url_user", site_data.get("url", ""))

                # Only include claimed/found accounts
                if isinstance(status, str) and status.lower() in [
                    "claimed",
                    "found",
                    "detected",
                ]:
                    accounts.append(
                        AccountInfo(
                            site_name=site_name,
                            url=url or f"https://{site_name.lower()}.com/{username}",
                            username=username,
                            status="claimed",
                            tags=tags if isinstance(tags, list) else [],
                        )
                    )

    # Format 2: Array of results (fallback)
    elif isinstance(raw_result, list):
        for item in raw_result:
            if isinstance(item, dict):
                status_obj = item.get("status", {})
                if isinstance(status_obj, dict):
                    status = status_obj.get("status", "Unknown")
                else:
                    status = str(status_obj)

                if isinstance(status, str) and status.lower() in ["claimed", "found"]:
                    accounts.append(
                        AccountInfo(
                            site_name=item.get("site", item.get("name", "Unknown")),
                            url=item.get("url", ""),
                            username=username,
                            status="claimed",
                            tags=item.get("tags", []),
                        )
                    )

    return accounts


async def run_maigret_scan(
    scan_id: str, username: str, options: Optional[Dict], timeout: int
) -> ScanResult:
    """Execute Maigret scan as subprocess with proxy rotation support."""
    start_time = datetime.now()
    result = active_scans[scan_id]
    result.status = ScanStatus.RUNNING
    result.started_at = start_time.isoformat()

    # Create output directory for this scan
    scan_output_dir = config.REPORTS_DIR / f"scan_{scan_id}"
    scan_output_dir.mkdir(parents=True, exist_ok=True)

    # Get proxy from rotation service
    proxy_url, proxy_id = await get_proxy_for_scan()
    if proxy_id:
        logger.info(f"[{scan_id}] Using rotating proxy: {proxy_id}")

    try:
        # Sanitize username
        safe_username = sanitize_username(username)

        # Build command with optional proxy
        cmd = build_maigret_command(safe_username, scan_output_dir, options, proxy_url)
        logger.info(f"[{scan_id}] Running Maigret: {' '.join(cmd)}")

        # Run Maigret with timeout
        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
            cwd=str(scan_output_dir),
        )

        try:
            stdout, stderr = await asyncio.wait_for(
                process.communicate(), timeout=timeout
            )
        except asyncio.TimeoutError:
            process.kill()
            await process.wait()
            # Record proxy failure on timeout
            duration_ms = int((datetime.now() - start_time).total_seconds() * 1000)
            await record_proxy_result(proxy_id, False, duration_ms, "Scan timeout")
            result.status = ScanStatus.TIMEOUT
            result.error_message = f"Scan timed out after {timeout} seconds"
            result.completed_at = datetime.now().isoformat()
            return result

        # Log output for debugging
        stdout_text = stdout.decode("utf-8", errors="replace")
        stderr_text = stderr.decode("utf-8", errors="replace")
        if stdout_text:
            logger.info(f"[{scan_id}] Maigret stdout: {stdout_text[:500]}")
        if stderr_text:
            logger.warning(f"[{scan_id}] Maigret stderr: {stderr_text[:500]}")

        # Check for errors (but Maigret may exit non-zero even on partial success)
        if process.returncode != 0 and not any(scan_output_dir.glob("*.json")):
            logger.error(f"[{scan_id}] Maigret failed with no output: {stderr_text}")
            duration_ms = int((datetime.now() - start_time).total_seconds() * 1000)
            await record_proxy_result(proxy_id, False, duration_ms, stderr_text[:100])
            result.status = ScanStatus.FAILED
            result.error_message = (
                f"Maigret exited with code {process.returncode}: {stderr_text[:500]}"
            )
            result.completed_at = datetime.now().isoformat()
            return result

        # Find JSON output file - Maigret creates files like report_<username>_simple.json
        json_files = list(scan_output_dir.glob("*.json"))

        if json_files:
            output_path = json_files[0]  # Take the first JSON file
            logger.info(f"[{scan_id}] Found output file: {output_path}")

            raw_result = parse_maigret_json(output_path)
            accounts = extract_accounts_from_result(raw_result, safe_username)

            # Calculate summary
            end_time = datetime.now()
            duration_ms = int((end_time - start_time).total_seconds() * 1000)

            # Record proxy success
            await record_proxy_result(proxy_id, True, duration_ms)

            # Count stats from raw result
            total_checked = len(raw_result) if isinstance(raw_result, dict) else 0
            claimed = len(accounts)

            result.summary = ScanSummary(
                total_sites_checked=total_checked,
                accounts_found=claimed,
                accounts_claimed=claimed,
                accounts_available=0,
                accounts_error=0,
                scan_duration_ms=duration_ms,
            )
            result.accounts = accounts
            result.raw_json_path = str(output_path)
            result.status = ScanStatus.COMPLETED
            result.completed_at = end_time.isoformat()

            logger.info(
                f"[{scan_id}] Scan completed: {claimed} accounts found in {duration_ms}ms"
            )
        else:
            duration_ms = int((datetime.now() - start_time).total_seconds() * 1000)
            await record_proxy_result(proxy_id, False, duration_ms, "No output file")
            result.status = ScanStatus.FAILED
            result.error_message = "Maigret did not produce output file"
            result.completed_at = datetime.now().isoformat()

    except ValueError as e:
        duration_ms = int((datetime.now() - start_time).total_seconds() * 1000)
        await record_proxy_result(proxy_id, False, duration_ms, str(e))
        result.status = ScanStatus.FAILED
        result.error_message = f"Invalid input: {str(e)}"
        result.completed_at = datetime.now().isoformat()
    except Exception as e:
        logger.exception(f"[{scan_id}] Unexpected error during scan")
        duration_ms = int((datetime.now() - start_time).total_seconds() * 1000)
        await record_proxy_result(proxy_id, False, duration_ms, str(e)[:100])
        result.status = ScanStatus.FAILED
        result.error_message = f"Unexpected error: {str(e)}"
        result.completed_at = datetime.now().isoformat()

    return result


async def execute_scan_with_semaphore(
    scan_id: str, username: str, options: Optional[Dict], timeout: int
):
    """Execute scan with concurrency control."""
    async with scan_semaphore:
        await run_maigret_scan(scan_id, username, options, timeout)


# --- Lifespan Management ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan handler."""
    logger.info("Maigret Worker service starting up...")
    logger.info(f"Max concurrent scans: {config.MAX_CONCURRENT_SCANS}")
    logger.info(f"Default timeout: {config.SCAN_TIMEOUT_SEC}s")
    logger.info(f"Reports directory: {config.REPORTS_DIR}")
    logger.info(f"Proxy rotation enabled: {USE_PROXY_ROTATION}")

    # Verify Maigret is installed
    try:
        result = subprocess.run(
            ["maigret", "--version"], capture_output=True, text=True, timeout=10
        )
        logger.info(f"Maigret version: {result.stdout.strip()}")
    except Exception as e:
        logger.warning(f"Could not verify Maigret installation: {e}")

    yield

    # Cleanup proxy client on shutdown
    if proxy_client:
        await proxy_client.close()
    logger.info("Maigret Worker service shutting down...")


# --- FastAPI App ---
app = FastAPI(
    title="Maigret OSINT Username Scanner",
    description="Performs username OSINT scanning using Maigret for social media account discovery",
    version="1.0.0",
    lifespan=lifespan,
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# --- API Endpoints ---
@app.get("/health", response_model=HealthResponse)
@app.head("/health")
async def health_check():
    """Health check endpoint."""
    # Check if Maigret is available
    maigret_available = False
    try:
        result = subprocess.run(
            ["maigret", "--version"], capture_output=True, timeout=5
        )
        maigret_available = result.returncode == 0
    except Exception:
        pass

    # Count active scans
    active_count = sum(
        1
        for s in active_scans.values()
        if s.status in [ScanStatus.PENDING, ScanStatus.RUNNING]
    )

    # Check proxy service health
    proxy_healthy = False
    if proxy_client:
        proxy_healthy = await proxy_client.health_check()

    return HealthResponse(
        status="ok" if maigret_available else "degraded",
        version="1.1.0",
        maigret_available=maigret_available,
        active_scans=active_count,
        max_concurrent_scans=config.MAX_CONCURRENT_SCANS,
        proxy_rotation_enabled=USE_PROXY_ROTATION,
        proxy_service_healthy=proxy_healthy,
    )


@app.post("/scan", response_model=ScanResponse)
async def start_scan(
    request: ScanRequest, background_tasks: BackgroundTasks, req: Request
):
    """
    Start a username OSINT scan using Maigret.

    The scan runs asynchronously. Use GET /scan/{scan_id} to check status and results.
    """
    trace_id = req.headers.get("X-Trace-Id", req.headers.get("X-Request-Id", "unknown"))

    # Generate scan ID
    scan_id = str(uuid.uuid4())

    logger.info(
        f"[{trace_id}] Starting scan {scan_id} for username: {request.username}"
    )

    # Validate username early
    try:
        sanitize_username(request.username)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))

    # Determine timeout
    timeout = request.timeout_sec or config.SCAN_TIMEOUT_SEC

    # Build options
    options = request.options or {}
    if request.top_sites_only is not None:
        options["top_sites_only"] = request.top_sites_only

    # Create initial scan result
    result = ScanResult(
        scan_id=scan_id, username=request.username, status=ScanStatus.PENDING
    )
    active_scans[scan_id] = result

    # Start scan in background
    background_tasks.add_task(
        execute_scan_with_semaphore, scan_id, request.username, options, timeout
    )

    return ScanResponse(
        status="ok",
        scan_id=scan_id,
        message=f"Scan started for username '{request.username}'. Use GET /scan/{scan_id} to check status.",
        result=result,
    )


@app.post("/scan/sync", response_model=ScanResponse)
async def run_scan_sync(request: ScanRequest, req: Request):
    """
    Run a username scan synchronously and wait for results.

    This endpoint blocks until the scan completes or times out.
    Recommended for shorter scans (use top_sites_only=true for faster results).
    """
    trace_id = req.headers.get("X-Trace-Id", req.headers.get("X-Request-Id", "unknown"))

    scan_id = str(uuid.uuid4())
    logger.info(
        f"[{trace_id}] Starting sync scan {scan_id} for username: {request.username}"
    )

    # Validate username
    try:
        sanitize_username(request.username)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))

    # Determine timeout
    timeout = request.timeout_sec or config.SCAN_TIMEOUT_SEC

    # Build options - default to top_sites for sync
    options = request.options or {}
    if request.top_sites_only is not None:
        options["top_sites_only"] = request.top_sites_only
    else:
        options["top_sites_only"] = True  # Default to top sites for sync requests

    # Create scan result
    result = ScanResult(
        scan_id=scan_id, username=request.username, status=ScanStatus.PENDING
    )
    active_scans[scan_id] = result

    # Run scan with semaphore
    async with scan_semaphore:
        result = await run_maigret_scan(scan_id, request.username, options, timeout)

    if result.status == ScanStatus.COMPLETED:
        return ScanResponse(
            status="ok",
            scan_id=scan_id,
            message=f"Scan completed. Found {len(result.accounts)} accounts.",
            result=result,
        )
    else:
        return ScanResponse(
            status="error",
            scan_id=scan_id,
            message=result.error_message or "Scan failed",
            result=result,
        )


@app.get("/scan/{scan_id}", response_model=ScanResponse)
async def get_scan_status(scan_id: str):
    """Get the status and results of a scan."""
    result = active_scans.get(scan_id)

    if not result:
        raise HTTPException(status_code=404, detail=f"Scan not found: {scan_id}")

    return ScanResponse(
        status="ok",
        scan_id=scan_id,
        message=f"Scan status: {result.status.value}",
        result=result,
    )


@app.get("/scans", response_model=Dict[str, Any])
async def list_scans(status: Optional[ScanStatus] = None, limit: int = 50):
    """List recent scans, optionally filtered by status."""
    scans = list(active_scans.values())

    if status:
        scans = [s for s in scans if s.status == status]

    # Sort by started_at descending
    scans.sort(key=lambda x: x.started_at or "", reverse=True)

    return {"total": len(scans), "scans": scans[:limit]}


@app.delete("/scan/{scan_id}")
async def delete_scan(scan_id: str):
    """Delete a completed scan and its report file."""
    result = active_scans.get(scan_id)

    if not result:
        raise HTTPException(status_code=404, detail=f"Scan not found: {scan_id}")

    if result.status in [ScanStatus.PENDING, ScanStatus.RUNNING]:
        raise HTTPException(status_code=400, detail="Cannot delete a running scan")

    # Delete report file if exists
    if result.raw_json_path:
        try:
            Path(result.raw_json_path).unlink(missing_ok=True)
        except Exception as e:
            logger.warning(f"Failed to delete report file: {e}")

    del active_scans[scan_id]

    return {"status": "ok", "message": f"Scan {scan_id} deleted"}


@app.get("/report/{scan_id}")
async def get_raw_report(scan_id: str):
    """Get the raw JSON report for a completed scan."""
    result = active_scans.get(scan_id)

    if not result:
        raise HTTPException(status_code=404, detail=f"Scan not found: {scan_id}")

    if result.status != ScanStatus.COMPLETED:
        raise HTTPException(
            status_code=400, detail=f"Scan not completed: {result.status}"
        )

    if not result.raw_json_path or not Path(result.raw_json_path).exists():
        raise HTTPException(status_code=404, detail="Report file not found")

    with open(result.raw_json_path, "r", encoding="utf-8") as f:
        return json.load(f)


@app.get("/proxy/stats")
async def get_proxy_stats():
    """Get proxy pool statistics."""
    if not proxy_client:
        return {"error": "Proxy rotation not enabled", "enabled": False}

    stats = await proxy_client.get_pool_stats()
    return stats or {"error": "Failed to get stats", "enabled": True}


# --- Main Entry Point ---
if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=config.PORT,
        reload=os.getenv("ENV", "production") == "development",
    )

```

---

## backend/data-collection-service/maigret-worker/state_store.py

```py
"""
State Storage Module for maigret-worker Service

Provides persistent storage for OSINT scan results using Redis.
Falls back to in-memory storage if Redis is unavailable.
"""

import os
import json
import asyncio
from datetime import datetime
from typing import Optional, Dict, Any, List
from dataclasses import asdict
from enum import Enum

# Redis configuration from environment
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/3")
REDIS_PREFIX = os.getenv("REDIS_PREFIX", "maigret_worker")
SCAN_TTL_DAYS = int(os.getenv("SCAN_TTL_DAYS", "7"))  # Scans expire after 7 days


class EnhancedJSONEncoder(json.JSONEncoder):
    """JSON encoder that handles dataclasses and enums."""
    
    def default(self, o: Any) -> Any:
        if hasattr(o, '__dataclass_fields__'):
            return asdict(o)
        if isinstance(o, Enum):
            return o.value
        if isinstance(o, datetime):
            return o.isoformat()
        return super().default(o)


class StateStore:
    """Persistent state storage with Redis backend."""
    
    def __init__(self):
        self._redis = None
        self._memory_store: Dict[str, Any] = {}
        self._using_redis = False
        self._lock = asyncio.Lock()
    
    async def connect(self) -> bool:
        """Connect to Redis."""
        try:
            import redis.asyncio as redis
            
            self._redis = redis.from_url(
                REDIS_URL,
                encoding="utf-8",
                decode_responses=True,
                socket_connect_timeout=5,
                socket_timeout=5,
            )
            
            await self._redis.ping()
            self._using_redis = True
            
            # Load existing scans from Redis
            await self._load_existing_scans()
            
            return True
            
        except ImportError:
            self._using_redis = False
            return False
        except Exception:
            self._using_redis = False
            return False
    
    async def _load_existing_scans(self):
        """Load existing scans from Redis into memory on startup."""
        if not self._using_redis or not self._redis:
            return
        
        try:
            pattern = f"{REDIS_PREFIX}:scan:*"
            cursor = 0
            
            while True:
                cursor, keys = await self._redis.scan(cursor, match=pattern, count=100)
                for key in keys:
                    scan_id = key.split(":")[-1]
                    scan_data = await self._redis.get(key)
                    if scan_data:
                        self._memory_store[scan_id] = json.loads(scan_data)
                
                if cursor == 0:
                    break
        except Exception:
            pass
    
    async def disconnect(self):
        """Close Redis connection."""
        if self._redis:
            await self._redis.close()
            self._redis = None
            self._using_redis = False
    
    def _key(self, scan_id: str) -> str:
        """Generate Redis key."""
        return f"{REDIS_PREFIX}:scan:{scan_id}"
    
    async def save_scan(self, scan_id: str, scan_result: Any) -> bool:
        """Save scan result."""
        async with self._lock:
            try:
                # Convert to dict if needed
                if hasattr(scan_result, '__dataclass_fields__'):
                    data = asdict(scan_result)
                elif hasattr(scan_result, 'model_dump'):
                    data = scan_result.model_dump()
                elif hasattr(scan_result, 'dict'):
                    data = scan_result.dict()
                elif isinstance(scan_result, dict):
                    data = scan_result
                else:
                    data = scan_result
                
                data_json = json.dumps(data, cls=EnhancedJSONEncoder)
                self._memory_store[scan_id] = json.loads(data_json)
                
                if self._using_redis and self._redis:
                    ttl_seconds = SCAN_TTL_DAYS * 24 * 60 * 60
                    await self._redis.set(self._key(scan_id), data_json, ex=ttl_seconds)
                
                return True
            except Exception:
                return False
    
    async def load_scan(self, scan_id: str) -> Optional[Dict[str, Any]]:
        """Load scan result."""
        if scan_id in self._memory_store:
            return self._memory_store[scan_id]
        
        if self._using_redis and self._redis:
            try:
                data_json = await self._redis.get(self._key(scan_id))
                if data_json:
                    data = json.loads(data_json)
                    self._memory_store[scan_id] = data
                    return data
            except Exception:
                pass
        
        return None
    
    async def delete_scan(self, scan_id: str) -> bool:
        """Delete scan result."""
        async with self._lock:
            self._memory_store.pop(scan_id, None)
            if self._using_redis and self._redis:
                try:
                    await self._redis.delete(self._key(scan_id))
                except Exception:
                    pass
            return True
    
    async def list_scans(self, status: Optional[str] = None, limit: int = 50) -> List[Dict[str, Any]]:
        """List scans with optional filtering."""
        scans = []
        for scan_id, scan_data in list(self._memory_store.items())[-limit:]:
            if status and scan_data.get('status', '').lower() != status.lower():
                continue
            scans.append(scan_data)
        return scans
    
    @property
    def is_redis_connected(self) -> bool:
        return self._using_redis
    
    def get_memory_store(self) -> Dict[str, Any]:
        return self._memory_store


# Singleton instance
_store: Optional[StateStore] = None


async def get_state_store() -> StateStore:
    """Get or create the singleton StateStore instance."""
    global _store
    if _store is None:
        _store = StateStore()
        await _store.connect()
    return _store

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/CollectorApplication.java

```java
package com.newsinsight.collector;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.client.discovery.EnableDiscoveryClient;
import org.springframework.scheduling.annotation.EnableAsync;
import org.springframework.scheduling.annotation.EnableScheduling;

/**
 * NewsInsight Collector Service Application
 * 
 * Spring Boot 기반의 뉴스 수집 서비스
 * - 다양한 소스(RSS, Web Scraping, API)에서 뉴스 수집
 * - 비동기 처리를 통한 효율적인 수집
 * - Consul을 통한 서비스 디스커버리 및 설정 관리
 * - PostgreSQL 데이터베이스를 통한 데이터 저장
 */
@SpringBootApplication
@EnableDiscoveryClient
@EnableAsync
@EnableScheduling
public class CollectorApplication {

    public static void main(String[] args) {
        SpringApplication.run(CollectorApplication.class, args);
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/client/AIDoveClient.java

```java
package com.newsinsight.collector.client;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import io.netty.channel.ChannelOption;
import io.netty.handler.timeout.ReadTimeoutHandler;
import io.netty.handler.timeout.WriteTimeoutHandler;
import jakarta.annotation.PostConstruct;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.MediaType;
import org.springframework.http.client.reactive.ReactorClientHttpConnector;
import org.springframework.stereotype.Component;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Mono;
import reactor.netty.http.client.HttpClient;

import java.time.Duration;
import java.util.Map;
import java.util.concurrent.TimeUnit;

/**
 * Client for AI Dove Agent API.
 * Provides AI-powered text analysis using the self-healing AI service.
 * 
 * API Endpoint: Configurable via COLLECTOR_AIDOVE_BASE_URL or Consul KV
 * 
 * Request:
 *   - chatInput: string (required) - The message/prompt
 *   - sessionId: string (optional) - Session ID for context continuity
 * 
 * Response:
 *   - reply: string - AI response
 *   - tokens_used: integer - Tokens consumed
 *   - model: string - Model used for generation
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class AIDoveClient {

    private final ObjectMapper objectMapper;

    @Value("${collector.ai-dove.base-url:${collector.aidove.base-url:${COLLECTOR_AIDOVE_BASE_URL:https://workflow.nodove.com/webhook/aidove}}}")
    private String baseUrl;

    @Value("${collector.ai-dove.timeout-seconds:${collector.aidove.timeout-seconds:180}}")
    private int timeoutSeconds;

    @Value("${collector.ai-dove.enabled:${collector.aidove.enabled:true}}")
    private boolean enabled;

    private WebClient aiDoveWebClient;

    @PostConstruct
    public void init() {
        // Create dedicated WebClient with extended timeout for AI operations
        HttpClient httpClient = HttpClient.create()
                .option(ChannelOption.CONNECT_TIMEOUT_MILLIS, 30000) // 30s connect timeout
                .responseTimeout(Duration.ofSeconds(timeoutSeconds))
                .doOnConnected(conn -> 
                    conn.addHandlerLast(new ReadTimeoutHandler(timeoutSeconds, TimeUnit.SECONDS))
                        .addHandlerLast(new WriteTimeoutHandler(timeoutSeconds, TimeUnit.SECONDS))
                )
                .followRedirect(true);

        this.aiDoveWebClient = WebClient.builder()
                .clientConnector(new ReactorClientHttpConnector(httpClient))
                .defaultHeader("User-Agent", "NewsInsight-AIDove/1.0")
                .build();
        
        log.info("AIDoveClient initialized with timeout: {}s, baseUrl: {}", timeoutSeconds, baseUrl);
    }

    /**
     * Check if AI Dove client is enabled
     */
    public boolean isEnabled() {
        return enabled;
    }

    /**
     * Send a prompt to AI Dove and get a response.
     * 
     * @param prompt The prompt to send
     * @param sessionId Optional session ID for context continuity
     * @return The AI response
     */
    public Mono<AIDoveResponse> chat(String prompt, String sessionId) {
        if (!enabled) {
            return Mono.error(new IllegalStateException("AI Dove client is disabled"));
        }

        Map<String, Object> payload = sessionId != null
                ? Map.of("chatInput", prompt, "sessionId", sessionId)
                : Map.of("chatInput", prompt);

        return aiDoveWebClient.post()
                .uri(baseUrl)
                .contentType(MediaType.APPLICATION_JSON)
                .bodyValue(payload)
                .retrieve()
                .bodyToMono(String.class)
                .timeout(Duration.ofSeconds(timeoutSeconds))
                .map(this::parseResponse)
                .doOnError(e -> log.error("AI Dove request failed: {}", e.getMessage()));
    }

    /**
     * Stream a response from AI Dove (simulated streaming by splitting response).
     * Note: AI Dove API doesn't support true streaming, so we simulate it.
     */
    public Flux<String> chatStream(String prompt, String sessionId) {
        return chat(prompt, sessionId)
                .flatMapMany(response -> {
                    if (response.reply() == null) {
                        return Flux.empty();
                    }
                    // Split response into chunks for simulated streaming
                    String[] sentences = response.reply().split("(?<=[.!?\\n])\\s*");
                    return Flux.fromArray(sentences)
                            .delayElements(Duration.ofMillis(50));
                })
                .onErrorResume(e -> {
                    log.error("AI Dove stream failed: {}", e.getMessage());
                    return Flux.just("AI 분석 중 오류가 발생했습니다: " + e.getMessage());
                });
    }

    private AIDoveResponse parseResponse(String json) {
        try {
            JsonNode node = objectMapper.readTree(json);
            return new AIDoveResponse(
                    node.has("reply") ? node.get("reply").asText() : null,
                    node.has("tokens_used") ? node.get("tokens_used").asInt() : 0,
                    node.has("model") ? node.get("model").asText() : "unknown"
            );
        } catch (Exception e) {
            log.error("Failed to parse AI Dove response: {}", e.getMessage());
            return new AIDoveResponse(json, 0, "unknown");
        }
    }

    /**
     * AI Dove API response
     */
    public record AIDoveResponse(
            String reply,
            int tokensUsed,
            String model
    ) {}
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/client/Crawl4aiClient.java

```java
package com.newsinsight.collector.client;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.jsoup.Jsoup;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.MediaType;
import org.springframework.stereotype.Component;
import org.springframework.web.reactive.function.client.ClientResponse;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Mono;

import java.net.URI;
import java.time.Duration;

/**
 * Lightweight client for the Crawl4AI service.
 * Tries to call /crawl at the configured base URL and extract text content.
 * If the API responds with JSON, attempts to read common fields like
 * "content", "markdown", "text", or "html". Falls back to plain text.
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class Crawl4aiClient {

    private final WebClient webClient;
    private final ObjectMapper objectMapper;

    @Value("${collector.crawler.base-url:http://web-crawler:11235}")
    private String baseUrl;

    @Value("${collector.http.timeout.read:30000}")
    private int readTimeoutMs;

    @Data
    @Builder
    @AllArgsConstructor
    public static class CrawlResult {
        private String title;
        private String content; // normalized text content
    }

    /**
     * Attempts to crawl the given URL via Crawl4AI. Returns null on failure.
     */
    public CrawlResult crawl(String targetUrl) {
        try {
            String endpoint = baseUrl.endsWith("/") ? baseUrl + "crawl" : baseUrl + "/crawl";

            Mono<CrawlResult> mono = webClient
                    .get()
                    .uri(uriBuilder -> {
                        URI uri = URI.create(endpoint);
                        return uriBuilder
                                .scheme(uri.getScheme())
                                .host(uri.getHost())
                                .port(uri.getPort())
                                .path(uri.getPath())
                                .queryParam("url", targetUrl)
                                .build();
                    })
                    .accept(MediaType.APPLICATION_JSON, MediaType.TEXT_PLAIN, MediaType.ALL)
                    .exchangeToMono(response -> handleResponse(response))
                    .timeout(Duration.ofMillis(Math.max(1000, readTimeoutMs)));

            return mono.onErrorResume(e -> {
                        log.warn("Crawl4AI request failed for {}: {}", targetUrl, e.toString());
                        return Mono.empty();
                    })
                    .block();
        } catch (Exception e) {
            log.warn("Crawl4AI client error for {}: {}", targetUrl, e.toString());
            return null;
        }
    }

    private Mono<CrawlResult> handleResponse(ClientResponse response) {
        MediaType ct = response.headers().contentType().orElse(MediaType.APPLICATION_JSON);
        if (ct.isCompatibleWith(MediaType.APPLICATION_JSON) || ct.getSubtype().contains("json")) {
            return response.bodyToMono(String.class).flatMap(body -> {
                try {
                    JsonNode node = objectMapper.readTree(body);
                    String title = textOf(node, "title");
                    String content = firstNonBlank(
                            textOf(node, "content"),
                            textOf(node, "markdown"),
                            textOf(node, "text"),
                            stripHtml(textOf(node, "html"))
                    );
                    if (isBlank(content)) return Mono.empty();
                    return Mono.just(CrawlResult.builder()
                            .title(title)
                            .content(normalize(content))
                            .build());
                } catch (Exception ex) {
                    log.debug("Failed to parse JSON from Crawl4AI: {}", ex.toString());
                    return Mono.empty();
                }
            });
        } else {
            return response.bodyToMono(String.class).map(body ->
                    CrawlResult.builder()
                            .title(null)
                            .content(normalize(stripHtml(body)))
                            .build()
            );
        }
    }

    private static boolean isBlank(String s) {
        return s == null || s.trim().isEmpty();
    }

    private static String firstNonBlank(String... values) {
        if (values == null) return null;
        for (String v : values) {
            if (!isBlank(v)) return v;
        }
        return null;
    }

    private static String normalize(String s) {
        if (s == null) return null;
        return s.replaceAll("\\s+", " ").trim();
    }

    private static String textOf(JsonNode node, String field) {
        if (node == null || node.isNull()) return null;
        JsonNode v = node.get(field);
        if (v == null || v.isNull()) return null;
        if (v.isTextual()) return v.asText();
        return v.toString();
    }

    private static String stripHtml(String html) {
        if (html == null) return null;
        try {
            return Jsoup.parse(html).text();
        } catch (Exception ignored) {
            return html;
        }
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/client/OpenAICompatibleClient.java

```java
package com.newsinsight.collector.client;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import io.netty.channel.ChannelOption;
import io.netty.handler.timeout.ReadTimeoutHandler;
import io.netty.handler.timeout.WriteTimeoutHandler;
import jakarta.annotation.PostConstruct;
import lombok.Getter;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.HttpHeaders;
import org.springframework.http.MediaType;
import org.springframework.http.client.reactive.ReactorClientHttpConnector;
import org.springframework.stereotype.Component;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Flux;
import reactor.netty.http.client.HttpClient;

import java.time.Duration;
import java.util.List;
import java.util.Map;
import java.util.concurrent.TimeUnit;

/**
 * OpenAI-compatible client that can connect to various LLM providers.
 * Supports: OpenAI, OpenRouter, Ollama, Azure OpenAI, and custom endpoints.
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class OpenAICompatibleClient {

    private final ObjectMapper objectMapper;
    private WebClient webClient;

    // OpenAI settings
    @Value("${LLM_OPENAI_API_KEY:${OPENAI_API_KEY:}}")
    private String openaiApiKey;

    @Value("${LLM_OPENAI_BASE_URL:https://api.openai.com/v1}")
    private String openaiBaseUrl;

    @Value("${LLM_OPENAI_MODEL:gpt-4o-mini}")
    private String openaiModel;

    // OpenRouter settings
    @Value("${LLM_OPENROUTER_API_KEY:${OPENROUTER_API_KEY:}}")
    private String openrouterApiKey;

    @Value("${LLM_OPENROUTER_BASE_URL:https://openrouter.ai/api/v1}")
    private String openrouterBaseUrl;

    @Value("${LLM_OPENROUTER_MODEL:anthropic/claude-3.5-sonnet}")
    private String openrouterModel;

    // Ollama settings
    @Value("${LLM_OLLAMA_BASE_URL:http://localhost:11434/v1}")
    private String ollamaBaseUrl;

    @Value("${LLM_OLLAMA_MODEL:llama3.2}")
    private String ollamaModel;

    // Azure OpenAI settings
    @Value("${LLM_AZURE_API_KEY:${AZURE_OPENAI_API_KEY:}}")
    private String azureApiKey;

    @Value("${LLM_AZURE_ENDPOINT:}")
    private String azureEndpoint;

    @Value("${LLM_AZURE_DEPLOYMENT:gpt-4o}")
    private String azureDeployment;

    @Value("${LLM_AZURE_API_VERSION:2024-02-15-preview}")
    private String azureApiVersion;

    // Custom endpoint settings
    @Value("${LLM_CUSTOM_BASE_URL:}")
    private String customBaseUrl;

    @Value("${LLM_CUSTOM_API_KEY:}")
    private String customApiKey;

    @Value("${LLM_CUSTOM_MODEL:}")
    private String customModel;

    @Value("${collector.openai.timeout-seconds:120}")
    private int timeoutSeconds;

    @Getter
    private ProviderStatus providerStatus;

    @PostConstruct
    public void init() {
        HttpClient httpClient = HttpClient.create()
                .option(ChannelOption.CONNECT_TIMEOUT_MILLIS, 30000)
                .responseTimeout(Duration.ofSeconds(timeoutSeconds))
                .doOnConnected(conn ->
                        conn.addHandlerLast(new ReadTimeoutHandler(timeoutSeconds, TimeUnit.SECONDS))
                            .addHandlerLast(new WriteTimeoutHandler(timeoutSeconds, TimeUnit.SECONDS))
                )
                .followRedirect(true);

        this.webClient = WebClient.builder()
                .clientConnector(new ReactorClientHttpConnector(httpClient))
                .defaultHeader("User-Agent", "NewsInsight-OpenAI/1.0")
                .codecs(configurer -> configurer.defaultCodecs().maxInMemorySize(10 * 1024 * 1024))
                .build();

        this.providerStatus = checkProviderStatus();
        log.info("OpenAICompatibleClient initialized - Available providers: {}", providerStatus);
    }

    /**
     * Check which providers are available
     */
    public ProviderStatus checkProviderStatus() {
        return new ProviderStatus(
                isNotBlank(openaiApiKey),
                isNotBlank(openrouterApiKey),
                true, // Ollama is always potentially available (local)
                isNotBlank(azureApiKey) && isNotBlank(azureEndpoint),
                isNotBlank(customBaseUrl)
        );
    }

    /**
     * Check if any OpenAI-compatible provider is enabled
     */
    public boolean isEnabled() {
        return providerStatus.openai() || providerStatus.openrouter() 
                || providerStatus.ollama() || providerStatus.azure() 
                || providerStatus.custom();
    }

    /**
     * Check if OpenAI is enabled
     */
    public boolean isOpenAIEnabled() {
        return isNotBlank(openaiApiKey);
    }

    /**
     * Check if OpenRouter is enabled
     */
    public boolean isOpenRouterEnabled() {
        return isNotBlank(openrouterApiKey);
    }

    /**
     * Check if Ollama is enabled (always returns true as it's local)
     */
    public boolean isOllamaEnabled() {
        return true;
    }

    /**
     * Check if Azure OpenAI is enabled
     */
    public boolean isAzureEnabled() {
        return isNotBlank(azureApiKey) && isNotBlank(azureEndpoint);
    }

    /**
     * Check if Custom endpoint is enabled
     */
    public boolean isCustomEnabled() {
        return isNotBlank(customBaseUrl);
    }

    /**
     * Stream completion from OpenAI
     */
    public Flux<String> streamFromOpenAI(String prompt) {
        if (!isOpenAIEnabled()) {
            return Flux.error(new IllegalStateException("OpenAI API key is not configured"));
        }
        return streamCompletion(openaiBaseUrl, openaiApiKey, openaiModel, prompt, "OpenAI");
    }

    /**
     * Stream completion from OpenRouter
     */
    public Flux<String> streamFromOpenRouter(String prompt) {
        if (!isOpenRouterEnabled()) {
            return Flux.error(new IllegalStateException("OpenRouter API key is not configured"));
        }
        return streamCompletion(openrouterBaseUrl, openrouterApiKey, openrouterModel, prompt, "OpenRouter");
    }

    /**
     * Stream completion from Ollama
     */
    public Flux<String> streamFromOllama(String prompt) {
        return streamCompletion(ollamaBaseUrl, null, ollamaModel, prompt, "Ollama");
    }

    /**
     * Stream completion from Azure OpenAI
     */
    public Flux<String> streamFromAzure(String prompt) {
        if (!isAzureEnabled()) {
            return Flux.error(new IllegalStateException("Azure OpenAI is not configured"));
        }
        String url = String.format("%s/openai/deployments/%s/chat/completions?api-version=%s",
                azureEndpoint, azureDeployment, azureApiVersion);
        return streamCompletionAzure(url, azureApiKey, prompt);
    }

    /**
     * Stream completion from Custom endpoint
     */
    public Flux<String> streamFromCustom(String prompt) {
        if (!isCustomEnabled()) {
            return Flux.error(new IllegalStateException("Custom endpoint is not configured"));
        }
        return streamCompletion(customBaseUrl, customApiKey, customModel, prompt, "Custom");
    }

    /**
     * Generic OpenAI-compatible streaming completion
     */
    private Flux<String> streamCompletion(String baseUrl, String apiKey, String model, String prompt, String providerName) {
        String url = baseUrl.endsWith("/") ? baseUrl + "chat/completions" : baseUrl + "/chat/completions";

        // System message to guide the AI to respond directly in report format
        String systemMessage = """
                당신은 뉴스 분석 전문가입니다. 사용자의 요청에 대해 직접 보고서 형식으로 답변해주세요.
                "알겠습니다", "네", "검색하겠습니다" 등의 서두 없이 바로 분석 결과를 작성하세요.
                요청받은 형식(마크다운 등)을 정확히 따르세요.
                """;

        Map<String, Object> body = Map.of(
                "model", model,
                "stream", true,
                "messages", List.of(
                        Map.of("role", "system", "content", systemMessage),
                        Map.of("role", "user", "content", prompt)
                )
        );

        log.debug("Calling {} API: {} with model {}", providerName, url, model);

        WebClient.RequestBodySpec request = webClient.post()
                .uri(url)
                .contentType(MediaType.APPLICATION_JSON)
                .accept(MediaType.TEXT_EVENT_STREAM);

        if (apiKey != null && !apiKey.isBlank()) {
            request = request.header(HttpHeaders.AUTHORIZATION, "Bearer " + apiKey);
        }

        return request
                .bodyValue(body)
                .retrieve()
                .bodyToFlux(String.class)
                .timeout(Duration.ofSeconds(timeoutSeconds))
                .doOnSubscribe(s -> log.debug("Starting {} stream request", providerName))
                .filter(chunk -> chunk != null && !chunk.isBlank() && !chunk.equals("[DONE]"))
                .mapNotNull(this::extractContent)
                .doOnComplete(() -> log.debug("{} stream completed", providerName))
                .doOnError(e -> log.error("{} stream failed: {}", providerName, e.getMessage()));
    }

    /**
     * Azure-specific streaming (uses api-key header instead of Authorization)
     */
    private Flux<String> streamCompletionAzure(String url, String apiKey, String prompt) {
        // System message to guide the AI to respond directly in report format
        String systemMessage = """
                당신은 뉴스 분석 전문가입니다. 사용자의 요청에 대해 직접 보고서 형식으로 답변해주세요.
                "알겠습니다", "네", "검색하겠습니다" 등의 서두 없이 바로 분석 결과를 작성하세요.
                요청받은 형식(마크다운 등)을 정확히 따르세요.
                """;

        Map<String, Object> body = Map.of(
                "stream", true,
                "messages", List.of(
                        Map.of("role", "system", "content", systemMessage),
                        Map.of("role", "user", "content", prompt)
                )
        );

        log.debug("Calling Azure OpenAI API: {}", url);

        return webClient.post()
                .uri(url)
                .header("api-key", apiKey)
                .contentType(MediaType.APPLICATION_JSON)
                .accept(MediaType.TEXT_EVENT_STREAM)
                .bodyValue(body)
                .retrieve()
                .bodyToFlux(String.class)
                .timeout(Duration.ofSeconds(timeoutSeconds))
                .doOnSubscribe(s -> log.debug("Starting Azure stream request"))
                .filter(chunk -> chunk != null && !chunk.isBlank() && !chunk.equals("[DONE]"))
                .mapNotNull(this::extractContent)
                .doOnComplete(() -> log.debug("Azure stream completed"))
                .doOnError(e -> log.error("Azure stream failed: {}", e.getMessage()));
    }

    /**
     * Extract content from SSE chunk
     */
    private String extractContent(String chunk) {
        try {
            // Handle SSE format: data: {...}
            String json = chunk.startsWith("data:") ? chunk.substring(5).trim() : chunk;
            if (json.isBlank() || json.equals("[DONE]")) {
                return null;
            }

            JsonNode node = objectMapper.readTree(json);
            JsonNode choices = node.get("choices");
            if (choices != null && choices.isArray() && !choices.isEmpty()) {
                JsonNode delta = choices.get(0).get("delta");
                if (delta != null && delta.has("content")) {
                    return delta.get("content").asText();
                }
            }
            return null;
        } catch (Exception e) {
            log.trace("Failed to parse chunk: {}", chunk);
            return null;
        }
    }

    private boolean isNotBlank(String str) {
        return str != null && !str.isBlank();
    }

    /**
     * Provider availability status
     */
    public record ProviderStatus(
            boolean openai,
            boolean openrouter,
            boolean ollama,
            boolean azure,
            boolean custom
    ) {
        @Override
        public String toString() {
            StringBuilder sb = new StringBuilder("[");
            if (openai) sb.append("OpenAI, ");
            if (openrouter) sb.append("OpenRouter, ");
            if (ollama) sb.append("Ollama, ");
            if (azure) sb.append("Azure, ");
            if (custom) sb.append("Custom, ");
            if (sb.length() > 1) {
                sb.setLength(sb.length() - 2);
            }
            sb.append("]");
            return sb.toString();
        }
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/client/PerplexityClient.java

```java
package com.newsinsight.collector.client;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import io.netty.channel.ChannelOption;
import io.netty.handler.timeout.ReadTimeoutHandler;
import io.netty.handler.timeout.WriteTimeoutHandler;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.HttpHeaders;
import org.springframework.http.MediaType;
import org.springframework.http.client.reactive.ReactorClientHttpConnector;
import org.springframework.stereotype.Component;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Flux;
import reactor.netty.http.client.HttpClient;

import jakarta.annotation.PostConstruct;
import java.time.Duration;
import java.util.List;
import java.util.Map;
import java.util.concurrent.TimeUnit;

@Component
@Slf4j
public class PerplexityClient {

    private final ObjectMapper objectMapper;
    private WebClient perplexityWebClient;

    @Value("${PERPLEXITY_API_KEY:}")
    private String apiKey;

    @Value("${PERPLEXITY_BASE_URL:https://api.perplexity.ai}")
    private String baseUrl;

    @Value("${PERPLEXITY_MODEL:llama-3.1-sonar-large-128k-online}")
    private String model;

    @Value("${collector.perplexity.timeout-seconds:120}")
    private int timeoutSeconds;

    public PerplexityClient(ObjectMapper objectMapper) {
        this.objectMapper = objectMapper;
    }

    @PostConstruct
    public void init() {
        // Create dedicated WebClient with longer timeout for AI streaming
        HttpClient httpClient = HttpClient.create()
                .option(ChannelOption.CONNECT_TIMEOUT_MILLIS, 30000)
                .responseTimeout(Duration.ofSeconds(timeoutSeconds))
                .doOnConnected(conn ->
                        conn.addHandlerLast(new ReadTimeoutHandler(timeoutSeconds, TimeUnit.SECONDS))
                            .addHandlerLast(new WriteTimeoutHandler(timeoutSeconds, TimeUnit.SECONDS))
                )
                .followRedirect(true);

        this.perplexityWebClient = WebClient.builder()
                .clientConnector(new ReactorClientHttpConnector(httpClient))
                .defaultHeader("User-Agent", "NewsInsight-Collector/1.0")
                .codecs(configurer -> configurer.defaultCodecs().maxInMemorySize(10 * 1024 * 1024))
                .build();

        log.info("PerplexityClient initialized with timeout: {}s, enabled: {}", timeoutSeconds, isEnabled());
    }

    /**
     * Check if Perplexity API is enabled (API key is configured)
     */
    public boolean isEnabled() {
        return apiKey != null && !apiKey.isBlank();
    }

    public Flux<String> streamCompletion(String prompt) {
        if (!isEnabled()) {
            return Flux.error(new IllegalStateException("Perplexity API key is not configured"));
        }

        String url = baseUrl.endsWith("/") ? baseUrl + "chat/completions" : baseUrl + "/chat/completions";

        // System message to guide the AI to respond directly in report format
        String systemMessage = """
                당신은 뉴스 분석 전문가입니다. 사용자의 요청에 대해 직접 보고서 형식으로 답변해주세요.
                "알겠습니다", "네", "검색하겠습니다" 등의 서두 없이 바로 분석 결과를 작성하세요.
                요청받은 형식(마크다운 등)을 정확히 따르세요.
                """;

        Map<String, Object> body = Map.of(
                "model", model,
                "stream", true,
                "messages", List.of(
                        Map.of("role", "system", "content", systemMessage),
                        Map.of("role", "user", "content", prompt)
                )
        );

        log.debug("Calling Perplexity API: {} with timeout {}s", url, timeoutSeconds);

        return perplexityWebClient.post()
                .uri(url)
                .header(HttpHeaders.AUTHORIZATION, "Bearer " + apiKey)
                .contentType(MediaType.APPLICATION_JSON)
                .accept(MediaType.TEXT_EVENT_STREAM)
                .bodyValue(body)
                .retrieve()
                .bodyToFlux(String.class)
                .timeout(Duration.ofSeconds(timeoutSeconds))
                .doOnSubscribe(s -> log.debug("Starting Perplexity stream request"))
                .doOnNext(chunk -> log.debug("Perplexity raw chunk: {}", chunk))
                .doOnError(e -> log.error("Perplexity API error: {}", e.getMessage()))
                .doOnComplete(() -> log.debug("Perplexity stream completed"))
                .flatMap(this::extractTextFromChunk);
    }

    private Flux<String> extractTextFromChunk(String chunk) {
        if (chunk == null || chunk.isBlank()) {
            return Flux.empty();
        }

        String trimmed = chunk.trim();
        if ("[DONE]".equalsIgnoreCase(trimmed) || "data: [DONE]".equalsIgnoreCase(trimmed)) {
            return Flux.empty();
        }

        String json;
        if (trimmed.startsWith("data:")) {
            json = trimmed.substring(5).trim();
        } else {
            json = trimmed;
        }

        if (json.isEmpty()) {
            return Flux.empty();
        }

        try {
            JsonNode root = objectMapper.readTree(json);
            JsonNode choices = root.get("choices");
            if (choices == null || !choices.isArray() || choices.isEmpty()) {
                return Flux.empty();
            }

            JsonNode choice = choices.get(0);
            JsonNode delta = choice.get("delta");
            if (delta != null && delta.has("content")) {
                String text = delta.get("content").asText();
                if (text != null && !text.isEmpty()) {
                    return Flux.just(text);
                }
            }

            JsonNode message = choice.get("message");
            if (message != null && message.has("content")) {
                String text = message.get("content").asText();
                if (text != null && !text.isEmpty()) {
                    return Flux.just(text);
                }
            }
        } catch (Exception e) {
            log.warn("Failed to parse Perplexity chunk: {}", chunk, e);
        }

        return Flux.empty();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/AsyncConfig.java

```java
package com.newsinsight.collector.config;

import lombok.extern.slf4j.Slf4j;
import org.springframework.aop.interceptor.AsyncUncaughtExceptionHandler;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.scheduling.annotation.AsyncConfigurer;
import org.springframework.scheduling.annotation.EnableAsync;
import org.springframework.scheduling.annotation.EnableScheduling;
import org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor;

import java.util.concurrent.Executor;

@Configuration
@EnableAsync
@EnableScheduling
@Slf4j
public class AsyncConfig implements AsyncConfigurer {

    @Value("${async.executor.core-pool-size:5}")
    private int corePoolSize;

    @Value("${async.executor.max-pool-size:20}")
    private int maxPoolSize;

    @Value("${async.executor.queue-capacity:100}")
    private int queueCapacity;

    @Value("${async.chat-sync.core-pool-size:3}")
    private int chatSyncCorePoolSize;

    @Value("${async.chat-sync.max-pool-size:10}")
    private int chatSyncMaxPoolSize;

    @Value("${async.chat-sync.queue-capacity:50}")
    private int chatSyncQueueCapacity;

    /**
     * 기본 비동기 작업 실행자
     */
    @Bean(name = "taskExecutor")
    public Executor taskExecutor() {
        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
        executor.setCorePoolSize(corePoolSize);
        executor.setMaxPoolSize(maxPoolSize);
        executor.setQueueCapacity(queueCapacity);
        executor.setThreadNamePrefix("async-collection-");
        executor.setWaitForTasksToCompleteOnShutdown(true);
        executor.setAwaitTerminationSeconds(60);
        executor.setRejectedExecutionHandler((r, e) -> 
                log.warn("Task rejected from taskExecutor: {}", r.toString()));
        executor.initialize();
        return executor;
    }

    /**
     * 채팅 동기화 전용 실행자
     */
    @Bean(name = "chatSyncExecutor")
    public Executor chatSyncExecutor() {
        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
        executor.setCorePoolSize(chatSyncCorePoolSize);
        executor.setMaxPoolSize(chatSyncMaxPoolSize);
        executor.setQueueCapacity(chatSyncQueueCapacity);
        executor.setThreadNamePrefix("chat-sync-");
        executor.setWaitForTasksToCompleteOnShutdown(true);
        executor.setAwaitTerminationSeconds(120); // 동기화 완료 대기 2분
        executor.setRejectedExecutionHandler((r, e) -> 
                log.warn("Task rejected from chatSyncExecutor: {}", r.toString()));
        executor.initialize();
        return executor;
    }

    /**
     * 벡터 임베딩 전용 실행자
     */
    @Bean(name = "embeddingExecutor")
    public Executor embeddingExecutor() {
        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
        executor.setCorePoolSize(2);
        executor.setMaxPoolSize(5);
        executor.setQueueCapacity(100);
        executor.setThreadNamePrefix("embedding-");
        executor.setWaitForTasksToCompleteOnShutdown(true);
        executor.setAwaitTerminationSeconds(180); // 임베딩 완료 대기 3분
        executor.setRejectedExecutionHandler((r, e) -> 
                log.warn("Task rejected from embeddingExecutor: {}", r.toString()));
        executor.initialize();
        return executor;
    }

    @Override
    public Executor getAsyncExecutor() {
        return taskExecutor();
    }

    @Override
    public AsyncUncaughtExceptionHandler getAsyncUncaughtExceptionHandler() {
        return (ex, method, params) -> {
            log.error("Uncaught async exception in method {}: {}", method.getName(), ex.getMessage(), ex);
        };
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/AutoCrawlInitializer.java

```java
package com.newsinsight.collector.config;

import com.newsinsight.collector.entity.autocrawl.CrawlTarget;
import com.newsinsight.collector.service.autocrawl.AutoCrawlDiscoveryService;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;
import org.springframework.boot.context.event.ApplicationReadyEvent;
import org.springframework.context.event.EventListener;
import org.springframework.stereotype.Component;

import java.util.Arrays;
import java.util.List;

/**
 * AutoCrawl 초기화 컴포넌트.
 * 
 * 애플리케이션 시작 시 seed URL들을 크롤링 큐에 자동으로 추가합니다.
 * docker-compose 실행 시 즉시 크롤링이 시작되도록 합니다.
 */
@Component
@RequiredArgsConstructor
@Slf4j
@ConditionalOnProperty(name = "autocrawl.enabled", havingValue = "true", matchIfMissing = false)
public class AutoCrawlInitializer {

    private final AutoCrawlDiscoveryService autoCrawlDiscoveryService;

    @Value("${autocrawl.seed.enabled:true}")
    private boolean seedEnabled;

    @Value("${autocrawl.seed.urls:}")
    private String seedUrlsConfig;

    @Value("${autocrawl.seed.keywords:뉴스,정치,경제,사회,IT,기술}")
    private String seedKeywords;

    @Value("${autocrawl.seed.priority:70}")
    private int seedPriority;

    /**
     * 기본 seed URL 목록 (한국 주요 뉴스 포털)
     */
    private static final List<String> DEFAULT_SEED_URLS = List.of(
            // 네이버 뉴스 메인
            "https://news.naver.com",
            "https://news.naver.com/section/100",  // 정치
            "https://news.naver.com/section/101",  // 경제
            "https://news.naver.com/section/102",  // 사회
            "https://news.naver.com/section/103",  // 생활/문화
            "https://news.naver.com/section/104",  // 세계
            "https://news.naver.com/section/105",  // IT/과학
            
            // 다음 뉴스 메인
            "https://news.daum.net",
            "https://news.daum.net/politics",
            "https://news.daum.net/economic",
            "https://news.daum.net/society",
            "https://news.daum.net/culture",
            "https://news.daum.net/digital",
            
            // 주요 언론사 메인
            "https://www.chosun.com",
            "https://www.donga.com",
            "https://www.joongang.co.kr",
            "https://www.hani.co.kr",
            "https://www.khan.co.kr",
            "https://www.yna.co.kr",
            
            // IT/기술 뉴스
            "https://www.etnews.com",
            "https://zdnet.co.kr",
            "https://www.bloter.net"
    );

    @EventListener(ApplicationReadyEvent.class)
    public void initializeSeedUrls() {
        if (!seedEnabled) {
            log.info("[AutoCrawl Init] Seed initialization is disabled. Set AUTOCRAWL_SEED_ENABLED=true to enable.");
            return;
        }

        log.info("[AutoCrawl Init] Starting seed URL initialization...");

        try {
            List<String> seedUrls = getSeedUrls();
            
            if (seedUrls.isEmpty()) {
                log.warn("[AutoCrawl Init] No seed URLs configured");
                return;
            }

            log.info("[AutoCrawl Init] Adding {} seed URLs to crawl queue", seedUrls.size());

            List<CrawlTarget> addedTargets = autoCrawlDiscoveryService.addManualTargets(
                    seedUrls,
                    seedKeywords,
                    seedPriority
            );

            log.info("[AutoCrawl Init] Successfully added {} seed URLs to crawl queue (skipped {} duplicates)",
                    addedTargets.size(),
                    seedUrls.size() - addedTargets.size());

            // 추가된 URL 로깅
            if (!addedTargets.isEmpty() && log.isDebugEnabled()) {
                addedTargets.forEach(target -> 
                    log.debug("[AutoCrawl Init] Added: {} (priority={})", target.getUrl(), target.getPriority())
                );
            }

        } catch (Exception e) {
            log.error("[AutoCrawl Init] Failed to initialize seed URLs: {}", e.getMessage(), e);
        }
    }

    /**
     * Seed URL 목록 가져오기
     * 환경 변수로 설정된 URL이 있으면 사용, 없으면 기본 URL 사용
     */
    private List<String> getSeedUrls() {
        if (seedUrlsConfig != null && !seedUrlsConfig.isBlank()) {
            // 환경 변수로 설정된 커스텀 URL 사용 (콤마로 구분)
            List<String> customUrls = Arrays.stream(seedUrlsConfig.split(","))
                    .map(String::trim)
                    .filter(url -> !url.isBlank())
                    .filter(url -> url.startsWith("http://") || url.startsWith("https://"))
                    .toList();
            
            if (!customUrls.isEmpty()) {
                log.info("[AutoCrawl Init] Using {} custom seed URLs from configuration", customUrls.size());
                return customUrls;
            }
        }

        // 기본 seed URL 사용
        log.info("[AutoCrawl Init] Using {} default seed URLs", DEFAULT_SEED_URLS.size());
        return DEFAULT_SEED_URLS;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/DataSourceInitializer.java

```java
package com.newsinsight.collector.config;

import com.newsinsight.collector.entity.DataSource;
import com.newsinsight.collector.entity.SourceType;
import com.newsinsight.collector.repository.DataSourceRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.boot.CommandLineRunner;
import org.springframework.stereotype.Component;
import org.springframework.transaction.annotation.Transactional;

import java.util.ArrayList;
import java.util.List;

/**
 * 웹 검색 소스 자동 초기화.
 * 
 * 애플리케이션 시작 시 기본 웹 검색 소스(네이버, 다음, 구글 뉴스)를
 * DB에 자동으로 등록합니다. 이미 등록된 소스는 건너뜁니다.
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class DataSourceInitializer implements CommandLineRunner {

    private final DataSourceRepository dataSourceRepository;

    @Override
    @Transactional
    public void run(String... args) {
        log.info("Initializing default web search sources...");
        
        List<DataSource> defaultSources = createDefaultWebSearchSources();
        int initialized = 0;
        
        for (DataSource source : defaultSources) {
            // 이미 존재하는지 확인 (이름으로)
            if (dataSourceRepository.findByName(source.getName()).isEmpty()) {
                dataSourceRepository.save(source);
                initialized++;
                log.info("Initialized web search source: {}", source.getName());
            } else {
                log.debug("Web search source already exists: {}", source.getName());
            }
        }
        
        if (initialized > 0) {
            log.info("Initialized {} new web search sources", initialized);
        } else {
            log.info("All default web search sources already exist");
        }
        
        // 현재 활성화된 웹 검색 소스 수 로깅
        long activeCount = dataSourceRepository.findActiveWebSearchSources().size();
        log.info("Total active web search sources: {}", activeCount);
    }

    /**
     * 기본 웹 검색 소스 생성
     */
    private List<DataSource> createDefaultWebSearchSources() {
        List<DataSource> sources = new ArrayList<>();
        
        // 1. 네이버 뉴스 (최고 우선순위)
        sources.add(DataSource.builder()
                .name("네이버 뉴스")
                .url("https://news.naver.com")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://search.naver.com/search.naver?where=news&query={query}")
                .searchPriority(10)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"country\":\"KR\",\"language\":\"ko\",\"portal\":\"naver\"}")
                .build());
        
        // 2. 다음 뉴스
        sources.add(DataSource.builder()
                .name("다음 뉴스")
                .url("https://news.daum.net")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://search.daum.net/search?w=news&q={query}")
                .searchPriority(20)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"country\":\"KR\",\"language\":\"ko\",\"portal\":\"daum\"}")
                .build());
        
        // 3. 구글 뉴스 (한국)
        sources.add(DataSource.builder()
                .name("구글 뉴스")
                .url("https://news.google.com")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://news.google.com/search?q={query}&hl=ko&gl=KR")
                .searchPriority(30)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"country\":\"KR\",\"language\":\"ko\",\"portal\":\"google\"}")
                .build());
        
        // 4. 네이트 뉴스 (비활성화 상태로 추가 - 사용자가 필요시 활성화 가능)
        sources.add(DataSource.builder()
                .name("네이트 뉴스")
                .url("https://news.nate.com")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://search.nate.com/search/all.html?q={query}&csn=1")
                .searchPriority(40)
                .isActive(false)
                .collectionFrequency(3600)
                .metadataJson("{\"country\":\"KR\",\"language\":\"ko\",\"portal\":\"nate\"}")
                .build());
        
        // 5. 줌 뉴스 (비활성화 상태로 추가)
        sources.add(DataSource.builder()
                .name("줌 뉴스")
                .url("https://news.zum.com")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://search.zum.com/search.zum?method=news&query={query}")
                .searchPriority(50)
                .isActive(false)
                .collectionFrequency(3600)
                .metadataJson("{\"country\":\"KR\",\"language\":\"ko\",\"portal\":\"zum\"}")
                .build());
        
        return sources;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/DataSourceSeeder.java

```java
package com.newsinsight.collector.config;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.newsinsight.collector.entity.BrowserAgentConfig;
import com.newsinsight.collector.entity.DataSource;
import com.newsinsight.collector.entity.SourceType;
import com.newsinsight.collector.repository.DataSourceRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.boot.ApplicationArguments;
import org.springframework.boot.ApplicationRunner;
import org.springframework.context.annotation.Profile;
import org.springframework.stereotype.Component;
import org.springframework.transaction.annotation.Transactional;

import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.stream.Collectors;

/**
 * Seeds the database with default Korean news sources on application startup.
 * Only runs if the data_sources table is empty.
 * 
 * Sources can be configured via:
 * 1. application.yml (collector.data-sources.sources)
 * 2. Default hardcoded sources (if no external config provided)
 * 
 * Profiles:
 * - default: Runs automatically
 * - no-seed: Skip seeding (for production or when using external config)
 */
@Component
@Profile("!no-seed")
@RequiredArgsConstructor
@Slf4j
public class DataSourceSeeder implements ApplicationRunner {

    private final DataSourceRepository dataSourceRepository;
    private final DataSourcesConfig dataSourcesConfig;
    private final ObjectMapper objectMapper;

    @Override
    @Transactional
    public void run(ApplicationArguments args) {
        if (!dataSourcesConfig.isSeedEnabled()) {
            log.info("DataSource seeding is disabled via configuration.");
            return;
        }

        log.info("Seeding data sources...");
        
        List<DataSource> sources;
        
        // Check if external configuration is provided
        if (dataSourcesConfig.getSources() != null && !dataSourcesConfig.getSources().isEmpty()) {
            log.info("Using {} data sources from external configuration.", dataSourcesConfig.getSources().size());
            sources = dataSourcesConfig.getSources().stream()
                    .map(this::convertToDataSource)
                    .collect(Collectors.toList());
        } else {
            log.info("No external configuration found, using default Korean news sources.");
            sources = createDefaultSources();
        }

        int created = 0;
        int skipped = 0;
        for (DataSource desired : sources) {
            DataSource existing = dataSourceRepository
                    .findByUrl(desired.getUrl())
                    .or(() -> dataSourceRepository.findByName(desired.getName()))
                    .orElse(null);

            if (existing == null) {
                dataSourceRepository.save(desired);
                created++;
                continue;
            }
            skipped++;
        }

        log.info(
                "Successfully seeded data sources. created={}, skipped={}, totalDesired={}",
                created,
                skipped,
                sources.size()
        );
    }

    /**
     * Convert external configuration entry to DataSource entity
     */
    private DataSource convertToDataSource(DataSourcesConfig.DataSourceEntry entry) {
        // Build metadata JSON from entry fields
        Map<String, String> metadata = new HashMap<>();
        if (entry.getRegion() != null) metadata.put("region", entry.getRegion());
        if (entry.getLanguage() != null) metadata.put("language", entry.getLanguage());
        if (entry.getReliability() != null) metadata.put("reliability", entry.getReliability());
        if (entry.getCategory() != null) metadata.put("category", entry.getCategory());
        if (entry.getStance() != null) metadata.put("stance", entry.getStance());
        
        // Merge with any additional metadata provided
        if (entry.getMetadata() != null) {
            metadata.putAll(entry.getMetadata());
        }
        
        String metadataJson;
        try {
            metadataJson = objectMapper.writeValueAsString(metadata);
        } catch (JsonProcessingException e) {
            log.warn("Failed to serialize metadata for source {}: {}", entry.getName(), e.getMessage());
            metadataJson = "{}";
        }
        
        DataSource.DataSourceBuilder builder = DataSource.builder()
                .name(entry.getName())
                .url(entry.getUrl())
                .sourceType(parseSourceType(entry.getSourceType()))
                .isActive(entry.isActive())
                .collectionFrequency(entry.getCollectionFrequency())
                .metadataJson(metadataJson);
        
        // Add search-related fields for WEB_SEARCH sources
        if (entry.getSearchUrlTemplate() != null) {
            builder.searchUrlTemplate(entry.getSearchUrlTemplate());
        }
        if (entry.getSearchPriority() != null) {
            builder.searchPriority(entry.getSearchPriority());
        }
        
        return builder.build();
    }

    private SourceType parseSourceType(String type) {
        if (type == null) return SourceType.RSS;
        try {
            return SourceType.valueOf(type.toUpperCase());
        } catch (IllegalArgumentException e) {
            log.warn("Unknown source type '{}', defaulting to RSS", type);
            return SourceType.RSS;
        }
    }

    private List<DataSource> createDefaultSources() {
        return List.of(
            // ========== Korean Major News (High Reliability) ==========
            DataSource.builder()
                .name("연합뉴스 (Yonhap)")
                .url("https://www.yna.co.kr/rss/news.xml")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(1800) // 30 min
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"news_agency\"}")
                .build(),
                
            DataSource.builder()
                .name("KBS 뉴스")
                .url("https://news.kbs.co.kr/rss/rss.xml")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(1800)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"broadcast\"}")
                .build(),
                
            DataSource.builder()
                .name("MBC 뉴스")
                .url("https://imnews.imbc.com/rss/news.xml")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(1800)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"broadcast\"}")
                .build(),
                
            DataSource.builder()
                .name("SBS 뉴스")
                .url("https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=01")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(1800)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"broadcast\"}")
                .build(),
                
            // ========== Korean Major Newspapers ==========
            DataSource.builder()
                .name("조선일보")
                .url("https://www.chosun.com/arc/outboundfeeds/rss/?outputType=xml")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"newspaper\",\"stance\":\"conservative\"}")
                .build(),
                
            DataSource.builder()
                .name("중앙일보")
                .url("https://rss.joins.com/joins_news_list.xml")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"newspaper\",\"stance\":\"center-right\"}")
                .build(),
                
            DataSource.builder()
                .name("동아일보")
                .url("https://rss.donga.com/total.xml")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"newspaper\",\"stance\":\"conservative\"}")
                .build(),
                
            DataSource.builder()
                .name("한겨레")
                .url("https://www.hani.co.kr/rss/")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"newspaper\",\"stance\":\"progressive\"}")
                .build(),
                
            DataSource.builder()
                .name("경향신문")
                .url("https://www.khan.co.kr/rss/rssdata/total_news.xml")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"newspaper\",\"stance\":\"progressive\"}")
                .build(),
                
            // ========== Korean Business/Economy News ==========
            DataSource.builder()
                .name("매일경제")
                .url("https://www.mk.co.kr/rss/30000001/")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"business\"}")
                .build(),
                
            DataSource.builder()
                .name("한국경제")
                .url("https://www.hankyung.com/feed/all-news")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"business\"}")
                .build(),
                
            // ========== Korean IT/Tech News ==========
            DataSource.builder()
                .name("ZDNet Korea")
                .url("https://zdnet.co.kr/rss/")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"tech\"}")
                .build(),
                
            DataSource.builder()
                .name("전자신문 (ETNews)")
                .url("https://www.etnews.com/rss")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(3600)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"tech\"}")
                .build(),
                
            DataSource.builder()
                .name("블로터 (Bloter)")
                .url("https://www.bloter.net/feed")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(7200)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"tech_startup\"}")
                .build(),
                
            // ========== International News (Korean Edition) ==========
            DataSource.builder()
                .name("BBC 코리아")
                .url("https://feeds.bbci.co.uk/korean/rss.xml")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(1800)
                .metadataJson("{\"region\":\"international\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"international\"}")
                .build(),
                
            DataSource.builder()
                .name("뉴시스 (Newsis)")
                .url("https://newsis.com/RSS/")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(1800)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"news_agency\"}")
                .build(),
                
            DataSource.builder()
                .name("뉴스1")
                .url("https://www.news1.kr/rss/")
                .sourceType(SourceType.RSS)
                .isActive(true)
                .collectionFrequency(1800)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"news_agency\"}")
                .build(),
                
            // ========== BROWSER_AGENT Sources (AI-based crawling) ==========
            // 네이버 뉴스 (Browser Agent)
            DataSource.builder()
                .name("네이버 뉴스 (Browser Agent)")
                .url("https://news.naver.com/")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(true)
                .collectionFrequency(1800) // 30 min
                .browserAgentConfig(BrowserAgentConfig.forNewsExploration())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"portal\",\"crawler\":\"browser_agent\"}")
                .build(),
                
            // 다음 뉴스 (Browser Agent)
            DataSource.builder()
                .name("다음 뉴스 (Browser Agent)")
                .url("https://news.daum.net/")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(true)
                .collectionFrequency(1800)
                .browserAgentConfig(BrowserAgentConfig.forNewsExploration())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"portal\",\"crawler\":\"browser_agent\"}")
                .build(),
                
            // 구글 뉴스 한국 (Browser Agent)
            DataSource.builder()
                .name("구글 뉴스 한국 (Browser Agent)")
                .url("https://news.google.com/home?hl=ko&gl=KR&ceid=KR:ko")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(true)
                .collectionFrequency(3600)
                .browserAgentConfig(BrowserAgentConfig.forNewsExploration())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"aggregator\",\"crawler\":\"browser_agent\"}")
                .build(),
                
            // 네이버 실시간 검색어 트렌드 (Browser Agent - Breaking News)
            DataSource.builder()
                .name("네이버 트렌드 (Browser Agent)")
                .url("https://datalab.naver.com/keyword/realtimeList.naver")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(true)
                .collectionFrequency(900) // 15 min - 트렌드는 자주 확인
                .browserAgentConfig(BrowserAgentConfig.forBreakingNews())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"trending\",\"crawler\":\"browser_agent\"}")
                .build(),
                
            // 조선일보 (Browser Agent - Archive mode for non-RSS content)
            DataSource.builder()
                .name("조선일보 (Browser Agent)")
                .url("https://www.chosun.com/")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(false) // RSS가 있으므로 기본 비활성
                .collectionFrequency(7200)
                .browserAgentConfig(BrowserAgentConfig.forNewsArchive())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"newspaper\",\"crawler\":\"browser_agent\",\"stance\":\"conservative\"}")
                .build(),
                
            // 한겨레 (Browser Agent)
            DataSource.builder()
                .name("한겨레 (Browser Agent)")
                .url("https://www.hani.co.kr/")
                .sourceType(SourceType.BROWSER_AGENT)
                .isActive(false) // RSS가 있으므로 기본 비활성
                .collectionFrequency(7200)
                .browserAgentConfig(BrowserAgentConfig.forNewsArchive())
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"newspaper\",\"crawler\":\"browser_agent\",\"stance\":\"progressive\"}")
                .build(),
                
            // ========== WEB_SEARCH Sources (Portal Search Integration) ==========
            DataSource.builder()
                .name("네이버 뉴스 검색")
                .url("https://search.naver.com")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://search.naver.com/search.naver?where=news&query={query}")
                .searchPriority(1)
                .isActive(true)
                .collectionFrequency(0) // 검색은 주기적 수집 없음
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"portal_search\"}")
                .build(),
                
            DataSource.builder()
                .name("다음 뉴스 검색")
                .url("https://search.daum.net")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://search.daum.net/search?w=news&q={query}")
                .searchPriority(2)
                .isActive(true)
                .collectionFrequency(0)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"portal_search\"}")
                .build(),
                
            DataSource.builder()
                .name("구글 뉴스 검색 (한국)")
                .url("https://news.google.com")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://news.google.com/search?q={query}&hl=ko&gl=KR")
                .searchPriority(3)
                .isActive(true)
                .collectionFrequency(0)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"high\",\"category\":\"aggregator_search\"}")
                .build(),
                
            DataSource.builder()
                .name("빙 뉴스 검색 (한국)")
                .url("https://www.bing.com")
                .sourceType(SourceType.WEB_SEARCH)
                .searchUrlTemplate("https://www.bing.com/news/search?q={query}&cc=kr")
                .searchPriority(4)
                .isActive(true)
                .collectionFrequency(0)
                .metadataJson("{\"region\":\"korea\",\"language\":\"ko\",\"reliability\":\"medium\",\"category\":\"aggregator_search\"}")
                .build()
        );
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/DataSourcesConfig.java

```java
package com.newsinsight.collector.config;

import lombok.Data;
import org.springframework.boot.context.properties.ConfigurationProperties;
import org.springframework.context.annotation.Configuration;

import java.util.ArrayList;
import java.util.List;
import java.util.Map;

/**
 * Configuration for externalized data sources.
 * 
 * Data sources can be configured via application.yml or environment variables.
 * This replaces hardcoded sources in DataSourceSeeder with configurable ones.
 */
@Configuration
@ConfigurationProperties(prefix = "collector.data-sources")
@Data
public class DataSourcesConfig {

    /**
     * Enable/disable automatic seeding of data sources
     */
    private boolean seedEnabled = true;

    /**
     * List of predefined data source configurations
     */
    private List<DataSourceEntry> sources = new ArrayList<>();

    @Data
    public static class DataSourceEntry {
        /**
         * Display name for the source
         */
        private String name;

        /**
         * URL for the data source (RSS feed, API endpoint, etc.)
         */
        private String url;

        /**
         * Type of source: RSS, API, WEB_SCRAPER, WEB_SEARCH, BROWSER_AGENT
         */
        private String sourceType = "RSS";

        /**
         * Whether this source is active and should be collected
         */
        private boolean active = true;

        /**
         * Collection frequency in seconds
         */
        private int collectionFrequency = 3600;

        /**
         * Search URL template for WEB_SEARCH sources.
         * Use {query} as placeholder for the encoded search query.
         * Example: "https://search.naver.com/search.naver?where=news&query={query}"
         */
        private String searchUrlTemplate;

        /**
         * Priority for web search sources (lower = higher priority).
         */
        private Integer searchPriority = 100;

        /**
         * Additional metadata as key-value pairs
         */
        private Map<String, String> metadata;

        /**
         * Region/country code (e.g., "korea", "international")
         */
        private String region;

        /**
         * Language code (e.g., "ko", "en")
         */
        private String language = "ko";

        /**
         * Reliability level: "high", "medium", "low"
         */
        private String reliability = "medium";

        /**
         * Category: "news_agency", "broadcast", "newspaper", "business", "tech", etc.
         */
        private String category;

        /**
         * Political stance (optional): "conservative", "progressive", "center", "center-right", "center-left"
         */
        private String stance;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/KafkaConfig.java

```java
package com.newsinsight.collector.config;

import com.newsinsight.collector.dto.AiRequestMessage;
import com.newsinsight.collector.dto.AiResponseMessage;
import com.newsinsight.collector.dto.AiTaskRequestMessage;
import com.newsinsight.collector.dto.BrowserTaskMessage;
import com.newsinsight.collector.dto.CrawlCommandMessage;
import com.newsinsight.collector.dto.CrawlResultMessage;
import com.newsinsight.collector.dto.SearchHistoryMessage;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.kafka.common.serialization.StringSerializer;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;
import org.springframework.kafka.core.*;
import org.springframework.kafka.listener.CommonErrorHandler;
import org.springframework.kafka.listener.ContainerProperties;
import org.springframework.kafka.listener.DeadLetterPublishingRecoverer;
import org.springframework.kafka.listener.DefaultErrorHandler;
import org.springframework.kafka.support.ExponentialBackOffWithMaxRetries;
import org.springframework.kafka.support.serializer.JsonDeserializer;
import org.springframework.kafka.support.serializer.JsonSerializer;

import java.util.HashMap;
import java.util.Map;

/**
 * Kafka Configuration with Production-grade reliability features:
 * - Dead Letter Queue (DLQ) for failed messages
 * - Exponential backoff retry with max attempts
 * - Producer reliability settings (acks=all, retries, idempotence)
 * - Manual acknowledgment mode for consumer reliability
 * - Centralized configuration to reduce duplication
 */
@Configuration
@Slf4j
public class KafkaConfig {

    // ========== Configuration Properties ==========
    
    @Value("${spring.kafka.bootstrap-servers:localhost:9092}")
    private String bootstrapServers;

    @Value("${spring.application.name:collector-service}")
    private String applicationName;

    // Producer reliability settings
    @Value("${spring.kafka.producer.acks:all}")
    private String producerAcks;

    @Value("${spring.kafka.producer.retries:3}")
    private int producerRetries;

    @Value("${spring.kafka.producer.retry-backoff-ms:1000}")
    private int producerRetryBackoffMs;

    @Value("${spring.kafka.producer.delivery-timeout-ms:120000}")
    private int producerDeliveryTimeoutMs;

    @Value("${spring.kafka.producer.enable-idempotence:true}")
    private boolean producerIdempotence;

    // Consumer reliability settings
    @Value("${spring.kafka.consumer.max-retry-attempts:3}")
    private int consumerMaxRetryAttempts;

    @Value("${spring.kafka.consumer.retry-backoff-ms:1000}")
    private long consumerRetryBackoffMs;

    @Value("${spring.kafka.consumer.retry-max-backoff-ms:30000}")
    private long consumerRetryMaxBackoffMs;

    @Value("${spring.kafka.consumer.concurrency:1}")
    private int consumerConcurrency;

    // DLQ suffix
    private static final String DLQ_SUFFIX = ".dlq";

    // ========== Common Producer Configuration ==========

    private Map<String, Object> buildProducerProps() {
        Map<String, Object> props = new HashMap<>();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, JsonSerializer.class);
        
        // Reliability settings
        props.put(ProducerConfig.ACKS_CONFIG, producerAcks);
        props.put(ProducerConfig.RETRIES_CONFIG, producerRetries);
        props.put(ProducerConfig.RETRY_BACKOFF_MS_CONFIG, producerRetryBackoffMs);
        props.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, producerDeliveryTimeoutMs);
        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, producerIdempotence);
        
        // Batching for throughput (can be tuned)
        props.put(ProducerConfig.LINGER_MS_CONFIG, 5);
        props.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384);
        
        return props;
    }

    // ========== Common Consumer Configuration ==========

    private Map<String, Object> buildConsumerProps(String groupIdSuffix) {
        Map<String, Object> props = new HashMap<>();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, applicationName + "-" + groupIdSuffix);
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, JsonDeserializer.class);
        props.put(JsonDeserializer.TRUSTED_PACKAGES, "com.newsinsight.collector.dto");
        
        // Reliability settings
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false); // Manual ack
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 100);
        props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, 300000); // 5 minutes
        
        return props;
    }

    // ========== DLQ Producer (Generic) ==========

    @Bean
    public ProducerFactory<String, Object> dlqProducerFactory() {
        return new DefaultKafkaProducerFactory<>(buildProducerProps());
    }

    @Bean
    public KafkaTemplate<String, Object> dlqKafkaTemplate() {
        return new KafkaTemplate<>(dlqProducerFactory());
    }

    /**
     * Dead Letter Publishing Recoverer - sends failed messages to DLQ topic.
     * Topic naming convention: original-topic.dlq
     */
    @Bean
    public DeadLetterPublishingRecoverer deadLetterPublishingRecoverer() {
        return new DeadLetterPublishingRecoverer(dlqKafkaTemplate(),
                (ConsumerRecord<?, ?> record, Exception ex) -> {
                    String dlqTopic = record.topic() + DLQ_SUFFIX;
                    log.error("Sending to DLQ: topic={}, key={}, offset={}, error={}",
                            dlqTopic, record.key(), record.offset(), ex.getMessage());
                    return new TopicPartition(dlqTopic, record.partition());
                });
    }

    /**
     * Common Error Handler with exponential backoff and DLQ.
     */
    @Bean
    public CommonErrorHandler kafkaErrorHandler(DeadLetterPublishingRecoverer recoverer) {
        ExponentialBackOffWithMaxRetries backOff = new ExponentialBackOffWithMaxRetries(consumerMaxRetryAttempts);
        backOff.setInitialInterval(consumerRetryBackoffMs);
        backOff.setMaxInterval(consumerRetryMaxBackoffMs);
        backOff.setMultiplier(2.0);

        DefaultErrorHandler errorHandler = new DefaultErrorHandler(recoverer, backOff);
        
        // Log retries
        errorHandler.setRetryListeners((record, ex, attempt) -> {
            log.warn("Retry attempt {} for record: topic={}, key={}, offset={}, error={}",
                    attempt, record.topic(), record.key(), record.offset(), ex.getMessage());
        });
        
        return errorHandler;
    }

    // ========== AI Request Producer ==========

    @Bean
    public ProducerFactory<String, AiRequestMessage> aiRequestProducerFactory() {
        return new DefaultKafkaProducerFactory<>(buildProducerProps());
    }

    @Bean
    public KafkaTemplate<String, AiRequestMessage> aiRequestKafkaTemplate() {
        KafkaTemplate<String, AiRequestMessage> template = new KafkaTemplate<>(aiRequestProducerFactory());
        template.setObservationEnabled(true); // Enable metrics
        return template;
    }

    // ========== Crawl Command Producer ==========

    @Bean
    public ProducerFactory<String, CrawlCommandMessage> crawlCommandProducerFactory() {
        return new DefaultKafkaProducerFactory<>(buildProducerProps());
    }

    @Bean
    public KafkaTemplate<String, CrawlCommandMessage> crawlCommandKafkaTemplate() {
        KafkaTemplate<String, CrawlCommandMessage> template = new KafkaTemplate<>(crawlCommandProducerFactory());
        template.setObservationEnabled(true);
        return template;
    }

    // ========== Crawl Result Producer ==========

    @Bean
    public ProducerFactory<String, CrawlResultMessage> crawlResultProducerFactory() {
        return new DefaultKafkaProducerFactory<>(buildProducerProps());
    }

    @Bean
    public KafkaTemplate<String, CrawlResultMessage> crawlResultKafkaTemplate() {
        KafkaTemplate<String, CrawlResultMessage> template = new KafkaTemplate<>(crawlResultProducerFactory());
        template.setObservationEnabled(true);
        return template;
    }

    // ========== AI Task Request Producer (for Orchestration) ==========

    @Bean
    public ProducerFactory<String, AiTaskRequestMessage> aiTaskRequestProducerFactory() {
        return new DefaultKafkaProducerFactory<>(buildProducerProps());
    }

    @Bean
    public KafkaTemplate<String, AiTaskRequestMessage> aiTaskRequestKafkaTemplate() {
        KafkaTemplate<String, AiTaskRequestMessage> template = new KafkaTemplate<>(aiTaskRequestProducerFactory());
        template.setObservationEnabled(true);
        return template;
    }

    // ========== Browser Task Producer (for autonomous browser crawling) ==========

    @Bean
    public ProducerFactory<String, BrowserTaskMessage> browserTaskProducerFactory() {
        return new DefaultKafkaProducerFactory<>(buildProducerProps());
    }

    @Bean
    public KafkaTemplate<String, BrowserTaskMessage> browserTaskKafkaTemplate() {
        KafkaTemplate<String, BrowserTaskMessage> template = new KafkaTemplate<>(browserTaskProducerFactory());
        template.setObservationEnabled(true);
        return template;
    }

    // ========== AI Response Consumer ==========

    @Bean
    public ConsumerFactory<String, AiResponseMessage> aiResponseConsumerFactory() {
        return new DefaultKafkaConsumerFactory<>(
                buildConsumerProps("ai"),
                new StringDeserializer(),
                new JsonDeserializer<>(AiResponseMessage.class)
        );
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, AiResponseMessage> aiResponseKafkaListenerContainerFactory(
            CommonErrorHandler kafkaErrorHandler) {
        ConcurrentKafkaListenerContainerFactory<String, AiResponseMessage> factory =
                new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(aiResponseConsumerFactory());
        factory.setConcurrency(consumerConcurrency);
        factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.RECORD);
        factory.setCommonErrorHandler(kafkaErrorHandler);
        return factory;
    }

    // ========== Crawl Command Consumer ==========

    @Bean
    public ConsumerFactory<String, CrawlCommandMessage> crawlCommandConsumerFactory() {
        return new DefaultKafkaConsumerFactory<>(
                buildConsumerProps("crawl"),
                new StringDeserializer(),
                new JsonDeserializer<>(CrawlCommandMessage.class)
        );
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, CrawlCommandMessage> crawlCommandKafkaListenerContainerFactory(
            CommonErrorHandler kafkaErrorHandler) {
        ConcurrentKafkaListenerContainerFactory<String, CrawlCommandMessage> factory =
                new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(crawlCommandConsumerFactory());
        factory.setConcurrency(consumerConcurrency);
        factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.RECORD);
        factory.setCommonErrorHandler(kafkaErrorHandler);
        return factory;
    }

    // ========== Crawl Result Consumer ==========

    @Bean
    public ConsumerFactory<String, CrawlResultMessage> crawlResultConsumerFactory() {
        return new DefaultKafkaConsumerFactory<>(
                buildConsumerProps("crawl-result"),
                new StringDeserializer(),
                new JsonDeserializer<>(CrawlResultMessage.class)
        );
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, CrawlResultMessage> crawlResultKafkaListenerContainerFactory(
            CommonErrorHandler kafkaErrorHandler) {
        ConcurrentKafkaListenerContainerFactory<String, CrawlResultMessage> factory =
                new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(crawlResultConsumerFactory());
        factory.setConcurrency(consumerConcurrency);
        factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.RECORD);
        factory.setCommonErrorHandler(kafkaErrorHandler);
        return factory;
    }

    // ========== Search History Producer (for async persistence) ==========

    @Bean
    public ProducerFactory<String, SearchHistoryMessage> searchHistoryProducerFactory() {
        return new DefaultKafkaProducerFactory<>(buildProducerProps());
    }

    @Bean
    public KafkaTemplate<String, SearchHistoryMessage> searchHistoryKafkaTemplate() {
        KafkaTemplate<String, SearchHistoryMessage> template = new KafkaTemplate<>(searchHistoryProducerFactory());
        template.setObservationEnabled(true);
        return template;
    }

    // ========== Search History Consumer ==========

    @Bean
    public ConsumerFactory<String, SearchHistoryMessage> searchHistoryConsumerFactory() {
        return new DefaultKafkaConsumerFactory<>(
                buildConsumerProps("search-history"),
                new StringDeserializer(),
                new JsonDeserializer<>(SearchHistoryMessage.class)
        );
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, SearchHistoryMessage> searchHistoryKafkaListenerContainerFactory(
            CommonErrorHandler kafkaErrorHandler) {
        ConcurrentKafkaListenerContainerFactory<String, SearchHistoryMessage> factory =
                new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(searchHistoryConsumerFactory());
        factory.setConcurrency(consumerConcurrency);
        factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.RECORD);
        factory.setCommonErrorHandler(kafkaErrorHandler);
        return factory;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/MlAddonSeeder.java

```java
package com.newsinsight.collector.config;

import com.newsinsight.collector.entity.addon.*;
import com.newsinsight.collector.repository.MlAddonRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.boot.ApplicationArguments;
import org.springframework.boot.ApplicationRunner;
import org.springframework.context.annotation.Profile;
import org.springframework.stereotype.Component;
import org.springframework.transaction.annotation.Transactional;

import java.util.List;
import java.util.Map;

/**
 * Seeds the database with default ML Add-ons on application startup.
 * Registers sentiment, factcheck, and bias analysis add-ons if not already present.
 * 
 * Profiles:
 * - default: Runs automatically
 * - no-seed: Skip seeding (for production or when using external config)
 */
@Component
@Profile("!no-seed")
@RequiredArgsConstructor
@Slf4j
public class MlAddonSeeder implements ApplicationRunner {

    private final MlAddonRepository mlAddonRepository;

    @Value("${ml.addon.sentiment.host:sentiment-addon}")
    private String sentimentHost;

    @Value("${ml.addon.sentiment.port:8100}")
    private int sentimentPort;

    @Value("${ml.addon.factcheck.host:factcheck-addon}")
    private String factcheckHost;

    @Value("${ml.addon.factcheck.port:8101}")
    private int factcheckPort;

    @Value("${ml.addon.bias.host:bias-addon}")
    private String biasHost;

    @Value("${ml.addon.bias.port:8102}")
    private int biasPort;

    @Value("${ml.addon.seed.enabled:true}")
    private boolean seedEnabled;

    @Override
    @Transactional
    public void run(ApplicationArguments args) {
        if (!seedEnabled) {
            log.info("ML Add-on seeding is disabled via configuration.");
            return;
        }

        log.info("Seeding ML Add-ons...");

        List<MlAddon> defaultAddons = createDefaultAddons();

        int created = 0;
        int skipped = 0;

        for (MlAddon addon : defaultAddons) {
            if (mlAddonRepository.existsByAddonKey(addon.getAddonKey())) {
                log.debug("ML Add-on '{}' already exists, skipping.", addon.getAddonKey());
                skipped++;
            } else {
                mlAddonRepository.save(addon);
                log.info("Created ML Add-on: {} ({})", addon.getName(), addon.getAddonKey());
                created++;
            }
        }

        log.info("Successfully seeded ML Add-ons. created={}, skipped={}, total={}", 
                created, skipped, defaultAddons.size());
    }

    private List<MlAddon> createDefaultAddons() {
        return List.of(
            // ========== Sentiment Analysis Add-on ==========
            MlAddon.builder()
                .addonKey("sentiment-v1")
                .name("한국어 감정 분석")
                .description("한국어 뉴스 기사의 감정(긍정/부정/중립)을 분석합니다. " +
                        "KoBERT/KoELECTRA 기반 모델을 사용하여 정확한 감정 분석을 제공합니다.")
                .category(AddonCategory.SENTIMENT)
                .invokeType(AddonInvokeType.HTTP_SYNC)
                .endpointUrl(String.format("http://%s:%d/analyze", sentimentHost, sentimentPort))
                .healthCheckUrl(String.format("http://%s:%d/health", sentimentHost, sentimentPort))
                .authType(AddonAuthType.NONE)
                .inputSchemaVersion("1.0")
                .outputSchemaVersion("1.0")
                .timeoutMs(30000)
                .maxQps(20)
                .maxRetries(3)
                .enabled(true)
                .priority(10)
                .healthStatus(AddonHealthStatus.UNKNOWN)
                .owner("system")
                .config(Map.of(
                    "model", "koelectra-sentiment",
                    "language", "ko",
                    "min_confidence", 0.5,
                    "include_emotions", true
                ))
                .build(),

            // ========== Fact Check Add-on ==========
            MlAddon.builder()
                .addonKey("factcheck-v1")
                .name("팩트체크 분석")
                .description("뉴스 기사의 주장을 추출하고 신뢰도를 검증합니다. " +
                        "KoELECTRA, Sentence Transformers, KLUE BERT를 활용한 다중 모델 앙상블 분석.")
                .category(AddonCategory.FACTCHECK)
                .invokeType(AddonInvokeType.HTTP_SYNC)
                .endpointUrl(String.format("http://%s:%d/analyze", factcheckHost, factcheckPort))
                .healthCheckUrl(String.format("http://%s:%d/health", factcheckHost, factcheckPort))
                .authType(AddonAuthType.NONE)
                .inputSchemaVersion("1.0")
                .outputSchemaVersion("1.0")
                .timeoutMs(60000) // Factcheck may take longer due to cross-reference
                .maxQps(10)
                .maxRetries(2)
                .enabled(true)
                .priority(20)
                .healthStatus(AddonHealthStatus.UNKNOWN)
                .owner("system")
                .config(Map.of(
                    "models", List.of("koelectra", "sentence-transformers", "klue-bert"),
                    "language", "ko",
                    "extract_claims", true,
                    "cross_reference", true,
                    "min_claim_confidence", 0.6
                ))
                .build(),

            // ========== Bias Analysis Add-on ==========
            MlAddon.builder()
                .addonKey("bias-v1")
                .name("편향도 분석")
                .description("뉴스 기사의 정치적/이념적 편향도를 분석합니다. " +
                        "출처 신뢰도, 언어 패턴, 프레이밍 분석을 통한 종합 편향 점수 제공.")
                .category(AddonCategory.BIAS)
                .invokeType(AddonInvokeType.HTTP_SYNC)
                .endpointUrl(String.format("http://%s:%d/analyze", biasHost, biasPort))
                .healthCheckUrl(String.format("http://%s:%d/health", biasHost, biasPort))
                .authType(AddonAuthType.NONE)
                .inputSchemaVersion("1.0")
                .outputSchemaVersion("1.0")
                .timeoutMs(30000)
                .maxQps(15)
                .maxRetries(3)
                .enabled(true)
                .priority(30)
                .healthStatus(AddonHealthStatus.UNKNOWN)
                .owner("system")
                .config(Map.of(
                    "model", "bias-detector-ko",
                    "language", "ko",
                    "analyze_source", true,
                    "analyze_language", true,
                    "analyze_framing", true,
                    "political_spectrum", true
                ))
                .build(),

            // ========== Source Quality Add-on ==========
            MlAddon.builder()
                .addonKey("source-quality-v1")
                .name("출처 신뢰도 분석")
                .description("뉴스 출처의 신뢰도와 품질을 평가합니다. " +
                        "미디어 출처 데이터베이스와 역사적 정확도 데이터를 기반으로 분석.")
                .category(AddonCategory.SOURCE_QUALITY)
                .invokeType(AddonInvokeType.HTTP_SYNC)
                .endpointUrl(String.format("http://%s:%d/analyze/source", biasHost, biasPort))
                .healthCheckUrl(String.format("http://%s:%d/health", biasHost, biasPort))
                .authType(AddonAuthType.NONE)
                .inputSchemaVersion("1.0")
                .outputSchemaVersion("1.0")
                .timeoutMs(15000)
                .maxQps(30)
                .maxRetries(2)
                .enabled(true)
                .priority(5) // Run early as other addons may depend on source info
                .healthStatus(AddonHealthStatus.UNKNOWN)
                .owner("system")
                .config(Map.of(
                    "include_history", true,
                    "check_domain_reputation", true,
                    "check_author", false
                ))
                .build(),

            // ========== Topic Classification Add-on ==========
            MlAddon.builder()
                .addonKey("topic-classifier-v1")
                .name("주제 분류")
                .description("뉴스 기사를 정치, 경제, 사회, 문화, IT 등의 카테고리로 분류합니다.")
                .category(AddonCategory.TOPIC_CLASSIFICATION)
                .invokeType(AddonInvokeType.HTTP_SYNC)
                .endpointUrl(String.format("http://%s:%d/analyze/topic", sentimentHost, sentimentPort))
                .healthCheckUrl(String.format("http://%s:%d/health", sentimentHost, sentimentPort))
                .authType(AddonAuthType.NONE)
                .inputSchemaVersion("1.0")
                .outputSchemaVersion("1.0")
                .timeoutMs(20000)
                .maxQps(25)
                .maxRetries(3)
                .enabled(false) // Disabled by default, enable when model is ready
                .priority(15)
                .healthStatus(AddonHealthStatus.UNKNOWN)
                .owner("system")
                .config(Map.of(
                    "model", "klue-ynat",
                    "language", "ko",
                    "categories", List.of("정치", "경제", "사회", "문화", "세계", "IT/과학", "스포츠", "연예"),
                    "multi_label", true
                ))
                .build(),

            // ========== Entity Extraction (NER) Add-on ==========
            MlAddon.builder()
                .addonKey("ner-v1")
                .name("개체명 인식 (NER)")
                .description("뉴스 기사에서 인물, 조직, 장소, 날짜 등의 개체명을 추출합니다.")
                .category(AddonCategory.NER)
                .invokeType(AddonInvokeType.HTTP_SYNC)
                .endpointUrl(String.format("http://%s:%d/analyze/ner", factcheckHost, factcheckPort))
                .healthCheckUrl(String.format("http://%s:%d/health", factcheckHost, factcheckPort))
                .authType(AddonAuthType.NONE)
                .inputSchemaVersion("1.0")
                .outputSchemaVersion("1.0")
                .timeoutMs(25000)
                .maxQps(20)
                .maxRetries(3)
                .enabled(false) // Disabled by default
                .priority(8)
                .healthStatus(AddonHealthStatus.UNKNOWN)
                .owner("system")
                .config(Map.of(
                    "model", "klue-ner",
                    "language", "ko",
                    "entity_types", List.of("PERSON", "ORGANIZATION", "LOCATION", "DATE", "QUANTITY"),
                    "link_entities", true
                ))
                .build()
        );
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/RedisCacheConfig.java

```java
package com.newsinsight.collector.config;

import com.fasterxml.jackson.annotation.JsonTypeInfo;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.jsontype.impl.LaissezFaireSubTypeValidator;
import com.fasterxml.jackson.datatype.jsr310.JavaTimeModule;
import io.micrometer.core.instrument.MeterRegistry;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.cache.Cache;
import org.springframework.cache.CacheManager;
import org.springframework.cache.annotation.CachingConfigurer;
import org.springframework.cache.annotation.EnableCaching;
import org.springframework.cache.caffeine.CaffeineCacheManager;
import org.springframework.cache.interceptor.CacheErrorHandler;
import org.springframework.cache.support.CompositeCacheManager;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.Primary;
import org.springframework.data.redis.cache.RedisCacheConfiguration;
import org.springframework.data.redis.cache.RedisCacheManager;
import org.springframework.data.redis.connection.RedisConnectionFactory;
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.data.redis.serializer.GenericJackson2JsonRedisSerializer;
import org.springframework.data.redis.serializer.RedisSerializationContext;
import org.springframework.data.redis.serializer.StringRedisSerializer;

import java.time.Duration;
import java.util.Arrays;
import java.util.HashMap;
import java.util.Map;

/**
 * Redis 캐싱 설정
 * 
 * 팩트체크 챗봇 세션을 Redis에 캐싱하여 빠른 조회를 지원합니다.
 * 
 * 개선사항:
 * - 캐시 키 prefix 추가 (충돌 방지)
 * - 로컬 캐시 폴백 (Caffeine)
 * - 캐시 에러 핸들러
 * - 캐시 통계 메트릭
 * - 다양한 캐시 프로파일
 */
@Configuration
@EnableCaching
@Slf4j
public class RedisCacheConfig implements CachingConfigurer {

    @Value("${spring.application.name:newsinsight}")
    private String applicationName;

    @Value("${spring.data.redis.enabled:true}")
    private boolean redisEnabled;

    // 캐시 TTL 설정
    @Value("${cache.chat-sessions.ttl-hours:2}")
    private int chatSessionsTtlHours;

    @Value("${cache.chat-messages.ttl-minutes:30}")
    private int chatMessagesTtlMinutes;

    @Value("${cache.default.ttl-hours:24}")
    private int defaultTtlHours;

    // 로컬 캐시 설정
    @Value("${cache.local.max-size:1000}")
    private int localCacheMaxSize;

    @Value("${cache.local.ttl-minutes:10}")
    private int localCacheTtlMinutes;

    private final MeterRegistry meterRegistry;

    public RedisCacheConfig(MeterRegistry meterRegistry) {
        this.meterRegistry = meterRegistry;
    }

    /**
     * ObjectMapper 설정 (타입 정보 포함)
     */
    private ObjectMapper createCacheObjectMapper() {
        ObjectMapper mapper = new ObjectMapper();
        mapper.registerModule(new JavaTimeModule());
        mapper.activateDefaultTyping(
                LaissezFaireSubTypeValidator.instance,
                ObjectMapper.DefaultTyping.NON_FINAL,
                JsonTypeInfo.As.PROPERTY
        );
        return mapper;
    }

    /**
     * Redis 캐시 매니저 설정
     */
    @Bean
    public RedisCacheManager redisCacheManager(RedisConnectionFactory connectionFactory) {
        // 캐시 키 prefix 설정
        String keyPrefix = applicationName + ":cache:";

        // 기본 캐시 설정
        RedisCacheConfiguration defaultConfig = RedisCacheConfiguration.defaultCacheConfig()
                .entryTtl(Duration.ofHours(defaultTtlHours))
                .prefixCacheNameWith(keyPrefix)
                .serializeKeysWith(
                        RedisSerializationContext.SerializationPair.fromSerializer(
                                new StringRedisSerializer()
                        )
                )
                .serializeValuesWith(
                        RedisSerializationContext.SerializationPair.fromSerializer(
                                new GenericJackson2JsonRedisSerializer(createCacheObjectMapper())
                        )
                )
                // null 값 캐싱 비활성화
                .disableCachingNullValues();

        // 캐시별 설정
        Map<String, RedisCacheConfiguration> cacheConfigurations = new HashMap<>();
        
        // 채팅 세션 캐시: 2시간
        cacheConfigurations.put("chatSessions", 
                defaultConfig.entryTtl(Duration.ofHours(chatSessionsTtlHours)));
        
        // 채팅 메시지 캐시: 30분
        cacheConfigurations.put("chatMessages", 
                defaultConfig.entryTtl(Duration.ofMinutes(chatMessagesTtlMinutes)));
        
        // 사용자 세션 목록 캐시: 1시간
        cacheConfigurations.put("userSessions", 
                defaultConfig.entryTtl(Duration.ofHours(1)));
        
        // 팩트체크 결과 캐시: 6시간
        cacheConfigurations.put("factCheckResults", 
                defaultConfig.entryTtl(Duration.ofHours(6)));
        
        // 유사 질문 검색 캐시: 1시간
        cacheConfigurations.put("similarQuestions", 
                defaultConfig.entryTtl(Duration.ofHours(1)));
        
        // 검색 결과 캐시: 5분 (자주 업데이트되는 데이터)
        cacheConfigurations.put("searchResults", 
                defaultConfig.entryTtl(Duration.ofMinutes(5)));
        
        // DB 검색 결과 캐시: 10분
        cacheConfigurations.put("dbSearchResults", 
                defaultConfig.entryTtl(Duration.ofMinutes(10)));

        RedisCacheManager cacheManager = RedisCacheManager.builder(connectionFactory)
                .cacheDefaults(defaultConfig)
                .withInitialCacheConfigurations(cacheConfigurations)
                .enableStatistics() // 통계 활성화
                .build();

        log.info("Redis Cache Manager initialized with prefix: {}", keyPrefix);
        return cacheManager;
    }

    /**
     * 로컬 캐시 매니저 (Caffeine) - 폴백용
     */
    @Bean
    public CaffeineCacheManager caffeineCacheManager() {
        CaffeineCacheManager cacheManager = new CaffeineCacheManager();
        cacheManager.setCaffeine(
                com.github.benmanes.caffeine.cache.Caffeine.newBuilder()
                        .maximumSize(localCacheMaxSize)
                        .expireAfterWrite(Duration.ofMinutes(localCacheTtlMinutes))
                        .recordStats() // 통계 활성화
        );
        cacheManager.setCacheNames(Arrays.asList(
                "chatSessions", 
                "chatMessages", 
                "userSessions",
                "factCheckResults",
                "similarQuestions",
                "searchResults",
                "dbSearchResults"
        ));

        log.info("Caffeine Cache Manager initialized (fallback)");
        return cacheManager;
    }

    /**
     * 복합 캐시 매니저 (Redis 우선, Caffeine 폴백)
     */
    @Bean
    @Primary
    @Override
    public CacheManager cacheManager() {
        CompositeCacheManager compositeCacheManager = new CompositeCacheManager();
        
        // Redis가 활성화되어 있으면 Redis 우선 사용
        // 그렇지 않으면 Caffeine만 사용
        if (redisEnabled) {
            log.info("Using Redis as primary cache with Caffeine fallback");
        } else {
            log.info("Redis disabled, using Caffeine as primary cache");
        }
        
        compositeCacheManager.setFallbackToNoOpCache(false);
        return compositeCacheManager;
    }

    /**
     * Redis 캐시 매니저를 Primary로 직접 반환
     */
    @Bean("primaryCacheManager")
    public CacheManager primaryCacheManager(RedisConnectionFactory connectionFactory) {
        return redisCacheManager(connectionFactory);
    }

    /**
     * RedisTemplate 설정
     */
    @Bean
    public RedisTemplate<String, Object> redisTemplate(RedisConnectionFactory connectionFactory) {
        RedisTemplate<String, Object> template = new RedisTemplate<>();
        template.setConnectionFactory(connectionFactory);
        
        // 키는 String으로 직렬화
        template.setKeySerializer(new StringRedisSerializer());
        template.setHashKeySerializer(new StringRedisSerializer());
        
        // 값은 JSON으로 직렬화
        GenericJackson2JsonRedisSerializer jsonSerializer = 
                new GenericJackson2JsonRedisSerializer(createCacheObjectMapper());
        template.setValueSerializer(jsonSerializer);
        template.setHashValueSerializer(jsonSerializer);
        
        template.afterPropertiesSet();
        return template;
    }

    /**
     * 캐시 에러 핸들러 - Redis 장애 시 로깅만 하고 계속 진행
     */
    @Override
    public CacheErrorHandler errorHandler() {
        return new CacheErrorHandler() {
            @Override
            public void handleCacheGetError(RuntimeException exception, Cache cache, Object key) {
                log.warn("Cache GET error - cache: {}, key: {}, error: {}", 
                        cache.getName(), key, exception.getMessage());
                // 메트릭 기록
                meterRegistry.counter("cache.error", 
                        "cache", cache.getName(), 
                        "operation", "get").increment();
            }

            @Override
            public void handleCachePutError(RuntimeException exception, Cache cache, Object key, Object value) {
                log.warn("Cache PUT error - cache: {}, key: {}, error: {}", 
                        cache.getName(), key, exception.getMessage());
                meterRegistry.counter("cache.error", 
                        "cache", cache.getName(), 
                        "operation", "put").increment();
            }

            @Override
            public void handleCacheEvictError(RuntimeException exception, Cache cache, Object key) {
                log.warn("Cache EVICT error - cache: {}, key: {}, error: {}", 
                        cache.getName(), key, exception.getMessage());
                meterRegistry.counter("cache.error", 
                        "cache", cache.getName(), 
                        "operation", "evict").increment();
            }

            @Override
            public void handleCacheClearError(RuntimeException exception, Cache cache) {
                log.warn("Cache CLEAR error - cache: {}, error: {}", 
                        cache.getName(), exception.getMessage());
                meterRegistry.counter("cache.error", 
                        "cache", cache.getName(), 
                        "operation", "clear").increment();
            }
        };
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/SecurityConfig.java

```java
package com.newsinsight.collector.config;

import io.jsonwebtoken.Claims;
import io.jsonwebtoken.Jwts;
import io.jsonwebtoken.security.Keys;
import jakarta.servlet.FilterChain;
import jakarta.servlet.ServletException;
import jakarta.servlet.http.HttpServletRequest;
import jakarta.servlet.http.HttpServletResponse;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.core.annotation.Order;
import org.springframework.http.HttpMethod;
import org.springframework.security.authentication.UsernamePasswordAuthenticationToken;
import org.springframework.security.config.annotation.method.configuration.EnableMethodSecurity;
import org.springframework.security.config.annotation.web.builders.HttpSecurity;
import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;
import org.springframework.security.config.annotation.web.configurers.AbstractHttpConfigurer;
import org.springframework.security.config.http.SessionCreationPolicy;
import org.springframework.security.core.authority.SimpleGrantedAuthority;
import org.springframework.security.core.context.SecurityContextHolder;
import org.springframework.security.web.SecurityFilterChain;
import org.springframework.security.web.authentication.UsernamePasswordAuthenticationFilter;
import org.springframework.security.web.authentication.WebAuthenticationDetailsSource;
import org.springframework.stereotype.Component;
import org.springframework.web.cors.CorsConfiguration;
import org.springframework.web.cors.CorsConfigurationSource;
import org.springframework.web.cors.UrlBasedCorsConfigurationSource;
import org.springframework.web.filter.OncePerRequestFilter;

import javax.crypto.SecretKey;
import java.io.IOException;
import java.nio.charset.StandardCharsets;
import java.util.Arrays;
import java.util.Collections;
import java.util.List;

/**
 * Security Configuration for Data Collection Service
 * 
 * 엔드포인트별 인증/인가 정책:
 * - Public: 헬스체크, Swagger, actuator
 * - Authenticated: 일반 API (search, data, analysis, reports 등)
 * - Admin Only: workspace/admin/*, 관리 엔드포인트
 */
@Configuration
@EnableWebSecurity
@EnableMethodSecurity(prePostEnabled = true)
public class SecurityConfig {

    private static final Logger log = LoggerFactory.getLogger(SecurityConfig.class);

    @Value("${security.jwt.secret:${ADMIN_SECRET_KEY:your-secret-key-change-in-production}}")
    private String jwtSecret;

    @Value("${security.enabled:true}")
    private boolean securityEnabled;

    @Bean
    public SecurityFilterChain securityFilterChain(HttpSecurity http, JwtAuthenticationFilter jwtFilter) throws Exception {
        if (!securityEnabled) {
            log.warn("Security is DISABLED. All endpoints are publicly accessible. DO NOT USE IN PRODUCTION!");
            return http
                    .csrf(AbstractHttpConfigurer::disable)
                    .cors(cors -> cors.configurationSource(corsConfigurationSource()))
                    .authorizeHttpRequests(auth -> auth.anyRequest().permitAll())
                    .build();
        }

        return http
                .csrf(AbstractHttpConfigurer::disable)
                .cors(cors -> cors.configurationSource(corsConfigurationSource()))
                .sessionManagement(session -> session.sessionCreationPolicy(SessionCreationPolicy.STATELESS))
                .authorizeHttpRequests(auth -> auth
                        // ========================================
                        // Public Endpoints (인증 불필요)
                        // ========================================
                        .requestMatchers("/actuator/**").permitAll()
                        .requestMatchers("/swagger-ui/**", "/v3/api-docs/**", "/swagger-resources/**").permitAll()
                        .requestMatchers(HttpMethod.OPTIONS, "/**").permitAll()
                        
                        // Health Check Endpoints (각 서비스별)
                        .requestMatchers("/api/v1/search/health").permitAll()
                        .requestMatchers("/api/v1/jobs/health").permitAll()
                        .requestMatchers("/api/v1/search-history/health").permitAll()
                        .requestMatchers("/api/v1/search-templates/health").permitAll()
                        .requestMatchers("/api/v1/projects/health").permitAll()
                        .requestMatchers("/api/v1/ai/health").permitAll()
                        .requestMatchers("/api/v1/analysis/deep/health").permitAll()
                        .requestMatchers("/api/v1/analysis/live/health").permitAll()
                        .requestMatchers("/api/v1/analysis/extract-claims/health").permitAll()
                        .requestMatchers("/api/v1/factcheck-chat/health").permitAll()
                        .requestMatchers("/api/v1/factcheck-chat/health/**").permitAll()
                        .requestMatchers("/api/v1/workspace/files/health").permitAll()
                        .requestMatchers("/api/v1/ml/status").permitAll()
                        
                        // ========================================
                        // SSE Endpoints (EventSource는 Authorization 헤더 미지원)
                        // 실시간 스트림은 별도 인증 없이 허용
                        // ========================================
                        .requestMatchers("/api/v1/jobs/stream").permitAll()
                        .requestMatchers("/api/v1/jobs/*/stream").permitAll()
                        .requestMatchers("/api/v1/events/stream").permitAll()
                        .requestMatchers("/api/v1/events/stats/stream").permitAll()
                        .requestMatchers("/api/v1/search/stream").permitAll()
                        .requestMatchers("/api/v1/search/deep/stream").permitAll()
                        .requestMatchers("/api/v1/search/jobs/*/stream").permitAll()
                        .requestMatchers("/api/v1/search/analysis/stream").permitAll()
                        .requestMatchers("/api/v1/search/analysis/stream/status").permitAll()
                        .requestMatchers("/api/v1/analysis/deep/*/stream").permitAll()
                        .requestMatchers("/api/v1/analysis/live").permitAll()
                        .requestMatchers("/api/v1/search-history/stream").permitAll()
                        .requestMatchers("/api/v1/factcheck-chat/session/*/message").permitAll()
                        .requestMatchers("/api/v1/factcheck-chat/session").permitAll()
                        .requestMatchers("/api/v1/factcheck-chat/session/**").permitAll()
                        
                        // ========================================
                        // Public Report Export (익명 세션에서도 PDF 다운로드 허용)
                        // ========================================
                        .requestMatchers("/api/v1/reports/*/export").permitAll()
                        .requestMatchers("/api/v1/reports/unified-search/*/export").permitAll()
                        .requestMatchers("/api/v1/reports/deep-search/*/export").permitAll()
                        .requestMatchers("/api/v1/reports/ml-analysis/*/export").permitAll()
                        
                        // ========================================
                        // Admin Only Endpoints (ADMIN 권한 필요)
                        // ========================================
                        .requestMatchers("/api/v1/workspace/admin/**").hasRole("ADMIN")
                        .requestMatchers("/api/v1/workspace/files/admin/**").hasRole("ADMIN")
                        .requestMatchers("/api/v1/admin/llm-providers/**").hasRole("ADMIN")
                        .requestMatchers("/api/v1/llm-providers/**").hasAnyRole("ADMIN", "OPERATOR")
                        
                        // ========================================
                        // Authenticated Endpoints (로그인 필요)
                        // ========================================
                        // Search API
                        .requestMatchers("/api/v1/search/**").authenticated()
                        .requestMatchers("/api/v1/search-history/**").authenticated()
                        .requestMatchers("/api/v1/search-templates/**").authenticated()
                        
                        // Data & Collections
                        .requestMatchers("/api/v1/data/**").authenticated()
                        .requestMatchers("/api/v1/collections/**").authenticated()
                        .requestMatchers("/api/v1/sources/**").authenticated()
                        
                        // Analysis & AI
                        .requestMatchers("/api/v1/analysis/**").authenticated()
                        .requestMatchers("/api/v1/ai/**").authenticated()
                        .requestMatchers("/api/v1/ml/**").authenticated()
                        
                        // Reports
                        .requestMatchers("/api/v1/reports/**").authenticated()
                        
                        // Projects & Workspace (일반)
                        .requestMatchers("/api/v1/projects/**").authenticated()
                        .requestMatchers("/api/v1/workspace/**").authenticated()
                        
                        // Articles
                        .requestMatchers("/api/v1/articles/**").authenticated()
                        
                        // Jobs & AutoCrawl
                        .requestMatchers("/api/v1/jobs/**").authenticated()
                        .requestMatchers("/api/v1/autocrawl/**").authenticated()
                        
                        // Events (SSE)
                        .requestMatchers("/api/v1/events/**").authenticated()
                        
                        // Factcheck Chat
                        .requestMatchers("/api/v1/factcheck-chat/**").authenticated()
                        
                        // Config (read-only for authenticated users)
                        .requestMatchers(HttpMethod.GET, "/api/v1/config/**").authenticated()
                        .requestMatchers("/api/v1/config/**").hasAnyRole("ADMIN", "OPERATOR")
                        
                        // Default: require authentication
                        .anyRequest().authenticated()
                )
                .addFilterBefore(jwtFilter, UsernamePasswordAuthenticationFilter.class)
                .build();
    }

    @Bean
    public CorsConfigurationSource corsConfigurationSource() {
        CorsConfiguration configuration = new CorsConfiguration();
        configuration.setAllowedOriginPatterns(List.of("*"));
        configuration.setAllowedMethods(Arrays.asList("GET", "POST", "PUT", "DELETE", "PATCH", "OPTIONS", "HEAD"));
        configuration.setAllowedHeaders(List.of("*"));
        configuration.setAllowCredentials(true);
        configuration.setMaxAge(3600L);

        UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource();
        source.registerCorsConfiguration("/**", configuration);
        return source;
    }

    /**
     * JWT Authentication Filter
     * Authorization 헤더에서 Bearer 토큰을 추출하고 검증합니다.
     */
    @Component
    public static class JwtAuthenticationFilter extends OncePerRequestFilter {

        private static final Logger log = LoggerFactory.getLogger(JwtAuthenticationFilter.class);

        @Value("${security.jwt.secret:${ADMIN_SECRET_KEY:your-secret-key-change-in-production}}")
        private String jwtSecret;

        @Value("${security.enabled:true}")
        private boolean securityEnabled;

        @Override
        protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain)
                throws ServletException, IOException {

            if (!securityEnabled) {
                filterChain.doFilter(request, response);
                return;
            }

            String authHeader = request.getHeader("Authorization");

            if (authHeader == null || !authHeader.startsWith("Bearer ")) {
                filterChain.doFilter(request, response);
                return;
            }

            try {
                String token = authHeader.substring(7);
                SecretKey key = Keys.hmacShaKeyFor(jwtSecret.getBytes(StandardCharsets.UTF_8));

                Claims claims = Jwts.parser()
                        .verifyWith(key)
                        .build()
                        .parseSignedClaims(token)
                        .getPayload();

                String userId = claims.getSubject();
                String username = claims.get("username", String.class);
                String role = claims.get("role", String.class);

                if (userId != null && SecurityContextHolder.getContext().getAuthentication() == null) {
                    List<SimpleGrantedAuthority> authorities;
                    
                    if (role != null) {
                        // role이 "admin", "operator", "user" 형태로 올 수 있음
                        String normalizedRole = role.toUpperCase();
                        authorities = List.of(new SimpleGrantedAuthority("ROLE_" + normalizedRole));
                    } else {
                        authorities = List.of(new SimpleGrantedAuthority("ROLE_USER"));
                    }

                    UsernamePasswordAuthenticationToken authentication = new UsernamePasswordAuthenticationToken(
                            userId,
                            null,
                            authorities
                    );
                    authentication.setDetails(new WebAuthenticationDetailsSource().buildDetails(request));

                    SecurityContextHolder.getContext().setAuthentication(authentication);
                    
                    log.debug("JWT authenticated: user={}, role={}", username, role);
                }

            } catch (Exception e) {
                log.warn("JWT authentication failed: {}", e.getMessage());
                // 인증 실패 시 SecurityContext를 비워두고 계속 진행
                // Spring Security가 인증되지 않은 요청으로 처리함
            }

            filterChain.doFilter(request, response);
        }

        @Override
        protected boolean shouldNotFilter(HttpServletRequest request) {
            String path = request.getServletPath();
            // Public 엔드포인트는 필터 스킵
            return path.startsWith("/actuator") ||
                   path.startsWith("/swagger-ui") ||
                   path.startsWith("/v3/api-docs") ||
                   path.endsWith("/health") ||
                   path.contains("/health/") ||
                   path.endsWith("/stream") ||
                   path.contains("/stream/");
        }
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/TrustScoreConfig.java

```java
package com.newsinsight.collector.config;

import lombok.Data;
import org.springframework.boot.context.properties.ConfigurationProperties;
import org.springframework.context.annotation.Configuration;

import java.util.HashMap;
import java.util.Map;

/**
 * Configuration for externalized trust scores.
 * 
 * Trust scores range from 0.0 to 1.0 where:
 * - 0.95+ : Very high trust (academic papers, official statistics)
 * - 0.90-0.94: High trust (encyclopedias, established fact-checkers)
 * - 0.80-0.89: Good trust (reputable news fact-check)
 * - 0.60-0.79: Moderate trust (community wikis, user-generated)
 * - 0.50 : Base trust (unknown sources)
 * - < 0.50: Low trust (unverified, suspicious)
 * 
 * Hierarchy: Academic > Official Statistics > Encyclopedia > News Fact Check
 */
@Configuration
@ConfigurationProperties(prefix = "collector.trust-scores")
@Data
public class TrustScoreConfig {

    /**
     * Trust scores for fact-check sources
     */
    private FactCheckSources factCheck = new FactCheckSources();

    /**
     * Trust scores for trusted reference sources (FactVerificationService)
     */
    private TrustedSources trusted = new TrustedSources();

    /**
     * Trust scores for collected data quality assessment
     */
    private DataQuality dataQuality = new DataQuality();

    /**
     * Additional custom source scores (can be configured dynamically)
     */
    private Map<String, Double> custom = new HashMap<>();

    @Data
    public static class FactCheckSources {
        /** CrossRef academic papers - highest trust */
        private double crossref = 0.95;
        
        /** OpenAlex academic database */
        private double openalex = 0.92;
        
        /** Wikipedia encyclopedia */
        private double wikipedia = 0.90;
        
        /** Google Fact Check verified results */
        private double googleFactCheck = 0.85;
    }

    @Data
    public static class TrustedSources {
        /** Korean Wikipedia */
        private double wikipediaKo = 0.90;
        
        /** English Wikipedia */
        private double wikipediaEn = 0.90;
        
        /** Britannica encyclopedia - very high trust */
        private double britannica = 0.95;
        
        /** Namu Wiki (community wiki - moderate trust) */
        private double namuWiki = 0.60;
        
        /** KOSIS Korean Statistics - official government data */
        private double kosis = 0.95;
        
        /** Google Scholar - academic search */
        private double googleScholar = 0.85;
    }

    @Data
    public static class DataQuality {
        /** Base score for unknown/unverified sources */
        private double baseScore = 0.50;
        
        /** Score for sources in domain whitelist */
        private double whitelistScore = 0.90;
        
        /** Bonus for successful HTTP connection */
        private double httpOkBonus = 0.10;
    }

    /**
     * Get trust score for a source by its key.
     * Falls back to custom map, then to base score.
     */
    public double getScoreForSource(String sourceKey) {
        if (sourceKey == null) return dataQuality.baseScore;
        
        String key = sourceKey.toLowerCase().replace("-", "_").replace(" ", "_");
        
        // Check fact-check sources
        if (key.contains("crossref")) return factCheck.crossref;
        if (key.contains("openalex")) return factCheck.openalex;
        if (key.contains("wikipedia")) {
            if (key.contains("en")) return trusted.wikipediaEn;
            if (key.contains("ko")) return trusted.wikipediaKo;
            return factCheck.wikipedia;
        }
        if (key.contains("google") && key.contains("fact")) return factCheck.googleFactCheck;
        
        // Check trusted sources
        if (key.contains("britannica")) return trusted.britannica;
        if (key.contains("namu")) return trusted.namuWiki;
        if (key.contains("kosis")) return trusted.kosis;
        if (key.contains("scholar")) return trusted.googleScholar;
        
        // Check custom sources
        if (custom.containsKey(key)) return custom.get(key);
        
        // Default
        return dataQuality.baseScore;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/config/WebClientConfig.java

```java
package com.newsinsight.collector.config;

import io.netty.channel.ChannelOption;
import io.netty.handler.timeout.ReadTimeoutHandler;
import io.netty.handler.timeout.WriteTimeoutHandler;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.http.client.SimpleClientHttpRequestFactory;
import org.springframework.http.client.reactive.ReactorClientHttpConnector;
import org.springframework.web.client.RestTemplate;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.netty.http.client.HttpClient;

import java.time.Duration;
import java.util.concurrent.TimeUnit;

@Configuration
public class WebClientConfig {

    @Value("${collector.http.user-agent:NewsInsight-Collector/1.0}")
    private String userAgent;

    @Value("${collector.http.timeout.connect:10000}")
    private int connectTimeout;

    @Value("${collector.http.timeout.read:30000}")
    private int readTimeout;

    @Bean
    public WebClient webClient() {
        HttpClient httpClient = HttpClient.create()
                .option(ChannelOption.CONNECT_TIMEOUT_MILLIS, connectTimeout)
                .responseTimeout(Duration.ofMillis(readTimeout))
                .doOnConnected(conn -> 
                    conn.addHandlerLast(new ReadTimeoutHandler(readTimeout, TimeUnit.MILLISECONDS))
                        .addHandlerLast(new WriteTimeoutHandler(readTimeout, TimeUnit.MILLISECONDS))
                )
                .followRedirect(true);

        return WebClient.builder()
                .clientConnector(new ReactorClientHttpConnector(httpClient))
                .defaultHeader("User-Agent", userAgent)
                .build();
    }

    @Bean
    public RestTemplate restTemplate() {
        SimpleClientHttpRequestFactory factory = new SimpleClientHttpRequestFactory();
        factory.setConnectTimeout(connectTimeout);
        factory.setReadTimeout(readTimeout);
        return new RestTemplate(factory);
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/AiOrchestrationController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.dto.AiJobDto;
import com.newsinsight.collector.dto.AiTaskCallbackRequest;
import com.newsinsight.collector.dto.DeepSearchRequest;
import com.newsinsight.collector.entity.ai.AiJobStatus;
import com.newsinsight.collector.entity.ai.AiProvider;
import com.newsinsight.collector.service.DeepOrchestrationService;
import jakarta.validation.Valid;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.data.domain.Page;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.List;
import java.util.Map;
import java.util.stream.Collectors;

/**
 * Controller for AI orchestration operations.
 * Provides endpoints for:
 * - Starting orchestrated AI analysis jobs
 * - Receiving callbacks from AI workers/n8n
 * - Managing job lifecycle
 */
@RestController
@RequestMapping("/api/v1/ai")
@RequiredArgsConstructor
@Slf4j
public class AiOrchestrationController {

    private final DeepOrchestrationService orchestrationService;

    @Value("${collector.ai.orchestration.callback-token:}")
    private String expectedCallbackToken;

    /**
     * Start a new orchestrated AI analysis job.
     * 
     * @param request The analysis request containing topic and optional base URL
     * @return 202 Accepted with job details
     */
    @PostMapping("/jobs")
    public ResponseEntity<AiJobDto> startAnalysis(
            @Valid @RequestBody DeepSearchRequest request,
            @RequestParam(required = false) List<String> providers
    ) {
        log.info("Starting orchestrated AI analysis for topic: {}", request.getTopic());

        List<AiProvider> providerList = null;
        if (providers != null && !providers.isEmpty()) {
            try {
                providerList = providers.stream()
                        .map(AiProvider::valueOf)
                        .collect(Collectors.toList());
            } catch (IllegalArgumentException e) {
                return ResponseEntity.badRequest()
                        .body(AiJobDto.builder()
                                .overallStatus("ERROR")
                                .errorMessage("Invalid provider: " + e.getMessage())
                                .build());
            }
        }

        AiJobDto job = orchestrationService.startDeepAnalysis(
                request.getTopic(),
                request.getBaseUrl(),
                providerList
        );

        return ResponseEntity.status(HttpStatus.ACCEPTED).body(job);
    }

    /**
     * Get the status of an AI job.
     * 
     * @param jobId The job ID
     * @return Job status details including sub-tasks
     */
    @GetMapping("/jobs/{jobId}")
    public ResponseEntity<AiJobDto> getJobStatus(@PathVariable String jobId) {
        try {
            AiJobDto job = orchestrationService.getJobStatus(jobId);
            return ResponseEntity.ok(job);
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    /**
     * List all AI jobs with optional filtering.
     * 
     * @param page Page number (0-based)
     * @param size Page size
     * @param status Optional status filter
     * @return Paginated list of jobs
     */
    @GetMapping("/jobs")
    public ResponseEntity<Page<AiJobDto>> listJobs(
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size,
            @RequestParam(required = false) String status
    ) {
        AiJobStatus statusFilter = null;
        if (status != null && !status.isBlank()) {
            try {
                statusFilter = AiJobStatus.valueOf(status.toUpperCase());
            } catch (IllegalArgumentException e) {
                log.warn("Invalid status filter: {}", status);
            }
        }

        Page<AiJobDto> jobs = orchestrationService.listJobs(page, size, statusFilter);
        return ResponseEntity.ok(jobs);
    }

    /**
     * Cancel a pending or in-progress job.
     * 
     * @param jobId The job ID to cancel
     * @return Updated job status
     */
    @PostMapping("/jobs/{jobId}/cancel")
    public ResponseEntity<AiJobDto> cancelJob(@PathVariable String jobId) {
        try {
            AiJobDto job = orchestrationService.cancelJob(jobId);
            return ResponseEntity.ok(job);
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    /**
     * Retry failed sub-tasks for a job.
     * 
     * @param jobId The job ID
     * @return Updated job status
     */
    @PostMapping("/jobs/{jobId}/retry")
    public ResponseEntity<AiJobDto> retryJob(@PathVariable String jobId) {
        try {
            AiJobDto job = orchestrationService.retryFailedTasks(jobId);
            return ResponseEntity.ok(job);
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    /**
     * Callback endpoint for AI workers/n8n to deliver results.
     * 
     * @param callbackToken Token for authentication (from header)
     * @param request The callback payload
     * @return Processing result
     */
    @PostMapping("/callback")
    public ResponseEntity<?> handleCallback(
            @RequestHeader(value = "X-Callback-Token", required = false) String callbackToken,
            @RequestBody AiTaskCallbackRequest request
    ) {
        log.info("Received AI callback: jobId={}, subTaskId={}, status={}", 
                request.jobId(), request.subTaskId(), request.status());

        try {
            // Validate callback token if configured
            if (expectedCallbackToken != null && !expectedCallbackToken.isBlank()) {
                String tokenToValidate = callbackToken != null ? callbackToken : request.callbackToken();
                if (!expectedCallbackToken.equals(tokenToValidate)) {
                    log.warn("Invalid callback token for job: {}", request.jobId());
                    return ResponseEntity.status(HttpStatus.UNAUTHORIZED)
                            .body(Map.of("error", "Invalid callback token"));
                }
            }

            orchestrationService.handleCallback(request);

            return ResponseEntity.ok(Map.of(
                    "status", "received",
                    "jobId", request.jobId(),
                    "subTaskId", request.subTaskId()
            ));

        } catch (Exception e) {
            log.error("Error processing AI callback", e);
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)
                    .body(Map.of("error", "Failed to process callback: " + e.getMessage()));
        }
    }

    /**
     * Get available AI providers.
     */
    @GetMapping("/providers")
    public ResponseEntity<List<Map<String, String>>> getProviders() {
        List<Map<String, String>> providers = java.util.Arrays.stream(AiProvider.values())
                .map(p -> Map.of(
                        "id", p.name(),
                        "workflowPath", p.getWorkflowPath(),
                        "description", p.getDescription(),
                        "external", String.valueOf(p.isExternal())
                ))
                .collect(Collectors.toList());
        
        return ResponseEntity.ok(providers);
    }

    /**
     * Health check for AI orchestration service.
     */
    @GetMapping("/health")
    public ResponseEntity<Map<String, Object>> healthCheck() {
        return ResponseEntity.ok(Map.of(
                "status", "UP",
                "service", "ai-orchestration",
                "providers", AiProvider.values().length
        ));
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/AnalysisController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.dto.AnalysisResponseDto;
import com.newsinsight.collector.dto.ArticlesResponseDto;
import com.newsinsight.collector.service.AnalysisService;
import lombok.RequiredArgsConstructor;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RequestParam;
import org.springframework.web.bind.annotation.RestController;

@RestController
@RequestMapping("/api/v1")
@RequiredArgsConstructor
public class AnalysisController {

    private final AnalysisService analysisService;

    @GetMapping("/analysis")
    public ResponseEntity<AnalysisResponseDto> getAnalysis(
            @RequestParam String query,
            @RequestParam(defaultValue = "7d") String window
    ) {
        return ResponseEntity.ok(analysisService.analyze(query, window));
    }

    @GetMapping("/articles")
    public ResponseEntity<ArticlesResponseDto> getArticles(
            @RequestParam String query,
            @RequestParam(defaultValue = "50") int limit
    ) {
        return ResponseEntity.ok(analysisService.searchArticles(query, limit));
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/AutoCrawlController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.entity.autocrawl.CrawlTarget;
import com.newsinsight.collector.entity.autocrawl.CrawlTargetStatus;
import com.newsinsight.collector.entity.autocrawl.DiscoverySource;
import com.newsinsight.collector.repository.CrawlTargetRepository;
import com.newsinsight.collector.service.autocrawl.AutoCrawlDiscoveryService;
import com.newsinsight.collector.service.autocrawl.CrawlQueueService;
import com.newsinsight.collector.scheduler.AutoCrawlScheduler;
import lombok.Builder;
import lombok.Data;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.PageRequest;
import org.springframework.data.domain.Sort;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.List;
import java.util.Map;

/**
 * 자동 크롤링 관리 REST API.
 * 
 * URL 발견, 큐 관리, 상태 조회, 수동 제어 기능을 제공합니다.
 */
@RestController
@RequestMapping("/api/v1/autocrawl")
@RequiredArgsConstructor
@Slf4j
public class AutoCrawlController {

    private final AutoCrawlDiscoveryService discoveryService;
    private final CrawlQueueService queueService;
    private final CrawlTargetRepository targetRepository;
    private final AutoCrawlScheduler autoCrawlScheduler;

    // ========================================
    // 상태 조회
    // ========================================

    /**
     * 큐 상태 및 통계 조회
     */
    @GetMapping("/status")
    public ResponseEntity<AutoCrawlStatusResponse> getStatus() {
        CrawlQueueService.QueueStats stats = queueService.getQueueStats();
        Map<DiscoverySource, Long> discoveryStats = discoveryService.getDiscoveryStats();
        Map<String, Long> domainStats = queueService.getPendingCountByDomain();

        AutoCrawlStatusResponse response = AutoCrawlStatusResponse.builder()
                .pendingCount(stats.getPendingCount())
                .inProgressCount(stats.getInProgressCount())
                .completedCount(stats.getCompletedCount())
                .failedCount(stats.getFailedCount())
                .skippedCount(stats.getSkippedCount())
                .sessionDispatched(stats.getTotalDispatched())
                .sessionCompleted(stats.getTotalCompleted())
                .sessionFailed(stats.getTotalFailed())
                .discoveryStats(discoveryStats)
                .domainPendingStats(domainStats)
                .domainConcurrency(stats.getDomainConcurrency())
                .build();

        return ResponseEntity.ok(response);
    }

    /**
     * 대기 중인 대상 목록 조회 (페이지네이션)
     */
    @GetMapping("/targets")
    public ResponseEntity<Page<CrawlTargetDto>> getTargets(
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size,
            @RequestParam(required = false) CrawlTargetStatus status,
            @RequestParam(required = false) DiscoverySource source) {

        PageRequest pageRequest = PageRequest.of(page, size, 
                Sort.by(Sort.Direction.DESC, "priority").and(Sort.by(Sort.Direction.ASC, "discoveredAt")));

        Page<CrawlTarget> targets;
        if (status != null) {
            targets = targetRepository.findByStatus(status, pageRequest);
        } else if (source != null) {
            targets = targetRepository.findByDiscoverySource(source, pageRequest);
        } else {
            targets = targetRepository.findAll(pageRequest);
        }

        Page<CrawlTargetDto> dtoPage = targets.map(this::toDto);
        return ResponseEntity.ok(dtoPage);
    }

    /**
     * 단일 대상 조회
     */
    @GetMapping("/targets/{id}")
    public ResponseEntity<CrawlTargetDto> getTarget(@PathVariable Long id) {
        return targetRepository.findById(id)
                .map(this::toDto)
                .map(ResponseEntity::ok)
                .orElse(ResponseEntity.notFound().build());
    }

    // ========================================
    // URL 발견 (수동)
    // ========================================

    /**
     * 수동으로 URL 추가
     */
    @PostMapping("/targets")
    public ResponseEntity<CrawlTargetDto> addTarget(@RequestBody AddTargetRequest request) {
        try {
            CrawlTarget target = discoveryService.addManualTarget(
                    request.getUrl(),
                    request.getKeywords(),
                    request.getPriority() != null ? request.getPriority() : 50
            );
            
            if (target == null) {
                return ResponseEntity.badRequest().build();
            }
            
            log.info("Manually added crawl target: url={}, priority={}", request.getUrl(), request.getPriority());
            return ResponseEntity.ok(toDto(target));
        } catch (IllegalArgumentException e) {
            return ResponseEntity.badRequest().build();
        }
    }

    /**
     * 여러 URL 일괄 추가
     */
    @PostMapping("/targets/batch")
    public ResponseEntity<BatchAddResponse> addTargetsBatch(@RequestBody BatchAddRequest request) {
        List<CrawlTarget> targets = discoveryService.addManualTargets(
                request.getUrls(),
                request.getKeywords(),
                request.getPriority() != null ? request.getPriority() : 50
        );

        BatchAddResponse response = BatchAddResponse.builder()
                .addedCount(targets.size())
                .requestedCount(request.getUrls().size())
                .build();

        log.info("Batch added {} crawl targets", targets.size());
        return ResponseEntity.ok(response);
    }

    /**
     * 검색 결과 URL에서 발견
     */
    @PostMapping("/discover/search")
    public ResponseEntity<DiscoverResponse> discoverFromSearch(@RequestBody DiscoverSearchRequest request) {
        List<CrawlTarget> targets = discoveryService.discoverFromSearchUrls(
                request.getQuery(),
                request.getUrls()
        );

        DiscoverResponse response = DiscoverResponse.builder()
                .discoveredCount(targets.size())
                .source(DiscoverySource.SEARCH)
                .build();

        log.info("Discovered {} targets from search query: '{}'", targets.size(), request.getQuery());
        return ResponseEntity.ok(response);
    }

    // ========================================
    // 큐 제어
    // ========================================

    /**
     * 수동으로 큐 처리 트리거
     */
    @PostMapping("/queue/process")
    public ResponseEntity<ProcessQueueResponse> processQueue(
            @RequestParam(defaultValue = "10") int batchSize) {
        int dispatched = autoCrawlScheduler.triggerQueueProcessing(batchSize);

        ProcessQueueResponse response = ProcessQueueResponse.builder()
                .dispatchedCount(dispatched)
                .batchSize(batchSize)
                .build();

        return ResponseEntity.ok(response);
    }

    /**
     * 특정 대상 즉시 분배
     */
    @PostMapping("/targets/{id}/dispatch")
    public ResponseEntity<Void> dispatchTarget(@PathVariable Long id) {
        boolean success = queueService.dispatchSingle(id);
        if (success) {
            log.info("Manually dispatched target: id={}", id);
            return ResponseEntity.ok().build();
        } else {
            return ResponseEntity.badRequest().build();
        }
    }

    /**
     * 특정 키워드 관련 대상 우선순위 부스트
     */
    @PostMapping("/queue/boost")
    public ResponseEntity<BoostResponse> boostKeyword(@RequestBody BoostRequest request) {
        int boosted = queueService.prioritizeKeyword(
                request.getKeyword(),
                request.getBoostAmount() != null ? request.getBoostAmount() : 20
        );

        BoostResponse response = BoostResponse.builder()
                .boostedCount(boosted)
                .keyword(request.getKeyword())
                .build();

        return ResponseEntity.ok(response);
    }

    /**
     * 대상 상태 변경
     */
    @PutMapping("/targets/{id}/status")
    public ResponseEntity<Void> updateTargetStatus(
            @PathVariable Long id,
            @RequestBody UpdateStatusRequest request) {
        boolean success = queueService.updateTargetStatus(id, request.getStatus(), request.getReason());
        if (success) {
            log.info("Updated target status: id={}, newStatus={}", id, request.getStatus());
            return ResponseEntity.ok().build();
        } else {
            return ResponseEntity.notFound().build();
        }
    }

    // ========================================
    // 정리 작업
    // ========================================

    /**
     * 수동으로 정리 트리거
     */
    @PostMapping("/cleanup")
    public ResponseEntity<CleanupResponse> triggerCleanup(
            @RequestParam(defaultValue = "7") int daysOld) {
        int cleaned = queueService.cleanupOldTargets(daysOld);
        int expired = queueService.expireOldPendingTargets(daysOld);

        CleanupResponse response = CleanupResponse.builder()
                .cleanedCount(cleaned)
                .expiredCount(expired)
                .daysOld(daysOld)
                .build();

        log.info("Manual cleanup completed: cleaned={}, expired={}", cleaned, expired);
        return ResponseEntity.ok(response);
    }

    /**
     * 멈춘 작업 복구
     */
    @PostMapping("/queue/recover")
    public ResponseEntity<RecoverResponse> recoverStuck() {
        int recovered = queueService.recoverStuckTargets();

        RecoverResponse response = RecoverResponse.builder()
                .recoveredCount(recovered)
                .build();

        return ResponseEntity.ok(response);
    }

    // ========================================
    // 크롤러 콜백 (autonomous-crawler-service에서 호출)
    // ========================================

    /**
     * 크롤링 완료 콜백
     */
    @PostMapping("/callback")
    public ResponseEntity<Void> handleCrawlerCallback(@RequestBody CrawlerCallbackRequest request) {
        log.debug("Received crawler callback: targetId={}, success={}", 
                request.getTargetId(), request.isSuccess());

        if (request.isSuccess()) {
            queueService.handleCrawlComplete(request.getUrlHash(), request.getCollectedDataId());
        } else {
            queueService.handleCrawlFailed(request.getUrlHash(), request.getError());
        }

        return ResponseEntity.ok().build();
    }

    // ========================================
    // DTO 변환
    // ========================================

    private CrawlTargetDto toDto(CrawlTarget target) {
        return CrawlTargetDto.builder()
                .id(target.getId())
                .url(target.getUrl())
                .urlHash(target.getUrlHash().substring(0, 8) + "...") // 축약
                .discoverySource(target.getDiscoverySource())
                .discoveryContext(target.getDiscoveryContext())
                .priority(target.getPriority())
                .status(target.getStatus())
                .domain(target.getDomain())
                .expectedContentType(target.getExpectedContentType())
                .relatedKeywords(target.getRelatedKeywords())
                .retryCount(target.getRetryCount())
                .maxRetries(target.getMaxRetries())
                .lastError(target.getLastError())
                .discoveredAt(target.getDiscoveredAt() != null ? target.getDiscoveredAt().toString() : null)
                .lastAttemptAt(target.getLastAttemptAt() != null ? target.getLastAttemptAt().toString() : null)
                .completedAt(target.getCompletedAt() != null ? target.getCompletedAt().toString() : null)
                .collectedDataId(target.getCollectedDataId())
                .build();
    }

    // ========================================
    // Request/Response DTOs
    // ========================================

    @Data
    @Builder
    public static class AutoCrawlStatusResponse {
        private long pendingCount;
        private long inProgressCount;
        private long completedCount;
        private long failedCount;
        private long skippedCount;
        private int sessionDispatched;
        private int sessionCompleted;
        private int sessionFailed;
        private Map<DiscoverySource, Long> discoveryStats;
        private Map<String, Long> domainPendingStats;
        private Map<String, Integer> domainConcurrency;
    }

    @Data
    @Builder
    public static class CrawlTargetDto {
        private Long id;
        private String url;
        private String urlHash;
        private DiscoverySource discoverySource;
        private String discoveryContext;
        private Integer priority;
        private CrawlTargetStatus status;
        private String domain;
        private com.newsinsight.collector.entity.autocrawl.ContentType expectedContentType;
        private String relatedKeywords;
        private Integer retryCount;
        private Integer maxRetries;
        private String lastError;
        private String discoveredAt;
        private String lastAttemptAt;
        private String completedAt;
        private Long collectedDataId;
    }

    @Data
    public static class AddTargetRequest {
        private String url;
        private String keywords;
        private Integer priority;
    }

    @Data
    public static class BatchAddRequest {
        private List<String> urls;
        private String keywords;
        private Integer priority;
    }

    @Data
    @Builder
    public static class BatchAddResponse {
        private int addedCount;
        private int requestedCount;
    }

    @Data
    public static class DiscoverSearchRequest {
        private String query;
        private List<String> urls;
    }

    @Data
    @Builder
    public static class DiscoverResponse {
        private int discoveredCount;
        private DiscoverySource source;
    }

    @Data
    @Builder
    public static class ProcessQueueResponse {
        private int dispatchedCount;
        private int batchSize;
    }

    @Data
    public static class BoostRequest {
        private String keyword;
        private Integer boostAmount;
    }

    @Data
    @Builder
    public static class BoostResponse {
        private int boostedCount;
        private String keyword;
    }

    @Data
    public static class UpdateStatusRequest {
        private CrawlTargetStatus status;
        private String reason;
    }

    @Data
    @Builder
    public static class CleanupResponse {
        private int cleanedCount;
        private int expiredCount;
        private int daysOld;
    }

    @Data
    @Builder
    public static class RecoverResponse {
        private int recoveredCount;
    }

    @Data
    public static class CrawlerCallbackRequest {
        private Long targetId;
        private String urlHash;
        private boolean success;
        private Long collectedDataId;
        private String error;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/ChatHealthController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.service.ChatSyncService;
import com.newsinsight.collector.service.VectorEmbeddingService;
import io.micrometer.core.instrument.MeterRegistry;
import lombok.Builder;
import lombok.Data;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.boot.actuate.health.Health;
import org.springframework.boot.actuate.health.HealthIndicator;
import org.springframework.data.mongodb.core.MongoTemplate;
import org.springframework.data.redis.connection.RedisConnectionFactory;
import org.springframework.http.ResponseEntity;
import org.springframework.stereotype.Component;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;

import java.time.LocalDateTime;
import java.util.HashMap;
import java.util.Map;

/**
 * 채팅 서비스 헬스 체크 컨트롤러
 * 
 * 채팅 서비스의 상태와 의존 서비스들의 상태를 확인합니다.
 */
@RestController
@RequestMapping("/api/v1/factcheck-chat/health")
@RequiredArgsConstructor
@Slf4j
public class ChatHealthController {

    private final MongoTemplate mongoTemplate;
    private final RedisConnectionFactory redisConnectionFactory;
    private final VectorEmbeddingService vectorEmbeddingService;
    private final ChatSyncService chatSyncService;
    private final MeterRegistry meterRegistry;

    /**
     * 종합 헬스 체크
     */
    @GetMapping
    public ResponseEntity<HealthResponse> getHealth() {
        HealthResponse response = HealthResponse.builder()
                .status("UP")
                .timestamp(LocalDateTime.now())
                .mongodb(checkMongoHealth())
                .redis(checkRedisHealth())
                .vectorDb(checkVectorDbHealth())
                .sync(getSyncStatus())
                .build();

        // 전체 상태 결정
        if (!response.getMongodb().isHealthy() || !response.getRedis().isHealthy()) {
            response.setStatus("DOWN");
        } else if (!response.getVectorDb().isHealthy()) {
            response.setStatus("DEGRADED");
        }

        return ResponseEntity.ok(response);
    }

    /**
     * MongoDB 상태 확인
     */
    @GetMapping("/mongodb")
    public ResponseEntity<ComponentHealth> getMongoHealth() {
        return ResponseEntity.ok(checkMongoHealth());
    }

    /**
     * Redis 상태 확인
     */
    @GetMapping("/redis")
    public ResponseEntity<ComponentHealth> getRedisHealth() {
        return ResponseEntity.ok(checkRedisHealth());
    }

    /**
     * 벡터 DB 상태 확인
     */
    @GetMapping("/vector")
    public ResponseEntity<ComponentHealth> getVectorHealth() {
        return ResponseEntity.ok(checkVectorDbHealth());
    }

    /**
     * 동기화 상태 확인
     */
    @GetMapping("/sync")
    public ResponseEntity<SyncHealthStatus> getSyncHealth() {
        return ResponseEntity.ok(getSyncStatus());
    }

    /**
     * 메트릭 요약
     */
    @GetMapping("/metrics")
    public ResponseEntity<Map<String, Object>> getMetrics() {
        Map<String, Object> metrics = new HashMap<>();
        
        // 세션 메트릭
        metrics.put("sessions", Map.of(
                "created", getCounterValue("factcheck.chat.sessions.created"),
                "closed", getCounterValue("factcheck.chat.sessions.closed"),
                "active", getGaugeValue("factcheck.chat.sessions.active")
        ));
        
        // 메시지 메트릭
        metrics.put("messages", Map.of(
                "processed", getCounterValue("factcheck.chat.messages.processed")
        ));
        
        // 팩트체크 메트릭
        metrics.put("factcheck", Map.of(
                "success", getCounterValue("factcheck.chat.factcheck.success"),
                "error", getCounterValue("factcheck.chat.factcheck.error")
        ));
        
        // 동기화 메트릭
        metrics.put("sync", Map.of(
                "rdb_success", getCounterValue("chat.sync.rdb.success"),
                "rdb_error", getCounterValue("chat.sync.rdb.error"),
                "embedding_success", getCounterValue("chat.sync.embedding.success"),
                "embedding_error", getCounterValue("chat.sync.embedding.error"),
                "pending_sync", getGaugeValue("chat.sync.rdb.pending"),
                "pending_embedding", getGaugeValue("chat.sync.embedding.pending")
        ));
        
        // 캐시 에러 메트릭
        metrics.put("cache_errors", Map.of(
                "total", getCounterValue("cache.error")
        ));
        
        return ResponseEntity.ok(metrics);
    }

    private ComponentHealth checkMongoHealth() {
        try {
            mongoTemplate.executeCommand("{ ping: 1 }");
            return ComponentHealth.builder()
                    .name("MongoDB")
                    .healthy(true)
                    .message("Connected")
                    .build();
        } catch (Exception e) {
            log.error("MongoDB health check failed: {}", e.getMessage());
            return ComponentHealth.builder()
                    .name("MongoDB")
                    .healthy(false)
                    .message("Connection failed: " + e.getMessage())
                    .build();
        }
    }

    private ComponentHealth checkRedisHealth() {
        try {
            redisConnectionFactory.getConnection().ping();
            return ComponentHealth.builder()
                    .name("Redis")
                    .healthy(true)
                    .message("Connected")
                    .build();
        } catch (Exception e) {
            log.error("Redis health check failed: {}", e.getMessage());
            return ComponentHealth.builder()
                    .name("Redis")
                    .healthy(false)
                    .message("Connection failed: " + e.getMessage())
                    .build();
        }
    }

    private ComponentHealth checkVectorDbHealth() {
        VectorEmbeddingService.VectorServiceStatus status = vectorEmbeddingService.getStatus();
        
        if (!status.isEnabled()) {
            return ComponentHealth.builder()
                    .name("VectorDB")
                    .healthy(true) // disabled는 에러가 아님
                    .message("Disabled")
                    .build();
        }
        
        return ComponentHealth.builder()
                .name("VectorDB")
                .healthy(status.isVectorDbHealthy())
                .message(status.isVectorDbHealthy() ? "Connected" : "Connection failed")
                .details(Map.of(
                        "url", status.getVectorDbUrl(),
                        "collection", status.getCollectionName(),
                        "embeddingServiceHealthy", status.isEmbeddingServiceHealthy(),
                        "queueSize", status.getQueueSize()
                ))
                .build();
    }

    private SyncHealthStatus getSyncStatus() {
        ChatSyncService.SyncStats stats = chatSyncService.getSyncStats();
        
        return SyncHealthStatus.builder()
                .healthy(stats.getActiveSyncCount() < 10) // 동시 동기화 10개 미만이면 정상
                .pendingSyncCount(stats.getPendingSyncCount())
                .pendingEmbeddingCount(stats.getPendingEmbeddingCount())
                .activeSyncCount(stats.getActiveSyncCount())
                .build();
    }

    private double getCounterValue(String name) {
        try {
            var counter = meterRegistry.find(name).counter();
            return counter != null ? counter.count() : 0;
        } catch (Exception e) {
            return 0;
        }
    }

    private double getGaugeValue(String name) {
        try {
            var gauge = meterRegistry.find(name).gauge();
            return gauge != null ? gauge.value() : 0;
        } catch (Exception e) {
            return 0;
        }
    }

    @Data
    @Builder
    public static class HealthResponse {
        private String status;
        private LocalDateTime timestamp;
        private ComponentHealth mongodb;
        private ComponentHealth redis;
        private ComponentHealth vectorDb;
        private SyncHealthStatus sync;
    }

    @Data
    @Builder
    public static class ComponentHealth {
        private String name;
        private boolean healthy;
        private String message;
        private Map<String, Object> details;
    }

    @Data
    @Builder
    public static class SyncHealthStatus {
        private boolean healthy;
        private long pendingSyncCount;
        private long pendingEmbeddingCount;
        private int activeSyncCount;
    }
}

/**
 * Spring Boot Actuator Health Indicator
 */
@Component
@RequiredArgsConstructor
@Slf4j
class ChatServiceHealthIndicator implements HealthIndicator {

    private final MongoTemplate mongoTemplate;
    private final RedisConnectionFactory redisConnectionFactory;
    private final VectorEmbeddingService vectorEmbeddingService;

    @Override
    public Health health() {
        Health.Builder builder = Health.up();
        
        // MongoDB 체크
        try {
            mongoTemplate.executeCommand("{ ping: 1 }");
            builder.withDetail("mongodb", "UP");
        } catch (Exception e) {
            builder.down().withDetail("mongodb", "DOWN: " + e.getMessage());
            return builder.build();
        }
        
        // Redis 체크
        try {
            redisConnectionFactory.getConnection().ping();
            builder.withDetail("redis", "UP");
        } catch (Exception e) {
            builder.down().withDetail("redis", "DOWN: " + e.getMessage());
            return builder.build();
        }
        
        // Vector DB 체크 (optional)
        VectorEmbeddingService.VectorServiceStatus vectorStatus = vectorEmbeddingService.getStatus();
        if (vectorStatus.isEnabled()) {
            if (vectorStatus.isVectorDbHealthy()) {
                builder.withDetail("vectorDb", "UP");
            } else {
                builder.withDetail("vectorDb", "DOWN");
                // Vector DB는 optional이므로 degraded 상태로
            }
        } else {
            builder.withDetail("vectorDb", "DISABLED");
        }
        
        return builder.build();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/ClaimExtractionController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.dto.ClaimExtractionRequest;
import com.newsinsight.collector.dto.ClaimExtractionResponse;
import com.newsinsight.collector.service.ClaimExtractionService;
import jakarta.validation.Valid;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;
import reactor.core.publisher.Mono;

import java.util.Map;

/**
 * Controller for claim extraction operations.
 * Extracts verifiable claims from URLs for fact-checking.
 */
@RestController
@RequestMapping("/api/v1/analysis")
@RequiredArgsConstructor
@Slf4j
public class ClaimExtractionController {

    private final ClaimExtractionService claimExtractionService;

    /**
     * Extract verifiable claims from a URL.
     * 
     * This endpoint:
     * 1. Crawls the given URL to extract page content
     * 2. Analyzes the content using AI to identify verifiable claims
     * 3. Returns structured claims with confidence scores
     * 
     * @param request The extraction request containing the URL
     * @return List of extracted claims with metadata
     */
    @PostMapping("/extract-claims")
    public Mono<ResponseEntity<ClaimExtractionResponse>> extractClaims(
            @Valid @RequestBody ClaimExtractionRequest request
    ) {
        log.info("Received claim extraction request for URL: {}", request.getUrl());

        return claimExtractionService.extractClaims(request)
                .map(response -> {
                    if (response == null) {
                        return ResponseEntity.internalServerError()
                                .body(ClaimExtractionResponse.builder()
                                        .url(request.getUrl())
                                        .message("추출 서비스 오류가 발생했습니다.")
                                        .build());
                    }

                    log.info("Extracted {} claims from URL: {}",
                            response.getClaims() != null ? response.getClaims().size() : 0,
                            request.getUrl());

                    return ResponseEntity.ok(response);
                })
                .onErrorResume(e -> {
                    log.error("Claim extraction failed for URL: {}", request.getUrl(), e);
                    return Mono.just(ResponseEntity.internalServerError()
                            .body(ClaimExtractionResponse.builder()
                                    .url(request.getUrl())
                                    .message("주장 추출 실패: " + e.getMessage())
                                    .build()));
                });
    }

    /**
     * Health check for claim extraction service.
     */
    @GetMapping("/extract-claims/health")
    public ResponseEntity<Map<String, Object>> healthCheck() {
        return ResponseEntity.ok(Map.of(
                "service", "ClaimExtractionService",
                "status", "READY"
        ));
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/CollectionController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.dto.CollectionJobDTO;
import com.newsinsight.collector.dto.CollectionRequest;
import com.newsinsight.collector.dto.CollectionResponse;
import com.newsinsight.collector.dto.CollectionStatsDTO;
import com.newsinsight.collector.dto.PageResponse;
import com.newsinsight.collector.entity.CollectionJob;
import com.newsinsight.collector.entity.CollectionJob.JobStatus;
import com.newsinsight.collector.mapper.EntityMapper;
import com.newsinsight.collector.service.CollectionService;
import jakarta.validation.Valid;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.time.LocalDateTime;
import java.util.List;

@RestController
@RequestMapping("/api/v1/collections")
public class CollectionController {

    private final CollectionService collectionService;
    private final EntityMapper entityMapper;

    public CollectionController(CollectionService collectionService, EntityMapper entityMapper) {
        this.collectionService = collectionService;
        this.entityMapper = entityMapper;
    }

    /**
     * POST /api/v1/collections/start - 수집 작업 시작 (전체 또는 특정 소스)
     */
    @PostMapping("/start")
    public ResponseEntity<CollectionResponse> startCollection(
            @Valid @RequestBody CollectionRequest request) {

        List<CollectionJob> jobs;

        if (request.sourceIds().isEmpty()) {
            // 활성화된 모든 소스 대상으로 수집
            jobs = collectionService.startCollectionForAllActive();
        } else {
            // 지정된 소스들만 수집
            jobs = collectionService.startCollectionForSources(request.sourceIds());
        }

        List<CollectionJobDTO> jobDTOs = jobs.stream()
                .map(entityMapper::toCollectionJobDTO)
                .toList();

        CollectionResponse response = new CollectionResponse(
                "Collection started for " + jobs.size() + " source(s)",
                jobDTOs,
                jobs.size(),
                LocalDateTime.now()
        );

        return ResponseEntity.status(HttpStatus.ACCEPTED).body(response);
    }

    /**
     * GET /api/v1/collections/jobs - 수집 작업 목록 조회 (상태별 필터링)
     */
    @GetMapping("/jobs")
    public ResponseEntity<PageResponse<CollectionJobDTO>> listJobs(
            Pageable pageable,
            @RequestParam(required = false) JobStatus status) {

        Page<CollectionJob> jobs = (status != null)
                ? collectionService.getJobsByStatus(status, pageable)
                : collectionService.getAllJobs(pageable);

        Page<CollectionJobDTO> jobDTOs = jobs.map(entityMapper::toCollectionJobDTO);

        return ResponseEntity.ok(PageResponse.from(jobDTOs));
    }

    /**
     * GET /api/v1/collections/jobs/{id} - 특정 작업 상세 조회
     */
    @GetMapping("/jobs/{id}")
    public ResponseEntity<CollectionJobDTO> getJob(@PathVariable Long id) {
        return collectionService.getJobById(id)
                .map(entityMapper::toCollectionJobDTO)
                .map(ResponseEntity::ok)
                .orElse(ResponseEntity.notFound().build());
    }

    /**
     * POST /api/v1/collections/jobs/{id}/cancel - 수집 작업 취소
     */
    @PostMapping("/jobs/{id}/cancel")
    public ResponseEntity<Void> cancelJob(@PathVariable Long id) {
        boolean cancelled = collectionService.cancelJob(id);
        return cancelled ? ResponseEntity.noContent().build() : ResponseEntity.notFound().build();
    }

    /**
     * GET /api/v1/collections/stats - 수집 통계 조회
     */
    @GetMapping("/stats")
    public ResponseEntity<CollectionStatsDTO> getStats() {
        CollectionStatsDTO stats = collectionService.getStatistics();
        return ResponseEntity.ok(stats);
    }

    /**
     * DELETE /api/v1/collections/jobs/cleanup - 오래된 작업 정리
     */
    @DeleteMapping("/jobs/cleanup")
    public ResponseEntity<String> cleanupOldJobs(
            @RequestParam(defaultValue = "30") int daysOld) {
        
        int cleaned = collectionService.cleanupOldJobs(daysOld);
        return ResponseEntity.ok("Cleaned up " + cleaned + " old jobs");
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/DashboardEventController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.dto.DashboardEventDto;
import com.newsinsight.collector.service.DashboardEventService;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.http.MediaType;
import org.springframework.http.codec.ServerSentEvent;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;
import reactor.core.publisher.Flux;

import java.time.Duration;

/**
 * 대시보드 실시간 이벤트 스트리밍 컨트롤러.
 * SSE(Server-Sent Events)를 통해 클라이언트에 실시간 업데이트를 푸시합니다.
 */
@RestController
@RequestMapping("/api/v1/events")
@RequiredArgsConstructor
@Slf4j
public class DashboardEventController {

    private final DashboardEventService dashboardEventService;

    /**
     * 대시보드 이벤트 스트림.
     * 클라이언트는 이 엔드포인트에 연결하여 실시간 이벤트를 수신합니다.
     * 
     * 이벤트 타입:
     * - HEARTBEAT: 연결 유지용 (30초마다)
     * - NEW_DATA: 새로운 데이터 수집됨
     * - SOURCE_UPDATED: 소스 상태 변경
     * - STATS_UPDATED: 통계 갱신
     * 
     * @return SSE 스트림
     */
    @GetMapping(value = "/stream", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
    public Flux<ServerSentEvent<DashboardEventDto>> streamEvents() {
        log.info("New SSE client connected to dashboard event stream");

        // 연결 확인 이벤트 (즉시 전송)
        Flux<ServerSentEvent<DashboardEventDto>> connected = Flux.just(
                ServerSentEvent.<DashboardEventDto>builder()
                        .event("connected")
                        .data(DashboardEventDto.heartbeat())
                        .build()
        );

        // 하트비트 스트림 (즉시 시작, 30초마다)
        Flux<ServerSentEvent<DashboardEventDto>> heartbeat = Flux.interval(Duration.ZERO, Duration.ofSeconds(30))
                .skip(1) // 첫 번째는 connected 이벤트로 대체
                .map(tick -> ServerSentEvent.<DashboardEventDto>builder()
                        .event("heartbeat")
                        .data(DashboardEventDto.heartbeat())
                        .build());

        // 이벤트 스트림
        Flux<ServerSentEvent<DashboardEventDto>> events = dashboardEventService.getEventStream()
                .map(event -> ServerSentEvent.<DashboardEventDto>builder()
                        .event(event.getEventType().name().toLowerCase())
                        .data(event)
                        .build());

        // 세 스트림 병합 (connected 먼저, 그 다음 heartbeat + events)
        return Flux.concat(connected, Flux.merge(heartbeat, events))
                .doOnCancel(() -> log.info("SSE client disconnected from dashboard event stream"))
                .doOnError(e -> log.error("SSE stream error", e));
    }

    /**
     * 데이터 통계 스트림.
     * 5초마다 최신 통계를 전송합니다.
     * 
     * @return SSE 스트림
     */
    @GetMapping(value = "/stats/stream", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
    public Flux<ServerSentEvent<DashboardEventDto>> streamStats() {
        log.debug("New SSE client connected to stats stream");

        return Flux.interval(Duration.ZERO, Duration.ofSeconds(5))
                .flatMap(tick -> dashboardEventService.getCurrentStats())
                .map(stats -> ServerSentEvent.<DashboardEventDto>builder()
                        .event("stats")
                        .data(stats)
                        .build())
                .doOnCancel(() -> log.debug("SSE client disconnected from stats stream"));
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/DataController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.dto.CollectedDataDTO;
import com.newsinsight.collector.entity.CollectedData;
import com.newsinsight.collector.mapper.EntityMapper;
import com.newsinsight.collector.service.CollectedDataService;
import lombok.RequiredArgsConstructor;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.PageRequest;
import org.springframework.data.domain.Pageable;
import org.springframework.data.domain.Sort;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

@RestController
@RequestMapping("/api/v1/data")
@RequiredArgsConstructor
public class DataController {

    private final CollectedDataService collectedDataService;
    private final EntityMapper entityMapper;

    /**
     * GET /api/v1/data - 수집된 데이터 목록 조회 (소스/처리상태/검색 필터링 지원)
     */
    @GetMapping
    public ResponseEntity<Page<CollectedDataDTO>> listData(
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size,
            @RequestParam(required = false) Long sourceId,
            @RequestParam(required = false) Boolean processed,
            @RequestParam(required = false) String query) {
        
        Pageable pageable = PageRequest.of(page, size, Sort.by(Sort.Direction.DESC, "collectedAt"));
        
        Page<CollectedData> data;
        
        // 검색어가 있는 경우
        if (query != null && !query.isBlank()) {
            data = collectedDataService.searchWithFilter(query, processed, pageable);
        } else if (sourceId != null && processed != null) {
            // 소스 + 처리상태 동시 필터링은 별도의 커스텀 쿼리 필요 (현재는 소스 기준 필터만 수행)
            data = collectedDataService.findBySourceId(sourceId, pageable);
        } else if (sourceId != null) {
            data = collectedDataService.findBySourceId(sourceId, pageable);
        } else if (Boolean.FALSE.equals(processed)) {
            data = collectedDataService.findUnprocessed(pageable);
        } else {
            data = collectedDataService.findAll(pageable);
        }
        
        Page<CollectedDataDTO> dataDTOs = data.map(entityMapper::toCollectedDataDTO);
        
        return ResponseEntity.ok(dataDTOs);
    }

    /**
     * GET /api/v1/data/unprocessed - 미처리 데이터 목록 조회
     */
    @GetMapping("/unprocessed")
    public ResponseEntity<Page<CollectedDataDTO>> listUnprocessedData(
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size) {
        
        Pageable pageable = PageRequest.of(page, size, Sort.by(Sort.Direction.DESC, "collectedAt"));
        Page<CollectedData> data = collectedDataService.findUnprocessed(pageable);
        Page<CollectedDataDTO> dataDTOs = data.map(entityMapper::toCollectedDataDTO);
        
        return ResponseEntity.ok(dataDTOs);
    }

    /**
     * GET /api/v1/data/{id} - 수집된 데이터 단건 조회 (ID)
     */
    @GetMapping("/{id}")
    public ResponseEntity<CollectedDataDTO> getData(@PathVariable Long id) {
        return collectedDataService.findById(id)
                .map(entityMapper::toCollectedDataDTO)
                .map(ResponseEntity::ok)
                .orElse(ResponseEntity.notFound().build());
    }

    /**
     * POST /api/v1/data/{id}/processed - 데이터 처리 완료 마킹
     */
    @PostMapping("/{id}/processed")
    public ResponseEntity<Void> markAsProcessed(@PathVariable Long id) {
        boolean marked = collectedDataService.markAsProcessed(id);
        return marked ? ResponseEntity.noContent().build() : ResponseEntity.notFound().build();
    }

    /**
     * GET /api/v1/data/stats - 데이터 통계 조회 (전체/미처리/처리완료)
     */
    @GetMapping("/stats")
    public ResponseEntity<DataStatsResponse> getDataStats() {
        long total = collectedDataService.countTotal();
        long unprocessed = collectedDataService.countUnprocessed();
        
        DataStatsResponse stats = new DataStatsResponse(total, unprocessed, total - unprocessed);
        return ResponseEntity.ok(stats);
    }

    /**
     * 간단한 통계 응답 구조체
     */
    public record DataStatsResponse(long total, long unprocessed, long processed) {}
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/DeepAnalysisController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.dto.*;
import com.newsinsight.collector.entity.CrawlJobStatus;
import com.newsinsight.collector.service.DeepAnalysisService;
import com.newsinsight.collector.service.DeepSearchEventService;
import com.newsinsight.collector.service.IntegratedCrawlerService;
import jakarta.validation.Valid;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.data.domain.Page;
import org.springframework.http.HttpStatus;
import org.springframework.http.MediaType;
import org.springframework.http.ResponseEntity;
import org.springframework.http.codec.ServerSentEvent;
import org.springframework.web.bind.annotation.*;
import reactor.core.publisher.Flux;

import java.util.List;
import java.util.Map;

/**
 * Controller for deep AI search operations.
 * Provides endpoints for:
 * - Starting a new deep search
 * - Receiving callbacks from internal workers
 * - Retrieving search results
 * - Real-time SSE streaming of search progress
 * 
 * Uses IntegratedCrawlerService for multi-strategy crawling:
 * - Crawl4AI for JS-rendered pages
 * - Browser-Use API for complex interactions
 * - Direct HTTP for simple pages
 * - Search Engines for topic-based searches
 */
@RestController
@RequestMapping("/api/v1/analysis/deep")
@RequiredArgsConstructor
@Slf4j
public class DeepAnalysisController {

    private final DeepAnalysisService deepAnalysisService;
    private final DeepSearchEventService deepSearchEventService;
    private final IntegratedCrawlerService integratedCrawlerService;

    /**
     * Start a new deep AI search job.
     * 
     * @param request The search request containing topic and optional base URL
     * @return 202 Accepted with job details
     */
    @PostMapping
    public ResponseEntity<DeepSearchJobDto> startDeepSearch(
            @Valid @RequestBody DeepSearchRequest request
    ) {
        log.info("Starting deep search for topic: {}", request.getTopic());
        
        if (!integratedCrawlerService.isAvailable()) {
            return ResponseEntity.status(HttpStatus.SERVICE_UNAVAILABLE)
                    .body(DeepSearchJobDto.builder()
                            .status("UNAVAILABLE")
                            .errorMessage("Deep search service is not available. Please check crawler configuration.")
                            .build());
        }

        DeepSearchJobDto job = deepAnalysisService.startDeepSearch(
                request.getTopic(),
                request.getBaseUrl()
        );

        return ResponseEntity.status(HttpStatus.ACCEPTED).body(job);
    }

    /**
     * Get the status of a deep search job.
     * 
     * @param jobId The job ID
     * @return Job status details
     */
    @GetMapping("/{jobId}")
    public ResponseEntity<DeepSearchJobDto> getJobStatus(@PathVariable String jobId) {
        try {
            DeepSearchJobDto job = deepAnalysisService.getJobStatus(jobId);
            return ResponseEntity.ok(job);
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    /**
     * Get the full results of a completed deep search.
     * 
     * @param jobId The job ID
     * @return Full search results including evidence
     */
    @GetMapping("/{jobId}/result")
    public ResponseEntity<DeepSearchResultDto> getSearchResult(@PathVariable String jobId) {
        try {
            DeepSearchResultDto result = deepAnalysisService.getSearchResult(jobId);
            return ResponseEntity.ok(result);
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    /**
     * SSE stream for real-time job updates.
     * Clients can subscribe to this endpoint to receive live updates for a job.
     * 
     * Events:
     * - status: Job status changes (PENDING, IN_PROGRESS, COMPLETED, FAILED)
     * - progress: Progress updates (0-100%)
     * - evidence: New evidence found during the search
     * - complete: Job completed successfully
     * - error: Job failed with error
     * - heartbeat: Keep-alive ping every 15 seconds
     * 
     * @param jobId The job ID to subscribe to
     * @return SSE event stream
     */
    @GetMapping(value = "/{jobId}/stream", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
    public Flux<ServerSentEvent<Object>> streamJobUpdates(@PathVariable String jobId) {
        log.info("New SSE client subscribed to job: {}", jobId);
        
        // Validate job exists
        try {
            DeepSearchJobDto job = deepAnalysisService.getJobStatus(jobId);
            
            // If job is already completed or failed, send immediate result and close
            if ("COMPLETED".equals(job.getStatus()) || "FAILED".equals(job.getStatus()) 
                    || "CANCELLED".equals(job.getStatus()) || "TIMEOUT".equals(job.getStatus())) {
                log.info("Job {} already finished with status: {}, sending immediate result", jobId, job.getStatus());
                return Flux.just(ServerSentEvent.builder()
                        .event("complete")
                        .data(Map.of(
                                "jobId", jobId,
                                "job", job,
                                "timestamp", System.currentTimeMillis()
                        ))
                        .build());
            }
        } catch (IllegalArgumentException e) {
            log.warn("SSE subscription for unknown job: {}", jobId);
            return Flux.just(ServerSentEvent.builder()
                    .event("error")
                    .data(Map.of(
                            "jobId", jobId,
                            "error", "Job not found: " + jobId,
                            "timestamp", System.currentTimeMillis()
                    ))
                    .build());
        }

        return deepSearchEventService.getJobEventStream(jobId);
    }

    /**
     * List all deep search jobs with optional filtering.
     * 
     * @param page Page number (0-based)
     * @param size Page size
     * @param status Optional status filter
     * @return Paginated list of jobs
     */
    @GetMapping
    public ResponseEntity<Page<DeepSearchJobDto>> listJobs(
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size,
            @RequestParam(required = false) String status
    ) {
        CrawlJobStatus statusFilter = null;
        if (status != null && !status.isBlank()) {
            try {
                statusFilter = CrawlJobStatus.valueOf(status.toUpperCase());
            } catch (IllegalArgumentException e) {
                log.warn("Invalid status filter: {}", status);
            }
        }

        Page<DeepSearchJobDto> jobs = deepAnalysisService.listJobs(page, size, statusFilter);
        return ResponseEntity.ok(jobs);
    }

    /**
     * Cancel a pending or in-progress job.
     * 
     * @param jobId The job ID to cancel
     * @return Updated job status
     */
    @PostMapping("/{jobId}/cancel")
    public ResponseEntity<DeepSearchJobDto> cancelJob(@PathVariable String jobId) {
        try {
            DeepSearchJobDto job = deepAnalysisService.cancelJob(jobId);
            return ResponseEntity.ok(job);
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    /**
     * Callback endpoint for internal async workers to deliver results.
     * This can be used by future Kafka-based workers or other internal services.
     * 
     * @param callbackToken Token for authentication (from header)
     * @param payload The callback payload
     * @return Processing result
     */
    @PostMapping("/callback")
    public ResponseEntity<?> handleCallback(
            @RequestHeader(value = "X-Crawl-Callback-Token", required = false) String callbackToken,
            @RequestBody DeepSearchCallbackDto payload
    ) {
        log.info("Received internal callback for job: {}, status: {}", payload.getJobId(), payload.getStatus());

        try {
            // Convert DTO evidence to service format
            List<EvidenceDto> evidenceList = payload.getEvidence() != null 
                    ? payload.getEvidence().stream()
                            .map(e -> EvidenceDto.builder()
                                    .url(e.getUrl())
                                    .title(e.getTitle())
                                    .stance(e.getStance())
                                    .snippet(e.getSnippet())
                                    .source(e.getSource())
                                    .build())
                            .toList()
                    : List.of();

            DeepSearchResultDto result = deepAnalysisService.processInternalCallback(
                    callbackToken, 
                    payload.getJobId(),
                    payload.getStatus(),
                    evidenceList
            );
            
            return ResponseEntity.ok(Map.of(
                    "status", "received",
                    "jobId", result.getJobId(),
                    "evidenceCount", result.getEvidenceCount()
            ));

        } catch (SecurityException e) {
            log.warn("Callback authentication failed: {}", e.getMessage());
            return ResponseEntity.status(HttpStatus.UNAUTHORIZED)
                    .body(Map.of("error", "Invalid callback token"));

        } catch (IllegalArgumentException e) {
            log.warn("Callback for unknown job: {}", e.getMessage());
            return ResponseEntity.status(HttpStatus.NOT_FOUND)
                    .body(Map.of("error", e.getMessage()));

        } catch (Exception e) {
            log.error("Error processing callback", e);
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)
                    .body(Map.of("error", "Failed to process callback: " + e.getMessage()));
        }
    }

    /**
     * Health check for deep search service.
     */
    @GetMapping("/health")
    public ResponseEntity<Map<String, Object>> healthCheck() {
        boolean isAvailable = integratedCrawlerService.isAvailable();
        return ResponseEntity.ok(Map.of(
                "available", isAvailable,
                "service", "IntegratedCrawlerService",
                "status", isAvailable ? "READY" : "UNAVAILABLE"
        ));
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/FactCheckChatController.java

```java
package com.newsinsight.collector.controller;

import com.fasterxml.jackson.databind.ObjectMapper;
import com.newsinsight.collector.service.FactCheckChatService;
import lombok.Data;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.http.MediaType;
import org.springframework.http.codec.ServerSentEvent;
import org.springframework.web.bind.annotation.*;
import reactor.core.publisher.Flux;

import java.time.Duration;
import java.util.List;
import java.util.UUID;

/**
 * 팩트체크 챗봇 컨트롤러
 * 
 * 사용자와 대화하며 실시간으로 팩트체크 결과를 제공합니다.
 * SSE를 통해 스트리밍 방식으로 응답을 전송합니다.
 */
@RestController
@RequestMapping("/api/v1/factcheck-chat")
@RequiredArgsConstructor
@Slf4j
@CrossOrigin(origins = "*")
public class FactCheckChatController {

    private final FactCheckChatService factCheckChatService;
    private final ObjectMapper objectMapper;

    /**
     * 팩트체크 챗봇 세션 시작
     * 
     * @param request 초기 메시지 요청
     * @return 세션 ID
     */
    @PostMapping("/session")
    public SessionResponse createSession(@RequestBody ChatRequest request) {
        String sessionId = UUID.randomUUID().toString();
        log.info("Created fact-check chat session: {}", sessionId);
        
        return SessionResponse.builder()
                .sessionId(sessionId)
                .message("팩트체크 챗봇 세션이 시작되었습니다.")
                .build();
    }

    /**
     * 팩트체크 챗봇 메시지 전송 및 SSE 스트리밍 응답
     * 
     * @param sessionId 세션 ID
     * @param request 사용자 메시지
     * @return SSE 이벤트 스트림
     */
    @PostMapping(value = "/session/{sessionId}/message", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
    public Flux<ServerSentEvent<String>> sendMessage(
            @PathVariable String sessionId,
            @RequestBody ChatRequest request
    ) {
        log.info("Received message for session {}: {}", sessionId, request.getMessage());

        return factCheckChatService.processMessage(sessionId, request.getMessage(), request.getClaims())
                .map(event -> {
                    try {
                        String data = objectMapper.writeValueAsString(event);
                        return ServerSentEvent.<String>builder()
                                .id(UUID.randomUUID().toString())
                                .event(event.getType())
                                .data(data)
                                .build();
                    } catch (Exception e) {
                        log.error("Failed to serialize chat event: {}", e.getMessage());
                        return ServerSentEvent.<String>builder()
                                .event("error")
                                .data("{\"error\": \"Serialization failed\"}")
                                .build();
                    }
                })
                .concatWith(Flux.just(
                        ServerSentEvent.<String>builder()
                                .event("done")
                                .data("{\"message\": \"Response completed\"}")
                                .build()
                ))
                .timeout(Duration.ofMinutes(3))
                .onErrorResume(e -> {
                    log.error("Error in chat stream for session {}: {}", sessionId, e.getMessage());
                    return Flux.just(
                            ServerSentEvent.<String>builder()
                                    .event("error")
                                    .data("{\"error\": \"" + e.getMessage() + "\"}")
                                    .build()
                    );
                });
    }

    /**
     * 세션 종료
     * 
     * @param sessionId 세션 ID
     */
    @DeleteMapping("/session/{sessionId}")
    public void closeSession(@PathVariable String sessionId) {
        log.info("Closing fact-check chat session: {}", sessionId);
        factCheckChatService.closeSession(sessionId);
    }

    /**
     * 세션 이력 조회
     * 
     * @param sessionId 세션 ID
     * @return 대화 이력
     */
    @GetMapping("/session/{sessionId}/history")
    public ChatHistoryResponse getHistory(@PathVariable String sessionId) {
        List<ChatMessage> history = factCheckChatService.getHistory(sessionId);
        return ChatHistoryResponse.builder()
                .sessionId(sessionId)
                .messages(history)
                .build();
    }

    // DTO Classes
    
    @Data
    public static class ChatRequest {
        private String message;
        private List<String> claims;
    }

    @Data
    @lombok.Builder
    public static class SessionResponse {
        private String sessionId;
        private String message;
    }

    @Data
    @lombok.Builder
    public static class ChatHistoryResponse {
        private String sessionId;
        private List<ChatMessage> messages;
    }

    @Data
    @lombok.Builder
    public static class ChatMessage {
        private String role; // user, assistant, system
        private String content;
        private Long timestamp;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/LiveAnalysisController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.client.AIDoveClient;
import com.newsinsight.collector.client.OpenAICompatibleClient;
import com.newsinsight.collector.client.PerplexityClient;
import com.newsinsight.collector.service.CrawlSearchService;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.http.MediaType;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RequestParam;
import org.springframework.web.bind.annotation.RestController;
import reactor.core.publisher.Flux;

import java.time.Duration;
import java.util.ArrayList;
import java.util.LinkedHashMap;
import java.util.List;
import java.util.Map;
import java.util.function.Supplier;

@RestController
@RequestMapping("/api/v1/analysis")
@RequiredArgsConstructor
@Slf4j
public class LiveAnalysisController {

    private final PerplexityClient perplexityClient;
    private final OpenAICompatibleClient openAICompatibleClient;
    private final AIDoveClient aiDoveClient;
    private final CrawlSearchService crawlSearchService;

    /**
     * Health check for live analysis service.
     * Returns whether the analysis APIs are configured and available.
     */
    @GetMapping("/live/health")
    public ResponseEntity<Map<String, Object>> liveAnalysisHealth() {
        List<String> availableProviders = getAvailableProviders();
        boolean anyEnabled = !availableProviders.isEmpty();

        String primaryProvider = availableProviders.isEmpty() ? "none" : availableProviders.get(0);
        String message = anyEnabled 
                ? "Live analysis is available (" + String.join(", ", availableProviders) + ")"
                : "Live analysis is disabled. No AI provider is configured.";

        Map<String, Object> response = new LinkedHashMap<>();
        response.put("enabled", anyEnabled);
        response.put("primaryProvider", primaryProvider);
        response.put("availableProviders", availableProviders);
        response.put("providerStatus", Map.of(
                "perplexity", perplexityClient.isEnabled(),
                "openai", openAICompatibleClient.isOpenAIEnabled(),
                "openrouter", openAICompatibleClient.isOpenRouterEnabled(),
                "azure", openAICompatibleClient.isAzureEnabled(),
                "aidove", aiDoveClient.isEnabled(),
                "ollama", true, // Ollama is always potentially available
                "custom", openAICompatibleClient.isCustomEnabled(),
                "crawl", crawlSearchService.isAvailable()
        ));
        response.put("message", message);

        return ResponseEntity.ok(response);
    }

    @GetMapping(value = "/live", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
    public Flux<String> streamLiveAnalysis(
            @RequestParam String query,
            @RequestParam(defaultValue = "7d") String window
    ) {
        String prompt = buildPrompt(query, window);
        log.info("Starting live analysis for query='{}', window='{}'", query, window);

        // Build provider chain and try with fallback
        List<ProviderAttempt> providers = buildProviderChain(prompt, query, window);
        
        if (providers.isEmpty()) {
            log.warn("Live analysis requested but no provider is available");
            return Flux.just(
                    "실시간 분석 기능이 현재 사용할 수 없습니다.\n\n" +
                    "설정된 AI 제공자가 없습니다.\n" +
                    "관리자에게 문의하세요.\n\n" +
                    "대안: Deep AI Search 또는 Browser AI Agent를 사용해 보세요."
            );
        }

        log.info("Live analysis using fallback chain: {}", 
                providers.stream().map(ProviderAttempt::name).toList());

        return tryProvidersInSequence(providers, 0);
    }

    /**
     * Get list of available providers
     */
    private List<String> getAvailableProviders() {
        List<String> available = new ArrayList<>();
        
        if (perplexityClient.isEnabled()) available.add("Perplexity");
        if (openAICompatibleClient.isOpenAIEnabled()) available.add("OpenAI");
        if (openAICompatibleClient.isOpenRouterEnabled()) available.add("OpenRouter");
        if (openAICompatibleClient.isAzureEnabled()) available.add("Azure");
        if (aiDoveClient.isEnabled()) available.add("AI Dove");
        available.add("Ollama"); // Always potentially available
        if (openAICompatibleClient.isCustomEnabled()) available.add("Custom");
        if (crawlSearchService.isAvailable()) available.add("Crawl+AIDove");
        
        return available;
    }

    /**
     * Build provider chain for live analysis
     */
    private List<ProviderAttempt> buildProviderChain(String prompt, String query, String window) {
        List<ProviderAttempt> chain = new ArrayList<>();

        // 1. Perplexity - Best for news analysis with online search
        if (perplexityClient.isEnabled()) {
            chain.add(new ProviderAttempt("Perplexity", () -> perplexityClient.streamCompletion(prompt)));
        }

        // 2. OpenAI
        if (openAICompatibleClient.isOpenAIEnabled()) {
            chain.add(new ProviderAttempt("OpenAI", () -> openAICompatibleClient.streamFromOpenAI(prompt)));
        }

        // 3. OpenRouter
        if (openAICompatibleClient.isOpenRouterEnabled()) {
            chain.add(new ProviderAttempt("OpenRouter", () -> openAICompatibleClient.streamFromOpenRouter(prompt)));
        }

        // 4. Azure OpenAI
        if (openAICompatibleClient.isAzureEnabled()) {
            chain.add(new ProviderAttempt("Azure", () -> openAICompatibleClient.streamFromAzure(prompt)));
        }

        // 5. AI Dove
        if (aiDoveClient.isEnabled()) {
            chain.add(new ProviderAttempt("AI Dove", () -> aiDoveClient.chatStream(prompt, null)));
        }

        // 6. CrawlSearchService (Crawl4AI + AI Dove)
        if (crawlSearchService.isAvailable()) {
            chain.add(new ProviderAttempt("Crawl+AIDove", () -> crawlSearchService.searchAndAnalyze(query, window)));
        }

        // 7. Ollama - Local LLM
        chain.add(new ProviderAttempt("Ollama", () -> openAICompatibleClient.streamFromOllama(prompt)));

        // 8. Custom endpoint
        if (openAICompatibleClient.isCustomEnabled()) {
            chain.add(new ProviderAttempt("Custom", () -> openAICompatibleClient.streamFromCustom(prompt)));
        }

        return chain;
    }

    /**
     * Try providers in sequence until one succeeds
     */
    private Flux<String> tryProvidersInSequence(List<ProviderAttempt> providers, int index) {
        if (index >= providers.size()) {
            log.error("All AI providers failed for live analysis");
            return Flux.just("모든 AI 제공자 연결에 실패했습니다. 나중에 다시 시도해주세요.");
        }

        ProviderAttempt current = providers.get(index);
        log.info("Trying AI provider: {} ({}/{})", current.name(), index + 1, providers.size());

        return current.streamSupplier().get()
                .timeout(Duration.ofSeconds(90))
                .onErrorResume(e -> {
                    log.warn("AI provider {} failed: {}. Trying next...", current.name(), e.getMessage());
                    return tryProvidersInSequence(providers, index + 1);
                })
                .switchIfEmpty(Flux.defer(() -> {
                    log.warn("AI provider {} returned empty. Trying next...", current.name());
                    return tryProvidersInSequence(providers, index + 1);
                }));
    }

    private String buildPrompt(String query, String window) {
        String normalizedQuery = (query == null || query.isBlank()) ? "지정된 키워드 없음" : query;

        String windowDescription;
        if ("1d".equals(window)) {
            windowDescription = "최근 1일";
        } else if ("30d".equals(window)) {
            windowDescription = "최근 30일";
        } else {
            windowDescription = "최근 7일";
        }

        return "다음 키워드 '" + normalizedQuery + "' 에 대해 " + windowDescription +
                " 동안의 주요 뉴스 흐름과 핵심 인사이트를 한국어로 자세히 요약해 주세요. " +
                "가능하면 bullet 형식으로 정리하고, 마지막에 전반적인 의미를 한 문단으로 정리해 주세요.";
    }

    /**
     * Provider attempt wrapper
     */
    private record ProviderAttempt(
            String name,
            Supplier<Flux<String>> streamSupplier
    ) {}
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/LlmProviderSettingsController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.dto.llm.LlmProviderSettingsDto;
import com.newsinsight.collector.dto.llm.LlmProviderSettingsRequest;
import com.newsinsight.collector.dto.llm.LlmTestResult;
import com.newsinsight.collector.entity.settings.LlmProviderType;
import com.newsinsight.collector.service.LlmProviderSettingsService;
import jakarta.validation.Valid;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.Arrays;
import java.util.List;
import java.util.Map;
import java.util.stream.Collectors;

/**
 * LLM Provider 설정 API 컨트롤러.
 * 
 * 관리자(전역) 설정과 사용자별 설정을 분리하여 관리.
 * - /api/v1/admin/llm-providers: 관리자 전역 설정
 * - /api/v1/llm-providers: 사용자별 설정
 */
@RestController
@RequestMapping("/api/v1")
@RequiredArgsConstructor
@Slf4j
public class LlmProviderSettingsController {

    private final LlmProviderSettingsService settingsService;

    // ========== 공통: Provider 타입 목록 ==========

    /**
     * 지원하는 LLM Provider 타입 목록
     */
    @GetMapping("/llm-providers/types")
    public ResponseEntity<List<Map<String, String>>> getProviderTypes() {
        List<Map<String, String>> types = Arrays.stream(LlmProviderType.values())
                .map(type -> Map.of(
                        "value", type.name(),
                        "displayName", type.getDisplayName(),
                        "defaultBaseUrl", type.getDefaultBaseUrl() != null ? type.getDefaultBaseUrl() : ""
                ))
                .collect(Collectors.toList());
        return ResponseEntity.ok(types);
    }

    // ========== 관리자 전역 설정 API ==========

    /**
     * 모든 전역 설정 조회
     */
    @GetMapping("/admin/llm-providers")
    public ResponseEntity<List<LlmProviderSettingsDto>> getAllGlobalSettings() {
        return ResponseEntity.ok(settingsService.getAllGlobalSettings());
    }

    /**
     * 특정 Provider의 전역 설정 조회
     */
    @GetMapping("/admin/llm-providers/{providerType}")
    public ResponseEntity<LlmProviderSettingsDto> getGlobalSetting(@PathVariable LlmProviderType providerType) {
        return settingsService.getGlobalSetting(providerType)
                .map(ResponseEntity::ok)
                .orElse(ResponseEntity.notFound().build());
    }

    /**
     * 전역 설정 생성/업데이트
     */
    @PutMapping("/admin/llm-providers")
    public ResponseEntity<LlmProviderSettingsDto> saveGlobalSetting(
            @Valid @RequestBody LlmProviderSettingsRequest request
    ) {
        LlmProviderSettingsDto saved = settingsService.saveGlobalSetting(request);
        return ResponseEntity.ok(saved);
    }

    /**
     * 전역 설정 삭제
     */
    @DeleteMapping("/admin/llm-providers/{providerType}")
    public ResponseEntity<Map<String, String>> deleteGlobalSetting(@PathVariable LlmProviderType providerType) {
        settingsService.deleteGlobalSetting(providerType);
        return ResponseEntity.ok(Map.of(
                "status", "deleted",
                "provider", providerType.name()
        ));
    }

    /**
     * 전역 설정 연결 테스트
     */
    @PostMapping("/admin/llm-providers/{id}/test")
    public ResponseEntity<LlmTestResult> testGlobalConnection(@PathVariable Long id) {
        LlmTestResult result = settingsService.testConnection(id);
        return ResponseEntity.ok(result);
    }

    /**
     * 전역 설정 활성화/비활성화
     */
    @PostMapping("/admin/llm-providers/{id}/toggle")
    public ResponseEntity<Map<String, Object>> toggleGlobalSetting(
            @PathVariable Long id,
            @RequestParam boolean enabled
    ) {
        settingsService.setEnabled(id, enabled);
        return ResponseEntity.ok(Map.of(
                "id", id,
                "enabled", enabled
        ));
    }

    // ========== 사용자별 설정 API ==========

    /**
     * 사용자의 유효 설정 조회 (사용자 설정 > 전역 설정)
     */
    @GetMapping("/llm-providers/effective")
    public ResponseEntity<List<LlmProviderSettingsDto>> getEffectiveSettings(
            @RequestHeader(value = "X-User-Id", required = false) String userId
    ) {
        return ResponseEntity.ok(settingsService.getEffectiveSettings(userId));
    }

    /**
     * 사용자의 활성화된 Provider 목록 (Fallback 체인용)
     */
    @GetMapping("/llm-providers/enabled")
    public ResponseEntity<List<LlmProviderSettingsDto>> getEnabledProviders(
            @RequestHeader(value = "X-User-Id", required = false) String userId
    ) {
        return ResponseEntity.ok(settingsService.getEnabledProviders(userId));
    }

    /**
     * 특정 Provider의 유효 설정 조회
     */
    @GetMapping("/llm-providers/config/{providerType}")
    public ResponseEntity<LlmProviderSettingsDto> getEffectiveSetting(
            @PathVariable LlmProviderType providerType,
            @RequestHeader(value = "X-User-Id", required = false) String userId
    ) {
        return settingsService.getEffectiveSetting(userId, providerType)
                .map(ResponseEntity::ok)
                .orElse(ResponseEntity.notFound().build());
    }

    /**
     * 사용자의 개인 설정만 조회
     */
    @GetMapping("/llm-providers/user")
    public ResponseEntity<List<LlmProviderSettingsDto>> getUserSettings(
            @RequestHeader("X-User-Id") String userId
    ) {
        return ResponseEntity.ok(settingsService.getUserSettings(userId));
    }

    /**
     * 사용자 설정 생성/업데이트
     */
    @PutMapping("/llm-providers/user")
    public ResponseEntity<LlmProviderSettingsDto> saveUserSetting(
            @RequestHeader("X-User-Id") String userId,
            @Valid @RequestBody LlmProviderSettingsRequest request
    ) {
        LlmProviderSettingsDto saved = settingsService.saveUserSetting(userId, request);
        return ResponseEntity.ok(saved);
    }

    /**
     * 사용자 설정 삭제 (전역 설정으로 폴백)
     */
    @DeleteMapping("/llm-providers/user/{providerType}")
    public ResponseEntity<Map<String, String>> deleteUserSetting(
            @RequestHeader("X-User-Id") String userId,
            @PathVariable LlmProviderType providerType
    ) {
        settingsService.deleteUserSetting(userId, providerType);
        return ResponseEntity.ok(Map.of(
                "status", "deleted",
                "provider", providerType.name(),
                "message", "Falling back to global settings"
        ));
    }

    /**
     * 사용자의 모든 개인 설정 삭제
     */
    @DeleteMapping("/llm-providers/user")
    public ResponseEntity<Map<String, String>> deleteAllUserSettings(
            @RequestHeader("X-User-Id") String userId
    ) {
        settingsService.deleteAllUserSettings(userId);
        return ResponseEntity.ok(Map.of(
                "status", "deleted",
                "message", "All user settings deleted, falling back to global settings"
        ));
    }

    /**
     * 사용자 설정 연결 테스트
     */
    @PostMapping("/llm-providers/user/{id}/test")
    public ResponseEntity<LlmTestResult> testUserConnection(@PathVariable Long id) {
        LlmTestResult result = settingsService.testConnection(id);
        return ResponseEntity.ok(result);
    }

    /**
     * 새 설정으로 연결 테스트 (저장 전)
     */
    @PostMapping("/llm-providers/test")
    public ResponseEntity<LlmTestResult> testNewConnection(
            @Valid @RequestBody LlmProviderSettingsRequest request
    ) {
        LlmTestResult result = settingsService.testConnection(request);
        return ResponseEntity.ok(result);
    }

    // ========== 예외 처리 ==========

    @ExceptionHandler(IllegalArgumentException.class)
    public ResponseEntity<Map<String, String>> handleIllegalArgument(IllegalArgumentException e) {
        return ResponseEntity.badRequest().body(Map.of("error", e.getMessage()));
    }

    @ExceptionHandler(Exception.class)
    public ResponseEntity<Map<String, String>> handleException(Exception e) {
        log.error("Unexpected error in LlmProviderSettingsController", e);
        return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)
                .body(Map.of("error", "Internal server error: " + e.getMessage()));
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/MlAddonController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.dto.addon.AddonResponse;
import com.newsinsight.collector.entity.addon.*;
import com.newsinsight.collector.repository.MlAddonExecutionRepository;
import com.newsinsight.collector.repository.MlAddonRepository;
import com.newsinsight.collector.service.AddonOrchestratorService;
import jakarta.validation.Valid;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.PageRequest;
import org.springframework.data.domain.Sort;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Map;
import java.util.concurrent.CompletableFuture;

/**
 * ML Add-on 관리 및 분석 실행 API.
 */
@RestController
@RequestMapping("/api/v1/ml")
@RequiredArgsConstructor
@Slf4j
public class MlAddonController {

    private final MlAddonRepository addonRepository;
    private final MlAddonExecutionRepository executionRepository;
    private final AddonOrchestratorService orchestratorService;

    // ========== Add-on Registry 관리 ==========

    /**
     * 모든 Add-on 목록 조회
     */
    @GetMapping("/addons")
    public ResponseEntity<List<MlAddon>> listAddons(
            @RequestParam(required = false) AddonCategory category,
            @RequestParam(required = false) Boolean enabled
    ) {
        List<MlAddon> addons;
        if (category != null && enabled != null && enabled) {
            addons = addonRepository.findByCategoryAndEnabledTrue(category);
        } else if (category != null) {
            addons = addonRepository.findByCategory(category);
        } else if (enabled != null && enabled) {
            addons = addonRepository.findByEnabledTrue();
        } else {
            addons = addonRepository.findAll();
        }
        return ResponseEntity.ok(addons);
    }

    /**
     * 특정 Add-on 조회
     */
    @GetMapping("/addons/{addonKey}")
    public ResponseEntity<MlAddon> getAddon(@PathVariable String addonKey) {
        return addonRepository.findByAddonKey(addonKey)
                .map(ResponseEntity::ok)
                .orElse(ResponseEntity.notFound().build());
    }

    /**
     * Add-on 등록
     */
    @PostMapping("/addons")
    public ResponseEntity<?> createAddon(@Valid @RequestBody MlAddon addon) {
        if (addonRepository.existsByAddonKey(addon.getAddonKey())) {
            return ResponseEntity.badRequest()
                    .body(Map.of("error", "Addon key already exists: " + addon.getAddonKey()));
        }

        MlAddon saved = addonRepository.save(addon);
        log.info("Created new addon: {}", addon.getAddonKey());
        return ResponseEntity.status(HttpStatus.CREATED).body(saved);
    }

    /**
     * Add-on 수정
     */
    @PutMapping("/addons/{addonKey}")
    public ResponseEntity<?> updateAddon(
            @PathVariable String addonKey,
            @RequestBody MlAddon updates
    ) {
        return addonRepository.findByAddonKey(addonKey)
                .map(existing -> {
                    // 수정 가능한 필드만 업데이트
                    if (updates.getName() != null) existing.setName(updates.getName());
                    if (updates.getDescription() != null) existing.setDescription(updates.getDescription());
                    if (updates.getEndpointUrl() != null) existing.setEndpointUrl(updates.getEndpointUrl());
                    if (updates.getTimeoutMs() != null) existing.setTimeoutMs(updates.getTimeoutMs());
                    if (updates.getMaxQps() != null) existing.setMaxQps(updates.getMaxQps());
                    if (updates.getMaxRetries() != null) existing.setMaxRetries(updates.getMaxRetries());
                    if (updates.getEnabled() != null) existing.setEnabled(updates.getEnabled());
                    if (updates.getPriority() != null) existing.setPriority(updates.getPriority());
                    if (updates.getConfig() != null) existing.setConfig(updates.getConfig());
                    if (updates.getDependsOn() != null) existing.setDependsOn(updates.getDependsOn());
                    if (updates.getAuthType() != null) existing.setAuthType(updates.getAuthType());
                    if (updates.getAuthCredentials() != null) existing.setAuthCredentials(updates.getAuthCredentials());
                    if (updates.getHealthCheckUrl() != null) existing.setHealthCheckUrl(updates.getHealthCheckUrl());

                    MlAddon saved = addonRepository.save(existing);
                    log.info("Updated addon: {}", addonKey);
                    return ResponseEntity.ok(saved);
                })
                .orElse(ResponseEntity.notFound().build());
    }

    /**
     * Add-on 활성화/비활성화
     */
    @PostMapping("/addons/{addonKey}/toggle")
    public ResponseEntity<?> toggleAddon(@PathVariable String addonKey) {
        return addonRepository.findByAddonKey(addonKey)
                .map(addon -> {
                    addon.setEnabled(!addon.getEnabled());
                    MlAddon saved = addonRepository.save(addon);
                    log.info("Toggled addon {}: enabled={}", addonKey, saved.getEnabled());
                    return ResponseEntity.ok(Map.of(
                            "addonKey", addonKey,
                            "enabled", saved.getEnabled()
                    ));
                })
                .orElse(ResponseEntity.notFound().build());
    }

    /**
     * Add-on 삭제
     */
    @DeleteMapping("/addons/{addonKey}")
    public ResponseEntity<?> deleteAddon(@PathVariable String addonKey) {
        return addonRepository.findByAddonKey(addonKey)
                .map(addon -> {
                    addonRepository.delete(addon);
                    log.info("Deleted addon: {}", addonKey);
                    return ResponseEntity.ok(Map.of("deleted", addonKey));
                })
                .orElse(ResponseEntity.notFound().build());
    }

    // ========== 분석 실행 ==========

    /**
     * 특정 Add-on으로 직접 분석 실행 (커스텀 입력)
     * POST /api/v1/ml/addons/{addonKey}/analyze
     * 
     * 프론트엔드에서 직접 특정 Add-on을 호출하여 분석을 실행할 때 사용.
     * 기사 ID 없이 커스텀 데이터로 분석 가능.
     */
    @PostMapping("/addons/{addonKey}/analyze")
    public ResponseEntity<?> analyzeWithAddon(
            @PathVariable String addonKey,
            @RequestBody Map<String, Object> request
    ) {
        return addonRepository.findByAddonKey(addonKey)
                .map(addon -> {
                    if (!addon.getEnabled()) {
                        return ResponseEntity.badRequest()
                                .body(Map.of("error", "Addon is disabled: " + addonKey));
                    }
                    
                    try {
                        // 요청에서 article 정보 추출
                        @SuppressWarnings("unchecked")
                        Map<String, Object> articleData = (Map<String, Object>) request.getOrDefault("article", Map.of());
                        
                        String requestId = java.util.UUID.randomUUID().toString();
                        String importance = (String) request.getOrDefault("importance", "batch");
                        
                        // Add-on 직접 호출
                        AddonResponse response = orchestratorService.executeAddonDirect(addon, articleData, requestId, importance);
                        
                        if (response == null) {
                            return ResponseEntity.status(HttpStatus.SERVICE_UNAVAILABLE)
                                    .body(Map.of("error", "Addon did not return a response"));
                        }
                        
                        return ResponseEntity.ok(response);
                    } catch (Exception e) {
                        log.error("Failed to execute addon {}: {}", addonKey, e.getMessage(), e);
                        return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)
                                .body(Map.of(
                                        "error", "Addon execution failed",
                                        "message", e.getMessage()
                                ));
                    }
                })
                .orElse(ResponseEntity.notFound().build());
    }

    /**
     * 단일 기사 분석 실행
     */
    @PostMapping("/analyze/{articleId}")
    public ResponseEntity<?> analyzeArticle(
            @PathVariable Long articleId,
            @RequestParam(defaultValue = "batch") String importance
    ) {
        try {
            CompletableFuture<String> future = orchestratorService.analyzeArticle(articleId, importance);
            String batchId = future.get();
            return ResponseEntity.accepted().body(Map.of(
                    "status", "accepted",
                    "articleId", articleId,
                    "batchId", batchId
            ));
        } catch (Exception e) {
            log.error("Failed to start analysis for article: {}", articleId, e);
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)
                    .body(Map.of("error", e.getMessage()));
        }
    }

    /**
     * 여러 기사 일괄 분석
     */
    @PostMapping("/analyze/batch")
    public ResponseEntity<?> analyzeArticles(
            @RequestBody List<Long> articleIds,
            @RequestParam(defaultValue = "batch") String importance
    ) {
        CompletableFuture<String> future = orchestratorService.analyzeArticles(articleIds, importance);
        return ResponseEntity.accepted().body(Map.of(
                "status", "accepted",
                "articleCount", articleIds.size(),
                "batchId", future.join()
        ));
    }

    /**
     * 특정 카테고리 Add-on만 실행
     */
    @PostMapping("/analyze/{articleId}/category/{category}")
    public ResponseEntity<?> analyzeByCategory(
            @PathVariable Long articleId,
            @PathVariable AddonCategory category
    ) {
        try {
            CompletableFuture<AddonResponse> future = orchestratorService.executeCategory(articleId, category);
            AddonResponse response = future.get();
            return ResponseEntity.ok(response);
        } catch (Exception e) {
            log.error("Failed to analyze article {} with category {}", articleId, category, e);
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)
                    .body(Map.of("error", e.getMessage()));
        }
    }

    // ========== 실행 이력 ==========

    /**
     * 실행 이력 조회 (status 필터 지원)
     */
    @GetMapping("/executions")
    public ResponseEntity<Page<MlAddonExecution>> listExecutions(
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size,
            @RequestParam(required = false) ExecutionStatus status
    ) {
        PageRequest pageRequest = PageRequest.of(page, size, Sort.by(Sort.Direction.DESC, "createdAt"));
        Page<MlAddonExecution> executions;
        if (status != null) {
            executions = executionRepository.findByStatus(status, pageRequest);
        } else {
            executions = executionRepository.findAll(pageRequest);
        }
        return ResponseEntity.ok(executions);
    }

    /**
     * 특정 기사의 실행 이력
     */
    @GetMapping("/executions/article/{articleId}")
    public ResponseEntity<List<MlAddonExecution>> getArticleExecutions(@PathVariable Long articleId) {
        return ResponseEntity.ok(executionRepository.findByArticleId(articleId));
    }

    // ========== 모니터링 ==========

    /**
     * Add-on 상태 요약
     * 프론트엔드 MlAddonStatusSummary 형식에 맞춰 반환
     */
    @GetMapping("/status")
    public ResponseEntity<?> getStatus() {
        List<MlAddon> allAddons = addonRepository.findAll();
        long enabled = allAddons.stream().filter(MlAddon::getEnabled).count();
        long healthy = allAddons.stream()
                .filter(a -> a.getHealthStatus() == AddonHealthStatus.HEALTHY)
                .count();
        long unhealthy = allAddons.stream()
                .filter(a -> a.getHealthStatus() != AddonHealthStatus.HEALTHY && a.getHealthStatus() != AddonHealthStatus.UNKNOWN)
                .count();

        // 오늘의 실행 통계 계산
        LocalDateTime todayStart = LocalDateTime.now().toLocalDate().atStartOfDay();
        List<MlAddonExecution> todayExecutions = executionRepository.findByCreatedAtAfter(todayStart);
        long totalExecutionsToday = todayExecutions.size();
        long successCount = todayExecutions.stream()
                .filter(e -> e.getStatus() == ExecutionStatus.SUCCESS)
                .count();
        double successRate = totalExecutionsToday > 0 
                ? (double) successCount / totalExecutionsToday * 100 
                : 0.0;
        
        // 평균 지연시간 계산
        double avgLatencyMs = todayExecutions.stream()
                .filter(e -> e.getLatencyMs() != null)
                .mapToLong(MlAddonExecution::getLatencyMs)
                .average()
                .orElse(0.0);
        
        // 카테고리별 addon 수
        Map<String, Long> byCategory = allAddons.stream()
                .collect(java.util.stream.Collectors.groupingBy(
                        a -> a.getCategory().name(),
                        java.util.stream.Collectors.counting()
                ));
        
        return ResponseEntity.ok(Map.of(
                "totalAddons", allAddons.size(),
                "enabledAddons", enabled,
                "healthyAddons", healthy,
                "unhealthyAddons", unhealthy,
                "totalExecutionsToday", totalExecutionsToday,
                "successRate", Math.round(successRate * 100.0) / 100.0,
                "avgLatencyMs", Math.round(avgLatencyMs * 100.0) / 100.0,
                "byCategory", byCategory
        ));
    }

    /**
     * 헬스체크 수동 실행
     */
    @PostMapping("/health-check")
    public ResponseEntity<?> runHealthCheck() {
        orchestratorService.runHealthChecks();
        return ResponseEntity.ok(Map.of("status", "Health check started"));
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/ProjectController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.dto.PageResponse;
import com.newsinsight.collector.entity.project.*;
import com.newsinsight.collector.service.ProjectService;
import com.newsinsight.collector.service.ProjectService.*;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.data.domain.Page;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.List;
import java.util.Map;

/**
 * REST Controller for Project API.
 * Provides endpoints for project CRUD, members, items, and activities.
 */
@RestController
@RequestMapping("/api/v1/projects")
@RequiredArgsConstructor
@Slf4j
public class ProjectController {

    private final ProjectService projectService;

    // ============================================
    // Project CRUD
    // ============================================

    /**
     * Create a new project.
     */
    @PostMapping
    public ResponseEntity<Project> createProject(@RequestBody CreateProjectRequest request) {
        log.info("Creating project: name='{}', owner={}", request.getName(), request.getOwnerId());

        if (request.getName() == null || request.getName().isBlank()) {
            return ResponseEntity.badRequest().build();
        }
        if (request.getOwnerId() == null || request.getOwnerId().isBlank()) {
            return ResponseEntity.badRequest().build();
        }

        Project project = projectService.createProject(request);
        return ResponseEntity.status(HttpStatus.CREATED).body(project);
    }

    /**
     * Get project by ID.
     */
    @GetMapping("/{id}")
    public ResponseEntity<Project> getProject(
            @PathVariable Long id,
            @RequestParam(required = false) String userId
    ) {
        if (userId != null) {
            return projectService.getProjectWithAccess(id, userId)
                    .map(ResponseEntity::ok)
                    .orElse(ResponseEntity.notFound().build());
        }
        return projectService.getProject(id)
                .map(ResponseEntity::ok)
                .orElse(ResponseEntity.notFound().build());
    }

    /**
     * Update project.
     */
    @PutMapping("/{id}")
    public ResponseEntity<Project> updateProject(
            @PathVariable Long id,
            @RequestBody UpdateProjectRequest request,
            @RequestParam String userId
    ) {
        try {
            Project updated = projectService.updateProject(id, request, userId);
            return ResponseEntity.ok(updated);
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        } catch (IllegalStateException e) {
            return ResponseEntity.status(HttpStatus.FORBIDDEN).build();
        }
    }

    /**
     * Update project status.
     */
    @PutMapping("/{id}/status")
    public ResponseEntity<Project> updateProjectStatus(
            @PathVariable Long id,
            @RequestBody Map<String, String> body,
            @RequestParam String userId
    ) {
        String statusStr = body.get("status");
        if (statusStr == null) {
            return ResponseEntity.badRequest().build();
        }

        try {
            Project.ProjectStatus status = Project.ProjectStatus.valueOf(statusStr.toUpperCase());
            Project updated = projectService.updateProjectStatus(id, status, userId);
            return ResponseEntity.ok(updated);
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        } catch (IllegalStateException e) {
            return ResponseEntity.status(HttpStatus.FORBIDDEN).build();
        }
    }

    /**
     * Delete project.
     */
    @DeleteMapping("/{id}")
    public ResponseEntity<Void> deleteProject(
            @PathVariable Long id,
            @RequestParam String userId
    ) {
        try {
            projectService.deleteProject(id, userId);
            return ResponseEntity.noContent().build();
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        } catch (IllegalStateException e) {
            return ResponseEntity.status(HttpStatus.FORBIDDEN).build();
        }
    }

    /**
     * Get projects by owner.
     */
    @GetMapping
    public ResponseEntity<PageResponse<Project>> getProjects(
            @RequestParam String ownerId,
            @RequestParam(required = false) String status,
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size
    ) {
        Page<Project> result;

        if (status != null) {
            Project.ProjectStatus projectStatus = Project.ProjectStatus.valueOf(status.toUpperCase());
            result = projectService.getProjectsByOwnerAndStatus(ownerId, projectStatus, page, size);
        } else {
            result = projectService.getProjectsByOwner(ownerId, page, size);
        }

        PageResponse<Project> response = new PageResponse<>(
                result.getContent(),
                result.getNumber(),
                result.getSize(),
                result.getTotalElements(),
                result.getTotalPages(),
                result.isFirst(),
                result.isLast(),
                result.hasNext(),
                result.hasPrevious()
        );

        return ResponseEntity.ok(response);
    }

    /**
     * Search projects.
     */
    @GetMapping("/search")
    public ResponseEntity<PageResponse<Project>> searchProjects(
            @RequestParam String q,
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size
    ) {
        Page<Project> result = projectService.searchProjects(q, page, size);

        PageResponse<Project> response = new PageResponse<>(
                result.getContent(),
                result.getNumber(),
                result.getSize(),
                result.getTotalElements(),
                result.getTotalPages(),
                result.isFirst(),
                result.isLast(),
                result.hasNext(),
                result.hasPrevious()
        );

        return ResponseEntity.ok(response);
    }

    /**
     * Get or create default project.
     */
    @GetMapping("/default")
    public ResponseEntity<Project> getDefaultProject(@RequestParam String userId) {
        Project project = projectService.getOrCreateDefaultProject(userId);
        return ResponseEntity.ok(project);
    }

    /**
     * Get project statistics.
     */
    @GetMapping("/{id}/stats")
    public ResponseEntity<Map<String, Object>> getProjectStats(@PathVariable Long id) {
        try {
            Map<String, Object> stats = projectService.getProjectStats(id);
            return ResponseEntity.ok(stats);
        } catch (Exception e) {
            return ResponseEntity.notFound().build();
        }
    }

    // ============================================
    // Member Management
    // ============================================

    /**
     * Get project members.
     */
    @GetMapping("/{id}/members")
    public ResponseEntity<List<ProjectMember>> getMembers(@PathVariable Long id) {
        List<ProjectMember> members = projectService.getMembers(id);
        return ResponseEntity.ok(members);
    }

    /**
     * Get active members.
     */
    @GetMapping("/{id}/members/active")
    public ResponseEntity<List<ProjectMember>> getActiveMembers(@PathVariable Long id) {
        List<ProjectMember> members = projectService.getActiveMembers(id);
        return ResponseEntity.ok(members);
    }

    /**
     * Invite member.
     */
    @PostMapping("/{id}/members/invite")
    public ResponseEntity<ProjectMember> inviteMember(
            @PathVariable Long id,
            @RequestBody Map<String, String> body,
            @RequestParam String invitedBy
    ) {
        String userId = body.get("userId");
        String roleStr = body.get("role");

        if (userId == null || userId.isBlank()) {
            return ResponseEntity.badRequest().build();
        }

        ProjectMember.MemberRole role = roleStr != null 
                ? ProjectMember.MemberRole.valueOf(roleStr.toUpperCase())
                : ProjectMember.MemberRole.VIEWER;

        try {
            ProjectMember member = projectService.inviteMember(id, userId, role, invitedBy);
            return ResponseEntity.status(HttpStatus.CREATED).body(member);
        } catch (IllegalStateException e) {
            return ResponseEntity.status(HttpStatus.CONFLICT).build();
        }
    }

    /**
     * Accept invitation.
     */
    @PostMapping("/invitations/{token}/accept")
    public ResponseEntity<ProjectMember> acceptInvitation(
            @PathVariable String token,
            @RequestParam String userId
    ) {
        try {
            ProjectMember member = projectService.acceptInvitation(token, userId);
            return ResponseEntity.ok(member);
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        } catch (IllegalStateException e) {
            return ResponseEntity.status(HttpStatus.FORBIDDEN).build();
        }
    }

    /**
     * Remove member.
     */
    @DeleteMapping("/{id}/members/{userId}")
    public ResponseEntity<Void> removeMember(
            @PathVariable Long id,
            @PathVariable String userId,
            @RequestParam String removedBy
    ) {
        try {
            projectService.removeMember(id, userId, removedBy);
            return ResponseEntity.noContent().build();
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        } catch (IllegalStateException e) {
            return ResponseEntity.status(HttpStatus.FORBIDDEN).build();
        }
    }

    /**
     * Update member role.
     */
    @PutMapping("/{id}/members/{userId}/role")
    public ResponseEntity<ProjectMember> updateMemberRole(
            @PathVariable Long id,
            @PathVariable String userId,
            @RequestBody Map<String, String> body,
            @RequestParam String updatedBy
    ) {
        String roleStr = body.get("role");
        if (roleStr == null) {
            return ResponseEntity.badRequest().build();
        }

        try {
            ProjectMember.MemberRole role = ProjectMember.MemberRole.valueOf(roleStr.toUpperCase());
            ProjectMember member = projectService.updateMemberRole(id, userId, role, updatedBy);
            return ResponseEntity.ok(member);
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        } catch (IllegalStateException e) {
            return ResponseEntity.status(HttpStatus.FORBIDDEN).build();
        }
    }

    // ============================================
    // Item Management
    // ============================================

    /**
     * Add item to project.
     */
    @PostMapping("/{id}/items")
    public ResponseEntity<ProjectItem> addItem(
            @PathVariable Long id,
            @RequestBody AddItemRequest request,
            @RequestParam String userId
    ) {
        try {
            ProjectItem item = projectService.addItem(id, request, userId);
            return ResponseEntity.status(HttpStatus.CREATED).body(item);
        } catch (IllegalStateException e) {
            return ResponseEntity.status(HttpStatus.FORBIDDEN).build();
        }
    }

    /**
     * Get project items.
     */
    @GetMapping("/{id}/items")
    public ResponseEntity<PageResponse<ProjectItem>> getItems(
            @PathVariable Long id,
            @RequestParam(required = false) String type,
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size
    ) {
        Page<ProjectItem> result;

        if (type != null) {
            ProjectItem.ItemType itemType = ProjectItem.ItemType.valueOf(type.toUpperCase());
            result = projectService.getItemsByType(id, itemType, page, size);
        } else {
            result = projectService.getItems(id, page, size);
        }

        PageResponse<ProjectItem> response = new PageResponse<>(
                result.getContent(),
                result.getNumber(),
                result.getSize(),
                result.getTotalElements(),
                result.getTotalPages(),
                result.isFirst(),
                result.isLast(),
                result.hasNext(),
                result.hasPrevious()
        );

        return ResponseEntity.ok(response);
    }

    /**
     * Search items.
     */
    @GetMapping("/{id}/items/search")
    public ResponseEntity<PageResponse<ProjectItem>> searchItems(
            @PathVariable Long id,
            @RequestParam String q,
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size
    ) {
        Page<ProjectItem> result = projectService.searchItems(id, q, page, size);

        PageResponse<ProjectItem> response = new PageResponse<>(
                result.getContent(),
                result.getNumber(),
                result.getSize(),
                result.getTotalElements(),
                result.getTotalPages(),
                result.isFirst(),
                result.isLast(),
                result.hasNext(),
                result.hasPrevious()
        );

        return ResponseEntity.ok(response);
    }

    /**
     * Mark item as read.
     */
    @PostMapping("/{projectId}/items/{itemId}/read")
    public ResponseEntity<Void> markItemAsRead(
            @PathVariable Long projectId,
            @PathVariable Long itemId,
            @RequestParam String userId
    ) {
        projectService.markItemAsRead(itemId, userId);
        return ResponseEntity.ok().build();
    }

    /**
     * Toggle item bookmark.
     */
    @PostMapping("/{projectId}/items/{itemId}/bookmark")
    public ResponseEntity<Void> toggleItemBookmark(
            @PathVariable Long projectId,
            @PathVariable Long itemId,
            @RequestParam String userId
    ) {
        projectService.toggleItemBookmark(itemId, userId);
        return ResponseEntity.ok().build();
    }

    /**
     * Delete item.
     */
    @DeleteMapping("/{projectId}/items/{itemId}")
    public ResponseEntity<Void> deleteItem(
            @PathVariable Long projectId,
            @PathVariable Long itemId,
            @RequestParam String userId
    ) {
        try {
            projectService.deleteItem(projectId, itemId, userId);
            return ResponseEntity.noContent().build();
        } catch (IllegalStateException e) {
            return ResponseEntity.status(HttpStatus.FORBIDDEN).build();
        }
    }

    // ============================================
    // Activity Log
    // ============================================

    /**
     * Get project activity log.
     */
    @GetMapping("/{id}/activities")
    public ResponseEntity<PageResponse<ProjectActivityLog>> getActivityLog(
            @PathVariable Long id,
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size
    ) {
        Page<ProjectActivityLog> result = projectService.getActivityLog(id, page, size);

        PageResponse<ProjectActivityLog> response = new PageResponse<>(
                result.getContent(),
                result.getNumber(),
                result.getSize(),
                result.getTotalElements(),
                result.getTotalPages(),
                result.isFirst(),
                result.isLast(),
                result.hasNext(),
                result.hasPrevious()
        );

        return ResponseEntity.ok(response);
    }

    /**
     * Get recent activity.
     */
    @GetMapping("/{id}/activities/recent")
    public ResponseEntity<List<ProjectActivityLog>> getRecentActivity(@PathVariable Long id) {
        List<ProjectActivityLog> activities = projectService.getRecentActivity(id);
        return ResponseEntity.ok(activities);
    }

    // ============================================
    // Notifications
    // ============================================

    /**
     * Get user notifications.
     */
    @GetMapping("/notifications")
    public ResponseEntity<PageResponse<ProjectNotification>> getUserNotifications(
            @RequestParam String userId,
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size
    ) {
        Page<ProjectNotification> result = projectService.getUserNotifications(userId, page, size);

        PageResponse<ProjectNotification> response = new PageResponse<>(
                result.getContent(),
                result.getNumber(),
                result.getSize(),
                result.getTotalElements(),
                result.getTotalPages(),
                result.isFirst(),
                result.isLast(),
                result.hasNext(),
                result.hasPrevious()
        );

        return ResponseEntity.ok(response);
    }

    /**
     * Get unread notifications.
     */
    @GetMapping("/notifications/unread")
    public ResponseEntity<List<ProjectNotification>> getUnreadNotifications(@RequestParam String userId) {
        List<ProjectNotification> notifications = projectService.getUnreadNotifications(userId);
        return ResponseEntity.ok(notifications);
    }

    /**
     * Mark notification as read.
     */
    @PostMapping("/notifications/{notificationId}/read")
    public ResponseEntity<Void> markNotificationAsRead(@PathVariable Long notificationId) {
        projectService.markNotificationAsRead(notificationId);
        return ResponseEntity.ok().build();
    }

    /**
     * Mark all notifications as read.
     */
    @PostMapping("/notifications/read-all")
    public ResponseEntity<Void> markAllNotificationsAsRead(@RequestParam String userId) {
        projectService.markAllNotificationsAsRead(userId);
        return ResponseEntity.ok().build();
    }

    // ============================================
    // Health
    // ============================================

    /**
     * Health check.
     */
    @GetMapping("/health")
    public ResponseEntity<Map<String, Object>> healthCheck() {
        return ResponseEntity.ok(Map.of(
                "status", "available",
                "features", Map.of(
                        "projects", true,
                        "members", true,
                        "items", true,
                        "activities", true,
                        "notifications", true
                )
        ));
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/ReportController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.dto.report.ReportMetadata;
import com.newsinsight.collector.dto.report.ReportRequest;
import com.newsinsight.collector.service.report.ReportGenerationService;
import io.swagger.v3.oas.annotations.Operation;
import io.swagger.v3.oas.annotations.tags.Tag;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.http.HttpHeaders;
import org.springframework.http.HttpStatus;
import org.springframework.http.MediaType;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.io.IOException;
import java.net.URLEncoder;
import java.nio.charset.StandardCharsets;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;

/**
 * 보고서 생성 및 다운로드 REST API 컨트롤러
 */
@RestController
@RequestMapping("/api/v1/reports")
@RequiredArgsConstructor
@Slf4j
@Tag(name = "Reports", description = "PDF 보고서 생성 및 다운로드 API")
public class ReportController {

    private final ReportGenerationService reportGenerationService;

    /**
     * 통합 검색 보고서 생성 요청 (비동기)
     * 
     * @param jobId 통합 검색 Job ID
     * @param request 보고서 생성 요청
     * @return 보고서 메타데이터
     */
    @PostMapping("/unified-search/{jobId}")
    @Operation(summary = "통합 검색 보고서 생성 요청", description = "비동기로 PDF 보고서를 생성합니다.")
    public ResponseEntity<ReportMetadata> requestUnifiedSearchReport(
            @PathVariable String jobId,
            @RequestBody ReportRequest request) {
        
        log.info("Report generation requested: jobId={}, query={}", jobId, request.getQuery());
        
        ReportMetadata metadata = reportGenerationService.requestUnifiedSearchReport(jobId, request);
        
        return ResponseEntity.accepted().body(metadata);
    }

    /**
     * 통합 검색 보고서 즉시 다운로드 (동기)
     * 
     * @param jobId 통합 검색 Job ID
     * @param request 보고서 생성 요청
     * @return PDF 파일
     */
    @PostMapping("/unified-search/{jobId}/export")
    @Operation(summary = "통합 검색 보고서 즉시 다운로드", description = "동기로 PDF 보고서를 생성하고 즉시 다운로드합니다.")
    public ResponseEntity<byte[]> exportUnifiedSearchReport(
            @PathVariable String jobId,
            @RequestBody ReportRequest request) {
        
        log.info("Report export requested: jobId={}, query={}", jobId, request.getQuery());
        
        try {
            byte[] pdfBytes = reportGenerationService.generateReportSync(jobId, request);
            
            String filename = generateFilename(request.getQuery(), "통합검색");
            
            HttpHeaders headers = new HttpHeaders();
            headers.setContentType(MediaType.APPLICATION_PDF);
            headers.setContentDispositionFormData("attachment", filename);
            headers.setContentLength(pdfBytes.length);
            
            return ResponseEntity.ok()
                    .headers(headers)
                    .body(pdfBytes);
                    
        } catch (IllegalArgumentException e) {
            log.warn("Report export failed - not found: jobId={}, error={}", jobId, e.getMessage());
            return ResponseEntity.notFound().build();
        } catch (IOException e) {
            log.error("Report export failed - IO error: jobId={}, error={}", jobId, e.getMessage(), e);
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).build();
        }
    }

    /**
     * 보고서 상태 조회
     * 
     * @param reportId 보고서 ID
     * @return 보고서 메타데이터
     */
    @GetMapping("/{reportId}")
    @Operation(summary = "보고서 상태 조회", description = "생성 중이거나 완료된 보고서의 상태를 조회합니다.")
    public ResponseEntity<ReportMetadata> getReportStatus(@PathVariable String reportId) {
        ReportMetadata metadata = reportGenerationService.getReportMetadata(reportId);
        
        if (metadata == null) {
            return ResponseEntity.notFound().build();
        }
        
        return ResponseEntity.ok(metadata);
    }

    /**
     * 생성된 보고서 다운로드
     * 
     * @param reportId 보고서 ID
     * @return PDF 파일
     */
    @GetMapping("/{reportId}/download")
    @Operation(summary = "보고서 다운로드", description = "생성된 PDF 보고서를 다운로드합니다.")
    public ResponseEntity<byte[]> downloadReport(@PathVariable String reportId) {
        ReportMetadata metadata = reportGenerationService.getReportMetadata(reportId);
        
        if (metadata == null) {
            return ResponseEntity.notFound().build();
        }
        
        if (metadata.getStatus() != ReportMetadata.ReportStatus.COMPLETED) {
            return ResponseEntity.status(HttpStatus.ACCEPTED)
                    .header("X-Report-Status", metadata.getStatus().name())
                    .build();
        }
        
        try {
            byte[] pdfBytes = reportGenerationService.downloadReport(reportId);
            
            String filename = generateFilename(metadata.getQuery(), "보고서");
            
            HttpHeaders headers = new HttpHeaders();
            headers.setContentType(MediaType.APPLICATION_PDF);
            headers.setContentDispositionFormData("attachment", filename);
            headers.setContentLength(pdfBytes.length);
            
            return ResponseEntity.ok()
                    .headers(headers)
                    .body(pdfBytes);
                    
        } catch (IllegalArgumentException e) {
            log.warn("Report download failed - not found: reportId={}", reportId);
            return ResponseEntity.notFound().build();
        }
    }

    /**
     * DeepSearch 보고서 즉시 다운로드 (동기)
     * 
     * @param jobId DeepSearch Job ID
     * @param request 보고서 생성 요청
     * @return PDF 파일
     */
    @PostMapping("/deep-search/{jobId}/export")
    @Operation(summary = "DeepSearch 보고서 즉시 다운로드", description = "DeepSearch 결과를 PDF 보고서로 내보냅니다.")
    public ResponseEntity<byte[]> exportDeepSearchReport(
            @PathVariable String jobId,
            @RequestBody ReportRequest request) {
        
        log.info("DeepSearch report export requested: jobId={}", jobId);
        
        // TODO: DeepSearch 전용 보고서 생성 로직 구현 필요
        // 현재는 통합 검색 보고서로 대체
        
        try {
            request = ReportRequest.builder()
                    .reportType(ReportRequest.ReportType.DEEP_SEARCH)
                    .targetId(jobId)
                    .query(request.getQuery())
                    .timeWindow(request.getTimeWindow())
                    .includeSections(request.getIncludeSections())
                    .chartImages(request.getChartImages())
                    .build();
            
            byte[] pdfBytes = reportGenerationService.generateReportSync(jobId, request);
            
            String filename = generateFilename(request.getQuery(), "DeepSearch");
            
            HttpHeaders headers = new HttpHeaders();
            headers.setContentType(MediaType.APPLICATION_PDF);
            headers.setContentDispositionFormData("attachment", filename);
            headers.setContentLength(pdfBytes.length);
            
            return ResponseEntity.ok()
                    .headers(headers)
                    .body(pdfBytes);
                    
        } catch (IllegalArgumentException e) {
            log.warn("DeepSearch report export failed - not found: jobId={}", jobId);
            return ResponseEntity.notFound().build();
        } catch (IOException e) {
            log.error("DeepSearch report export failed: jobId={}, error={}", jobId, e.getMessage(), e);
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).build();
        }
    }

    /**
     * ML 분석 보고서 즉시 다운로드 (동기)
     * 
     * @param articleId 기사 ID
     * @param request 보고서 생성 요청
     * @return PDF 파일
     */
    @PostMapping("/ml-analysis/{articleId}/export")
    @Operation(summary = "ML 분석 보고서 즉시 다운로드", description = "기사의 ML 분석 결과를 PDF 보고서로 내보냅니다.")
    public ResponseEntity<byte[]> exportMlAnalysisReport(
            @PathVariable Long articleId,
            @RequestBody ReportRequest request) {
        
        log.info("ML analysis report export requested: articleId={}", articleId);
        
        // TODO: ML 분석 전용 보고서 생성 로직 구현
        
        return ResponseEntity.status(HttpStatus.NOT_IMPLEMENTED)
                .header("X-Message", "ML analysis report is not yet implemented")
                .build();
    }

    // ===== 헬퍼 메서드 =====

    /**
     * PDF 파일명 생성
     */
    private String generateFilename(String query, String type) {
        String dateStr = LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyyMMdd_HHmm"));
        String safeQuery = query != null ? query.replaceAll("[^가-힣a-zA-Z0-9]", "_") : "report";
        if (safeQuery.length() > 30) {
            safeQuery = safeQuery.substring(0, 30);
        }
        
        String filename = String.format("NewsInsight_%s_%s_%s.pdf", type, safeQuery, dateStr);
        
        // URL 인코딩 (한글 파일명 지원)
        return URLEncoder.encode(filename, StandardCharsets.UTF_8)
                .replace("+", "%20");
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/SearchHistoryController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.dto.PageResponse;
import com.newsinsight.collector.dto.SearchHistoryDto;
import com.newsinsight.collector.dto.SearchHistoryMessage;
import com.newsinsight.collector.entity.search.SearchHistory;
import com.newsinsight.collector.entity.search.SearchType;
import com.newsinsight.collector.service.SearchHistoryEventService;
import com.newsinsight.collector.service.SearchHistoryService;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.data.domain.Page;
import org.springframework.http.HttpStatus;
import org.springframework.http.MediaType;
import org.springframework.http.ResponseEntity;
import org.springframework.http.codec.ServerSentEvent;
import org.springframework.web.bind.annotation.*;
import reactor.core.publisher.Flux;

import java.util.List;
import java.util.Map;

/**
 * REST Controller for Search History API.
 * Provides endpoints for saving, querying, and managing search history.
 */
@RestController
@RequestMapping("/api/v1/search-history")
@RequiredArgsConstructor
@Slf4j
public class SearchHistoryController {

    private final SearchHistoryService searchHistoryService;
    private final SearchHistoryEventService searchHistoryEventService;

    // ============================================
    // Create / Save
    // ============================================

    /**
     * Save search result asynchronously via Kafka.
     * This is the primary endpoint for saving search results.
     */
    @PostMapping
    public ResponseEntity<Map<String, Object>> saveSearchHistory(@RequestBody SearchHistoryDto request) {
        log.info("Saving search history: type={}, query='{}'", request.getSearchType(), request.getQuery());
        
        if (request.getQuery() == null || request.getQuery().isBlank()) {
            return ResponseEntity.badRequest().body(Map.of(
                    "error", "Query is required"
            ));
        }
        
        if (request.getSearchType() == null) {
            return ResponseEntity.badRequest().body(Map.of(
                    "error", "Search type is required"
            ));
        }

        // Convert to message and send to Kafka
        SearchHistoryMessage message = request.toMessage();
        searchHistoryService.sendToKafka(message);

        return ResponseEntity.status(HttpStatus.ACCEPTED).body(Map.of(
                "message", "Search history queued for saving",
                "externalId", message.getExternalId() != null ? message.getExternalId() : "",
                "searchType", message.getSearchType().name(),
                "query", message.getQuery()
        ));
    }

    /**
     * Save search result synchronously (for immediate persistence).
     */
    @PostMapping("/sync")
    public ResponseEntity<SearchHistoryDto> saveSearchHistorySync(@RequestBody SearchHistoryDto request) {
        log.info("Saving search history synchronously: type={}, query='{}'", 
                request.getSearchType(), request.getQuery());
        
        SearchHistoryMessage message = request.toMessage();
        SearchHistory saved = searchHistoryService.saveFromMessage(message);
        
        return ResponseEntity.status(HttpStatus.CREATED)
                .body(SearchHistoryDto.fromEntity(saved));
    }

    // ============================================
    // Read / Query
    // ============================================

    /**
     * Get search history by ID.
     */
    @GetMapping("/{id}")
    public ResponseEntity<SearchHistoryDto> getById(@PathVariable Long id) {
        return searchHistoryService.findById(id)
                .map(SearchHistoryDto::fromEntity)
                .map(ResponseEntity::ok)
                .orElse(ResponseEntity.notFound().build());
    }

    /**
     * Get search history by external ID (e.g., jobId).
     */
    @GetMapping("/external/{externalId}")
    public ResponseEntity<SearchHistoryDto> getByExternalId(@PathVariable String externalId) {
        return searchHistoryService.findByExternalId(externalId)
                .map(SearchHistoryDto::fromEntity)
                .map(ResponseEntity::ok)
                .orElse(ResponseEntity.notFound().build());
    }

    /**
     * Get paginated search history.
     */
    @GetMapping
    public ResponseEntity<PageResponse<SearchHistoryDto>> getAll(
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size,
            @RequestParam(defaultValue = "createdAt") String sortBy,
            @RequestParam(defaultValue = "DESC") String sortDirection,
            @RequestParam(required = false) String type,
            @RequestParam(required = false) String userId,
            @RequestHeader(value = "X-User-Id", required = false) String headerUserId,
            @RequestHeader(value = "X-Session-Id", required = false) String sessionId
    ) {
        // Use header userId if not provided in query param
        String effectiveUserId = userId != null ? userId : headerUserId;
        
        Page<SearchHistory> result;
        
        if (type != null && effectiveUserId != null) {
            SearchType searchType = SearchType.valueOf(type.toUpperCase());
            result = searchHistoryService.findByUserAndType(effectiveUserId, searchType, page, size);
        } else if (type != null) {
            SearchType searchType = SearchType.valueOf(type.toUpperCase());
            result = searchHistoryService.findByType(searchType, page, size);
        } else if (effectiveUserId != null) {
            result = searchHistoryService.findByUser(effectiveUserId, page, size);
        } else {
            result = searchHistoryService.findAll(page, size, sortBy, sortDirection);
        }

        PageResponse<SearchHistoryDto> response = new PageResponse<>(
                result.getContent().stream()
                        .map(SearchHistoryDto::fromEntity)
                        .toList(),
                result.getNumber(),
                result.getSize(),
                result.getTotalElements(),
                result.getTotalPages(),
                result.isFirst(),
                result.isLast(),
                result.hasNext(),
                result.hasPrevious()
        );

        return ResponseEntity.ok(response);
    }

    /**
     * Search history by query text.
     */
    @GetMapping("/search")
    public ResponseEntity<PageResponse<SearchHistoryDto>> searchByQuery(
            @RequestParam String q,
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size
    ) {
        Page<SearchHistory> result = searchHistoryService.searchByQuery(q, page, size);
        
        PageResponse<SearchHistoryDto> response = new PageResponse<>(
                result.getContent().stream()
                        .map(SearchHistoryDto::fromEntity)
                        .toList(),
                result.getNumber(),
                result.getSize(),
                result.getTotalElements(),
                result.getTotalPages(),
                result.isFirst(),
                result.isLast(),
                result.hasNext(),
                result.hasPrevious()
        );

        return ResponseEntity.ok(response);
    }

    /**
     * Get bookmarked searches.
     */
    @GetMapping("/bookmarked")
    public ResponseEntity<PageResponse<SearchHistoryDto>> getBookmarked(
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size
    ) {
        Page<SearchHistory> result = searchHistoryService.findBookmarked(page, size);
        
        PageResponse<SearchHistoryDto> response = new PageResponse<>(
                result.getContent().stream()
                        .map(SearchHistoryDto::fromEntity)
                        .toList(),
                result.getNumber(),
                result.getSize(),
                result.getTotalElements(),
                result.getTotalPages(),
                result.isFirst(),
                result.isLast(),
                result.hasNext(),
                result.hasPrevious()
        );

        return ResponseEntity.ok(response);
    }

    /**
     * Get derived searches from a parent.
     */
    @GetMapping("/{id}/derived")
    public ResponseEntity<List<SearchHistoryDto>> getDerivedSearches(@PathVariable Long id) {
        List<SearchHistory> derived = searchHistoryService.findDerivedSearches(id);
        List<SearchHistoryDto> response = derived.stream()
                .map(SearchHistoryDto::fromEntity)
                .toList();
        return ResponseEntity.ok(response);
    }

    /**
     * Get searches by session.
     */
    @GetMapping("/session/{sessionId}")
    public ResponseEntity<List<SearchHistoryDto>> getBySession(@PathVariable String sessionId) {
        List<SearchHistory> searches = searchHistoryService.findBySession(sessionId);
        List<SearchHistoryDto> response = searches.stream()
                .map(SearchHistoryDto::fromEntity)
                .toList();
        return ResponseEntity.ok(response);
    }

    // ============================================
    // Update
    // ============================================

    /**
     * Toggle bookmark status.
     */
    @PostMapping("/{id}/bookmark")
    public ResponseEntity<SearchHistoryDto> toggleBookmark(@PathVariable Long id) {
        try {
            SearchHistory updated = searchHistoryService.toggleBookmark(id);
            return ResponseEntity.ok(SearchHistoryDto.fromEntity(updated));
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    /**
     * Update tags.
     */
    @PutMapping("/{id}/tags")
    public ResponseEntity<SearchHistoryDto> updateTags(
            @PathVariable Long id,
            @RequestBody List<String> tags
    ) {
        try {
            SearchHistory updated = searchHistoryService.updateTags(id, tags);
            return ResponseEntity.ok(SearchHistoryDto.fromEntity(updated));
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    /**
     * Update notes.
     */
    @PutMapping("/{id}/notes")
    public ResponseEntity<SearchHistoryDto> updateNotes(
            @PathVariable Long id,
            @RequestBody Map<String, String> body
    ) {
        String notes = body.get("notes");
        try {
            SearchHistory updated = searchHistoryService.updateNotes(id, notes);
            return ResponseEntity.ok(SearchHistoryDto.fromEntity(updated));
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    // ============================================
    // Delete
    // ============================================

    /**
     * Delete search history by ID.
     */
    @DeleteMapping("/{id}")
    public ResponseEntity<Void> delete(@PathVariable Long id) {
        if (searchHistoryService.findById(id).isEmpty()) {
            return ResponseEntity.notFound().build();
        }
        searchHistoryService.delete(id);
        return ResponseEntity.noContent().build();
    }

    // ============================================
    // Derived Search (Drill-down)
    // ============================================

    /**
     * Create a derived search from a parent.
     * Used for drill-down functionality.
     */
    @PostMapping("/{parentId}/derive")
    public ResponseEntity<Map<String, Object>> createDerivedSearch(
            @PathVariable Long parentId,
            @RequestBody SearchHistoryDto request
    ) {
        log.info("Creating derived search from parent={}, query='{}'", parentId, request.getQuery());
        
        if (request.getQuery() == null || request.getQuery().isBlank()) {
            return ResponseEntity.badRequest().body(Map.of(
                    "error", "Query is required"
            ));
        }

        try {
            SearchHistoryMessage message = request.toMessage();
            SearchHistory derived = searchHistoryService.createDerivedSearch(parentId, message);
            
            return ResponseEntity.status(HttpStatus.CREATED).body(Map.of(
                    "id", derived.getId(),
                    "parentSearchId", parentId,
                    "depthLevel", derived.getDepthLevel(),
                    "query", derived.getQuery(),
                    "message", "Derived search created"
            ));
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    // ============================================
    // Statistics & Utilities
    // ============================================

    /**
     * Get search statistics.
     */
    @GetMapping("/stats")
    public ResponseEntity<Map<String, Object>> getStatistics(
            @RequestParam(defaultValue = "30") int days
    ) {
        return ResponseEntity.ok(searchHistoryService.getStatistics(days));
    }

    /**
     * Get recently discovered URLs.
     */
    @GetMapping("/discovered-urls")
    public ResponseEntity<List<String>> getDiscoveredUrls(
            @RequestParam(defaultValue = "7") int days,
            @RequestParam(defaultValue = "100") int limit
    ) {
        return ResponseEntity.ok(searchHistoryService.getRecentDiscoveredUrls(days, limit));
    }

    /**
     * Health check.
     */
    @GetMapping("/health")
    public ResponseEntity<Map<String, Object>> healthCheck() {
        return ResponseEntity.ok(Map.of(
                "status", "available",
                "features", Map.of(
                        "asyncSave", true,
                        "syncSave", true,
                        "derivedSearch", true,
                        "bookmarks", true,
                        "tags", true,
                        "statistics", true,
                        "sse", true
                ),
                "kafkaTopic", SearchHistoryService.SEARCH_HISTORY_TOPIC,
                "sseSubscribers", searchHistoryEventService.getSubscriberCount()
        ));
    }

    // ============================================
    // Continue Work Feature
    // ============================================

    /**
     * Get items for "Continue Work" feature.
     * Returns actionable searches: in-progress, failed, partial, draft, or unviewed completed.
     */
    @GetMapping("/continue-work")
    public ResponseEntity<Map<String, Object>> getContinueWorkItems(
            @RequestParam(required = false) String userId,
            @RequestParam(required = false) String sessionId,
            @RequestParam(required = false, defaultValue = "10") int limit,
            @RequestHeader(value = "X-User-Id", required = false) String headerUserId,
            @RequestHeader(value = "X-Session-Id", required = false) String headerSessionId
    ) {
        // Use headers if not provided in query params
        String effectiveUserId = userId != null ? userId : headerUserId;
        String effectiveSessionId = sessionId != null ? sessionId : headerSessionId;
        
        log.debug("Continue work request: userId={}, sessionId={}", effectiveUserId, effectiveSessionId);
        
        List<SearchHistory> items = searchHistoryService.findContinueWorkItems(
                effectiveUserId != null ? effectiveUserId : "", 
                effectiveSessionId != null ? effectiveSessionId : "", 
                limit
        );
        
        List<SearchHistoryDto> dtos = items.stream()
                .map(SearchHistoryDto::fromEntity)
                .toList();

        Map<String, Object> stats = searchHistoryService.getContinueWorkStats(
                effectiveUserId != null ? effectiveUserId : "", 
                effectiveSessionId != null ? effectiveSessionId : ""
        );

        return ResponseEntity.ok(Map.of(
                "items", dtos,
                "count", dtos.size(),
                "stats", stats
        ));
    }

    /**
     * Mark search as viewed.
     */
    @PostMapping("/{id}/viewed")
    public ResponseEntity<SearchHistoryDto> markAsViewed(@PathVariable Long id) {
        try {
            SearchHistory updated = searchHistoryService.markAsViewed(id);
            return ResponseEntity.ok(SearchHistoryDto.fromEntity(updated));
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    /**
     * Mark search as viewed by external ID.
     */
    @PostMapping("/external/{externalId}/viewed")
    public ResponseEntity<SearchHistoryDto> markAsViewedByExternalId(@PathVariable String externalId) {
        try {
            SearchHistory updated = searchHistoryService.markAsViewedByExternalId(externalId);
            return ResponseEntity.ok(SearchHistoryDto.fromEntity(updated));
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    /**
     * Update completion status.
     */
    @PutMapping("/{id}/status")
    public ResponseEntity<SearchHistoryDto> updateCompletionStatus(
            @PathVariable Long id,
            @RequestBody Map<String, String> body
    ) {
        String statusStr = body.get("status");
        if (statusStr == null || statusStr.isBlank()) {
            return ResponseEntity.badRequest().build();
        }

        try {
            SearchHistory.CompletionStatus status = SearchHistory.CompletionStatus.valueOf(statusStr.toUpperCase());
            SearchHistory updated = searchHistoryService.updateCompletionStatus(id, status);
            return ResponseEntity.ok(SearchHistoryDto.fromEntity(updated));
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    /**
     * Get searches by completion status.
     */
    @GetMapping("/status/{status}")
    public ResponseEntity<PageResponse<SearchHistoryDto>> getByCompletionStatus(
            @PathVariable String status,
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size
    ) {
        try {
            SearchHistory.CompletionStatus completionStatus = 
                    SearchHistory.CompletionStatus.valueOf(status.toUpperCase());
            
            Page<SearchHistory> result = searchHistoryService.findByCompletionStatus(completionStatus, page, size);
            
            PageResponse<SearchHistoryDto> response = new PageResponse<>(
                    result.getContent().stream()
                            .map(SearchHistoryDto::fromEntity)
                            .toList(),
                    result.getNumber(),
                    result.getSize(),
                    result.getTotalElements(),
                    result.getTotalPages(),
                    result.isFirst(),
                    result.isLast(),
                    result.hasNext(),
                    result.hasPrevious()
            );

            return ResponseEntity.ok(response);
        } catch (IllegalArgumentException e) {
            return ResponseEntity.badRequest().build();
        }
    }

    /**
     * Get searches by project ID.
     */
    @GetMapping("/project/{projectId}")
    public ResponseEntity<PageResponse<SearchHistoryDto>> getByProjectId(
            @PathVariable Long projectId,
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size
    ) {
        Page<SearchHistory> result = searchHistoryService.findByProjectId(projectId, page, size);
        
        PageResponse<SearchHistoryDto> response = new PageResponse<>(
                result.getContent().stream()
                        .map(SearchHistoryDto::fromEntity)
                        .toList(),
                result.getNumber(),
                result.getSize(),
                result.getTotalElements(),
                result.getTotalPages(),
                result.isFirst(),
                result.isLast(),
                result.hasNext(),
                result.hasPrevious()
        );

        return ResponseEntity.ok(response);
    }

    /**
     * Get failed searches for potential retry.
     */
    @GetMapping("/failed")
    public ResponseEntity<List<SearchHistoryDto>> getFailedSearches(
            @RequestParam(defaultValue = "7") int daysBack,
            @RequestParam(defaultValue = "20") int limit
    ) {
        List<SearchHistory> failed = searchHistoryService.findFailedSearches(daysBack, limit);
        List<SearchHistoryDto> response = failed.stream()
                .map(SearchHistoryDto::fromEntity)
                .toList();
        return ResponseEntity.ok(response);
    }

    // ============================================
    // SSE Real-time Stream
    // ============================================

    /**
     * SSE endpoint for real-time search history updates.
     * Clients can subscribe to receive notifications when:
     * - new_search: A new search was saved
     * - updated_search: An existing search was updated
     * - deleted_search: A search was deleted
     * - heartbeat: Keep-alive signal (every 30 seconds)
     */
    @GetMapping(value = "/stream", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
    public Flux<ServerSentEvent<SearchHistoryEventService.SearchHistoryEventDto>> streamSearchHistory() {
        log.info("New SSE client connected to search history stream");
        
        return searchHistoryEventService.getEventStream()
                .map(event -> ServerSentEvent.<SearchHistoryEventService.SearchHistoryEventDto>builder()
                        .id(String.valueOf(event.timestamp()))
                        .event(event.eventType())
                        .data(event)
                        .build());
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/SearchJobController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.entity.search.SearchType;
import com.newsinsight.collector.service.SearchJobQueueService;
import com.newsinsight.collector.service.SearchJobQueueService.*;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.http.HttpStatus;
import org.springframework.http.MediaType;
import org.springframework.http.ResponseEntity;
import org.springframework.http.codec.ServerSentEvent;
import org.springframework.web.bind.annotation.*;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Sinks;

import java.time.Duration;
import java.util.List;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;

/**
 * REST Controller for Search Job Queue API.
 * Enables concurrent search execution and real-time job monitoring.
 */
@RestController
@RequestMapping("/api/v1/jobs")
@RequiredArgsConstructor
@Slf4j
public class SearchJobController {

    private final SearchJobQueueService searchJobQueueService;

    // SSE sinks for job-specific streaming
    private final Map<String, Sinks.Many<SearchJobEvent>> jobSinks = new ConcurrentHashMap<>();

    // ============================================
    // Job Creation
    // ============================================

    /**
     * Start a new search job.
     * Supports concurrent execution of multiple job types.
     */
    @PostMapping
    public ResponseEntity<Map<String, Object>> startJob(
            @RequestBody JobStartRequest request,
            @RequestHeader(value = "X-User-Id", required = false) String userId,
            @RequestHeader(value = "X-Session-Id", required = false) String sessionId
    ) {
        log.info("Starting new search job: type={}, query='{}', userId={}, sessionId={}", 
                request.type(), request.query(), userId, sessionId);

        if (request.query() == null || request.query().isBlank()) {
            return ResponseEntity.badRequest().body(Map.of(
                    "error", "Query is required"
            ));
        }

        if (request.type() == null) {
            return ResponseEntity.badRequest().body(Map.of(
                    "error", "Search type is required"
            ));
        }

        // Use headers if request doesn't specify userId/sessionId
        String effectiveUserId = request.userId() != null ? request.userId() : userId;
        String effectiveSessionId = request.sessionId() != null ? request.sessionId() : sessionId;

        SearchJobRequest jobRequest = SearchJobRequest.builder()
                .type(request.type())
                .query(request.query())
                .timeWindow(request.timeWindow() != null ? request.timeWindow() : "7d")
                .userId(effectiveUserId)
                .sessionId(effectiveSessionId)
                .projectId(request.projectId())
                .options(request.options())
                .build();

        String jobId = searchJobQueueService.startJob(jobRequest);

        return ResponseEntity.status(HttpStatus.ACCEPTED).body(Map.of(
                "jobId", jobId,
                "type", request.type().name(),
                "query", request.query(),
                "status", "PENDING",
                "message", "검색 작업이 시작되었습니다"
        ));
    }

    /**
     * Start multiple search jobs concurrently.
     * Enables running Unified Search, Deep Search, etc. at the same time.
     */
    @PostMapping("/batch")
    public ResponseEntity<Map<String, Object>> startBatchJobs(@RequestBody List<JobStartRequest> requests) {
        log.info("Starting batch jobs: count={}", requests.size());

        if (requests.isEmpty()) {
            return ResponseEntity.badRequest().body(Map.of(
                    "error", "At least one job request is required"
            ));
        }

        List<Map<String, Object>> startedJobs = requests.stream()
                .map(request -> {
                    SearchJobRequest jobRequest = SearchJobRequest.builder()
                            .type(request.type())
                            .query(request.query())
                            .timeWindow(request.timeWindow() != null ? request.timeWindow() : "7d")
                            .userId(request.userId())
                            .sessionId(request.sessionId())
                            .projectId(request.projectId())
                            .options(request.options())
                            .build();

                    String jobId = searchJobQueueService.startJob(jobRequest);

                    return Map.<String, Object>of(
                            "jobId", jobId,
                            "type", request.type().name(),
                            "query", request.query(),
                            "status", "PENDING"
                    );
                })
                .toList();

        return ResponseEntity.status(HttpStatus.ACCEPTED).body(Map.of(
                "jobs", startedJobs,
                "count", startedJobs.size(),
                "message", String.format("%d개의 검색 작업이 시작되었습니다", startedJobs.size())
        ));
    }

    // ============================================
    // Job Status & Query
    // ============================================

    /**
     * Get status of a specific job.
     */
    @GetMapping("/{jobId}")
    public ResponseEntity<SearchJob> getJobStatus(@PathVariable String jobId) {
        return searchJobQueueService.getJobStatus(jobId)
                .map(ResponseEntity::ok)
                .orElse(ResponseEntity.notFound().build());
    }

    /**
     * Get active jobs for user.
     */
    @GetMapping("/active")
    public ResponseEntity<List<SearchJob>> getActiveJobs(
            @RequestParam(required = false) String userId,
            @RequestHeader(value = "X-User-Id", required = false) String headerUserId
    ) {
        // Use header userId if not provided in query param
        String effectiveUserId = userId != null ? userId : headerUserId;
        List<SearchJob> activeJobs = searchJobQueueService.getActiveJobs(effectiveUserId);
        return ResponseEntity.ok(activeJobs);
    }

    /**
     * Get all jobs for user (with limit).
     */
    @GetMapping
    public ResponseEntity<List<SearchJob>> getAllJobs(
            @RequestParam(required = false) String userId,
            @RequestParam(required = false, defaultValue = "20") int limit,
            @RequestHeader(value = "X-User-Id", required = false) String headerUserId
    ) {
        // Use header userId if not provided in query param
        String effectiveUserId = userId != null ? userId : headerUserId;
        List<SearchJob> jobs = searchJobQueueService.getAllJobs(effectiveUserId, limit);
        return ResponseEntity.ok(jobs);
    }

    // ============================================
    // Job Control
    // ============================================

    /**
     * Cancel a running job.
     */
    @PostMapping("/{jobId}/cancel")
    public ResponseEntity<Map<String, Object>> cancelJob(@PathVariable String jobId) {
        log.info("Cancelling job: jobId={}", jobId);

        boolean cancelled = searchJobQueueService.cancelJob(jobId);

        if (cancelled) {
            return ResponseEntity.ok(Map.of(
                    "jobId", jobId,
                    "status", "CANCELLED",
                    "message", "작업이 취소되었습니다"
            ));
        } else {
            return ResponseEntity.badRequest().body(Map.of(
                    "jobId", jobId,
                    "error", "작업을 취소할 수 없습니다 (이미 완료되었거나 존재하지 않음)"
            ));
        }
    }

    // ============================================
    // SSE Real-time Job Streaming
    // ============================================

    /**
     * SSE endpoint for real-time job updates.
     * Stream updates for a specific job.
     */
    @GetMapping(value = "/{jobId}/stream", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
    public Flux<ServerSentEvent<SearchJobEvent>> streamJobUpdates(@PathVariable String jobId) {
        log.info("New SSE client connected to job stream: jobId={}", jobId);

        // Create or get sink for this job
        Sinks.Many<SearchJobEvent> sink = jobSinks.computeIfAbsent(
                jobId,
                id -> Sinks.many().multicast().onBackpressureBuffer()
        );

        // Register listener with the service
        searchJobQueueService.registerListener(jobId, event -> {
            sink.tryEmitNext(event);

            // Cleanup on completion
            if ("completed".equals(event.getEventType()) ||
                    "failed".equals(event.getEventType()) ||
                    "cancelled".equals(event.getEventType())) {
                // Emit complete signal after a delay
                sink.tryEmitComplete();
                jobSinks.remove(jobId);
            }
        });

        // Add heartbeat to keep connection alive
        Flux<ServerSentEvent<SearchJobEvent>> heartbeat = Flux.interval(Duration.ofSeconds(15))
                .map(i -> ServerSentEvent.<SearchJobEvent>builder()
                        .id(String.valueOf(System.currentTimeMillis()))
                        .event("heartbeat")
                        .data(SearchJobEvent.builder()
                                .jobId(jobId)
                                .eventType("heartbeat")
                                .timestamp(System.currentTimeMillis())
                                .build())
                        .build());

        Flux<ServerSentEvent<SearchJobEvent>> events = sink.asFlux()
                .map(event -> ServerSentEvent.<SearchJobEvent>builder()
                        .id(String.valueOf(event.getTimestamp()))
                        .event(event.getEventType())
                        .data(event)
                        .build())
                .doOnCancel(() -> {
                    searchJobQueueService.unregisterListener(jobId);
                    jobSinks.remove(jobId);
                });

        return Flux.merge(events, heartbeat)
                .doFinally(signal -> {
                    searchJobQueueService.unregisterListener(jobId);
                    jobSinks.remove(jobId);
                });
    }

    /**
     * SSE endpoint for all active jobs of a user.
     * Stream updates for all active jobs.
     */
    @GetMapping(value = "/stream", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
    public Flux<ServerSentEvent<Map<String, Object>>> streamAllJobs(
            @RequestParam(required = false, defaultValue = "anonymous") String userId
    ) {
        log.info("New SSE client connected to all-jobs stream: userId={}", userId);

        // Poll for job updates every 2 seconds
        return Flux.interval(Duration.ofSeconds(2))
                .map(i -> {
                    List<SearchJob> activeJobs = searchJobQueueService.getActiveJobs(userId);
                    return ServerSentEvent.<Map<String, Object>>builder()
                            .id(String.valueOf(System.currentTimeMillis()))
                            .event("jobs_update")
                            .data(Map.of(
                                    "jobs", activeJobs,
                                    "count", activeJobs.size(),
                                    "timestamp", System.currentTimeMillis()
                            ))
                            .build();
                })
                .takeUntilOther(Flux.never()); // Keep alive until client disconnects
    }

    // ============================================
    // Health & Stats
    // ============================================

    /**
     * Health check endpoint.
     */
    @GetMapping("/health")
    public ResponseEntity<Map<String, Object>> healthCheck() {
        return ResponseEntity.ok(Map.of(
                "status", "available",
                "features", Map.of(
                        "concurrentJobs", true,
                        "batchStart", true,
                        "jobCancellation", true,
                        "sseStreaming", true
                ),
                "supportedTypes", List.of(
                        SearchType.UNIFIED.name(),
                        SearchType.DEEP_SEARCH.name(),
                        SearchType.FACT_CHECK.name(),
                        SearchType.BROWSER_AGENT.name()
                )
        ));
    }

    // ============================================
    // DTOs
    // ============================================

    /**
     * Request DTO for starting a job.
     */
    public record JobStartRequest(
            SearchType type,
            String query,
            String timeWindow,
            String userId,
            String sessionId,
            Long projectId,
            Map<String, Object> options
    ) {}
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/SearchTemplateController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.dto.PageResponse;
import com.newsinsight.collector.dto.SearchTemplateDto;
import com.newsinsight.collector.entity.search.SearchTemplate;
import com.newsinsight.collector.service.SearchTemplateService;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.data.domain.Page;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.List;
import java.util.Map;

/**
 * REST Controller for Search Template API.
 * Provides endpoints for managing search templates (SmartSearch feature).
 */
@RestController
@RequestMapping("/api/v1/search-templates")
@RequiredArgsConstructor
@Slf4j
public class SearchTemplateController {

    private final SearchTemplateService searchTemplateService;

    // ============================================
    // Create
    // ============================================

    /**
     * Create a new search template
     */
    @PostMapping
    public ResponseEntity<?> createTemplate(@RequestBody SearchTemplateDto request) {
        log.info("Creating template: name='{}', mode={}, userId={}", 
                request.getName(), request.getMode(), request.getUserId());

        try {
            SearchTemplate created = searchTemplateService.create(request);
            return ResponseEntity.status(HttpStatus.CREATED)
                    .body(SearchTemplateDto.fromEntity(created));
        } catch (IllegalArgumentException e) {
            return ResponseEntity.badRequest().body(Map.of("error", e.getMessage()));
        }
    }

    // ============================================
    // Read
    // ============================================

    /**
     * Get template by ID
     */
    @GetMapping("/{id}")
    public ResponseEntity<SearchTemplateDto> getById(@PathVariable Long id) {
        return searchTemplateService.findById(id)
                .map(SearchTemplateDto::fromEntity)
                .map(ResponseEntity::ok)
                .orElse(ResponseEntity.notFound().build());
    }

    /**
     * Get paginated templates with optional filtering
     */
    @GetMapping
    public ResponseEntity<PageResponse<SearchTemplateDto>> getAll(
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size,
            @RequestParam(defaultValue = "createdAt") String sortBy,
            @RequestParam(defaultValue = "DESC") String sortDirection,
            @RequestParam(required = false) String userId,
            @RequestParam(required = false) String mode
    ) {
        Page<SearchTemplate> result;

        if (userId != null && mode != null) {
            result = searchTemplateService.findByUserAndMode(userId, mode, page, size);
        } else if (userId != null) {
            result = searchTemplateService.findByUser(userId, page, size);
        } else {
            result = searchTemplateService.findAll(page, size, sortBy, sortDirection);
        }

        PageResponse<SearchTemplateDto> response = new PageResponse<>(
                result.getContent().stream()
                        .map(SearchTemplateDto::fromEntity)
                        .toList(),
                result.getNumber(),
                result.getSize(),
                result.getTotalElements(),
                result.getTotalPages(),
                result.isFirst(),
                result.isLast(),
                result.hasNext(),
                result.hasPrevious()
        );

        return ResponseEntity.ok(response);
    }

    /**
     * Get all templates for a user (list format, no pagination)
     */
    @GetMapping("/user/{userId}")
    public ResponseEntity<List<SearchTemplateDto>> getAllByUser(@PathVariable String userId) {
        List<SearchTemplate> templates = searchTemplateService.findAllByUser(userId);
        List<SearchTemplateDto> response = templates.stream()
                .map(SearchTemplateDto::fromEntity)
                .toList();
        return ResponseEntity.ok(response);
    }

    /**
     * Get favorite templates for a user
     */
    @GetMapping("/user/{userId}/favorites")
    public ResponseEntity<List<SearchTemplateDto>> getFavorites(@PathVariable String userId) {
        List<SearchTemplate> templates = searchTemplateService.findFavoritesByUser(userId);
        List<SearchTemplateDto> response = templates.stream()
                .map(SearchTemplateDto::fromEntity)
                .toList();
        return ResponseEntity.ok(response);
    }

    /**
     * Get most used templates for a user
     */
    @GetMapping("/user/{userId}/most-used")
    public ResponseEntity<List<SearchTemplateDto>> getMostUsed(
            @PathVariable String userId,
            @RequestParam(defaultValue = "10") int limit
    ) {
        List<SearchTemplate> templates = searchTemplateService.findMostUsed(userId, limit);
        List<SearchTemplateDto> response = templates.stream()
                .map(SearchTemplateDto::fromEntity)
                .toList();
        return ResponseEntity.ok(response);
    }

    /**
     * Get recently used templates for a user
     */
    @GetMapping("/user/{userId}/recent")
    public ResponseEntity<List<SearchTemplateDto>> getRecentlyUsed(
            @PathVariable String userId,
            @RequestParam(defaultValue = "10") int limit
    ) {
        List<SearchTemplate> templates = searchTemplateService.findRecentlyUsed(userId, limit);
        List<SearchTemplateDto> response = templates.stream()
                .map(SearchTemplateDto::fromEntity)
                .toList();
        return ResponseEntity.ok(response);
    }

    /**
     * Search templates by name
     */
    @GetMapping("/search")
    public ResponseEntity<PageResponse<SearchTemplateDto>> searchByName(
            @RequestParam String q,
            @RequestParam(required = false) String userId,
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size
    ) {
        Page<SearchTemplate> result = searchTemplateService.searchByName(q, userId, page, size);

        PageResponse<SearchTemplateDto> response = new PageResponse<>(
                result.getContent().stream()
                        .map(SearchTemplateDto::fromEntity)
                        .toList(),
                result.getNumber(),
                result.getSize(),
                result.getTotalElements(),
                result.getTotalPages(),
                result.isFirst(),
                result.isLast(),
                result.hasNext(),
                result.hasPrevious()
        );

        return ResponseEntity.ok(response);
    }

    // ============================================
    // Update
    // ============================================

    /**
     * Update a template
     */
    @PutMapping("/{id}")
    public ResponseEntity<?> updateTemplate(
            @PathVariable Long id,
            @RequestBody SearchTemplateDto request
    ) {
        try {
            SearchTemplate updated = searchTemplateService.update(id, request);
            return ResponseEntity.ok(SearchTemplateDto.fromEntity(updated));
        } catch (IllegalArgumentException e) {
            if (e.getMessage().contains("not found")) {
                return ResponseEntity.notFound().build();
            }
            return ResponseEntity.badRequest().body(Map.of("error", e.getMessage()));
        }
    }

    /**
     * Toggle favorite status
     */
    @PostMapping("/{id}/favorite")
    public ResponseEntity<?> toggleFavorite(@PathVariable Long id) {
        try {
            SearchTemplate updated = searchTemplateService.toggleFavorite(id);
            return ResponseEntity.ok(SearchTemplateDto.fromEntity(updated));
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    /**
     * Record template usage (when user loads a template)
     */
    @PostMapping("/{id}/use")
    public ResponseEntity<Map<String, Object>> recordUsage(@PathVariable Long id) {
        if (searchTemplateService.findById(id).isEmpty()) {
            return ResponseEntity.notFound().build();
        }
        searchTemplateService.recordUsage(id);
        return ResponseEntity.ok(Map.of(
                "message", "Usage recorded",
                "templateId", id
        ));
    }

    /**
     * Duplicate a template
     */
    @PostMapping("/{id}/duplicate")
    public ResponseEntity<?> duplicateTemplate(
            @PathVariable Long id,
            @RequestParam(required = false) String newName,
            @RequestParam(required = false) String userId
    ) {
        try {
            SearchTemplate duplicated = searchTemplateService.duplicate(id, newName, userId);
            return ResponseEntity.status(HttpStatus.CREATED)
                    .body(SearchTemplateDto.fromEntity(duplicated));
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    // ============================================
    // Delete
    // ============================================

    /**
     * Delete a template
     */
    @DeleteMapping("/{id}")
    public ResponseEntity<Void> deleteTemplate(@PathVariable Long id) {
        try {
            searchTemplateService.delete(id);
            return ResponseEntity.noContent().build();
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        }
    }

    // ============================================
    // Statistics
    // ============================================

    /**
     * Get template statistics
     */
    @GetMapping("/stats")
    public ResponseEntity<Map<String, Object>> getStatistics(
            @RequestParam(required = false) String userId
    ) {
        return ResponseEntity.ok(searchTemplateService.getStatistics(userId));
    }

    /**
     * Health check
     */
    @GetMapping("/health")
    public ResponseEntity<Map<String, Object>> healthCheck() {
        return ResponseEntity.ok(Map.of(
                "service", "SearchTemplateService",
                "status", "available",
                "features", Map.of(
                        "create", true,
                        "favorites", true,
                        "duplicate", true,
                        "usageTracking", true
                )
        ));
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/SourceController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.dto.*;
import com.newsinsight.collector.entity.DataSource;
import com.newsinsight.collector.mapper.EntityMapper;
import com.newsinsight.collector.service.DataSourceService;
import jakarta.validation.Valid;
import lombok.RequiredArgsConstructor;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.PageRequest;
import org.springframework.data.domain.Pageable;
import org.springframework.data.domain.Sort;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

@RestController
@RequestMapping("/api/v1/sources")
@RequiredArgsConstructor
public class SourceController {

    private final DataSourceService dataSourceService;
    private final EntityMapper entityMapper;

    /**
     * GET /api/v1/sources - 모든 데이터 소스 목록 조회 (페이징/정렬 지원)
     */
    @GetMapping
    public ResponseEntity<Page<DataSourceDTO>> listSources(
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size,
            @RequestParam(defaultValue = "id") String sortBy,
            @RequestParam(defaultValue = "DESC") String sortDirection) {
        
        Sort.Direction direction = Sort.Direction.fromString(sortDirection);
        Pageable pageable = PageRequest.of(page, size, Sort.by(direction, sortBy));
        
        Page<DataSource> sources = dataSourceService.findAll(pageable);
        Page<DataSourceDTO> sourceDTOs = sources.map(entityMapper::toDataSourceDTO);
        
        return ResponseEntity.ok(sourceDTOs);
    }

    /**
     * GET /api/v1/sources/active - 활성 데이터 소스 목록 조회
     */
    @GetMapping("/active")
    public ResponseEntity<Page<DataSourceDTO>> listActiveSources(
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size) {
        
        Pageable pageable = PageRequest.of(page, size, Sort.by(Sort.Direction.DESC, "id"));
        Page<DataSource> sources = dataSourceService.findAllActive(pageable);
        Page<DataSourceDTO> sourceDTOs = sources.map(entityMapper::toDataSourceDTO);
        
        return ResponseEntity.ok(sourceDTOs);
    }

    /**
     * GET /api/v1/sources/{id} - ID로 데이터 소스 조회
     */
    @GetMapping("/{id}")
    public ResponseEntity<DataSourceDTO> getSource(@PathVariable Long id) {
        return dataSourceService.findById(id)
                .map(entityMapper::toDataSourceDTO)
                .map(ResponseEntity::ok)
                .orElse(ResponseEntity.notFound().build());
    }

    /**
     * POST /api/v1/sources - 새로운 데이터 소스 등록
     */
    @PostMapping
    public ResponseEntity<DataSourceDTO> createSource(@Valid @RequestBody DataSourceCreateRequest request) {
        DataSource source = entityMapper.toDataSource(request);
        DataSource savedSource = dataSourceService.create(source);
        DataSourceDTO dto = entityMapper.toDataSourceDTO(savedSource);
        
        return ResponseEntity.status(HttpStatus.CREATED).body(dto);
    }

    /**
     * PUT /api/v1/sources/{id} - 데이터 소스 수정
     */
    @PutMapping("/{id}")
    public ResponseEntity<DataSourceDTO> updateSource(
            @PathVariable Long id,
            @Valid @RequestBody DataSourceUpdateRequest request) {
        
        return dataSourceService.findById(id)
                .map(existingSource -> {
                    entityMapper.updateDataSourceFromRequest(request, existingSource);
                    DataSource updated = dataSourceService.save(existingSource);
                    return ResponseEntity.ok(entityMapper.toDataSourceDTO(updated));
                })
                .orElse(ResponseEntity.notFound().build());
    }

    /**
     * DELETE /api/v1/sources/{id} - 데이터 소스 삭제
     */
    @DeleteMapping("/{id}")
    public ResponseEntity<Void> deleteSource(@PathVariable Long id) {
        boolean deleted = dataSourceService.delete(id);
        return deleted ? ResponseEntity.noContent().build() : ResponseEntity.notFound().build();
    }

    /**
     * POST /api/v1/sources/{id}/activate - 데이터 소스 활성화
     */
    @PostMapping("/{id}/activate")
    public ResponseEntity<DataSourceDTO> activateSource(@PathVariable Long id) {
        return dataSourceService.findById(id)
                .map(source -> {
                    source.setIsActive(true);
                    DataSource updated = dataSourceService.save(source);
                    return ResponseEntity.ok(entityMapper.toDataSourceDTO(updated));
                })
                .orElse(ResponseEntity.notFound().build());
    }

    /**
     * POST /api/v1/sources/{id}/deactivate - 데이터 소스 비활성화
     */
    @PostMapping("/{id}/deactivate")
    public ResponseEntity<DataSourceDTO> deactivateSource(@PathVariable Long id) {
        return dataSourceService.findById(id)
                .map(source -> {
                    source.setIsActive(false);
                    DataSource updated = dataSourceService.save(source);
                    return ResponseEntity.ok(entityMapper.toDataSourceDTO(updated));
                })
                .orElse(ResponseEntity.notFound().build());
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/UnifiedSearchController.java

```java
package com.newsinsight.collector.controller;

import com.fasterxml.jackson.databind.ObjectMapper;
import com.newsinsight.collector.service.AnalysisEventService;
import com.newsinsight.collector.service.FactVerificationService;
import com.newsinsight.collector.service.UnifiedSearchEventService;
import com.newsinsight.collector.service.UnifiedSearchService;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.http.HttpStatus;
import org.springframework.http.MediaType;
import org.springframework.http.ResponseEntity;
import org.springframework.http.codec.ServerSentEvent;
import org.springframework.web.bind.annotation.*;
import reactor.core.publisher.Flux;

import java.time.Duration;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.UUID;

/**
 * 통합 검색 컨트롤러
 * 
 * 병렬 검색 및 심층 분석 기능을 SSE 스트리밍으로 제공합니다.
 * 특정 기술/API 이름을 노출하지 않고 통합된 경험을 제공합니다.
 */
@RestController
@RequestMapping("/api/v1/search")
@RequiredArgsConstructor
@Slf4j
public class UnifiedSearchController {

    private final UnifiedSearchService unifiedSearchService;
    private final UnifiedSearchEventService unifiedSearchEventService;
    private final FactVerificationService factVerificationService;
    private final AnalysisEventService analysisEventService;
    private final ObjectMapper objectMapper;

    /**
     * 통합 병렬 검색 (SSE 스트리밍)
     * 
     * DB, 웹, AI 검색을 병렬로 실행하고 결과가 나오는 대로 스트리밍합니다.
     * 
     * @param query 검색어
     * @param window 시간 범위 (1d, 7d, 30d)
     * @return SSE 이벤트 스트림
     */
    @GetMapping(value = "/stream", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
    public Flux<ServerSentEvent<String>> streamSearch(
            @RequestParam String query,
            @RequestParam(defaultValue = "7d") String window
    ) {
        log.info("Starting streaming search for query: '{}', window: {}", query, window);

        // 즉시 연결 확인 이벤트 전송 (클라이언트가 연결 성공을 확인할 수 있도록)
        Flux<ServerSentEvent<String>> initialEvent = Flux.just(
                ServerSentEvent.<String>builder()
                        .id("init")
                        .event("connected")
                        .data("{\"message\": \"검색 시스템에 연결되었습니다. 병렬 검색을 시작합니다...\", \"query\": \"" + query + "\"}")
                        .build()
        );

        Flux<ServerSentEvent<String>> searchEvents = unifiedSearchService.searchParallel(query, window)
                .map(event -> {
                    try {
                        String data = objectMapper.writeValueAsString(event);
                        return ServerSentEvent.<String>builder()
                                .id(java.util.UUID.randomUUID().toString())
                                .event(event.getEventType())
                                .data(data)
                                .build();
                    } catch (Exception e) {
                        log.error("Failed to serialize search event: {}", e.getMessage());
                        return ServerSentEvent.<String>builder()
                                .event("error")
                                .data("{\"error\": \"Serialization failed\"}")
                                .build();
                    }
                });

        Flux<ServerSentEvent<String>> doneEvent = Flux.just(
                ServerSentEvent.<String>builder()
                        .event("done")
                        .data("{\"message\": \"Search completed\"}")
                        .build()
        );

        return Flux.concat(initialEvent, searchEvents, doneEvent)
                .doOnError(e -> log.error("Stream search error: {}", e.getMessage()))
                .timeout(Duration.ofMinutes(2))
                .onErrorResume(e -> Flux.just(
                        ServerSentEvent.<String>builder()
                                .event("error")
                                .data("{\"error\": \"" + e.getMessage() + "\"}")
                                .build()
                ));
    }

    /**
     * 심층 분석 및 팩트 검증 (SSE 스트리밍)
     * 
     * 주어진 주제에 대해 Wikipedia 등 신뢰할 수 있는 출처와 대조하여
     * 타당성을 검증하고 심층 분석을 수행합니다.
     * 
     * @param request 분석 요청 (topic, claims)
     * @return SSE 이벤트 스트림
     */
    @PostMapping(value = "/deep/stream", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
    public Flux<ServerSentEvent<String>> streamDeepAnalysis(
            @RequestBody DeepAnalysisRequest request
    ) {
        log.info("Starting deep analysis for topic: '{}'", request.getTopic());

        return factVerificationService.analyzeAndVerify(request.getTopic(), request.getClaims())
                .map(event -> {
                    try {
                        String data = objectMapper.writeValueAsString(event);
                        return ServerSentEvent.<String>builder()
                                .id(java.util.UUID.randomUUID().toString())
                                .event(event.getEventType())
                                .data(data)
                                .build();
                    } catch (Exception e) {
                        log.error("Failed to serialize deep analysis event: {}", e.getMessage());
                        return ServerSentEvent.<String>builder()
                                .event("error")
                                .data("{\"error\": \"Serialization failed\"}")
                                .build();
                    }
                })
                .concatWith(Flux.just(
                        ServerSentEvent.<String>builder()
                                .event("done")
                                .data("{\"message\": \"Analysis completed\"}")
                                .build()
                ))
                .timeout(Duration.ofMinutes(3))
                .onErrorResume(e -> Flux.just(
                        ServerSentEvent.<String>builder()
                                .event("error")
                                .data("{\"error\": \"" + e.getMessage() + "\"}")
                                .build()
                ));
    }

    // ============================================
    // Job-based Search API (supports SSE reconnection)
    // ============================================

    /**
     * Start a new search job.
     * Returns immediately with jobId. Results are streamed via SSE.
     * 
     * @param request Search request with query and window
     * @return 202 Accepted with job details
     */
    @PostMapping("/jobs")
    public ResponseEntity<Map<String, Object>> startSearchJob(@RequestBody SearchJobRequest request) {
        if (request.getQuery() == null || request.getQuery().isBlank()) {
            return ResponseEntity.badRequest().body(Map.of(
                    "error", "Query is required"
            ));
        }

        String jobId = UUID.randomUUID().toString();
        String window = request.getWindow() != null ? request.getWindow() : "7d";
        List<String> priorityUrls = request.getPriorityUrls();
        String startDate = request.getStartDate();
        String endDate = request.getEndDate();
        
        log.info("Starting search job: {} for query: '{}', window: {}, priorityUrls: {}, startDate: {}, endDate: {}", 
                jobId, request.getQuery(), window, 
                priorityUrls != null ? priorityUrls.size() : 0,
                startDate, endDate);

        // Create job in event service
        var metadata = unifiedSearchEventService.createJob(jobId, request.getQuery(), window);
        
        // Start async search execution with priorityUrls and custom date range
        unifiedSearchService.executeSearchAsync(jobId, request.getQuery(), window, priorityUrls, startDate, endDate);

        return ResponseEntity.status(HttpStatus.ACCEPTED).body(Map.of(
                "jobId", jobId,
                "query", request.getQuery(),
                "window", window,
                "status", metadata.status(),
                "createdAt", metadata.createdAt(),
                "streamUrl", "/api/v1/search/jobs/" + jobId + "/stream"
        ));
    }

    /**
     * Get job status.
     * 
     * @param jobId The job ID
     * @return Job status
     */
    @GetMapping("/jobs/{jobId}")
    public ResponseEntity<Map<String, Object>> getJobStatus(@PathVariable String jobId) {
        var metadata = unifiedSearchEventService.getJobMetadata(jobId);
        
        if (metadata == null) {
            return ResponseEntity.notFound().build();
        }

        return ResponseEntity.ok(Map.of(
                "jobId", metadata.jobId(),
                "query", metadata.query(),
                "window", metadata.window(),
                "status", metadata.status(),
                "createdAt", metadata.createdAt(),
                "completedAt", metadata.completedAt() != null ? metadata.completedAt() : ""
        ));
    }

    /**
     * Stream search job results via SSE.
     * Supports reconnection - client can reconnect with same jobId.
     * 
     * @param jobId The job ID
     * @return SSE event stream
     */
    @GetMapping(value = "/jobs/{jobId}/stream", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
    public Flux<ServerSentEvent<Object>> streamJobResults(@PathVariable String jobId) {
        log.info("SSE connection request for search job: {}", jobId);

        if (!unifiedSearchEventService.hasJob(jobId)) {
            return Flux.just(ServerSentEvent.builder()
                    .event("error")
                    .data(Map.of("error", "Job not found: " + jobId))
                    .build());
        }

        return unifiedSearchEventService.getJobEventStream(jobId)
                .timeout(Duration.ofMinutes(5))
                .onErrorResume(e -> {
                    log.error("SSE stream error for job: {}", jobId, e);
                    return Flux.just(ServerSentEvent.builder()
                            .event("error")
                            .data(Map.of("error", e.getMessage()))
                            .build());
                });
    }

    /**
     * 검색 서비스 상태 확인
     */
    @GetMapping("/health")
    public ResponseEntity<Map<String, Object>> healthCheck() {
        return ResponseEntity.ok(Map.of(
                "status", "available",
                "features", Map.of(
                        "parallelSearch", true,
                        "deepAnalysis", true,
                        "factVerification", true,
                        "analysisStreaming", true
                ),
                "description", "통합 검색 및 심층 분석 서비스"
        ));
    }

    /**
     * 분석 결과 실시간 업데이트 스트림 (SSE)
     * 
     * 특정 기사 ID들의 분석 완료 이벤트를 실시간으로 구독합니다.
     * 검색 결과 페이지에서 분석 중인 기사들의 상태를 실시간으로 업데이트할 때 사용합니다.
     * 
     * @param articleIds 구독할 기사 ID 목록 (comma-separated)
     * @return SSE 이벤트 스트림
     */
    @GetMapping(value = "/analysis/stream", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
    public Flux<ServerSentEvent<Object>> streamAnalysisUpdates(
            @RequestParam(required = false) String articleIds
    ) {
        Set<Long> ids = new HashSet<>();
        if (articleIds != null && !articleIds.isBlank()) {
            try {
                for (String idStr : articleIds.split(",")) {
                    ids.add(Long.parseLong(idStr.trim()));
                }
            } catch (NumberFormatException e) {
                log.warn("Invalid article IDs format: {}", articleIds);
            }
        }

        log.info("Starting analysis stream for {} article IDs", ids.size());

        return analysisEventService.subscribeToAnalysisUpdates(ids)
                .timeout(Duration.ofMinutes(30))
                .onErrorResume(e -> {
                    log.error("Analysis stream error: {}", e.getMessage());
                    return Flux.just(
                            ServerSentEvent.builder()
                                    .event("error")
                                    .data(Map.of("error", e.getMessage()))
                                    .build()
                    );
                });
    }

    /**
     * 분석 구독 기사 추가
     * 
     * @param articleIds 추가할 기사 ID 목록
     */
    @PostMapping("/analysis/watch")
    public ResponseEntity<Map<String, Object>> watchArticles(@RequestBody List<Long> articleIds) {
        if (articleIds != null && !articleIds.isEmpty()) {
            analysisEventService.watchArticles(new HashSet<>(articleIds));
        }
        return ResponseEntity.ok(Map.of(
                "message", "Articles added to watch list",
                "watchedCount", analysisEventService.getWatchedCount()
        ));
    }

    /**
     * 분석 스트리밍 상태 확인
     */
    @GetMapping("/analysis/stream/status")
    public ResponseEntity<Map<String, Object>> analysisStreamStatus() {
        return ResponseEntity.ok(Map.of(
                "subscriberCount", analysisEventService.getSubscriberCount(),
                "watchedArticleCount", analysisEventService.getWatchedCount()
        ));
    }

    // ============================================
    // Request DTOs
    // ============================================

    public static class DeepAnalysisRequest {
        private String topic;
        private List<String> claims;

        public String getTopic() {
            return topic;
        }

        public void setTopic(String topic) {
            this.topic = topic;
        }

        public List<String> getClaims() {
            return claims;
        }

        public void setClaims(List<String> claims) {
            this.claims = claims;
        }
    }

    public static class SearchJobRequest {
        private String query;
        private String window;
        private List<String> priorityUrls;
        private String startDate;  // ISO 8601 format (e.g., "2024-01-01T00:00:00")
        private String endDate;    // ISO 8601 format (e.g., "2024-01-31T23:59:59")

        public String getQuery() {
            return query;
        }

        public void setQuery(String query) {
            this.query = query;
        }

        public String getWindow() {
            return window;
        }

        public void setWindow(String window) {
            this.window = window;
        }

        public List<String> getPriorityUrls() {
            return priorityUrls;
        }

        public void setPriorityUrls(List<String> priorityUrls) {
            this.priorityUrls = priorityUrls;
        }

        public String getStartDate() {
            return startDate;
        }

        public void setStartDate(String startDate) {
            this.startDate = startDate;
        }

        public String getEndDate() {
            return endDate;
        }

        public void setEndDate(String endDate) {
            this.endDate = endDate;
        }
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/controller/WorkspaceController.java

```java
package com.newsinsight.collector.controller;

import com.newsinsight.collector.dto.PageResponse;
import com.newsinsight.collector.entity.workspace.WorkspaceFile;
import com.newsinsight.collector.service.WorkspaceFileService;
import com.newsinsight.collector.service.WorkspaceFileService.*;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.core.io.Resource;
import org.springframework.data.domain.Page;
import org.springframework.http.HttpHeaders;
import org.springframework.http.HttpStatus;
import org.springframework.http.MediaType;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;
import org.springframework.web.multipart.MultipartFile;

import java.net.URLEncoder;
import java.nio.charset.StandardCharsets;
import java.util.List;
import java.util.Map;

/**
 * REST Controller for Workspace File API.
 * Provides endpoints for file upload, download, listing, and deletion.
 */
@RestController
@RequestMapping("/api/v1/workspace/files")
@RequiredArgsConstructor
@Slf4j
public class WorkspaceController {

    private final WorkspaceFileService fileService;

    // ============================================
    // File Upload
    // ============================================

    /**
     * Upload a file.
     * Supports both session-based (anonymous) and user-based uploads.
     */
    @PostMapping(consumes = MediaType.MULTIPART_FORM_DATA_VALUE)
    public ResponseEntity<WorkspaceFile> uploadFile(
            @RequestParam("file") MultipartFile file,
            @RequestHeader(value = "X-Session-ID", required = false) String sessionId,
            @RequestParam(value = "userId", required = false) String userId,
            @RequestParam(value = "projectId", required = false) Long projectId,
            @RequestParam(value = "description", required = false) String description
    ) {
        log.info("File upload request: name='{}', size={}, sessionId={}, userId={}",
                file.getOriginalFilename(), file.getSize(), sessionId, userId);

        if (sessionId == null && userId == null) {
            log.warn("Neither sessionId nor userId provided for file upload");
            return ResponseEntity.badRequest().build();
        }

        try {
            UploadRequest request = UploadRequest.builder()
                    .projectId(projectId)
                    .description(description)
                    .build();

            WorkspaceFile uploaded;
            if (userId != null) {
                uploaded = fileService.uploadFileForUser(file, userId, request);
            } else {
                uploaded = fileService.uploadFile(file, sessionId, request);
            }

            return ResponseEntity.status(HttpStatus.CREATED).body(uploaded);

        } catch (IllegalArgumentException e) {
            log.warn("Invalid upload request: {}", e.getMessage());
            return ResponseEntity.badRequest().build();
        } catch (IllegalStateException e) {
            log.warn("Upload denied: {}", e.getMessage());
            return ResponseEntity.status(HttpStatus.FORBIDDEN).build();
        } catch (Exception e) {
            log.error("File upload failed", e);
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).build();
        }
    }

    /**
     * Upload multiple files.
     */
    @PostMapping(value = "/batch", consumes = MediaType.MULTIPART_FORM_DATA_VALUE)
    public ResponseEntity<List<WorkspaceFile>> uploadFiles(
            @RequestParam("files") MultipartFile[] files,
            @RequestHeader(value = "X-Session-ID", required = false) String sessionId,
            @RequestParam(value = "userId", required = false) String userId,
            @RequestParam(value = "projectId", required = false) Long projectId
    ) {
        log.info("Batch upload request: {} files, sessionId={}, userId={}", files.length, sessionId, userId);

        if (sessionId == null && userId == null) {
            return ResponseEntity.badRequest().build();
        }

        try {
            UploadRequest request = UploadRequest.builder()
                    .projectId(projectId)
                    .build();

            List<WorkspaceFile> uploaded = java.util.Arrays.stream(files)
                    .map(file -> {
                        if (userId != null) {
                            return fileService.uploadFileForUser(file, userId, request);
                        } else {
                            return fileService.uploadFile(file, sessionId, request);
                        }
                    })
                    .toList();

            return ResponseEntity.status(HttpStatus.CREATED).body(uploaded);

        } catch (Exception e) {
            log.error("Batch upload failed", e);
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).build();
        }
    }

    // ============================================
    // File Download
    // ============================================

    /**
     * Download a file by UUID.
     */
    @GetMapping("/{fileUuid}/download")
    public ResponseEntity<Resource> downloadFile(
            @PathVariable String fileUuid,
            @RequestHeader(value = "X-Session-ID", required = false) String sessionId,
            @RequestParam(value = "userId", required = false) String userId
    ) {
        log.info("File download request: uuid={}, sessionId={}, userId={}", fileUuid, sessionId, userId);

        try {
            FileDownloadResponse download = fileService.getFileForDownload(fileUuid, sessionId, userId);

            String encodedFilename = URLEncoder.encode(download.getFilename(), StandardCharsets.UTF_8)
                    .replace("+", "%20");

            return ResponseEntity.ok()
                    .contentType(MediaType.parseMediaType(
                            download.getContentType() != null ? download.getContentType() : "application/octet-stream"))
                    .contentLength(download.getFileSize())
                    .header(HttpHeaders.CONTENT_DISPOSITION, 
                            "attachment; filename=\"" + encodedFilename + "\"; filename*=UTF-8''" + encodedFilename)
                    .body(download.getResource());

        } catch (IllegalArgumentException e) {
            log.warn("File not found: {}", fileUuid);
            return ResponseEntity.notFound().build();
        } catch (IllegalStateException e) {
            log.warn("Access denied to file: {}", fileUuid);
            return ResponseEntity.status(HttpStatus.FORBIDDEN).build();
        } catch (Exception e) {
            log.error("File download failed", e);
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).build();
        }
    }

    /**
     * Get file metadata by UUID.
     */
    @GetMapping("/{fileUuid}")
    public ResponseEntity<WorkspaceFile> getFile(
            @PathVariable String fileUuid,
            @RequestHeader(value = "X-Session-ID", required = false) String sessionId,
            @RequestParam(value = "userId", required = false) String userId
    ) {
        return fileService.getFileWithAccess(fileUuid, sessionId, userId)
                .map(ResponseEntity::ok)
                .orElse(ResponseEntity.notFound().build());
    }

    // ============================================
    // File Listing
    // ============================================

    /**
     * List files for current session/user.
     */
    @GetMapping
    public ResponseEntity<PageResponse<WorkspaceFile>> listFiles(
            @RequestHeader(value = "X-Session-ID", required = false) String sessionId,
            @RequestParam(value = "userId", required = false) String userId,
            @RequestParam(value = "projectId", required = false) Long projectId,
            @RequestParam(value = "type", required = false) String fileType,
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size
    ) {
        log.debug("List files request: sessionId={}, userId={}, projectId={}, type={}", 
                sessionId, userId, projectId, fileType);

        Page<WorkspaceFile> result;

        if (projectId != null) {
            result = fileService.listFilesForProject(projectId, page, size);
        } else if (userId != null) {
            if (fileType != null) {
                WorkspaceFile.FileType type = WorkspaceFile.FileType.valueOf(fileType.toUpperCase());
                result = fileService.listFilesByTypeForSession(userId, type, page, size);
            } else {
                result = fileService.listFilesForUser(userId, page, size);
            }
        } else if (sessionId != null) {
            if (fileType != null) {
                WorkspaceFile.FileType type = WorkspaceFile.FileType.valueOf(fileType.toUpperCase());
                result = fileService.listFilesByTypeForSession(sessionId, type, page, size);
            } else {
                result = fileService.listFilesForSession(sessionId, page, size);
            }
        } else {
            return ResponseEntity.badRequest().build();
        }

        return ResponseEntity.ok(PageResponse.from(result));
    }

    /**
     * Search files by name.
     */
    @GetMapping("/search")
    public ResponseEntity<PageResponse<WorkspaceFile>> searchFiles(
            @RequestParam String q,
            @RequestHeader(value = "X-Session-ID", required = false) String sessionId,
            @RequestParam(value = "userId", required = false) String userId,
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "20") int size
    ) {
        Page<WorkspaceFile> result;

        if (userId != null) {
            result = fileService.searchFilesForUser(userId, q, page, size);
        } else if (sessionId != null) {
            result = fileService.searchFilesForSession(sessionId, q, page, size);
        } else {
            return ResponseEntity.badRequest().build();
        }

        return ResponseEntity.ok(PageResponse.from(result));
    }

    // ============================================
    // File Deletion
    // ============================================

    /**
     * Delete a file.
     */
    @DeleteMapping("/{fileUuid}")
    public ResponseEntity<Void> deleteFile(
            @PathVariable String fileUuid,
            @RequestHeader(value = "X-Session-ID", required = false) String sessionId,
            @RequestParam(value = "userId", required = false) String userId
    ) {
        log.info("File delete request: uuid={}, sessionId={}, userId={}", fileUuid, sessionId, userId);

        try {
            fileService.deleteFile(fileUuid, sessionId, userId);
            return ResponseEntity.noContent().build();
        } catch (IllegalArgumentException e) {
            return ResponseEntity.notFound().build();
        } catch (IllegalStateException e) {
            return ResponseEntity.status(HttpStatus.FORBIDDEN).build();
        }
    }

    /**
     * Delete all files for session (for cleanup).
     */
    @DeleteMapping("/session/{sessionId}")
    public ResponseEntity<Void> deleteAllSessionFiles(@PathVariable String sessionId) {
        log.info("Delete all files for session: {}", sessionId);
        fileService.deleteAllFilesForSession(sessionId);
        return ResponseEntity.noContent().build();
    }

    // ============================================
    // File Migration
    // ============================================

    /**
     * Transfer session files to user (when anonymous user logs in).
     */
    @PostMapping("/transfer")
    public ResponseEntity<Map<String, Object>> transferFiles(
            @RequestParam String sessionId,
            @RequestParam String userId
    ) {
        log.info("Transfer files from session {} to user {}", sessionId, userId);

        int count = fileService.transferSessionFilesToUser(sessionId, userId);
        
        return ResponseEntity.ok(Map.of(
                "transferred", count,
                "sessionId", sessionId,
                "userId", userId
        ));
    }

    // ============================================
    // Storage Statistics
    // ============================================

    /**
     * Get storage statistics.
     */
    @GetMapping("/stats")
    public ResponseEntity<StorageStats> getStorageStats(
            @RequestHeader(value = "X-Session-ID", required = false) String sessionId,
            @RequestParam(value = "userId", required = false) String userId
    ) {
        StorageStats stats;

        if (userId != null) {
            stats = fileService.getStorageStatsForUser(userId);
        } else if (sessionId != null) {
            stats = fileService.getStorageStatsForSession(sessionId);
        } else {
            return ResponseEntity.badRequest().build();
        }

        return ResponseEntity.ok(stats);
    }

    // ============================================
    // Admin Operations (Internal)
    // ============================================

    /**
     * Cleanup expired files (should be called by scheduler).
     */
    @PostMapping("/admin/cleanup/expired")
    public ResponseEntity<Map<String, Object>> cleanupExpiredFiles() {
        int count = fileService.cleanupExpiredFiles();
        return ResponseEntity.ok(Map.of("markedForDeletion", count));
    }

    /**
     * Cleanup old session files.
     */
    @PostMapping("/admin/cleanup/sessions")
    public ResponseEntity<Map<String, Object>> cleanupOldSessionFiles(
            @RequestParam(defaultValue = "48") int olderThanHours
    ) {
        int count = fileService.cleanupOldSessionFiles(olderThanHours);
        return ResponseEntity.ok(Map.of("markedForDeletion", count));
    }

    /**
     * Purge deleted files permanently.
     */
    @PostMapping("/admin/purge")
    public ResponseEntity<Map<String, Object>> purgeDeletedFiles() {
        int count = fileService.purgeDeletedFiles();
        return ResponseEntity.ok(Map.of("purged", count));
    }

    // ============================================
    // Health Check
    // ============================================

    /**
     * Health check endpoint.
     */
    @GetMapping("/health")
    public ResponseEntity<Map<String, Object>> healthCheck() {
        return ResponseEntity.ok(Map.of(
                "status", "available",
                "features", Map.of(
                        "upload", true,
                        "download", true,
                        "delete", true,
                        "search", true,
                        "transfer", true
                )
        ));
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/AiJobDto.java

```java
package com.newsinsight.collector.dto;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.time.LocalDateTime;
import java.util.List;

/**
 * DTO for AI Job response (includes sub-tasks status).
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class AiJobDto {
    private String jobId;
    private String topic;
    private String baseUrl;
    private String overallStatus;
    private List<AiSubTaskDto> subTasks;
    private int totalTasks;
    private int completedTasks;
    private int failedTasks;
    private String errorMessage;
    private LocalDateTime createdAt;
    private LocalDateTime updatedAt;
    private LocalDateTime completedAt;
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/AiRequestMessage.java

```java
package com.newsinsight.collector.dto;

import java.util.Map;

public record AiRequestMessage(
        String requestId,
        String type,
        String query,
        String window,
        String message,
        Map<String, Object> context,
        String providerId,
        String modelId,
        String agentRole,
        String outputSchema,
        String source
) {
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/AiResponseMessage.java

```java
package com.newsinsight.collector.dto;

import java.util.Map;

public record AiResponseMessage(
        String requestId,
        String status,
        String completedAt,
        String providerId,
        String modelId,
        String text,
        Map<String, Object> raw
) {
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/AiSubTaskDto.java

```java
package com.newsinsight.collector.dto;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.time.LocalDateTime;

/**
 * DTO for AI Sub-Task response.
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class AiSubTaskDto {
    private String subTaskId;
    private String jobId;
    private String providerId;
    private String taskType;
    private String status;
    private String resultJson;
    private String errorMessage;
    private int retryCount;
    private LocalDateTime createdAt;
    private LocalDateTime updatedAt;
    private LocalDateTime completedAt;
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/AiTaskCallbackRequest.java

```java
package com.newsinsight.collector.dto;

import java.util.List;

/**
 * Callback request payload from AI worker/n8n.
 * Received at /api/v1/ai/callback endpoint.
 */
public record AiTaskCallbackRequest(
        /**
         * Parent job ID
         */
        String jobId,

        /**
         * Individual sub-task ID
         */
        String subTaskId,

        /**
         * AI provider identifier
         */
        String providerId,

        /**
         * Task completion status (COMPLETED, FAILED, etc.)
         */
        String status,

        /**
         * JSON result data from the AI task
         */
        String resultJson,

        /**
         * Error message if task failed
         */
        String errorMessage,

        /**
         * Callback authentication token
         */
        String callbackToken,

        /**
         * Evidence list (for DEEP_READER provider)
         */
        List<EvidenceDto> evidence
) {
    /**
     * Check if the callback indicates success
     */
    public boolean isSuccess() {
        return "COMPLETED".equalsIgnoreCase(status) || "completed".equalsIgnoreCase(status);
    }

    /**
     * Check if the callback indicates failure
     */
    public boolean isFailed() {
        return "FAILED".equalsIgnoreCase(status) || "failed".equalsIgnoreCase(status);
    }

    /**
     * Create a builder for AiTaskCallbackRequest
     */
    public static Builder builder() {
        return new Builder();
    }

    public static class Builder {
        private String jobId;
        private String subTaskId;
        private String providerId;
        private String status;
        private String resultJson;
        private String errorMessage;
        private String callbackToken;
        private List<EvidenceDto> evidence;

        public Builder jobId(String jobId) {
            this.jobId = jobId;
            return this;
        }

        public Builder subTaskId(String subTaskId) {
            this.subTaskId = subTaskId;
            return this;
        }

        public Builder providerId(String providerId) {
            this.providerId = providerId;
            return this;
        }

        public Builder status(String status) {
            this.status = status;
            return this;
        }

        public Builder resultJson(String resultJson) {
            this.resultJson = resultJson;
            return this;
        }

        public Builder errorMessage(String errorMessage) {
            this.errorMessage = errorMessage;
            return this;
        }

        public Builder callbackToken(String callbackToken) {
            this.callbackToken = callbackToken;
            return this;
        }

        public Builder evidence(List<EvidenceDto> evidence) {
            this.evidence = evidence;
            return this;
        }

        public AiTaskCallbackRequest build() {
            return new AiTaskCallbackRequest(
                    jobId, subTaskId, providerId, status,
                    resultJson, errorMessage, callbackToken, evidence
            );
        }
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/AiTaskRequestMessage.java

```java
package com.newsinsight.collector.dto;

import java.time.LocalDateTime;
import java.util.Map;

/**
 * Kafka message for AI task requests.
 * Sent to ai.tasks.requests topic for worker/n8n processing.
 */
public record AiTaskRequestMessage(
        /**
         * Parent job ID
         */
        String jobId,

        /**
         * Individual sub-task ID
         */
        String subTaskId,

        /**
         * AI provider identifier (UNIVERSAL_AGENT, DEEP_READER, SCOUT, etc.)
         */
        String providerId,

        /**
         * Type of task to perform
         */
        String taskType,

        /**
         * Search topic/query
         */
        String topic,

        /**
         * Base URL for crawling (optional)
         */
        String baseUrl,

        /**
         * Additional payload data for the provider
         */
        Map<String, Object> payload,

        /**
         * URL for callback after task completion
         */
        String callbackUrl,

        /**
         * Token for callback authentication
         */
        String callbackToken,

        /**
         * Message creation timestamp
         */
        LocalDateTime createdAt
) {
    /**
     * Create a builder for AiTaskRequestMessage
     */
    public static Builder builder() {
        return new Builder();
    }

    public static class Builder {
        private String jobId;
        private String subTaskId;
        private String providerId;
        private String taskType;
        private String topic;
        private String baseUrl;
        private Map<String, Object> payload;
        private String callbackUrl;
        private String callbackToken;
        private LocalDateTime createdAt;

        public Builder jobId(String jobId) {
            this.jobId = jobId;
            return this;
        }

        public Builder subTaskId(String subTaskId) {
            this.subTaskId = subTaskId;
            return this;
        }

        public Builder providerId(String providerId) {
            this.providerId = providerId;
            return this;
        }

        public Builder taskType(String taskType) {
            this.taskType = taskType;
            return this;
        }

        public Builder topic(String topic) {
            this.topic = topic;
            return this;
        }

        public Builder baseUrl(String baseUrl) {
            this.baseUrl = baseUrl;
            return this;
        }

        public Builder payload(Map<String, Object> payload) {
            this.payload = payload;
            return this;
        }

        public Builder callbackUrl(String callbackUrl) {
            this.callbackUrl = callbackUrl;
            return this;
        }

        public Builder callbackToken(String callbackToken) {
            this.callbackToken = callbackToken;
            return this;
        }

        public Builder createdAt(LocalDateTime createdAt) {
            this.createdAt = createdAt;
            return this;
        }

        public AiTaskRequestMessage build() {
            return new AiTaskRequestMessage(
                    jobId, subTaskId, providerId, taskType, topic, baseUrl,
                    payload, callbackUrl, callbackToken,
                    createdAt != null ? createdAt : LocalDateTime.now()
            );
        }
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/AnalysisResponseDto.java

```java
package com.newsinsight.collector.dto;

import com.fasterxml.jackson.annotation.JsonProperty;

import java.util.List;

public record AnalysisResponseDto(
        String query,
        String window,
        @JsonProperty("article_count") long articleCount,
        SentimentDataDto sentiments,
        @JsonProperty("top_keywords") List<KeywordDataDto> topKeywords,
        @JsonProperty("analyzed_at") String analyzedAt
) {}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/ArticleDto.java

```java
package com.newsinsight.collector.dto;

import com.fasterxml.jackson.annotation.JsonProperty;

public record ArticleDto(
        String id,
        String title,
        String source,
        @JsonProperty("published_at") String publishedAt,
        String url,
        String snippet,
        String content  // 전체 본문 (export/저장용)
) {}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/ArticleWithAnalysisDto.java

```java
package com.newsinsight.collector.dto;

import com.newsinsight.collector.entity.analysis.ArticleAnalysis;
import com.newsinsight.collector.entity.analysis.ArticleDiscussion;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Map;

/**
 * 검색 결과에 분석 정보를 포함한 DTO.
 * 
 * 프론트엔드가 검색 결과를 표시할 때 사용.
 * 분석이 완료되지 않은 경우 null로 표시하여 skeleton UI 렌더링 유도.
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class ArticleWithAnalysisDto {

    // ========== 기본 기사 정보 ==========
    private Long id;
    private String title;
    private String content;
    private String url;
    private String source;
    private LocalDateTime publishedDate;
    private LocalDateTime collectedAt;

    // ========== 분석 상태 ==========
    /**
     * 분석 완료 여부 (true면 analysis 필드 사용 가능)
     */
    private Boolean analyzed;
    
    /**
     * 분석 진행 상태 (pending, partial, complete)
     */
    private String analysisStatus;

    // ========== 요약 정보 (간략 표시용) ==========
    /**
     * AI 생성 요약 (1-2문장)
     */
    private String summary;

    // ========== 신뢰도 배지 ==========
    /**
     * 신뢰도 점수 (0-100)
     */
    private Double reliabilityScore;
    
    /**
     * 신뢰도 등급 (high, medium, low)
     */
    private String reliabilityGrade;
    
    /**
     * 신뢰도 색상 코드 (green, yellow, red)
     */
    private String reliabilityColor;

    // ========== 감정 분석 ==========
    /**
     * 감정 레이블 (positive, negative, neutral)
     */
    private String sentimentLabel;
    
    /**
     * 감정 점수 (-1 ~ 1)
     */
    private Double sentimentScore;
    
    /**
     * 감정 분포 (긍정/부정/중립 비율)
     */
    private Map<String, Double> sentimentDistribution;

    // ========== 편향도 ==========
    /**
     * 편향 레이블 (left, right, center 등)
     */
    private String biasLabel;
    
    /**
     * 편향 점수 (-1 ~ 1)
     */
    private Double biasScore;

    // ========== 팩트체크 ==========
    /**
     * 팩트체크 상태 (verified, suspicious, conflicting, unverified)
     */
    private String factcheckStatus;
    
    /**
     * 허위정보 위험도 (low, mid, high)
     */
    private String misinfoRisk;

    // ========== 위험 태그 ==========
    /**
     * 경고 태그 목록 (clickbait, sensational 등)
     */
    private List<String> riskTags;

    // ========== 토픽/키워드 ==========
    /**
     * 주요 토픽
     */
    private List<String> topics;

    // ========== 커뮤니티 여론 요약 ==========
    /**
     * 여론 있음 여부
     */
    private Boolean hasDiscussion;
    
    /**
     * 전체 댓글 수
     */
    private Integer totalCommentCount;
    
    /**
     * 전체 여론 감정 (positive, negative, neutral, mixed)
     */
    private String discussionSentiment;
    
    /**
     * 여론 감정 분포
     */
    private Map<String, Double> discussionSentimentDistribution;
    
    /**
     * 여론 요약 문장
     */
    private String discussionSummary;

    // ========== 정적 팩토리 메서드 ==========

    /**
     * 분석 결과가 없는 기사용
     */
    public static ArticleWithAnalysisDto fromArticleOnly(
            Long id, String title, String content, String url, 
            String source, LocalDateTime publishedDate, LocalDateTime collectedAt
    ) {
        return ArticleWithAnalysisDto.builder()
                .id(id)
                .title(title)
                .content(content)
                .url(url)
                .source(source)
                .publishedDate(publishedDate)
                .collectedAt(collectedAt)
                .analyzed(false)
                .analysisStatus("pending")
                .build();
    }

    /**
     * 분석 결과 포함
     */
    public static ArticleWithAnalysisDto fromArticleWithAnalysis(
            Long id, String title, String content, String url,
            String source, LocalDateTime publishedDate, LocalDateTime collectedAt,
            ArticleAnalysis analysis, ArticleDiscussion discussion
    ) {
        ArticleWithAnalysisDtoBuilder builder = ArticleWithAnalysisDto.builder()
                .id(id)
                .title(title)
                .content(content)
                .url(url)
                .source(source)
                .publishedDate(publishedDate)
                .collectedAt(collectedAt);

        if (analysis != null) {
            builder.analyzed(true)
                    .analysisStatus(analysis.getFullyAnalyzed() ? "complete" : "partial")
                    .summary(analysis.getSummary())
                    .reliabilityScore(analysis.getReliabilityScore())
                    .reliabilityGrade(analysis.getReliabilityGrade())
                    .reliabilityColor(analysis.getReliabilityColor())
                    .sentimentLabel(analysis.getSentimentLabel())
                    .sentimentScore(analysis.getSentimentScore())
                    .sentimentDistribution(analysis.getSentimentDistribution())
                    .biasLabel(analysis.getBiasLabel())
                    .biasScore(analysis.getBiasScore())
                    .factcheckStatus(analysis.getFactcheckStatus())
                    .misinfoRisk(analysis.getMisinfoRisk())
                    .riskTags(analysis.getRiskTags())
                    .topics(analysis.getTopics());
        } else {
            builder.analyzed(false)
                    .analysisStatus("pending");
        }

        if (discussion != null) {
            builder.hasDiscussion(true)
                    .totalCommentCount(discussion.getTotalCommentCount())
                    .discussionSentiment(discussion.getOverallSentiment())
                    .discussionSentimentDistribution(discussion.getSentimentDistribution())
                    .discussionSummary(discussion.getSentimentSummary());
        } else {
            builder.hasDiscussion(false);
        }

        return builder.build();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/ArticlesResponseDto.java

```java
package com.newsinsight.collector.dto;

import java.util.List;

public record ArticlesResponseDto(
        String query,
        List<ArticleDto> articles,
        long total
) {}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/BrowserAgentConfigDto.java

```java
package com.newsinsight.collector.dto;

import com.newsinsight.collector.entity.BrowserAgentConfig;
import com.newsinsight.collector.entity.BrowserAgentPolicy;

/**
 * DTO for browser agent configuration in API requests/responses.
 */
public record BrowserAgentConfigDto(
        Integer maxDepth,
        Integer maxPages,
        Integer budgetSeconds,
        String policy,
        String focusKeywords,
        String customPrompt,
        Boolean captureScreenshots,
        Boolean extractStructured,
        String excludedDomains
) {
    /**
     * Convert to entity.
     */
    public BrowserAgentConfig toEntity() {
        return BrowserAgentConfig.builder()
                .maxDepth(maxDepth != null ? maxDepth : 2)
                .maxPages(maxPages != null ? maxPages : 50)
                .budgetSeconds(budgetSeconds != null ? budgetSeconds : 300)
                .policy(policy != null ? BrowserAgentPolicy.fromValue(policy) : BrowserAgentPolicy.FOCUSED_TOPIC)
                .focusKeywords(focusKeywords)
                .customPrompt(customPrompt)
                .captureScreenshots(captureScreenshots != null ? captureScreenshots : false)
                .extractStructured(extractStructured != null ? extractStructured : true)
                .excludedDomains(excludedDomains)
                .build();
    }

    /**
     * Create from entity.
     */
    public static BrowserAgentConfigDto fromEntity(BrowserAgentConfig config) {
        if (config == null) {
            return null;
        }
        return new BrowserAgentConfigDto(
                config.getMaxDepth(),
                config.getMaxPages(),
                config.getBudgetSeconds(),
                config.getPolicy() != null ? config.getPolicy().getValue() : null,
                config.getFocusKeywords(),
                config.getCustomPrompt(),
                config.getCaptureScreenshots(),
                config.getExtractStructured(),
                config.getExcludedDomains()
        );
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/BrowserTaskMessage.java

```java
package com.newsinsight.collector.dto;

import com.fasterxml.jackson.annotation.JsonFormat;
import lombok.Builder;

import java.time.LocalDateTime;
import java.util.Map;

/**
 * Kafka message for browser-based autonomous crawling tasks.
 * Consumed by autonomous-crawler-service (Python/Browser-use).
 */
@Builder
public record BrowserTaskMessage(
        /**
         * Unique job ID for tracking.
         */
        Long jobId,
        
        /**
         * Data source ID.
         */
        Long sourceId,
        
        /**
         * Source name for logging/display.
         */
        String sourceName,
        
        /**
         * Seed URL to start exploration from.
         */
        String seedUrl,
        
        /**
         * Maximum link traversal depth.
         */
        Integer maxDepth,
        
        /**
         * Maximum pages to visit.
         */
        Integer maxPages,
        
        /**
         * Time budget in seconds.
         */
        Integer budgetSeconds,
        
        /**
         * Exploration policy (focused_topic, domain_wide, news_only, etc.)
         */
        String policy,
        
        /**
         * Focus keywords for FOCUSED_TOPIC policy.
         */
        String focusKeywords,
        
        /**
         * Custom prompt/instructions for AI agent.
         */
        String customPrompt,
        
        /**
         * Whether to capture screenshots.
         */
        Boolean captureScreenshots,
        
        /**
         * Whether to extract structured data.
         */
        Boolean extractStructured,
        
        /**
         * Domains to exclude.
         */
        String excludedDomains,
        
        /**
         * Callback URL for session completion notification.
         */
        String callbackUrl,
        
        /**
         * Callback authentication token.
         */
        String callbackToken,
        
        /**
         * Additional metadata.
         */
        Map<String, Object> metadata,
        
        /**
         * Task creation timestamp.
         * Serialized as ISO-8601 string for Python compatibility.
         */
        @JsonFormat(shape = JsonFormat.Shape.STRING, pattern = "yyyy-MM-dd'T'HH:mm:ss")
        LocalDateTime createdAt
) {
    public BrowserTaskMessage {
        createdAt = createdAt != null ? createdAt : LocalDateTime.now();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/ClaimExtractionRequest.java

```java
package com.newsinsight.collector.dto;

import jakarta.validation.constraints.NotBlank;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

/**
 * Request DTO for extracting claims from a URL
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class ClaimExtractionRequest {
    
    @NotBlank(message = "URL is required")
    private String url;
    
    /** Optional: Maximum number of claims to extract */
    private Integer maxClaims;
    
    /** Optional: Minimum confidence threshold (0.0 - 1.0) */
    private Double minConfidence;
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/ClaimExtractionResponse.java

```java
package com.newsinsight.collector.dto;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.util.List;

/**
 * Response DTO for claim extraction
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class ClaimExtractionResponse {
    
    /** The URL that was analyzed */
    private String url;
    
    /** Title of the page */
    private String pageTitle;
    
    /** List of extracted claims */
    private List<ExtractedClaim> claims;
    
    /** Processing time in milliseconds */
    private Long processingTimeMs;
    
    /** Source of extraction (e.g., "crawl4ai", "direct", "browser-use") */
    private String extractionSource;
    
    /** Any warning or info messages */
    private String message;
    
    /**
     * Individual claim extracted from the content
     */
    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class ExtractedClaim {
        /** Unique identifier for the claim */
        private String id;
        
        /** The claim text */
        private String text;
        
        /** Confidence score (0.0 - 1.0) */
        private Double confidence;
        
        /** Context where the claim was found */
        private String context;
        
        /** Type of claim: factual, opinion, prediction, etc. */
        private String claimType;
        
        /** Whether this claim is verifiable */
        private Boolean verifiable;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/CollectedDataDTO.java

```java
package com.newsinsight.collector.dto;

import java.time.LocalDateTime;
import java.util.Map;

public record CollectedDataDTO(
        Long id,
        Long sourceId,
        String title,
        String content,
        String url,
        LocalDateTime publishedDate,
        LocalDateTime collectedAt,
        String contentHash,
        Map<String, Object> metadata,
        Boolean processed
) {
    public CollectedDataDTO {
        /**
         * Map.copyOf()는 원본 맵의 '읽기 전용 복사본'을 만듭니다.
         * 이로써 이 record는 외부의 어떤 변경에도 영향을 받지 않는
         * 완전한 불변 객체로써 동작합니다.
         */
        metadata = metadata == null ? Map.of() : Map.copyOf(metadata);
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/CollectionJobDTO.java

```java
package com.newsinsight.collector.dto;

import com.newsinsight.collector.entity.CollectionJob.JobStatus;

import java.time.LocalDateTime;

public record CollectionJobDTO(
        Long id,
        Long sourceId,
        JobStatus status,
        LocalDateTime startedAt,
        LocalDateTime completedAt,
        Integer itemsCollected,
        String errorMessage,
        LocalDateTime createdAt
) {}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/CollectionRequest.java

```java
package com.newsinsight.collector.dto;

import java.util.List;

public record CollectionRequest(List<Long> sourceIds, boolean force) {
    public CollectionRequest {
        sourceIds = sourceIds == null ? List.of() : List.copyOf(sourceIds);
    }

    public CollectionRequest(List<Long> sourceIds) {
        this(sourceIds, false);
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/CollectionResponse.java

```java
package com.newsinsight.collector.dto;

import java.time.LocalDateTime;
import java.util.List;

public record CollectionResponse(
        String message,
        List<CollectionJobDTO> jobs,
        Integer totalJobsStarted,
        LocalDateTime timestamp
) {
    public CollectionResponse {
        jobs = jobs == null ? List.of() : List.copyOf(jobs);
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/CollectionStatsDTO.java

```java
package com.newsinsight.collector.dto;

import java.time.LocalDateTime;

public record CollectionStatsDTO(
        Long totalSources,
        Long activeSources,
        Long totalItemsCollected,
        Long itemsCollectedToday,
        LocalDateTime lastCollection
) {}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/CrawlCommandMessage.java

```java
package com.newsinsight.collector.dto;

public record CrawlCommandMessage(
        Long jobId,
        Long sourceId,
        String sourceType,
        String url,
        String sourceName
) {
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/CrawlResultMessage.java

```java
package com.newsinsight.collector.dto;

public record CrawlResultMessage(
        Long jobId,
        Long sourceId,
        String title,
        String content,
        String url,
        String publishedAt,
        String metadataJson
) {
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/CrawledPage.java

```java
package com.newsinsight.collector.dto;

import java.util.List;

/**
 * DTO representing a crawled web page
 * Used by IntegratedCrawlerService to pass crawl results
 */
public record CrawledPage(
        String url,
        String title,
        String content,
        String source,  // e.g., "crawl4ai", "browser-use", "direct"
        List<String> links
) {
    /**
     * Create a CrawledPage with no extracted links
     */
    public static CrawledPage of(String url, String title, String content, String source) {
        return new CrawledPage(url, title, content, source, List.of());
    }

    /**
     * Check if this page has valid content
     */
    public boolean hasContent() {
        return content != null && !content.isBlank();
    }

    /**
     * Get a truncated snippet of the content
     */
    public String getSnippet(int maxLength) {
        if (content == null) return "";
        if (content.length() <= maxLength) return content;
        return content.substring(0, maxLength) + "...";
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/DashboardEventDto.java

```java
package com.newsinsight.collector.dto;

import com.fasterxml.jackson.annotation.JsonInclude;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.time.Instant;
import java.util.Map;

/**
 * 대시보드 실시간 이벤트 DTO.
 * SSE를 통해 클라이언트에 전송되는 이벤트 데이터를 담습니다.
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
@JsonInclude(JsonInclude.Include.NON_NULL)
public class DashboardEventDto {

    /**
     * 이벤트 타입
     */
    private EventType eventType;

    /**
     * 이벤트 발생 시각
     */
    @Builder.Default
    private Instant timestamp = Instant.now();

    /**
     * 이벤트 메시지
     */
    private String message;

    /**
     * 추가 데이터 (이벤트 타입에 따라 다름)
     */
    private Map<String, Object> data;

    /**
     * 이벤트 타입 열거형
     */
    public enum EventType {
        HEARTBEAT,      // 연결 유지용 하트비트
        NEW_DATA,       // 새로운 데이터 수집됨
        SOURCE_UPDATED, // 소스 상태 변경
        STATS_UPDATED,  // 통계 갱신
        COLLECTION_STARTED,  // 수집 시작
        COLLECTION_COMPLETED, // 수집 완료
        ERROR           // 에러 발생
    }

    /**
     * 하트비트 이벤트 생성
     */
    public static DashboardEventDto heartbeat() {
        return DashboardEventDto.builder()
                .eventType(EventType.HEARTBEAT)
                .message("Connection alive")
                .build();
    }

    /**
     * 새 데이터 수집 이벤트 생성
     */
    public static DashboardEventDto newData(String message, Map<String, Object> data) {
        return DashboardEventDto.builder()
                .eventType(EventType.NEW_DATA)
                .message(message)
                .data(data)
                .build();
    }

    /**
     * 통계 갱신 이벤트 생성
     */
    public static DashboardEventDto statsUpdated(Map<String, Object> stats) {
        return DashboardEventDto.builder()
                .eventType(EventType.STATS_UPDATED)
                .message("Statistics updated")
                .data(stats)
                .build();
    }

    /**
     * 소스 업데이트 이벤트 생성
     */
    public static DashboardEventDto sourceUpdated(String sourceId, String status) {
        return DashboardEventDto.builder()
                .eventType(EventType.SOURCE_UPDATED)
                .message("Source " + sourceId + " status changed to " + status)
                .data(Map.of("sourceId", sourceId, "status", status))
                .build();
    }

    /**
     * 에러 이벤트 생성
     */
    public static DashboardEventDto error(String message) {
        return DashboardEventDto.builder()
                .eventType(EventType.ERROR)
                .message(message)
                .build();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/DataSourceCreateRequest.java

```java
package com.newsinsight.collector.dto;

import com.newsinsight.collector.entity.SourceType;
import jakarta.validation.constraints.Min;
import jakarta.validation.constraints.NotBlank;
import jakarta.validation.constraints.NotNull;

import java.util.Map;

public record DataSourceCreateRequest(
        @NotBlank(message = "Name is required") String name,
        @NotBlank(message = "URL is required") String url,
        @NotNull(message = "Source type is required") SourceType sourceType,
        @Min(value = 60, message = "Collection frequency must be at least 60 seconds") Integer collectionFrequency,
        Map<String, Object> metadata,
        BrowserAgentConfigDto browserAgentConfig
) {
    public DataSourceCreateRequest {
        collectionFrequency = collectionFrequency == null ? 3600 : collectionFrequency;
        metadata = metadata == null ? Map.of() : Map.copyOf(metadata);
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/DataSourceDTO.java

```java
package com.newsinsight.collector.dto;

import com.newsinsight.collector.entity.SourceType;

import java.time.LocalDateTime;
import java.util.Map;

public record DataSourceDTO(
        Long id,
        String name,
        String url,
        SourceType sourceType,
        Boolean isActive,
        LocalDateTime lastCollected,
        Integer collectionFrequency,
        Map<String, Object> metadata,
        LocalDateTime createdAt,
        LocalDateTime updatedAt,
        BrowserAgentConfigDto browserAgentConfig
) {
    public DataSourceDTO {
        metadata = metadata == null ? Map.of() : Map.copyOf(metadata);
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/DataSourceUpdateRequest.java

```java
package com.newsinsight.collector.dto;

import jakarta.validation.constraints.Min;

import java.util.Map;

public record DataSourceUpdateRequest(
        String name,
        String url,
        Boolean isActive,
        @Min(value = 60, message = "Collection frequency must be at least 60 seconds") Integer collectionFrequency,
        Map<String, Object> metadata,
        BrowserAgentConfigDto browserAgentConfig
) {
    public DataSourceUpdateRequest {
        metadata = metadata == null ? Map.of() : Map.copyOf(metadata);
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/DeepSearchCallbackDto.java

```java
package com.newsinsight.collector.dto;

import com.fasterxml.jackson.annotation.JsonProperty;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.util.List;

/**
 * DTO for n8n callback payload
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class DeepSearchCallbackDto {
    
    @JsonProperty("job_id")
    private String jobId;
    
    private String status;
    
    private String topic;
    
    @JsonProperty("base_url")
    private String baseUrl;
    
    private List<CallbackEvidence> evidence;
    
    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class CallbackEvidence {
        private String url;
        private String title;
        private String stance;
        private String snippet;
        private String source;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/DeepSearchJobDto.java

```java
package com.newsinsight.collector.dto;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.time.LocalDateTime;

/**
 * DTO for deep search job status
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class DeepSearchJobDto {
    private String jobId;
    private String topic;
    private String baseUrl;
    private String status;
    private Integer evidenceCount;
    private String errorMessage;
    private String failureReason;      // Code like "timeout_job_overall"
    private String failureCategory;     // High-level category like "timeout", "network", "service"
    private LocalDateTime createdAt;
    private LocalDateTime completedAt;
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/DeepSearchRequest.java

```java
package com.newsinsight.collector.dto;

import jakarta.validation.constraints.NotBlank;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

/**
 * Request DTO for starting a deep search
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class DeepSearchRequest {
    
    @NotBlank(message = "Topic is required")
    private String topic;
    
    /**
     * Optional base URL to start crawling from.
     * If not provided, a default news aggregator will be used.
     */
    private String baseUrl;
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/DeepSearchResultDto.java

```java
package com.newsinsight.collector.dto;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.time.LocalDateTime;
import java.util.List;

/**
 * DTO for deep search result including evidence
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class DeepSearchResultDto {
    private String jobId;
    private String topic;
    private String baseUrl;
    private String status;
    private Integer evidenceCount;
    private List<EvidenceDto> evidence;
    private StanceDistributionDto stanceDistribution;
    private LocalDateTime createdAt;
    private LocalDateTime completedAt;
    private String errorMessage;
    private String failureReason;      // Code like "timeout_job_overall"
    private String failureCategory;     // High-level category like "timeout", "network", "service"
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/EvidenceDto.java

```java
package com.newsinsight.collector.dto;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

/**
 * DTO for evidence item from deep search
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class EvidenceDto {
    private Long id;
    private String url;
    private String title;
    private String stance;  // pro, con, neutral
    private String snippet;
    private String source;
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/KeywordDataDto.java

```java
package com.newsinsight.collector.dto;

public record KeywordDataDto(String word, double score) {}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/PageResponse.java

```java
package com.newsinsight.collector.dto;

import java.util.List;
import java.util.Objects;

import org.springframework.data.domain.Page;

public record PageResponse<T>(
        List<T> content,
        int page,
        int size,
        long totalElements,
        int totalPages,
        boolean first,
        boolean last,
        boolean hasNext,
        boolean hasPrevious
) {
    public PageResponse {
        content = content == null ? List.of() : List.copyOf(content);
    }

    public static <T> PageResponse<T> from(Page<T> page) {
        Objects.requireNonNull(page, "page must not be null");
        return new PageResponse<>(
                page.getContent(),
                page.getNumber(),
                page.getSize(),
                page.getTotalElements(),
                page.getTotalPages(),
                page.isFirst(),
                page.isLast(),
                page.hasNext(),
                page.hasPrevious()
        );
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/SearchHistoryDto.java

```java
package com.newsinsight.collector.dto;

import com.newsinsight.collector.entity.search.SearchHistory;
import com.newsinsight.collector.entity.search.SearchType;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Map;

/**
 * DTO for SearchHistory API responses.
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class SearchHistoryDto {

    private Long id;
    private String externalId;
    private SearchType searchType;
    private String query;
    private String timeWindow;
    private String userId;
    private String sessionId;
    private Long parentSearchId;
    private Integer depthLevel;
    private Integer resultCount;
    private List<Map<String, Object>> results;
    private Map<String, Object> aiSummary;
    private List<String> discoveredUrls;
    private List<Map<String, Object>> factCheckResults;
    private Double credibilityScore;
    private Map<String, Object> stanceDistribution;
    private Map<String, Object> metadata;
    private Boolean bookmarked;
    private List<String> tags;
    private String notes;
    private Long durationMs;
    private String errorMessage;
    private Boolean success;
    private LocalDateTime createdAt;
    private LocalDateTime updatedAt;

    /**
     * Convert entity to DTO.
     */
    public static SearchHistoryDto fromEntity(SearchHistory entity) {
        return SearchHistoryDto.builder()
                .id(entity.getId())
                .externalId(entity.getExternalId())
                .searchType(entity.getSearchType())
                .query(entity.getQuery())
                .timeWindow(entity.getTimeWindow())
                .userId(entity.getUserId())
                .sessionId(entity.getSessionId())
                .parentSearchId(entity.getParentSearchId())
                .depthLevel(entity.getDepthLevel())
                .resultCount(entity.getResultCountSafe())
                .results(entity.getResults())
                .aiSummary(entity.getAiSummary())
                .discoveredUrls(entity.getDiscoveredUrls())
                .factCheckResults(entity.getFactCheckResults())
                .credibilityScore(entity.getCredibilityScore())
                .stanceDistribution(entity.getStanceDistribution())
                .metadata(entity.getMetadata())
                .bookmarked(entity.getBookmarked())
                .tags(entity.getTags())
                .notes(entity.getNotes())
                .durationMs(entity.getDurationMs())
                .errorMessage(entity.getErrorMessage())
                .success(entity.getSuccess())
                .createdAt(entity.getCreatedAt())
                .updatedAt(entity.getUpdatedAt())
                .build();
    }

    /**
     * Convert DTO to message for Kafka.
     */
    public SearchHistoryMessage toMessage() {
        return SearchHistoryMessage.builder()
                .externalId(this.externalId)
                .searchType(this.searchType)
                .query(this.query)
                .timeWindow(this.timeWindow)
                .userId(this.userId)
                .sessionId(this.sessionId)
                .parentSearchId(this.parentSearchId)
                .depthLevel(this.depthLevel)
                .resultCount(this.resultCount)
                .results(this.results)
                .aiSummary(this.aiSummary)
                .discoveredUrls(this.discoveredUrls)
                .factCheckResults(this.factCheckResults)
                .credibilityScore(this.credibilityScore)
                .stanceDistribution(this.stanceDistribution)
                .metadata(this.metadata)
                .durationMs(this.durationMs)
                .errorMessage(this.errorMessage)
                .success(this.success)
                .timestamp(System.currentTimeMillis())
                .build();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/SearchHistoryMessage.java

```java
package com.newsinsight.collector.dto;

import com.newsinsight.collector.entity.search.SearchType;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.util.List;
import java.util.Map;

/**
 * Kafka message DTO for search history events.
 * Used for asynchronous search result persistence via Kafka.
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class SearchHistoryMessage {

    /**
     * External reference ID (e.g., jobId)
     */
    private String externalId;

    /**
     * Type of search performed
     */
    private SearchType searchType;

    /**
     * The search query or topic
     */
    private String query;

    /**
     * Time window for search (e.g., 1d, 7d, 30d)
     */
    private String timeWindow;

    /**
     * Optional user ID
     */
    private String userId;

    /**
     * Session ID for grouping searches
     */
    private String sessionId;

    /**
     * Parent search ID for derived searches
     */
    private Long parentSearchId;

    /**
     * Depth level for drilldown searches
     */
    @Builder.Default
    private Integer depthLevel = 0;

    /**
     * Total number of results
     */
    @Builder.Default
    private Integer resultCount = 0;

    /**
     * Search results as JSON list
     */
    private List<Map<String, Object>> results;

    /**
     * AI summary/response
     */
    private Map<String, Object> aiSummary;

    /**
     * URLs discovered during search
     */
    private List<String> discoveredUrls;

    /**
     * Fact check results
     */
    private List<Map<String, Object>> factCheckResults;

    /**
     * Overall credibility score (0-100)
     */
    private Double credibilityScore;

    /**
     * Stance distribution
     */
    private Map<String, Object> stanceDistribution;

    /**
     * Additional metadata
     */
    private Map<String, Object> metadata;

    /**
     * Search duration in milliseconds
     */
    private Long durationMs;

    /**
     * Error message if search failed
     */
    private String errorMessage;

    /**
     * Whether the search succeeded
     */
    @Builder.Default
    private Boolean success = true;

    /**
     * Timestamp when search was performed (epoch millis)
     */
    private Long timestamp;
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/SearchResultSummaryDto.java

```java
package com.newsinsight.collector.dto;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.util.List;
import java.util.Map;

/**
 * 검색 결과 페이지 전체 요약 DTO.
 * 
 * 검색 결과 상단에 표시되는 종합 분석 정보.
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class SearchResultSummaryDto {

    /**
     * 총 검색 결과 수
     */
    private Integer totalResults;

    /**
     * 분석 완료된 결과 수
     */
    private Integer analyzedResults;

    // ========== 주제 요약 ==========
    
    /**
     * 주요 키워드/토픽 (상위 5개)
     */
    private List<String> mainTopics;

    /**
     * AI 생성 이슈 요약 (1-2문장)
     */
    private String issueSummary;

    /**
     * 상반된 관점 요약
     * [{"view": "찬성측", "summary": "..."}, {"view": "반대측", "summary": "..."}]
     */
    private List<Map<String, String>> contrastingViews;

    // ========== 신뢰도/편향 요약 ==========

    /**
     * 신뢰도 분포
     * {"high": 0.3, "medium": 0.5, "low": 0.2}
     */
    private Map<String, Double> reliabilityDistribution;

    /**
     * 편향도 분포
     * {"left": 0.2, "center": 0.6, "right": 0.2}
     */
    private Map<String, Double> biasDistribution;

    /**
     * 허위정보 위험 기사 비율
     */
    private Double misinfoRiskRatio;

    // ========== 감정 요약 ==========

    /**
     * 전체 기사 감정 분포
     */
    private Map<String, Double> overallSentiment;

    // ========== 여론 요약 ==========

    /**
     * 전체 댓글 수 합계
     */
    private Integer totalCommentCount;

    /**
     * 전체 여론 감정 분포
     */
    private Map<String, Double> overallDiscussionSentiment;

    /**
     * 여론 요약 문장
     */
    private String discussionSummary;

    /**
     * 시간대별 여론 변화 (그래프용)
     */
    private List<Map<String, Object>> discussionTimeSeries;

    // ========== 경고/주의 ==========

    /**
     * 검색 결과 관련 경고 메시지
     */
    private List<String> warnings;

    /**
     * 팩트체크 필요 기사 수
     */
    private Integer factcheckNeededCount;
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/SearchTemplateDto.java

```java
package com.newsinsight.collector.dto;

import com.newsinsight.collector.entity.search.SearchTemplate;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Map;

/**
 * DTO for SearchTemplate API requests and responses.
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class SearchTemplateDto {

    private Long id;
    private String name;
    private String query;
    private String mode;
    private String userId;
    private List<Map<String, Object>> items;
    private String description;
    private Boolean favorite;
    private List<String> tags;
    private Map<String, Object> metadata;
    private Long sourceSearchId;
    private Integer useCount;
    private LocalDateTime lastUsedAt;
    private LocalDateTime createdAt;
    private LocalDateTime updatedAt;
    
    // Computed field
    private Integer itemCount;

    /**
     * Convert entity to DTO
     */
    public static SearchTemplateDto fromEntity(SearchTemplate entity) {
        return SearchTemplateDto.builder()
                .id(entity.getId())
                .name(entity.getName())
                .query(entity.getQuery())
                .mode(entity.getMode())
                .userId(entity.getUserId())
                .items(entity.getItems())
                .description(entity.getDescription())
                .favorite(entity.getFavorite())
                .tags(entity.getTags())
                .metadata(entity.getMetadata())
                .sourceSearchId(entity.getSourceSearchId())
                .useCount(entity.getUseCount())
                .lastUsedAt(entity.getLastUsedAt())
                .createdAt(entity.getCreatedAt())
                .updatedAt(entity.getUpdatedAt())
                .itemCount(entity.getItemCount())
                .build();
    }

    /**
     * Convert DTO to entity for creation
     */
    public SearchTemplate toEntity() {
        return SearchTemplate.builder()
                .name(this.name)
                .query(this.query)
                .mode(this.mode)
                .userId(this.userId)
                .items(this.items)
                .description(this.description)
                .favorite(this.favorite != null ? this.favorite : false)
                .tags(this.tags)
                .metadata(this.metadata)
                .sourceSearchId(this.sourceSearchId)
                .build();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/SentimentDataDto.java

```java
package com.newsinsight.collector.dto;

public record SentimentDataDto(double pos, double neg, double neu) {}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/StanceDistributionDto.java

```java
package com.newsinsight.collector.dto;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

/**
 * DTO for stance distribution statistics
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class StanceDistributionDto {
    private Long pro;
    private Long con;
    private Long neutral;
    private Double proRatio;
    private Double conRatio;
    private Double neutralRatio;
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/addon/AddonRequest.java

```java
package com.newsinsight.collector.dto.addon;

import com.fasterxml.jackson.annotation.JsonProperty;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.util.Map;

/**
 * Add-on으로 보내는 분석 요청 DTO.
 * 
 * 모든 Add-on은 이 형식의 요청을 받아서 처리.
 * 내부 서비스, 외부 Colab, 서드파티 API 모두 동일한 스펙 사용.
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class AddonRequest {

    /**
     * 요청 고유 ID (추적용)
     */
    @JsonProperty("request_id")
    private String requestId;

    /**
     * Add-on 식별자
     */
    @JsonProperty("addon_id")
    private String addonId;

    /**
     * 작업 유형 (article_analysis, comment_analysis, batch_analysis 등)
     */
    @JsonProperty("task")
    private String task;

    /**
     * 입력 스키마 버전
     */
    @JsonProperty("input_schema_version")
    @Builder.Default
    private String inputSchemaVersion = "1.0";

    /**
     * 분석 대상 기사 정보
     */
    @JsonProperty("article")
    private ArticleInput article;

    /**
     * 분석 대상 댓글/커뮤니티 (해당되는 경우)
     */
    @JsonProperty("comments")
    private CommentsInput comments;

    /**
     * 추가 컨텍스트 (언어, 국가, 이전 분석 결과 등)
     */
    @JsonProperty("context")
    private AnalysisContext context;

    /**
     * 실행 옵션
     */
    @JsonProperty("options")
    private ExecutionOptions options;

    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class ArticleInput {
        /**
         * 기사 ID
         */
        private Long id;

        /**
         * 기사 제목
         */
        private String title;

        /**
         * 기사 본문
         */
        private String content;

        /**
         * 기사 URL
         */
        private String url;

        /**
         * 출처/언론사
         */
        private String source;

        /**
         * 발행일시 (ISO 8601)
         */
        @JsonProperty("published_at")
        private String publishedAt;

        /**
         * 추가 메타데이터
         */
        private Map<String, Object> metadata;
    }

    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class CommentsInput {
        /**
         * 대상 기사 ID
         */
        @JsonProperty("article_id")
        private Long articleId;

        /**
         * 댓글 목록
         */
        private java.util.List<CommentItem> items;

        /**
         * 수집 플랫폼
         */
        private String platform;

        @Data
        @Builder
        @NoArgsConstructor
        @AllArgsConstructor
        public static class CommentItem {
            private String id;
            private String content;
            @JsonProperty("created_at")
            private String createdAt;
            private Integer likes;
            private Integer replies;
            @JsonProperty("author_id")
            private String authorId;
        }
    }

    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class AnalysisContext {
        /**
         * 언어 코드 (ko, en, ja 등)
         */
        private String language;

        /**
         * 국가 코드
         */
        private String country;

        /**
         * 이전 Add-on들의 분석 결과 (의존성 체인에서 사용)
         */
        @JsonProperty("previous_results")
        private Map<String, Object> previousResults;

        /**
         * 관련 기사 ID들 (교차 검증용)
         */
        @JsonProperty("related_article_ids")
        private java.util.List<Long> relatedArticleIds;
    }

    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class ExecutionOptions {
        /**
         * 중요도 (realtime: 즉시 처리, batch: 배치 처리)
         */
        @Builder.Default
        private String importance = "batch";

        /**
         * 디버그 모드 (상세 로그 포함)
         */
        @Builder.Default
        private Boolean debug = false;

        /**
         * 타임아웃 (ms)
         */
        @JsonProperty("timeout_ms")
        private Integer timeoutMs;

        /**
         * 추가 파라미터
         */
        private Map<String, Object> params;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/addon/AddonResponse.java

```java
package com.newsinsight.collector.dto.addon;

import com.fasterxml.jackson.annotation.JsonProperty;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.util.List;
import java.util.Map;

/**
 * Add-on이 반환하는 분석 결과 DTO.
 * 
 * 모든 Add-on은 이 형식으로 결과를 반환.
 * Orchestrator가 이를 파싱하여 ArticleAnalysis/ArticleDiscussion에 저장.
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class AddonResponse {

    /**
     * 요청 ID (추적용)
     */
    @JsonProperty("request_id")
    private String requestId;

    /**
     * Add-on 식별자
     */
    @JsonProperty("addon_id")
    private String addonId;

    /**
     * 처리 상태 (success, error, partial)
     */
    private String status;

    /**
     * 출력 스키마 버전
     */
    @JsonProperty("output_schema_version")
    @Builder.Default
    private String outputSchemaVersion = "1.0";

    /**
     * 분석 결과 (Add-on 카테고리별로 다른 구조)
     */
    private AnalysisResults results;

    /**
     * 에러 정보 (실패 시)
     */
    private ErrorInfo error;

    /**
     * 메타데이터
     */
    private ResponseMeta meta;

    // ========== 결과 구조 ==========

    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class AnalysisResults {

        // === 감정 분석 (SENTIMENT) ===
        @JsonProperty("sentiment")
        private SentimentResult sentiment;

        // === 신뢰도 분석 (SOURCE_QUALITY) ===
        @JsonProperty("reliability")
        private ReliabilityResult reliability;

        // === 편향도 분석 ===
        @JsonProperty("bias")
        private BiasResult bias;

        // === 팩트체크 (FACTCHECK) ===
        @JsonProperty("factcheck")
        private FactcheckResult factcheck;

        // === 개체명 인식 (ENTITY_EXTRACTION) ===
        @JsonProperty("entities")
        private EntitiesResult entities;

        // === 요약 (SUMMARIZATION) ===
        @JsonProperty("summary")
        private SummaryResult summary;

        // === 주제 분류 (TOPIC_CLASSIFICATION) ===
        @JsonProperty("topics")
        private TopicsResult topics;

        // === 커뮤니티 분석 (COMMUNITY) ===
        @JsonProperty("discussion")
        private DiscussionResult discussion;

        // === 독성 분석 (TOXICITY) ===
        @JsonProperty("toxicity")
        private ToxicityResult toxicity;

        // === 허위정보 탐지 (MISINFORMATION) ===
        @JsonProperty("misinformation")
        private MisinfoResult misinformation;

        // === 원시 결과 (구조화되지 않은 추가 데이터) ===
        @JsonProperty("raw")
        private Map<String, Object> raw;
    }

    // ========== 개별 결과 타입들 ==========

    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class SentimentResult {
        private Double score; // -1 ~ 1 or 0 ~ 100
        private String label; // positive, negative, neutral
        private Map<String, Double> distribution;
        private Map<String, Double> emotions; // anger, joy, sadness, etc.
        private List<String> explanations;
    }

    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class ReliabilityResult {
        private Double score; // 0 ~ 100
        private String grade; // high, medium, low
        private Map<String, Double> factors;
        private List<String> warnings;
        private List<String> explanations;
    }

    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class BiasResult {
        private String label; // left, right, center
        private Double score; // -1 ~ 1
        private Map<String, Double> details;
        private List<String> explanations;
    }

    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class FactcheckResult {
        private String status; // verified, suspicious, conflicting, unverified
        private Double confidence;
        private List<ClaimVerification> claims;
        private List<String> sources;
        private String notes;
    }

    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class ClaimVerification {
        private String claim;
        private Boolean verified;
        private Double confidence;
        private List<String> supportingSources;
        private List<String> conflictingSources;
        private String verdict;
    }

    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class EntitiesResult {
        private List<Entity> persons;
        private List<Entity> organizations;
        private List<Entity> locations;
        private List<Entity> misc;
    }

    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class Entity {
        private String text;
        private String type;
        private Integer startPos;
        private Integer endPos;
        private Double confidence;
        private Map<String, Object> metadata;
    }

    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class SummaryResult {
        @JsonProperty("abstractive")
        private String abstractiveSummary;
        @JsonProperty("extractive")
        private List<String> extractiveSentences;
        private List<String> keyPoints;
    }

    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class TopicsResult {
        private List<String> labels;
        private Map<String, Double> scores;
        private String primaryTopic;
    }

    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class DiscussionResult {
        @JsonProperty("overall_sentiment")
        private String overallSentiment;
        @JsonProperty("sentiment_distribution")
        private Map<String, Double> sentimentDistribution;
        @JsonProperty("stance_distribution")
        private Map<String, Double> stanceDistribution;
        @JsonProperty("toxicity_score")
        private Double toxicityScore;
        @JsonProperty("top_keywords")
        private List<Map<String, Object>> topKeywords;
        @JsonProperty("time_series")
        private List<Map<String, Object>> timeSeries;
        @JsonProperty("bot_likelihood")
        private Double botLikelihood;
    }

    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class ToxicityResult {
        private Double score;
        private Map<String, Double> categories; // hate, threat, insult, etc.
        private List<String> flaggedPhrases;
    }

    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class MisinfoResult {
        @JsonProperty("risk_level")
        private String riskLevel; // low, mid, high
        private Double score;
        private List<String> indicators;
        private List<String> explanations;
    }

    // ========== 에러/메타 ==========

    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class ErrorInfo {
        private String code;
        private String message;
        private String details;
    }

    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class ResponseMeta {
        @JsonProperty("model_version")
        private String modelVersion;

        @JsonProperty("latency_ms")
        private Long latencyMs;

        @JsonProperty("processed_at")
        private String processedAt;

        @JsonProperty("token_usage")
        private Map<String, Integer> tokenUsage;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/llm/LlmProviderSettingsDto.java

```java
package com.newsinsight.collector.dto.llm;

import com.newsinsight.collector.entity.settings.LlmProviderType;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.time.LocalDateTime;

/**
 * LLM Provider 설정 응답 DTO.
 * API 키는 마스킹되어 반환됨.
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class LlmProviderSettingsDto {

    private Long id;

    /**
     * Provider 타입
     */
    private LlmProviderType providerType;

    /**
     * Provider 표시명
     */
    private String providerDisplayName;

    /**
     * 사용자 ID (null이면 전역 설정)
     */
    private String userId;

    /**
     * 전역 설정 여부
     */
    private Boolean isGlobal;

    /**
     * 마스킹된 API 키 (예: sk-a***...xyz)
     */
    private String apiKeyMasked;

    /**
     * API 키 존재 여부
     */
    private Boolean hasApiKey;

    /**
     * 기본 모델
     */
    private String defaultModel;

    /**
     * Base URL
     */
    private String baseUrl;

    /**
     * 활성화 여부
     */
    private Boolean enabled;

    /**
     * 우선순위
     */
    private Integer priority;

    /**
     * 최대 토큰
     */
    private Integer maxTokens;

    /**
     * Temperature
     */
    private Double temperature;

    /**
     * 타임아웃 (ms)
     */
    private Integer timeoutMs;

    /**
     * 분당 최대 요청 수
     */
    private Integer maxRequestsPerMinute;

    /**
     * Azure Deployment Name
     */
    private String azureDeploymentName;

    /**
     * Azure API Version
     */
    private String azureApiVersion;

    /**
     * 마지막 테스트 시간
     */
    private LocalDateTime lastTestedAt;

    /**
     * 마지막 테스트 성공 여부
     */
    private Boolean lastTestSuccess;

    /**
     * 생성일시
     */
    private LocalDateTime createdAt;

    /**
     * 수정일시
     */
    private LocalDateTime updatedAt;
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/llm/LlmProviderSettingsRequest.java

```java
package com.newsinsight.collector.dto.llm;

import com.newsinsight.collector.entity.settings.LlmProviderType;
import jakarta.validation.constraints.Max;
import jakarta.validation.constraints.Min;
import jakarta.validation.constraints.NotNull;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

/**
 * LLM Provider 설정 요청 DTO.
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class LlmProviderSettingsRequest {

    /**
     * Provider 타입 (필수)
     */
    @NotNull(message = "Provider type is required")
    private LlmProviderType providerType;

    /**
     * API 키
     */
    private String apiKey;

    /**
     * 기본 모델명
     */
    private String defaultModel;

    /**
     * Base URL (커스텀 엔드포인트용)
     */
    private String baseUrl;

    /**
     * 활성화 여부
     */
    private Boolean enabled;

    /**
     * 우선순위 (1-999)
     */
    @Min(value = 1, message = "Priority must be at least 1")
    @Max(value = 999, message = "Priority must be at most 999")
    private Integer priority;

    /**
     * 최대 토큰 수
     */
    @Min(value = 1, message = "Max tokens must be positive")
    @Max(value = 128000, message = "Max tokens must be at most 128000")
    private Integer maxTokens;

    /**
     * Temperature (0.0 ~ 2.0)
     */
    @Min(value = 0, message = "Temperature must be at least 0")
    @Max(value = 2, message = "Temperature must be at most 2")
    private Double temperature;

    /**
     * 요청 타임아웃 (밀리초)
     */
    @Min(value = 1000, message = "Timeout must be at least 1000ms")
    @Max(value = 300000, message = "Timeout must be at most 300000ms")
    private Integer timeoutMs;

    /**
     * 분당 최대 요청 수
     */
    @Min(value = 1, message = "Max requests per minute must be positive")
    private Integer maxRequestsPerMinute;

    /**
     * Azure Deployment Name
     */
    private String azureDeploymentName;

    /**
     * Azure API Version
     */
    private String azureApiVersion;
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/llm/LlmTestResult.java

```java
package com.newsinsight.collector.dto.llm;

import com.newsinsight.collector.entity.settings.LlmProviderType;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

/**
 * LLM Provider 연결 테스트 결과 DTO.
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class LlmTestResult {

    /**
     * 테스트 성공 여부
     */
    private boolean success;

    /**
     * Provider 타입
     */
    private LlmProviderType providerType;

    /**
     * 결과 메시지
     */
    private String message;

    /**
     * 에러 메시지 (실패 시)
     */
    private String error;

    /**
     * 응답 시간 (밀리초)
     */
    private Long responseTime;

    /**
     * 사용 가능한 모델 목록 (성공 시)
     */
    private java.util.List<String> availableModels;
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/report/ChartData.java

```java
package com.newsinsight.collector.dto.report;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.util.List;

/**
 * 차트 데이터 DTO - 서버 사이드 차트 생성용
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class ChartData {

    /**
     * 차트 유형
     */
    private ChartType chartType;

    /**
     * 차트 제목
     */
    private String title;

    /**
     * X축 라벨
     */
    private String xAxisLabel;

    /**
     * Y축 라벨
     */
    private String yAxisLabel;

    /**
     * 데이터 라벨 목록
     */
    private List<String> labels;

    /**
     * 데이터 값 목록
     */
    private List<Number> values;

    /**
     * 다중 시리즈 데이터
     */
    private List<DataSeries> series;

    /**
     * 색상 팔레트
     */
    private List<String> colors;

    /**
     * 차트 너비 (픽셀)
     */
    @Builder.Default
    private int width = 600;

    /**
     * 차트 높이 (픽셀)
     */
    @Builder.Default
    private int height = 400;

    /**
     * 차트 유형 Enum
     */
    public enum ChartType {
        PIE,            // 파이 차트
        DOUGHNUT,       // 도넛 차트
        BAR,            // 바 차트
        HORIZONTAL_BAR, // 수평 바 차트
        LINE,           // 라인 차트
        AREA,           // 영역 차트
        RADAR,          // 레이더 차트
        GAUGE,          // 게이지 차트
        STACKED_BAR,    // 스택 바 차트
        HISTOGRAM       // 히스토그램
    }

    /**
     * 데이터 시리즈 (다중 라인/바 차트용)
     */
    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class DataSeries {
        private String name;
        private List<Number> data;
        private String color;
    }

    // ===== 빌더 헬퍼 메서드 =====

    /**
     * 파이 차트 생성 헬퍼
     */
    public static ChartData pie(String title, List<String> labels, List<Number> values, List<String> colors) {
        return ChartData.builder()
                .chartType(ChartType.PIE)
                .title(title)
                .labels(labels)
                .values(values)
                .colors(colors)
                .build();
    }

    /**
     * 바 차트 생성 헬퍼
     */
    public static ChartData bar(String title, String xLabel, String yLabel, List<String> labels, List<Number> values) {
        return ChartData.builder()
                .chartType(ChartType.BAR)
                .title(title)
                .xAxisLabel(xLabel)
                .yAxisLabel(yLabel)
                .labels(labels)
                .values(values)
                .build();
    }

    /**
     * 라인 차트 생성 헬퍼
     */
    public static ChartData line(String title, String xLabel, String yLabel, List<String> labels, List<DataSeries> series) {
        return ChartData.builder()
                .chartType(ChartType.LINE)
                .title(title)
                .xAxisLabel(xLabel)
                .yAxisLabel(yLabel)
                .labels(labels)
                .series(series)
                .build();
    }

    /**
     * 게이지 차트 생성 헬퍼
     */
    public static ChartData gauge(String title, double value, double min, double max) {
        return ChartData.builder()
                .chartType(ChartType.GAUGE)
                .title(title)
                .values(List.of(value, min, max))
                .build();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/report/ReportMetadata.java

```java
package com.newsinsight.collector.dto.report;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.time.LocalDateTime;

/**
 * 생성된 보고서 메타데이터 DTO
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class ReportMetadata {

    /**
     * 보고서 고유 ID
     */
    private String reportId;

    /**
     * 보고서 제목
     */
    private String title;

    /**
     * 보고서 유형
     */
    private ReportRequest.ReportType reportType;

    /**
     * 대상 ID (jobId 또는 articleId)
     */
    private String targetId;

    /**
     * 검색 쿼리
     */
    private String query;

    /**
     * 생성 상태: PENDING, GENERATING, COMPLETED, FAILED
     */
    private ReportStatus status;

    /**
     * 파일 크기 (bytes)
     */
    private Long fileSize;

    /**
     * 페이지 수
     */
    private Integer pageCount;

    /**
     * 생성 소요 시간 (ms)
     */
    private Long generationTimeMs;

    /**
     * 생성 일시
     */
    private LocalDateTime createdAt;

    /**
     * 만료 일시 (자동 삭제 예정)
     */
    private LocalDateTime expiresAt;

    /**
     * 다운로드 URL
     */
    private String downloadUrl;

    /**
     * 에러 메시지 (실패 시)
     */
    private String errorMessage;

    /**
     * 보고서 상태 Enum
     */
    public enum ReportStatus {
        PENDING,
        GENERATING,
        COMPLETED,
        FAILED,
        EXPIRED
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/dto/report/ReportRequest.java

```java
package com.newsinsight.collector.dto.report;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.util.List;
import java.util.Map;

/**
 * PDF 보고서 생성 요청 DTO
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class ReportRequest {

    /**
     * 보고서 유형: UNIFIED_SEARCH, DEEP_SEARCH, ML_ANALYSIS
     */
    private ReportType reportType;

    /**
     * 관련 Job ID 또는 Article ID
     */
    private String targetId;

    /**
     * 검색 쿼리 (통합검색, DeepSearch용)
     */
    private String query;

    /**
     * 시간 범위 (1d, 7d, 30d)
     */
    private String timeWindow;

    /**
     * 포함할 섹션 목록
     */
    @Builder.Default
    private List<ReportSection> includeSections = List.of(ReportSection.values());

    /**
     * 프론트엔드에서 생성한 차트 이미지 (Base64)
     */
    private Map<String, String> chartImages;

    /**
     * 보고서 제목 (커스텀)
     */
    private String customTitle;

    /**
     * 회사 로고 URL 또는 Base64
     */
    private String logoImage;

    /**
     * 워터마크 텍스트
     */
    private String watermark;

    /**
     * 언어 설정 (ko, en)
     */
    @Builder.Default
    private String language = "ko";

    /**
     * 보고서 유형 Enum
     */
    public enum ReportType {
        UNIFIED_SEARCH,
        DEEP_SEARCH,
        ML_ANALYSIS,
        ARTICLE_DETAIL
    }

    /**
     * 보고서 섹션 Enum
     */
    public enum ReportSection {
        COVER,              // 표지
        EXECUTIVE_SUMMARY,  // 요약
        DATA_SOURCE,        // 데이터 소스 분석
        TREND_ANALYSIS,     // 시간별 트렌드
        KEYWORD_ANALYSIS,   // 키워드 분석
        SENTIMENT_ANALYSIS, // 감정 분석
        RELIABILITY,        // 신뢰도 분석
        BIAS_ANALYSIS,      // 편향성 분석
        FACTCHECK,          // 팩트체크
        EVIDENCE_LIST,      // 증거 목록
        DETAILED_RESULTS    // 상세 결과
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/BrowserAgentConfig.java

```java
package com.newsinsight.collector.entity;

import jakarta.persistence.Column;
import jakarta.persistence.Embeddable;
import jakarta.persistence.EnumType;
import jakarta.persistence.Enumerated;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

/**
 * Configuration for browser-based AI agent exploration.
 * Embedded in DataSource for BROWSER_AGENT source type.
 * 
 * autonomous-crawler-service의 BrowserTaskMessage와 매핑됩니다.
 */
@Embeddable
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class BrowserAgentConfig {

    /**
     * Maximum depth of link traversal from seed URL.
     * 0 = seed page only, 1 = seed + direct links, etc.
     */
    @Column(name = "agent_max_depth")
    @Builder.Default
    private Integer maxDepth = 2;

    /**
     * Maximum number of pages to visit in a single session.
     */
    @Column(name = "agent_max_pages")
    @Builder.Default
    private Integer maxPages = 50;

    /**
     * Maximum time budget for exploration in seconds.
     */
    @Column(name = "agent_budget_seconds")
    @Builder.Default
    private Integer budgetSeconds = 300; // 5 minutes

    /**
     * Exploration behavior policy.
     */
    @Enumerated(EnumType.STRING)
    @Column(name = "agent_policy", length = 50)
    @Builder.Default
    private BrowserAgentPolicy policy = BrowserAgentPolicy.FOCUSED_TOPIC;

    /**
     * Keywords or topics for focused exploration.
     * Comma-separated list.
     */
    @Column(name = "agent_focus_keywords", columnDefinition = "TEXT")
    private String focusKeywords;

    /**
     * Custom prompt/instructions for the AI agent.
     */
    @Column(name = "agent_custom_prompt", columnDefinition = "TEXT")
    private String customPrompt;

    /**
     * Whether to capture screenshots during exploration.
     */
    @Column(name = "agent_capture_screenshots")
    @Builder.Default
    private Boolean captureScreenshots = false;

    /**
     * Whether to extract structured data (tables, lists).
     */
    @Column(name = "agent_extract_structured")
    @Builder.Default
    private Boolean extractStructured = true;

    /**
     * Domains to exclude from exploration.
     * Comma-separated list.
     */
    @Column(name = "agent_excluded_domains", columnDefinition = "TEXT")
    private String excludedDomains;

    // ========================================
    // 기본 프리셋 팩토리 메서드
    // ========================================

    /**
     * Create default config for news exploration.
     * 일반적인 뉴스 기사 수집에 적합한 설정.
     */
    public static BrowserAgentConfig forNewsExploration() {
        return BrowserAgentConfig.builder()
                .maxDepth(2)
                .maxPages(30)
                .budgetSeconds(180)
                .policy(BrowserAgentPolicy.NEWS_ONLY)
                .extractStructured(true)
                .captureScreenshots(false)
                .build();
    }

    /**
     * Create config for deep single-page extraction.
     * 단일 페이지에서 상세 정보 추출에 적합한 설정.
     */
    public static BrowserAgentConfig forSinglePageExtraction() {
        return BrowserAgentConfig.builder()
                .maxDepth(0)
                .maxPages(1)
                .budgetSeconds(60)
                .policy(BrowserAgentPolicy.SINGLE_PAGE)
                .extractStructured(true)
                .captureScreenshots(false)
                .build();
    }

    // ========================================
    // 뉴스 특화 프리셋 팩토리 메서드 (신규)
    // ========================================

    /**
     * Create config for breaking news monitoring.
     * 속보/긴급 뉴스 우선 수집에 적합한 설정.
     */
    public static BrowserAgentConfig forBreakingNews() {
        return BrowserAgentConfig.builder()
                .maxDepth(1)
                .maxPages(20)
                .budgetSeconds(120)
                .policy(BrowserAgentPolicy.NEWS_BREAKING)
                .extractStructured(true)
                .captureScreenshots(false)
                .build();
    }

    /**
     * Create config for news archive exploration.
     * 과거 기사 아카이브 수집에 적합한 설정.
     */
    public static BrowserAgentConfig forNewsArchive() {
        return BrowserAgentConfig.builder()
                .maxDepth(3)
                .maxPages(100)
                .budgetSeconds(600) // 10분
                .policy(BrowserAgentPolicy.NEWS_ARCHIVE)
                .extractStructured(true)
                .captureScreenshots(false)
                .build();
    }

    /**
     * Create config for opinion/editorial collection.
     * 오피니언/칼럼/사설 수집에 적합한 설정.
     */
    public static BrowserAgentConfig forOpinionContent() {
        return BrowserAgentConfig.builder()
                .maxDepth(2)
                .maxPages(30)
                .budgetSeconds(180)
                .policy(BrowserAgentPolicy.NEWS_OPINION)
                .extractStructured(true)
                .captureScreenshots(false)
                .build();
    }

    /**
     * Create config for local news collection.
     * 지역 뉴스 수집에 적합한 설정.
     */
    public static BrowserAgentConfig forLocalNews() {
        return BrowserAgentConfig.builder()
                .maxDepth(2)
                .maxPages(40)
                .budgetSeconds(240)
                .policy(BrowserAgentPolicy.NEWS_LOCAL)
                .extractStructured(true)
                .captureScreenshots(false)
                .build();
    }

    /**
     * Create config for topic-focused news collection.
     * 특정 키워드/토픽 중심 수집에 적합한 설정.
     * 
     * @param keywords Comma-separated focus keywords
     */
    public static BrowserAgentConfig forFocusedTopic(String keywords) {
        return BrowserAgentConfig.builder()
                .maxDepth(2)
                .maxPages(50)
                .budgetSeconds(300)
                .policy(BrowserAgentPolicy.FOCUSED_TOPIC)
                .focusKeywords(keywords)
                .extractStructured(true)
                .captureScreenshots(false)
                .build();
    }

    /**
     * Create config for domain-wide exploration.
     * 전체 도메인 탐색에 적합한 설정.
     */
    public static BrowserAgentConfig forDomainExploration() {
        return BrowserAgentConfig.builder()
                .maxDepth(3)
                .maxPages(100)
                .budgetSeconds(600)
                .policy(BrowserAgentPolicy.DOMAIN_WIDE)
                .extractStructured(true)
                .captureScreenshots(false)
                .build();
    }

    // ========================================
    // 유틸리티 메서드
    // ========================================

    /**
     * Create a copy of this config with a different policy.
     * 
     * @param newPolicy The new policy to use
     * @return A new BrowserAgentConfig with the updated policy
     */
    public BrowserAgentConfig withPolicy(BrowserAgentPolicy newPolicy) {
        return BrowserAgentConfig.builder()
                .maxDepth(this.maxDepth)
                .maxPages(this.maxPages)
                .budgetSeconds(this.budgetSeconds)
                .policy(newPolicy)
                .focusKeywords(this.focusKeywords)
                .customPrompt(this.customPrompt)
                .captureScreenshots(this.captureScreenshots)
                .extractStructured(this.extractStructured)
                .excludedDomains(this.excludedDomains)
                .build();
    }

    /**
     * Check if this config uses a news-focused policy.
     * 
     * @return true if the policy is news-focused
     */
    public boolean isNewsFocused() {
        return policy != null && policy.isNewsFocused();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/BrowserAgentPolicy.java

```java
package com.newsinsight.collector.entity;

/**
 * Policy for browser agent autonomous exploration behavior.
 * 
 * 이 enum은 autonomous-crawler-service의 CrawlPolicy와 1:1 매핑됩니다.
 * Python: src/crawler/policies.py의 CrawlPolicy enum
 */
public enum BrowserAgentPolicy {
    // ========================================
    // 기본 정책
    // ========================================
    
    /**
     * Focus on specific topic/keywords only.
     * Agent will prioritize links containing relevant keywords.
     */
    FOCUSED_TOPIC("focused_topic"),
    
    /**
     * Explore within the same domain broadly.
     * Agent will visit multiple pages within the seed domain.
     */
    DOMAIN_WIDE("domain_wide"),
    
    /**
     * Focus on news articles only.
     * Agent will identify and prioritize news content patterns.
     */
    NEWS_ONLY("news_only"),
    
    /**
     * Follow links to external domains as well.
     * Agent can navigate to linked external sites.
     */
    CROSS_DOMAIN("cross_domain"),
    
    /**
     * Minimal exploration - only the seed URL.
     * Useful for single-page deep extraction.
     */
    SINGLE_PAGE("single_page"),
    
    // ========================================
    // 뉴스 특화 정책 (신규)
    // ========================================
    
    /**
     * Priority collection of breaking news and urgent updates.
     * Agent focuses on articles marked as 속보, Breaking, 긴급, 단독.
     */
    NEWS_BREAKING("news_breaking"),
    
    /**
     * Historical article collection from archives.
     * Agent navigates through pagination and older content.
     */
    NEWS_ARCHIVE("news_archive"),
    
    /**
     * Focus on opinion pieces, editorials, and columns.
     * Agent targets 오피니언, 칼럼, 사설 sections.
     */
    NEWS_OPINION("news_opinion"),
    
    /**
     * Local and regional news collection.
     * Agent focuses on geographically specific news content.
     */
    NEWS_LOCAL("news_local");

    private final String value;

    BrowserAgentPolicy(String value) {
        this.value = value;
    }

    public String getValue() {
        return value;
    }

    /**
     * Convert string value to enum.
     * 
     * @param value The policy value string (e.g., "news_only", "news_breaking")
     * @return The corresponding BrowserAgentPolicy
     * @throws IllegalArgumentException if the value is not recognized
     */
    public static BrowserAgentPolicy fromValue(String value) {
        if (value == null || value.isBlank()) {
            return NEWS_ONLY; // Default fallback
        }
        for (BrowserAgentPolicy policy : BrowserAgentPolicy.values()) {
            if (policy.value.equalsIgnoreCase(value)) {
                return policy;
            }
        }
        throw new IllegalArgumentException("Unknown browser agent policy: " + value);
    }
    
    /**
     * Check if this policy is a news-specific policy.
     * 
     * @return true if this is a news-focused policy
     */
    public boolean isNewsFocused() {
        return this == NEWS_ONLY || this == NEWS_BREAKING || 
               this == NEWS_ARCHIVE || this == NEWS_OPINION || this == NEWS_LOCAL;
    }
    
    /**
     * Check if this policy supports multi-page crawling.
     * 
     * @return true if the policy allows visiting multiple pages
     */
    public boolean supportsMultiPage() {
        return this != SINGLE_PAGE;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/CollectedData.java

```java
package com.newsinsight.collector.entity;

import jakarta.persistence.*;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.hibernate.annotations.CreationTimestamp;
import org.hibernate.annotations.JdbcTypeCode;
import org.hibernate.type.SqlTypes;

import java.time.LocalDateTime;

@Entity
@Table(name = "collected_data", indexes = {
    @Index(name = "idx_source_id", columnList = "source_id"),
    @Index(name = "idx_content_hash", columnList = "content_hash"),
    @Index(name = "idx_processed", columnList = "processed"),
    @Index(name = "idx_collected_at", columnList = "collected_at")
})
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class CollectedData {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(name = "source_id", nullable = false)
    private Long sourceId;

    @Column(name = "title", columnDefinition = "TEXT")
    private String title;

    @Column(name = "content", columnDefinition = "TEXT")
    private String content;

    @Column(name = "url", columnDefinition = "TEXT")
    private String url;

    @Column(name = "published_date")
    private LocalDateTime publishedDate;

    @CreationTimestamp
    @Column(name = "collected_at", nullable = false, updatable = false)
    private LocalDateTime collectedAt;

    @Column(name = "content_hash", length = 64)
    private String contentHash;

    @Column(name = "metadata_json", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private String metadataJson;

    @Column(name = "processed", nullable = false)
    @Builder.Default
    private Boolean processed = false;

    // QA pipeline results
    @Column(name = "http_ok")
    private Boolean httpOk;

    @Column(name = "has_content")
    private Boolean hasContent;

    @Column(name = "duplicate")
    private Boolean duplicate;

    @Column(name = "normalized")
    private Boolean normalized;

    @Column(name = "quality_score")
    private Double qualityScore;

    @Column(name = "semantic_consistency")
    private Double semanticConsistency;

    @Column(name = "outlier_score")
    private Double outlierScore;

    @Column(name = "trust_score")
    private Double trustScore;
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/CollectionJob.java

```java
package com.newsinsight.collector.entity;

import jakarta.persistence.*;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.hibernate.annotations.CreationTimestamp;

import java.time.LocalDateTime;

@Entity
@Table(name = "collection_jobs", indexes = {
    @Index(name = "idx_source_id", columnList = "source_id"),
    @Index(name = "idx_status", columnList = "status"),
    @Index(name = "idx_created_at", columnList = "created_at")
})
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class CollectionJob {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(name = "source_id", nullable = false)
    private Long sourceId;

    @Enumerated(EnumType.STRING)
    @Column(name = "status", nullable = false, length = 50)
    @Builder.Default
    private JobStatus status = JobStatus.PENDING;

    @Column(name = "started_at")
    private LocalDateTime startedAt;

    @Column(name = "completed_at")
    private LocalDateTime completedAt;

    @Column(name = "items_collected", nullable = false)
    @Builder.Default
    private Integer itemsCollected = 0;

    @Column(name = "error_message", columnDefinition = "TEXT")
    private String errorMessage;

    @CreationTimestamp
    @Column(name = "created_at", nullable = false, updatable = false)
    private LocalDateTime createdAt;

    public enum JobStatus {
        PENDING,
        RUNNING,
        COMPLETED,
        FAILED,
        CANCELLED
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/CrawlEvidence.java

```java
package com.newsinsight.collector.entity;

import com.newsinsight.collector.dto.EvidenceDto;
import jakarta.persistence.*;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.hibernate.annotations.CreationTimestamp;

import java.time.LocalDateTime;

/**
 * Entity representing a piece of evidence collected by the deep AI search.
 * Each evidence item contains a URL, stance classification, and content snippet.
 */
@Entity
@Table(name = "crawl_evidence", indexes = {
        @Index(name = "idx_crawl_evidence_job_id", columnList = "job_id"),
        @Index(name = "idx_crawl_evidence_stance", columnList = "stance")
})
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class CrawlEvidence {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(name = "job_id", nullable = false, length = 64)
    private String jobId;

    @Column(length = 2048)
    private String url;

    @Column(length = 512)
    private String title;

    /**
     * Stance classification: pro, con, or neutral
     */
    @Enumerated(EnumType.STRING)
    @Column(length = 16)
    private EvidenceStance stance;

    @Column(columnDefinition = "TEXT")
    private String snippet;

    @Column(length = 255)
    private String source;

    @CreationTimestamp
    @Column(name = "created_at", updatable = false)
    private LocalDateTime createdAt;

    /**
     * Create evidence from EvidenceDto
     */
    public static CrawlEvidence fromEvidenceDto(String jobId, EvidenceDto evidence) {
        EvidenceStance stance = EvidenceStance.NEUTRAL;
        if (evidence.getStance() != null) {
            try {
                stance = EvidenceStance.valueOf(evidence.getStance().toUpperCase());
            } catch (IllegalArgumentException ignored) {
                // Keep default NEUTRAL
            }
        }

        return CrawlEvidence.builder()
                .jobId(jobId)
                .url(evidence.getUrl())
                .title(evidence.getTitle())
                .stance(stance)
                .snippet(evidence.getSnippet())
                .source(evidence.getSource())
                .build();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/CrawlFailureReason.java

```java
package com.newsinsight.collector.entity;

/**
 * Enum representing specific timeout/failure reasons for deep search jobs.
 * Used for diagnostic logging and monitoring dashboards.
 */
public enum CrawlFailureReason {
    // Timeout reasons
    TIMEOUT_INTEGRATED_CRAWLER("timeout_integrated_crawler", "Integrated crawler exceeded time limit"),
    TIMEOUT_CRAWL4AI("timeout_crawl4ai", "Crawl4AI service timeout"),
    TIMEOUT_BROWSER_USE("timeout_browser_use", "Browser-Use API timeout"),
    TIMEOUT_AIDOVE("timeout_aidove", "AI Dove analysis timeout"),
    TIMEOUT_JOB_OVERALL("timeout_job_overall", "Overall job timeout exceeded"),
    TIMEOUT_HTTP_REQUEST("timeout_http_request", "HTTP request timeout"),
    TIMEOUT_POLLING("timeout_polling", "Polling timeout for async result"),

    // Connection/Network errors
    CONNECTION_REFUSED("connection_refused", "Connection refused by remote service"),
    CONNECTION_TIMEOUT("connection_timeout", "Connection establishment timeout"),
    DNS_RESOLUTION_FAILED("dns_resolution_failed", "DNS resolution failed"),
    NETWORK_UNREACHABLE("network_unreachable", "Network unreachable"),
    SSL_HANDSHAKE_FAILED("ssl_handshake_failed", "SSL handshake failed"),

    // Service errors
    SERVICE_UNAVAILABLE("service_unavailable", "External service unavailable"),
    SERVICE_OVERLOADED("service_overloaded", "Service overloaded, rate limited"),
    SERVICE_ERROR("service_error", "External service returned error"),
    CRAWL4AI_UNAVAILABLE("crawl4ai_unavailable", "Crawl4AI service not available"),
    BROWSER_USE_UNAVAILABLE("browser_use_unavailable", "Browser-Use service not available"),
    AIDOVE_UNAVAILABLE("aidove_unavailable", "AI Dove service not available"),

    // Content/Parsing errors
    EMPTY_CONTENT("empty_content", "No content extracted from pages"),
    PARSE_ERROR("parse_error", "Failed to parse response"),
    INVALID_URL("invalid_url", "Invalid URL provided"),
    BLOCKED_BY_ROBOTS("blocked_by_robots", "Blocked by robots.txt"),
    BLOCKED_BY_CAPTCHA("blocked_by_captcha", "Blocked by CAPTCHA"),
    CONTENT_TOO_LARGE("content_too_large", "Content too large to process"),

    // Processing errors
    AI_ANALYSIS_FAILED("ai_analysis_failed", "AI analysis/extraction failed"),
    EVIDENCE_EXTRACTION_FAILED("evidence_extraction_failed", "Evidence extraction failed"),
    STANCE_ANALYSIS_FAILED("stance_analysis_failed", "Stance analysis failed"),
    
    // Job management errors
    JOB_CANCELLED("job_cancelled", "Job was cancelled"),
    DUPLICATE_CALLBACK("duplicate_callback", "Duplicate callback received"),
    INVALID_CALLBACK_TOKEN("invalid_callback_token", "Invalid callback token"),

    // Unknown/Other
    UNKNOWN("unknown", "Unknown error occurred");

    private final String code;
    private final String description;

    CrawlFailureReason(String code, String description) {
        this.code = code;
        this.description = description;
    }

    public String getCode() {
        return code;
    }

    public String getDescription() {
        return description;
    }

    /**
     * Get failure reason from exception message
     */
    public static CrawlFailureReason fromException(Throwable e) {
        if (e == null) return UNKNOWN;
        
        String message = e.getMessage() != null ? e.getMessage().toLowerCase() : "";
        String className = e.getClass().getSimpleName().toLowerCase();
        
        // Timeout detection
        if (className.contains("timeout") || message.contains("timeout") || message.contains("timed out")) {
            if (message.contains("crawl4ai")) return TIMEOUT_CRAWL4AI;
            if (message.contains("browser") || message.contains("browser-use")) return TIMEOUT_BROWSER_USE;
            if (message.contains("aidove") || message.contains("ai dove") || message.contains("dove")) return TIMEOUT_AIDOVE;
            if (message.contains("connect")) return CONNECTION_TIMEOUT;
            if (message.contains("poll")) return TIMEOUT_POLLING;
            return TIMEOUT_HTTP_REQUEST;
        }
        
        // Connection errors
        if (message.contains("connection refused") || className.contains("connectexception")) {
            return CONNECTION_REFUSED;
        }
        if (message.contains("dns") || message.contains("unknown host") || message.contains("unresolved")) {
            return DNS_RESOLUTION_FAILED;
        }
        if (message.contains("ssl") || message.contains("certificate") || message.contains("tls")) {
            return SSL_HANDSHAKE_FAILED;
        }
        if (message.contains("network") || message.contains("unreachable")) {
            return NETWORK_UNREACHABLE;
        }
        
        // Service errors
        if (message.contains("503") || message.contains("service unavailable")) {
            return SERVICE_UNAVAILABLE;
        }
        if (message.contains("429") || message.contains("rate limit") || message.contains("too many requests")) {
            return SERVICE_OVERLOADED;
        }
        if (message.contains("500") || message.contains("internal server error")) {
            return SERVICE_ERROR;
        }
        
        // Content errors
        if (message.contains("empty") && (message.contains("content") || message.contains("response"))) {
            return EMPTY_CONTENT;
        }
        if (message.contains("parse") || message.contains("json") || message.contains("malformed")) {
            return PARSE_ERROR;
        }
        if (message.contains("captcha")) {
            return BLOCKED_BY_CAPTCHA;
        }
        if (message.contains("robots")) {
            return BLOCKED_BY_ROBOTS;
        }
        
        return UNKNOWN;
    }

    /**
     * Get failure reason from error message string
     */
    public static CrawlFailureReason fromErrorMessage(String errorMessage) {
        if (errorMessage == null || errorMessage.isBlank()) return UNKNOWN;
        
        String message = errorMessage.toLowerCase();
        
        // Match specific codes first
        for (CrawlFailureReason reason : values()) {
            if (message.contains(reason.code)) {
                return reason;
            }
        }
        
        // Fallback to pattern matching
        if (message.contains("timeout")) {
            if (message.contains("crawl4ai")) return TIMEOUT_CRAWL4AI;
            if (message.contains("browser")) return TIMEOUT_BROWSER_USE;
            if (message.contains("aidove") || message.contains("dove")) return TIMEOUT_AIDOVE;
            if (message.contains("overall") || message.contains("job")) return TIMEOUT_JOB_OVERALL;
            return TIMEOUT_HTTP_REQUEST;
        }
        
        if (message.contains("cancelled") || message.contains("canceled")) {
            return JOB_CANCELLED;
        }
        
        if (message.contains("unavailable")) {
            if (message.contains("crawl4ai")) return CRAWL4AI_UNAVAILABLE;
            if (message.contains("browser")) return BROWSER_USE_UNAVAILABLE;
            if (message.contains("aidove")) return AIDOVE_UNAVAILABLE;
            return SERVICE_UNAVAILABLE;
        }
        
        return UNKNOWN;
    }

    @Override
    public String toString() {
        return code;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/CrawlJob.java

```java
package com.newsinsight.collector.entity;

import jakarta.persistence.*;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.hibernate.annotations.CreationTimestamp;
import org.hibernate.annotations.UpdateTimestamp;

import java.time.LocalDateTime;

/**
 * Entity representing a deep AI search job.
 * Tracks the status and metadata of asynchronous crawl agent requests.
 */
@Entity
@Table(name = "crawl_jobs", indexes = {
        @Index(name = "idx_crawl_jobs_status", columnList = "status"),
        @Index(name = "idx_crawl_jobs_topic", columnList = "topic"),
        @Index(name = "idx_crawl_jobs_created_at", columnList = "created_at")
})
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class CrawlJob {

    @Id
    @Column(length = 64)
    private String id;

    @Column(nullable = false, length = 512)
    private String topic;

    @Column(name = "base_url", length = 2048)
    private String baseUrl;

    @Enumerated(EnumType.STRING)
    @Column(nullable = false, length = 32)
    @Builder.Default
    private CrawlJobStatus status = CrawlJobStatus.PENDING;

    @Column(name = "evidence_count")
    @Builder.Default
    private Integer evidenceCount = 0;

    @Column(name = "error_message", length = 1024)
    private String errorMessage;

    @Enumerated(EnumType.STRING)
    @Column(name = "failure_reason", length = 64)
    private CrawlFailureReason failureReason;

    @Column(name = "callback_received")
    @Builder.Default
    private Boolean callbackReceived = false;

    @CreationTimestamp
    @Column(name = "created_at", updatable = false)
    private LocalDateTime createdAt;

    @UpdateTimestamp
    @Column(name = "updated_at")
    private LocalDateTime updatedAt;

    @Column(name = "completed_at")
    private LocalDateTime completedAt;

    /**
     * Mark the job as completed successfully
     */
    public void markCompleted(int evidenceCount) {
        this.status = CrawlJobStatus.COMPLETED;
        this.evidenceCount = evidenceCount;
        this.completedAt = LocalDateTime.now();
        this.callbackReceived = true;
    }

    /**
     * Mark the job as failed
     */
    public void markFailed(String errorMessage) {
        markFailed(errorMessage, CrawlFailureReason.fromErrorMessage(errorMessage));
    }

    /**
     * Mark the job as failed with a specific failure reason
     */
    public void markFailed(String errorMessage, CrawlFailureReason failureReason) {
        this.status = CrawlJobStatus.FAILED;
        this.errorMessage = errorMessage;
        this.failureReason = failureReason;
        this.completedAt = LocalDateTime.now();
        this.callbackReceived = true;
    }

    /**
     * Mark the job as failed from an exception
     */
    public void markFailedFromException(Throwable e) {
        CrawlFailureReason reason = CrawlFailureReason.fromException(e);
        String message = e.getMessage() != null ? e.getMessage() : e.getClass().getSimpleName();
        markFailed(message, reason);
    }

    /**
     * Mark the job as timed out with a specific reason
     */
    public void markTimedOut(CrawlFailureReason timeoutReason) {
        this.status = CrawlJobStatus.TIMEOUT;
        this.errorMessage = timeoutReason.getDescription();
        this.failureReason = timeoutReason;
        this.completedAt = LocalDateTime.now();
        this.callbackReceived = true;
    }

    /**
     * Mark the job as in progress
     */
    public void markInProgress() {
        this.status = CrawlJobStatus.IN_PROGRESS;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/CrawlJobStatus.java

```java
package com.newsinsight.collector.entity;

/**
 * Status of a deep AI search crawl job
 */
public enum CrawlJobStatus {
    /**
     * Job has been created but not yet started
     */
    PENDING,

    /**
     * Job is currently being processed by n8n workflow
     */
    IN_PROGRESS,

    /**
     * Job completed successfully with evidence
     */
    COMPLETED,

    /**
     * Job failed due to an error
     */
    FAILED,

    /**
     * Job was cancelled before completion
     */
    CANCELLED,

    /**
     * Job timed out waiting for callback
     */
    TIMEOUT
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/DataSource.java

```java
package com.newsinsight.collector.entity;

import jakarta.persistence.*;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.hibernate.annotations.CreationTimestamp;
import org.hibernate.annotations.JdbcTypeCode;
import org.hibernate.annotations.UpdateTimestamp;
import org.hibernate.type.SqlTypes;

import java.time.LocalDateTime;

@Entity
@Table(name = "data_sources", indexes = {
    @Index(name = "idx_source_type", columnList = "source_type"),
    @Index(name = "idx_is_active", columnList = "is_active")
})
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class DataSource {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(name = "name", nullable = false, length = 255)
    private String name;

    @Column(name = "url", nullable = false, columnDefinition = "TEXT")
    private String url;

    @Enumerated(EnumType.STRING)
    @Column(name = "source_type", nullable = false, length = 50)
    private SourceType sourceType;

    @Column(name = "is_active", nullable = false)
    @Builder.Default
    private Boolean isActive = true;

    @Column(name = "last_collected")
    private LocalDateTime lastCollected;

    @Column(name = "collection_frequency", nullable = false)
    @Builder.Default
    private Integer collectionFrequency = 3600; // seconds

    @JdbcTypeCode(SqlTypes.JSON)
    @Column(name = "metadata_json", columnDefinition = "jsonb")
    private String metadataJson;

    /**
     * Search URL template for web search sources.
     * Use {query} as placeholder for the encoded search query.
     * Example: "https://search.naver.com/search.naver?where=news&query={query}"
     * Only applicable when sourceType = WEB_SEARCH.
     */
    @Column(name = "search_url_template", columnDefinition = "TEXT")
    private String searchUrlTemplate;

    /**
     * Priority for web search sources (lower = higher priority).
     * Used for ordering when selecting search sources.
     */
    @Column(name = "search_priority")
    @Builder.Default
    private Integer searchPriority = 100;

    /**
     * Browser agent configuration.
     * Only applicable when sourceType = BROWSER_AGENT.
     */
    @Embedded
    private BrowserAgentConfig browserAgentConfig;

    @CreationTimestamp
    @Column(name = "created_at", nullable = false, updatable = false)
    private LocalDateTime createdAt;

    @UpdateTimestamp
    @Column(name = "updated_at")
    private LocalDateTime updatedAt;

    /**
     * Check if this source requires browser-based collection.
     */
    public boolean requiresBrowserAgent() {
        return sourceType != null && sourceType.requiresBrowser();
    }

    /**
     * Get browser agent config, creating default if null and source type requires it.
     */
    public BrowserAgentConfig getEffectiveBrowserAgentConfig() {
        if (browserAgentConfig != null) {
            return browserAgentConfig;
        }
        if (requiresBrowserAgent()) {
            return BrowserAgentConfig.forNewsExploration();
        }
        return null;
    }

    /**
     * Check if this source supports web search.
     */
    public boolean supportsWebSearch() {
        return sourceType == SourceType.WEB_SEARCH && searchUrlTemplate != null && !searchUrlTemplate.isBlank();
    }

    /**
     * Generate search URL from template with the given query.
     * 
     * @param encodedQuery URL-encoded search query
     * @return Generated search URL or null if template is not set
     */
    public String buildSearchUrl(String encodedQuery) {
        if (searchUrlTemplate == null || searchUrlTemplate.isBlank()) {
            return null;
        }
        return searchUrlTemplate.replace("{query}", encodedQuery);
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/EvidenceStance.java

```java
package com.newsinsight.collector.entity;

/**
 * Stance classification for evidence items.
 * Represents the position of the evidence relative to the search topic.
 */
public enum EvidenceStance {
    /**
     * Evidence supports or is favorable to the topic
     */
    PRO,

    /**
     * Evidence opposes or is unfavorable to the topic
     */
    CON,

    /**
     * Evidence is neutral or balanced
     */
    NEUTRAL
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/SourceType.java

```java
package com.newsinsight.collector.entity;

/**
 * Types of data sources for collection.
 * 
 * - RSS: RSS/Atom feed parsing (Rome library)
 * - WEB: Static HTML scraping (Crawl4AI/Jsoup)
 * - WEB_SEARCH: Web search portal integration (Naver, Daum, Google, etc.)
 * - API: External API integration (future)
 * - WEBHOOK: Passive event reception (future)
 * - BROWSER_AGENT: AI-driven autonomous browser exploration (Browser-use/Puppeteer)
 */
public enum SourceType {
    RSS("rss"),
    WEB("web"),
    WEB_SEARCH("web_search"),
    API("api"),
    WEBHOOK("webhook"),
    BROWSER_AGENT("browser_agent");

    private final String value;

    SourceType(String value) {
        this.value = value;
    }

    public String getValue() {
        return value;
    }

    /**
     * Check if this source type requires browser-based collection.
     */
    public boolean requiresBrowser() {
        return this == BROWSER_AGENT;
    }

    /**
     * Check if this source type supports autonomous exploration.
     */
    public boolean supportsAutonomousExploration() {
        return this == BROWSER_AGENT;
    }

    /**
     * Check if this source type is for web search portals.
     */
    public boolean isWebSearch() {
        return this == WEB_SEARCH;
    }

    public static SourceType fromValue(String value) {
        for (SourceType type : SourceType.values()) {
            if (type.value.equalsIgnoreCase(value)) {
                return type;
            }
        }
        throw new IllegalArgumentException("Unknown source type: " + value);
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/addon/AddonAuthType.java

```java
package com.newsinsight.collector.entity.addon;

/**
 * Add-on 인증 타입.
 */
public enum AddonAuthType {
    
    /**
     * 인증 없음
     */
    NONE,
    
    /**
     * API Key 인증 (헤더 또는 쿼리 파라미터)
     */
    API_KEY,
    
    /**
     * Bearer Token
     */
    BEARER_TOKEN,
    
    /**
     * Basic Auth
     */
    BASIC,
    
    /**
     * OAuth 2.0
     */
    OAUTH2
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/addon/AddonCategory.java

```java
package com.newsinsight.collector.entity.addon;

/**
 * Add-on 카테고리 분류.
 * 각 카테고리는 분석 기능의 유형을 나타냄.
 */
public enum AddonCategory {
    
    /**
     * 감정 분석 (긍정/부정/중립)
     */
    SENTIMENT,
    
    /**
     * 문맥/의도 분석 (주제 분류, 스탠스 분석)
     */
    CONTEXT,
    
    /**
     * 팩트체크 (주장 검증, 교차 출처 비교)
     */
    FACTCHECK,
    
    /**
     * 커뮤니티/여론 분석 (댓글, SNS)
     */
    COMMUNITY,
    
    /**
     * 출처 신뢰도/편향도 분석
     */
    SOURCE_QUALITY,

    BIAS,
    
    /**
     * 개체명 인식 (NER)
     */
    ENTITY_EXTRACTION,
    
    /**
     * 요약 생성
     */
    SUMMARIZATION,
    
    /**
     * 주제 분류
     */
    TOPIC_CLASSIFICATION,
    
    /**
     * 독성 댓글 탐지
     */
    TOXICITY,
    
    BOT_DETECTION,
    
    NER,
    
    /**
     * 허위정보 탐지
     */
    MISINFORMATION,
    
    /**
     * 기타 범주
     */
    OTHER
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/addon/AddonHealthStatus.java

```java
package com.newsinsight.collector.entity.addon;

/**
 * Add-on 헬스체크 상태.
 */
public enum AddonHealthStatus {
    
    /**
     * 정상
     */
    HEALTHY,
    
    /**
     * 불안정 (간헐적 오류)
     */
    DEGRADED,
    
    /**
     * 장애
     */
    UNHEALTHY,
    
    /**
     * 알 수 없음 (아직 체크 안 됨)
     */
    UNKNOWN
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/addon/AddonInvokeType.java

```java
package com.newsinsight.collector.entity.addon;

/**
 * Add-on 호출 타입.
 */
public enum AddonInvokeType {
    
    /**
     * HTTP 동기 호출 (응답 대기)
     */
    HTTP_SYNC,
    
    /**
     * HTTP 비동기 호출 (웹훅 콜백)
     */
    HTTP_ASYNC,
    
    /**
     * 메시지 큐 기반 (Kafka, RabbitMQ 등)
     */
    QUEUE,
    
    /**
     * 파일/스토리지 폴링 (S3, GCS 등)
     */
    FILE_POLL
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/addon/ExecutionStatus.java

```java
package com.newsinsight.collector.entity.addon;

/**
 * Add-on 실행 상태.
 */
public enum ExecutionStatus {
    
    /**
     * 대기 중 (큐에 있음)
     */
    PENDING,
    
    /**
     * 실행 중
     */
    RUNNING,
    
    /**
     * 성공
     */
    SUCCESS,
    
    /**
     * 실패
     */
    FAILED,
    
    /**
     * 타임아웃
     */
    TIMEOUT,
    
    /**
     * 취소됨
     */
    CANCELLED,
    
    /**
     * 건너뜀 (의존성 실패 등)
     */
    SKIPPED
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/addon/MlAddon.java

```java
package com.newsinsight.collector.entity.addon;

import jakarta.persistence.*;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.hibernate.annotations.CreationTimestamp;
import org.hibernate.annotations.JdbcTypeCode;
import org.hibernate.annotations.UpdateTimestamp;
import org.hibernate.type.SqlTypes;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Map;

/**
 * ML Add-on Registry Entity.
 * 
 * 각 ML 분석 기능(감정분석, 팩트체크, 편향도 분석 등)을 플러그인 형태로 등록/관리.
 * 내부 서비스(Spring/Python), 외부 Colab, 또는 서드파티 API 모두 동일한 방식으로 연결 가능.
 */
@Entity
@Table(name = "ml_addon", indexes = {
    @Index(name = "idx_addon_category", columnList = "category"),
    @Index(name = "idx_addon_enabled", columnList = "enabled"),
    @Index(name = "idx_addon_invoke_type", columnList = "invoke_type")
})
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class MlAddon {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    /**
     * Add-on 고유 식별자 (예: "sentiment-v1", "factcheck-korean-v2")
     */
    @Column(name = "addon_key", nullable = false, unique = true, length = 100)
    private String addonKey;

    /**
     * 표시용 이름
     */
    @Column(name = "name", nullable = false, length = 200)
    private String name;

    /**
     * Add-on 설명
     */
    @Column(name = "description", columnDefinition = "TEXT")
    private String description;

    /**
     * 분류 카테고리
     */
    @Enumerated(EnumType.STRING)
    @Column(name = "category", nullable = false, length = 50)
    private AddonCategory category;

    /**
     * 호출 타입
     */
    @Enumerated(EnumType.STRING)
    @Column(name = "invoke_type", nullable = false, length = 30)
    private AddonInvokeType invokeType;

    /**
     * HTTP 호출 시 엔드포인트 URL
     */
    @Column(name = "endpoint_url", length = 500)
    private String endpointUrl;

    /**
     * 큐 기반 호출 시 토픽명
     */
    @Column(name = "queue_topic", length = 200)
    private String queueTopic;

    /**
     * 파일 폴링 시 스토리지 경로
     */
    @Column(name = "storage_path", length = 500)
    private String storagePath;

    /**
     * 인증 타입
     */
    @Enumerated(EnumType.STRING)
    @Column(name = "auth_type", length = 30)
    @Builder.Default
    private AddonAuthType authType = AddonAuthType.NONE;

    /**
     * 인증 정보 (암호화 저장 권장)
     * API Key, OAuth credentials 등
     */
    @Column(name = "auth_credentials", columnDefinition = "TEXT")
    private String authCredentials;

    /**
     * Input 스키마 버전 (호환성 체크용)
     */
    @Column(name = "input_schema_version", length = 20)
    @Builder.Default
    private String inputSchemaVersion = "1.0";

    /**
     * Output 스키마 버전
     */
    @Column(name = "output_schema_version", length = 20)
    @Builder.Default
    private String outputSchemaVersion = "1.0";

    /**
     * 타임아웃 (밀리초)
     */
    @Column(name = "timeout_ms")
    @Builder.Default
    private Integer timeoutMs = 30000;

    /**
     * 초당 최대 요청 수 (Rate limiting)
     */
    @Column(name = "max_qps")
    @Builder.Default
    private Integer maxQps = 10;

    /**
     * 재시도 횟수
     */
    @Column(name = "max_retries")
    @Builder.Default
    private Integer maxRetries = 3;

    /**
     * 의존하는 다른 Add-on들의 addonKey 목록 (DAG 구성용)
     * 예: ["entity_extractor_v1", "topic_classifier_v1"]
     */
    @Column(name = "depends_on", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private List<String> dependsOn;

    /**
     * 활성화 여부
     */
    @Column(name = "enabled", nullable = false)
    @Builder.Default
    private Boolean enabled = true;

    /**
     * 우선순위 (낮을수록 먼저 실행)
     */
    @Column(name = "priority")
    @Builder.Default
    private Integer priority = 100;

    /**
     * 추가 설정 (JSON)
     * - 모델 파라미터
     * - 언어 설정
     * - 임계값 등
     */
    @Column(name = "config", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private Map<String, Object> config;

    /**
     * 헬스체크 엔드포인트 (옵션)
     */
    @Column(name = "health_check_url", length = 500)
    private String healthCheckUrl;

    /**
     * 마지막 헬스체크 상태
     */
    @Enumerated(EnumType.STRING)
    @Column(name = "health_status", length = 20)
    @Builder.Default
    private AddonHealthStatus healthStatus = AddonHealthStatus.UNKNOWN;

    /**
     * 마지막 헬스체크 시간
     */
    @Column(name = "last_health_check")
    private LocalDateTime lastHealthCheck;

    /**
     * 관리자/소유자
     */
    @Column(name = "owner", length = 100)
    private String owner;

    /**
     * 생성일시
     */
    @CreationTimestamp
    @Column(name = "created_at", nullable = false, updatable = false)
    private LocalDateTime createdAt;

    /**
     * 수정일시
     */
    @UpdateTimestamp
    @Column(name = "updated_at")
    private LocalDateTime updatedAt;

    // === 운영 통계 (캐시용, 주기적 업데이트) ===

    /**
     * 총 실행 횟수
     */
    @Column(name = "total_executions")
    @Builder.Default
    private Long totalExecutions = 0L;

    /**
     * 성공 횟수
     */
    @Column(name = "success_count")
    @Builder.Default
    private Long successCount = 0L;

    /**
     * 실패 횟수
     */
    @Column(name = "failure_count")
    @Builder.Default
    private Long failureCount = 0L;

    /**
     * 평균 응답 시간 (ms)
     */
    @Column(name = "avg_latency_ms")
    private Double avgLatencyMs;

    /**
     * 통계 마지막 갱신 시간
     */
    @Column(name = "stats_updated_at")
    private LocalDateTime statsUpdatedAt;

    // === Helper Methods ===

    public boolean isHttpBased() {
        return invokeType == AddonInvokeType.HTTP_SYNC || invokeType == AddonInvokeType.HTTP_ASYNC;
    }

    public boolean isQueueBased() {
        return invokeType == AddonInvokeType.QUEUE;
    }

    public double getSuccessRate() {
        if (totalExecutions == null || totalExecutions == 0) return 0.0;
        return (successCount != null ? successCount : 0) / (double) totalExecutions;
    }

    public void incrementSuccess(long latencyMs) {
        this.totalExecutions = (this.totalExecutions != null ? this.totalExecutions : 0) + 1;
        this.successCount = (this.successCount != null ? this.successCount : 0) + 1;
        // Simple moving average for latency
        if (this.avgLatencyMs == null) {
            this.avgLatencyMs = (double) latencyMs;
        } else {
            this.avgLatencyMs = (this.avgLatencyMs * 0.9) + (latencyMs * 0.1);
        }
        this.statsUpdatedAt = LocalDateTime.now();
    }

    public void incrementFailure() {
        this.totalExecutions = (this.totalExecutions != null ? this.totalExecutions : 0) + 1;
        this.failureCount = (this.failureCount != null ? this.failureCount : 0) + 1;
        this.statsUpdatedAt = LocalDateTime.now();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/addon/MlAddonExecution.java

```java
package com.newsinsight.collector.entity.addon;

import jakarta.persistence.*;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.hibernate.annotations.CreationTimestamp;
import org.hibernate.annotations.JdbcTypeCode;
import org.hibernate.type.SqlTypes;

import java.time.LocalDateTime;
import java.util.Map;

/**
 * Add-on 실행 이력 엔티티.
 * 
 * 각 분석 작업의 요청/응답/상태를 기록.
 * 디버깅, 모니터링, 감사 추적에 활용.
 */
@Entity
@Table(name = "ml_addon_execution", indexes = {
    @Index(name = "idx_exec_addon_id", columnList = "addon_id"),
    @Index(name = "idx_exec_article_id", columnList = "article_id"),
    @Index(name = "idx_exec_status", columnList = "status"),
    @Index(name = "idx_exec_created", columnList = "created_at"),
    @Index(name = "idx_exec_batch_id", columnList = "batch_id")
})
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class MlAddonExecution {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    /**
     * 요청 고유 ID (UUID)
     */
    @Column(name = "request_id", nullable = false, unique = true, length = 50)
    private String requestId;

    /**
     * 배치 ID (여러 기사를 한 번에 처리할 때)
     */
    @Column(name = "batch_id", length = 50)
    private String batchId;

    /**
     * 대상 Add-on
     */
    @ManyToOne(fetch = FetchType.LAZY)
    @JoinColumn(name = "addon_id", nullable = false)
    private MlAddon addon;

    /**
     * 분석 대상 기사 ID
     */
    @Column(name = "article_id")
    private Long articleId;

    /**
     * 분석 대상 URL (기사가 아닌 경우)
     */
    @Column(name = "target_url", length = 1000)
    private String targetUrl;

    /**
     * 실행 상태
     */
    @Enumerated(EnumType.STRING)
    @Column(name = "status", nullable = false, length = 20)
    @Builder.Default
    private ExecutionStatus status = ExecutionStatus.PENDING;

    /**
     * 요청 페이로드 (디버깅용, 민감정보 주의)
     */
    @Column(name = "request_payload", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private Map<String, Object> requestPayload;

    /**
     * 응답 결과 (분석 결과 전체)
     */
    @Column(name = "response_payload", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private Map<String, Object> responsePayload;

    /**
     * 에러 메시지 (실패 시)
     */
    @Column(name = "error_message", columnDefinition = "TEXT")
    private String errorMessage;

    /**
     * 에러 코드
     */
    @Column(name = "error_code", length = 50)
    private String errorCode;

    /**
     * 재시도 횟수
     */
    @Column(name = "retry_count")
    @Builder.Default
    private Integer retryCount = 0;

    /**
     * 요청 시작 시간
     */
    @Column(name = "started_at")
    private LocalDateTime startedAt;

    /**
     * 요청 완료 시간
     */
    @Column(name = "completed_at")
    private LocalDateTime completedAt;

    /**
     * 실행 소요 시간 (ms)
     */
    @Column(name = "latency_ms")
    private Long latencyMs;

    /**
     * 모델 버전 (Add-on이 반환)
     */
    @Column(name = "model_version", length = 100)
    private String modelVersion;

    /**
     * 생성 시간
     */
    @CreationTimestamp
    @Column(name = "created_at", nullable = false, updatable = false)
    private LocalDateTime createdAt;

    /**
     * 중요도/우선순위 (realtime, batch)
     */
    @Column(name = "importance", length = 20)
    @Builder.Default
    private String importance = "batch";

    // === Helper Methods ===

    public void markStarted() {
        this.status = ExecutionStatus.RUNNING;
        this.startedAt = LocalDateTime.now();
    }

    public void markSuccess(Map<String, Object> response, String modelVersion) {
        this.status = ExecutionStatus.SUCCESS;
        this.completedAt = LocalDateTime.now();
        this.responsePayload = response;
        this.modelVersion = modelVersion;
        if (this.startedAt != null) {
            this.latencyMs = java.time.Duration.between(startedAt, completedAt).toMillis();
        }
    }

    public void markFailed(String errorCode, String errorMessage) {
        this.status = ExecutionStatus.FAILED;
        this.completedAt = LocalDateTime.now();
        this.errorCode = errorCode;
        this.errorMessage = errorMessage;
        if (this.startedAt != null) {
            this.latencyMs = java.time.Duration.between(startedAt, completedAt).toMillis();
        }
    }

    public void incrementRetry() {
        this.retryCount = (this.retryCount != null ? this.retryCount : 0) + 1;
        this.status = ExecutionStatus.PENDING;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/ai/AiJob.java

```java
package com.newsinsight.collector.entity.ai;

import jakarta.persistence.*;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.hibernate.annotations.CreationTimestamp;
import org.hibernate.annotations.UpdateTimestamp;

import java.time.LocalDateTime;
import java.util.ArrayList;
import java.util.List;

/**
 * Entity representing an AI orchestration job.
 * A job consists of multiple sub-tasks that can be processed by different AI providers.
 * Tracks the overall status aggregated from all sub-tasks.
 */
@Entity
@Table(name = "ai_jobs", indexes = {
        @Index(name = "idx_ai_jobs_overall_status", columnList = "overall_status"),
        @Index(name = "idx_ai_jobs_created_at", columnList = "created_at"),
        @Index(name = "idx_ai_jobs_topic", columnList = "topic")
})
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class AiJob {

    @Id
    @Column(name = "job_id", length = 64)
    private String id;

    @Column(nullable = false, length = 512)
    private String topic;

    @Column(name = "base_url", length = 2048)
    private String baseUrl;

    @Enumerated(EnumType.STRING)
    @Column(name = "overall_status", nullable = false, length = 32)
    @Builder.Default
    private AiJobStatus overallStatus = AiJobStatus.PENDING;

    @OneToMany(mappedBy = "aiJob", cascade = CascadeType.ALL, orphanRemoval = true, fetch = FetchType.LAZY)
    @Builder.Default
    private List<AiSubTask> subTasks = new ArrayList<>();

    @CreationTimestamp
    @Column(name = "created_at", updatable = false)
    private LocalDateTime createdAt;

    @UpdateTimestamp
    @Column(name = "updated_at")
    private LocalDateTime updatedAt;

    @Column(name = "completed_at")
    private LocalDateTime completedAt;

    @Column(name = "error_message", length = 1024)
    private String errorMessage;

    /**
     * Add a sub-task to this job (manages bidirectional relationship)
     */
    public void addSubTask(AiSubTask task) {
        subTasks.add(task);
        task.setAiJob(this);
    }

    /**
     * Remove a sub-task from this job
     */
    public void removeSubTask(AiSubTask task) {
        subTasks.remove(task);
        task.setAiJob(null);
    }

    /**
     * Mark the job as in progress
     */
    public void markInProgress() {
        this.overallStatus = AiJobStatus.IN_PROGRESS;
    }

    /**
     * Mark the job as completed successfully
     */
    public void markCompleted() {
        this.overallStatus = AiJobStatus.COMPLETED;
        this.completedAt = LocalDateTime.now();
    }

    /**
     * Mark the job as failed
     */
    public void markFailed(String errorMessage) {
        this.overallStatus = AiJobStatus.FAILED;
        this.errorMessage = errorMessage;
        this.completedAt = LocalDateTime.now();
    }

    /**
     * Mark the job as partially successful (some tasks completed, some failed)
     */
    public void markPartialSuccess() {
        this.overallStatus = AiJobStatus.PARTIAL_SUCCESS;
        this.completedAt = LocalDateTime.now();
    }

    /**
     * Mark the job as cancelled
     */
    public void markCancelled() {
        this.overallStatus = AiJobStatus.CANCELLED;
        this.completedAt = LocalDateTime.now();
    }

    /**
     * Mark the job as timed out
     */
    public void markTimeout() {
        this.overallStatus = AiJobStatus.TIMEOUT;
        this.completedAt = LocalDateTime.now();
    }

    /**
     * Check if the job is in a terminal state
     */
    public boolean isTerminal() {
        return overallStatus == AiJobStatus.COMPLETED
                || overallStatus == AiJobStatus.FAILED
                || overallStatus == AiJobStatus.PARTIAL_SUCCESS
                || overallStatus == AiJobStatus.CANCELLED
                || overallStatus == AiJobStatus.TIMEOUT;
    }

    /**
     * Get count of sub-tasks by status
     */
    public long countSubTasksByStatus(AiTaskStatus status) {
        return subTasks.stream()
                .filter(task -> task.getStatus() == status)
                .count();
    }

    /**
     * Generate a new job ID
     */
    public static String generateJobId() {
        return "aijob_" + java.util.UUID.randomUUID().toString()
                .replace("-", "").substring(0, 16);
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/ai/AiJobStatus.java

```java
package com.newsinsight.collector.entity.ai;

/**
 * Status of an AI orchestration job.
 * Represents the aggregate state across all sub-tasks.
 */
public enum AiJobStatus {
    /**
     * Job has been created but no sub-tasks have started
     */
    PENDING,

    /**
     * At least one sub-task is currently being processed
     */
    IN_PROGRESS,

    /**
     * All sub-tasks completed successfully
     */
    COMPLETED,

    /**
     * Some sub-tasks completed, some failed/timed out
     */
    PARTIAL_SUCCESS,

    /**
     * All sub-tasks failed
     */
    FAILED,

    /**
     * Job was cancelled before completion
     */
    CANCELLED,

    /**
     * Job timed out waiting for sub-task callbacks
     */
    TIMEOUT
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/ai/AiProvider.java

```java
package com.newsinsight.collector.entity.ai;

/**
 * AI provider/workflow types for task routing.
 * Each provider represents a different n8n workflow or external AI service.
 */
public enum AiProvider {
    /**
     * Universal agent for general-purpose AI tasks.
     * n8n workflow: /webhook/universal-agent
     */
    UNIVERSAL_AGENT("universal-agent", "General-purpose AI agent"),

    /**
     * Deep reader for in-depth content analysis.
     * n8n workflow: /webhook/deep-reader (crawl-agent)
     */
    DEEP_READER("deep-reader", "Deep content analysis and evidence extraction"),

    /**
     * Scout agent for quick reconnaissance and URL discovery.
     * n8n workflow: /webhook/scout-agent
     */
    SCOUT("scout-agent", "Quick reconnaissance and URL discovery"),

    /**
     * Local quick processing for simple tasks without external calls.
     * Processed internally without n8n.
     */
    LOCAL_QUICK("local-quick", "Local quick processing");

    private final String workflowPath;
    private final String description;

    AiProvider(String workflowPath, String description) {
        this.workflowPath = workflowPath;
        this.description = description;
    }

    public String getWorkflowPath() {
        return workflowPath;
    }

    public String getDescription() {
        return description;
    }

    /**
     * Check if this provider requires external n8n workflow
     */
    public boolean isExternal() {
        return this != LOCAL_QUICK;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/ai/AiSubTask.java

```java
package com.newsinsight.collector.entity.ai;

import jakarta.persistence.*;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.hibernate.annotations.CreationTimestamp;
import org.hibernate.annotations.UpdateTimestamp;

import java.time.LocalDateTime;
import java.util.UUID;

/**
 * Entity representing an individual AI sub-task within a job.
 * Each sub-task is processed by a specific AI provider (n8n workflow).
 */
@Entity
@Table(name = "ai_sub_tasks", indexes = {
        @Index(name = "idx_ai_sub_tasks_job_id", columnList = "job_id"),
        @Index(name = "idx_ai_sub_tasks_status", columnList = "status"),
        @Index(name = "idx_ai_sub_tasks_provider_id", columnList = "provider_id"),
        @Index(name = "idx_ai_sub_tasks_created_at", columnList = "created_at")
})
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class AiSubTask {

    @Id
    @Column(name = "sub_task_id", length = 64)
    private String id;

    @ManyToOne(fetch = FetchType.LAZY)
    @JoinColumn(name = "job_id", nullable = false)
    private AiJob aiJob;

    @Enumerated(EnumType.STRING)
    @Column(name = "provider_id", nullable = false, length = 32)
    private AiProvider providerId;

    @Column(name = "task_type", nullable = false, length = 64)
    private String taskType;

    @Enumerated(EnumType.STRING)
    @Column(name = "status", nullable = false, length = 32)
    @Builder.Default
    private AiTaskStatus status = AiTaskStatus.PENDING;

    @Lob
    @Column(name = "result_json", columnDefinition = "TEXT")
    private String resultJson;

    @Column(name = "error_message", length = 1024)
    private String errorMessage;

    @Column(name = "retry_count")
    @Builder.Default
    private Integer retryCount = 0;

    @CreationTimestamp
    @Column(name = "created_at", updatable = false)
    private LocalDateTime createdAt;

    @UpdateTimestamp
    @Column(name = "updated_at")
    private LocalDateTime updatedAt;

    @Column(name = "completed_at")
    private LocalDateTime completedAt;

    /**
     * Mark the task as in progress
     */
    public void markInProgress() {
        this.status = AiTaskStatus.IN_PROGRESS;
    }

    /**
     * Mark the task as completed with result
     */
    public void markCompleted(String resultJson) {
        this.status = AiTaskStatus.COMPLETED;
        this.resultJson = resultJson;
        this.completedAt = LocalDateTime.now();
    }

    /**
     * Mark the task as failed
     */
    public void markFailed(String errorMessage) {
        this.status = AiTaskStatus.FAILED;
        this.errorMessage = errorMessage;
        this.completedAt = LocalDateTime.now();
    }

    /**
     * Mark the task as cancelled
     */
    public void markCancelled() {
        this.status = AiTaskStatus.CANCELLED;
        this.completedAt = LocalDateTime.now();
    }

    /**
     * Mark the task as timed out
     */
    public void markTimeout() {
        this.status = AiTaskStatus.TIMEOUT;
        this.completedAt = LocalDateTime.now();
    }

    /**
     * Increment retry count
     */
    public void incrementRetry() {
        this.retryCount++;
    }

    /**
     * Check if the task is in a terminal state
     */
    public boolean isTerminal() {
        return status == AiTaskStatus.COMPLETED
                || status == AiTaskStatus.FAILED
                || status == AiTaskStatus.CANCELLED
                || status == AiTaskStatus.TIMEOUT;
    }

    /**
     * Check if the task can be retried
     */
    public boolean canRetry(int maxRetries) {
        return retryCount < maxRetries && !isTerminal();
    }

    /**
     * Get the job ID (helper for when job is lazy loaded)
     */
    public String getJobId() {
        return aiJob != null ? aiJob.getId() : null;
    }

    /**
     * Create a new sub-task for a job
     */
    public static AiSubTask create(AiJob job, AiProvider provider, String taskType) {
        AiSubTask task = AiSubTask.builder()
                .id(generateSubTaskId())
                .providerId(provider)
                .taskType(taskType)
                .status(AiTaskStatus.PENDING)
                .retryCount(0)
                .build();
        job.addSubTask(task);
        return task;
    }

    /**
     * Generate a new sub-task ID
     */
    public static String generateSubTaskId() {
        return "subtask_" + UUID.randomUUID().toString()
                .replace("-", "").substring(0, 16);
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/ai/AiTaskStatus.java

```java
package com.newsinsight.collector.entity.ai;

/**
 * Status of an individual AI sub-task.
 */
public enum AiTaskStatus {
    /**
     * Task has been created but not yet started
     */
    PENDING,

    /**
     * Task is currently being processed by a worker/n8n
     */
    IN_PROGRESS,

    /**
     * Task completed successfully
     */
    COMPLETED,

    /**
     * Task failed due to an error
     */
    FAILED,

    /**
     * Task was cancelled before completion
     */
    CANCELLED,

    /**
     * Task timed out waiting for callback
     */
    TIMEOUT
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/analysis/ArticleAnalysis.java

```java
package com.newsinsight.collector.entity.analysis;

import jakarta.persistence.*;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.hibernate.annotations.CreationTimestamp;
import org.hibernate.annotations.JdbcTypeCode;
import org.hibernate.annotations.UpdateTimestamp;
import org.hibernate.type.SqlTypes;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Map;

/**
 * 기사 분석 결과 엔티티.
 * 
 * 각종 ML Add-on의 분석 결과를 통합 저장.
 * 감정 분석, 신뢰도, 편향도, 팩트체크 결과 등을 한 곳에서 조회 가능.
 */
@Entity
@Table(name = "article_analysis", indexes = {
    @Index(name = "idx_analysis_article_id", columnList = "article_id"),
    @Index(name = "idx_analysis_reliability", columnList = "reliability_score"),
    @Index(name = "idx_analysis_sentiment", columnList = "sentiment_label"),
    @Index(name = "idx_analysis_bias", columnList = "bias_label"),
    @Index(name = "idx_analysis_misinfo", columnList = "misinfo_risk"),
    @Index(name = "idx_analysis_updated", columnList = "updated_at")
})
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class ArticleAnalysis {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    /**
     * 분석 대상 기사 ID (collected_data.id와 연결)
     */
    @Column(name = "article_id", nullable = false, unique = true)
    private Long articleId;

    // ========== 요약 ==========

    /**
     * AI 생성 요약
     */
    @Column(name = "summary", columnDefinition = "TEXT")
    private String summary;

    /**
     * 핵심 문장 (추출 요약)
     */
    @Column(name = "key_sentences", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private List<String> keySentences;

    // ========== 감정 분석 ==========

    /**
     * 감정 점수 (-1.0 ~ 1.0 또는 0 ~ 100)
     * -1 = 매우 부정, 0 = 중립, 1 = 매우 긍정
     */
    @Column(name = "sentiment_score")
    private Double sentimentScore;

    /**
     * 감정 레이블 (positive, negative, neutral)
     */
    @Column(name = "sentiment_label", length = 20)
    private String sentimentLabel;

    /**
     * 감정 분포 (긍정/부정/중립 비율)
     * {"positive": 0.2, "negative": 0.7, "neutral": 0.1}
     */
    @Column(name = "sentiment_distribution", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private Map<String, Double> sentimentDistribution;

    /**
     * 톤 분석 (보도형 vs 의견형)
     * {"factual": 0.8, "opinion": 0.2}
     */
    @Column(name = "tone_analysis", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private Map<String, Double> toneAnalysis;

    // ========== 편향도 분석 ==========

    /**
     * 편향 레이블 (left, right, center, pro_government, pro_corporate 등)
     */
    @Column(name = "bias_label", length = 50)
    private String biasLabel;

    /**
     * 편향 점수 (-1.0 ~ 1.0)
     * -1 = 극좌, 0 = 중립, 1 = 극우 (정치적 스펙트럼)
     */
    @Column(name = "bias_score")
    private Double biasScore;

    /**
     * 편향 세부 분석
     * {"political_left": 0.3, "pro_government": 0.2, ...}
     */
    @Column(name = "bias_details", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private Map<String, Double> biasDetails;

    // ========== 신뢰도 분석 ==========

    /**
     * 신뢰도 점수 (0 ~ 100)
     */
    @Column(name = "reliability_score")
    private Double reliabilityScore;

    /**
     * 신뢰도 등급 (high, medium, low)
     */
    @Column(name = "reliability_grade", length = 20)
    private String reliabilityGrade;

    /**
     * 신뢰도 요인 분석
     * {"source_reputation": 0.8, "citation_quality": 0.6, "consistency": 0.7}
     */
    @Column(name = "reliability_factors", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private Map<String, Double> reliabilityFactors;

    // ========== 허위정보/팩트체크 ==========

    /**
     * 허위정보 위험도 (low, mid, high)
     */
    @Column(name = "misinfo_risk", length = 20)
    private String misinfoRisk;

    /**
     * 허위정보 점수 (0 ~ 1)
     */
    @Column(name = "misinfo_score")
    private Double misinfoScore;

    /**
     * 팩트체크 상태 (verified, suspicious, conflicting, unverified)
     */
    @Column(name = "factcheck_status", length = 30)
    private String factcheckStatus;

    /**
     * 팩트체크 상세 노트/근거
     */
    @Column(name = "factcheck_notes", columnDefinition = "TEXT")
    private String factcheckNotes;

    /**
     * 검증된 주장들
     * [{"claim": "...", "verified": true, "sources": [...]}]
     */
    @Column(name = "verified_claims", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private List<Map<String, Object>> verifiedClaims;

    // ========== 주제/토픽 ==========

    /**
     * 주요 토픽/카테고리
     * ["정치", "외교", "북한"]
     */
    @Column(name = "topics", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private List<String> topics;

    /**
     * 토픽별 연관도
     * {"정치": 0.9, "외교": 0.7, "북한": 0.5}
     */
    @Column(name = "topic_scores", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private Map<String, Double> topicScores;

    // ========== 개체명 인식 (NER) ==========

    /**
     * 추출된 인물
     * [{"name": "홍길동", "role": "장관", "sentiment": "neutral"}]
     */
    @Column(name = "entities_person", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private List<Map<String, Object>> entitiesPerson;

    /**
     * 추출된 기관/조직
     */
    @Column(name = "entities_org", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private List<Map<String, Object>> entitiesOrg;

    /**
     * 추출된 장소/지역
     */
    @Column(name = "entities_location", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private List<Map<String, Object>> entitiesLocation;

    /**
     * 기타 개체 (날짜, 금액, 수치 등)
     */
    @Column(name = "entities_misc", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private List<Map<String, Object>> entitiesMisc;

    // ========== 위험 태그 ==========

    /**
     * 위험 태그 목록
     * ["clickbait", "sensational", "unverified_source"]
     */
    @Column(name = "risk_tags", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private List<String> riskTags;

    /**
     * 독성/혐오 점수 (0 ~ 1)
     */
    @Column(name = "toxicity_score")
    private Double toxicityScore;

    /**
     * 선정성 점수 (0 ~ 1)
     */
    @Column(name = "sensationalism_score")
    private Double sensationalismScore;

    // ========== 분석 메타데이터 ==========

    /**
     * 분석에 사용된 Add-on 목록
     * ["sentiment-v1", "factcheck-v2", "ner-korean-v1"]
     */
    @Column(name = "analyzed_by", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private List<String> analyzedBy;

    /**
     * 분석 완료 상태
     * {"sentiment": true, "factcheck": false, "ner": true}
     */
    @Column(name = "analysis_status", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private Map<String, Boolean> analysisStatus;

    /**
     * 전체 분석 완료 여부
     */
    @Column(name = "fully_analyzed")
    @Builder.Default
    private Boolean fullyAnalyzed = false;

    /**
     * 생성일시
     */
    @CreationTimestamp
    @Column(name = "created_at", nullable = false, updatable = false)
    private LocalDateTime createdAt;

    /**
     * 마지막 업데이트
     */
    @UpdateTimestamp
    @Column(name = "updated_at")
    private LocalDateTime updatedAt;

    // ========== Helper Methods ==========

    public String getReliabilityColor() {
        if (reliabilityScore == null) return "gray";
        if (reliabilityScore >= 70) return "green";
        if (reliabilityScore >= 40) return "yellow";
        return "red";
    }

    public String getSentimentEmoji() {
        if (sentimentLabel == null) return "⚪";
        return switch (sentimentLabel.toLowerCase()) {
            case "positive" -> "😊";
            case "negative" -> "😠";
            default -> "😐";
        };
    }

    public boolean needsFactCheck() {
        return misinfoRisk != null && 
               (misinfoRisk.equals("high") || misinfoRisk.equals("mid"));
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/analysis/ArticleDiscussion.java

```java
package com.newsinsight.collector.entity.analysis;

import jakarta.persistence.*;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.hibernate.annotations.CreationTimestamp;
import org.hibernate.annotations.JdbcTypeCode;
import org.hibernate.annotations.UpdateTimestamp;
import org.hibernate.type.SqlTypes;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Map;

/**
 * 기사 관련 커뮤니티/댓글/여론 분석 결과 엔티티.
 * 
 * 포털 댓글, SNS, 커뮤니티 등에서 수집된 반응 데이터를 분석하여 저장.
 */
@Entity
@Table(name = "article_discussion", indexes = {
    @Index(name = "idx_discussion_article_id", columnList = "article_id"),
    @Index(name = "idx_discussion_sentiment", columnList = "overall_sentiment"),
    @Index(name = "idx_discussion_updated", columnList = "updated_at")
})
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class ArticleDiscussion {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    /**
     * 분석 대상 기사 ID
     */
    @Column(name = "article_id", nullable = false, unique = true)
    private Long articleId;

    // ========== 수집 메타데이터 ==========

    /**
     * 총 댓글/반응 수
     */
    @Column(name = "total_comment_count")
    @Builder.Default
    private Integer totalCommentCount = 0;

    /**
     * 분석된 댓글 수
     */
    @Column(name = "analyzed_count")
    @Builder.Default
    private Integer analyzedCount = 0;

    /**
     * 수집 플랫폼 목록
     * ["portal_comments", "twitter", "community_dcinside", "community_fmkorea"]
     */
    @Column(name = "platforms", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private List<String> platforms;

    /**
     * 플랫폼별 댓글 수
     * {"portal_comments": 150, "twitter": 45, "community": 80}
     */
    @Column(name = "platform_counts", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private Map<String, Integer> platformCounts;

    // ========== 전체 감정 분석 ==========

    /**
     * 전체 감정 레이블 (positive, negative, neutral, mixed)
     */
    @Column(name = "overall_sentiment", length = 20)
    private String overallSentiment;

    /**
     * 감정 분포
     * {"positive": 0.2, "negative": 0.6, "neutral": 0.2}
     */
    @Column(name = "sentiment_distribution", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private Map<String, Double> sentimentDistribution;

    /**
     * 세부 감정 분석 (분노, 슬픔, 불안, 기쁨 등)
     * {"anger": 0.4, "anxiety": 0.2, "sadness": 0.15, "joy": 0.1, "surprise": 0.15}
     */
    @Column(name = "emotion_distribution", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private Map<String, Double> emotionDistribution;

    /**
     * 지배적 감정
     */
    @Column(name = "dominant_emotion", length = 30)
    private String dominantEmotion;

    // ========== 스탠스/입장 분석 ==========

    /**
     * 찬반 분포
     * {"agree": 0.3, "disagree": 0.5, "neutral": 0.2}
     */
    @Column(name = "stance_distribution", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private Map<String, Double> stanceDistribution;

    /**
     * 전체적인 여론 방향 (supportive, opposing, divided, neutral)
     */
    @Column(name = "overall_stance", length = 30)
    private String overallStance;

    // ========== 독성/품질 분석 ==========

    /**
     * 전체 독성 점수 (0 ~ 1)
     */
    @Column(name = "toxicity_score")
    private Double toxicityScore;

    /**
     * 혐오발언 비율
     */
    @Column(name = "hate_speech_ratio")
    private Double hateSpeechRatio;

    /**
     * 욕설 비율
     */
    @Column(name = "profanity_ratio")
    private Double profanityRatio;

    /**
     * 여론 건전성 점수 (0 ~ 100)
     */
    @Column(name = "discussion_quality_score")
    private Double discussionQualityScore;

    // ========== 키워드/토픽 ==========

    /**
     * 상위 키워드
     * [{"word": "정부", "count": 45}, {"word": "반대", "count": 32}]
     */
    @Column(name = "top_keywords", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private List<Map<String, Object>> topKeywords;

    /**
     * 댓글에서만 언급되는 이슈 (기사에 없는 관점)
     * ["언론이 숨기는 진실", "과거 사례 비교"]
     */
    @Column(name = "emerging_topics", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private List<String> emergingTopics;

    // ========== 시계열 분석 ==========

    /**
     * 시간대별 여론 변화
     * [{"hour": "2025-01-15T10:00", "sentiment": -0.3, "volume": 25}, ...]
     */
    @Column(name = "time_series", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private List<Map<String, Object>> timeSeries;

    /**
     * 여론 반전 시점 (있는 경우)
     */
    @Column(name = "sentiment_shift_at")
    private LocalDateTime sentimentShiftAt;

    /**
     * 피크 시점 (가장 많은 반응이 있던 시간)
     */
    @Column(name = "peak_activity_at")
    private LocalDateTime peakActivityAt;

    // ========== 조작/봇 탐지 ==========

    /**
     * 의심스러운 패턴 탐지 여부
     */
    @Column(name = "suspicious_pattern_detected")
    @Builder.Default
    private Boolean suspiciousPatternDetected = false;

    /**
     * 봇/조작 의심 점수 (0 ~ 1)
     */
    @Column(name = "bot_likelihood_score")
    private Double botLikelihoodScore;

    /**
     * 탐지된 의심 패턴 목록
     * ["repeated_text", "coordinated_posting", "new_account_surge"]
     */
    @Column(name = "suspicious_patterns", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private List<String> suspiciousPatterns;

    // ========== 대표 댓글 ==========

    /**
     * 대표 긍정 댓글 샘플
     */
    @Column(name = "sample_positive_comments", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private List<Map<String, Object>> samplePositiveComments;

    /**
     * 대표 부정 댓글 샘플
     */
    @Column(name = "sample_negative_comments", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private List<Map<String, Object>> sampleNegativeComments;

    /**
     * 가장 많은 공감을 받은 댓글
     */
    @Column(name = "top_engaged_comments", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private List<Map<String, Object>> topEngagedComments;

    // ========== 플랫폼별 비교 ==========

    /**
     * 플랫폼별 감정 비교
     * {"portal": {"positive": 0.3, "negative": 0.5}, "twitter": {"positive": 0.4, ...}}
     */
    @Column(name = "platform_sentiment_comparison", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private Map<String, Map<String, Double>> platformSentimentComparison;

    // ========== 메타데이터 ==========

    /**
     * 분석에 사용된 Add-on
     */
    @Column(name = "analyzed_by", columnDefinition = "jsonb")
    @JdbcTypeCode(SqlTypes.JSON)
    private List<String> analyzedBy;

    /**
     * 마지막 크롤링 시점
     */
    @Column(name = "last_crawled_at")
    private LocalDateTime lastCrawledAt;

    /**
     * 생성일시
     */
    @CreationTimestamp
    @Column(name = "created_at", nullable = false, updatable = false)
    private LocalDateTime createdAt;

    /**
     * 마지막 업데이트
     */
    @UpdateTimestamp
    @Column(name = "updated_at")
    private LocalDateTime updatedAt;

    // ========== Helper Methods ==========

    public String getSentimentSummary() {
        if (sentimentDistribution == null) return "분석 대기 중";
        
        double negative = sentimentDistribution.getOrDefault("negative", 0.0);
        double positive = sentimentDistribution.getOrDefault("positive", 0.0);
        
        if (negative > 0.6) return "부정적 여론 우세";
        if (positive > 0.6) return "긍정적 여론 우세";
        if (Math.abs(negative - positive) < 0.1) return "여론 분분";
        return "중립적";
    }

    public boolean isControversial() {
        if (stanceDistribution == null) return false;
        double agree = stanceDistribution.getOrDefault("agree", 0.0);
        double disagree = stanceDistribution.getOrDefault("disagree", 0.0);
        return Math.abs(agree - disagree) < 0.2 && (agree + disagree) > 0.6;
    }

    public String getDiscussionHealthGrade() {
        if (discussionQualityScore == null) return "N/A";
        if (discussionQualityScore >= 70) return "양호";
        if (discussionQualityScore >= 40) return "보통";
        return "주의";
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/autocrawl/ContentType.java

```java
package com.newsinsight.collector.entity.autocrawl;

/**
 * 예상 콘텐츠 타입
 */
public enum ContentType {
    /**
     * 뉴스 기사
     */
    NEWS,
    
    /**
     * 블로그/개인 사이트
     */
    BLOG,
    
    /**
     * 포럼/커뮤니티
     */
    FORUM,
    
    /**
     * 소셜 미디어
     */
    SOCIAL,
    
    /**
     * 공식 문서/보고서
     */
    OFFICIAL,
    
    /**
     * 학술/연구
     */
    ACADEMIC,
    
    /**
     * 미분류
     */
    UNKNOWN
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/autocrawl/CrawlTarget.java

```java
package com.newsinsight.collector.entity.autocrawl;

import jakarta.persistence.*;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.hibernate.annotations.CreationTimestamp;
import org.hibernate.annotations.UpdateTimestamp;

import java.time.LocalDateTime;

/**
 * 자동 크롤링 대상 URL 엔티티.
 * 검색, 기사 분석, 외부 링크 등에서 자동으로 발견된 URL을 관리합니다.
 */
@Entity
@Table(name = "crawl_targets", indexes = {
        @Index(name = "idx_crawl_target_url_hash", columnList = "urlHash"),
        @Index(name = "idx_crawl_target_status", columnList = "status"),
        @Index(name = "idx_crawl_target_priority", columnList = "priority DESC"),
        @Index(name = "idx_crawl_target_discovered", columnList = "discoveredAt DESC")
})
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class CrawlTarget {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    /**
     * 크롤링 대상 URL
     */
    @Column(nullable = false, columnDefinition = "TEXT")
    private String url;

    /**
     * URL 해시 (중복 체크용)
     */
    @Column(nullable = false, length = 64)
    private String urlHash;

    /**
     * 발견 출처 (SEARCH, ARTICLE_LINK, TRENDING, RSS_MENTION, MANUAL, DEEP_SEARCH)
     */
    @Enumerated(EnumType.STRING)
    @Column(nullable = false, length = 30)
    private DiscoverySource discoverySource;

    /**
     * 발견 컨텍스트 (검색어, 원본 기사 ID 등)
     */
    @Column(columnDefinition = "TEXT")
    private String discoveryContext;

    /**
     * 크롤링 우선순위 (0-100, 높을수록 우선)
     */
    @Column(nullable = false)
    @Builder.Default
    private Integer priority = 50;

    /**
     * 상태
     */
    @Enumerated(EnumType.STRING)
    @Column(nullable = false, length = 20)
    @Builder.Default
    private CrawlTargetStatus status = CrawlTargetStatus.PENDING;

    /**
     * 도메인 (파싱된 호스트)
     */
    @Column(length = 255)
    private String domain;

    /**
     * 예상 콘텐츠 타입 (NEWS, BLOG, FORUM, SOCIAL, UNKNOWN)
     */
    @Enumerated(EnumType.STRING)
    @Column(length = 20)
    @Builder.Default
    private ContentType expectedContentType = ContentType.UNKNOWN;

    /**
     * 관련 키워드 (쉼표 구분)
     */
    @Column(columnDefinition = "TEXT")
    private String relatedKeywords;

    /**
     * 재시도 횟수
     */
    @Column(nullable = false)
    @Builder.Default
    private Integer retryCount = 0;

    /**
     * 최대 재시도 횟수
     */
    @Column(nullable = false)
    @Builder.Default
    private Integer maxRetries = 3;

    /**
     * 마지막 시도 시각
     */
    private LocalDateTime lastAttemptAt;

    /**
     * 다음 시도 가능 시각 (재시도 백오프용)
     */
    private LocalDateTime nextAttemptAfter;

    /**
     * 마지막 오류 메시지
     */
    @Column(columnDefinition = "TEXT")
    private String lastError;

    /**
     * 크롤링 성공 시 저장된 CollectedData ID
     */
    private Long collectedDataId;

    /**
     * 발견 시각
     */
    @CreationTimestamp
    @Column(nullable = false, updatable = false)
    private LocalDateTime discoveredAt;

    /**
     * 마지막 수정 시각
     */
    @UpdateTimestamp
    private LocalDateTime updatedAt;

    /**
     * 처리 완료 시각
     */
    private LocalDateTime completedAt;

    // ========================================
    // 유틸리티 메서드
    // ========================================

    public void markInProgress() {
        this.status = CrawlTargetStatus.IN_PROGRESS;
        this.lastAttemptAt = LocalDateTime.now();
    }

    public void markCompleted(Long collectedDataId) {
        this.status = CrawlTargetStatus.COMPLETED;
        this.completedAt = LocalDateTime.now();
        this.collectedDataId = collectedDataId;
    }

    public void markFailed(String error) {
        this.retryCount++;
        this.lastError = error;
        this.lastAttemptAt = LocalDateTime.now();

        if (this.retryCount >= this.maxRetries) {
            this.status = CrawlTargetStatus.FAILED;
        } else {
            this.status = CrawlTargetStatus.PENDING;
            // 지수 백오프: 2^retry * 5분
            int delayMinutes = (int) Math.pow(2, this.retryCount) * 5;
            this.nextAttemptAfter = LocalDateTime.now().plusMinutes(delayMinutes);
        }
    }

    public void markSkipped(String reason) {
        this.status = CrawlTargetStatus.SKIPPED;
        this.lastError = reason;
        this.completedAt = LocalDateTime.now();
    }

    public boolean isRetryable() {
        if (status != CrawlTargetStatus.PENDING) return false;
        if (retryCount >= maxRetries) return false;
        if (nextAttemptAfter != null && LocalDateTime.now().isBefore(nextAttemptAfter)) return false;
        return true;
    }

    /**
     * 우선순위 부스트 (특정 조건에서 우선순위 상승)
     */
    public void boostPriority(int amount) {
        this.priority = Math.min(100, this.priority + amount);
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/autocrawl/CrawlTargetStatus.java

```java
package com.newsinsight.collector.entity.autocrawl;

/**
 * 크롤링 대상 URL 상태
 */
public enum CrawlTargetStatus {
    /**
     * 대기 중 (처리 가능)
     */
    PENDING,
    
    /**
     * 처리 중
     */
    IN_PROGRESS,
    
    /**
     * 완료
     */
    COMPLETED,
    
    /**
     * 실패 (재시도 횟수 초과)
     */
    FAILED,
    
    /**
     * 건너뜀 (중복, 블랙리스트 등)
     */
    SKIPPED,
    
    /**
     * 취소됨
     */
    CANCELLED,
    
    /**
     * 만료됨 (오래 대기 중인 상태로 방치됨)
     */
    EXPIRED
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/autocrawl/DiscoverySource.java

```java
package com.newsinsight.collector.entity.autocrawl;

/**
 * 크롤링 대상 URL 발견 출처
 */
public enum DiscoverySource {
    /**
     * 사용자 검색 결과에서 발견
     */
    SEARCH,
    
    /**
     * 기사 본문 내 외부 링크에서 발견
     */
    ARTICLE_LINK,
    
    /**
     * 트렌딩 토픽/급상승 검색어에서 발견
     */
    TRENDING,
    
    /**
     * RSS 피드 본문 내 언급에서 발견
     */
    RSS_MENTION,
    
    /**
     * Deep Search 결과에서 발견
     */
    DEEP_SEARCH,
    
    /**
     * AI 분석 추천 URL
     */
    AI_RECOMMENDATION,
    
    /**
     * 관리자 수동 등록
     */
    MANUAL,
    
    /**
     * 외부 API에서 수신
     */
    EXTERNAL_API
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/browser/BrowserJobHistory.java

```java
package com.newsinsight.collector.entity.browser;

import jakarta.persistence.*;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.hibernate.annotations.CreationTimestamp;
import org.hibernate.annotations.JdbcTypeCode;
import org.hibernate.annotations.UpdateTimestamp;
import org.hibernate.type.SqlTypes;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Map;

/**
 * Entity for storing Browser-Use automation job history.
 * Tracks all browser automation tasks with their results,
 * screenshots, and extracted data.
 */
@Entity
@Table(name = "browser_job_history", indexes = {
        @Index(name = "idx_browser_job_job_id", columnList = "job_id"),
        @Index(name = "idx_browser_job_user_id", columnList = "user_id"),
        @Index(name = "idx_browser_job_status", columnList = "status"),
        @Index(name = "idx_browser_job_created_at", columnList = "created_at")
})
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class BrowserJobHistory {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    /**
     * Unique job ID from browser-use service
     */
    @Column(name = "job_id", length = 64, unique = true)
    private String jobId;

    /**
     * Task description
     */
    @Column(name = "task", nullable = false, length = 2048)
    private String task;

    /**
     * Target URL if specified
     */
    @Column(name = "target_url", length = 2048)
    private String targetUrl;

    /**
     * User ID
     */
    @Column(name = "user_id", length = 64)
    private String userId;

    /**
     * Session ID
     */
    @Column(name = "session_id", length = 64)
    private String sessionId;

    /**
     * Job status
     */
    @Enumerated(EnumType.STRING)
    @Column(name = "status", length = 32)
    @Builder.Default
    private BrowserJobStatus status = BrowserJobStatus.PENDING;

    /**
     * Job result/output
     */
    @Column(name = "result", columnDefinition = "text")
    private String result;

    /**
     * Structured result data
     */
    @JdbcTypeCode(SqlTypes.JSON)
    @Column(name = "result_data", columnDefinition = "jsonb")
    private Map<String, Object> resultData;

    /**
     * Extracted data (forms, tables, etc.)
     */
    @JdbcTypeCode(SqlTypes.JSON)
    @Column(name = "extracted_data", columnDefinition = "jsonb")
    private List<Map<String, Object>> extractedData;

    /**
     * Screenshot file paths or URLs
     */
    @JdbcTypeCode(SqlTypes.JSON)
    @Column(name = "screenshots", columnDefinition = "jsonb")
    private List<String> screenshots;

    /**
     * Action history/steps taken
     */
    @JdbcTypeCode(SqlTypes.JSON)
    @Column(name = "action_history", columnDefinition = "jsonb")
    private List<Map<String, Object>> actionHistory;

    /**
     * Error message if failed
     */
    @Column(name = "error_message", length = 2048)
    private String errorMessage;

    /**
     * Number of steps executed
     */
    @Column(name = "steps_count")
    @Builder.Default
    private Integer stepsCount = 0;

    /**
     * Execution time in milliseconds
     */
    @Column(name = "duration_ms")
    private Long durationMs;

    /**
     * Browser agent configuration used
     */
    @JdbcTypeCode(SqlTypes.JSON)
    @Column(name = "agent_config", columnDefinition = "jsonb")
    private Map<String, Object> agentConfig;

    /**
     * Related search history ID
     */
    @Column(name = "search_history_id")
    private Long searchHistoryId;

    /**
     * Project ID if associated with a project
     */
    @Column(name = "project_id")
    private Long projectId;

    @CreationTimestamp
    @Column(name = "created_at", updatable = false)
    private LocalDateTime createdAt;

    @UpdateTimestamp
    @Column(name = "updated_at")
    private LocalDateTime updatedAt;

    @Column(name = "started_at")
    private LocalDateTime startedAt;

    @Column(name = "completed_at")
    private LocalDateTime completedAt;

    /**
     * Mark job as started
     */
    public void markStarted() {
        this.status = BrowserJobStatus.RUNNING;
        this.startedAt = LocalDateTime.now();
    }

    /**
     * Mark job as completed
     */
    public void markCompleted(String result, Map<String, Object> resultData) {
        this.status = BrowserJobStatus.COMPLETED;
        this.result = result;
        this.resultData = resultData;
        this.completedAt = LocalDateTime.now();
        if (startedAt != null) {
            this.durationMs = java.time.Duration.between(startedAt, completedAt).toMillis();
        }
    }

    /**
     * Mark job as failed
     */
    public void markFailed(String errorMessage) {
        this.status = BrowserJobStatus.FAILED;
        this.errorMessage = errorMessage;
        this.completedAt = LocalDateTime.now();
        if (startedAt != null) {
            this.durationMs = java.time.Duration.between(startedAt, completedAt).toMillis();
        }
    }

    /**
     * Job status enum
     */
    public enum BrowserJobStatus {
        PENDING,
        RUNNING,
        WAITING_HUMAN,
        COMPLETED,
        FAILED,
        CANCELLED,
        TIMEOUT
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/chat/ChatHistory.java

```java
package com.newsinsight.collector.entity.chat;

import jakarta.persistence.*;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.hibernate.annotations.CreationTimestamp;
import org.hibernate.annotations.JdbcTypeCode;
import org.hibernate.type.SqlTypes;

import java.time.LocalDateTime;
import java.util.Map;

/**
 * 채팅 이력 (PostgreSQL)
 * 
 * MongoDB에서 동기화된 채팅 메시지를 RDB에 저장합니다.
 * 검색, 분석, 보고서 생성 등에 활용됩니다.
 */
@Entity
@Table(name = "chat_history", indexes = {
        @Index(name = "idx_chat_session_id", columnList = "session_id"),
        @Index(name = "idx_chat_user_id", columnList = "user_id"),
        @Index(name = "idx_chat_role", columnList = "role"),
        @Index(name = "idx_chat_created_at", columnList = "created_at")
})
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class ChatHistory {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    /**
     * MongoDB 세션 ID
     */
    @Column(name = "session_id", nullable = false, length = 64)
    private String sessionId;

    /**
     * MongoDB 메시지 ID
     */
    @Column(name = "message_id", nullable = false, length = 64)
    private String messageId;

    /**
     * 사용자 ID
     */
    @Column(name = "user_id", length = 64)
    private String userId;

    /**
     * 메시지 역할
     */
    @Column(nullable = false, length = 32)
    private String role; // user, assistant, system

    /**
     * 메시지 내용
     */
    @Column(columnDefinition = "TEXT")
    private String content;

    /**
     * 메시지 타입
     */
    @Column(name = "message_type", length = 32)
    private String messageType;

    /**
     * 메시지 메타데이터 (JSON)
     */
    @JdbcTypeCode(SqlTypes.JSON)
    @Column(name = "metadata", columnDefinition = "jsonb")
    private Map<String, Object> metadata;

    /**
     * 메시지 생성 시간
     */
    @CreationTimestamp
    @Column(name = "created_at", nullable = false, updatable = false)
    private LocalDateTime createdAt;

    /**
     * 벡터 임베딩 ID (참조용)
     */
    @Column(name = "embedding_id", length = 64)
    private String embeddingId;
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/chat/FactCheckChatSession.java

```java
package com.newsinsight.collector.entity.chat;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.springframework.data.annotation.CreatedDate;
import org.springframework.data.annotation.Id;
import org.springframework.data.annotation.LastModifiedDate;
import org.springframework.data.annotation.Version;
import org.springframework.data.mongodb.core.index.CompoundIndex;
import org.springframework.data.mongodb.core.index.CompoundIndexes;
import org.springframework.data.mongodb.core.index.Indexed;
import org.springframework.data.mongodb.core.mapping.Document;

import java.io.Serializable;
import java.time.LocalDateTime;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;

/**
 * 팩트체크 챗봇 세션 (MongoDB)
 * 
 * 채팅 세션 정보와 대화 이력을 저장합니다.
 * 
 * 개선사항:
 * - 복합 인덱스 추가
 * - 버전 관리 (낙관적 락)
 * - Audit 필드 추가
 * - 메시지 타입 세분화
 * - 직렬화 지원
 */
@Document(collection = "factcheck_chat_sessions")
@CompoundIndexes({
    @CompoundIndex(name = "idx_user_status", def = "{'userId': 1, 'status': 1}"),
    @CompoundIndex(name = "idx_status_sync", def = "{'status': 1, 'syncedToRdb': 1}"),
    @CompoundIndex(name = "idx_status_embed", def = "{'status': 1, 'embeddedToVectorDb': 1}"),
    @CompoundIndex(name = "idx_activity_status", def = "{'lastActivityAt': 1, 'status': 1}")
})
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class FactCheckChatSession implements Serializable {

    private static final long serialVersionUID = 1L;

    @Id
    private String id; // MongoDB ObjectId

    /**
     * 세션 ID (UUID)
     */
    @Indexed(unique = true)
    private String sessionId;

    /**
     * 사용자 ID (선택)
     */
    @Indexed
    private String userId;

    /**
     * 세션 시작 시간
     */
    @CreatedDate
    @Indexed
    private LocalDateTime startedAt;

    /**
     * 마지막 활동 시간
     */
    @LastModifiedDate
    @Indexed
    private LocalDateTime lastActivityAt;

    /**
     * 세션 종료 시간
     */
    private LocalDateTime endedAt;

    /**
     * 세션 상태
     */
    @Indexed
    @Builder.Default
    private SessionStatus status = SessionStatus.ACTIVE;

    /**
     * 대화 메시지 목록
     */
    @Builder.Default
    private List<ChatMessage> messages = new ArrayList<>();

    /**
     * 세션 메타데이터
     */
    private SessionMetadata metadata;

    /**
     * RDB 동기화 여부
     */
    @Indexed
    @Builder.Default
    private boolean syncedToRdb = false;

    /**
     * 벡터 DB 임베딩 여부
     */
    @Indexed
    @Builder.Default
    private boolean embeddedToVectorDb = false;

    /**
     * 마지막 RDB 동기화 시간
     */
    private LocalDateTime lastSyncedAt;

    /**
     * 마지막 임베딩 시간
     */
    private LocalDateTime lastEmbeddedAt;

    /**
     * 동기화된 메시지 수
     */
    @Builder.Default
    private int syncedMessageCount = 0;

    /**
     * 임베딩된 메시지 수
     */
    @Builder.Default
    private int embeddedMessageCount = 0;

    /**
     * 버전 (낙관적 락용)
     */
    @Version
    private Long version;

    /**
     * 세션 상태
     */
    public enum SessionStatus {
        ACTIVE,      // 활성 - 대화 진행 중
        COMPLETED,   // 완료 - 사용자가 종료
        EXPIRED,     // 만료 - 비활성으로 인한 자동 만료
        ARCHIVED,    // 아카이브 - 장기 보관
        ERROR        // 에러 - 처리 중 오류 발생
    }

    /**
     * 채팅 메시지
     */
    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class ChatMessage implements Serializable {
        private static final long serialVersionUID = 1L;
        
        private String messageId;
        private String role; // user, assistant, system
        private String content;
        private Long timestamp;
        private MessageType type;
        private Map<String, Object> metadata; // 추가 데이터 (증거, 검증 결과 등)
        
        // 추가 필드
        private Integer tokenCount; // 토큰 수 (비용 추적용)
        private Long processingTimeMs; // 처리 시간
        private String parentMessageId; // 부모 메시지 (스레드 지원)
        private Boolean synced; // RDB 동기화 여부
        private Boolean embedded; // 벡터 DB 임베딩 여부
    }

    /**
     * 메시지 타입
     */
    public enum MessageType {
        // 기본 메시지 타입
        MESSAGE,           // 일반 메시지
        SYSTEM,            // 시스템 메시지
        
        // 상태 관련
        STATUS,            // 상태 업데이트
        PROGRESS,          // 진행 상황
        
        // 팩트체크 관련
        CLAIM,             // 추출된 주장
        EVIDENCE,          // 수집된 증거
        VERIFICATION,      // 검증 결과
        ASSESSMENT,        // 신뢰도 평가
        
        // AI 관련
        AI_SYNTHESIS,      // AI 종합 분석
        AI_SUMMARY,        // AI 요약
        
        // 결과 관련
        COMPLETE,          // 완료
        ERROR,             // 에러
        WARNING,           // 경고
        
        // 피드백 관련
        FEEDBACK,          // 사용자 피드백
        RATING             // 평가
    }

    /**
     * 세션 메타데이터
     */
    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class SessionMetadata implements Serializable {
        private static final long serialVersionUID = 1L;
        
        // 클라이언트 정보
        private String userAgent;
        private String ipAddress;
        private String language;
        private String timezone;
        private String platform; // web, mobile, api
        
        // 세션 통계
        private Integer messageCount;
        private Integer factCheckCount;
        private Integer errorCount;
        private Double averageResponseTime;
        private Long totalTokensUsed;
        
        // 첫 번째/마지막 주제
        private String firstTopic;
        private String lastTopic;
        
        // 세션 품질 지표
        private Double satisfactionScore; // 사용자 만족도 (1-5)
        private Boolean feedbackProvided; // 피드백 제공 여부
        
        // 기타
        private Map<String, Object> customData; // 커스텀 데이터
    }

    // =====================
    // 편의 메서드
    // =====================

    /**
     * 메시지 추가
     */
    public void addMessage(ChatMessage message) {
        if (messages == null) {
            messages = new ArrayList<>();
        }
        messages.add(message);
        updateMetadataOnMessage();
    }

    /**
     * 메시지 추가 후 메타데이터 업데이트
     */
    private void updateMetadataOnMessage() {
        if (metadata == null) {
            metadata = SessionMetadata.builder()
                    .messageCount(0)
                    .factCheckCount(0)
                    .errorCount(0)
                    .build();
        }
        metadata.setMessageCount(messages.size());
    }

    /**
     * 세션 종료
     */
    public void close() {
        this.status = SessionStatus.COMPLETED;
        this.endedAt = LocalDateTime.now();
    }

    /**
     * 세션 만료
     */
    public void expire() {
        this.status = SessionStatus.EXPIRED;
        this.endedAt = LocalDateTime.now();
    }

    /**
     * 활성 세션인지 확인
     */
    public boolean isActive() {
        return status == SessionStatus.ACTIVE;
    }

    /**
     * 동기화 필요 여부 확인
     */
    public boolean needsSync() {
        return !syncedToRdb && (status == SessionStatus.COMPLETED || status == SessionStatus.EXPIRED);
    }

    /**
     * 임베딩 필요 여부 확인
     */
    public boolean needsEmbedding() {
        return syncedToRdb && !embeddedToVectorDb;
    }

    /**
     * 마지막 사용자 메시지 조회
     */
    public ChatMessage getLastUserMessage() {
        if (messages == null || messages.isEmpty()) {
            return null;
        }
        for (int i = messages.size() - 1; i >= 0; i--) {
            if ("user".equals(messages.get(i).getRole())) {
                return messages.get(i);
            }
        }
        return null;
    }

    /**
     * 마지막 어시스턴트 메시지 조회
     */
    public ChatMessage getLastAssistantMessage() {
        if (messages == null || messages.isEmpty()) {
            return null;
        }
        for (int i = messages.size() - 1; i >= 0; i--) {
            if ("assistant".equals(messages.get(i).getRole())) {
                return messages.get(i);
            }
        }
        return null;
    }

    /**
     * 특정 타입의 메시지 수 조회
     */
    public long countMessagesByType(MessageType type) {
        if (messages == null) {
            return 0;
        }
        return messages.stream()
                .filter(m -> m.getType() == type)
                .count();
    }

    /**
     * 동기화되지 않은 메시지 조회
     */
    public List<ChatMessage> getUnsyncedMessages() {
        if (messages == null) {
            return new ArrayList<>();
        }
        return messages.stream()
                .filter(m -> m.getSynced() == null || !m.getSynced())
                .toList();
    }

    /**
     * 세션 지속 시간 (초)
     */
    public long getDurationSeconds() {
        if (startedAt == null) {
            return 0;
        }
        LocalDateTime end = endedAt != null ? endedAt : LocalDateTime.now();
        return java.time.Duration.between(startedAt, end).getSeconds();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/feedback/SearchFeedback.java

```java
package com.newsinsight.collector.entity.feedback;

import jakarta.persistence.*;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.hibernate.annotations.CreationTimestamp;
import org.hibernate.annotations.JdbcTypeCode;
import org.hibernate.type.SqlTypes;

import java.time.LocalDateTime;
import java.util.Map;

/**
 * Entity for storing user feedback on search results.
 * Enables quality improvement through user ratings and comments.
 */
@Entity
@Table(name = "search_feedback", indexes = {
        @Index(name = "idx_feedback_search_history_id", columnList = "search_history_id"),
        @Index(name = "idx_feedback_user_id", columnList = "user_id"),
        @Index(name = "idx_feedback_rating", columnList = "rating"),
        @Index(name = "idx_feedback_created_at", columnList = "created_at")
})
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class SearchFeedback {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    /**
     * Related search history ID
     */
    @Column(name = "search_history_id", nullable = false)
    private Long searchHistoryId;

    /**
     * User who provided feedback
     */
    @Column(name = "user_id", length = 64)
    private String userId;

    /**
     * Session ID for anonymous feedback
     */
    @Column(name = "session_id", length = 64)
    private String sessionId;

    /**
     * Overall rating (1-5 stars)
     */
    @Column(name = "rating")
    private Integer rating;

    /**
     * Usefulness rating (1-5)
     */
    @Column(name = "usefulness_rating")
    private Integer usefulnessRating;

    /**
     * Accuracy rating (1-5)
     */
    @Column(name = "accuracy_rating")
    private Integer accuracyRating;

    /**
     * Relevance rating (1-5)
     */
    @Column(name = "relevance_rating")
    private Integer relevanceRating;

    /**
     * User's comment/feedback text
     */
    @Column(name = "comment", length = 2048)
    private String comment;

    /**
     * Improvement suggestions
     */
    @Column(name = "suggestions", length = 2048)
    private String suggestions;

    /**
     * Feedback type
     */
    @Enumerated(EnumType.STRING)
    @Column(name = "feedback_type", length = 32)
    @Builder.Default
    private FeedbackType feedbackType = FeedbackType.GENERAL;

    /**
     * Specific result index being rated (for individual result feedback)
     */
    @Column(name = "result_index")
    private Integer resultIndex;

    /**
     * Specific result URL being rated
     */
    @Column(name = "result_url", length = 2048)
    private String resultUrl;

    /**
     * Quick feedback (thumbs up/down)
     */
    @Column(name = "thumbs_up")
    private Boolean thumbsUp;

    /**
     * Issue categories selected
     */
    @JdbcTypeCode(SqlTypes.JSON)
    @Column(name = "issue_categories", columnDefinition = "jsonb")
    private java.util.List<String> issueCategories;

    /**
     * Additional metadata
     */
    @JdbcTypeCode(SqlTypes.JSON)
    @Column(name = "metadata", columnDefinition = "jsonb")
    private Map<String, Object> metadata;

    /**
     * Whether feedback has been reviewed by admin
     */
    @Column(name = "reviewed")
    @Builder.Default
    private Boolean reviewed = false;

    /**
     * Review notes by admin
     */
    @Column(name = "review_notes", length = 1024)
    private String reviewNotes;

    /**
     * Whether this feedback was used for model improvement
     */
    @Column(name = "used_for_training")
    @Builder.Default
    private Boolean usedForTraining = false;

    @CreationTimestamp
    @Column(name = "created_at", updatable = false)
    private LocalDateTime createdAt;

    /**
     * Feedback type enum
     */
    public enum FeedbackType {
        /** General search feedback */
        GENERAL,
        /** Feedback on specific result */
        RESULT_SPECIFIC,
        /** AI summary feedback */
        AI_SUMMARY,
        /** Fact-check accuracy feedback */
        FACT_CHECK,
        /** Report quality feedback */
        REPORT,
        /** Bug report */
        BUG_REPORT,
        /** Feature request */
        FEATURE_REQUEST
    }

    /**
     * Calculate average rating
     */
    public Double getAverageRating() {
        int count = 0;
        int sum = 0;
        
        if (usefulnessRating != null) { sum += usefulnessRating; count++; }
        if (accuracyRating != null) { sum += accuracyRating; count++; }
        if (relevanceRating != null) { sum += relevanceRating; count++; }
        
        // @CHECK 
        // 평균 평가점수 계산 - 평가점수가 하나라도 있는 경우 평균 평가점수를 반환, 그렇지 않으면 0을 반환
        return count > 0 ? (double) sum / count : (rating != null ? rating.doubleValue() : (double) 0);
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/project/Project.java

```java
package com.newsinsight.collector.entity.project;

import jakarta.persistence.*;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.hibernate.annotations.CreationTimestamp;
import org.hibernate.annotations.JdbcTypeCode;
import org.hibernate.annotations.UpdateTimestamp;
import org.hibernate.type.SqlTypes;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Map;

/**
 * Entity representing a user's project workspace.
 * Projects allow users to organize searches, collect news,
 * and collaborate on specific topics over time.
 */
@Entity
@Table(name = "projects", indexes = {
        @Index(name = "idx_project_owner_id", columnList = "owner_id"),
        @Index(name = "idx_project_status", columnList = "status"),
        @Index(name = "idx_project_category", columnList = "category"),
        @Index(name = "idx_project_created_at", columnList = "created_at"),
        @Index(name = "idx_project_last_activity", columnList = "last_activity_at")
})
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class Project {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    /**
     * Project name
     */
    @Column(name = "name", nullable = false, length = 255)
    private String name;

    /**
     * Project description
     */
    @Column(name = "description", length = 2048)
    private String description;

    /**
     * Keywords for automatic collection
     */
    @JdbcTypeCode(SqlTypes.JSON)
    @Column(name = "keywords", columnDefinition = "jsonb")
    private List<String> keywords;

    /**
     * Project category
     */
    @Enumerated(EnumType.STRING)
    @Column(name = "category", length = 32)
    @Builder.Default
    private ProjectCategory category = ProjectCategory.CUSTOM;

    /**
     * Project status
     */
    @Enumerated(EnumType.STRING)
    @Column(name = "status", length = 32)
    @Builder.Default
    private ProjectStatus status = ProjectStatus.ACTIVE;

    /**
     * Project visibility
     */
    @Enumerated(EnumType.STRING)
    @Column(name = "visibility", length = 32)
    @Builder.Default
    private ProjectVisibility visibility = ProjectVisibility.PRIVATE;

    /**
     * Project owner ID
     */
    @Column(name = "owner_id", nullable = false, length = 64)
    private String ownerId;

    /**
     * Project color for UI
     */
    @Column(name = "color", length = 16)
    private String color;

    /**
     * Project icon name
     */
    @Column(name = "icon", length = 32)
    private String icon;

    /**
     * Whether this is the default project for the user
     */
    @Column(name = "is_default")
    @Builder.Default
    private Boolean isDefault = false;

    /**
     * Project settings
     */
    @JdbcTypeCode(SqlTypes.JSON)
    @Column(name = "settings", columnDefinition = "jsonb")
    private ProjectSettings settings;

    /**
     * Project statistics (cached)
     */
    @JdbcTypeCode(SqlTypes.JSON)
    @Column(name = "stats", columnDefinition = "jsonb")
    private Map<String, Object> stats;

    /**
     * Tags for organization
     */
    @JdbcTypeCode(SqlTypes.JSON)
    @Column(name = "tags", columnDefinition = "jsonb")
    private List<String> tags;

    /**
     * Custom metadata
     */
    @JdbcTypeCode(SqlTypes.JSON)
    @Column(name = "metadata", columnDefinition = "jsonb")
    private Map<String, Object> metadata;

    @CreationTimestamp
    @Column(name = "created_at", updatable = false)
    private LocalDateTime createdAt;

    @UpdateTimestamp
    @Column(name = "updated_at")
    private LocalDateTime updatedAt;

    @Column(name = "last_activity_at")
    private LocalDateTime lastActivityAt;

    /**
     * When auto-collection last ran
     */
    @Column(name = "last_collected_at")
    private LocalDateTime lastCollectedAt;

    // ============ Enums ============

    public enum ProjectCategory {
        /** Research/Investigation project */
        RESEARCH,
        /** Continuous monitoring project */
        MONITORING,
        /** Fact-checking project */
        FACT_CHECK,
        /** Trend analysis project */
        TREND_ANALYSIS,
        /** Custom/other project */
        CUSTOM
    }

    public enum ProjectStatus {
        /** Active project */
        ACTIVE,
        /** Temporarily paused */
        PAUSED,
        /** Completed project */
        COMPLETED,
        /** Archived project */
        ARCHIVED
    }

    public enum ProjectVisibility {
        /** Only owner can see */
        PRIVATE,
        /** Team members can see */
        TEAM,
        /** Anyone with link can see */
        PUBLIC
    }

    // ============ Embedded classes ============

    /**
     * Project settings configuration
     */
    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class ProjectSettings {
        /** Enable automatic news collection */
        @Builder.Default
        private Boolean autoCollect = false;
        
        /** Collection interval */
        @Builder.Default
        private String collectInterval = "daily"; // hourly, daily, weekly
        
        /** News sources to collect from */
        private List<String> collectSources;
        
        /** Time window for collection */
        @Builder.Default
        private String timeWindow = "7d";
        
        /** Notification settings */
        private NotificationSettings notifications;
        
        /** AI analysis settings */
        private AiAnalysisSettings aiAnalysis;
    }

    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class NotificationSettings {
        @Builder.Default
        private Boolean newArticles = true;
        @Builder.Default
        private Boolean importantUpdates = true;
        @Builder.Default
        private Boolean weeklyDigest = false;
        @Builder.Default
        private Boolean emailEnabled = false;
        private String slackWebhook;
    }

    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class AiAnalysisSettings {
        @Builder.Default
        private Boolean enabled = true;
        @Builder.Default
        private Boolean autoSummarize = true;
        @Builder.Default
        private Boolean sentimentTracking = true;
        @Builder.Default
        private Boolean trendDetection = true;
        @Builder.Default
        private Boolean factCheck = false;
    }

    // ============ Helper methods ============

    /**
     * Update last activity timestamp
     */
    public void touchActivity() {
        this.lastActivityAt = LocalDateTime.now();
    }

    /**
     * Check if auto-collection is enabled
     */
    public boolean isAutoCollectEnabled() {
        return settings != null && Boolean.TRUE.equals(settings.getAutoCollect());
    }

    /**
     * Archive the project
     */
    public void archive() {
        this.status = ProjectStatus.ARCHIVED;
    }

    /**
     * Pause the project
     */
    public void pause() {
        this.status = ProjectStatus.PAUSED;
    }

    /**
     * Activate the project
     */
    public void activate() {
        this.status = ProjectStatus.ACTIVE;
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/project/ProjectActivityLog.java

```java
package com.newsinsight.collector.entity.project;

import jakarta.persistence.*;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.hibernate.annotations.CreationTimestamp;
import org.hibernate.annotations.JdbcTypeCode;
import org.hibernate.type.SqlTypes;

import java.time.LocalDateTime;
import java.util.Map;

/**
 * Entity for tracking project activity.
 * Provides audit trail and activity feed for collaborative projects.
 */
@Entity
@Table(name = "project_activity_log", indexes = {
        @Index(name = "idx_pal_project_id", columnList = "project_id"),
        @Index(name = "idx_pal_user_id", columnList = "user_id"),
        @Index(name = "idx_pal_type", columnList = "activity_type"),
        @Index(name = "idx_pal_created_at", columnList = "created_at")
})
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class ProjectActivityLog {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    /**
     * Project ID
     */
    @Column(name = "project_id", nullable = false)
    private Long projectId;

    /**
     * User who performed the action
     */
    @Column(name = "user_id", length = 64)
    private String userId;

    /**
     * Activity type
     */
    @Enumerated(EnumType.STRING)
    @Column(name = "activity_type", nullable = false, length = 64)
    private ActivityType activityType;

    /**
     * Human-readable description
     */
    @Column(name = "description", length = 1024)
    private String description;

    /**
     * Related entity type (e.g., "item", "member", "search")
     */
    @Column(name = "entity_type", length = 64)
    private String entityType;

    /**
     * Related entity ID
     */
    @Column(name = "entity_id", length = 255)
    private String entityId;

    /**
     * Additional metadata/context
     */
    @JdbcTypeCode(SqlTypes.JSON)
    @Column(name = "metadata", columnDefinition = "jsonb")
    private Map<String, Object> metadata;

    /**
     * Changes made (for updates)
     */
    @JdbcTypeCode(SqlTypes.JSON)
    @Column(name = "changes", columnDefinition = "jsonb")
    private Map<String, Object> changes;

    /**
     * IP address for audit
     */
    @Column(name = "ip_address", length = 64)
    private String ipAddress;

    /**
     * User agent for audit
     */
    @Column(name = "user_agent", length = 512)
    private String userAgent;

    @CreationTimestamp
    @Column(name = "created_at", updatable = false)
    private LocalDateTime createdAt;

    // ============ Enums ============

    public enum ActivityType {
        // Project lifecycle
        PROJECT_CREATED,
        PROJECT_UPDATED,
        PROJECT_ARCHIVED,
        PROJECT_DELETED,
        PROJECT_RESTORED,
        PROJECT_STATUS_CHANGED,
        
        // Member management
        MEMBER_ADDED,
        MEMBER_INVITED,
        MEMBER_JOINED,
        MEMBER_ROLE_CHANGED,
        MEMBER_REMOVED,
        MEMBER_LEFT,
        
        // Item management
        ITEM_ADDED,
        ITEM_UPDATED,
        ITEM_DELETED,
        ITEM_BOOKMARKED,
        ITEM_TAGGED,
        
        // Search activities
        SEARCH_EXECUTED,
        SEARCH_SAVED,
        SEARCH_SHARED,
        
        // Report activities
        REPORT_GENERATED,
        REPORT_DOWNLOADED,
        REPORT_SHARED,
        
        // Collection activities
        AUTO_COLLECT_RAN,
        AUTO_COLLECTION,
        MANUAL_COLLECTION,
        ITEMS_COLLECTED,
        COLLECTION_FAILED,
        
        // Settings
        SETTINGS_CHANGED,
        KEYWORDS_UPDATED,
        NOTIFICATIONS_CHANGED,
        
        // Comments
        COMMENT_ADDED,
        COMMENT_EDITED,
        COMMENT_DELETED
    }

    // ============ Static factory methods ============

    public static ProjectActivityLog projectCreated(Long projectId, String userId, String projectName) {
        return ProjectActivityLog.builder()
                .projectId(projectId)
                .userId(userId)
                .activityType(ActivityType.PROJECT_CREATED)
                .description("프로젝트 '" + projectName + "'이(가) 생성되었습니다")
                .build();
    }

    public static ProjectActivityLog memberInvited(Long projectId, String userId, String invitedUserId, String role) {
        return ProjectActivityLog.builder()
                .projectId(projectId)
                .userId(userId)
                .activityType(ActivityType.MEMBER_INVITED)
                .description("새 멤버가 " + role + " 역할로 초대되었습니다")
                .entityType("member")
                .entityId(invitedUserId)
                .metadata(Map.of("invitedUserId", invitedUserId, "role", role))
                .build();
    }

    public static ProjectActivityLog itemAdded(Long projectId, String userId, Long itemId, String itemTitle) {
        return ProjectActivityLog.builder()
                .projectId(projectId)
                .userId(userId)
                .activityType(ActivityType.ITEM_ADDED)
                .description("새 항목이 추가되었습니다: " + itemTitle)
                .entityType("item")
                .entityId(String.valueOf(itemId))
                .build();
    }

    public static ProjectActivityLog searchExecuted(Long projectId, String userId, String query, int resultCount) {
        return ProjectActivityLog.builder()
                .projectId(projectId)
                .userId(userId)
                .activityType(ActivityType.SEARCH_EXECUTED)
                .description("검색 실행: '" + query + "' (" + resultCount + "개 결과)")
                .metadata(Map.of("query", query, "resultCount", resultCount))
                .build();
    }

    public static ProjectActivityLog autoCollectRan(Long projectId, int itemsCollected) {
        return ProjectActivityLog.builder()
                .projectId(projectId)
                .activityType(ActivityType.AUTO_COLLECT_RAN)
                .description("자동 수집 완료: " + itemsCollected + "개 항목 수집")
                .metadata(Map.of("itemsCollected", itemsCollected))
                .build();
    }

    public static ProjectActivityLog reportGenerated(Long projectId, String userId, Long reportId, String reportTitle) {
        return ProjectActivityLog.builder()
                .projectId(projectId)
                .userId(userId)
                .activityType(ActivityType.REPORT_GENERATED)
                .description("보고서 생성: " + reportTitle)
                .entityType("report")
                .entityId(String.valueOf(reportId))
                .build();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/project/ProjectItem.java

```java
package com.newsinsight.collector.entity.project;

import jakarta.persistence.*;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.hibernate.annotations.CreationTimestamp;
import org.hibernate.annotations.JdbcTypeCode;
import org.hibernate.type.SqlTypes;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Map;

/**
 * Entity representing an item within a project.
 * Can be a collected article, search result, report, or note.
 */
@Entity
@Table(name = "project_items", indexes = {
        @Index(name = "idx_pi_project_id", columnList = "project_id"),
        @Index(name = "idx_pi_type", columnList = "item_type"),
        @Index(name = "idx_pi_source_id", columnList = "source_id"),
        @Index(name = "idx_pi_added_at", columnList = "added_at"),
        @Index(name = "idx_pi_published_at", columnList = "published_at"),
        @Index(name = "idx_pi_bookmarked", columnList = "bookmarked")
})
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class ProjectItem {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    /**
     * Project ID
     */
    @Column(name = "project_id", nullable = false)
    private Long projectId;

    /**
     * Item type
     */
    @Enumerated(EnumType.STRING)
    @Column(name = "item_type", nullable = false, length = 32)
    private ItemType itemType;

    /**
     * Source reference ID (SearchHistory ID, Article ID, etc.)
     */
    @Column(name = "source_id", length = 255)
    private String sourceId;

    /**
     * Source type identifier
     */
    @Column(name = "source_type", length = 64)
    private String sourceType;

    /**
     * Item title
     */
    @Column(name = "title", length = 512)
    private String title;

    /**
     * Item summary/excerpt
     */
    @Column(name = "summary", length = 4096)
    private String summary;

    /**
     * Full content (for notes, etc.)
     */
    @Column(name = "content", columnDefinition = "text")
    private String content;

    /**
     * Original URL
     */
    @Column(name = "url", length = 2048)
    private String url;

    /**
     * Thumbnail/image URL
     */
    @Column(name = "thumbnail_url", length = 1024)
    private String thumbnailUrl;

    /**
     * Original publish date
     */
    @Column(name = "published_at")
    private LocalDateTime publishedAt;

    /**
     * Source name (news outlet, etc.)
     */
    @Column(name = "source_name", length = 255)
    private String sourceName;

    /**
     * Author name
     */
    @Column(name = "author", length = 255)
    private String author;

    /**
     * User-defined tags
     */
    @JdbcTypeCode(SqlTypes.JSON)
    @Column(name = "tags", columnDefinition = "jsonb")
    private List<String> tags;

    /**
     * Category within project
     */
    @Column(name = "category", length = 128)
    private String category;

    /**
     * Whether bookmarked/starred
     */
    @Column(name = "bookmarked")
    @Builder.Default
    private Boolean bookmarked = false;

    /**
     * Importance level (1-5)
     */
    @Column(name = "importance")
    private Integer importance;

    /**
     * User notes about this item
     */
    @Column(name = "notes", columnDefinition = "text")
    private String notes;

    /**
     * Read status
     */
    @Column(name = "is_read")
    @Builder.Default
    private Boolean isRead = false;

    /**
     * Sentiment score (-1 to 1)
     */
    @Column(name = "sentiment_score")
    private Double sentimentScore;

    /**
     * Sentiment label
     */
    @Column(name = "sentiment_label", length = 32)
    private String sentimentLabel;

    /**
     * Relevance score (0-100)
     */
    @Column(name = "relevance_score")
    private Double relevanceScore;

    /**
     * AI-generated analysis
     */
    @JdbcTypeCode(SqlTypes.JSON)
    @Column(name = "ai_analysis", columnDefinition = "jsonb")
    private Map<String, Object> aiAnalysis;

    /**
     * Additional metadata
     */
    @JdbcTypeCode(SqlTypes.JSON)
    @Column(name = "metadata", columnDefinition = "jsonb")
    private Map<String, Object> metadata;

    /**
     * User who added this item
     */
    @Column(name = "added_by", length = 64)
    private String addedBy;

    @CreationTimestamp
    @Column(name = "added_at", updatable = false)
    private LocalDateTime addedAt;

    @Column(name = "updated_at")
    private LocalDateTime updatedAt;

    // ============ Enums ============

    public enum ItemType {
        /** News article */
        ARTICLE,
        /** Search result reference */
        SEARCH_RESULT,
        /** Generated report */
        REPORT,
        /** User note */
        NOTE,
        /** External URL/link */
        LINK,
        /** File attachment */
        FILE,
        /** Social media post */
        SOCIAL_POST
    }

    // ============ Helper methods ============

    /**
     * Mark as read
     */
    public void markRead() {
        this.isRead = true;
    }

    /**
     * Toggle bookmark
     */
    public void toggleBookmark() {
        this.bookmarked = !Boolean.TRUE.equals(this.bookmarked);
    }

    /**
     * Update importance
     */
    public void setImportanceLevel(int level) {
        this.importance = Math.max(1, Math.min(5, level));
    }

    @PreUpdate
    protected void onUpdate() {
        this.updatedAt = LocalDateTime.now();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/project/ProjectMember.java

```java
package com.newsinsight.collector.entity.project;

import jakarta.persistence.*;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.hibernate.annotations.CreationTimestamp;
import org.hibernate.annotations.JdbcTypeCode;
import org.hibernate.type.SqlTypes;

import java.time.LocalDateTime;
import java.util.List;

/**
 * Entity representing a project member.
 * Manages team access and permissions for collaborative projects.
 */
@Entity
@Table(name = "project_members", indexes = {
        @Index(name = "idx_pm_project_id", columnList = "project_id"),
        @Index(name = "idx_pm_user_id", columnList = "user_id"),
        @Index(name = "idx_pm_role", columnList = "role")
}, uniqueConstraints = {
        @UniqueConstraint(name = "uk_project_member", columnNames = {"project_id", "user_id"})
})
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class ProjectMember {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    /**
     * Project ID
     */
    @Column(name = "project_id", nullable = false)
    private Long projectId;

    /**
     * User ID
     */
    @Column(name = "user_id", nullable = false, length = 64)
    private String userId;

    /**
     * Member role
     */
    @Enumerated(EnumType.STRING)
    @Column(name = "role", length = 32)
    @Builder.Default
    private MemberRole role = MemberRole.VIEWER;

    /**
     * Specific permissions (optional, overrides role defaults)
     */
    @JdbcTypeCode(SqlTypes.JSON)
    @Column(name = "permissions", columnDefinition = "jsonb")
    private List<String> permissions;

    /**
     * User who invited this member
     */
    @Column(name = "invited_by", length = 64)
    private String invitedBy;

    /**
     * Invitation status
     */
    @Enumerated(EnumType.STRING)
    @Column(name = "status", length = 32)
    @Builder.Default
    private MemberStatus status = MemberStatus.PENDING;

    /**
     * Invitation token (for email invites)
     */
    @Column(name = "invite_token", length = 128)
    private String inviteToken;

    /**
     * When the invitation expires
     */
    @Column(name = "invite_expires_at")
    private LocalDateTime inviteExpiresAt;

    @CreationTimestamp
    @Column(name = "joined_at", updatable = false)
    private LocalDateTime joinedAt;

    @Column(name = "last_active_at")
    private LocalDateTime lastActiveAt;

    // ============ Enums ============

    public enum MemberRole {
        /** Full control including delete */
        OWNER,
        /** Can manage members and settings */
        ADMIN,
        /** Can add/edit items */
        EDITOR,
        /** Read-only access */
        VIEWER
    }

    public enum MemberStatus {
        /** Invitation pending acceptance */
        PENDING,
        /** Active member */
        ACTIVE,
        /** Membership revoked */
        REVOKED,
        /** User left the project */
        LEFT
    }

    // ============ Permission constants ============

    public static class Permission {
        public static final String MANAGE_PROJECT = "manage_project";
        public static final String DELETE_PROJECT = "delete_project";
        public static final String INVITE_MEMBERS = "invite_members";
        public static final String REMOVE_MEMBERS = "remove_members";
        public static final String CHANGE_ROLES = "change_roles";
        public static final String ADD_ITEMS = "add_items";
        public static final String EDIT_ITEMS = "edit_items";
        public static final String DELETE_ITEMS = "delete_items";
        public static final String RUN_SEARCH = "run_search";
        public static final String GENERATE_REPORT = "generate_report";
        public static final String CHANGE_SETTINGS = "change_settings";
        public static final String VIEW_ANALYTICS = "view_analytics";
    }

    // ============ Helper methods ============

    /**
     * Check if member has a specific permission
     */
    public boolean hasPermission(String permission) {
        // Owner has all permissions
        if (role == MemberRole.OWNER) return true;
        
        // Check explicit permissions first
        if (permissions != null && permissions.contains(permission)) {
            return true;
        }
        
        // Check role-based permissions
        return switch (role) {
            case ADMIN -> !permission.equals(Permission.DELETE_PROJECT);
            case EDITOR -> permission.equals(Permission.ADD_ITEMS) 
                    || permission.equals(Permission.EDIT_ITEMS)
                    || permission.equals(Permission.RUN_SEARCH)
                    || permission.equals(Permission.GENERATE_REPORT)
                    || permission.equals(Permission.VIEW_ANALYTICS);
            case VIEWER -> permission.equals(Permission.VIEW_ANALYTICS);
            default -> false;
        };
    }

    /**
     * Accept invitation
     */
    public void accept() {
        this.status = MemberStatus.ACTIVE;
        this.inviteToken = null;
        this.inviteExpiresAt = null;
    }

    /**
     * Touch last active timestamp
     */
    public void touchActive() {
        this.lastActiveAt = LocalDateTime.now();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/project/ProjectNotification.java

```java
package com.newsinsight.collector.entity.project;

import jakarta.persistence.*;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.hibernate.annotations.CreationTimestamp;
import org.hibernate.annotations.JdbcTypeCode;
import org.hibernate.type.SqlTypes;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Map;

/**
 * Entity for project notifications.
 * Manages alerts for project events like new articles, trend spikes, etc.
 */
@Entity
@Table(name = "project_notifications", indexes = {
        @Index(name = "idx_pn_project_id", columnList = "project_id"),
        @Index(name = "idx_pn_type", columnList = "notification_type"),
        @Index(name = "idx_pn_priority", columnList = "priority"),
        @Index(name = "idx_pn_created_at", columnList = "created_at")
})
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class ProjectNotification {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    /**
     * Project ID
     */
    @Column(name = "project_id", nullable = false)
    private Long projectId;

    /**
     * Target user ID (single recipient for simple notifications)
     */
    @Column(name = "user_id", length = 64)
    private String userId;

    /**
     * Whether this notification has been read
     */
    @Column(name = "is_read")
    @Builder.Default
    private Boolean isRead = false;

    /**
     * Notification type
     */
    @Enumerated(EnumType.STRING)
    @Column(name = "notification_type", nullable = false, length = 64)
    private NotificationType notificationType;

    /**
     * Priority level
     */
    @Enumerated(EnumType.STRING)
    @Column(name = "priority", length = 32)
    @Builder.Default
    private NotificationPriority priority = NotificationPriority.MEDIUM;

    /**
     * Notification title
     */
    @Column(name = "title", nullable = false, length = 255)
    private String title;

    /**
     * Notification message
     */
    @Column(name = "message", length = 2048)
    private String message;

    /**
     * Action URL (click to navigate)
     */
    @Column(name = "action_url", length = 1024)
    private String actionUrl;

    /**
     * Action button label
     */
    @Column(name = "action_label", length = 64)
    private String actionLabel;

    /**
     * Recipients (user IDs)
     */
    @JdbcTypeCode(SqlTypes.JSON)
    @Column(name = "recipients", columnDefinition = "jsonb")
    private List<String> recipients;

    /**
     * Delivery channels
     */
    @JdbcTypeCode(SqlTypes.JSON)
    @Column(name = "channels", columnDefinition = "jsonb")
    private List<String> channels;

    /**
     * Users who have read this notification
     */
    @JdbcTypeCode(SqlTypes.JSON)
    @Column(name = "read_by", columnDefinition = "jsonb")
    private List<String> readBy;

    /**
     * Delivery status per channel
     */
    @JdbcTypeCode(SqlTypes.JSON)
    @Column(name = "delivery_status", columnDefinition = "jsonb")
    private Map<String, Object> deliveryStatus;

    /**
     * Additional data
     */
    @JdbcTypeCode(SqlTypes.JSON)
    @Column(name = "metadata", columnDefinition = "jsonb")
    private Map<String, Object> metadata;

    /**
     * Whether notification has been dismissed by all
     */
    @Column(name = "dismissed")
    @Builder.Default
    private Boolean dismissed = false;

    @CreationTimestamp
    @Column(name = "created_at", updatable = false)
    private LocalDateTime createdAt;

    @Column(name = "sent_at")
    private LocalDateTime sentAt;

    @Column(name = "expires_at")
    private LocalDateTime expiresAt;

    // ============ Enums ============

    public enum NotificationType {
        /** New articles collected */
        NEW_ARTICLES,
        /** Significant trend change */
        TREND_SPIKE,
        /** Important news alert */
        IMPORTANT_UPDATE,
        /** Team member activity */
        MEMBER_ACTIVITY,
        /** Member invited to project */
        MEMBER_INVITED,
        /** Report ready for download */
        REPORT_READY,
        /** Collection completed */
        COLLECTION_COMPLETE,
        /** Collection failed */
        COLLECTION_FAILED,
        /** System notification */
        SYSTEM_ALERT,
        /** Weekly/monthly digest */
        DIGEST,
        /** Keyword match alert */
        KEYWORD_MATCH
    }

    public enum NotificationPriority {
        LOW,
        MEDIUM,
        HIGH,
        URGENT
    }

    public static class Channel {
        public static final String IN_APP = "in_app";
        public static final String EMAIL = "email";
        public static final String SLACK = "slack";
        public static final String WEBHOOK = "webhook";
        public static final String PUSH = "push";
    }

    // ============ Helper methods ============

    /**
     * Mark as read by user
     */
    public void markReadBy(String userId) {
        if (readBy == null) {
            readBy = new java.util.ArrayList<>();
        }
        if (!readBy.contains(userId)) {
            readBy.add(userId);
        }
    }

    /**
     * Check if read by user
     */
    public boolean isReadBy(String userId) {
        return readBy != null && readBy.contains(userId);
    }

    /**
     * Check if expired
     */
    public boolean isExpired() {
        return expiresAt != null && LocalDateTime.now().isAfter(expiresAt);
    }

    /**
     * Mark as sent
     */
    public void markSent() {
        this.sentAt = LocalDateTime.now();
    }

    // ============ Static factory methods ============

    public static ProjectNotification newArticles(Long projectId, int count, List<String> recipients) {
        return ProjectNotification.builder()
                .projectId(projectId)
                .notificationType(NotificationType.NEW_ARTICLES)
                .priority(NotificationPriority.MEDIUM)
                .title("새로운 기사 수집")
                .message(count + "개의 새로운 기사가 수집되었습니다")
                .recipients(recipients)
                .channels(List.of(Channel.IN_APP))
                .actionLabel("보기")
                .metadata(Map.of("articleCount", count))
                .build();
    }

    public static ProjectNotification trendSpike(Long projectId, String keyword, double changePercent, List<String> recipients) {
        return ProjectNotification.builder()
                .projectId(projectId)
                .notificationType(NotificationType.TREND_SPIKE)
                .priority(NotificationPriority.HIGH)
                .title("트렌드 급등 감지")
                .message("'" + keyword + "' 키워드가 " + String.format("%.1f", changePercent) + "% 증가했습니다")
                .recipients(recipients)
                .channels(List.of(Channel.IN_APP, Channel.EMAIL))
                .actionLabel("분석 보기")
                .metadata(Map.of("keyword", keyword, "changePercent", changePercent))
                .build();
    }

    public static ProjectNotification reportReady(Long projectId, Long reportId, String reportTitle, List<String> recipients) {
        return ProjectNotification.builder()
                .projectId(projectId)
                .notificationType(NotificationType.REPORT_READY)
                .priority(NotificationPriority.MEDIUM)
                .title("보고서 생성 완료")
                .message("'" + reportTitle + "' 보고서가 준비되었습니다")
                .recipients(recipients)
                .channels(List.of(Channel.IN_APP))
                .actionLabel("다운로드")
                .actionUrl("/reports/" + reportId)
                .metadata(Map.of("reportId", reportId, "reportTitle", reportTitle))
                .build();
    }
}

```

---

## backend/data-collection-service/src/main/java/com/newsinsight/collector/entity/report/GeneratedReport.java

```java
package com.newsinsight.collector.entity.report;

import jakarta.persistence.*;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.hibernate.annotations.CreationTimestamp;
import org.hibernate.annotations.JdbcTypeCode;
import org.hibernate.type.SqlTypes;

import java.time.LocalDateTime;
import java.util.Map;

/**
 * Entity for storing generated reports.
 * Tracks PDF/document generation from search results
 * enabling re-download and sharing features.
 */
@Entity
@Table(name = "generated_reports", indexes = {
        @Index(name = "idx_report_search_history_id", columnList = "search_history_id"),
        @Index(name = "idx_report_user_id", columnList = "user_id"),
        @Index(name = "idx_report_project_id", columnList = "project_id"),
        @Index(name = "idx_report_created_at", columnList = "created_at")
})
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class GeneratedReport {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    /**
     * Related search history ID
     */
    @Column(name = "search_history_id")
    private Long searchHistoryId;

    /**
     * Project ID if part of a project
     */
    @Column(name = "project_id")
    private Long projectId;

    /**
     * User who generated the report
     */
    @Column(name = "user_id", length = 64)
    private String userId;

    /**
     * Report title
     */
    @Column(name = "title", length = 512)
    private String title;

    /**
     * Report type (PDF, MARKDOWN, HTML, JSON)
     */
    @Enumerated(EnumType.STRING)
    @Column(name = "report_type", length = 32)
    @Builder.Default
    private ReportType reportType = ReportType.PDF;

    /**
     * Report format/template used
     */
    @Column(name = "template_name", length = 64)
    private String templateName;

    /**
     * File storage path or URL
     */
    @Column(name = "file_path", length = 1024)
    private String filePath;

    /**
     * Public URL for sharing (if enabled)
     */
    @Column(name = "public_url", length = 1024)
    private String publicUrl;

    /**
     * File size in bytes
     */
    @Column(name = "file_size")
    private Long fileSize;

    /**
     * MIME type
     */
    @Column(name = "mime_type", length = 64)
    private String mimeType;

    /**
     * Generation status
     */
    @Enumerated(EnumType.STRING)
    @Column(name = "status", length = 32)
    @Builder.Default
    private ReportStatus status = ReportStatus.PENDING;

    /**
     * Error message if generation failed
     */
    @Column(name = "error_message", length = 1024)
    private String errorMessage;

    /**
     * Report metadata (sections, charts included, etc.)
     */
    @JdbcTypeCode(SqlTypes.JSON)
    @Column(name = "metadata", columnDefinition = "jsonb")
    private Map<String, Object> metadata;

    /**
     * Report configuration/options used
     */
    @JdbcTypeCode(SqlTypes.JSON)
    @Column(name = "config", columnDefinition = "jsonb")
    private Map<String, Object> config;

    /**
     * Number of times downloaded
     */
    @Column(name = "download_count")
    @Builder.Default
    private Integer downloadCount = 0;

    /**
     * Last download time
     */
    @Column(name = "last_downloaded_at")
    private LocalDateTime lastDownloadedAt;

    /**
     * Whether report is shared publicly
     */
    @Column(name = "is_public")
    @Builder.Default
    private Boolean isPublic = false;

    /**
     * Share link expiry time
     */
    @Column(name = "share_expires_at")
    private LocalDateTime shareExpiresAt;

    @CreationTimestamp
    @Column(name = "created_at", updatable = false)
    private LocalDateTime createdAt;

    @Column(name = "generated_at")
    private LocalDateTime generatedAt;

    /**
     * Report type enum
     */
    public enum ReportType {
        PDF,
        MARKDOWN,
        HTML,
        JSON,
        DOCX,
        XLSX
    }

    /**
     * Report status enum
     */
    public enum ReportStatus {
        PENDING,
        GENERATING,
        COMPLETED,
        FAILED,
        EXPIRED
    }

    /**
     * Mark report as generated
     */
    public void markGenerated(String filePath, Long fileSize) {
        this.status = ReportStatus.COMPLETED;
        this.filePath = filePath;
        this.fileSize = fileSize;
        this.generatedAt = LocalDateTime.now();
    }

    /**
     * Mark report as failed
     */
    public void markFailed(String errorMessage) {
        this.status = ReportStatus.FAILED;
        this.errorMessage = errorMessage;
    }

    /**
     * Increment download count
     */
    public void incrementDownload() {
        this.downloadCount++;
        this.lastDownloadedAt = LocalDateTime.now();
    }
}

```
