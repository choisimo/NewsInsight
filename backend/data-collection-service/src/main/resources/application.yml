spring:
  application:
    name: collector-service
  config:
    # Consul config import (optional - Consul 없어도 시작 가능)
    import: ${SPRING_CONFIG_IMPORT:optional:consul:}
  kafka:
    bootstrap-servers: ${KAFKA_BOOTSTRAP_SERVERS:localhost:9092}
    # Producer reliability settings
    producer:
      acks: ${KAFKA_PRODUCER_ACKS:all}
      retries: ${KAFKA_PRODUCER_RETRIES:3}
      retry-backoff-ms: ${KAFKA_PRODUCER_RETRY_BACKOFF_MS:1000}
      # delivery.timeout.ms must be >= linger.ms + request.timeout.ms
      # linger.ms=5 (hardcoded in KafkaConfig), request.timeout.ms=30000
      # So delivery-timeout-ms must be >= 30005
      delivery-timeout-ms: ${KAFKA_PRODUCER_DELIVERY_TIMEOUT_MS:120000}
      enable-idempotence: ${KAFKA_PRODUCER_ENABLE_IDEMPOTENCE:true}
      properties:
        # Fast-fail settings to avoid long blocking on Kafka unavailability
        max.block.ms: ${KAFKA_PRODUCER_MAX_BLOCK_MS:5000}
        request.timeout.ms: ${KAFKA_PRODUCER_REQUEST_TIMEOUT_MS:30000}
        metadata.max.age.ms: ${KAFKA_PRODUCER_METADATA_MAX_AGE_MS:60000}
    # Consumer reliability settings
    consumer:
      max-retry-attempts: ${KAFKA_CONSUMER_MAX_RETRY_ATTEMPTS:3}
      retry-backoff-ms: ${KAFKA_CONSUMER_RETRY_BACKOFF_MS:1000}
      retry-max-backoff-ms: ${KAFKA_CONSUMER_RETRY_MAX_BACKOFF_MS:30000}
      concurrency: ${KAFKA_CONSUMER_CONCURRENCY:1}
  data:
    mongodb:
      uri: ${MONGODB_URI:mongodb://localhost:27017/newsinsight}
    redis:
      host: ${REDIS_HOST:localhost}
      port: ${REDIS_PORT:6379}
      password: ${REDIS_PASSWORD:}
      timeout: 3000ms
      lettuce:
        pool:
          max-active: 8
          max-idle: 8
          min-idle: 0
          max-wait: -1ms
  
  datasource:
    url: jdbc:postgresql://${DB_HOST:localhost}:${DB_PORT:5432}/${DB_NAME:newsinsight}
    username: ${DB_USER:postgres}
    password: ${DB_PASSWORD:postgres}
    driver-class-name: org.postgresql.Driver
    hikari:
      maximum-pool-size: 10
      minimum-idle: 5
      connection-timeout: 30000
      idle-timeout: 600000
      max-lifetime: 1800000
  
  jpa:
    hibernate:
      ddl-auto: update
    show-sql: ${JPA_SHOW_SQL:false}
    properties:
      hibernate:
        dialect: org.hibernate.dialect.PostgreSQLDialect
        format_sql: true
        jdbc:
          batch_size: 20
        order_inserts: true
        order_updates: true
    open-in-view: false
  
  cloud:
    consul:
      enabled: ${CONSUL_ENABLED:true}
      host: ${CONSUL_HOST:localhost}
      port: ${CONSUL_PORT:8500}
      discovery:
        enabled: ${CONSUL_DISCOVERY_ENABLED:true}
        instance-id: ${spring.application.name}:${random.value}
        service-name: ${spring.application.name}
        prefer-ip-address: false
        hostname: ${CONSUL_DISCOVERY_HOSTNAME:collector-service}
        health-check-path: /actuator/health
        health-check-interval: 10s
        # Consul 연결 실패해도 시작 가능
        fail-fast: false
      config:
        enabled: ${CONSUL_CONFIG_ENABLED:true}
        prefix: config
        default-context: ${spring.application.name}
        profile-separator: '::'
        format: PROPERTIES
        # Consul config 없어도 시작 가능
        fail-fast: false
  
  task:
    execution:
      pool:
        core-size: 5
        max-size: 20
        queue-capacity: 100
      thread-name-prefix: async-task-

  flyway:
    enabled: ${SPRING_FLYWAY_ENABLED:false}

server:
  port: ${SERVER_PORT:8081}
  servlet:
    context-path: /
  compression:
    enabled: true
  error:
    include-message: always
    include-binding-errors: always

management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics,prometheus
  endpoint:
    health:
      show-details: always
  health:
    consul:
      enabled: ${CONSUL_ENABLED:true}
  # Kafka metrics via Micrometer
  metrics:
    tags:
      application: ${spring.application.name}
    distribution:
      percentiles-histogram:
        kafka: true
      slo:
        kafka.producer.request.latency: 10ms,50ms,100ms,500ms
        kafka.consumer.fetch.latency: 10ms,50ms,100ms,500ms

logging:
  level:
    root: INFO
    com.newsinsight: ${LOG_LEVEL:DEBUG}
    org.springframework.web: INFO
    org.hibernate.SQL: ${JPA_SHOW_SQL:false}
    org.hibernate.type.descriptor.sql.BasicBinder: ${JPA_SHOW_SQL:false}
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss} - %msg%n"

# Application specific configuration
collector:
  scheduling:
    enabled: ${SCHEDULING_ENABLED:true}
    # Cron expression: run every hour
    cron: ${COLLECTION_CRON:0 0 * * * ?}
    # Skip scheduled collection if there are already running jobs
    skip-if-running: ${SCHEDULING_SKIP_IF_RUNNING:true}
  
  http:
    user-agent: ${HTTP_USER_AGENT:NewsInsight-Collector/1.0}
    timeout:
      connect: ${HTTP_CONNECT_TIMEOUT:10000}
      read: ${HTTP_READ_TIMEOUT:30000}
    max-redirects: 5
  
  rss:
    enabled: true
    fetch-timeout: 30000
  
  web-scraper:
    enabled: true
    max-content-length: 1048576  # 1MB
    respect-robots-txt: true
  
  collection:
    max-concurrent: ${MAX_CONCURRENT_COLLECTIONS:5}
    retry:
      max-attempts: 3
      backoff-delay: 1000
    cleanup:
      old-jobs-days: 30
  
  quality-assurance:
    min-content-length: ${QA_MIN_CONTENT_LENGTH:50}
    enable-network-checks: ${QA_ENABLE_NETWORK_CHECKS:false}
    domain-whitelist: ${QA_DOMAIN_WHITELIST:example.com,news.example.com}
    expected-keywords: ${QA_EXPECTED_KEYWORDS:news,article,report}

  ai:
    topic:
      request: ${COLLECTOR_AI_REQUEST_TOPIC:newsinsight.ai.requests}
      response: ${COLLECTOR_AI_RESPONSE_TOPIC:newsinsight.ai.responses}
    default-provider-id: ${COLLECTOR_AI_PROVIDER_ID:openai}
    default-model-id: ${COLLECTOR_AI_MODEL_ID:gpt-4.1}
    
    # AI Orchestration Configuration (Multi-provider Job Management)
    orchestration:
      # Kafka topic for AI task requests
      topic: ${COLLECTOR_AI_ORCHESTRATION_TOPIC:ai.tasks.requests}
      # Base URL for callback (this service's public URL)
      callback-base-url: ${COLLECTOR_AI_ORCHESTRATION_CALLBACK_BASE_URL:${collector.deep-search.callback-base-url}}
      # Token for validating callbacks from internal workers
      callback-token: ${COLLECTOR_AI_ORCHESTRATION_CALLBACK_TOKEN:${collector.deep-search.callback-token}}
      # Minutes before marking a job as timed out
      timeout-minutes: ${COLLECTOR_AI_ORCHESTRATION_TIMEOUT_MINUTES:30}
      # Interval for checking timed out jobs (milliseconds)
      timeout-check-interval: ${COLLECTOR_AI_ORCHESTRATION_TIMEOUT_CHECK_INTERVAL:300000}
      # Days to keep completed jobs before cleanup
      cleanup-days: ${COLLECTOR_AI_ORCHESTRATION_CLEANUP_DAYS:7}
      # Cron expression for cleanup job (3 AM daily)
      cleanup-cron: ${COLLECTOR_AI_ORCHESTRATION_CLEANUP_CRON:0 0 3 * * ?}

  crawl:
    topic:
      command: ${COLLECTOR_CRAWL_COMMAND_TOPIC:newsinsight.crawl.commands}
      result: ${COLLECTOR_CRAWL_RESULT_TOPIC:newsinsight.crawl.results}
      browser-task: ${COLLECTOR_CRAWL_BROWSER_TASK_TOPIC:newsinsight.crawl.browser.tasks}

  crawler:
    enabled: ${COLLECTOR_SERVICE_CRAWLER_ENABLED:true}
    base-url: ${COLLECTOR_SERVICE_CRAWLER_BASE_URL:http://web-crawler:11235}

  # Browser Agent Configuration (autonomous-crawler-service)
  browser-agent:
    enabled: ${COLLECTOR_BROWSER_AGENT_ENABLED:true}
    # Callback URL for session completion
    callback-base-url: ${COLLECTOR_CALLBACK_BASE_URL:${COLLECTOR_BROWSER_AGENT_CALLBACK_BASE_URL:http://collector-service:8081}}
    callback-token: ${COLLECTOR_BROWSER_AGENT_CALLBACK_TOKEN:}

  # Deep AI Search Configuration
  # DeepSearch runs entirely on internal services (IntegratedCrawlerService + AIDove).
  # Legacy n8n webhook integration has been removed.
  deep-search:
    # Base URL for internal callbacks
    callback-base-url: ${COLLECTOR_CALLBACK_BASE_URL:${DEEP_SEARCH_CALLBACK_BASE_URL:http://collector-service:8081}}
    # Token for validating internal callbacks
    callback-token: ${DEEP_SEARCH_CALLBACK_TOKEN:}
    # Minutes before marking a job as timed out
    timeout-minutes: ${DEEP_SEARCH_TIMEOUT_MINUTES:30}
    # Interval for checking timed out jobs (milliseconds)
    timeout-check-interval: ${DEEP_SEARCH_TIMEOUT_CHECK_INTERVAL:300000}
    # Days to keep completed jobs before cleanup
    cleanup-days: ${DEEP_SEARCH_CLEANUP_DAYS:7}
    # Cron expression for cleanup job (3 AM daily)
    cleanup-cron: ${DEEP_SEARCH_CLEANUP_CRON:0 0 3 * * ?}

  # Integrated Crawler Configuration (multi-strategy crawling)
  integrated-crawler:
    # Maximum crawl depth (how many links deep to follow)
    max-depth: ${COLLECTOR_INTEGRATED_CRAWLER_MAX_DEPTH:3}
    # Maximum pages to crawl per job
    max-pages: ${COLLECTOR_INTEGRATED_CRAWLER_MAX_PAGES:50}
    # Timeout per page crawl in seconds
    timeout-seconds: ${COLLECTOR_INTEGRATED_CRAWLER_TIMEOUT_SECONDS:30}
    # Number of concurrent crawls
    concurrent-crawls: ${COLLECTOR_INTEGRATED_CRAWLER_CONCURRENT:10}

  # Browser-Use API Configuration (for JS-rendered content)
  browser-use:
    base-url: ${COLLECTOR_BROWSER_USE_BASE_URL:${BROWSER_USE_API_URL:http://browser-use-api:8500}}

  # AI Dove Configuration (for evidence analysis)
  ai-dove:
    enabled: ${COLLECTOR_AI_DOVE_ENABLED:${AI_DOVE_ENABLED:true}}
    base-url: ${COLLECTOR_AI_DOVE_BASE_URL:${AI_DOVE_BASE_URL:https://workflow.nodove.com/webhook/aidove}}
    # Timeout for AI Dove API calls in seconds (evidence analysis can take time)
    timeout-seconds: ${COLLECTOR_AI_DOVE_TIMEOUT_SECONDS:${AI_DOVE_TIMEOUT_SECONDS:180}}

  # Fact Check Sources Configuration
  fact-check:
    # Timeout for all fact-check API calls
    timeout-seconds: ${COLLECTOR_FACT_CHECK_TIMEOUT_SECONDS:15}
    
    # CrossRef - Academic paper search (free, no API key required)
    crossref:
      enabled: ${COLLECTOR_CROSSREF_ENABLED:true}
      # Email for polite pool (faster rate limits)
      mailto: ${COLLECTOR_CROSSREF_MAILTO:newsinsight@example.com}
    
    # OpenAlex - Open academic database (free, no API key required)
    openalex:
      enabled: ${COLLECTOR_OPENALEX_ENABLED:true}
      # Email for polite pool
      mailto: ${COLLECTOR_OPENALEX_MAILTO:newsinsight@example.com}
    
    # Google Fact Check Tools API (requires API key)
    # Get API key: https://developers.google.com/fact-check/tools/api
    google:
      enabled: ${COLLECTOR_GOOGLE_FACTCHECK_ENABLED:true}
      api-key: ${COLLECTOR_GOOGLE_FACTCHECK_API_KEY:}
    
    # Semantic Scholar - AI-powered academic search (free, optional API key)
    # API docs: https://api.semanticscholar.org/api-docs/
    semantic-scholar:
      enabled: ${COLLECTOR_SEMANTIC_SCHOLAR_ENABLED:true}
      api-key: ${COLLECTOR_SEMANTIC_SCHOLAR_API_KEY:}
    
    # PubMed/NCBI - Medical and life sciences (free, optional API key)
    # API docs: https://www.ncbi.nlm.nih.gov/books/NBK25500/
    pubmed:
      enabled: ${COLLECTOR_PUBMED_ENABLED:true}
      api-key: ${COLLECTOR_PUBMED_API_KEY:}
    
    # CORE - Open access research papers (requires free API key)
    # Get API key: https://core.ac.uk/services/api
    core:
      enabled: ${COLLECTOR_CORE_ENABLED:true}
      api-key: ${COLLECTOR_CORE_API_KEY:}
    
    # RRF (Reciprocal Rank Fusion) Configuration
    # 다중 쿼리 병렬 검색 및 결과 융합 설정
    rrf:
      # RRF 기반 검색 활성화 (비활성화 시 기존 순차 검색 사용)
      enabled: ${COLLECTOR_RRF_ENABLED:true}
      # RRF 상수 k (낮을수록 상위 랭크에 가중치, 기본 60)
      k: ${COLLECTOR_RRF_K:60}
      # 최대 병렬 쿼리 수 (의도 분석 + 확장 쿼리)
      max-queries: ${COLLECTOR_RRF_MAX_QUERIES:5}
      # 최대 융합 결과 수
      max-results: ${COLLECTOR_RRF_MAX_RESULTS:50}
      # 최소 관련성 점수 (이 점수 이하의 결과는 필터링)
      min-relevance: ${COLLECTOR_RRF_MIN_RELEVANCE:0.1}
      # URL 실존 여부 검증 활성화 (LLM 환각, 죽은 링크 필터링)
      url-validation-enabled: ${COLLECTOR_RRF_URL_VALIDATION_ENABLED:true}
  
  # ============================================
  # URL Validation Configuration (실존 여부 검증)
  # ============================================
  # URL이 실제로 접근 가능한지, LLM 환각인지, 삭제된 페이지인지 검증합니다.
  url-validation:
    # URL 검증 활성화
    enabled: ${COLLECTOR_URL_VALIDATION_ENABLED:true}
    # HTTP HEAD/GET 요청 타임아웃 (초)
    timeout-seconds: ${COLLECTOR_URL_VALIDATION_TIMEOUT_SECONDS:5}
    # 검증 결과 캐시 TTL (분)
    cache-ttl-minutes: ${COLLECTOR_URL_VALIDATION_CACHE_TTL_MINUTES:30}
  
  # ============================================
  # Evidence Validation Configuration (증거 품질 검증)
  # ============================================
  # 수집된 증거의 품질을 검증하여 불량 데이터를 필터링합니다.
  evidence-validation:
    # 증거 검증 활성화
    enabled: ${COLLECTOR_EVIDENCE_VALIDATION_ENABLED:true}
    # Strict 모드 (URL과 콘텐츠 모두 유효해야 함)
    strict-mode: ${COLLECTOR_EVIDENCE_VALIDATION_STRICT_MODE:false}
    # 최소 콘텐츠 길이 (이하인 경우 삭제된 페이지로 간주)
    min-content-length: ${COLLECTOR_EVIDENCE_VALIDATION_MIN_CONTENT_LENGTH:50}

  # ============================================
  # IP Rotation Configuration (for rate limit bypass)
  # ============================================
  ip-rotation:
    # IP rotation 서비스 활성화
    enabled: ${COLLECTOR_IP_ROTATION_ENABLED:true}
    # IP rotation 서비스 URL
    base-url: ${COLLECTOR_IP_ROTATION_URL:http://ip-rotation:8050}
    # 프록시 요청 타임아웃
    timeout-seconds: ${COLLECTOR_IP_ROTATION_TIMEOUT:15}
    # 최대 재시도 횟수
    max-retries: ${COLLECTOR_IP_ROTATION_MAX_RETRIES:3}
    # 재시도 간 대기 시간 (밀리초)
    retry-delay-ms: ${COLLECTOR_IP_ROTATION_RETRY_DELAY:1000}

  # ============================================
  # Hybrid Search Configuration (Keyword + Semantic + RRF)
  # ============================================
  
  # Embedding Service (HuggingFace Text Embeddings Inference)
  embedding:
    enabled: ${EMBEDDING_ENABLED:true}
    base-url: ${EMBEDDING_BASE_URL:http://localhost:8011}
    model: ${EMBEDDING_MODEL:intfloat/multilingual-e5-large}
    dimension: ${EMBEDDING_DIMENSION:1024}
    timeout-seconds: ${EMBEDDING_TIMEOUT_SECONDS:30}
  
  # Vector Search (PostgreSQL pgvector)
  vector-search:
    enabled: ${VECTOR_SEARCH_ENABLED:true}
    top-k: ${VECTOR_SEARCH_TOP_K:20}
    min-similarity: ${VECTOR_SEARCH_MIN_SIMILARITY:0.5}
  
  # Hybrid Search Orchestration
  hybrid-search:
    enabled: ${HYBRID_SEARCH_ENABLED:true}
    # Number of results from keyword search
    keyword-top-k: ${HYBRID_SEARCH_KEYWORD_TOP_K:30}
    # Number of results from semantic search
    semantic-top-k: ${HYBRID_SEARCH_SEMANTIC_TOP_K:20}
    # Final number of results after RRF fusion
    final-top-k: ${HYBRID_SEARCH_FINAL_TOP_K:20}
    # Source weights for RRF (adjustable)
    keyword-weight: ${HYBRID_SEARCH_KEYWORD_WEIGHT:1.0}
    semantic-weight: ${HYBRID_SEARCH_SEMANTIC_WEIGHT:1.0}

  # Trust Score Configuration
  # Externalized trust scores for fact-check sources and data quality assessment.
  # Hierarchy: Academic (0.95) > Official Stats (0.95) > Encyclopedia (0.90) > Fact Check (0.85)
  trust-scores:
    # Fact-check sources (used by FactCheckSource implementations)
    fact-check:
      crossref: ${TRUST_SCORE_CROSSREF:0.95}            # Academic papers (highest)
      openalex: ${TRUST_SCORE_OPENALEX:0.92}            # Academic database
      wikipedia: ${TRUST_SCORE_WIKIPEDIA:0.90}          # Encyclopedia
      google-fact-check: ${TRUST_SCORE_GOOGLE_FC:0.85}  # Verified fact-check
    
    # Trusted reference sources (used by FactVerificationService)
    trusted:
      wikipedia-ko: ${TRUST_SCORE_WIKI_KO:0.90}         # Korean Wikipedia
      wikipedia-en: ${TRUST_SCORE_WIKI_EN:0.90}         # English Wikipedia
      britannica: ${TRUST_SCORE_BRITANNICA:0.95}        # Britannica (very high)
      namu-wiki: ${TRUST_SCORE_NAMU:0.60}               # Namu Wiki (community)
      kosis: ${TRUST_SCORE_KOSIS:0.95}                  # Korean Statistics (official)
      google-scholar: ${TRUST_SCORE_SCHOLAR:0.85}       # Academic search
    
    # Data quality assessment (used by CollectedDataService)
    data-quality:
      base-score: ${TRUST_SCORE_BASE:0.50}              # Unknown sources
      whitelist-score: ${TRUST_SCORE_WHITELIST:0.90}    # Whitelisted domains
      http-ok-bonus: ${TRUST_SCORE_HTTP_BONUS:0.10}     # Successful HTTP connection
    
    # Custom source scores (add your own)
    # custom:
    #   my-trusted-source: 0.88

# ============================================
# Auto-Crawl Configuration (실시간 확장형 크롤링)
# ============================================
# 
# 검색, 기사, 트렌딩 등에서 자동으로 URL을 발견하고 크롤링합니다.
# autonomous-crawler-service와 연동하여 동작합니다.
#
autocrawl:
  # 자동 크롤링 활성화 여부
  enabled: ${AUTOCRAWL_ENABLED:true}
  
  # 한 번에 처리할 대상 수
  batch-size: ${AUTOCRAWL_BATCH_SIZE:10}
  
  # 도메인당 최대 동시 크롤링 수 (rate limiting)
  max-concurrent-per-domain: ${AUTOCRAWL_MAX_CONCURRENT_PER_DOMAIN:3}
  
  # IN_PROGRESS 상태 타임아웃 (분) - 이 시간 초과 시 PENDING으로 복구
  stuck-timeout-minutes: ${AUTOCRAWL_STUCK_TIMEOUT_MINUTES:30}
  
  # 오래된 완료/실패 대상 정리 기준 (일)
  cleanup-days: ${AUTOCRAWL_CLEANUP_DAYS:7}
  
  # 오래된 PENDING 대상 만료 기준 (일)
  expire-pending-days: ${AUTOCRAWL_EXPIRE_PENDING_DAYS:7}
  
  # 스케줄러 간격 (밀리초)
  queue-interval-ms: ${AUTOCRAWL_QUEUE_INTERVAL_MS:30000}        # 30초마다 큐 처리
  recovery-interval-ms: ${AUTOCRAWL_RECOVERY_INTERVAL_MS:300000}  # 5분마다 복구 체크
  stats-interval-ms: ${AUTOCRAWL_STATS_INTERVAL_MS:600000}        # 10분마다 통계 로깅
  
  # 정리 작업 cron (새벽 3시)
  cleanup-cron: ${AUTOCRAWL_CLEANUP_CRON:0 0 3 * * *}
  
  # URL 발견 소스별 활성화
  discover-from-search: ${AUTOCRAWL_DISCOVER_FROM_SEARCH:true}
  discover-from-articles: ${AUTOCRAWL_DISCOVER_FROM_ARTICLES:true}
  discover-from-deep-search: ${AUTOCRAWL_DISCOVER_FROM_DEEP_SEARCH:true}
  
  # 최소 콘텐츠 길이 (이 이상인 경우에만 링크 추출)
  min-content-length: ${AUTOCRAWL_MIN_CONTENT_LENGTH:200}
  
  # ============================================
  # Seed URL 자동 초기화 설정
  # ============================================
  # docker-compose 시작 시 자동으로 seed URL을 큐에 추가합니다.
  seed:
    # seed 자동 초기화 활성화 (true로 설정 시 시작 시 seed URL 추가)
    enabled: ${AUTOCRAWL_SEED_ENABLED:false}
    
    # 커스텀 seed URL (콤마로 구분, 비어있으면 기본 뉴스 포털 URL 사용)
    urls: ${AUTOCRAWL_SEED_URLS:}
    
    # seed URL에 연결할 키워드
    keywords: ${AUTOCRAWL_SEED_KEYWORDS:뉴스,정치,경제,사회,IT,기술}
    
    # seed URL 우선순위 (0-100, 높을수록 먼저 크롤링)
    priority: ${AUTOCRAWL_SEED_PRIORITY:70}

# ============================================
# Vector Database 설정 (Qdrant)
# ============================================
vector:
  db:
    enabled: ${VECTOR_DB_ENABLED:false}
    url: ${VECTOR_DB_URL:http://localhost:6333}
    collection: ${VECTOR_DB_COLLECTION:factcheck_chat}
  embedding:
    model: ${EMBEDDING_MODEL:text-embedding-ada-002}
    api-key: ${OPENAI_API_KEY:}
    dimension: ${EMBEDDING_DIMENSION:1536}
    timeout-seconds: ${EMBEDDING_TIMEOUT_SECONDS:30}
    max-retry: ${EMBEDDING_MAX_RETRY:3}
    batch-size: ${EMBEDDING_BATCH_SIZE:10}
    local:
      enabled: ${LOCAL_EMBEDDING_ENABLED:false}
      url: ${LOCAL_EMBEDDING_URL:http://localhost:8011}

# ============================================
# 채팅 서비스 설정
# ============================================
chat:
  sync:
    # 동기화 트리거 조건
    min-messages: ${CHAT_SYNC_MIN_MESSAGES:10}
    max-idle-minutes: ${CHAT_SYNC_MAX_IDLE_MINUTES:5}
    # 배치 처리
    batch-size: ${CHAT_SYNC_BATCH_SIZE:50}
    # 재시도
    max-retry: ${CHAT_SYNC_MAX_RETRY:3}
    # 세션 만료
    session-expire-hours: ${CHAT_SESSION_EXPIRE_HOURS:24}
    # 스케줄러 간격
    scheduler:
      interval: ${CHAT_SYNC_SCHEDULER_INTERVAL:300000}
    expire:
      interval: ${CHAT_SYNC_EXPIRE_INTERVAL:3600000}

# ============================================
# 캐시 설정
# ============================================
cache:
  chat-sessions:
    ttl-hours: ${CACHE_CHAT_SESSIONS_TTL_HOURS:2}
  chat-messages:
    ttl-minutes: ${CACHE_CHAT_MESSAGES_TTL_MINUTES:30}
  default:
    ttl-hours: ${CACHE_DEFAULT_TTL_HOURS:24}
  local:
    max-size: ${CACHE_LOCAL_MAX_SIZE:1000}
    ttl-minutes: ${CACHE_LOCAL_TTL_MINUTES:10}

# ============================================
# 비동기 실행자 설정
# ============================================
async:
  executor:
    core-pool-size: ${ASYNC_CORE_POOL_SIZE:5}
    max-pool-size: ${ASYNC_MAX_POOL_SIZE:20}
    queue-capacity: ${ASYNC_QUEUE_CAPACITY:100}
  chat-sync:
    core-pool-size: ${CHAT_SYNC_CORE_POOL_SIZE:3}
    max-pool-size: ${CHAT_SYNC_MAX_POOL_SIZE:10}
    queue-capacity: ${CHAT_SYNC_QUEUE_CAPACITY:50}

# ============================================
# Workspace File Storage Configuration
# ============================================
workspace:
  storage:
    # Local storage path (Docker volume mount point)
    path: ${WORKSPACE_STORAGE_PATH:/data/workspace}
    # Maximum file size in bytes (default: 100MB)
    max-file-size: ${WORKSPACE_MAX_FILE_SIZE:104857600}
    # Maximum files per anonymous session
    max-files-per-session: ${WORKSPACE_MAX_FILES_PER_SESSION:100}
    # Session file TTL in hours (files auto-expire after this)
    session-file-ttl-hours: ${WORKSPACE_SESSION_FILE_TTL_HOURS:24}
  cleanup:
    # Cron for expired file cleanup (3:30 AM daily)
    cron: ${WORKSPACE_CLEANUP_CRON:0 30 3 * * *}
    # Hours before orphaned session files are marked for deletion
    orphan-hours: ${WORKSPACE_ORPHAN_HOURS:48}

# ============================================
# API Key Encryption Configuration
# ============================================
# AES-256-GCM encryption for secure API key storage.
# IMPORTANT: Change these values in production!
newsinsight:
  encryption:
    # Secret key for encryption (must be at least 32 characters for AES-256)
    # In production, use a strong random string and store securely
    secret: ${NEWSINSIGHT_ENCRYPTION_SECRET:defaultSecretKeyForDevelopmentOnly32}
    # Salt for key derivation (must be at least 16 characters)
    salt: ${NEWSINSIGHT_ENCRYPTION_SALT:defaultSaltValue16ch}
    # Enable migration on startup to encrypt existing plain text API keys
    # Set to true once to migrate, then set back to false
    migrate-on-startup: ${NEWSINSIGHT_ENCRYPTION_MIGRATE:false}
