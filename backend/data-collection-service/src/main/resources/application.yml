spring:
  application:
    name: collector-service
  config:
    import: "consul:"
  kafka:
    bootstrap-servers: ${KAFKA_BOOTSTRAP_SERVERS:localhost:9092}
  data:
    mongodb:
      uri: ${MONGODB_URI:mongodb://localhost:27017/newsinsight}
  
  datasource:
    url: jdbc:postgresql://${DB_HOST:localhost}:${DB_PORT:5432}/${DB_NAME:newsinsight}
    username: ${DB_USER:postgres}
    password: ${DB_PASSWORD:postgres}
    driver-class-name: org.postgresql.Driver
    hikari:
      maximum-pool-size: 10
      minimum-idle: 5
      connection-timeout: 30000
      idle-timeout: 600000
      max-lifetime: 1800000
  
  jpa:
    hibernate:
      ddl-auto: update
    show-sql: ${JPA_SHOW_SQL:false}
    properties:
      hibernate:
        dialect: org.hibernate.dialect.PostgreSQLDialect
        format_sql: true
        jdbc:
          batch_size: 20
        order_inserts: true
        order_updates: true
    open-in-view: false
  
  cloud:
    consul:
      enabled: ${CONSUL_ENABLED:true}
      host: ${CONSUL_HOST:localhost}
      port: ${CONSUL_PORT:8500}
      discovery:
        enabled: true
        instance-id: ${spring.application.name}:${random.value}
        service-name: ${spring.application.name}
        prefer-ip-address: false
        hostname: ${CONSUL_DISCOVERY_HOSTNAME:collector-service}
        health-check-path: /actuator/health
        health-check-interval: 10s
      config:
        enabled: true
        prefix: config
        default-context: ${spring.application.name}
        profile-separator: '::'
        format: PROPERTIES
  
  task:
    execution:
      pool:
        core-size: 5
        max-size: 20
        queue-capacity: 100
      thread-name-prefix: async-task-

  flyway:
    enabled: ${SPRING_FLYWAY_ENABLED:false}

server:
  port: ${SERVER_PORT:8081}
  servlet:
    context-path: /
  compression:
    enabled: true
  error:
    include-message: always
    include-binding-errors: always

management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics,prometheus
  endpoint:
    health:
      show-details: always
  health:
    consul:
      enabled: ${CONSUL_ENABLED:true}

logging:
  level:
    root: INFO
    com.newsinsight: ${LOG_LEVEL:DEBUG}
    org.springframework.web: INFO
    org.hibernate.SQL: ${JPA_SHOW_SQL:false}
    org.hibernate.type.descriptor.sql.BasicBinder: ${JPA_SHOW_SQL:false}
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss} - %msg%n"

# Application specific configuration
collector:
  scheduling:
    enabled: ${SCHEDULING_ENABLED:true}
    # Cron expression: run every hour
    cron: ${COLLECTION_CRON:0 0 * * * ?}
  
  http:
    user-agent: ${HTTP_USER_AGENT:NewsInsight-Collector/1.0}
    timeout:
      connect: ${HTTP_CONNECT_TIMEOUT:10000}
      read: ${HTTP_READ_TIMEOUT:30000}
    max-redirects: 5
  
  rss:
    enabled: true
    fetch-timeout: 30000
  
  web-scraper:
    enabled: true
    max-content-length: 1048576  # 1MB
    respect-robots-txt: true
  
  collection:
    max-concurrent: ${MAX_CONCURRENT_COLLECTIONS:5}
    retry:
      max-attempts: 3
      backoff-delay: 1000
    cleanup:
      old-jobs-days: 30
  
  quality-assurance:
    min-content-length: ${QA_MIN_CONTENT_LENGTH:50}
    enable-network-checks: ${QA_ENABLE_NETWORK_CHECKS:false}
    domain-whitelist: ${QA_DOMAIN_WHITELIST:example.com,news.example.com}
    expected-keywords: ${QA_EXPECTED_KEYWORDS:news,article,report}

  ai:
    topic:
      request: ${COLLECTOR_AI_REQUEST_TOPIC:newsinsight.ai.requests}
      response: ${COLLECTOR_AI_RESPONSE_TOPIC:newsinsight.ai.responses}
    default-provider-id: ${COLLECTOR_AI_PROVIDER_ID:openai}
    default-model-id: ${COLLECTOR_AI_MODEL_ID:gpt-4.1}
    
    # AI Orchestration Configuration (Multi-provider Job Management)
    orchestration:
      # Kafka topic for AI task requests
      topic: ${COLLECTOR_AI_ORCHESTRATION_TOPIC:ai.tasks.requests}
      # Base URL for callback (this service's public URL)
      callback-base-url: ${COLLECTOR_AI_ORCHESTRATION_CALLBACK_BASE_URL:${collector.deep-search.callback-base-url}}
      # Token for validating callbacks from workers/n8n
      callback-token: ${COLLECTOR_AI_ORCHESTRATION_CALLBACK_TOKEN:${collector.deep-search.callback-token}}
      # Minutes before marking a job as timed out
      timeout-minutes: ${COLLECTOR_AI_ORCHESTRATION_TIMEOUT_MINUTES:30}
      # Interval for checking timed out jobs (milliseconds)
      timeout-check-interval: ${COLLECTOR_AI_ORCHESTRATION_TIMEOUT_CHECK_INTERVAL:300000}
      # Days to keep completed jobs before cleanup
      cleanup-days: ${COLLECTOR_AI_ORCHESTRATION_CLEANUP_DAYS:7}
      # Cron expression for cleanup job (3 AM daily)
      cleanup-cron: ${COLLECTOR_AI_ORCHESTRATION_CLEANUP_CRON:0 0 3 * * ?}
      # n8n webhook base URL for AI agents
      n8n-base-url: ${COLLECTOR_AI_N8N_BASE_URL:https://n8n.nodove.com/webhook}

  crawl:
    topic:
      command: ${COLLECTOR_CRAWL_COMMAND_TOPIC:newsinsight.crawl.commands}
      result: ${COLLECTOR_CRAWL_RESULT_TOPIC:newsinsight.crawl.results}

  crawler:
    enabled: ${COLLECTOR_SERVICE_CRAWLER_ENABLED:false}
    base-url: ${COLLECTOR_SERVICE_CRAWLER_BASE_URL:http://web-crawler:11235}

  # Deep AI Search Configuration (n8n Crawl Agent Workflow)
  # These settings can be managed via Consul KV under config/collector-service/
  # Example Consul keys:
  #   config/collector-service/DEEP_SEARCH_ENABLED=true
  #   config/collector-service/DEEP_SEARCH_WEBHOOK_URL=https://n8n.nodove.com/webhook/crawl-agent
  deep-search:
    enabled: ${COLLECTOR_SERVICE_DEEP_SEARCH_ENABLED:${DEEP_SEARCH_ENABLED:true}}
    # n8n webhook URL for crawl-agent workflow
    webhook-url: ${COLLECTOR_SERVICE_DEEP_SEARCH_WEBHOOK_URL:${DEEP_SEARCH_WEBHOOK_URL:https://n8n.nodove.com/webhook/crawl-agent}}
    # Base URL for callback (this service's public URL)
    callback-base-url: ${COLLECTOR_SERVICE_DEEP_SEARCH_CALLBACK_BASE_URL:${DEEP_SEARCH_CALLBACK_BASE_URL:http://localhost:8081}}
    # Token for validating callbacks from n8n
    callback-token: ${COLLECTOR_SERVICE_DEEP_SEARCH_CALLBACK_TOKEN:${DEEP_SEARCH_CALLBACK_TOKEN:}}
    # Timeout for webhook requests in seconds
    timeout-seconds: ${COLLECTOR_SERVICE_DEEP_SEARCH_TIMEOUT_SECONDS:${DEEP_SEARCH_TIMEOUT_SECONDS:120}}
    # Minutes before marking a job as timed out
    timeout-minutes: ${COLLECTOR_SERVICE_DEEP_SEARCH_TIMEOUT_MINUTES:${DEEP_SEARCH_TIMEOUT_MINUTES:30}}
    # Interval for checking timed out jobs (milliseconds)
    timeout-check-interval: ${COLLECTOR_SERVICE_DEEP_SEARCH_TIMEOUT_CHECK_INTERVAL:${DEEP_SEARCH_TIMEOUT_CHECK_INTERVAL:300000}}
    # Days to keep completed jobs before cleanup
    cleanup-days: ${COLLECTOR_SERVICE_DEEP_SEARCH_CLEANUP_DAYS:${DEEP_SEARCH_CLEANUP_DAYS:7}}
    # Cron expression for cleanup job (3 AM daily)
    cleanup-cron: ${COLLECTOR_SERVICE_DEEP_SEARCH_CLEANUP_CRON:${DEEP_SEARCH_CLEANUP_CRON:0 0 3 * * ?}}
